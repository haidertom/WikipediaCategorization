{"id": "50401410", "url": "https://en.wikipedia.org/wiki?curid=50401410", "title": "3 by 5 Initiative", "text": "3 by 5 Initiative\n\nThe 3 by 5 Initiative was an initiative of the World Health Organization (WHO) to provide antiretroviral treatment to patients with HIV/AIDS in low- and middle-income countries. The program lasted from December 2003 to December 2005, and the name \"3 by 5\" refers to the goal of treating 3 million people by 2005. The 3 by 5 Initiative is seen as the beginning of the scaling of antiretroviral treatments, and evaluations of the Initiative take this into account.\n\nThe 3 by 5 Initiative was guided by five \"strategic pillars\":\n\nAn evaluation team commissioned by the Canadian International Development Agency (CIDA) and WHO reviewed the 3 by 5 initiative between July 2005 and March 2006. The evaluation team published their report in March 2006, titled \"Progress on global access to HIV antiretroviral therapy: a report on \"3 by 5\" and beyond\". It found that 1.3 million people were receiving antiretroviral treatment in the target countries by December 2005, up from 400,000 before the start of the initiative. The evaluation team also found that many of the worst-affected countries were still far from containing their AIDS crises. It also credits the 3 by 5 Initiative with establishing antiretroviral treatment as an essential public health intervention.\n\nThe evaluation team attributes some of the difficulties in achieving the goals of the Initiative to an initial lack of secured funding (resulting in delays) and an unstable management of the HIV/AIDS program (due to the high turnover rate of WHO directors).\n\nThe 3 by 5 Initiative has been criticized by Arthur J. Ammann for focusing only on treatment of HIV (instead of also focusing on testing and prevention), for the high costs of the program, and for being \"top down\" instead of allowing target countries to take charge of their own AIDS programs.\n\n"}
{"id": "52433221", "url": "https://en.wikipedia.org/wiki?curid=52433221", "title": "Alison Tedstone", "text": "Alison Tedstone\n\nAlison Tedstone RNutr FAfN (born April 1961) is Chief Nutritionist (National Director of Diet & Obesity) at Public Health England (PHE).\n\nShe was born Alison Tedstone. She has a BSc and PhD from the University of London. She conducted research into nutrition at the University of Oxford.\n\nShe is a Registered Public Health Nutritionist. She is a founding fellow of the Association for Nutrition (AfN).\n\nShe was an academic at the London School of Hygiene & Tropical Medicine.\n\nShe joined the Food Standards Agency (FSA) in 2001, working there until 2010, becoming Head of Nutrition Science.\n\nShe became Chief Nutritionist at PHE. Work includes the National Diet and Nutrition Survey. she gives evidence to the Scientific Advisory Committee on Nutrition (SACN).\n\nShe married Alan Penn in the 1980s in Worcestershire.\n\n\n"}
{"id": "23640750", "url": "https://en.wikipedia.org/wiki?curid=23640750", "title": "Antibiotic misuse", "text": "Antibiotic misuse\n\nAntibiotic misuse, sometimes called antibiotic abuse or antibiotic overuse, refers to the misuse or overuse of antibiotics, with potentially serious effects on health. It is a contributing factor to the development of antibiotic resistance, including the creation of multidrug-resistant bacteria, informally called \"super bugs\": relatively harmless bacteria (such as staphylococcus, enterococcus and acinetobacter) can develop resistance to multiple antibiotics and cause life-threatening infections.\n\nAntibiotics have been around since 1928 when penicillin was discovered by Alexander Fleming. In the 1980s, antibiotics that were determined medically important for treatment of animals could be approved under veterinary oversight. In 1996, the National Antimicrobial Resistance Monitoring System (NARMS) was established. Starting in 2010, publications regarding antimicrobial drugs in food become an annual report. Starting in 2012, there was publicly solicited input on how data is to be collected and reported for matters relating to the use of antimicrobials for food-producing animals. Resulting from this, the FDA revised its sampling structure within NARMS with the goal of obtaining more representative livestock data for the key organisms under surveillance. “NARMS partners at CDC and USDA have published over 150 peer-reviewed research articles examining the nature and magnitude of antimicrobial resistance hazards associated with antibiotic use in food-producing animals.” In 2014, the FDA began working with the United States Department of Agriculture (USDA) and the Centers of Disease Control and Prevention (CDC) to explore additional mechanisms to obtain data that is representative of antibiotic use in food-producing animals. In 2015, the FDA issues the Veterinary Feed Directive (VFD) final rule. Under this rule veterinarians must authorize the use of antimicrobials within feed for the animals they serve.\n\nCommon situations in which antibiotics are overused include the following:\n\nAntibiotics can cause severe reactions and add significantly to the cost of care. In the United States, antibiotics and anti-infectives are the leading cause of adverse effect from drugs. In a study of 32 States in 2011, antibiotics and anti-infectives accounted for nearly 24 percent of ADEs that were present on admission, and 28 percent of those that occurred during a hospital stay.\n\nIf antimicrobial resistance continues to increase from current levels, it is estimated that by 2050 ten million people would die every year due to lack of available treatment and the world’s GDP would be 2 - 3.5% lower in 2050. If worldwide action is not taken to combat antibiotic misuse and the development of antimicrobial resistance, from 2014 - 2050 it is estimated that 300 million people could die prematurely due to drug resistance and $60 – 100 trillion of economic output would be lost. If the current worldwide development of antimicrobial resistance is delayed by just 10 years, $65 trillion of the world’s GDP output can be saved from 2014-2050.\n\nPrescribing by an infectious disease specialist compared with prescribing by a non-infectious disease specialist decreases antibiotic consumption and reduces costs.\n\nThough antibiotics are required to treat severe bacterial infections, misuse has contributed to a rise in bacterial resistance. The overuse of fluoroquinolone and other antibiotics fuels antibiotic resistance in bacteria, which can inhibit the treatment of antibiotic-resistant infections. Their excessive use in children with otitis media has given rise to a breed of bacteria resistant to antibiotics entirely. Additionally, the use of antimicrobial substances in building materials and personal care products has contributed to a higher percentage of antibiotic resistant bacteria in the indoor environment, where humans spend a large majority of their lives.\n\nWidespread use of fluoroquinolones as a first-line antibiotic has led to decreased antibiotic sensitivity, with negative implications for serious bacterial infections such as those associated with cystic fibrosis, where quinolones are among the few viable antibiotics.\n\nAntibiotics have no effect on viral infections such as the common cold. They are also ineffective against sore throats, which are usually viral and self-resolving. Most cases of bronchitis (90–95%) are viral as well, passing after a few weeks—the use of antibiotics against bronchitis is superfluous and can put the patient at risk of suffering adverse reactions.\n\nOfficial guidelines by the American Heart Association for dental antibiotic prophylaxis call for the administration of antibiotics to prevent infective endocarditis. Though the current (2007) guidelines dictate more restricted antibiotic use, many dentists and dental patients follow the 1997 guidelines instead, leading to overuse of antibiotics.\n\nA study by Imperial College London in February 2017 found that of 20 online websites, 9 would provide antibiotics (illegally) without a prescription to UK residents.\n\nThere has been massive use of antibiotics in animal husbandry. The most abundant use of antimicrobials worldwide is in livestock; they are typically distributed in animal feed or water for purposes such as disease prevention and growth promotion.\nDebates have arisen surrounding the extent of the impact of these antibiotics, particularly antimicrobial growth promoters, on human antibiotic resistance. Although some sources assert that there remains a lack of knowledge on which antibiotic use generates the most risk to humans, policies and regulations have been placed to limit any harmful effects, such as the potential of bacteria developing antibiotic resistance within livestock, and that bacteria transferring resistance genes to human pathogens \nOn January 1, 2017, the FDA enacted legislation to require that all human medically important feed-grade antibiotics (many prior over-the-counter-drugs) become classified as Veterinary Feed Directive drugs (VFD). This action requires that farmers establish and work with veterinaries to receive a written VFD order. The effect of this act places a requirement on an established veterinarian-client-patient relationship (VCPR). Through this relationship, farmers will receive an increased education in the form of advice and guidance from their veterinarian.\n\n\n"}
{"id": "1914", "url": "https://en.wikipedia.org/wiki?curid=1914", "title": "Antimicrobial resistance", "text": "Antimicrobial resistance\n\nAntimicrobial resistance (AMR or AR) is the ability of a microbe to resist the effects of medication that once could successfully treat the microbe. The term antibiotic resistance (AR or ABR) is a subset of AMR, as it applies only to bacteria becoming resistant to antibiotics. Resistant microbes are more difficult to treat, requiring alternative medications or higher doses of antimicrobials. These approaches may be more expensive, more toxic or both. Microbes resistant to multiple antimicrobials are called multidrug resistant (MDR). Those considered extensively drug resistant (XDR) or totally drug resistant (TDR) are sometimes called \"superbugs\".\n\nResistance arises through one of three mechanisms: natural resistance in certain types of bacteria, genetic mutation, or by one species acquiring resistance from another. All classes of microbes can develop resistance. Fungi develop antifungal resistance. Viruses develop antiviral resistance. Protozoa develop antiprotozoal resistance, and bacteria develop antibiotic resistance. Resistance can appear spontaneously because of random mutations. However, extended use of antimicrobials appears to encourage selection for mutations which can render antimicrobials ineffective.\nPreventive measures include only using antibiotics when needed, thereby stopping misuse of antibiotics or antimicrobials. Narrow-spectrum antibiotics are preferred over broad-spectrum antibiotics when possible, as effectively and accurately targeting specific organisms is less likely to cause resistance. For people who take these medications at home, education about proper use is essential. Health care providers can minimize spread of resistant infections by use of proper sanitation and hygiene, including handwashing and disinfecting between patients, and should encourage the same of the patient, visitors, and family members.\nRising drug resistance is caused mainly by use of antimicrobials in humans and other animals, and spread of resistant strains between the two. Growing resistance has also been linked to dumping of inadequately treated effluents from the pharmaceutical industry, especially in countries where bulk drugs are manufactured. Antibiotics increase selective pressure in bacterial populations, causing vulnerable bacteria to die; this increases the percentage of resistant bacteria which continue growing. Even at very low levels of antibiotic, resistant bacteria can have a growth advantage and grow faster than vulnerable bacteria. With resistance to antibiotics becoming more common there is greater need for alternative treatments. Calls for new antibiotic therapies have been issued, but new drug development is becoming rarer.\n\nAntimicrobial resistance is increasing globally because of greater access to antibiotic drugs in developing countries. Estimates are that 700,000 to several million deaths result per year. Each year in the United States, at least 2 million people become infected with bacteria that are resistant to antibiotics and at least 23,000 people die as a result. There are public calls for global collective action to address the threat that include proposals for international treaties on antimicrobial resistance. Worldwide antibiotic resistance is not completely identified, but poorer countries with weaker healthcare systems are more affected.\n\nThe WHO defines antimicrobial resistance as a microorganism's resistance to an antimicrobial drug that was once able to treat an infection by that microorganism.\nA person cannot become resistant to antibiotics. Resistance is a property of the microbe, not a person or other organism infected by a microbe.\n\nA World Health Organization (WHO) report released April 2014 stated, \"this serious threat is no longer a prediction for the future, it is happening right now in every region of the world and has the potential to affect anyone, of any age, in any country. Antibiotic resistance—when bacteria change so antibiotics no longer work in people who need them to treat infections—is now a major threat to public health.\"\n\nBacteria with resistance to antibiotics predate medical use of antibiotics by humans. However, widespread antibiotic use has made more bacteria resistant through the process of evolutionary pressure.\n\nReasons for the widespread use of antibiotics in human medicine include:\n\nOther causes include:\n\nIncreasing bacterial resistance is linked with the volume of antibiotic prescribed, as well as missing doses when taking antibiotics. Inappropriate prescribing of antibiotics has been attributed to a number of causes, such as patients insisting on antibiotics and physicians prescribing them as they do not have time to explain why they are not necessary. Another cause can be physicians not knowing when to prescribe antibiotics or being overly cautious for medical or legal reasons. For example, 70 to 80 percent of diarrhea is caused by viral pathogens, for which antibiotics are not effective. But nevertheless, around 40 percent of these cases are attempted to be treated with antibiotics. In some areas even over 80 percent of such cases are attempted to be treated with antibiotics.\n\nLower antibiotic concentration contributes to the increase of AMR by introducing more mutations that support bacterial growth in higher antibiotic concentration. For example, sub-inhibitory concentration have induced genetic mutation in bacteria such as \"Pseudomonas aeruginosa\" and \"Bacteroides fragilis\".\n\nUp to half of antibiotics used in humans are unnecessary and inappropriate. For example, a third of people believe that antibiotics are effective for the common cold, and the common cold is the most common reason antibiotics are prescribed even though antibiotics are useless against viruses. A single regimen of antibiotics even in compliant individuals leads to a greater risk of resistant organisms to that antibiotic in the person for a month to possibly a year.\n\nAntibiotic resistance increases with duration of treatment. Therefore, as long as an effective minimum is kept, shorter courses of antibiotics are likely to decrease rates of resistance, reduce cost, and have better outcomes with fewer complications. Short course regimens exist for community-acquired pneumonia spontaneous bacterial peritonitis, suspected lung infections in intense care wards, so-called acute abdomen, middle ear infections, sinusitis and throat infections, and penetrating gut injuries. In some situations a short course may not cure the infection as well as a long course. A BMJ editorial recommended that antibiotics can often be safely stopped 72 hours after symptoms resolve.\n\nBecause individuals may feel better before the infection is eradicated, doctors must provide instructions to them so they know when it is safe to stop taking a prescription. Some researchers advocate doctors' using a very short course of antibiotics, reevaluating the patient after a few days, and stopping treatment if there are no clinical signs of infection.\n\nCertain antibiotic classes result in resistance more than others. Increased rates of MRSA infections are seen when using glycopeptides, cephalosporins, and quinolone antibiotics. Cephalosporins, and particularly quinolones and clindamycin, are more likely to produce colonisation with \"Clostridium difficile\".\n\nFactors within the intensive care unit setting such as mechanical ventilation and multiple underlying diseases also appear to contribute to bacterial resistance. Poor hand hygiene by hospital staff has been associated with the spread of resistant organisms.\n\nThe World Health Organization concluded that inappropriate use of antibiotics in animal husbandry is an underlying contributor to the emergence and spread of antibiotic-resistant germs, and that the use of antibiotics as growth promoters in animal feeds should be restricted. The World Organisation for Animal Health has added to the Terrestrial Animal Health Code a series of guidelines with recommendations to its members for the creation and harmonization of national antimicrobial resistance surveillance and monitoring programs, monitoring of the quantities of antibiotics used in animal husbandry, and recommendations to ensure the proper and prudent use of antibiotic substances. Another guideline is to implement methodologies that help to establish associated risk factors and assess the risk of antibiotic resistance.\n\nNaturally occurring antibiotic resistance is common. Genes for resistance to antibiotics, like antibiotics themselves, are ancient. The genes that confer resistance are known as the environmental resistome. These genes may be transferred from non-disease-causing bacteria to those that do cause disease, leading to clinically significant antibiotic resistance.\n\nIn 1952 it was shown that penicillin-resistant bacteria existed before penicillin treatment; and also preexistent bacterial resistance to streptomycin. In 1962, the presence of penicillinase was detected in dormant endospores of \"Bacillus licheniformis\", revived from dried soil on the roots of plants, preserved since 1689 in the British Museum. Six strains of \"Clostridium\", found in the bowels of William Braine and John Hartnell (members of the Franklin Expedition) showed resistance to cefoxitin and clindamycin.\n\nPenicillinase may have emerged as a defense mechanism for bacteria in their habitats, such as the case of penicillinase-rich \"Staphylococcus aureus\", living with penicillin-producing \"Trichophyton\"; however, this may be circumstantial. Search for a penicillinase ancestor has focused on the class of proteins that must be \"a priori\" capable of specific combination with penicillin. The resistance to cefoxitin and clindamycin in turn was attributed to Braine's and Hartnell's contact with microorganisms that naturally produce them or random mutation in the chromosomes of \"Clostridium\" strains.\n\nThere is evidence that heavy metals and other pollutants may select for antibiotic-resistant bacteria, generating a constant source of them in small numbers.\n\nAntibiotic resistance is a growing problem among humans and wildlife in terrestrial or aquatic environments. In this respect, the spread and contamination of the environment, especially through water pollution \"hot spots\" such as hospital wastewater and untreated urban wastewater, is a growing and serious public health problem. Antibiotics have been polluting the environment since their introduction through human waste (medication, farming), animals, and the pharmaceutical industry. The contribution of the pharmaceutical industry is so significant that parallels can be drawn between countries with highest rate of increasing antibiotic resistance and countries with largest footprint of pharmaceutical industry. China, which contributes to nearly 90 per cent of the world's active pharmaceutical ingredient (API) manufacturing, has seen a 22 per cent increase in rate of antimicrobial resistance in six years, compared to a 6 per cent increase in the United States.\n\nAlong with antibiotic waste, resistant bacteria follow, thus introducing antibiotic-resistant bacteria into the environment. Already in 2011, mapping of sewage and water supply samples in New Delhi showed widespread and uncontrolled infection as indicated by the presence of NDM-1-positive enteric bacteria (New Delhi metallo-beta-lactamase 1).\n\nAs bacteria replicate quickly, the resistant bacteria that enter water bodies through wastewater replicate their resistance genes as they continue to divide. In addition, bacteria carrying resistance genes have the ability to spread those genes to other species via horizontal gene transfer. Therefore, even if the specific antibiotic is no longer introduced into the environment, antibiotic-resistance genes will persist through the bacteria that have since replicated without continuous exposure. Antibiotic resistance is widespread in marine vertebrates, and they may be important reservoirs of antibiotic-resistant bacteria in the marine environment.\n\nThere have been increasing public calls for global collective action to address the threat, including a proposal for international treaty on antimicrobial resistance. Further detail and attention is still needed in order to recognize and measure trends in resistance on the international level; the idea of a global tracking system has been suggested but implementation has yet to occur. A system of this nature would provide insight to areas of high resistance as well as information necessary for evaluation of programs and other changes made to fight or reverse antibiotic resistance.\n\nAntibiotic treatment duration should be based on the infection and other health problems a person may have. For many infections once a person has improved there is little evidence that stopping treatment causes more resistance. Some therefore feel that stopping early may be reasonable in some cases. Other infections, however, do require long courses regardless of whether a person feels better.\n\nThere are multiple national and international monitoring programs for drug-resistant threats, including methicillin-resistant \"Staphylococcus aureus\" (MRSA), vancomycin-resistant \"S. aureus\" (VRSA), extended spectrum beta-lactamase (ESBL), vancomycin-resistant \"Enterococcus\" (VRE), multidrug-resistant \"A. baumannii\" (MRAB).\n\nResistanceOpen is an online global map of antimicrobial resistance developed by HealthMap which displays aggregated data on antimicrobial resistance from publicly available and user submitted data. The website can display data for a 25-mile radius from a location. Users may submit data from antibiograms for individual hospitals or laboratories. European data is from the EARS-Net (European Antimicrobial Resistance Surveillance Network), part of the ECDC.\n\nResistanceMap is a website by the Center for Disease Dynamics, Economics & Policy and provides data on antimicrobial resistance on a global level.\n\nAntibiotic stewardship programmes appear useful in reducing rates of antibiotic resistance.\n\nExcessive antibiotic use has become one of the top contributors to the development of antibiotic resistance. Since the beginning of the antibiotic era, antibiotics have been used to treat a wide range of disease. Overuse of antibiotics has become the primary cause of rising levels of antibiotic resistance. The main problem is that doctors are willing to prescribe antibiotics to ill-informed individuals who believe that antibiotics can cure nearly all illnesses, including viral infections like the common cold. In an analysis of drug prescriptions, 36% of individuals with a cold or an upper respiratory infection (both viral in origin) were given prescriptions for antibiotics. These prescriptions accomplished nothing other than increasing the risk of further evolution of antibiotic resistant bacteria.\n\nAntimicrobial stewardship teams in hospitals are encouraging optimal use of antimicrobials. The goals of antimicrobial stewardship are to help practitioners pick the right drug at the right dose and duration of therapy while preventing misuse and minimizing the development of resistance. Stewardship may reduce the length of stay by an average of slightly over 1 day while not increasing the risk of death.\n\nGiven the volume of care provided in primary care (General Practice), recent strategies have focused on reducing unnecessary antibiotic prescribing in this setting. Simple interventions, such as written information explaining the futility of antibiotics for common infections such as upper respiratory tract infections, have been shown to reduce antibiotic prescribing.\n\nThe prescriber should closely adhere to the five rights of drug administration: the right patient, the right drug, the right dose, the right route, and the right time.\n\nCultures should be taken before treatment when indicated and treatment potentially changed based on the susceptibility report.\n\nAbout a third of antibiotic prescriptions written in outpatient settings in the United States were not appropriate in 2010 and 2011. Doctors in the U.S. wrote 506 annual antibiotic scripts for every 1,000 people, with 353 being medically necessary.\n\nHealth workers and pharmacists can help tackle resistance by: enhancing infection prevention and control; only prescribing and dispensing antibiotics when they are truly needed; prescribing and dispensing the right antibiotic(s) to treat the illness.\n\nPeople can help tackle resistance by using antibiotics only when prescribed by a doctor; completing the full prescription, even if they feel better; never sharing antibiotics with others or using leftover prescriptions.\n\n\nInfectious disease control through improved water, sanitation and hygiene (WASH) infrastructure needs to be included in the antimicrobial resistance (AMR) agenda. The \"Interagency Coordination Group on Antimicrobial Resistance\" stated in 2018 that \"the spread of pathogens through unsafe water results in a high burden of gastrointestinal disease, increasing even further the need for antibiotic treatment.\" This is particularly a problem in developing countries where the spread of infectious diseases caused by inadequate WASH standards is a major driver of antibiotic demand. Growing usage of antibiotics together with persistent infectious disease levels have led to a dangerous cycle in which reliance on antimicrobials increases while the efficacy of drugs diminishes. The proper use of infrastructure for water, sanitation and hygiene (WASH) can result in a 47–72 percent decrease of diarrhea cases treated with antibiotics depending on the type of intervention and its effectiveness. A reduction of the diarrhea disease burden through improved infrastructure would result in large decreases in the number of diarrhea cases treated with antibiotics. This was estimated as ranging from 5 million in Brazil to up to 590 million in India by the year 2030. The strong link between increased consumption and resistance indicates that this will directly mitigate the accelerating spread of AMR. Sanitation and water for all by 2030 is Goal Number 6 of the Sustainable Development Goals.\n\nAn increase in hand washing compliance by hospital staff results in decreased rates of resistant organisms.\n\nWater supply and sanitation infrastructure in health facilities offer significant co-benefits for combatting AMR, and investment should be increased. There is much room for improvement: WHO and UNICEF estimated in 2015 that globally 38% of health facilities did not have a source of water, nearly 19% had no toilets and 35% had no water and soap or alcohol-based hand rub for handwashing.\n\nManufacturers of antimicrobials need to improve the treatment of their wastewater (by using industrial wastewater treatment processes) to reduce the release of residues into the environment.\n\nIn 1997, European Union health ministers voted to ban avoparcin and four additional antibiotics used to promote animal growth in 1999. In 2006 a ban on the use of antibiotics in European feed, with the exception of two antibiotics in poultry feeds, became effective. In Scandinavia, there is evidence that the ban has led to a lower prevalence of antibiotic resistance in (nonhazardous) animal bacterial populations. As of 2004, several European countries established a decline of antimicrobial resistance in humans through limiting the usage antimicrobials in agriculture and food industries without jeopardizing animal health or economic cost.\n\nThe United States Department of Agriculture (USDA) and the Food and Drug Administration (FDA) collect data on antibiotic use in humans and in a more limited fashion in animals.\nThe FDA first determined in 1977 that there is evidence of emergence of antibiotic-resistant bacterial strains in livestock. The long-established practice of permitting OTC sales of antibiotics (including penicillin and other drugs) to lay animal owners for administration to their own animals nonetheless continued in all states.\nIn 2000, the FDA announced their intention to revoke approval of fluoroquinolone use in poultry production because of substantial evidence linking it to the emergence of fluoroquinolone-resistant \"Campylobacter\" infections in humans. Legal challenges from the food animal and pharmaceutical industries delayed the final decision to do so until 2006. Fluroquinolones have been banned from extra-label use in food animals in the USA since 2007. However, they remain widely used in companion and exotic animals.\n\nThe increasing interconnectedness of the world and the fact that new classes of antibiotics have not been developed and approved for more than 25 years highlight the extent to which antimicrobial resistance is a global health challenge. A global action plan to tackle the growing problem of resistance to antibiotics and other antimicrobial medicines was endorsed at the Sixty-eighth World Health Assembly in May 2015. One of the key objectives of the plan is to improve awareness and understanding of antimicrobial resistance through effective communication, education and training. This global action plan developed by the World Health Organization was created to combat the issue of antimicrobial resistance and was guided by the advice of countries and key stakeholders. The WHO's global action plan is composed of five key objectives that can be targeted through different means, and represents countries coming together to solve a major problem that can have future health consequences.\n\n\nThe World Health Organization has promoted the first World Antibiotic Awareness Week running from 16–22 November 2015. The aim of the week is to increase global awareness of antibiotic resistance. It also wants to promote the correct usage of antibiotics across all fields in order to prevent further instances of antibiotic resistance.\n\nWorld Antibiotic Awareness Week has been held every November since 2015. For 2017, the Food and Agriculture Organization of the United Nations (FAO), the World Health Organization (WHO) and the World Organisation for Animal Health (OIE) are together calling for responsible use of antibiotics in humans and animals to reduce the emergence of antibiotic resistance.\n\nThe four main mechanisms by which microorganisms exhibit resistance to antimicrobials are:\nAntibiotic resistance can be a result of horizontal gene transfer, and also of unlinked point mutations in the pathogen genome at a rate of about 1 in 10 per chromosomal replication. Mutations are rare but the fact that bacteria reproduce at such a high rate allows for the effect to be significant. A mutation may produce a change in the binding site of the antibiotic, which may allow the site to continue proper functioning in the presence of the antibiotic or prevent the binding of the antibiotic to the site altogether.\n\nAntibiotic action against a pathogen can be seen as an environmental pressure. Those bacteria with a mutation that allows them to survive will reproduce, pass the trait to their offspring, which leads to the microevolution of a fully resistant colony. Chromosomal mutations providing antibiotic resistance benefit the bacteria but also confer a cost of fitness. For example, a ribosomal mutation may protect a bacterial cell by changing the binding site of an antibiotic but will also slow protein synthesis. manifesting, in slower growth rate.\n\nIn Gram-negative bacteria, plasmid-mediated resistance genes produce proteins that can bind to DNA gyrase, protecting it from the action of quinolones. Finally, mutations at key sites in DNA gyrase or topoisomerase IV can decrease their binding affinity to quinolones, decreasing the drug's effectiveness.\n\nBacteria can often develop antibiotic resistance. Mutations that confer increased survival are selected for in natural selection, which can happen quickly in bacteria because lifespans and production of new generations can be on a timescale of mere hours. A new (de novo) mutation in a parent cell can quickly become an inherited mutation of widespread prevalence. Moreover, some adaptive mutations can propagate not only through inheritance but also through horizontal gene transfer. Very often this is done via plasmids, however, through means of Transduction (genetics), Transformation (genetics) and chromosomal Conjugation (genetics), resistance genes residing on bacterial chromosomes can also be transferred. If the new DNA is maintained in the receiving bacterium, this transfer is followed by inheritance of the new resistance from parents to offspring.\n\nRecent findings show no necessity of large populations of bacteria for the appearance of antibiotic resistance. Small populations of \"E. coli\" in an antibiotic gradient can become resistant. Any heterogeneous environment with respect to nutrient and antibiotic gradients may facilitate antibiotic resistance in small bacterial populations. Researchers hypothesize that the mechanism of resistance development is based on four SNP mutations in the genome of \"E. coli\" produced by the gradient of antibiotic.\n\nAntibiotic resistance can be introduced artificially into a microorganism through laboratory protocols, sometimes used as a selectable marker to examine the mechanisms of gene transfer or to identify individuals that absorbed a piece of DNA that included the resistance gene and another gene of interest.\n\nNew Delhi metallo-beta-lactamase 1 (NDM-1) is an enzyme that makes bacteria resistant to a broad range of beta-lactam antibiotics. The most common bacteria that make this enzyme are gram-negative such as \"Escherichia coli\" and \"Klebsiella pneumoniae\", but the gene for NDM-1 can spread from one strain of bacteria to another by horizontal gene transfer.\n\nSpecific antiviral drugs are used to treat some viral infections. These drugs prevent viruses from reproducing by inhibiting essential stages of the virus's replication cycle in infected cells. Antivirals are used to treat HIV, hepatitis B, hepatitis C, influenza, herpes viruses including varicella zoster virus, cytomegalovirus and Epstein-Barr virus. With each virus, some strains have become resistant to the administered drugs.\n\nResistance to HIV antivirals is problematic, and even multi-drug resistant strains have evolved. Resistant strains of the HIV virus emerge rapidly if only one antiviral drug is used. Using three or more drugs together has helped to control this problem, but new drugs are needed because of the continuing emergence of drug-resistant HIV strains.\n\nInfections by fungi are a cause of high morbidity and mortality in immunocompromised persons, such as those with HIV/AIDS, tuberculosis or receiving chemotherapy. The fungi candida, \"Cryptococcus neoformans\" and \"Aspergillus fumigatus\" cause most of these infections and antifungal resistance occurs in all of them. Multidrug resistance in fungi is increasing because of the widespread use of antifungal drugs to treat infections in immunocompromised individuals.\n\nOf particular note, Fluconazole-resistant Candida species have been highlighted as a growing problem by the CDC. More than 20 species of Candida can cause Candidiasis infection, the most common of which is \"Candida albicans\". Candida yeasts normally inhabit the skin and mucous membranes without causing infection. However, overgrowth of Candida can lead to Candidiasis. Some Candida strains are becoming resistant to first-line and second-line antifungal agents such as azoles and echinocandins.\n\nThe protozoan parasites that cause the diseases malaria, trypanosomiasis, toxoplasmosis, cryptosporidiosis and leishmaniasis are important human pathogens.\n\nMalarial parasites that are resistant to the drugs that are currently available to infections are common and this has led to increased efforts to develop new drugs. Resistance to recently developed drugs such as artemisinin has also been reported. The problem of drug resistance in malaria has driven efforts to develop vaccines.\n\nTrypanosomes are parasitic protozoa that cause African trypanosomiasis and Chagas disease (American trypanosomiasis). There are no vaccines to prevent these infections so drugs such as pentamidine and suramin, benznidazole and nifurtimox are used to treat infections. These drugs are effective but infections caused by resistant parasites have been reported.\n\nLeishmaniasis is caused by protozoa and is an important public health problem worldwide, especially in sub-tropical and tropical countries. Drug resistance has \"become a major concern\".\n\nThe discovery of penicillin in 1928 and other antibiotics in the 20th century proved to be a significant medical achievement, saving millions of lives and significantly reducing the burden of infectious diseases. The 1950s to 1970s represented the golden age of antibiotic discovery, where countless new classes of antibiotics were discovered to treat previously incurable diseases such as tuberculosis and syphilis. However, since that time the discovery of new classes of antibiotics has been almost nonexistent, and represents a situation that is especially problematic considering the resiliency of bacteria shown over time and the continued misuse and overuse of antibiotics in treatment.\n\nThe phenomenon of antimicrobial resistance caused by overuse of antibiotics was predicted by Alexander Fleming who said \"The time may come when penicillin can be bought by anyone in the shops. Then there is the danger that the ignorant man may easily under-dose himself and by exposing his microbes to nonlethal quantities of the drug make them resistant.\" Without the creation of new and stronger antibiotics an era where common infections and minor injuries can kill, and where complex procedures such as surgery and chemotherapy become too risky, is a very real possibility. Antimicrobial resistance threatens the world as we know it, and can lead to epidemics of enormous proportions if preventive actions are not taken. In this day and age current antimicrobial resistance leads to longer hospital stays, higher medical costs, and increased mortality.\n\nFor the fiscal year 2016 budget, President Obama suggested to nearly double the amount of federal funding to \"combat and prevent\" antibiotic resistance to more than $1.2 billion. Many international funding agencies like USAID, DFID, SIDA and Bill & Melinda Gates Foundation have pledged money for developing strategies to counter antimicrobial resistance.\n\nSince the mid-1980s pharmaceutical companies have invested in medications for cancer or chronic disease that have greater potential to make money and have \"de-emphasized or dropped development of antibiotics\". On January 20, 2016 at the World Economic Forum in Davos, Switzerland, more than \"80 pharmaceutical and diagnostic companies\" from around the world called for \"transformational commercial models\" at a global level to spur research and development on antibiotics and on the \"enhanced use of diagnostic tests that can rapidly identify the infecting organism\".\n\nSome global health scholars have argued that a global, legal framework is needed to prevent and control antimicrobial resistance. For instance, binding global policies could be used to create antimicrobial use standards, regulate antibiotic marketing, and strengthen global surveillance systems. Ensuring compliance of involved parties is a challenge. Global antimicrobial resistance policies could take lessons from the environmental sector by adopting strategies that have made international environmental agreements successful in the past such as: sanctions for non-compliance, assistance for implementation, majority vote decision-making rules, an independent scientific panel, and specific commitments.\n\nOn March 27, 2015, the White House released a comprehensive plan to address the increasing need for agencies to combat the rise of antibiotic-resistant bacteria. The Task Force for Combating Antibiotic-Resistant Bacteria developed \"The National Action Plan for Combating Antibiotic-Resistant Bacteria\" with the intent of providing a roadmap to guide the US in the antibiotic resistance challenge and with hopes of saving many lives. This plan outlines steps taken by the Federal government over the next five years needed in order to prevent and contain outbreaks of antibiotic-resistant infections; maintain the efficacy of antibiotics already on the market; and to help to develop future diagnostics, antibiotics, and vaccines.\n\nThe Action Plan was developed around five goals with focuses on strengthening health care, public health veterinary medicine, agriculture, food safety and research, and manufacturing. These goals, as listed by the White House, are as follows:\nThe following are goals set to meet by 2020:\n\nAccording to WHO policymakers can help tackle resistance by strengthening resistance tracking and laboratory capacity; regulating and promoting appropriate use of medicines. Policymakers and industry can help tackle resistance by: fostering innovation and research and development of new tools; promoting cooperation and information sharing among all stakeholders.\n\nIt is unclear if rapid viral testing affects antibiotic use in children.\n\nMicroorganisms do not develop resistance to vaccines because a vaccine enhances the body's immune system, whereas an antibiotic operates separately from the body's normal defenses. Furthermore, if the use of vaccines increases, there is evidence that antibiotic resistant strains of pathogens will decrease; the need for antibiotics will naturally decrease as vaccines prevent infection before it occurs. However, new strains that escape immunity induced by vaccines may evolve; for example, an updated influenza vaccine is needed each year.\n\nWhile theoretically promising, antistaphylococcal vaccines have shown limited efficacy, because of immunological variation between \"Staphylococcus\" species, and the limited duration of effectiveness of the antibodies produced. Development and testing of more effective vaccines is underway.\n\nAlternating therapy is a proposed method in which two or three antibiotics are taken in a rotation versus taking just one antibiotic such that bacteria resistant to one antibiotic are killed when the next antibiotic is taken. Studies have found that this method reduces the rate at which antibiotic resistant bacteria emerge in vitro relative to a single drug for the entire duration.\n\nStudies have found that bacteria that evolve antibiotic resistance towards one group of antibiotic may become more sensitive to others. This phenomenon can be utilized to select against resistant bacteria using an approach termed collateral sensitivity cycling, which has recently been found to be relevant in developing treatment strategies for chronic infections caused by \"Pseudomonas aeruginosa\".\n\nSince the discovery of antibiotics, research and development (R&D) efforts have provided new drugs in time to treat bacteria that became resistant to older antibiotics, but in the 2000s there has been concern that development has slowed enough that seriously ill people may run out of treatment options. Another concern is that doctors may become reluctant to perform routine surgeries because of the increased risk of harmful infection. Backup treatments can have serious side-effects; for example, treatment of multi-drug-resistant tuberculosis can cause deafness or psychological disability. The potential crisis at hand is the result of a marked decrease in industry R&D. Poor financial investment in antibiotic research has exacerbated the situation. The pharmaceutical industry has little incentive to invest in antibiotics because of the high risk and because the potential financial returns are less likely to cover the cost of development than for other pharmaceuticals. In 2011, Pfizer, one of the last major pharmaceutical companies developing new antibiotics, shut down its primary research effort, citing poor shareholder returns relative to drugs for chronic illnesses. However, small and medium-sized pharmaceutical companies are still active in antibiotic drug research.\n\nIn the United States, drug companies and the administration of President Barack Obama have been proposing changing the standards by which the FDA approves antibiotics targeted at resistant organisms. On 12 December 2013, the Antibiotic Development to Advance Patient Treatment (ADAPT) Act of 2013 was introduced in the U.S. Congress. The ADAPT Act aims to fast-track the drug development in order to combat the growing public health threat of 'superbugs'. Under this Act, the FDA can approve antibiotics and antifungals needed for life-threatening infections based on data from smaller clinical trials. The Centers for Disease Control and Prevention (CDC) will reinforce the monitoring of the use of antibiotics that treat serious and life-threatening infections and the emerging resistance, and make the data publicly available. The FDA antibiotics labeling process, 'Susceptibility Test Interpretive Criteria for Microbial Organisms' or 'breakpoints' is also streamlined to allow the most up-to-date and cutting-edge data available to healthcare professionals under the new Act.\n\nOn 18 September 2014 Obama signed an executive order to implement the recommendations proposed in a report by the President's Council of Advisors on Science and Technology (PCAST) which outlines strategies to stream-line clinical trials and speed up the R&D of new antibiotics. Among the proposals:\n\nThe U.S. National Institutes of Health plans to fund a new research network on the issue up to $62 million from 2013 to 2019. Using authority created by the Pandemic and All Hazards Preparedness Act of 2006, the Biomedical Advanced Research and Development Authority in the U.S. Department of Health and Human Services announced that it will spend between $40 million and $200 million in funding for R&D on new antibiotic drugs under development by GlaxoSmithKline.\n\nAntimicrobial susceptibility testing (AST) can help practitioners avoid prescribing unnecessary antibiotics in the style of precision medicine, and help them prescribe effective antibiotics, but with the traditional approach it could take 12 to 48 hours. Rapid testing, possible from molecular diagnostics innovations, is defined as \"being feasible within an 8-h working shift\". Progress has been slow due to a range of reasons including cost and regulation. \n\nIn 2016, the United States National Institutes of Health (NIH) announced the Antimicrobial Resistance Diagnostic Challenge as a $20 million prize to encourage the development of diagnostic tests to identify highly resistant bacterial infections. As of 2017, point-of-care resistance diagnostics (POCRD) was available for methicillin-resistant Staphyloccus aureus (MRSA), rifampin-resistant Mycobacterium tuberculosis (TB), and vancomycin-resistant Enterococci (VRE) through GeneXpert by molecular diagnostics company Cepheid.\n\nPhage therapy is the therapeutic use of bacteriophages to treat pathogenic bacterial infections. Phage therapy has many potential applications in human medicine as well as dentistry, veterinary science, and agriculture.\n\nPhage therapy relies on the use of naturally-occurring bacteriophages to infect and lyse bacteria at the site of infection in a host. Due to current advances in genetics and biotechnology these bacteriophages can possibly be manufactured to treat specific infections. Phages can be bioengineered to target multidrug-resistant bacterial infections, and their use involves the added benefit of preventing the elimination of beneficial bacteria in the human body. Phages destroy bacterial cell walls and membrane through the use of lytic proteins which kill bacteria by making many holes from the inside out. Bacteriophages can even possess the ability to digest the biofilm that many bacteria develop that protect them from antibiotics in order to effectively infect and kill bacteria. Bioengineering can play a role in creating successful bacteriophages.\n\nUnderstanding the mutual interactions and evolutions of bacterial and phage populations in the environment of a human or animal body is essential for rational phage therapy. \n\nBacteriophagics are used against antibiotic resistant bacteria in Georgia (George Eliava Institute) and in one institute in Wrocław, Poland. Bacteriophage cocktails are common drugs sold over the counter in pharmacies in eastern countries.\n\n"}
{"id": "18978770", "url": "https://en.wikipedia.org/wiki?curid=18978770", "title": "Birth control", "text": "Birth control\n\nBirth control, also known as contraception and fertility control, is a method or device used to prevent pregnancy. Birth control has been used since ancient times, but effective and safe methods of birth control only became available in the 20th century. Planning, making available, and using birth control is called family planning. Some cultures limit or discourage access to birth control because they consider it to be morally, religiously, or politically undesirable.\nThe most effective methods of birth control are sterilization by means of vasectomy in males and tubal ligation in females, intrauterine devices (IUDs), and implantable birth control. This is followed by a number of hormone-based methods including oral pills, patches, vaginal rings, and injections. Less effective methods include physical barriers such as condoms, diaphragms and birth control sponges and fertility awareness methods. The least effective methods are spermicides and withdrawal by the male before ejaculation. Sterilization, while highly effective, is not usually reversible; all other methods are reversible, most immediately upon stopping them. Safe sex practices, such as with the use of male or female condoms, can also help prevent sexually transmitted infections. Other methods of birth control do not protect against sexually transmitted diseases. Emergency birth control can prevent pregnancy if taken within the 72 to 120 hours after unprotected sex. Some argue not having sex as a form of birth control, but abstinence-only sex education may increase teenage pregnancies if offered without birth control education, due to non-compliance.\nIn teenagers, pregnancies are at greater risk of poor outcomes. Comprehensive sex education and access to birth control decreases the rate of unwanted pregnancies in this age group. While all forms of birth control can generally be used by young people, long-acting reversible birth control such as implants, IUDs, or vaginal rings are more successful in reducing rates of teenage pregnancy. After the delivery of a child, a woman who is not exclusively breastfeeding may become pregnant again after as few as four to six weeks. Some methods of birth control can be started immediately following the birth, while others require a delay of up to six months. In women who are breastfeeding, progestin-only methods are preferred over combined oral birth control pills. In women who have reached menopause, it is recommended that birth control be continued for one year after the last period.\nAbout 222 million women who want to avoid pregnancy in developing countries are not using a modern birth control method. Birth control use in developing countries has decreased the number of deaths during or around the time of pregnancy by 40% (about 270,000 deaths prevented in 2008) and could prevent 70% if the full demand for birth control were met. By lengthening the time between pregnancies, birth control can improve adult women's delivery outcomes and the survival of their children. In the developing world women's earnings, assets, weight, and their children's schooling and health all improve with greater access to birth control. Birth control increases economic growth because of fewer dependent children, more women participating in the workforce, and less use of scarce resources.\n\nBirth control methods include barrier methods, hormonal birth control, intrauterine devices (IUDs), sterilization, and behavioral methods. They are used before or during sex while emergency contraceptives are effective for up to five days after sex. Effectiveness is generally expressed as the percentage of women who become pregnant using a given method during the first year, and sometimes as a lifetime failure rate among methods with high effectiveness, such as tubal ligation.\n\nThe most effective methods are those that are long acting and do not require ongoing health care visits. Surgical sterilization, implantable hormones, and intrauterine devices all have first-year failure rates of less than 1%. Hormonal contraceptive pills, patches or vaginal rings, and the lactational amenorrhea method (LAM), if adhered to strictly, can also have first-year (or for LAM, first-6-month) failure rates of less than 1%. With typical use, first-year failure rates are considerably high, at 9%, due to inconsistent use. Other methods such as condoms, diaphragms, and spermicides have higher first-year failure rates even with perfect usage. The American Academy of Pediatrics recommends long acting reversible birth control as first line for young individuals.\n\nWhile all methods of birth control have some potential adverse effects, the risk is less than that of pregnancy. After stopping or removing many methods of birth control, including oral contraceptives, IUDs, implants and injections, the rate of pregnancy during the subsequent year is the same as for those who used no birth control.\n\nFor individuals with specific health problems, certain forms of birth control may require further investigations. For women who are otherwise healthy, many methods of birth control should not require a medical exam—including birth control pills, injectable or implantable birth control, and condoms. For example, a pelvic exam, breast exam, or blood test before starting birth control pills does not appear to affect outcomes. In 2009, the World Health Organization (WHO) published a detailed list of medical eligibility criteria for each type of birth control.\n\nHormonal contraception is available in a number of different forms, including oral pills, implants under the skin, injections, patches, IUDs and a vaginal ring. They are currently available only for women, although hormonal contraceptives for men have been and are being clinically tested. There are two types of oral birth control pills, the combined oral contraceptive pills (which contain both estrogen and a progestin) and the progestogen-only pills (sometimes called minipills). If either is taken during pregnancy, they do not increase the risk of miscarriage nor cause birth defects. Both types of birth control pills prevent fertilization mainly by inhibiting ovulation and thickening cervical mucus. They may also change the lining of the uterus and thus decrease implantation. Their effectiveness depends on the user's adherence to taking the pills.\nCombined hormonal contraceptives are associated with a slightly increased risk of venous and arterial blood clots. Venous clots, on average, increase from 2.8 to 9.8 per 10,000 women years which is still less than that associated with pregnancy. Due to this risk, they are not recommended in women over 35 years of age who continue to smoke. Due to the increased risk, they are included in decision tools such as the DASH score and PERC rule used to predict the risk of blood clots.\n\nThe effect on sexual desire is varied, with increase or decrease in some but with no effect in most. Combined oral contraceptives reduce the risk of ovarian cancer and endometrial cancer and do not change the risk of breast cancer. They often reduce menstrual bleeding and painful menstruation cramps. The lower doses of estrogen released from the vaginal ring may reduce the risk of breast tenderness, nausea, and headache associated with higher dose estrogen products.\nProgestin-only pills, injections and intrauterine devices are not associated with an increased risk of blood clots and may be used by women with a history of blood clots in their veins. In those with a history of arterial blood clots, non-hormonal birth control or a progestin-only method other than the injectable version should be used. Progestin-only pills may improve menstrual symptoms and can be used by breastfeeding women as they do not affect milk production. Irregular bleeding may occur with progestin-only methods, with some users reporting no periods. The progestins drospirenone and desogestrel minimize the androgenic side effects but increase the risks of blood clots and are thus not first line. The perfect use first-year failure rate of injectable progestin is 0.2%; the typical use first failure rate is 6%.\n\nBarrier contraceptives are devices that attempt to prevent pregnancy by physically preventing sperm from entering the uterus. They include male condoms, female condoms, cervical caps, diaphragms, and contraceptive sponges with spermicide.\n\nGlobally, condoms are the most common method of birth control. Male condoms are put on a man's erect penis and physically block ejaculated sperm from entering the body of a sexual partner. Modern condoms are most often made from latex, but some are made from other materials such as polyurethane, or lamb's intestine. Female condoms are also available, most often made of nitrile, latex or polyurethane. Male condoms have the advantage of being inexpensive, easy to use, and have few adverse effects. Making condoms available to teenagers does not appear to affect the age of onset of sexual activity or its frequency. In Japan, about 80% of couples who are using birth control use condoms, while in Germany this number is about 25%, and in the United States it is 18%.\n\nMale condoms and the diaphragm with spermicide have typical use first-year failure rates of 18% and 12%, respectively. With perfect use condoms are more effective with a 2% first-year failure rate versus a 6% first-year rate with the diaphragm. Condoms have the additional benefit of helping to prevent the spread of some sexually transmitted infections such as HIV/AIDS, however, condoms made from animal intestine do not.\n\nContraceptive sponges combine a barrier with a spermicide. Like diaphragms, they are inserted vaginally before intercourse and must be placed over the cervix to be effective. Typical failure rates during the first year depend on whether or not a woman has previously given birth, being 24% in those who have and 12% in those who have not. The sponge can be inserted up to 24 hours before intercourse and must be left in place for at least six hours afterward. Allergic reactions and more severe adverse effects such as toxic shock syndrome have been reported.\n\nThe current intrauterine devices (IUD) are small devices, often 'T'-shaped, containing either copper or levonorgestrel, which are inserted into the uterus. They are one form of long-acting reversible contraception which are the most effective types of reversible birth control. Failure rates with the copper IUD is about 0.8% while the levonorgestrel IUD has a failure rates of 0.2% in the first year of use. Among types of birth control, they, along with birth control implants, result in the greatest satisfaction among users. As of 2007, IUDs are the most widely used form of reversible contraception, with more than 180 million users worldwide.\n\nEvidence supports effectiveness and safety in adolescents and those who have and have not previously had children. IUDs do not affect breastfeeding and can be inserted immediately after delivery. They may also be used immediately after an abortion. Once removed, even after long term use, fertility returns to normal immediately.\n\nWhile copper IUDs may increase menstrual bleeding and result in more painful cramps, hormonal IUDs may reduce menstrual bleeding or stop menstruation altogether. Cramping can be treated with painkillers like non-steroidal anti-inflammatory drugs. Other potential complications include expulsion (2–5%) and rarely perforation of the uterus (less than 0.7%). A previous model of the intrauterine device (the Dalkon shield) was associated with an increased risk of pelvic inflammatory disease, however the risk is not affected with current models in those without sexually transmitted infections around the time of insertion. IUDs appear to decrease the risk of ovarian cancer.\n\nSurgical sterilization is available in the form of tubal ligation for women and vasectomy for men. There are no significant long term side effects, and tubal ligation decreases the risk of ovarian cancer. Short term complications are twenty times less likely from a vasectomy than a tubal ligation. After a vasectomy, there may be swelling and pain of the scrotum which usually resolves in one or two weeks. With tubal ligation, complications occur in 1 to 2 percent of procedures with serious complications usually due to the anesthesia. Neither method offers protection from sexually transmitted infections.\n\nThis decision may cause regret in some men and women. Of women aged over 30 who have undergone tubal ligation, about 5% regret their decision, as compared with 20% of women aged under 30. By contrast, less than 5% of men are likely to regret sterilization. Men who are more likely to regret sterilization are younger, have young or no children, or have an unstable marriage. In a survey of biological parents, 9% stated they would not have had children if they were able to do it over again.\n\nAlthough sterilization is considered a permanent procedure, it is possible to attempt a tubal reversal to reconnect the fallopian tubes or a vasectomy reversal to reconnect the vasa deferentia. In women, the desire for a reversal is often associated with a change in spouse. Pregnancy success rates after tubal reversal are between 31 and 88 percent, with complications including an increased risk of ectopic pregnancy. The number of males who request reversal is between 2 and 6 percent. Rates of success in fathering another child after reversal are between 38 and 84 percent; with success being lower the longer the time period between the vasectomy and the reversal. Sperm extraction followed by in vitro fertilization may also be an option in men.\n\nBehavioral methods involve regulating the timing or method of intercourse to prevent introduction of sperm into the female reproductive tract, either altogether or when an egg may be present. If used perfectly the first-year failure rate may be around 3.4%, however if used poorly first-year failure rates may approach 85%.\n\nFertility awareness methods involve determining the most fertile days of the menstrual cycle and avoiding unprotected intercourse. Techniques for determining fertility include monitoring basal body temperature, cervical secretions, or the day of the cycle. They have typical first-year failure rates of 24%; perfect use first-year failure rates depend on which method is used and range from 0.4% to 5%. The evidence on which these estimates are based, however, is poor as the majority of people in trials stop their use early. Globally, they are used by about 3.6% of couples. If based on both basal body temperature and another primary sign, the method is referred to as symptothermal. First-year failure rates of 20% overall and 0.4% for perfect use have been reported in clinical studies of the symptothermal method. A number of fertility tracking apps are available, as of 2016, but they are more commonly designed to assist those trying to get pregnant rather than prevent pregnancy.\n\nThe withdrawal method (also known as coitus interruptus) is the practice of ending intercourse (\"pulling out\") before ejaculation. The main risk of the withdrawal method is that the man may not perform the maneuver correctly or in a timely manner. First-year failure rates vary from 4% with perfect usage to 22% with typical usage. It is not considered birth control by some medical professionals.\n\nThere is little data regarding the sperm content of pre-ejaculatory fluid. While some tentative research did not find sperm, one trial found sperm present in 10 out of 27 volunteers. The withdrawal method is used as birth control by about 3% of couples.\n\nSexual abstinence may be used as a form of birth control, meaning either not engaging in any type of sexual activity, or specifically not engaging in vaginal intercourse, while engaging in other forms of non-vaginal sex. Complete sexual abstinence is 100% effective in preventing pregnancy. However, among those who take a pledge to abstain from premarital sex, as many as 88% who engage in sex, do so prior to marriage. The choice to abstain from sex cannot protect against pregnancy as a result of rape, and public health efforts emphasizing abstinence to reduce unwanted pregnancy may have limited effectiveness, especially in developing countries and among disadvantaged groups.\n\nDeliberate non-penetrative sex without vaginal sex or deliberate oral sex without vaginal sex are also sometimes considered birth control. While this generally avoids pregnancy, pregnancy can still occur with intercrural sex and other forms of penis-near-vagina sex (genital rubbing, and the penis exiting from anal intercourse) where sperm can be deposited near the entrance to the vagina and can travel along the vagina's lubricating fluids.\n\nAbstinence-only sex education does not reduce teenage pregnancy. Teen pregnancy rates and STI rates are generally the same or higher in states where students are given abstinence-only education, as compared with comprehensive sex education. Some authorities recommend that those using abstinence as a primary method have backup methods available (such as condoms or emergency contraceptive pills).\n\nThe lactational amenorrhea method involves the use of a woman's natural postpartum infertility which occurs after delivery and may be extended by breastfeeding. This usually requires the presence of no periods, exclusively breastfeeding the infant, and a child younger than six months. The World Health Organization states that if breastfeeding is the infant's only source of nutrition, the failure rate is 2% in the six months following delivery. Six uncontrolled studies of lactational amenorrhea method users found failure rates at 6 months postpartum between 0% and 7.5%. Failure rates increase to 4–7% at one year and 13% at two years. Feeding formula, pumping instead of nursing, the use of a pacifier, and feeding solids all increase its failure rate. In those who are exclusively breastfeeding, about 10% begin having periods before three months and 20% before six months. In those who are not breastfeeding, fertility may return four weeks after delivery.\n\nEmergency contraceptive methods are medications (sometimes misleadingly referred to as \"morning-after pills\") or devices used after unprotected sexual intercourse with the hope of preventing pregnancy. They work primarily by preventing ovulation or fertilization. They are unlikely to affect implantation, but this has not been completely excluded. A number of options exist, including high dose birth control pills, levonorgestrel, mifepristone, ulipristal and IUDs. Levonorgestrel pills, when used within 3 days, decrease the chance of pregnancy after a single episode of unprotected sex or condom failure by 70% (resulting in a pregnancy rate of 2.2%). Ulipristal, when used within 5 days, decreases the chance of pregnancy by about 85% (pregnancy rate 1.4%) and is more effective than levonorgestrel. Mifepristone is also more effective than levonorgestrel, while copper IUDs are the most effective method. IUDs can be inserted up to five days after intercourse and prevent about 99% of pregnancies after an episode of unprotected sex (pregnancy rate of 0.1 to 0.2%). This makes them the most effective form of emergency contraceptive. In those who are overweight or obese, levonorgestrel is less effective and an IUD or ulipristal is recommended.\n\nProviding emergency contraceptive pills to women in advance does not affect rates of sexually transmitted infections, condom use, pregnancy rates, or sexual risk-taking behavior. All methods have minimal side effects.\n\nDual protection is the use of methods that prevent both sexually transmitted infections and pregnancy. This can be with condoms either alone or along with another birth control method or by the avoidance of penetrative sex.\n\nIf pregnancy is a high concern, using two methods at the same time is reasonable. For example, two forms of birth control are recommended in those taking the anti-acne drug isotretinoin or anti-epileptic drugs like carbamazepine, due to the high risk of birth defects if taken during pregnancy.\n\nContraceptive use in developing countries is estimated to have decreased the number of maternal deaths by 40% (about 270,000 deaths prevented in 2008) and could prevent 70% of deaths if the full demand for birth control were met. These benefits are achieved by reducing the number of unplanned pregnancies that subsequently result in unsafe abortions and by preventing pregnancies in those at high risk.\n\nBirth control also improves child survival in the developing world by lengthening the time between pregnancies. In this population, outcomes are worse when a mother gets pregnant within eighteen months of a previous delivery. Delaying another pregnancy after a miscarriage however does not appear to alter risk and women are advised to attempt pregnancy in this situation whenever they are ready.\n\nTeenage pregnancies, especially among younger teens, are at greater risk of adverse outcomes including early birth, low birth weight, and death of the infant. In the United States 82% of pregnancies in those between 15 and 19 are unplanned. Comprehensive sex education and access to birth control are effective in decreasing pregnancy rates in this age group.\n\nIn the developing world, birth control increases economic growth due to there being fewer dependent children and thus more women participating in or increased contribution to the workforce. Women's earnings, assets, body mass index, and their children's schooling and body mass index all improve with greater access to birth control. Family planning, via the use of modern birth control, is one of the most cost-effective health interventions. For every dollar spent, the United Nations estimates that two to six dollars are saved. These cost savings are related to preventing unplanned pregnancies and decreasing the spread of sexually transmitted illnesses. While all methods are beneficial financially, the use of copper IUDs resulted in the greatest savings.\n\nThe total medical cost for a pregnancy, delivery and care of a newborn in the United States is on average $21,000 for a vaginal delivery and $31,000 for a caesarean delivery as of 2012. In most other countries, the cost is less than half. For a child born in 2011, an average US family will spend $235,000 over 17 years to raise them.\n\nGlobally, as of 2009, approximately 60% of those who are married and able to have children use birth control. How frequently different methods are used varies widely between countries. The most common method in the developed world is condoms and oral contraceptives, while in Africa it is oral contraceptives and in Latin America and Asia it is sterilization. In the developing world overall, 35% of birth control is via female sterilization, 30% is via IUDs, 12% is via oral contraceptives, 11% is via condoms, and 4% is via male sterilization.\n\nWhile less used in the developed countries than the developing world, the number of women using IUDs as of 2007 was more than 180 million. Avoiding sex when fertile is used by about 3.6% of women of childbearing age, with usage as high as 20% in areas of South America. As of 2005, 12% of couples are using a male form of birth control (either condoms or a vasectomy) with higher rates in the developed world. Usage of male forms of birth control has decreased between 1985 and 2009. Contraceptive use among women in Sub-Saharan Africa has risen from about 5% in 1991 to about 30% in 2006.\n\nAs of 2012, 57% of women of childbearing age want to avoid pregnancy (867 of 1,520 million). About 222 million women however were not able to access birth control, 53 million of whom were in sub-Saharan Africa and 97 million of whom were in Asia. This results in 54 million unplanned pregnancies and nearly 80,000 maternal deaths a year. Part of the reason that many women are without birth control is that many countries limit access due to religious or political reasons, while another contributor is poverty. Due to restrictive abortion laws in Sub-Saharan Africa, many women turn to unlicensed abortion providers for unintended pregnancy, resulting in about 2–4% obtaining unsafe abortions each year.\n\nThe Egyptian Ebers Papyrus from 1550 BC and the Kahun Papyrus from 1850 BC have within them some of the earliest documented descriptions of birth control: the use of honey, acacia leaves and lint to be placed in the vagina to block sperm. Silphium, a species of giant fennel native to north Africa, may have been used as birth control in ancient Greece and the ancient Near East. Due to its supposed desirability, by the first century AD, it had become so rare that it was worth more than its weight in silver and, by late antiquity, it was fully extinct. Most methods of birth control used in antiquity were probably ineffective.\n\nThe ancient Greek philosopher Aristotle ( 384–322 BC) recommended applying cedar oil to the womb before intercourse, a method which was probably only effective on occasion. A Hippocratic text \"On the Nature of Women\" recommended that a woman drink a copper salt dissolved in water, which it claimed would prevent pregnancy for a year. This method was not only ineffective, but also dangerous, as the later medical writer Soranus of Ephesus ( 98–138 AD) pointed out. Soranus attempted to list reliable methods of birth control based on rational principles. He rejected the use of superstition and amulets and instead prescribed mechanical methods such as vaginal plugs and pessaries using wool as a base covered in oils or other gummy substances. Many of Soranus's methods were probably also ineffective.\n\nIn medieval Europe, any effort to halt pregnancy was deemed immoral by the Catholic Church, although it is believed that women of the time still used a number of birth control measures, such as coitus interruptus and inserting lily root and rue into the vagina. Women in the Middle Ages were also encouraged to tie weasel testicles around their thighs during sex to prevent pregnancy. The oldest condoms discovered to date were recovered in the ruins of Dudley Castle in England, and are dated back to 1640. They were made of animal gut, and were most likely used to prevent the spread of sexually transmitted diseases during the English Civil War. Casanova, living in 18th century Italy, described the use of a lambskin covering to prevent pregnancy; however, condoms only became widely available in the 20th century.\n\nThe birth control movement developed during the 19th and early 20th centuries. The Malthusian League, based on the ideas of Thomas Malthus, was established in 1877 in the United Kingdom to educate the public about the importance of family planning and to advocate for getting rid of penalties for promoting birth control. It was founded during the \"Knowlton trial\" of Annie Besant and Charles Bradlaugh, who were prosecuted for publishing on various methods of birth control.\n\nIn the United States, Margaret Sanger and Otto Bobsein popularized the phrase \"birth control\" in 1914. Sanger primarily advocated for birth control on the idea that it would prevent women from seeking unsafe abortions, but during her lifetime, she began to campaign for it on the grounds that it would reduce mental and physical defects. She was mainly active in the United States but had gained an international reputation by the 1930s. At the time, under the Comstock Law, distribution of birth control information was illegal. She jumped bail in 1914 after her arrest for distributing birth control information and left the United States for the United Kingdom. In the U.K., Sanger, influenced by Havelock Ellis, further developed her arguments for birth control. She believed women needed to enjoy sex without fearing a pregnancy. During her time abroad, Sanger also saw a more flexible diaphragm in a Dutch clinic, which she thought was a better form of contraceptive. Once Sanger returned to the United States, she established a short-lived birth-control clinic with the help of her sister, Ethel Bryne, based in the Brownville section of Brooklyn, New York in 1916. It was shut down after eleven days and resulted in her arrest. The publicity surrounding the arrest, trial, and appeal sparked birth control activism across the United States. Besides her sister, Sanger was helped in the movement by her first husband, William Sanger, who distributed copies of “Family Limitation.” Sanger’s second husband, James Noah H. Slee, would also later become involved in the movement, acting as its main funder.\n\nThe first permanent birth-control clinic was established in Britain in 1921 by Marie Stopes working with the Malthusian League. The clinic, run by midwives and supported by visiting doctors, offered women's birth-control advice and taught them the use of a cervical cap. Her clinic made contraception acceptable during the 1920s by presenting it in scientific terms. In 1921, Sanger founded the American Birth Control League, which later became the Planned Parenthood Federation of America. In 1924 the Society for the Provision of Birth Control Clinics was founded to campaign for municipal clinics; this led to the opening of a second clinic in Greengate, Salford in 1926. Throughout the 1920s, Stopes and other feminist pioneers, including Dora Russell and Stella Browne, played a major role in breaking down taboos about sex. In April 1930 the Birth Control Conference assembled 700 delegates and was successful in bringing birth control and abortion into the political sphere – three months later, the Ministry of Health, in the United Kingdom, allowed local authorities to give birth-control advice in welfare centres.\n\nThe National Birth Control Association was founded in Britain in 1931, and became the Family Planning Association eight years later. The Association amalgamated several British birth control-focused groups into 'a central organisation' for administering and overseeing birth control in Britain. The group incorporated the Birth Control Investigation Committee, a collective of physicians and scientists that was founded to investigate scientific and medical aspects of contraception with 'neutrality and impartiality'. Subsequently, the Association effected a series of 'pure' and 'applied' product and safety standards that manufacturers must meet to ensure their contraceptives could be prescribed as part of the Association's standard two-part-technique combining ‘a rubber appliance to protect the mouth of the womb’ with a ‘chemical preparation capable of destroying... sperm’. Between 1931 and 1959, the Association founded and funded a series of tests to assess chemical efficacy and safety and rubber quality. These tests became the basis for the Association's Approved List of contraceptives, which was launched in 1937, and went on to become an annual publication that the expanding network of FPA clinics relied upon as a means to 'establish facts [about contraceptives] and to publish these facts as a basis on which a sound public and scientific opinion can be built'.\n\nIn 1936 the U.S. court ruled in U.S. v. One Package that medically prescribing contraception to save a person's life or well-being was not illegal under the Comstock Law; following this decision, the American Medical Association Committee on Contraception revoked its 1936 statement condemning birth control. A national survey in 1937 showed 71 percent of the adult population supported the use of contraception. By 1938 347 birth control clinics were running in the United States despite their advertisement still being illegal. First Lady Eleanor Roosevelt publicly supported birth control and family planning. In 1966, President Lyndon B. Johnson started endorsing public funding for family planning services, and the Federal Government began subsidizing birth control services for low-income families. The Affordable Care Act, passed into law on March 23, 2010 under President Barack Obama, requires all plans in the Health Insurance Marketplace to cover contraceptive methods. These include barrier methods, hormonal methods, implanted devices, emergency contraceptives, and sterilization procedures.\n\nIn 1909, Richard Richter developed the first intrauterine device made from silkworm gut, which was further developed and marketed in Germany by Ernst Gräfenberg in the late 1920s. In 1951, a chemist, named Carl Djerassi from Mexico City made the hormones in progesterone pills using Mexican yams. Djerassi had chemically created the pill but was not equipped to distribute it to patients. Meanwhile, Gregory Pincus and John Rock with help from the Planned Parenthood Federation of America developed the first birth control pills in the 1950s, such as mestranol/noretynodrel, which became publicly available in the 1960s through the Food and Drug Administration under the name \"Enovid\". Medical abortion became an alternative to surgical abortion with the availability of prostaglandin analogs in the 1970s and mifepristone in the 1980s.\n\nHuman rights agreements require most governments to provide family planning and contraceptive information and services. These include the requirement to create a national plan for family planning services, remove laws that limit access to family planning, ensure that a wide variety of safe and effective birth control methods are available including emergency contraceptives, make sure there are appropriately trained healthcare providers and facilities at an affordable price, and create a process to review the programs implemented. If governments fail to do the above it may put them in breach of binding international treaty obligations.\n\nIn the United States, the 1965 Supreme Court decision \"Griswold v. Connecticut\" overturned a state law prohibiting dissemination of contraception information based on a constitutional right to privacy for marital relationships. In 1971, \"Eisenstadt v. Baird\" extended this right to privacy to single people.\n\nIn 2010, the United Nations launched the \"Every Woman Every Child\" movement to assess the progress toward meeting women's contraceptive needs. The initiative has set a goal of increasing the number of users of modern birth control by 120 million women in the world's 69 poorest countries by the year 2020. Additionally, they aim to eradicate discrimination against girls and young women who seek contraceptives. The American Congress of Obstetricians and Gynecologists (ACOG) recommended in 2014 that oral birth control pills should be over the counter medications.\n\nSince at least the 1870s, American religious, medical, legislative, and legal commentators have debated contraception laws. Ana Garner and Angela Michel have found that in these discussions men often attach reproductive rights to moral and political matters, as part of an ongoing attempt to regulate human bodies. In press coverage between 1873–2013 they found a divide between institutional ideology and real-life experiences of women.\n\nReligions vary widely in their views of the ethics of birth control. The Roman Catholic Church officially only accepts natural family planning, although large numbers of Catholics in developed countries accept and use modern methods of birth control. Among Protestants, there is a wide range of views from supporting none, such as in the Quiverfull movement, to allowing all methods of birth control. Views in Judaism range from the stricter Orthodox sect, which prohibits all methods of birth control, to the more relaxed Reform sect, which allows most. Hindus may use both natural and modern contraceptives. A common Buddhist view is that preventing conception is acceptable, while intervening after conception has occurred is not. In Islam, contraceptives are allowed if they do not threaten health, although their use is discouraged by some.\n\nSeptember 26 is World Contraception Day, devoted to raising awareness and improving education about sexual and reproductive health, with a vision of \"a world where every pregnancy is wanted.\" It is supported by a group of governments and international NGOs, including the Office of Population Affairs, the Asian Pacific Council on Contraception, Centro Latinamericano Salud y Mujer, the European Society of Contraception and Reproductive Health, the German Foundation for World Population, the International Federation of Pediatric and Adolescent Gynecology, International Planned Parenthood Federation, the Marie Stopes International, Population Services International, the Population Council, the United States Agency for International Development (USAID), and Women Deliver.\n\nThere are a number of common misconceptions regarding sex and pregnancy. Douching after sexual intercourse is not an effective form of birth control. Additionally, it is associated with a number of health problems and thus is not recommended. Women can become pregnant the first time they have sexual intercourse and in any sexual position. It is possible, although not very likely, to become pregnant during menstruation.\n\nImprovements of existing birth control methods are needed, as around half of those who get pregnant unintentionally are using birth control at the time. A number of alterations of existing contraceptive methods are being studied, including a better female condom, an improved diaphragm, a patch containing only progestin, and a vaginal ring containing long-acting progesterone. This vaginal ring appears to be effective for three or four months and is currently available in some areas of the world. For women who rarely have sex, the taking of the hormonal birth control levonorgestrel around the time of sex looks promising.\n\nA number of methods to perform sterilization via the cervix are being studied. One involves putting quinacrine in the uterus which causes scarring and infertility. While the procedure is inexpensive and does not require surgical skills, there are concerns regarding long-term side effects. Another substance, polidocanol, which functions in the same manner is being looked at. A device called Essure, which expands when placed in the fallopian tubes and blocks them, was approved in the United States in 2002.\n\nMethods of male birth control include condoms, vasectomies and withdrawal. Between 25 and 75% of males who are sexually active would use hormonal birth control if it was available for them. A number of hormonal and non-hormonal methods are in trials, and there is some research looking at the possibility of contraceptive vaccines.\n\nA reversible surgical method under investigation is reversible inhibition of sperm under guidance (RISUG) which consists of injecting a polymer gel, styrene maleic anhydride in dimethyl sulfoxide, into the vas deferens. An injection with sodium bicarbonate washes out the substance and restores fertility. Another is an intravas device which involves putting a urethane plug into the vas deferens to block it. A combination of an androgen and a progestin seems promising, as do selective androgen receptor modulators. Ultrasound and methods to heat the testicles have undergone preliminary studies.\n\nNeutering or spaying, which involves removing some of the reproductive organs, is often carried out as a method of birth control in household pets. Many animal shelters require these procedures as part of adoption agreements. In large animals the surgery is known as castration.\n\nBirth control is also being considered as an alternative to hunting as a means of controlling overpopulation in wild animals. Contraceptive vaccines have been found to be effective in a number of different animal populations. Kenyan goat herders fix a skirt, called an olor, to male goats to prevent them from impregnating female goats.\n\n\n"}
{"id": "33680836", "url": "https://en.wikipedia.org/wiki?curid=33680836", "title": "Calvin C.J. Sia", "text": "Calvin C.J. Sia\n\nCalvin C.J. Sia (born Calvin Chia Jung Sia on June 3, 1927) is a primary care pediatrician from Hawaii who developed innovative programs to improve the quality of medical care for children in the United States and Asia. Two particular programs have been implemented throughout America: the Medical Home concept for primary care that has been promoted by the American Academy of Pediatrics and the federal Emergency Medical Services for Children program administered by the U.S. Department of Health and Human Services’ Health Resources and Services Administration, Maternal and Child Health Bureau. His Medical Home model for pediatric care and early childhood development began to take root in several Asian countries in 2003.\n\nSia is also creator of Hawaii Healthy Start Home Visiting Program to prevent child abuse and neglect and co-founder of Hawaii's Zero to Three program and Healthy and Ready to Learn Center. The Hawaii Healthy Start program, which targets expecting and new parents who may be at risk of abusing or neglecting their children, became the model for the Healthy Families America home visiting program that the United States Department of Justice's Office of Justice Programs identified in 2010 as a \"promising\" approach to child abuse prevention. The Healthy and Ready to Learn Center was a three-year pilot project to initiate training and health delivery services in an integrated system of care, with pediatric residents and graduate students in social work and early childhood education working as a team.\n\nIn addition, Sia spearheaded the creation of the Variety School for learning disabled children, a Honolulu-based educational institution for children ages 5 through 13. Sia retired from his Honolulu-based medical practice in 1996, after almost 40 years of treating patients, but continues to promote Medical Home and community pediatrics as professor of Pediatrics at the University of Hawaii John A. Burns School of Medicine. Although he retired as chairman of the American Medical Association Section Council on Pediatrics in 2007, a post he assumed in 1983, Sia continues to play a national role as an emeritus member of the executive committee of the National Center for Medical Home Implementation Project Advisory Committee, an organization he formerly served as chairman.\n\nSia is a 1945 graduate of Punahou School in Honolulu and a graduate of Dartmouth College in 1950. He received his medical degree at Western Reserve University School of Medicine in 1955 and did a general rotating internship as a lieutenant in the U.S. Army Medical Corps at William Beaumont Army Hospital in El Paso, Texas from 1955-1956. Sia then served his pediatric residency under Dr. Irvine McQuarrie at Kauikeolani Children's Hospital in Honolulu, and obtained his license to practice medicine in Hawaii in 1958. He was certified by the American Board of Pediatrics in 1960 and recertified in 1987. The University of Hawaii awarded Sia an honorary Doctor of Humane Letters degree in 1992.\n\nAs a young practicing pediatrician, Sia joined the early cadre of American Academy of Pediatrics consultants for Head Start and Parent Child Centers in Hawaii in the 1960s and developed a strong interest in prenatal, neonatal, and postnatal causes of physical and mental disabilities in children. In a paper he presented in 1964 to the Hawaii Academy of Sciences on advances in neonatology, Sia cited progress in the care of premature babies but also noted that \"completeness\" of the first physical exam and the education of nurses to be on the alert for early signs of disabilities were possible ways to save newborns with previously lethal birth defects. He concluded by observing, \"One of the basic problems will be in solving the causes and prevention of prematurity.\"\n\nInspired by one of his mentors, Dr. Robert E. Cooke, the Johns Hopkins pediatrician behind the creation of the Hopkins hospital's Kennedy Institute for Handicapped Children, Sia helped establish Hawaii's Variety School for Learning Disabilities in 1967 and served as chairman of its board of directors for many years. Sia broadened the scope of his community work to address all children with special health care needs. In the early 1970s, he invited Dr. C. Henry Kempe, founder of the Denver-based National Center for the Prevention and Treatment of Child Abuse and Neglect, and Dr. Ray E. Helfer of Michigan—two pioneers in the identification and treatment of child abuse—to help him and a small group of child advocates develop a plan to prevent and treat child abuse and neglect in the islands. That effort netted one of the first 12 demonstration grant awards by the newly created National Center on Child Abuse and Neglect in 1975, with $1 million going to establish the first Hawaii Family Stress Center. The center, later renamed the Hawaii Family Support Center, established several child abuse and neglect programs on Oahu, including a home-visiting program based on Kempe's effective use of \"lay therapists.\" These were home visitors from the community, properly trained and supervised by public health nurses and social workers who could earn the trust of at-risk families and focus on family strengths to reduce environmental risk and prevent child abuse and neglect. The center's goal was to identify vulnerable families before their day-to-day stresses, isolation, and lack of parenting knowledge and good role models gave rise to abusive and neglectful behavior.\n\nThe center's operations coincided with an effort launched by Dr. Vince L. Hutchins and Dr. Merle McPherson of the Maternal and Child Health Bureau in 1977 to revise and update the\nmission of the federal agency's Title V and companion \"crippled children's\" programs to address child development and the prevention of developmental, behavioral and psychosocial problems. McPherson took note of Sia's call for a continuous system of care originating with the primary care pediatrician. The AAP collaborated in this effort by asking each state’s AAP chapter to develop a Child Health Plan that set priorities for using MCHB block grants. Sia spearheaded the Hawaii planning effort, bringing together representatives from the Hawaii AAP Chapter, the UH medical school, the Hawaii Medical Association, and Kapiolani Medical Center for Women and Children. Armed with anecdotal evidence showing home visitors were able to promote\neffective parenting and ultimately improve outcomes, the group wrote a plan that incorporated a coordinated system of care that emphasized wellness and prevention for\nchildren, especially those with special needs.\n\nThis was the birth of the Medical Home concept for primary care, to which Sia attached the slogan, “Every Child Deserves a Medical Home.” Under this idea, which the American Academy of Pediatrics adopted as a policy statement in 1992, the medical care of all infants, children and adolescents should be accessible, continuous, comprehensive, family-centered, coordinated, compassionate, and culturally effective. It should be delivered or directed by well-trained physicians who provide primary care and help to manage and facilitate essentially all aspects of pediatric care. The physician should be known to the child and family and should be able to develop a partnership of mutual responsibility and trust with them. As Sia and his co-authors of a 2006 monograph on the Medical Home noted, this new model broadens the traditional focus on acute care to include prevention and well care at one end of the continuum and chronic care management of children with special health care needs at the other. One expert observed, for example, that for a child born with spina bifida, Sia's Medical Home model would have the family and its health care provider compose a list of specialists and therapists who would be caring for the child and a timeline of anticipated surgeries and interventions. The aim would be to have as few emergencies and unanticipated events as possible.\n\nAs the lead author of an often-cited article published by the journal Pediatrics in May 2004, Sia traced the development of the Medical Home concept.\n\nBy 1984, Sia had begun to implement the Medical Home concept in Hawaii. As chairman of an ad hoc state legislative task force on child abuse, he persuaded Hawaii lawmakers to authorize the Hawaii Healthy Start Home Visiting Program for the prevention of child abuse and neglect. This state-funded pilot program, carried out by Hawaii Family Support Center in collaboration with the Hawaii Department of Health, focused on a neighborhood in the Ewa community on Oahu, a community with relatively high rates of child abuse and neglect. A year later, he spearheaded the Hawaii Medical Association's effort to obtain a grant from the U.S. Maternal and Child Health Bureau, under the Special Projects of Regional and National Significance (SPRANS) initiative, to train primary care physicians to provide a \"Medical Home\" for all children with special health care needs. The demonstration project—which sought to help first-time families give their newborn children the best start in life—was so successful it was expanded from a small part of Oahu to other areas of Hawaii, and as word of the demonstrated positive outcomes spread, Hawaii’s Healthy Start became a model for parenting education programs nationwide. In the early 1990s, Healthy Families America and the National Healthy Start Association began to standardize and credential programs to ensure effectiveness and research-based practices. Across the United States, according to the MCHB, the home visiting program has shown that it can reduce child maltreatment and increase children’s readiness for school.\n\nMeanwhile, Sia launched the Hawaii Early Intervention Program for infants and toddlers in 1986 and also became actively involved with Hawaii’s Early Intervention Coordinating Council for Zero to Three, placing this under Hawaii’s Department of Health instead of the Department of Education. The focus of this effort was to support the Medical Home system of care with prevention and early intervention programs.\n\nAt a June 1987 conference called by Surgeon General C. Everett Koop and sponsored by the AAP and MCHB to address children with special needs, Sia and his delegation from Hawaii made a presentation of the Medical Home concept. Koop appeared to embrace it by issuing a report that endorsed a system of family-centered, community-based, coordinated care for children with special needs. This was followed in 1989 by the first National Medical Home Conference, which drew 26 AAP state chapters to Hawaii for presentations organized by Sia and MCHB officials on how to train pediatricians in the Medical Home system of care. This led to consultations to introduce the Medical Home training program to interdisciplinary teams of pediatricians, families, and other health care–related professionals in Florida, Minnesota, Nebraska, Pennsylvania, Washington and other states.\n\nThe pace of activity prompted Sia to close his private medical practice in 1996 so he could devote his time as principal investigator on various early childhood grant projects promoting the Medical Home and its integrated system of care. He launched several initiatives with a MCHB Health Education Collaboration grant in support of interprofessional training in early childhood, a Carnegie Corporation of New York Starting Points planning grant in early childhood, and Consuelo Foundation of Hawaii's Healthy and Ready to Learn grant–all with the emphasis on integrating the continuum of care of the Medical Home with other health, family, and community services from a holistic approach. The MCHB funding enabled him to travel across the country to promote the Medical Home concept to various\ncommunities, state AAP chapters, family advocacy groups and state Title V maternal and child health officers.\n\nA three-year pilot project creating a Healthy and Ready to Learn Center in Hawaii began in 1992 and helped gauge the effectiveness of Sia's family-centered interprofessional collaboration approach. Lessons learned from this project were subsequently adopted by the Office of Children and Youth of the Governor's Office of Hawaii with Sia as Co-Principal Investigator. The Carnegie Corp. Starting Points grant then was assumed by the Good Beginnings Alliance in Hawaii.\n\nSia, serving as chairman of the American Medical Association's Section Council on Pediatrics and other AMA- and AAP-related posts, used those platforms and his network of contacts with other groups to help introduce the Medical Home concept into the care of adults as well as children, although his primary focus has remained on pediatric care. In 2007, the AAP, American Academy of Family Physicians, American Academy of Pediatrics, American College of Physicians and the American Osteopathic Association adopted the Joint Principles of the Patient-Centered Medical Home that set a standard definition of a Medical Home. A year later, the AMA adopted the principles, which have since received support from over 700 member organizations of the Patient Centered Primary Care Collaborative, including primary care and specialty care societies, all major health plans and consumer organizations. In addition, the term Medical Home now regularly shows up in the literature of parent groups such as Family Voices, in family practice journals and on the websites of state public health and medical agencies.\n\nBeginning in 2000, Sia expanded his efforts related to early child development and the Medical Home to Asia. In 2003, he created the Asia-US Partnership, a think tank based at the University of Hawaii medical school whose mission is to improve child health in Asia and the United States through cross-cultural\nexchanges with leaders in pediatrics. That same year, Sia initiated and chaired the first of several AUSP Early Child Development and Primary Care conferences, bringing together pediatric and early childhood development experts from Asia and the United States to translate the science of early child development into policy and action. Participants have come from China (Beijing, Shanghai and Hong Kong), the Philippines, Singapore and Thailand and the United States. According to conference reports, these international exchanges have stimulated translation of the science on early child development and primary care into action programs in the broad areas of advocacy, service delivery, research, and training among the Asian early childhood professionals leadership. Sia has continued to serve as co-chairman of these events, including the sixth international conference, held in the Philippines capital of Manila, in May 2011. After hosting the earliest AUSP conferences in Hawaii, Sia decided to move the 2009 event to Shanghai and tapped a team of Chinese doctors to serve as conference host, signaling what he called a new phase of activity aimed at developing greater shared leadership and stronger \"country teams.\"\n\nWhile planting the seeds of the Medical Home concept in Hawaii, Sia embarked on a related advocacy campaign focused on emergency care for children. In 1979, as president of the Hawaii Medical Association, Sia urged members of the American Academy of Pediatrics to develop multifaceted Emergency Medical Services programs designed to decrease disability and death in children. By January 1981, AAP's Executive Board had approved formation of a Section on Emergency Medicine, with Sia as one of its seven charter members. He along with José B. Lee then-executive officer of the Hawaii Medical Association Emergency Medical Services Program began working closely with Senator Daniel Inouye, whom he happened to meet on a flight to Washington, D.C., to create a National Emergency Medical Services for Children System (EMSC) demonstration grant program to address acute injuries, illnesses and other childhood crises. The program was launched after the October 1984 enactment of EMSC legislation (Public Law 98-555), a bipartisan measure sponsored by Inouye and Republican Senators Orrin Hatch of Utah and Lowell Weicker of Connecticut and endorsed by Surgeon General C. Everett Koop. States receiving these demonstration grants established an emergency medical care service system for children that upgraded training and equipment for first responders and emergency departments to treat children. Hawaii ultimately received a grant to initiate its own emergency care system for children, which improved care coordination with the primary care physician. EMSC is now an established statewide system of care for children in all 50 states and territories.\n\nSeveral national and state organizations have recognized Sia for developing innovative and responsive family-centered grassroots services. Among the awards he has received are these:\n\n\nSia was born in Beijing, China to Dr. Richard Ho Ping Sia, a physician and former Rockefeller Institute researcher in infectious diseases whose work laid the groundwork for the Avery–MacLeod–McCarty experiment on DNA and bacterial transformation, and Mary Li Sia, a Honolulu-born author of several Chinese cookbooks. His mother's parents were Kong Tai Heong and Li Khai Fai, doctors who worked on the 1899 plague outbreak. Sia and his older sister Sylvia and younger sister Julia, all United States citizens by birth, grew up in Hawaii, where the family settled in 1939 after living under Japanese occupation in Beijing for nearly two years.\n\nSia married Katherine Li in 1951. Sia has three sons, Richard H.P. Sia, a journalist; Jeffrey H.K. Sia, a Honolulu-based attorney and former president of the Hawaii State Bar Association; and Dr. Michael H.T. Sia, a pediatrician and chairman of Pediatrics at Kapiolani Medical Center for Women and Children; and six grandchildren.\n\n"}
{"id": "3992341", "url": "https://en.wikipedia.org/wiki?curid=3992341", "title": "Case mix index", "text": "Case mix index\n\nCase mix index (CMI) is a relative value assigned to a diagnosis-related group of patients in a medical care environment. The CMI value is used in determining the allocation of resources to care for and/or treat the patients in the group.\n\nPatients are classified into groups having the same condition (based on main and secondary diagnosis, procedures, age), complexity (comorbidity) and needs. These groups are known as Diagnosis Related Groups (DRG), or Resource Use Groups (RUG).\n\nEach DRG has a relative average value assigned to it that indicates the amount of resources required to treat patients in the group, as compared to all the other diagnosis-related groups within the system. The relative average value assigned to each group is its CMI.\n\nThe CMI of a hospital reflects the diversity, clinical complexity and the needs for resources in the population of all the patients in the hospital. \n\nThe CMI value of a hospital can be used to adjust the average cost per patient (or per day) for a given hospital relative to the adjusted average cost for other hospitals by dividing the average cost per patient (or day) by the hospital's calculated CMI. The adjusted average cost per patient would reflect the charges reported for the types of cases treated in that year. If a hospital has a CMI greater than 1.00, their adjusted cost per patient or per day will be lower and conversely if a hospital has a CMI less than 1.00, their adjusted cost will be higher.\n\nExample:\n\nA link to the 2011 spreadsheet of the CMI for all US providers is located here\nAn analysis of that file shows that there are 3619 hospital records. The number of cases for the hospitals ranges from a low of 1 to a high of 36,282 cases at Florida Hospital in Orlando, FL (Medicare ID 100007). That hospital has a Case Mix Index of 1.57. The mean number of cases across all the hospitals in the database is 3,098 with a standard deviation of 3,102. As far as the Case Mix Index, the average is 1.37 with a minimum of .58 and a max of 3.73 and a standard deviation of 0.31.\n\n"}
{"id": "7172", "url": "https://en.wikipedia.org/wiki?curid=7172", "title": "Chemotherapy", "text": "Chemotherapy\n\nChemotherapy (often abbreviated to chemo and sometimes CTX or CTx) is a type of cancer treatment that uses one or more anti-cancer drugs (chemotherapeutic agents) as part of a standardized chemotherapy regimen. Chemotherapy may be given with a curative intent (which almost always involves combinations of drugs), or it may aim to prolong life or to reduce symptoms (palliative chemotherapy). Chemotherapy is one of the major categories of the medical discipline specifically devoted to pharmacotherapy for cancer, which is called \"medical oncology\".\n\nThe term \"chemotherapy\" has come to connote non-specific usage of intracellular poisons to inhibit mitosis, cell division. The connotation excludes more selective agents that block extracellular signals (signal transduction). The development of therapies with specific molecular or genetic targets, which inhibit growth-promoting signals from classic endocrine hormones (primarily estrogens for breast cancer and androgens for prostate cancer) are now called hormonal therapies. By contrast, other inhibitions of growth-signals like those associated with receptor tyrosine kinases are referred to as targeted therapy.\n\nImportantly, the use of drugs (whether chemotherapy, hormonal therapy or targeted therapy) constitutes \"systemic therapy\" for cancer in that they are introduced into the blood stream and are therefore in principle able to address cancer at any anatomic location in the body. Systemic therapy is often used in conjunction with other modalities that constitute \"local therapy\" (i.e. treatments whose efficacy is confined to the anatomic area where they are applied) for cancer such as radiation therapy, surgery or hyperthermia therapy.\n\nTraditional chemotherapeutic agents are cytotoxic by means of interfering with cell division (mitosis) but cancer cells vary widely in their susceptibility to these agents. To a large extent, chemotherapy can be thought of as a way to damage or stress cells, which may then lead to cell death if apoptosis is initiated. Many of the side effects of chemotherapy can be traced to damage to normal cells that divide rapidly and are thus sensitive to anti-mitotic drugs: cells in the bone marrow, digestive tract and hair follicles. This results in the most common side-effects of chemotherapy: myelosuppression (decreased production of blood cells, hence also immunosuppression), mucositis (inflammation of the lining of the digestive tract), and alopecia (hair loss). Because of the effect on immune cells (especially lymphocytes), chemotherapy drugs often find use in a host of diseases that result from harmful overactivity of the immune system against self (so-called autoimmunity). These include rheumatoid arthritis, systemic lupus erythematosus, multiple sclerosis, vasculitis and many others.\n\nThere are a number of strategies in the administration of chemotherapeutic drugs used today. Chemotherapy may be given with a curative intent or it may aim to prolong life or to palliate symptoms.\n\nAll chemotherapy regimens require that the recipient be capable of undergoing the treatment. Performance status is often used as a measure to determine whether a person can receive chemotherapy, or whether dose reduction is required. Because only a fraction of the cells in a tumor die with each treatment (fractional kill), repeated doses must be administered to continue to reduce the size of the tumor. Current chemotherapy regimens apply drug treatment in cycles, with the frequency and duration of treatments limited by toxicity.\n\nThe efficacy of chemotherapy depends on the type of cancer and the stage. The overall effectiveness ranges from being curative for some cancers, such as some leukemias, to being ineffective, such as in some brain tumors, to being needless in others, like most non-melanoma skin cancers.\n\nDosage of chemotherapy can be difficult: If the dose is too low, it will be ineffective against the tumor, whereas, at excessive doses, the toxicity (side-effects) will be intolerable to the person receiving it. The standard method of determining chemotherapy dosage is based on calculated body surface area (BSA). The BSA is usually calculated with a mathematical formula or a nomogram, using the recipient's weight and height, rather than by direct measurement of body area. This formula was originally derived in a 1916 study and attempted to translate medicinal doses established with laboratory animals to equivalent doses for humans. The study only included 9 human subjects. When chemotherapy was introduced in the 1950s, the BSA formula was adopted as the official standard for chemotherapy dosing for lack of a better option.\n\nRecently, the validity of this method in calculating uniform doses has been questioned. The reason for this is that the formula only takes into account the individual's weight and height. Drug absorption and clearance are influenced by multiple factors, including age, gender, metabolism, disease state, organ function, drug-to-drug interactions, genetics, and obesity, which has a major impact on the actual concentration of the drug in the person's bloodstream. As a result, there is high variability in the systemic chemotherapy drug concentration in people dosed by BSA, and this variability has been demonstrated to be more than 10-fold for many drugs. In other words, if two people receive the same dose of a given drug based on BSA, the concentration of that drug in the bloodstream of one person may be 10 times higher or lower compared to that of the other person. This variability is typical with many chemotherapy drugs dosed by BSA, and, as shown below, was demonstrated in a study of 14 common chemotherapy drugs.\nThe result of this pharmacokinetic variability among people, is that many people do not receive the right dose to achieve optimal treatment effectiveness with minimized toxic side effects. Some people are overdosed while others are underdosed. For example, in a randomized clinical trial, investigators found 85% of metastatic colorectal cancer patients treated with 5-fluorouracil (5-FU) did not receive the optimal therapeutic dose when dosed by the BSA standard—68% were underdosed and 17% were overdosed.\n\nThere has been controversy over the use of BSA to calculate chemotherapy doses for people who are obese. Because of their higher BSA, clinicians often arbitrarily reduce the dose prescribed by the BSA formula for fear of overdosing. In many cases, this can result in sub-optimal treatment.\n\nSeveral clinical studies have demonstrated that when chemotherapy dosing is individualized to achieve optimal systemic drug exposure, treatment outcomes are improved and toxic side effects are reduced. In the 5-FU clinical study cited above, people whose dose was adjusted to achieve a pre-determined target exposure realized an 84% improvement in treatment response rate and a six-month improvement in overall survival (OS) compared with those dosed by BSA.\n\nIn the same study, investigators compared the incidence of common 5-FU-associated grade 3/4 toxicities between the dose-adjusted people and people dosed per BSA. The incidence of debilitating grades of diarrhea was reduced from 18% in the BSA-dosed group to 4% in the dose-adjusted group and serious hematologic side effects were eliminated. Because of the reduced toxicity, dose-adjusted patients were able to be treated for longer periods of time. BSA-dosed people were treated for a total of 680 months while people in the dose-adjusted group were treated for a total of 791 months. Completing the course of treatment is an important factor in achieving better treatment outcomes.\n\nSimilar results were found in a study involving people with colorectal cancer who were treated with the popular FOLFOX regimen. The incidence of serious diarrhea was reduced from 12% in the BSA-dosed group of patients to 1.7% in the dose-adjusted group, and the incidence of severe mucositis was reduced from 15% to 0.8%.\n\nThe FOLFOX study also demonstrated an improvement in treatment outcomes. Positive response increased from 46% in the BSA-dosed group to 70% in the dose-adjusted group. Median progression free survival (PFS) and overall survival (OS) both improved by six months in the dose adjusted group.\n\nOne approach that can help clinicians individualize chemotherapy dosing is to measure the drug levels in blood plasma over time and adjust dose according to a formula or algorithm to achieve optimal exposure. With an established target exposure for optimized treatment effectiveness with minimized toxicities, dosing can be personalized to achieve target exposure and optimal results for each person. Such an algorithm was used in the clinical trials cited above and resulted in significantly improved treatment outcomes.\n\nOncologists are already individualizing dosing of some cancer drugs based on exposure. Carboplatin and busulfan dosing rely upon results from blood tests to calculate the optimal dose for each person. Simple blood tests are also available for dose optimization of methotrexate, 5-FU, paclitaxel, and docetaxel.\n\nAlkylating agents are the oldest group of chemotherapeutics in use today. Originally derived from mustard gas used in World War I, there are now many types of alkylating agents in use. They are so named because of their ability to alkylate many molecules, including proteins, RNA and DNA. This ability to bind covalently to DNA via their alkyl group is the primary cause for their anti-cancer effects. DNA is made of two strands and the molecules may either bind twice to one strand of DNA (intrastrand crosslink) or may bind once to both strands (interstrand crosslink). If the cell tries to replicate crosslinked DNA during cell division, or tries to repair it, the DNA strands can break. This leads to a form of programmed cell death called apoptosis. Alkylating agents will work at any point in the cell cycle and thus are known as cell cycle-independent drugs. For this reason the effect on the cell is dose dependent; the fraction of cells that die is directly proportional to the dose of drug.\n\nThe subtypes of alkylating agents are the nitrogen mustards, nitrosoureas, tetrazines, aziridines, cisplatins and derivatives, and non-classical alkylating agents. Nitrogen mustards include mechlorethamine, cyclophosphamide, melphalan, chlorambucil, ifosfamide and busulfan. Nitrosoureas include N-Nitroso-N-methylurea (MNU), carmustine (BCNU), lomustine (CCNU) and semustine (MeCCNU), fotemustine and streptozotocin. Tetrazines include dacarbazine, mitozolomide and temozolomide. Aziridines include thiotepa, mytomycin and diaziquone (AZQ). Cisplatin and derivatives include cisplatin, carboplatin and oxaliplatin. They impair cell function by forming covalent bonds with the amino, carboxyl, sulfhydryl, and phosphate groups in biologically important molecules. Non-classical alkylating agents include procarbazine and hexamethylmelamine.\n\nAnti-metabolites are a group of molecules that impede DNA and RNA synthesis. Many of them have a similar structure to the building blocks of DNA and RNA. The building blocks are nucleotides; a molecule comprising a nucleobase, a sugar and a phosphate group. The nucleobases are divided into purines (guanine and adenine) and pyrimidines (cytosine, thymine and uracil). Anti-metabolites resemble either nucleobases or nucleosides (a nucleotide without the phosphate group), but have altered chemical groups. These drugs exert their effect by either blocking the enzymes required for DNA synthesis or becoming incorporated into DNA or RNA. By inhibiting the enzymes involved in DNA synthesis, they prevent mitosis because the DNA cannot duplicate itself. Also, after misincorporation of the molecules into DNA, DNA damage can occur and programmed cell death (apoptosis) is induced. Unlike alkylating agents, anti-metabolites are cell cycle dependent. This means that they only work during a specific part of the cell cycle, in this case S-phase (the DNA synthesis phase). For this reason, at a certain dose, the effect plateaus and proportionally no more cell death occurs with increased doses. Subtypes of the anti-metabolites are the anti-folates, fluoropyrimidines, deoxynucleoside analogues and thiopurines.\n\nThe anti-folates include methotrexate and pemetrexed. Methotrexate inhibits dihydrofolate reductase (DHFR), an enzyme that regenerates tetrahydrofolate from dihydrofolate. When the enzyme is inhibited by methotrexate, the cellular levels of folate coenzymes diminish. These are required for thymidylate and purine production, which are both essential for DNA synthesis and cell division. Pemetrexed is another anti-metabolite that affects purine and pyrimidine production, and therefore also inhibits DNA synthesis. It primarily inhibits the enzyme thymidylate synthase, but also has effects on DHFR, aminoimidazole carboxamide ribonucleotide formyltransferase and glycinamide ribonucleotide formyltransferase. The fluoropyrimidines include fluorouracil and capecitabine. Fluorouracil is a nucleobase analogue that is metabolised in cells to form at least two active products; 5-fluourouridine monophosphate (FUMP) and 5-fluoro-2'-deoxyuridine 5'-phosphate (fdUMP). FUMP becomes incorporated into RNA and fdUMP inhibits the enzyme thymidylate synthase; both of which lead to cell death. Capecitabine is a prodrug of 5-fluorouracil that is broken down in cells to produce the active drug. The deoxynucleoside analogues include cytarabine, gemcitabine, decitabine, azacitidine, fludarabine, nelarabine, cladribine, clofarabine, and pentostatin. The thiopurines include thioguanine and mercaptopurine.\n\nAnti-microtubule agents are plant-derived chemicals that block cell division by preventing microtubule function. Microtubules are an important cellular structure composed of two proteins; α-tubulin and β-tubulin. They are hollow rod shaped structures that are required for cell division, among other cellular functions. Microtubules are dynamic structures, which means that they are permanently in a state of assembly and disassembly. \"Vinca\" alkaloids and taxanes are the two main groups of anti-microtubule agents, and although both of these groups of drugs cause microtubule dysfunction, their mechanisms of action are completely opposite. The vinca alkaloids prevent the formation of the microtubules, whereas the taxanes prevent the microtubule disassembly. By doing so, they prevent the cancer cells from completing mitosis. Following this, cell cycle arrest occurs, which induces programmed cell death (apoptosis). Also, these drugs can affect blood vessel growth; an essential process that tumours utilise in order to grow and metastasise.\n\n\"Vinca\" alkaloids are derived from the Madagascar periwinkle, \"Catharanthus roseus\" (formerly known as \"Vinca rosea\"). They bind to specific sites on tubulin, inhibiting the assembly of tubulin into microtubules. The original \"vinca\" alkaloids are natural products that include vincristine and vinblastine. Following the success of these drugs, semi-synthetic \"vinca\" alkaloids were produced: vinorelbine (used in the treatment of non-small-cell lung cancer), vindesine, and vinflunine. These drugs are cell cycle-specific. They bind to the tubulin molecules in S-phase and prevent proper microtubule formation required for M-phase.\n\nTaxanes are natural and semi-synthetic drugs. The first drug of their class, paclitaxel, was originally extracted from the Pacific Yew tree, \"Taxus brevifolia\". Now this drug and another in this class, docetaxel, are produced semi-synthetically from a chemical found in the bark of another Yew tree; \"Taxus baccata\". These drugs promote microtubule stability, preventing their disassembly. Paclitaxel prevents the cell cycle at the boundary of G2-M, whereas docetaxel exerts its effect during S-phase. Taxanes present difficulties in formulation as medicines because they are poorly soluble in water.\n\nPodophyllotoxin is an antineoplastic lignan obtained primarily from the American Mayapple (\"Podophyllum peltatum\") and Himalayan Mayapple (\"Podophyllum hexandrum\" or \"Podophyllum emodi\"). It has anti-microtubule activity, and its mechanism is similar to that of \"vinca\" alkaloids in that they bind to tubulin, inhibiting microtubule formation. Podophyllotoxin is used to produce two other drugs with different mechanisms of action: etoposide and teniposide.\n\nTopoisomerase inhibitors are drugs that affect the activity of two enzymes: topoisomerase I and topoisomerase II. When the DNA double-strand helix is unwound, during DNA replication or transcription, for example, the adjacent unopened DNA winds tighter (supercoils), like opening the middle of a twisted rope. The stress caused by this effect is in part aided by the topoisomerase enzymes. They produce single- or double-strand breaks into DNA, reducing the tension in the DNA strand. This allows the normal unwinding of DNA to occur during replication or transcription. Inhibition of topoisomerase I or II interferes with both of these processes.\n\nTwo topoisomerase I inhibitors, irinotecan and topotecan, are semi-synthetically derived from camptothecin, which is obtained from the Chinese ornamental tree \"Camptotheca acuminata\". Drugs that target topoisomerase II can be divided into two groups. The topoisomerase II poisons cause increased levels enzymes bound to DNA. This prevents DNA replication and transcription, causes DNA strand breaks, and leads to programmed cell death (apoptosis). These agents include etoposide, doxorubicin, mitoxantrone and teniposide. The second group, catalytic inhibitors, are drugs that block the activity of topoisomerase II, and therefore prevent DNA synthesis and translation because the DNA cannot unwind properly. This group includes novobiocin, merbarone, and aclarubicin, which also have other significant mechanisms of action.\n\nThe cytotoxic antibiotics are a varied group of drugs that have various mechanisms of action. The common theme that they share in their chemotherapy indication is that they interrupt cell division. The most important subgroup is the anthracyclines and the bleomycins; other prominent examples include mitomycin C, mitoxantrone, and actinomycin.\n\nAmong the anthracyclines, doxorubicin and daunorubicin were the first, and were obtained from the bacterium \"Streptomyces peucetius\". Derivatives of these compounds include epirubicin and idarubicin. Other clinically used drugs in the anthracyline group are pirarubicin, aclarubicin, and mitoxantrone. The mechanisms of anthracyclines include DNA intercalation (molecules insert between the two strands of DNA), generation of highly reactive free radicals that damage intercellular molecules and topoisomerase inhibition.\n\nActinomycin is a complex molecule that intercalates DNA and prevents RNA synthesis.\n\nBleomycin, a glycopeptide isolated from \"Streptomyces verticillus\", also intercalates DNA, but produces free radicals that damage DNA. This occurs when bleomycin binds to a metal ion, becomes chemically reduced and reacts with oxygen.\n\nMitomycin is a cytotoxic antibiotic with the ability to alkylate DNA.\n\nMost chemotherapy is delivered intravenously, although a number of agents can be administered orally (e.g., melphalan, busulfan, capecitabine).\n\nThere are many intravenous methods of drug delivery, known as vascular access devices. These include the winged infusion device, peripheral venous catheter, midline catheter, peripherally inserted central catheter (PICC), central venous catheter and implantable port. The devices have different applications regarding duration of chemotherapy treatment, method of delivery and types of chemotherapeutic agent.\n\nDepending on the person, the cancer, the stage of cancer, the type of chemotherapy, and the dosage, intravenous chemotherapy may be given on either an inpatient or an outpatient basis. For continuous, frequent or prolonged intravenous chemotherapy administration, various systems may be surgically inserted into the vasculature to maintain access. Commonly used systems are the Hickman line, the Port-a-Cath, and the PICC line. These have a lower infection risk, are much less prone to phlebitis or extravasation, and eliminate the need for repeated insertion of peripheral cannulae.\n\nIsolated limb perfusion (often used in melanoma), or isolated infusion of chemotherapy into the liver or the lung have been used to treat some tumors. The main purpose of these approaches is to deliver a very high dose of chemotherapy to tumor sites without causing overwhelming systemic damage. These approaches can help control solitary or limited metastases, but they are by definition not systemic, and, therefore, do not treat distributed metastases or micrometastases.\n\nTopical chemotherapies, such as 5-fluorouracil, are used to treat some cases of non-melanoma skin cancer.\n\nIf the cancer has central nervous system involvement, or with meningeal disease, intrathecal chemotherapy may be administered.\n\nChemotherapeutic techniques have a range of side-effects that depend on the type of medications used. The most common medications affect mainly the fast-dividing cells of the body, such as blood cells and the cells lining the mouth, stomach, and intestines. Chemotherapy-related toxicities can occur acutely after administration, within hours or days, or chronically, from weeks to years.\n\nVirtually all chemotherapeutic regimens can cause depression of the immune system, often by paralysing the bone marrow and leading to a decrease of white blood cells, red blood cells, and platelets.\nAnemia and thrombocytopenia may require blood transfusion. Neutropenia (a decrease of the neutrophil granulocyte count below 0.5 x 10/litre) can be improved with synthetic G-CSF (granulocyte-colony-stimulating factor, e.g., filgrastim, lenograstim).\n\nIn very severe myelosuppression, which occurs in some regimens, almost all the bone marrow stem cells (cells that produce white and red blood cells) are destroyed, meaning \"allogenic\" or \"autologous\" bone marrow cell transplants are necessary. (In autologous BMTs, cells are removed from the person before the treatment, multiplied and then re-injected afterward; in \"allogenic\" BMTs, the source is a donor.) However, some people still develop diseases because of this interference with bone marrow.\n\nAlthough people receiving chemotherapy are encouraged to wash their hands, avoid sick people, and take other infection-reducing steps, about 85% of infections are due to naturally occurring microorganisms in the person's own gastrointestinal tract (including oral cavity) and skin. This may manifest as systemic infections, such as sepsis, or as localized outbreaks, such as Herpes simplex, shingles, or other members of the Herpesviridea. The risk of illness and death can be reduced by taking common antibiotics such as quinolones or trimethoprim/sulfamethoxazole before any fever or sign of infection appears. Quinolones show effective prophylaxis mainly with hematological cancer. However, in general, for every five people who are immunosuppressed following chemotherapy who take an antibiotic, one fever can be prevented; for every 34 who take an antibiotic, one death can be prevented. Sometimes, chemotherapy treatments are postponed because the immune system is suppressed to a critically low level.\n\nIn Japan, the government has approved the use of some medicinal mushrooms like \"Trametes versicolor\", to counteract depression of the immune system in people undergoing chemotherapy.\n\nDue to immune system suppression, neutropenic enterocolitis (typhlitis) is a \"life-threatening gastrointestinal complication of chemotherapy.\" Typhlitis is an intestinal infection which may manifest itself through symptoms including nausea, vomiting, diarrhea, a distended abdomen, fever, chills, or abdominal pain and tenderness.\n\nTyphlitis is a medical emergency. It has a very poor prognosis and is often fatal unless promptly recognized and aggressively treated. Successful treatment hinges on early diagnosis provided by a high index of suspicion and the use of CT scanning, nonoperative treatment for uncomplicated cases, and sometimes elective right hemicolectomy to prevent recurrence.\n\nNausea, vomiting, anorexia, diarrhoea, abdominal cramps, and constipation are common side-effects of chemotherapeutic medications that kill fast-dividing cells. Malnutrition and dehydration can result when the recipient does not eat or drink enough, or when the person vomits frequently, because of gastrointestinal damage. This can result in rapid weight loss, or occasionally in weight gain, if the person eats too much in an effort to allay nausea or heartburn. Weight gain can also be caused by some steroid medications. These side-effects can frequently be reduced or eliminated with antiemetic drugs. Self-care measures, such as eating frequent small meals and drinking clear liquids or ginger tea, are often recommended. In general, this is a temporary effect, and frequently resolves within a week of finishing treatment. However, a high index of suspicion is appropriate, since diarrhea and bloating are also symptoms of typhlitis, a very serious and potentially life-threatening medical emergency that requires immediate treatment.\n\nAnemia can be a combined outcome caused by myelosuppressive chemotherapy, and possible cancer-related causes such as bleeding, blood cell destruction (hemolysis), hereditary disease, kidney dysfunction, nutritional\ndeficiencies or anemia of chronic disease. Treatments to mitigate anemia include hormones to boost blood production (erythropoietin), iron supplements, and blood transfusions. Myelosuppressive therapy can cause a tendency to bleed easily, leading to anemia. Medications that kill rapidly dividing cells or blood cells can reduce the number of platelets in the blood, which can result in bruises and bleeding. Extremely low platelet counts may be temporarily boosted through platelet transfusions and new drugs to increase platelet counts during chemotherapy are being developed. Sometimes, chemotherapy treatments are postponed to allow platelet counts to recover.\n\nFatigue may be a consequence of the cancer or its treatment, and can last for months to years after treatment. One physiological cause of fatigue is anemia, which can be caused by chemotherapy, surgery, radiotherapy, primary and metastatic disease or nutritional depletion. Anaerobic exercise has been found to be beneficial in reducing fatigue in people with solid tumours.\n\nNausea and vomiting are two of the most feared cancer treatment-related side-effects for people with cancer and their families. In 1983, Coates et al. found that people receiving chemotherapy ranked nausea and vomiting as the first and second most severe side-effects, respectively. Up to 20% of people receiving highly emetogenic agents in this era postponed, or even refused, potentially curative treatments. Chemotherapy-induced nausea and vomiting (CINV) are common with many treatments and some forms of cancer. Since the 1990s, several novel classes of antiemetics have been developed and commercialized, becoming a nearly universal standard in chemotherapy regimens, and helping to successfully manage these symptoms in many people. Effective mediation of these unpleasant and sometimes-crippling symptoms results in increased quality of life for the recipient and more efficient treatment cycles, due to less stoppage of treatment due to better tolerance and better overall health.\n\nHair loss (alopecia) can be caused by chemotherapy that kills rapidly dividing cells; other medications may cause hair to thin. These are most often temporary effects: hair usually starts to regrow a few weeks after the last treatment, but sometimes with a change in colour, texture, thickness or style. Sometimes hair has a tendency to curl after regrowth, resulting in \"chemo curls.\" Severe hair loss occurs most often with drugs such as doxorubicin, daunorubicin, paclitaxel, docetaxel, cyclophosphamide, ifosfamide and etoposide. Permanent thinning or hair loss can result from some standard chemotherapy regimens.\n\nChemotherapy induced hair loss occurs by a non-androgenic mechanism, and can manifest as alopecia totalis, telogen effluvium, or less often alopecia areata. It is usually associated with systemic treatment due to the high mitotic rate of hair follicles, and more reversible than androgenic hair loss, although permanent cases can occur. Chemotherapy induces hair loss in women more often than men.\n\nScalp cooling offers a means of preventing both permanent and temporary hair loss; however, concerns about this method have been raised.\n\nDevelopment of secondary neoplasia after successful chemotherapy or radiotherapy treatment can occur. The most common secondary neoplasm is secondary acute myeloid leukemia, which develops primarily after treatment with alkylating agents or topoisomerase inhibitors. Survivors of childhood cancer are more than 13 times as likely to get a secondary neoplasm during the 30 years after treatment than the general population. Not all of this increase can be attributed to chemotherapy.\n\nSome types of chemotherapy are gonadotoxic and may cause infertility. Chemotherapies with high risk include procarbazine and other alkylating drugs such as cyclophosphamide, ifosfamide, busulfan, melphalan, chlorambucil, and chlormethine. Drugs with medium risk include doxorubicin and platinum analogs such as cisplatin and carboplatin. On the other hand, therapies with low risk of gonadotoxicity include plant derivatives such as vincristine and vinblastine, antibiotics such as bleomycin and dactinomycin, and antimetabolites such as methotrexate, mercaptopurine, and 5-fluorouracil.\n\nFemale infertility by chemotherapy appears to be secondary to premature ovarian failure by loss of primordial follicles. This loss is not necessarily a direct effect of the chemotherapeutic agents, but could be due to an increased rate of growth initiation to replace damaged developing follicles.\n\nPeople may choose between several methods of fertility preservation prior to chemotherapy, including cryopreservation of semen, ovarian tissue, oocytes, or embryos. As more than half of cancer patients are elderly, this adverse effect is only relevant for a minority of patients. A study in France between 1999 and 2011 came to the result that embryo freezing before administration of gonadotoxic agents to females caused a delay of treatment in 34% of cases, and a live birth in 27% of surviving cases who wanted to become pregnant, with the follow-up time varying between 1 and 13 years.\n\nPotential protective or attenuating agents include GnRH analogs, where several studies have shown a protective effect \"in vivo\" in humans, but some studies show no such effect. Sphingosine-1-phosphate (S1P) has shown similar effect, but its mechanism of inhibiting the sphingomyelin apoptotic pathway may also interfere with the apoptosis action of chemotherapy drugs.\n\nIn chemotherapy as a conditioning regimen in hematopoietic stem cell transplantation, a study of people conditioned with cyclophosphamide alone for severe aplastic anemia came to the result that ovarian recovery occurred in all women younger than 26 years at time of transplantation, but only in five of 16 women older than 26 years.\n\nChemotherapy is teratogenic during pregnancy, especially during the first trimester, to the extent that abortion usually is recommended if pregnancy in this period is found during chemotherapy. Second- and third-trimester exposure does not usually increase the teratogenic risk and adverse effects on cognitive development, but it may increase the risk of various complications of pregnancy and fetal myelosuppression.\n\nIn males previously having undergone chemotherapy or radiotherapy, there appears to be no increase in genetic defects or congenital malformations in their children conceived after therapy. The use of assisted reproductive technologies and micromanipulation techniques might increase this risk. In females previously having undergone chemotherapy, miscarriage and congenital malformations are not increased in subsequent conceptions. However, when in vitro fertilization and embryo cryopreservationis practised between or shortly after treatment, possible genetic risks to the growing oocytes exist, and hence it has been recommended that the babies be screened.\n\nBetween 30 and 40 percent of people undergoing chemotherapy experience chemotherapy-induced peripheral neuropathy (CIPN), a progressive, enduring, and often irreversible condition, causing pain, tingling, numbness and sensitivity to cold, beginning in the hands and feet and sometimes progressing to the arms and legs. Chemotherapy drugs associated with CIPN include thalidomide, epothilones, \"vinca\" alkaloids, taxanes, proteasome inhibitors, and the platinum-based drugs. Whether CIPN arises, and to what degree, is determined by the choice of drug, duration of use, the total amount consumed and whether the person already has peripheral neuropathy. Though the symptoms are mainly sensory, in some cases motor nerves and the autonomic nervous system are affected. CIPN often follows the first chemotherapy dose and increases in severity as treatment continues, but this progression usually levels off at completion of treatment. The platinum-based drugs are the exception; with these drugs, sensation may continue to deteriorate for several months after the end of treatment. Some CIPN appears to be irreversible. Pain can often be managed with drug or other treatment but the numbness is usually resistant to treatment.\n\nSome people receiving chemotherapy report fatigue or non-specific neurocognitive problems, such as an inability to concentrate; this is sometimes called post-chemotherapy cognitive impairment, referred to as \"chemo brain\" in popular and social media.\n\nIn particularly large tumors and cancers with high white cell counts, such as lymphomas, teratomas, and some leukemias, some people develop tumor lysis syndrome. The rapid breakdown of cancer cells causes the release of chemicals from the inside of the cells. Following this, high levels of uric acid, potassium and phosphate are found in the blood. High levels of phosphate induce secondary hypoparathyroidism, resulting in low levels of calcium in the blood. This causes kidney damage and the high levels of potassium can cause cardiac arrhythmia. Although prophylaxis is available and is often initiated in people with large tumors, this is a dangerous side-effect that can lead to death if left untreated.\n\nCardiotoxicity (heart damage) is especially prominent with the use of anthracycline drugs (doxorubicin, epirubicin, idarubicin, and liposomal doxorubicin). The cause of this is most likely due to the production of free radicals in the cell and subsequent DNA damage. Other chemotherapeutic agents that cause cardiotoxicity, but at a lower incidence, are cyclophosphamide, docetaxel and clofarabine.\n\nHepatotoxicity (liver damage) can be caused by many cytotoxic drugs. The susceptibility of an individual to liver damage can be altered by other factors such as the cancer itself, viral hepatitis, immunosuppression and nutritional deficiency. The liver damage can consist of damage to liver cells, hepatic sinusoidal syndrome (obstruction of the veins in the liver), cholestasis (where bile does not flow from the liver to the intestine) and liver fibrosis.\n\nNephrotoxicity (kidney damage) can be caused by tumor lysis syndrome and also due direct effects of drug clearance by the kidneys. Different drugs will affect different parts of the kidney and the toxicity may be asymptomatic (only seen on blood or urine tests) or may cause acute renal failure.\n\nOtotoxicity (damage to the inner ear) is a common side effect of platinum based drugs that can produce symptoms such as dizziness and vertigo.\n\nLess common side-effects include red skin (erythema), dry skin, damaged fingernails, a dry mouth (xerostomia), water retention, and sexual impotence. Some medications can trigger allergic or pseudoallergic reactions.\n\nSpecific chemotherapeutic agents are associated with organ-specific toxicities, including cardiovascular disease (e.g., doxorubicin), interstitial lung disease (e.g., bleomycin) and occasionally secondary neoplasm (e.g., MOPP therapy for Hodgkin's disease).\n\nHand-foot syndrome is another side effect to cytotoxic chemotherapy.\n\nChemotherapy does not always work, and even when it is useful, it may not completely destroy the cancer. People frequently fail to understand its limitations. In one study of people who had been newly diagnosed with incurable, stage 4 cancer, more than two-thirds of people with lung cancer and more than four-fifths of people with colorectal cancer still believed that chemotherapy was likely to cure their cancer.\n\nThe blood–brain barrier poses an obstacle to delivery of chemotherapy to the brain. This is because the brain has an extensive system in place to protect it from harmful chemicals. Drug transporters can pump out drugs from the brain and brain's blood vessel cells into the cerebrospinal fluid and blood circulation. These transporters pump out most chemotherapy drugs, which reduces their efficacy for treatment of brain tumors. Only small lipophilic alkylating agents such as lomustine or temozolomide are able to cross this blood–brain barrier.\n\nBlood vessels in tumors are very different from those seen in normal tissues. As a tumor grows, tumor cells furthest away from the blood vessels become low in oxygen (hypoxic). To counteract this they then signal for new blood vessels to grow. The newly formed tumor vasculature is poorly formed and does not deliver an adequate blood supply to all areas of the tumor. This leads to issues with drug delivery because many drugs will be delivered to the tumor by the circulatory system.\n\nResistance is a major cause of treatment failure in chemotherapeutic drugs. There are a few possible causes of resistance in cancer, one of which is the presence of small pumps on the surface of cancer cells that actively move chemotherapy from inside the cell to the outside. Cancer cells produce high amounts of these pumps, known as p-glycoprotein, in order to protect themselves from chemotherapeutics. Research on p-glycoprotein and other such chemotherapy efflux pumps is currently ongoing. Medications to inhibit the function of p-glycoprotein are undergoing investigation, but due to toxicities and interactions with anti-cancer drugs their development has been difficult. Another mechanism of resistance is gene amplification, a process in which multiple copies of a gene are produced by cancer cells. This overcomes the effect of drugs that reduce the expression of genes involved in replication. With more copies of the gene, the drug can not prevent all expression of the gene and therefore the cell can restore its proliferative ability. Cancer cells can also cause defects in the cellular pathways of apoptosis (programmed cell death). As most chemotherapy drugs kill cancer cells in this manner, defective apoptosis allows survival of these cells, making them resistant. Many chemotherapy drugs also cause DNA damage, which can be repaired by enzymes in the cell that carry out DNA repair. Upregulation of these genes can overcome the DNA damage and prevent the induction of apoptosis. Mutations in genes that produce drug target proteins, such as tubulin, can occur which prevent the drugs from binding to the protein, leading to resistance to these types of drugs. Drugs used in chemotherapy can induce cell stress, which can kill a cancer cell; however, under certain conditions, cells stress can induce changes in gene expression that enables resistance to several types of drugs.\n\nTargeted therapies are a relatively new class of cancer drugs that can overcome many of the issues seen with the use of cytotoxics. They are divided into two groups: small molecule and antibodies. The massive toxicity seen with the use of cytotoxics is due to the lack of cell specificity of the drugs. They will kill any rapidly dividing cell, tumor or normal. Targeted therapies are designed to affect cellular proteins or processes that are utilised by the cancer cells. This allows a high dose to cancer tissues with a relatively low dose to other tissues. Although the side effects are often less severe than that seen of cytotoxic chemotherapeutics, life-threatening effects can occur. Initially, the targeted therapeutics were supposed to be solely selective for one protein. Now it is clear that there is often a range of protein targets that the drug can bind. An example target for targeted therapy is the protein produced by the Philadelphia chromosome, a genetic lesion found commonly in chronic myelomonocytic leukemia. This fusion protein has enzyme activity that can be inhibited by imatinib, a small molecule drug.\n\nCancer is the uncontrolled growth of cells coupled with malignant behaviour: invasion and metastasis (among other features). It is caused by the interaction between genetic susceptibility and environmental factors. These factors lead to accumulations of genetic mutations in oncogenes (genes that control the growth rate of cells) and tumor suppressor genes (genes that help to prevent cancer), which gives cancer cells their malignant characteristics, such as uncontrolled growth.\n\nIn the broad sense, most chemotherapeutic drugs work by impairing mitosis (cell division), effectively targeting fast-dividing cells. As these drugs cause damage to cells, they are termed \"cytotoxic\". They prevent mitosis by various mechanisms including damaging DNA and inhibition of the cellular machinery involved in cell division. One theory as to why these drugs kill cancer cells is that they induce a programmed form of cell death known as apoptosis.\n\nAs chemotherapy affects cell division, tumors with high growth rates (such as acute myelogenous leukemia and the aggressive lymphomas, including Hodgkin's disease) are more sensitive to chemotherapy, as a larger proportion of the targeted cells are undergoing cell division at any time. Malignancies with slower growth rates, such as indolent lymphomas, tend to respond to chemotherapy much more modestly. Heterogeneic tumours may also display varying sensitivities to chemotherapy agents, depending on the subclonal populations within the tumor.\n\nCells from the immune system also make crucial contributions to the antitumor effects of chemotherapy. For example, the chemotherapeutic drugs oxaliplatin and cyclophosphamide can cause tumor cells to die in a way that is detectable by the immune system (called immunogenic cell death), which mobilizes immune cells with antitumor functions. Chemotherapeutic drugs that cancer immunogenic tumor cell death can make unresponsive tumors sensitive to immune checkpoint therapy.\n\nSome chemotherapy drugs are used in diseases other than cancer, such as in autoimmune disorders, and noncancerous plasma cell dyscrasia. In some cases they are often used at lower doses, which means that the side effects are minimized, while in other cases doses similar to ones used to treat cancer are used. Methotrexate is used in the treatment of rheumatoid arthritis (RA), psoriasis, ankylosing spondylitis and multiple sclerosis. The anti-inflammatory response seen in RA is thought to be due to increases in adenosine, which causes immunosuppression; effects on immuno-regulatory cyclooxygenase-2 enzyme pathways; reduction in pro-inflammatory cytokines; and anti-proliferative properties. Although methotrexate is used to treat both multiple sclerosis and ankylosing spondylitis, its efficacy in these diseases is still uncertain. Cyclophosphamide is sometimes used to treat lupus nephritis, a common symptom of systemic lupus erythematosus. Dexamethasone along with either bortezomib or melphalan is commonly used as a treatment for AL amyloidosis. Recently, bortezomid in combination with cyclophosphamide and dexamethasone has also shown promise as a treatment for AL amyloidosis. Other drugs used to treat myeloma such as lenalidomide have shown promise in treating AL amyloidosis.\n\nChemotherapy drugs are also used in conditioning regimens prior to bone marow transplant (hematopoietic stem cell transplant). Conditioning regimens are used to suppress the recipient's immune system in order to allow a transplant to engraft. Cyclophosphamide is a common cytotoxic drug used in this manner, and is often used in conjunction with total body irradiation. Chemotherapeutic drugs may be used at high doses to permanently remove the recipient's bone marrow cells (myeloablative conditioning) or at lower doses that will prevent permanent bone marrow loss (non-myeloablative and reduced intensity conditioning). When used in non-cancer setting, the treatment is still called \"chemotherapy\", and is often done in the same treatment centers used for people with cancer.\n\nIn the 1970s, antineoplastic drugs were identified as hazardous, and the American Society of Health-System Pharmacists (ASHP) has since then introduced the concept of hazardous drugs after publishing a recommendation in 1983 regarding handling hazardous drugs. The adaptation of federal regulations came when the Occupational Safety and Health Administration (OSHA) first released its guidelines in 1986 and then updated them in 1996, 1999, and, most recently, 2006. The National Institute for Occupational Safety and Health (NIOSH) has been conducting an assessment in the workplace since then regarding these drugs. Occupational exposure to antineoplastic drugs has been linked to multiple health effects, including infertility and possible carcinogenic effects. A few cases have been reported by the NIOSH alert report, such as one in which a female pharmacist was diagnosed with papillary transitional cell carcinoma. Twelve years before the pharmacist was diagnosed with the condition, she had worked for 20 months in a hospital where she was responsible for preparing multiple antineoplastic drugs. The pharmacist didn't have any other risk factor for cancer, and therefore, her cancer was attributed to the exposure to the antineoplastic drugs, although a cause-and-effect relationship has not been established in the literature. Another case happened when a malfunction in biosafety cabinetry is believed to have exposed nursing personnel to antineoplastic drugs. Investigations revealed evidence of genotoxic biomarkers two and nine months after that exposure.\n\nAntineoplastic drugs are usually given through intravenous, intramuscular. intrathecal, or subcutaneous administration. In most cases, before the medication is administered to the patient, it needs to be prepared and handled by several workers. Any worker who is involved in handling, preparing, or administering the drugs, or with cleaning objects that have come into contact with antineoplastic drugs, is potentially exposed to hazardous drugs. Health care workers are exposed to drugs in different circumstances, such as when pharmacists and pharmacy technicians prepare and handle antineoplastic drugs and when nurses and physicians administer the drugs to patients. Additionally, those who are responsible for disposing antineoplastic drugs in health care facilities are also at risk of exposure.\n\nDermal exposure is thought to be the main route of exposure due to the fact that significant amounts of the antineoplastic agents have been found in the gloves worn by healthcare workers who prepare, handle, and administer the agents. Another noteworthy route of exposure is inhalation of the drugs' vapors. Multiple studies have investigated inhalation as a route of exposure, and although air sampling has not shown any dangerous levels, it is still a potential route of exposure. Ingestion by hand to mouth is a route of exposure that is less likely compared to others because of the enforced hygienic standard in the health institutions. However, it is still a potential route, especially in the workplace, outside of a health institute. One can also be exposed to these hazardous drugs through injection by needle sticks. Research conducted in this area has established that occupational exposure occurs by examining evidence in multiple urine samples from health care workers.\n\nHazardous drugs expose health care workers to serious health risks. Many studies show that antineoplastic drugs could have many side effects on the reproductive system, such as fetal loss, congenital malformation, and infertility. Health care workers who are exposed to antineoplastic drugs on many occasions have adverse reproductive outcomes such as spontaneous abortions, stillbirths, and congenital malformations. Moreover, studies have shown that exposure to these drugs leads to menstrual cycle irregularities. Antineoplastic drugs may also increase the risk of learning disabilities among children of health care workers who are exposed to these hazardous substances. \n\nMoreover, these drugs have carcinogenic effects. In the past five decades, multiple studies have shown the carcinogenic effects of exposure to antineoplastic drugs. Similarly, there have been research studies that linked alkylating agents with humans developing leukemias. Studies have reported elevated risk of breast cancer, nonmelanoma skin cancer, and cancer of the rectum among nurses who are exposed to these drugs. Other investigations revealed that there is a potential genotoxic effect from anti-neoplastic drugs to workers in health care settings.\n\nAs of 2018, there were no occupational exposure limits set for antineoplastic drugs, i.e., OSHA or the American Conference of Governmental Industrial Hygienists (ACGIH) have not set workplace safety guidelines.\n\nNIOSH recommends using a ventilated cabinet that is designed to decrease worker exposure. Additionally, it recommends training of all staff, the use of cabinets, implementing an initial evaluation of the technique of the safety program, and wearing protective gloves and gowns when opening drug packaging, handling vials, or labeling. When wearing personal protective equipment, one should inspect gloves for physical defects before use and always wear double gloves and protective gowns. Health care workers are also required to wash their hands with water and soap before and after working with antineoplastic drugs, change gloves every 30 minutes or whenever punctured, and discard them immediately in a chemotherapy waste container.\n\nThe gowns used should be disposable gowns made of polyethylene-coated polypropylene. When wearing gowns, individuals should make sure that the gowns are closed and have long sleeves. When preparation is done, the final product should be completely sealed in a plastic bag.\n\nThe health care worker should also wipe all waste containers inside the ventilated cabinet before removing them from the cabinet. Finally, workers should remove all protective wear and put them in a bag for their disposal inside the ventilated cabinet.\n\nDrugs should only be administered using protective medical devices such as needle lists and closed systems and techniques such as priming of IV tubing by pharmacy personnel inside a ventilated cabinet. Workers should always wear personal protective equipment such as double gloves, goggles, and protective gowns when opening the outer bag and assembling the delivery system to deliver the drug to the patient, and when disposing of all material used in the administration of the drugs. \n\nHospital workers should never remove tubing from an IV bag that contains an antineoplastic drug, and when disconnecting the tubing in the system, they should make sure the tubing has been thoroughly flushed. After removing the IV bag, the workers should place it together with other disposable items directly in the yellow chemotherapy waste container with the lid closed. Protective equipment should be removed and put into a disposable chemotherapy waste container. After this has been done, one should double bag the chemotherapy waste before or after removing one’s inner gloves. Moreover, one must always wash one’s hands with soap and water before leaving the drug administration site.\n\nAll employees whose jobs in health care facilities expose them to hazardous drugs must receive training. Training should include shipping and receiving personnel, housekeepers, pharmacists, assistants, and all individuals involved in the transportation and storage of antineoplastic drugs. These individuals should receive information and training to inform them of the hazards of the drugs present in their areas of work. They should be informed and trained on operations and procedures in their work areas where they can encounter hazards, different methods used to detect the presence of hazardous drugs and how the hazards are released, and the physical and health hazards of the drugs, including their reproductive and carcinogenic hazard potential. Additionally, they should be informed and trained on the measures they should take to avoid and protect themselves from these hazards. This information ought to be provided when health care workers come into contact with the drugs, that is, perform the initial assignment in a work area with hazardous drugs. Moreover, training should also be provided when new hazards emerge as well as when new drugs, procedures, or equipment are introduced.\n\nWhen performing cleaning and decontaminating the work area where antineoplastic drugs are used, one should make sure that there is sufficient ventilation to prevent the buildup of airborne drug concentrations. When cleaning the work surface, hospital workers should use deactivation and cleaning agents before and after each activity as well as at the end of their shifts. Cleaning should always be done using double protective gloves and disposable gowns. After employees finish up cleaning, they should dispose of the items used in the activity in a yellow chemotherapy waste container while still wearing protective gloves. After removing the gloves, they should thoroughly wash their hands with soap and water. Anything that comes into contact or has a trace of the antineoplastic drugs, such as needles, empty vials, syringes, gowns, and gloves, should be put in the chemotherapy waste container.\n\nA written policy needs to be in place in case of a spill of antineoplastic products. The policy should address the possibility of various sizes of spills as well as the procedure and personal protective equipment required for each size. A trained worker should handle a large spill and always dispose of all cleanup materials in the chemical waste container according to EPA regulations, not in a yellow chemotherapy waste container.\n\nA medical surveillance program must be established. In case of exposure, occupational health professionals need to ask for a detailed history and do a thorough physical exam. They should test the urine of the potentially exposed worker by doing a urine dipstick or microscopic examination, mainly looking for blood, as several antineoplastic drugs are known to cause bladder damage.\n\nUrinary mutagenicity is a marker of exposure to antineoplastic drugs that was first used by Falck and colleagues in 1979 and uses bacterial mutagenicity assays. Apart from being nonspecific, the test can be influenced by extraneous factors such as dietary intake and smoking and is, therefore, used sparingly. However, the test played a significant role in changing the use of horizontal flow cabinets to vertical flow biological safety cabinets during the preparation of antineoplastic drugs because the former exposed health care workers to high levels of drugs. This changed the handling of drugs and effectively reduced workers’ exposure to antineoplastic drugs. \n\nBiomarkers of exposure to antineoplastic drugs commonly include urinary platinum, methotrexate, urinary cyclophosphamide and ifosfamide, and urinary metabolite of 5-fluorouracil. In addition to this, there are other drugs used to measure the drugs directly in the urine, although they are rarely used. A measurement of these drugs directly in one’s urine is a sign of high exposure levels and that an uptake of the drugs is happening either through inhalation or dermally.  \n\nThere is an extensive list of antineoplastic agents. Several classification schemes have been used to subdivide the medicines used for cancer into several different types.\n\nThe first use of small-molecule drugs to treat cancer was in the early 20th century, although the specific chemicals first used were not originally intended for that purpose. Mustard gas was used as a chemical warfare agent during World War I and was discovered to be a potent suppressor of hematopoiesis (blood production). A similar family of compounds known as nitrogen mustards were studied further during World War II at the Yale School of Medicine. It was reasoned that an agent that damaged the rapidly growing white blood cells might have a similar effect on cancer. Therefore, in December 1942, several people with advanced lymphomas (cancers of the lymphatic system and lymph nodes) were given the drug by vein, rather than by breathing the irritating gas. Their improvement, although temporary, was remarkable. Concurrently, during a military operation in World War II, following a German air raid on the Italian harbour of Bari, several hundred people were accidentally exposed to mustard gas, which had been transported there by the Allied forces to prepare for possible retaliation in the event of German use of chemical warfare. The survivors were later found to have very low white blood cell counts. After WWII was over and the reports declassified, the experiences converged and led researchers to look for other substances that might have similar effects against cancer. The first chemotherapy drug to be developed from this line of research was mustine. Since then, many other drugs have been developed to treat cancer, and drug development has exploded into a multibillion-dollar industry, although the principles and limitations of chemotherapy discovered by the early researchers still apply.\n\nThe word \"chemotherapy\" without a modifier usually refers to cancer treatment, but its historical meaning was broader. The term was coined in the early 1900s by Paul Ehrlich as meaning any use of chemicals to treat any disease (\"chemo-\" + \"-therapy\"), such as the use of antibiotics (\"antibacterial chemotherapy\"). Ehrlich was not optimistic that effective chemotherapy drugs would be found for the treatment of cancer. The first modern chemotherapeutic agent was arsphenamine, an arsenic compound discovered in 1907 and used to treat syphilis. This was later followed by sulfonamides (sulfa drugs) and penicillin. In today's usage, the sense \"any treatment of disease with drugs\" is often expressed with the word \"pharmacotherapy\".\n\nThe top 10 best-selling (in terms of revenue) cancer drugs of 2013:\n\nSpecially targeted delivery vehicles aim to increase effective levels of chemotherapy for tumor cells while reducing effective levels for other cells. This should result in an increased tumor kill or reduced toxicity or both.\n\nAntibody-drug conjugates (ADCs) comprise an antibody, drug and a linker between them. The antibody will be targeted at a preferentially expressed protein in the tumour cells (known as a tumor antigen) or on cells that the tumor can utilise, such as blood vessel endothelial cells. They bind to the tumor antigen and are internalised, where the linker releases the drug into the cell. These specially targeted delivery vehicles vary in their stability, selectivity, and choice of target, but, in essence, they all aim to increase the maximum effective dose that can be delivered to the tumor cells. Reduced systemic toxicity means that they can also be used in people who are sicker, and that they can carry new chemotherapeutic agents that would have been far too toxic to deliver via traditional systemic approaches.\n\nThe first approved drug of this type was gemtuzumab ozogamicin (Mylotarg), released by Wyeth (now Pfizer). The drug was approved to treat acute myeloid leukemia, but has now been withdrawn from the market because the drug did not meet efficacy targets in further clinical trials. Two other drugs, trastuzumab emtansine and brentuximab vedotin, are both in late clinical trials, and the latter has been granted accelerated approval for the treatment of refractory Hodgkin's lymphoma and systemic anaplastic large cell lymphoma.\n\nNanoparticles are 1–1000 nanometer (nm) sized particles that can promote tumor selectivity and aid in delivering low-solubility drugs. Nanoparticles can be targeted passively or actively. Passive targeting exploits the difference between tumor blood vessels and normal blood vessels. Blood vessels in tumors are \"leaky\" because they have gaps from 200–2000 nm, which allow nanoparticles to escape into the tumor. Active targeting uses biological molecules (antibodies, proteins, DNA and receptor ligands) to preferentially target the nanoparticles to the tumor cells. There are many types of nanoparticle delivery systems, such as silica, polymers, liposomes and magnetic particles. Nanoparticles made of magnetic material can also be used to concentrate agents at tumor sites using an externally applied magnetic field. They have emerged as a useful vehicle in magnetic drug delivery for poorly soluble agents such as paclitaxel.\n\nElectrochemotherapy is the combined treatment in which injection of a chemotherapeutic drug is followed by application of high-voltage electric pulses locally to the tumor. The treatment enables the chemotherapeutic drugs, which otherwise cannot or hardly go through the membrane of cells (such as bleomycin and cisplatin), to enter the cancer cells. Hence, greater effectiveness of antitumor treatment is achieved.\n\nClinical electrochemotherapy has been successfully used for treatment of cutaneous and subcutaneous tumors irrespective of their histological origin. The method has been reported as safe, simple and highly effective in all reports on clinical use of electrochemotherapy. According to the ESOPE project (European Standard Operating Procedures of Electrochemotherapy), the Standard Operating Procedures (SOP) for electrochemotherapy were prepared, based on the experience of the leading European cancer centres on electrochemotherapy. Recently, new electrochemotherapy modalities have been developed for treatment of internal tumors using surgical procedures, endoscopic routes or percutaneous approaches to gain access to the treatment area.\n\nHyperthermia therapy is heat treatment for cancer that can be a powerful tool when used in combination with chemotherapy (thermochemotherapy) or radiation for the control of a variety of cancers. The heat can be applied locally to the tumor site, which will dilate blood vessels to the tumor, allowing more chemotherapeutic medication to enter the tumor. Additionally, the tumor cell membrane will become more porous, further allowing more of the chemotherapeutic medicine to enter the tumor cell.\n\nHyperthermia has also been shown to help prevent or reverse \"chemo-resistance.\" Chemotherapy resistance sometimes develops over time as the tumors adapt and can overcome the toxicity of the chemo medication. \"Overcoming chemoresistance has been extensively studied within the past, especially using CDDP-resistant cells. In regard to the potential benefit that drug-resistant cells can be recruited for effective therapy by combining chemotherapy with hyperthermia, it was important to show that chemoresistance against several anticancer drugs (e.g. mitomycin C, anthracyclines, BCNU, melphalan) including CDDP could be reversed at least partially by the addition of heat.\n\nChemotherapy is used in veterinary medicine similar to how it is used in human medicine.\n\n"}
{"id": "47939168", "url": "https://en.wikipedia.org/wiki?curid=47939168", "title": "Circus Remedy", "text": "Circus Remedy\n\nCircus Remedy is a circus outreach Non-Profit Organization founded in 2006 by friends Anthony Lucero, Terry Notary, and Christine Harnos. Circus Remedy has made visits to ailing children worldwide, including children's hospitals throughout North America, The Children's Aid Society in Bronx, NY, orphanages in Tanzania and South Africa, Amma's (Mata Amritanandamayi) hospitals and orphanages in Kerala, India, Paul Newman's Hole in the Wall Gang Camp in Ashford, CT, and IDP camps in Eastern Europe. Circus Remedy sponsors partner organizations such as the Afghan Mini Mobile Circus in Kabul, Afghanistan, and Circus Harmony's ongoing collaboration with the Jewish and Arab Galilee Youth Circus in Israel. Circus Remedy has produced two short films, \"Bipsy Twirlarina and the Make Believe Kid\", and \"The Fable of Profitt the Fox\", both for children's hospitals. In 2009, Circus Remedy introduced 'The Little Hands Project' to California schools, bridging healthy children to children with illness.\n"}
{"id": "9341141", "url": "https://en.wikipedia.org/wiki?curid=9341141", "title": "Clinic management system", "text": "Clinic management system\n\nClinic management systems are computer software products that coordinate and integrate all the inherent activities involved in the management and running of a healthcare facility. They must meet specified security, technology and functionality standards for managing electronic medical records and practice management information. Some systems maintain the complete Patient Information coupled with the electronic medical records (EMR), medical billing, EDI/HCFA claim sending and meeting the stipulated security, technology & functional standards.\n\nAside from general patient care, a clinic may also participate in running clinical trials. In order to manage the additional responsibilities in managing a clinical trial, a clinic may also use a CTMS or clinical data management system (CDMS) in addition to and combined with their EMR and billing systems.\n\nHealthcare computer system, commonly known as clinic management system, is created to computerize manual operations in clinics. The primary purpose is to digitize patient records so as to make data retrieval easy and efficient. Being in the digital form, patient data can be conveniently shared and accessed by multiple simultaneous users at different locations, resulting in smoother clinical operations and collaboration among clinicians. It also means that patient data can be easily backed up, and be protected for confidentiality and from tampering through access control. In addition, clinical tasks involving panel billing, inventory management and accounting are all made easy, and in some manner automated. Economically, clinics benefit from constant cost savings as a result of increased productivity and overall efficiency. Essentially, everyone in the clinic benefits from the system – doctor, nurse, clerk, administrator and the clinic owner – which makes their lives easy and removes unnecessary human errors from their daily activities.\n\n"}
{"id": "7759491", "url": "https://en.wikipedia.org/wiki?curid=7759491", "title": "Crown-rump length", "text": "Crown-rump length\n\nCrown-rump length (CRL) is the measurement of the length of human embryos and fetuses from the top of the head (crown) to the bottom of the buttocks (rump). It is typically determined from ultrasound imagery and can be used to estimate gestational age.\n\nThe embryo and fetus float in the amniotic fluid inside the uterus of the mother usually in a curved posture resembling the letter \"C\". The measurement can actually vary slightly if the fetus is temporarily stretching (straightening) its body. The measurement needs to be in the natural state with an unstretched body which is actually \"C\" shaped. The measurement of CRL is useful in determining the gestational age (menstrual age starting from the first day of the last menstrual period) and thus the expected date of delivery (EDD). Different babies do grow at different rates and thus the gestational age is an approximation. Recent evidence has indicated that CRL growth (and thus the approximation of gestational age) may be influenced by maternal factors such as age, smoking, and folic acid intake. Early in pregnancy it is accurate within +/- 4 days but later in pregnancy due to different growth rates, the accuracy is less. In that situation, other parameters can be used in addition to CRL. The length of the umbilical cord is approximately equal to the CRL throughout pregnancy.\n\nGestational age is not the same as fertilization age. It takes about 14 days from the first day of the last menstrual period for conception to take place and thus for the conceptus to form. The age from this point in time (conception) is called the fertilization age and is thus 2 weeks shorter than the gestational age. Thus a 6-week gestational age would be a 4-week fertilization age. Some authorities however casually interchange these terms and the reader is advised to be cautious. An average gestational period (duration of pregnancy from the first day of the last menstrual period up to delivery) is 280 days. On average, this is 9 months and 6 days.\n\nThe following formula is an approximation that can be used up to 14 weeks of gestational age, with CRL in mm and gestational age in days:\n\nGestational age = (CRL x 1.037) x 8.052 + 23.73\n\n"}
{"id": "34827520", "url": "https://en.wikipedia.org/wiki?curid=34827520", "title": "Declaration of Sexual Rights", "text": "Declaration of Sexual Rights\n\nThe Declaration of Sexual Rights is a statement on sexual rights that was first proclaimed at the 13th World Congress of Sexology, run by the World Association for Sexual Health, in Valencia 1997. It was revised and expanded in 2014.\n\nThe 2014 version names 16 positions:\n\n\n"}
{"id": "25714482", "url": "https://en.wikipedia.org/wiki?curid=25714482", "title": "Disability abuse", "text": "Disability abuse\n\nDisability abuse is when a person with a disability is abused physically, financially, sexually and/or psychologically due to the person having a disability. Since many disabilities are not visible (for example, asthma, schizophrenia, or learning disabilities) some abusers cannot rationalise the non-physical disability with a need for understanding, support, and so on. Since some disabled people are in need of additional support from others throughout their lives, they are also vulnerable to neglect. Disability abuse has also been considered a hate crime. The abuse is not limited to those who are visibly disabled such as wheelchair-users or physically deformed such as those with a cleft lip but also those with learning disabilities or difficulties such as dyslexia and dysgraphia, and other disabilities, including autism spectrum disorder, Down syndrome and developmental coordination disorder. In the latter case, this is linked to a poor ability in physical education, and this behaviour can be encouraged by an unthinking physical education teacher. Abuse of the disabled is not limited to schools. There are many known cases in which the disabled have been abused by staff of a care institution, such as the case revealed in a BBC \"Panorama\" programme on a Castlebeck care home (Winterbourne View) near Bristol which led to its closure and the suspension or dismissal of some of the staff.\n\nThose with learning disabilities are often not as able to explain things to other people so are more likely to be disbelieved or ignored if they do complain. There was one study done that shows 60 percent of the children with disabilities come forth about being bullied regularly, versus 25 percent of the students who are being bullied with no disabilities. This can also affect their learning and school and education. Their grades are more at risk in dropping, they have a more difficult time concentrating, and there is no interest in school and the learning material. All of this can lead to the child dropping out of school.\n\nThere have been numerous cases of parents of children with disabilities who have murdered their children because of their disabilities. Sometimes the parents kill themselves alongside their child. It was even advocated by Aristotle in the case of congenital deformity—\"As to the exposure of children, let there be a law that no deformed child shall live.\" and is documented in various indigenous societies. \nDisabled girls and women are particularly vulnerable to abuse.\n\nBullying is an aggressive behavior or attitude towards a person that is unwanted. It is usually a repeated behavior. There is more than just one type of bullying, there is verbal bullying, social, and physical. Verbal bullying consist of name calling a person, teasing, and threatening to cause harm to the person. Social abuse is intentionally leaving a person out while in a social setting or gathering, or taunting the person. Physical abuse consist of hitting or pushing someone, pulling their hair, tripping them, or breaking their belongings intentionally. Bullying happens usually before or after school hours, and it is usually done on campus or on the playground of the school. However, it can also happen while on the school bus or going to and from the school bus. Bullying can also happen through the internet or through the phone as well, this is known as cyber bullying.\n\nA 2012 survey by the Interactive Autism Network found that 63% of children with autism are bullied in the United States.\n\nOver a third of autistic adults said they had been bullied at work in a survey by the UK's National Autistic Society.\n\n82% of children with learning disability in the UK are bullied, according to Mencap, and 79% are scared to go out in case they are bullied.\n\nA survey that was done shows that roughly 7 out of 10 people with disabilities have been abused, and that it is an ongoing problem. It was also found that bullying people with disabilities is a problem that is going on in different countries as well, and that problem is not always surfaced and given attention to.\n\nBullying is also a cause of disability and exacerbates existing disabilities.\n\nBullying can take occur in a variety of forms. They aren't always physical. Verbal bullying and nonverbal bullying are the ones that occur very often. Catherine Thornberry and Karin Olson claim that carers often dehumanize disabled people, taking away their abilities and qualities that make them a person and lowering them to the level of just an object or a thing. They found that the caregivers or assistants are often the ones who are unintentionally bullying the disabled individuals. The caregivers look at the individuals at lower standard than they do other people, leading to their labeling of disabled individuals as a hate crime.\n\nBullying a child with a disability is considered to be harassment.\n\nDisabled people are more vulnerable to sexual abuse than the general population for numerous reasons. As they are less likely to report what has happened to them, their rapists are able to get away with the abuse. Victims often not taken seriously due to ableism which intersects with societal myths about sexual violence, for example, that 'ugly' people aren't raped, since society's beauty standard devalues disability.\n\nAccording to Valenti-Hein & Schwartz, only 3% of sexual abuse cases involving developmentally disabled people are ever reported, more than 90% of developmentally disabled people will experience sexual abuse at some point in their lives, and 49% will experience 10 or more abusive incidents.\n\nA study published in the \"British Journal of Psychiatry\" by Sequeira, Howlin, & Hollins found that sexual abuse is associated with a higher incidence of psychiatric and behavioural disorder in people with learning disabilities in a case-control study. Sexual abuse was associated with increased rates of mental illness and behavioural problems, and with symptoms of post-traumatic stress. Psychological reactions to abuse were similar to those observed in the general population, but with the addition of stereotypical behaviour. The more serious the abuse, the more severe the symptoms that were reported.\n\nSexual abuse is less likely to be reported by individuals with disabilities. The people that surround these individuals are often found to be less likely to report these cases of abuse. Society sees the disabled as weak and vulnerable targets, making it easy for the abuser to not feel guilty or to blame themselves. More often than not people figure they can trust their physicians or doctors who provide care for these individuals. In a clinical study it was found that the physicians would provide poor quality of care to individuals with disabilities. They would suppress the problems instead of addressing them by giving them drugs to make them be quiet. It was also found that physicians were less likely to report sexual abuse or any abuse that they found present on these individuals. The justified these actions by believing that in society that disabled people matter less than any other person.\n\nExtreme disability abuse can lead directly or indirectly to death. In Tanzania, people with albinism are hunted by witchcraft practitioners for their organs.\n\nIn England and Wales over 1700 disability hate crimes were recorded by police in 2011 and 2012, but a review by the Crown Prosecution Service said that they are 'overlooked' and 'under-reported'.\n\n\n"}
{"id": "51391626", "url": "https://en.wikipedia.org/wiki?curid=51391626", "title": "Discretionary food", "text": "Discretionary food\n\nDiscretionary food is a term for foods and drinks not necessary to provide the nutrients the human body's needs, but that may add variety to a person's diet.\n\nAustralia's National Health and Medical Research Council describes discretionary foods as \"“foods and drinks not necessary to provide the nutrients the body needs, but that may add variety. However, many of these are high in saturated fats, sugars, salt and/or alcohol, and are therefore described as energy dense. They can be included sometimes in small amounts by those who are physically active, but are not a necessary part of the diet.”\"\n"}
{"id": "230563", "url": "https://en.wikipedia.org/wiki?curid=230563", "title": "Earthquake preparedness", "text": "Earthquake preparedness\n\nEarthquake preparedness is a set of measures taken at the individual, organisational and societal level to minimise the effects of an earthquake. Preparedness measures can range from securing heavy objects, structural modifications and storing supplies, to having insurance, an emergency kit, and evacuation plans.\n\nPreparedness can consist of survival measures, preparation that will improve survival in the event of an earthquake, or mitigating measures, that seek to minimise the effect of an earthquake. Common survival measures include storing food and water for an emergency, and educating individuals what to do during an earthquake. Mitigating measures can include firmly securing large items of furniture (such as bookcases and large cabinets), TV and computer screens that may otherwise fall over in an earthquake. Likewise, avoiding storing items above beds or sofas reduces the chance of objects falling on individuals.\n\nPlanning for a related tsunami, tsunami preparedness, can also be part of earthquake preparedness.\n\nBuilding codes in earthquake prone areas may have specific requirements designed to increase new buildings' resistance to earthquakes. Older buildings and homes that are not up to code may be modified to increase their resistance. Modification and earthquake resistant design are also employed in elevated freeways and bridges.\n\nCodes are not designed to make buildings earthquake proof in the sense of them suffering zero damage. The goal of most building designs is to reduce earthquake damage to a building such that it protects the lives of occupants and thus tolerance of some limited damage is accepted and considered a necessary tradeoff. A supplement or precursor to retrofitting can be the implementation of earthquake proof furniture.\n\nEarthquake modification techniques and modern building codes are designed to prevent total destruction of buildings for earthquakes of no greater than 8.5 on the Richter Scale. Although the Richter Scale is referenced, the localized shaking intensity is one of the largest factors to be considered in building resiliency.\n\nThe basic theme behind preparedness is to be ready for an earthquake. Preparedness starts with an individual's everyday life and involves items and training that would be useful in an earthquake. Preparedness continues on a continuum from individual preparedness through family preparedness, community preparedness and then business, non-profit and governmental preparedness. Some organisations blend these various levels. Business continuity planning encourages businesses to have a Disaster Recovery Plan. The US FEMA breaks down preparedness generally into a pyramid, with citizens on the foundational bottom, on top of which rests local government, state government and federal government in that order.\n\nChildren may present particular issues and some planning and resources are directly focused on supporting them. The US FEMA has advice noting that \"Disasters can leave children feeling frightened, confused, and insecure\" whether a child has experienced it first hand, had it happen to a friend or simply seen it on television. People with disabilities or other special needs may have special emergency preparation needs. FEMA's suggestions for people with disabilities include having copies of prescriptions, charging devices for medical devices such as motorized wheel chairs and a week's supply of medication readily available. Preparedness can also cover pets.\n\nPreparedness can also encompass psychological preparedness: resources are designed to support both community members affected by a disaster and the disaster workers serving them.\n\nA multi-hazard approach, where communities are prepared for several hazards, are more resilient than single hazard approaches and have been gaining popularity.\n\nLong term power outages can cause damage beyond the original disaster that can be mitigated with emergency generators or other power sources to provide an emergency power system. The United States Department of Energy states: \"homeowners, business owners, and local leaders may have to take an active role in dealing with energy disruptions on their own.\" Major institutions like hospitals, military bases and educational institutions often have extensive backup power systems.\nPreparedness does not stop at home or at school. The United States Department of Health and Human Services addresses specific emergency preparedness issues hospitals may have to respond to, including maintaining a safe temperature, providing adequate electricity for life support systems and even carrying out evacuations under extreme circumstances. FEMA encourages all businesses to have an emergency response plan and the Small Business Administration specifically advises small business owners to also focus emergency preparedness and provides a variety of different worksheets and resources.\n\nLevels of preparedness generally remain low, despite attempts to increase public awareness.\n\nVarious methods exist to promote disaster preparedness, but they are rarely well documented and their efficacy is rarely tested. Hands on training, drills and face-to-face interaction have proven more successful at changing behaviour. Digital methods have also been used, including for examples educational videogames.\n\n\n"}
{"id": "56263767", "url": "https://en.wikipedia.org/wiki?curid=56263767", "title": "Effects of sleep deprivation on college students", "text": "Effects of sleep deprivation on college students\n\nSleep deprivation is simply a condition of having an inadequate amount of sleep. At least 50% of college students exhibit daytime sleepiness due to sleep deprivation compared to 36% of adolescents and adults. College students, on average, get about 6 to 6.9 hours of sleep per night. According to Stanford University's Department for the Diagnosis and Treatment for Sleep Disorders, the recommended amount of sleep needed for college students is around 8 hours. Most college students are sleep deprived, as 70.6% of students reported that they get less than 8 hours of sleep.There are various effects that sleep deprivation can have on college students, including lower academic performance, impaired learning, and decreased physical activity. One primary cause as to why college students experience a lack of sleep is improper sleep hygiene.\n\nInadequate sleep hygiene is one primary cause as to why college students experience sleep deprivation. Sleep hygiene is defined as habits or practices that allow for healthy amounts of sleep on a daily basis. Good sleep hygiene habits include keeping a consistent sleep schedule, having a quiet sleep environment, and avoiding consuming caffeine and alcohol before sleeping. Many students have inadequate sleep hygiene that leads to sleep deprivation. \n\nAccording to a study by the US Department of Health and Human Services, approximately 80% of college students have drunk alcohol, with nearly 40% of the students reporting to \"binge drinking,\" or the consumption of an excessive amount of alcohol over a short amount of time. While alcohol may help with faster sleep, it can affect the quality of the sleep. Alcohol may effect one's REM sleep, the stage of sleep where most dreams and memories occur and are stored. Without REM sleep, one would go straight into deep sleep. This may be why people who have insomnia resort to consuming alcohol in order to fall asleep. However, this can result in alcohol dependence. Relying on alcohol as a sleeping aid can lead to sleepwalk, sleep talk and memory problems. Drinking alcohol has also been linked to causing obstructive sleep apnea, where it causes breathing to repeatedly stop and start during sleep. In addition, it has been proven that alcohol can cause insomnia. Researchers from the University of Missouri School of Medicine observed the effects of alcohol on sleep. The results showed that drinking alcohol boosted the adenosine levels. Adenosine levels go up when one is awake and go down when one is asleep. The participants in this study had extended periods of regular drinking and as a result, sleep came quickly to them. However, the participants woke up in a few hours and suffered from insomnia.\n\nThe blue light that is emitted from the screens of phones, computers and other devices stops the production of melatonin, the hormone that controls the sleep-wake cycle of the circadian rhythm. Reducing the amount of melatonin produced makes it harder to fall and stay asleep. In a 2011 poll conducted by the National Sleep Foundation, it reported that approximately 90% of Americans used technology in the hour before bed. The poll noted that young adults and teenagers were more likely to use cell phones, computers, and video game consoles. Additionally, the authors of the poll found that technology use was connected to sleep patterns. 22% of participants reported going to sleep with cell phone ringers on in their bedroom and 10% reported awakenings in at least a few nights per week due to their cell phones' ringers. Among those with the cell phone ringers on, being awakened by their cell phone was correlated to difficulty sustaining sleep.\n\nA study from the University of Kentucky depicted that more than 78% of college freshmen consume above the recommended amount of caffeine each day. In one group of researchers' study from the Henry Ford Hospital's Sleep and Research Center and Wayne State College of Medicine, they discovered that caffeine consumed at least six hours before bedtime can significantly disrupt sleep. Participants in this study who consumed caffeine right before bedtime, three hours before bedtime, and six hours before bedtime all experienced a shorter night's sleep, lower sleep quality and spent more time awake at night.\n\nThere is evidence that sleep is correlated with GPA. GPA is associated with the students' overall academic performances in their classes. Students who had longer amounts of sleep had higher GPAs than students who had shorter amounts of sleep. In another study, researchers from the Sleep Health Institute and Division of Sleep and Circadian Disorder at the Departments of Medicine and Neurology in the Brigham and Women’s Hospital, created a study to discover how irregular sleep patterns are associated with poorer academic performances. The students who participated in this study kept online diaries of their sleep schedules for 30 days. The researchers divided the participants into two groups: regular sleepers, or those who went to bed and woke up around the same time every day and irregular sleepers, or those who have different sleep patterns every day. Using a scoring index from zero to 100 to calculate a student’s sleep schedule, students with irregular sleeping patterns were given lower scores closer to zero while the students with regular sleeping patterns had scores closer to 100. The researchers found that for every score increase of 10 on the scoring index, the student had an average increase of 0.10 of their GPA. The researchers concluded that college students who do not go to bed or wake up at consistent time are likely to have lower GPAs. The study suggests that irregular sleeping schedules do indeed affect student’s GPA and academic performance.\n\nStudent's physical shape seems to be affected by sleep deprivation as well. Researchers from the Imperial College School of Medicine conducted a randomized controlled study to investigate the effects of a night of sleep deprivation on the physical performance in students. The participants were randomized into two conditions, normal sleep or one night of sleep deprivation. Sleep deprivation was monitored using an online-time stamped questionnaire at 45 minute intervals, done in the participant’s homes. The results show that reaction time and exercise performance were significantly affected by sleep deprivation. The researchers concluded that sleep deprivation can affect physical performance in university students.\n\nDaytime naps offer a potential intervention for sleep deprivation in college students. Napping may improve certain memory tasks as students who were excelling in their academics were more likely to nap than those who had low academic success.\n\nIn one study, participants took a two-credit, 18-week course that included group discussions, lectures, and self-evaluation of topics that included circadian rhythms, sleep hygiene, muscle relaxation, and public sleep education. Participants reported that they had improved sleep quality and sleep hygiene throughout the course.\n\nIn one study, a revised type of cognitive behavioral for treating insomnia was sent to participating students by email. The email contained an attachment addressing several aspects of sleep, such as techniques for relaxation and methods to stabilize the circadian rhythm. This method was compared to another program called Breathe which was created in order to reduce symptoms of depression and to cope with high stress. Participants in the Breathe program showed improvements in sleep quality and a decrease in depressive symptoms.While this study only had a small number of participants, the final results showed hope as an effective method that can help with sleep deprivation for college students.\n"}
{"id": "15865418", "url": "https://en.wikipedia.org/wiki?curid=15865418", "title": "Exposure action value", "text": "Exposure action value\n\nAn Exposure Action Value (EAV) or Action Value (AV) is a limit set on occupational exposure to noise where, when those values are exceeded, employers must take steps to monitor the exposure levels. These levels are measured in decibels. The American Occupational Safety and Health Administration (OSHA) set the EAV to 85 dB(A). When the eight-hour time-weighted average (TWA) reaches 85 dB(A), employers are required to administer a continuing, effective hearing conservation program. The program consists of monitoring, employee notification, observation, an audiometric testing program, hearing protectors, training programs, and record keeping requirements.\n\nThe purpose of the EAV is to ensure that employees are not suffering from high levels of noise exposure. OSHA requires employers to take steps to reduce exposure levels when the TWA reaches 90 dB(A). The EAV is to ensure that the exposure levels do not reach 90 dB(A) or more. It is also to ensure that employees are not experiencing noise-induced hearing loss.\n\nA noise dosimeter is used to measure noise exposures to employees. Dosimeters can be used to determine the TWA. If it is determined that levels of noise exposure have reached the EAV, employers are required to implement a hearing conservation program. The hearing conservation program consists of many different aspects.\n\nThe first aspect is monitoring. The employer is required to monitor noise exposure for all of its employees who may be exposed at a TWA at or above 85 dB(A). This is to identify employees for inclusion in the hearing conservation program and to enable the proper selection of hearing protectors.\n\nThe second aspect is the audiometric testing program. Employees exposed to levels at or above the EAV will undergo audiometric testing. The first test is called a baseline. It provides a standard to compare future audiometric tests to. If a significant change in hearing capabilities occurs (called a standard threshold shift) greater steps must be taken to ensure the employee is protected from high levels of noise exposure.\n\nThe third aspect is the implementation of hearing protection. Employers must make hearing protection available to all employees who are exposed to noise levels of 85 dB(A) or greater. This is to be at no cost to employees. Employees can pick whichever type of hearing protection they prefer. This also requires an ongoing evaluation of the hearing protection.\n\nThe fourth aspect is a training program. The training program must cover the effects of noise on hearing, the purpose of hearing protection, and the purpose of audiometric testing.\n\nThe last aspect is record keeping. Records of employee audiometric tests must be retained for two years. This information must also be available to the employees.\n\nJust like the Exposure Action Value or Action Value for noise, safety administrations across the world are publishing values for vibrations. As of right now, The American Occupational Safety and Health Administration has not officially published a list of appropriate, time-weighted EAV guidelines for employers to follow. However, companies in the United States are encouraged by the National Institute for Occupational Safety and Health to follow the vibration limits set up by the ACGIH, which publishes a \"Threshold Limit Value\" or \"TLV\" in their annual book of TLVs and BEIs. The goal is for employers to have these numbers in order to make a conscious effort to lower the amount of harmful exposure absorbed by their workers. Since the link from physical vibration to damage caused by Raynaud's phenomenon is less clearly defined between the EAV for noise and noise-induced hearing loss, an upper extreme limit \"Exposure Limit Value\" or \"ELV\" is provided as well to give a margin.\n\n\n"}
{"id": "2348482", "url": "https://en.wikipedia.org/wiki?curid=2348482", "title": "Gin Craze", "text": "Gin Craze\n\nThe Gin Craze was a period in the first half of the 18th century when the consumption of gin increased rapidly in Great Britain, especially in London. Daniel Defoe commented: \"the Distillers have found out a way to hit the palate of the Poor, by their new fashion'd compound Waters called Geneva, so that the common People seem not to value the French-brandy as usual, and even not to desire it\". Many people overconsumed and the city had an epidemic of extreme drunkenness; this provoked moral outrage and a legislative backlash that some compare to the modern drug wars.\n\nParliament passed five major Acts, in 1729, 1736, 1743, 1747 and 1751, designed to control the consumption of gin. Though many similar drinks were available and alcohol consumption was considerable at all levels of society, gin caused the greatest public concern. Although it is commonly thought gin or Jenever was the singular drink, \"gin\" was a blanket statement for all grain-based alcohols at the time.\n\nGin was popularised in England following the accession of William of Orange in 1688. Gin provided an alternative to French brandy at a time of both political and religious conflict between Britain and France. Between 1689 and 1697, the Government passed a range of legislation aimed at restricting brandy imports and encouraging gin production. Most importantly, the monopoly of the London Guild of Distillers was broken in 1690, thereby opening up the market in gin distillation. The production and consumption of English gin, which was then popular amongst politicians and even Queen Anne, was encouraged by the government. This encouragement was shown in the reduced taxes on the distillation of spirits. Additionally, no licenses were needed to make spirits, so distillers of spirits could have smaller, more simple workshops than brewers, who needed to serve food and provide shelter for patrons. \n\nEconomic protectionism was a major factor in beginning the Gin Craze; as the price of food dropped and income grew, consumers suddenly had the opportunity to spend excess funds on spirits. By 1721, however, Middlesex magistrates were already decrying gin as \"the principal cause of all the vice & debauchery committed among the inferior sort of people\". In 1736, the Middlesex Magistrates complained: It is with the deepest concern your committee observe the strong Inclination of the inferior Sort of People to these destructive Liquors, and how surprisingly this Infection has spread within these few Years … it is scarce possible for Persons in low Life to go anywhere or to be anywhere, without being drawn in to taste, and, by Degrees, to like and approve of this pernicious Liquor.\n\nThe British government tried a number of times to stop the flow of gin. The Gin Act 1736 taxed retail sales at a rate of 20 shillings a gallon on spirits and required licensees to take out a £50 annual licence to sell gin, a fee equivalent to about £ today. The aim was to effectively prohibit the trade by making it economically unfeasible. Only two licences were ever taken out. The trade became illegal, consumption dipped but then continued to rise and the law was effectively repealed in 1743 following mass law-breaking and violence (particularly towards informers who were paid £5 to reveal the whereabouts of illegal gin shops). The illegally distilled gin which was produced following the 1736 Act was less reliable and more likely to result in poisoning.\n\nBy 1743, England was drinking 2.2 gallons (10 litres) of gin per person per year. As consumption levels increased, an organised campaign for more effective legislation began to emerge, led by the Bishop of Sodor and Man, Thomas Wilson, who, in 1736, had complained that gin produced a \"drunken ungovernable set of people\". Prominent anti-gin campaigners included Henry Fielding (whose 1751 \"Enquiry into the Late Increase in Robbers\" blamed gin consumption for both increased crime and increased ill health among children), Josiah Tucker, Daniel Defoe (who had originally campaigned for the liberalisation of distilling, but later complained that drunken mothers were threatening to produce a \"fine spindle-shanked generation\" of children), and – briefly – William Hogarth. Hogarth's engraving \"Gin Lane\" is a well known image of the gin craze, and is often paired with \"Beer Street\", creating a contrast between the miserable lives of gin drinkers and the healthy and enjoyable lives of beer drinkers.\n\nThe Gin Craze began to diminish after the Gin Act 1751. This Act lowered the annual licence fees, but encouraged \"respectable\" gin selling by requiring licensees to trade from premises rented for at least £10 a year. Historians suggest that gin consumption was reduced not as a result of legislation but because of the rising cost of grain. Landowners could afford to abandon the production of gin, and this fact, coupled with population growth and a series of poor harvests, resulted in lower wages and increased food prices. The Gin Craze had mostly ended by 1757. The government tried to ensure this by temporarily banning the manufacture of spirits from domestic grain. There was a resurgence of gin consumption during the Victorian era, with numerous \"Gin Palaces\" appearing. In 1840, the amount of gin consumed in London (but by that time with a population in excess of one million) finally matched that from when prohibition ended in 1743.\n\n\n\n"}
{"id": "12950", "url": "https://en.wikipedia.org/wiki?curid=12950", "title": "Glucose", "text": "Glucose\n\nGlucose (also called dextrose) is a simple sugar with the molecular formula . Glucose is the most abundant monosaccharide, a subcategory of carbohydrates. Glucose is mainly made by plants and most algae during photosynthesis from water and carbon dioxide, using energy from sunlight. There it is used to make cellulose in cell walls, which is the most abundant carbohydrate. In energy metabolism, glucose is the most important source of energy in all organisms. Glucose for metabolism is partially stored as a polymer, in plants mainly as starch and amylopectin and in animals as glycogen. Glucose circulates in the blood of animals as blood sugar. The naturally occurring form of glucose is D-glucose, while L-glucose is produced synthetically in comparably small amounts and is of lesser importance.\n\nGlucose is on the World Health Organization's List of Essential Medicines, the most important medications needed in a basic health system. The name glucose derives through the French from the Greek γλυκός, which means \"sweet,\" in reference to must, the sweet, first press of grapes in the making of wine. The suffix \"-ose\" is a chemical classifier, denoting a sugar.\n\nGlucose was first isolated from raisins in 1747 by the German chemist Andreas Marggraf. Glucose was discovered in grapes by Johann Tobias Lowitz in 1792 and recognized as different from cane sugar (sucrose). Glucose is the term coined by Jean Baptiste Dumas in 1838, which has prevailed in the chemical literature. Friedrich August Kekulé proposed the term dextrose (from Latin dexter = right), because in aqueous solution of glucose, the plane of linearly polarized light is turned to the right. In contrast, D-fructose (a ketohexose) and L-glucose turn linearly polarized light to the left. The earlier notation according to the rotation of the plane of linearly polarized light (\"d\" and \"l\"-nomenclature) was later abandoned in favor of the D- and L-notation, which refers to the absolute configuration of the asymmetric center farthest from the carbonyl group, and in concordance with the configuration of D- or L-glyceraldehyde.\n\nSince glucose is a basic necessity of many organisms, a correct understanding of its chemical makeup and structure contributed greatly to a general advancement in organic chemistry. This understanding occurred largely as a result of the investigations of Emil Fischer, a German chemist who received the 1902 Nobel Prize in Chemistry for his findings. The synthesis of glucose established the structure of organic material and consequently formed the first definitive validation of Jacobus Henricus van 't Hoff's theories of chemical kinetics and the arrangements of chemical bonds in carbon-bearing molecules. Between 1891 and 1894, Fischer established the stereochemical configuration of all the known sugars and correctly predicted the possible isomers, applying van 't Hoff's theory of asymmetrical carbon atoms. The names initially referred to the natural substances. Their enantiomers were given the same name with the introduction of systematic nomenclatures, taking into account absolute stereochemistry (e.g. Fischer nomenclature, D/L nomenclature).\n\nFor the discovery of the metabolism of glucose Otto Meyerhof received the Nobel Prize in Physiology or Medicine in 1922. Hans von Euler-Chelpin was awarded the Nobel Prize in Chemistry along with Arthur Harden in 1929 for their \"research on the fermentation of sugar and their share of enzymes in this process\". In 1947, Bernardo Houssay (for his discovery of the role of the pituitary gland in the metabolism of glucose and the derived carbohydrates) as well as Carl and Gerty Cori (for their discovery of the conversion of glycogen from glucose) received the Nobel Prize in Physiology or Medicine. In 1970, Luis Leloir was awarded the Nobel Prize in Chemistry for the discovery of glucose-derived sugar nucleotides in the biosynthesis of carbohydrates.\n\nWith six carbon atoms, it is classed as a hexose, a subcategory of the monosaccharides. -Glucose is one of the sixteen aldohexose stereoisomers. The -isomer, -glucose, also known as dextrose, occurs widely in nature, but the -isomer, -glucose, does not. Glucose can be obtained by hydrolysis of carbohydrates such as milk sugar (lactose), cane sugar (sucrose), maltose, cellulose, glycogen, etc. It is commonly commercially manufactured from cornstarch by hydrolysis via pressurized steaming at controlled pH in a jet followed by further enzymatic depolymerization. Unbonded glucose is one of the main ingredients of honey. All forms of glucose are colorless and easily soluble in water, acetic acid, and several other solvents. They are only sparingly soluble in methanol and ethanol.\n\nGlucose is a monosaccharide with formula CHO or H-(C=O)-(CHOH)-H, whose five hydroxyl (OH) groups are arranged in a specific way along its six-carbon back. Glucose is usually present in solid form as a monohydrate with a closed pyran ring (dextrose hydrate). In aqueous solution, on the other hand, it is has an open-chain to a small extent and is present predominantly as α- or β-pyranose, which partially mutually merge by mutarotation. From aqueous solutions, the three known forms can be crystallized: α-glucopyranose, β-glucopyranose and β-glucopyranose hydrate. Glucose is a building block of the disaccharides lactose and sucrose (cane or beet sugar), of oligosaccharides such as raffinose and of polysaccharides such as starch and amylopectin, glycogen or cellulose. The glass transition temperature of glucose is 31 °C and the Gordon-Taylor constant (an experimentally determined constant for the prediction of the glass transition temperature for different mass fractions of a mixture of two substances) is 4.5.\n\nIn its fleeting open-chain form, the glucose molecule has an open (as opposed to cyclic) and unbranched backbone of six carbon atoms, C-1 through C-6; where C-1 is part of an aldehyde group H(C=O)-, and each of the other five carbons bears one hydroxyl group -OH. The remaining bonds of the backbone carbons are satisfied by hydrogen atoms -H. Therefore, glucose is both a hexose and an aldose, or an aldohexose. The aldehyde group makes glucose a reducing sugar giving a positive reaction with the Fehling test.\n\nEach of the four carbons C-2 through C-5 is a stereocenter, meaning that its four bonds connect to four different substituents. (Carbon C-2, for example, connects to -(C=O)H, -OH, -H, and -(CHOH)H.) In -glucose, these four parts must be in a specific three-dimensional arrangement. Namely, when the molecule is drawn in the Fischer projection, the hydroxyls on C-2, C-4, and C-5 must be on the right side, while that on C-3 must be on the left side.\n\nThe positions of those four hydroxyls are exactly reversed in the Fischer diagram of -glucose. - and -glucose are two of the 16 possible aldohexoses; the other 14 are allose, altrose, galactose, gulose, idose, mannose, and talose, each with two enantiomers, “-” and “-”.\n\nIt is important to note that the linear form of glucose makes up less than 0.02% of the glucose molecules in a water solution. The rest is one of two cyclic forms of glucose that are formed when the hydroxyl group on carbon 5 (C5) bonds to the aldehyde carbon 1 (C1).\n\nIn solutions, the open-chain form of glucose (either \"-\" or \"-\") exists in equilibrium with several cyclic isomers, each containing a ring of carbons closed by one oxygen atom. In aqueous solution however, more than 99% of glucose molecules, at any given time, exist as pyranose forms. The open-chain form is limited to about 0.25% and furanose forms exists in negligible amounts. The terms \"glucose\" and \"-glucose\" are generally used for these cyclic forms as well. The ring arises from the open-chain form by an intramolecular nucleophilic addition reaction between the aldehyde group (at C-1) and either the C-4 or C-5 hydroxyl group, forming a hemiacetal linkage, -C(OH)H-O-.\n\nThe reaction between C-1 and C-5 yields a six-membered heterocyclic system called a pyranose, which is a monosaccharide sugar (hence \"–ose\") containing a derivatised pyran skeleton. The (much rarer) reaction between C-1 and C-4 yields a five-membered furanose ring, named after the cyclic ether furan. In either case, each carbon in the ring has one hydrogen and one hydroxyl attached, except for the last carbon (C-4 or C-5) where the hydroxyl is replaced by the remainder of the open molecule (which is -(C(CHOH)HOH)-H or -(CHOH)-H, respectively).\n\nThe ring-closing reaction makes carbon C-1 chiral, too, since its four bonds lead to -H, to -OH, to carbon C-2, and to the ring oxygen. These four parts of the molecule may be arranged around C-1 (the anomeric carbon) in two distinct ways, designated by the prefixes \"α-\" and \"β-\". When a glucopyranose molecule is drawn in the Haworth projection, the designation \"α-\" means that the hydroxyl group attached to C-1 and the -CHOH group at C-5 lies on opposite sides of the ring's plane (a \"trans\" arrangement), while \"β-\" means that they are on the same side of the plane (a \"cis\" arrangement). Therefore, the open-chain isomer -glucose gives rise to four distinct cyclic isomers: α--glucopyranose, β--glucopyranose, α--glucofuranose, and β--glucofuranose. These five structures exist in equilibrium and interconvert, and the interconversion is much more rapid with acid catalysis.\n\nThe other open-chain isomer -glucose similarly gives rise to four distinct cyclic forms of -glucose, each the mirror image of the corresponding -glucose.\n\nThe rings are not planar, but are twisted in three dimensions. The glucopyranose ring (α or β) can assume several non-planar shapes, analogous to the \"chair\" and \"boat\" conformations of cyclohexane. Similarly, the glucofuranose ring may assume several shapes, analogous to the \"envelope\" conformations of cyclopentane.\n\nIn the solid state, only the glucopyranose forms are observed, forming colorless crystalline solids that are highly soluble in water and acetic acid but poorly soluble in methanol and ethanol. They melt at (\"α\") and (\"β\"), and decompose at higher temperatures into carbon and water.\n\nEach glucose isomer is subject to rotational isomerism. Within the cyclic form of glucose, rotation may occur around the O6-C6-C5-O5 torsion angle, termed the \"ω\"-angle, to form three staggered rotamer conformations called \"gauche\"-\"gauche\" (gg), \"gauche\"-\"trans\" (gt) and \"trans\"-\"gauche\" (tg). There is a tendency for the \"ω\"-angle to adopt a \"gauche\" conformation, a tendency that is attributed to the gauche effect.\n\nMutarotation consists of a temporary reversal of the ring-forming reaction, resulting in the open-chain form, followed by a reforming of the ring. The ring closure step may use a different -OH group than the one recreated by the opening step (thus switching between pyranose and furanose forms), or the new hemiacetal group created on C-1 may have the same or opposite handedness as the original one (thus switching between the α and β forms). Thus, though the open-chain form is barely detectable in solution, it is an essential component of the equilibrium.\n\nThe open-chain form is thermodynamically unstable, and it spontaneously isomerizes to the cyclic forms. (Although the ring closure reaction could in theory create four- or three-atom rings, these would be highly strained, and are not observed in practice.) In solutions at room temperature, the four cyclic isomers interconvert over a time scale of hours, in a process called mutarotation. Starting from any proportions, the mixture converges to a stable ratio of α:β 36:64. The ratio would be α:β 11:89 if it were not for the influence of the anomeric effect. Mutarotation is considerably slower at temperatures close to .\n\nWhether in water or in the solid form, -(+)-glucose is dextrorotatory, meaning it will rotate the direction of polarized light clockwise as seen looking toward the light source. The effect is due to the chirality of the molecules, and indeed the mirror-image isomer, -(−)-glucose, is levorotatory (rotates polarized light counterclockwise) by the same amount. The strength of the effect is different for each of the five tautomers.\n\nNote that the - prefix does not refer directly to the optical properties of the compound. It indicates that the C-5 chiral center has the same handedness as that of -glyceraldehyde (which was so labeled because it is dextrorotatory). The fact that -glucose is dextrorotatory is a combined effect of its four chiral centers, not just of C-5; and indeed some of the other -aldohexoses are levorotatory.\n\nThe conversion between the two anomers can be observed in a polarimeter, since pure α- D- glucose has a specific rotation angle of +112.2 ° · ml · dm · g, pure β- D- glucose of +17.5 ° · ml · dm · g. When equilibrium has been reached after a certain time due to mutarotation, the angle of rotation is +52.7 ° · ml · dm · g. By adding acid or base, this transformation is much accelerated. The equilibration takes place via the open-chain aldehyde form.\n\nIn dilute sodium hydroxide or other dilute bases, the monosaccharides mannose, glucose and fructose interconvert (via a Lobry de Bruyn–Alberda–van Ekenstein transformation), so that a balance between these isomers is formed. This reaction proceeds via an enediol:\n\nGlucose is the most abundant monosaccharide. Glucose is also the most widely used aldohexose in most living organisms. One possible explanation for this is that glucose has a lower tendency than other aldohexoses to react nonspecifically with the amine groups of proteins. This reaction—glycation—impairs or destroys the function of many proteins, e.g. in glycated hemoglobin. Glucose's low rate of glycation can be attributed to its having a more stable cyclic form compared to other aldohexoses, which means it spends less time than they do in its reactive open-chain form. The reason for glucose having the most stable cyclic form of all the aldohexoses is that its hydroxy groups (with the exception of the hydroxy group on the anomeric carbon of -glucose) are in the equatorial position. Presumably, glucose is the most abundant natural monosaccharide because it is less glycated with proteins than other monosaccharides. Another hypothesis is that glucose, being the only D-aldohexose that has all five hydroxy substituents in the equatorial position in the form of β-D-glucose, is more readily accessible to chemical reactions, for example, for esterification or acetal formation. For this reason, D-glucose is also a highly preferred building block in natural polysaccharides (glycans). Polysaccharides that are composed solely of Glucose are termed glucans.\n\nGlucose is produced by plants through the photosynthesis using sunlight, water and carbon dioxide and can be used by all living organisms as an energy and carbon source. However, most glucose does not occur in its free form, but in the form of its polymers, i.e. lactose, sucrose, starch and others which are energy reserve substances, and cellulose and chitin, which are components of the cell wall in plants or fungi and arthropods, respectively. These polymers are degraded to glucose during food intake by animals, fungi and bacteria using enzymes. All animals are also able to produce glucose themselves from certain precursors as the need arises. Nerve cells, cells of the renal medulla and erythrocytes depend on glucose for their energy production.<ref name=\"Löffler/Petrides 195\">Peter C. Heinrich: \"Löffler/Petrides Biochemie und Pathobiochemie.\" Springer-Verlag, 2014, , p. 195. (german)</ref> In adult humans, there are about 18 g of glucose, of which about 4 g are present in the blood. Approximately 180 to 220 g of glucose are produced in the liver of an adult in 24 hours.\n\nMany of the long-term complications of diabetes (e.g., blindness, kidney failure, and peripheral neuropathy) are probably due to the glycation of proteins or lipids. In contrast, enzyme-regulated addition of sugars to protein is called glycosylation and is essential for the function of many proteins.\n\nIngested glucose initially binds to the receptor for sweet taste on the tongue in humans. This complex of the proteins T1R2 and T1R3 makes it possible to identify glucose-containing food sources. Glucose mainly comes from food - about 300 g per day are produced by conversion of food,<ref name=\"Löffler/Petrides 404\">Peter C. Heinrich: \"Löffler/Petrides Biochemie und Pathobiochemie.\" Springer-Verlag, 2014, , p. 404.</ref> but it is also synthesized from other metabolites in the body's cells. In humans, the breakdown of glucose-containing polysaccharides happens in part already during chewing by means of amylase, which is contained in saliva, as well as by maltase, lactase and sucrase on the brush border of the small intestine. Glucose is a building block of many carbohydrates and can be split off from them using certain enzymes. Glucosidases, a subgroup of the glycosidases, first catalyze the hydrolysis of long-chain glucose-containing polysaccharides, removing terminal glucose. In turn, disaccharides are mostly degraded by specific glycosidases to glucose. The names of the degrading enzymes are often derived from the particular poly- and disaccharide; inter alia, for the degradation of polysaccharide chains there are amylases (named after amylose, a component of starch), cellulases (named after cellulose), chitinases (named after chitin) and more. Furthermore, for the cleavage of disaccharides, there are maltase, lactase, sucrase, trehalase and others. In humans, about 70 genes are known that code for glycosidases. They have functions in the digestion and degradation of glycogen, sphingolipids, mucopolysaccharides and poly(ADP-ribose). Humans do not produce cellulases, chitinases and trehalases, but the bacteria in the gut flora do.\n\nIn order to get into or out of cell membranes of cells and membranes of cell compartments, glucose requires special transport proteins from the major facilitator superfamily. In the small intestine (more precisely, in the jejunum), glucose is taken up into the intestinal epithelial cells with the help of glucose transporters via a secondary active transport mechanism called sodium ion-glucose symport via the sodium/glucose cotransporter 1. The further transfer occurs on the basolateral side of the intestinal epithelial cells via the glucose transporter GLUT2, as well as their uptake into liver cells, kidney cells, cells of the islets of Langerhans, nerve cells, astrocytes and tanyocytes. Glucose enters the liver via the vena portae and is stored there as a cellular glycogen. In the liver cell, it is phosphorylated by glucokinase at position 6 to glucose-6-phosphate, which can not leave the cell. With the help of glucose-6-phosphatase, glucose-6-phosphate is converted back into glucose exclusively in the liver, if necessary, so that it is available for maintaining a sufficient blood glucose concentration. In other cells, uptake happens by passive transport through one of the 14 GLUT proteins. In the other cell types, phosphorylation occurs through a hexokinase, whereupon glucose can no longer diffuse out of the cell.\n\nThe glucose transporter GLUT1 is produced by most cell types and is of particular importance for nerve cells and pancreatic β-cells. GLUT3 is highly expressed in nerve cells. Glucose from the bloodstream is taken up by GLUT4 from muscle cells (of the skeletal muscle and heart muscle) and fat cells. GLUT14 is formed exclusively in testes. Excess glucose is broken down and converted into fatty acids, which are stored as triacylglycerides. In the kidneys, glucose in the urine is absorbed via SGLT1 and SGLT2 in the apical cell membranes and transmitted via GLUT2 in the basolateral cell membranes. About 90% of kidney glucose reabsorption is via SGLT2 and about 3% via SGLT1.\n\nIn plants and some prokaryotes, glucose is a product of photosynthesis. Glucose is also formed by the breakdown of polymeric forms of glucose like glycogen (in animals and mushrooms) or starch (in plants). The cleavage of glycogen is termed glycogenolysis, the cleavage of starch is called starch degradation.\n\nThe metabolic pathway that begins with molecules containing two to four carbon atoms (C) and ends in the glucose molecule containing six carbon atoms is called gluconeogenesis and occurs in all living organisms. The smaller starting materials are the result of other metabolic pathways. Ultimately almost all biomolecules come from the assimilation of carbon dioxide in plants during photosynthesis. The free energy of formation of α-D-glucose is 917.2 kilojoules per mole. In humans, gluconeogenesis occurs in the liver and kidney, but also in other cell types. In the liver about 150 g of glycogen are stored, in skeletal muscle about 250 g.<ref name=\"Löffler/Petrides 389\">Peter C. Heinrich: \"Löffler/Petrides Biochemie und Pathobiochemie.\" Springer-Verlag, 2014, , p. 389. (german)</ref> However, the glucose released in muscle cells upon cleavage of the glycogen can not be delivered to the circulation because glucose is phosphorylated by the hexokinae, and a glucose-6-phosphatase is not expressed to remove the phosphate group. Unlike for glucose, there is no transport protein for glucose-6-phosphate. Gluconeogenesis allows the organism to build up glucose from other metabolites, including lactate or certain amino acids, while consuming energy. The renal tubular cells can also produce glucose.\n\nIn humans, glucose is metabolised by glycolysis and the pentose phosphate pathway. Glycolysis is used by all living organisms, with small variations, and all organisms generate energy from the breakdown of monosaccharides. In the further course of the metabolism, it can be completely degraded via oxidative decarboxylation, the Krebs cycle (synonym \"citric acid cycle\") and the respiratory chain to water and carbon dioxide. If there is not enough oxygen available for this, the glucose degradation in animals occurs anaerobic to lactate via lactic acid fermentation and releases less energy. Muscular lactate enters the liver through the bloodstream in mammals, where gluconeogenesis occurs (Cori cycle). With a high supply of glucose, the metabolite acetyl-CoA from the Krebs cycle can also be used for fatty acid synthesis. Glucose is also used to replenish the body's glycogen stores, which are mainly found in liver and skeletal muscle. These processes are hormonally regulated.\n\nIn other living organisms, other forms of fermentation can occur. The bacterium \"Escherichia coli\" can grow on nutrient media containing glucose as the sole carbon source. In some bacteria and, in modified form, also in archaea, glucose is degraded via the Entner-Doudoroff pathway.\n\nUse of glucose as an energy source in cells is by either aerobic respiration, anaerobic respiration, or fermentation. The first step of glycolysis is the phosphorylation of glucose by a hexokinase to form glucose 6-phosphate. The main reason for the immediate phosphorylation of glucose is to prevent its diffusion out of the cell as the charged phosphate group prevents glucose 6-phosphate from easily crossing the cell membrane. Furthermore, addition of the high-energy phosphate group activates glucose for subsequent breakdown in later steps of glycolysis. At physiological conditions, this initial reaction is irreversible.\n\nIn anaerobic respiration, one glucose molecule produces a net gain of two ATP molecules (four ATP molecules are produced during glycolysis through substrate-level phosphorylation, but two are required by enzymes used during the process). In aerobic respiration, a molecule of glucose is much more profitable in that a maximum net production of 30 or 32 ATP molecules (depending on the organism) through oxidative phosphorylation is generated.\n\nTumor cells often grow comparatively quickly and consume an above-average amount of glucose by glycolysis, which leads to the formation of lactate, the end product of fermentation in mammals, even in the presence of oxygen. This effect is called the Warburg effect. For the increased uptake of glucose in tumors various SGLT and GLUT are overly produced.\n\nIn yeast, ethanol is fermented at high glucose concentrations, even in the presence of oxygen (which normally leads to respiration but not to fermentation). This effect is called the Crabtree effect.\n\nGlucose is a ubiquitous fuel in biology. It is used as an energy source in organisms, from bacteria to humans, through either aerobic respiration, anaerobic respiration (in bacteria), or fermentation. Glucose is the human body's key source of energy, through aerobic respiration, providing about 3.75 kilocalories (16 kilojoules) of food energy per gram. Breakdown of carbohydrates (e.g., starch) yields mono- and disaccharides, most of which is glucose. Through glycolysis and later in the reactions of the citric acid cycle and oxidative phosphorylation, glucose is oxidized to eventually form carbon dioxide and water, yielding energy mostly in the form of ATP. The insulin reaction, and other mechanisms, regulate the concentration of glucose in the blood. The physiological caloric value of glucose, depending on the source, is 16.2 kilojoules per gram and 15.7 kJ/g (3.74 kcal/g), respectively. The high availability of carbohydrates from plant biomass has led to a variety of methods during evolution, especially in microorganisms, to utilize the energy and carbon storage glucose. Differences exist in which end product can no longer be used for energy production. The presence of individual genes, and their gene products, the enzymes, determine which reactions are possible. The metabolic pathway of glycolysis is used by almost all living beings. An essential difference in the use of glycolysis is the recovery of NADPH as a reductant for anabolism that would otherwise have to be generated indirectly.\n\nGlucose supplies almost all the energy for the brain, so its availability influences psychological processes. When glucose is low, psychological processes requiring mental effort (e.g., self-control, effortful decision-making) are impaired. In the brain, which is dependent on glucose as the major source of energy, the glucose concentration is usually 4 to 6 mM (5 mM equals 90 mg / dL), but decreases to 2 to 3 mM when fasting. Confusion occurs below 1 mM and coma at lower levels.\n\nThe glucose in the blood is called blood sugar. Blood sugar levels are regulated by glucose-binding nerve cells in the hypothalamus. In addition, glucose in the brain binds to glucose receptors of the reward system in the nucleus accumbens. The binding of glucose to the sweet receptor on the tongue induces a release of various hormones of energy metabolism, either through glucose or through other sugars, leading to an increased cellular uptake and lower blood sugar levels. Artificial sweeteners do not lower blood sugar levels.\n\nThe blood sugar content of a healthy person in the short-time fasting state, e.g. after overnight fasting, is about 70 to 100 mg/dl of blood (4 to 5.5 mM). In blood plasma, the measured values are about 10-15 % higher. In addition, the values in the arterial blood are higher than the concentrations in the venous blood since glucose is absorbed into the tissue during the passage of the capillary bed. Also in the capillary blood, which is often used for blood sugar determination, the values are sometimes higher than in the venous blood. The glucose content of the blood is regulated by the hormones insulin, incretin and glucagon. Insulin lowers the glucose level, glucagon increases it. Furthermore, the hormones adrenaline, thyroxine, glucocorticoids, somatotropin and adrenocorticotropin lead to an increase in the glucose level. In addition, there is also a hormone-independent regulation, which is referred to as glucose autoregulation. After food intake the blood sugar concentration increases. Values over 180 mg/dl in venous whole blood are pathological and are termed hyperglycemia, values below 40 mg/dl are termed hypoglycaemia. When needed, glucose is released into the bloodstream by glucose-6-phosphatase from glucose-6-phosphate originating from liver and kidney glycogen, thereby regulating the homeostasis of blood glucose concentration. In ruminants, the blood glucose concentration is lower (60 mg/dL in cattle and 40 mg/dL in sheep), because the carbohydrates are converted more by their gut flora into short-chain fatty acids.\nSome glucose is converted to lactic acid by astrocytes, which is then utilized as an energy source by brain cells; some glucose is used by intestinal cells and red blood cells, while the rest reaches the liver, adipose tissue and muscle cells, where it is absorbed and stored as glycogen (under the influence of insulin). Liver cell glycogen can be converted to glucose and returned to the blood when insulin is low or absent; muscle cell glycogen is not returned to the blood because of a lack of enzymes. In fat cells, glucose is used to power reactions that synthesize some fat types and have other purposes. Glycogen is the body's \"glucose energy storage\" mechanism, because it is much more \"space efficient\" and less reactive than glucose itself.\n\nAs a result of its importance in human health, glucose is an analyte in glucose tests that are common medical blood tests. Eating or fasting prior to taking a blood sample has an effect on analyses for glucose in the blood; a high fasting glucose blood sugar level may be a sign of prediabetes or diabetes mellitus.\n\nThe glycemic index is an indicator of the speed of resorption and conversion to blood glucose levels from ingested carbohydrates, measured as the area under the curve of blood glucose levels after consumption in comparison to glucose (glucose is defined as 100). The clinical importance of the glycemic index is controversial, as foods with high fat contents slow the resorption of carbohydrates and lower the glycemic index, e.g. ice cream. An alternative indicator is the insulin index, measured as the impact of carbohydrate consumption on the blood insulin levels. The glycemic load is an indicator for the amount of glucose added to blood glucose levels after consumption, based on the glycemic index and the amount of consumed food.\n\nOrganisms use glucose as a precursor for the synthesis of several important substances. Starch, cellulose, and glycogen (\"animal starch\") are common glucose polymers (polysaccharides). Some of these polymers (starch or glycogen) serve as energy stores, while others (cellulose and chitin, which is made from a derivative of glucose) have structural roles. Oligosaccharides of glucose combined with other sugars serve as important energy stores. These include lactose, the predominant sugar in milk, which is a glucose-galactose disaccharide, and sucrose, another disaccharide which is composed of glucose and fructose. Glucose is also added onto certain proteins and lipids in a process called glycosylation. This is often critical for their functioning. The enzymes that join glucose to other molecules usually use phosphorylated glucose to power the formation of the new bond by coupling it with the breaking of the glucose-phosphate bond.\n\nOther than its direct use as a monomer, glucose can be broken down to synthesize a wide variety of other biomolecules. This is important, as glucose serves both as a primary store of energy and as a source of organic carbon. Glucose can be broken down and converted into lipids. It is also a precursor for the synthesis of other important molecules such as vitamin C (ascorbic acid). In living organisms, glucose is converted to several other chemical compounds that are the starting material for various metabolic pathways. Among them, all other monosaccharides<ref name=\"Löffler/Petrides 27\">Peter C. Heinrich: \"Löffler/Petrides Biochemie und Pathobiochemie.\" Springer-Verlag, 2014, , p. 27. (german)</ref> such as fructose (via the polyol pathway),<ref name=\"Löffler/Petrides 199\">Peter C. Heinrich: \"Löffler/Petrides Biochemie und Pathobiochemie.\" Springer-Verlag, 2014, , p. 199, 200. (german)</ref> mannose (the epimer of glucose at position 2), galactose (the epimer at position 4), fucose, various uronic acids and the amino sugars are produced from glucose.<ref name=\"Löffler/Petrides 214\">Peter C. Heinrich: \"Löffler/Petrides Biochemie und Pathobiochemie.\" Springer-Verlag, 2014, , p. 214. (german)</ref> In addition to the phosphorylation to glucose-6-phosphate, which is part of the glycolysis, glucose can be oxidized during its degradation to glucono-1,5-lactone. Glucose is used in some bacteria as a building block in the trehalose or the dextran biosynthesis and in animals as a building block of glycogen. Glucose can also be converted from bacterial xylose isomerase to fructose. In addition, glucose metabolites produce all nonessential amino acids, sugar alcohols such as mannitol and sorbitol, fatty acids, cholesterol and nucleic acids. Finally, glucose is used as a building block in the glycosylation of proteins to glycoproteins, glycolipids, peptidoglycans, glycosides and other substances (catalyzed by glycosyltransferases) and can be cleaved from them by glycosidases.\n\nDiabetes is a metabolic disorder where the body is unable to regulate levels of glucose in the blood either because of a lack of insulin in the body or the failure, by cells in the body, to respond properly to insulin. Each of these situations can be caused by persistently high elevations of blood glucose levels, through pancreatic burnout and insulin resistance. The pancreas is the organ responsible for the secretion of the hormones insulin and glucagon. Insulin is a hormone that regulates glucose levels, allowing the body's cells to absorb and use glucose. Without it, glucose cannot enter the cell and therefore cannot be used as fuel for the body's functions. If the pancreas is exposed to persistently high elevations of blood glucose levels, the insulin-producing cells in the pancreas could be damaged, causing a lack of insulin in the body. Insulin resistance occurs when the pancreas tries to produce more and more insulin in response to persistently elevated blood glucose levels. Eventually, the rest of the body becomes resistant to the insulin that the pancreas is producing, thereby requiring more insulin to achieve the same blood glucose-lowering effect, and forcing the pancreas to produce even more insulin to compete with the resistance. This negative spiral contributes to pancreatic burnout, and the disease progression of diabetes.\n\nTo monitor the body's response to blood glucose-lowering therapy, glucose levels can be measured. Blood glucose monitoring can be performed by multiple methods, such as the fasting glucose test which measures the level of glucose in the blood after 8 hours of fasting. Another test is the 2-hour glucose tolerance test (GTT) – for this test, the person has a fasting glucose test done, then drinks a 75-gram glucose drink and is retested. This test measures the ability of the person's body to process glucose. Over time the blood glucose levels should decrease as insulin allows it to be taken up by cells and exit the blood stream.\n\nAn increased intake of glucose leads to obesity and, in consequence, partly to the metabolic syndrome with non-alcoholic fatty liver disease, but not the consumption of glucose as part of a normal calorie intake.\n\nIndividuals with diabetes or other conditions that result in low blood sugar often carry small amounts of sugar in various forms. One sugar commonly used is glucose, often in the form of glucose tablets (glucose pressed into a tablet shape sometimes with one or more other ingredients as a binder), hard candy, or sugar packet.\n\nMost dietary carbohydrates contain glucose, either as their only building block (as in the polysaccharides starch and glycogen), or together with another monosaccharide (as in the hetero-polysaccharides sucrose and lactose). Unbounded glucose is one of the main ingredients of honey.\n\nAll data with a unit of g (gram) are based on 100 g of a food item.\n\nGlucose is produced industrially from starch by enzymatic hydrolysis using glucose amylase or by the use of acids. The enzymatic hydrolysis has largely displaced the acid-catalyzed hydrolysis. The result is glucose syrup (enzymatically with more than 90% glucose in the dry matter) with an annual worldwide production volume of 20 million tonnes (as of 2011). This is the reason for the former common name \"starch sugar\". The amylases most often come from \"Bacillus licheniformis\" or \"Bacillus subtilis\" (strain MN-385), which are more thermostable than the originally used enzymes. Starting in 1982, pullulanases from \"Aspergillus niger\" were used in the production of glucose syrup to convert amylopectin to starch (amylose), thereby increasing the yield of glucose. The reaction is carried out at a pH of 4.6-5.2 and a temperature of 55-60 °C. Corn syrup has between 20% and 95% glucose in the dry matter. The Japanese form of the glucose syrup, Mizuame, is made from sweet potato or rice starch. Maltodextrin contains about 20% glucose.\n\nMany crops can be used as the source of starch. Maize, rice, wheat, cassava, potato, barley, sweet potato, corn husk and sago are all used in various parts of the world. In the United States, corn starch (from maize) is used almost exclusively. Some commercial glucose occurs as a component of invert sugar, a roughly 1:1 mixture of glucose and fructose that is produced from sucrose. In principle, cellulose could be hydrolysed to glucose, but this process is not yet commercially practical.\n\nIn the USA almost exclusively corn (more precisely: corn syrup) is used as glucose source for the production of isoglucose, which is a mixture of glucose and fructose, since fructose has a higher sweetening power — with same physiological calorific value of 374 kilocalories per 100 g. The annual world production of isoglucose is eight million tonnes (as of 2011). When made from corn syrup, the final product is high fructose corn syrup (HFCS).\n\nGlucose is mainly used for the production of fructose and in the production of glucose-containing foods. In foods, it is used as a sweetener, humectant, to increase the volume and to create a softer mouthfeel. Various sources of glucose, such as grape juice (for wine) or malt (for beer), are used for fermentation to ethanol during the production of alcoholic beverages. Most soft drinks in the US use HFCS-55 (with a fructose content of 55% in the dry mass), while most other HFCS-sweetened foods in the US use HFCS-42 (with a fructose content of 42% in the dry mass). In the neighboring country Mexico, on the other hand, cane sugar is used in the soft drink as a sweetener, which has a higher sweetening power. In addition, glucose syrup is used, inter alia, in the production of confectionery such as candies, toffee and fondant. Typical chemical reactions of glucose when heated under water-free conditions are the caramelization and, in presence of amino acids, the maillard reaction.\n\nIn addition, various organic acids can be biotechnologically produced from glucose, for example by fermentation with \"Clostridium thermoaceticum\" to produce acetic acid, with \"Penicilium notatum\" for the production of araboascorbic acid, with \"Rhizopus delemar\" for the production of fumaric acid, with \"Aspergillus niger\" for the production of gluconic acid, with \"Candida brumptii\" to produce isocitric acid, with \"Aspergillus terreus\" for the production of itaconic acid, with \"Pseudomonas fluorescens\" for the production of 2-ketogluconic acid, with \"Gluconobacter suboxydans\" for the production of 5-ketogluconic acid, with \"Aspergillus oryzae\" for the production of kojic acid, with \"Lactobacillus delbrueckii\" for the production of lactic acid, with \"Lactobacillus brevis\" for the production of malic acid, with \"Propionibacter shermanii\" for the production of propionic acid, with \"Pseudomonas aeruginosa\" for the production of pyruvic acid and with \"Gluconobacter suboxydans\" for the production of tartaric acid.\n\nSpecifically, when a glucose molecule is to be detected at a certain position in a larger molecule, nuclear magnetic resonance spectroscopy, X-ray crystallography analysis or lectin immunostaining is performed with concanavalin A reporter enzyme conjugate (that binds only glucose or mannose).\n\nThese reactions have only historical significance:\n\nThe Fehling test is a classic method for the detection of aldoses. Due to mutarotation, glucose is always present to a small extent as an open-chain aldehyde. By adding the Fehling reagents (Fehling (I) solution and Fehling (II) solution), the aldehyde group is oxidized to a carboxylic acid, while the Cu tartrate complex is reduced to Cu and forming a brick red precipitate (CuO).\n\nIn the Tollens test, after addition of ammoniacal AgNO to the sample solution, Ag is reduced by glucose to elemental silver.\n\nIn Barfoed's test, a solution of dissolved copper acetate, sodium acetate and acetic acid is added to the solution of the sugar to be tested and subsequently heated in a water bath for a few minutes. Glucose and other monosaccharides rapidly produce a reddish color and reddish brown copper(I) oxide (CuO).\n\nAs a reducing sugar, glucose reacts in the Nylander's test.\n\nUpon heating a dilute potassium hydroxide solution with glucose to 100 °C, a strong reddish browning and a caramel-like odor develops. Concentrated sulfuric acid dissolves dry glucose without blackening at room temperature forming sugar sulfuric acid. In a yeast solution, alcoholic fermentation produces carbon dioxide in the ratio of 2.0454 molecules of glucose to one molecule of CO. Glucose forms a black mass with stannous chloride. In an ammoniacal silver solution, glucose (as well as lactose and dextrin) leads to the deposition of silver. In an ammoniacal lead acetate solution, white lead glycoside is formed in the presence of glucose, which becomes less soluble on cooking and turns brown. In an ammoniacal copper solution, yellow copper oxide hydrate is formed with glucose at room temperature, while red copper oxide is formed during boiling (same with dextrin, except for with an ammoniacal copper acetate solution). With Hager's reagent, glucose forms mercury oxide during boiling. An alkaline bismuth solution is used to precipitate elemental, black-brown bismuth with glucose. Glucose boiled in an ammonium molybdate solution turns the solution blue. A solution with indigo carmine and sodium carbonate destains when boiled with glucose.\n\nIn concentrated solutions of glucose with a low proportion of other carbohydrates, its concentration can be determined with a polarimeter. For sugar mixtures, the concentration can be determined with a refractometer, for example in the Oechsle determination in the course of the production of wine.\n\nThe enzyme glucose oxidase (GOx) converts glucose into gluconic acid and hydrogen peroxide while consuming oxygen. Another enzyme, peroxidase, catalyzes a chromogenic reaction (Trinder reaction) of phenol with 4-aminoantipyrine to a purple dye.\n\nThe test strip method employs the above-mentioned enzymatic conversion of glucose to gluconic acid to form hydrogen peroxide. The reagents are immobilised on a polymer matrix, the so-called test strip, which assumes a more or less intense color. This can be measured reflectometrically at 510 nm with the aid of an LED-based handheld photometer. This allows for routine blood sugar determination by laymen. In addition to the reaction of phenol with 4-aminoantipyrine, new chromogenic reactions have been developed that allow photometry at higher wavelengths (550 nm, 750 nm).\n\nThe electroanalysis of glucose is also based on the enzymatic reaction mentioned above. The produced hydrogen peroxide can be amperometrically quantified by anodic oxidation at a potential of 600 mV. The GOx is immobilised on the electrode surface or in a membrane placed close to the electrode. Precious metals such as platinum or gold are used in electrodes, as well as carbon nanotube electrodes, which e.g. are doped with boron. Cu-CuO nanowires are also used as enzyme-free amperometric electrodes. This way a detection limit of 50 µmol/L has been achieved. A particularly promising method is the so-called \"enzyme wiring\". In this case, the electron flowing during the oxidation is transferred directly from the enzyme via a molecular wire to the electrode.\n\nThere are a variety of other chemical sensors for measuring glucose. Given the importance of glucose analysis in the life sciences, numerous optical probes have also been developed for saccharides based on the use of boronic acids, which are particularly useful for intracellular sensory applications where other (optical) methods are not or only conditionally usable. In addition to the organic boronic acid derivatives, which often bind highly specifically to the 1,2-diol groups of sugars, there are also other probe concepts classified by functional mechanisms which use selective glucose-binding proteins (e.g. concanavalin A) as a receptor. Furthermore, methods were developed which indirectly detect the glucose concentration via the concentration of metabolised products, e.g. by the consumption of oxygen using fluorescence-optical sensors. Finally, there are enzyme-based concepts that use the intrinsic absorbance or fluorescence of (fluorescence-labeled) enzymes as reporters.\n\nGlucose can be quantified by copper iodometry.\n\nIn particular, for the analysis of complex mixtures containing glucose, e.g. in honey, chromatographic methods such as high performance liquid chromatography and gas chromatography are often used in combination with mass spectrometry. Taking into account the isotope ratios, it is also possible to reliably detect honey adulteration by added sugars with these methods. Derivatisation using silylation reagents is commonly used. Also, the proportions of di- and trisaccharides can be quantified.\n\nGlucose uptake in cells of organisms is measured with 2-deoxy-D-glucose or fluorodeoxyglucose. (F)fluorodeoxyglucose is used as a tracer in positron emission tomography in oncology and neurology, where it is by far the most commonly used diagnostic agent.\n"}
{"id": "11038718", "url": "https://en.wikipedia.org/wiki?curid=11038718", "title": "Health administration", "text": "Health administration\n\nHealth administration or healthcare administration is the field relating to leadership, management, and administration of public health systems, health care systems, hospitals, and hospital networks.\n\nHealth systems management or health care systems management describes the leadership and general management of hospitals, hospital networks, and/or health care systems. In international use, the term refers to management at all levels. In the United States, management of a single institution (e.g. a hospital) is also referred to as \"medical and health services management\", \"healthcare management\", or \"health administration\".\n\nHealth systems management ensures that specific outcomes are attained, that departments within a health facility are running smoothly, that the right people are in the right jobs, that people know what is expected of them, that resources are used efficiently and that all departments are working towards a common goal.\n\nHospital administrators are individuals or groups of people who act as the central point of control within hospitals. These individuals may be previous or current clinicians, or individuals with other backgrounds. There are two types of administrators, generalists and specialists. Generalists are individuals who are responsible for managing or helping to manage an entire facility. Specialists are individuals who are responsible for the efficient operations of a specific department such as policy analysis, finance, accounting, budgeting, human resources, or marketing.\n\nIt was reported in September 2014, that the United States spends roughly $218 billion per year on hospital's administration costs, which is equivalent to 1.43 percent of the total U.S. economy. Hospital administration has grown as a percent of the U.S. economy from .9 percent in 2000 to 1.43 percent in 2012, according to \"Health Affairs\". In 11 different countries, hospitals allocate approximately 12 percent of their budget toward administrative costs. In the United States, hospitals spend 25 percent on administrative costs.\n\nNCHL competencies that require to engage with credibility, creativity, and motivation in complex and dynamic health care environments. \n\n\nHealth care management is usually studied through healthcare administration or healthcare management programs in a business school or, in some institutions, in a school of public health.\n\nAlthough many colleges and universities are offering a bachelor's degree in healthcare administration or human resources, a master's degree is considered the \"standard credential\" for most health administrators in the United States. Research and academic-based doctorate level degrees, such as the Doctor of Philosophy (PhD) in Health Administration and the Doctor of Health Administration (DHA) degree, prepare health care professionals to turn their clinical or administrative experiences into opportunities to develop new knowledge and practice, teach, shape public policy and/or lead complex organizations. There are multiple recognized degree types that are considered equivalent from the perspective of professional preparation.\n\nThe Commission on the Accreditation of Healthcare Management Education (CAHME) is the accrediting body overseeing master's-level programs in the United States and Canada on behalf of the United States Department of Education. It accredits several degree program types, including Master of Hospital Administration (MHA), Master of Health Services Administration (MHSA), Master of Business Administration in Hospital Management (MBA-HM), Master of Health Administration (MHA), Master of Public Health (MPH, MSPH, MSHPM), Master of Science (MS-HSM, MS-HA), and Master of Public Administration (MPA).\n\nHealth care management study is a new discipline in Nepal. Pokhara University offers a Hospital Management course. National Open College launched a four-year Bachelor's level (BHCM) course in September 2000 with an enrolment of 40 students, and the next year it also started a one-year postgraduate diploma (PGDHCM) and a two-year master's course (MHCM) in health care management. Nobel College at Sinamangal has also been offering a Bachelor’s level (BHCM) course since 2006. MD Hospital administration (MDHA) and Master in Hospital Management (MHM) are being started from 2013. It is uncertain how many citizens of Nepal are gaining healthcare management qualifications in other countries. There is an absence of professional organization and regulation in the health care management profession in Nepal.\n\nThere are a variety of different professional associations related to health systems management, which can be subcategorized as either personal or institutional membership groups. Personal membership groups are joined by individuals, and typically have individual skills and career development as their focus. Larger personal membership groups include the Healthcare Financial Management Association, and the Healthcare Information and Management Systems Society. Institutional membership groups are joined by organizations; whereas they typically focus on organizational effectiveness, and may also include data-sharing agreements and other medical related or administrative practice sharing vehicles for member organizations. Prominent examples include the American Hospital Association and the University Healthsystems Consortium.\n\nEarly hospital administrators were called patient directors or superintendents. At the time, many were nurses who had taken on administrative responsibilities. Over half of the members of the American Hospital Association were graduate nurses in 1916. Other superintendents were medical doctors, laymen and members of the clergy. \nIn the United States, the first degree granting program in the United States was established at Marquette University in Milwaukee, Wisconsin. By 1927, the first two students received their degrees. The original idea is credited to Father Moulinier, associated with the Catholic Hospital Association. The first modern health systems management program was established in 1934 at the University of Chicago. At the time, programs were completed in two years – one year of formal graduate study and one year of practicing internship. In 1958, the Sloan program at Cornell University began offering a special program requiring two years of formal study, which remains the dominant structure in the United States and Canada today (see also \"Academic Preparation\").\n\nHealth systems management has been described as a \"hidden\" health profession because of the relatively low-profile role managers take in health systems, in comparison to direct-care professions such as nursing and medicine. However the visibility of the management profession within healthcare has been rising in recent years, due largely to the widespread problems developed countries are having in balancing cost, access, and quality in their hospitals and health systems.\n\n\n\n"}
{"id": "25533720", "url": "https://en.wikipedia.org/wiki?curid=25533720", "title": "Health effects of radon", "text": "Health effects of radon\n\nRadon () is a radioactive, colorless, odorless, tasteless noble gas, occurring naturally as the decay product of radium. It is one of the densest substances that remains a gas under normal conditions, and is considered to be a health hazard due to its radioactivity. Its most stable isotope, Rn, has a half-life of 3.8 days. Due to its high radioactivity, it has been less well-studied by chemists, but a few compounds are known.\n\nRadon is formed as part of the normal radioactive decay chain of uranium into Pb. Uranium has been present since the earth was formed and its most common isotope has a very long half-life (4.5 billion years), which is the time required for one-half of uranium to break down. Thus, uranium and radon, will continue to occur for millions of years at about the same concentrations as they do now.\n\nRadon is responsible for the majority of the mean public exposure to ionizing radiation. It is often the single largest contributor to an individual's background radiation dose, and is the most variable from location to location. Radon gas from natural sources can accumulate in buildings, especially in confined areas such as attics, and basements. It can also be found in some spring waters and hot springs.\n\nAccording to a 2003 report \"EPA's Assessment of Risks from Radon in Homes\" from the United States Environmental Protection Agency, epidemiological evidence shows a clear link between lung cancer and high concentrations of radon, with 21,000 radon-induced U.S. lung cancer deaths per year—second only to cigarette smoking. Thus in geographic areas where radon is present in heightened concentrations, radon is considered a significant indoor air contaminant.\n\nRadon concentration is usually measured in the atmosphere in becquerels per cubic meter (Bq/m), which is an SI derived unit. As a frame of reference, typical domestic exposures are about 100 Bq/m indoors and 10-20 Bq/m outdoors. In the US, radon concentrations are often measured in picocuries per liter (pCi/l), with 1 pCi/l = 37 Bq/m.\n\nThe mining industry traditionally measures exposure using the \"working level\" (WL) index, and the cumulative exposure in \"working level months\" (WLM): 1 WL equals any combination of short-lived Rn progeny (Po, Pb, Bi, and Po) in 1 liter of air that releases 1.3 × 10 MeV of potential alpha energy; one WL is equivalent to 2.08 × 10 joules per cubic meter of air (J/m). The SI unit of cumulative exposure is expressed in joule-hours per cubic meter (J·h/m). One WLM is equivalent to 3.6 × 10 J·h/m. An exposure to 1 WL for 1 working month (170 hours) equals 1 WLM cumulative exposure.\n\nA cumulative exposure of 1 WLM is roughly equivalent to living one year in an atmosphere with a radon concentration of 230 Bq/m.\n\nThe radon (Rn) released into the air decays to Pb and other radioisotopes. The levels of Pb can be measured. The rate of deposition of this radioisotope is dependent on the weather.\n\nRadon concentrations found in natural environments are much too low to be detected by chemical means: for example, a 1000 Bq/m (relatively high) concentration corresponds to 0.17 pico-gram per cubic meter. The average concentration of radon in the atmosphere is about 6 atoms of radon for each molecule in the air, or about 150 atoms in each ml of air. The entire radon activity of the Earth's atmosphere at a time is due to some tens of grams of radon, consistently replaced by decay of larger amounts of radium and uranium. Concentrations can vary greatly from place to place. In the open air, it ranges from 1 to 100 Bq/m, even less (0.1 Bq/m) above the ocean. In caves, aerated mines, or in poorly ventilated dwellings, its concentration can climb to 20-2,000 Bq/m.\n\nIn mining contexts, radon concentrations can be much higher. Ventilation regulations try to maintain concentrations in uranium mines under the \"working level\", and under 3 WL (546 pCi Rn per liter of air; 20.2 kBq/m measured from 1976 to 1985) 95 percent of the time.\nThe concentration in the air at the (unventilated) Gastein Healing Gallery averages 43 kBq/m (about 1.2 nCi/L) with maximal value of 160 kBq/m (about 4.3 nCi/L).\n\nRadon emanates naturally from the ground and from some building materials all over the world, wherever traces of uranium or thorium can be found, and particularly in regions with soils containing granite or shale, which have a higher concentration of uranium. Every square mile of surface soil, to a depth of 6 inches (2.6 km to a depth of 15 cm), contains approximately 1 gram of radium, which releases radon in small amounts to the atmosphere Sand used in making concrete is the major source of radon in buildings. \n\nOn a global scale, it is estimated that 2,400 million curies (91 TBq) of radon are released from soil annually. Not all granitic regions are prone to high emissions of radon. Being a rare gas, it usually migrates freely through faults and fragmented soils, and may accumulate in caves or water. Due to its very small half-life (four days for Rn), its concentration decreases very quickly when the distance from the production area increases.\n\nIts atmospheric concentration varies greatly depending on the season and conditions. For instance, it has been shown to accumulate in the air if there is a meteorological inversion and little wind.\n\nBecause atmospheric radon concentrations are very low, radon-rich water exposed to air continually loses radon by volatilization. Hence, ground water generally has higher concentrations of Rn than surface water, because the radon is continuously produced by radioactive decay of Ra present in rocks. Likewise, the saturated zone of a soil frequently has a higher radon content than the unsaturated zone because of diffusional losses to the atmosphere. As a below-ground source of water, some springs—including hot springs—contain significant amounts of radon. The towns of Boulder, Montana; Misasa; Bad Kreuznach, Germany; and the country of Japan have radium-rich springs which emit radon. To be classified as a radon mineral water, radon concentration must be above a minimum of 2 nCi/L (74 Bq/L). The activity of radon mineral water reaches 2,000 Bq/L in Merano and 4,000 Bq/L in the village of Lurisia (Ligurian Alps, Italy).\n\nRadon is also found in some petroleum. Because radon has a similar pressure and temperature curve as propane, and oil refineries separate petrochemicals based on their boiling points, the piping carrying freshly separated propane in oil refineries can become partially radioactive due to radon decay particles. Residues from the oil and gas industry often contain radium and its daughters. The sulfate scale from an oil well can be radium rich, while the water, oil, and gas from a well often contains radon. The radon decays to form solid radioisotopes which form coatings on the inside of pipework. In an oil processing plant, the area of the plant where propane is processed is often one of the more contaminated areas, because radon has a similar boiling point as propane.\n\nTypical domestic exposures are of ≈ 100 Bq/m indoors, but specifics of construction and ventilation strongly affect levels of accumulation; a further complications for risk assessment is that concentrations in a single location may differ by a factor of two over an hour, and concentrations can vary greatly even between two adjoining rooms in the same structure.\n\nThe distribution of radon concentrations tends to be asymmetrical around the average, the larger concentrations have a disproportionately greater weight. Indoor radon concentration is usually assumed to follow a lognormal distribution on a given territory. Thus, the geometric mean is generally used for estimating the \"average\" radon concentration in an area.\nThe mean concentration ranges from less than 10 Bq/m to over 100 Bq/m in some European countries. Typical geometric standard deviations found in studies range between 2 and 3, meaning (given the 68-95-99.7 rule) that the radon concentration is expected to be more than a hundred times the mean concentration for 2 to 3% of the cases.\n\nThe so-called Watras incident (named after the American construction engineer Stanley Watras), in which an employee at a U.S. nuclear plant triggered radiation monitors while leaving work over several days—despite the fact that the plant had yet to be fueled, and despite the employee being decontaminated and sent home \"clean\" each evening, dramatized that radon levels in particular dwellings can occasionally be orders of magnitude higher than typical.\nThis implied a source of contamination outside the plant, which turned out to be radon levels of 100,000 Bq/m (2.7 nCi/L) in the worker's basement.\nRadon soon became a standard homeowner concern,\nthough typical domestic exposures are two to three orders of magnitude lower (100 Bq/m, or 2.5 pCi/L),\n\nRadon exists in every state and approximately 6% of all houses have elevated levels. The highest average radon concentrations in the United States are found in Iowa and in the Appalachian Mountain areas in southeastern Pennsylvania. Some of the highest readings have been recorded in Mallow, County Cork, Ireland. Iowa has the highest average radon concentrations in the United States due to significant glaciation that ground the granitic rocks from the Canadian Shield and deposited it as soils making up the rich Iowa farmland. Many cities within the state, such as Iowa City, have passed requirements for radon-resistant construction in new homes.\nIn a few locations, uranium tailings have been used for landfills and were subsequently built on, resulting in possible increased exposure to radon.\n\nIn the early 20th century, Pb-contaminated gold, from gold seeds that were used in radiotherapy which had held Rn, were melted down and made into a small number of jewelry pieces, such as rings, in the U.S.\nWearing such a contaminated ring could lead to a skin exposure of 10 to 100 millirad/day (0.004 to 0.04 mSv/h).\n\nThe health effects of high exposure to radon in mines, where exposures reaching 1,000,000 Bq/m can be found, can be recognized in Paracelsus' 1530 description of a wasting disease of miners, the \"mala metallorum.\" Though at the time radon itself was not understood to be the cause—indeed, neither it nor radiation had even been discovered—mineralogist Georg Agricola recommended ventilation of mines to avoid this mountain sickness (\"Bergsucht\"). In 1879, the \"wasting\" was identified as lung cancer by Herting and Hesse in their investigation of miners from Schneeberg, Germany.\n\nBeyond mining in general, radon is a particular problem in the mining of uranium;\nsignificant excess lung cancer deaths have been identified in epidemiological studies of uranium miners and other hard-rock miners employed in the 1940s and 1950s.\n\nThe first major studies with radon and health occurred in the context of uranium mining, first in the Joachimsthal region of Bohemia and then in the Southwestern United States during the early Cold War. Because radon is a product of the radioactive decay of uranium, underground uranium mines may have high concentrations of radon. Many uranium miners in the Four Corners region contracted lung cancer and other pathologies as a result of high levels of exposure to radon in the mid-1950s. The increased incidence of lung cancer was particularly pronounced among Native American and Mormon miners, because those groups normally have low rates of lung cancer.\nSafety standards requiring expensive ventilation were not widely implemented or policed during this period.\n\nIn studies of uranium miners, workers exposed to radon levels of 50 to 150 picocuries of radon per liter of air (2000–6000 Bq/m) for about 10 years have shown an increased frequency of lung cancer.\nStatistically significant excesses in lung cancer deaths were present after cumulative exposures of less than 50 WLM.\nThere is, however, unexplained heterogeneity in these results (whose confidence interval do not always overlap).\nThe size of the radon-related increase in lung cancer risk varied by more than an order of magnitude between the different studies.\n\nHeterogeneities are possibly due to systematic errors in exposure ascertainment, unaccounted for differences in the study populations (genetic, lifestyle, etc.), or confounding mine exposures. There are a number of confounding factors to consider, including exposure to other agents, ethnicity, smoking history, and work experience. The cases reported in these miners cannot be attributed solely to radon or radon daughters but may be due to exposure to silica, to other mine pollutants, to smoking, or to other causes.\nThe majority of miners in the studies are smokers and all inhale dust and other pollutants in mines. Because radon and cigarette smoke both cause lung-cancer, and since the effect of smoking is far above that of radon, it is complicated to disentangle the effects of the two kinds of exposure; misinterpreting the smoking habit by a few percent can blur out the radon effect.\n\nSince that time, ventilation and other measures have been used to reduce radon levels in most affected mines that continue to operate. In recent years, the average annual exposure of uranium miners has fallen to levels similar to the concentrations inhaled in some homes. This has reduced the risk of occupationally induced cancer from radon, although it still remains an issue both for those who are currently employed in affected mines and for those who have been employed in the past.\nThe power to detect any excess risks in miners nowadays is likely to be small, exposures being much smaller than in the early years of mining.\n\nA confounding factor with mines is that both radon concentration and carcinogenic dust (such as quartz dust) depend on the amount of ventilation. This makes it very difficult to state that radon causes cancer in miners; the lung cancers could be partially or wholly caused by high dust concentrations from poor ventilation.\n\nRadon-222 has been classified by International Agency for Research on Cancer as being carcinogenic to humans. In September 2009, the World Health Organization released a comprehensive global initiative on radon that recommended a reference level of 100 Bq/m for radon, urging establishment or strengthening of radon measurement and mitigation programs as well as development building codes requiring radon prevention measures in homes under construction.\nElevated lung cancer rates have been reported from a number of cohort and case-control studies of underground miners exposed to radon and its decay products. There is sufficient evidence for the carcinogenicity of radon and its decay products in humans for such exposures. However, the discussion about the opposite results is still going on, especially a recent retrospective case-control study of lung cancer risk showed substantial cancer rate reduction between 50 and 123 Bq per cubic meter relative to a group at zero to 25 Bq per cubic meter.\n\nThe primary route of exposure to radon and its progeny is inhalation. Radiation exposure from radon is indirect. The health hazard from radon does not come primarily from radon itself, but rather from the radioactive products formed in the decay of radon. The general effects of radon to the human body are caused by its radioactivity and consequent risk of radiation-induced cancer. Lung cancer is the only observed consequence of high concentration radon exposures; both human and animal studies indicate that the lung and respiratory system are the primary targets of radon daughter-induced toxicity.\n\nRadon has a short half-life (3.8 days) and decays into other solid particulate radium-series radioactive nuclides.\nTwo of these decay products, polonium-218 and 214, present a significant radiologic hazard.\nIf the gas is inhaled, the radon atoms decay in the airways or the lungs, resulting in radioactive polonium and ultimately lead atoms attaching to the nearest tissue. If dust or aerosol is inhaled that already carries radon decay products, the deposition pattern of the decay products in the respiratory tract depends on the behaviour of the particles in the lungs. Smaller diameter particles diffuse further into the respiratory system, whereas the larger — tens to hundreds of micron-sized — particles often deposit higher in the airways and are cleared by the body's mucociliary staircase. Deposited radioactive atoms or dust or aerosol particles continue to decay, causing continued exposure by emitting energetic alpha radiation with some associated gamma radiation too, that can damage vital molecules in lung cells,\nby either creating free radicals or causing DNA breaks or damage,\nperhaps causing mutations that sometimes turn cancerous. In addition, through ingestion and blood transport, following crossing of the lung membrane by radon, radioactive progeny may also be transported to other parts of the body.\n\nThe risk of lung cancer caused by smoking is much higher than the risk of lung cancer caused by indoor radon. Radiation from radon has been attributed to increase of lung cancer among smokers too. It is generally believed that exposure to radon and cigarette smoking are synergistic; that is, that the combined effect exceeds the sum of their independent effects. This is because the daughters of radon often become attached to smoke and dust particles, and are then able to lodge in the lungs.\n\nIt is unknown whether radon causes other types of cancer, but recent studies suggest a need for further studies to assess the relationship between radon and leukemia.\n\nThe effects of radon, if found in food or drinking water, are unknown. Following ingestion of radon dissolved in water, the biological half-life for removal of radon from the body ranges from 30 to 70 minutes. More than 90% of the absorbed radon is eliminated by exhalation within 100 minutes, By 600 minutes, only 1% of the absorbed amount remains in the body.\n\nWhile radon presents the aforementioned risks in adults, exposure in children leads to a unique set of health hazards that are still being researched. The physical composition of children leads to faster rates of exposure through inhalation given that their respiratory rate is higher than that of adults, resulting in more gas exchange and more potential opportunities for radon to be inhaled. In addition to this potentially higher dose of radon inhalation, children have smaller lungs, which can become damaged much more quickly than adults’ lungs. For example, children who are exposed to radon and who live in a household where they are exposed to tobacco smoke have a 20 times greater risk of developing lung cancer.\n\nThe resulting health effects in children are similar to those of adults, predominantly including lung cancer and respiratory illnesses such as asthma, bronchitis, and pneumonia. While there have been numerous studies assessing the link between radon exposure and childhood leukemia, the results are largely varied. Many ecological studies show a positive association between radon exposure and childhood leukemia; however, most case control studies have produced a weak correlation. Genotoxicity has been noted in children exposed to high levels of radon, specifically a significant increase of frequency of aberrant cells was noted, as well as an “increase in the frequencies of single and double fragments, chromosome interchanges, [and] number of aberrations chromatid and chromosome type”.\n\nBecause radon is generally associated with diseases that are not detected until many years after elevated exposure, the public may not consider the amount of radon that children are currently being exposed to. Aside from the exposure in the home, one of the major contributors to radon exposure in children are the schools in which they attend almost every day. A survey was conducted in schools across the United States to detect radon levels, and it was estimated that about one in five schools has at least one room (more than 70,000 schoolrooms) with short-term levels above 4pCi/L. The only way to know the level of radon is to test for it, and levels of radon vary across the world. This EPA mapping tool shows average levels of radon by area: <nowiki>https://www.epa.gov/radon/find-information-about-local-radon-zones-and-state-contact-information</nowiki>.\n\nMany states have active radon testing and mitigation programs in place, which require testing in buildings such as public schools. However, these are not standardized nationwide, and the rules and regulations on reducing high radon levels are even less common. The School Health Policies and Practices Study (SHPPS), conducted by the CDC in 2012, found that of schools located in counties with high predicted indoor radon levels, only 42.4% had radon testing policies, and a mere 37.5% had policy for radon-resistant new construction practices. Only about 20% of all schools nationwide have done testing, even though the EPA recommends that every school be tested. These numbers are arguably not high enough to ensure protection of the majority of children from elevated radon exposures. For exposure standards to be effective, they should be set for those most susceptible.\n\nUNSCEAR recommends a reference value of 9 nSv (Bq·h/m).\nFor example, a person living (7000 h/year) in a concentration of 40 Bq/m receives an effective dose of 1 mSv/year.\n\nStudies of miners exposed to radon and its decay products provide a direct basis for assessing their lung cancer risk. The BEIR VI report, entitled \"Health Effects of Exposure to Radon\", reported an excess relative risk from exposure to radon that was equivalent to 1.8% per megabecquerel hours per cubic meter (MBq·h/m) (95% confidence interval: 0.3, 35) for miners with cumulative exposures below 30 MBq·h/m. Estimates of risk per unit exposure are 5.38×10 per WLM; 9.68×10/WLM for ever smokers; and 1.67×10 per WLM for never smokers.\n\nAccording to the UNSCEAR modeling, based on these miner's studies, the excess relative risk from long-term residential exposure to radon at 100 Bq/m is considered to be about 0.16 (after correction for uncertainties in exposure assessment), with about a threefold factor of uncertainty higher or lower than that value.\nIn other words, the absence of ill effects (or even positive hormesis effects) at 100 Bq/m are compatible with the known data.\n\nThe ICPR 65 model follows the same approach, and estimates the relative lifelong risk probability of radon-induced cancer death to 1.23 × 10 per Bq/(m·year). This relative risk is a global indicator; the risk estimation is independent of sex, age, or smoking habit. Thus, if a smoker's chances of dying of lung cancer are 10 times that of a nonsmoker's, the relative risks for a given radon exposure will be the same according to that model, meaning that the absolute risk of a radon-generated cancer for a smoker is (implicitly) tenfold that of a nonsmoker.\nThe risk estimates correspond to a unit risk of approximately 3–6 × 10 per Bq/m, assuming a lifetime risk of lung cancer of 3%. This means that a person living in an average European dwelling with 50 Bq/m has a lifetime excess lung cancer risk of 1.5–3 × 10. Similarly, a person living in a dwelling with a high radon concentration of 1000 Bq/m has a lifetime excess lung cancer risk of 3–6%, implying a doubling of background lung cancer risk.\n\nThe BEIR VI model proposed by the National Academy of Sciences of the USA is more complex. It is a multiplicative model that estimates an excess risk per exposure unit. It takes into account age, elapsed time since exposure, and duration and length of exposure, and its parameters allow for taking smoking habits into account.\nIn the absence of other causes of death, the absolute risks of lung cancer by age 75 at usual radon concentrations of 0, 100, and 400 Bq/m would be about 0.4%, 0.5%, and 0.7%, respectively, for lifelong nonsmokers, and about 25 times greater (10%, 12%, and 16%) for cigarette smokers.\n\nThere is great uncertainty in applying risk estimates derived from studies in miners to the effects of residential radon, and direct estimates of the risks of residential radon are needed.\n\nAs with the miner data, the same confounding factor of other carcinogens such as dust applies. Radon concentration is high in poorly ventilated homes and buildings and such buildings tend to have poor air quality, larger concentrations of dust etc. BEIR VI did not consider that other carcinogens such as dust might be the cause of some or all of the lung cancers, thus omitting a possible spurious relationship.\n\nThe largest natural contributor to public radiation dose is radon, a naturally occurring, radioactive gas found in soil and rock, which comprises approximately 55% of the annual background dose.\nRadon gas levels vary by locality and the composition of the underlying soil and rocks.\n\nRadon (at concentrations encountered in mines) was recognized as carcinogenic in the 1980s, in view of the lung cancer statistics for miners' cohorts.\nAlthough radon may present significant risks, thousands of persons annually go to radon-contaminated mines for deliberate exposure to help with the symptoms of arthritis without any serious health effects.\n\nRadon as a terrestrial source of background radiation is of particular concern because, although overall very rare, where it does occur it often does so in high concentrations. Some of these areas, including parts of Cornwall and Aberdeenshire have high enough natural radiation levels that nuclear licensed sites cannot be built there—the sites would already exceed legal limits before they opened, and the natural topsoil and rock would all have to be disposed of as low-level nuclear waste.\nPeople in affected localities can receive up to 10 mSv per year background radiation.\n\nThis led to a health policy problem: what is the health impact of exposure to radon concentrations (100 Bq/m) typically found in some buildings?\n\nWhen exposure to a carcinogenic substance is suspected, the cause/effect relationship on any given case can never be ascertained. Lung cancer occurs spontaneously, and there is no difference between a \"natural\" cancer and another one caused by radon (or smoking). Furthermore, it takes years for a cancer to develop, so that determining the past exposure of a case is usually very approximative. The health effect of radon can only be demonstrated through theory and statistical observation.\n\nThe study design for epidemiological methods may be of three kinds:\n\nFurthermore, theory and observation must confirm each other for a relationship to be accepted as fully proven. Even when a statistical link between factor and effect appears significant, it must be backed by a theoretical explanation; and a theory is not accepted as factual unless confirmed by observations.\n\nCohort studies are impractical for the study of domestic radon exposure. With the expected effect of small exposures being very small, the direct observation of this effect would require huge cohorts: the populations of whole countries.\n\nSeveral ecological studies have been performed to assess possible relationships between selected cancers and estimated radon levels within particular geographic regions where environmental radon levels appear to be higher than other geographic regions.\nResults of such ecological studies are mixed; both positive and negative associations, as well as no significant associations, have been suggested.\n\nThe most direct way to assess the risks posed by radon in homes is through case-control studies.\n\nThe studies have not produced a definitive answer, primarily because the risk is likely to be very small at the low exposure encountered from most homes and because it is difficult to estimate radon exposures that people have received over their lifetimes. In addition, it is clear that far more lung cancers are caused by smoking than are caused by radon.\n\nEpidemiologic radon studies have found trends to increased lung cancer risk from radon with a no evidence of a threshold, and evidence against a threshold above high as 150 Bq/m (almost exactly the EPA's action level of 4 pCi/L). Another study similarly found that there is no evidence of a threshold but lacked the statistical power to clearly identify the threshold at this low level. Notably, the latter deviance from zero at low level convinced the World Health Organization that, \"The dose-response relation seems to be linear without evidence of a threshold, meaning that the lung cancer risk increases proportionally with increasing radon exposure.\"\n\nThe most elaborate case-control epidemiologic radon study performed by R. William Field and colleagues identified a 50% increased lung cancer risk with prolonged radon exposure at the EPA's action level of 4 pCi/L. Iowa has the highest average radon concentrations in the United States and a very stable population which added to the strength of the study. For that study, the odds ratio was found to be increased slightly above the confidence interval (95% CI) for cumulative radon exposures above 17 WLM (6.2 pC/L=230 Bq/m and above).\n\nThe results of a methodical ten-year-long, case-controlled study of residential radon exposure in Worcester County, Massachusetts, found an apparent 60% \"reduction\" in lung cancer risk amongst people exposed to low levels (0–150 Bq/m) of radon gas; levels typically encountered in 90% of American homes—an apparent support for the idea of radiation hormesis. In that study, a significant result (95% CI) was obtained for the 75-150 Bq/m category.\nThe study paid close attention to the cohort's levels of smoking, occupational exposure to carcinogens and education attainment. However, unlike the majority of the residential radon studies, the study was not population-based. Errors in retrospective exposure assessment could not be ruled out in the finding at low levels. Other studies into the effects of domestic radon exposure have not reported a hormetic effect; including for example the respected \"Iowa Radon Lung Cancer Study\" of Field et al. (2000), which also used sophisticated radon exposure dosimetry.\n\n\"Radon therapy\" is an intentional exposure to radon via inhalation or ingestion. Nevertheless, epidemiological evidence shows a clear link between breathing high concentrations of radon and incidence of lung cancer.\n\nIn the late 20th century and early 21st century, some \"health mines\" were established in Basin, Montana, which attracted people seeking relief from health problems such as arthritis through limited exposure to radioactive mine water and radon. The practice is controversial because of the \"well-documented ill effects of high-dose radiation on the body.\" Radon has nevertheless been found to induce beneficial long-term effects.\n\nRadioactive water baths have been applied since 1906 in Jáchymov, Czech Republic, but even before radon discovery they were used in Bad Gastein, Austria. Radium-rich springs are also used in traditional Japanese onsen in Misasa, Tottori Prefecture. Drinking therapy is applied in Bad Brambach, Germany. Inhalation therapy is carried out in Gasteiner-Heilstollen, Austria, in Kowary, Poland and in Boulder, Montana, United States. In the United States and Europe there are several \"radon spas\", where people sit for minutes or hours in a high-radon atmosphere in the belief that low doses of radiation will invigorate or energize them.\n\nRadon has been produced commercially for use in radiation therapy, but for the most part has been replaced by radionuclides made in accelerators and nuclear reactors. Radon has been used in implantable seeds, made of gold or glass, primarily used to treat cancers.\nThe gold seeds were produced by filling a long tube with radon pumped from a radium source, the tube being then divided into short sections by crimping and cutting. The gold layer keeps the radon within, and filters out the alpha and beta radiations, while allowing the gamma rays to escape (which kill the diseased tissue). The activities might range from 0.05 to 5 millicuries per seed (2 to 200 MBq). The gamma rays are produced by radon and the first short-lived elements of its decay chain (Po, Pb, Bi, Po).\n\nRadon and its first decay products being very short-lived, the seed is left in place. After 12 half-lives (43 days), radon radioactivity is at 1/2000 of its original level. At this stage, the predominant residual activity is due to the radon decay product Pb, whose half-life (22.3 year) is 2000 times that of radon (and whose activity is thus 1/2000 of radon's), and its descendants Bi and Po, totalizing 0.03% of the initial seed activity.\n\nThe Federal Radon Action Plan, also known as FRAP, was created in 2010 and launched in 2011. It was piloted by the U.S. Environmental Protection Agency in conjunction with the U.S. Departments of Health and Human Services, Agriculture, Defense, Energy, Housing and Urban Development, the Interior, Veterans Affairs, and the General Services Administration. The goal set forth by FRAP was to eliminate radon induced cancer that can be prevented by expanding radon testing, mitigating high levels of radon exposure, and developing radon resistant construction, and to meet Healthy People 2020 radon objectives. They identified the barriers to change as limited public knowledge of the dangers of radon exposure, the perceived high costs of mitigation, and the availability of radon testing. As a result, they also identified major ways to create change: demonstrate the importance of testing and the ease of mitigation, provide incentives for testing and mitigation, and build the radon services industry. To complete these goals, representatives from each organization and department established specific commitments and timelines to complete tasks and continued to meet periodically. However, FRAP was concluded in 2016 as The National Radon Action Plan took over. In the final report on commitments, it was found that FRAP completed 88% of their commitments. They reported achieving the highest rates of radon mitigation and new construction mitigation in the United States as of 2014. FRAP concluded that because of their efforts, at least 1.6 million homes, schools, and childcare facilities received direct and immediate positive effects.\n\nThe National Radon Action Plan, also known as NRAP, was created in 2014 and launched in 2015. It is led by The American Lung Association with collaborative efforts from the American Association of Radon Scientists and Technologists, American Society of Home Inspectors, Cancer Survivors Against Radon, Children’s Environmental Health Network, Citizens for Radioactive Radon Reduction, Conference of Radiation Control Program Directors, Environmental Law Institute, National Center for Healthy Housing, U.S. Environmental Protection Agency, U.S. Department of Health and Human Services, and U.S. Department of Housing and Urban Development. The goals of NRAP are to continue efforts set forth by FRAP to eliminate radon induced cancer that can be prevented by expanding radon testing, mitigating high levels of radon exposure, and developing radon resistant construction. NRAP also aims to reduce radon risk in 5 million homes, and save 3,200 lives by 2020. To complete these goals, representatives from each organization have established the following action plans: embed radon risk reduction as a standard practice across housing sectors, provide incentives and support to test and mitigate radon, promote the use of certified radon services and build the industry, and increase public attention to radon risk and the importance of reduction. The NRAP is currently in action, implementing programs, identifying approaches, and collaborating across organizations to achieve these goals.\n\nThe only dose-effect relationship available are those of miners cohorts (for much higher exposures), exposed to radon. Studies of Hiroshima and Nagasaki survivors are less informative (the exposure to radon is chronic, localized, and the ionizing radiations are alpha rays).\nAlthough low-exposed miners experienced exposures comparable to long-term residence in high-radon dwellings, the mean cumulative exposure among miners is approximately 30-fold higher than that associated with long-term residency in a typical home. Moreover, the smoking is a significant confounding factor in all miners' studies. It can be concluded from miner studies that when the radon exposure in dwellings compares to that in mines (above 1000 Bq/m), radon is a proven health hazard; but in the 1980s very little was known on the dose-effect relationship, both theoretically and statistical.\n\nStudies have been made since the 1980s, both on epidemiological studies and in the radiobiology field.\nIn the radiobiology and carcinogenesis studies, progress has been made in understanding the first steps of cancer development, but not to the point of validating a reference dose-effect model. The only certainty gained is that the process is very complex, the resulting dose-effect response being complex, and most probably not a linear one.\nBiologically based models have also been proposed that could project substantially reduced carcinogenicity at low doses.\nIn the epidemiological field, no definite conclusion has been reached. However, from the evidence now available, a threshold exposure, that is, a level of exposure below which there is no effect of radon, cannot be excluded. L\n\nGiven the radon distribution observed in dwellings, and the dose-effect relationship proposed by a given model, a theoretical number of victims can be calculated, and serve as a basis for public health policies.\n\nWith the BEIR VI model, the main health effect (nearly 75% of the death toll) is to be found at low radon concentration exposures, because most of the population (about 90%) lives in the 0-200 Bq/m range. Under this modeling, the best policy is obviously to reduce the radon levels of all homes where the radon level is above average, because this leads to a significant decrease of radon exposure on a significant fraction of the population; but this effect is predicted in the 0-200 Bq/m range, where the linear model has its maximum uncertainty. From the statistical evidence available, a threshold exposure cannot be excluded; if such a threshold exists, the real radon health effect would in fact be limited to those homes where the radon concentrations reaches that observed in mines — at most a few percent. If a radiation hormesis effect exists after all, the situation would be even worse: under that hypothesis, suppressing the natural low exposure to radon (in the 0-200 Bq/m range) would actually lead to an increase of cancer incidence, due to the suppression of this (hypothetical) protecting effect. As the low-dose response is unclear, the choice of a model is very controversial.\n\nNo conclusive statistics being available for the levels of exposure usually found in homes, the risks posed by domestic exposures is usually estimated on the basis of observed lung-cancer deaths caused by higher exposures in mines, under the assumption that the risk of developing lung-cancer increases linearly as the exposure increases. This was the basis for the model proposed by BEIR IV in the 1980s. The linear no-threshold model has since been kept in a conservative approach by the UNSCEAR report and the BEIR VI and BEIR VII publications, essentially for lack of a better choice:Until the [...] uncertainties on low-dose response are resolved, the Committee believes that [\"the linear no-threshold model\"] is consistent with developing knowledge and that it remains, accordingly, the most scientifically defensible approximation of low-dose response. However, a strictly linear dose response should not be expected in all circumstances.\nThe BEIR VI committee adopted the linear no-threshold assumption based on its understanding of the mechanisms of radon-induced lung cancer, but recognized that this understanding is incomplete and that therefore the evidence for this assumption is not conclusive.\n\nIn discussing these figures, it should be kept in mind that both the radon distribution in dwelling and its effect at low exposures are not precisely known, and the radon health effect has to be computed (deaths caused by radon domestic exposure cannot be observed as such). These estimations are strongly dependent on the model retained.\n\nAccording to these models, radon exposure is thought to be the second major cause of lung cancer after smoking.\nIowa has the highest average radon concentration in the United States; studies performed there have demonstrated a 50% increased lung cancer risk with prolonged radon exposure above the EPA's action level of 4 pCi/L.\n\nBased on studies carried out by the National Academy of Sciences in the United States, radon would thus be the second leading cause of lung cancer after smoking, and accounts for 15,000 to 22,000 cancer deaths per year in the US alone.\nThe United States Environmental Protection Agency (EPA) says that radon is the number one cause of lung cancer among non-smokers.\nThe general population is exposed to small amounts of polonium as a radon daughter in indoor air; the isotopes Po and Po are thought to cause the majority of the estimated 15,000–22,000 lung cancer deaths in the US every year that have been attributed to indoor radon.\nThe Surgeon General of the United States has reported that over 20,000 Americans die each year of radon-related lung cancer.\n\nIn the United Kingdom, residential radon would be, after cigarette smoking, the second most frequent cause of lung cancer deaths: according to models, 83.9% of deaths are attributed to smoking only, 1.0% to radon only, and 5.5% to a combination of radon and smoking.\n\nThe World Health Organization has recommended a radon reference concentration of 100 Bq/m (2.7 pCi/L). The European Union recommends that action should be taken starting from concentrations of 400 Bq/m (11 pCi/L) for older dwellings and 200 Bq/m (5 pCi/L) for newer ones. After publication of the North American and European Pooling Studies, Health Canada proposed a new guideline that lowers their action level from 800 to 200 Bq/m (22 to 5 pCi/L).\nThe United States Environmental Protection Agency (EPA) strongly recommends action for any dwelling with a concentration higher than 148 Bq/m (4 pCi/L),\nand encourages action starting at 74 Bq/m (2 pCi/L).\n\nEPA recommends that all homes should be monitored for radon. If testing shows levels less than 4 picocuries radon per liter of air (160 Bq/m), then no action is necessary. For levels of 20 picocuries radon per liter of air (800 Bq/m) or higher, the home owner should consider some type of procedure to decrease indoor radon levels. For instance, as radon has a half-life of four days, opening the windows once a day can cut the mean radon concentration to one fourth of its level.\n\nThe United States Environmental Protection Agency (EPA) recommends homes be fixed if an occupant's long-term exposure will average 4 picocuries per liter (pCi/L) that is 148 Bq/m. EPA estimates that one in 15 homes in the United States has radon levels above the recommended guideline of 4 pCi/L.\nEPA radon risk level tables including comparisons to other risks encountered in life are available in their citizen's guide.\nThe EPA estimates that nationally, 8% to 12% of all dwellings are above their maximum \"safe levels\" (four picocuries per liter—the equivalent to roughly 200 chest x-rays). The United States Surgeon General and the EPA both recommend that all homes be tested for radon.\n\nThe limits retained do not correspond to a known threshold in the biological effect, but are determined by a cost-efficiency analysis. EPA believes that a 150 Bq/m level (4 pCi/L) is achievable in the majority of homes for a reasonable cost, the average cost per life saved by using this action level is about $700,000.\n\nFor radon concentration in drinkable water, the World Health Organization issued as guidelines (1988) that remedial action should be considered when the radon activity exceeded 100 kBq/m in a building, and remedial action should be considered \"without long delay\" if exceeding 400 kBq/m.\n\nThere are relatively simple tests for radon gas. Radon test kits are commercially available. The short-term radon test kits used for screening purposes are inexpensive, in many cases free. Discounted test kits can be purchased online through The National Radon Program Services at Kansas State University or through state radon offices. Information about local radon zones and specific state contact information can be accessed through the EPA Map at <nowiki>https://www.epa.gov/radon/find-information-about-local-radon-zones-and-state-contact-information</nowiki>. The kit includes a collector that the user hangs in the lowest livable floor of the dwelling for 2 to 7 days. Charcoal canisters are another type of short-term radon test, and are designed to be used for 2 to 4 days. The user then sends the collector to a laboratory for analysis. Both devices are passive, meaning that they do not need power to function.\n\nIt should be noted that the accuracy of the residential radon test depends upon the lack of ventilation in the house when the sample is being obtained. Thus, the occupants will be instructed not to open windows, etc., for ventilation during the pendency of test, usually two days or more.\n\nLong-term kits, taking collections for 3 months up to one year, are also available. An open-land test kit can test radon emissions from the land before construction begins. A Lucas cell is one type of long-term device. A Lucas cell is also an active device, or one that requires power to function. Active devices provide continuous monitoring, and some can report on the variation of radon and interference within the testing period. These tests usually require operation by trained testers and are often more expensive than passive testing. The National Radon Proficiency Program (NRPP) provides a list of radon measurement professionals.\n\nRadon levels fluctuate naturally. An initial test might not be an accurate assessment of a home's average radon level. Transient weather can affect short term measurements. Therefore, a high result (over 4 pCi/L) justifies repeating the test before undertaking more expensive abatement projects. Measurements between 4 and 10 pCi/L warrant a long-term radon test. Measurements over 10 pCi/L warrant only another short-term test so that abatement measures are not unduly delayed. Purchasers of real estate are advised to delay or decline a purchase if the seller has not successfully abated radon to 4 pCi/L or less.\n\nSince radon concentrations vary substantially from day to day, single grab-type measurements are generally not very useful, except as a means of identifying a potential problem area, and indicating a need for more sophisticated testing. The EPA recommends that an initial short-term test be performed in a closed building. An initial short-term test of 2 to 90 days allows residents to be informed quickly in case a home contains high levels of radon. Long-term tests provide a better estimate of the average annual radon level.\n\nTransport of radon in indoor air is almost entirely controlled by the ventilation rate in the enclosure. Since air pressure is usually lower inside houses than it is outside, the home acts like a vacuum, drawing radon gas in through cracks in the foundation or other openings such as ventilation systems. Generally, the indoor radon concentrations increase as ventilation rates decrease. In a well ventilated place, the radon concentration tends to align with outdoor values (typically 10 Bq/m, ranging from 1 to 100 Bq/m).\n\nRadon levels in indoor air can be lowered in several ways, from sealing cracks in floors and walls to increasing the ventilation rate of the building. Listed here are some of the accepted ways of reducing the amount of radon accumulating in a dwelling:\n\n· Improving the ventilation of the dwelling and avoiding the transport of radon from the basement, or ground, into living areas;\n\n· Installing crawlspace or basement ventilation systems;\n\n· Installing sub-slab depressurization radon mitigation systems, which vacuum radon from under slab-on-grade foundations;\n\n· Installing sub-membrane depressurization radon mitigation systems, which vacuum radon from under a membrane that covers the ground used in crawlspace\n\nfoundations;\n\n· Installing a radon sump system in the basement;\n\n· Sealing floors and walls (not a stand-alone solution); and\n\n· Installing a positive pressurization or positive supply ventilation system.\n\nThe half-life for radon is 3.8 days, indicating that once the source is removed, the hazard will be greatly reduced within approximately one month (seven half-lives).\n\nPositive-pressure ventilation systems can be combined with a heat exchanger to recover energy in the process of exchanging air with the outside, and simply exhausting basement air to the outside is not necessarily a viable solution as this can draw radon gas \"into\" a dwelling. Homes built on a crawl space may benefit from a radon collector installed under a \"radon barrier, or membrane\" (a sheet of plastic or laminated polyethylene film that covers the crawl space floor).\n\nASTM E-2121 is a standard for reducing radon in homes as far as practicable below 4 picocuries per liter (pCi/L) in indoor air.\n\nIn the US, approximately 14 states have a state radon programs which train and license radon mitigation contractors and radon measurement professionals. To determine if your state licenses radon professionals contact your state health department. The National Environmental Health Association and the National Radon Safety Board administer voluntary National Radon Proficiency Programs for radon professionals consisting of individuals and companies wanting to take training courses and examinations to demonstrate their competency. Without the proper equipment or technical knowledge, radon levels can actually increase or create other potential hazards and additional costs. A list of certified mitigation service providers is available through state radon offices, which are listed on the EPA website at www.epa.gov/radon/whereyoulive.html. Indoor radon can be mitigated by sealing basement foundations, water drainage, or by sub-slab, or sub-membrane depressurization. In many cases, mitigators can use PVC piping and specialized radon suction fans to exhaust sub-slab, or sub-membrane radon and other soil gases to the outside atmosphere. Most of these solutions for radon mitigation require maintenance, and it is important to continually replace any fans or filters as needed to continue proper functioning.\n\nSince radon gas is found in most soil and rocks, it is not only able to move into the air, but also into underground water sources. Radon may be present in well water and can be released into the air in homes when water is used for showering and other household uses. If it is suspected that a private well or drinking water may be affected by radon, the National Radon Program Services Hotline at 1-800-SOS-RADON can be contacted for information regarding state radon office phone numbers. State radon offices can provide additional resources, such as local laboratories that can test water for radon.\n\nIf it is determined that radon is present in a private well, installing either a point-of-use or point-of-entry solution may be necessary. Point-of-use treatments are installed at the tap, and are only helpful in removing radon from drinking water. To address the more common problem of breathing in radon released from water used during showers and other household activities, a point-of-entry solution may be more reliable. Point-of-entry systems usually involve a granular activated carbon filter, or an aeration system; both methods can help to remove radon before it enters the home’s water distribution system. Aeration systems and granular activation carbon filters both have advantages and disadvantages, so it is recommended to contact state radon departments or a water treatment professional for specific recommendations.\n\nThe high cost of radon remediation in the 1980s led to detractors arguing that the issue is a financial boondoggle reminiscent of the swine flu scare of 1976. They further argued that the results of mitigation are inconsistent with lowered cancer risk, especially when indoor radon levels are in the lower range of the actionable exposure level.\n\n\n"}
{"id": "60976", "url": "https://en.wikipedia.org/wiki?curid=60976", "title": "Health system", "text": "Health system\n\nA health system, also sometimes referred to as health care system or as healthcare system, is the organization of people, institutions, and resources that deliver health care services to meet the health needs of target populations.\n\nThere is a wide variety of health systems around the world, with as many histories and organizational structures as there are nations. Implicitly, nations must design and develop health systems in accordance with their needs and resources, although common elements in virtually all health systems are primary healthcare and public health measures. In some countries, health system planning is distributed among market participants. In others, there is a concerted effort among governments, trade unions, charities, religious organizations, or other co-ordinated bodies to deliver planned health care services targeted to the populations they serve. However, health care planning has been described as often evolutionary rather than revolutionary.\n\nThe World Health Organization (WHO), the directing and coordinating authority for health within the United Nations system, is promoting a goal of universal health care: to ensure that all people obtain the health services they need without suffering financial hardship when paying for them. According to WHO, healthcare systems' goals are good health for the citizens, responsiveness to the expectations of the population, and fair means of funding operations. Progress towards them depends on how systems carry out four vital functions: provision of health care services, resource generation, financing, and stewardship. Other dimensions for the evaluation of health systems include quality, efficiency, acceptability, and equity. They have also been described in the United States as \"the five C's\": Cost, Coverage, Consistency, Complexity, and Chronic Illness. Also, continuity of health care is a major goal.\n\nOften health system has been defined with a reductionist perspective, for example reducing it to healthcare system. In many publications, for example, both expressions are used interchangeably. Some authors have developed arguments to expand the concept of health systems, indicating additional dimensions that should be considered:\n\nThe World Health Organization defines health systems as follows:\nA health system consists of all organizations, people and actions whose primary intent is to promote, restore or maintain health. This includes efforts to influence determinants of health as well as more direct health-improving activities. A health system is therefore more than the pyramid of publicly owned facilities that deliver personal health services. It includes, for example, a mother caring for a sick child at home; private providers; behaviour change programmes; vector-control campaigns; health insurance organizations; occupational health and safety legislation. It includes inter-sectoral action by health staff, for example, encouraging the ministry of education to promote female education, a well known determinant of better health.\n\nHealthcare providers are institutions or individuals providing healthcare services. Individuals including health professionals and allied health professions can be self-employed or working as an employee in a hospital, clinic, or other health care institution, whether government operated, private for-profit, or private not-for-profit (e.g. non-governmental organization). They may also work outside of direct patient care such as in a government health department or other agency, medical laboratory, or health training institution. Examples of health workers are doctors, nurses, midwives, dietitians, paramedics, dentists, medical laboratory technologists, therapists, psychologists, pharmacists, chiropractors, optometrists, community health workers, traditional medicine practitioners, and others.\n\nThere are generally five primary methods of funding health systems:\n\n\nMost countries' systems feature a mix of all five models. One study based on data from the OECD concluded that all types of health care finance \"are compatible with\" an efficient health system. The study also found no relationship between financing and cost control.\n\nThe term health insurance is generally used to describe a form of insurance that pays for medical expenses. It is sometimes used more broadly to include insurance covering disability or long-term nursing or custodial care needs. It may be provided through a social insurance program, or from private insurance companies. It may be obtained on a group basis (e.g., by a firm to cover its employees) or purchased by individual consumers. In each case premiums or taxes protect the insured from high or unexpected health care expenses.\n\nBy estimating the overall cost of health care expenses, a routine finance structure (such as a monthly premium or annual tax) can be developed, ensuring that money is available to pay for the health care benefits specified in the insurance agreement. The benefit is typically administered by a government agency, a non-profit health fund or a corporation operating seeking to make a profit.\n\nMany forms of commercial health insurance control their costs by restricting the benefits that are paid by through deductibles, co-payments, coinsurance, policy exclusions, and total coverage limits and will severely restrict or refuse coverage of pre-existing conditions. Many government schemes also have co-payment schemes but exclusions are rare because of political pressure. The larger insurance schemes may also negotiate fees with providers.\n\nMany forms of social insurance schemes control their costs by using the bargaining power of their community they represent to control costs in the health care delivery system. For example, by negotiating drug prices directly with pharmaceutical companies negotiating standard fees with the medical profession, or reducing unnecessary health care costs. Social schemes sometimes feature contributions related to earnings as part of a scheme to deliver universal health care, which may or may not also involve the use of commercial and non-commercial insurers. Essentially the more wealthy pay proportionately more into the scheme to cover the needs of the relatively poor who therefore contribute proportionately less. There are usually caps on the contributions of the wealthy and minimum payments that must be made by the insured (often in the form of a minimum contribution, similar to a deductible in commercial insurance models).\n\nIn addition to these traditional health care financing methods, some lower income countries and development partners are also implementing non-traditional or innovative financing mechanisms for scaling up delivery and sustainability of health care, such as micro-contributions, public-private partnerships, and market-based financial transaction taxes. For example, as of June 2011, UNITAID had collected more than one billion dollars from 29 member countries, including several from Africa, through an air ticket solidarity levy to expand access to care and treatment for HIV/AIDS, tuberculosis and malaria in 94 countries.\n\nIn most countries, wage costs for healthcare practitioners are estimated to represent between 65% and 80% of renewable health system expenditures. There are three ways to pay medical practitioners: fee for service, capitation, and salary. There has been growing interest in blending elements of these systems.\n\n\"Fee-for-service arrangements\" pay general practitioners (GPs) based on the service. They are even more\nwidely used for specialists working in ambulatory care.\n\nThere are two ways to set fee levels:<ref name=\"docteur/oxley\"></ref>\n\nIn \"capitation payment systems\", GPs are paid for each patient on their \"list\", usually with adjustments for factors such as age and gender. According to OECD, \"these systems are used in Italy (with some fees), in all four countries of the United Kingdom (with some fees and allowances for specific services), Austria (with fees for specific services), Denmark (one third of income with remainder fee for service), Ireland (since 1989), the Netherlands (fee-for-service for privately insured patients and public employees) and Sweden (from 1994). Capitation payments have become more frequent in \"managed care\" environments in the United States.\"\n\nAccording to OECD, \"Capitation systems allow funders to control the overall level of primary health expenditures, and the allocation of funding among GPs is determined by patient registrations. However, under this approach, GPs may register too many patients and under-serve them, select the better risks and refer on patients who could have been treated by the GP directly. Freedom of consumer choice over doctors, coupled with the principle of \"money following the patient\" may moderate some of these risks. Aside from selection, these problems are likely to be less marked than under salary-type arrangements.\"\n\nIn several OECD countries, general practitioners (GPs) are employed on \"salaries\" for the government. According to OECD, \"Salary arrangements allow funders to control primary care costs directly; however, they may lead to under-provision of services (to ease workloads), excessive referrals to secondary providers and lack of attention to the preferences of patients.\" There has been movement away from this system.\n\nSound information plays an increasingly critical role in the delivery of modern health care and efficiency of health systems. Health informatics – the intersection of information science, medicine and healthcare – deals with the resources, devices, and methods required to optimize the acquisition and use of information in health and biomedicine. Necessary tools for proper health information coding and management include clinical guidelines, formal medical terminologies, and computers and other information and communication technologies. The kinds of health data processed may include patients' medical records, hospital administration and clinical functions, and human resources information.\n\nThe use of health information lies at the root of evidence-based policy and evidence-based management in health care. Increasingly, information and communication technologies are being utilised to improve health systems in developing countries through: the standardisation of health information; computer-aided diagnosis and treatment monitoring; informing population groups on health and treatment.\n\nThe management of any health system is typically directed through a set of policies and plans adopted by government, private sector business and other groups in areas such as personal healthcare delivery and financing, pharmaceuticals, health human resources, and public health.\n\nPublic health is concerned with threats to the overall health of a community based on population health analysis. The population in question can be as small as a handful of people, or as large as all the inhabitants of several continents (for instance, in the case of a pandemic). Public health is typically divided into epidemiology, biostatistics and health services. Environmental, social, behavioral, and occupational health are also important subfields.\nToday, most governments recognize the importance of public health programs in reducing the incidence of disease, disability, the effects of ageing and health inequities, although public health generally receives significantly less government funding compared with medicine. For example, most countries have a vaccination policy, supporting public health programs in providing vaccinations to promote health. Vaccinations are voluntary in some countries and mandatory in some countries. Some governments pay all or part of the costs for vaccines in a national vaccination schedule.\n\nThe rapid emergence of many chronic diseases, which require costly long-term care and treatment, is making many health managers and policy makers re-examine their healthcare delivery practices. An important health issue facing the world currently is HIV/AIDS. Another major public health concern is diabetes. In 2006, according to the World Health Organization, at least 171 million people worldwide suffered from diabetes. Its incidence is increasing rapidly, and it is estimated that by the year 2030, this number will double. A controversial aspect of public health is the control of tobacco smoking, linked to cancer and other chronic illnesses.\n\nAntibiotic resistance is another major concern, leading to the reemergence of diseases such as tuberculosis. The World Health Organization, for its World Health Day 2011 campaign, is calling for intensified global commitment to safeguard antibiotics and other antimicrobial medicines for future generations.\n\nSince 2000, more and more initiatives have been taken at the international and national levels in order to strengthen national health systems as the core components of the global health system. Having this scope in mind, it is essential to have a clear, and unrestricted, vision of national health systems that might generate further progresses in global health. The elaboration and the selection of performance indicators are indeed both highly dependent on the conceptual framework adopted for the evaluation of the health systems performances. Like most social systems, health systems are complex adaptive systems where change does not necessarily follow rigid epidemiological models. In complex systems path dependency, emergent properties and other non-linear patterns are under-explored and unmeasured, which can lead to the development of inappropriate guidelines for developing responsive health systems.\nAn increasing number of tools and guidelines are being published by international agencies and development partners to assist health system decision-makers to monitor and assess health systems strengthening including human resources development using standard definitions, indicators and measures. In response to a series of papers published in 2012 by members of the World Health Organization's Task Force on Developing Health Systems Guidance, researchers from the Future Health Systems consortium argue that there is insufficient focus on the 'policy implementation gap'. Recognizing the diversity of stakeholders and complexity of health systems is crucial to ensure that evidence-based guidelines are tested with requisite humility and without a rigid adherence to models dominated by a limited number of disciplines. Healthcare services often implement Quality Improvement Initiatives to overcome this policy implementation gap. Although many deliver improved healthcare a large proportion fail to sustain. Numerous tools and frameworks have been created to respond to this challenge and increase improvement longevity. One tool highlighted the need for these tools to respond to user preferences and settings to optimize impact. \n\nHealth Policy and Systems Research (HPSR) is an emerging multidisciplinary field that challenges 'disciplinary capture' by dominant health research traditions, arguing that these traditions generate premature and inappropriately narrow definitions that impede rather than enhance health systems strengthening. HPSR focuses on low- and middle-income countries and draws on the relativist social science paradigm which recognises that all phenomena are constructed through human behaviour and interpretation. In using this approach, HPSR offers insight into health systems by generating a complex understanding of context in order to enhance health policy learning. HPSR calls for greater involvement of local actors, including policy makers, civil society and researchers, in decisions that are made around funding health policy research and health systems strengthening.\n\nHealth systems can vary substantially from country to country, and in the last few years, comparisons have been made on an international basis. The World Health Organization, in its \"World Health Report 2000\", provided a ranking of health systems around the world according to criteria of the overall level and distribution of health in the populations, and the responsiveness and fair financing of health care services. The goals for health systems, according to the WHO's \"World Health Report 2000 – Health systems: improving performance\" (WHO, 2000), are good health, responsiveness to the expectations of the population, and fair financial contribution. There have been several debates around the results of this WHO exercise, and especially based on the country ranking linked to it, insofar as it appeared to depend mostly on the choice of the retained indicators.\n\nDirect comparisons of health statistics across nations are complex. The Commonwealth Fund, in its annual survey, \"Mirror, Mirror on the Wall\", compares the performance of the health systems in Australia, New Zealand, the United Kingdom, Germany, Canada and the United States Its 2007 study found that, although the United States system is the most expensive, it consistently underperforms compared to the other countries. A major difference between the United States and the other countries in the study is that the United States is the only country without universal health care. The OECD also collects comparative statistics, and has published brief country profiles. Health Consumer Powerhouse makes comparisons between both national health care systems in the Euro health consumer index and specific areas of health care such as diabetes\n\nPhysicians and hospital beds per 1000 inhabitants vs Health Care Spending in 2008 for OECD Countries. The data source is http://www.oecd.org.\n\n"}
{"id": "30940837", "url": "https://en.wikipedia.org/wiki?curid=30940837", "title": "International Hygiene Exhibition", "text": "International Hygiene Exhibition\n\nThe International Hygiene Exhibition was a world's fair focusing on medicine and public health, held in Dresden, Germany, in 1911.\n\nThe leading figure organizing the exhibition was German philanthropist and businessman , who had grown wealthy from his Odol mouthwash brand, and was enthusiastic to educate the public about advances in public health. Lingner had previously organized a public-health exhibition as part of the 1903 Dresden municipal expo, and its success led him to plan a larger endeavor.\n\nThe exhibition opened on May 6, 1911, with 30 countries participating, 100 buildings built for the event, and 5 million visitors over its duration. It emphasized accessible visual representations of the body, and a particular sensation were the transparent organs preserved and displayed according to a method devised by Werner Spalteholz.\n\nFollowing the exhibition, its contents became the permanent German Hygiene Museum in Dresden. Its success spawned several follow-up expos, most notably the 1926 \"GeSoLei\" exhibition in Düsseldorf.\n\nOther International Exhibitions of Hygiene were held in:\n\n"}
{"id": "410851", "url": "https://en.wikipedia.org/wiki?curid=410851", "title": "Kegel exercise", "text": "Kegel exercise\n\nKegel exercise, also known as pelvic floor exercise, consists of repeatedly contracting and relaxing the muscles that form part of the pelvic floor, now sometimes colloquially referred to as the \"Kegel muscles\". The exercise can be performed multiple times each day, for several minutes at a time, for one to three months, to begin to have an effect.\n\nKegel exercises can make the pelvic floor muscles stronger. These are the muscles that hold up the bladder and help keep it from leaking. Exercises are usually done to reduce urinary stress incontinence (especially after childbirth) and reduce premature ejaculation in men.\n\nSeveral tools exist to help with these exercises, although various studies debate the relative effectiveness of different tools versus traditional exercises. They were first described in 1948 by American gynecologist Arnold Kegel.\n\nFactors such as pregnancy, childbirth, aging, being overweight, and abdominal surgery such as cesarean section, often result in the weakening of the pelvic muscles. This can be assessed by either digital examination of vaginal pressure or using a Kegel perineometer. Kegel exercises are useful in regaining pelvic floor muscle strength in such cases.\n\nPelvic floor exercises (muscle training) can be included in conservative treatment approaches for women with urinary incontinence. There is tentative evidence that biofeedback may give added benefit when used with pelvic floor muscle training. After a prostatectomy there is no clear evidence that teaching pelvic floor exercises alters the risk of urinary incontinence in men.\n\nThe symptoms of prolapse and its severity can be decreased with pelvic floor exercises. Effectiveness can be improved with feedback on how to do the exercises.\n\nThe aim of Kegel exercises is to improve muscle tone by strengthening the pubococcygeus muscles of the pelvic floor. Kegel is a popular prescribed exercise for pregnant women to prepare the pelvic floor for physiological stresses of the later stages of pregnancy and childbirth. Kegel exercises are said to be good for treating vaginal prolapse and preventing uterine prolapse in women and for treating prostate pain and swelling resulting from benign prostatic hyperplasia (BPH) and prostatitis in men. Kegel exercises may be beneficial in treating urinary incontinence in both men and women. Kegel exercises may also increase sexual gratification, allowing women to complete pompoir and aid in reducing premature ejaculation in men. The many actions performed by Kegel muscles include holding in urine and avoiding defecation. Reproducing this type of muscle action can strengthen the Kegel muscles. The action of slowing or stopping the flow of urine may be used as a test of correct pelvic floor exercise technique.\n\nIt is now known that the components of levator ani (the pelvic diaphragm), namely pubococcygeus, puborectalis and ileococcygeus, contract and relax as one muscle. Hence, pelvic floor exercises involve the entire levator ani rather than pubococcygeus alone. Pelvic floor exercises may be of benefit in cases of fecal incontinence and pelvic organ prolapse conditions e.g. rectal prolapse.\n\nSome devices have been marketed to women exercise their pelvic floor muscle and improve the muscle tone of the pubococcygeal or vaginal muscle.\n\nAs of 2013 there was no evidence that doing pelvic floor exercise with weights worked better than doing Kegel exercises without weights; there is greater risk with weights, because a foreign object is introduced into the vagina.\n\nInserting foreign objects into the vagina increases the risk of infections and can lead to vaginosis or toxic shock syndrome. Overuse of such Kegel exercises, with or without a device, can lead to pelvic pain and pain during sex.\n\nDuring the latter part of the 20th century, a number of medical and pseudo-medical devices were marketed to consumers as improving sex or orgasms, increasing \"energy\", \"balancing hormones\", and as having other health or lifestyle benefits. There is no evidence for any of these claims, and many of them are pseudoscience.\n\n\n"}
{"id": "35417634", "url": "https://en.wikipedia.org/wiki?curid=35417634", "title": "Labial thermistor clip", "text": "Labial thermistor clip\n\nThe labial thermistor clip is a device used measure the skin temperature of the labia minora and is associated blood engorgement. This device consists of a thermistor affixed to a small metal clip that can be attached to the labia minora. The labial thermistor clip is the second most commonly used physiological measure of female genital response, next to the vaginal photoplethysmograph (VPG). Both devices can be used simultaneously.\nThe labial thermistor clip has some advantages over VPG, including better test-retest reliability, greater correlation between genital and self-reported sexual arousal, and an absolute unit of change (temperature). Like VPG, the labial thermistor clip has discriminant validity; that is, it detects differences between sexual and nonsexual stimuli. It is also sensitive to different levels of sexual arousal. The labial thermistor clip has some disadvantages because participants have difficulty with placing the device correctly and some report discomfort with using the device.\n\n\n"}
{"id": "31768127", "url": "https://en.wikipedia.org/wiki?curid=31768127", "title": "Mature minor doctrine", "text": "Mature minor doctrine\n\nThe mature minor doctrine is an American term for the statutory, regulatory, or common law policy accepting that an unemancipated minor patient may possess the maturity to choose or reject a particular health care treatment, sometimes without the knowledge or agreement of parents, and should be permitted to do so. It is now generally considered a form of patients rights; formerly, the mature minor rule was largely seen as protecting health care providers from criminal and civil claims by parents of minors at least 15.\n\nJurisdictions may codify an age of medical consent, accept the judgment of licensed providers regarding an individual minor, or accept a formal court decision following a request that a patient be designated a mature minor, or may rely on some combination. For example, patients at least 16 may be assumed to be mature minors for this purpose, patients aged 13 to 15 may be designated so by licensed providers, and pre-teen patients may be so-designated after evaluation by an agency or court. The mature minor doctrine is sometimes connected with enforcing confidentiality of minor patients from their parents.\n\nIn the United States, a typical statute lists: \"Who may consent [or withhold consent for] surgical or medical treatment or procedures.\"\n\nBy definition, a \"mature minor\" has been found to have the capacity to make serious medical decisions alone. By contrast, \"medical emancipation\" formally releases children from some parental involvement requirements but does not necessarily grant that decision making to children themselves. Pursuant to statute, several jurisdictions grant medical emancipation to a minor who has become pregnant or requires sexual-health services, thereby permitting medical treatment without parental consent and, often, confidentiality from parents. A limited guardianship may be appointed to make medical decisions for the medically emancipated minor and the minor may not be permitted to refuse or even choose treatment.\n\nOne early U.S. case of significance, \"Smith v. Seibly\", 72 Wn.2d 16, 431 P.2d 719 (1967), before the Washington Supreme Court, establishes precedent on the mature minor doctrine. The plaintiff, Albert G. Smith, an 18-year-old married father, was suffering from myasthenia gravis, a progressive disease. Because of this, Smith expressed concern that his wife might become burdened in caring for him for their existing child and possibly for additional children. On March 9, 1961, while still 18, Smith requested a vasectomy. His doctor required written consent, which Smith provided, and the surgery was performed. Later, after reaching Washington's statutory age of majority, then 21, the doctor was sued by Smith, who now claimed that he had been a minor and thus unable to grant surgical or medical consent. The Court rejected Smith's argument: \"Thus, age, intelligence, maturity, training, experience, economic independence or lack thereof, general conduct as an adult and freedom from the control of parents are all factors to be considered in such a case [involving consent to surgery].\"\n\nThe court further quoted another recently decided case, \"Grannum v. Berard\", 70 Wn.2d 304, 307, 422 P.2d 812 (1967): \"The mental capacity necessary to consent to a surgical operation is a question of fact to be determined from the circumstances of each individual case.\" The court explicitly stated that a minor may grant surgical consent even without formal emancipation.\n\nEspecially since the 1970s, older pediatric patients sought to make autonomous decisions regarding their own treatment and sometimes sued successfully to do so. The decades of accumulated evidence tended to demonstrate that children are capable of participating in medical decision-making in a meaningful way; and legal and medical communities have demonstrated an increasing willingness of formally affirming decisions made by young people, even regarding life and death.\n\nReligious beliefs have repeatedly influenced a patient's decision to choose treatment or not. In a case in 1989 in Illinois, a 17 year old female Jehovah's Witness was permitted to refuse necessary life saving treatments.\n\nIn 1990, the United States Congress passed the Patient Self-Determination Act; even though key provisions apply only to patients over age 18, the legislation advanced patient involvement in decisionmaking. The West Virginia Supreme Court, in \"Belcher v. Charleston Area Medical Center\", 422 S.E.2d 827, 188 W.Va. 105 (1992), defined a \"mature minor\" exception to parental consent, according consideration to seven factors to be weighed regarding such a minor: age, ability, experience, education, exhibited judgment, conduct, and appreciation of relevant risks and consequences.\n\nIn the United States, bodily integrity has long been considered a common law right; the United States Supreme Court, in 1891's \"Union Pacific Railway Company v. Botsford\", found, \"No right is held more sacred, or is more carefully guarded, by the common law, than the right of every individual to the possession and control of his own person, free from all restraint or interference of others, unless by clear and unquestionable authority of law.\" The Supreme Court in 1990 (\"Cruzan v. Director, Missouri Department of Health\") allowed that \"constitutionally protected liberty interest in refusing unwanted medical treatment may be inferred\" in the \"Due Process Clause\" of the Fourteenth Amendment to the United States Constitution, but the Court refrained from explicitly establishing what would have been a newly enumerated right. Nevertheless, lower courts have increasingly held that competent patients have the right to refuse any treatment for themselves.\n\nIn 1989, the Supreme Court of Illinois interpreted the Supreme Court of the United States to have already adopted major aspects of mature minor doctrine, concluding,\n\nAn ongoing case of Z.M is being heard in Maryland regarding the minor's right to refuse chemotherapy.\n\nIn Connecticut, Cassandra C. a seventeen year old was ordered by the Connecticut Supreme Court to receive treatment. The court decided that Cassandra was not mature enough to make medical decisions.\n\nIn 2009, the Supreme Court of Canada ruling in \"A.C. v. Manitoba\" [2009] SCC 30 (CanLII) found that children \"may\" make life and death decisions about their medical treatment. In the majority opinion, Justice Rosalie Abella wrote:\nA \"dissenting\" opinion by Justice Ian Binnie would have gone further:\nAnalysts note that the Canadian decision merely requires that younger patients be permitted a \"hearing\", and still allows a judge to \"decide whether or not to order a medical procedure on an unwilling minor\".\n"}
{"id": "11059367", "url": "https://en.wikipedia.org/wiki?curid=11059367", "title": "MedInfo", "text": "MedInfo\n\nMedInfo is the name of the international medical informatics conference organized initially every 3 years and now every other year by the International Medical Informatics Association. It is the most important international conference in the field with 3000+ health and medical informatics professions attending from all over the world. MedInfo also serves to bring together all officers of the International Medical Informatics Association (IMIA) Board together with national representatives in the General Assembly of IMIA.\n\nThe General Assembly elects the officers of IMIA. The IMIA Board consists of the President (the Past or the Elect President), Treasurer and Secretary as its officers. In addition it has other Vice Presidents for targeted areas: Membership, MedInfo, Services, Special Affairs, Strategic Plan Implementation, and Working Groups. With the exception of the President and the Vice President of MedInfo all officers serve a three-year term that can be extended for a second three-year term. The President is on a 5-year cycle and the Vice President of MedInfo has one 3-year cycle and elected the year before the next Medinfo meeting so that he/she can be mentored through one MedInfo cycle.\n\nMedInfo was held every 3 years since its inception in 1974, after 2013 it is now held every two years. The table below gives an overview of these conferences.\n\nMedinfo is also an acronym for Medical Information, often a pharmaceutical information service provided by pharmacovigilance or medical affairs departments.\n\n\n\n"}
{"id": "16908870", "url": "https://en.wikipedia.org/wiki?curid=16908870", "title": "Medical Officer for Health", "text": "Medical Officer for Health\n\nMedical Officer of Health, Medical Health Officer or District Medical Officer, is a title and commonly used for the senior government official of a health department or agency, usually at a municipal, county/district, state/province, or regional level. The post is held by a physician who serves to advise and lead a team of public health professionals such as environmental health officers and public health nurses on matters of public health importance.\n\nThe equivalent senior health official at the national level is often referred to as the Chief Medical Officer, although the title varies across countries, for example known as the Surgeon General in the United States and the Chief Public Health Officer in Canada.\n\nIn Canada, all communities are under the jurisdiction of an MOH. The roles of the MOH vary across jurisdictions, but always include responsibilities related to public health and safety, and may include the following functions:\n\n\nHealth officers in India are expected to have prescribed qualifications, such as a Bachelor of Sanitary Science (B.S.Sc.), a degree of an institution recognized by the Medical Council of India or a diploma in Public Health after a 2-year study at the University of Calcutta.\n\nIn the United Kingdom, the municipal position was an elected head of the local board of health, however the term MOH has also been used to refer to the Chief Medical Officer. Under the Metropolis Local Management Act 1855, London municipalities were each required to appoint a medical officer. In 1856, 48 officers took up appointments in the city, and these specialists formed Metropolitan Association of Medical Officers of Health. They were important and influential in the establishment of municipal hospitals under the provisions of the Local Government Act 1929. In the 1974 NHS reorganisation they were replaced by Community Physicians who were attached to the different levels of the NHS.\n\n\nHealth Officer is a common term used in the United States for public health officials, such as medical health officers and environmental health officers. They may serve at the global, federal, state, county, or municipal level.\nThe end of the 20th century and beginning of the 21st saw major issues for health officials and health officers include tobacco control, injury prevention, public health surveillance, disease control, access to health care, health equity, health disparities, cultural competence, access to preventive services such as immunizations and health promotion.\n\n\n"}
{"id": "25734471", "url": "https://en.wikipedia.org/wiki?curid=25734471", "title": "Medical nutrition therapy", "text": "Medical nutrition therapy\n\nMedical nutrition therapy (MNT) is a therapeutic approach to treating medical conditions and their associated symptoms via the use of a specifically tailored diet devised and monitored by a medical doctor physician or registered dietitian nutritionist (RDN). The diet is based upon the patient's medical record, physical examination, functional examination and dietary history.\n\nThe role of MNT when administered by a physician or dietitian nutritionist (RDN) is to reduce the risk of developing complications in pre-existing conditions such as type 2 diabetes as well as ameliorate the effects any existing conditions such as high cholesterol. Many medical conditions either develop or are made worse by an improper or unhealthy diet.\n\nAn example is the use of macronutrient preload in type 2 diabetes.\n\n\"Medical nutrition therapy\" (MNT) is the use of specific nutrition services to treat an illness, injury, or condition. It was introduced in 1994 by the American Dietetic Association to better articulate the nutrition therapy process. It involves the assessment of the nutritional status of the client and the actual treatment, which includes nutrition therapy, counseling, and the use of specialized nutrition supplements. Registered dietitians started using MNT as a dietary intervention for preventing or treating other health conditions that are caused by or made worse by unhealthy eating habits.\n\nOn the other hand, Medical nutrition is a broader term describing nutrition in a medical context.\n\nNormally, individuals obtain the necessary nutrients their bodies require through normal daily diets that process the foods accordingly within the body. Nevertheless, there are circumstances such as disease, distress, stress, and so on that may prevent the body from obtaining sufficient nutrients through diets alone. In such conditions, a dietary supplementation specifically formulated for their individual condition may be required to fill the void created by the specific condition. This can come in form of Medical Nutrition.\n\nIn most cases the use of Medical Nutrition is recommended within international and professional guidelines. It can be an integral part of managing acute and short-term diseases. It can also play a major role in supporting patients for extended periods of time and even for a lifetime in some special cases. Medical Nutrition is not meant to replace the treatment of disease but rather complement the normal use of drug therapies prescribed by physicians and other licensed healthcare providers.\n\nUnlike Medical Foods which are defined by the U.S. Department of Health and Human Services' Food and Drug Administration, {within their 'Medical Foods Guidance Documents & Regulatory Information' guide in section 5(b) of the Orphan Drug Act (21 U.S.C. 30ee (b) (3))}; as “a food which is formulated to be consumed or administered enterally under the supervision of a physician and which is intended for the specific dietary management of a disease or condition for which distinctive nutritional requirements, based on recognized scientific principles, are established by medical evaluation,”\n\nThe following advantages come with medical nutrition:\n\nThe following are some disadvantages of medical nutrition:\n\n\n"}
{"id": "22373489", "url": "https://en.wikipedia.org/wiki?curid=22373489", "title": "Mozart and smallpox", "text": "Mozart and smallpox\n\nIn 1767, the 11-year-old composer Wolfgang Amadeus Mozart was struck by smallpox. Like all smallpox victims, he was at serious risk of dying, but he survived the disease. This article discusses smallpox as it existed in Mozart's time, the decision taken in 1764 by Mozart's father Leopold not to inoculate his children against the disease, the course of Mozart's illness, and the aftermath.\n\nSmallpox in 18th-century Europe was a devastating disease, recurring in frequent epidemics and killing or disfiguring millions of people. The 18th century was probably a particularly terrible time for smallpox in Europe: urbanization had increased crowding, making it easier for the disease to spread; yet effective protection from smallpox through a smallpox vaccine was discovered only at the end of the century (see below).\n\nThe disease was a terrible one for its victims. Ian and Jenifer Glynn write:\nSo what was it like? As children, we were told it was like chickenpox but worse. In fact it is not related to chickenpox, and it was unimaginably worse. In an unvaccinated population, something like 10–30 percent of all patients with smallpox would be expected to die. And dying was not easy; smallpox was, as Macaulay wrote, 'the most terrible of all the ministers of death.'\n\nThose who survived smallpox did not always survive intact; it frequently inflicted blindness on its survivors. The survival rate was particularly low for children.\n\nThe physical appearance of the disease was frightening to patients and to their caretakers: the patient's skin became covered with large, bulging pustules, which often left conspicuous pitting on the skin of patients who survived the disease.\n\nMedicine had made only slight progress against the disease in Mozart's time. Around the second decade of the 18th century the method of inoculation, which had originated in Asia, reached European countries. Inoculation was not the same as the vaccination which later succeeded in eradicating the disease; rather, an inoculated person was treated with live smallpox virus, taken from pustules of the mildest variety of smallpox that could be found.\n\nInoculation offered immunity to smallpox, but the procedure carried a definite risk that the inoculated person could die from smallpox as a result. Thus, many parents felt that they would rather do nothing, risking future smallpox arriving at random, rather than carry out a deliberate act that might well kill their children immediately.\n\nAs Mozart biographer Ruth Halliwell points out, it is in this context that we must interpret a letter sent by Leopold Mozart on 22 February 1764 to his landlord and friend Lorenz Hagenauer concerning smallpox:\nThey are trying to persuade me to let my boy be inoculated with smallpox. But as I have expressed sufficiently clearly my aversion to this impertinence they are leaving me in peace. Here inoculation is the general fashion. But for my part I leave the matter to the grace of God. It depends on His grace whether He wishes to keep this prodigy of nature in this world in which He placed it or to take it to Himself.\n\nFrom the modern perspective—with most children now made safe from several terrible diseases by vaccination—it is easy to make the superficial interpretation that Leopold was acting foolishly, relying on divine will when direct action was available that would have helped his children. However, since in Leopold's day it was not firmly established that inoculation was beneficial, his remarks can be seen to be more of appeal to religion to resolve what must have seemed an impossible dilemma.\n\nThe Mozart family (Wolfgang, his father Leopold, his mother Anna Maria, and his older sister Nannerl) left their home in Salzburg for Vienna on 11 September, 1767. They had been there before, exhibiting the children's talents, in 1762; by this time they had completed their \"Grand Tour\" of Europe, performing in England, France, and elsewhere, and hoped to achieve even greater recognition (and income) in the Imperial capital. The forthcoming marriage of the 16-year-old Archduchess Maria Josepha, daughter of Empress Maria Theresa, scheduled for October 14, promised many festivities and thus opportunities for visiting musicians.\n\nUnfortunately, there was an outbreak of smallpox in Vienna at the time. On 28 May of that year, Emperor Joseph II had lost his second wife Maria Josepha to the disease, and his mother Maria Theresa also caught it (she survived). The imperial bride-to-be Maria Josepha caught the disease in October and died of it on the 15th, the day after she had been scheduled to be married.\n\nThe Mozarts were renting rooms in the home of the goldsmith Johann Schmalecker, and were horrified when all three of Schmalecker's children came down with smallpox. Alarmed, Leopold first left Schmalacker's house, taking Wolfgang (only) with him (17 October). Six days later (23 October), the entire family fled the city.\n\nThey headed north, into what today is the Czech Republic, first reaching Brno (then called by its German name, Brünn), where they called on the Count Franz Anton Schrattenbach, brother of Leopold's employer in Salzburg, the Prince-Archbishop Sigismund von Schrattenbach. Count Schrattenbach invited them to give a concert, but Leopold, impelled by an \"inner urge,\" wanted to go farther, and the family continued northward after two days to Olmütz (today Olomouc). It was there that, on 26 October, Wolfgang showed the first symptoms of smallpox. Given the incubation period of the disease (roughly, 12 days), it can be ascertained that he had already caught it in Vienna.\n\nLeopold consulted an acquaintance, Count Leopold Anton Podstatsky, who was dean of the Cathedral and rector of the University in Olmütz. Leopold had known Podstatsky when the Count had previously worked in Salzburg. The count, learning that Wolfgang was showing symptoms of smallpox, insisted that the Mozarts move into his home, and he placed Mozart under the excellent care of his personal physician, Dr. Joseph Wolff.\n\nLeopold later wrote:\nWolfgang was complaining of his eyes. I noticed his head was warm, that his cheeks were hot and very red, but that his hands were cold as ice. Moreover, his pulse was not right. So we gave him some black powder and put him to bed. During the night he was rather restless and in the morning he still had the dry fever.\n\nA frightening symptom of Wolfgang's illness, not made explicit in Leopold's letter, was an inability to see. In a letter written much later (1800), his sister Nannerl reported:\nHe caught the smallpox, which made him so ill that he could see nothing for nine days and had to spare his eyes for several weeks after his recovery.\n\nAlthough blindness was indeed a common result of smallpox, ophthalmologist Richard H. C. Zegers suggests that Mozart's symptoms did not represent actual blindness, but rather resulted from the pustular rash of the disease affecting his eyelids.\n\nBy 10 November, Wolfgang was feeling better, but then Nannerl also contracted smallpox, and was ill for three weeks. The Mozart children were thereafter safe from the disease, which confers immunity on its survivors. According to Leopold, both children were pitted in the locations of the former pustules, but neither seriously.\n\nDuring his recovery, Wolfgang, who needed to spare his eyes, spent the time learning card tricks and fencing.\n\nWith both children's illness to contend with, the Mozarts spent a total of four months away from Vienna. They eventually returned there and were received in the Imperial court on 19 January, 1768. The Empress, who had by now lost three children to smallpox, conversed with Frau Mozart about the disease.\n\nThe remainder of the trip was not especially successful. Leopold apparently misinterpreted a chance remark of the Emperor as a firm invitation for Wolfgang to compose an opera; this resulted in Wolfgang writing \"La finta semplice\". However, the opera went unperformed in Vienna; the singers and musicians did not like it, and intrigues prevented the work from reaching the stage. \"La finta semplice\" eventually was premiered in Salzburg, following the Mozarts' return there on 5 January 1769.\n\nThe experience of losing three of her children to smallpox led Empress Maria Theresa to become a convert to inoculation. In 1768 she engaged the Dutch physician Jan Ingenhousz to conduct an inoculation program. Ingenhousz's program worked first among poor people, with the goal of developing a weakened strain of the disease; poor parents in Vienna were paid a ducat to have their children inoculated. The inoculations performed with this weakened strain on the imperial family were successful, and led to greater public acceptance for the procedure.\n\nSmallpox struck the Mozart family again in the next generation: Nannerl's eldest son Leopold and two of her stepchildren caught the disease during an outbreak in the Salzburg area in 1787. All three children survived.\n\nIn 1796, the discovery of vaccination—the use of the related cowpox virus to immunize against smallpox—by Edward Jenner revolutionized the ability of medicine to deal with smallpox. Vaccination reached Vienna around 1800, when yet another local epidemic created impetus for its adoption. One of the doctors trained in the Vienna campaign, named Doutrepout, then brought vaccination to Mozart's native city of Salzburg. According to Halliwell, \"popular resistance was fierce,\" and both the government and the Roman Catholic Church (previously an opponent) took stern measures to promote vaccination. The first relative of Mozart's known to have been vaccinated was Johanna Berchtold von Sonnenberg, called \"Jeannette\" (1789–1805), Nannerl's youngest child; she was vaccinated during the 1802 campaign in Salzburg.\n\nWith vaccination, great progress was made in reducing incidence of the disease, and it was eventually confirmed as eradicated in 1979.\n\n"}
{"id": "57724586", "url": "https://en.wikipedia.org/wiki?curid=57724586", "title": "National Data Guardian for Health and Care in England", "text": "National Data Guardian for Health and Care in England\n\nThe National Data Guardian for Health and Care in England is an independent, non-regulatory, advice giving body sponsored by the Department of Health and Social Care. Dame Fiona Caldicott has held the position since its inauguration in November 2014.\n\nThe National Data Guardian provides guidance to the UK Government and the health and adult social care system on data confidentiality, security and patient data choice. Its role is to \"help make sure the public can trust their confidential information is securely safeguarded and make sure that it is used to support citizens’ care and to achieve better outcomes from health and care services\"\n\nAs a non-regulatory body, the National Data Guardian does not issue or enforce sanctions; it works with existing regulators such as the Information Commissioner’s Office and the Care Quality Commission where this is required.\n\nA Private Members' Bill to place the National Data Guardian role on a statutory footing was introduced to the Parliament of the United Kingdom|Parliament]] in 2017. The Health and Social Care (National Data Guardian) Bill 2017-19 is sponsored by Member of Parliament Peter Bone.\n\nA consultation on the roles and functions of the National Data Guardian was held in 2015 in preparation for the bill's drafting. When enacted, it will give the role formal, advice-giving powers on matters related to the processing of health and adult social care data in England\n\nIn September 2015, the Secretary of State for Health Jeremy Hunt asked the National Data Guardian and the Care Quality Commission conduct a formal review into data security and use, delivering as its outcomes: recommendations for new data security standards for health and care; a method for testing compliance against the standards; and a new consent or opt-out model for data sharing in relation to patient confidential data.\n\nThe National Data Guardian's \"Review of Data Security, Consent and Opt-outs\" was published in July 2016. It made 20 recommendations, including the introduction of 10 national data security standards for health and care and a new tool for measuring performance against them.\n\nThe Care Quality Commission published its report \"Safe Data Safe Care\" in tandem.\n\nThe Government's 2017 response, \"'Your Data: Better Security, Better Choice, Better Care\", accepted the recommendations and reported on plans to deliver against them, including the development of a new national data opt-out system for patients, which was launched by NHS Digital in May 2018.\n\nThe review also led to the closure of the care.data programme by NHS England\n\nThe \"Review of Data Security, Consent and Opt-outs\" was the first formal report delivered by Dame Fiona Caldicott in her role as National Data Guardian. It is referred to as Caldicott 3, as it is her third formal report to Government on the protection and use of patient information. The first, her \"Report on the Review of Patient-identifiable Information\" is known as the Caldicott Report and was published in 1997. The second report known as Caldicott 2 was published in 2013.\n"}
{"id": "29146741", "url": "https://en.wikipedia.org/wiki?curid=29146741", "title": "Population Impact Measures", "text": "Population Impact Measures\n\nPopulation Impact Measures (PIMs) are biostatistical measures of risk and benefit used in epidemiological and public health research. They are used to describe the impact of health risks and benefits in a population, to inform health policy.\n\nFrequently used measures of risk and benefit identified by Jerkel, Katz and Elmore, describe measures of risk difference (attributable risk), rate difference (often expressed as the odds ratio or relative risk), Population Attributable Risk (PAR), and the relative risk reduction, which can be recalculated into a measure of \"absolute benefit\", called the Number needed to treat. Population Impact Measures are an extension of these statistics, as they are measures of absolute risk at the population level, which are calculations of number of people in the population who are at risk to be harmed, or who will benefit from Public Health interventions. \n\nThey are measures of absolute risk and benefit, producing numbers of people who will benefit from an intervention or be at risk from a risk factor within a particular local or national population. They provide local context to previous measures, allowing policy-makers to identify and prioritise the potential benefits of interventions on their own population. They are simple to compute, and contain the elements to which policy-makers would have to pay attention in the commissioning or improvement of services. They may have special relevance for local policy-making. They depend on the ability to obtain and use local data, and by being explicit about the data required may have the added benefit of encouraging the collection of such data.\n\nTo describe the impact of preventive and treatment interventions, the Number of Events Prevented in a Population (NEPP) is defined as \"the number of events prevented by the intervention in your population over a defined time period\". NEPP extends the well-known measure Number needed to treat (NNT) beyond the individual patient to the population. To describe the impact of a risk factor on causing ill health and disease the Population Impact Number of Eliminating a Risk factor (PIN-ER-t) is defined as \"the potential number of disease events prevented in a population over the next t years by eliminating a risk factor\". The PIN-ER-t extends the well-known Population Attributable Risk (PAR) to a particular population and relates it to disease incidence, converting the PAR from a measure of relative to absolute risk.\n\nThe components for the calculations are as follows: Population denominator (size of the population); Proportion of the population with the disease; Proportion of the population exposed to the risk factor or the incremental proportion of the diseased population eligible for the proposed intervention (the latter requires the actual or estimated proportion who are currently receiving the interventions ‘subtracted’ from best practice goal from guidelines or targets, adjusted for likely compliance with the intervention); Baseline risk – the probability of the outcome of interest in this or similar populations; and Relative Risk of outcome given exposure to a risk factor or Relative Risk Reduction associated with the intervention.\n\nThe formula is: NEPP=N*Pd*Pe*ru*RRR where: N = population size, Pd = prevalence of the disease, Pe = proportion eligible for treatment, ru = risk of the event of interest in the untreated group or baseline risk over appropriate time period (can be multiplied by life expectancy to produce life-years), RRR = relative risk reduction associated with treatment.\n\nIn order to reflect the incremental effect of changing from current to ‘best’ practice, and to adjust for levels of compliance, the proportion eligible for treatment, Pe, is (Pb-Pt)*Pc, where Pt is the proportion currently treated, Pb is the proportion that would be treated if best practice was adopted, and Pc is the proportion of the population who are compliant with the intervention.\n\nYou can calculate the NEPP and its confidence intervals at this web site from the Population Health Decision Support & Simulation site.\n\n[Note: Number Needed to Treat (NNT): 1/(Baseline risk x Relative Risk Reduction)]\n\nThe formula is: PIN-ER-t = N*Ip*PAR Where: N is the number of people in the population; Ip the baseline risk of the outcome of interest in the population as a whole; t is the time period over which the outcome is measured.\n\nThe PAR/F, Population Attributable Risk (or Fraction), is calculated for two or multiple strata. The basic formula to compute the PAR for dichotomous variables is PAR = Pe*(RR-1)/1+ Pe*(RR-1). Where: Pe is the prevalence of the population within each income stratum as the exposure, and RR is the prevalence of risk factors in each stratum relative to the highest income fifth. This is modified where there are multiple strata to: PAR = [Pe1(RR1-1)+Pe2(RR2-1)+Pe3(RR3-1)…]/[1+Pe1(RR1-1)+Pe2(RR2-1)+ Pe3(RR3-1)...].\nYou can calculate the PIN-ER-t and its confidence intervals at this web site from the Population Health Decision Support & Simulation site.\n"}
{"id": "49509083", "url": "https://en.wikipedia.org/wiki?curid=49509083", "title": "President's Malaria Initiative", "text": "President's Malaria Initiative\n\nThe President's Malaria Initiative (PMI) is a U.S. Government initiative to control and eliminate malaria, one of the leading global causes of premature death and disability. The initiative was originally launched by U.S. president George W. Bush in 2005, and has been continued by each successive U.S. president.\n\nPMI was originally created with a mission to \"reduce malaria-related mortality by 50 percent across 15 high-burden countries in sub-Saharan Africa\". PMI has since expanded to 24 malaria-endemic countries in sub-Saharan Africa and 3 additional countries in the Greater Mekong Subregion of Southeast Asia, where it seeks to further reduce malaria burden and assist countries in achieving malaria elimination. \n\nPMI works closely with national malaria programs and global partners including the World Health Organization, Roll Back Malaria, and Global Fund. Global malaria efforts, led largely part by PMI, have cut malaria mortality by over 60%, saved nearly 7 million lives, and prevented more than 1 billion malaria cases between 2000 and 2015. PMI currently supports malaria prevention and control for over 500 million at-risk people in Africa.\n\nThe \"U.S. Leadership Against HIV/AIDS, Tuberculosis, and Malaria Act of 2003\" originally authorized the U.S. Government to provide 5 years of malaria funding to bilateral partners and the Global Fund. PMI was subsequently launched by President George W. Bush in 2005. In 2008, PMI was reauthorized for another 5 years of funding by the \"Lantos-Hyde Act,\" which also called for development of a comprehensive U.S. Global Malaria Strategy, the latest version of which is the U.S. Global Malaria Strategy 2015-2020. PMI served as a major component of the Global Health Initiative, a six-year, $63-billion effort proposed by President Obama in May 2009.\n\nThe US Government, including through PMI, is currently the largest international source of financing for malaria. PMI's global budget for FY2017 was $723 million.\nPMI is interagency initiative overseen by the U.S. Global Malaria Coordinator in consultation with an Interagency Advisory Group composed of representatives from USAID, CDC, the Department of State, the Department of Defense, the National Security Council, and Office of Management and Budget. The initiative is led by USAID and implemented together with CDC. In addition to US-based staff at USAID and CDC headquarters, PMI maintains resident advisors from both agencies in each focus country.\n\nPMI currently provides direct support to 24 \"focus\" countries and 3 additional country programs in the Great Mekong Subregion. At the time of its launch in 2005, PMI provided support to just three countries: Angola, Tanzania, and Uganda. Four additional countries (Malawi, Mozambique, Rwanda, and Senegal) were added the following year. By 2007, PMI had added Benin, Ethiopia, Ghana, Kenya, Liberia, Madagascar, Mali, and Zambia—bringing the total number of focus countries to the originally envisioned total of 15 high-burden nations. \n\nIn 2010, PMI added the Democratic Republic of the Congo, Nigeria, Guinea, Zimbabwe, and the Mekong Subregion. \n\nIn 2017, with additional funding from Congress, PMI expanded to 5 more countries: Burkina Faso, Cameroon, Cote d'Ivoire, Niger, and Sierra Leone.\n\nPMI is estimated to have prevented 185 million malaria cases and nearly 1 million deaths between 2005 and 2017. Globally, malaria mortality fell by more than 60% between 2000 and 2015. The presence of a PMI program in a country has also been associated with a significant reduction in all-cause under-5 child mortality.\n\n\n"}
{"id": "19454783", "url": "https://en.wikipedia.org/wiki?curid=19454783", "title": "Prevention through design", "text": "Prevention through design\n\nPrevention through design (PtD), also called safety by design usually in Europe, is the concept of applying methods to minimize occupational hazards early in the design process, with an emphasis on optimizing employee health and safety throughout the life cycle of materials and processes. It is a concept and movement that encourages construction or product designers to \"design out\" health and safety risks during design development. The concept supports the view that along with quality, programme and cost; safety is determined during the design stage. It increases the cost-effectiveness of enhancements to occupational safety and health.\n\nThis method for reducing workplace safety risks lessens workers' reliance on personal protective equipment, which is the least effective of the hierarchy of hazard control.\n\nEach year in the U.S., 55,000 people die from work-related injuries and diseases, 294,000 are made sick, and 3.8 million are injured. The annual direct and indirect costs have been estimated to range from $128 billion to $155 billion. Recent studies in Australia indicate that design is a significant contributor in 37% of work-related fatalities; therefore, the successful implementation of prevention through design concepts can have substantial impacts on worker health and safety.\n\nThe National Institute for Occupational Safety and Health (NIOSH) in the United States is a major contributor and promoter of PtD policy and guidelines. NIOSH considers PtD to be \"the most effective and reliable type\" of prevention of occupational injuries. A core tenet of PtD philosophy the concept of addressing workplace hazards using methods at the top of the Hierarch of Controls, namely elimination and substitution.\n\nWithin Europe, construction designers are legally bound to design out risks during design development to reduce hazards in the construction and end use phases via the Mobile Worksite Directive (also known as CDM regulations in the UK). The concept supports this legal requirement. Some Notified Bodies provide testing and design verification services to ensure compliance with the safety standards defined in regulation codes such as the American Society of Mechanical Engineers. Many non-governmental organizations have been established to support this aim, principally in the UK, Australia and the United States.\n\nWhile engineering as a rule factors human safety into the design process, a modern appraisal of specific links to design and workers' safety can be seen in efforts beginning in the 1800s. Trends included the widespread implementation of guards for machinery, controls for elevators, and boiler safety practices. This was followed by enhanced design for ventilation, enclosures, system monitors, lockout/tagout controls, and hearing protectors. More recently, there has been the development of chemical process safety, ergonomically engineered tools, chairs, and work stations, lifting devices, retractable needles, latex-free gloves, and a parade of other safety devices and processes.\n\nIn 2007, the National Institute for Occupational Health and Safety began its National Initiative on Prevention through Design with the goal of promoting prevention through design philosophy, practice, and policy.\n\nPrevention through design represents a shift in approach for on-the-job safety. It involves evaluating potential risks associated with processes, structures, equipment, and tools. It takes into consideration the construction, maintenance, decommissioning, and disposal or recycling of waste material.\nThe idea of redesigning job tasks and work environments has begun to gain momentum in business and government as a cost-effective means to enhance occupational safety and health. Many U.S. companies openly support PtD concepts and have developed management practices to implement them. Other countries are actively promoting PtD concepts as well. The United Kingdom began requiring construction companies, project owners, and architects to address safety and health during the design phase of projects in 1994. Australia developed the Australian National OHS Strategy 2002–2012, which set \"eliminating hazards at the design stage\" as one of five national priorities. As a result, the Australian Safety and Compensation Council (ASCC) developed the Safe Design National Strategy and Action Plans for Australia encompassing a wide range of design areas.\n\nThe National Institute for Occupational Safety and Health is a large contributor to prevention through design efforts in the United States. Several NIOSH initiatives and guidelines directly or indirectly advocate for PtD practices. Through NIOSH efforts, the U.S. Green Building Council posted new PtD credits available for Leadership in Energy and Environmental Design (LEED) certification for construction. Additionally, they provide a wide variety of educational and guidance materials on the topic of PtD The NIOSH \"Buy Quiet\" initiative uses elements of prevention through design to encourage companies to buy quieter machinery, thereby reducing occupational hearing loss for their workers.\n\n\n"}
{"id": "8543665", "url": "https://en.wikipedia.org/wiki?curid=8543665", "title": "Pseudonymization", "text": "Pseudonymization\n\nPseudonymization is a data management and de-identification procedure by which personally identifiable information fields within a data record are replaced by one or more artificial identifiers, or pseudonyms. A single pseudonym for each replaced field or collection of replaced fields makes the data record less identifiable while remaining suitable for data analysis and data processing. \n\nPseudonymization can be one way to comply with the European Union's new General Data Protection Regulation demands for secure data storage of personal information. Pseudonymized data can be restored to its original state with the addition of information which then allows individuals to be re-identified, while anonymized data can never be restored to its original state.\n\nThe choice of which data fields are to be pseudonymized is partly subjective. Less selective fields, such as Birth Date or Postal Code are often also included because they are usually available from other sources and therefore make a record easier to identify. Pseudonymizing these less identifying fields removes most of their analytic value and is therefore normally accompanied by the introduction of new derived and less identifying forms, such as year of birth or a larger postal code region.\n\nData fields that are less identifying, such as date of attendance, are usually not pseudonymized. It is important to realize that this is because too much statistical utility is lost in doing so, not because the data cannot be identified. For example, given prior knowledge of a few attendance dates it is easy to identify someone's data in a pseudonymized dataset by selecting only those people with that pattern of dates. This is an example of an inference attack.\n\nThe weakness of pseudonymized data to inference attacks is commonly overlooked. A famous example is the AOL search data scandal.\n\nProtecting statistically useful pseudonymized data from re-identification requires:\n\nThe pseudonym allows tracking back of data to its origins, which distinguishes pseudonymization from anonymization, where all person-related data that could allow backtracking has been purged. Pseudonymization is an issue in, for example, patient-related data that has to be passed on securely between clinical centers.\n\nThe application of pseudonymization to e-health intends to preserve the patient's privacy and data confidentiality. It allows primary use of medical records by authorized health care providers and privacy preserving secondary use by researchers. However, plain pseudonymization for privacy preservation often reaches its limits when genetic data are involved (see also genetic privacy). Due to the identifying nature of genetic data, depersonalization is often not sufficient to hide the corresponding person. Potential solutions are the combination of pseudonymization with fragmentation and encryption.\n\nAn example of application of pseudonymization procedure is creation of datasets for de-identification research by replacing identifying words with words from the same category (e.g. replacing a name with a random name from the names dictionary), however, in this case it is in general not possible to track data back to its origins.\n\n"}
{"id": "2859527", "url": "https://en.wikipedia.org/wiki?curid=2859527", "title": "Pulsus paradoxus", "text": "Pulsus paradoxus\n\nPulsus paradoxus, also paradoxic pulse or paradoxical pulse, is an abnormally large decrease in stroke volume, systolic blood pressure and pulse wave amplitude during inspiration. The normal fall in pressure is less than 10 mmHg. When the drop is more than 10 mmHg, it is referred to as pulsus paradoxus. Pulsus paradoxus is not related to pulse rate or heart rate and it is not a paradoxical rise in systolic pressure. The normal variation of blood pressure during breathing/respiration is a decline in blood pressure during inhalation and an increase during exhalation. Pulsus paradoxus is a sign that is indicative of several conditions, including cardiac tamponade, chronic sleep apnea, croup, and obstructive lung disease (e.g. asthma, COPD).\n\nThe \"paradox\" in \"pulsus paradoxus\" is that, on physical examination, one can detect beats on cardiac auscultation during inspiration that cannot be palpated at the radial pulse. It results from an accentuated decrease of the blood pressure, which leads to the (radial) pulse not being palpable and may be accompanied by an increase in the jugular venous pressure height (Kussmaul's sign). As is usual with inspiration, the heart rate is slightly increased, due to decreased left ventricular output.\n\nDuring inspiration, the negative intra-thoracic pressure results in an increased right venous return, filling the right atrium more than during an exhalation. The increased blood volume dilates the right atrium, reducing the compliance of the left atrium due to their shared septum. Lower left atrial compliance reduces the left atrium venous return and as a consequence causes a reduction in left ventricular preload. This results in a reduction in left ventricular stroke volume, and will be noted as a reduction in systolic blood pressure in inspiration.\nPulses paradoxus is therefore an exaggeration or an increase in the fall of systolic BP beyond 10 mmHG during inspiration.\n\nNormally during inspiration, a person's systolic blood pressure decreases by ≤10 mmHg and pulse slightly increases. This is because inspiration decreases intra-thoracic pressure relative to atmospheric pressure, which increases blood flow (systemic venous return) to the right atrium of the heart by reducing pressure on the veins, particularly the venae cavae. However, the decrease in intra-thoracic pressure and stretching of the lungs during inhalation also expands the compliant pulmonary vasculature so that blood pools in the lungs and decreases pulmonary venous return to the left atrium. Also, the increased systemic venous return to the right side of the heart expands the right heart and directly compromises filling of the left side of the heart by slightly bulging the septum to the left, reducing maximum volume. Reduced left-heart filling leads to a reduced stroke volume which manifests as a decrease in systolic blood pressure, leading to a faster heart rate due to the baroreceptor reflex, which stimulates sympathetic outflow to the heart.\n\nUnder normal physiologic conditions the large pressure gradient between the right and left ventricles prevents the septum from bulging dramatically into the left ventricle during inspiration. However such bulging does occur during cardiac tamponade where pressure equalizes between all of the chambers of the heart. Following a zero-sum game principle, as the right ventricle receives more volume it can push the septum into the left ventricle further reducing its volume in turn. This additional loss of volume of the left ventricle that \"only\" occurs with equalization of the pressures (as in tamponade) allows for the further reduction in volume, so cardiac output is reduced, leading to a further decline in BP. However, in situations where the left ventricular pressure remains higher than the pericardial sac (most frequently from coexisting disease with an elevated left ventricular diastolic pressure), there is no pulsus paradoxus.\n\nAlthough one or both of these mechanisms may occur, a third may additionally contribute. The large negative intra-thoracic pressure increases the pressure across the wall of the left ventricle (increased transmural pressure, equivalent to [pressure within ventricle] - [pressure outside of ventricle]). This pressure gradient, resisting the contraction of the left ventricle, causes an increase in afterload. This results in a decrease in stroke volume, contributing to the decreased pulse pressure and increased heart rate as described above.\n\nPulsus paradoxus occurs not only with severe cardiac tamponade, but also with asthma, obstructive sleep apnea and croup. The mechanism, at least with severe tamponade, is likely very similar to those of hypertrophic and restrictive cardiomyopathies (diastolic dysfunction), where a decrease in Left Ventricular (LV) filling corresponds to an increasingly reduced stroke volume. In other words, with these cardiomyopathies, as LV filling decreases, ejection fraction decreases directly, yet non-linearly and with a negative concavity (negative first and second derivatives). Similarly with tamponade, the degree of diastolic dysfunction is inversely proportional to the LV end-diastolic volume. So during inspiration, since LV filling is lesser relative to that during expiration, the diastolic dysfunction is also proportionally greater, so the systolic pressure drops >10 mmHg. This mechanism is also likely with pericarditis, where diastolic function is chastened.\n\nPP is quantified using a blood pressure cuff and stethoscope (Korotkoff sounds), by measuring the variation of the systolic pressure during expiration and inspiration. Inflate cuff until no sounds (as is normally done when taking a BP) slowly decrease cuff pressure until systolic sounds are first heard during \"expiration\" but not during inspiration, (note this reading), slowly continue decreasing the cuff pressure until sounds are heard \"throughout\" the respiratory cycle, (inspiration and expiration)(note this second reading). If the pressure difference between the two readings is >10mmHg, it can be classified as pulsus paradoxus.\n\nPulsus paradoxus can be caused by several physiologic mechanisms. Anatomically, these can be grouped into:\n\nConsidered physiologically, PP is caused by:\n\nCardiac:\n\nPulmonary:\n\nNon-pulmonary and non-cardiac:\n\nPP has been shown to be predictive of the severity of cardiac tamponade. Pulsus paradoxus may not be seen with cardiac tamponade if an atrial septal defect or significant aortic regurgitation is also present.\n\n\n"}
{"id": "55151676", "url": "https://en.wikipedia.org/wiki?curid=55151676", "title": "Pure, White and Deadly", "text": "Pure, White and Deadly\n\nPure, White, and Deadly is a book written by John Yudkin in 1972. It summarised the research that Yudkin had been conduction at Queen Elizabeth College where he was professor of Nutrition. It set out the physiological impact of sugar on the western diet leading to increased rates of obesity and heart disease. This was at a time when consumption of fat was regarded as the key causative agent of the increases in obesity.\n"}
{"id": "2815180", "url": "https://en.wikipedia.org/wiki?curid=2815180", "title": "Sexual health clinic", "text": "Sexual health clinic\n\nSexual health clinics specialize in the prevention and treatment of sexually transmitted infections.\n\nSexual health clinics are also called \"sexually transmitted disease (STD) clinics\", \"sexually transmitted infection (STI) clinics\", \"venereal disease (VD) clinics\", or \"genitourinary medicine (GUM) clinics\".\n\nSexual health clinics differ from reproductive health and family planning clinics. Sexual health clinics offer only some reproductive health services. Reproductive health clinics, such as Planned Parenthood, offer most of the services of sexual health clinics.\n\nSexual health clinics provide some or all of the following:\n\n\nMany clinics provide vaccinations to prevent infections from the hepatitis A and B viruses.\nYoung women may receive vaccinations to prevent infection from some strains of the human papillomavirus (HPV).\n\nMany clinics provide interpreting for the hearing impaired or speakers of other languages.\n\nMany clinics will help patients tell their sexual contacts if they have a sexually transmitted infection, anonymously if needed.\n\nPublic governmental and non-profit clinics often provide services for free or adjust the fee based on a patient's ability to pay.\n\nSexual health clinics often offer services without appointments. Some clinics open evenings or weekends.\n\nSome clinics have separate hours or facilities for men and women. Some clinics serve only specific populations such as women, men, MSM, youths, LGBT, ethnic groups, the poor, or students.\n\nWith the patient's consent, a clinician will inspect the patient visually and by touch. If needed, the clinician will take samples to test for sexually transmitted infections.\n\nIn a private room or space, the patient will partially undress.\n\nThe clinician may inspect the patient's:\n\nThe clinician may swab the patient's:\n\nThe clinician may take small blood samples by pricking a finger or from a vein\nto test for HIV, syphilis, and possibly herpes\nand hepatitis C.\n\nThe clinician may ask for a small urine sample, given in private, to test for chlamydia and possibly gonorrhea.\n\nThe inspections and taking samples don't hurt, but swabbing the urethra and cervix, and a finger prick blood sample feel uncomfortable.\n\nWomen will often receive a pelvic exam, both external and internal, but usually less thorough than a reproductive health exam.\n\nA patient can choose a female or male clinician if available.\nA patient can have a chaperone.\nSome clinics have separate hours or facilities for men and women.\n\nMedical confidentiality is an important part of the medical ethics of a doctor–patient relationship. Sexual health clinics follow local standards of medical confidentiality to protect the privacy of patients. Some clinics provide anonymous services or protect confidentiality by having a patient use a number or a pseudonym.\n\nAdditional privacy protections sometimes apply to matters of sexuality and reproduction, since these areas are sensitive in many cultures. The diagnosis of HIV/AIDS has legal restrictions in patient confidentiality, and some clinics use rapid antibody tests to provide results to a patient within 30 minutes, without holding the patient's records.\n\nIn the United States, clinics receiving federal funding from Medicaid or Title X of the Public Health Service Act must treat all patients confidentially. Thus minors can receive services without parental notification or consent. Additionally, medical records for all patients age 18 and above are strictly confidential under HIPAA.\n\nMedical standards of informed consent apply to sexual health clinics. A patient needs information about the purposes and consequences of examinations, tests, treatments, and other procedures. A patient may then choose whether to consent to these procedures.\n\nA minor may consent to receive some or all of the procedures at many sexual health clinics.\n\n \n\n\n"}
{"id": "52905034", "url": "https://en.wikipedia.org/wiki?curid=52905034", "title": "Social Support Questionnaire", "text": "Social Support Questionnaire\n\nThe Social Support Questionnaire (SSQ) is a quantitative, psychometrically sound survey questionnaire intended to measure social support and satisfaction with said social support from the perspective of the interviewee. Degree of social support has been shown to influence the onset and course of certain psychiatric disorders such as clinical depression or schizophrenia. The SSQ was approved for public release in 1981 by Irwin Sarason, Henry Levine, Robert Basham and Barbara Sarason under the University of Washington Department of Psychology and consists of 27 questions. Overall, the SSQ has good test-retest reliability and convergent internal construct validity.\n\nThe questionnaire is designed so that each question has a two-part answer. The first part asks the interviewee to list up to nine people available to provide support that meet the criteria stated in the question. These support individuals are specified using their initials in addition to the relationship to the interviewee. Example questions from the first part includes questions such as “Whom could you count on to help if you had just been fired from your job or expelled from school?” and “Whom do you feel would help if a family member very close to you died?”.\n\nThe second part asks the interviewee to specify how satisfied they are with each of the people stated in the first part. The SSQ respondents use a 6 -point Likert scale to indicate their degree of satisfaction with the support from the above people ranging from “1 - very dissatisfied” to “6 - very satisfied”.\n\nThe Social Support Questionnaire has multiple short forms such as the SSQ3 and the SSQ6.\n\nThe SSQ is based on 4 original studies. The first study set out to determine whether the SSQ had the desired psychometric properties. The second study tried to relate SSQ and a diversity of personality measures such as anxiety, depression and hostility in connection with the Multiple Affect Adjective Checklist. The third study considered the relationship between social support, the prior year’s negative and positive life events, internal-external locus of control and self- esteem in conjunction with the Life Experiences Survey. The fourth study tested the idea that social support could serve as a buffer when faced with difficult life situations via trying to solve a maze and subsequently completing the Cognitive Interference Questionnaire.\n\nThe overall support score (SSQN) is calculated by taking an average of the individual scores across the 27 items. A high score on the SSQ indicates more optimism about life than a low score. Respondents with low SSQ scores have a higher prevalence of negative life events and illness. Scoring is as follows:\n\n1. Add the total number of people for all 27 items (questions). (Max. is 243). Divide by 27 for average item score. This gives you SSQ Number Score, or SSQN.\n\n2. Add the total satisfaction scores for all 27 items (questions). (Max is 162). Divide by 27 for average item score. This gives you SSQ Satisfaction score or SSQS.\n\n3. Finally, you can average the above for the total number of people that are family members - this results in the SSQ family score.\n\nAccording to Sarason, the SSQ takes between fifteen to eighteen minutes to properly administer and has “good” test-retest reliability.\n\nThe SSQ was compared with the depression scale and validity tests show significant negative correlation ranging from -0.22 to -0.43. The SSQ and the optimism scale have a correlation of 0.57. The SSQ and the satisfaction score have a correlation of 0.34. The SSQ has high internal consistency among items.\n\nThe SSQ has been used to show a positive correlation and dependence between Post Traumatic Stress Disorder and Social Support in a study of adolescents and long-term outcomes in Gonaives, Haiti. The study looked at the traumas stemming from the natural disasters of 2004, 2008 and 2010. The SSQ has also been used to show that higher levels of social support correlated with less suicide ideation in Military Medical University Soldiers in Iran in 2015. A low level of social support is an important risk factor in women for dysmenorrhea or menstrual cramps. Low Social Support is the strongest predictor of dysmenorrhea when compared to affect, personality and alexithymia.\n\nThe SSQ3 is a short form of the SSQ and has only three questions. The SSQ3 has acceptable test-test reliability and correlation with personality variables as compared to the long form of the Social Support Questionnaire. The internal reliability was borderline but this low level of internal reliability is as expected since there are only three questions.\n\nThe SSQ6 is a short form of the SSQ. The SSQ6 has been shown to have high correlation with: the SSQ, SSQ personality variables and internal reliability. In the development of the SSQ6, the research suggests that professed social support in adults may be a connected to “early attachment experience.” The SSQ6 consists of the below 6 questions:\n\n1. Whom can you really count on to be dependable when you need help?\n\n2. Whom can you really count on to help you feel more relaxed when you are under pressure or tense?\n\n3. Who accepts you totally, including both your worst and your best points?\n\n4. Whom can you really count on to care about you, regardless of what is happening to you?\n\n5. Whom can you really count on to help you feel better when you are feeling generally down-in-the-dumps?\n\n6. Whom can you count on to console you when you are very upset?\n\nThe Interpersonal Support Evaluation List includes 40 items (questions) with four sub-scales in the areas of Tangible Support, Belonging Support, Self-Esteem Support and Appraisal Support. The interviewee rates each item based on how true or false they feel the item is for themselves. The four total response options are “Definitely True”, “Probably True”, “Probably False”, and “Definitely False”.\n\n"}
{"id": "59111", "url": "https://en.wikipedia.org/wiki?curid=59111", "title": "Socialized medicine", "text": "Socialized medicine\n\nSocialized medicine is a term used in the United States to describe and discuss systems of universal health care: medical and hospital care for all at a nominal cost by means of government regulation of health care and subsidies derived from taxation. Because of historically negative associations with socialism in American culture, the term is usually used pejoratively in American political discourse. The term was first widely used in the United States by advocates of the American Medical Association in opposition to President Harry S. Truman's 1947 health-care initiative.\n\nThe original meaning was confined to systems in which the government operates health care facilities and employs health care professionals. This narrower usage would apply to the British National Health Service hospital trusts and health systems that operate in other countries as diverse as Finland, Spain, Israel, and Cuba. The United States Veterans Health Administration and the medical departments of the U.S. Army, Navy, and Air Force, would also fall under this narrow definition. When used in that way, the narrow definition permits a clear distinction from single payer health insurance systems, in which the government finances health care but is not involved in care delivery.\n\nMore recently, American conservative critics of health care reform have attempted to broaden the term by applying it to any publicly funded system. Canada's Medicare system and most of the UK's NHS general practitioner and dental services, which are systems where health care is delivered by private business with partial or total government funding, fit the broader definition, as do the health care systems of most of Western Europe. In the United States, Medicare, Medicaid, and the US military's TRICARE fall under that definition. In specific regard to military benefits of a (currently) volunteer military, such care is an owed benefit to a specific group as part of an economic exchange, which muddies the definition yet further.\n\nMost industrialized countries and many developing countries operate some form of publicly funded health care with universal coverage as the goal. According to the Institute of Medicine and others, the United States is the only wealthy, industrialized nation that does not provide universal health care.\n\nJonathan Oberlander, a professor of health policy at the University of North Carolina, maintains that the term is merely a political pejorative that has been defined to mean different levels of government involvement in health care, depending on what the speaker was arguing against at the time.\n\nThe term is often used by conservatives in the U.S. to imply that the privately run health care system would become controlled by the government, thereby associating it with socialism, which has negative connotations to some people in American political culture. As such, its usage is controversial, and at odds with the views of conservatives in other countries prepared to defend socialized medicine such as Margaret Thatcher.\n\nWhen the term \"socialized medicine\" first appeared in the United States in the early 1900s, it bore no negative connotations. Otto P. Geier, chairman of the Preventive Medicine Section of the American Medical Association, was quoted in \"The New York Times\" in 1917 as praising socialized medicine as a way to \"discover disease in its incipiency,\" help end \"venereal diseases, alcoholism, tuberculosis,\" and \"make a fundamental contribution to social welfare.\" However, by the 1930s, the term socialized medicine was routinely used negatively by conservative opponents of publicly funded health care who wished to imply it represented socialism, and by extension, communism. Universal health care and national health insurance were first proposed by U.S. President Theodore Roosevelt. President Franklin D. Roosevelt later championed it, as did Harry S. Truman as part of his Fair Deal and many others. Truman announced before describing his proposal that: \"This is not socialized medicine\".\n\nGovernment involvement in health care was ardently opposed by the AMA, which distributed posters to doctors with slogans such as \"Socialized medicine ... will undermine the democratic form of government.\" According to T.R. Reid (\"The Healing of America\", 2009): The term [\"socialized medicine\"] was popularized by the public relations firm Whitaker and Baxter working for the American Medical Association in 1947 to disparage President Truman's proposal for a national health care system. It was a label, at the dawn of the cold war, meant to suggest that anybody advocating universal access to health care must be a communist. And the phrase has retained its political power for six decades.\n\nThe AMA conducted a nationwide campaign called Operation Coffee Cup during the late 1950s and early 1960s in opposition to the Democrats' plans to extend Social Security to include health insurance for the elderly, later known as Medicare. As part of the plan, doctors' wives would organize coffee meetings in an attempt to convince acquaintances to write letters to Congress opposing the program. In 1961, Ronald Reagan recorded a disc entitled \"Ronald Reagan Speaks Out Against Socialized Medicine\" warning its audience the \"dangers\" that socialized medicine could bring. The recording was widely played at Operation Coffee Cup meetings. Other pressure groups began to extend the definition from state managed health care to any form of state finance in health care. President Dwight Eisenhower opposed plans to expand government role in healthcare during his time in office.\n\nIn more recent times, the term was brought up again by Republicans in the 2008 U.S. presidential election. In July 2007, one month after the release of Michael Moore's film \"Sicko\", Rudy Giuliani, the front-runner for the 2008 Republican presidential nomination, attacked the health care plans of Democratic presidential candidates as socialized medicine that was European and socialist, Giuliani claimed that he had a better chance of surviving prostate cancer in the US than he would have had in England and went on to repeat the claim in campaign speeches for three months before making them in a radio advertisement. After the radio ad began running, the use of the statistic was widely criticized by FactCheck.org, PolitiFact.com, by \"The Washington Post\", and others who consulted leading cancer experts and found that Giuliani's cancer survival statistics to be false, misleading or \"flat wrong,\" the numbers having been reported to have been obtained from an opinion article by Giuliani health care advisor David Gratzer, a Canadian psychiatrist in the Manhattan Institute's \"City Journal\" where Gratzer was a senior fellow. \"The Times\" reported that the British Health Secretary pleaded with Giuliani to stop using the NHS as a political football in American presidential politics. The article reported that not only the figures were five years out of date and wrong but also that US health experts disputed both the accuracy of Giuliani's figures and questioned whether it was fair to make a direct comparison. The \"St. Petersburg Times\" said that Giuliani's tactic of \"injecting a little fear\" exploited cancer, which was \"apparently not beneath a survivor with presidential aspirations.\" Giuliani's repetition of the error even after it had been pointed out to him earned him more criticism and was awarded four \"Pinnochios\" by the \"Washington Post\" for recidivism.\n\nHealth care professionals have tended to avoid the term because of its pejorative nature, but if they use it, they do not include publicly funded private medical schemes such as Medicaid. Opponents of state involvement in health care tend to use the looser definition.\n\nThe term is widely used by the American media and pressure groups. Some have even stretched use of the term to cover any regulation of health care, publicly financed or not. The term is often used to criticize publicly provided health care outside the US, but rarely to describe similar health care programs there, such as the Veterans Administration clinics and hospitals, military health care, or the single payer programs such as Medicaid and Medicare. Many conservatives use the term to evoke negative sentiment toward health care reform that would involve increasing government involvement in the US health care system.\n\nMedical staff, academics and most professionals in the field and international bodies such as the World Health Organization tend to avoid use of the term. Outside the US, the terms most commonly used are universal health care or public health care. According to health economist Uwe Reinhardt, \"strictly speaking, the term 'socialized medicine' should be reserved for health systems in which the government operates the production of health care and provides its financing.\" Still others say the term has no meaning at all.\n\nIn more recent times, the term has gained a more positive reappraisal. Documentary movie maker Michael Moore in his documentary \"Sicko\" pointed out that Americans do not talk about public libraries or the police or the fire department as being \"socialized\" and do not have negative opinions of these. Media personalities such as Oprah Winfrey have also weighed in behind the concept of public involvement in healthcare. A 2008 poll indicates that Americans are sharply divided when asked about their views of the expression \"socialized medicine\", with a large percentage of Democrats holding favorable views, while a large percentage of Republicans holding unfavorable views. Independents tend to somewhat favor it.\n\nThe Veterans Health Administration, the Military Health System, and the Indian Health Service are examples of socialized medicine in the stricter sense of government administered care, but they are for limited populations.\n\nMedicare and Medicaid are forms of publicly funded health care, which fits the looser definition of socialized medicine. Part B coverage (Medical) requires a monthly premium of $96.40 (and possibly higher) and the first $135 of costs per year also fall to the senior, not the government.\n\nA poll released in February 2008, conducted by the Harvard School of Public Health and Harris Interactive, indicated that Americans are currently divided in their opinions of socialized medicine, and this split correlates strongly with their political party affiliation.\n\nTwo thirds of those polled said they understood the term \"socialized medicine\" very well or somewhat well. When offered descriptions of what such a system could mean, strong majorities believed that it means \"the government makes sure everyone has health insurance\" (79%) and \"the government pays most of the cost of health care\" (73%). One third (32%) felt that socialized medicine is a system in which \"the government tells doctors what to do.\" The poll showed \"striking differences\" by party affiliation. Among Republicans polled, 70% said that socialized medicine would be worse than the current system. The same percentage of Democrats (70%) said that a socialized medical system would be better than the current system. Independents were more evenly split, with 43% saying socialized medicine would be better and 38% worse.\n\nAccording to Robert J. Blendon, professor of health policy and political analysis at the Harvard School of Public Health, \"The phrase 'socialized medicine' really resonates as a pejorative with Republicans. However, that so many Democrats believe that socialized medicine would be an improvement is an indication of their dissatisfaction with our current system.\" Physicians' opinions have become more favorable toward \"socialized medicine.\"\n\nA 2008 survey of doctors, published in \"Annals of Internal Medicine\", shows that physicians support universal health care and national health insurance by almost 2 to 1.\n\nAlthough the marginal scope of free or subsidized medicine provided is much discussed within the political body in most countries with socialized health care systems, there is little or no evidence of strong public pressure for the removal of subsidies or the privatization of health care in those countries. The political distaste for government involvement in health care in the U.S. is a unique counter to the trend found in other developed countries.\n\nIn the United States, neither of the main parties favors a socialized system that puts the government in charge of hospitals or doctors, but they do have different approaches to financing and access. Democrats tend to be favorably inclined towards reform that involves more government control over health care financing and citizens' right of access to health care. Republicans are broadly in favor of the status quo, or a reform of the financing system that gives more power to the citizen, often through tax credits.\n\nSupporters of government involvement in health care argue that government involvement ensures access, quality, and addresses market failures specific to the health care markets. When the government covers the cost of health care, there is no need for individuals or their employers to pay for private insurance.\n\nOpponents also claim that the absence of a market mechanism may slow innovation in treatment and research.\n\nBoth sides have also looked to more philosophical arguments, debating whether people have a fundamental right to have health care provided to them by their government.\n\nSocialized medicine amongst industrialized countries tends to be more affordable than in systems where there is little government involvement. A 2003 study examined costs and outputs in the U.S. and other industrialized countries and broadly concluded that the U.S. spends so much because its health care system is more costly. It noted that \"the United States spent considerably more on health care than any other country ... [yet] most measures of aggregate utilization such as physician visits per capita and hospital days per capita were below the OECD median. Since spending is a product of both the goods and services used and their prices, this implies that much higher prices are paid in the United States than in other countries. The researchers examined possible reasons and concluded that input costs were high (salaries, cost of pharmaceutical), and that the complex payment system in the U.S. added higher administrative costs. Comparison countries in Canada and Europe were much more willing to exert monopsony power to drive down prices, whilst the highly fragmented buy side of the U.S. health system was one factor that could explain the relatively high prices in the United States of America. The current fee-for-service payment system also stimulates expensive care by promoting procedures over visits through financially rewarding the former ($1,500 – for doing a 10-minute procedure) vs. the latter ($50 – for a 30–45 minute visit). This causes the proliferation of specialists (more expensive care) and creating, what Don Berwick refers to as, \"the world's best healthcare system for rescue care\".\n\nOther studies have found no consistent and systematic relationship between the type of financing of health care and cost containment; the efficiency of operation of the health care system itself appears to depend much more on how providers are paid and how the delivery of care is organized than on the method used to raise these funds.\n\nSome supporters argue that government involvement in health care would reduce costs not just because of the exercise of monopsony power, e.g. in drug purchasing, but also because it eliminates profit margins and administrative overhead associated with private insurance, and because it can make use of economies of scale in administration. In certain circumstances, a volume purchaser may be able to guarantee sufficient volume to reduce overall prices while providing greater profitability to the seller, such as in so-called \"purchase commitment\" programs. Economist Arnold Kling attributes the present cost crisis mainly to the practice of what he calls \"premium medicine\", which overuses expensive forms of technology that is of marginal or no proven benefit.\n\nMilton Friedman has argued that government has weak incentives to reduce costs because \"nobody spends somebody else’s money as wisely or as frugally as he spends his own\". Others contend that health care consumption is not like other consumer consumption. Firstly there is a negative utility of consumption (consuming more health care does not make one better off) and secondly there is an information asymmetry between consumer and supplier.\n\nPaul Krugman and Robin Wells argue that all of the evidence indicates that public insurance of the kind available in several European countries achieves equal or better results at much lower cost, a conclusion that also applies within the United States. In terms of actual administrative costs, Medicare spent less than 2% of its resources on administration, while private insurance companies spent more than 13%. The Cato Institute argues that the 2% Medicare cost figure ignores all costs shifted to doctors and hospitals, and alleges that Medicare is not very efficient at all when those costs are incorporated. Some studies have found that the U.S. wastes more on bureaucracy (compared to the Canadian level), and that this excess administrative cost would be sufficient to provide health care to the uninsured population in the U.S.\n\nNotwithstanding the arguments about Medicare, there is overall less bureaucracy in socialized systems than in the present mixed U.S. system. Spending on administration in Finland is 2.1% of all health care costs, and in the UK the figure is 3.3% whereas the U.S. spends 7.3% of all expenditures on administration.\n\nSome in the U.S. claim that socialized medicine would reduce health care quality. The quantitative evidence for this claim is not clear. The WHO has used Disability Adjusted Life Expectancy (the number of years an average person can expect to live in good health) as a measure of a nation's health achievement, and has ranked its member nations by this measure. The U.S. ranking was 24th, worse than similar industrial countries with high public funding of health such as Canada (ranked 5th), the UK (12th), Sweden (4th), France (3rd) and Japan (1st). But the U.S. ranking was better than some other European countries such as Ireland, Denmark and Portugal, which came 27th, 28th and 29th respectively. Finland, with its relatively high death rate from guns and renowned high suicide rate came above the U.S. in 20th place. The British have a Care Quality Commission that commissions independent surveys of the quality of care given in its health institutions and these are publicly accessible over the internet. These determine whether health organizations are meeting public standards for quality set by government and allows regional comparisons. Whether these results indicate a better or worse situation to that in other countries such as the U.S. is hard to tell because these countries tend to lack a similar set of standards.\n\nOpponents claim that socialized medicine would require higher taxes but international comparisons do not support this; the ratio of public to private spending on health is lower in the U.S. than that of Canada, Australia, New Zealand, Japan, or any EU country, yet the per capita tax funding of health in those countries is already lower than that of the United States.\n\nTaxation is not necessarily an unpopular form of funding for health care. In England, a survey for the British Medical Association of the general public showed overwhelming support for the tax funding of health care. Nine out of ten people agreed or strongly agreed with a statement that the NHS should be funded from taxation with care being free at the point of use.\n\nAn opinion piece in \"The Wall Street Journal\" by two conservative Republicans argues that government sponsored health care will legitimatize support for government services generally, and make an activist government acceptable. \"Once a large number of citizens get their health care from the state, it dramatically alters their attachment to government. Every time a tax cut is proposed, the guardians of the new medical-welfare state will argue that tax cuts would come at the expense of health care -- an argument that would resonate with middle-class families entirely dependent on the government for access to doctors and hospitals.\"\n\nSome in the U.S. argue that if government were to use its size to bargain down health care prices, this would undermine American leadership in medical innovation. It is argued that the high level of spending in the U.S. health care system and its tolerance of waste is actually beneficial because it underpins American leadership in medical innovation, which is crucial not just for Americans, but for the entire world.\n\nOthers point out that the American health care system spends more on state-of-the-art treatment for people who have good insurance, and spending is reduced on those lacking it and question the costs and benefits of some medical innovations, noting, for example, that \"rising spending on new medical technologies designed to address heart disease has not meant that more patients have survived.\"\n\nOne of the goals of socialized medicine systems is ensuring universal access to health care. Opponents of socialized medicine say that access for low-income individuals can be achieved by means other than socialized medicine, for example, income-related subsidies can function without public provision of either insurance or medical services. Economist Milton Friedman said the role of the government in health care should be restricted to financing hard cases. Universal coverage can also be achieved by making purchase of insurance compulsory. For example, European countries with socialized medicine in the broader sense, such as Germany and The Netherlands, operate in this way. A legal obligation to purchase health insurance is akin to a mandated health tax, and the use of public subsidies is a form of directed income redistribution via the tax system. Such systems give the consumer a free choice amongst competing insurers whilst achieving universality to a government directed minimum standard.\n\nCompulsory health insurance or savings are not limited to so-called socialized medicine, however. Singapore's health care system, which is often referred to as a free-market or mixed system, makes use of a combination of compulsory participation and state price controls to achieve the same goals.\n\nPart of the current debate about health care in the United States revolves around whether the Affordable Care Act as part of health care reform will result in a more systematic and logical allocation of health care. Opponents tend to believe that the law will eventually result in a government takeover of health care and ultimately to socialized medicine and rationing based not on being able to afford the care you want but on whether a third party other than the patient and the doctor decides whether the procedure or the cost is justifiable. Supporters of reform point out that health care rationing already exists in the United States through insurance companies either denying coverage for pre-existing conditions or applying differential pricing for this coverage, or issuing denial for reimbursement on the grounds that the insurance company believes the procedure is experimental or will not assist even though the doctor has recommended it. A public insurance plan was not included in the Affordable Care Act but some argue that it would have added to health care access choices, and others argue that the central issue is whether health care is rationed sensibly.\n\nOpponents of reform invoke the term socialized medicine because they say it will lead to health care rationing by denial of coverage, denial of access, and use of waiting lists, but often do so without acknowledging coverage denial, lack of access and waiting lists exist in the U.S. health care system currently or that waiting lists in the U.S. are sometimes longer than the waiting lists in countries with socialized medicine. Proponents of the reform proposal point out a public insurer is not akin to a socialized medicine system because it will have to negotiate rates with the medical industry just as other insurers do and cover its cost with premiums charged to policyholders just as other insurers do without any form of subsidy.\n\nThere is a frequent misunderstanding to think that waiting happens in places like the United Kingdom and Canada but does not happen in the United States. For instance it is not uncommon even for emergency cases in some U.S. hospitals to be boarded on beds in hallways for 48 hours or more due to lack of inpatient beds and people in the U.S. rationed out by being unable to afford their care are simply never counted and may never receive the care they need, a factor that is often overlooked. Statistics about waiting times in national systems are an honest approach to the issue of those waiting for access to care. Everyone waiting for care is reflected in the data, which, in the UK for example, are used to inform debate, decision-making and research within the government and the wider community. Some people in the U.S. are rationed out of care by unaffordable care or denial of access by HMOs and insurers or simply because they cannot afford co-pays or deductibles even if they have insurance. These people wait an indefinitely long period and may never get care they need, but actual numbers are simply unknown because they are not recorded in official statistics.\n\nOpponents of the current reform care proposals fear that U.S. comparative effective research (a plan introduced in the stimulus bill) will be used to curtail spending and ration treatments, which is one function of the National Institute for Health and Care Excellence (NICE), arguing that rationing by market pricing rather by government is the best way for care to be rationed. However, when defining any group scheme, the same rules must apply to everyone in the scheme so some coverage rules had to be established. Britain has a national budget for public funded health care, and recognizes there has to be a logical trade off between spending on expensive treatments for some against, for example, caring for sick children. NICE is therefore applying the same market pricing principles to make the hard job of deciding between funding some treatments and not funding others on behalf of everyone in the insured pool. This rationing does not preclude choice of obtaining insurance coverage for excluded treatment as insured persons do having the choice to take out supplemental health insurance for drugs and treatments that the NHS does not cover (at least one private insurer offers such a plan) or from meeting treatment costs out-of-pocket.\n\nThe debate in the U.S. over rationing has enraged some in the UK and statements made by politicians such as Sarah Palin and Chuck Grassley resulted in a mass Internet protest on websites such as Twitter and Facebook under the banner title \"welovetheNHS\" with positive stories of NHS experiences to counter the negative ones being expressed by these politicians and others and by certain media outlets such as \"Investor's Business Daily\" and Fox News. In the UK, it is private health insurers that ration care (in the sense of not covering the most common services such as access to a primary care physician or excluding pre-existing conditions) rather than the NHS. Free access to a general practitioner is a core right in the NHS, but private insurers in the UK will not pay for payments to a private primary care physician. Private insurers exclude many of the most common services as well as many of the most expensive treatments, whereas the vast majority of these are not excluded from the NHS but are obtainable at no cost to the patient. According to the Association of British Insurers (ABI), a typical policy will exclude the following: going to a general practitioner; going to accident and emergency; drug abuse; HIV/AIDS; normal pregnancy; gender reassignment; mobility aids, such as wheelchairs; organ transplant; injuries arising from dangerous hobbies (often called hazardous pursuits); pre-existing conditions; dental services; outpatient drugs and dressings; deliberately self-inflicted injuries; infertility; cosmetic treatment; experimental or unproven treatment or drugs; and war risks. Chronic illnesses, such as diabetes and end stage renal disease requiring dialysis are also excluded from coverage. Insurers do not cover these because they feel they do not need to since the NHS already provides coverage and to provide the choice of a private provider would make the insurance prohibitively expensive. Thus in the UK there is cost shifting from the private sector to the public sector, which again is the \"opposite\" of the allegation of cost shifting in the U.S. from public providers such as Medicare and Medicaid to the private sector.\n\nPalin had alleged that America will create rationing \"death panels\" to decide whether old people could live or die, again widely taken to be a reference to NICE. U.S. Senator Chuck Grassley alleged that he was told that Senator Edward Kennedy would have been refused the brain tumor treatment he was receiving in the United States had he instead lived a country with government run health care. This, he alleged, would have been due to rationing because of Kennedy's age (77 years) and the high cost of treatment. The UK Department of Health said that Grassley’s claims were \"just wrong\" and reiterated health service in Britain provides health care on the basis of clinical need regardless of age or ability to pay. The chairman of the British Medical Association, Hamish Meldrum, said he was dismayed by the \"jaw-droppingly untruthful attacks\" made by American critics. The chief executive of the National Institute for Health and Clinical Excellence (NICE), told \"The Guardian\" newspaper that \"it is neither true, nor is it anything you could extrapolate from anything we've ever recommended\" that Kennedy would be denied treatment by the NHS. The business journal \"Investor's Business Daily\" recently claimed mathematician and astrophysicist Stephen Hawking, who has ALS and speaks with the aid of an American-accented voice synthesizer, would not have survived if he had been treated in the British National Health Service. Hawking is British and been treated throughout his life (67 years) by the NHS and issued a statement to the effect he owed his life to the quality of care he has received from the NHS.\n\nSome argue that countries with national health care may use waiting lists as a form of rationing compared to countries that ration by price, such as the United States, according to several commentators and healthcare experts. \"The Washington Post\" columnist Ezra Klein compared 27% of Canadians reportedly waiting four months or more for elective surgery with 26% of Americans reporting that they did not fulfill a prescription due to cost (compared to only 6% of Canadians). Britain's former age-based policy that once prevented the use of kidney dialysis as treatment for older patients with renal problems, even to those who can privately afford the costs, has been cited as another example. A 1999 study in the \"Journal of Public Economics\" analyzed the British National Health Service and found that its waiting times function as an effective market disincentive, with a low elasticity of demand with respect to time.\n\nSupporters of private price rationing over waiting time rationing, such as \"The Atlantic\" columnist Megan McArdle, argue time rationing leaves patients worse off since their time (measured as an opportunity cost) is worth much more than the price they would pay. Opponents also state categorizing patients based on factors such as social value to the community or age will not work in a heterogeneous society without a common ethical consensus such as the U.S. Doug Bandow of the CATO Institute wrote that government decision making would \"override the differences in preferences and circumstances\" for individuals and that it is a matter of personal liberty to be able to buy as much or as little care as one wants. Neither argument recognizes the fact that in most countries with socialized medicine, a parallel system of private health care allows people to pay extra to reduce their waiting time. The exception is that some provinces in Canada disallow the right to bypass queuing unless the matter is one in which the rights of the person under the constitution.\n\nA 1999 article in the \"British Medical Journal\", stated \"there is much merit in using waiting lists as a rationing mechanism for elective health care if the waiting lists are managed efficiently and fairly.\" Dr. Arthur Kellermann, associate dean for health policy at Emory University, stated rationing by ability to pay rather than by anticipated medical benefits in the U.S. makes its system more unproductive, with poor people avoiding preventive care and eventually using expensive emergency treatment. Ethicist Daniel Callahan has written that U.S. culture overly emphasizes individual autonomy rather than communitarian morals and that stops beneficial rationing by social value, which benefits everyone.\n\nSome argue that waiting lists result in great pain and suffering, but again evidence for this is unclear. In a recent survey of patients admitted to hospital in the UK from a waiting list or by planned appointment, only 10% reported they felt they should have been admitted sooner than they were. 72% reported the admission was as timely as they felt necessary. Medical facilities in the U.S. do not report waiting times in national statistics as is done in other countries and it is a myth to believe there is no waiting for care in the U.S. Some argue that wait times in the U.S. could actually be as long as or longer than in other countries with universal health care.\n\nThere is considerable argument about whether any of the health bills currently before congress will introduce rationing. Howard Dean for example contested in an interview that they do not. However, \"Politico\" has pointed out that all health systems contain elements of rationing (such as coverage rules) and the public health care plan will therefore implicitly involve some element of rationing.\n\nIn the UK, where government employees or government-employed sub-contractors deliver most health care, political interference is quite hard to discern. Most supply-side decisions are in practice under the control of medical practitioners and of boards comprising the medical profession. There is some antipathy towards the target-setting by politicians in the UK. Even the NICE criteria for public funding of medical treatments were never set by politicians. Nevertheless, politicians have set targets, for instance to reduce waiting times and to improve choice. Academics have pointed out that the claims of success of the targeting are statistically flawed.\n\nThe veracity and significance of the claims of targeting interfering with clinical priorities are often hard to judge. For example, some UK ambulance crews have complained that hospitals would deliberately leave patients with ambulance crews to prevent an accident and emergency department (A&E, or emergency room) target-time for treatment from starting to run. The Department of Health vehemently denied the claim, because the A&E time begins when the ambulance arrives at the hospital and not after the handover. It defended the A&E target by pointing out that the percentage of people waiting four hours or more in A&E had dropped from just under 25% in 2004 to less than 2% in 2008. The original \"Observer\" article reported that in London, 14,700 ambulance turnarounds were longer than an hour and 332 were more than two hours when the target turnaround time is 15 minutes. However, in the context of the total number of emergency ambulance attendances by the London Ambulance Service each year (approximately 865,000), these represent just 1.6% and 0.03% of all ambulance calls. The proportion of these attributable to patients left with ambulance crews is not recorded. At least one junior doctor has complained that the four-hour A&E target is too high and leads to unwarranted actions that are not in the best interests of patients.\n\nPolitical targeting of waiting-times in Britain has had dramatic effects. The National Health Service reports that the median admission wait-time for elective inpatient treatment (non-urgent hospital treatment) in England at the end of August 2007, was just under 6 weeks, and 87.5% of patients were admitted within 13 weeks. Reported waiting times in England also overstate the true waiting-time. This is because the clock starts ticking when the patient has been referred to a specialist by the GP and it only stops when the medical procedure is completed. The 18-week maximum waiting period target thus includes all the time taken for the patient to attend the first appointment with the specialist, time for any tests called for by the specialist to determine precisely the root of the patient's problem and the best way to treat it. It excludes time for any intervening steps deemed necessary prior to treatment, such as recovery from some other illness or the losing of excessive weight.\n\n"}
{"id": "39218390", "url": "https://en.wikipedia.org/wiki?curid=39218390", "title": "Start School Later movement", "text": "Start School Later movement\n\nThe movement to start school later consists of efforts by health care professionals, sleep scientists, educators, economists, legislators, parents, students, and other concerned citizens to restore a later start to the school day. Based on a growing body of evidence that starting middle and high schools too early in the morning is unhealthy, counterproductive, and incompatible with adolescent sleep needs and patterns. During the second half of the 20th century, many public schools in the United States began shifting instructional time earlier than the more conventional bell time, thought to be about 9 a.m. Today it is common for American schools to begin the instructional day in the 7 a.m. hour and end about seven hours later, around 2 p.m. Most sleep research suggests that morning classes should begin no earlier than 8:30 a.m. for middle and high school students.\n\nAdvocates of a return to later school start times argue that sleep and school hours should be viewed as a public health issue, citing evidence linking early school start times to widespread sleep deprivation among teenagers as well as a wide array of acute and chronic physical, psychological, and educational problems. Not only do students consistently get significantly more sleep on school nights when their schools move to later start times, but later school hours have been consistently linked with improved school performance, reduced impulsiveness, and greater motivation, as well as with lower rates of depression, tardiness, truancy, and morning automobile accidents. Recent (2011) studies suggest that early school start times disproportionately hurt economically disadvantaged students and may even negatively impact future earning potential of students, offsetting any financial savings to the school system attributed to earlier hours.\n\nIn the early 1990s, the University of Minnesota's landmark School Start Time Study tracked high school students from two Minneapolis-area districts – Edina, a suburban district that changed its opening hour from 7:20 a.m. to 8:30 a.m. and the Minneapolis Public Schools, which changed their opening from 7:20 a.m. to 8:40 a.m. Many positive benefits to students were found, including:\n\nA longitudinal follow-up study of the Minneapolis Public Schools after five years revealed that these and other positive benefits had persisted. In 2014, a three-year project using data from more than 9,000 students attending eight high schools in three states, found that when schools switched to a start time of 8:30 a.m. or later, attendance, standardized test scores and academic performance improved, and tardiness, substance abuse, symptoms of depression, consumption of caffeinated drinks, and the rate of traffic accidents involving teen drivers decreased.\n\nSince the 1990s over 250 individual schools or districts in 45 states have delayed their start times, and scores of others are considering a change at any given time. Despite sporadic reform efforts on the part of educators, public health officials, legislators, and grassroots groups, however, most American middle and high schools still require students to begin instruction prior to 8:30 a.m. Failure of efforts to delay start times over the years has primarily been due to pushback from community members who fear that a shift to later school hours will be prohibitively expensive and/or disrupt after-school sports and other extracurricular schedules, student jobs, daycare arrangement, teacher training, or time for homework\n\nProponents of a return to later school hours cite abundant evidence that starting middle and high school before about 8:30 or 9 a.m. is incompatible with the biological clocks of teenagers and young adults. In 1993, a team led by Mary Carskadon, PhD, of Brown University showed that changes in circadian biology during puberty drive a \"sleep-phase delay,\" a shift in the sleep-wake patterns of adolescents that leads them to fall asleep and wake up later than younger and older people. Subsequent studies have confirmed these findings, explored the impact of school start times on the sleep needs and patterns of adolescents., and demonstrated a \"phase shift\" in the release of melatonin at puberty, which appears to be involved in shifting the sleep-wake cycle several hours later during the adolescent years. This same shift to a delayed phase in the release of melatonin during puberty has also been seen in other mammals.\n\nThe shifted circadian rhythms associated with puberty make it difficult, if not impossible, for many teenagers—who may have to rise at 5 or 6 a.m. to get ready and commute to school in time for 7 a.m. school bells—to get sufficient sleep on school nights. Even discounting for the distractions of homework and extracurricular demands and electronics, most adolescents find it difficult to fall asleep before about 11 p.m. or rise before about 8 a.m. In addition, they need to sleep in until 8 a.m. or so to get the 9 or more hours of sleep that most sleep research suggests they need. As a result, many teenagers arrive at school sleep-deprived. The most recent data from the Youth Risk Behavior Survey show that 70% of American high school students are sleep-deprived and about 40% get six or fewer hours of sleep per night.\n\nDue to the fact that almost 70% of teens don't get enough sleep, there are increases in stimulant abuse, weight gain, risk of diabetes, immune disorders, mood swings, depression, and suicidal ideation, as well as reduced impulse control. In addition, early school start times have been associated with drowsy driving in new teen drivers and higher car crash rates. Schools ending early in the afternoon may also increase the risk of engaging in unhealthy, risky behaviors among sleep-deprived adolescents. Sending children to school before sunrise also means they must wait or walk in dark, with low visibility.\n\nSleep deprivation can result in low motivation, difficulty concentrating, restlessness, slowed reaction times, lack of energy, frequent errors, forgetfulness, and impaired decision-making skills. Studies, many spearheaded by Kyla Wahlstrom and her research team at University of Minnesota's Center for Applied Research and Educational Improvement (CAREI), have tied these effects to early school start times, which, in turn, have repeatedly been linked to increased rates of tardiness, truancy, absenteeism, and dropping out. In 2012 a study using data from Wake County, North Carolina, showed that delaying middle school start times by one hour, from roughly 7:30 to 8:30, increases standardized test scores by at least two percentile points in math and one percentile point in reading. The effect was largest for students with below-average test scores, suggesting that later start times would narrow gaps in student achievement.\n\nThe impact of later start times on school performance—including reduced truancy, absenteeism, and increased overall academic achievement—is approximately double in economically disadvantaged students. This discrepancy may be explained, at least in part, by the fact that privileged students have opportunities to attend private schools (most of which start instruction after 8 a.m.) and/or save time by driving or being driven to school.\n\nA 2017 report by the RAND Corporation concluded that delaying school start times to 8:30 a.m. is a cost-effective, population-level strategy that would significantly impact public health and the U.S. economy, with benefits quickly outweighing any immediate costs. After just two years, the study conservatively projected a gain of $8.6 billion to the U.S. economy. After a decade, this gain would increase to $83 billion, and after 15 years to $140 billion, amounting to an average annual gain to the U.S. economy of $9.3 billion. An earlier Hamilton Project Report published by the Brookings Institution concluded that starting class later can be an immediate and inexpensive way to boost health, safety, and achievement for all students. The authors, economists Brian A. Jacob and Jonah E. Rockoff, predicted that starting high schools one hour later, at about 9 a.m., would result in roughly $17,500 in increased future earnings per student in present value—a benefit:cost ratio of at least 9:1 even when changing schedules requires upfront investment to alter bus schedules and/or accommodate later after-school activities.\n\nAs early as 1993, sleep researchers and healthcare leaders began encouraging school administrators to move middle and high school hours to 8:30 a.m. or later.\nToday numerous health, educational, and civic leaders are calling for a return to later, healthier school start times, including former U.S. Education Secretary Arne Duncan, the National Sleep Foundation, and the National Institutes of Health. In 2014 the American Academy of Pediatrics issued a policy statement recommending that middle and high schools start no earlier than 8:30 a.m. as an effective means of addressing the serious public health issue of insufficient sleep in adolescents, a position echoed in 2015 by the Centers for Disease Control and Prevention and in 2016 by the American Medical Association, and supported by the American Academy of Child and Adolescent Psychiatry, the American Thoracic Society, the National Association of School Nurses, and the Society of Pediatric Nurses. The National Education Association issued a resolution supporting school schedules that follow research-based recommendations regarding the sleep patterns of age groups. Several state medical societies have issued position statements or resolutions supporting later school start times, as have both the Washington and Virginia state Parent Teachers Associations (PTAs) and the Seattle Educators Association A move to a later school start time is also consistent with the Healthy People 2020 Objective to increase the proportion of students in grades 9 to 12 who get sufficient sleep.\n\nCalifornia Congresswoman Zoe Lofgren has repeatedly introduced versions of a \"ZZZ's to A's\" Bill and Resolution to the U.S. Congress since 1998, all proposing limits on the hours at which American high schools can begin required instruction. All thus far have failed.\n\nNumerous bills related to sleep and school start times have been introduced in state legislatures since the 1990s, including the California, Florida, Maryland, Virginia, Tennessee, New Jersey, Maine, Rhode Island, Utah, Washington, Nevada, and Massachusetts legislatures. In 2014 Maryland was the first state to pass school start time legislation via HB 883, sponsored by Delegate Aruna Miller (D, Montgomery County) and passed unanimously by the Maryland General Assembly. This legislation required the state Department of Health and Mental Hygiene to conduct a study on student sleep needs, explore ways school systems can shift hours to accommodate them, and develop recommendations about whether state public schools should adopt a policy of start times at 8 a.m. or later, resulting in the \"Report on Safe and Healthy School Hours\". Two years later the Maryland state legislature passed the Orange Ribbon Bill for Healthy School Hours, also sponsored by Delegate Miller, a voluntary, no-cost, incentive program recognizing districts for implementing evidence-based school hours. In 2015 New Jersey passed a law (S2484) requiring the state Department of Education to study the options and benefits of starting school later at middle and high schools and recommend whether the state should establish a pilot program to test later school start times in select middle schools and high schools throughout the state. A 2017 California bill introduced by Senator Anthony J. Portantino that would have prevented state middle and high schools from requiring attendance before 8:30 a.m. failed to gain legislative support but will be reconsidered in 2018.\n\nThe Fairfax County (Virginia) Public Schools (FCPS) Board of Education passed a resolution in April 2012 affirming their goal to find ways to start county high schools after 8 a.m. to allow students to get sufficient sleep, a resolution supported by the \"Washington Post's\" editorial board. In June 2013 FCPS contracted a team from the Children’s National Medical Center's Division of Sleep Medicine to partner with Fairfax County students, parents, educators, administrators, and other community stakeholders to develop a plan to accomplish this task. This effort led the Washington Post's editorial board to endorse later school start times as a \"smarter way to start high schoolers' days\" in August 2013. This editorial was tweeted by U.S. Education Secretary Arne Duncan along with the comment that starting high school later and letting teens sleep more was a \"common sense\" way \"to improve student achievement that too few have implemented.\" As new school superintendent Karen Garza laid out her vision for the district in September 2013, she vowed to push for later school start times. On October 1, 2013, Montgomery County, MD School Superintendent Joshua Starr recommended that high school start times be moved 50 minutes to 8:15 a.m., with a proposed start in the 2015–16 school year.\n\nIn England, Oxford's Sleep and Circadian Neurosciences Institute (SCNi) in 2015 began a study involving sleep education for teenagers in England and Wales. Professors Russell Foster and Colin Espie with their project \"Teensleep\" will assess whether ten half-hour lessons in year 10 will improve academic performance and well-being. Taught by specially trained teachers, the lessons introduce scientific theory on the importance of sleep and the effects of sleep deprivation as well as techniques for stress management. The study has been redesigned since it was originally announced; it was previously considerably more ambitious, including a later start time at about twenty-five schools, sleep education in others, both in some schools and neither in others. Students will be evaluated before and after the sleep education periods.\n\nCommunity groups have sporadically arisen in various school communities since the 1990s, sometimes prompting studies and reform proposals but often dying out as concerned, active families grew out of the system. Probably the most visible and longest lasting of the grassroots advocacy groups is SLEEP in Fairfax County, Virginia, which was formed in 2004 to increase awareness of teen sleep needs and to change middle and high school start times in the Fairfax County Public Schools (FCPS) to later in the morning. More recently, social media tools have allowed once isolated advocates to unite efforts and share resources. In fall 2011, an online petition effort galvanized a national non-profit organization, Start School Later, a coalition of health professionals, sleep scientists, educators, parents, students, and other concerned citizens dedicated to increasing public awareness about the relationship between sleep and school hours and to ensuring school start times compatible with health, safety, education, and equity. Active petition drives in 2012 and 2013 among the coalition's Washington DC Metro area chapters have spurred several counties to re-open their discussions and helped spearhead a study group to reconsider the issue in the Montgomery County Public Schools. Start School Later also maintains a website with links to references and other educational materials on sleep and school start times, and in 2013 partnered with The Lloyd Society to co-sponsor an educational symposium featuring keynote speaker Judith Owens, MD, MPH, Director of Sleep Medicine at the Children's National Medical Center, whose research interests include the neurobehavioral and health consequences of sleep problems in children, pharmacologic treatment of pediatric sleep disorders, and cultural and psychosocial issues that impact sleep. Many advocates also support campaigns using materials from California attorney Dennis Nolan's website, an exhaustive and frequently updated compilation of research about adolescent sleep deprivation and its relationship to early school start times. In spring 2013 the Mayo Clinic updated its online information about teen sleep to recognize grassroots efforts to start school at later times in sync with the internal clocks of adolescents.\n\n"}
{"id": "34104355", "url": "https://en.wikipedia.org/wiki?curid=34104355", "title": "Subjective well-being", "text": "Subjective well-being\n\nSubjective well-being (SWB) is a self-reported measure of well-being, typically obtained by questionnaire.\n\nEd Diener developed a tripartite model of subjective well-being in 1984, which describes how people experience the quality of their lives and includes both emotional reactions and cognitive judgments. It posits \"three distinct but often related components of wellbeing: frequent positive affect, infrequent negative affect, and cognitive evaluations such as life satisfaction.\"\n\nSWB therefore encompasses moods and emotions as well as evaluations of one's satisfaction with general and specific areas of one's life. Concepts encompassed by SWB include happiness. \n\nSWB tends to be stable over time and is strongly related to personality traits. There is evidence that health and SWB may mutually influence each other, as good health tends to be associated with greater happiness, and a number of studies have found that positive emotions and optimism can have a beneficial influence on health.\n\nDiener et al. argued that the various components of SWB represent distinct constructs that need to be understood separately, even though they are closely related. Hence, SWB may be considered \"a general area of scientific interest rather than a single specific construct\". Due to the specific focus on the \"subjective\" aspects of well-being, definitions of SWB typically exclude \"objective\" conditions such as material conditions or health, although these can influence ratings of SWB. Definitions of SWB therefore focus on how a person evaluates his/her own life, including emotional experiences of pleasure versus pain in response to specific events and cognitive evaluations of what a person considers a good life. Components of SWB relating to affect include positive affect (experiencing pleasant emotions and moods) and low negative affect (experiencing unpleasant, distressing emotions and moods), as well as \"overall affect\" or \"hedonic balance\", defined as the overall equilibrium between positive and negative affect, and usually measured as the difference between the two. High positive affect and low negative affect are often highly correlated, but not always.\n\nThere are two components of SWB. One is Affective Balance and the other is Life Satisfaction. An individual's scores on the two measures are summed to produce a total SWB score. In some cases, these scores are kept separate.\nAffective balance refers to the emotions, moods, and feelings a person has. These can be all positive, all negative, or a combination of both positive and negative. Some research shows also that feelings of reward are separate from positive and negative affect.\nLife satisfaction (global judgments of one's life) and satisfaction with specific life domains (e.g. work satisfaction) are considered cognitive components of SWB. The term \"happiness\" is also commonly used in regards to SWB and has been defined variously as \"satisfaction of desires and goals\" (therefore related to life satisfaction), as a \"preponderance of positive over negative affect\" (therefore related to emotional components of SWB), as \"contentment\", and as a \"consistent, optimistic mood state\" and may imply an affective evaluation of one's life as a whole. Life satisfaction can also be known as the \"stable\" component in one's life. Affective concepts of SWB can be considered in terms of momentary emotional states as well as in terms of longer-term moods and tendencies (i.e. how much positive and/or negative affect a person generally experiences over any given period of time). Life satisfaction and in some research happiness are typically considered over long durations, up to one's lifetime. \"Quality of life\" has also been studied as a conceptualization of SWB. Although its exact definition varies, it is usually measured as an aggregation of well-being across several life domains and may include both subjective and objective components.\n\nLife satisfaction and Affect balance are generally measured separately and independently. \nSometimes a single SWB question attempts to capture an overall picture.\n\nThe issue with the such measurements of life satisfaction and affective balance is that they are self-reports. The problem with self-reports is that the participants may be lying or at least not telling the whole truth on the questionnaires. Participants may be lying or holding back from revealing certain things because they are either embarrassed or they may be filling in what they believe the researcher wants to see in the results. To gain more accurate results, other methods of measurement have been used to determine one’s SWB. \n\nAnother way to corroborate or confirm that the self-report results are accurate is through informant reports. Informant reports are given to the participant’s closest friends and family and they are asked to fill out either a survey or a form asking about the participants mood, emotions, and overall lifestyle. The participant may write in the self-report that they are very happy, however that participant’s friends and family record that he/she is always depressed. This would obviously be a contradiction in results which would ultimately lead to inaccurate results. \n\nAnother method of gaining a better understanding of the true results is through ESM, or the Experience Sampling Method. In this measure, participants are given a beeper/pager that will randomly ring throughout the day. Whenever the beeper/pager sounds, the participant will stop what he/she is doing and record the activity they are currently engaged in and their current mood and feelings. Tracking this over a period of a week or a month will give researchers a better understanding of the true emotions, moods, and feelings the participant is experiencing, and how these factors interact with other thoughts and behaviors. A third measurement to ensure validity is the Day Reconstruction Method. In this measure, participants fill out a diary of the previous days’ activities. The participant is then asked to describe each activity and provide a report of how they were feeling, what mood they were experiencing, and any emotions that surfaced. Thus to ensure valid results, a researcher may tend to use self-reports along with another form of measurement mentioned above. Someone with a high level of life satisfaction and a positive affective balance is said to have a high level of SWB.\n\nTheories of the causes of SWB tend to emphasise either top-down or bottom-up influences.\n\nIn the top-down view, global features of personality influence the way a person perceives events. Individuals may therefore have a global tendency to perceive life in a consistently positive or negative manner, depending on their stable personality traits. Top-down theories of SWB suggest that people have a genetic predisposition to be happy or unhappy and this predisposition determines their SWB \"setpoint\". Set Point theory implies that a person's baseline or equilibrium level of SWB is a consequence of hereditary characteristics and therefore, almost entirely predetermined at birth. Evidence for this genetic predisposition derives from behavior-genetic studies that have found that positive and negative affectivity each have high heritability (40% and 55% respectively in one study). Numerous twin studies confirm the notion of set point theory, however, they do not rule out the possibility that is it possible for individuals to experience long term changes in SWB.\n\nDiener et al. note that heritability studies are limited in that they describe long-term SWB in a sample of people in a modern western society but may not be applicable to more extreme environments that might influence SWB and do not provide absolute indicators of genetic effects. Additionally, heritability estimates are inconsistent across studies.\n\nFurther evidence for a genetically influenced predisposition to SWB comes from findings that personality has a large influence on long-term SWB. This has led to the \"dynamic equilibrium model\" of SWB. This model proposes that personality provides a baseline for emotional responses. External events may move people away from the baseline, sometimes dramatically, but these movements tend to be of limited duration, with most people returning to their baseline eventually.\n\nFrom a bottom-up perspective, happiness represents an accumulation of happy experiences. Bottom-up influences include external events, and broad situational and demographic factors, including health and marital status. Bottom-up approaches are based on the idea that there are universal basic human needs and that happiness results from their fulfilment. In support of this view, there is evidence that daily pleasurable events are associated with increased positive affect, and daily unpleasant events or hassles are associated with increased negative affect.\n\nHowever, research suggests that external events account for a much smaller proportion of the variance in self-reports of SWB than top-down factors, such as personality. A theory proposed to explain the limited impact of external events on SWB is hedonic adaptation. Based originally on the concept of a \"hedonic treadmill\", this theory proposes that positive or negative external events temporarily increase or decrease feelings of SWB, but as time passes people tend to become habituated to their circumstances and have a tendency to return to a personal SWB \"setpoint\" or baseline level.\n\nThe hedonic treadmill theory originally proposed that most people return to a neutral level of SWB (i.e. neither happy nor unhappy) as they habituate to events. However, subsequent research has shown that for most people, the baseline level of SWB is at least mildly positive, as most people tend to report being at least somewhat happy in general and tend to experience positive mood when no adverse events are occurring. Additional refinements to this theory have shown that people do not adapt to all life events equally, as people tend to adapt rapidly to some events (e.g. imprisonment), slowly to others (e.g. the death of a loved one), and not at all to others (e.g. noise and sex).\n\nA number of studies have found that SWB constructs are strongly associated with a range of personality traits, including those in the five factor model. Findings from numerous personality studies show that genetics account for 20-48% of the variance in Five-Factor Model and the variance in subjective well-being is also heritable. Specifically, neuroticism predicts poorer subjective well-being whilst extraversion, agreeableness, conscientiousness and openness to experience tend to predict higher subjective well-being. A meta-analysis found that neuroticism, extraversion, agreeableness, and conscientiousness were significantly related to all facets of SWB examined (positive, negative, and overall affect; happiness; life satisfaction; and quality of life). Neuroticism was the strongest predictor of overall SWB and is the strongest predictor of negative affect.\n\nA large number of personality traits are related to SWB constructs, although intelligence has negligible relationships. Positive affect is most strongly predicted by extraversion, to a lesser extent agreeableness, and more weakly by openness to experience. Happiness was most strongly predicted by extraversion, and also strongly predicted by neuroticism, and to a lesser extent by the other three factors. Life satisfaction was significantly predicted by neuroticism, extraversion, agreeableness, and conscientiousness. Quality of life was very strongly predicted by neuroticism, and also strongly predicted by extraversion and conscientiousness, and to a modest extent by agreeableness and openness to experience. One study found that subjective well-being was genetically indistinct from personality traits, especially those that reflected emotional stability (low Neuroticism), and social and physical activity (high Extraversion), and constraint (high Conscientiousness).\n\nDeNeve (1999) argued that there are three trends in the relationship between personality and SWB. Firstly, SWB is closely tied to traits associated with emotional tendencies (emotional stability, positive affectivity, and tension). Secondly, relationship enhancing traits (e.g. trust, affiliation) are important for subjective well-being. Happy people tend to have strong relationships and be good at fostering them. Thirdly, the way people think about and explain events is important for subjective well-being. Appraising events in an optimistic fashion, having a sense of control, and making active coping efforts facilitates subjective well-being. Trust, a trait substantially related to SWB, as opposed to cynicism involves making positive rather than negative attributions about others. Making positive, optimistic attributions rather than negative pessimistic ones facilitates subjective well-being.\n\nThe related trait of eudaimonia or psychological well-being, is also heritable. Evidence from one study supports 5 independent genetic mechanisms underlying the Ryff facets of psychological well-being, leading to a genetic construct of eudaimonia in terms of general self-control, and four subsidiary biological mechanisms enabling the psychological capabilities of purpose, agency, growth, and positive social relations\n\nA person's level of subjective well-being is determined by many different factors and social influences prove to be a strong one. Results from the famous Framingham Heart Study indicate that friends three degrees of separation away (that is, friends of friends of friends) can affect a person's happiness. From abstract: \"A friend who lives within a mile (about 1.6 km) and who becomes happy increases the probability that a person is happy by 25%.\"\n\nResearch indicates that wealth is related to many positive outcomes in life. Such outcomes include: improved health and mental health, greater longevity, lower rates of infant mortality, experience fewer stressful life events, and less frequently the victims of violent crimes However, research suggests that wealth has a smaller impact on SWB than people generally think, even though higher incomes do correlate substantially with life satisfaction reports.\n\nThe relative influence of wealth together with other material components on overall subjective well-being of a person is being studied through new researches. The Well-being Project at Human Science Lab investigates how material well-being and perceptual well-being works as relative determinants in conditioning our mind for positive emotions.\n\nIn a study done by Aknin, Norton, & Dunn (2009), researchers asked participants from across the income spectrum to report their own happiness and to predict the happiness of others and themselves at different income levels. In study 1, predicted happiness ranged between 2.4-7.9 and actual happiness ranged between 5.2-7.7. In study 2, predicted happiness ranged between 15-80 and actual happiness ranged between 50-80. These findings show that people believe that money does more for happiness than it really does. However, some research indicates that while socioeconomic measures of status do not correspond to greater happiness, measures of sociometric status (status compared to people encountered face-to-face on a daily basis) do correlate to increased subjective well-being, above and beyond the effects of extroversion and other factors.\n\nThe Easterlin Paradox also suggests that there is no connection between a society's economic development and its average level of happiness. Through time, the Easterlin has looked at the relationship between happiness and Gross Domestic Product (GDP) across countries and within countries. There are three different phenomena to look at when examining the connection between money and Subjective well-being; rising GDP within a country, relative income within a country, and differences in GDP between countries.\n\nMore specifically, when making comparisons between countries, a principle called the Diminishing Marginal Utility of Income (DMUI) stands strong. Veenhoven (1991) said, \"[W]e not only see a clear positive relationship [between happiness and GNP per capita], but also a curvilinear pattern; which suggest that wealth is subject to a law of diminishing happiness returns.\" Meaning a $1,000 increase in real income, becomes progressively smaller the higher the initial level of income, having less of an impact on subjective well-being. Easterlin (1995) proved that the DMUI is true when comparing countries, but not when looking at rising gross domestic product within countries.\n\nThere are substantial positive associations between health and SWB so that people who rate their general health as \"good\" or \"excellent\" tend to experience better SWB compared to those who rate their health as \"fair\" or \"poor\". A meta-analysis found that self-ratings of general health were more strongly related to SWB than physician ratings of health. The relationship between health and SWB may be bidirectional. There is evidence that good subjective well-being contributes to better health.\nA review of longitudinal studies found that measures of baseline subjective well-being constructs such as optimism and positive affect predicted longer-term health status and mortality. Conversely, a number of studies found that baseline depression predicted poorer longer-term health status and mortality. Baseline health may well have a causal influence on subjective well-being so causality is difficult to establish.\nA number of studies found that positive emotions and optimism had a beneficial impact on cardiovascular health and on immune functioning. Changes in mood are also known to be associated with changes in immune and cardiovascular response.\nThere is evidence that interventions that are successful in improving subjective well-being can have beneficial effects on aspects of health. For example, meditation and relaxation training have been found to increase positive affect and to reduce blood pressure. The effect of specific types of subjective well-being is not entirely clear. For example, how durable the effects of mood and emotions on health are remains unclear. Whether some types of subjective well-being predict health independently of others is also unclear. Meditation has the power to increase happiness because it can improve self-confidence and reduces anxiety, which increases your well-being. Cultivating personal strengths and resources, like humour, social/animal company, and daily occupations, also appears to help people preserve acceptable levels of SWB despite the presence of symptoms of depression, anxiety, and stress.\n\nResearch suggests that probing a patient's happiness is one of the most important things a doctor can do to predict that patient's health and longevity. In health-conscious modern societies, most people overlook the emotions as a vital component of one's health, while over focusing on diet and exercise. According to Diener & Biswas-Diener, people who are happy become less sick than people who are unhappy. There are three types of health: morbidity, survival, and longevity. Evidence suggests that all three can be improved through happiness:\n\n\nA positive relationship has been found between the volume of gray matter in the right precuneus area of the brain, and the subject's subjective happiness score. A 6 week mindfulness based intervention was found to correlate with a significant gray matter increase within the precuneus.\n\nThere are a number of domains that are thought to contribute to subjective well-being. In a study by Hribernik and Mussap (2010), leisure satisfaction was found to predict unique variance in life satisfaction, supporting its inclusion as a distinct life domain contributing to subjective well-being. Additionally, relationship status interacted with age group and gender on differences in leisure satisfaction. The relationship between leisure satisfaction and life satisfaction, however, was reduced when considering the impact of core affect (underlying mood state). This suggests that leisure satisfaction may primarily be influenced by an individual's subjective well-being level as represented by core affect. This has implications for possible limitations in the extent to which leisure satisfaction may be improved beyond pre-existing levels of well-being and mood in individuals.\n\nAlthough all cultures seem to value happiness, cultures vary in how they define happiness. There is also evidence that people in more individualistic cultures tend to rate themselves as higher in subjective well-being compared to people in more collectivistic cultures.\n\nIn Western cultures, predictors of happiness include elements that support personal independence, a sense of personal agency, and self-expression. In Eastern cultures, predictors of happiness focus on an interdependent self that is inseparable from significant others. Compared to people in individualistic cultures, people in collectivistic cultures are more likely to base their judgments of life satisfaction on how significant others appraise their life than on the balance of inner emotions experienced as pleasant versus unpleasant. Pleasant emotional experiences have a stronger social component in East Asian cultures compared to Western ones. For example, people in Japan are more likely to associate happiness with interpersonally engaging emotions (such as friendly feelings), whereas people in the United States are more likely to associate happiness with interpersonally disengaging emotions (pride, for example). There are also cultural differences in motives and goals associated with happiness. For example, Asian Americans tend to experience greater happiness after achieving goals that are pleasing to or approved of by significant others compared to European Americans. There is also evidence that high self-esteem, a sense of personal control and a consistent sense of identity relate more strongly to SWB in Western cultures than they do in Eastern ones. However, this is not to say that these things are unimportant to SWB in Eastern cultures. Research has found that even within Eastern cultures, people with high self-esteem and a more consistent sense of identity are somewhat happier than those who are low in these characteristics. There is no evidence that low self-esteem and so on are actually beneficial to SWB in any known culture.\n\nA large body of research evidence has confirmed that people in individualistic societies report higher levels of happiness than people in collectivistic ones and that socioeconomic factors alone are insufficient to explain this difference. In addition to political and economic differences, individualistic versus collectivistic nations reliably differ in a variety of psychological characteristics that are related to SWB, such as emotion norms and attitudes to the expression of individual needs. Collectivistic cultures are based around the belief that the individual exists for the benefit of the larger social unit, whereas more individualistic cultures assume the opposite. Collectivistic cultures emphasise maintaining social order and harmony and therefore expect members to suppress their personal desires when necessary in order to promote collective interests. Such cultures therefore consider self-regulation more important than self-expression or than individual rights. Individualistic cultures by contrast emphasise the inalienable value of each person and expect individuals to become self-directive and self-sufficient. Although people in collectivistic cultures may gain happiness from the social approval they receive from suppressing self-interest, research seems to suggest that self-expression produces a greater happiness \"payoff\" compared to seeking approval outside oneself.\n\nPositive psychology is particularly concerned with the study of SWB. Positive psychology was founded by Seligman and Csikszentmihalyi (2000) who identified that psychology is not just the study of pathology, weakness, and damage; but it is also the study of strength and virtue. Researchers in positive psychology have pointed out that in almost every culture studied the pursuit of happiness is regarded as one of the most valued goals in life. Understanding individual differences in SWB is of key interest in positive psychology, particularly the issue of why some people are happier than others. Some people continue to be happy in the face of adversity whereas others are chronically unhappy at the best of times. \n\nPositive psychology has investigated how people might improve their level of SWB and maintain these improvements over the longer term, rather than returning to baseline. Lyubomirsky (2001) argued that SWB is influenced by a combination of personality/genetics (studies have found that genetic influences usually account for 35-50% of the variance in happiness measures), external circumstances, and activities that affect SWB. She argued that changing one's external circumstances tends to have only a temporary effect on SWB, whereas engaging in activities (mental and/or physical) that enhance SWB can lead to more lasting improvements in SWB.\nSWB is often used in appraising the wellbeing of populations.\n\n\n"}
{"id": "50166890", "url": "https://en.wikipedia.org/wiki?curid=50166890", "title": "Timeline of global health", "text": "Timeline of global health\n\nThis page is a timeline of global health, including major conferences, interventions, cures, and crises.\n\nDuring this pre-WWII era, there are three big trends that operate separately, but sometimes affect each other in development and outcomes.\n\nFirst, a trend of urbanization (fueled by the Industrial Revolution) as well as greater global trade and migration leads to new challenges, including those in urban sanitation and infectious diseases/pandemics. Six global cholera pandemics happen in this period because of increased commerce and migration.\n\nSecond, there is a lot of development on the underlying theory of disease, advancements in vaccine and antibiotic development, and a variety of experimental large-scale eradication and control programs. One big example: the germ theory of diseases begins to become accepted and popularized starting around 1850. Another big example is the development of the smallpox vaccine by Edward Jenner in 1796. Systematic eradication and control efforts include the Rockefeller Sanitary Commission and efforts to eradicate smallpox. Antitoxins and vaccines for numerous diseases including cholera and tuberculosis are developed during this period, building on a trend of greater understanding of and control over microorganisms.\n\nA third theme during this era is the formation of various preliminary international alliances and conferences, including the International Sanitary Conferences, Pan American Health Organization, Office International d'Hygiène Publique, and the League of Nations Health Committee. This is closely intertwined with the other two trends. For instance, the cholera pandemics mentioned above, as well as the growing scientific understanding of the germ theory of disease, are both key impetuses for the International Sanitary Conferences.\n\nFollowing the end of World War II, the first batch of big organizations, both international and national (with international cooperation), including the United Nations and World Health Organization (WHO), form. Beginning with the United Nations Relief and Rehabilitation Administration for relief of victims of war in 1943, there is a big push to begin creating large scale health initiatives, non-governmental organizations, and worldwide global health programs by the United Nations to improve quality of life around the world. UNICEF, the World Health Organization, as well as the UNRRA are all part of United Nations efforts to benefit global health beginning with developing countries. These various programs aim to aid in economic endeavors by providing loans, direct disease prevention programs, health education, etc.\n\nAfter wrapping up complications caused by the end of the war, there is an international energy put in into eradication, beginning with the complete smallpox eradication in 1979. There is greater dissatisfaction with WHO for its focus on disease/infection control at the expense of trying to improve general living conditions, as well as disappointment at its low budget and staffing. This atmosphere spurs other organizations to provide their own forms of aid. The Alma Ata Declaration and selective primary healthcare are created to express urgent action by all governments and citizens to protect and promote the health of all people equally. More organizations form following these new active attitudes toward global health, including the International Agency for Research on Cancer and the Doctors Without Borders organization. Publications like the WHO Model List of Essential Medicines highlight basic medicines required by most adults and children to survive, and set priorities for healthcare fund allocation in developing countries. Generally, there is more buy-in for the idea that direct, targeted efforts to address healthcare could be worthwhile and benefit many countries.\n\nCertain specific efforts increase in efficiency and productivity, including improvement in maternal and child health and a focus on HIV/AIDS, tuberculosis, and malaria (the 'Big Three') in developing countries. During this time period, the child survival revolution (CSR), which helps reduce child mortality in the developing world, and GOBI-FFF are both advocated by James P. Grant. The World Summit for Children also takes place, becoming one of the largest ever gathering of heads of states and government to commit a set of goals to improve the well-being of children. Finally, HIV/AIDS becomes the focus of many governmental and non-governmental organizations, leading to the formation of the Global Programme on AIDS (GPA) by efforts of the World Health Organization. However, these health organizations also make significant advancements to tuberculosis treatments, including the DOTS strategy and the formation of the Stop TB Partnership.\n\nUN's Millennium Development Goals establishes health care as an important goal (not just combating infectious diseases). Later in 2015, the Sustainable Development Goals build on the MDGs to outline the objectives that will transform our world by ending poverty, helping the environment, and improving health and education. More specific disease-targeting organizations are created primarily to fund healthcare plans in developing countries, including the President's Emergency Plan for AIDS Relief and The Global Fund to Fight AIDS, Tuberculosis and Malaria. These organizations (especially the WHO) adopt new strategies and initiatives, including the 3 by 5 Initiative to widen the access to antiretroviral treatment, the WHO Framework Convention on Tobacco Control, etc. Private large donors such as the Bill & Melinda Gates Foundation begin to play an important role in shaping the funding landscape and direction of efforts in global health.\n\nThe following events are selected for inclusion in the timeline:\n\n\nWe do \"not\" include:\n\n\n\n\n\n"}
{"id": "51099085", "url": "https://en.wikipedia.org/wiki?curid=51099085", "title": "To Save Humanity", "text": "To Save Humanity\n\nTo Save Humanity is a 2015 collection of 96 essays on global health from a collection of authors who range from heads of states, movie stars, scientists at leading universities, activists, and Nobel Prize winners. Each contributor was asked the same question: \"What is the single most important thing for the future of global health over the next fifty years?\" The collection was edited by Julio Frenk and Steven J. Hoffman.\n\nThe Global Strategy Lab called the collection \"unparalleled\" and \"a primer on the major issues of our time and a blueprint for post-2015 health and development,\" and featured it in their annual conference.\n\nThe Health Impact Fund also featured the collection at their conference.\n\nThe Lancet described the book as \"testimony to the complexity of global health politics,\" and called it \"a reminder that the breadth of individual and institutional engagement with global health cannot be fully captured by one set of global goals.\"\n\nVox has republished several of the articles for free online as part of a series entitled \"One Change to Save the World.\"\n"}
{"id": "275206", "url": "https://en.wikipedia.org/wiki?curid=275206", "title": "Vaginismus", "text": "Vaginismus\n\nVaginismus is a condition in which involuntary muscle spams prevents vaginal penetration. This often results in pain with attempts at sex. Often it begins when sexual intercourse is first attempted.\nThe underlying cause is generally a fear that penetration will hurt. Risk factors include a history of sexual assault, endometriosis, vaginitis, or a prior episiotomy. Diagnosis is based on the symptoms and examination. It requires there to be no anatomical or physical problems and a desire for penetration on the part of the women.\nTreatment may include behavior therapy such as graduated exposure therapy and gradual vaginal dilatation. Surgery is not generally indicated. Botulinum toxin is being studied. About 0.5% of women are affected. Outcomes are generally good with treatment.\n\nSeverity as well as the pain during penetration varies between women.\n\nA woman is said to have primary vaginismus when she is unable to have penetrative sex or experience vaginal penetration without pain. It is commonly discovered in teenage girls and women in their early twenties, as this is when many girls and young women first attempt to use tampons, have penetrative sex, or undergo a Pap smear. Women with vaginismus may be unaware of the condition until they attempt vaginal penetration. A woman may be unaware of the reasons for their condition.\n\nA few of the main factors that may contribute to primary vaginismus include:\n\n\nPrimary vaginismus is often idiopathic.\n\nVaginismus has been classified by Lamont according to the severity of the condition. Lamont describes four degrees of vaginismus: In first degree vaginismus, the patient has spasm of the pelvic floor that can be relieved with reassurance. In second degree, the spasm is present but maintained throughout the pelvis even with reassurance. In third degree, the patient elevates the buttocks to avoid being examined. In fourth degree vaginismus (also known as grade 4 vaginismus), the most severe form of vaginismus, the patient elevates the buttocks, retreats and tightly closes the thighs to avoid examination. Pacik expanded the Lamont classification to include a fifth degree in which the patient experiences a visceral reaction such as sweating, hyperventilation, palpitations, trembling, shaking, nausea, vomiting, losing consciousness, wanting to jump off the table, or attacking the doctor. The Lamont classification continues to be used to the present and allows for a common language among researchers and therapists.\n\nAlthough the pubococcygeus muscle is commonly thought to be the primary muscle involved in vaginismus, Pacik identified two additionally-involved spastic muscles in treated patients under sedation. These include the entry muscle (bulbocavernosum) and the mid-vaginal muscle (puborectalis). Spasm of the entry muscle accounts for the common complaint that patients often report when trying to have intercourse: \"It's like hitting a brick wall\".\n\nSecondary vaginismus occurs when a person who has previously been able to achieve penetration develops vaginismus. This may be due to physical causes such as a yeast infection or trauma during childbirth, while in some cases it may be due to psychological causes, or to a combination of causes. The treatment for secondary vaginismus is the same as for primary vaginismus, although, in these cases, previous experience with successful penetration can assist in a more rapid resolution of the condition. Peri-menopausal and menopausal vaginismus, often due to a drying of the vulvar and vaginal tissues as a result of reduced estrogen, may occur as a result of \"micro-tears\" first causing sexual pain then leading to vaginismus.\n\nFurther factors that may contribute to either secondary or primary vaginismus include:\n\nWhich muscles are involved is unclear but may include the pubococcygeus muscle, levator ani, bulbocavernosus, circumvaginal, and perivaginal muscles.\n\nA Cochrane review found little high quality evidence regarding the treatment of vaginismus in 2012. Specifically it is unclear if systematic desensitisation is better than other measures including nothing.\n\nAccording to Ward and Ogden's qualitative study on the experience of vaginismus (1994), the three most common contributing factors to vaginismus are fear of painful sex; the belief that sex is wrong or shameful (often the case with patients who had a strict religious upbringing); and traumatic early childhood experiences (not necessarily sexual in nature).\n\nPeople with vaginismus are twice as likely to have a history of childhood sexual interference and held less positive attitudes about their sexuality, whereas no correlation was noted for lack of sexual knowledge or (non-sexual) physical abuse.\n\nOften, when faced with a person experiencing painful intercourse, a gynecologist will recommend Kegel exercises and provide some additional lubricants. Strengthening the muscles that unconsciously tighten during vaginismus may be extremely counter-intuitive for some people. Although vaginismus has not been shown to affect a person's ability to produce lubrication, providing additional lubricant can be helpful in achieving successful penetration. This is due to the fact that women may not produce natural lubrication if anxious or in pain. Treatment of vaginismus may involve the use Hegar dilators, (sometimes called vaginal trainers) progressively increasing the size of the dilator inserted into the vagina.\n\nBotulinum toxin A (Botox) has been considered as a treatment option, under the idea of temporarily reducing the hypertonicity of the pelvic floor muscles. Although no random controlled trials have been done with this treatment, experimental studies with small samples have shown it to be effective, with sustained positive results through 10 months. Similar in its mechanism of treatment, lidocaine has also been tried as an experimental option.\n\nAnxiolytics and antidepressants are other pharmacotherapies that have been offered to people in conjunction with other psychotherapy modalities, or if these patients experience high levels of anxiety from their condition. Results from these medications have not been consistent.\n\nTrue epidemiological studies of vaginismus have not been done, as diagnosis would require painful examinations that such women would most likely avoid. Data available is primarily reported statistics from clinical settings.\n\nA study of vaginismus in people in Morocco and Sweden found a prevalence of 6%. 18-20% of people in British and Australian studies were found to have manifest dyspareunia, while the rate among elderly British people was as low as 2%.\n\nA 1990 study of people presenting to sex therapy clinics found reported vaginismus rates of between 12% and 17%, while a random sampling and structured interview survey conducted in 1994 by National Health and Sexual Life Survey documented 10%-15% of people reported that in the past six months they had experienced pain during intercourse.\n\nThe most recent study-based estimates of vaginismus incidence range from 5% to 47% of people presenting for sex therapy or complaining of sexual problems, with significant differences across cultures. It seems likely that a society's expectations of person's sexuality may particularly impact on the people with the condition.\n\n"}
{"id": "39327248", "url": "https://en.wikipedia.org/wiki?curid=39327248", "title": "Well-woman examination", "text": "Well-woman examination\n\nA well woman examination is an exam offered to women to review elements of their reproductive health. It is recommended once a year for most women. The exam includes a breast examination, a pelvic examination and a pap smear but may also include other procedures. Hospitals employ strict policies relating to the provision of consent by the patient, the availability of chaperones at the examination, and the absence of other parties.\n\nMost healthcare providers also allow the patient to specify if they have any preferences towards the examiner's gender.\n\nThe well-woman examination by a medical professional is recommended at least once a year to women over 18 years old and/or women who are sexually active. Its importance lies in identifying potential early health problems. The most important tests included in an examination is the breast exam, pelvic exam and the pap test, although some doctors consider other tests in the examination, including measurement of blood pressure, HIV testing, and other laboratory tests such as urinalysis, CBC (Complete blood count) and testing for other sexually transmitted diseases. The procedure is important also to detect certain cancers, especially breast and cervical cancer.\n\nThe breast examination begins with a visual inspection. With the patient in a prone or seated position, the medical professional will look at both breasts to check the color, symmetry, dimensions according to age, lean body mass, the physiological (pregnancy and lactation) and race, looking for abnormalities, such as bulges and shrinkage. One of these abnormalities is changed in the areola or nipple. If it is flattened or retracted (umbilicated), it is necessary to consider the possibility of a cancerous lesion which has caused the malformation.\n\nNext, the breasts are palpated, again with the patient lying or sitting. The patient has to lift the arm and put one hand behind her head. With this position, the entire gland is palpated. It is also important to examine the armpits, because of masses that may be found there. The test is executed pressing the gland with two or three fingers against the chest wall, making a radial route or by quadrants. The nipple is also squeezed check for secretions, such as secretion of milk (galactorrhea), serous, blood or purulent secretions. If a node is detected, it is necessary to determine its place, size, shape, edges, consistency and sensitivity.\n\nIn addition to the yearly check by a professional, women over the age of 18 should also perform this examination monthly.\nIt is important because regular and comprehensive examinations of the breasts can be used to find breast changes that occur between every clinical examination and detect early breast cancer. This auto examination should to be performed seven days after the onset of the menstrual period. If a woman finds a lump or notice any changes in her breast, she should seek medical attention promptly.\n\nA mammogram or mammography is a special x-ray of the breasts. They are the procedure most likely to detect early breast cancer in asymptomatic women. Mammograms can show tumors long before they are large enough to palpate. They are recommended for women who have symptoms of breast cancer or who are at increased risk of developing the disease. They are performed with the patient standing, the breast pressed between two plastic plates, as the image is taken. The interpretation has to be performed by a specialist.\n\nBreast ultrasound is a complementary study of mammography. In many women the tissue that makes up the breast is very dense, representing fibrous tissue and glandular tissue, which produces milk during lactation. This limits the radiologist interpreting the study, so, in these cases, the ultrasound is helpful, since this is capable of distinguishing tumors in women with dense breast tissue, where identification is otherwise difficult. Additionally, it is advisable to follow up a mammogram that shows indications of tumors with an ultrasound, to confirm, before more invasive procedures are undertaken.\n\nThe pelvic exam is part of the physical examination of the pelvic area of a woman, which generally also includes the taking of a sample for a pap smear. This test includes three parts. These are the general inspection of the external genitalia, bimanual examination, and inspection of the vaginal canal using a speculum.\n\nThe patient is placed in a supine position on a special examination table, which has two protrusions called \"stirrups.\" With the feet in these stirrups, the legs are placed in a position such that the medical professional can access the pelvic area. The external genitalia is examined first, looking for abnormalities like lesions, ulcers, warts and color changes. The elements of this exam include the vulva, which contains the mons pubis, of which there are two longitudinal folds of skin forming the labia majora; then the labia minora and hair follicles. The clitoral hood is also checked.\n\nThe purpose of this exam is to palpate organs which cannot be seen with visual inspection. The index and middle finger are inserted into the vagina. This maneuver allows the doctor to check the orientation, length and width of the vagina. Next, the cervix and vaginal fornices are palpated, to check position, size, consistency, mobility and sensibility. The other hand is placed in the pubis pressing it to feel the uterus between both hands. The most important characteristics to examine are the size of the uterus, presence of nodes or agglomerations, consistency, size, tilt, and mobility. With this technique, the ovaries are also palpable.\n\nThe speculum examination is recommended for only women over 21 years old, irrespective of her sexual activity. The speculum is an instrument made of metal or plastic and is constructed with two flaps. Its purpose is to separate and widen the vaginal opening and keep it open. This allows direct observation by the physician into the vaginal canal with the help of a lamp or a mirror.\n\nThere are different types of speculums used within the different characteristics of each patient such as age, sex life, and other factors. The first step is to open the vaginal opening with two fingers at the vulvo-perineal angle, then separate the fingers slightly and press down, then insert the speculum arranging the width of the tip of the flaps in anteroposterior. Then the speculum is moved into the vagina at an angle of 45°, following the natural contour of the posterior vaginal wall. When the speculum is in place, the fingers are removed and the device is rotated so that the flaps are horizontal. The flaps are then separated and locked into place when the cervical neck is completely visible.\n\nThe pap smear is a screening test to test for abnormalities such as cervical cancer and human papillomavirus infections, which require early treatment. To be viable, the patient should not be menstruating and had not used a vaginal contraceptive in the prior 24–48 hours. The procedure begins with the scraping of cells from the cervix and the uterine fornix, done during the speculum examination as there is access to the cervix. The scraping is done with a spatula, cervical brush or swab. Some women experience temporary bleeding from this procedure. The scrapings are placed on a slide, covered with a fixative for later examination under a microscope to determine if they are normal or abnormal.\n"}
