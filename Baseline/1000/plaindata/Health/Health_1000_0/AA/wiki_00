{"id": "42608368", "url": "https://en.wikipedia.org/wiki?curid=42608368", "title": "Admission, discharge, and transfer system", "text": "Admission, discharge, and transfer system\n\nAn admission, discharge, and transfer (ADT) system is a backbone system for the structure of other types of business systems. An ADT system is one of four types of core business systems: ADT, financial, scheduling, and acuity (McGonigle, D., & Mastrain, K., 2012). Core business systems are systems used in a health care facility for financial payment, quality improvement, and encouraging best practices that research has proven beneficial. \n\nUsed in health care, an ADT system is usually the foundation for other types of health care information systems because it holds valuable patient information such as a medical record number, age, name, and contact information. Using the ADT system, patient information can be shared, when appropriate, with other health care facilities and systems (McGonigle, D., & Mastrain, K., 2012). ADT systems can also be used as an alert system upon a patient's admission (Pittet, D., Safran, E., et al., 1996). This can be helpful if a patient has had a history of an infectious disease or heart ailments. When admitted, the ADT system may alert the admitting staff that the patient needs to be in an isolation room or on a cardiac floor, for example.\n\n"}
{"id": "41901406", "url": "https://en.wikipedia.org/wiki?curid=41901406", "title": "American College of Occupational and Environmental Medicine", "text": "American College of Occupational and Environmental Medicine\n\nAmerican College of Occupational and Environmental Medicine is a United States-based professional society which promotes health care professionals in the field of occupational safety and health.\n\nIn the early 1990s the ACOEM expressed concern about a shortage of physicians in its field.\n\n"}
{"id": "21492663", "url": "https://en.wikipedia.org/wiki?curid=21492663", "title": "Asbestos", "text": "Asbestos\n\nAsbestos is a set of six naturally occurring silicate minerals, which all have in common their eponymous asbestiform habit: i.e. long (roughly 1:20 aspect ratio), thin fibrous crystals, with each visible fiber composed of millions of microscopic \"fibrils\" that can be released by abrasion and other processes. They are commonly known by their colors, as \"blue asbestos\", \"brown asbestos\", \"white asbestos\", and \"green asbestos\".\n\nAsbestos mining existed more than 4,000 years ago, but large-scale mining began at the end of the 19th century, when manufacturers and builders began using asbestos for its desirable physical properties. Some of those properties are sound absorption, average tensile strength, affordability, and resistance to fire, heat, and electricity. It was used in such applications as electrical insulation for hotplate wiring and in building insulation. When asbestos is used for its resistance to fire or heat, the fibers are often mixed with cement or woven into fabric or mats. These desirable properties made asbestos very widely used. Asbestos use continued to grow through most of the 20th century until public knowledge of the health hazards of asbestos dust led to its outlawing by courts and legislatures in mainstream construction and fireproofing in most countries.\n\nInhalation of asbestos fibers can cause serious and fatal illnesses including lung cancer, mesothelioma, and asbestosis (a type of pneumoconiosis). Concern of asbestos-related illness in modern times began with the 20th century and escalated during the 1920s and 1930s. By the 1980s and 1990s, asbestos trade and use were heavily restricted, phased out, or banned outright in an increasing number of countries.\n\nDespite the severity of asbestos-related diseases, the material has extremely widespread use in many areas. Continuing long-term use of asbestos after harmful health effects were known or suspected, and the slow emergence of symptoms decades after exposure ceased, made asbestos litigation the longest, most expensive mass tort in U.S. history though a much lesser legal issue in most other countries involved. Asbestos-related liability also remains an ongoing concern for many manufacturers, insurers and reinsurers. On July 12, 2018, a Missouri jury ordered Johnson & Johnson to pay a record $4.69 billion to 22 women who alleged the company’s talc-based products, including its baby powder, contain asbestos and caused them to develop ovarian cancer.\nAsbestos derives from the ancient Greek ἄσβεστος, meaning “unquenchable” or “inextinguishable”. The name reflects use of the substance for wicks that would never burn up. The word is pronounced , or .\n\nSix mineral types are defined by the United States Environmental Protection Agency (EPA) as \"asbestos\" including those belonging to the serpentine class and those belonging to the amphibole class. All six asbestos mineral types are known to be human carcinogens. The visible fibers are themselves each composed of millions of microscopic \"fibrils\" that can be released by abrasion and other processes.\n\nSerpentine class fibers are curly. Chrysotile is the only member of the serpentine class.\nChrysotile, CAS No. 12001-29-5, is obtained from serpentinite rocks which are common throughout the world. Its idealized chemical formula is Mg(SiO)(OH). Chrysotile appears under the microscope as a white fiber.\n\nChrysotile has been used more than any other type and accounts for about 95% of the asbestos found in buildings in America. Chrysotile is more flexible than amphibole types of asbestos, and can be spun and woven into fabric. The most common use was corrugated asbestos cement roofing primarily for outbuildings, warehouses and garages. It may also be found in sheets or panels used for ceilings and sometimes for walls and floors. Chrysotile has been a component in joint compound and some plasters. Numerous other items have been made containing chrysotile including brake linings, fire barriers in fuseboxes, pipe insulation, floor tiles, residential shingles, and gaskets for high temperature equipment.\n\nAmphibole class fibers are needle-like. Amosite, crocidolite, tremolite, anthophyllite and actinolite are members of the amphibole class.\n\nAmosite, CAS No. 12172-73-5, often referred to as brown asbestos, is a trade name for the amphiboles belonging to the cummingtonite-grunerite solid solution series, commonly from South Africa, named as a partial acronym for \"Asbestos Mines of South Africa\". One formula given for amosite is FeSiO(OH). Amosite is seen under a microscope as a grey-white vitreous fiber. It is found most frequently as a fire retardant in thermal insulation products, asbestos insulating board and ceiling tiles.\n\nCrocidolite, CAS No. 12001-28-4, commonly known as blue asbestos, is the fibrous form of the amphibole riebeckite, found primarily in southern Africa, but also in Australia and Bolivia. One formula given for crocidolite is NaFeFeSiO(OH). Crocidolite is seen under a microscope as a blue fiber.\n\nCrocidolite commonly occurs as soft friable fibers. Asbestiform amphibole may also occur as soft friable fibers but some varieties such as amosite are commonly straighter. All forms of asbestos are fibrillar in that they are composed of fibers with breadths less than 1 micrometer in bundles of very great widths. Asbestos with particularly fine fibers is also referred to as \"amianthus\".\n\nOther regulated asbestos minerals, such as tremolite asbestos, CAS No. 77536-68-6, CaMgSiO(OH); actinolite asbestos, CAS No. 77536-66-4, Ca(Mg,Fe)(SiO)(OH); and anthophyllite asbestos, CAS No. 77536-67-5, (Mg,Fe)SiO(OH); are less commonly used industrially but can still be found in a variety of construction materials and insulation materials and have been used in a few consumer products.\n\nOther natural asbestiform minerals, such as richterite, Na(CaNa)(Mg,Fe)(SiO)(OH), and winchite, (CaNa)Mg(Al,Fe)(SiO)(OH), though not regulated, are said by some to be no less harmful than tremolite, amosite, or crocidolite. They are termed \"asbestiform\" rather than asbestos. Although the U.S. Occupational Safety and Health Administration (OSHA) has not included them in the asbestos standard, NIOSH and the American Thoracic Society have recommended them for inclusion as regulated materials because they may also be hazardous to health.\n\nIn 2009, about 9% of the world's asbestos production was mined in Canada. In late 2011, Canada's remaining two asbestos mines, both located in Quebec, halted operations. In September 2012, the Quebec government halted asbestos mining.\n\nIn 2015, 2 million tonnes of asbestos was mined worldwide. Russia was the largest producer with about 55% of the world total, followed by China (20%), Brazil (15.6%), and Kazakhstan (10.8%).\n\nPeople have used asbestos for thousands of years to create flexible objects, such as napkins, that resist fire. In the modern era, companies began producing asbestos consumer goods on an industrial scale. Now people recognize the health hazard that asbestos poses, and it is banned or strictly regulated around the world. \nAsbestos use dates back at least 4,500 years, when the inhabitants of the Lake Juojärvi region in East Finland strengthened earthenware pots and cooking utensils with the asbestos mineral anthophyllite (see Asbestos-ceramic). One of the first descriptions of a material that may have been asbestos is in Theophrastus, \"On Stones\", from around 300 BC, although this identification has been questioned. In both modern and ancient Greek, the usual name for the material known in English as \"asbestos\" is \"amiantos\" (\"undefiled\", \"pure\"), which was adapted into the French \"amiante\" and Portuguese \"amianto\". In modern Greek, the word ἀσβεστος or ασβέστης stands consistently and solely for lime.\n\nThe term \"asbestos\" is traceable to Roman naturalist Pliny the Elder's manuscript \"Natural History\", and his use of the term \"asbestinon\", meaning \"unquenchable\". While Pliny or his nephew Pliny the Younger is popularly credited with recognising the detrimental effects of asbestos on human beings, examination of the primary sources reveals no support for either claim.\n\nWealthy Persians amazed guests by cleaning a cloth by exposing it to fire. For example, according to Tabari, one of the curious items belonging to Khosrow II Parviz, the great Sassanian king (r. 531–579), was a napkin () that he cleaned simply by throwing it into fire. Such cloth is believed to have been made of asbestos imported over the Hindu Kush. According to Biruni in his book, \"Gems\", any cloths made of asbestos (, \"āzarshost\") were called \"shostakeh\" (). Some Persians believed the fiber was the fur of an animal, called the \"samandar\" (), which lived in fire and died when exposed to water, which was where the former belief that the salamander could tolerate fire originated.\n\nCharlemagne, the first Holy Roman Emperor (800–814), is said to have had a tablecloth made of asbestos.\n\nMarco Polo recounts having been shown, in a place he calls \"Ghinghin talas\", \"a good vein from which the cloth which we call of salamander, which cannot be burnt if it is thrown into the fire, is made ...\"\n\nSome archaeologists believe that ancients made shrouds of asbestos, wherein they burned the bodies of their kings, in order to preserve only their ashes, and prevent them being mixed with those of wood or other combustible materials commonly used in funeral pyres. Others assert that the ancients used asbestos to make perpetual wicks for sepulchral or other lamps. A famous example is the golden lamp \"asbestos lychnis\", which the sculptor Callimachus made for the Erechtheion. In more recent centuries, asbestos was indeed used for this purpose. Although asbestos causes skin to itch upon contact, ancient literature indicates that it was prescribed for diseases of the skin, and particularly for the itch. It is possible that they used the term \"asbestos\" for soapstone, because the two terms have often been confused throughout history.\n\nThe large scale asbestos industry began in the mid-19th century. Early attempts at producing asbestos paper and cloth in Italy began in the 1850s, but were unsuccessful in creating a market for such products. Canadian samples of asbestos were displayed in London in 1862, and the first companies were formed in England and Scotland to exploit this resource. Asbestos was first used in the manufacture of yarn, and German industrialist Louis Wertheim adopted this process in his factories in Germany.\n\nIndustrial scale mining began in the Thetford hills, Quebec from the 1870s. Sir William Edmond Logan was the first to notice the large deposits of chrysotile in the hills in his capacity as head of Geological Survey of Canada. Samples of the minerals from here were displayed in London, and excited much interest. With the opening up of the Quebec Central Railway in 1876, mining entrepreneurs, such as Andrew Stuart Johnson established the asbestos industry in the province. The 50 ton output of the mines in 1878 rose to over 10,000 tons in the 1890s with the adoption of machine technologies and expanded production. For a long time, the world's largest asbestos mine was the Jeffrey mine in the town of Asbestos, Quebec.\nAsbestos production began in the Urals of the Russian Empire in the 1880s, and in the Alpine regions of Northern Italy with the formation in Turin of the Italo-English Pure Asbestos Company in 1876, although this was soon swamped by the greater production levels from the Canadian mines. Mining also took off in South Africa from 1893 under the aegis of the British businessman Francis Oates, the Director of the De Beers company. It was in South Africa that the production of amosite began in 1910. The U.S. asbestos industry had an early start in 1858, when fibrous anthophyllite was mined for use as asbestos insulation by the Johns Company, a predecessor to the current Johns Manville, at a quarry at Ward's Hill on Staten Island, New York. US production began in earnest in 1899, with the discovery of large deposits in the Belvidere Mountain.\n\nThe use of asbestos became increasingly widespread towards the end of the 19th century, when its diverse applications included fire retardant coatings, concrete, bricks, pipes and fireplace cement, heat, fire, and acid resistant gaskets, pipe insulation, ceiling insulation, fireproof drywall, flooring, roofing, lawn furniture, and drywall joint compound. In 2011 it was reported that over 50% of UK houses still contained asbestos, despite a ban on asbestos products some years earlier.\n\nIn Japan, particularly after World War II, asbestos was used in the manufacture of ammonium sulfate for purposes of rice production, sprayed upon the ceilings, iron skeletons, and walls of railroad cars and buildings (during the 1960s), and used for energy efficiency reasons as well. Production of asbestos in Japan peaked in 1974 and went through ups and downs until about 1990, when production began to drop dramatically.\n\nIn 1899, Montague Murray noted the negative health effects of asbestos. The first documented death related to asbestos was in 1906.\n\nIn the early 1900s researchers began to notice a large number of early deaths and lung problems in asbestos-mining towns. The first such study was conducted by H. Montague Murray at the Charing Cross Hospital, London, in 1900, in which a postmortem investigation of a young man who had died from pulmonary fibrosis after having worked for 14 years in an asbestos textile factory, discovered asbestos traces in the victim's lungs. Adelaide Anderson, the Inspector of Factories in Britain, included asbestos in a list of harmful industrial substances in 1902. Similar investigations were conducted in France and Italy, in 1906 and 1908, respectively.\nThe first diagnosis of asbestosis was made in the UK in 1924. Nellie Kershaw was employed at Turner Brothers Asbestos in Manchester, England from 1917, spinning raw asbestos fibre into yarn. Her death in 1924 led to a formal inquest. Pathologist William Edmund Cooke testified that his examination of the lungs indicated old scarring indicative of a previous, healed, tuberculosis infection, and extensive fibrosis, in which were visible \"particles of mineral matter ... of various shapes, but the large majority have sharp angles.\" Having compared these particles with samples of asbestos dust provided by S. A. Henry, His Majesty's Medical Inspector of Factories, Cooke concluded that they \"originated from asbestos and were, beyond a reasonable doubt, the primary cause of the fibrosis of the lungs and therefore of death\".\n\nAs a result of Cooke's paper, parliament commissioned an inquiry into the effects of asbestos dust by E. R. A. Merewether, Medical Inspector of Factories, and , a factory inspector and pioneer of dust monitoring and control. Their subsequent report, \"Occurrence of Pulmonary Fibrosis & Other Pulmonary Affections in Asbestos Workers\", was presented to parliament on 24 March 1930. It concluded that the development of asbestosis was irrefutably linked to the prolonged inhalation of asbestos dust, and included the first health study of asbestos workers, which found that 66% of those employed for 20 years or more suffered from asbestosis. The report led to the publication of the first Asbestos Industry Regulations in 1931, which came into effect on 1 March 1932. These regulated ventilation and made asbestosis an excusable work-related disease. The term mesothelioma was first used in medical literature in 1931; its association with asbestos was first noted sometime in the 1940s. Similar legislation followed in the U.S. about ten years later.\n\nApproximately 100,000 people in the United States have died, or are terminally ill, from asbestos exposure related to ship building. In the Hampton Roads area, a shipbuilding center, mesothelioma occurrence is seven times the national rate. Thousands of tons of asbestos were used in World War II ships to insulate piping, boilers, steam engines, and steam turbines. There were approximately 4.3 million shipyard workers in the United States during WWII; for every 1,000 workers about 14 died of mesothelioma and an unknown number died from asbestosis.\n\nThe United States government and asbestos industry have been criticized for not acting quickly enough to inform the public of dangers, and to reduce public exposure. In the late 1970s, court documents proved that asbestos industry officials knew of asbestos dangers since the 1930s and had concealed them from the public.\n\nIn Australia, asbestos was widely used in construction and other industries between 1946 and 1980. From the 1970s there was increasing concern about the dangers of asbestos, and its use was phased out. Mining ceased in 1983. The use of asbestos was phased out in 1989 and banned entirely in December 2003. The dangers of asbestos are now well known in Australia and there is help and support for sufferers from asbestosis or mesothelioma.\n\nSerpentine minerals have a sheet or layered structure. Chrysotile is the only asbestos mineral in the serpentine group. In the United States, chrysotile has been the most commonly used type of asbestos. According to the U.S. EPA Asbestos Building Inspectors Manual, chrysotile accounts for approximately 95% of asbestos found in buildings in the United States. Chrysotile is often present in a wide variety of products and materials, including:\n\n\nIn the European Union and Australia it has been banned as a potential health hazard and is not used at all. Japan is moving in the same direction, but at a slower pace.\n\nAmphiboles including amosite (brown asbestos) and crocidolite (blue asbestos) were formerly used in many products until the early 1980s. Tremolite asbestos constituted a contaminant of many if not all naturally occurring chrysotile deposits. The use of all types of asbestos in the amphibole group was banned in much of the Western world by the mid-1980s, and in Japan by 1995. Some products that included amphibole types of asbestos included the following:\n\n\nCigarette manufacturer Lorillard (Kent's filtered cigarette) used crocidolite asbestos in its \"Micronite\" filter from 1952 to 1956.\n\nWhile mostly chrysotile asbestos fibers were once used in automobile brake pads, shoes, and clutch discs, contaminants of amphiboles were present. Since approximately the mid-1990s, brake pads, new or replacement, have been manufactured instead with linings made of ceramic, carbon, metallic and aramid fiber (Twaron or Kevlar—the same material used in bulletproof vests).\n\nArtificial Christmas snow, known as flocking, was previously made with asbestos. It was used as an effect in films including \"The Wizard of Oz\" and department store window displays and it was marketed for use in private homes under brand names that included \"Pure White\", \"Snow Drift\" and \"White Magic\".\n\nThe use of asbestos in new construction projects has been banned for health and safety reasons in many developed countries or regions, including the European Union, Australia, Hong Kong, Japan, and New Zealand. A notable exception is the United States, where asbestos continues to be used in construction such as cement asbestos pipes. The 5th Circuit Court prevented the EPA from banning asbestos in 1991 because EPA research showed the ban would cost between $450 and 800 million while only saving around 200 lives in a 13-year timeframe, and that the EPA did not provide adequate evidence for the safety of alternative products. Until the mid-1980s, small amounts of white asbestos were used in the manufacture of Artex, a decorative stipple finish, however, some of the lesser-known suppliers of Artex were still adding white asbestos until 1999.\n\nPrior to the ban, asbestos was widely used in the construction industry in thousands of materials. Some are judged to be more dangerous than others due to the amount of asbestos and the material's friable nature. Sprayed coatings, pipe insulation and Asbestos Insulating Board (AIB) are thought to be the most dangerous due to their high content of asbestos and friable nature. Many older buildings built before the late 1990s contain asbestos. In the United States, there is a minimum standard for asbestos surveys as described by ASTM Standard E 2356–04. In the UK the Health and Safety Executive have issued guidance called HSG264 describing how surveys should be completed although other methods can be used if they can demonstrate they have met the regulations by other means.\nThe U.S. Environmental Protection Agency includes some but not all asbestos-contaminated facilities on the Superfund National Priorities list (NPL). Renovation and demolition of asbestos-contaminated buildings is subject to EPA NESHAP and OSHA Regulations. Asbestos is not a material covered under CERCLA's innocent purchaser defense. In the UK, the removal and disposal of asbestos and of substances containing it are covered by the Control of Asbestos Regulations 2006.\n\nU.S. asbestos consumption hit a peak of 804,000 tons in 1973; world asbestos demand peaked around 1977, with 25 countries producing nearly 4.8 million metric tons annually.\n\nIn older buildings (e.g. those built prior to 1999 in the UK, before white asbestos was banned), asbestos may still be present in some areas. Being aware of asbestos locations reduces the risk of disturbing asbestos.\n\nRemoval of asbestos building components can also remove the fire protection they provide, therefore fire protection substitutes are required for proper fire protection that the asbestos originally provided.\n\nSome countries, such as India, Indonesia, China, Russia and Brazil, have continued widespread use of asbestos. The most common is corrugated asbestos-cement sheets or \"A/C sheets\" for roofing and for side walls. Millions of homes, factories, schools or sheds and shelters continue to use asbestos. Cutting these sheets to size and drilling holes to receive 'J' bolts to help secure the sheets to roof framing is done on-site. There has been no significant change in production and use of A/C sheets in developing countries following the widespread restrictions in developed nations.\n\nAs New York City's World Trade Center collapsed following the September 11 attacks, Lower Manhattan was blanketed in a mixture of building debris and combustible materials. This complex mixture gave rise to the concern that thousands of residents and workers in the area would be exposed to known hazards in the air and in the dust, such as asbestos, lead, glass fibers, and pulverized concrete. More than 1,000 tons of asbestos are thought to have been released into the air following the buildings' destruction. Inhalation of a mixture of asbestos and other toxicants is thought to be linked to the unusually high death rate from cancer of emergency service workers since the disaster. Thousands more are now thought to be at risk of developing cancer due to this exposure with those who have died so far being only the \"tip of the iceberg\". Some commentators have criticised authorities for using asbestos in the buildings' construction.\n\nIn May 2002, after numerous cleanup, dust collection, and air monitoring activities were conducted outdoors by EPA, other federal agencies, New York City, and the state of New York, New York City formally requested federal assistance to clean and test residences in the vicinity of the World Trade Center site for airborne asbestos.\n\nVermiculite is a hydrated laminar magnesium-aluminum-iron silicate which resembles mica. It can be used for many industrial applications and has been used as insulation. Some deposits of vermiculite have been found to be contaminated with small amounts of asbestos.\n\nOne vermiculite mine operated by W. R. Grace and Company in Libby, Montana exposed workers and community residents to danger by mining vermiculite contaminated with asbestos, typically richterite, winchite, actinolite or tremolite. Vermiculite contaminated with asbestos from the Libby mine was used as insulation in residential and commercial buildings through Canada and the United States. W. R. Grace and Company's loose-fill vermiculite was marketed as Zonolite but was also used in sprayed-on products such as Monokote.\n\nIn 1999 the EPA began cleanup efforts in Libby and now the area is a Superfund cleanup area. The EPA has determined that harmful asbestos is released from the mine as well as through other activities that disturb soil in the area.\n\nTalc can sometimes be contaminated with asbestos due to the proximity of asbestos ore (usually tremolite) in underground talc deposits. By 1973, US federal law required all talc products to be asbestos-free, and today there is strict quality control in the production of talc products, separating cosmetic-grade talc (e.g. talcum powder) from industrial-grade talc (often used in friction products) has largely eliminated this issue for consumers.\n\nIn 2000, tests in a certified asbestos-testing laboratory found the tremolite form of amphibole asbestos used to be found in three out of eight popular brands of children's crayons that were made partly from talc: Crayola, Prang, and RoseArt. In Crayola crayons, the tests found asbestos levels around 0.05% in \"Carnation Pink\" and 2.86% in \"Orchid\"; in Prang crayons, the range was from 0.3% in \"Periwinkle\" to 0.54% in \"Yellow\"; in Rose Art crayons, it was from 0.03% in \"Brown\" to 1.20% in \"Orange\". Overall, 32 different types of crayons from these brands used to contain more than trace amounts of asbestos, and eight others contained trace amounts. The Art and Creative Materials Institute, a trade association which tested the safety of crayons on behalf of the makers, initially insisted the test results must have been incorrect, although they later said they do not test for asbestos. In May 2000, Crayola said tests by Richard Lee, a materials analyst whose testimony on behalf of the asbestos industry has been accepted in lawsuits over 250 times, found its crayons tested negative for asbestos. In spite of that, in June 2000 Binney & Smith, the maker of Crayola, and the other makers agreed to stop using talc in their products, and changed their product formulations in the United States.\n\nThe mining company, R T Vanderbilt Co of Gouverneur, New York, which supplied the talc to the crayon makers, states that \"to the best of our knowledge and belief\" there is no asbestos in its talc. However media reports claim that the United States Mine Safety and Health Administration (MSHA) had found asbestos in four talc samples tested in 2000. The Assistant Secretary for Mine Safety and Health subsequently wrote to the news reporter, stating that \"In fact, the abbreviation ND (non detect) in the laboratory report – indicates no asbestos fibers actually were found in the samples.\" Multiple studies by mineral chemists, cell biologists, and toxicologists between 1970 and 2000 found neither samples of asbestos in talc products nor symptoms of asbestos exposure among workers dealing with talc, but more recent work has rejected these conclusions in favor of \"same as\" asbestos risk.\n\nOn July 12, 2018, a Missouri jury ordered Johnson & Johnson to pay a record $4.69 billion to 22 women who alleged the company’s talc-based products, including its baby powder, contain asbestos and caused them to develop ovarian cancer.\n\nAll types of asbestos fibers are known to cause serious health hazards in humans. Amosite and crocidolite are considered the most hazardous asbestos fiber types; however, chrysotile asbestos has also produced tumors in animals and is a recognized cause of asbestosis and malignant mesothelioma in humans, and mesothelioma has been observed in people who were occupationally exposed to chrysotile, family members of the occupationally exposed, and residents who lived close to asbestos factories and mines.\n\nDuring the 1980s and again in the 1990s it was suggested at times that the process of making asbestos cement could \"neutralize\" the asbestos, either via chemical processes or by causing cement to attach to the fibers and changing their physical size; subsequent studies showed that this was untrue, and that decades-old asbestos cement, when broken, releases asbestos fibers identical to those found in nature, with no detectable alteration.\n\nExposure to asbestos in the form of fibers is always considered dangerous. Working with, or exposure to, material that is friable, or materials or works that could cause release of loose asbestos fibers, is considered high risk. In general, people who become ill from inhaling asbestos have been regularly exposed in a job where they worked directly with the material.\n\nThe most common diseases associated with chronic exposure to asbestos are asbestosis and mesothelioma.\n\nFiberglass insulation was invented in 1938 and is now the most commonly used type of insulation material. The safety of this material has also been called into question due to similarities in material structure. However, the International Agency for Research on Cancer removed fiberglass from its list of possible human carcinogens in 2001 and a scientific review article from 2011 claimed epidemiology data was inconsistent and concluded that the IARC's decision to downgrade the carcinogenic potential of fiberglass was valid (however, this study was funded by sponsored research contract from the North American Insulation Manufacturer’s Association).\n\nIn 1978, a highly texturized fiberglass fabric was invented by Bal Dixit, called Zetex. This fabric is lighter than asbestos, but offers the same bulk, thickness, hand, feel, and abrasion resistance as asbestos. The fiberglass was texturized to eliminate some of the problems that arise with fiberglass, such as poor abrasion resistance and poor seam strength.\n\nIn Europe, mineral wool and glass wool are the main insulators in houses.\n\nMany companies that produced asbestos-cement products that were reinforced with asbestos fibers have developed products incorporating organic fibers. One such product was known as \"Eternit\" and another \"Everite\" now use \"Nutec\" fibers which consist of organic fibers, portland cement and silica. Cement-bonded wood fiber is another substitute. Stone fibers are used in gaskets and friction materials.\n\nAnother potential fiber is polybenzimidazole or PBI fiber. Polybenzimidazole fiber is a synthetic fiber with a high melting point of that also does not ignite. Because of its exceptional thermal and chemical stability, it is often used by fire departments and space agencies.\n\nAsbestos alternatives for industrial use include sleeves, rope, tape, fabric, textiles and insulation batt materials made from fiberglass and silica.\n\nIn most developed countries, asbestos is typically disposed of as hazardous waste in landfill sites.\n\nThe demolition of buildings containing large amounts of asbestos based materials pose particular problems for builders and property developers – such buildings often have to be deconstructed piece by piece, or the asbestos has to be painstakingly removed before the structure can be razed by mechanical or explosive means. One such example is the Red Road Flats in Glasgow, Scotland which used huge amounts of asbestos cement board for wall panelling – here British health and safety regulations stipulate that asbestos material has to be removed to a landfill site via an approved route at certain times of the day in specially adapted vehicles.\n\nIn the United States, the EPA governs the removal and disposal of asbestos strictly. Companies that remove asbestos must comply with EPA licensing. These companies are called EPA licensed asbestos contractors. Anytime one of these asbestos contractors performs work a test consultant has to conduct strict testing to ensure the asbestos is completely removed. \n\nAsbestos can be recycled by transforming it into harmless silicate glass. A process of thermal decomposition at produces a mixture of non-hazardous silicate phases, and at temperatures above it produces silicate glass. Microwave thermal treatment can be used in an industrial manufacturing process to transform asbestos and asbestos-containing waste into porcelain stoneware tiles, porous single-fired wall tiles, and ceramic bricks.\n\nThe combination of oxalic acid with ultrasound fully degrades chrysotile asbestos fibers.\n\n\n\n"}
{"id": "1995486", "url": "https://en.wikipedia.org/wiki?curid=1995486", "title": "Birth weight", "text": "Birth weight\n\nBirth weight is the body weight of a baby at its birth. The average birth weight in babies of European heritage is , though the range of normal is between and . Babies of south Asian and Chinese heritage weigh about \"less\". The birth weight of a baby is notable because very low birth weight babies are 100 times more likely to die compared to normal birth weight babies. As far as low birth weights prevalence rates changing over time, there has been a slight decrease from 7.9% (1970) to 6.8% (1980), then a slight increase to 8.3% (2006), to current levels of 8.2% (2016). The prevalence of low birth weight has trended slightly upward from 2012 to present day.\n\nThere have been numerous studies that have attempted, with varying degrees of success, to show links between birth weight and later-life conditions, including diabetes, obesity, tobacco smoking, and intelligence. Low birth weight is associated with neonatal infection and infant mortality.\n\n\nThere are two genetic loci that have been strongly linked to birth weight, ADCY5 and CCNL1, as well four that show some evidence (CDKAL1, HHEX-IDE, GCK, and TCF7L2). The heritability of birth weight ranges from 25-40 %. There is a complex relationship between a baby's genes and the maternal environment that the child is developing in. Foetal genes influence how the fetus grows in utero, and the maternal genes influence how the environment affects the growing fetus.\n\nThe health of the mother, particularly during the pregnancy. Intercurrent diseases in pregnancy are sometimes associated with decreased birth weight. For example, Celiac disease confers an odds ratio of low birth weight of approximately 1.8. Certain medications ( high blood pressure, epilepsy, etc. )can put a mother at a higher risk for delivering a low birth weight baby.\nGiving birth to a child when you are younger than 15 or older than 35 puts you at a higher risk to have a low-birth weight baby. Multiple births, where a mother has more than one child at one time, can also be a determinant in birth weight as each baby is likely to be outside the AGA (appropriate for gestational age). Multiple births put children at a higher rate to have low birth weight (56.6%) compared to children born in a single birth ( 6.2%). Low birth weight can also vary by maternal age. In 2008 the rate of low birth weight was the highest in babies born to women at an age that is younger than 15 years old which is 12.4 percent. Women aged 40–54 had a rate of low birth weight at 11.8 percent. The lowest rates of birth weight happened among babies to mothers that were between the ages of 25–29 years at 4.4 percent and 30–34 years at 7.6 percent.\n\nStressful events have been demonstrated to produce significant effects on birth weight. Those mothers who have stressful events during pregnancy, especially during the first and second trimester, are at higher risk to deliver low-birth weight babies. Researchers furthered this study and found that maternal stressful events that occur prior to conception have a negative impact on birth weight as well, and can result in a higher risk for preterm and lower birth weight babies. Women who experienced abuse (physical, sexual, or emotional) during pregnancy are also at increased risk of delivering a low-birth weight baby. For example, in a study completed by Witt et. al, those women who experienced a stressful event (ie. dealth of close family member, infertility issues, separation from partner) prior to conception had 38% more of a chance to have a very low birth weight baby compared to those who had not experienced a stressful life event. The theory is that stress can impact a baby based on two different mechanisms: neuroendocrine pathway or immune/inflammatory pathway. Stress causes the body to produce stress hormones called glucocorticoids that can suppress the immune system., as well as raises levels of placental corticotropin-releasing hormone (CRH) which can lead to preterm labor. These findings can pose evidence for future prevention efforts for low birth weight babies. One way to decrease rates of low birth weight and premature delivery is to focus on the health of women prior to conception through reproductive education, screening and counseling regarding mental health issues and stress, and access to primary care.\n\nNon-Hispanic Blacks have the highest infant mortality rate in the United States (11.4%, compared to the national average of 5.9%). Subsequently, there has been growing research supporting the idea of racial discrimination as a risk factor for low birth weight. In one study by Collins et. al, evidence suggested that African American mothers who experienced high levels of racial discrimination were at significantly higher risk of delivering a very low-birth weight baby compared to African American mothers who had not experienced racial discrimination. Black infants (13.2%) are more likely to have low birth weight compared to Asian and Pacific Islander (8.1%), American Indian and Alaska Native (7.6%), Non-Hispanic White (7.0%), and Hispanic Infants (7.1%).\n\nEnvironmental factors, including exposure of the mother to secondhand smoke can be a factor in determining the birth weight of child. In 2014, 13% of children exposed to smoke were born with low birth weight compared with 7.5% of those children born to nonsmokers. Children born to mothers who smoked or were exposed to secondhand smoke are more likely to develop health problems earlier in life such as neurodevelopmental delays. When mothers actively smoke during pregnancy, their child is at a higher risk of being born with a low birth weight. Smoking can also be a stress management tool used by expecting mothers. There is some support for lower socioeconomic status of the parents being a determinant of low birth weight, but there is conflicting evidence, as socioeconomic status is tied to many other factors.\n\nMost babies admitted to the NICU are born before 37 weeks of pregnancy or have low birth weight which is less than 5.5 pounds. They could also have a medical condition that requires special care. In the United States nearly half a million babies are born preterm because of this many of these babies also have low birth weights. There are four levels of care in the neonatal care units. Intensive Care, High Dependency Care, Low Dependency, and Transitional Care are the four levels:\n\n\nChildren born with an abnormally low birth weight can have significant problems within the first few years of life. They may have trouble gaining weight, obtaining adequate nutrition, and supporting a strong immune system. They also have higher risks for mortality, behavior problems, and mental deficiencies. Low birth weight babies are more likely to develop the following conditions after birth compared to normal birth weight babies:\n\n\nThat said, the effects of low birth weight on a child's first few years of life are often intertwined with other maternal, environmental, and genetic factors and most effects of low birth weight are only slightly negatively significant on a child's life when these factors are controlled for. When these factors are controlled, the only significant effect low birth weight has on a child's development is physical growth in the early years and the likelihood of being underweight compared to normal birth weight babies.\nStudies have been conducted to investigate how a person's birth weight can influence aspects of their future life. This includes theorised links with obesity, diabetes and intelligence.\n\nA baby born small or large for gestational age (either of the two extremes) is thought to have an increased risk of obesity in later life, but it was also shown that this relationship is fully explained by maternal weight. Middle aged adults with low birth weight present with a higher chance of obesity and diabetes. Children that are born under six pounds were 1.27 times more likely to develop diabetes compared to babies born at a healthy weight over six pounds.\n\nGrowth hormone (GH) therapy at a certain dose induced catch-up of lean body mass (LBM). However percentage body fat decreased in the GH-treated subjects. Bone mineral density SDS measured by DEXA increased significantly in the GH-treated group compared to the untreated subjects, though there is much debate over whether or not SGA (small for gestational age) is significantly adverse to children to warrant inducing catch-up.\nBabies that have a low birth weight are thought to have an increased risk of developing type 2 diabetes in later life.\n\nLow birth weight is linked with increase rates of obesity, insulin resistance, and type 2 diabetes and it is shown that children with the low birth weights have increased leptin levels after they catch up growth during childhood. Adiponection levels are positively related with birth weight and BMI in babies with an increase of risk of type 2 diabetes. The leptin and adiponection mechanisms are still being studied when involving low birth weight.\n\nSome studies have shown a direct link between an increased birth weight and an increased intelligence quotient. Increased birth weight is also linked to greater risk of developing autism.\n\nThere is much variation regarding birth weight within continents, countries, and cities. Even though over 20 million babies are born each year with low birth weight, it is hard to know the exact number as more than half of babies born in the world are not weighed at birth. The baby’s weight is an indicator of the mother and babies health. In 2013 22 million newborns had low birth weight, around 16 percent of all babies globally. Data on low birth weight is adjusted to account for under reporting. South Asia has the highest rate of babies not weighted at birth with 66 percent, but also have the highest low birth weight at 28 percent worldwide. West and Central Africa and least developed countries are next with 14 percent low birth weight worldwide.\n\nMore than 96.5% of low birth weight babies are born in developing countries around the world. Because low birth weight babies can require more extensive care, it places a financial burden on communities.\n\nThe World Health Organization (WHO) recently announced an initiative to have a thirty percent reduction in low birth weight worldwide. This is public health priority, as birth weight can have short and long term effects. WHO estimates that worldwide, 15-20 % of all births each year are considered low birth weight, which is about 20 million births.\n\nThe start of prenatal care is very important to help prevent low birth weight and early medical problems. Going to regular doctor’s visits is very important for the health of the mother and the baby. At the visits the OB/GYN will be checking maternal nutrition and weight gain because that is linked with the baby’s weight gain. The mother having a healthy diet is essential for the baby. Maintaining good nutrition by taking folic acid which is found in fruits and vegetables is linked to premature births and low birth weight. Alcohol, cigarettes, and drugs should also be avoided during pregnancy because they can also lead to poor growth and other complications. By seeing the doctor they are also able to monitor pre-existing medical illnesses to make sure they are under control during pregnancy. Mothers with high blood pressure and type 2 diabetes are more likely to have infants with low birth weights. One essential action to increase normal birth weights is to have affordable ,accessible, and culturally sensitive prenatal care worldwide. This is essential not just for treating low birth weight, but also preventing it. Other prevention efforts include: smoking cessation programs, food-distribution systems, stress reduction and social service supports.\n\n\n"}
{"id": "34899711", "url": "https://en.wikipedia.org/wiki?curid=34899711", "title": "Blue Button", "text": "Blue Button\n\nThe Blue Button is a system for patients to view online and download their own personal health records. Several Federal agencies, including the Departments of Defense, Health and Human Services, and Veterans Affairs, implemented this capability for their beneficiaries. In addition, Blue Button has pledges of support from numerous health plans and some vendors of personal health record vendors across the United States. Data from Blue Button-enabled sites can be used to create portable medical histories that facilitate dialog among health care providers, caregivers, and other trusted individuals or entities.\n\nCurrently widespread Blue Button usage supports downloading human-readable data in ASCII. In January 2013, the Office of the National Coordinator for Health IT announced an implementation guide for data holders and developers to enable automated data exchange among Blue Button+ compliant applications using structured data formats. Blue Button+ is designed to enhance the ways consumers get and share their health information in human-readable and machine-readable formats; and to enable the use of this information in third-party applications.\n\nThe Blue Button initiative began during the Markle Foundation Work Group on Consumer Engagement meeting in New York City on January 27, 2010. At the time, Meaningful Use was newly authorized and dominated health care IT dialogue in the United States. Members of the Markle Connecting for Health Community felt that while Meaningful Use criteria embraced patient engagement, the rules should also “[c]onsider individuals as information participants—not as mere recipients, but as information contributors, knowledge creators, and shared decision makers and care planners.” Markle convened the Work Group on Consumer Engagement to focus on using health information technology (IT) to achieve this kind of patient engagement.\n\nThe January 2010 meeting included representatives from private industry, not-for-profit foundations, and the federal government. Kim Nazi, performance and evaluation manager for VA's My HealtheVet personal health record (PHR) and a VA representative to the Work Group, said the attendees were from diverse backgrounds and that the discussion was “passionate”.\n\nThe Department of Defense (DoD) first provided secure on-line access to patient health information in December 2009 with its TriCare Online (TOL) portal. In 2010, the DoD rebranded under the \"Blue Button\" construct and enabled health record downloads as well as access. The DoD has added additional health record information over time and continues to enhance its Blue Button features.\n\nThe Work Group discussed many challenges related to engaging patients: numerous health IT data standards, data confidentiality and privacy laws, and fortress-like health databases. VA Chief Technology Officer (CTO) Peter Levin, also an attendee at the Markle Work Group meeting, described trying to balance the benefits of data standards with getting a solution to patients quickly. He assessed that it was only when the group decided to break the problem down into the simplest possible solutions that they were able to progress.\n\nA central theme emerged from the dialogue: give patients their data. HHS CTO and U.S. CTO Todd Park summarized the decision as: \"Look, there's all this complicated stuff happening with health information. But why can't we just do this: why can't we just let an American get a copy of their own information? [A]nd don't worry about the format, don't worry about the standards.\" The group agreed to create a large, prominent button that would represent data liquidity and access.\n\nIn August 2010, President Barack Obama announced that Veterans could soon “go to the VA website, click a simple blue button, download or print your personal health records, so you have them when you need them and can share them with your doctors . . . .”\n\nVA launched the Blue Button later that month (see \"Growth of Blue Button\", below). In October 2010, US Chief Technology Officer Aneesh Chopra, Health & Human Services Chief Technology Officer Todd Park, and VA Chief Technology Officer Dr. Peter L. Levin announced that VA and HHS’s Center for Medicare and Medicaid Services were offering Blue Button downloads to Veterans and to Medicare beneficiaries.\n\nBlue Button was designed to empower patients with their own health data and improve the quality of patient-clinician interactions with the expectation that these would contribute to enhanced quality of life, better treatment outcomes and potential reduction in costs. The framework on which Blue Button is based—including patient empowerment, data security and privacy protection – draws heavily on work by the Markle Foundation’s collaborative of industry stakeholders.\n\nIn September 2012, the Blue Button trademark was transferred from the Veterans Administration to the U.S. Department of Health and Human Services.\n\nIn 2012, efforts began by the Standards & Interoperability Framework to standardize the content formats and transport mechanisms for Blue Button data in order to make them more interoperable between health data-holding organizations, patients, and patient-authorized 3rd-parties such as physicians, caregivers, as well as applications and services.\n\nIn January 2013, ONC unveiled the Blue Button+ Implementation Guide to provide guidance to both data holders and third-party application developers to enable automated data exchange and data parsing features. The Blue Button+ implementation guide specifies appropriate structured data formats, transmission protocols, and APIs for developers to use when creating applications that rely on automated exchange of Blue Button-accessible health record data.\n\nThe Blue Button is a symbol on a website —for example, an online patient portal provided by a health care provider or insurer — that patients may use to download their health information. Depending on the implementation, users can download a variety of information in multiple formats, including text and PDF. At the Department of Veterans Affairs, Veterans can download their self-entered information, such as additional insurance, and information from their medical record, including medications, allergies, and lab results. They can also download their military personnel information like occupation specialty and pay details. Users of Department of Defense's TRICARE Online can use the Blue Button to download their medications, allergies, and lab results as a PDF or text file. Organizations like Medicare or Aetna offer health claims information as a downloadable text file.\n\nUsing Blue Button, patients have an easy way to retrieve and keep track of their health. Blue Button offers physicians an easy way to provide that data to patients. The simplicity of the Blue Button format allows users to carry their health information which ever way suits them—print, thumb drive, or on their smart phone. Developers can create applications to enhance the use of this data, such as the application offered by Northrop Grumman.\n\nVA launched the Blue Button function on its patient portal, My HealtheVet, in August, 2010. By May 2012, more than 500,000 individual (unique) Veterans had used the Blue Button to download their data. They opt-in to be able to download their health data, first by registering for a My HealtheVet account, and second, by validating their identity for privacy and security reasons, which includes appointment information, prescriptions and medications, laboratory results, vital signs and readings, military health history, and military occupations. In August 2012, the Department of Veterans Affairs announced that the Blue Button has one million registered patients.\n\nThe Center for Medicare and Medicaid Services (CMS) launched its version of Blue Button in September, 2010 on the My Medicare patient portal, giving 40 million beneficiaries online access to their Medicare claims. The Department of Defense also added a Blue Button function to its Tricare Online patient portal in 2010.\n\nIn July, 2011, the VA Innovation Initiative (VAi2) sponsored the Blue Button for All Americans Contest to encourage widespread use and assure that all Veterans had access to their Blue Button health data regardless of whether they sought care from VA or from a private non-VA health care provider.\n\nIn September, 2011, the Robert Wood Johnson Foundation launched a website to advocate cross-industry use of the Blue Button; its website collects the pledges of industry adopters of the technology, including United Health Care, Humana, Patients Like Me, Walgreens, and others.\n\nOne of those pledging support, Aetna, announced in September 2011 that it had added the Blue Button function to its patient portal, and in addition offered its beneficiaries the ability to share their Blue Button downloads with Aetna providers. At the time, Aetna said it served more than 36 million people. United Health Group began offering Blue Button downloads to its commercial health plan beneficiaries in July, 2012, rolling out the capability to its customers. The company expects 26 million plan beneficiaries will have access to Blue Button downloads by mid-2013.\n\nOther private sector organizations contributed to the growth of Blue Button.\nFor example:\n\nIn December 2011, the U.S. Office of Personnel Management announced that it had requested all federal employee health benefit plans to add the Blue Button function to their patient portals. FEHB’s health benefit plans are offered by more than 200 insurance carriers and serve approximately eight million federal employees (including Members of Congress), their families, and retirees.\n\nIn May 2012, the Obama administration announced the White House Presidential Innovation Fellows which will focus on five program areas including expanding Blue Button capabilities nationwide. The Office of the National Coordinator for Health IT wants to expand Blue Button to any patient in America. In 2012, the ONC launched the Automate Blue Button Standards and Interoperability Framework Initiative, which culminated in 2013 with the release of the Blue Button+ Implementation Guide.\n\nIn 2012, ONC held the Blue Button Mashup Challenge to encourage the development of third-party applications that increase the usefulness of data downloaded via Blue Button. The first-prize winner, iBlueButton, is a smart phone app that takes data from Medicare’s online portal and organizes it into an easier to read, easier to navigate format. The second-prize winner, ID BlueButton, is a tablet-based application that parses health information downloaded through Blue Button and presents it in a form that more clearly shows changes in health indicators and medication use over time. The third-prize winner, InstantPHR, is a web app that uses data downloaded through Blue Button to automatically populate personal health records in Microsoft HealthVault.\n\nBlue Button+ extends the Blue Button concept to include a standardized data format and additional functionality for trusted, automated exchange of health data, and advanced parsing of health data to improve human readability. In January 2013, ONC released the Blue Button+ Implementation Guide, offering guidance and a toolkit for both data holders (such as health care providers and insurers) and third-party application developers seeking to add this functionality to their products and services. The Blue Button+ Implementation Guide is the result of a collaboration among more than 68 volunteer organizations.\n\nSMART Platforms, Harvard Medical School, and Boston Children’s Hospital built a proof-of-concept Blue Button+ app called Growth-tastic. With Growth-tastic, parents can view charts of their child’s height, weight, and BMI trended over time.\n\nOriginally, in 2010, organizations needed a license to use the Blue Button marks. As of September 2012, application and licensing is no longer required. Usage of the Blue Button logo and brand is free, but must conform to the established usage guidelines set by the U.S. Department of Health and Human Services.\n\nBlue Button is part of a larger “My Data Initiative” that aims to empower consumers with the tools and information they need to make optimal choices. Other, similar \"button\" projects include the Green Button (for personal energy usage data) and the Red Button (for personal educational data).\n\nIn September 2011, US CTO Aneesh Chopra challenged the energy industry to model a Green Button, off the successful Blue Button, where energy providers would give energy users their consumption data in an easy to read and use format at the click of the button. In January 2012, two major California utilities—Pacific Gas & Electric and San Diego Gas & Electric—announced their implementation of Green Button. Energy customers can manage their consumption via their smart phones using the standard Green Button data format.\n\nThe National Institute of Standards and Technology awarded a contract to HyperTek Inc. to work with developers and users to expand the Green Button initiative.\n\nIn healthcare, there is a related proposal for a Green Button, as a way for doctors to use summarized patient data for real time decision making at the point of care. A related, patient-driven, green button initiative to promote the donation of medical data analogous to donation of organs.\n\nDepartment of Defense's implementation of the Blue Button was awarded one of the 10 GCN 2011 awards.\n\nThe Federal Blue Button team was selected as a finalist by the Partnership for Public Service for the Citizen Services Medal for their contributions to implementing Blue Button at Departments of Defense, Health and Human Services, and Veterans Affairs.\n\n"}
{"id": "26409649", "url": "https://en.wikipedia.org/wiki?curid=26409649", "title": "Bob Sang", "text": "Bob Sang\n\nBob Sang (27 March 1948 - 5 June 2009) was appointed by London South Bank University as the UK's first professor of Patient and Public Involvement in 2006. He had also been a Visiting Fellow at CENTRIM.\n\nHe was educated at the University of East Anglia but failed his chemistry degree there. He later returned to studying and completed a public administration degree at the University of Sussex.\n\nHe was a pioneer of the idea of involving patients, service users and the public in health and social care, describing himself as a 'constructive subversive'.\n\nSang was a prolific networker and writer, contributing articles to journals including \"Health Policy Insight\", \"British Journal of Healthcare Management\", the \"Journal of Integrated Care\" and the \"Health Service Journal\".\n\nFollowing a career which combined academia, research and working in mental health, he established his own independent consultancy, Sang Jacobsson. His work through Sang Jacobsson included mentoring, training, policy work, facilitation and analysis, and was mainly for clients in the NHS and the UK health economy.\n\nSang was the co-founder with Jane Keep of the Engaging Communities Learning Network for the National Association of Primary and Care Trusts (NatPaCT), which was part of the now-closed NHS Modernisation Agency. The Engaging Communities Learning Network was designed to help NHS primary care trusts (PCTs) share good practice and learning about patient and public involvement and engagement.\n\nHe provided professional supervision to the ground-breaking Health Foundation Leadership Fellows Scheme which over five years directly involved patients and users in the leadership development of 64 NHS clinicians and managers.\n\nHe died of acute pancreatitis on 5 June 2009.\n\n"}
{"id": "5374", "url": "https://en.wikipedia.org/wiki?curid=5374", "title": "Condom", "text": "Condom\n\nA condom is a sheath-shaped barrier device, used during sexual intercourse to reduce the probability of pregnancy or a sexually transmitted infection (STI). There are both male and female condoms. With proper use—and use at every act of intercourse—women whose partners use male condoms experience a 2% per-year pregnancy rate. With typical use the rate of pregnancy is 18% per-year. Their use greatly decreases the risk of gonorrhea, chlamydia, trichomoniasis, hepatitis B, and HIV/AIDS. They also to a lesser extent protect against genital herpes, human papillomavirus (HPV), and syphilis.\nThe male condom is rolled onto an erect penis before intercourse and works by blocking semen from entering the body of a sexual partner. Male condoms are typically made from latex and less commonly from polyurethane or lamb intestine. Male condoms have the advantages of ease of use, easy to access, and few side effects. In those with a latex allergy a polyurethane or other synthetic version should be used. Female condoms are typically made from polyurethane and may be used multiple times.\nCondoms as a method of preventing STIs have been used since at least 1564. Rubber condoms become available in 1855, followed by latex condoms in the 1920s. They are on the World Health Organization's List of Essential Medicines, the most effective and safe medicines needed in a health system. The wholesale cost in the developing world is about 0.03 to 0.08 USD each. In the United States condoms usually cost less than 1.00 USD. Globally less than 10% of those using birth control are using the condom. Rates of condom use are higher in the developed world. In United Kingdom the condom is the second most common method of birth control (22%) while in the United States it is the third most common (15%). About six to nine billion are sold a year.\n\nThe effectiveness of condoms, as of most forms of contraception, can be assessed two ways. \"Perfect use\" or \"method\" effectiveness rates only include people who use condoms properly and consistently. \"Actual use\", or \"typical use\" effectiveness rates are of all condom users, including those who use condoms incorrectly or do not use condoms at every act of intercourse. Rates are generally presented for the first year of use. Most commonly the Pearl Index is used to calculate effectiveness rates, but some studies use decrement tables.\n\nThe typical use pregnancy rate among condom users varies depending on the population being studied, ranging from 10 to 18% per year. The perfect use pregnancy rate of condoms is 2% per year. Condoms may be combined with other forms of contraception (such as spermicide) for greater protection.\n\nCondoms are widely recommended for the prevention of sexually transmitted infections (STIs). They have been shown to be effective in reducing infection rates in both men and women. While not perfect, the condom is effective at reducing the transmission of organisms that cause AIDS, genital herpes, cervical cancer, genital warts, syphilis, chlamydia, gonorrhea, and other diseases. Condoms are often recommended as an adjunct to more effective birth control methods (such as IUD) in situations where STD protection is also desired.\n\nAccording to a 2000 report by the National Institutes of Health (NIH), consistent use of latex condoms reduces the risk of HIV/AIDS transmission by approximately 85% relative to risk when unprotected, putting the seroconversion rate (infection rate) at 0.9 per 100 person-years with condom, down from 6.7 per 100 person-years. Analysis published in 2007 from the University of Texas Medical Branch and the World Health Organization found similar risk reductions of 80–95%.\n\nThe 2000 NIH review concluded that condom use significantly reduces the risk of gonorrhea for men. A 2006 study reports that proper condom use decreases the risk of transmission of human papillomavirus (HPV) to women by approximately 70%. Another study in the same year found consistent condom use was effective at reducing transmission of herpes simplex virus-2 also known as genital herpes, in both men and women.\n\nAlthough a condom is effective in limiting exposure, some disease transmission may occur even with a condom. Infectious areas of the genitals, especially when symptoms are present, may not be covered by a condom, and as a result, some diseases like HPV and herpes may be transmitted by direct contact. The primary effectiveness issue with using condoms to prevent STDs, however, is inconsistent use.\n\nCondoms may also be useful in treating potentially precancerous cervical changes. Exposure to human papillomavirus, even in individuals already infected with the virus, appears to increase the risk of precancerous changes. The use of condoms helps promote regression of these changes. In addition, researchers in the UK suggest that a hormone in semen can aggravate existing cervical cancer, condom use during sex can prevent exposure to the hormone.\n\nCondoms may slip off the penis after ejaculation, break due to improper application or physical damage (such as tears caused when opening the package), or break or slip due to latex degradation (typically from usage past the expiration date, improper storage, or exposure to oils). The rate of breakage is between 0.4% and 2.3%, while the rate of slippage is between 0.6% and 1.3%. Even if no breakage or slippage is observed, 1–3% of women will test positive for semen residue after intercourse with a condom.\n\n\"Double bagging\", using two condoms at once, is often believed to cause a higher rate of failure due to the friction of rubber on rubber. This claim is not supported by research. The limited studies that have been done found that the simultaneous use of multiple condoms decreases the risk of condom breakage.\n\nDifferent modes of condom failure result in different levels of semen exposure. If a failure occurs during application, the damaged condom may be disposed of and a new condom applied before intercourse begins – such failures generally pose no risk to the user. One study found that semen exposure from a broken condom was about half that of unprotected intercourse; semen exposure from a slipped condom was about one-fifth that of unprotected intercourse.\n\nStandard condoms will fit almost any penis, with varying degrees of comfort or risk of slippage. Many condom manufacturers offer \"snug\" or \"magnum\" sizes. Some manufacturers also offer custom sized-to-fit condoms, with claims that they are more reliable and offer improved sensation/comfort. Some studies have associated larger penises and smaller condoms with increased breakage and decreased slippage rates (and vice versa), but other studies have been inconclusive.\n\nIt is recommended for condoms manufacturers to avoid very thick or very thin condoms, because they are both considered less effective. Some authors encourage users to choose thinner condoms \"for greater durability, sensation, and comfort\", but others warn that \"the thinner the condom, the smaller the force required to break it\".\n\nExperienced condom users are significantly less likely to have a condom slip or break compared to first-time users, although users who experience one slippage or breakage are more likely to suffer a second such failure. An article in \"Population Reports\" suggests that education on condom use reduces behaviors that increase the risk of breakage and slippage. A Family Health International publication also offers the view that education can reduce the risk of breakage and slippage, but emphasizes that more research needs to be done to determine all of the causes of breakage and slippage.\n\nAmong people who intend condoms to be their form of birth control, pregnancy may occur when the user has sex without a condom. The person may have run out of condoms, or be traveling and not have a condom with them, or simply dislike the feel of condoms and decide to \"take a chance\". This type of behavior is the primary cause of typical use failure (as opposed to method or perfect use failure).\n\nAnother possible cause of condom failure is sabotage. One motive is to have a child against a partner's wishes or consent. Some commercial sex workers from Nigeria reported clients sabotaging condoms in retaliation for being coerced into condom use. Using a fine needle to make several pinholes at the tip of the condom is believed to significantly impact on their effectiveness. Cases of such condom sabotage have occurred.\n\nThe use of latex condoms by people with an allergy to latex can cause allergic symptoms, such as skin irritation. In people with severe latex allergies, using a latex condom can potentially be life-threatening. Repeated use of latex condoms can also cause the development of a latex allergy in some people. Irritation may also occur due to spermicides that may be present.\n\nMale condoms are usually packaged inside a foil or plastic wrapper, in a rolled-up form, and are designed to be applied to the tip of the penis and then unrolled over the erect penis. It is important that some space be left in the tip of the condom so that semen has a place to collect; otherwise it may be forced out of the base of the device. Most condoms have a teat end for this purpose. After use, it is recommended the condom be wrapped in tissue or tied in a knot, then disposed of in a trash receptacle. Condoms are used to reduce the likelihood of pregnancy during intercourse and to reduce the likelihood of contracting sexually-transmitted infections (STIs). Condoms are also used during fellatio to reduce the likelihood of contracting STIs.\n\nSome couples find that putting on a condom interrupts sex, although others incorporate condom application as part of their foreplay. Some men and women find the physical barrier of a condom dulls sensation. Advantages of dulled sensation can include prolonged erection and delayed ejaculation; disadvantages might include a loss of some sexual excitement. Advocates of condom use also cite their advantages of being inexpensive, easy to use, and having few side effects.\n\nIn 2012 proponents gathered 372,000 voter signatures through a citizens' initiative in Los Angeles County to put Measure B on the 2012 ballot. As a result, Measure B, a law requiring the use of condoms in the production of pornographic films, was passed. This requirement has received much criticism and is said by some to be counter-productive, merely forcing companies that make pornographic films to relocate to other places without this requirement. Producers claim that condom use depresses sales.\n\nCondoms are often used in sex education programs, because they have the capability to reduce the chances of pregnancy and the spread of some sexually transmitted diseases when used correctly. A recent American Psychological Association (APA) press release supported the inclusion of information about condoms in sex education, saying \"\"comprehensive sexuality education programs... discuss the appropriate use of condoms\", and \"promote condom use for those who are sexually active\".\"\n\nIn the United States, teaching about condoms in public schools is opposed by some religious organizations. Planned Parenthood, which advocates family planning and sex education, argues that no studies have shown abstinence-only programs to result in delayed intercourse, and cites surveys showing that 76% of American parents want their children to receive comprehensive sexuality education including condom use.\n\nCommon procedures in infertility treatment such as semen analysis and intrauterine insemination (IUI) require collection of semen samples. These are most commonly obtained through masturbation, but an alternative to masturbation is use of a special \"collection condom\" to collect semen during sexual intercourse.\n\nCollection condoms are made from silicone or polyurethane, as latex is somewhat harmful to sperm. Many men prefer collection condoms to masturbation, and some religions prohibit masturbation entirely. Also, compared with samples obtained from masturbation, semen samples from collection condoms have higher total sperm counts, sperm motility, and percentage of sperm with normal morphology. For this reason, they are believed to give more accurate results when used for semen analysis, and to improve the chances of pregnancy when used in procedures such as intracervical or intrauterine insemination. Adherents of religions that prohibit contraception, such as Catholicism, may use collection condoms with holes pricked in them.\n\nFor fertility treatments, a collection condom may be used to collect semen during sexual intercourse where the semen is provided by the woman's partner. Private sperm donors may also use a collection condom to obtain samples through masturbation or by sexual intercourse with a partner and will transfer the ejaculate from the collection condom to a specially designed container. The sperm is transported in such containers, in the case of a donor, to a recipient woman to be used for insemination, and in the case of a woman's partner, to a fertility clinic for processing and use. However, transportation may reduce the fecundity of the sperm. Collection condoms may also be used where semen is produced at a sperm bank or fertility clinic.\n\n\"Condom therapy\" is sometimes prescribed to infertile couples when the female has high levels of antisperm antibodies. The theory is that preventing exposure to her partner's semen will lower her level of antisperm antibodies, and thus increase her chances of pregnancy when condom therapy is discontinued. However, condom therapy has not been shown to increase subsequent pregnancy rates.\n\nCondoms excel as multipurpose containers and barriers because they are waterproof, elastic, durable, and (for military and espionage uses) will not arouse suspicion if found.\n\nOngoing military utilization began during World War II, and includes covering the muzzles of rifle barrels to prevent fouling, the waterproofing of firing assemblies in underwater demolitions, and storage of corrosive materials and garrotes by paramilitary agencies.\n\nCondoms have also been used to smuggle alcohol, cocaine, heroin, and other drugs across borders and into prisons by filling the condom with drugs, tying it in a knot and then either swallowing it or inserting it into the rectum. These methods are very dangerous and potentially lethal; if the condom breaks, the drugs inside become absorbed into the bloodstream and can cause an overdose.\n\nMedically, condoms can be used to cover endovaginal ultrasound probes, or in field chest needle decompressions they can be used to make a one-way valve.\n\nCondoms have also been used to protect scientific samples from the environment, and to waterproof microphones for underwater recording.\n\nMost condoms have a reservoir tip or teat end, making it easier to accommodate the man's ejaculate. Condoms come in different sizes, from snug to larger, and shapes. Width often varies from 49 mm to 56 mm. Sizes from 45 mm to 60 mm, however exist.\n\nThey also come in a variety of surfaces intended to stimulate the user's partner. Condoms are usually supplied with a lubricant coating to facilitate penetration, while flavored condoms are principally used for oral sex. As mentioned above, most condoms are made of latex, but polyurethane and lambskin condoms also exist.\n\nMale condoms have a tight ring to form a seal around the penis while female condoms usually have a large stiff ring to prevent them from slipping into the body orifice. The Female Health Company produced a female condom that was initially made of polyurethane, but newer versions are made of nitrile. Medtech Products produces a female condom made of latex.\n\nLatex has outstanding elastic properties: Its tensile strength exceeds 30 MPa, and latex condoms may be stretched in excess of 800% before breaking. In 1990 the ISO set standards for condom production (ISO 4074, Natural latex rubber condoms), and the EU followed suit with its CEN standard (Directive 93/42/EEC concerning medical devices). Every latex condom is tested for holes with an electric current. If the condom passes, it is rolled and packaged. In addition, a portion of each batch of condoms is subject to water leak and air burst testing.\n\nWhile the advantages of latex have made it the most popular condom material, it does have some drawbacks. Latex condoms are damaged when used with oil-based substances as lubricants, such as petroleum jelly, cooking oil, baby oil, mineral oil, skin lotions, suntan lotions, cold creams, butter or margarine. Contact with oil makes latex condoms more likely to break or slip off due to loss of elasticity caused by the oils. Additionally, latex allergy precludes use of latex condoms and is one of the principal reasons for the use of other materials. In May 2009 the U.S. Food and Drug Administration granted approval for the production of condoms composed of Vytex, latex that has been treated to remove 90% of the proteins responsible for allergic reactions. An allergen-free condom made of synthetic latex (polyisoprene) is also available.\n\nThe most common non-latex condoms are made from polyurethane. Condoms may also be made from other synthetic materials, such as AT-10 resin, and most recently polyisoprene.\n\nPolyurethane condoms tend to be the same width and thickness as latex condoms, with most polyurethane condoms between 0.04 mm and 0.07 mm thick.\n\nPolyurethane can be considered better than latex in several ways: it conducts heat better than latex, is not as sensitive to temperature and ultraviolet light (and so has less rigid storage requirements and a longer shelf life), can be used with oil-based lubricants, is less allergenic than latex, and does not have an odor. Polyurethane condoms have gained FDA approval for sale in the United States as an effective method of contraception and HIV prevention, and under laboratory conditions have been shown to be just as effective as latex for these purposes.\n\nHowever, polyurethane condoms are less elastic than latex ones, and may be more likely to slip or break than latex, lose their shape or bunch up more than latex, and are more expensive.\n\nPolyisoprene is a synthetic version of natural rubber latex. While significantly more expensive, it has the advantages of latex (such as being softer and more elastic than polyurethane condoms) without the protein which is responsible for latex allergies. Like polyurethane condoms, polyisoprene condoms are said to do a better job of transmitting body heat. Unlike polyurethane condoms, they cannot be used with an oil-based lubricant.\n\nCondoms made from sheep intestines, labeled \"lambskin\", are also available. Although they are generally effective as a contraceptive by blocking sperm, it is presumed that they are likely less effective than latex in preventing the transmission of agents that cause STDs, because of pores in the material. This is based on the idea that intestines, by their nature, are porous, permeable membranes, and while sperm are too large to pass through the pores, viruses—such as HIV, herpes, and genital warts—are small enough to pass through. However, there are to date no clinical data confirming or denying this theory. Some believe that lambskin condoms provide a more \"natural\" sensation, and they lack the allergens that are inherent to latex, but because of their lesser protection against infection, other hypoallergenic materials such as polyurethane are recommended for latex-allergic users and/or partners. Lambskin condoms are also significantly more expensive than other types and as slaughter by-products they are also not vegetarian.\n\nSome latex condoms are lubricated at the manufacturer with a small amount of a nonoxynol-9, a spermicidal chemical. According to Consumer Reports, condoms lubricated with spermicide have no additional benefit in preventing pregnancy, have a shorter shelf life, and may cause urinary-tract infections in women. In contrast, application of separately packaged spermicide \"is\" believed to increase the contraceptive efficacy of condoms.\n\nNonoxynol-9 was once believed to offer additional protection against STDs (including HIV) but recent studies have shown that, with frequent use, nonoxynol-9 may increase the risk of HIV transmission. The World Health Organization says that spermicidally lubricated condoms should no longer be promoted. However, it recommends using a nonoxynol-9 lubricated condom over no condom at all. , nine condom manufacturers have stopped manufacturing condoms with nonoxynol-9 and Planned Parenthood has discontinued the distribution of condoms so lubricated.\n\nTextured condoms include studded and ribbed condoms which can provide extra sensations to both partners. The studs or ribs can be located on the inside, outside, or both; alternatively, they are located in specific sections to provide directed stimulation to either the g-spot or frenulum. Many textured condoms which advertise \"mutual pleasure\" also are bulb-shaped at the top, to provide extra stimulation to the penis. Some women experience irritation during vaginal intercourse with studded condoms.\n\nThe anti-rape condom is another variation designed to be worn by women. It is designed to cause pain to the attacker, hopefully allowing the victim a chance to escape.\n\nA collection condom is used to collect semen for fertility treatments or sperm analysis. These condoms are designed to maximize sperm life.\n\nSome condom-like devices are intended for entertainment only, such as glow-in-the dark condoms. These novelty condoms may not provide protection against pregnancy and STDs.\n\nThe prevalence of condom use varies greatly between countries. Most surveys of contraceptive use are among married women, or women in informal unions. Japan has the highest rate of condom usage in the world: in that country, condoms account for almost 80% of contraceptive use by married women. On average, in developed countries, condoms are the most popular method of birth control: 28% of married contraceptive users rely on condoms. In the average less-developed country, condoms are less common: only 6–8% of married contraceptive users choose condoms.\n\nWhether condoms were used in ancient civilizations is debated by archaeologists and historians. In ancient Egypt, Greece, and Rome, pregnancy prevention was generally seen as a woman's responsibility, and the only well documented contraception methods were female-controlled devices. In Asia before the 15th century, some use of glans condoms (devices covering only the head of the penis) is recorded. Condoms seem to have been used for contraception, and to have been known only by members of the upper classes. In China, glans condoms may have been made of oiled silk paper, or of lamb intestines. In Japan, they were made of tortoise shell or animal horn.\n\nIn 16th-century Italy, anatomist and physician Gabriele Falloppio wrote a treatise on syphilis. The earliest documented strain of syphilis, first appearing in Europe in a 1490s outbreak, caused severe symptoms and often death within a few months of contracting the disease. Falloppio's treatise is the earliest uncontested description of condom use: it describes linen sheaths soaked in a chemical solution and allowed to dry before use. The cloths he described were sized to cover the glans of the penis, and were held on with a ribbon. Falloppio claimed that an experimental trial of the linen sheath demonstrated protection against syphilis.\n\nAfter this, the use of penis coverings to protect from disease is described in a wide variety of literature throughout Europe. The first indication that these devices were used for birth control, rather than disease prevention, is the 1605 theological publication \"De iustitia et iure\" (On justice and law) by Catholic theologian Leonardus Lessius, who condemned them as immoral. In 1666, the English Birth Rate Commission attributed a recent downward fertility rate to use of \"condons\", the first documented use of that word (or any similar spelling). (Other early spellings include \"condam\" and \"quondam\", from which the Italian derivation \"guantone\" has been suggested, from \"guanto\", \"a glove.\")\nIn addition to linen, condoms during the Renaissance were made out of intestines and bladder. In the late 16th century, Dutch traders introduced condoms made from \"fine leather\" to Japan. Unlike the horn condoms used previously, these leather condoms covered the entire penis.\nCasanova in the 18th century was one of the first reported using \"assurance caps\" to prevent impregnating his mistresses.\n\nFrom at least the 18th century, condom use was opposed in some legal, religious, and medical circles for essentially the same reasons that are given today: condoms reduce the likelihood of pregnancy, which some thought immoral or undesirable for the nation; they do not provide full protection against sexually transmitted infections, while belief in their protective powers was thought to encourage sexual promiscuity; and, they are not used consistently due to inconvenience, expense, or loss of sensation.\n\nDespite some opposition, the condom market grew rapidly. In the 18th century, condoms were available in a variety of qualities and sizes, made from either linen treated with chemicals, or \"skin\" (bladder or intestine softened by treatment with sulfur and lye). They were sold at pubs, barbershops, chemist shops, open-air markets, and at the theater throughout Europe and Russia. They later spread to America, although in every place there were generally used only by the middle and upper classes, due to both expense and lack of sex education.\n\nThe early 19th century saw contraceptives promoted to the poorer classes for the first time. Writers on contraception tended to prefer other methods of birth control to the condom. By the late 19th century many feminists expressed distrust of the condom as a contraceptive, as its use was controlled and decided upon by men alone. They advocated instead for methods which were controlled by women, such as diaphragms and spermicidal douches. Other writers cited both the expense of condoms and their unreliability (they were often riddled with holes, and often fell off or broke), but they discussed condoms as a good option for some, and as the only contraceptive that also protected from disease.\n\nMany countries passed laws impeding the manufacture and promotion of contraceptives. In spite of these restrictions, condoms were promoted by traveling lecturers and in newspaper advertisements, using euphemisms in places where such ads were illegal. Instructions on how to make condoms at home were distributed in the United States and Europe. Despite social and legal opposition, at the end of the 19th century the condom was the Western world's most popular birth control method.\nBeginning in the second half of the 19th century, American rates of sexually transmitted diseases skyrocketed. Causes cited by historians include effects of the American Civil War, and the ignorance of prevention methods promoted by the Comstock laws. To fight the growing epidemic, sex education classes were introduced to public schools for the first time, teaching about venereal diseases and how they were transmitted. They generally taught that abstinence was the only way to avoid sexually transmitted diseases. Condoms were not promoted for disease prevention because the medical community and moral watchdogs considered STDs to be punishment for sexual misbehavior. The stigma against victims of these diseases was so great that many hospitals refused to treat people who had syphilis.\nThe German military was the first to promote condom use among its soldiers, beginning in the later 19th century. Early 20th century experiments by the American military concluded that providing condoms to soldiers significantly lowered rates of sexually transmitted diseases. During World War I, the United States and (at the beginning of the war only) Britain were the only countries with soldiers in Europe who did not provide condoms and promote their use.\n\nIn the decades after World War I, there remained social and legal obstacles to condom use throughout the U.S. and Europe. Founder of psychoanalysis Sigmund Freud opposed all methods of birth control on the grounds that their failure rates were too high. Freud was especially opposed to the condom because he thought it cut down on sexual pleasure. Some feminists continued to oppose male-controlled contraceptives such as condoms. In 1920 the Church of England's Lambeth Conference condemned all \"unnatural means of conception avoidance\". London's Bishop Arthur Winnington-Ingram complained of the huge number of condoms discarded in alleyways and parks, especially after weekends and holidays.\n\nHowever, European militaries continued to provide condoms to their members for disease protection, even in countries where they were illegal for the general population. Through the 1920s, catchy names and slick packaging became an increasingly important marketing technique for many consumer items, including condoms and cigarettes. Quality testing became more common, involving filling each condom with air followed by one of several methods intended to detect loss of pressure. Worldwide, condom sales doubled in the 1920s.\n\nIn 1839, Charles Goodyear discovered a way of processing natural rubber, which is too stiff when cold and too soft when warm, in such a way as to make it elastic. This proved to have advantages for the manufacture of condoms; unlike the sheep's gut condoms, they could stretch and did not tear quickly when used. The rubber vulcanization process was patented by Goodyear in 1844. The first rubber condom was produced in 1855. The earliest rubber condoms had a seam and were as thick as a bicycle inner tube. Besides this type, small rubber condoms covering only the glans were often used in England and the United States. There was more risk of losing them and if the rubber ring was too tight, it would constrict the penis. This type of condom was the original \"capote\" (French for condom), perhaps because of its resemblance to a woman's bonnet worn at that time, also called a capote.\n\nFor many decades, rubber condoms were manufactured by wrapping strips of raw rubber around penis-shaped molds, then dipping the wrapped molds in a chemical solution to cure the rubber. In 1912, Polish inventor Julius Fromm developed a new, improved manufacturing technique for condoms: dipping glass molds into a raw rubber solution. Called \"cement dipping\", this method required adding gasoline or benzene to the rubber to make it liquid. Latex, rubber suspended in water, was invented in 1920. Latex condoms required less labor to produce than cement-dipped rubber condoms, which had to be smoothed by rubbing and trimming. The use of water to suspend the rubber instead of gasoline and benzene eliminated the fire hazard previously associated with all condom factories. Latex condoms also performed better for the consumer: they were stronger and thinner than rubber condoms, and had a shelf life of five years (compared to three months for rubber).\n\nUntil the twenties, all condoms were individually hand-dipped by semi-skilled workers. Throughout the decade of the 1920s, advances in the automation of the condom assembly line were made. The first fully automated line was patented in 1930. Major condom manufacturers bought or leased conveyor systems, and small manufacturers were driven out of business. The skin condom, now significantly more expensive than the latex variety, became restricted to a niche high-end market.\n\nIn 1930 the Anglican Church's sanctioned the use of birth control by married couples. In 1931 the Federal Council of Churches in the U.S. issued a similar statement. The Roman Catholic Church responded by issuing the encyclical \"Casti connubii\" affirming its opposition to all contraceptives, a stance it has never reversed. In the 1930s, legal restrictions on condoms began to be relaxed. But during this period Fascist Italy and Nazi Germany increased restrictions on condoms (limited sales as disease preventatives were still allowed). During the Depression, condom lines by Schmid gained in popularity. Schmid still used the cement-dipping method of manufacture which had two advantages over the latex variety. Firstly, cement-dipped condoms could be safely used with oil-based lubricants. Secondly, while less comfortable, these older-style rubber condoms could be reused and so were more economical, a valued feature in hard times. More attention was brought to quality issues in the 1930s, and the U.S. Food and Drug Administration began to regulate the quality of condoms sold in the United States.\n\nThroughout World War II, condoms were not only distributed to male U.S. military members, but also heavily promoted with films, posters, and lectures. European and Asian militaries on both sides of the conflict also provided condoms to their troops throughout the war, even Germany which outlawed all civilian use of condoms in 1941. In part because condoms were readily available, soldiers found a number of non-sexual uses for the devices, many of which continue to this day. After the war, condom sales continued to grow. From 1955–1965, 42% of Americans of reproductive age relied on condoms for birth control. In Britain from 1950–1960, 60% of married couples used condoms. The birth control pill became the world's most popular method of birth control in the years after its 1960 début, but condoms remained a strong second. The U.S. Agency for International Development pushed condom use in developing countries to help solve the \"world population crises\": by 1970 hundreds of millions of condoms were being used each year in India alone.(This number has grown in recent decades: in 2004, the government of India purchased 1.9 billion condoms for distribution at family planning clinics.)\n\nIn the 1960s and 1970s quality regulations tightened, and more legal barriers to condom use were removed. In Ireland, legal condom sales were allowed for the first time in 1978. Advertising, however was one area that continued to have legal restrictions. In the late 1950s, the American National Association of Broadcasters banned condom advertisements from national television: this policy remained in place until 1979.\n\nAfter learning in the early 1980s that AIDS can be a sexually transmitted infection, the use of condoms was encouraged to prevent transmission of HIV. Despite opposition by some political, religious, and other figures, national condom promotion campaigns occurred in the U.S. and Europe. These campaigns increased condom use significantly.\n\nDue to increased demand and greater social acceptance, condoms began to be sold in a wider variety of retail outlets, including in supermarkets and in discount department stores such as Wal-Mart. Condom sales increased every year until 1994, when media attention to the AIDS pandemic began to decline. The phenomenon of decreasing use of condoms as disease preventatives has been called \"prevention fatigue\" or \"condom fatigue\". Observers have cited condom fatigue in both Europe and North America. As one response, manufacturers have changed the tone of their advertisements from scary to humorous.\n\nNew developments continued to occur in the condom market, with the first polyurethane condom—branded Avanti and produced by the manufacturer of Durex—introduced in the 1990s. Worldwide condom use is expected to continue to grow: one study predicted that developing nations would need 18.6 billion condoms by 2015. , condoms are available inside prisons in Canada, most of the European Union, Australia, Brazil, Indonesia, South Africa, and the US states of Vermont (on September 17, 2013, the Californian Senate approved a bill for condom distribution inside the state's prisons, but the bill was not yet law at the time of approval).\n\nThe term \"condom\" first appears in the early 18th century. Its etymology is unknown.\nIn popular tradition, the invention and naming of the condom came to be attributed to an associate of England's King Charles II, one \"Dr. Condom\" or \"Earl of Condom\". There is however no evidence of the existence of such a person, and condoms had been used for over one hundred years before King Charles II ascended to the throne.\n\nA variety of unproven Latin etymologies have been proposed, including \"condon\" (receptacle), \"condamina\" (house), and \"cumdum\" (scabbard or case). It has also been speculated to be from the Italian word \"guantone\", derived from \"guanto\", meaning glove. William E. Kruck wrote an article in 1981 concluding that, \"As for the word 'condom', I need state only that its origin remains completely unknown, and there ends this search for an etymology.\" Modern dictionaries may also list the etymology as \"unknown\".\nOther terms are also commonly used to describe condoms. In North America condoms are also commonly known as \"prophylactics\", or \"rubbers\". In Britain they may be called \"French letters\". Additionally, condoms may be referred to using the manufacturer's name.\n\nSome moral and scientific criticism of condoms exists despite the many benefits of condoms agreed on by scientific consensus and sexual health experts.\n\nCondom usage is typically recommended for new couples who have yet to develop full trust in their partner with regard to STDs. Established couples on the other hand have few concerns about STDs, and can use other methods of birth control such as the pill, which does not act as a barrier to intimate sexual contact. Note that the polar debate with regard to condom usage is attenuated by the target group the argument is directed. Notably the age category and stable partner question are factors, as well as the distinction between heterosexual and homosexuals, who have different kinds of sex and have different risk consequences and factors.\n\nAmong the prime objections to condom usage is the blocking of erotic sensation, or the intimacy that barrier-free sex provides. As the condom is held tightly to the skin of the penis, it diminishes the delivery of stimulation through rubbing and friction. Condom proponents claim this has the benefit of making sex last longer, by diminishing sensation and delaying male ejaculation. Those who promote condom-free heterosexual sex (slang: \"bareback\") claim that the condom puts a prophylactic barrier between partners, diminishing what is normally a highly sensual, intimate, and spiritual connection between partners.\n\nThe Roman Catholic Church opposes all kinds of sexual acts outside of marriage, as well as any sexual act in which the chance of successful conception has been reduced by direct and intentional acts (for example, surgery to prevent conception) or foreign objects (for example, condoms).\n\nThe use of condoms to prevent STI transmission is not specifically addressed by Catholic doctrine, and is currently a topic of debate among theologians and high-ranking Catholic authorities. A few, such as Belgian Cardinal Godfried Danneels, believe the Catholic Church should actively support condoms used to prevent disease, especially serious diseases such as AIDS. However, the majority view—including all statements from the Vatican—is that condom-promotion programs encourage promiscuity, thereby actually increasing STI transmission. This view was most recently reiterated in 2009 by Pope Benedict XVI.\n\nThe Roman Catholic Church is the largest organized body of any world religion. The church has hundreds of programs dedicated to fighting the AIDS epidemic in Africa, but its opposition to condom use in these programs has been highly controversial.\n\nIn a November 2011 interview, Pope Benedict XVI discussed for the first time the use of condoms to prevent STI transmission. He said that the use of a condom can be justified in a few individual cases if the purpose is to reduce the risk of an HIV infection. He gave as an example male prostitutes. There was some confusion at first whether the statement applied only to homosexual prostitutes and thus not to heterosexual intercourse at all. However, Federico Lombardi, spokesman for the Vatican, clarified that it applied to heterosexual and transsexual prostitutes, whether male or female, as well. He did, however, also clarify that the Vatican's principles on sexuality and contraception had not been changed.\n\nMore generally, some scientific researchers have expressed objective concern over certain ingredients sometimes added to condoms, notably talc and nitrosamines. Dry dusting powders are applied to latex condoms before packaging to prevent the condom from sticking to itself when rolled up. Previously, talc was used by most manufacturers, but cornstarch is currently the most popular dusting powder. Talc is known to be toxic if it enters the abdominal cavity (i.e., via the vagina). Cornstarch is generally believed to be safe; however, some researchers have raised concerns over its use as well.\n\nNitrosamines, which are potentially carcinogenic in humans, are believed to be present in a substance used to improve elasticity in latex condoms. A 2001 review stated that humans regularly receive 1,000 to 10,000 times greater nitrosamine exposure from food and tobacco than from condom use and concluded that the risk of cancer from condom use is very low. However, a 2004 study in Germany detected nitrosamines in 29 out of 32 condom brands tested, and concluded that exposure from condoms might exceed the exposure from food by 1.5- to 3-fold.\n\nIn addition, the large-scale use of disposable condoms has resulted in concerns over their environmental impact via littering and in landfills, where they can eventually wind up in wildlife environments if not incinerated or otherwise permanently disposed of first. Polyurethane condoms in particular, given they are a form of plastic, are not biodegradable, and latex condoms take a very long time to break down. Experts, such as AVERT, recommend condoms be disposed of in a garbage receptacle, as flushing them down the toilet (which some people do) may cause plumbing blockages and other problems. Furthermore, the plastic and foil wrappers condoms are packaged in are also not biodegradable. However, the benefits condoms offer are widely considered to offset their small landfill mass. Frequent condom or wrapper disposal in public areas such as a parks have been seen as a persistent litter problem.\n\nWhile biodegradable, latex condoms damage the environment when disposed of improperly. According to the Ocean Conservancy, condoms, along with certain other types of trash, cover the coral reefs and smother sea grass and other bottom dwellers. The United States Environmental Protection Agency also has expressed concerns that many animals might mistake the litter for food.\n\nIn much of the Western world, the introduction of the pill in the 1960s was associated with a decline in condom use. In Japan, oral contraceptives were not approved for use until September 1999, and even then access was more restricted than in other industrialized nations. Perhaps because of this restricted access to hormonal contraception, Japan has the highest rate of condom usage in the world: in 2008, 80% of contraceptive users relied on condoms.\n\nCultural attitudes toward gender roles, contraception, and sexual activity vary greatly around the world, and range from extremely conservative to extremely liberal. But in places where condoms are misunderstood, mischaracterised, demonised, or looked upon with overall cultural disapproval, the prevalence of condom use is directly affected. In less-developed countries and among less-educated populations, misperceptions about how disease transmission and conception work negatively affect the use of condoms; additionally, in cultures with more traditional gender roles, women may feel uncomfortable demanding that their partners use condoms.\n\nAs an example, Latino immigrants in the United States often face cultural barriers to condom use. A study on female HIV prevention published in the \"Journal of Sex Health Research\" asserts that Latino women often lack the attitudes needed to negotiate safe sex due to traditional gender-role norms in the Latino community, and may be afraid to bring up the subject of condom use with their partners. Women who participated in the study often reported that because of the general machismo subtly encouraged in Latino culture, their male partners would be angry or possibly violent at the woman's suggestion that they use condoms. A similar phenomenon has been noted in a survey of low-income American black women; the women in this study also reported a fear of violence at the suggestion to their male partners that condoms be used.\n\nA telephone survey conducted by Rand Corporation and Oregon State University, and published in the \"Journal of Acquired Immune Deficiency Syndromes\" showed that belief in AIDS conspiracy theories among United States black men is linked to rates of condom use. As conspiracy beliefs about AIDS grow in a given sector of these black men, consistent condom use drops in that same sector. Female use of condoms was not similarly affected.\n\nIn the African continent, condom promotion in some areas has been impeded by anti-condom campaigns by some Muslim and Catholic clerics. Among the Maasai in Tanzania, condom use is hampered by an aversion to \"wasting\" sperm, which is given sociocultural importance beyond reproduction. Sperm is believed to be an \"elixir\" to women and to have beneficial health effects. Maasai women believe that, after conceiving a child, they must have sexual intercourse repeatedly so that the additional sperm aids the child's development. Frequent condom use is also considered by some Maasai to cause impotence. Some women in Africa believe that condoms are \"for prostitutes\" and that respectable women should not use them. A few clerics even promote the idea that condoms are deliberately laced with HIV. In the United States, possession of many condoms has been used by police to accuse women of engaging in prostitution. The Presidential Advisory Council on HIV/AIDS has condemned this practice and there are efforts to end it.\n\nIn March 2013, technology mogul Bill Gates offered a US$100,000 grant through his foundation for a condom design that \"significantly preserves or enhances pleasure\" to encourage more males to adopt the use of condoms for safer sex. The grant information states: \"The primary drawback from the male perspective is that condoms decrease pleasure as compared to no condom, creating a trade-off that many men find unacceptable, particularly given that the decisions about use must be made just prior to intercourse. Is it possible to develop a product without this stigma, or better, one that is felt to enhance pleasure?\". The project has been named the \"Next Generation Condom\" and anyone who can provide a \"testable hypothesis\" is eligible to apply.\n\nMiddle-Eastern couples who have not had children, because of the strong desire and social pressure to establish fertility as soon as possible within marriage, rarely use condoms.\n\nIn 2017, India restricted TV advertisements for condoms to between the hours of 10PM to 6AM. Family planning advocates were against this, saying it was liable to \"undo decades of progress on sexual and reproductive health\".\n\nOne analyst described the size of the condom market as something that \"boggles the mind\". Numerous small manufacturers, nonprofit groups, and government-run manufacturing plants exist around the world. Within the condom market, there are several major contributors, among them both for-profit businesses and philanthropic organizations. Most large manufacturers have ties to the business that reach back to the end of the 19th century.\n\nA spray-on condom made of latex is intended to be easier to apply and more successful in preventing the transmission of diseases. , the spray-on condom was not going to market because the drying time could not be reduced below two to three minutes.\n\nThe Invisible Condom, developed at Université Laval in Quebec, Canada, is a gel that hardens upon increased temperature after insertion into the vagina or rectum. In the lab, it has been shown to effectively block HIV and herpes simplex virus. The barrier breaks down and liquefies after several hours. , the invisible condom is in the clinical trial phase, and has not yet been approved for use.\n\nAlso developed in 2005 is a condom treated with an erectogenic compound. The drug-treated condom is intended to help the wearer maintain his erection, which should also help reduce slippage. If approved, the condom would be marketed under the Durex brand. , it was still in clinical trials. In 2009, Ansell Healthcare, the makers of Lifestyle condoms, introduced the X2 condom lubricated with \"Excite Gel\" which contains the amino acid l-arginine and is intended to improve the strength of the erectile response.\n\n"}
{"id": "8313008", "url": "https://en.wikipedia.org/wiki?curid=8313008", "title": "Condoms, needles, and negotiation", "text": "Condoms, needles, and negotiation\n\nCondoms, needles, and negotiation, also known as the CNN approach, is a harm reduction approach to reducing the rate of transmission of sexually transmitted infections such as HIV/AIDS by:\n\nIn contrast with the abstinence, be faithful, use a condom, or \"ABC\" approach to this problem, the \"CNN\" approach aims primarily at reducing the rate of transmission among high-risk groups such as women in areas where women have low levels of social power, prostitutes and their clients, and intravenous drug users.\n\nPope Benedict XVI has strongly criticized reduction policies with regards to HIV/AIDS, saying that \"it is a tragedy that cannot be overcome by money alone, that cannot be overcome through the distribution of condoms, which even aggravates the problems\". This position has been widely criticised for misrepresenting and oversimplifying the role of condoms in preventing infections. Other experts, including the Director of Harvard University's AIDS Prevention Research Project, have supported the Pope's position.\n\n\n"}
{"id": "7759491", "url": "https://en.wikipedia.org/wiki?curid=7759491", "title": "Crown-rump length", "text": "Crown-rump length\n\nCrown-rump length (CRL) is the measurement of the length of human embryos and fetuses from the top of the head (crown) to the bottom of the buttocks (rump). It is typically determined from ultrasound imagery and can be used to estimate gestational age.\n\nThe embryo and fetus float in the amniotic fluid inside the uterus of the mother usually in a curved posture resembling the letter \"C\". The measurement can actually vary slightly if the fetus is temporarily stretching (straightening) its body. The measurement needs to be in the natural state with an unstretched body which is actually \"C\" shaped. The measurement of CRL is useful in determining the gestational age (menstrual age starting from the first day of the last menstrual period) and thus the expected date of delivery (EDD). Different babies do grow at different rates and thus the gestational age is an approximation. Recent evidence has indicated that CRL growth (and thus the approximation of gestational age) may be influenced by maternal factors such as age, smoking, and folic acid intake. Early in pregnancy it is accurate within +/- 4 days but later in pregnancy due to different growth rates, the accuracy is less. In that situation, other parameters can be used in addition to CRL. The length of the umbilical cord is approximately equal to the CRL throughout pregnancy.\n\nGestational age is not the same as fertilization age. It takes about 14 days from the first day of the last menstrual period for conception to take place and thus for the conceptus to form. The age from this point in time (conception) is called the fertilization age and is thus 2 weeks shorter than the gestational age. Thus a 6-week gestational age would be a 4-week fertilization age. Some authorities however casually interchange these terms and the reader is advised to be cautious. An average gestational period (duration of pregnancy from the first day of the last menstrual period up to delivery) is 280 days. On average, this is 9 months and 6 days.\n\nThe following formula is an approximation that can be used up to 14 weeks of gestational age, with CRL in mm and gestational age in days:\n\nGestational age = (CRL x 1.037) x 8.052 + 23.73\n\n"}
{"id": "42327240", "url": "https://en.wikipedia.org/wiki?curid=42327240", "title": "Dignity of risk", "text": "Dignity of risk\n\nDignity of risk is the idea that self-determination and the right to take reasonable risks are essential for dignity and self esteem and so should not be impeded by excessively-cautious caregivers, concerned about their duty of care. The concept is applicable to adults who are under care such as elderly people, disabled people, and people with mental health problems.\n\nThe concept was first articulated in a 1972 article \"The dignity of risk and the mentally retarded\" by Robert Perske:\nOverprotection may appear on the surface to be kind, but it can be really evil. An oversupply can smother people emotionally, squeeze the life out of their hopes and expectations, and strip them of their dignity. Overprotection can keep people from becoming all they could become. Many of our best achievements came the hard way: We took risks, fell flat, suffered, picked ourselves up, and tried again. Sometimes we made it and sometimes we did not. Even so, we were given the chance to try. Persons with special needs need these chances, too. Of course, we are talking about prudent risks. People should not be expected to blindly face challenges that, without a doubt, will explode in their faces. Knowing which chances are prudent and which are not – this is a new skill that needs to be acquired. On the other hand, a risk is really only when it is not known beforehand whether a person can succeed. The real world is not always safe, secure, and predictable, it does not always say “please,” “excuse me”, or “I’m sorry”. Every day we face the possibility of being thrown into situations where we will have to risk everything … In the past, we found clever ways to build avoidance of risk into the lives of persons living with disabilities. Now we must work equally hard to help find the proper amount of risk these people have the right to take. We have learned that there can be healthy development in risk taking and there can be crippling indignity in safety!\n\nAllowing people under care to take risks is often perceived to be in conflict with the caregivers' duty of care. Finding a balance between these competing considerations can be difficult when formulating policies and guidelines for caregiving.\n\nOverprotection of people with disabilities causes low self-esteem and underachievement because of lowered expectations that come with overprotection. Internalisation of low expectations causes the disabled person to believe that they are less capable than others in similar situations.\n\nIn elderly people, overprotection can result in learned dependency and a decreased ability for self-care:\n\n\"It is possible to deliver physical care that has positive outcomes and returns a person to full function, yet, if during that care they have not been involved, allowed to make choices and respectfully assisted with activities of daily living, it may be possible to cause psychological damage through undermining that person's dignity.\"\n\nThe right to fail and the dignity of risk is one of the basic tenets of the philosophy of the independent living movement.\n\nThe first of eight \"guiding principles\" of the United Nations' Convention on the Rights of Persons with Disabilities states: \"Respect for inherent dignity, individual autonomy including the freedom to make one’s own choices, and independence of persons.\"\n\n"}
{"id": "199433", "url": "https://en.wikipedia.org/wiki?curid=199433", "title": "Early Warning and Response System", "text": "Early Warning and Response System\n\nThe Early Warning and Response System (EWRS) for communicable diseases in the European Union was created by the European Commission to \"ensure a rapid and effective response by the EU to events (including emergencies) related to communicable diseases.\"\n\n"}
{"id": "6716259", "url": "https://en.wikipedia.org/wiki?curid=6716259", "title": "Global Burden of Disease Study", "text": "Global Burden of Disease Study\n\nThe Global Burden of Disease Study (GBD) is a comprehensive regional and global research program of disease burden that assesses mortality and disability from major diseases, injuries, and risk factors. GBD is a collaboration of over 1,800 researchers from 127 countries. Under principal investigator Christopher J.L. Murray, GBD is based out of the Institute for Health Metrics and Evaluation (IHME) at the University of Washington and funded by the Bill and Melinda Gates Foundation.\n\nThe Global Burden of Disease Study began in 1990 as a single World Bank-commissioned study, now called GBD 1990. The original project quantified the health effects of more than 100 diseases and injuries for eight regions of the world, giving estimates of morbidity and mortality by age, sex, and region. It also introduced the disability-adjusted life year (DALY) as a new metric to quantify the burden of diseases, injuries, and risk factors, to aid comparisons. GBD 1990 was \"institutionalized\" at the World Health Organization (WHO) and the research was \"conducted mainly by researchers at Harvard and WHO\".\n\nIn 2000–2002, the 1990 study was updated by WHO to include a more extensive analysis using a framework known as \"comparative risk factor assessment\".\n\nThe WHO estimates were again updated for 2004 in \"The global burden of disease: 2004 update\" (published in 2008) and in \"Global health risks\" (published in 2009).\n\nOfficial DALY estimates had not been updated by WHO since 2004 until the Global Burden of Diseases, Injuries, and Risk Factors Study 2010 (GBD 2010), also known as the Global Burden of Disease Study 2010, was published in December 2012. The work quantified the burdens of 291 major causes of death and disability and 67 risk factors disaggregated by 21 geographic regions and various age–sex groups. GBD 2010 had the Institute for Health Metrics and Evaluation as its coordinating center, but was a collaboration between several institutions including WHO and the Harvard School of Public Health. The work was funded by the Gates Foundation. The GBD 2010 estimates contributed to WHO's own estimates published in 2013, although WHO did not acknowledge the GBD 2010 estimates.\n\nThe Global Burden of Disease Study 2013 (GBD 2013) was published in 2014. The first installment, \"Smoking Prevalence and Cigarette Consumption in 187 Countries, 1980–2012\", was published in the \"Journal of the American Medical Association\" in January, and further installments were published throughout the year. IHME continued to act as the coordinating center for the work.\n\nIn October 2016, Global Burden of Disease Study 2015 (GBD 2015) was published. The work was still coordinated at IHME.\n\nThe following table summarizes GBD's growth over the years.\n\nThe GBD has three specific aims:\n\n\nThe burden of disease can be viewed as the gap between current health status and an ideal situation in which everyone lives into old age free of disease and disability. Causes of the gap are premature mortality, disability and exposure to certain risk factors that contribute to illness.\n\nThe 2013 report showed that global life expectancy for both sexes increased from 65.3 years in 1990, to 71.5 years in 2013, while the number of deaths increased from 47.5 million to 54.9 million over the same interval. Progress varied widely across demographic and national groups. Reductions in age-standardised death rates for cardiovascular diseases and cancers in high-income regions, and reductions in child deaths from diarrhoea, lower respiratory infections and neonatal causes in low-income regions drove the changes. HIV/AIDS reduced life expectancy in southern sub-Saharan Africa.\n\nFor most communicable causes of death both numbers of deaths and age-standardised death rates fell, while for most non-communicable causes, demographic shifts increased numbers of deaths but decreased age-standardised death rates.\n\nGlobal deaths from injury increased by 10.7%, from 4.3 million deaths in 1990 to 4.8 million in 2013; but age-standardised rates declined over the same period by 21%. For some causes of more than 100,000 deaths per year in 2013, age-standardised death rates increased between 1990 and 2013, including HIV/AIDS, pancreatic cancer, atrial fibrillation and flutter, drug use disorders, diabetes, chronic kidney disease and sickle-cell anaemias. Diarrhoeal diseases, lower respiratory infections, neonatal causes and malaria remain in the top five causes of death in children younger than 5 years. The most important pathogens are rotavirus for diarrhoea and pneumococcus for lower respiratory infections.\n\nGBD 2015 found that for the first time, annual deaths from measles had fallen below 100,000 in 2013 and 2015. It also found that the global annual rate of new HIV infections has largely stayed the same during the past 10 years.\n\nGBD 2015 also introduced the Socio-demographic Index (SDI) as a measure of a location's socio-demographic development that takes into account average income per person, educational attainment, and total fertility rate.\n\nThe results of the Global Burden of Disease Study have been cited by \"The New York Times\", \"The Washington Post\", Vox, and \"The Atlantic\".\n\nThe World Health Organization did not acknowledge the GBD 2010 estimates.\n\nThe following is a table of GBD publications .\n\n\"GBD 2010\" proper means the paper was published as part of the original triple issue in \"The Lancet\".\n\n\n"}
{"id": "59105192", "url": "https://en.wikipedia.org/wiki?curid=59105192", "title": "Glucose paradox", "text": "Glucose paradox\n\nThe glucose paradox was the observation that the large amount of glycogen in the liver was not explained by the small amount of glucose absorbed. The explanation was that the majority of glycogen is made from a number of substances other than glucose. The glucose paradox was first formulated by biochemists J. Denis McGarry and Joseph Katz in 1984. \n\nThe glucose paradox demonstrates the importance of the chemical compound lactic acid in the biochemical process of carbohydrate metabolism. The paradox is that the large amount of glycogen (10%) found in the liver cannot be explained by the liver's small absorption of glucose. After the body's digestion of carbohydrates and the entering the circulatory system in the form of glucose, some will be absorbed directly into the muscle tissue and will be converted into lactic acid throughout the anaerobic energy system, rather than going directly to the liver and being converted into glycogen. The lactic acid is then taken and converted by the liver, forming the material for liver glycogen. The majority of the body's liver glycogen is produced indirectly, rather than directly from glucose in the blood. Under normal physiological conditions, glucose is a poor precursor compound and use by the liver is limited.\n"}
{"id": "56797070", "url": "https://en.wikipedia.org/wiki?curid=56797070", "title": "Grossman model of health demand", "text": "Grossman model of health demand\n\nThe Grossman model of health demand is a model for studying the demand for health and medical care outlined by Michael Grossman in a monograph in 1972 entitled: \"The demand for health: A theoretical and empirical investigation\". The model based demand for medical care on the interaction between a demand function for health and a production function for health. Andrew Jones, Nigel Rice, and Paul Contoyannis call the model the \"founding father of demand for health models\".\n\nIn this model, health is a durable capital good which is inherited and depreciates over time. Investment in health takes the form of medical care purchases and other inputs and depreciation is interpreted as natural deterioration of health over time. In the model, health enters the utility function directly as a good people derive pleasure from and indirectly as an investment which makes more healthy time available for market and non-market activities.\n\nThe model creates a dynamic system of equations which can be cast as an optimization problem where utility is optimized over gross investment in health in each period, consumption of medical care, and time inputs in the gross investment function in each period. In this way, the length of life of the agent is partially endogenous to the model.\n\nDynamic optimization problems are often optimized using comparative statics, setting partial derivatives of the outcome function of interest, in this case the utility function, equal to zero. When the partial derivative of the utility function with respect to health consumption is assumed to equal zero, the resulting sub-model is the investment model. Solutions to the problem of this sub-model generally show that the rate of return on health capital must equal the opportunity cost of said capital. Thus, increases in the depreciation rate over time cause the optimal stock of health to decrease. If the marginal efficiency of capital curve is inelastic, gross investment grows over time. In practical terms, this model thus predicts that older people will have more sick time and time spent on increasing health and have higher medical expenditures than younger people. Another implication is that since increases in wages shift the marginal efficiency of capital curve to the right and increases the curve's slope, an increase in wage will increase the demand for health capital.\n\nThe Grossman model was extended in a number of directions. Among the first to include uncertainty in the model were Charles Phelps and Maureen Cropper.\n\nThe relationship between education and health was expanded in the model by Isaac Ehrlich. Regarding the relationship between education and medical care demand, one important question is whether the marginal efficiency of capital elasticity with respect to education is less than or greater than one. If the curve is elastic (elasticity greater than one), education will increase medical care demand. On the other hand, if the curve is inelastic, education will decrease medical care demand.\n\nJan Acton expanded the time constraint by including travel and waiting time in health care.\n\nImportant empirical studies into these components include the RAND Health Insurance Study led by Joseph Newhouse in the 1970s, 1980s, and 1990s. This study sought to estimate income, money price, and time price elasticities of demand for medical care. Another important study was the Oregon Health Insurance in the late 2000s and also estimated the role of public health provision on healthcare demand and was led by Katherine Baicker and Amy Finkelstein.\n\nIn the model, health is neither a pure investment good nor a pure consumption good, but health stock benefits the agent in two ways, directly increasing utility and second by increasing healthy time available for other activities. One early criticism is that framing health as a dicotomous concept is intuitively wrong in that health is simultaneously both and health provides both alternatives simultaneously.\n\n"}
{"id": "5069516", "url": "https://en.wikipedia.org/wiki?curid=5069516", "title": "HIV/AIDS", "text": "HIV/AIDS\n\nHuman immunodeficiency virus infection and acquired immune deficiency syndrome (HIV/AIDS) is a spectrum of conditions caused by infection with the human immunodeficiency virus (HIV). Following initial infection, a person may not notice any symptoms or may experience a brief period of influenza-like illness. Typically, this is followed by a prolonged period with no symptoms. As the infection progresses, it interferes more with the immune system, increasing the risk of developing common infections such as tuberculosis, as well as other opportunistic infections, and tumors that rarely affect people who have working immune systems. These late symptoms of infection are referred to as acquired immunodeficiency syndrome (AIDS). This stage is often also associated with unintended weight loss.\nHIV is spread primarily by unprotected sex (including anal and oral sex), contaminated blood transfusions, hypodermic needles, and from mother to child during pregnancy, delivery, or breastfeeding. Some bodily fluids, such as saliva and tears, do not transmit HIV. Methods of prevention include safe sex, needle exchange programs, treating those who are infected, and male circumcision. Disease in a baby can often be prevented by giving both the mother and child antiretroviral medication. There is no cure or vaccine; however, antiretroviral treatment can slow the course of the disease and may lead to a near-normal life expectancy. Treatment is recommended as soon as the diagnosis is made. Without treatment, the average survival time after infection is 11 years.\nIn 2016, about 36.7 million people were living with HIV and it resulted in 1 million deaths. There were 300,000 fewer new HIV cases in 2016 than in 2015. Most of those infected live in sub-Saharan Africa. From the time AIDS was identified in the early 1980s to 2017, the disease has caused an estimated 35 million deaths worldwide. HIV/AIDS is considered a pandemic—a disease outbreak which is present over a large area and is actively spreading. HIV originated in west-central Africa during the late 19th or early 20th century. AIDS was first recognized by the United States Centers for Disease Control and Prevention (CDC) in 1981 and its cause—HIV infection—was identified in the early part of the decade.\nHIV/AIDS has had a great impact on society, both as an illness and as a source of discrimination. The disease also has large economic impacts. There are many misconceptions about HIV/AIDS such as the belief that it can be transmitted by casual non-sexual contact. The disease has become subject to many controversies involving religion including the Catholic Church's position not to support condom use as prevention. It has attracted international medical and political attention as well as large-scale funding since it was identified in the 1980s.\n\nThere are three main stages of HIV infection: acute infection, clinical latency and AIDS.\n\nThe initial period following the contraction of HIV is called acute HIV, primary HIV or acute retroviral syndrome. Many individuals develop an influenza-like illness or a mononucleosis-like illness 2–4 weeks after exposure while others have no significant symptoms. Symptoms occur in 40–90% of cases and most commonly include fever, large tender lymph nodes, throat inflammation, a rash, headache, tiredness, and/or sores of the mouth and genitals. The rash, which occurs in 20–50% of cases, presents itself on the trunk and is maculopapular, classically. Some people also develop opportunistic infections at this stage. Gastrointestinal symptoms, such as vomiting or diarrhea may occur. Neurological symptoms of peripheral neuropathy or Guillain–Barré syndrome also occurs. The duration of the symptoms varies, but is usually one or two weeks.\n\nDue to their nonspecific character, these symptoms are not often recognized as signs of HIV infection. Even cases that do get seen by a family doctor or a hospital are often misdiagnosed as one of the many common infectious diseases with overlapping symptoms. Thus, it is recommended that HIV be considered in people presenting with an unexplained fever who may have risk factors for the infection.\n\nThe initial symptoms are followed by a stage called clinical latency, asymptomatic HIV, or chronic HIV. Without treatment, this second stage of the natural history of HIV infection can last from about three years to over 20 years (on average, about eight years). While typically there are few or no symptoms at first, near the end of this stage many people experience fever, weight loss, gastrointestinal problems and muscle pains. Between 50 and 70% of people also develop persistent generalized lymphadenopathy, characterized by unexplained, non-painful enlargement of more than one group of lymph nodes (other than in the groin) for over three to six months.\n\nAlthough most HIV-1 infected individuals have a detectable viral load and in the absence of treatment will eventually progress to AIDS, a small proportion (about 5%) retain high levels of CD4 T cells (T helper cells) without antiretroviral therapy for more than 5 years. These individuals are classified as \"HIV controllers\" or long-term nonprogressors (LTNP). Another group consists of those who maintain a low or undetectable viral load without anti-retroviral treatment, known as \"elite controllers\" or \"elite suppressors\". They represent approximately 1 in 300 infected persons.\n\nAcquired immunodeficiency syndrome (AIDS) is defined in terms of either a CD4 T cell count below 200 cells per µL or the occurrence of specific diseases in association with an HIV infection. In the absence of specific treatment, around half of people infected with HIV develop AIDS within ten years. The most common initial conditions that alert to the presence of AIDS are pneumocystis pneumonia (40%), cachexia in the form of HIV wasting syndrome (20%), and esophageal candidiasis. Other common signs include recurrent respiratory tract infections.\nOpportunistic infections may be caused by bacteria, viruses, fungi, and parasites that are normally controlled by the immune system. Which infections occur depends partly on what organisms are common in the person's environment. These infections may affect nearly every organ system.\nPeople with AIDS have an increased risk of developing various viral-induced cancers, including Kaposi's sarcoma, Burkitt's lymphoma, primary central nervous system lymphoma, and cervical cancer. Kaposi's sarcoma is the most common cancer occurring in 10 to 20% of people with HIV. The second most common cancer is lymphoma, which is the cause of death of nearly 16% of people with AIDS and is the initial sign of AIDS in 3 to 4%. Both these cancers are associated with human herpesvirus 8 (HHV-8). Cervical cancer occurs more frequently in those with AIDS because of its association with human papillomavirus (HPV). Conjunctival cancer (of the layer that lines the inner part of eyelids and the white part of the eye) is also more common in those with HIV.\nAdditionally, people with AIDS frequently have systemic symptoms such as prolonged fevers, sweats (particularly at night), swollen lymph nodes, chills, weakness, and unintended weight loss. Diarrhea is another common symptom, present in about 90% of people with AIDS. They can also be affected by diverse psychiatric and neurological symptoms independent of opportunistic infections and cancers.\n\nHIV is transmitted by three main routes: sexual contact, significant exposure to infected body fluids or tissues, and from mother to child during pregnancy, delivery, or breastfeeding (known as vertical transmission). There is no risk of acquiring HIV if exposed to feces, nasal secretions, saliva, sputum, sweat, tears, urine, or vomit unless these are contaminated with blood. It is also possible to be co-infected by more than one strain of HIV—a condition known as HIV superinfection.\n\nThe most frequent mode of transmission of HIV is through sexual contact with an infected person. Globally, the most common mode of HIV transmission is via sexual contacts between people of the opposite sex; however, the pattern of transmission varies among countries. , most HIV transmission in the United States occurred among men who had sex with men (83% of new HIV diagnoses among males aged 13 and older and 67% of total new diagnoses). In the US, gay and bisexual men aged 13 to 24 accounted for an estimated 92% of new HIV diagnoses among all men in their age group and 27% of new diagnoses among all gay and bisexual men. About 15% of gay and bisexual men have HIV while 28% of transgender women test positive in the US.\nWith regard to unprotected heterosexual contacts, estimates of the risk of HIV transmission per sexual act appear to be four to ten times higher in low-income countries than in high-income countries. In low-income countries, the risk of female-to-male transmission is estimated as 0.38% per act, and of male-to-female transmission as 0.30% per act; the equivalent estimates for high-income countries are 0.04% per act for female-to-male transmission, and 0.08% per act for male-to-female transmission. The risk of transmission from anal intercourse is especially high, estimated as 1.41.7% per act in both heterosexual and homosexual contacts. While the risk of transmission from oral sex is relatively low, it is still present. The risk from receiving oral sex has been described as \"nearly nil\"; however, a few cases have been reported. The per-act risk is estimated at 00.04% for receptive oral intercourse. In settings involving prostitution in low income countries, risk of female-to-male transmission has been estimated as 2.4% per act and male-to-female transmission as 0.05% per act.\nRisk of transmission increases in the presence of many sexually transmitted infections and genital ulcers. Genital ulcers appear to increase the risk approximately fivefold. Other sexually transmitted infections, such as gonorrhea, chlamydia, trichomoniasis, and bacterial vaginosis, are associated with somewhat smaller increases in risk of transmission.\n\nThe viral load of an infected person is an important risk factor in both sexual and mother-to-child transmission. During the first 2.5 months of an HIV infection a person's infectiousness is twelve times higher due to the high viral load associated with acute HIV. If the person is in the late stages of infection, rates of transmission are approximately eightfold greater. An HIV-positive person who has an undetectable viral load as a result of long-term treatment has effectively no risk of transmitting HIV sexually.\n\nCommercial sex workers (including those in pornography) have an increased rate of HIV. Rough sex can be a factor associated with an increased risk of transmission. Sexual assault is also believed to carry an increased risk of HIV transmission as condoms are rarely worn, physical trauma to the vagina or rectum is likely, and there may be a greater risk of concurrent sexually transmitted infections.\n\nThe second most frequent mode of HIV transmission is via blood and blood products. Blood-borne transmission can be through needle-sharing during intravenous drug use, needle stick injury, transfusion of contaminated blood or blood product, or medical injections with unsterilized equipment. The risk from sharing a needle during drug injection is between 0.63 and 2.4% per act, with an average of 0.8%. The risk of acquiring HIV from a needle stick from an HIV-infected person is estimated as 0.3% (about 1 in 333) per act and the risk following mucous membrane exposure to infected blood as 0.09% (about 1 in 1000) per act. In the United States intravenous drug users made up 12% of all new cases of HIV in 2009, and in some areas more than 80% of people who inject drugs are HIV positive.\nHIV is transmitted in about 93% of blood transfusions using infected blood. In developed countries the risk of acquiring HIV from a blood transfusion is extremely low (less than one in half a million) where improved donor selection and HIV screening is performed; for example, in the UK the risk is reported at one in five million and in the United States it was one in 1.5 million in 2008. In low income countries, only half of transfusions may be appropriately screened (as of 2008), and it is estimated that up to 15% of HIV infections in these areas come from transfusion of infected blood and blood products, representing between 5% and 10% of global infections. Although rare because of screening, it is possible to acquire HIV from organ and tissue transplantation.\nUnsafe medical injections play a significant role in HIV spread in sub-Saharan Africa. In 2007, between 12 and 17% of infections in this region were attributed to medical syringe use. The World Health Organization estimates the risk of transmission as a result of a medical injection in Africa at 1.2%. Significant risks are also associated with invasive procedures, assisted delivery, and dental care in this area of the world.\n\nPeople giving or receiving tattoos, piercings, and scarification are theoretically at risk of infection but no confirmed cases have been documented. It is not possible for mosquitoes or other insects to transmit HIV.\n\nHIV can be transmitted from mother to child during pregnancy, during delivery, or through breast milk, resulting in the baby also contracting HIV. This is the third most common way in which HIV is transmitted globally. In the absence of treatment, the risk of transmission before or during birth is around 20% and in those who also breastfeed 35%. As of 2008, vertical transmission accounted for about 90% of cases of HIV in children. With appropriate treatment the risk of mother-to-child infection can be reduced to about 1%. Preventive treatment involves the mother taking antiretrovirals during pregnancy and delivery, an elective caesarean section, avoiding breastfeeding, and administering antiretroviral drugs to the newborn. Antiretrovirals when taken by either the mother or the infant decrease the risk of transmission in those who do breastfeed. However, many of these measures are not available in the developing world. If blood contaminates food during pre-chewing it may pose a risk of transmission.\n\nIf a woman is untreated, two years of breastfeeding results in an HIV/AIDS risk in her baby of about 17%. Treatment decreases this risk to 1 to 2% per year. Due to the increased risk of death without breastfeeding in many areas in the developing world, the World Health Organization recommends either: (1) the mother and baby being treated with antiretroviral medication while breastfeeding being continued (2) the provision of safe formula. Infection with HIV during pregnancy is also associated with miscarriage.\n\nHIV is the cause of the spectrum of disease known as HIV/AIDS. HIV is a retrovirus that primarily infects components of the human immune system such as CD4 T cells, macrophages and dendritic cells. It directly and indirectly destroys CD4 T cells.\n\nHIV is a member of the genus \"Lentivirus\", part of the family \"Retroviridae\". Lentiviruses share many morphological and biological characteristics. Many species of mammals are infected by lentiviruses, which are characteristically responsible for long-duration illnesses with a long incubation period. Lentiviruses are transmitted as single-stranded, positive-sense, enveloped RNA viruses. Upon entry into the target cell, the viral RNA genome is converted (reverse transcribed) into double-stranded DNA by a virally encoded reverse transcriptase that is transported along with the viral genome in the virus particle. The resulting viral DNA is then imported into the cell nucleus and integrated into the cellular DNA by a virally encoded integrase and host co-factors. Once integrated, the virus may become latent, allowing the virus and its host cell to avoid detection by the immune system. Alternatively, the virus may be transcribed, producing new RNA genomes and viral proteins that are packaged and released from the cell as new virus particles that begin the replication cycle anew.\n\nHIV is now known to spread between CD4 T cells by two parallel routes: cell-free spread and cell-to-cell spread, i.e. it employs hybrid spreading mechanisms. In the cell-free spread, virus particles bud from an infected T cell, enter the blood/extracellular fluid and then infect another T cell following a chance encounter. HIV can also disseminate by direct transmission from one cell to another by a process of cell-to-cell spread. The hybrid spreading mechanisms of HIV contribute to the virus's ongoing replication against antiretroviral therapies.\n\nTwo types of HIV have been characterized: HIV-1 and HIV-2. HIV-1 is the virus that was originally discovered (and initially referred to also as LAV or HTLV-III). It is more virulent, more infective, and is the cause of the majority of HIV infections globally. The lower infectivity of HIV-2 as compared with HIV-1 implies that fewer people exposed to HIV-2 will be infected per exposure. Because of its relatively poor capacity for transmission, HIV-2 is largely confined to West Africa.\n\nAfter the virus enters the body there is a period of rapid viral replication, leading to an abundance of virus in the peripheral blood. During primary infection, the level of HIV may reach several million virus particles per milliliter of blood. This response is accompanied by a marked drop in the number of circulating CD4 T cells. The acute viremia is almost invariably associated with activation of CD8 T cells, which kill HIV-infected cells, and subsequently with antibody production, or seroconversion. The CD8 T cell response is thought to be important in controlling virus levels, which peak and then decline, as the CD4 T cell counts recover. A good CD8 T cell response has been linked to slower disease progression and a better prognosis, though it does not eliminate the virus.\n\nUltimately, HIV causes AIDS by depleting CD4 T cells. This weakens the immune system and allows opportunistic infections. T cells are essential to the immune response and without them, the body cannot fight infections or kill cancerous cells. The mechanism of CD4 T cell depletion differs in the acute and chronic phases. During the acute phase, HIV-induced cell lysis and killing of infected cells by cytotoxic T cells accounts for CD4 T cell depletion, although apoptosis may also be a factor. During the chronic phase, the consequences of generalized immune activation coupled with the gradual loss of the ability of the immune system to generate new T cells appear to account for the slow decline in CD4 T cell numbers.\n\nAlthough the symptoms of immune deficiency characteristic of AIDS do not appear for years after a person is infected, the bulk of CD4 T cell loss occurs during the first weeks of infection, especially in the intestinal mucosa, which harbors the majority of the lymphocytes found in the body. The reason for the preferential loss of mucosal CD4 T cells is that the majority of mucosal CD4 T cells express the CCR5 protein which HIV uses as a co-receptor to gain access to the cells, whereas only a small fraction of CD4 T cells in the bloodstream do so. A specific genetic change that alters the CCR5 protein when present in both chromosomes very effectively prevents HIV-1 infection.\n\nHIV seeks out and destroys CCR5 expressing CD4 T cells during acute infection. A vigorous immune response eventually controls the infection and initiates the clinically latent phase. CD4 T cells in mucosal tissues remain particularly affected. Continuous HIV replication causes a state of generalized immune activation persisting throughout the chronic phase. Immune activation, which is reflected by the increased activation state of immune cells and release of pro-inflammatory cytokines, results from the activity of several HIV gene products and the immune response to ongoing HIV replication. It is also linked to the breakdown of the immune surveillance system of the gastrointestinal mucosal barrier caused by the depletion of mucosal CD4 T cells during the acute phase of disease.\n\nHIV/AIDS is diagnosed via laboratory testing and then staged based on the presence of certain signs or symptoms. HIV screening is recommended by the United States Preventive Services Task Force for all people 15 years to 65 years of age including all pregnant women. Additionally, testing is recommended for those at high risk, which includes anyone diagnosed with a sexually transmitted illness. In many areas of the world, a third of HIV carriers only discover they are infected at an advanced stage of the disease when AIDS or severe immunodeficiency has become apparent.\n\nMost people infected with HIV develop specific antibodies (i.e. seroconvert) within three to twelve weeks after the initial infection. Diagnosis of primary HIV before seroconversion is done by measuring HIV-RNA or p24 antigen. Positive results obtained by antibody or PCR testing are confirmed either by a different antibody or by PCR.\n\nAntibody tests in children younger than 18 months are typically inaccurate due to the continued presence of maternal antibodies. Thus HIV infection can only be diagnosed by PCR testing for HIV RNA or DNA, or via testing for the p24 antigen. Much of the world lacks access to reliable PCR testing and many places simply wait until either symptoms develop or the child is old enough for accurate antibody testing. In sub-Saharan Africa as of 2007–2009, between 30 and 70% of the population were aware of their HIV status. In 2009, between 3.6 and 42% of men and women in Sub-Saharan countries were tested which represented a significant increase compared to previous years.\n\nTwo main clinical staging systems are used to classify HIV and HIV-related disease for surveillance purposes: the WHO disease staging system for HIV infection and disease, and the CDC classification system for HIV infection. The CDC's classification system is more frequently adopted in developed countries. Since the WHO's staging system does not require laboratory tests, it is suited to the resource-restricted conditions encountered in developing countries, where it can also be used to help guide clinical management. Despite their differences, the two systems allow comparison for statistical purposes.\n\nThe World Health Organization first proposed a definition for AIDS in 1986. Since then, the WHO classification has been updated and expanded several times, with the most recent version being published in 2007. The WHO system uses the following categories:\n\nThe United States Center for Disease Control and Prevention also created a classification system for HIV, and updated it in 2008 and 2014. This system classifies HIV infections based on CD4 count and clinical symptoms, and describes the infection in five groups. In those greater than six years of age it is:\n\nFor surveillance purposes, the AIDS diagnosis still stands even if, after treatment, the CD4 T cell count rises to above 200 per µL of blood or other AIDS-defining illnesses are cured.\n\nConsistent condom use reduces the risk of HIV transmission by approximately 80% over the long term. When condoms are used consistently by a couple in which one person is infected, the rate of HIV infection is less than 1% per year. There is some evidence to suggest that female condoms may provide an equivalent level of protection. Application of a vaginal gel containing tenofovir (a reverse transcriptase inhibitor) immediately before sex seems to reduce infection rates by approximately 40% among African women. By contrast, use of the spermicide nonoxynol-9 may increase the risk of transmission due to its tendency to cause vaginal and rectal irritation.\nCircumcision in Sub-Saharan Africa \"reduces the acquisition of HIV by heterosexual men by between 38% and 66% over 24 months\". Due to these studies, both the World Health Organization and UNAIDS recommended male circumcision in 2007 as a method of preventing female-to-male HIV transmission in areas with high rates of HIV. However, whether it protects against male-to-female transmission is disputed, and whether it is of benefit in developed countries and among men who have sex with men is undetermined. The International Antiviral Society, however, does recommend it for all sexually active heterosexual males and that it be discussed as an option with men who have sex with men. Some experts fear that a lower perception of vulnerability among circumcised men may cause more sexual risk-taking behavior, thus negating its preventive effects.\nPrograms encouraging sexual abstinence do not appear to affect subsequent HIV risk. Evidence of any benefit from peer education is equally poor. Comprehensive sexual education provided at school may decrease high risk behavior. A substantial minority of young people continues to engage in high-risk practices despite knowing about HIV/AIDS, underestimating their own risk of becoming infected with HIV. Voluntary counseling and testing people for HIV does not affect risky behavior in those who test negative but does increase condom use in those who test positive. It is not known whether treating other sexually transmitted infections is effective in preventing HIV.\n\nAntiretroviral treatment among people with HIV whose CD4 count ≤ 550 cells/µL is a very effective way to prevent HIV infection of their partner (a strategy known as treatment as prevention, or TASP). TASP is associated with a 10 to 20 fold reduction in transmission risk. Pre-exposure prophylaxis (PrEP) with a daily dose of the medications tenofovir, with or without emtricitabine, is effective in a number of groups including men who have sex with men, couples where one is HIV positive, and young heterosexuals in Africa. It may also be effective in intravenous drug users with a study finding a decrease in risk of 0.7 to 0.4 per 100 person years. The USPSTF, ini a 2018 draft, recommended PrEP in those who are at high risk.\n\nUniversal precautions within the health care environment are believed to be effective in decreasing the risk of HIV. Intravenous drug use is an important risk factor and harm reduction strategies such as needle-exchange programs and opioid substitution therapy appear effective in decreasing this risk.\n\nA course of antiretrovirals administered within 48 to 72 hours after exposure to HIV-positive blood or genital secretions is referred to as post-exposure prophylaxis (PEP). The use of the single agent zidovudine reduces the risk of a HIV infection five-fold following a needle-stick injury. , the prevention regimen recommended in the United States consists of three medications—tenofovir, emtricitabine and raltegravir—as this may reduce the risk further.\n\nPEP treatment is recommended after a sexual assault when the perpetrator is known to be HIV positive, but is controversial when their HIV status is unknown. The duration of treatment is usually four weeks and is frequently associated with adverse effects—where zidovudine is used, about 70% of cases result in adverse effects such as nausea (24%), fatigue (22%), emotional distress (13%) and headaches (9%).\n\nPrograms to prevent the vertical transmission of HIV (from mothers to children) can reduce rates of transmission by 92–99%. This primarily involves the use of a combination of antiviral medications during pregnancy and after birth in the infant and potentially includes bottle feeding rather than breastfeeding. If replacement feeding is acceptable, feasible, affordable, sustainable, and safe, mothers should avoid breastfeeding their infants; however exclusive breastfeeding is recommended during the first months of life if this is not the case. If exclusive breastfeeding is carried out, the provision of extended antiretroviral prophylaxis to the infant decreases the risk of transmission. In 2015, Cuba became the first country in the world to eradicate mother-to-child transmission of HIV.\n\nCurrently, there is no licensed vaccine for HIV or AIDS. The most effective vaccine trial to date, RV 144, was published in 2009 and found a partial reduction in the risk of transmission of roughly 30%, stimulating some hope in the research community of developing a truly effective vaccine. Further trials of the RV 144 vaccine are ongoing.\n\nThere is currently no cure or effective HIV vaccine. Treatment consists of highly active antiretroviral therapy (HAART) which slows progression of the disease. more than 6.6 million people were taking them in low and middle income countries. Treatment also includes preventive and active treatment of opportunistic infections.\n\nCurrent HAART options are combinations (or \"cocktails\") consisting of at least three medications belonging to at least two types, or \"classes,\" of antiretroviral agents. Initially treatment is typically a non-nucleoside reverse transcriptase inhibitor (NNRTI) plus two nucleoside analog reverse transcriptase inhibitors (NRTIs). Typical NRTIs include: zidovudine (AZT) or tenofovir (TDF) and lamivudine (3TC) or emtricitabine (FTC). Combinations of agents which include protease inhibitors (PI) are used if the above regimen loses effectiveness.\nThe World Health Organization and United States recommends antiretrovirals in people of all ages including pregnant women as soon as the diagnosis is made regardless of CD4 count. Once treatment is begun it is recommended that it is continued without breaks or \"holidays\". Many people are diagnosed only after treatment ideally should have begun. The desired outcome of treatment is a long term plasma HIV-RNA count below 50 copies/mL. Levels to determine if treatment is effective are initially recommended after four weeks and once levels fall below 50 copies/mL checks every three to six months are typically adequate. Inadequate control is deemed to be greater than 400 copies/mL. Based on these criteria treatment is effective in more than 95% of people during the first year.\nBenefits of treatment include a decreased risk of progression to AIDS and a decreased risk of death. In the developing world treatment also improves physical and mental health. With treatment there is a 70% reduced risk of acquiring tuberculosis. Additional benefits include a decreased risk of transmission of the disease to sexual partners and a decrease in mother-to-child transmission. The effectiveness of treatment depends to a large part on compliance. Reasons for non-adherence include poor access to medical care, inadequate social supports, mental illness and drug abuse. The complexity of treatment regimens (due to pill numbers and dosing frequency) and adverse effects may reduce adherence. Even though cost is an important issue with some medications, 47% of those who needed them were taking them in low and middle income countries and the rate of adherence is similar in low-income and high-income countries.\nSpecific adverse events are related to the antiretroviral agent taken. Some relatively common adverse events include: lipodystrophy syndrome, dyslipidemia, and diabetes mellitus, especially with protease inhibitors. Other common symptoms include diarrhea, and an increased risk of cardiovascular disease. Newer recommended treatments are associated with fewer adverse effects. Certain medications may be associated with birth defects and therefore may be unsuitable for women hoping to have children.\nTreatment recommendations for children are somewhat different from those for adults. The World Health Organization recommends treating all children less than 5 years of age; children above 5 are treated like adults. The United States guidelines recommend treating all children less than 12 months of age and all those with HIV RNA counts greater than 100,000 copies/mL between one year and five years of age.\n\nMeasures to prevent opportunistic infections are effective in many people with HIV/AIDS. In addition to improving current disease, treatment with antiretrovirals reduces the risk of developing additional opportunistic infections. Adults and adolescents who are living with HIV (even on anti-retroviral therapy) with no evidence of active tuberculosis in settings with high tuberculosis burden should receive isoniazid preventive therapy (IPT), the tuberculin skin test can be used to help decide if IPT is needed. Vaccination against hepatitis A and B is advised for all people at risk of HIV before they become infected; however it may also be given after infection. Trimethoprim/sulfamethoxazole prophylaxis between four and six weeks of age and ceasing breastfeeding in infants born to HIV positive mothers is recommended in resource limited settings. It is also recommended to prevent PCP when a person's CD4 count is below 200 cells/uL and in those who have or have previously had PCP. People with substantial immunosuppression are also advised to receive prophylactic therapy for toxoplasmosis and MAC. Appropriate preventive measures have reduced the rate of these infections by 50% between 1992 and 1997. Influenza vaccination and pneumococcal polysaccharide vaccine are often recommended in people with HIV/AIDS with some evidence of benefit.\n\nThe World Health Organization (WHO) has issued recommendations regarding nutrient requirements in HIV/AIDS. A generally healthy diet is promoted. Dietary intake of micronutrients at RDA levels by HIV-infected adults is recommended by the WHO; higher intake of vitamin A, zinc, and iron can produce adverse effects in HIV positive adults, and is not recommended unless there is documented deficiency. Dietary supplementation for people who are infected with HIV and who have inadequate nutrition or dietary deficiencies may strengthen their immune systems or help them recover from infections, however evidence indicating an overall benefit in morbidity or reduction in mortality is not consistent.\n\nEvidence for supplementation with selenium is mixed with some tentative evidence of benefit. For pregnant and lactating women with HIV, multivitamin supplement improves outcomes for both mothers and children. If the pregnant or lactating mother has been advised to take anti-retroviral medication to prevent mother-to-child HIV transmission, multivitamin supplements should not replace these treatments. There is some evidence that vitamin A supplementation in children with an HIV infection reduces mortality and improves growth.\n\nIn the US, approximately 60% of people with HIV use various forms of complementary or alternative medicine, even though the effectiveness of most of these therapies has not been established. There is not enough evidence to support the use of herbal medicines. There is insufficient evidence to recommend or support the use of medical cannabis to try to increase appetite or weight gain.\n\nHIV/AIDS has become a chronic rather than an acutely fatal disease in many areas of the world. Prognosis varies between people, and both the CD4 count and viral load are useful for predicted outcomes. Without treatment, average survival time after infection with HIV is estimated to be 9 to 11 years, depending on the HIV subtype. After the diagnosis of AIDS, if treatment is not available, survival ranges between 6 and 19 months. HAART and appropriate prevention of opportunistic infections reduces the death rate by 80%, and raises the life expectancy for a newly diagnosed young adult to 20–50 years. This is between two thirds and nearly that of the general population. If treatment is started late in the infection, prognosis is not as good: for example, if treatment is begun following the diagnosis of AIDS, life expectancy is ~10–40 years. Half of infants born with HIV die before two years of age without treatment.\nThe primary causes of death from HIV/AIDS are opportunistic infections and cancer, both of which are frequently the result of the progressive failure of the immune system. Risk of cancer appears to increase once the CD4 count is below 500/μL. The rate of clinical disease progression varies widely between individuals and has been shown to be affected by a number of factors such as a person's susceptibility and immune function; their access to health care, the presence of co-infections; and the particular strain (or strains) of the virus involved.\n\nTuberculosis co-infection is one of the leading causes of sickness and death in those with HIV/AIDS being present in a third of all HIV-infected people and causing 25% of HIV-related deaths. HIV is also one of the most important risk factors for tuberculosis. Hepatitis C is another very common co-infection where each disease increases the progression of the other. The two most common cancers associated with HIV/AIDS are Kaposi's sarcoma and AIDS-related non-Hodgkin's lymphoma. Other cancers that are more frequent include anal cancer, Burkitt's lymphoma, primary central nervous system lymphoma, and cervical cancer.\n\nEven with anti-retroviral treatment, over the long term HIV-infected people may experience neurocognitive disorders, osteoporosis, neuropathy, cancers, nephropathy, and cardiovascular disease. Some conditions, such as lipodystrophy, may be caused both by HIV and its treatment.\n\nHIV/AIDS is a global pandemic. , approximately 36.7 million people have HIV worldwide with the number of new infections that year being about 1.8 million. This is down from 3.1 million new infections in 2001. Slightly over half the infected population are women and 2.1 million are children. It resulted in about 1 million deaths in 2016, down from a peak of 1.9 million in 2005.\n\nSub-Saharan Africa is the region most affected. In 2010, an estimated 68% (22.9 million) of all HIV cases and 66% of all deaths (1.2 million) occurred in this region. This means that about 5% of the adult population is infected and it is believed to be the cause of 10% of all deaths in children. Here in contrast to other regions women compose nearly 60% of cases. South Africa has the largest population of people with HIV of any country in the world at 5.9 million. Life expectancy has fallen in the worst-affected countries due to HIV/AIDS; for example, in 2006 it was estimated that it had dropped from 65 to 35 years in Botswana. Mother-to-child transmission, , in Botswana and South Africa has decreased to less than 5% with improvement in many other African nations due to improved access to antiretroviral therapy.\n\nSouth & South East Asia is the second most affected; in 2010 this region contained an estimated 4 million cases or 12% of all people living with HIV resulting in approximately 250,000 deaths. Approximately 2.4 million of these cases are in India.\n\nIn 2008 in the United States approximately 1.2 million people were living with HIV, resulting in about 17,500 deaths. The US Centers for Disease Control and Prevention estimated that in 2008 20% of infected Americans were unaware of their infection. about 675,000 people have died of HIV/AIDS in the USA since the beginning of the HIV epidemic. In the United Kingdom there were approximately 101,200 cases which resulted in 594 deaths. In Canada as of 2008 there were about 65,000 cases causing 53 deaths. Between the first recognition of AIDS in 1981 and 2009 it has led to nearly 30 million deaths. Prevalence is lowest in Middle East and North Africa at 0.1% or less, East Asia at 0.1% and Western and Central Europe at 0.2%. The worst affected European countries, in 2009 and 2012 estimates, are Russia, Ukraine, Latvia, Moldova, Portugal and Belarus, in decreasing order of prevalence.\n\nAIDS was first clinically observed in 1981 in the United States. The initial cases were a cluster of injecting drug users and homosexual men with no known cause of impaired immunity who showed symptoms of \"Pneumocystis carinii\" pneumonia (PCP), a rare opportunistic infection that was known to occur in people with very compromised immune systems. Soon thereafter, an unexpected number of homosexual men developed a previously rare skin cancer called Kaposi's sarcoma (KS). Many more cases of PCP and KS emerged, alerting U.S. Centers for Disease Control and Prevention (CDC) and a CDC task force was formed to monitor the outbreak.\n\nIn the early days, the CDC did not have an official name for the disease, often referring to it by way of the diseases that were associated with it, for example, lymphadenopathy, the disease after which the discoverers of HIV originally named the virus. They also used \"Kaposi's sarcoma and opportunistic infections\", the name by which a task force had been set up in 1981. At one point, the CDC coined the phrase \"the 4H disease\", since the syndrome seemed to affect heroin users, homosexuals, hemophiliacs, and Haitians. In the general press, the term \"GRID\", which stood for gay-related immune deficiency, had been coined. However, after determining that AIDS was not isolated to the gay community, it was realized that the term GRID was misleading and the term AIDS was introduced at a meeting in July 1982. By September 1982 the CDC started referring to the disease as AIDS.\n\nIn 1983, two separate research groups led by Robert Gallo and Luc Montagnier declared that a novel retrovirus may have been infecting people with AIDS, and published their findings in the same issue of the journal \"Science\". Gallo claimed that a virus his group had isolated from a person with AIDS was strikingly similar in shape to other human T-lymphotropic viruses (HTLVs) his group had been the first to isolate. Gallo's group called their newly isolated virus HTLV-III. At the same time, Montagnier's group isolated a virus from a person presenting with swelling of the lymph nodes of the neck and physical weakness, two characteristic symptoms of AIDS. Contradicting the report from Gallo's group, Montagnier and his colleagues showed that core proteins of this virus were immunologically different from those of HTLV-I. Montagnier's group named their isolated virus lymphadenopathy-associated virus (LAV). As these two viruses turned out to be the same, in 1986, LAV and HTLV-III were renamed HIV.\n\nBoth HIV-1 and HIV-2 are believed to have originated in non-human primates in West-central Africa and were transferred to humans in the early 20th century. HIV-1 appears to have originated in southern Cameroon through the evolution of SIV(cpz), a simian immunodeficiency virus (SIV) that infects wild chimpanzees (HIV-1 descends from the SIVcpz endemic in the chimpanzee subspecies \"Pan troglodytes troglodytes\"). The closest relative of HIV-2 is SIV(smm), a virus of the sooty mangabey (\"Cercocebus atys atys\"), an Old World monkey living in coastal West Africa (from southern Senegal to western Côte d'Ivoire). New World monkeys such as the owl monkey are resistant to HIV-1 infection, possibly because of a genomic fusion of two viral resistance genes.\nHIV-1 is thought to have jumped the species barrier on at least three separate occasions, giving rise to the three groups of the virus, M, N, and O.\n\nThere is evidence that humans who participate in bushmeat activities, either as hunters or as bushmeat vendors, commonly acquire SIV. However, SIV is a weak virus which is typically suppressed by the human immune system within weeks of infection. It is thought that several transmissions of the virus from individual to individual in quick succession are necessary to allow it enough time to mutate into HIV. Furthermore, due to its relatively low person-to-person transmission rate, SIV can only spread throughout the population in the presence of one or more high-risk transmission channels, which are thought to have been absent in Africa before the 20th century.\n\nSpecific proposed high-risk transmission channels, allowing the virus to adapt to humans and spread throughout the society, depend on the proposed timing of the animal-to-human crossing. Genetic studies of the virus suggest that the most recent common ancestor of the HIV-1 M group dates back to circa 1910. Proponents of this dating link the HIV epidemic with the emergence of colonialism and growth of large colonial African cities, leading to social changes, including a higher degree of sexual promiscuity, the spread of prostitution, and the accompanying high frequency of genital ulcer diseases (such as syphilis) in nascent colonial cities. While transmission rates of HIV during vaginal intercourse are low under regular circumstances, they are increased many fold if one of the partners suffers from a sexually transmitted infection causing genital ulcers. Early 1900s colonial cities were notable due to their high prevalence of prostitution and genital ulcers, to the degree that, as of 1928, as many as 45% of female residents of eastern Kinshasa were thought to have been prostitutes, and, as of 1933, around 15% of all residents of the same city had syphilis.\n\nAn alternative view holds that unsafe medical practices in Africa after World War II, such as unsterile reuse of single use syringes during mass vaccination, antibiotic and anti-malaria treatment campaigns, were the initial vector that allowed the virus to adapt to humans and spread.\n\nThe earliest well-documented case of HIV in a human dates back to 1959 in the Congo. The earliest retrospectively described case of AIDS is believed to have been in Norway beginning in 1966. In July 1960, in the wake of Congo's independence, the United Nations recruited Francophone experts and technicians from all over the world to assist in filling administrative gaps left by Belgium, who did not leave behind an African elite to run the country. By 1962, Haitians made up the second largest group of well-educated experts (out of the 48 national groups recruited), that totaled around 4500 in the country. Dr. Jacques Pépin, a Quebecer author of \"The Origins of AIDS\", stipulates that Haiti was one of HIV's entry points to the United States and that one of them may have carried HIV back across the Atlantic in the 1960s. Although the virus may have been present in the United States as early as 1966, the vast majority of infections occurring outside sub-Saharan Africa (including the U.S.) can be traced back to a single unknown individual who became infected with HIV in Haiti and then brought the infection to the United States some time around 1969. The epidemic then rapidly spread among high-risk groups (initially, sexually promiscuous men who have sex with men). By 1978, the prevalence of HIV-1 among homosexual male residents of New York City and San Francisco was estimated at 5%, suggesting that several thousand individuals in the country had been infected.\n\nAIDS stigma exists around the world in a variety of ways, including ostracism, rejection, discrimination and avoidance of HIV infected people; compulsory HIV testing without prior consent or protection of confidentiality; violence against HIV infected individuals or people who are perceived to be infected with HIV; and the quarantine of HIV infected individuals. Stigma-related violence or the fear of violence prevents many people from seeking HIV testing, returning for their results, or securing treatment, possibly turning what could be a manageable chronic illness into a death sentence and perpetuating the spread of HIV.\n\nAIDS stigma has been further divided into the following three categories:\n\nOften, AIDS stigma is expressed in conjunction with one or more other stigmas, particularly those associated with homosexuality, bisexuality, promiscuity, prostitution, and intravenous drug use.\n\nIn many developed countries, there is an association between AIDS and homosexuality or bisexuality, and this association is correlated with higher levels of sexual prejudice, such as anti-homosexual/bisexual attitudes. There is also a perceived association between AIDS and all male-male sexual behavior, including sex between uninfected men. However, the dominant mode of spread worldwide for HIV remains heterosexual transmission.\n\nIn 2003, as part of an overall reform of marriage and population legislation, it became legal for people with AIDS to marry in China.\n\nIn 2013 the U.S. National Library of Medicine developed a traveling exhibition titled, \"Surviving and Thriving: AIDS, Politics, and Culture\", covering medical research, U.S. government's response, and personal stories from people with AIDS, caregivers, and activists.\n\nHIV/AIDS affects the economics of both individuals and countries. The gross domestic product of the most affected countries has decreased due to the lack of human capital. Without proper nutrition, health care and medicine, large numbers of people die from AIDS-related complications. They will not only be unable to work, but will also require significant medical care. It is estimated that as of 2007 there were 12 million AIDS orphans. Many are cared for by elderly grandparents.\n\nReturning to work after beginning treatment for HIV/AIDS is difficult, and affected people often work less than the average worker. Unemployment in people with HIV/AIDS also is associated with suicidal ideation, memory problems, and social isolation. Employment increases self-esteem, sense of dignity, confidence, and quality of life for people with HIV/AIDS. Anti-retroviral treatment may help people with HIV/AIDS work more, and may increase the chance that a person with HIV/AIDS will be employed (low quality evidence).\n\nBy affecting mainly young adults, AIDS reduces the taxable population, in turn reducing the resources available for public expenditures such as education and health services not related to AIDS resulting in increasing pressure for the state's finances and slower growth of the economy. This causes a slower growth of the tax base, an effect that is reinforced if there are growing expenditures on treating the sick, training (to replace sick workers), sick pay and caring for AIDS orphans. This is especially true if the sharp increase in adult mortality shifts the responsibility and blame from the family to the government in caring for these orphans.\n\nAt the household level, AIDS causes both loss of income and increased spending on healthcare. A study in Côte d'Ivoire showed that households having a person with HIV/AIDS spent twice as much on medical expenses as other households. This additional expenditure also leaves less income to spend on education and other personal or family investment.\n\nThe topic of religion and AIDS has become highly controversial in the past twenty years, primarily because some religious authorities have publicly declared their opposition to the use of condoms. The religious approach to prevent the spread of AIDS according to a report by American health expert Matthew Hanley titled \"The Catholic Church and the Global AIDS Crisis\" argues that cultural changes are needed including a re-emphasis on fidelity within marriage and sexual abstinence outside of it.\n\nSome religious organizations have claimed that prayer can cure HIV/AIDS. In 2011, the BBC reported that some churches in London were claiming that prayer would cure AIDS, and the Hackney-based Centre for the Study of Sexual Health and HIV reported that several people stopped taking their medication, sometimes on the direct advice of their pastor, leading to a number of deaths. The Synagogue Church Of All Nations advertised an \"anointing water\" to promote God's healing, although the group denies advising people to stop taking medication.\n\nOne of the first high-profile cases of AIDS was the American Rock Hudson, a gay actor who had been married and divorced earlier in life, who died on October 2, 1985 having announced that he was suffering from the virus on July 25 that year. He had been diagnosed during 1984. A notable British casualty of AIDS that year was Nicholas Eden, a gay politician and son of the late prime minister Anthony Eden. On November 24, 1991, the virus claimed the life of British rock star Freddie Mercury, lead singer of the band Queen, who died from an AIDS-related illness having only revealed the diagnosis on the previous day. However, he had been diagnosed as HIV positive in 1987. One of the first high-profile heterosexual cases of the virus was Arthur Ashe, the American tennis player. He was diagnosed as HIV positive on August 31, 1988, having contracted the virus from blood transfusions during heart surgery earlier in the 1980s. Further tests within 24 hours of the initial diagnosis revealed that Ashe had AIDS, but he did not tell the public about his diagnosis until April 1992. He died as a result on February 6, 1993 at age 49.\n\nTherese Frare's photograph of gay activist David Kirby, as he lay dying from AIDS while surrounded by family, was taken in April 1990. \"LIFE magazine\" said the photo became the one image \"most powerfully identified with the HIV/AIDS epidemic.\" The photo was displayed in \"LIFE magazine\", was the winner of the World Press Photo, and acquired worldwide notoriety after being used in a United Colors of Benetton advertising campaign in 1992.\nIn 1996, Johnson Aziga, a Ugandan-born Canadian was diagnosed with HIV, but subsequently had unprotected sex with 11 women without disclosing his diagnosis. By 2003 seven had contracted HIV, and two died from complications related to AIDS. Aziga was convicted of first-degree murder and was sentenced for life.\n\nCriminal transmission of HIV is the intentional or reckless infection of a person with the human immunodeficiency virus (HIV). Some countries or jurisdictions, including some areas of the United States, have laws that criminalize HIV transmission or exposure. Others may charge the accused under laws enacted before the HIV pandemic.\n\nThere are many misconceptions about HIV and AIDS. Three of the most common are that AIDS can spread through casual contact, that sexual intercourse with a virgin will cure AIDS, and that HIV can infect only gay men and drug users. In 2014, some among the British public wrongly thought one could get HIV from kissing (16%), sharing a glass (5%), spitting (16%), a public toilet seat (4%), and coughing or sneezing (5%). Other misconceptions are that any act of anal intercourse between two uninfected gay men can lead to HIV infection, and that open discussion of HIV and homosexuality in schools will lead to increased rates of AIDS.\n\nA small group of individuals continue to dispute the connection between HIV and AIDS, the existence of HIV itself, or the validity of HIV testing and treatment methods. These claims, known as AIDS denialism, have been examined and rejected by the scientific community. However, they have had a significant political impact, particularly in South Africa, where the government's official embrace of AIDS denialism (1999–2005) was responsible for its ineffective response to that country's AIDS epidemic, and has been blamed for hundreds of thousands of avoidable deaths and HIV infections.\n\nSeveral discredited conspiracy theories have held that HIV was created by scientists, either inadvertently or deliberately. Operation INFEKTION was a worldwide Soviet active measures operation to spread the claim that the United States had created HIV/AIDS. Surveys show that a significant number of people believed – and continue to believe – in such claims.\n\nHIV/AIDS research includes all medical research which attempts to prevent, treat, or cure HIV/AIDS along with fundamental research about the nature of HIV as an infectious agent and AIDS as the disease caused by HIV.\n\nMany governments and research institutions participate in HIV/AIDS research. This research includes behavioral health interventions such as sex education, and drug development, such as research into microbicides for sexually transmitted diseases, HIV vaccines, and antiretroviral drugs. Other medical research areas include the topics of pre-exposure prophylaxis, post-exposure prophylaxis, and circumcision and HIV. Public health officials, researchers, and programs can gain a more comprehensive picture of the barriers they face, and the efficacy of current approaches to HIV treatment and prevention, by tracking standard HIV indicators. Use of common indicators is an increasing focus of development organizations and researchers.\n\n"}
{"id": "528679", "url": "https://en.wikipedia.org/wiki?curid=528679", "title": "Hazard symbol", "text": "Hazard symbol\n\nHazard symbols or warning symbols are recognisable symbols designed to warn about hazardous or dangerous materials, locations, or objects, including electric currents, poisons, and radioactivity. The use of hazard symbols is often regulated by law and directed by standards organisations. Hazard symbols may appear with different colors, backgrounds, borders and supplemental information in order to specify the type of hazard and the level of threat (for example, toxicity classes). Warning symbols are used in many places in lieu of or addition to written warnings as they are quickly recognized (faster than reading a written warning) and more commonly understood (the same symbol can be recognized as having the same meaning to speakers of different languages).\n\nOn roadside warning signs, an exclamation mark is often used to draw attention to a generic warning of danger, hazards, and the unexpected. In Europe, this type of sign is used if there are no more-specific signs to denote a particular hazard. When used for traffic signs, it is accompanied by a supplementary sign describing the hazard, usually mounted under the exclamation mark.\n\nThis symbol has also been more widely adopted for generic use in many other contexts not associated with road traffic. It often appears on hazardous equipment or in instruction manuals to draw attention to a precaution, when a more-specific warning symbol is not available.\n\nThe skull-and-crossbones symbol (☠), consisting of a human skull and two bones crossed together behind the skull, is today generally used as a warning of danger, particularly in regard to poisonous substances.\n\nThe symbol, or some variation thereof, specifically with the bones (or swords) below the skull, was also featured on the Jolly Roger, the traditional flag of European and American seagoing pirates. It is also part of the Canadian WHMIS home symbols placed on containers to warn that the contents are poisonous.\n\nIn the United States, due to concerns that the skull-and-crossbones symbol's association with pirates might encourage children to play with toxic materials, the Mr. Yuk symbol is also used to denote poison.\n\nThe international radiation symbol (also known as the trefoil) first appeared in 1946, at the University of California, Berkeley Radiation Laboratory. At the time, it was rendered as magenta, and was set on a blue background. The original version used in America is magenta against a yellow background, and it is drawn with a central circle of radius \"R\", an internal radius of 1.5\"R\" and an external radius of 5\"R\" for the blades, which are separated from each other by 60°. The trefoil is black in the international version, which is also used in America.\n\nThe sign is commonly referred to as a radioactivity warning sign, but it is actually a warning sign of ionizing radiation. Ionizing radiation is a much broader category than radioactivity alone, as many non-radioactive sources also emit potentially dangerous levels of ionizing radiation. This includes x-ray apparatus, radiotherapy linear accelerators, and particle accelerators. Non-ionizing radiation can also reach potentially dangerous levels, but this warning sign is different from the trefoil ionizing radiation warning symbol.\n\nOn February 15, 2007, two groups—the International Atomic Energy Agency (IAEA) and the International Organization for Standardization (ISO)—jointly announced the adoption of a new ionizing radiation warning symbol to supplement the traditional trefoil symbol. The new symbol, to be used on sealed radiation sources, is aimed at alerting anyone, anywhere to the danger of being close to a strong source of ionizing radiation. It depicts, on a red background, a black trefoil with waves of radiation streaming from it, along with a black skull and crossbones, and a running figure with an arrow pointing away from the scene. The radiating trefoil suggests the presence of radiation, while the red background and the skull and crossbones warn of the danger. The figure running away from the scene is meant to suggest taking action to avoid the labeled material. The new symbol is not intended to be generally visible, but rather to appear on internal components of devices that house radiation sources so that if anybody attempts to disassemble such devices they will see an explicit warning not to proceed any further.\n\nThe biohazard symbol is used in the labeling of biological materials that carry a significant health risk (biohazards), including viral samples and used hypodermic needles (see sharps waste).\n\nThe biohazard symbol was developed by the Dow Chemical Company in 1966 for their containment products.\n\nAccording to Charles Baldwin, an environmental-health engineer who contributed to its development: \"We wanted something that was memorable but meaningless, so we could educate people as to what it means.\" In an article in \"Science\" in 1967, the symbol was presented as the new standard for all biological hazards (\"biohazards\"). The article explained that over 40 symbols were drawn up by Dow artists, and all of the symbols investigated had to meet a number of criteria: \"(i) striking in form in order to draw immediate attention; (ii) unique and unambiguous, in order not to be confused with symbols used for other purposes; (iii) quickly recognizable and easily recalled; (iv) easily stenciled; (v) symmetrical, in order to appear identical from all angles of approach; and (vi) acceptable to groups of varying ethnic backgrounds.\" The chosen scored the best on nationwide testing for memorability.\n\nAll parts of the biohazard sign can be drawn with a compass and straightedge. The basic outline of the symbol is a plain trefoil, which is three circles overlapping each other equally like in a triple Venn diagram with the overlapping parts erased. The diameter of the overlapping part is equal to half the radius of the three circles. Then three inner circles are drawn in with radius of the original circles so that it is tangent to the outside three overlapping circles. A tiny circle in center has a diameter of the radius of the three inner circles, and arcs are erased at 90°, 210°, and 330°. The arcs of the inner circles and the tiny circle are connected by a line. Finally, the ring under is drawn from the distance to the perimeter of the equilateral triangle that forms between the centers of the three intersecting circles. An outer circle of the ring under is drawn and finally enclosed with the arcs from the center of the inner circles with a shorter radius from the inner circles.\n\nA chemical hazard symbol is a pictogram applied to containers of dangerous chemical compounds to indicate the specific hazard, and thus the required precautions. There are several systems of labels, depending on the purpose, such as on the container for end use, or on a vehicle during transportation. \n\nThe United Nations has designed GHS hazard pictograms and GHS hazard statements to internationally harmonize chemical hazard warnings. Several European countries have started to implement these new global standards, but older warning symbols are still used in many parts of the world.\n\nEuropean standards are set by:\n\nThe Workplace Hazardous Materials Information System, or WHMIS, is Canada's national workplace hazard communication standard.\n\nThe US-based National Fire Protection Association (NFPA) has a standard NFPA 704 using a diamond with four colored sections each with a number indicating severity 0—4 (0 for no hazard, 4 indicates a severe hazard). The red section denotes flammability. The blue section denotes health risks. Yellow represents reactivity (tendency to explode). The white section denotes special hazard information. One example of a special hazard would be the capital letter W crossed out (pictured left), indicating it is water reactant. \n\nA large number of warning symbols with non-standard designs are in use around the world.\n\nSome warning symbols have been redesigned to be more comprehensible to children, such as the Mr. Ouch (depicting an electricity danger as a snarling, spiky creature) and Mr. Yuk (a green frowny face sticking its tongue out, to represent poison) designs in the United States.\n\n\n"}
{"id": "39842091", "url": "https://en.wikipedia.org/wiki?curid=39842091", "title": "Heterogeneous condition", "text": "Heterogeneous condition\n\nHeterogeneous medical condition in medicine are those medical conditions which have several etiologies, like hepatitis or diabetes. Medical conditions are normally defined pathologically (liver inflammation) or clinically (excessive urination) and not etiologically, and therefore it is normal to have more than one cause for them. The word is used as an opposition to homogeneous, meaning that given a group of patients, the disease is the same for all of them.\n\nWhen a condition is heterogeneous, it is normally divided in endotypes.\n\nAn endotype is a subtype of a condition, which is defined by a distinct functional or pathobiological mechanism. This is distinct from a phenotype, which is any observable characteristic or trait of a disease, such as morphology, development, biochemical or physiological properties, or behavior, without any implication of a mechanism. It is envisaged that patients with a specific endotype present themselves within phenotypic clusters of diseases.\n\nOne example is asthma, which is considered to be a syndrome, consisting of a series of endotypes. This is related to the concept of disease entity\n\nThe term medical condition is a nosological broad term that includes all diseases, disorders, injuries and syndromes, and it is specially suitable in the last case, in which it is not possible to speak about a single disease associated to the clinical course of the patient.\n\nWhile the term \"medical condition\" generally includes mental illnesses, in some contexts the term is used specifically to denote any illness, injury, or disease except for mental illnesses. The Diagnostic and Statistical Manual of Mental Disorders (DSM), the widely used psychiatric manual that defines all mental disorders, uses the term \"general medical condition\" to refer to all diseases, illnesses, and injuries except for mental disorders. This usage is also commonly seen in the psychiatric literature. Some health insurance policies also define a \"medical condition\" as any illness, injury, or disease except for psychiatric illnesses.\n\nAs it is more value-neutral than terms like \"disease\", the term \"medical condition\" is sometimes preferred by people with health issues that they do not consider deleterious. It is also preferred when etiology is not unique, because the word disease is normally associated to the cause of the clinical problems. On the other hand, by emphasizing the medical nature of the condition, this term is sometimes rejected, such as by proponents of the autism rights movement.\n\nThe term is also used in specialized areas of the medicine. A genetic or allelic heterogeneous condition is one where the same disease or condition can be caused, or contributed to, by varying different genes or alleles. In clinical trials and statistics the concepts of homogeneous and heterogeneous populations is important. The same applies for epidemiology\n\n"}
{"id": "7963692", "url": "https://en.wikipedia.org/wiki?curid=7963692", "title": "Hispanic paradox", "text": "Hispanic paradox\n\nThe Hispanic paradox, or Latino paradox, also known as the \"epidemiologic paradox,\" refers to the epidemiological finding that Hispanic and Latino Americans tend to have health outcomes that \"paradoxically\" are comparable to, or in some cases better than, those of their U.S. non-Hispanic White counterparts, even though Hispanics have lower average income and education. (Low socioeconomic status is almost universally associated with worse population health and higher death rates everywhere in the world.) The paradox usually refers in particular to low mortality among Latinos in the United States relative to non-Hispanic Whites. First coined the \"Hispanic Epidemiological Paradox\" in 1986 by Kyriakos Markides, the phenomenon is also known as the \"Latino Epidemiological Paradox\". According to Markides, a professor of sociomedical sciences at the University of Texas Medical Branch in Galveston, this paradox was ignored by past generations, but is now \"the leading theme in the health of the Hispanic population in the United States.\"\n\nThe specific cause of the phenomenon is poorly understood, although the decisive factor appears to be place of birth, raising the possibility that differing birthing or neonatal practices might be involved via a lack of breastfeeding combined with birth trauma imprinting (both common in American obstetrics) and consequent mental and physical illness, the latter compounded by the impact of psychological problems on the capacity for social networking. It appears that the Hispanic Paradox cannot be explained by either the \"salmon bias hypothesis\" or the \"healthy migrant effect,\" two theories that posit low mortality among immigrants due to, respectively, a possible tendency for sick immigrants to return to their home country before death and a possible tendency for new immigrants to be unusually healthy compared to the rest of their home-country population. Historical differences in smoking habits by ethnicity and place of birth may explain much of the paradox, at least at adult ages.\nOthers have proposed that the lower mortality of Hispanics could reflect a slower biological aging rate of Hispanics. However, some believe that there is no Hispanic Paradox, and that inaccurate counting of Hispanic deaths in the United States leads to an underestimate of Hispanic or Latino mortality.\n\nThough they are often at lower socioeconomic standing, most Hispanic groups, excepting Puerto Ricans, demonstrate lower or equal levels of mortality to their non-Hispanic White counterparts. The Center for Disease Control reported in 2003 that Hispanic’s mortality rate was 25 percent lower than non-Hispanic whites and 43 percent lower than African Americans. This mortality advantage most commonly found among middle-aged and elderly Hispanics. The death rates of Hispanics to non-Hispanic whites was found to exceed 1.00 in the twenties, decreases by age 45, then is severely reduced to 0.75-.90 by at age 65, persisting until death. When controlling for socioeconomic factors, the health advantage gap for Mexican Americans, the largest Hispanic population in the US, increases noticeably.\n\nHispanics do not have a mortality advantage over non-Hispanic Whites in all mortality rates; they have higher rates for mortality from liver disease, cervical cancer, AIDS, homicide (males), and diabetes.\n\nAnother important indicator of health is the infant mortality rate, which is also either equal or better in Hispanic Americans than in non-Hispanic Americans. A study by Hummer, et al. found that infants born to Mexican Immigrant women in the United States have about a 10% lower mortality in the first hour, first day, and first week than that of infants born to non-Hispanic white, U.S.-born women. In 2003, the national Hispanic infant mortality rate was found to be 5.7, nearly equal to that of non-Hispanic Americans and 58 percent lower than that of African Americans. Hispanic immigrants also have a 20% lower infant mortality rate than that of U.S.-born Hispanics, though the latter population usually has a higher income and education, and are much more likely to have health insurance.\n\nAccording to Alder and Estrove (2006), the more socioeconomically advantaged individuals are, the better their health. Access to health insurance and preventative medical services are one of the main reasons for socioeconomic health disparities. Economic hardship within the household can cause distress and affect parenting, causing health problems among children leading to depression, substance abuse, and behavior problems. Low socioeconomic status is correlated with increased rates of morbidity and mortality. Mental health disorders are an important health problem for those of low socioeconomic status; they are two to five times more likely to suffer from a diagnosable disorder than those of high socioeconomic status, and are more likely to face barriers to getting treatment. Furthermore, this lack of treatment for mental disorders can affect educational and employment opportunities and achievement.\n\nImportant to the understanding of migrant community health is the increasingly stratified American society, manifested in residential segregation. Beginning in the 1970s, the low to moderate levels of income segregation in the United States began to degrade. As the rich became richer, so did their neighborhoods. This trend was inversely reflected in the poor, as their neighborhoods became poorer. As sociologist Douglas Massey explains, “As a result, poverty and affluence both became more concentrated geographically.” Professor of public administration and economics John Yinger writes that “one way for poor people to win the spatial competition for housing is to rent small or low-quality housing.” However, he continues, low-quality housing often features serious health risks such as lead paint and animal pests. Though lead-based paint was deemed illegal in 1978, it remains on the walls of older apartments and houses, posing a serious neurological risk to children. Asthma, a possible serious health risk, also has a clear link to poverty. Moreover, asthma attacks have been associated with certain aspects of poor housing quality such as the presence of cockroaches, mice, dust, dust mites, mold, and mildew. The 1997 American Housing Survey found that signs of rats or mice are almost twice as likely to be detected in poor households as in non-poor households.\n\nOne hypothesis for the Hispanic Paradox proposes that living in the same neighborhood as people with similar ethnic backgrounds confers significant advantages to one’s health. In a study of elderly Mexican-Americans, those living in areas with a higher percentage of Mexican-Americans had lower seven-year mortality as well as a decreased prevalence of medical conditions, including stroke, cancer, and hip fracture. Despite these neighborhoods' relatively high rates of poverty due to lack of formal education and a preponderance of low paying service sector jobs, residents do not suffer from the same mortality and morbidity levels seen in similarly disadvantaged socioeconomic neighborhoods. These neighborhoods do have intact family structures, community institutions, and kinship structures that span households, all of which are thought to provide significant benefits to an individual’s health. These social network support structures are especially important to the health of the elderly population as they deal with declining physical function. Another reason for this phenomenon could be that those Hispanic-Americans that live among those of similar cultural and social backgrounds are shielded from some of the negative effects of assimilation to American culture.\n\nThe extent of a Hispanic American’s acculturation in the United States, or their assimilation to mainstream American culture, is relative to his or her health. One of the main negative effects of acculturation on health has been on substance abuse. More assimilated Latinos have higher rates of illicit drug use, alcohol consumption, and smoking, especially among women. Another negative effect of acculturation is changes in diet and nutrition. More acculturated Latinos eat less fruits, vegetables, vitamins, fiber and protein and consume more fat than their less acculturated counterparts. One of the most significant impacts of acculturation on Latino health is birth outcomes. Studies have found that more acculturated Latinas have higher rates of low birthweight, premature births, teenage pregnancy and undesirable prenatal and postnatal behaviors such as smoking or drinking during pregnancy, and lower rates of breastfeeding. Acculturation and greater time in the United States has also been associated with negative mental health impacts. US-born Latinos or long term residents of the United States had higher rates of mental illness than recent Latino immigrants. In addition, foreign-born Mexican Americans are at significantly lower risk of suicide and depression than those born in the United States. The increased rates of mental illness is thought to be due to increased distress associated with alienation, discrimination and Mexican Americans attempting to advance themselves economically and socially stripping themselves of traditional resources and ethnically-based social support.\n\nThe “healthy migrant effect” hypothesizes that the selection of healthy Hispanic immigrants into the United States is reason for the paradox. International immigration statistics demonstrate that the mortality rate of immigrants is lower than in their country of origin. In the United States, foreign-born individuals have better self-reported health than American-born respondents. Furthermore, Hispanic immigrants have better health than those living in the US for a long amount of time.\n\nA second popular hypothesis, called the “Salmon Bias”, attempts to factor in the occurrence of returning home. This hypothesis purports that many Hispanic people return home after temporary employment, retirement, or severe illness, meaning that their deaths occur in their native land and are not taken into account by mortality reports in the United States. This hypothesis considers those people as “statistically immortal” because they artificially lower the Hispanic mortality rate. Certain studies hint that it could be reasonable. These studies report that though return migration, both temporary and permanent, depend upon specific economic and social situations in communities, up to 75% of household in immigrant neighborhoods do some kind of return migration from the U.S. However, Abraido-Lanza, \"et al.\" found in 1999 that the “Salmon Hypothesis” cannot account for the lower mortality of Hispanics in the US because, according to their findings, the Hispanic paradox is still present when non-returning migrants are observed (e.g. Cubans).\n\nHorvath et al. (2013) have proposed that the lower mortality of Hispanics could reflect a slower biological aging rate of Hispanics. This hypothesis is based on the finding that blood and saliva from Hispanics ages more slowly than that of non-Hispanic whites, African Americans, and other populations according to a biomarker of tissue age known as epigenetic clock.\n\nOne of the most important aspects of this phenomenon is the comparison of Hispanics' health to non-Hispanic African Americans' health. Both the current and historical poverty rates for Hispanic and non-Hispanic African American populations in the United States are consistently starkly higher than that of non-Hispanic White and non-Hispanic Asian Americans. Dr. Hector Flores explains that “You can predict in the African–American population, for example, a high infant-mortality rate, so we would think a [similar] poor minority would have the same health outcomes.” However, he said, the health poor outcomes are not present in the Hispanic population. For example, the age-adjusted mortality rate for Hispanics living in Los Angeles County was 52 percent less than the blacks living in the same county.\n\nSome public health researchers have argued that the Hispanic paradox is not actually a national phenomenon in the United States. In 2006, Smith and Bradshaw argued that no Hispanic paradox exists. They maintain that life expectancies were nearly equal for non-Hispanic White and Hispanic females, but less close for non-Hispanic White and Hispanic Males. Turra and Goldman argue that the paradox is concentrated among the foreign born from specific national origins, and is only present in those of middle to older ages. At younger ages, they explain, deaths are highly related to environmental factors such as homicides and accidents. Deaths at older ages, they maintain, are more related to detrimental health-related behaviors and health status at younger ages. Therefore, immigration-related processes only offer survival protection to those at middle and older ages; the negative impact of assimilation into poor neighborhoods is higher on the mortality of immigrants at a younger age. In contrast, Palloni and Arias hypothesize that this phenomenon is most likely caused by across-the-board bias in underestimating mortality rates, caused by ethnic misidentification or an overstatement of ages. These errors could also be related to mistakes in matching death records to the National Health Interview Survey, missing security numbers, or complex surnames.\n\n"}
{"id": "11186931", "url": "https://en.wikipedia.org/wiki?curid=11186931", "title": "Hydrocolpos", "text": "Hydrocolpos\n\nHydrocolpos is the distension of the vagina caused by accumulation of fluid due to congenital vaginal obstruction. The obstruction is often caused by an imperforate hymen or less commonly a transverse vaginal septum. The fluid consists of cervical and endometrial mucus or in rare instances urine accumulated through a vesicovaginal fistula proximal to the obstruction. In some cases, it is associated with Bardet-Biedl Syndrome. If it occurs in prepubertal girls, it may show up as abdominal swelling. It may be detected by using ultrasound. It may also present at birth as a distended lower abdomen and vagina. It also associated with vaginal atresia.\n"}
{"id": "31403992", "url": "https://en.wikipedia.org/wiki?curid=31403992", "title": "Informatics for Consumer Health", "text": "Informatics for Consumer Health\n\nInformatics for Consumer Health (ICH) is a government initiative coordinated by the National Cancer Institute (NCI) within the National Institutes of Health (NIH). ICH focuses on a coordination of health information, technology, and health care delivery that empowers providers to manage care and increases the ability of consumers to gain mastery over their own health. The ICH online initiative involved stakeholders from various sectors—commercial IT, government, health care, education, research, and advocacy—exchanging ideas and resources to bridge information technology and health care with the goal of improving behavioral support for all consumers. The Informatics for Consumer Health field is related to health informatics, medical informatics, consumer health informatics, eHealth, and health information technology.\n\nChanging health behaviors is key to improving health outcomes. Research indicates that changes in basic preventive behavior - smoking cessation, better diet and exercise, and routine screenings—can lead to potential reductions in disability and death due to cancer, heart disease, and diabetes. Effective consumer health information technology (health IT) applications hold great promise for encouraging and supporting behavior change.\n\nWith the behavioral and population health evidence-base as a backdrop, the “Informatics for Consumer Health” came out of to two events that occurred in 2009. The first event occurred in early 2009, when the U.S.-based National Research Council released a report titled “Computational Technology for Effective Healthcare: Immediate Steps and Future Directions”. This report concluded that many of the current deployments of health information technology had become disconnected from their primary objectives: to ensure the health of real consumers in real world settings. The research portfolio in health systems should be rebalanced, authors of the report argued, to emphasize cognitive support for providers, patients, and their families over technology development for its own sake.\n\nThe second event occurred in March 2009, when the U.S. Congress passed the American Recovery and Reinvestment Act (ARRA) of 2009. Title XIII of the Act, also referred to as the Health Information Technology for Economic and Clinical Health (HITECH) Act, gave authority to the Department of Health and Human Services to offer incentives. “Meaningful use”, rather than “use” measured in technological terms, was to be gauged in terms of outcomes for patients and their families.\n\nIn this context, a number of Federal agencies came together to convene the “Informatics for Consumer Health: Summit on Communication, Collaboration, and Quality,” in November 2009. The summit brought together nearly 200 leaders from commercial IT, government, health care, education, research, and advocacy organizations to open a dialogue and begin creating a blueprint for improving health care quality through enhanced behavioral support for consumers across the healthcare spectrum. Key Summit objectives to foster collaborations and spur innovation led to the publication of a special supplement on cyberinfrastructure for consumer health in the \"American Journal of Preventative Medicine\" and the development of the online Informatics for Consumer Health (ICH) platform in 2010.\n\nFor three years the platform, informaticsforconsumehealth.org, served as a rallying place for summit stakeholders representing a wide range of sectors to exchange resources that bridge information technology and health care, and improve behavioral support for all consumers. By sharing news, resources, and funding opportunities the ICH community helped disseminate information for facilitating collaboration among the public, private, and research communities to improve consumer health.\n\nGoals of the ICH platform included:\n\n\nIn 2013, the ICH platform was retired and all original content is now archived on the National Cancer Institute’s Health Communication and Informatics Research Branch (HCIRB) website. As the science and practice of behavioral and public health informatics evolve, the activities and initiatives frequently aggregated and posted on the ICH platform continue to be a priority for the public and private sector.\n\nThe Informatics for Consumer Health initiative is led by a collaboration of Federal agencies with active intramural and extramural programs focused on aspects of consumer health IT.\n\n\n"}
{"id": "48878328", "url": "https://en.wikipedia.org/wiki?curid=48878328", "title": "Jesse's Journey", "text": "Jesse's Journey\n\nJesse's Journey () is a Canadian charity focused on funding research into Duchenne muscular dystrophy. Founded in 1994 and named for Jesse Davidson, Jesse's Journey is based out of London, Ontario with a satellite office in Montreal, Quebec. Jesse's Journey is the only Canadian charity that solely exists to fund research into Duchenne.\n\nJesse Davidson was six years old when he was diagnosed with Duchenne muscular dystrophy. \nIn 1994, The Davidsons created The Foundation for Gene and Cell Therapy (known as Jesse's Journey). On May 20, 1995, Jesse's Journey began its first steps at the Manitoba/Ontario border. Jesse's father, John Davidson, walked 3,300 kilometres to Ottawa, pushing Jesse in his wheelchair. This first journey raised $1.5M for research. Davidson is the holder of the Guinness World Record for the fastest crossing of Canada on foot.\n\nOn April 10, 1998 – Jesse's 18th birthday – Jesse's Journey: A Father's Tribute began. John Davidson's goal was to cross Canada on foot, raising funds and awareness across the country. Davidson succeeded in walking the 8,300 kilometres across Canada and raised $2M for research. This allowed The Foundation for Gene and Cell Therapy to create the Jesse Davidson Endowment – a fund that in 2015 stands at $11M and is a vehicle through which Jesse’s Journey can fund research in perpetuity, until a cure is found.\n\nIn 1998 Davidson and his mother Sherene along with Journey volunteer Mario Chioini flew to Paris, France to assist the French Association le Dystrophe Musculaire with their 30-hour telethon. The telethon generated $116,000,000.\n\nOn November 6, 2009, at the age of 29, Davidson lost his battle with Duchenne muscular dystrophy.\n\nOn Christmas Eve 2009, one month after his son Jesse died, Davidson carried the Olympic Torch as London's final torch-bearer in the 2010 Olympic Torch Relay, in honour of his son who had been named London's official torch-bearer earlier in the year.\n\nJesse's Journey is the only Canadian charity that solely exists to fund research into treatments and a cure for Duchenne muscular dystrophy. Through a rigorous annual granting process, Jesse's Journey seeks to fund the most promising research from around the world. Proposals are peer-reviewed by external reviewers.\n\nIn 2015, the 20th anniversary of John and Jesse's journey across Ontario, Jesse's Journey reached a milestone by for the first time granting more than $1M in a single year.\n"}
{"id": "8430609", "url": "https://en.wikipedia.org/wiki?curid=8430609", "title": "Localized disease", "text": "Localized disease\n\nA localized disease is an infectious or neoplastic process that originates in and is confined to one organ system or general area in the body, such as a sprained ankle, a boil on the hand, an abscess of finger.\n\nA localized cancer that has not extended beyond the margins of the organ involved can also be described as localized disease, while cancers that extend into other tissues are described as invasive. Tumors that are non-hematologic in origin but extend into the bloodstream or lymphatic system are known as metastatic.\n\nLocalized diseases are contrasted with disseminated diseases and systemic diseases. \n\nSome diseases are capable of changing from local to disseminated diseases. Pneumonia, for example, is generally confined to one or both lungs but can become disseminated through sepsis, in which the microbe responsible for the pneumonia \"seeds\" the bloodstream or lymphatic system and is transported to distant sites in the body. When that occurs, the process is no longer described as a localized disease, but rather as a disseminated disease.\n\n"}
{"id": "1973352", "url": "https://en.wikipedia.org/wiki?curid=1973352", "title": "Low birth weight", "text": "Low birth weight\n\nLow birth weight (LBW) is defined by the World Health Organization as a birth weight of a\ninfant of 2,499 g or less, regardless of gestational age. Subcategories include very low birth weight (VLBW), which is less than 1500 g (3 pounds 5 ounces), and extremely low birth weight (ELBW), which is less than 1000 g (2 pounds 3 ounces). Normal weight at term delivery is 2500–4200 g (5 pounds 8 ounces – 9 pounds 4 ounces).\n\nLBW is either caused by preterm birth (that is, a low gestational age at birth, commonly defined as younger than 37 weeks of gestation) or the infant being small for gestational age (that is, a slow prenatal growth rate), or a combination of both.\n\nIn general, risk factors in the mother that may contribute to low birth weight include young ages, multiple pregnancies, previous LBW infants, poor nutrition, heart disease or hypertension, untreated coeliac disease, drug addiction, alcohol abuse, and insufficient prenatal care. Environmental risk factors include smoking, lead exposure, and other types of air pollutions.\n\nFour different pathways have been identified that can result in preterm birth and have considerable evidence: precocious fetal endocrine activation, uterine overdistension, decidual bleeding, and intrauterine inflammation/infection. From a practical point a number of factors have been identified that are associated with preterm birth, however, an association does not establish causality.\n\nBeing small for gestational age can be constitutional, that is, without an underlying pathological cause, or it can be secondary to intrauterine growth restriction, which, in turn, can be secondary to many possible factors. For example, babies with congenital anomalies or chromosomal abnormalities are often associated with LBW. Problems with the placenta can prevent it from providing adequate oxygen and nutrients to the fetus. Infections during pregnancy that affect the fetus, such as rubella, cytomegalovirus, toxoplasmosis, and syphilis, may also affect the baby's weight.\n\nWhile active maternal tobacco smoking has well established adverse perinatal outcomes such as LBW, that mothers who smoke during pregnancy are twice as likely to give birth to low-birth weight infants. Review on the effects of passive maternal smoking, also called environmental tobacco exposure (ETS), demonstrated that increased risks of infants with LBW were more likely to be expected in ETS-exposed mothers.\n\nRegarding environmental toxins in pregnancy, elevated blood lead levels in pregnant women, even those well below 10 ug/dL can cause miscarriage, premature birth, and LBW in the offspring. With 10 ug/dL as the Centers for Disease Control and Prevention's “level of concern”, this cut-off value really needs to arise more attentions and implementations in the future.\n\nThe combustion products of solid fuel in developing countries can cause many adverse health issues in people. Because a majority of pregnant women in developing countries, where rate of LBW is high, are heavily exposed to indoor air pollution, increased relative risk translates into substantial population attributable risk of 21% of LBW.\n\nOne environmental exposure which has been found to increase the risk of low birth weight is particulate matter, a component of ambient air pollution. Because particulate matter is composed of extremely small particles, even nonvisible levels can be inhaled and present harm to the fetus. Particulate matter exposure can cause inflammation, oxidative stress, endocrine disruption, and impaired oxygen transport access to the placenta, all of which are mechanisms for heightening the risk of low birth weight. To reduce exposure to particulate matter, pregnant women can monitor the EPA’s Air Quality Index and take personal precautionary measures such as reducing outdoor activity on low quality days, avoiding high-traffic roads/intersections, and/or wearing personal protective equipment (i.e., facial mask of industrial design). Indoor exposure to particulate matter can also be reduced through adequate ventilation, as well as use of clean heating and cooking methods.\n\nA correlation between maternal exposure to CO and low birth weight has been reported that the effect on birth weight of increased ambient CO was as large as the effect of the mother smoking a pack of cigarettes per day during pregnancy. \nIt has been revealed that adverse reproductive effects (e.g., risk for LBW) were correlated with maternal exposure to air pollution combustion emissions in Eastern Europe and North America.\nMercury is a known toxic heavy metal that can harm fetal growth and health, and there has been evidence showing that exposure to mercury (via consumption of large oily fish) during pregnancy may be related to higher risks of LBW in the offspring.\n\nIt was revealed that, exposure of pregnant women to airplane noise was found to be associated with low birth weight. Aircraft noise exposure caused adverse effects on fetal growth leading to low birth weight and preterm infants.\n\nLow birthweight, pre-term birth and pre-eclampsia have been associated with maternal periodontitis exposure. But the strength of the observed associations is inconsistent and vary according to the population studied, the means of periodontal assessment and the periodontal disease classification employed. However the best is that the risk of low birth weight can be reduced with very simple therapy. Treatment of periodontal disease during gestation period is safe and reduction in inflammatory burden reduces the risk of preterm birth as well as low birth weight.\n\nLBW is closely associated with fetal and Perinatal mortality and Morbidity, inhibited growth and cognitive development, and chronic diseases later in life. At the population level, the proportion of babies with a LBW is an indicator of a multifaceted public-health problem that includes long-term maternal malnutrition, ill health, hard work and poor health care in pregnancy. On an individual basis, LBW is an important predictor of newborn health and survival and is associated with higher risk of infant and childhood mortality.\n\nLow birth weight constitutes as sixty to eighty percent of the infant mortality rate in developing countries. Infant mortality due to low birth weight is usually directly causal, stemming from other medical complications such as preterm birth, poor maternal nutritional status, lack of prenatal care, maternal sickness during pregnancy, and an unhygienic home environment. According to an analysis by University of Oregon, reduced brain volume in children is also tied to low birth-weight.\n\nA study by the Agency for Healthcare Research and Quality (AHRQ) found that of the 3.8 million births that occurred in the United States in 2011, approximately 6.1% (231,900) were diagnosed with low birth weight (<2,500 g). Approximately 49,300 newborns (1.3%) weighed less than 1,500 grams (VLBW). Infants born at low birth weight are at a higher risk for developing neonatal infection.\n"}
{"id": "16908870", "url": "https://en.wikipedia.org/wiki?curid=16908870", "title": "Medical Officer for Health", "text": "Medical Officer for Health\n\nMedical Officer of Health, Medical Health Officer or District Medical Officer, is a title and commonly used for the senior government official of a health department or agency, usually at a municipal, county/district, state/province, or regional level. The post is held by a physician who serves to advise and lead a team of public health professionals such as environmental health officers and public health nurses on matters of public health importance.\n\nThe equivalent senior health official at the national level is often referred to as the Chief Medical Officer, although the title varies across countries, for example known as the Surgeon General in the United States and the Chief Public Health Officer in Canada.\n\nIn Canada, all communities are under the jurisdiction of an MOH. The roles of the MOH vary across jurisdictions, but always include responsibilities related to public health and safety, and may include the following functions:\n\n\nHealth officers in India are expected to have prescribed qualifications, such as a Bachelor of Sanitary Science (B.S.Sc.), a degree of an institution recognized by the Medical Council of India or a diploma in Public Health after a 2-year study at the University of Calcutta.\n\nIn the United Kingdom, the municipal position was an elected head of the local board of health, however the term MOH has also been used to refer to the Chief Medical Officer. Under the Metropolis Local Management Act 1855, London municipalities were each required to appoint a medical officer. In 1856, 48 officers took up appointments in the city, and these specialists formed Metropolitan Association of Medical Officers of Health. They were important and influential in the establishment of municipal hospitals under the provisions of the Local Government Act 1929. In the 1974 NHS reorganisation they were replaced by Community Physicians who were attached to the different levels of the NHS.\n\n\nHealth Officer is a common term used in the United States for public health officials, such as medical health officers and environmental health officers. They may serve at the global, federal, state, county, or municipal level.\nThe end of the 20th century and beginning of the 21st saw major issues for health officials and health officers include tobacco control, injury prevention, public health surveillance, disease control, access to health care, health equity, health disparities, cultural competence, access to preventive services such as immunizations and health promotion.\n\n\n"}
{"id": "46704707", "url": "https://en.wikipedia.org/wiki?curid=46704707", "title": "Medical–industrial complex", "text": "Medical–industrial complex\n\nThe medical–industrial complex is the network of corporations which supply health care services and products for a profit. The term is analogous to \"military–industrial complex\" and builds from the social precedent of discussion on that concept.\n\nThe medical–industrial complex is often discussed in the context of conflict of interest in the health care industry.\n\nThe concept of a \"medical–industrial complex\" was first advanced by Barbara and John Ehrenreich in the November 1969 issue of the Bulletin of the Health Policy Advisory Center in an article entitled \"The Medical Industrial Complex\" and in a subsequent book (with Health-PAC), The American Health Empire: Power, Profits, and Politics (Random House, 1970). The concept was widely discussed throughout the 1970s, including reviews in the New England Journal of Medicine (November 4, 1971, 285:1095). It was further popularized in 1980, Arnold S. Relman while he served as editor of \"The New England Journal of Medicine\". in a paper titled \"The New Medical-Industrial Complex.\" Relman commented, \"The past decade has seen the rise of another kind of private \"industrial complex\" with an equally great potential for influence on public policy — this time in health care...\" Oddly, Relman added, \" In searching for information on this subject, I have found no standard literature and have had to draw on a variety of unconventional sources...\" Subsequently, this paper and the concept have been discussed continually. An updated history and analysis can be found in John Ehrenreich, \"Third Wave Capitalism: How Money, Power, and the Pursuit of Self-Interest have Imperiled the American Dream\" (Cornell University Press, May 2016).\n\nManufacturers of medical devices fund medical education programs and physicians and hospitals directly to adopt the use of their devices.\n\nThe management of health care organizations by business staff rather than local medical practice is one of the trends of the increasing influence of the medical-industrial complex.\n\nAnother trend is that increased pressure to generate profit for providing services can decrease the influence of creativity or innovation in medical research.\n\nIn the 1970s profit-seeking companies became significant stakeholders in the United States healthcare system.\n\nThe influence of economic policy on the practice of medicine has a long history.\n\nBecause the General Agreement on Trade in Services regulates international marketplaces, in countries where the industrial-medical complex is more strong there can be legal limitations to consumer options for accessing diverse healthcare services.\n\nBecause the industrial-medical complex funds continuing medical education, this education has a bias to promote the interests of its funders.\n\n"}
{"id": "11211918", "url": "https://en.wikipedia.org/wiki?curid=11211918", "title": "National Survey of Sexual Attitudes and Lifestyles", "text": "National Survey of Sexual Attitudes and Lifestyles\n\nThe National Survey of Sexual Attitudes and Lifestyles (NATSAL) is the name given to a series of face-to-face surveys of people in the United Kingdom regarding their sexual behaviour and patterns. The three rounds of interviews completed to date are NATSAL-1 (1990–91) and NATSAL-2 (2000-01) and NATSAL-3 (2010–12). The results are widely used in research and policy making. NATSAL's principal investigator is Anne Johnson, a professor at University College, London, and co-leader Kaye Wellings, a professor at the London School of Hygiene & Tropical Medicine.\n\nThe Natsal-3 survey revealed, among other things, that British people are having sex less often than they did 20 years ago.\n\n"}
{"id": "66575", "url": "https://en.wikipedia.org/wiki?curid=66575", "title": "Nutrient", "text": "Nutrient\n\nA nutrient is a substance used by an organism to survive, grow, and reproduce. The requirement for dietary nutrient intake applies to animals, plants, fungi, and protists. Nutrients can be incorporated into cells for metabolic purposes or excreted by cells to create non-cellular structures, such as hair, scales, feathers, or exoskeletons. Some nutrients can be metabolically converted to smaller molecules in the process of releasing energy, such as for carbohydrates, lipids, proteins, and fermentation products (ethanol or vinegar), leading to end-products of water and carbon dioxide. All organisms require water. Essential nutrients for animals are the energy sources, some of the amino acids that are combined to create proteins, a subset of fatty acids, vitamins and certain minerals. Plants require more diverse minerals absorbed through roots, plus carbon dioxide and oxygen absorbed through leaves. Fungi live on dead or living organic matter and meet nutrient needs from their host.\n\nDifferent types of organism have different essential nutrients. Ascorbic acid (vitamin C) is essential, meaning it must be consumed in sufficient amounts, to humans and some other animal species, but not to all animals and not to plants, which are able to synthesize it. Nutrients may be organic or inorganic: organic compounds include most compounds containing carbon, while all other chemicals are inorganic. Inorganic nutrients include nutrients such as iron, selenium, and zinc, while organic nutrients include, among many others, energy-providing compounds and vitamins.\n\nA classification used primarily to describe nutrient needs of animals divides nutrients into macronutrients and micronutrients. Consumed in relatively large amounts (grams or ounces), macronutrients (carbohydrates, fats, proteins, water) are used primarily to generate energy or to incorporate into tissues for growth and repair. Micronutrients are needed in smaller amounts (milligrams or micrograms); they have subtle biochemical and physiological roles in cellular processes, like vascular functions or nerve conduction. Inadequate amounts of essential nutrients, or diseases that interfere with absorption, result in a deficiency state that compromises growth, survival and reproduction. Consumer advisories for dietary nutrient intakes, such as the United States Dietary Reference Intake, are based on deficiency outcomes and provide macronutrient and micronutrient guides for both lower and upper limits of intake. In many countries, macronutrients and micronutrients in significant content are required by regulations to be displayed on food product labels. Nutrients in larger quantities than the body needs may have harmful effects. Edible plants also contain thousands of compounds generally called phytochemicals which have unknown effects on disease or health, including a diverse class with non-nutrient status called polyphenols, which remain poorly understood as of 2017.\n\nPlant nutrients consist of more than a dozen minerals absorbed through roots, plus carbon dioxide and oxygen absorbed or released through leaves. All organisms obtain all their nutrients from the surrounding environment.\n\nPlants absorb carbon, hydrogen and oxygen from air. These three, in the form of water and carbon dioxide. Other nutrients are absorbed from soil (exceptions include some parasitic or carnivorous plants). Counting these, there are 17 important nutrients for plants: the macronutrients nitrogen (N), phosphorus (P), potassium (K), calcium (Ca), sulfur (S), magnesium (Mg), carbon (C), oxygen(O) and hydrogen (H), and the micronutrients iron (Fe), boron (B), chlorine (Cl), manganese (Mn), zinc (Zn), copper (Cu), molybdenum (Mo) and nickel (Ni). In addition to carbon, hydrogen and oxygen, nitrogen, phosphorus, and sulfur are also needed in relatively large quantities. Together, the \"Big Six\" are the elemental macronutrients for all organisms.\nThey are sourced from inorganic matter (for example, carbon dioxide, water, nitrates, phosphates, sulfates, and diatomic molecules of nitrogen and, especially, oxygen) and organic matter (carbohydrates, lipids, proteins).\n\nMacronutrients are defined in several ways.\n\nMacronutrients provide energy:\n\n\nFat has an energy content of 9kcal/g (~37.7 kJ/g) and proteins and carbohydrates 4kcal/g (~16.7 kJ/g).\n\nMicronutrients support metabolism.\n\n\nAn essential nutrient is a nutrient required for normal physiological function that cannot be synthesized in the body – either at all or in sufficient quantities – and thus must be obtained from a dietary source. Apart from water, which is universally required for the maintenance of homeostasis in mammals, essential nutrients are indispensable for various cellular metabolic processes and maintaining tissue and organ function. In the case of humans, there are nine amino acids, two fatty acids, thirteen vitamins and fifteen minerals that are considered essential nutrients. In addition, there are several molecules that are considered conditionally essential nutrients since they are indispensable in certain developmental and pathological states.\n\nAn essential amino acid is an amino acid that is required by an organism but cannot be synthesized \"de novo\" by it, and therefore must be supplied in its diet. Out of the twenty standard protein-producing amino acids, nine cannot be endogenously synthesized by humans: phenylalanine, valine, threonine, tryptophan, methionine, leucine, isoleucine, lysine, and histidine.\n\nEssential fatty acids (EFAs) are fatty acids that humans and other animals must ingest because the body requires them for good health but cannot synthesize them. Only two fatty acids are known to be essential for humans: alpha-linolenic acid (an omega-3 fatty acid) and linoleic acid (an omega-6 fatty acid).\n\nVitamins are organic molecules essential for an organism that are not classified as amino acids or fatty acids. They commonly function as enzymatic cofactors, metabolic regulators or antioxidants. Humans require thirteen vitamins in their diet, most of which are actually groups of related molecules (e.g. vitamin E includes tocopherols and tocotrienols): vitamins A, C, D, E, K, thiamine (B), riboflavin (B), niacin (B), pantothenic acid (B), vitamin B (e.g., pyridoxine), biotin (B), folate (B), and cobalamin (B). The requirement for vitamin D is conditional, as people who get sufficient exposure to ultraviolet light, either from the sun or an artificial source, synthesize vitamin D in the skin.\n\nMinerals are the exogenous chemical elements indispensable for life. Although the four elements: carbon, hydrogen, oxygen, and nitrogen, are essential for life, they are so plentiful in food and drink that these are not considered nutrients and there are no recommended intakes for these as minerals. The need for nitrogen is addressed by requirements set for protein, which is composed of nitrogen-containing amino acids. Sulfur is essential, but again does not have a recommended intake. Instead, recommended intakes are identified for the sulfur-containing amino acids methionine and cysteine.\n\nThe essential nutrient elements for humans, listed in order of Recommended Dietary Allowance (expressed as a mass), are potassium, chlorine, sodium, calcium, phosphorus, magnesium, iron, zinc, manganese, copper, iodine, chromium, molybdenum, selenium and cobalt (the last as a component of vitamin B). There are other minerals which are essential for some plants and animals, but may or may not be essential for humans, such as boron and silicon.\n\nConditionally essential nutrients are certain organic molecules that can normally be synthesized by an organism, but under certain conditions in insufficient quantities. In humans, such conditions include premature birth, limited nutrient intake, rapid growth, and certain disease states. Choline, inositol, taurine, arginine, glutamine and nucleotides are classified as conditionally essential and are particularly important in neonatal diet and metabolism.\n\nNon-essential nutrients are substances within foods that can have a significant impact on health; these substances can be beneficial or toxic. For example, dietary fiber is not absorbed in the human digestive tract, but is important in maintaining the bulk of a bowel movement to avoid constipation. A subset of dietary fiber, soluble fiber, can be metabolized by bacteria residing in the large intestine. Soluble fiber is marketed as serving a prebiotic functionpromoting \"healthy\" intestinal bacteria. Bacterial metabolism of soluble fiber also produces short-chain fatty acids like butyric acid, which may be absorbed into intestinal cells as a source of calories.\n\nEthanol (CHOH) supplies calories. For spirits (vodka, gin, rum, etc.) a standard serving in the United States is , which at 40%ethanol (80proof) would be 14 grams and 98 calories. At 50%alcohol, 17.5grams and 122.5calories. Wine and beer contain a similar amount of ethanol in servings of and , respectively, but these beverages also contain non-ethanol calories. A 5-ounce serving of wine contains 100 to 130 calories. A 12-ounce serving of beer contains 95 to 200 calories. According to the U.S. Department of Agriculture, based on NHANES 2013–2014 surveys, women ages 20 and up consume on average 6.8grams of alcohol per day and men consume on average 15.5 grams per day. Ignoring the non-alcohol contribution of those beverages, the average ethanol calorie contributions are 48 and 108 cal/day, respectively. Alcoholic beverages are considered empty calorie foods because, other than calories, they contribute no essential nutrients.\n\nBy definition, phytochemicals include all nutritional and non-nutritional components of edible plants. Included as nutritional constituents are provitamin A carotenoids, whereas those without nutrient status are diverse polyphenols, flavonoids, resveratrol, and lignans – often claimed to have antioxidant effects – that are present in numerous plant foods. A number of phytochemical compounds are under preliminary research for their potential effects on human diseases and health. However, the qualification for nutrient status of compounds with poorly defined properties \"in vivo\" is that they must first be defined with a Dietary Reference Intake level to enable accurate food labeling, a condition not established for most phytochemicals that are claimed to be antioxidant nutrients.\n\n\"See Vitamin, Mineral (nutrient), Protein (nutrient)\"\n\nAn inadequate amount of a nutrient is a deficiency. Deficiencies can be due to a number of causes including an inadequacy in nutrient intake, called a dietary deficiency, or any of several conditions that interfere with the utilization of a nutrient within an organism. Some of the conditions that can interfere with nutrient utilization include problems with nutrient absorption, substances that cause a greater than normal need for a nutrient, conditions that cause nutrient destruction, and conditions that cause greater nutrient excretion. Nutrient toxicity occurs when excess consumption of a nutrient does harm to an organism.\n\nIn the United States and Canada, recommended dietary intake levels of essential nutrients are based on the minimum level that \"will maintain a defined level of nutriture in an individual\", a definition somewhat different from that used by the World Health Organization and Food and Agriculture Organization of a \"basal requirement to indicate the level of intake needed to prevent pathologically relevant and clinically detectable signs of a dietary inadequacy\".\n\nIn setting human nutrient guidelines, government organizations do not necessarily agree on amounts needed to avoid deficiency or maximum amounts to avoid the risk of toxicity. For example, for vitamin C, recommended intakes range from 40 mg/day in India to 155 mg/day for the European Union. The table below shows U.S. Estimated Average Requirements (EARs) and Recommended Dietary Allowances (RDAs) for vitamins and minerals, PRIs for the European Union (same concept as RDAs), followed by what three government organizations deem to be the safe upper intake. RDAs are set higher than EARs to cover people with higher than average needs. Adequate Intakes (AIs) are set when there is not sufficient information to establish EARs and RDAs. Governments are slow to revise information of this nature. For the U.S. values, with the exception of calcium and vitamin D, all of the data date to 1997-2004.\n\n\nEAR U.S. Estimated Average Requirements.\n\nRDA U.S. Recommended Dietary Allowances; higher for adults than for children, and may be even higher for women who are pregnant or lactating.\n\nAI U.S. Adequate Intake; AIs established when there is not sufficient information to set EARs and RDAs.\n\nPRI Population Reference Intake is European Union equivalent of RDA; higher for adults than for children, and may be even higher for women who are pregnant or lactating. For Thiamin and Niacin the PRIs are expressed as amounts per MJ of calories consumed. MJ = megajoule = 239 food calories.\n\nUpper Limit Tolerable upper intake levels.\n\nND ULs have not been determined.\n\nNE EARs, PRIs or AIs have not yet been established or will not be (EU does not consider chromium an essential nutrient).\n\n"}
{"id": "3055427", "url": "https://en.wikipedia.org/wiki?curid=3055427", "title": "Nutrition and pregnancy", "text": "Nutrition and pregnancy\n\nNutrition and pregnancy refers to the nutrient intake, and dietary planning that is undertaken before, during and after pregnancy. Nutrition of the fetus begins at conception. For this reason, the nutrition of the mother is important from before conception (probably several months before) as well as throughout pregnancy and breast feeding. An ever-increasing number of studies have shown that the nutrition of the mother will have an effect on the child, up to and including the risk for cancer, cardiovascular disease, hypertension and diabetes throughout life.\n\nAn inadequate or excessive amount of some nutrients may cause malformations or medical problems in the fetus, and neurological disorders and handicaps are a risk that is run by mothers who are malnourished. An estimated 24% of babies worldwide are born with lower than optimal weights at birth due to lack of proper nutrition. Personal habits such as consumption of alcohol or large amounts of caffeine can negatively and irreversibly affect the development of the baby, which happens in the early stages of pregnancy.\n\nCaffeine consumption during pregnancy is associated with increased risk of pregnancy loss. The available research favors the notion that the benefits of fish consumption during pregnancy outweigh the risks; however, the type of fish is important. Folic acid, which is the synthetic form of the vitamin folate, is critical both in pre-and peri-conception. \n\nAs with most diets, there are chances of over-supplementing, however, as general advice, both state and medical recommendations are that mothers follow instructions listed on particular vitamin packaging as to the correct or recommended daily allowance (RDA). Daily prenatal use of iron substantially improves birth weight, potentially reducing the risk of Low birth weight.\n\n\nThe United States and the European Union have established vitamin and mineral recommendations for during pregnancy and lactation. The amounts in the table below are the higher of the two. The citations separately list recommendations for pregnancy and lactation. Recommendations (RDAs = Recommended Dietary Allowances and PRIs = Population Reference Intakes) are set higher than what has been determined to be average requirements so as to address women who have above average needs. For some nutrients there is not enough information to set a recommendation, so the term Adequate Intake (AI) is used based on what appears to be sufficient.\n\n<nowiki>*</nowiki>Adequate Intake\n†Not established. EU has not identified an AI for sodium or chloride, and does not consider chromium to be an essential mineral nutrient.\n\nMultiple micronutrient supplements taken with iron and folic acid can improve birth outcomes for women in low income countries. These supplements reduce numbers of low birth weight babies, small for gestational age babies and stillbirths in women who may not have many micronutrients in their usual diets. Undernourished women can benefit from having dietary education sessions and, balanced energy and protein supplements. A review showed that dietary education increased the mother’s protein intake and helped the baby grow more inside the womb. The balanced protein and energy supplement lowered risk of stillbirth and small babies and increased weight gain for both the mother and baby. Although more research is needed into the longer term effects on the mothers’ and infants’ health, the short term effects look promising.\n\nSupplementing one's diet with foods rich in folic acid, fruits and dark green leafy vegetables helps to prevent neural tube birth defects in the fetus. In addition, prenatal vitamins typically contain increased amounts of folic acid, iodine, iron, vitamin A, vitamin D, zinc, and calcium over the amounts found in standard multi-vitamins. Zinc supplements have reduced preterm births by around 14% mainly in low income countries where zinc deficiency is common. However, the World Health Organisation does not routinely recommend zinc supplementation for all pregnant women.\n\nFor women with low calcium diets, taking calcium supplementation can reduce their risk of preeclampsia. It has also been suggested that calcium can reduce numbers of births that happen before the 37th week of pregnancy (preterm birth). However a more recent review looking into other benefits of calcium supplementation did not find any improvement in numbers of preterm or low birth weight babies. There is not enough good quality to research to suggest best doses and timing of calcium supplementation.\n\nPregnant women are advised to pay attention to the foods they eat during pregnancy in order to reduce the risk of exposure to substances or bacteria that may be harmful to the developing fetus. This can include potentially harmful pathogens such as listeria, toxoplasmosis, and salmonella. Intake of large amounts of retinol has been linked to birth defects and abnormalities. Although seafood contains high levels of Omega-3 fatty acids which are beneficial for both mother and the baby, but there is no consensus on consuming seafood during pregnancy. Pregnant women are advised to eat seafood in moderation.\n\nDuring pregnancy, a woman's mass increases by about 12 kg (26 lb). The European Food Safety Authority recommends an increase of 300 mL per day compared to the normal intake for non-pregnant women, taking the total adequate water intake (from food and fluids) to 2,300 mL, or approximately 1,850 mL/ day from fluids alone\n\nA mother's nutritional intake during pregnancy is believed to influence and possibly offer protective effects against the development of allergenic diseases and asthma in children. Maternal intake of vitamin D, vitamin E, and zinc have all been associated with a lower likelihood of wheezing in childhood, suggesting a protective effect. Additionally, maternal intake of omega-3 long chain polyunsaturated fatty acids (n-3 LC-PUFAs) has been associated with a reduced risk of development of eczema in childhood and reduced likelihood to for infants exhibit sensitivity to foods in the first year of life.\n\nCaffeine consumption during pregnancy is associated with increased risk of pregnancy loss and increased risk of low birth weight, defined as below 2500 grams (5.5 pounds). The European Food Safety Authority and the American Congress of Obstetricians and Gynecologists concur that habitual caffeine consumption up to 200 mg per day by pregnant women does not give rise to safety concerns for the fetus. The United Kingdom Food Standards Agency had recommended that pregnant women should limit their caffeine intake to less than 300 mg of caffeine a day, but in 2009 revised that downward to less than 200 mg of caffeine a day.\n\nFetal alcohol spectrum disorders are a group of conditions that can occur in a person whose mother drank alcohol during pregnancy. The most severe form of the condition is known as fetal alcohol syndrome. Problems may include an abnormal appearance, short height, low body weight, small head size, poor coordination, low intelligence, behavior problems, hearing loss and vision problems. Those affected are more likely to have trouble in school, legal problems, participate in high-risk behaviors, and have trouble with alcohol and recreational drug use. Fetal alcohol syndrome usually occurs when a pregnant woman has more than four drinks per day. Milder symptoms have been found with two drinks per day during the early part of pregnancy. Evidence of harm from less than two drinks per day or 10 drinks per week is not clear.\n\nThe American Academy of Pediatrics established a conservative set of recommendations in 2015: \"During pregnancy:no amount of alcohol intake should be considered safe; there is no safe trimester to drink alcohol; all forms of alcohol, such as beer, wine, and liquor, pose similar risk; and binge drinking poses dose-related risk to the developing fetus.\" The World Health Organization recommends that alcohol should be avoided entirely during pregnancy, given the relatively unknown effects of even small amounts of alcohol during pregnancy.\n\nFish consumption during pregnancy is encouraged by European, Australian, and American guidelines. The reason given is that fat-containing fish such as salmon and tuna contain eicosapentaenoic acid (EPA) and docosahexaenoic acid (DHA). These are termed long chain, omega-3, polyunsaturated fatty acids, and are considered as important for fetal neurodevelopment. Additionally, fish are good sources of vitamins A, D, and B12, as well as the mineral iodine.\n\nDue to risks of heavy-metal toxicity on fetal neurodevelopment, many mothers are concerned about eating fish during pregnancy. Overall, current research favors the notion that the benefits of fish consumption during pregnancy outweigh the risks; however, the type of fish is important. Current research suggests that 2-3 servings of low-methylmercury containing fish per week in pregnancy is both safe and beneficial. Mercury accumulates in fish through their own diet (bioaccumulation). A general rule of thumb is that fish higher up on the food chain, and with longer life spans will contain higher levels of mercury. Fish lower on the food chain and with shorter life spans will have lower metal content. However, it is important to note that the bioaccumulation of metals in fish is also dependent on geographical location, so it is hard to make global recommendations on specific fish species. An alternative to consuming fish is to use fish oil dietary supplements containing both EPA and DHA, or algae-derived DHA-only oils. The New York Times reported on a laboratory evaluation of 30 popular fish oil supplements. Some of those had less than the label claimed amounts of DHA. As for safety, \"All of the products tested contained only very low levels of mercury, ranging from one to six parts per billion per serving. That range is far below the upper safety limit of 100 parts per billion set by the Global Organization for EPA and DHA Omega-3s, an industry trade group.\"\n\nFolic acid, which is the synthetic form of the vitamin folate, is critical both in pre-and peri-conception. Deficiencies in folic acid may cause neural tube defects (NTDs). Women who had 0.4 mg of folic acid in their systems due to supplementing 3 months before childbirth significantly reduced the risk of NTDs. More than 80 countries use fortification of certain foods with folic acid as a measure to decrease the rate of NTDs.\n\nThe combination of vitamin E and vitamin C supplemented to pregnant women does not appear to be efficacious for reducing risk of stillbirth, neonatal death, preterm birth, preeclampsia or any other maternal or infant outcomes, either in healthy women or those considered at risk for pregnancy complications. Antioxidant vitamins as dietary supplements had been proposed as having benefits if consumed during pregnancy. For the combination of vitamin E with vitamin C supplemented to pregnant women, a Cochrane review of 21 clinical trials concluded that the data do not support vitamin E supplementation - majority of trials alpha-tocopherol at 400 IU/day plus vitamin C at 1000 mg/day - as being efficacious for reducing risk of stillbirth, neonatal death, preterm birth, preeclampsia or any other maternal or infant outcomes, either in healthy women or those considered at risk for pregnancy complications. The review identified only three small trials in which vitamin E was supplemented without co-supplementation with vitamin C. None of these trials reported any clinically meaningful information. A second Cochrane review of 29 trials, published same year, reported on the same combination trials but added analysis of trials with vitamin C alone. The conclusion was that the data do not support routine vitamin C supplementation alone or in combination with other supplements for the prevention of fetal or neonatal death, poor fetal growth, preterm birth or preeclampsia.\n\nProper nutrition is important after delivery to help the mother recover, and to provide enough food energy and nutrients for a woman to breastfeed her child. Women having serum ferritin less than 70 µg/L may need iron supplements to prevent iron deficiency anaemia during pregnancy and postpartum.\n\nDuring lactation, water intake may need to be increased. Human milk is made of 88% water, and the IOM recommends that breastfeeding women increase their water intake by about 300 mL/day to a total volume of 3000 mL/day (from food and drink); approximately 2,400 mL/day from fluids.\n\n\n"}
{"id": "8417064", "url": "https://en.wikipedia.org/wiki?curid=8417064", "title": "Occupational medicine", "text": "Occupational medicine\n\nOccupational medicine, until 1960 called industrial medicine, is the branch of medicine which is concerned with the maintenance of health in the workplace, including prevention and treatment of diseases and injuries, with secondary objectives of maintaining and increasing productivity and social adjustment in the workplace.\n\nIt is, thus, the branch of clinical medicine active in the field of occupational health and safety. OM specialists work to ensure that the highest standards of occupational health and safety are achieved and maintained in the workplace. While OM may involve a wide number of disciplines, it centers on preventive medicine and the management of illness, injury, and disability related to the workplace. Occupational physicians must have a wide knowledge of clinical medicine and be competent in a number of important areas. They often advise international bodies, governmental and state agencies, organizations and trade unions. There are contextual links to physical medicine and rehabilitation and to insurance medicine.\n\nOccupational medicine aims to prevent diseases and promote wellness among workers. Occupational health physicians must:\n\nOM can be described as:\n\n\"work that combines clinical medicine, research, and advocacy for people who need the assistance of health professionals to obtain some measure of justice and health care for illnesses they suffer as a result of companies pursuing the biggest profits they can make, no matter what the effect on workers or the communities they operate in.\"\n\nThe first textbook of occupational medicine, \"De Morbis Artificum Diatriba\" (\"Diseases of Workers)\", was written by Italian physician Bernardino Ramazzini in 1700.\n\n\n\n\n\n\n"}
{"id": "763780", "url": "https://en.wikipedia.org/wiki?curid=763780", "title": "Occupational noise", "text": "Occupational noise\n\nOccupational noise is the amount of acoustic energy received by an employee's auditory system when they are working in the industry. Occupational noise, or industrial noise, is often a term used in occupational safety and health, as sustained exposure can cause permanent hearing damage.\n\n\"Twenty-two million workers are exposed to potentially damaging noise at work each year. Last year, U.S. business paid more than $1.5 million in penalties for not protecting workers from noise.\" - OSHA\n\nOccupational noise is considered an occupational hazard traditionally linked to loud industries such as ship-building, mining, railroad work, welding, and construction, but can be present in any workplace where hazardous noise is present.\n\nIn the United States, the National Institute for Occupational Safety and Health (NIOSH) and the Occupational Safety and Health Administration (OSHA) work together to provide standards and regulations for noise in the workplace.\n\nNational Institute for Occupational Safety and Health (NIOSH), Occupational Safety and Health Administration (OSHA), Mine Safety and Health Administration (MSHA), Federal Railroad Administration (FRA) have all set standards on hazardous occupational noise in their respective industries. Each industry is different, as workers' tasks and equipment differ, but most regulations agree that noise becomes hazardous when it exceeds 85 decibels, for an 8-hour time exposure (typical work shift). This relationship between allotted noise level and exposure time is known as an Exposure action value (EAV) or Permissible exposure limit (PEL). The EAV or PEL can be seen as equations which manipulate the allotted exposure time according to the intensity of the industrial noise. This equation works as an inverse, exponential, relationship. As the industrial noise intensity increases, the allotted exposure tie, to still remain safe, decreases.\n\nThe above calculations of PEL and EAV are based on measurements taken to determine the intensity of that particular industrial noise. A-weighted measurements are commonly used to determine noise levels that can cause harm to the human ear. There are also special exposure meters available that integrate noise over a period of time to give an value (equivalent sound pressure level), defined by standards.\n\nOccupational noise, if experienced repeatedly, at high intensity, for an extended period of time, can cause noise-induced hearing loss (NIHL) which is then classified as occupational hearing loss.\n\nNoise, in the context of industrial noise, is hazardous to a person's hearing because of its loud intensity through repeated long-term exposure. In order for noise to cause hearing impairment for the worker, the noise has to be close enough, loud enough, and sustained long enough to damage the hair cells in the auditory system. Please see Occupational hearing loss or Noise-induced hearing loss for more information regarding the physiology of hearing loss. These factors have been taken into account by the governing occupational health and safety organization to determine the unsafe noise exposure levels and durations for their respective industries.\n\nNoise can also affect the safety of the employee and others. Noise can be a causal factor in work accidents as it may mask hazards and warning signals and impede concentration. High intensity noise interferes with vital workplace communication which increases the chance of accidents and decreases productivity.\n\nNoise may also act synergistically with other hazards to increase the risk of harm to workers. In particular, toxic materials (e.g. some solvents, metals, asphyxiants and pesticides) have some ototoxic properties that may affect hearing function.\n\nModern thinking in occupational safety and health further identifies noise as hazardous to workers' safety and health. This hazard is experienced in various places of employment and through a variety of sources.\n\nNoise, in the context of industrial noise, is hazardous to a persons hearing because of its loud intensity through repeated long-term exposure. In order for Noise to cause Hearing impairment for the worker, the noise has to be close enough, loud enough and the listener has to be exposed for long enough. These factors have been taken into account by the governing occupational health and safety organizations as they determine the unsafe noise exposure levels and durations for their respective industries.\n\nNational Institute for Occupational Safety and Health (NIOSH), Occupational Safety and Health Administration (OSHA), Mine Safety and Health Administration (MSHA), Federal Railroad Administration (FRA) have all set standards on hazardous occupational noise in their respective industries. Each industry is different, as workers tasks and equipment differ, but most regulations agree that noise becomes hazardous when it exceeds 85 Decibel, for an 8-hour exposure (typical work shift). This relationship between allotted noise level and exposure time is known as an Exposure action value (EAV) or Permissible exposure limit (PEL). The EAV or PEL can be seen as equations which manipulate the allotted exposure time according to the intensity of the industrial noise. This equation works as an inverse relationship. As the industrial noise intensity increases, the allotted exposure time, to still remain safe, decreases.\n\nThese above calculations of PEL and EAV are based on measurements taken to determine the intensity of that particular industrial noise. A-weighted measurements are commonly used to determine noise levels that can cause harm to the human ear. There are also special exposure meters available that integrate noise over a period of time to give an value (equivalent sound pressure level), defined by standards.\n\nThere are several ways to limit your exposure to hazardous occupational noise. The hierarchy of controls is a guideline for reducing hazardous noise. First, the company can eliminate the noise source. If the noise source cannot be eliminated, the company must try to reduce the noise with alternative methods. This process is called acoustic quieting. \n\nAcoustic quieting is the process of making machinery quieter by damping vibrations to prevent them from reaching the observer. The company can isolate the certain piece of machinery by placing materials on the machine or in between the machine and the worker to decreases the signal intensity that reaches the worker's ear.\n\nTo decrease an employee's exposure to hazardous noise, the company can also take administrative control by limiting the employee's exposure time. This can be done by changing work shifts and switching employees out from the noise exposure area. Lastly, to decrease occupational noise exposure, hearing protection should be used. There are several types of earplugs and earmuffs that can be used to attenuate the noise to a safe level.\n\nFor a more detailed description of the hierarchy of controls, please see Occupational hearing loss. \n\nSince the hazards of occupational noise exposure were realized, programs and initiatives such as the US Buy Quiet program have been set up to regulate or discourage noise exposure. The Buy Quiet initiative promotes the purchase of quieter tools and equipment and encourages manufacturers to design quieter machines. Additionally, the Safe-In-Sound Award was created to recognize successes in hearing loss prevention programs or initiatives.\n\n\nGeneral:\n\n"}
{"id": "34463932", "url": "https://en.wikipedia.org/wiki?curid=34463932", "title": "Peto's paradox", "text": "Peto's paradox\n\nPeto's paradox is the observation, named after Richard Peto, that at the species level, the incidence of cancer does not appear to correlate with the number of cells in an organism. For example, the incidence of cancer in humans is much higher than the incidence of cancer in whales. This is despite the fact that a whale has many more cells than a human. If the probability of carcinogenesis were constant across cells, one would expect whales to have a higher incidence of cancer than humans.\n\nPeto, a statistical epidemiologist at the University of Oxford, first formulated the paradox in 1977. Writing an overview of the multistage model of cancer, Peto noted that, on a cell-for-cell basis, humans were much less susceptible to cancer than mice:\n\nPeto went on to suggest that evolutionary considerations were likely responsible for varying per-cell carcinogenesis rates across species.\n\nWithin members of the same species, cancer risk and body size appear to be positively correlated, even once other risk factors are controlled for. A 25-year longitudinal study of 17,738 male British civil servants, published in 1998, showed a positive correlation between height and cancer incidence with a high degree of statistical confidence, even after risk factors like smoking were controlled for. A similar 2011 study of more than one million British women found strong statistical evidence of a relationship between cancer and height, even after controlling for a number of socioeconomic and behavioral risk factors. A 2011 analysis of the causes of death of 74,556 domesticated North American dogs found that cancer incidence was lowest in the smaller breeds, confirming the results of earlier studies.\n\nAcross species, however, the relationship breaks down. A 2015 study, using data from necropsies performed by the San Diego Zoo, surveyed results from 36 different mammalian species, ranging in size from the 51-gram striped grass mouse to the 4,800-kilogram elephant, nearly 100,000 times larger. The study found no relationship between body size and cancer incidence, offering empirical support for Peto's initial observation.\n\nThe evolution of multicellularity has required the suppression of cancer to some extent, and connections have been found between the origins of multicellularity and cancer. In order to build larger and longer-lived bodies, organisms required greater cancer suppression. Evidence suggests that large organisms such as elephants have more adaptations that allow them to evade cancer. The reason that intermediate-sized organisms have relatively few of these genes may be because the advantage of preventing cancer these genes conferred was, for moderately-sized organisms, outweighed by their disadvantages—particularly reduced fertility.\n\nVarious species have evolved different mechanisms for suppressing cancer. A paper in \"Cell Reports\" in January 2015 claimed to have found genes in the bowhead whale (\"Balaena mysticetus\") that may be associated with longevity. Around the same time, a second team of researchers identified a polysaccharide in the naked mole-rat that appeared to block the development of tumors. In October 2015, two independent studies showed that elephants have 20 copies of tumor suppressor gene TP53 in their genome, where humans and other mammals have only one. Additional research showed 14 copies of the gene present in the DNA of preserved mammoths, but only one copy of the gene in the DNA of manatees and hyraxes, the elephant's closest living relatives. The results suggest an evolutionary relationship between animal size and tumor suppression, as Peto had theorized.\n\nA 2014 paper in \"Evolutionary Applications\" by Maciak and Michalak emphasized what they termed \"a largely underappreciated relation of cell size to both metabolism and cell-division rates across species\" as key factors underlying the paradox, and concluded that \"larger organisms have bigger and slowly dividing cells with lower energy turnover, all significantly reducing the risk of cancer initiation.\"\n\nMaciak and Michalak argue that cell size is not uniform across mammalian species, making body size an imperfect proxy for the number of cells in an organism. (For example, the volume of an individual red blood cell of an elephant is roughly four times that of one from a common shrew.) Furthermore, larger cells divide more slowly than smaller ones, a difference which compounds exponentially over the life-span of the organism. Fewer cell divisions means fewer opportunities for cancer mutations, and mathematical models of cancer incidence are highly sensitive to cell-division rates. Additionally, larger animals generally have lower basal metabolic rates, following a well-defined inverse logarithmic relationship. Consequently, their cells will incur less damage over time per unit of body mass. Combined, these factors may explain much of the apparent paradox.\n\nThe apparent ability of bigger animals to suppress cancer across very large numbers of cells has spurred an active field of medical research. In one experiment, laboratory mice were genetically altered to express active TP53 tumor antigens, similar to the ones found in elephants. The mutated mice exhibited increased tumor suppression ability, but also showed signs of premature aging.\n\n"}
{"id": "33713269", "url": "https://en.wikipedia.org/wiki?curid=33713269", "title": "PhenX Toolkit", "text": "PhenX Toolkit\n\nPhenX (consensus measures for Phenotypes and eXposures) is a community-driven effort to provide standard measures for use in biomedical research. The goals are to help investigators identify opportunities for collaborative biomedical research and improve the consistency of data collection. The PhenX Toolkit is a web-based catalog of high-priority measures related to complex diseases, phenotypic traits and environmental exposures. These measures were selected by working groups of experts using a consensus process. Use of PhenX measures facilitates combining data from a variety of studies, and makes it easy for investigators to expand a study design beyond the primary research focus.\nThe Toolkit is funded by the National Institute of Health's National Human Genome Research Institute (NHGRI) with co-funding by the Office of Behavioral and Social Sciences Research (OBSSR) and the National Institute on Drug Abuse (NIDA). Supplemental funding is provided by the National Heart, Lung, and Blood Institute (NHLBI) and the National Institute on Minority Health and Health Disparities (NIMHD). The PhenX Toolkit is available to the scientific community at no cost.\n\nFor genome-wide association studies (GWAS) and other studies involving human subjects, the use of standard measures can facilitate cross-study analyses.\nSuch analyses compare independent findings to validate results or combine studies to increase sample size and statistical power. This increased power makes it possible to identify more subtle and complex associations such as gene-gene and gene-environment interactions.\n\nThe Toolkit has a broad scope and currently provides assessments for 24 research domains.\nIn addition, the Toolkit includes more focused add-on collections for specialists in 4 areas.\n\nThe PhenX Toolkit is a catalog of measures recommended by domain experts, with browse and search capabilities and bioinformatics support. PhenX measures and protocols are included in the Cancer Data Standards Repository (caDSR) Common Data Elements (CDE) Dictionary and the National Center for Biomedical Ontology (NCBO) BioPortal. PhenX protocols are available as REDCap Instrument Zip files, and work is underway to map PhenX variables to all studies in dbGaP. PhenX provides easy access to standard measures with the potential to increase the overall impact of individual studies via cross-study analyses. For each measure, the PhenX Toolkit provides a brief description, the protocol for measurement with supporting images and tables, the reasons for selecting the protocol, details about training and equipment, translations available and selected references. Users can browse research domains, measures, or collections, search using a “Smart Search” or a full text search, collect measures of interest in “My Toolkit”, and request custom data dictionaries and custom data collection worksheets.\n\nThe Toolkit can be particularly helpful when an investigator wants to expand a study to include measures that are outside his or her primary area of expertise. Whereas study specific measures are needed to address the primary research goal, common measures are needed to increase the overall impact of the study; both types of measures are important to overall study design.\n\nThe \"Link Your Study\" feature allows registered users to share their study with other registered users or find other studies using the same protocols and identify opportunities for cross-study analysis.\n\nIn addition to measures, PhenX also defines protocols. There may be multiple protocols defined for a measure. For example, for smoking cessation, there is an adult protocol (id: 030901) and adolescent protocol (id:030902).\n\nPhenX also provides a datatype (e.g., integer, string, date, enumerated, or encoded values) for each PhenX variable. For example, variable 'PX070501_Pregnancy_Outcome_1' for 'Outcome of Pregnancy?' is of type enumerated with value set of four possible values (Live birth,Still born,Miscarriage,Induced abortion).\n\n\n\n"}
{"id": "13213821", "url": "https://en.wikipedia.org/wiki?curid=13213821", "title": "Practice-based commissioning", "text": "Practice-based commissioning\n\nPractice-Based Commissioning (PBC) was a United Kingdom Department of Health initiative introduced in 2005 to improve primary care services by enabling healthcare professionals to decide how services are funded to meet the needs of the local population. PBC was designed to give healthcare staff, usually general medical practitioners (GPs), the resources and support to become directly involved in decisions on commissioning health services.\n\nPolicymakers wanted PBC to lead to high quality services for patients in local and convenient settings. The incentive for GP involvement was that their practices could retain a proportion of any savings they made to invest in their own practices. In this respect, it was seen as a new Labour policy successor to the early 1990s Conservative policy of GP fundholding. \n\nLord Warner, former Labour Minister of State for Health, described the benefits of PBC as: \"If there is an alternative that is better for the patient and better for the NHS, then practice-based commissioning provides the basis on which they can change the way that services are delivered.\" \n\nThe government intended GP practices to be supported by the PCTs (Primary Care Trusts) to buy in (\"commission\") services for their patients based on cost and quality of care. This process was expected to generate financial savings of which 7/10ths may be retained by the practice for further investment while the remainder is passed back to the PCT.\n\nTwo years after the initiative was introduced, both doctors and Primary Care Trusts were struggling to understand how to implement the scheme. By 2010, GP practices were forming coalitions known as consortia, in order to commission services at the necessary scale and efficiency. While this process was initially voluntary, eventually practices were ordered to become part of commissioning groups as part of the Health and Social Care Act 2012.\n\n"}
{"id": "788091", "url": "https://en.wikipedia.org/wiki?curid=788091", "title": "Psychological trauma", "text": "Psychological trauma\n\nPsychological trauma is a type of damage to the mind that occurs as a result of a distressing event. Trauma is often the result of an overwhelming amount of stress that exceeds one's ability to cope, or integrate the emotions involved with that experience. Trauma may result from a single distressing experience or recurring events of being overwhelmed that can be precipitated in weeks, years, or even decades as the person struggles to cope with the immediate circumstances, eventually leading to serious, long-term negative consequences.\n\nBecause trauma differs between individuals, according to their subjective experiences, people will react to similar traumatic events differently. In other words, not all people who experience a potentially traumatic event will actually become psychologically traumatized. However, it is possible for some people to develop post-traumatic stress disorder (PTSD) after being exposed to a major traumatic event.\nThis discrepancy in risk rate can be attributed to protective factors some individuals may have that enable them to cope with trauma; they are related to temperamental and environmental factors from among others. Some examples are mild exposure to stress early in life, resilience characteristics, and active seeking of help.\n\nThe Diagnostic and Statistical Manual of Mental Disorders (DSM-IV-TR) defines trauma as direct personal experience of an event that involves actual or threatened death or serious injury; threat to one's physical integrity, witnessing an event that involves the above experience, learning about unexpected or violent death, serious harm, or threat of death, or injury experienced by a family member or close associate. Memories associated with trauma are implicit, pre-verbal and cannot be recalled, but can be triggered by stimuli from the in vivo environment. The person's response to aversive details of traumatic event involve intense fear, helplessness or horror. In children it is manifested as disorganized or agitative behaviors.\n\nTrauma can be caused by a wide variety of events, but there are a few common aspects. There is frequently a violation of the person's core assumptions about the world and their human rights, putting the person in a state of extreme confusion and insecurity. This is seen when institutions depended upon for survival violate, humiliate, betray, or cause major losses or separations instead of evoking aspects like positive self worth, safe boundaries and personal freedom.\n\nPsychologically traumatic experiences often involve physical trauma that threatens one's survival and sense of security. Typical causes and dangers of psychological trauma include harassment, embarrassment, abandonment, abusive relationships, rejection, co-dependence, physical assault, sexual abuse, partner battery, employment discrimination, police brutality, judicial corruption and misconduct, bullying, paternalism, domestic violence, indoctrination, being the victim of an alcoholic parent, the threat or the witnessing of violence (particularly in childhood), life-threatening medical conditions, and medication-induced trauma. Catastrophic natural disasters such as earthquakes and volcanic eruptions, large scale transportation accidents, house or domestic fire, motor vehicle accident, mass interpersonal violence like war, terrorist attacks or other mass victimization like sex trafficking, being taken as a hostage or being kidnapped can also cause psychological trauma. Long-term exposure to situations such as extreme poverty or other forms of abuse, such as verbal abuse, exist independently of physical trauma but still generate psychological trauma.\n\nSome theories suggest childhood trauma can increase one's risk for mental disorders including post-traumatic stress disorder (PTSD), depression, and substance abuse. Childhood adversity is associated with neuroticism during adulthood. \nParts of the brain in a growing child are developing in a sequential and hierarchical order, from least complex to most complex. The brain's neurons are designed to change in response to the constant external signals and stimulation, receiving and storing new information. This allows the brain to continually respond to its surroundings and promote survival. Our five main sensory signals contribute to the developing brain structure and its function. \nInfants and children begin to create internal representations of their external environment, and in particular, key attachment relationships, shortly after birth. Violent and victimizing attachment figures impact infants' and young children's internal representations. The more frequent a specific pattern of brain neurons is activated, the more permanent the internal representation associated with the pattern becomes. This causes sensitization in the brain towards the specific neural network. Because of this sensitization, the neural pattern can be activated by decreasingly less external stimuli. \nChildhood abuse tends to have the most complications with long-term effects out of all forms of trauma because it occurs during the most sensitive and critical stages of psychological development. It could also lead to violent behavior, possibly as extreme as serial murder. For example, Hickey's Trauma-Control Model suggests that \"childhood trauma for serial murderers may serve as a triggering mechanism resulting in an individual's inability to cope with the stress of certain events.\"\n\nOften psychodynamic aspects of trauma are overlooked even by health professionals: \"If clinicians fail to look through a trauma lens and to conceptualize client problems as related possibly to current or past trauma, they may fail to see that trauma victims, young and old, organize much of their lives around repetitive patterns of reliving and warding off traumatic memories, reminders, and affects.\"\n\nPeople who go through these types of extremely traumatic experiences often have certain symptoms and problems afterward. The severity of these symptoms depends on the person, the type of trauma involved, and the emotional support they receive from others. The range of reactions to and symptoms of trauma can be wide and varied, and differ in severity from person to person. A traumatized individual may experience one or several of them.\n\nAfter a traumatic experience, a person may re-experience the trauma mentally and physically, hence trauma reminders, also called triggers, can be uncomfortable and even painful. Re-experiencing can damage people’s sense of safety, self, self-efficacy, as well as their ability to regulate emotions and navigate relationships. They may turn to psychoactive substances including alcohol to try to escape or dampen the feelings. These triggers cause flashbacks, which are dissociative experiences where the person feels as though the events are recurring. Flashbacks can range from distraction to complete dissociation or loss of awareness of the current context. Re-experiencing of symptoms is a sign that the body and mind are actively struggling to cope with the traumatic experience.\n\nTriggers and cues act as reminders of the trauma and can cause anxiety and other associated emotions. Often the person can be completely unaware of what these triggers are. In many cases this may lead a person suffering from traumatic disorders to engage in disruptive behaviors or self-destructive coping mechanisms, often without being fully aware of the nature or causes of their own actions. Panic attacks are an example of a psychosomatic response to such emotional triggers.\n\nConsequently, intense feelings of anger may frequently surface, sometimes in inappropriate or unexpected situations, as danger may always seem to be present due to re-experiencing past events. Upsetting memories such as images, thoughts, or flashbacks may haunt the person, and nightmares may be frequent. Insomnia may occur as lurking fears and insecurity keep the person vigilant and on the lookout for danger, both day and night. Trauma doesn't only cause changes in one's daily functions, but could also lead to morphological changes. Such epigenetic changes can be passed on to the next generation, thus making genetics one of the components of psychological trauma. However, some people are born with or later develop protective factors such as genetics and sex that help lower their risk of psychological trauma.\n\nThe person may not remember what actually happened, while emotions experienced during the trauma may be re-experienced without the person understanding why (see Repressed Memory). This can lead to the traumatic events being constantly experienced as if they were happening in the present, preventing the subject from gaining perspective on the experience. This can produce a pattern of prolonged periods of acute arousal punctuated by periods of physical and mental exhaustion. This can lead to mental health disorders like acute stress and anxiety disorder, traumatic grief, undifferentiated somatoform disorder, conversion disorders, brief psychotic disorder, borderline personality disorder, adjustment disorder, etc.\n\nIn time, emotional exhaustion may set in, leading to distraction, and clear thinking may be difficult or impossible. Emotional detachment, as well as dissociation or \"numbing out\" can frequently occur. Dissociating from the painful emotion includes numbing all emotion, and the person may seem emotionally flat, preoccupied, distant, or cold. Dissociation includes depersonalisation disorder, dissociative amnesia, dissociative fugue, dissociative identity disorder, etc. Exposure to and re-experiencing trauma can cause neurophysiological changes like slowed myelination, abnormalities in synaptic pruning, shrinking of the hippocampus, cognitive and affective impairment. This is significant in brain scan studies done regarding higher order function assessment with children and youth who were in vulnerable environments.\n\nSome traumatized people may feel permanently damaged when trauma symptoms do not go away and they do not believe their situation will improve. This can lead to feelings of despair, transient paranoid ideation, loss of self-esteem, profound emptiness, suicidality, and frequently depression. If important aspects of the person's self and world understanding have been violated, the person may call their own identity into question. Often despite their best efforts, traumatized parents may have difficulty assisting their child with emotion regulation, attribution of meaning, and containment of post-traumatic fear in the wake of the child's traumatization, leading to adverse consequences for the child. In such instances, seeking counselling in appropriate mental health services is in the best interests of both the child and the parent(s).\n\nTrauma can be caused by man-made, technological disasters and natural disasters, including war, abuse, violence, mechanized accidents (car, train, or plane crashes, etc.) or medical emergencies.\n\nResponses to psychological trauma:\nResponse to psychological trauma can be varied based on the type of trauma, as well as socio demographic and background factors.\nThere are several behavioral responses common towards stressors including the proactive, reactive, and passive responses. Proactive responses include attempts to address and correct a stressor before it has a noticeable effect on lifestyle. Reactive responses occur after the stress and possible trauma has occurred and are aimed more at correcting or minimizing the damage of a stressful event. A passive response is often characterized by an emotional numbness or ignorance of a stressor.\n\nThose who are able to be proactive can often overcome stressors and are more likely to be able to cope well with unexpected situations. On the other hand, those who are more reactive will often experience more noticeable effects from an unexpected stressor. In the case of those who are passive, victims of a stressful event are more likely to suffer from long-term traumatic effects and often enact no intentional coping actions. These observations may suggest that the level of trauma associated with a victim is related to such independent coping abilities.\n\nThere is also a distinction between trauma induced by recent situations and long-term trauma which may have been buried in the unconscious from past situations such as childhood abuse. Trauma is sometimes overcome through healing; in some cases this can be achieved by recreating or revisiting the origin of the trauma under more psychologically safe circumstances, such as with a therapist.\n\nFrench neurologist Jean-Martin Charcot argued in the 1890s that psychological trauma was the origin of all instances of the mental illness known as hysteria. Charcot's \"traumatic hysteria\" often manifested as a paralysis that followed a physical trauma, typically years later after what Charcot described as a period of \"incubation\".\nSigmund Freud, Charcot's student and the father of psychoanalysis, examined the concept of psychological trauma throughout his career. Jean Laplanche has given a general description of Freud's understanding of trauma, which varied significantly over the course of Freud's career: \"An event in the subject's life, defined by its intensity, by the subject's incapacity to respond adequately to it and by the upheaval and long-lasting effects that it brings about in the psychical organization\".\n\nThe French psychoanalyst Jacques Lacan claimed that what he called \"The Real\" had a traumatic quality external to symbolization. As an object of anxiety, Lacan maintained that The Real is \"the essential object which isn't an object any longer, but this something faced with which all words cease and all categories fail, the object of anxiety \"par excellence\"\".\n\nAll psychological traumas originate from stress, a physiological response to an unpleasant stimulus. Long term stress increases the risk of poor mental health and mental disorders, which can be attributed to secretion of glucocorticoids for a long period of time. Such prolonged exposure causes many physiological dysfunctions such as the suppression of the immune system and increase in blood pressure. Not only does it affect the body physiologically, but a morphological change in the hippocampus also takes place. Studies showed that extreme stress early in life can disrupt normal development of hippocampus and impact its functions in adulthood. Studies surely show a correlation between the size of hippocampus and one's susceptibility to stress disorders. In times of war, psychological trauma has been known as shell shock or combat stress reaction. Psychological trauma may cause an acute stress reaction which may lead to post-traumatic stress disorder (PTSD). PTSD emerged as the label for this condition after the Vietnam War in which many veterans returned to their respective countries demoralized, and sometimes, addicted to psychoactive substances. The symptoms of PTSD must persist for at least a month for diagnosis. The main symptoms of PTSD consist of four main categories: trauma (i.e. intense fear), reliving (i.e. flashbacks), avoidance behavior (i.e. emotional numbing), and hypervigilance (i.e. continuous scanning of the environment for danger). Research shows that about 60% of the US population reported as having experienced at least one traumatic symptom in their lives, but only a small proportion actually develops PTSD. There is a correlation between the risk of PTSD and whether or not the act was inflicted deliberately by the offender. Psychological trauma is treated with therapy and, if indicated, psychotropic medications.\n\nThe term \"continuous post traumatic stress disorder\" (CTSD) was introduced into the trauma literature by Gill Straker (1987). It was originally used by South African clinicians to describe the effects of exposure to frequent, high levels of violence usually associated with civil conflict and political repression. The term is also applicable to the effects of exposure to contexts in which gang violence and crime are endemic as well as to the effects of ongoing exposure to life threats in high-risk occupations such as police, fire and emergency services.\n\nAs one of the processes of treatment, confrontation with their sources of trauma plays a crucial role. While debriefing people immediately after a critical incident has not been shown to reduce incidence of PTSD, coming alongside people experiencing trauma in a supportive way has become standard practice.\n\nVicarious trauma affects workers who witnesses their clients' trauma. It is more likely to occur in situations where trauma related work is the norm rather than the exception. Listening with empathy to the clients generates feeling, and seeing oneself in clients' trauma may compound the risk for developing trauma symptoms. Trauma may also result if workers witness situations that happen in the course of their work (e.g. violence in the workplace, reviewing violent video tapes.) Risk increases with exposure and with the absence of help seeking protective factors and pre-preparation of preventive strategies.\n\nAs \"trauma\" adopted a more widely defined scope, traumatology as a field developed a more interdisciplinary approach. This is in part due to the field's diverse professional representation including: psychologists, medical professionals, and lawyers. As a result, findings in this field are adapted for various applications, from individual psychiatric treatments to sociological large-scale trauma management. However, novel fields require novel methodologies. While the field has adopted a number of diverse methodological approaches, many pose their own limitations in practical application.\n\nThe experience and outcomes of psychological trauma can be assessed in a number of ways. Within the context of a clinical interview, the risk of imminent danger to the self or others is important to address but is not the focus of assessment. In most cases, it will not be necessary to involve contacting emergency services (e.g., medical, psychiatric, law enforcement) to ensure the individuals safety; members of the individual's social support network are much more critical.\n\nUnderstanding and accepting the psychological state an individual is in is paramount. There are many misconceptions of what it means for a traumatized individual to be in psychological crisis. These are times when an individual is in inordinate amounts of pain and incapable of self-comfort. If treated humanely and respectfully the individual is less likely to resort to self harm. In these situations it is best to provide a supportive, caring environment and to communicate to the individual that no matter the circumstance, the individual will be taken seriously rather than being treated as delusional. It is vital for the assessor to understand that what is going on in the traumatized person's head is valid and real. If deemed appropriate, the assessing clinician may proceed by inquiring about both the traumatic event and the outcomes experienced (e.g., post-traumatic symptoms, dissociation, substance abuse, somatic symptoms, psychotic reactions). Such inquiry occurs within the context of established rapport and is completed in an empathic, sensitive, and supportive manner. The clinician may also inquire about possible relational disturbance, such as alertness to interpersonal danger, abandonment issues, and the need for self-protection via interpersonal control. Through discussion of interpersonal relationships, the clinician is better able to assess the individual's ability to enter and sustain a clinical relationship.\n\nDuring assessment, individuals may exhibit activation responses in which reminders of the traumatic event trigger sudden feelings (e.g., distress, anxiety, anger), memories, or thoughts relating to the event. Because individuals may not yet be capable of managing this distress, it is necessary to determine how the event can be discussed in such a way that will not \"retraumatize\" the individual. It is also important to take note of such responses, as these responses may aid the clinician in determining the intensity and severity of possible post traumatic stress as well as the ease with which responses are triggered. Further, it is important to note the presence of possible avoidance responses. Avoidance responses may involve the absence of expected activation or emotional reactivity as well as the use of avoidance mechanisms (e.g., substance use, effortful avoidance of cues associated with the event, dissociation).\n\nIn addition to monitoring activation and avoidance responses, clinicians carefully observe the individual's strengths or difficulties with affect regulation (i.e., affect tolerance and affect modulation). Such difficulties may be evidenced by mood swings, brief yet intense depressive episodes, or self-mutilation. The information gathered through observation of affect regulation will guide the clinician's decisions regarding the individual's readiness to partake in various therapeutic activities.\n\nThough assessment of psychological trauma may be conducted in an unstructured manner, assessment may also involve the use of a structured interview. Such interviews might include the Clinician-Administered PTSD Scale (CAPS; Blake et al., 1995), Acute Stress Disorder Interview (ASDI; Bryant, Harvey, Dang, & Sackville, 1998), Structured Interview for Disorders of Extreme Stress (SIDES; Pelcovitz et al., 1997), Structured Clinical Interview for DSM-IV Dissociative Disorders- Revised (SCID-D; Steinberg, 1994), and Brief Interview for post-traumatic Disorders (BIPD; Briere, 1998).\n\nLastly, assessment of psychological trauma might include the use of self-administered psychological tests. Individual scores on such tests are compared to normative data in order to determine how the individual's level of functioning compares to others in a sample representative of the general population. Psychological testing might include the use of generic tests (e.g., MMPI-2, MCMI-III, SCL-90-R) to assess non-trauma-specific symptoms as well as difficulties related to personality. In addition, psychological testing might include the use of trauma-specific tests to assess post-traumatic outcomes. Such tests might include the post-traumatic Stress Diagnostic Scale (PDS; Foa, 1995), Davidson Trauma Scale (DTS: Davidson et al., 1997), Detailed Assessment of post-traumatic Stress (DAPS; Briere, 2001), Trauma Symptom Inventory (TSI: Briere, 1995), Trauma Symptom Checklist for Children (TSCC; Briere, 1996), Traumatic Life Events Questionnaire (TLEQ: Kubany et al., 2000), and Trauma-related Guilt Inventory (TRGI: Kubany et al., 1996).\n\nChildren are assessed through activities and therapeutic relationship, some of the activities are play genogram, sand worlds, coloring feelings, Self and Kinetic family drawing, symbol work, dramatic-puppet play, story telling, Briere's TSCC, etc.\n\nA number of psychotherapy approaches have been designed with the treatment of trauma in mind—EMDR, progressive counting (PC), somatic experiencing, biofeedback, Internal Family Systems Therapy, and sensorimotor psychotherapy.\n\nThere is a large body of empirical support for the use of cognitive behavioral therapy for the treatment of trauma-related symptoms, including post-traumatic stress disorder. Institute of Medicine guidelines identify cognitive behavioral therapies as the most effective treatments for PTSD. Two of these cognitive behavioral therapies, prolonged exposure and cognitive processing therapy, are being disseminated nationally by the Department of Veterans Affairs for the treatment of PTSD. Seeking Safety is another type of cognitive behavioral therapy that focuses on learning safe coping skills for co-occurring PTSD and substance use problems. While some sources highlight Seeking Safety as effective with strong research support, others have suggested that it did not lead to improvements beyond usual treatment. Recent studies show that a combination of treatments involving dialectical behavior therapy (DBT), often used for borderline personality disorder, and exposure therapy is highly effective in treating psychological trauma. If, however, psychological trauma has caused dissociative disorders or complex PTSD, the trauma model approach (also known as phase-oriented treatment of structural dissociation) has been proven to work better than simple cognitive approach. Studies funded by pharmaceuticals have also shown that medications such as the new anti-depressants are effective when used in combination with other psychological approaches.\n\nTrauma therapy allows processing trauma-related memories and allows growth towards more adaptive psychological functioning. It helps to develop positive coping instead of negative coping and allows the individual to integrate upsetting-distressing material (thoughts, feelings and memories) and to resolve these internally. It also aids in growth of personal skills like resilience, ego regulation, empathy, etc.\n\nProcesses involved in trauma therapy are:\n\n\nA number of complementary approaches to trauma treatment have been implicated as well, including yoga and meditation. Trauma-sensitive yoga has been specifically developed for the purposes of use with traumatized individuals.\n\n\n"}
{"id": "4905289", "url": "https://en.wikipedia.org/wiki?curid=4905289", "title": "Public health informatics", "text": "Public health informatics\n\nPublic health informatics has been defined as the systematic application of information and computer science and technology to public health practice, research, and learning. It is one of the subdomains of health informatics.\n\nPublic health informatics is defined as the use of computers, clinical guidelines, communication and information systems, which apply to vast majority of public health, related professions, such as nursing, clinical/ hospital care/ public health and medical research.\n\nIn developed countries like the United States, public health informatics is practiced by individuals in public health agencies at the federal and state levels and in the larger local health jurisdictions. Additionally, research and training in public health informatics takes place at a variety of academic institutions.\n\nAt the federal Centers for Disease Control and Prevention in US states like Atlanta, Georgia, the Public Health Surveillance and Informatics Program Office (PHSIPO) focuses on advancing the state of information science and applies digital information technologies to aid in the detection and management of diseases and syndromes in individuals and populations.\n\nThe bulk of the work of public health informatics in the United States, as with public health generally, takes place at the state and local level, in the state departments of health and the county or parish departments of health. At a state health department the activities may include: collection and storage of \"vital statistics\" (birth and death records); collection of reports of communicable disease cases from doctors, hospitals, and laboratories, used for infectious disease surveillance; display of infectious disease statistics and trends; collection of child immunization and lead screening information; daily collection and analysis of emergency room data to detect early evidence of biological threats; collection of hospital capacity information to allow for planning of responses in case of emergencies. Each of these activities presents its own information processing challenge.\n\nSince the beginning of the World Wide Web, public health agencies with sufficient information technology resources have been transitioning to web-based collection of public health data, and, more recently, to automated messaging of the same information. In the years roughly 2000 to 2005 the Centers for Disease Control and Prevention, under its National Electronic Disease Surveillance System (NEDSS), built and provided free to states a comprehensive web and message-based reporting system called the NEDSS Base System (NBS). Due to the funding being limited and it not being wise to have fiefdom-based systems, only a few states and larger counties have built their own versions of electronic disease surveillance systems, such as Pennsylvania's PA-NEDSS. These do not provide timely full intestate notification services causing an increase in disease rates versus the NEDSS federal product.\n\nTo promote interoperability, the CDC has encouraged the adoption in public health data exchange of several standard vocabularies and messaging formats from the health care world. The most prominent of these are: the Health Level 7 (HL7) standards for health care messaging; the LOINC system for encoding laboratory test and result information; and the Systematized Nomenclature of Medicine (SNOMED) vocabulary of health care concepts.\n\nSince about 2005, the CDC has promoted the idea of the Public Health Information Network to facilitate the transmission of data from various partners in the health care industry and elsewhere (hospitals, clinical and environmental laboratories, doctors' practices, pharmacies) to local health agencies, then to state health agencies, and then to the CDC. At each stage the entity must be capable of receiving the data, storing it, aggregating it appropriately, and transmitting it to the next level. A typical example would be infectious disease data, which hospitals, labs, and doctors are legally required to report to local health agencies; local health agencies must report to their state public health department; and which the states must report in aggregate form to the CDC. Among other uses, the CDC publishes the Morbidity and Mortality Weekly Report (MMWR) based on these data acquired systematically from across the United States.\n\nMajor issues in the collection of public health data are: awareness of the need to report data; lack of resources of either the reporter or collector; lack of interoperability of data interchange formats, which can be at the purely syntactic or at the semantic level; variation in reporting requirements across the states, territories, and localities.\n\nPublic health informatics can be thought or divided into three categories.\n\nThe first category is to discover and study models of complex systems, such as disease transmission. This can be done through different types of data collections, such as hospital surveys, or electronic surveys submitted to the organization (such as the CDC). Transmission rates or disease incidence rates/surveillance can be obtained through government organizations, such as the CDC, or global organizations, such as WHO. Not only disease transmission/rates can be looked at. Public health informatics can also delve into people with/without health insurance and the rates at which they go to the doctor. Before the advent of the internet, public health data in the United States, like other healthcare and business data, were collected on paper forms and stored centrally at the relevant public health agency. If the data were to be computerized they required a distinct data entry process, were stored in the various file formats of the day and analyzed by mainframe computers using standard batch processing.\n\nThe second category is to find ways to improve the efficiency of different public health systems. This is done through various collections methods, storage of data and how the data is used to improve current health problems. In order to keep everything standardized, vocabulary and word usage needs to be consistent throughout all systems. Finding new ways to link together and share new data with current systems is important to keep everything up to date.\n\nStorage of public health data shares the same data management issues as other industries. And like other industries, the details of how these issues play out are affected by the nature of the data being managed.\n\nDue to the complexity and variability of public health data, like health care data generally, the issue of data modeling presents a particular challenge. While a generation ago flat data sets for statistical analysis were the norm, today's requirements of interoperability and integrated sets of data across the public health enterprise require more sophistication. The relational database is increasingly the norm in public health informatics. Designers and implementers of the many sets of data required for various public health purposes must find a workable balance between very complex and abstract data models such as HL7's Reference Information Model (RIM) or CDC's Public Health Logical Data Model, and simplistic, ad hoc models that untrained public health practitioners come up with and feel capable of working with.\n\nDue to the variability of the incoming data to public health jurisdictions, data quality assurance is also a major issue.\n\nFinally, the last category can be thought as maintaining and enriching current systems and models to adapt to overflow of data and storing/sorting of this new data. This can be as simple as connecting directly to an electronic data collection source, such as health records from the hospital, or can go public information (CDC) about disease rates/transmission. Finding new algorithms that will sort through large quantities of data quickly and effectively is necessary as well.\n\nThe need to extract usable public health information from the mass of data available requires the public health informaticist to become familiar with a range of analysis tools, ranging from business intelligence tools to produce routine or ad hoc reports, to sophisticated statistical analysis tools such as DAP/SAS and PSPP/SPSS, to Geographical Information Systems (GIS) to expose the geographical dimension of public health trends.Such analyses usually require methods that appropriately secure the privacy of the health data. One approach is to separate the individually identifiable variables of the data from the rest\n\nThere are a few organizations out there that provide useful information for those professionals that want to be more involved in public health informatics. Such as the American Medical Informatics Association (AMIA). AMIA is for professions that are involved in health care, informatics research, biomedical research, including physicians, scientists, researchers, and students. The main goals of AMIA are to move from ‘bench to bedside’, help improve the impact of health innovations and advance the public health informatics field. They hold annual conferences, online classes and webinars, which are free to their members. There is also a career center specific for the biomedical and health informatics community.\n\nMany jobs or fellowships in public health informatics are offered. The CDC (Center for Disease Control) has various fellowship programs, while multiple colleges/companies offer degree programs or training in this field.\n\nFor more information on these topics, follow the links below:\n\nhttp://www.jhsph.edu/departments/health-policy-and-management/certificates/public-health-informatics/what-is-health-informatics.html\n\nhttps://web.archive.org/web/20150406033743/http://www.phii.org/what-we-do\n\n\nSince the late 2000s, data from social media websites such as Twitter and Facebook, as well as search engines such as Google and Bing, have been used extensively in detecting trends in public health.\n\n"}
{"id": "19064373", "url": "https://en.wikipedia.org/wiki?curid=19064373", "title": "Refugee health", "text": "Refugee health\n\nRefugee health, also known as migrant health or immigrant health, is the field of study on the health effects experienced by people who have moved into another country or even to another part of the world, either by choice or as a result of unsafe circumstances such as war or persecution. Displaced populations' health is mainly affected by infectious disease, mental health, and chronic diseases that are uncommon in the country in which they eventually settle. Refugee health status is largely due to factors such as the migrant's geographic origin, conditions of refugee camps or urban settings where they lived, and personal, physical, and psychological conditions of the migrant, either pre-existing or acquired while traveling from their homeland to a camp or eventually to their new home.\n\nRefugees may be at a higher risk for contracting certain diseases or having other health problems due to factors such as poor nutrition, poor sanitation and lack of adequate medical care. The most common health concerns are listed below.\n\nNon-communicable disease (NCD) is a medical condition that is not transmissible and non-infectious. This means it is not caused by an infectious agent but instead is perpetuated by individual and environmental behaviors. According to the WHO, NCDs lead to an estimated 40 million deaths per year—worldwide. Accordingly, 70% of deaths worldwide are credited to NCDs. It is found that NCD development and control is directly linked with nutrition and healthy behaviors. Non-communicable diseases have accounted for 19-46% of mortality from the top five refugee-producing countries in 2015. Accordingly, the overall proportion of deaths credited to NCDs has risen over 50%. Reports indicate that more than half of Syrian refugee households (resettled in Jordan) have a member suffering from an non-communicable disease.\n\nDiabetes is a group of chronic metabolic diseases that affect the body's use of blood sugar. There are two main forms of diabetes: type 1 and type 2. Type 1 diabetes is characterized by insulin deficiency and requires daily administered doses of insulin. Causes of Type 1 diabetes are unknown and are currently, not preventable. It is typically onset at an early age. Type 2 diabetes is characterized by the body's inability to properly utilize insulin. Type 2 diabetes is typically onset in adults and is linked with unhealthy behaviors. Another common form of diabetes is gestational diabetes. This occurs in pregnant women and does not necessarily lead to Type 1 or Type 2 diabetes permanently.\n\nRefugees are at an increased risk of developing diabetes because of the tendency towards inadequate nutritional behaviors. According to the CDC, amongst Syrian refugees, there is a 6.1% prevalence of adult-onset diabetes. Iraqi refugees saw a 3% prevalence and Congolese refugees faced less than 1%. A literary analysis on diabetes risk amongst refugee populations suggests that increased diabetes risk among adult refugees may be associated with longer migration histories. The analysis also links increased diabetes prevalence with the transition from traditional, agricultural lifestyles with potentially protective foods, to urbanized, westernized lifestyles that come with migration.\n\nAnemia is a condition in which an individual does not have enough healthy red blood cells. This will consequently lead to reduced oxygen flow to the body's organs. Most commonly, this is caused by not consuming enough iron. Anemia is used as a marker for overall micronutrient deficiency. Symptoms usually involve overall fatigue and tiredness, as a result of reduced oxygen flow. There are various treatments for anemia, including iron supplements and vitamin B supplements. Blood transfusions may also be used if blood production is low.\n\nAccording to the CDC, “an evaluation of anemia prevalence in the Zaatari refugee camp and surrounding areas showed that 48.4% of children younger than 5 years of age, and 44.8% of women 15-49 years of age suffered from anemia”. Amongst Congolese refugees, Sickle Cell Anemia (SCD) is of a much larger concern. In Central America, refugees coming from El Salvador, Guatemala, and Honduras show the highest incidence of anemia cases. The CDC reports that the prevalence for children under 5 years old is 30% in El Salvador, 47% in Guatemala, and 40% in Honduras. In Guatemala, 22% of pregnant women are also anemic. These cases are mostly credited to poor nutrition or a chronic parasitic infection.\n\nCardiovascular disease is a general term for various heart conditions such as coronary artery disease, cardiac arrest, arrhythmias, and many more. Hypertension is high blood pressure—this is usually defined as blood pressure over 130/80. Cardiovascular disease and hypertension are associated with poor nutrition/diet, sedentary lifestyles, and genetic risk factors.\n\nAmongst Syrian refugees, 4.1% of adults suffered from cardiovascular disease and 10.7% suffered from hypertension. There is also substantial risk amongst Congolese refugees. According to the CDC, amongst Iraqi refugees in Jordan, 33% of those over 15 years of age had hypertension. Another 42% were pre-hypertensive. Bhutanese refugee adults had a 3% prevalence of hypertension, and nearly 1% prevalence of chronic obstructive pulmonary disorder.\n\nTuberculosis (TB) is a bacterial infection that mainly affects the lungs. As an airborne disease, TB is spread via inhalation of the bacteria, which subsequently travel to the lungs and other body parts to manifest infection. Once a person is infected, TB can either become latent or active. If latent, the disease is asymptomatic and non-contagious; however, latent TB can become active at any point. Active TB is symptomatic and contagious. Either way, TB should be treated immediately, as untreated infections can be fatal.\n\nAn estimated third of the world's population is infected with Mycobacterium tuberculosis. This high incidence necessitates that those conducting the overseas exam (Panel Physicians) screen all refugees for TB and further test anyone suspected of having active TB. Screening for tuberculosis generally involves a tuberculin skin test, followed by a chest X-ray when necessary, and laboratory testing depending on those results. Anyone between the ages of 2 and 14, living in a country with a tuberculosis incidence rate of 20 or more cases per 100,000 people (as identified by the WHO), is required to have a tuberculin skin test. Those aged 15 and older must have a chest x-ray.\n\nIn the US, refugee individuals identified as having active tuberculosis must complete treatment before being permitted to enter. Upon arriving in the US, the CDC recommends that all refugees be screened for tuberculosis using a tuberculin skin test. A follow-up chest x-ray is required if the tuberculin skin test is positive, or if the refugee was identified as having TB (either Class A or Class B) in their overseas exam, or if they are infected with HIV.\n\nOver 2 billion people are infected with TB worldwide. Specifically amongst refugee populations, the risk of contracting TB are higher than in the general population, as overcrowding and international travel is higher and more frequent. According to the WHO, as of 2016, the TB incidence rate in Syria is 17 per 100,000 people. Compare this to 3.1 per 100,000 people in the United States.\n\nThere are multiple types of hepatitis, which most broadly can be described as viral infections of the liver. The most common types are viral Hepatitis A, B, and C. Hepatitis B and C can result in chronic infections, while Hepatitis A is solely infectious. As such, Hepatitis A is also referred to as Infectious Hepatitis, and is caused by the Hepatitis A Virus (HAV). HAV can be spread directly or indirectly via fecal contact, causal contact, sexual contact, and foodborne or waterborne pathways. Because of this, refugee populations are more susceptible to this infection. According to a 2016 study conducted in Greece, the rate of Infectious Hepatitis amongst Syrian refugees in certain Greek facilities is 152 per 1,000 people; rates in refugees from Afghanistan and Iraq were much lower, at 8 per 1,000 and 9 per 1,000 people, respectively. The disproportionately higher rate in Syrian refugees can be attributed to the higher proportion of Syrian refugees in the camps, as compared to refugees from other countries of origin. There is no treatment for HAV infections, so hygienic intervention measures and vaccinations are of the highest priority in preventative measures. However, health care is often not prioritized in refugee populations and resources are limited, thus making it difficult to properly control the rate and spread of infection.\n\nHepatitis B infection is endemic in Africa, Southeast Asia, East Asia, Northern Asia, and most of the Pacific Islands. According to the CDC, the rate of chronic infection among persons emigrating to the US from these areas is between 5% and 15%. Many states require or recommend that all refugees be screened for hepatitis B, and proceed with immunizations for all who are susceptible to this infection.\n\nRefugees can be at a higher risk for contracting sexually transmitted infections because of a lack of access to protection and/or treatment, as well as the circumstances of war and flight, making them subject to higher incidences of rape and sexual abuse. Refugees are regularly screened for syphilis, gonorrhea, chlamydia, and HIV infection when they relocate.\n\nIntestinal parasites are a major health problem for many groups, including refugees, and the presence of pathogenic parasites requires medical attention. \"Over one billion persons worldwide are estimated to be carriers of Ascaris. Approximately 480 million people are infected with Entamoeba histolytica. At least 500 million carry Trichuris. At present, 200 to 300 million people are infected with one or more of the Schistosoma species and it is estimated that more than 20 million persons throughout the world are infected with Hymenolepsis nana\". Consequences of parasitic infection can include anemia due to blood loss and iron deficiency, malnutrition, growth retardation, invasive disease, and death. Refugees are particularly at risk given the likelihood of poor or contaminated water and poor hygienic conditions in camps. Since 1999, the CDC has recommended that US-bound refugee populations from Africa and Southeast Asia undergo presumptive treatment for parasitic infections prior to departure. The US Protocol includes a single dose of albendazole. In many states, the domestic health screening exam recommends that all refugees be screened for parasitic infections whether or not they appear symptomatic. Screening often includes two stool specimens obtained more than 24 hours apart and/or a CBC with differential for evaluation of eosinophilia.\n\nMalaria is considered endemic in the Americas from as far north as Mexico to as far south as Argentina, in Africa from Egypt to South Africa, in Asia from Turkey to Indonesia, and in the islands of Oceania. It is estimated that 300 to 500 million people are infected each year with malaria, and over one million people die every year from the disease, predominantly in sub-Saharan Africa. Based on the high prevalence of asymptomatic malaria in sub-Saharan Africa, the CDC recommends that US-bound refugee populations from this region undergo presumptive treatment prior to departure to the US. For those refugee arrivals from sub-Saharan Africa with no pre-departure treatment documentation, the CDC recommends either they receive presumptive treatment on arrival (preferred) or have laboratory screening to detect Plasmodium infection. For refugees from other areas of the world where asymptomatic malaria is not prevalent, the CDC recommends that any refugee with signs or symptoms of malaria should receive diagnostic testing for Plasmodium, and subsequent treatment for confirmed infections, but not presumptive treatment.\n\nGiardiasis is an intestinal parasitic infection, where the protozoa is in its flagellate mode of movement. It is most commonly spread through contaminated water and food in developing countries. Symptoms are rather mild, and include abdominal pain, flatulence, and loose stool. Studies have found that Giardiasis is common amongst refugee populations, specifically those coming from Afghanistan. However, the parasite is not particularly adept at sustaining infection within children.\n\nLeishmaniasis is another parasitic infection with a high burden of disease amongst refugee populations. It is a vector-borne parasite, commonly spread by the bite of an infected sand flies. There are two common types of manifestation: cutaneous (skin lesions) and visceral leishmaniasis (infection of internal organs). In 2012, there was an outbreak of Leishmaniasis amongst Syrians. Leishmaniasis is of major concern in the eastern Mediterranean, which is home to the majority of the globe's prevalence (≈57%). Leishmaniasis is most common in this region, as well as in Afghanistan, Iraq, and the Syrian Arab Republic. Thus, refugees coming from these regions, which is the majority of all refugees, are highly susceptible to becoming infected by this parasite. Additionally, refugees from other countries are put at a high risk of contraction, as they often share temporary settlements with refugees from Syria, Afghanistan, and Iraq. As a preventative measure, refugees are administered, when available, albendazole and ivermectin prior to their asylum seeking journey to other countries like the United States. Upon arrival, refugees are typically screened for these infections in order to prevent spread and fatality.\n\nAs mental illnesses are not necessarily tangible or easily quantifiable, it is easy to disregard the real ramifications that poor mental health can have on a person. These repercussions can materialize in any aspect of a person's life, whether that be physical, social, financial, etc. Further, the manifestations of poor mental health are deeply rooted when trauma is experienced at a young age. Thus, populations vulnerable to traumatic experiences are at a concerningly high risk of mental illnesses and poor mental health.\n\nPrior to World War II, immigrants were mainly driven from their countries by forces such as unemployment, famine and poverty, often combined with various forms of prejudice and oppression whilst war and ethnopolitical conflict were less common causes for emigration. They have known social oppression, including inadequate education, lack of job opportunities, inability to practice their faith or marry whom they wished, and inability to live where they want. Beginning with World War II, however, civilians were increasingly targeted as a strategy of warfare, and since then most newcomers (especially refugees) have been victims of war and/or political repression. Many of them have also experienced or witnessed government-sponsored torture and/or terror. That said, refugees are often survivors who possess amazing resiliency, strength and resourcefulness. An assessment of mental health may be included in a refugee's domestic health screening.\n\nRefugee mental health and integration into a new society are exquisitely interwoven. Traumatic experiences that occurred in the home country or during the resulting flight from that country are common. These experiences, in addition to the stresses of resettling in the host country, increase the chances of a less successful adjustment to the society of the host country. Mental health problems are one of the key barriers to the labor market integration of refugees in host societies. The influence of these traumatic and stressful events may be temporary and manageable with straightforward solutions or may be disabling and enduring.\n\nHigh rates of mental health concerns have been documented in various refugee populations. Most studies reveal high rates of post-traumatic stress disorder (PTSD), anxiety, depression, and somatization among newly arrived refugees. Variations reported in the prevalence of PTSD and depression may be ascribed to a number of factors, including prior life in their homeland, the experience of flight from that homeland, life in refugee camps, and stressors during and after resettlement in a third country. More specifically, socioeconomic status, educational background, and gender all affect levels of mental illness. In 2015, a study focused on the impacts of traumatic events on displaced persons from Syria, Lebanon, Turkey, and Jordan. It revealed that 54% of the population studied suffered from a severe emotional disorder. Of the children who participated in the study, 44% revealed depressive symptoms, and 45% showed signs of PTSD. Compared to other children around the globe, these statistics show a 10-fold increase in mental health disorders. Similarly to topics surrounding menstrual health, mental health is considered to be another taboo topic amongst certain cultures. This prevents people from seeking psychiatric help. Currently, there is only one functioning mental health hospital in Syria that tends to psychiatric needs. In 2016, a Syrian-American doctor named M.K. Hamza coined a new term to more accurately describe the effects felt by nearly all refugees affected by the ongoing crisis—human devastation syndrome. There is a severe lack of, and a dire need for, mental health attention and care. These traumatic events typically worsen and amplify progressively in the years following.\n\nIt is critical that mental health issues be addressed in the screening process. Leaving behind all that is familiar and starting a new life in a new country with a different language and culture in addition to previous trauma and dislocation produces an immediate challenge that can have long-term effects. This is true whether an individual is coming from Europe, sub-Saharan Africa, Central America, or elsewhere in the world. Many refugees will not share a Western perspective or vocabulary, so questions will need to be explained through specific examples or re-framed in culturally congruent terms with the assistance of an interpreter or bicultural worker. One option is to administer an efficient and valid screener for emotional distress, such as the Refugee Health Screener - 15, in the context of the overall health screening.\n\nMethods of treatment for refugees with mental health issues must also be culturally congruent. Western psychiatric methods may not applicable to individuals who do not conceive of the body and mind in the same way as people in the United States. For example, studies of Tibetan refugees have shown how important the Tibetan religion of Buddhism is in helping the refugees cope with their situation. The religion provides them with an explanation for their situation and hope for a better future. In some cases, indigenous methods of coping and psychological therapy can be integrated with Western methods of therapy to provide a wide spectrum of mental help to refugees.\n\nAdditionally, refugee children face unique barriers to adequate psychological health support due to significant trauma during their vulnerable developmental years.\n\nEvery woman from every country experiences her own menstrual process. However, some countries are more adept than others at providing proper resources and accessibility for women to easily maintain good hygiene. Menstrual health requires constant and proper upkeep in order to avoid subsequent infections. Menstruation requires attentive care and proper hygienic supplies. Thus, it is no wonder that while in the high income countries, menstrual health is not a major public health concern, but in developing countries or in times of crises, menstruation can pose a distinct problem for women in vulnerable populations.\n\nProper menstrual care includes washing oneself with soap on a daily basis, and changing menstrual supplies (such as pads or tampons) multiple times per day. Improper care can cause progressive infections, such as bacterial vaginosis (BV) or reproductive tract infection (RTI). With limited access to clean, running water and hygienic supplies (soap, pads, tampons) within refugee camps, monthly periods create health problems for women and girls.\n\nAs such, studies have been conducted in various refugee camps to assess the degree of burden that menstruation has on women. Refugees staying in temporary settlements in Myanmar reported poor latrine conditions, describing them as unsafe and dirty, with locks on the doors being a rare occurrence. Additionally, many young girls reported dark, unlit paths at nighttime causing unwarranted assaults by intruders in the camps. Thus, girls reportedly would not use the bathroom once it was dark outside, even if in need of a shower or a fresh pad.\n\nAnother obstacle that refugee women face in maintaining their menstrual health is limited to no access to an adequate amount of sanitary supplies. Many refugees do not have the luxury of changing their pads every few hours per day, so a buildup of bacteria is common. Other studies have revealed that when desperate, women will resort to using leaves or old pads to absorb the discharged blood, according to a report by Sommer's team in the journal Conflict and Health.\n\nIn addition to limited supplies and sanitary facilities, cultural attitudes towards menstruation create a difficult, taboo environment surrounding the topic. Thus, women and girls may feel too uncomfortable to seek help or advice on tending to their personal needs.\n\nDemand for labor is an important reason for migration. Despite the difficulty in researching immigrant populations, there is evidence that occupational health is an area in which immigrants face disparities. Many migrant or foreign-born workers fill low-wage, temporary or seasonal work in industries and jobs that may pose greater risks for worker health and safety such as agriculture, construction and services. In the United States, agriculture sector occupational risks such as asthma are more likely to affect immigrant workers. Overall, immigrants have higher rates of occupational morbidity and mortality than those who are native born, including higher rates of fatal and non-fatal injury. Evidence from Southern Europe points to higher rates of occupational risks such as working many hours per day and extreme temperatures and greater exposure to poor employment conditions and job precariousness. Health prevention and training programs related to occupational safety and health may not reach immigrants due to language, cultural and/or economic barriers. However, interventions tailored to their needs have been shown to be effective. Developing partnerships with institutions in the immigrant communities is one way of improving access to information and resources to immigrant workers. Improving work conditions can also improve other aspects of immigrant health however the work is often underutilized in efforts to promote migrant health. An emerging occupational health issue for immigrants relates to the health risks faced by people who are trafficked into situations of forced labor and debt bondage.\n\nHealth literacy is a crucial component to preventative healthcare and improved public health. A cross-sectional study conducted amongst refugees in Sweden, found that 60% of those assessed had inadequate functioning health literacy and 27% of them had inadequate comprehensive health literacy. The study concluded that health literacy should be taken into consideration when assessing refugee health and that more research is needed to assess the current dynamics and develop strategies to overcome the gaps in health literacy amongst refugees. Through the provision of targeted, adequate health literacy tool kits, populations are more likely to adhere to treatment plans and prevention efforts—particularly in the realm of infectious disease. These health literacy tools must be relevant to the communities, administered in familiar language and vocabulary, and must truly take into account the competencies and limitations of the target audience.Within health literacy initiatives, collaborative learning and social support could contribute to people's understanding and ability to judge, sift and use health information. Consequently, adding these practices to the definition of critical health literacy could prove to be hugely beneficial to patient communities.\n\nWhen addressing the needs of NCD patients within humanitarian crises, there needs to be a more epidemiological approach to assessing prevalence of NCDs to ensure a better understanding of the local needs and risks. After such assessment is made, those new understandings must be targeted to create novel, innovative approaches to mitigate risks and promote healthy behaviors—in an infectious manner. Finally, in order to adequately provide such resources, there must be strong guidance and education continuously available.\n\nRefugees arrive in their new countries with a variety of immunization needs. While refugees may have had vaccinations in their country of origin, often they lack documentation because they were forced to depart their home country in haste. Some may have received immunizations as part of their overseas exam, and some may have received no immunizations. Recommendations by the World Health Organization's (WHO) Expanded Program on Immunizations (EPI) are generally followed by countries worldwide with minor variations in vaccine schedules, spacing of vaccine doses, and documentation. The majority of vaccines used worldwide are from reliable local or international manufacturers, and no potency problems have been detected, with the occasional exception of tetanus toxoid and the oral polio vaccine (OPV).\n\nIn the United States, entering refugees are not required to have vaccinations. However, it is mandated that at the time of applying for adjustment of status from legal temporary resident to legal permanent resident, a refugee must be fully vaccinated in accordance with recommendations of the Advisory Committee on Immunization Practices (ACIP). A list of required vaccines in the US can be found on the vaccine schedule page.\n\nSocial support can be very helpful in preventing mental health issues and for coping with living in a new land, so refugees from the same areas should be able to live close to each other. However, even in this case, it may be necessary for social support to be offered by statutory or voluntary agencies from outside the refugees' and asylum seekers' communities in line with local informal and formal structures and networks. One model for such support was proposed by British authors in 2014, the WAMBA process, in which five essential components of support for refugees and asylum seekers were identified:\n\n\n\n"}
{"id": "180162", "url": "https://en.wikipedia.org/wiki?curid=180162", "title": "Religion and abortion", "text": "Religion and abortion\n\nMany religious traditions have taken a stance on abortion, and these stances span a broad spectrum, as highlighted below.\n\nAbortion, for the purpose of eliminating an unwanted child, and permanent sterilization are generally forbidden to Bahá'ís unless there is some medical reason for it. At present, Bahá'ís are encouraged to decide based on their own conscience in light of general guidance found in Bahá'í writings.\n\nThere is no single Buddhist view concerning abortion. Some traditional sources, including some Buddhist monastic codes, hold that life begins at conception, and that abortion, which would then involve the deliberate destruction of life, should be rejected. Complicating the issue is the Buddhist belief that \"life is a continuum with no discernible starting point\". Among Buddhists, there is no official or preferred viewpoint regarding abortion.\n\nThe Dalai Lama has said that abortion is \"negative\", but there are exceptions. He said, \"I think abortion should be approved or disapproved according to each circumstance.\"\n\nInducing or otherwise causing an abortion is regarded as a serious matter in the monastic rules followed by both Theravada and Vajrayana monks; monks and nuns must be expelled for assisting a woman in procuring an abortion. Traditional sources do not recognize a distinction between early- and late-term abortion, but in Sri Lanka and Thailand the \"moral stigma\" associated with an abortion grows with the development of the foetus. While traditional sources do not seem to be aware of the possibility of abortion as relevant to the health of the mother, modern Buddhist teachers from many traditions – and abortion laws in many Buddhist countries – recognize a threat to the life or physical health of the mother as an acceptable justification for abortion as a practical matter, though it may still be seen as a deed with negative moral or karmic consequences.\n\nThere is scholarly disagreement on how early Christians felt about abortion, and no explicit prohibition of abortion in either the \"Old Testament\" or \"New Testament\" books of the Christian Bible. Some scholars have concluded that early Christians took a nuanced stance on what is now called abortion, and that at different times, and in separate places, early Christians have taken different stances. Other scholars have concluded that early Christians considered abortion a sin at all stages; although there is disagreement over their thoughts on what type of sin it was and how grave a sin it was held to be, it was seen as at least as grave as sexual immorality. Some early Christians believed that the embryo did not have a soul from conception, and consequently, opinion was divided as to whether or not early abortion was murder or ethically equivalent to murder.\n\nEarly church councils punished women for abortions that were combined with other sexual crimes, as well as makers of abortifacient drugs, but, like some early Church Fathers such as Basil of Caesarea, did not make distinction between \"formed\" and \"unformed\" foetuses. While Gregory of Nyssa and Maximus the Confessor held that human life already began at conception, Augustine of Hippo affirmed Aristotle's concepts of ensoulment occurring some time after conception, after which point abortion was to be considered homicide, while still maintaining the condemnation of abortion at any time from conception onward. Aquinas reiterated Aristotle's views of successive souls: vegetative, animal, and rational. This would be the Catholic Church's position until 1869, when the limitation of automatic excommunication to abortion of a \"formed\" foetus was removed, a change that has been interpreted as an implicit declaration that conception was the moment of ensoulment. Most early penitentials imposed equal penances for abortion whether early-term or late-term, but later penitentials in the Middle Ages normally distinguished between the two, imposing heavier penances for late-term abortions and a less severe penance was imposed for the sin of abortion <nowiki>\"before [the foetus] has life\".</nowiki>\n\nContemporary Christian denominations have nuanced positions, thoughts, and teachings about abortion, especially in extenuating circumstances. The Catholic Church, the Eastern Orthodox Church Oriental Orthodoxy, and most evangelical Protestants oppose deliberate abortion as immoral, while allowing what is sometimes called indirect abortion, namely, an action that does not seek the death of the foetus as an end or a means, but that is followed by the death as a side effect. Some mainline Protestant denominations such as the Methodist Church, United Church of Christ, Presbyterian Church (USA), and the Evangelical Lutheran Church of America, among others, are more permissive of abortion. More generally, some Christian denominations can be considered pro-life, while others may be considered pro-choice. Additionally, there are sizable minorities in some denominations that disagree with their denomination's stance on abortion.\n\nClassical Hindu texts strongly condemn abortion. The British Broadcasting Corporation writes, \"When considering abortion, the Hindu way is to choose the action that will do least harm to all involved: the mother and father, the foetus and society.\" The BBC goes on to state, \"In practice, however, abortion is practiced in Hindu culture in India, because the religious ban on abortion is sometimes overruled by the cultural preference for sons. This can lead to abortion to prevent the birth of girl babies, which is called 'female foeticide'.\" Hindu scholars and women's rights advocates have supported bans on sex-selective abortions. Some Hindus support abortion in cases where the mother's life is at imminent risk or when the foetus has a life-threatening developmental anomaly.\n\nSome Hindu theologians and Brahma Kumaris believe personhood begins at three months and develops through to five months of gestation, possibly implying permitting abortion up to the third month and considering any abortion past the third month to be destruction of the soul's current incarnate body.\n\nAlthough there are different opinions among Islamic scholars about when life begins, and when abortion is permissible, most agree that the termination of a pregnancy after 120 days - the point at which, in Islam, a foetus is thought to become a living soul - is not permissible. Several Islamic thinkers contend that in cases prior to four months of gestation, abortion should be permissible only in instances in which the mother's life is in danger or in cases of rape.\n\nSome schools of Muslim law permit abortion in the first sixteen weeks of pregnancy, whereas others only allow it in the first seven weeks of pregnancy. The further along the pregnancy has progressed, the greater the wrong. The Qur'an does not specifically mention abortion, but it skirts the issue by condemning intentional murder. All schools accept abortion as a means to save the mother's life.\n\nOrthodox Jewish teaching allows abortion if necessary to safeguard the life of the pregnant woman. While the Reform, Reconstructionist, and Conservative movements openly advocate for the right to a safe and accessible abortion, the Orthodox movement is less unified on the issue. Many Orthodox Jews oppose abortion, except when it is necessary to save a woman's life (or, according to some, the woman's health).\n\nIn Judaism, views on abortion draw primarily upon the legal and ethical teachings of the Hebrew Bible, the Talmud, the case-by-case decisions of responsa, and other rabbinic literature. Generally speaking, Orthodox Jews oppose abortion after the 40th day, with health-related exceptions, and reform Jews tend to allow greater latitude for abortion. There are rulings that often appear conflicting on the matter. The Talmud states that a foetus is not legally a person until it is delivered. The Torah contains the law that, \"When men fight, and one of them pushes a pregnant woman, and a miscarriage results, but no other misfortune, the one responsible shall be fined...but if other misfortune ensues, the penalty shall be life (nefesh) for life (nefesh).\" (). That is, causing a woman to miscarry is a crime, but not a capital crime, because the foetus is not considered a person.\n\nJeremiah 1:5 states that, \"Before I formed you in the womb, I knew you, before you were born, I set you apart; I appointed you as a prophet to the nations.\" For some, this verse, while talking specifically about Jeremiah, is an indication that God is aware of the identity of \"developing unborn human beings even before they enter the womb\", or that for everyone, God has a plan that abortion might be seen as frustrating. Others say that this interpretation is incorrect, and that the verse is not related to personhood or abortion, as Jeremiah is asserting his prophetic status as distinct and special.\n\nThe Hebrew Bible has a few references to abortion; Exodus 21:22-25 addresses miscarriage by way of another's actions, which it describes as a non-capital offense punishable through a fine. The Book of Numbers in the Hebrew Bible describes the Ordeal of the bitter water (\"sotah\") to be administered by a priest to a wife whose husband thinks she was unfaithful. Some scholars interpret the text as involving an abortifacient potion or otherwise that induces a miscarriage if the woman is pregnant with another man's child. Rabbinical scholar Arnold Ehrlich interprets the ordeal such that it ends either harmlessly if the woman is faithful, or with an induced abortion: \"the embryo falls\".\n\nAlthough the Sikh code of conduct does not deal directly with abortion (or indeed many other bioethical issues), it is generally forbidden in Sikhism because it is said to interfere with the creative work of God. Despite this theoretical viewpoint, abortion is not uncommon among the Sikh community in India, and there is growing concern that female foetuses are being aborted because of the cultural preference for sons.\n\nThe Unitarian Universalist Church strongly supports abortion rights. In 1978, the Unitarian Universalist Association passed a resolution that declared, \"...<nowiki>[the]</nowiki> right to choice on contraception and abortion are important aspects of the right of privacy, respect for human life, and freedom of conscience of women and their families\". The Association had released earlier statements in 1963 and 1968 favoring the reform of restrictive abortion laws.\n\nAlthough views differ, most Wiccans consider abortion to be a spiritual decision that should be free from interference by the state or politicians.\n\n"}
{"id": "34157464", "url": "https://en.wikipedia.org/wiki?curid=34157464", "title": "Religion and health", "text": "Religion and health\n\nScholarly studies have investigated the effects of religion on health. The World Health Organization (WHO) discerns four dimensions of health, namely physical, social, mental, and spirirtual health. Having a religious belief may have both positive and negative impacts on health and morbidity.\n\nSpirituality has been ascribed many different definitions in different contexts, but a general definition is: an individual’s search for meaning and purpose in life. Spirituality is distinct from organized religion in that spirituality does not necessarily need a religious framework. That is, one does not necessarily need to follow certain rules, guidelines or practices to be spiritual, but an organized religion often has some combination of these in place. Some people who suffer from severe mental disorders may find comfort in religion. People who report themselves to be spiritual people may not observe any specific religious practices or traditions.\n\nMore than 3000 empirical studies have examined relationships between religion and health, including more than 1200 in the 20th century, and more than 2000 additional studies between 2000 and 2009.\nVarious other reviews of the religion/spirituality and health literature have been published. These include two reviews from an NIH-organized expert panel that appeared in a 4-article special section of \"American Psychologist\". Several chapters in edited academic books have also reviewed the empirical literature.\n\nThe World Health Organization (WHO) discerns four dimensions of health, namely physical, social, mental, and spirirtual health.\n\nAccording to Ellison & Levin (1998), some studies indicate that religiosity appears to positively correlate with physical health. For instance, mortality rates are lower among people who frequently attend religious events and consider themselves both religious and spiritual. Accoridng to Seybold & Hill (2001), almost all studies involved in the effect of religion on a person's physical health have revealed it has a positive attribution to their lifestyle. These studies have been carried out among all ages,genders and religions. These are based on the experience of religion is positive in itself.\n\nOne possibility is that religion provides physical health benefits indirectly. Church attendees present with lower rates of alcohol consumption and improvement in mood, which is associated with better physical health. Kenneth Pargament is a major contributor to the theory of how individuals may use religion as a resource in coping with stress, His work seems to show the influence of attribution theory. Additional evidence suggests that this relationship between religion and physical health may be causal. Religion may reduce likelihood of certain diseases. Studies suggest that it guards against cardiovascular disease by reducing blood pressure, and also improves immune system functioning.\nSimilar studies have been done investigating religious emotions and health. Although religious emotions, such as humility, forgiveness, and gratitude confer health benefits, it is unclear if religious people cultivate and experience those emotions more frequently than non-religious peoples.\n\nChurch attendance has been found to increase life expectancy (Hummer et al. 1999) with a life expectancy at age 20 of 83 years for frequent attendees and 75 years for non-attendees. The finding, however, does not prove that religion in itself increases life expectancy.\n\nKark et. (1996) included almost 4,000 Israelis, over 16 years (beginning in 1970), death rates were compared between the experimental group (people belonging to 11 religious kibbutzim) versus the control group (people belonging to secular kibbutzim). Some determining factors for the groups included the date the kibbutz was created, geography of the different groups, and the similarity in age. It was determined that “belonging to a religious collective was associated with a strong protective effect\". Not only do religious people tend to exhibit healthier lifestyles, they also have a strong support system that secular people would not normally have. A religious community can provide support especially through a stressful life event such as the death of a loved one or illness. There is the belief that a higher power will provide healing and strength through the rough times which also can explain the lower mortality rate of religious people vs. secular people.\n\nThe existence of ‘religious struggle’ in elderly patients was predictive of greater risk of mortality in a study by Pargament et al. (2001). Results indicate that patients, with a previously sound religious life, experienced a 19% to 28% greater mortality due to the belief that God was supposedly punishing them or abandoning them.\n\nA number of religious practices have been reported to cause infections. These happened during an ultra-orthodox Jewish circumcisions practice known as metzitzah b'peh, the ritual 'side roll' in Hinduism, the Christian communion chalice, during the Islamic Hajj and after the Muslim ritual ablution.\n\nSome religions claim that praying for somebody who is sick can have positive effects on the health of the person being prayed for. Meta-studies of the literature in the field have been performed showing evidence only for no effect or a potentially small effect. For instance, a 2006 meta analysis on 14 studies concluded that there is \"no discernible effect\" while a 2007 systemic review of intercessory prayer reported inconclusive results, noting that 7 of 17 studies had \"small, but significant, effect sizes\" but the review noted that the most methodologically rigorous studies failed to produce significant findings.\n\nRandomized controlled trials of intercessory prayer have not yielded significant effects on health. These trials have compared personal, focused, committed and organized intercessory prayer with those interceding holding some belief that they are praying to God or a god versus any other intervention. A Cochrane collaboration review of these trials concluded that 1) results were equivocal, 2) evidence does not support a recommendation either in favor or against the use of intercessory prayer and 3) any resources available for future trials should be used to investigate other questions in health research.\nIn a case-control study done following 5,286 Californians over a 28-year period in which variables were controlled for (i.e. age, race/ethnicity, gender, education level), participants who went to church on a frequent basis (defined as attending a religious service once a week or more) were 36% less likely to die during that period. However, this can be partly be attributed to a better lifestyle since religious people tend to drink and smoke less and eat a healthier diet.\n\nEvidence suggests that religiosity can be a pathway to both mental health and mental disorder. For example, religiosity is positively associated with mental disorders that involve an excessive amount of self-control and negatively associated with mental disorders that involve a lack of self-control. Other studies have found indications of mental health among both the religious and the secular. For instance, Vilchinsky & Kravetz found negative correlations with psychological distress among religious and secular subgroups of Jewish students. In addition, intrinsic religiosity has been inversely related to depression in the elderly, while extrinsic religiosity has no relation or even a slight positive relation to depression. Religiosity has been found to mitigate the negative impact of injustice and income inequality on life satisfaction.\n\nThe link between religion and mental health may be due to the guiding framework or social support that it offers to individuals. By these routes, religion has the potential to offer security and significance in life, as well as valuable human relationships, to foster mental health. Some theorists have suggested that the benefits of religion and religiosity are accounted for by the social support afforded by membership in a religious group.\n\nReligion may also provide coping skills to deal with stressors, or demands perceived as straining. Pargament’s three primary styles of religious coping are 1) self-directing, characterized by self-reliance and acknowledgement of God, 2) deferring, in which a person passively attributes responsibility to God, and 3) collaborative, which involves an active partnership between the individual and God and is most commonly associated with positive adjustment. This model of religious coping has been criticized for its over-simplicity and failure to take into account other factors, such as level of religiosity, specific religion, and type of stressor. Additional work by Pargament involves a detailed delineation of positive and negative forms of religious coping, captured in the BREIF-RCOPE questionnaire which have been linked to a range of positive and negative psychological outcomes.\n\nStudies have shown a negative relationship between spiritual well-being and depressive symptoms. In one study, those who were assessed to have a higher spiritual quality of life on a spiritual well-being scale had less depressive symptoms. Cancer and AIDS patients who were more spiritual had lower depressive symptoms than religious patients. Spirituality shows beneficial effects possibly because it speaks to one’s ability to intrinsically find meaning in life, strength, and inner peace, which is especially important for very ill patients.\n\nExline et al. 1999 showed that the difficulty in forgiving God and alienation from God were associated with higher levels of depression and anxiety. Among those who currently believed in God, forgiving God for a specific, unfortunate incident predicted lower levels of anxious and depressed mood.\n\nStudies have reported beneficial effects of spirituality on the lives of patients with schizophrenia, major depression, and other psychotic disorders. Schizophrenic patients were less likely to be re-hospitalized if families encouraged religious practice, and in depressed patients who underwent religiously based interventions, their symptoms improved faster than those who underwent secular interventions. Furthermore, a few cross-sectional studies have shown that more religiously involved people had less instance of psychosis.\n\nResearch shows that religiosity moderates the relationship between “thinking about meaning of life” and life satisfaction. For individuals scoring low and moderately on religiosity, thinking about the meaning of life is negatively correlated with life satisfaction. For people scoring highly on religiosity, however, this relationship is positive. Religiosity has also been found to moderate the relationship between negative affect and life satisfaction, such that life satisfaction is less strongly influenced by the frequency of negative emotions in more religious (vs less religious) individuals.\n\nOne of the most common ways that people cope with trauma is through the comfort found in religious or spiritual practices. Psychologists of religion have performed multiple studies to measure the positive and negative effects of this coping style. Leading researchers have split religious coping into two categories: positive religious coping and negative religious coping. Individuals who use positive religious coping are likely to seek spiritual support and look for meaning in a traumatic situation. Negative religious coping (or spiritual struggles) expresses conflict, question, and doubt regarding issues of God and faith.\n\nThe effects of religious coping are measured in many different circumstances, each with different outcomes. Some common experiences where people use religious coping are fear-inflicting events such as 9/11 or the holocaust, death and sickness, and near death experiences. Research also shows that people also use religious coping to deal with everyday stressors in addition to life-changing traumas. The underlying assumption of the ability of religion to influence the coping process lies in the hypothesis that religion is more than a defence mechanism as it was viewed by Sigmund Freud. Rather than inspiring denial, religion stimulates reinterpretations of negative events through the sacred lens.\n\nSpiritual Health is one of four dimensions to well-being as defined by the World Health Organization (WHO), which include physical, social, and mental.\n\nThe preamble to Constitution of the World Health Organization (WHO) adopted by the International Health Conference held in New York from 19 June to 22 July 1946 and signed on 22 July 1946 by the representatives of 61 States defined health as a state of \"physical, mental and social well-being and not merely the absence of disease or infirmity\" and it has not been amended.\n\nHowever, in 1983 twenty-two WHO member countries from the Eastern Mediterranean Region proposed a draft resolution to this preamble to include reference to spiritual health, such that it would redefine health as a state of \"physical, mental, spiritual and social well-being and not merely the absence of disease or infirmity\".\n\nWhilst WHO did not amend the preamble to its constitution, resolution WHA31.13 passed by the Thirty-seventh World Health Assembly, in 1984 called upon Member States to consider including in their Health For All strategies a spiritual dimension as defined in that resolution in accordance with their own social and cultural patterns\n\nThe complete description of the spiritual dimension as articulated by the Health Assembly is as follows:\nThe spiritual dimension is understood to imply a phenomenon that is not material in nature, but belongs to the realm of ideas, beliefs, values and ethics that have arisen in the minds and conscience of human beings, particularly ennobling ideas. Ennobling ideas have given rise to health ideals, which have led to a practical strategy for Health for All that aims at attaining a goal that has both a material and non-material component. If the material component of the strategy can be provided to people, the non-material or spiritual one is something that has to arise within people and communities in keeping with their social and cultural patterns. The spiritual dimension plays a great role in motivating people’s achievement in all aspects of life.\n\nSince the inclusion of spiritual health within WHO's purview, a number of other significant organizations have also attended to spirituality and incorporated reference to it in key documents, including the United Nations action plan Agenda 21 which recognizes the right of individuals to \"healthy physical, mental, and spiritual development\".\n\n\n\n"}
{"id": "42079498", "url": "https://en.wikipedia.org/wiki?curid=42079498", "title": "Respectful workplace", "text": "Respectful workplace\n\nA respectful workplace is a safe place of employment where employees are valued, recognised, treated fairly have clear expectations, and work harmoniously.\nBenefits of a respectful workplace include better moral, teamwork, lower absenteeism, lower turnover of staff, reduced worker's compensation claims,\nbetter ability to handle change and recover from problems, work seems less onerous, and improved productivity.\nPositively viewed teams will retain and employ better staff.\n\nLack of respect and what is sometimes called \"incivility\"—low level negative behaviours (such as rudeness, discourteousness, not acknowledging other staff)—can create a dysfunctional team environment, relationship breakdown, decline in productivity, and the risk of psychological injury.\nManagers that want to encourage a respectful workplace must model the appropriate example.\nThey should talk about what behaviours are encouraged.\nThe managers must be willing to talk about problem behaviours.\nThere should be safe ways to report problems, which could be anonymous, or independent people such as an ombudsman. Measures of the culture could include competitiveness, formality, respect, hospitality and supportiveness.\n\nRespect can be included in performance appraisals, with feedback given in a formal process. Disrespectful behaviour must not be ignored but be named and its impact brought to the attention of the responsible person. By ignoring problematic behaviour, others will perceive it as condoned.\n\nThe Compassionate organisation will have strategies that deal with problems affecting the well being of employees such as redundancies, disasters, workplace conflict. There would be strategies that recover from distress.\n"}
{"id": "30324643", "url": "https://en.wikipedia.org/wiki?curid=30324643", "title": "Retman", "text": "Retman\n\nRETMAN is a comics’ character, associated to Rational Emotive Behavior Therapy (REBT). Through it, REBT tries more efficiently address children, adolescents and the general public. This form of therapy approaches the treatment of emotional disorders and the promotion of mental health by modifying maladaptive/irrational behaviors and cognitions (thoughts).\n\nThe first RETMAN concept was thought up at the Albert Ellis Institute, US, in the 80s (Calvin Merrifield and Rebecca Merriefield: Call Me Retman and Have a Ball, 1979) inspired by the name Rational Emotive Therapy (that is how this form of cognitive-behavioral psychotherapy was referred to then). The prototype for the superhero was Albert Ellis himself. As “RET-Man”, Ellis is arrested by the police but not before helping a depressed jilted lover give up his thoughts of suicide in favor of more rational thinking. \nHowever, because of the high production costs at the time, the project was abandoned after the publication of a comic.\n\nThe concept was resumed in 2008, with the approval of the Albert Ellis Institute by a team of Romanian psychologists and graphic designers led by Prof. Daniel David, with a modern technology. For the first time they also investigated the efficiency of the concept in promoting psychological health and in the treatment of psychological disorders in children and adolescents (see \"Meet RETMAN\" ). The team developed the RETMAN concept in an innovative way, creating an entire system that includes: (1) therapeutic comics; (2) therapeutic stories; (3) therapeutic cartoons; and (4) the therapeutic system RoboRETMAN.\n\nThe Retman technology is being used by (selection) The Child and Adolescent Psychiatric Clinic, UMF, Cluj-Napoca, the “PsyTech” University Psychological Clinic at Babes-Bolyai, Cluj-Napoca, kindergartens and private clinical psychology/counseling/psychotherapy practices in Romania.\nIt is also used on the REThink Project, a research grant with the goal to design, develop, and evaluate a therapeutic video game accessible over the Internet, meant to be used primarily as a standalone application to promote emotional resilience in children and adolescents.\n\nThe concept was resumed by a team of designers and psychologists at the International Institute for the Advanced Studies of Psychotherapy and Applied Mental Health (link), led by “Aaron T.Beck” professor Daniel David. The institute was co-founded by the Albert Ellis Institute and Babes-Bolyai University. The concept put forward by this team was a completely changed one, using the character Superman as a model for Retman. A planet was created for Retman (Rationalia), as well as new adventures and a dangerous opponent (the evil sorcerer Irationalius)\nThe stories of Retman are based on an original story, developed by dr. Daniel David: “...Retman was the ruler of Planet Rationalia from the System of Ataraxia in the Happiness Galaxy. He lived in a town called Equilibrium and was a Great Sorcerer Psychotherapist, who is a person who helps you when you suffer and teaches you how to be happy. The Great Sorcerer Psychotherapist practiced “Retmagic” (a rational-emotional therapeutic magic). This gift had been given to him by the Almighty Creator of the World. All the inhabitants of the planet were happy because they had everything they wanted, but mostly because they wanted only what they knew they deserved. The secret of their happiness was therefore their minds and their way of thinking. But while Retman fostered rational thinking on the planet, his enemy, the Sorcerer Irationalius became envious of his power and stole the minds of the inhabitants of the planet. They became mean and envious and the whole planet was overtaken by evil and despair. Then the Almighty Creator of the World, in anger, destroyed everyone, sparing just Retman, as he was the only person whose mind Irationalius could not steal. He was sent on Earth with the goal of teaching everyone here to be rational and happy, so they wouldn’t end up like the inhabitants of Rationalia...”\nThe stories of Retman have been published in Romanian and English.\n"}
{"id": "11724224", "url": "https://en.wikipedia.org/wiki?curid=11724224", "title": "School Health Education Study", "text": "School Health Education Study\n\nThe School Health Education Study (SHES) was a crucial event in transforming health education as practiced in American public schools. It has been called, \"the most significant school health education initiative of the 1960s\" and was largely responsible for establishing the value of comprehensive health education rather than separate disease-specific units and in introducing the concept-based approach to education in general. Most health curricula developed since have followed the model set by the SHES in its School Health Curriculum Project.\n\nIn 1960 millionaire distiller and philanthropist Samuel Bronfman asked Dr. Granville Larimore, then Deputy Commissioner of the New York State Department of Health and a member of the Joint Committee on Health Problems in Education of the American Medical Association (AMA) and the National Education Association (NEA), to suggest several projects in health or education that should receive funding but were being neglected by governmental and private funders. Dr. Larimore suggested three priorities: (I) graduate medical education, (2) effectiveness of the mass media for health education, and (3) school health education. After hearing presentations on each of these three priorities, the Samuel Bronfman Foundation’s board decided to provide $200,000 for a study of the status of health education in the nation’s schools.\n\nThe Study was envisioned as an independent, two-year-long investigation, affiliated with the American Association for Health, Physical Education and Recreation (AAHPER) and the National Education Association. Bronfman sought the advice of Delbert Oberteuffer, professor at the Ohio State University and widely regarded as the leading figure in health education at that time, regarding who could best lead the study. Oberteuffer recommended one of his young OSU colleagues, Elena Sliepcevich. Dr. Sliepcevich accepted the appointment and moved to Washington, DC where the SHES leased office space on Dupont Circle in the building next door to the NEA.\n\nDuring its first year, the Study assessed the state of health education offerings in a total of 135 school systems covering 38 states and involving some 1101 individual elementary schools and 359 secondary schools. This survey remains the broadest of its type ever completed in the United States. In the second year test instruments were administered to students in grades 6, 9, and 12 of the participating schools. Of 17,634 usable answer sheets re¬turned to the researchers, a weighted sample of 2000 scores for each of the three grade levels representative of the makeup of the school sample was selected for analysis. Analysis of the results required a third year of Bronfman Foundation support and led to the conclusion that the state of health education in the nation’s public schools was \"appalling\".\n\nThe [3M] Corporation funded SHES for a further six years (1963–1969) to develop a model curriculum—the School Health Curriculum Project or SHCP. Ann E. Nolte, of Ohio State University, joined SHES as associate director of the study and a curriculum writing team was assembled, consisting of: William H. Creswell, Jr., professor of health education at the University of Illinois; Gus T. Dalis, of the Los Angeles County Schools; Edward B. Johns, professor of school health education at the University of California, Los Angeles; Marion B. Pollock, assistant professor of health education at California State College, Long Beach; Richard K. Means, professor of health education at Auburn University; and Robert D. Russell, associate professor of health education at Southern Illinois University.\n\nProf. Russell proposed as the initial point of view for the SHCP that health was a unified concept of well-being. This was expressed in the curriculum as follows, \"Health is a quality of life involving dynamic interaction and interdependence among the individual's physical well-being, his (or her) mental and emotional reactions, and the social complex in which he (or she) exists\". From this starting point, the SHCP writers identified ten key concepts. Sub concepts were then developed in the physical, mental, and social dimen¬sions for each of the ten concepts. The 31 sub concepts were each linked to behavioral objectives written at four progressive levels—grades K-3, 4-6, 7-9, and 10-12—in the cognitive, affective, and behavioral domains.\n\nThe ten concepts developed by the SHES as the basis for the SHCP were:\n\n"}
{"id": "1207316", "url": "https://en.wikipedia.org/wiki?curid=1207316", "title": "Sexual medicine", "text": "Sexual medicine\n\nSexual medicine is a medical specialty that deals with sexual health. At times heavily influenced by current local views on morality, with heavy cultural overlay, in broad terms this specialty is concerned with diagnosing, assessing and treating all aspects which relate to sexuality. \n\nIssues can be divided into two main areas of concern: \n\n\nSexual medicine has four dimensions: \n\nCongenital or acquired, these conditions refer to any pathology which interferes with the perception of satisfactory sexual health. Varied conditions include absent sexual organs, hermaphrodite and other genetic malformations, or trauma such as amputation or lacerations. Sexually transmitted disease accounts for by far the largest proportion of patients in this category. Of these, HIV and consequently AIDS represents a significant threat to populations throughout the world, but more especially Africa, and within Africa in the sub-Saharan area.\n\nA wide range of disorders can be mentioned in this section. While those unaffected can – and often do – mock the afflicted, such issues can be earth shattering in their consequences for the individual, resulting in depression, murder and suicide. Whole cultures have been adversely affected by adhering to specific attitudes regarding sexuality. Issues such as genital mutilation (e.g. circumcision), institutionalised rape, and honour killings can be attributed to such problems which, although accepted as \"the norm\" for a specific culture, would in other circumstances be regarded as deviant behaviour. \n\nExamples of conditions which may be treated by specialists in this field include, but are not limited to:\n\n\nThe anamnesis or medical history taking of issues related to sexual or reproductive medicine may be inhibited by a reluctance of the patient to disclose intimate or uncomfortable information. Even if such an issue is on the patient's mind, he or she often doesn't start talking about such an issue without the physician initiating the subject by a specific question about sexual or reproductive health. Some familiarity with the doctor generally makes it easier for patients to talk about intimate issues such as sexual subjects, but for some patients, a very high degree of familiarity may make the patient reluctant to reveal such intimate issues. When visiting a health provider about sexual issues, having both partners of a couple present is often necessary, and is typically a good thing, but may also prevent the disclosure of certain subjects, and, according to one report, increases the stress level.\n\n"}
{"id": "20777623", "url": "https://en.wikipedia.org/wiki?curid=20777623", "title": "Shoulder presentation", "text": "Shoulder presentation\n\nA shoulder presentation refers to a malpresentation at childbirth where the baby is in a transverse lie (its vertebral column is perpendicular to that of the mother), thus the leading part (the part that enters first the birth canal) is an arm, shoulder, or the trunk. While a baby can be delivered vaginally when either the head or the feet/buttocks are the leading part, it usually cannot be expected to be delivered successfully with a shoulder presentation unless a cesarean section (C/S) is performed.\n\nShoulder presentations are uncommon (about 0.5% of births) as usually towards the end of gestation either the head or the buttocks start to enter the upper part of the pelvis anchoring the fetus in a longitudinal lie. It is not known in all cases of shoulder presentation why the longitudinal lie is not reached, but possible causes include bony abnormalities of the pelvis, uterine abnormalities such as malformations or tumors (fibroids), and other tumors in the pelvis or abdomen can also lead to a shoulder presentation. Other factors are a lax abdominal musculature, uterine overdistension (i.e. polyhydramnios), multiple gestation, placenta previa, a small fetus, or a fetus with some abnormality. Further, if the amniotic fluid sac ruptures the shoulder or arm may become wedged as a shoulder presentation.\n\nInspection of the abdomen may already give a clue as it is wide from side to side. Usually performing the Leopold's maneuvers will demonstrate the transverse lie of the fetus. Ultrasound examination delivers the diagnosis and may indicate possible causes such as multiple gestation or a tumor. On vaginal examination, the absence of a head or feet/breech is apparent.\n\nShoulder presentations are classified into four types, based on the location of the scapula:\n\nWhile a transverse lie prior to labor can be manually versed to a longitudinal lie, once the uterus starts contracting the uterus normally will not allow any version procedure. A shoulder presentation is an indication for a caesarean section. Generally, as it is diagnosed early, the baby is not damaged by the time of delivery. With the rupture of the membranes, there is an increased risk of a cord prolapse as the shoulder may not completely block the birth canal. Thus the caesarean section is ideally performed before the membranes break.\n\nThe delivery of the second twin in a transverse lie with a shoulder presentation represents a special situation that may be amenable to a vaginal delivery. As the first twin has just been delivered and the cervix is fully dilated the obstetrician may perform an internal version, that is inserting one hand into the uterus, find the baby’s feet, and then bring the baby into a breech position and deliver the baby as such.\n\nDuring labor the shoulder will be wedged into the pelvis and the head lie in one iliac fossa, the breech in the other. With further uterine contractions the baby suffocates. The uterus continues to try to expel the impacted fetus and as its retraction ring rises, the musculature in the lower segments thins out leading eventually to a uterine rupture and the death of the mother. Impacted shoulder presentations contribute to maternal mortality. Obviously a cesarean section should be performed before the baby has died, but even when the baby has died or impaction has occurred, C/S is the method of choice of delivery, as alternative methods of delivery are potentially too traumatic for the mother. If the baby is preterm or macerated and very small a spontaneous delivery has been observed.\n\nPrior to the arrival of C/S the fetus usually died during protracted labor and the mother's life was at risk as well due to infection, uterine rupture and bleeding. On occasion, if the baby was macerated and small, it collapsed sufficiently to be delivered. The shoulder presentation was a feared obstetrical complication.\n\nIn 1690 Justine Siegemundin, a German midwife, published \"Die Kgl. Preußische und Chur-Brandenburgische Hof-Wehemutter\". This treatise for midwives demonstrated abnormal presentations at birth and their management. She was the first to describe a two-handed method of performing an internal rotation of the baby to extract it as a breech (a variation of which is performed today on the second twin, see above) using a sling. The procedure was useful provided the fetus was not impacted. Once the uterus had contracted around the baby tightly, destructive interventions were used to save the life of the mother.\n\nShoulder presentation is a malpresentation that is not to be confused with shoulder dystocia, a complication during the birth of a baby in a vertex presentation.\n\n\n"}
{"id": "33026432", "url": "https://en.wikipedia.org/wiki?curid=33026432", "title": "Soul, Mind, Body Medicine", "text": "Soul, Mind, Body Medicine\n\nSoul, Mind, Body Medicine: A Complete Soul Healing System for Optimum Health and Vitality is a self-help book written by spiritual healer Zhi Gang Sha which provides a controversial interpretation of Traditional Chinese medicine and quantum physics. Published in 2006, within three weeks of its release the book was placed in the top five of \"The New York Times\" Best Seller list.\n\n"}
{"id": "10016622", "url": "https://en.wikipedia.org/wiki?curid=10016622", "title": "Spermarche", "text": "Spermarche\n\nSpermarche—also known as semenarche—is the beginning of development of sperm in boys' testicles at puberty. It is the counterpart of menarche in girls. Depending on their upbringing, cultural differences, and prior sexual knowledge, boys may have different reactions to spermarche, ranging from fear to excitement. Spermarche is one of the first events in the life of a male leading to sexual maturity. It occurs at the time when the secondary sexual characteristics are just beginning to develop. The age when spermarche occurs is not easy to determine. However, researchers have tried to determine the age in various populations by taking urine samples of boys and determining the presence of spermatozoa. The presence of sperm in urine is referred to as \"spermaturia\". From various sources, it appears that spermarche occurs between 13 and 15 years of age in most cases.\n\nIn one study, boys were asked the circumstances in which their first ejaculation occurred. Most commonly this occurred via a nocturnal emission, with a significant number experiencing semenarche via masturbation. Less commonly, the first ejaculation occurred during sexual intercourse with a partner.\n\n"}
{"id": "55703898", "url": "https://en.wikipedia.org/wiki?curid=55703898", "title": "Text nailing", "text": "Text nailing\n\nText Nailing (TN) is an information extraction method of semi-automatically extracting structured information from unstructured documents. TN was developed at Massachusetts General Hospital and was tested in multiple scenarios including the extraction of smoking status, family history of coronary artery disease, classify patients with sleep disorders, improve the accuracy of the Framingham risk score for patients with non-alcoholic fatty liver disease, and classify non-adherence to type-2 diabetes. A comprehensive review regarding extracting information from textual documents in the electronic health record is available..\n\nTN combines two concepts: 1) human-interaction with narrative text to identify highly prevalent non-negated expressions, and 2) conversion of all expressions and notes into non-negated alphabetical-only representations to create homogeneous representations. The importance of using non-negated expressions to achieve an increased accuracy of text-based classifiers was emphasized in a letter published in Communications of the ACM in October 2018.\n\nIn traditional machine learning approaches for text classification, a human expert is required to label phrases or entire notes, and then a supervised learning algorithm attempts to generalize the associations and apply them to new data. In contrast, using non-negated distinct expressions eliminates the need for an additional computational method to achieve generalizability.\n\nA sample code for extracting smoking status from narrative notes using \"nailed expressions\" is available in GitHub.\n\nIn July 2018 researchers from Virginia Tech and University of Illinois at Urbana-Champaign referred TN as an example for progressive cyber-human intelligence (PCHI).\n\nChen & Asch 2017 wrote \"With machine learning situated at the peak of inflated expectations, we can soften a subsequent crash into a “trough of disillusionment” by fostering a stronger appreciation of the technology’s capabilities and limitations.\"\n\nA letter published in Communications of the ACM, \"Beyond brute force\", emphasized that a brute force approach may perform better than traditional machine learning algorithms when applied to text. The letter stated \"... machine learning algorithms, when applied to text, rely on the assumption that any language includes an infinite number of possible expressions. In contrast, across a variety of medical conditions, we observed that clinicians tend to use the same expressions to describe patients' conditions.\"\n\nIn his viewpoint published in June 2018 concerning slow adoption of data-driven findings in medicine, Uri Kartoun, co-creator of Text Nailing states that \" ...Text Nailing raised skepticism in reviewers of medical informatics journals who claimed that it relies on simple tricks to simplify the text, and leans heavily on human annotation. TN indeed may seem just like a trick of the light at ﬁrst glance, but it is actually a fairly sophisticated method that ﬁnally caught the attention of more adventurous reviewers and editors who ultimately accepted it for publication.\"\n\nThe human in-the-loop process is a way to generate features using domain experts. Using domain experts to come up with features is not a novel concept. However, the specific interfaces and method which helps the domain experts create the features are most likely novel.\n\nIn this case the features the experts create are equivalent to regular expressions. Removing non-alphabetical characters and matching on \"smokesppd\" is equal to the regular expression /smokes[^a-zA-Z]*ppd/. Using regular expressions as features for text classification is not novel.\n\nGiven these features the classifier is a manually set threshold by the authors, decided by the performance on a set of documents. This is a classifier, it's just that the parameters of the classifier, in this case a threshold, is set manually. Given the same features and documents almost any machine learning algorithm should be able to find the same threshold or (more likely) a better one.\n\nThe authors note that using support vector machines (SVM) and hundreds of documents give inferior performance, but does not specify which features or documents the SVM was trained/tested on. A fair comparison would use the same features and document sets as those used by the manual threshold classifier.\n"}
{"id": "46764211", "url": "https://en.wikipedia.org/wiki?curid=46764211", "title": "Tubo-ovarian abscess", "text": "Tubo-ovarian abscess\n\nTubo-ovarian abscesses (TOA) are one of the late complications of pelvic inflammatory disease (PID) and can be life-threatening if the abscess ruptures and results in sepsis. It consists of an encapsulated or confined 'pocket of pus' with defined boundaries that forms during an infection of a fallopian tube and ovary. These abscesses are found most commonly in reproductive age women and typically result from upper genital tract infection. It is an inflammatory mass involving the fallopian tube, ovary and, occasionally, other adjacent pelvic organs. A TOA can also develop as a complication of a hysterectomy.\n\nPatients typically present with fever, elevated white blood cell count, lower abdominal-pelvic pain, and/or vaginal discharge. Fever and leukocytosis may be absent. TOAs are often polymicrobial with a high percentage of anaerobic bacteria. The cost of treatment in the United States is approximately $2,000 per patient, which equals about $1.5 billion annually. Though rare, TOA can occur without a preceding episode of PID or sexual activity.\n\nThe signs and symptoms of tubo-ovarian abscess (TOA) are the same as with pelvic inflammatory disease (PID) with the exception that the abscess can be found with magnetic resonance imaging (MRI), sonography and x-ray. It also differs from PID in that it can create symptoms of acute-onset pelvic pain. Typically this disease is found in sexually active women.\n\nThe development of TOA is thought to begin with the pathogens spreading from the cervix to the endometrium, through the salpinx, into the peritoneal cavity and forming the tubo-ovarian abscess with (in some cases) pelvic peritonitis. TOA can develop from the lymphatic system with infection of the parametrium from an intrauterine device (IUD). Bacteria recovered from TOAs are \"Escherichia coli\", \"Bacteroides fragilis\", other \"Bacteroides\" species, \"Peptostreptococcus\", \"Peptococcus\", and aerobic \"streptococci\". Long term IUD use is associated with TOA. Actinomyces is also recovered from TOA.\n\nLaparoscopy and other imaging tools can visualize the abscess. Physicians are able to make the diagnosis if the abscess ruptures when the woman begins to have lower abdominal pain that then begins to spread. The symptoms then become the same as the symptoms for peritonitis. Sepsis occurs, if left untreated. Ultrasonography is a sensitive enough imaging tool that it can accurately differentiate between pregnancy, hemorrhagic ovarian cysts, endometriosis, ovarian torsion, and tubo-ovarian abscess. Its availability, the relative advancement in the training of its use, its low cost, and because it does not expose the woman (or fetus) to ionizing radiation, ultrasonography an ideal imaging procedure for women of reproductive age.\n\nRisk factors have been identified which indicate what women will be more likely to develop TOA. These are: increased age, IUD insertion, chlamydia infection, and increased levels of certain proteins (CRP and CA-125) and will alert clinicians to follow up on unresolved symptoms of PID.\n\nTreatment for TOA differs from PID in that some clinicians recommend patients with tubo-ovarian abscesses have at least 24 hours of inpatient parenteral treatment with antibiotics, and that they may require surgery. If surgery becomes necessary, pre-operative administration of broad-spectrum antibiotics is started and removal of the abscess, the affected ovary and fallopian tube is done. After discharge from the hospital, oral antibiotics are continued for the length of time prescribed by the physician.\n\nTreatment is different if the TOA is discovered before it ruptures and can be treated with IV antibiotics. During this treatment, IV antibiotics are usually replaced with oral antibiotics on an outpatient basis. Patients are usually seen three days after hospital discharge and then again one to two weeks later to confirm that the infection has cleared. Ampicillin/sulbactam plus doxycycline is effective against C. trachomatis, N. gonorrhoeae, and anaerobes in women with tubo-ovarian abscess. Parenteral Regimens described by the Centers for Disease Control and prevention are Ampicillin/Sulbactam 3 g IV every 6 hours and Doxycycline 200 mg orally or IV every 24 hours, though other regiemes that are used for pelvic inflammatory disease have been effective.\n\nComplications of TOA are related to the possible removal of one or both ovaries and fallopian tubes. Without these reproductive structures, fertility can be affected. Surgical complications can develop and include:\n\n\nThe epidemiology of TOA is closely related to that of pelvic inflammatory disease which is estimated to one million people yearly.\n"}
{"id": "25384450", "url": "https://en.wikipedia.org/wiki?curid=25384450", "title": "Uni Health", "text": "Uni Health\n\nUni Research Health is a department in Uni Research, one of the largest research companies in Norway. Research Director of Uni Research Health is Professor Hege R. Eriksen.\n\nUni research health has approximately 125 employees, most of them located in Bergen, Norway.\n\nThe research and educational activities of Uni Health are concentrated in the following research units:\n\nCentre for Child and Adolescent Mental Health Research\nChild Protection Research Unit\nDental Biomaterials: Adverse Reaction Unit\nGAMUT (the Grieg Academy Music Therapy Research Centre)\nHEMIL Centre (Research Centre for Health Promotion)\nNational Centre for Emergency Primary Health Care\nOccupational and Environmental Medicine\nResearch Unit for General Practice in Bergen\nResearch Centre for Sick Leave and Rehabilitation\nStress, Health and Rehabilitation (formerly the Research Unit of the Norwegian Network for Back Pain)\n\n"}
{"id": "37581508", "url": "https://en.wikipedia.org/wiki?curid=37581508", "title": "WHO Regional Office for the Eastern Mediterranean", "text": "WHO Regional Office for the Eastern Mediterranean\n\nThe WHO Regional Office for the Eastern Mediterranean is the regional office of the World Health Organization that serves 22 countries and territories in the Middle East, the North Africa, the Horn of Africa and Central Asia. It is one of the WHO's six regional offices around the world.\n\nAll the regional divisions of WHO were created between 1949 and 1952. They are based on article 44 of WHO's constitution, which allows the WHO to \"establish a [single] regional organization to meet the special needs of [each defined] area\". Many decisions are made at regional level, including importance discussions over WHO's budget, and in deciding the members of the next assembly, which are designated by the regions.\n\nThe WHO Regional Office for the Eastern Mediterranean aims to work with local governments, specialized agencies, partners and other stakeholders in the field of public health to develop health policies and strengthen national health systems.\n\nIt serves the WHO Eastern Mediterranean Region, which includes 21 member states in the Middle East, North Africa, the Horn of Africa and Central Asia, as well as the occupied Palestinian territory (West Bank and Gaza Strip). The office covers an area of nearly 583 million people. The countries and territories in the WHO Regional Office for the Eastern Mediterranean are:\nThe WHO Regional Office for the Eastern Mediterranean was originally based in Alexandria, Egypt. It was later moved to its new location in Nasr City, Cairo.\n\nThe official languages of WHO in the Eastern Mediterranean Region are Arabic, English and French. However, other national languages such as Persian, Urdu, Dari, Pashto and Somali are also used in communicating health messages and delivering health programs.\n\n\n\n"}
