{"id": "15037222", "url": "https://en.wikipedia.org/wiki?curid=15037222", "title": "Adolescent health", "text": "Adolescent health\n\nAdolescent health, or youth health, is the range of approaches to preventing, detecting or treating young people’s health and well being . \n\nThe term adolescent and young people are often used interchangeably, as are the terms Adolescent Health and Youth Health. Young people's health is often complex and requires a comprehensive, biopsychosocial approach.\n\nSome young people engage in risky behaviours that affect their health and therefore the majority of health problems are psychosocial. Many young people experience multiple problems. These behaviours are established as a young person and go on to become the lifestyles of adults leading to chronic health problems. Social, cultural and environmental factors are all important.\nYoung people have specific health problems and developmental needs that differ from those of children or adults: The causes of ill-health in adolescents are mostly psychosocial rather than biological. Young people often engage in health risk behaviours that reflect the processes of adolescent development: experimentation and exploration, including using drugs and alcohol, sexual behaviour, and other risk taking that affect their physical and mental health. Adolescent health also encompasses children's and young people's sexual and reproductive health (SRH). \n\nThe World Health Organisation describes the leading health related problems in the age group 10 – 19 years to include:\n\nYoung people often lack awareness of the harm associated with risk behaviours, and the skills to protect themselves as well as the lack knowledge about how and where to seek help for their health concerns . By intervening at this early life stage, many chronic conditions later in life can be prevented.\n\nEvidence-based practices include harm reduction and health promotion to intervene early in the life course and illness trajectory. Youth health is founded on collaborative approaches that address social justice. Youth development approaches include youth empowerment and youth participation. Their aim is to promote youth rights, youth voice and youth engagement.\n\nStudies about young people's access to healthcare have identified major barriers including concerns about confidentiality, practitioners attitudes and communication style, environment, availability of services, cost and the developmental characteristics of young people. Marginalised young people can have greater difficulty accessing health services and need support to navigate the health system.\n\nThe World Health Organisation 'Global standards for quality health-care services for adolescents' include: \n\nYouth Health includes adolescent medicine as a speciality, along with other primary and tertiary care services. Health services for young people include mental health services, child protection, drug and alcohol services, sexual health services. General Practitioners work alongside multidisciplinary health practitioners including psychology, social Work and Youth health nursing and school health services. Youth work and youth development services support and engage young people. Web based supports, such as Reach Out!, provide early intervention.\n\nYouth health services ('one-stop-shops' for young people) are specialist services providing multi-disciplinary, primary health care to young people. Focusing on engaging disadvantaged young people, they deliver flexible and unique services to young people in relaxed and comfortable youth-friendly environments. Youth health services work in partnership with other government and non-government services. Youth health services provide a range of entry-points and non-threatening services (such as creative arts, basic services such as showers and laundries, a drop in service, sports and recreational facilities), which encourage young people to connect with the service on their own terms. They also provide informal links to other support services and sectors including education, housing, financial support and legal services, offering support to young people who are dealing with complex issues. Youth health services understand the need to respond immediately to young people’s requests for support and assistance and they share a common operating philosophy, which values social justice, equity, and a holistic view of young people’s health and well being.\n\nCapacity building organisations support the Youth Health sector by providing access to information and resources, conducting research and providing training.\n\nThe Egyptian Society for Adolescent Medicine\nThe Arab Coalition for Adolescent Medicine\n\n"}
{"id": "19880333", "url": "https://en.wikipedia.org/wiki?curid=19880333", "title": "American Society for Nutrition", "text": "American Society for Nutrition\n\nThe American Society for Nutrition (ASN) is an American society for professional researchers and practitioners in the field of nutrition. ASN publishes four journals in the field of nutrition. It has been criticized for its financial ties to the food and beverage industry.\n\nIn 1928 a group of United States biochemists and physiologists grouped together to form the first scientific society focused on nutrition, the American Institute for Nutrition. The Society held its first meeting at the Cornell Medical School in 1934. The society was renamed the American Society for Nutritional Sciences in 1996.\n\nIn 2005, the American Society for Nutritional Sciences, the American Society for Clinical Nutrition (established 1961); and the Society for International Nutrition (established 1996) merged to form The American Society for Nutrition (ASN).\n\nASN currently (2015) has a membership of about 5,000. It is one of the constituent societies comprising the Federation of American Societies for Experimental Biology, a non-profit organization that is the principal umbrella organization of U.S. societies in the field of biological and medical research.\n\nIn October 2010, the American College of Nutrition and American Society for Nutrition proposed to merge.\n\nThe ASN administered the \"Smart Choices\" food labelling program, which was suspended in 2009 after criticism.\n\nASN publishes four academic journals. Both publisher and some editorial staff accept funding from food industry organizations.\n\nThe 2017 \"Current Developments in Nutrition\" is an open-access journal aiming for rapid publication and a broader range of topics than the ASN's other journals.\n\nEligibility for membership:\n\nASN uses the term \"Sustaining Partners\" for corporate sponsors donating over $10,000 per year. According to their website:\n\nIndustry companies with the highest level of commitment to the nutrition profession are recognized as Sustaining Partners of the American Society for Nutrition. Engage with ASN as a Sustaining Partner today, and benefit from a number of advantages! Recognition includes print and online exposure, annual meeting benefits, and the ability to sponsor educational opportunities, grants and other items. However, you will derive the greatest benefit by aligning your company with ASN's superlative scientific reputation.\n\nThe American Society for Nutrition's sustaining partners, as listed on its website as of March 2018, are:\nAbbott Nutrition,\nAlmond Board of California,\nBayer HealthCare,\nBiofortis Clinical Research,\nCalifornia Walnut Commission,\nCargill, Inc.,\nCorn Refiners Association,\nCouncil for Responsible Nutrition,\nDairy Research Institute,\nDSM Nutritional Products (LLC),\nDuPont Nutrition & Health,\nthe \"Egg Nutrition Center\" of the American Egg Board,\nGeneral Mills Bell Institute of Health and Nutrition,\nHerbalife/Herbalife Nutrition Institute,\nInternational Bottled Water Foundation,\nKellogg Company,\nKyowa Hakko USA Inc.,\nMars Inc.,\nMcCormick Science Institute,\nMondelez International Technical Center,\nMonsanto Company,\nNational Cattlemen's Beef Association (a contractor to \"The Beef Checkoff\"),\nNestlé Nutrition, Medical Affairs,\nPepsiCo,\nPfizer, Inc.,\nPharmavite (LLC),\nTate & Lyle,\nThe a2 Milk Company,\nThe Coca Cola Company,\nThe Dannon Company Inc.,\nThe Sugar Association, and \nUnilever.\n\nThe ASN has conflicting interests in taking funding from food industry marketing groups while providing unbiassed information on nutrition; these conflicting interests have caused criticism and concerns of bias. ASN actions have also been criticized for being better-aligned with the nutritional advice of sponsors than the advice of the World Health Organization and other public health, public interest, and government organizations.\n\nLong-time member Marion Nestle has voiced concerns about what she sees as a \"too-cozy relationship with food company sponsors\" within the organization. In a 2015 report, Michele Simon also voiced concerns regarding corporate involvement with the society.\n\n"}
{"id": "50607069", "url": "https://en.wikipedia.org/wiki?curid=50607069", "title": "Amino acid score", "text": "Amino acid score\n\nAmino acid score, in combination with protein digestibility, is the method used to determine if a protein is complete. PDCAAS and DIAAS are the two major protein standards which determine the completeness of proteins by their unique composition of essential amino acids.\n"}
{"id": "50792872", "url": "https://en.wikipedia.org/wiki?curid=50792872", "title": "Bartholin gland carcinoma", "text": "Bartholin gland carcinoma\n\nBartholin gland carcinoma is an uncommon type of malignancy in the Bartholin gland that accounts for 1% of all vulvar malignant neoplasms. It is most common in women in their mid-60s. The tumor can become large before a woman is aware of symptoms. One of the first symptoms can be dyspareunia. In other instances a woman may find a mass or ulcer in the vulva area. Many clinicians assume that an enlarged Bartholin gland is malignant in postmenopausal woman until proven otherwise. The growth of the tumor can spread to nearby areas such as the ischiorectal fossa and inguinal lymph nodes. Approximately 50% of bartholin gland carcinomas originate from squamous cell carcinomas. Another uncommon characteristic of Bartholin gland malignancies is that the growth of a lesion originates from the three types of epithelial tissue present in the gland: mucinous, transitional, and squamous.\n\nBartholin gland can be differentiated by histology to determine whether the malignancy is due to squamous cell carcinoma, adenoid cystic carcinoma, or adenocarcinomas. \n\nThough Bartholin gland carcinoma is rare, along with other unusual Bartholin gland growths, it many not be the typical practice for clinicians to consider lesions malignant. Early diagnosis can help to prevent the cancer from the glands to surrounding. Though malignancies of the Bartholin gland are rare clinicians biopsy Bartholin gland lesions in older women or when the growth reoccurs or does not respond to original treatment.\n\nThe prognosis is optimistic as long as the growth has not metastasized to the lymph nodes.\nBartholin glands were described in cattle by Casper Barthlin in 1677. Their existence in humans was postulated at that time. \n\nTreatment can be a vulvectomy that results in the removal of the growth along with an extensive removal of adjacent tissue. An inguinal lymphadenectomy often accompanies the vulvectomy. The tissue that is removed sometimes includes sections of the vagina and rectum.\n\nThe Adenoid cystic carcinoma of the Bartholin gland is another uncommon malignancy with symptoms that include local painful intermittent recurrences. The disease is slow to progress but it can result in lung cancer after a long time after the initial treatment. Treatment consists of surgical removal of the growth. Sometimes radiation and chemotherapy is performed.\n\n"}
{"id": "2526456", "url": "https://en.wikipedia.org/wiki?curid=2526456", "title": "Biomedical technology", "text": "Biomedical technology\n\nBiomedical technology broadly refers to the application of engineering and technology principles to the domain of living or biological systems. Usually inclusion of the term \"biomedical\" denotes a principal emphasis on problems related to human health and diseases, whereas terms like \"biotechnology\" can be medical, environmental, or agricultural in application. But most terms in this general realm still lack clear boundaries. Biomedical engineering and Biotechnology alike are often loosely called \"Biomedical Technology\" or Bioengineering. The Biomedical technology field is currently growing at a rapid pace. Required jobs for the industry expect to grow 23% by 2024, and with the pay averaging over $86,000. \nBiomedical technology involves:\n\n\nBiomedical technologies:\n"}
{"id": "33680836", "url": "https://en.wikipedia.org/wiki?curid=33680836", "title": "Calvin C.J. Sia", "text": "Calvin C.J. Sia\n\nCalvin C.J. Sia (born Calvin Chia Jung Sia on June 3, 1927) is a primary care pediatrician from Hawaii who developed innovative programs to improve the quality of medical care for children in the United States and Asia. Two particular programs have been implemented throughout America: the Medical Home concept for primary care that has been promoted by the American Academy of Pediatrics and the federal Emergency Medical Services for Children program administered by the U.S. Department of Health and Human Services’ Health Resources and Services Administration, Maternal and Child Health Bureau. His Medical Home model for pediatric care and early childhood development began to take root in several Asian countries in 2003.\n\nSia is also creator of Hawaii Healthy Start Home Visiting Program to prevent child abuse and neglect and co-founder of Hawaii's Zero to Three program and Healthy and Ready to Learn Center. The Hawaii Healthy Start program, which targets expecting and new parents who may be at risk of abusing or neglecting their children, became the model for the Healthy Families America home visiting program that the United States Department of Justice's Office of Justice Programs identified in 2010 as a \"promising\" approach to child abuse prevention. The Healthy and Ready to Learn Center was a three-year pilot project to initiate training and health delivery services in an integrated system of care, with pediatric residents and graduate students in social work and early childhood education working as a team.\n\nIn addition, Sia spearheaded the creation of the Variety School for learning disabled children, a Honolulu-based educational institution for children ages 5 through 13. Sia retired from his Honolulu-based medical practice in 1996, after almost 40 years of treating patients, but continues to promote Medical Home and community pediatrics as professor of Pediatrics at the University of Hawaii John A. Burns School of Medicine. Although he retired as chairman of the American Medical Association Section Council on Pediatrics in 2007, a post he assumed in 1983, Sia continues to play a national role as an emeritus member of the executive committee of the National Center for Medical Home Implementation Project Advisory Committee, an organization he formerly served as chairman.\n\nSia is a 1945 graduate of Punahou School in Honolulu and a graduate of Dartmouth College in 1950. He received his medical degree at Western Reserve University School of Medicine in 1955 and did a general rotating internship as a lieutenant in the U.S. Army Medical Corps at William Beaumont Army Hospital in El Paso, Texas from 1955-1956. Sia then served his pediatric residency under Dr. Irvine McQuarrie at Kauikeolani Children's Hospital in Honolulu, and obtained his license to practice medicine in Hawaii in 1958. He was certified by the American Board of Pediatrics in 1960 and recertified in 1987. The University of Hawaii awarded Sia an honorary Doctor of Humane Letters degree in 1992.\n\nAs a young practicing pediatrician, Sia joined the early cadre of American Academy of Pediatrics consultants for Head Start and Parent Child Centers in Hawaii in the 1960s and developed a strong interest in prenatal, neonatal, and postnatal causes of physical and mental disabilities in children. In a paper he presented in 1964 to the Hawaii Academy of Sciences on advances in neonatology, Sia cited progress in the care of premature babies but also noted that \"completeness\" of the first physical exam and the education of nurses to be on the alert for early signs of disabilities were possible ways to save newborns with previously lethal birth defects. He concluded by observing, \"One of the basic problems will be in solving the causes and prevention of prematurity.\"\n\nInspired by one of his mentors, Dr. Robert E. Cooke, the Johns Hopkins pediatrician behind the creation of the Hopkins hospital's Kennedy Institute for Handicapped Children, Sia helped establish Hawaii's Variety School for Learning Disabilities in 1967 and served as chairman of its board of directors for many years. Sia broadened the scope of his community work to address all children with special health care needs. In the early 1970s, he invited Dr. C. Henry Kempe, founder of the Denver-based National Center for the Prevention and Treatment of Child Abuse and Neglect, and Dr. Ray E. Helfer of Michigan—two pioneers in the identification and treatment of child abuse—to help him and a small group of child advocates develop a plan to prevent and treat child abuse and neglect in the islands. That effort netted one of the first 12 demonstration grant awards by the newly created National Center on Child Abuse and Neglect in 1975, with $1 million going to establish the first Hawaii Family Stress Center. The center, later renamed the Hawaii Family Support Center, established several child abuse and neglect programs on Oahu, including a home-visiting program based on Kempe's effective use of \"lay therapists.\" These were home visitors from the community, properly trained and supervised by public health nurses and social workers who could earn the trust of at-risk families and focus on family strengths to reduce environmental risk and prevent child abuse and neglect. The center's goal was to identify vulnerable families before their day-to-day stresses, isolation, and lack of parenting knowledge and good role models gave rise to abusive and neglectful behavior.\n\nThe center's operations coincided with an effort launched by Dr. Vince L. Hutchins and Dr. Merle McPherson of the Maternal and Child Health Bureau in 1977 to revise and update the\nmission of the federal agency's Title V and companion \"crippled children's\" programs to address child development and the prevention of developmental, behavioral and psychosocial problems. McPherson took note of Sia's call for a continuous system of care originating with the primary care pediatrician. The AAP collaborated in this effort by asking each state’s AAP chapter to develop a Child Health Plan that set priorities for using MCHB block grants. Sia spearheaded the Hawaii planning effort, bringing together representatives from the Hawaii AAP Chapter, the UH medical school, the Hawaii Medical Association, and Kapiolani Medical Center for Women and Children. Armed with anecdotal evidence showing home visitors were able to promote\neffective parenting and ultimately improve outcomes, the group wrote a plan that incorporated a coordinated system of care that emphasized wellness and prevention for\nchildren, especially those with special needs.\n\nThis was the birth of the Medical Home concept for primary care, to which Sia attached the slogan, “Every Child Deserves a Medical Home.” Under this idea, which the American Academy of Pediatrics adopted as a policy statement in 1992, the medical care of all infants, children and adolescents should be accessible, continuous, comprehensive, family-centered, coordinated, compassionate, and culturally effective. It should be delivered or directed by well-trained physicians who provide primary care and help to manage and facilitate essentially all aspects of pediatric care. The physician should be known to the child and family and should be able to develop a partnership of mutual responsibility and trust with them. As Sia and his co-authors of a 2006 monograph on the Medical Home noted, this new model broadens the traditional focus on acute care to include prevention and well care at one end of the continuum and chronic care management of children with special health care needs at the other. One expert observed, for example, that for a child born with spina bifida, Sia's Medical Home model would have the family and its health care provider compose a list of specialists and therapists who would be caring for the child and a timeline of anticipated surgeries and interventions. The aim would be to have as few emergencies and unanticipated events as possible.\n\nAs the lead author of an often-cited article published by the journal Pediatrics in May 2004, Sia traced the development of the Medical Home concept.\n\nBy 1984, Sia had begun to implement the Medical Home concept in Hawaii. As chairman of an ad hoc state legislative task force on child abuse, he persuaded Hawaii lawmakers to authorize the Hawaii Healthy Start Home Visiting Program for the prevention of child abuse and neglect. This state-funded pilot program, carried out by Hawaii Family Support Center in collaboration with the Hawaii Department of Health, focused on a neighborhood in the Ewa community on Oahu, a community with relatively high rates of child abuse and neglect. A year later, he spearheaded the Hawaii Medical Association's effort to obtain a grant from the U.S. Maternal and Child Health Bureau, under the Special Projects of Regional and National Significance (SPRANS) initiative, to train primary care physicians to provide a \"Medical Home\" for all children with special health care needs. The demonstration project—which sought to help first-time families give their newborn children the best start in life—was so successful it was expanded from a small part of Oahu to other areas of Hawaii, and as word of the demonstrated positive outcomes spread, Hawaii’s Healthy Start became a model for parenting education programs nationwide. In the early 1990s, Healthy Families America and the National Healthy Start Association began to standardize and credential programs to ensure effectiveness and research-based practices. Across the United States, according to the MCHB, the home visiting program has shown that it can reduce child maltreatment and increase children’s readiness for school.\n\nMeanwhile, Sia launched the Hawaii Early Intervention Program for infants and toddlers in 1986 and also became actively involved with Hawaii’s Early Intervention Coordinating Council for Zero to Three, placing this under Hawaii’s Department of Health instead of the Department of Education. The focus of this effort was to support the Medical Home system of care with prevention and early intervention programs.\n\nAt a June 1987 conference called by Surgeon General C. Everett Koop and sponsored by the AAP and MCHB to address children with special needs, Sia and his delegation from Hawaii made a presentation of the Medical Home concept. Koop appeared to embrace it by issuing a report that endorsed a system of family-centered, community-based, coordinated care for children with special needs. This was followed in 1989 by the first National Medical Home Conference, which drew 26 AAP state chapters to Hawaii for presentations organized by Sia and MCHB officials on how to train pediatricians in the Medical Home system of care. This led to consultations to introduce the Medical Home training program to interdisciplinary teams of pediatricians, families, and other health care–related professionals in Florida, Minnesota, Nebraska, Pennsylvania, Washington and other states.\n\nThe pace of activity prompted Sia to close his private medical practice in 1996 so he could devote his time as principal investigator on various early childhood grant projects promoting the Medical Home and its integrated system of care. He launched several initiatives with a MCHB Health Education Collaboration grant in support of interprofessional training in early childhood, a Carnegie Corporation of New York Starting Points planning grant in early childhood, and Consuelo Foundation of Hawaii's Healthy and Ready to Learn grant–all with the emphasis on integrating the continuum of care of the Medical Home with other health, family, and community services from a holistic approach. The MCHB funding enabled him to travel across the country to promote the Medical Home concept to various\ncommunities, state AAP chapters, family advocacy groups and state Title V maternal and child health officers.\n\nA three-year pilot project creating a Healthy and Ready to Learn Center in Hawaii began in 1992 and helped gauge the effectiveness of Sia's family-centered interprofessional collaboration approach. Lessons learned from this project were subsequently adopted by the Office of Children and Youth of the Governor's Office of Hawaii with Sia as Co-Principal Investigator. The Carnegie Corp. Starting Points grant then was assumed by the Good Beginnings Alliance in Hawaii.\n\nSia, serving as chairman of the American Medical Association's Section Council on Pediatrics and other AMA- and AAP-related posts, used those platforms and his network of contacts with other groups to help introduce the Medical Home concept into the care of adults as well as children, although his primary focus has remained on pediatric care. In 2007, the AAP, American Academy of Family Physicians, American Academy of Pediatrics, American College of Physicians and the American Osteopathic Association adopted the Joint Principles of the Patient-Centered Medical Home that set a standard definition of a Medical Home. A year later, the AMA adopted the principles, which have since received support from over 700 member organizations of the Patient Centered Primary Care Collaborative, including primary care and specialty care societies, all major health plans and consumer organizations. In addition, the term Medical Home now regularly shows up in the literature of parent groups such as Family Voices, in family practice journals and on the websites of state public health and medical agencies.\n\nBeginning in 2000, Sia expanded his efforts related to early child development and the Medical Home to Asia. In 2003, he created the Asia-US Partnership, a think tank based at the University of Hawaii medical school whose mission is to improve child health in Asia and the United States through cross-cultural\nexchanges with leaders in pediatrics. That same year, Sia initiated and chaired the first of several AUSP Early Child Development and Primary Care conferences, bringing together pediatric and early childhood development experts from Asia and the United States to translate the science of early child development into policy and action. Participants have come from China (Beijing, Shanghai and Hong Kong), the Philippines, Singapore and Thailand and the United States. According to conference reports, these international exchanges have stimulated translation of the science on early child development and primary care into action programs in the broad areas of advocacy, service delivery, research, and training among the Asian early childhood professionals leadership. Sia has continued to serve as co-chairman of these events, including the sixth international conference, held in the Philippines capital of Manila, in May 2011. After hosting the earliest AUSP conferences in Hawaii, Sia decided to move the 2009 event to Shanghai and tapped a team of Chinese doctors to serve as conference host, signaling what he called a new phase of activity aimed at developing greater shared leadership and stronger \"country teams.\"\n\nWhile planting the seeds of the Medical Home concept in Hawaii, Sia embarked on a related advocacy campaign focused on emergency care for children. In 1979, as president of the Hawaii Medical Association, Sia urged members of the American Academy of Pediatrics to develop multifaceted Emergency Medical Services programs designed to decrease disability and death in children. By January 1981, AAP's Executive Board had approved formation of a Section on Emergency Medicine, with Sia as one of its seven charter members. He along with José B. Lee then-executive officer of the Hawaii Medical Association Emergency Medical Services Program began working closely with Senator Daniel Inouye, whom he happened to meet on a flight to Washington, D.C., to create a National Emergency Medical Services for Children System (EMSC) demonstration grant program to address acute injuries, illnesses and other childhood crises. The program was launched after the October 1984 enactment of EMSC legislation (Public Law 98-555), a bipartisan measure sponsored by Inouye and Republican Senators Orrin Hatch of Utah and Lowell Weicker of Connecticut and endorsed by Surgeon General C. Everett Koop. States receiving these demonstration grants established an emergency medical care service system for children that upgraded training and equipment for first responders and emergency departments to treat children. Hawaii ultimately received a grant to initiate its own emergency care system for children, which improved care coordination with the primary care physician. EMSC is now an established statewide system of care for children in all 50 states and territories.\n\nSeveral national and state organizations have recognized Sia for developing innovative and responsive family-centered grassroots services. Among the awards he has received are these:\n\n\nSia was born in Beijing, China to Dr. Richard Ho Ping Sia, a physician and former Rockefeller Institute researcher in infectious diseases whose work laid the groundwork for the Avery–MacLeod–McCarty experiment on DNA and bacterial transformation, and Mary Li Sia, a Honolulu-born author of several Chinese cookbooks. His mother's parents were Kong Tai Heong and Li Khai Fai, doctors who worked on the 1899 plague outbreak. Sia and his older sister Sylvia and younger sister Julia, all United States citizens by birth, grew up in Hawaii, where the family settled in 1939 after living under Japanese occupation in Beijing for nearly two years.\n\nSia married Katherine Li in 1951. Sia has three sons, Richard H.P. Sia, a journalist; Jeffrey H.K. Sia, a Honolulu-based attorney and former president of the Hawaii State Bar Association; and Dr. Michael H.T. Sia, a pediatrician and chairman of Pediatrics at Kapiolani Medical Center for Women and Children; and six grandchildren.\n\n"}
{"id": "1296030", "url": "https://en.wikipedia.org/wiki?curid=1296030", "title": "Carbonless copy paper", "text": "Carbonless copy paper\n\nCarbonless copy paper (CCP), non-carbon copy paper, or NCR paper (No Carbon Required, taken from the initials of its creator, National Cash Register) is a type of coated paper designed to transfer information written on the front onto sheets beneath. It was developed by chemists Lowell Schleicher and Barry Green, as an alternative to carbon paper and is sometimes misidentified as such.\n\nInstead of inserting a special sheet in between the original and the intended copy, carbonless copy paper has micro-encapsulated dye or ink on the back side of the top sheet, and a clay coating on the front side of the bottom sheet. When pressure is applied (from writing or impact printing), the dye capsules rupture and react with the clay to form a permanent mark duplicating the markings made to the top sheet. Intermediary sheets, with clay on the front and dye capsules on the back, can be used to create multiple copies; this may be referred to as multipart stationery.\n\nCarbonless copy paper consists of sheets of paper that are coated with micro-encapsulated dye or ink or a reactive clay. The back of the first sheet is coated with micro-encapsulated dye (referred to as a Coated Back or CB sheet). The lowermost sheet is coated on the top surface with a clay that quickly reacts with the dye to form a permanent mark (Coated Front, CF). Any intermediate sheets are coated with clay on top and dye on the bottom (Coated Front and Back, CFB).\n\nWhen the sheets are written on with pressure (e.g., ball-point pen) or impact (e.g., typewriter, dot-matrix printer), the pressure causes the micro-capsules to break and release their dye. Since the capsules are so small, the resulting print is very accurate.\n\nCarbonless copy paper was also available in a self-contained version that had both the ink and the clay on the same side of the paper.\n\nCarbonless copy paper was first produced by the NCR Corporation, applying for a patent on June 30, 1953. Formerly, the options were to write documents more than once or use carbon paper, which was inserted between the sheet being written upon and the copy. Carbonless paper was used as business stationery requiring one or more copies of the original, such as invoices and receipts. The copies were often paper of different colors (e.g., white original for customer, yellow copy for supplier's records, and other colors for subsequent copies). Stationery with carbonless copy paper can be supplied collated either in pads or books bound into sets, or as loose sets, or as continuous stationery for printers designed to use it.\n\nThe first dye used commercially in this application was crystal violet lactone, which is widely used today. Other dyes and supporting chemicals used are PTSMH (\"p\"-toluene sulfinate of Michler's hydrol), TMA (trimellitic anhydride), phenol-formaldehyde resins, azo dyes, DIPN (diisopropyl naphthalenes), formaldehyde isocyanates, hydrocarbon-based solvents, polycyclic aromatic hydrocarbons, polyoxypropylene diamine, epoxy resins, aliphatic isocyanates, Bisphenol A, diethylene triamine, and others. The dyes in carbonless copy papers may cause contact dermatitis in sensitive persons.\n\nUntil the 1970s, when the use of polychlorinated biphenyls (PCBs) was banned due to health and environmental concerns, PCBs were used as a transfer agent in carbonless copy paper. PCBs are readily transferred to human skin during handling of such papers, and it is difficult to achieve decontamination by ordinary washing with soap and water. In Japan, carbonless copy paper is still treated as a PCB-contaminated waste.\n\nExposure to certain types of carbonless copy paper or its components has resulted, under some conditions, in mild to moderate symptoms of skin irritation and irritation of the mucosal membranes of the eyes and upper respiratory tract. A 2000 review found no irritation or sensitization on contact with carbonless copy paper produced after 1987. In most cases, good industrial hygiene and work practices should be adequate to reduce or eliminate symptoms. These include adequate ventilation, humidity, and temperature controls; proper housekeeping; minimal hand-to-mouth and hand-to-eye contact; and periodic cleansing of hands.\n\nIn a 1997 study, the University of Florida found that a poorly-ventilated office where large amounts of carbonless copy paper were used had significant levels of volatile organic compounds present in its air, whereas a well-ventilated office where little such paper was used did not. The study also found that there were higher rates of sick leave and illness complaints at the office using large amounts of carbonless copy paper. Another study, which was published in \"Environmental Health Perspectives\", connected chronic occupational exposure to paper dust and carbonless copy paper with an increased risk of adult-onset asthma.\n\nThe average carbonless copy paper contains a high concentration of Bisphenol A (BPA), an endocrine disruptor.\n\nIn 2001, three employees of a medical center in San Francisco filed a lawsuit against their employer, blaming exposure to carbonless copy paper and other chemicals for their inflammatory breast cancer.\n\n\n\n"}
{"id": "31837766", "url": "https://en.wikipedia.org/wiki?curid=31837766", "title": "Ciberobn", "text": "Ciberobn\n\nThe Spanish Biomedical Research Centre in Physiopathology of Obesity and Nutrition (Centro de Investigación Biomédica en Red de Fisiopatología de la Obesidad y Nutrición: CIBERObn) is a public research consortium which was founded on November 28, 2006 financed by the Instituto de Salud Carlos III (ISCIII) and the Ministerio de Ciencia e Innovación (MICINN).\n\nThe CIBERObn gathers 31 investigation groups from different Spanish Hospitals, Universities and Research Centres. Its mission is to promote a better knowledge about the mechanisms contributing to obesity development in order to reduce its incidence and prevalence, as well as its complications, in addition to nutrition-related diseases. \n\nThe CIBERObn is structured into 8 scientific programs intended to increase the collaboration between researchers, to strengthen synergies and to boost new research lines. Programs are as follows: \n\n\nAdditionally, CIBERObn lays a particular emphasis on translational research, specially focusing on research transfer to clinical applications and practices. To this end, two cross-cutting programs have been created: \n\nThe Fat Bank is a strategic platform of the CIBERObn which offers the Scientific Community different kinds of biological material which are associated to thorough metabolic phenotyping. This information is entered by means of a tailor-made individualised software. This fat-bank- launched in 2009- currently contains 3000 samples of biologic material from more than 300 individuals.\n\nIn 2009, 287 indexed articles were published. Their average impact factor is 4.05, which is very high for this subject area. Of them, 67 (23%) belong to the first decile and 105 more (total 172 papers, 60%) belong to the first quartile of the subject area of indexed journals. They accumulate a total impact factor of 1,165. Provisional data of 2010 show an increase of 10%, highly improving the international visibility of the consortium.\n\nCIBEROBN website\n"}
{"id": "13522147", "url": "https://en.wikipedia.org/wiki?curid=13522147", "title": "Clinical coder", "text": "Clinical coder\n\nA clinical coder – also known as clinical coding officer, diagnostic coder, medical coder, nosologist or medical records technician – is a health information professional whose main duties are to analyse clinical statements and assign standard codes using a classification system. The data produced are an integral part of health information management, and are used by local and national governments, private healthcare organizations and international agencies for various purposes, including medical and health services research, epidemiological studies, health resource allocation, case mix management, public health programming, medical billing, and public education.\n\nFor example, a clinical coder may use a set of published codes on medical diagnoses and procedures, such as the International Classification of Diseases (ICD) or the Common Coding System for Healthcare Procedures (HCPCS), for reporting to the health insurance provider of the recipient of the care. The use of standard codes allows insurance providers to map equivalencies across different service providers who may use different terminologies or abbreviations in their written claims forms, and be used to justify reimbursement of fees and expenses. The codes may cover topics related to diagnoses, procedures, pharmaceuticals or topography. The medical notes may also be divided into specialities for example cardiology, gastroenterology, nephrology, neurology , pulmonology or orthopedic care.\n\nA clinical coder therefore requires a good knowledge of medical terminology, anatomy and physiology, a basic knowledge of clinical procedures and diseases and injuries and other conditions, medical illustrations, clinical documentation (such as medical or surgical reports and patient charts), legal and ethical aspects of health information, health data standards, classification conventions, and computer- or paper-based data management, usually as obtained through formal education and/or on-the-job training.\n\nThe basic task of a clinical coder is to classify medical and health care concepts using a standardised classification. Most clinical coders are employed in coding inpatient episodes of care. However, mortality events, outpatient episodes, general practitioner visits and population health studies can all be coded.\n\nClinical coding has three key phases: a) Abstraction; b) Assignment; and c) Review.\n\nThe abstraction phase involves reading the entire record of the health encounter and analysing the information to determine what condition(s) the patient had, what caused it and how it was treated. The information comes from a variety of sources within the medical record, such as clinical notes, laboratory and radiology results, and operation notes.\n\nThe assignment phase has two parts: finding the appropriate code(s) from the classification for the abstraction; and entering the code into the system being used to collect the coded data.\n\nReviewing the code set produced from the assignment phase is very important. Clinical coder must ask themselves, \"does this code set fairly represent what happened to this patient in this health encounter at this facility?\" By doing this, clinical coders are checking that they have covered everything that they must, but not used extraneous codes. For health encounters that are funded through a case mix mechanism, the clinical coder will also review the diagnosis-related group (DRG) to ensure that it does fairly represent the health encounter.\n\nClinical coders may have different competency levels depending on the specific tasks and employment setting.\n\nAn entry level coder has completed (or nearly completed) an introductory training program in using clinical classifications. Depending on the country; this program may be in the form of a certificate, or even a degree; which has to be earned before the trainee is allowed to start coding. All trainee coders will have some form of continuous, on-the-job training; often being overseen by a more senior coder. \n\nAn intermediate level coder has acquired the skills necessary to code many cases independently. Coders at this level are also able to code cases with incomplete information. They have a good understanding of anatomy and physiology along with disease processes. Intermediate level coders have their work audited periodically by an Advanced coder.\n\nAdvanced level and senior coders are authorized to code all cases including the most complex. Advanced coders will usually be credentialed and will have several years of experience. An advanced coder is also able to train entry-level coders.\n\nA nosologist understands how the classification is underpinned. Nosologists consult nationally and internationally to resolve issues in the classification and are viewed as experts who can not only code, but design and deliver education, assist in the development of the classification and the rules for using it.\n\nNosologists are usually expert in more than one classification, including morbidity, mortality and casemix. In some countries the term \"nosologist\" is used as a catch-all term for all levels.\n\nClinical coders may use many different classifications, which fall into two main groupings: statistical classifications and nomenclatures.\n\nA statistical classification, such as ICD-10 or DSM-5, will bring together similar clinical concepts, and group them into one category. This allows the number of categories to be limited so that the classification does not become too big, but still allows statistical analysis. An example of this is in ICD-10 at code I47.1. The code title (or rubric) is Supraventricular tachycardia. However, there are several other clinical concepts that are also classified here. Amongst them are paroxysmal atrial tachycardia, paroxysmal junctional tachycardia, auricular tachycardia and nodal tachycardia.\n\nWith a nomenclature, for example SNOMED CT, there is a separate listing and code for every clinical concept. So, in the tachycardia example above, each type and clinical term for tachycardia would have its own code listed. This makes nomenclatures unwieldy for compiling health statistics.\n\nIn some countries, clinical coders may seek voluntary certification or accreditation through assessments conducted by professional associations, health authorities or, in some instances, universities. The options available to the coder will depend on the country, and, occasionally, even between states within a country. \n\nClinical coders start as trainees, and there are no conversion courses for coders immigrating to the United Kingdom.\n\nThe National Clinical Coding Qualification (NCCQ) is an exam for experienced coders, and is recognised by the four health agencies of the UK.\nIn England, a novice coder will complete the national standards course written by NHS Digital within 6 months of being in post. They will then start working towards the NCCQ. \n\nThree years after passing the NCCQ, two further professional qualifications are made available to the coder in the form of NHS Digital's clinical coding auditor and trainer programmes.\n\nIn 2015, National Services Scotland, in collaboration with Health Boards, launched the Certificate of Technical Competence (CTC) in Clinical Coding (Scotland). Awarded by the Institute of Health Records & Information Management (IHRIM); the aims of the certificate include supporting staff new to clinical coding, and providing a standardised framework of clinical coding training across NHS Scotland.\n\nThe NCCQ is a recognized coding qualification in Scotland.\n\nAs of 2016; the typical qualification for an entry-level medical coder in the United States is completion of a diploma or certificate, or, where they are offered, an associate degree. The diploma, certificate, or degree will usually always include an Internet-based and/or in-person internship, at some form of a medical office or facility, at the conclusion. Some form of on-the-job training, or at least oversight, is also usually provided in the first months on the job, until the coder can earn an intermediate or advanced level of certification and accumulate time on the job. For further academic training, a baccalaureate or master's degree in medical information technology, or a related field, can be earned by those who wish to advance to a supervisory or academic role. That option would be recommended for those wishing to teach medical billing or coding at a college or university, community college, or technical or vocational institute, or who wish to become heads of medical billing and coding departments, especially if the doctor's office or clinic, or other facility (among other working options, a medical school or hospital, a skilled nursing facility or other nursing home, a psychiatric facility, an assisted or independent living facility, a rehabilitation facility, a rest home or domiciliary or boarding house, etc.) is very large and receives complex cases, such as a referral facility or a Level I trauma teaching hospital center. A nosologist (medical coding expert) in the U.S. will usually be certified by either AHIMA or the AAPC (often both) at their highest level of certification and specialty inpatient and/or outpatient certification (pediatrics, obstetrics/gynecology, gerontology, oncology are among those offered by AHIMA and/or the AAPC), have at least 3-5 years of intermediate experience beyond entry-level certification and employment, and often holds an associate, bachelor's, or graduate degree. \n\nThe AAPC offers the following entry-level certifications in the U.S.: Certified Professional Coder (CPC); which tests on most areas of medical coding, and also the Certified Inpatient Coder (CIC) and Certified Outpatient Coder (COC). Also in the American Health Information Management Association (AHIMA) offers the entry-level Certified Coding Associate (CCA); which is, like the AAPC's CPC, a wide-ranging introductory test. \n\nSome U.S. states, though decidedly not the majority, as it is a very recent trend, now mandate or at least strongly encourage certification or a degree from a college- or at the minimum, some evidence of competency beyond the record of on the job training- and/or from either the AAPC or AHIMA, to be employed. Some states have registries of medical coders, though these can be voluntary listings- which is, for those few who do, most often the case- and so not mandatory. This trend was accelerated in part by the passage of HIPAA (which enforces among other things, patient privacy and access to and the form of medical records) and the Affordable Care Act (U.S. President Barack Obama's health care reform law); and similar changes in other developed and developing countries, many of which, especially in the Western developed countries, and beyond, use the ICD-10 for diagnostic medical coding, which is a quite complex system of codes. The change to more regulation and training has also been driven by the need to create accurate, detailed, and secure medical records- especially patient charts, bills, and claim form submissions, that can be recorded efficiently in an electronic era of medical records where they need to be carefully shared between different providers or institutions of care, which was encouraged and later required by legislation and institutional policy.\n\nIn many countries clinical coders are accommodated for by both professional bodies specific to coding, and organisations who represent the health information management profession as a whole.\n\n\n\n\nIHRIM are the awarding body for the National Clinical Coding Qualification (NCCQ).\n\nThere are several associations that medical coders in the United States may join, including:\n\nThe AHIMA and AAPC societies' accredited programs will generally train medical coders at a sufficient level to work in their respective states. Some medical coders elect to be certified by both societies. \n\nAHIMA maintains a list of accredited medical coding certificate (and health information management associate, bachelor's, and graduate programs, through a link on the AHIMA accredited programs page, to CAHIIM) here.\n\n\n"}
{"id": "2998435", "url": "https://en.wikipedia.org/wiki?curid=2998435", "title": "Disease mongering", "text": "Disease mongering\n\n]\nDisease mongering is a term for the practice of widening the diagnostic boundaries of illnesses and aggressively promoting their public awareness in order to expand the markets for treatment. \nAmong the entities benefiting from selling and delivering treatments are pharmaceutical companies, physicians, alternative practitioners and other professional or consumer organizations. It is distinct from the promulgation of bogus or unrecognised diagnoses.\n\nThe term “monger” has ancient roots, providing the basis for many common compound forms such as cheesemonger, fishmonger, and fleshmonger for those who peddle such wares respectively. “Disease mongering” as a label for the \"invention\" or promotion of diseases in order to capitalize on their treatment was first used in 1992 by health writer Lynn Payer, who applied it to the Listerine mouthwash campaign against halitosis (bad breath).\n\nPayer defined disease mongering as a set of practices which include the following:\n\n\nThe incidence of conditions not previously defined as illness being medicalised as \"diseases\" is difficult to scientifically assess due to the inherent social and political nature of the definition of what constitutes a disease, and what aspects of the human condition should be managed according to a medical model. For example, halitosis, the condition which prompted Payer to coin the phrase \"disease mongering\", isn't merely an imagined social stigma but can stem from any of a wide spectrum of conditions spanning from bacterial infection of the gums to kidney failure, and is recognized by the Scientific Council of the American Dental Association as \"a recognizable condition which deserves professional attention\".\n\nAustralian journalist Ray Moynihan has argued that the pharmaceutical industry engages in disease mongering to enlarge its profits, and that it harms patients. His use of osteoporosis as an example of a \"made up\" disease in this article prompted an angry retort from the president of the British National Osteoporosis Society, stating that the article was insulting to people with osteoporosis and vastly understated the risk of disabling fractures associated with the disorder. Moynihan published a satire of disease mongering in the 2006 April Fool's Day issue of BMJ titled \"Scientists find new disease: motivational deficiency disorder\".\n\nOther conditions which have been cited as examples of disease mongering include restless leg syndrome, testosterone deficiency, erectile dysfunction, hypoactive sexual desire disorder. Some of these conditions are recognized as medical disorders by professional medical societies and the National Institute of Health and Clinical Excellence. In 2014 an FDA advisory committee voted to limit the use of testosterone replacement therapy products due to potentially increased cardiovascular risk associated with their use.\n\nA 2006 Newcastle, New South Wales international conference, reported in \"PLoS Medicine\", explored the phenomenon of disease mongering.\n\n\n"}
{"id": "46461872", "url": "https://en.wikipedia.org/wiki?curid=46461872", "title": "EQ-5D", "text": "EQ-5D\n\nEQ-5D is a standardized instrument for measuring generic health status. The health status measured with EQ-5D is used for estimating preference weight for that health status, then by combining the weight with time, quality-adjusted life year (QALY) can be computed. QALYs gained is used as an outcome in cost-utility analysis which is a type of economic evaluation that compares the benefit and cost of health care programs or interventions. Many countries generated a value set (preference weights) of their own population and have used it for estimating QALY to make decisions in resource allocation. There are currently 171 language versions of EQ-5D questionnaire available. EQ-5D is one of the most commonly used generic health status measurement, and its good validity and reliability have been reported in various health conditions.\n\nEQ-5D was first introduced in 1990 by the EuroQol Group. This group was initially formed in 1987 with the researchers of multidisciplinary areas from five European countries; Netherlands, UK, Sweden, Finland, and Norway. They worked cooperatively with the aim of developing an instrument which is not specific to disease but standardized and can be used as a complement for existing health-related quality of life (HRQoL) measures. Other required characteristics of the new instrument were capable of being sent as a postal questionnaire for self-completion, easy to complete, applicable to everyone, can produce a single index value, and can take into account the health status ‘worse than dead’.\n\nThe EQ-5D questionnaire is made up for two components; health state description and evaluation.\n\nIn description part, health status is measured in terms of five dimensions (5D); mobility, self-care, usual activities, pain/discomfort, and anxiety/depression. Mobility dimension asks about the person's walking ability. Self-care dimension asks about the ability to wash or dress by oneself, and usual activities dimension measures performance in \"work, study, housework, family or leisure activities\". In pain/discomfort dimension, it asks how much pain or discomfort they have, and in anxiety/depression dimension, it asks how anxious or depressed they are. The respondents self-rate their level of severity for each dimension using three-level (EQ-5D-3L) or five-level (EQ-5D-5L) scale.\n\nIn evaluation part, the respondents evaluate their overall health status using the visual analogue scale (EQ-VAS).\n\nWhen EQ-5D was first developed, the scale used in the health state description part was three-level; having no problems, having some or moderate problems, being unable to do/having extreme problems. As an example, three levels of mobility dimension are phrased as \"I have no problems in walking about\", \"I have some problems in walking about\", and \"I am confined to bed\". The respondents are asked to choose one of the statements which best describes their health status of surveyed day. Rated level can be coded as a number 1, 2, or 3, which indicates having no problems for 1, having some problems for 2, and having extreme problems for 3. As a result, a person's health status can be defined by a 5-digit number, ranging from 11111 (having no problems in all dimensions) to 33333 (having extreme problems in all dimensions). 12321 indicates having no problems in mobility and anxiety/depression, having slight problems in self-care and pain/discomfort, and having extreme problems in usual activities. There are potentially 243 (=3) different health states.\n\nA 'youth version' of the EQ-5D-3L descriptive system was developed for self-completion by children and younger people. It includes equivalent dimensions to the original EQ-5D-3L, phrased so as to be more easily understood and relevant for younger people. The dimensions are: 'mobility', 'looking after myself', 'doing usual activities', 'having pain or discomfort' and 'feeling worried, sad or unhappy'.\n\nAlthough its brevity contributed a lot for the wide use of EQ-5D, the three-level scale showed some limitations. The major drawback is that it has much fewer descriptive capability of health status compared to other generic instruments. For example, the Health Utilities Index Mark 2 and Mark 3 (HUI 2 and HUI 3) and the Short Form 6D (SF-6D) can define 24,000, 972,000, and 18,000 unique health states, while EQ-5D-3L can do only 243. As a consequence, it suffers from ceiling and flowing effect and showed low sensitivity to catch small changes of health status especially when used to patients with milder conditions. To improve such constraints of the three-level scale, the new version of EQ-5D with five-level scale was developed (EQ-5D-5L). The number of levels of severity was increased to five in this new version; having no problems, having slight problems, having moderate problems, having severe problems and being unable to do/having extreme problems. The new version can define 3,125 (=5) different health states. Some of the wordings of the scale were revisited to be clearer and the instruction for EQ-VAS was simplified. No changes were made for the five dimensions.\n\nThe validity and reliability of the EQ-5D have been assessed for the different language versions and various health conditions, including cancer, type 2 diabetes, COPD, asthma, and cardiovascular disease, and so on. The 5L system showed improved responsiveness compared to the 3L system, and also good validity and reliability . EQ-5D-5L has also been recommended to the elderly population as a generic health status measurement, in combination with other supplementary measurements to capture all related aspects in their quality-of-life.\n\nVisual analogue scale is the second part of the questionnaire, asking to mark health status on the day of the interview on a 20 cm vertical scale with end points of 0 and 100. There are notes at the both ends of the scale that the bottom rate (0) corresponds to \" the worst health you can imagine\", and the highest rate (100) corresponds to \"the best health you can imagine\". In the EQ-5D-3L version, the respondents has to draw a line from the box on the questionnaire to the scale indicates the health state of the interviewed day, while the EQ-5D-5L version asks to mark X on the scale to indicate the today's health and write the number of the scale marked in the empty box on the questionnaire. A well-known limitation of visual analogue scale is end-of-scale bias that respondents are less likely to use the extreme ends of the scale for rating their health status. However, it is still useful and the simplest direct method for valuing health-related quality of life (HRQoL) weights.\n\nOnce the health status is assessed from the description part, the 5-digit number can be converted into a preference weight which is also referred to as a single weighted index score. It can be done by using methods for generating HRQoL weights, such as visual analogue scale (VAS), the time-trade-off (TTO), and the standard gamble (SG). The choice of the valuation methods can vary. The initial purpose of having visual analogue scale in the EQ-5D questionnaire was to get preference weights using the scale, but the time-trade-off has become favored because it is a \"choice task\", not a \"rating task\" which easily involves some scaling bias. Time-trade-off is recommended when performing cost-utility analysis using quality-adjusted life year (QALY) as an outcome, but other methods could be chosen for different type of analyses. A value set was developed using time-trade-off in many countries, including the United Kingdom, United States, Spain, Japan, and Germany. The index score of a value set derived from the general population sample can be regarded as a \"\"societal\" valuation of the respondent's health state\" in that country. In contrast, the scores from the visual analogue scale in the questionnaire indicates the respondent's \"own\" assessment of his/her health status\". A value set derived from the general population sample has been criticized for a lack of a compelling theoretical support.\n"}
{"id": "25096340", "url": "https://en.wikipedia.org/wiki?curid=25096340", "title": "Effects of sleep deprivation on cognitive performance", "text": "Effects of sleep deprivation on cognitive performance\n\nIt has been estimated that over 20% of adults suffer from some form of sleep deprivation. Insomnia and sleep deprivation are common symptoms of depression and can be an indication of other mental disorders. The consequences of not getting enough sleep could have dire results; not only to the health of the individual, but those around them as sleep deprivation increases the risk of human-error related accidents, especially with vigilance-based tasks involving technology. \n\nThe parietal lobes of the brain are largely involved in attention. Lesions to this region of the brain in humans result in difficulty or inability to attend to events that are contralateral to the lesioned hemisphere. Those with lesions to the posterior parietal lobe have little to no difficulty shifting attention to and from stimuli appearing in the space ipsilateral to the lesioned hemisphere. However, they do display a slowed response in shifting their focus of current attention to events and stimuli appearing contralateral to the lesioned hemisphere.\n\nStudies involving single-unit recordings from the parietal lobes of monkeys have indicated that there are neurons solely involved in integrating visual spatial information with postural information. Without this apparent combining of spatial information, it would be difficult or impossible to locate objects in external space, as information provided solely by the retina is insufficient. The position of the eyes, head and body must also be taken into consideration.\n\nIn addition, studies involving transcranial magnetic stimulation application over the parietal lobes as well as positron emission tomography (PET) analysis of the parietal lobes suggest that this region is involved in conjunction searches, but not in single-feature searches. (See Visual search for supplementary information.)\n\nAuditory attention has been examined following sleep deprivation. Researchers examined the auditory attention of twelve non-sleep-deprived subjects and twelve sleep-deprived subjects at various time intervals. Subjects were involved in an auditory attention task, which required the reproduction of the spatial relationships between four letters, using a graph composed of six squares, immediately following the presentation of an item from a tape recorder. It was found that auditory attention of sleep-deprived individuals is affected as the total amount of sleep-deprivation increases, possibly due to lowered perceptual vigilance.\n\nFunctional magnetic resonance imaging (fMRI) scans of the brains of subjects exposed to thirty-five hours of sleep deprivation indicate that sleep deprivation is related to increases in prefrontal cortex and parietal lobe activation during tasks that combine verbal learning and arithmetic. This is particularly apparent in the right hemisphere. In non sleep-deprived individuals involved in verbal learning and arithmetic tasks the anterior cingulate cortex and the right prefrontal cortex are active. Following sleep deprivation there is increased activation of the left inferior frontal gyrus and the bilateral parietal lobes. This information suggests that divided attention tasks require more attentional resources than normally required by a non sleep-deprived individual.\n\nStudies utilizing event-related potential (ERP) recordings have found that twenty-four hours of sleep deprivation decreases ERP response to signal inputs from endogenous, but not exogenous, sources. Therefore, it is suggested that sleep deprivation affects endogenously driven selective attention to a greater extent than exogenously driven selected attention.\n\nTwenty-four hours of sleep deprivation has been found to affect the functional connectivity between the inferior frontal parietal region (IPS) and the parahippocampal place area (PPA). However, sleep deprivation does not affect the attention modulation index of the PPA. With this information, researchers have concluded that the psychophysiological interaction (PPI) is more involved in selective attention than the IPS and PPA.\n\nResearch has found that together, attention and sleep deprivation modulate the parahippocampal place area (PPA) activation and scene processing. Specifically, sleep deprivation was related to significant decreases in PPA activation while attending to scenes and when ignoring scenes. This is explained by the absence of change in the Attention Modulation Index (AMI). Face recognition is not affected by sleep deprivation.\n\nSleep deprivation has been shown to negatively affect picture classification speed and accuracy, as well as recognition memory. It results in an inability to avoid attending to irrelevant information displayed during attention-related tasks. (Norton) It also decreases activation in the ventral visual area and the frontal parietal control regions.\n\nStudies involving sleep deprived subjects’ performance on choice reaction time tests, in which response inhibition, task shifting skill and task strategy were involved, have been conducted and analyzed. These three cognitive processes are involved and critical in tasks involving supervisory attention, which is defined as behaviour that arises through the selection and implementation of schemas. Following one night of total sleep deprivation, subjects showed no decline in task shifting or response inhibition performance. However, sleep deprivation does affect the ability to use preparatory bias to increase performance speed. It is suggested that the brain’s cognitive resources make an active effort to succeed in a challenging task when subjected to sleep deprivation, and that this deficit becomes apparent in tasks involving preparatory bias.\n\nDeficits in cognitive performance due to continuous sleep restriction are not well understood. However, there have been studies looking into physiological arousal of the sleep-deprived brain. Participants, whose total amount of sleep had been restricted by 33% throughout one week, were subjected to reaction time tests. The results of these tests were analyzed using quantitative EEG analysis. The results indicate that the frontal regions of the brain are first to be affected, whereas the parietal regions remain active until the effects of sleep deprivation become more severe, which occurred towards the end of the week. In addition, EEG and ERP analysis reveals that activation deficits are more apparent in the non-dominant hemisphere than in the dominant hemisphere.\n\nThe effects of sleep deprivation on cognitive performance have been studied through the use of parametric visual attention tasks. Functional magnetic resonance imaging of participants' brains who were involved in ball-tracking tasks of various difficulty levels were obtained. These images were taken during rested wakefulness and again after one night of sleep deprivation. The thalamus is more highly activated when accompanied by sleep deprivation than when the subject is in a state of rested wakefulness. Contrarily, the thalamus is more highly activated during difficult tasks accompanied by rested wakefulness, but not during a state of sleep deprivation. Researchers propose that the thalamic resources, which are normally activated during difficult tasks, are being activated in an attempt to maintain alertness during states of sleep deprivation. In addition, an increase in thalamic activation is related to a decrease in the parietal, prefrontal and cingulate cortex activation, resulting in the overall impairment of attentional networks, which are necessary for visuospatial attention performance.\n\nFunctional Magnetic Resonance Imaging (fMRI) studies indicate that the posterior cingulate (PCC) and medial prefrontal cortex are involved in the anticipatory allocation of spatial attention. When sleep-deprived, PCC activity decreases, impairing selective attention. Subjects were exposed to an attention-shifting task involving spatially informative, misleading and uninformative cues preceding the stimuli. When sleep-deprived, subjects showed increased activation in the left intraparietal sulcus. This region is activated when exposed to stimuli in unexpected locations. These findings suggest that sleep-deprived individuals may be impaired in their ability to anticipate the locations of upcoming events. In addition, inability to avoid attending to irrelevant events may also be induced by sleep-deprivation.\n\nBy contrast, other studies have indicated that the effects of sleep deprivation on cognitive performance, specifically, sustained visual attention, are more global and bilateral in nature, as opposed to more lateralized deficit explanations. In a study utilizing the Choice Visual Perception Task, subjects were exposed to stimuli appearing in various locations in visual space. Results indicate that sleep deprivation results in a general decline in visual attention. It is also suggested that the sleep-deprived brain is able to maintain a certain level of cognitive performance during tasks requiring divided attention by recruiting additional cortical regions that are not normally used for such tasks.\n\nExecutive functioning is \"the ability to plan and coordinate a willful action in the face of alternatives, to monitor and update action as necessary and suppress distracting material by focusing attention on the task at hand\". The prefrontal cortex has been identified as the most important region involved in executive functioning.\n\nResearchers believe that three of the most 'basic' executive functions are: shifting, updating, and inhibition. Shifting back and forth between different tasks is considered a very important mental behavior involved in executive functioning as it involves active disengagement from the present task and engaging in a new task. Updating is believed to be involved in working memory (closely associated with the function of the prefrontal cortex), where the information that is active needs to be updated by replacing old, now irrelevant information with new, relevant information based on the objective. Inhibition involves controlled and deliberate impedance of automatic, predominant responses.\n\nThe anterior cingulate cortex has been implemented in the process of inhibiting a habitual response or detecting possible conflicts in responses; this is shown by the Stroop test. Studies have found that as little as 36 hours of sleep deprivation cause a performance reduction in tasks requiring these executive functions.\nThe processes above illustrate a model of controlled versus automatic behavior that was hypothesized by Shallice et al. (1989), called the supervisory attentional system. This system is believed to come into play when intervention of habitual response is necessary. Damage to the prefrontal cortex will cause a breakdown in this system, resulting in utilization behaviors. These behaviors would include spontaneous sequences of action on irrelevant objects in the surroundings with no clear goal in mind. This theory has helped to extend the knowledge we now have on executive functions.\n\nDecision making involves a range of executive functions that need to be combined and organized in order to respond in the most appropriate manner, i.e. respond with the most advantageous decision. There are many aspects to the process of decision making, including those discussed above. Other processes involved that correlate to executive function will be discussed below.\n\nWhile most important decisions are made over a longer period of time involving more in-depth cognitive analysis, usually we have limited time in which to assimilate a large amount of information into an informed decision. Lack of sleep appears to negatively affect our ability to appreciate and respond to increasing complexity, as was found in performance deficits after 1 night of sleep deprivation on a simulated marketing game.\n\nThe game involved subjects promoting a fictional product while getting feedback on the financial effects of their decisions. They would continuously have to take into account new variables to succeed which would increase the game's complexity.\n\nOther examples of inability to process complex information includes a decrease in ability to assess facial expressions, an increase in resolving to the use of stereotypes and racial biases in evaluations, and an increase in taking the easier solution to solving interpersonal problems.\n\nIntuitively, because sleep deprivation had a negative effect on handling the complexity of the simulated marketing game, it also affected innovative processes as subjects failed to adopt a more innovative (and rewarding) style of play. Innovative thinking involves the construction of new behaviors based on the assimilation of continuously changing or novel information. In a study of military personnel who had undergone two nights of sleep deprivation, results showed marked reductions in the ability to generate ideas about a given topic (categories test); this is known as ideational fluency.\n\nRisk versus reward analysis is an important part of decision making. Attempting to create a representation and response to a risky situation highly involves the orbitofrontal cortex. In a study that involved risk taking analysis of drivers who had been driving for 12 hours straight, it was found that they were more willing to make hazardous maneuvers and were reluctant to adopt any form of a cautious driving style.\n\nSome studies shed further light on this phenomenon. One study used real life decision making scenarios involving choosing cards from 1 of 4 decks of cards. Different cards were considered as a reward while the others were a penalty. The sleep deprived subjects failed to alter their selection methods, continuing to choose cards from decks that were producing a high amount of penalty cards, whereas the control subjects were able to change their choosing strategy by a cost-benefit analysis based on monitoring the outcomes they were getting as they went along.\n\nThe process of planning would be done congruently with decision making in determining the outcome behavior. As has been shown so far, sleep deprivation has many detrimental effects on executive functions and planning is not spared. One study involved cadets who were required to complete simulated military operations under sleep deprived conditions. Results showed a decrease in the subjects ability to 'plan on the fly' and overall outcomes were less than those for well rested cadets.\n\nAnother psychological test used to assess planning and decision making is the Tower of London test. This test has been widely used in the testing of executive functions as well as studies of sleep deprived subjects. In a study examining performance on this test after 45–50 hours of sleep deprivation, it was found that the sleep deprived subjects not only took longer, but required more moves to complete the task than did the controls.\n\nBeing able to show insight into one's performance on different tasks and correct or modify those behaviors if they are incorrect is an important part of executive functioning. The problems that could be associated with being unable to learn from a mistake or adapt to a mistake could impair many behaviors.\n\nA common test used to assess error correction and trouble shooting with regards to the frontal lobe is the Wisconsin Card Sorting Test. This test involves a change in the rules which requires a shift in strategy. In the same study discussed above, detriments were also found on this task in the sleep deprived individuals.\n\nResearch evidence suggests that sleep is involved in the acquisition, maintenance and retrieval of memories as well as memory consolidation. Subsequently, sleep deprivation has been shown to affect both working memory and long-term memory processes.\n\nSleep deprivation increases the number of errors made on working memory tasks. In one study, the working memory task involved illuminating a sequence of 3 or 4 coloured lights, then asking both sleep deprived and non-sleep deprived individuals to memorize and repeat back the sequence. The sleep deprived performed the task much faster than those in the control condition (i.e., not sleep deprived), which initially appeared to be a positive effect. However, there was a significant difference in the number of errors made, with the fatigued group performing much worse.\n\nEvidence from imaging studies also demonstrates the influence of sleep deprivation on working memory. EEG studies have documented lower accuracy and slower reaction times among sleep deprived participants performing working memory tasks. Decreasing alertness and lack of focus triggered deficits in working memory that are accompanied by significant degradation of event-related potentials.\n\nPET scans shows global decrease in glucose metabolism in response to sleep deprivation. As subjects become increasingly impaired on working memory tasks, a more specific decrease of glucose occurs in the thalamus, prefrontal cortex and posterior parietal cortex.\n\nfMRI scans following brief sleep deprivation (24 hours or less) show increases in thalamic activation. Verbal working memory tasks normally cause increases in left temporal lobe activity. However, after 35 hours of deprivation, there are noted decreases in temporal lobe activation and increases in parietal lobe activation.\n\nThe working memory span is also affected by sleep deprivation. When sleep deprived participants in a study were asked to remember a nonsense word and locate it among a number of similar words, the length of time they could hold it in their working memory decreased by 38% compared to rested individuals.\n\nOne way sleep is involved in the creation of long-term memories is through memory consolidation, which is the process by which a new memory is changed into a more permanent form. This is believed to be accomplished by creating connections between the medial temporal lobes and neocortical areas. NREM (non-REM) and REM sleep are both believed to be implicated, with current theories suggesting NREM is most particularly involved in procedural memory and REM with declarative memory.\n\nAnimal studies have partly validated these claims. For instance, one study conducted with rats showed that REM sleep deprivation after learning a new task disrupted their ability to perform the task again later. This was especially true if the task was complex (i.e., involved using unusual information or developing novel adaptive behaviours).\n\nThere is similar evidence for the role of sleep in procedural memory in humans. Participants in one study were trained on a procedural memory skill involving perceptual-motor skills. Those who were NREM sleep deprived performed significantly worse on subsequent trials compared to those who were fully rested. Another study using a visuo-motor procedural memory task documented similar results. Participants who were sleep deprived following the initial training showed no improvement on trials the next day, while those who received sleep showed significant positive changes.\nStudies such as these clearly demonstrate the disruptive influence sleep deprivation has on memory consolidation of procedural and declarative memories.\n\nSleep deprivation also has a documented effect on the ability to acquire new memories for subsequent consolidation. A study done on mice that were sleep deprived before learning a new skill but allowed to rest afterward displayed a similar number of errors on later trials as the mice that were sleep deprived only after the initial learning. In this case, it is hypothesized that rather than preventing the memory from being consolidated, sleep deprivation interfered with the initial acquisition of the memory. Mice with pre-trial sleep deprivation also took significantly longer to learn a task than well-rested mice.\n\nSleep deprivation is also implicated in impaired ability to retrieve stored long-term memories. When an aversive stimulus was included in a trial (i.e., a blowdryer would blast hot air and noise at a mouse), mice that were sleep deprived were less anxious on subsequent trials. This suggests they had not retrieved all of the memory related to the unpleasant experience.\n\nSeveral theories have been put forth to explain the effect sleep deprivation has on memory.\n\nOne early study into neurochemical influences on sleep and memory was conducted with cats and demonstrated that sleep deprivation increased brain protein synthesis. There is evidence that these altered levels of proteins could increase the excitability of the central nervous system, thus increasing the susceptibility of the brain to other neurochemical agents that can cause amnesia. Further research has revealed that the protein kinase A (PKA) signalling pathway is crucial to long-term memory. If PKA or protein synthesis inhibition occurs at certain moments during sleep, memory consolidation can be disrupted. In addition, mice with genetic inhibition of PKA have been shown to have long-term memory deficits. Thus, sleep deprivation may act through the inhibition of these protein synthesis pathways.\n\nAcetylcholine (ACh) may also be involved in the effects of sleep deprivation, particularly with regards to spatial memory. Muscarinic antagonists, or chemicals that block ACh, impair spatial learning when administered prior to a training task among rats. ACh levels are also found to be lower when measured following a period of sleep deprivation. ACh has also been shown to increase the activity of the PKA pathway, which is needed for memory consolidation.\n\nSerotonin levels (in the form of 5-HT) have been shown to decrease during REM and NREM sleep, leading some researchers to believe that it is also involved in memory consolidation during sleep. Mice lacking the receptor gene for 5-HT engage in more REM sleep and perform better on spatial memory tasks. Researchers have hypothesized that sleep deprivation interferes with the normal reduction in levels of 5-HT, impairing the process of memory consolidation.\n\nAnother theory suggests that the stress brought on by sleep deprivation affects memory consolidation by changing the concentration of corticosteroids in the body. This was simulated in one study by elevating the concentration of glucocorticoids during early sleep stages. The observed effects on memory retention the next day were similar to those obtained from individuals who had received no sleep.\n\nSleep deprivation may affect memory by interfering with neuroplasticity as measured by long-term potentiation in the hippocampus. This reduced plasticity may be the root cause of impairments in both working memory among humans and spatial memory among rats. Sleep deprivation may additionally affect memory by reducing the proliferation of cells in the hippocampus.\n\nSleep deprivation has also been associated with decreased overall membrane excitability of neurons in the brain. Activation of these membranes is critical for the formation of memories. Mitochondria play an essential role in modulating neuron excitability, and research has shown that sleep deprivation is involved in inhibiting mitochondrial metabolism.\n\nReduced duration of sleep, as well as an increase in time spent awake, are factors that highly contribute to the risk of traffic collisions, the severity and fatality rates of which are on the same level as driving under the influence of alcohol, with 19 hours of wakefulness corresponding to a BAC of 0.05%, and 24 hours of wakefulness corresponding to a BAC of 0.10%. Compounding this issue is the proven dissociation between objective performance and subjective alertness; individuals vastly underestimate the effect that sleep deprivation has on their cognitive performance, particularly during the circadian night. Much of the effect of acute sleep deprivation can be countered by napping, with longer naps giving more benefit than shorter naps. Some industries, particularly the Fire Service, have traditionally allowed workers to sleep while on duty, between calls for service. In one study of EMS providers, 24-hour shifts were not associated with a higher frequency of negative safety outcomes when compared to shorter shifts.\n\nThis is especially relevant for young adults as they require 8–9 hours of sleep at night to overcome excessive daytime sleepiness and are among the highest risk group for driving while feeling tired and sleep deprivation related crashes.\n\n"}
{"id": "5639702", "url": "https://en.wikipedia.org/wiki?curid=5639702", "title": "Environment, health and safety", "text": "Environment, health and safety\n\nEnvironment, health and safety (EHS) is a discipline and specialty that studies and implements practical aspects of environmental protection and safety at work. In simple terms it is what organizations must do to make sure that their activities do not cause harm to anyone.\n\nRegulatory requirements play an important role in EHS discipline and EHS managers must identify and understand relevant EHS regulations, the implications of which must be communicated to executive management so the company can implement suitable measures. Organisations based in the United States are subject to EHS regulations in the Code of Federal Regulations, particularly CFR 29, 40, and 49. Still, EHS management is not limited to legal compliance and companies should be encouraged to do more than is required by law, if appropriate.\n\nFrom a health & safety standpoint, it involves creating organized efforts and procedures for identifying workplace hazards and reducing accidents and exposure to harmful situations and substances. It also includes training of personnel in accident prevention, accident response, emergency preparedness, and use of protective clothing and equipment.\n\nFrom an environmental standpoint, it involves creating a systematic approach to complying with environmental regulations, such as managing waste or air emissions all the way to helping site's reduce the company's carbon footprint.\n\nSuccessful HSE programs also include measures to address ergonomics, air quality, and other aspects of workplace safety that could affect the health and well-being of employees and the overall community.\n\nHSE goes by a number of acronyms which may exclude environment or include security and quality.\n\nEHS guidelines cover categories specific to each industry as wells as those that are general to most industry sectors. Examples of general categories and subcategories are:\n\nThe chemical industry introduced the first formal EHS management approach in 1985 as a reaction to several catastrophic accidents (like the Seveso disaster of July 1976 and the Bhopal disaster of December 1984). This worldwide voluntary initiative, called \"Responsible Care\", started by the Chemistry Industry Association of Canada (formerly the Canadian Chemical Producers' Association - CCPA), operates in about 50 countries, with central coordination provided by the International Council of Chemical Associations (ICCA). It involves eight fundamental features which ensure plant and product safety, occupational health and environmental protection, but which also try to demonstrate by image-building campaigns that the chemical industry acts in a responsible manner. Being an initiative of the ICCA, it is restricted to the chemical industry.\n\nSince the 1990s, general approaches to EHS management that may fit any type of organisation have appeared in international standards such as:\n\n\nIn 1998 the International Finance Corporation established EHS guidelines.\n\nAs a typical example, the activities of a health, safety and environment (HSE) working group might focus on:\n\n\n\n"}
{"id": "44632395", "url": "https://en.wikipedia.org/wiki?curid=44632395", "title": "European Union Statistics on Income and Living Conditions", "text": "European Union Statistics on Income and Living Conditions\n\nThe European Union Statistics on Income and Living Conditions, (also known as \"European Union Survey on Income and Living Conditions\") abbreviated EU-SILC, is a survey department of the EU.\nIt replaced in 2004 the \"European Community Household Panel\" (ECHP), which covered the years 1994-2003. The EU-SILC includes only minor changes relative to its predecessor the ECHP; most importantly, it significantly expands the number of countries included in the sample.\n\n"}
{"id": "2370329", "url": "https://en.wikipedia.org/wiki?curid=2370329", "title": "Euthenics", "text": "Euthenics\n\nEuthenics is the study of the improvement of human functioning and well-being by improvement of living conditions. Affecting the \"improvement\" through altering external factors such as education and the controllable environment, including the prevention and removal of contagious disease and parasites, environmentalism, education regarding employment, home economics, sanitation, and housing.\n\nRose Field notes of the definition in a May 23, 1926 \"New York Times\" article, \"the simplest being efficient living\". A right to environment.\n\nThe Flynn effect has been often cited as an example of euthenics. Another example is the steady increase in body size in industrialized countries since the beginning of the 20th century.\n\nEuthenics is not normally interpreted to have anything to do with changing the composition of the human gene pool by definition, although everything that affects society has some effect on who reproduces and who does not.\n\nThe term was derived in the late 19th century from the Greek verb \"eutheneo\", εὐθηνέω (\"eu\", well; the, root of τίθημι \"tithemi\", to cause).\n\nAlso from the Greek Euthenia, Εὐθηνία. Good state of the body: prosperity, good fortune, abundance.—\"Herodotus\".\n\nThe opposite of Euthenia is Penia, Πενία (\"deficiency\" or \"poverty\") the personification of poverty and need.\n\nEllen H. Swallow Richards (1842–1911; Vassar Class of '70) was one of the first writers to use the term, in \"The Cost of Shelter\" (1905), with the meaning \"the science of better living\". It is unclear if (and probably unlikely that) any of the study programs of euthenics ever completely embraced Richards' multidisciplinary concept, though several nuances remain today, especially that of interdisciplinarity.\n\nAfter Richards' death in 1911, Julia Lathrop (1858–1932; VC '80)—one of Vassar's most distinguished alumnae—continued to promote the development of an interdisciplinary program in euthenics at the college. Lathrop soon teamed with alumna Minnie Cumnock Blodgett (1862–1931; VC '84), who with her husband, John Wood Blodgett, offered financial support to create a program of euthenics at Vassar College. Curriculum planning, suggested by Vassar president Henry Noble MacCracken in 1922, began in earnest by 1923, under the direction of Professor Annie Louise Macleod (Chemistry; First woman PhD, McGill University, 1910).\n\nAccording to Vassar's chronology entry for March 17, 1924, \"the faculty recognized euthenics as a satisfactory field for sequential study (major). A Division of Euthenics was authorized to offer a multidisciplinary program [radical at the time] focusing the techniques and disciplines of the arts, sciences and social sciences on the life experiences and relationships of women. Students in euthenics could take courses in horticulture, food chemistry, sociology and statistics, education, child study, economics, economic geography, physiology, hygiene, public health, psychology and domestic architecture and furniture. With the new division came the first major in child study at an American liberal arts college.\"\n\nFor example, a typical major in child study in euthenics includes introductory psychology, laboratory psychology, applied psychology, child study and social psychology in the Department of Psychology; the three courses offered in the Department of Child Study; beginning economics, programs of social reorganization and the family in Economics; and in the Department of Physiology, human physiology, child hygiene, principles of public health.\n\nThe Vassar Summer Institute of Euthenics accepted its first students in June 1926. Created to supplement the controversial euthenics major which began February 21, 1925, it was also located in the new Minnie Cumnock Blodgett Hall of Euthenics (York & Sawyer, architects; ground broke October 25, 1925). Some Vassar faculty members (perhaps emotionally upset with being displaced on campus to make way, or otherwise politically motivated) contentiously \"believed the entire concept of euthenics was vague and counter-productive to women's progress.\"\n\nHaving overcome a lukewarm reception, Vassar College officially opened its Minnie Cumnock Blodgett Hall of Euthenics in 1929. Dr. Ruth Wheeler (Physiology and Nutrition – VC '99) took over as director of euthenics studies in 1924. Wheeler remained director until Mary Shattuck Fisher Langmuir (VC '20) succeeded her in 1944, until 1951.\n\nThe college continued for the 1934–35 academic year its successful cooperative housing experiment in three residence halls. Intended to help students meet their college costs by working in their residences. For example, in Main, students earned $40 a year by doing relatively light work such as cleaning their rooms.\n\nIn 1951, Katharine Blodgett Hadley (VC '20) donated $400,000, through the Rubicon Foundation, to Vassar to help fund operating deficits in the current and succeeding years and to improve faculty salaries.\n\n\"Discontinued for financial reasons, the Vassar Summer Institute for Family and Community Living, founded in 1926 as the Vassar Summer Institute of Euthenics, held its last session, July 2, 1958. This was the first and last session for the institute's new director, Dr. Mervin Freedman.\"\n\nElmira College is noted as the oldest college still in existence which (as a college for women) granted degrees to women which were the equivalent of those given to men (the first to do so was the now-defunct Mary Sharp College). Elmira College became coeducational in all of its programs in 1969.\n\nA special article was written in the December 12, 1937 \"New York Times\", quoting recent graduates of Elmira College, urging for courses in colleges for men on the care of children. Reporting that \"preparation for the greatest of all professions, that of motherhood and child-training, is being given the students at Elmira College in the Nursery School which is Conducted as part of the Department of Euthenics.\"\n\nElmira College was one of the first of the liberal arts colleges to recognize the fact that women should have some special training, integrated with the so-called liberal studies, which would prepare them to carry on, with less effort and fewer mistakes, a successful family life. Courses in nutrition, household economics, clothing selection, principles of foods and meal planning, child psychology, and education in family relations are a part of the curriculum.\n\nThe Elmira College nursery school for fifteen children between the ages of two and five years was opened primarily as a laboratory for college students, but it had become so popular with parents in the community that there was always a long waiting list.\n\nThe \"New York Times\" article notes how the nursery had become one of the essential laboratories of the college, where recent mothers testified to the value of the training they received while in college. \"Today,\" one graduate said, \"when it is often necessary for young women to continue professional work outside the home after marriage, it is important that young fathers, who must share in the actual care and training of the children, should have some knowledge of correct methods.\"\n\nMany factors led to the movement never getting the funding it needed to remain relevant, including: vigorous debate about the exact meaning of euthenics, a strong antifeminism movement paralleling even stronger women's rights movements, confusion with the term eugenics, the economic impact of the Great Depression and two world wars. These factors also prevented the discipline from gaining the attention it needed to put together a lasting, vastly multidisciplinary curriculum. Therefore, it split off into separate disciplines. Child Study is one such curriculum.\n\nMartin Heggestad of the Mann Library notes that \"Starting around 1920, however, home economists tended to move into other fields, such as nutrition and textiles, that offered more career opportunities, while health issues were dealt with more in the hard sciences and in the professions of nursing and public health. Also, improvements in public sanitation (for example, the wider availability of sewage systems and of food inspection) led to a decline in infectious diseases and thus a decreasing need for the largely household-based measures taught by home economists.\" Thus, the end of euthenics as originally defined by Ellen Swallow Richards ensued.\n\nAccording to Ellen Richards, in her book \"Euthenics: the science of controllable environment\" (1910):\n\nDebate over misconceptions about the movement started almost from the beginning. In his comparison \"Eugenics, Euthenics, And Eudemics\", (American Journal of Sociology, Vol. 18, No. 6, May 1913), Lester F. Ward of Brown University opens the second section regarding euthenics lamenting: \nWard later noted about the organic environment that:\nVassar historians note that \"critics faulted the new program as a weakening of science and a slide into vocationalism. The influential educator and historian of education, Abraham Flexner—one of the founders of the Princeton Institute for Advanced Study—attacked the program, along with other “ad hoc” innovations like intercollegiate athletics and student governments, in Universities, American, English, German (1930).\"\n\nIn the summer of 1926, Margaret Sanger created a stir when she gave a radio address, called \"Racial Betterment\", in the first Euthenics Institute, where she praised attempts to \"close our gates to the so-called 'undesirables'\" and proposed efforts to \"discourage or cut down on the rapid multiplication of the unfit and undesirable at home\", by government-subsidized voluntary sterilization. (from \"The Selected Papers of Margaret Sanger\", vol. 1 (2003), Esther Katz, ed.)\n\nEugenicist, Charles Benedict Davenport, noted in his article \"Euthenics and Eugenics,\" found reprinted in the \"Popular Science Monthly\" of January 1911, page 18, 20:\n\nIn a New York Times op-ed dated October 24, 1926, entitled \"Eugenics and euthenics\", in response to an op-ed entitled \"Bright Children Who Fail\" which appeared the previous October 15, student of child psychology, Joseph A. Krisses observes:\n\n\n\n"}
{"id": "25249846", "url": "https://en.wikipedia.org/wiki?curid=25249846", "title": "Functional beverage", "text": "Functional beverage\n\nA functional beverage is a drink typically intended to convey a health benefit. Some include ingredients like herbs, vitamins, minerals, nootropics, amino acids, or additional raw fruit or vegetables.\n\nExamples of functional beverages include sports and performance drinks, energy drinks, ready to drink (RTD) teas, smart drinks (shine+), enhanced fruit drinks, soy beverages, and enhanced water.\n\nFunctional beverages have become popular among people who want specific health benefits from their foods and beverages. Both convenience and health have been identified as important factors in consumers' decision-making about food and beverage purchases. Functional drinks are advertised as having various health benefits. For example, some claim to improve heart health, immunity, digestion, and joint health, while others promote themselves as satiating and energy-boosting.\n\nThe functional beverage industry is a subsector of the functional food and non-alcoholic beverage industry. It is the fastest-growing sector of the industry, partially due to the maturity of the carbonated soft drink sector and heavy investments by major food and beverage companies. Another reason for the industry's growth may be the consumer-oriented market scheme whereby innovative ideas come from consumers. By 2008, in the U.S., the market share of functional beverages accounted for 48.9% of the non-alcoholic industry, which is worth $118.3 billion.\n\nIn 2006, functional beverage consumption per capita rose to 66.4 gallons, while the carbonated soft drink sector saw a decline in per-capita consumption to 50.4 gallons (decreased from an average per-capita consumption of 192.5 gallons in 2006).\n\nFunctional beverage industry players are generally categorized into four types: \n\nThe functional beverage industry encompasses a wide range of varieties targeting different health-related concerns. One trend has been toward hybrid drinks, which are marketed as having benefits like thirst-quenching ability, with daily dosages of vitamins or other nutrients. Another trend is the rise of probiotics, exemplified by Activia yogurt, marketed for intestinal and immune health. Other beverages, like , a carambola punch energy drink in the Function Drinks line, advertise improved memory and mental sharpness. Functional drinks marketed to children have also been developed, and received attention with Nestlé's Boost.\n\nA 2005 trend toward single-serve functional beverages was fueled by consumers' demands for convenience. According to Campbell's director of single-serve beverages, \"People know they will be seen when they are drinking single-serving beverages, so the package is critical.\" Drinks marketed toward weight loss, health, and beauty (like Nova the Essential Drink) account for a considerable market share. Lastly, \"energy-boosting\" functional beverage products, such as Red Bull and 5-Hour Energy, have been rated fastest in growth in the functional beverage market.\n\nThe functional beverage industry generally competes using four primary strategies:\n\nMarket segments of the functional beverage industry are divided mainly into four parts. Those include hydration; energy/rejuvenation; health and wellness; and weight management. Each segment has its own target market and consumers. Overlapping of target consumers does occur—not because of undefined market needs, but due to consumer acceptance of functional beverages.\n\nRecently, there has been an increase in the promotion of hydration drinks. Fowhich it marketed as an exclusive hydration drink, sold only in Neiman Marcus stores. Among the drink's ingredient list were antioxidant vitamins and fruit extracts, which the company claims \"hydrate the inner and outer layer of the skin\" and protect drinkers from free radicals.\n\nGatorade has also created several drinks marketed as hydration beverages with various health benefits. Its \"Thirst Quencher\" drinks, according to advertisements, each contain an \"excellent source\" of various vitamins:\n\nIn tandem with these adjustments, a low-calorie version called G2 was also reformulated. According to Gatorade, G2 now also provides:\n\nGatorade's new products are a good example of the first major strategy of competition, listed in the \"Market\" section above. By reformulating its products, Gatorade's goals were to promote their own new products as healthier, and to emphasize the healthy ingredients in the drinks.\n\nHighly caffeinated, often highly sweetened \"energy drinks\" have become popular on the beverage market in the United States, as well as globally, in the past decade. Consumer demand has helped generate a new generation of \"energy drink\" brands containing similar amounts of caffeine, calories, and sugar.\n\nVarious stimulants found in energy drinks include taurine, glucoronolactone, caffeine, B vitamins, guarana, ginseng, ginkgo biloba, L-carnitine, sugars, antioxidants, yerba maté, creatine, and milk thistle. Although these ingredients have been approved by the FDA, health experts still recommend that consumers read their energy drinks' labels, as these ingredients may not improve health.\n\n\"Health-conscious\" individuals are among the target consumers of many functional beverage companies. To target these individuals, many companies have introduced functional beverages which contain less sugar, and therefore less calories. For example, VitaminWater 10 contains only 10 calories per serving (25 calories for a 351mL bottle, with 7.5 grams of sugar). An entire bottle also contains 250% of the RDI of vitamin C, and 25% of the RDI of vitamins B, B, B, and B. VitaminWater 10 lowered its calorie content by using an all-natural sweetener (stevia) extracted from the \"Stevia rebaudiana\" plant. This also makes it possible for the company to advertise VitaminWater 10 as \"natural\".\n\nWith increased worries about obesity and its implications on health, combined with consumer demand for convenience goods, consumer demand has increased for easy weight loss methods that can be easily integrated into daily lifestyles. Functional beverages are striving to market themselves as such, by adding ingredients that are claimed to promote weight loss.\n\nFor example:\n\nSome investigators report slimming actions induced by chlorogenic acids from green coffee, but further investigations need to be performed.\n\nAs of 2008, based on dollar sales, the most popular functional beverages, in order, were:\n\nAccording to a 2006 article, the functional beverage market's consumer group is characterized as well-educated females aged 35–55, belonging to the upper middle, middle, and lower middle classes. This is thought to result from this group's perceptions that functional drinks produce positive health beliefs, as well as their relatively high disposable income. A 2002 article stated that within the energy and stimulant drink sector, young adults aged 18 to 34 are considered to be the main target market, as evidenced by high consumption rates. However, due to constant changes in attitudes about different types of functional beverages, these target markets could change.\n\nHealth experts are concerned about the increased consumption and popularity of functional beverages. Although these beverages may serve to hydrate the individual, they may not mitigate or even address today's major health issues, such as obesity, heart disease, and cancer. Most functional beverages are sweetened, and consumption of sweetened beverages is associated with higher levels of obesity and heart disease. Most of these drinks contain significant amounts of sugars and hence calories, which would add to discretionary and total caloric intake. As such, these ingredients pose health risks because of what they contain (sugar and caffeine) or what they replace in the diet (vitamin and mineral-rich foods).\n\nAnother set of concerns is that some functional beverages contain ingredients that have not been sufficiently studied for health benefits, safety, and dosage. At the same time, many functional beverages have higher levels of a certain ingredient, like caffeine—which, when consumed in large amounts, is associated with heart disease and cancer. \n\nMany functional drinks have high levels of sugar, even if they have other \"healthy\" ingredients. For example, a 20oz bottle of Glacéau's VitaminWater has been reported to contain approximately 33 g of sugar, which is similar to the sugar content of a can of Coca-Cola. This prompted The Coca-Cola Company to be sued for claiming that VitaminWater was a healthy beverage.\n\nGiven their sugar content, many functional beverages may not be as healthy an alternative as other commonly consumed beverages. In addition, the sugar content of such beverages promotes dental cavities amongst frequent users.\n\nIn some functional beverages, particularly energy drinks, the caffeine content can be up to 141 milligrams per serving, more than an average 8-ounce cup of coffee containing 133 mg of caffeine. There have been reports to Health Canada of adverse reactions involving energy drinks.\n\n\n"}
{"id": "23504904", "url": "https://en.wikipedia.org/wiki?curid=23504904", "title": "Functional disorder", "text": "Functional disorder\n\nA functional disorder is a medical condition that impairs normal functioning of bodily processes that remains largely undetected under examination, dissection or even under a microscope. At the exterior, there is no appearance of abnormality. This stands in contrast to a structural disorder (in which some part of the body can be seen to be abnormal) or a psychosomatic disorder (in which symptoms are caused by psychological or psychiatric illness). Definitions vary somewhat between fields of medicine.\n\nGenerally, the mechanism that causes a functional disorder is unknown, poorly understood, or occasionally unimportant for treatment purposes. The brain or nerves are often believed to be involved. It is common that a person with one functional disorder will have others.\n\nWhether a given medical condition is termed a \"functional disorder\" depends in part on the state of knowledge. Some diseases, including epilepsy, schizophrenia, and migraine headaches were once considered functional disorders, but are no longer generally classified that way.\n\n\n"}
{"id": "1660162", "url": "https://en.wikipedia.org/wiki?curid=1660162", "title": "Group purchasing organization", "text": "Group purchasing organization\n\nIn the United States, a group purchasing organization (GPO) is an entity that is created to leverage the purchasing power of a group of businesses to obtain discounts from vendors based on the collective buying power of the GPO members.\n\nMany GPOs are funded by administrative fees that are paid by the vendors that GPOs oversee. Some GPOs are funded by fees paid by the buying members. Some GPOs are funded by a combination of both of these methods. These fees can be set as a percentage of the purchase or set as an annual flat rate. Some GPOs set mandatory participation levels for their members, while others are completely voluntary. Members participate based on their purchasing needs and their level of confidence in what should be competitive pricing negotiated by their GPOs.\n\nGroup purchasing is used in many industries to purchase raw materials and supplies, but it is common practice in the grocery industry, health care, electronics, industrial manufacturing and agricultural industries. As the electrical industry has started deregulating, it has also become an increasing trend there. Also, in recent years, group purchasing has begun to take root in the nonprofit community. Group purchasing amongst nonprofits is still relatively new, but is quickly becoming common place as nonprofits aim to find ways to reduce overhead expenses. In the healthcare field, GPOs have most commonly been accessed by acute-care organizations, but non-profit Community Clinics and Health Centers throughout the U.S. have also been engaging in group purchasing.\n\nThe first healthcare GPO was established in 1910 by the Hospital Bureau of New York. For many decades, healthcare GPOs grew slowly in number, to only 10 in 1962.\n\nMedicare and Medicaid stimulated growth in the number of GPOs to 40 in 1974. That number tripled between 1974 and 1977. The institution of the Medicare Prospective Payment System (PPS) in 1983 focused greater scrutiny on costs and fostered further rapid GPO expansion. In 1986, Congress granted GPOs in healthcare \"Safe Harbor\" from federal anti-kickback statutes after successful lobbying efforts.\nBy 2007, there were hundreds of healthcare GPOs, \"affiliates\" and cooperatives in the United States that were availing themselves of substantial revenues obtained from vendors in the form of administrative fees, or \"remuneration.\" 96 percent of all acute-care hospitals and 98 percent of all community hospitals held at least one GPO membership. Importantly, 97 percent of all not-for-profit, non-governmental hospitals participated in some form of group purchasing.\n\nWith healthcare costs rising sharply in the early 1980s, the federal government revised Medicare from a system of fee-for-service (FFS) payments to PPS, under which hospitals receive a fixed amount for each patient with a given diagnosis. Other insurers also limited what hospitals could charge. The result was a financial squeeze on hospitals, compelling them to seek new ways to manage their costs.\n\nIn specifically exempting GPOs from the Federal Anti-Kickback Law, many healthcare providers interpreted the act as an encouragement to the expansion of GPOs.\nCongress did not specify any limit on contract administration fees, but required the United States Department of Health and Human Services (HHS) to monitor such fees for possible abuse – particularly with respect to fees in excess of 3.0 percent.\n\nIn 1991, HHS promulgated safe harbor regulations, reflecting Congress’ intent to permit contract administration fees and creating the additional safeguard that GPOs inform members of administrative fees in excess of 3.0 percent. Despite these safeguards, the Government Accounting Office (GAO) published a study in 2002 indicating that GPOs did not always in fact reduce the cost of supplies and equipment for hospitals, but in some cases increased these costs by as much as 37%. Further examining the practices of GPOs, the Federal Trade Commission (FTC) clarified that \"safety zone thresholds do not prevent and should not be appropriately read as preventing antitrust challenges to any of the alleged anticompetitive contracting practices...\" of GPOs.\n\nIn 2002, the Senate Judiciary Committee's Antitrust Subcommittee imposed stricter standards on GPOs in healthcare, requiring the adoption of a Code of Conduct to which GPOs must subscribe.\n\nCritics of GPOs charge that, as long as GPOs receive fees from the vendors they are charged with policing, the industry has anti-competitive contracting potential that should be subjected to further scrutiny and/or regulation.\n\nA vertical GPO assists companies and organizations within a specific industry or market segment.\n\nA healthcare group purchasing organization (GPO) assists in promoting quality healthcare relief and assists diverse providers in effectively managing expenses. A GPO aggregates the purchasing volume of its members for various goods and services and develops contracts with suppliers through which members may buy at group price and terms if they choose to. GPOs typically provide contracted discounts on medical supplies, nutrition, pharmacy and laboratory. Some of the large GPOs have expanded contract portfolios to also offer discounts on office supplies and non-medical related services.\nA GPO's earnings come from an \"Administrative\" fee. GPOs may collect an \"Administrative\" fee up to 3.0% of all sales volumes from the vendors that they negotiate a contract from, upon selling products to their member hospitals. These fees do not influence the prices negotiated. They are used to cover the GPO's operating expenses. If there is a remainder it is distributed back to the GPO owners; thus, GPO owners achieve cost-savings on the goods they choose to buy through group contracts, and also receive distributions back from the GPO. General GPO members may also receive some fee share as part of their member agreement, though this practice is no longer typical; thus the primary benefit to a GPO member is the access to deeply discounted pricing.\n\nGPOs submit that their services allow for improved operating margins for healthcare providers, and that members enjoy value added benefits like clinical support, benchmarking data, supply chain support and comprehensive portfolios of products and services to address specific needs.\n\nGPOs vary in their strategy for negotiating discounts with suppliers - from requiring that its members not join other GPOs (exclusivity) to requiring compliance to awarding single source contracts. As the healthcare industry becomes saturated with GPOs, pricing is one way for GPOs to bring in new members or convince members of another GPO to switch.\n\nA foodservice or grocery GPO focuses exclusively on the $600 billion foodservice marketplace, including food and food-related purchasing for multi-unit foodservice operators, contract negotiation and supply chain services. \nThese negotiations are made with supplier/manufacturing agreements and purchasing contracts. Categories for grocer purchases include: poultry, fresh produce, frozen food products, fresh and frozen meats, candy and snacks, dairy and bakery, dry goods, disposables and beverages.\n\nA manufacturer’s GPO succeeds in solving procurement and sourcing concerns by aggregating the demand for products and services used in the manufacturing and production process and delivering deep savings on raw materials, services and components by issuing rebates, discounts, and preferred pricing to its members. \nThe combined buying power helps manufacturers save money on their purchases and more effectively compete against the largest global manufacturers.\n\nA GPO in the electrical industry attempts to help energy consumers and businesses solve issues with the rising cost of electricity by pooling the electrical demand of multiple consumers, grouping them together, and through this, delivering savings to its members by obtaining more favorable electrical rates. The difference between residential and commercial rates can be quite large in many regions, and are at least 7% cheaper in every US state. The buying power of the resulting groups allows companies and individuals alike to save greatly on their electrical bills.\n\nNon-profit GPOs are similar to regular GPOs, but they are typically made of up a board of non-profit, community-based organizations working together to help lower their costs.\n\nWhereas a vertical GPO assists organizations in specific industries, such as health care, food service, legal, dairy, and industrial manufacturing, a horizontal GPO assists companies across a broad spectrum of industries.\n\nA horizontal indirect spend GPO succeeds in reducing procurement costs by aggregating the demand for non-strategic, or indirect cost supplies and services used by a broad horizontal market spectrum of member client organizations by consolidating purchasing power and establishing contracts to achieve preferred pricing, terms, and service standards.\n\nThe resulting combined buying power helps the usually mid-size and larger member client organizations save money on their purchases of categories such as temporary labor services, office products, safety supplies, office equipment, packaging supplies, uniform & laundry services, pest control, and expedited parcel delivery. The consolidation of purchasing effort with the GPO also allows member clients savings from reduced staffing overhead costs. According to an analysis by SpendMatters, from an adoption perspective, 15-20% of the Fortune 1000 currently use buying consortiums and 85% of the time, they’re seeing 10%+ of savings in the categories in which they put through a consortium model.\n\nThe suppliers to this type of GPO offer preferred pricing, terms, and service standards because they experience lower overall selling expenses and the increased volume usually associated with the addition of a single very large customer.\n\n\n"}
{"id": "80381", "url": "https://en.wikipedia.org/wiki?curid=80381", "title": "Health", "text": "Health\n\nHealth, as defined by the World Health Organization (WHO), is \"a state of complete physical, mental and social well-being and not merely the absence of disease or infirmity.\" This definition has been subject to controversy, as it may have limited value for implementation. Health may be defined as the ability to adapt and manage physical, mental and social challenges throughout life. \n\nThe meaning of health has evolved over time. In keeping with the biomedical perspective, early definitions of health focused on the theme of the body's ability to function; health was seen as a state of normal function that could be disrupted from time to time by disease. An example of such a definition of health is: \"a state characterized by anatomic, physiologic, and psychological integrity; ability to perform personally valued family, work, and community roles; ability to deal with physical, biological, psychological, and social stress\". Then in 1948, in a radical departure from previous definitions, the World Health Organization (WHO) proposed a definition that aimed higher: linking health to well-being, in terms of \"physical, mental, and social well-being, and not merely the absence of disease and infirmity\". Although this definition was welcomed by some as being innovative, it was also criticized as being vague, excessively broad and was not construed as measurable. For a long time, it was set aside as an impractical ideal and most discussions of health returned to the practicality of the biomedical model.\n\nJust as there was a shift from viewing disease as a state to thinking of it as a process, the same shift happened in definitions of health. Again, the WHO played a leading role when it fostered the development of the health promotion movement in the 1980s. This brought in a new conception of health, not as a state, but in dynamic terms of resiliency, in other words, as \"a resource for living\". 1984 WHO revised the definition of health defined it as \"the extent to which an individual or group is able to realize aspirations and satisfy needs and to change or cope with the environment. Health is a resource for everyday life, not the objective of living; it is a positive concept, emphasizing social and personal resources, as well as physical capacities\". Thus, health referred to the ability to maintain homeostasis and recover from insults. Mental, intellectual, emotional and social health referred to a person's ability to handle stress, to acquire skills, to maintain relationships, all of which form resources for resiliency and independent living. This opens up many possibilities for health to be taught, strengthened and learned. \n\nSince the late 1970s, the federal Healthy People Initiative has been a visible component of the United States’ approach to improving population health. In each decade, a new version of Healthy People is issued, featuring updated goals and identifying topic areas and quantifiable objectives for health improvement during the succeeding ten years, with assessment at that point of progress or lack thereof. Progress has been limited to many objectives, leading to concerns about the effectiveness of Healthy People in shaping outcomes in the context of a decentralized and uncoordinated US health system. Healthy People 2020 gives more prominence to health promotion and preventive approaches and adds a substantive focus on the importance of addressing social determinants of health. A new expanded digital interface facilitates use and dissemination rather than bulky printed books as produced in the past. The impact of these changes to Healthy People will be determined in the coming years.\n\nSystematic activities to prevent or cure health problems and promote good health in humans are undertaken by health care providers. Applications with regard to animal health are covered by the veterinary sciences. The term \"healthy\" is also widely used in the context of many types of non-living organizations and their impacts for the benefit of humans, such as in the sense of healthy communities, healthy cities or healthy environments. In addition to health care interventions and a person's surroundings, a number of other factors are known to influence the health status of individuals, including their background, lifestyle, and economic, social conditions and spirituality; these are referred to as \"determinants of health.\" Studies have shown that high levels of stress can affect human health.\n\nIn the first decade of the 21st century, the conceptualization of health as an ability opened the door for self-assessments to become the main indicators to judge the performance of efforts aimed at improving human health. It also created the opportunity for every person to feel healthy, even in the presence of multiple chronic diseases, or a terminal condition, and for the re-examination of determinants of health, away from the traditional approach that focuses on the reduction of the prevalence of diseases.\n\nGenerally, the context in which an individual lives is of great importance for both his health status and quality of their life. It is increasingly recognized that health is maintained and improved not only through the advancement and application of health science, but also through the efforts and intelligent lifestyle choices of the individual and society. According to the World Health Organization, the main determinants of health include the social and economic environment, the physical environment and the person's individual characteristics and behaviors.\n\nMore specifically, key factors that have been found to influence whether people are healthy or unhealthy include the following:\n\nAn increasing number of studies and reports from different organizations and contexts examine the linkages between health and different factors, including lifestyles, environments, health care organization and health policy, one specific health policy brought into many countries in recent years was the introduction of the sugar tax. Beverage taxes came into light with increasing concerns about obesity, particularly among youth. Sugar-sweetened beverages have become a target of anti-obesity initiatives with increasing evidence of their link to obesity.– such as the 1974 Lalonde report from Canada; the Alameda County Study in California; and the series of World Health Reports of the World Health Organization, which focuses on global health issues including access to health care and improving public health outcomes, especially in developing countries.\n\nThe concept of the \"health field,\" as distinct from medical care, emerged from the Lalonde report from Canada. The report identified three interdependent fields as key determinants of an individual's health. These are:\n\nThe maintenance and promotion of health is achieved through different combination of physical, mental, and social well-being, together sometimes referred to as the \"health triangle.\" The WHO's 1986 \"Ottawa Charter for Health Promotion\" further stated that health is not just a state, but also \"a resource for everyday life, not the objective of living. Health is a positive concept emphasizing social and personal resources, as well as physical capacities.\"\n\nFocusing more on lifestyle issues and their relationships with functional health, data from the Alameda County Study suggested that people can improve their health via exercise, enough sleep, maintaining a healthy body weight, limiting alcohol use, and avoiding smoking. Health and illness can co-exist, as even people with multiple chronic diseases or terminal illnesses can consider themselves healthy.\n\nThe environment is often cited as an important factor influencing the health status of individuals. This includes characteristics of the natural environment, the built environment and the social environment. Factors such as clean water and air, adequate housing, and safe communities and roads all have been found to contribute to good health, especially to the health of infants and children. Some studies have shown that a lack of neighborhood recreational spaces including natural environment leads to lower levels of personal satisfaction and higher levels of obesity, linked to lower overall health and well being. This suggests that the positive health benefits of natural space in urban neighborhoods should be taken into account in public policy and land use.\n\nGenetics, or inherited traits from parents, also play a role in determining the health status of individuals and populations. This can encompass both the predisposition to certain diseases and health conditions, as well as the habits and behaviors individuals develop through the lifestyle of their families. For example, genetics may play a role in the manner in which people cope with stress, either mental, emotional or physical. For example, obesity is a significant problem in the United States that contributes to bad mental health and causes stress in the lives of great numbers of people. (One difficulty is the issue raised by the debate over the relative strengths of genetics and other factors; interactions between genetics and environment may be of particular importance.)\n\nA number of types of health issues are common around the globe. Disease is one of the most common. According to GlobalIssues.org, approximately 36 million people die each year from non-communicable (not contagious) disease including cardiovascular disease, cancer, diabetes and chronic lung disease (Shah, 2014).\n\nAmong communicable diseases, both viral and bacterial, AIDS/HIV, tuberculosis, and malaria are the most common, causing millions of deaths every year (Shah, 2014).\n\nAnother health issue that causes death or contributes to other health problems is malnutrition, especially among children. One of the groups malnutrition affects most is young children. Approximately 7.5 million children under the age of 5 die from malnutrition, usually brought on by not having the money to find or make food (Shah, 2014).\n\nBodily injuries are also a common health issue worldwide. These injuries, including broken bones, fractures, and burns can reduce a person's quality of life or can cause fatalities including infections that resulted from the injury or the severity injury in general (Moffett, 2013).\n\nLifestyle choices are contributing factors to poor health in many cases. These include smoking cigarettes, and can also include a poor diet, whether it is overeating or an overly constrictive diet. Inactivity can also contribute to health issues and also a lack of sleep, excessive alcohol consumption, and neglect of oral hygiene (Moffett2013).There are also genetic disorders that are inherited by the person and can vary in how much they affect the person and when they surface (Moffett, 2013).\n\nThough the majority of these health issues are preventable, a major contributor to global ill health is the fact that approximately 1 billion people lack access to health care systems (Shah, 2014). Arguably, the most common and harmful health issue is that a great many people do not have access to quality remedies.\n\nThe World Health Organization describes mental health as \"a state of well-being in which the individual realizes his or her own abilities, can cope with the normal stresses of life, can work productively and fruitfully, and is able to make a contribution to his or her community\". Mental Health is not just the absence of mental illness.\n\nMental illness is described as 'the spectrum of cognitive, emotional, and behavioral conditions that interfere with social and emotional well-being and the lives and productivity of people. Having a mental illness can seriously impair, temporarily or permanently, the mental functioning of a person. Other terms include: 'mental health problem', 'illness', 'disorder', 'dysfunction'.\n\nRoughly a quarter of all adults 18 and over in the US are considered diagnosable with mental illness. Mental illnesses are the leading cause of disability in the US and Canada. Examples include, schizophrenia, ADHD, major depressive disorder, bipolar disorder, anxiety disorder, post-traumatic stress disorder and autism.\n\nMany teens suffer from mental health issues in response to the pressures of society and social problems they encounter. Some of the key mental health issues seen in teens are: depression, eating disorders, and drug abuse. There are many ways to prevent these health issues from occurring such as communicating well with a teen suffering from mental health issues. Mental health can be treated and be attentive to teens' behavior. \n\n Many factors contribute to mental health problems, including:\n\nAchieving and maintaining health is an ongoing process, shaped by both the evolution of health care knowledge and practices as well as personal strategies and organized interventions for staying healthy.\n\nAn important way to maintain your personal health is to have a healthy diet. A healthy diet includes a variety of plant-based and animal-based foods that provide nutrients to your body. Such nutrients give you energy and keep your body running. Nutrients help build and strengthen bones, muscles, and tendons and also regulate body processes (i.e. blood pressure). The food guide pyramid is a pyramid-shaped guide of healthy foods divided into sections. Each section shows the recommended intake for each food group (i.e. Protein, Fat, Carbohydrates, and Sugars). Making healthy food choices is important because it can lower your risk of heart disease, developing some types of cancer, and it will contribute to maintaining a healthy weight.\n\nThe Mediterranean diet is commonly associated with health-promoting effects due to the fact that it contains some bioactive compounds like phenolic compounds, isoprenoids and alkaloids.\n\nPhysical exercise enhances or maintains physical fitness and overall health and wellness. It strengthens muscles and improves the cardiovascular system. According to the National Institutes of Health, there are four types of exercise: endurance, strength, flexibility, and balance.\n\nSleep is an essential component to maintaining health. In children, sleep is also vital for growth and development. Ongoing sleep deprivation has been linked to an increased risk for some chronic health problems. In addition, sleep deprivation has been shown to correlate with both increased susceptibility to illness and slower recovery times from illness. In one study, people with chronic insufficient sleep, set as six hours of sleep a night or less, were found to be four times more likely to catch a cold compared to those who reported sleeping for seven hours or more a night. Due to the role of sleep in regulating metabolism, insufficient sleep may also play a role in weight gain or, conversely, in impeding weight loss. Additionally, in 2007, the International Agency for Research on Cancer, which is the cancer research agency for the World Health Organization, declared that \"shiftwork that involves circadian disruption is probably carcinogenic to humans,\" speaking to the dangers of long-term nighttime work due to its intrusion on sleep. In 2015, the National Sleep Foundation released updated recommendations for sleep duration requirements based on age and concluded that \"Individuals who habitually sleep outside the normal range may be exhibiting signs or symptoms of serious health problems or, if done volitionally, may be compromising their health and well-being.\"\n\nHealth science is the branch of science focused on health. There are two main approaches to health science: the study and research of the body and health-related issues to understand how humans (and animals) function, and the application of that knowledge to improve health and to prevent and cure diseases and other physical and mental impairments. The science builds on many sub-fields, including biology, biochemistry, physics, epidemiology, pharmacology, medical sociology. Applied health sciences endeavor to better understand and improve human health through applications in areas such as health education, biomedical engineering, biotechnology and public health.\n\nOrganized interventions to improve health based on the principles and procedures developed through the health sciences are provided by practitioners trained in medicine, nursing, nutrition, pharmacy, social work, psychology, occupational therapy, physical therapy and other health care professions. Clinical practitioners focus mainly on the health of individuals, while public health practitioners consider the overall health of communities and populations. Workplace wellness programs are increasingly adopted by companies for their value in improving the health and well-being of their employees, as are school health services in order to improve the health and well-being of children.\n\nPublic health has been described as \"the science and art of preventing disease, prolonging life and promoting health through the organized efforts and informed choices of society, organizations, public and private, communities and individuals.\" It is concerned with threats to the overall health of a community based on population health analysis. The population in question can be as small as a handful of people or as large as all the inhabitants of several continents (for instance, in the case of a pandemic). Public health has many sub-fields, but typically includes the interdisciplinary categories of epidemiology, biostatistics and health services. Environmental health, community health, behavioral health, and occupational health are also important areas of public health.\n\nThe focus of public health interventions is to prevent and manage diseases, injuries and other health conditions through surveillance of cases and the promotion of healthy behavior, communities, and (in aspects relevant to human health) environments. Its aim is to prevent health problems from happening or re-occurring by implementing educational programs, developing policies, administering services and conducting research. In many cases, treating a disease or controlling a pathogen can be vital to preventing it in others, such as during an outbreak. Vaccination programs and distribution of condoms to prevent the spread of communicable diseases are examples of common preventive public health measures, as are educational campaigns to promote vaccination and the use of condoms (including overcoming resistance to such).\n\nPublic health also takes various actions to limit the health disparities between different areas of the country and, in some cases, the continent or world. One issue is the access of individuals and communities to health care in terms of financial, geographical or socio-cultural constraints to accessing and using services. Applications of the public health system include the areas of maternal and child health, health services administration, emergency response, and prevention and control of infectious and chronic diseases.\n\nThe great positive impact of public health programs is widely acknowledged. Due in part to the policies and actions developed through public health, the 20th century registered a decrease in the mortality rates for infants and children and a continual increase in life expectancy in most parts of the world. For example, it is estimated that life expectancy has increased for Americans by thirty years since 1900, and worldwide by six years since 1990.\n\nPersonal health depends partially on the active, passive, and assisted cues people observe and adopt about their own health. These include personal actions for preventing or minimizing the effects of a disease, usually a chronic condition, through integrative care. They also include personal hygiene practices to prevent infection and illness, such as bathing and washing hands with soap; brushing and flossing teeth; storing, preparing and handling food safely; and many others. The information gleaned from personal observations of daily living – such as about sleep patterns, exercise behavior, nutritional intake and environmental features – may be used to inform personal decisions and actions (\"e.g.\", \"I feel tired in the morning so I am going to try sleeping on a different pillow\"), as well as clinical decisions and treatment plans (\"e.g.\", a patient who notices his or her shoes are tighter than usual may be having exacerbation of left-sided heart failure, and may require diuretic medication to reduce fluid overload).\n\nPersonal health also depends partially on the social structure of a person's life. The maintenance of strong social relationships, volunteering, and other social activities have been linked to positive mental health and also increased longevity. One American study among seniors over age 70, found that frequent volunteering was associated with reduced risk of dying compared with older persons who did not volunteer, regardless of physical health status. Another study from Singapore reported that volunteering retirees had significantly better cognitive performance scores, fewer depressive symptoms, and better mental well-being and life satisfaction than non-volunteering retirees.\n\nProlonged psychological stress may negatively impact health, and has been cited as a factor in cognitive impairment with aging, depressive illness, and expression of disease. Stress management is the application of methods to either reduce stress or increase tolerance to stress. Relaxation techniques are physical methods used to relieve stress. Psychological methods include cognitive therapy, meditation, and positive thinking, which work by reducing response to stress. Improving relevant skills, such as problem solving and time management skills, reduces uncertainty and builds confidence, which also reduces the reaction to stress-causing situations where those skills are applicable.\n\nIn addition to safety risks, many jobs also present risks of disease, illness and other long-term health problems. Among the most common occupational diseases are various forms of pneumoconiosis, including silicosis and coal worker's pneumoconiosis (black lung disease). Asthma is another respiratory illness that many workers are vulnerable to. Workers may also be vulnerable to skin diseases, including eczema, dermatitis, urticaria, sunburn, and skin cancer. Other occupational diseases of concern include carpal tunnel syndrome and lead poisoning.\n\nAs the number of service sector jobs has risen in developed countries, more and more jobs have become sedentary, presenting a different array of health problems than those associated with manufacturing and the primary sector. Contemporary problems, such as the growing rate of obesity and issues relating to stress and overwork in many countries, have further complicated the interaction between work and health.\n\nMany governments view occupational health as a social challenge and have formed public organizations to ensure the health and safety of workers. Examples of these include the British Health and Safety Executive and in the United States, the National Institute for Occupational Safety and Health, which conducts research on occupational health and safety, and the Occupational Safety and Health Administration, which handles regulation and policy relating to worker safety and health.\n"}
{"id": "15561700", "url": "https://en.wikipedia.org/wiki?curid=15561700", "title": "Helicopter Underwater Escape Training", "text": "Helicopter Underwater Escape Training\n\nHelicopter Underwater Escape Training (known as Helicopter Underwater Egress Training in the United States; often abbreviated HUET, pronounced \"hue-wet\", \"hue-way\" or \"you-way\") is training provided to helicopter flight crews, offshore oil and gas industry staff law enforcement personnel, and military personnel who are regularly transported by helicopters over water. As the name implies, the purpose is to prepare them for emergency exit in the case of a crash landing over water. While the training may be taken by crew and frequent passengers in all small aircraft that could ditch over water, a fixed wing aircraft such as a seaplane or small jet is not as prone to roll over as quickly or sink as rapidly as a helicopter with its main rotor on top of the aircraft. In addition, due to the slow speed of helicopters versus fixed wing aircraft and the lower height above the water, their cabins have a greater chance of being intact with passengers inside when ditched in the water.\n\nThe training involves simulated sinking in a pool while rotating the training module upside down and focuses students on bracing for impact, identifying primary and secondary exit points, avoiding smoke inhalation, surfacing for air, and head count. The conventional simulators simulate an immersed cabin rotating around single axis, usually lengthwise. While the traditional simulators can only be turned left or right in the horizontal plane, the new generation simulators can turn 360° in both the horizontal and vertical planes. The HUET training aims to increase time-saving awareness of situational characteristics that can prevent successful escape, such as the role of the seat belt to control excess buoyancy caused by the safety suits inside the helicopter, how to operate the external breathing system, and to carefully store away excess seat belt length.\n\nRoyal Navy personnel train for helicopter emergency survival in water at Royal Naval Air Station Yeovilton while United States Marine Corps and Navy personnel train at Naval Amphibious Base Coronado, California, Camp Lejeune, North Carolina, or Camp Hansen, Okinawa, Japan.\n\nHUET training is provided in Australia by LifeFlight.\n"}
{"id": "21964979", "url": "https://en.wikipedia.org/wiki?curid=21964979", "title": "Hunting reaction", "text": "Hunting reaction\n\nThe hunting reaction or hunting response is a process of alternating vasoconstriction and vasodilation in extremities exposed to cold. The term Lewis reaction is used too, named after Thomas Lewis, who first described the effect in 1930.\n\nVasoconstriction occurs first to reduce heat loss, but also results in strong cooling of the extremities. Approximately five to ten minutes after the start of cold exposure, the blood vessels in the extremities will suddenly vasodilate. This is probably caused by a sudden decrease in the release of neurotransmitters from the sympathetic nerves to the muscular coat of the arteriovenous anastomoses due to local cold. This cold-induced vasodilation increases blood flow and subsequently the temperature of the fingers. A new phase of vasoconstriction follows the vasodilation, after which the process repeats itself.\n\nThe hunting reaction is one out of four possible responses to immersion of the finger in cold water. The other responses observed in the fingers after immersion in cold water are a continuous state of vasoconstriction, slow steady and continuous rewarming and a proportional control form in which the blood vessel diameter remains constant after an initial phase of vasoconstriction. However, the vast majority of the vascular responses to immersion of the finger in cold water can be classified as the hunting reaction.\n\nThere are many factors which influence the strength of the response. People who live or regularly work in cold environments show an increased response. Through acclimatization tropical residents can develop an increased response which is indistinguishable from arctic residents. The role of genetic factors is not clear because it is difficult to differentiate between adaptation and acclimatization.\n\nIt was thought that the hunting reaction protected the fingers against cold injury and improved muscle function in the fingers. An experiment has shown cold acclimation minimizes the hunting reaction (reduced mean temperature of the digits and a prolonged time of cold exposure prior to initial vasodilation), thus putting the hand at a greater risk of cold injury when it is exposed to cold.\n\n"}
{"id": "52822695", "url": "https://en.wikipedia.org/wiki?curid=52822695", "title": "Infection rate", "text": "Infection rate\n\nAn infection rate (or incident rate) is the probability or risk of an infection in a population. It is used to measure the frequency of occurrence of new instances of infection within a population during a specific time period.\n\nThe number of infections equals the cases identified in the study or observed. An example might by HIV infection during a specific time period in the defined population. The population at risk are the cases appearing in the population during the same time period. An example would be all the people in a city during a specific time period. The constant, or K is assigned a value of 100 to represent a percentage. An example would be to find the percentage of people in a city who are infected with HIV: 6,000 cases in March divided by the population of a city (one million) multiplied by the constant (K) would give an infection rate of 0.6%.\nCalculating the infection rate is used to analyze trends for the purpose of infection and disease control.\n\nAn online infection rate calculator has been developed by the Centers for Disease Control and Prevention that allows the determination of the Streptococcal A infection rate in a population.\n\nHealth care facilities routinely track their infection rates according to the guidelines issued by the Joint Commission. The healthcare-associated infection (HAI) rates measure infection of patients in a particular hospital. This allows rates to compared with other hospitals. These infections can often be prevented when healthcare facilities follow guidelines for safe care. To get payment from Medicare, hospitals are required to report data about some infections to the Centers for Disease Control and Prevention’s (CDC’s) National Healthcare Safety Network (NHSN). Hospitals currently submit information on central line-associated bloodstream infections (CLABSIs), catheter-associated urinary tract infections (CAUTIs), surgical site infections (SSIs), MRSA Bacteremia, and \"C. difficile\" laboratory-identified events. The public reporting of these data is an effort by the Department of Health and Human Services.\n\nFor meaningful comparisons of infection rates, populations must be very similar between the two or more assessments. However, a problem with mean rates is that they cannot reflect differences in risk between populations,\n\n"}
{"id": "5167532", "url": "https://en.wikipedia.org/wiki?curid=5167532", "title": "Laboratory informatics", "text": "Laboratory informatics\n\nLaboratory informatics is the specialized application of information technology aimed at optimizing and extending laboratory operations. It encompasses data acquisition, instrument interfacing, laboratory networking, data processing, specialized data management systems (such as a chromatography data system), a laboratory information management system, scientific data management (including data mining and data warehousing), and knowledge management (including the use of an electronic lab notebook). It has become more prevalent with the rise of other \"informatics\" disciplines such as bioinformatics, cheminformatics and health informatics. Several graduate programs are focused on some form of laboratory informatics, often with a clinical emphasis. A closely related - some consider subsuming - field is laboratory automation.\n\nIn the context of Public Health Laboratories, the Association of Public Health Laboratories has identified 19 areas for self-assessment of laboratory informatics in their Laboratories Efficiencies Initiative. These include the following Capability Areas. \n\n\n\n\n\n\n"}
{"id": "57689353", "url": "https://en.wikipedia.org/wiki?curid=57689353", "title": "Learning health systems", "text": "Learning health systems\n\nLearning health systems (LHS) are healthcare systems in which knowledge generation processes are embedded in daily practice to produce continual improve in care.\n\nThe idea was first conceptualised in a 2007 workshop organised by the US Institute of Medicine, building on ideas around evidence-based medicine and \"practice-based evidence\". There has since been increasing interest in the topic, including the creation of the Wiley journal \"Learning Health Systems\".\n\nLHS can be described as having four key elements:\n\nThey are thus dependent on the use of electronic health records (EHRs) and have inherited the adoption challenges of EHRs. \n\nLHS entail a clinical lifecycle. Patient data is collected, it is amalgamated across multiple patients and a problem is defined. These are activities largely driven by healthcare professionals. With the support of technology, an analysis is performed, which returns evidence, from which knowledge is generated, which leads to changed clinical practice, and thus to new patient data being collected.\n\nMcLachlan and colleagues (2018) suggest a taxonomy of nine LHS classification types:\n"}
{"id": "258979", "url": "https://en.wikipedia.org/wiki?curid=258979", "title": "Malnutrition", "text": "Malnutrition\n\nMalnutrition is a condition that results from eating a diet in which one or more nutrients are either not enough or are too much such that the diet causes health problems. It may involve calories, protein, carbohydrates, vitamins or minerals. Not enough nutrients is called undernutrition or undernourishment while too much is called overnutrition. Malnutrition is often used to specifically refer to undernutrition where an individual is not getting enough calories, protein, or micronutrients. If undernutrition occurs during pregnancy, or before two years of age, it may result in permanent problems with physical and mental development. Extreme undernourishment, known as starvation, may have symptoms that include: a short height, thin body, very poor energy levels, and swollen legs and abdomen. People also often get infections and are frequently cold. The symptoms of micronutrient deficiencies depend on the micronutrient that is lacking.\nUndernourishment is most often due to not enough high-quality food being available to eat. This is often related to high food prices and poverty. A lack of breastfeeding may contribute, as may a number of infectious diseases such as: gastroenteritis, pneumonia, malaria, and measles, which increase nutrient requirements. There are two main types of undernutrition: protein-energy malnutrition and dietary deficiencies. Protein-energy malnutrition has two severe forms: marasmus (a lack of protein and calories) and kwashiorkor (a lack of just protein). Common micronutrient deficiencies include: a lack of iron, iodine, and vitamin A. During pregnancy, due to the body's increased need, deficiencies may become more common. In some developing countries, overnutrition in the form of obesity is beginning to present within the same communities as undernutrition. Other causes of malnutrition include anorexia nervosa and bariatric surgery.\nEfforts to improve nutrition are some of the most effective forms of development aid. Breastfeeding can reduce rates of malnutrition and death in children, and efforts to promote the practice increase the rates of breastfeeding. In young children, providing food (in addition to breastmilk) between six months and two years of age improves outcomes. There is also good evidence supporting the supplementation of a number of micronutrients to women during pregnancy and among young children in the developing world. To get food to people who need it most, both delivering food and providing money so people can buy food within local markets are effective. Simply feeding students at school is insufficient. Management of severe malnutrition within the person's home with ready-to-use therapeutic foods is possible much of the time. In those who have severe malnutrition complicated by other health problems, treatment in a hospital setting is recommended. This often involves managing low blood sugar and body temperature, addressing dehydration, and gradual feeding. Routine antibiotics are usually recommended due to the high risk of infection. Longer-term measures include: improving agricultural practices, reducing poverty, improving sanitation, and the empowerment of women.\nThere were 815 million undernourished people in the world in 2017 (11% of the total population). This is a reduction of 176 million people since 1990 when 23% were undernourished. In 2012 it was estimated that another billion people had a lack of vitamins and minerals. In 2015, protein-energy malnutrition was estimated to have resulted in 323,000 deaths—down from 510,000 deaths in 1990. Other nutritional deficiencies, which include iodine deficiency and iron deficiency anemia, result in another 83,000 deaths. In 2010, malnutrition was the cause of 1.4% of all disability adjusted life years. About a third of deaths in children are believed to be due to undernutrition, although the deaths are rarely labelled as such. In 2010, it was estimated to have contributed to about 1.5 million deaths in women and children, though some estimate the number may be greater than 3 million. An additional 165 million children were estimated to have stunted growth from malnutrition in 2013. Undernutrition is more common in developing countries. Certain groups have higher rates of undernutrition, including women—in particular while pregnant or breastfeeding—children under five years of age, and the elderly. In the elderly, undernutrition becomes more common due to physical, psychological, and social factors.\n\nUnless specifically mentioned otherwise, the term malnutrition refers to undernutrition for the remainder of this article. Malnutrition can be divided into two different types, SAM and MAM. SAM refers to children with severe acute malnutrition. MAM refers to moderate acute malnutrition.\n\nMalnutrition is caused by eating a diet in which nutrients are \"not enough\" or is \"too much\" such that it causes health problems. It is a category of diseases that includes undernutrition and overnutrition. Overnutrition can result in obesity and being overweight. In some developing countries, overnutrition in the form of obesity is beginning to present within the same communities as undernutrition.\n\nHowever, the term malnutrition is commonly used to refer to undernutrition only. This applies particularly to the context of development cooperation. Therefore, \"malnutrition\" in documents by the World Health Organization, UNICEF, Save the Children or other international non-governmental organizations (NGOs) usually is equated to undernutrition.\n\nUndernutrition is sometimes used as a synonym of protein–energy malnutrition (PEM). While other include both micronutrient deficiencies and protein energy malnutrition in its definition. It differs from calorie restriction in that calorie restriction may not result in negative health effects. The term hypoalimentation means underfeeding.\n\nThe term \"severe malnutrition\" or \"severe undernutrition\" is often used to refer specifically to PEM. PEM is often associated with micronutrient deficiency. Two forms of PEM are kwashiorkor and marasmus, and they commonly coexist.\n\nKwashiorkor is mainly caused by inadequate protein intake. The main symptoms are edema, wasting, liver enlargement, hypoalbuminaemia, steatosis, and possibly depigmentation of skin and hair. Kwashiorkor is further identified by swelling of the belly, which is deceiving of actual nutritional status. The term means ‘displaced child’ and is derived from a Ghana language of West Africa, means \"the sickness the older one gets when the next baby is born,\" as this is when the older child is deprived of breast feeding and weaned to a diet composed largely of carbohydrates.\n\nMarasmus (‘to waste away’) is caused by an inadequate intake of protein and energy. The main symptoms are severe wasting, leaving little or no edema, minimal subcutaneous fat, severe muscle wasting, and non-normal serum albumin levels. Marasmus can result from a sustained diet of inadequate energy and protein, and the metabolism adapts to prolong survival. It is traditionally seen in famine, significant food restriction, or more severe cases of anorexia. Conditions are characterized by extreme wasting of the muscles and a gaunt expression.\n\nUndernutrition encompasses stunted growth (stunting), wasting, and deficiencies of essential vitamins and minerals (collectively referred to as micronutrients). The term hunger, which describes a feeling of discomfort from not eating, has been used to describe undernutrition, especially in reference to food insecurity.\n\nIn 1956, Gómez and Galvan studied factors associated with death in a group of malnourished (undernourished) children in a hospital in Mexico City, Mexico and defined categories of malnutrition: first, second, and third degree. The degrees were based on weight below a specified percentage of median weight for age. The risk of death increases with increasing degree of malnutrition. An adaptation of Gomez's original classification is still used today. While it provides a way to compare malnutrition within and between populations, the classification has been criticized for being \"arbitrary\" and for not considering overweight as a form of malnutrition. Also, height alone may not be the best indicator of malnutrition; children who are born prematurely may be considered short for their age even if they have good nutrition.\n\nJohn Conrad Waterlow established a new classification for malnutrition. Instead of using just weight for age measurements, the classification established by Waterlow combines weight-for-height (indicating acute episodes of malnutrition) with height-for-age to show the stunting that results from chronic malnutrition. One advantage of the Waterlow classification over the Gomez classification is that weight for height can be examined even if ages are not known.\n\nThese classifications of malnutrition are commonly used with some modifications by WHO.\n\nMalnutrition increases the risk of infection and infectious disease, and moderate malnutrition weakens every part of the immune system. For example, it is a major risk factor in the onset of active tuberculosis. Protein and energy malnutrition and deficiencies of specific micronutrients (including iron, zinc, and vitamins) increase susceptibility to infection. Malnutrition affects HIV transmission by increasing the risk of transmission from mother to child and also increasing replication of the virus. In communities or areas that lack access to safe drinking water, these additional health risks present a critical problem. Lower energy and impaired function of the brain also represent the downward spiral of malnutrition as victims are less able to perform the tasks they need to in order to acquire food, earn an income, or gain an education.\n\nVitamin-deficiency-related diseases (such as scurvy and rickets).\n\nHypoglycemia (low blood sugar) can result from a child not eating for 4 to 6 hours. Hypoglycemia should be considered if there is lethargy, limpness, convulsion, or loss of consciousness. If blood sugar can be measured immediately and quickly, perform a finger or heel stick.\n\nIn those with malnutrition some of the signs of dehydration differ. Children; however, may still be interested in drinking, have decreased interactions with the world around them, have decreased urine output, and may be cool to touch.\n\nProtein-calorie malnutrition can cause cognitive impairments. For humans, \"critical period varies from the final third of gestation to the first 2 years of life\". Iron deficiency anemia in children under two years of age likely affects brain function acutely and probably also chronically. Folate deficiency has been linked to neural tube defects.\n\nMalnutrition in the form of iodine deficiency is \"the most common preventable cause of mental impairment worldwide.\" \"Even moderate deficiency, especially in pregnant women and infants, lowers intelligence by 10 to 15 I.Q. points, shaving incalculable potential off a nation's development. The most visible and severe effects — disabling goiters, cretinism and dwarfism — affect a tiny minority, usually in mountain villages. But 16 percent of the world's people have at least mild goiter, a swollen thyroid gland in the neck.\"\n\nMajor causes of malnutrition include poverty and food prices, dietary practices and agricultural productivity, with many individual cases being a mixture of several factors. Clinical malnutrition, such as cachexia, is a major burden also in developed countries. Various scales of analysis also have to be considered in order to determine the sociopolitical causes of malnutrition. For example, the population of a community that is within poor governments, may be at risk if the area lacks health-related services, but on a smaller scale certain households or individuals may be at an even higher risk due to differences in income levels, access to land, or levels of education.\n\nMalnutrition can be a consequence of health issues such as gastroenteritis or chronic illness, especially the HIV/AIDS pandemic. Diarrhea and other infections can cause malnutrition through decreased nutrient absorption, decreased intake of food, increased metabolic requirements, and direct nutrient loss. Parasite infections, in particular intestinal worm infections (helminthiasis), can also lead to malnutrition. A leading cause of diarrhea and intestinal worm infections in children in developing countries is lack of sanitation and hygiene.\n\nPeople may become malnourished due to abnormal nutrient loss (due to diarrhea or chronic illness affecting the small bowel). This conditions may include Crohn's disease or untreated coeliac disease. Malnutrition may also occur due to increased energy expenditure (secondary malnutrition).\n\nA lack of adequate breastfeeding leads to malnutrition in infants and children, associated with the deaths of an estimated one million children annually. Illegal advertising of breast milk substitutes contributed to malnutrition and continued three decades after its 1981 prohibition under the \"WHO International Code of Marketing Breast Milk Substitutes\".\n\nMaternal malnutrition can also factor into the poor health or death of a baby. Over 800,000 neonatal death have occurred because of deficient growth of the fetus in the mother's womb.\n\nDeriving too much of one's diet from a single source, such as eating almost exclusively corn or rice, can cause malnutrition. This may either be from a lack of education about proper nutrition, or from only having access to a single food source.\n\nIt is not just the total amount of calories that matters but specific nutritional deficiencies such as vitamin A deficiency, iron deficiency or zinc deficiency can also increase risk of death.\n\nOvernutrition caused by overeating is also a form of malnutrition. In the United States, more than half of all adults are now overweight — a condition that, like hunger, increases susceptibility to disease and disability, reduces worker productivity, and lowers life expectancy. Overeating is much more common in the United States, where for the majority of people, access to food is not an issue. Many parts of the world have access to a surplus of non-nutritious food, in addition to increased sedentary lifestyles. Yale psychologist Kelly Brownell calls this a \"toxic food environment\" where fat and sugar laden foods have taken precedence over healthy nutritious foods.\n\nThe issue in these developed countries is choosing the right kind of food. More fast food is consumed per capita in the United States than in any other country. The reason for this mass consumption of fast food is its affordability and accessibility. Often fast food, low in cost and nutrition, is high in calories and heavily promoted. When these eating habits are combined with increasingly urbanized, automated, and more sedentary lifestyles, it becomes clear why weight gain is difficult to avoid.\n\nNot only does obesity occur in developed countries, problems are also occurring in developing countries in areas where income is on the rise. Overeating is also a problem in countries where hunger and poverty persist. In China, consumption of high-fat foods has increased while consumption of rice and other goods has decreased.\n\nOvereating leads to many diseases, such as heart disease and diabetes, that may result in death.\n\nIn Bangladesh, poor socioeconomic position was associated with chronic malnutrition since it inhibits purchase of nutritious foods such as milk, meat, poultry, and fruits. As much as food shortages may be a contributing factor to malnutrition in countries with lack of technology, the FAO (Food and Agriculture Organization) has estimated that eighty percent of malnourished children living in the developing world live in countries that produce food surpluses. The economist Amartya Sen observed that, in recent decades, famine has always been a problem of food distribution and/or poverty, as there has been sufficient food to feed the whole population of the world. He states that malnutrition and famine were more related to problems of food distribution and purchasing power.\n\nIt is argued that commodity speculators are increasing the cost of food. As the real estate bubble in the United States was collapsing, it is said that trillions of dollars moved to invest in food and primary commodities, causing the 2007–2008 food price crisis.\n\nThe use of biofuels as a replacement for traditional fuels raises the price of food. The United Nations special rapporteur on the right to food, Jean Ziegler proposes that agricultural waste, such as corn cobs and banana leaves, rather than crops themselves be used as fuel.\n\nLocal food shortages can be caused by a lack of arable land, adverse weather, lower farming skills such as crop rotation, or by a lack of technology or resources needed for the higher yields found in modern agriculture, such as fertilizers, pesticides, irrigation, machinery and storage facilities. As a result of widespread poverty, farmers cannot afford or governments cannot provide the resources necessary to improve local yields. The World Bank and some wealthy donor countries also press nations that depend on aid to cut or eliminate subsidized agricultural inputs such as fertilizer, in the name of free market policies even as the United States and Europe extensively subsidized their own farmers. Many, if not most, farmers cannot afford fertilizer at market prices, leading to low agricultural production and wages and high, unaffordable food prices.\nReasons for the unavailability of fertilizer include moves to stop supplying fertilizer on environmental grounds, cited as the obstacle to feeding Africa by the Green Revolution pioneers Norman Borlaug and Keith Rosenberg.\n\nThere are a number of potential disruptions to global food supply that could cause widespread malnutrition.\n\nGlobal warming is of importance to food security, with 95 percent of all malnourished peoples living in the relatively stable climate region of the sub-tropics and tropics. According to the latest IPCC reports, temperature increases in these regions are \"very likely.\" Even small changes in temperatures can lead to increased frequency of extreme weather conditions. Many of these have great impact on agricultural production and hence nutrition. For example, the 1998–2001 central Asian drought brought about an 80 percent livestock loss and 50 percent reduction in wheat and barley crops in Iran. Similar figures were present in other nations. An increase in extreme weather such as drought in regions such as Sub-Saharan Africa would have even greater consequences in terms of malnutrition. Even without an increase of extreme weather events, a simple increase in temperature reduces the productivity of many crop species, also decreasing food security in these regions.\n\nColony collapse disorder is a phenomenon where bees die in large numbers. Since many agricultural crops worldwide are pollinated by bees, this represents a threat to the supply of food.\n\nThe effort to bring modern agricultural techniques found in the West, such as nitrogen fertilizers and pesticides, to Asia, called the Green Revolution, resulted in increased food production and corresponding decreases in prices and malnutrition similar to those seen earlier in Western nations. This was possible because of existing infrastructure and institutions that are in short supply in Africa, such as a system of roads or public seed companies that made seeds available. Investments in agriculture, such as subsidized fertilizers and seeds, increases food harvest and reduces food prices. For example, in the case of Malawi, almost five million of its 13 million people used to need emergency food aid. However, after the government changed policy and subsidies for fertilizer and seed were introduced against World Bank strictures, farmers produced record-breaking corn harvests as production leaped to 3.4 million in 2007 from 1.2 million in 2005, making Malawi a major food exporter. This lowered food prices and increased wages for farm workers. Such investments in agriculture are still needed in other African countries like the Democratic Republic of the Congo. The country has one of the highest prevalence of malnutrition even though it is blessed with great agricultural potential John Ulimwengu explains in his article for D+C. Proponents for investing in agriculture include Jeffrey Sachs, who has championed the idea that wealthy countries should invest in fertilizer and seed for Africa’s farmers.\n\nIn Nigeria, the use of imported Ready to Use Therapeutic Food (RUTF) has been used to treat malnutrition in the North. \"Soy Kunu\", a locally sourced and prepared blend consisting of peanut, millet and soya beans may also be used.\n\nNew technology in agricultural production also has great potential to combat undernutrition. By improving agricultural yields, farmers could reduce poverty by increasing income as well as open up area for diversification of crops for household use. The World Bank itself claims to be part of the solution to malnutrition, asserting that the best way for countries to succeed in breaking the cycle of poverty and malnutrition is to build export-led economies that will give them the financial means to buy foodstuffs on the world market.\n\nThere is a growing realization among aid groups that giving cash or cash vouchers instead of food is a cheaper, faster, and more efficient way to deliver help to the hungry, particularly in areas where food is available but unaffordable. The UN's World Food Program, the biggest non-governmental distributor of food, announced that it will begin distributing cash and vouchers instead of food in some areas, which Josette Sheeran, the WFP's executive director, described as a \"revolution\" in food aid. The aid agency Concern Worldwide is piloting a method through a mobile phone operator, Safaricom, which runs a money transfer program that allows cash to be sent from one part of the country to another.\n\nHowever, for people in a drought living a long way from and with limited access to markets, delivering food may be the most appropriate way to help. Fred Cuny stated that \"the chances of saving lives at the outset of a relief operation are greatly reduced when food is imported. By the time it arrives in the country and gets to people, many will have died.\" U.S. law, which requires buying food at home rather than where the hungry live, is inefficient because approximately half of what is spent goes for transport. Cuny further pointed out \"studies of every recent famine have shown that food was available in-country — though not always in the immediate food deficit area\" and \"even though by local standards the prices are too high for the poor to purchase it, it would usually be cheaper for a donor to buy the hoarded food at the inflated price than to import it from abroad.\"\nFood banks and soup kitchens address malnutrition in places where people lack money to buy food. A basic income has been proposed as a way to ensure that everyone has enough money to buy food and other basic needs; it is a form of social security in which all citizens or residents of a country regularly receive an unconditional sum of money, either from a government or some other public institution, in addition to any income received from elsewhere.\n\nEthiopia has been pioneering a program that has now become part of the World Bank's prescribed method for coping with a food crisis and had been seen by aid organizations as a model of how to best help hungry nations. Through the country's main food assistance program, the Productive Safety Net Program, Ethiopia has been giving rural residents who are chronically short of food, a chance to work for food or cash. Foreign aid organizations like the World Food Program were then able to buy food locally from surplus areas to distribute in areas with a shortage of food. Ethiopia been pioneering a program, and Brazil has established a recycling program for organic waste that benefits farmers, urban poor, and the city in general. City residents separate organic waste from their garbage, bag it, and then exchange it for fresh fruit and vegetables from local farmers. As a result, the country's waste is reduced and the urban poor get a steady supply of nutritious food.\n\nRestricting population size is a proposed solution. Thomas Malthus argued that population growth could be controlled by natural disasters and voluntary limits through \"moral restraint.\" Robert Chapman suggests that an intervention through government policies is a necessary ingredient of curtailing global population growth. The interdependence and complementarity of population growth with poverty and malnutrition (as well as the environment) is also recognised by the United Nations. More than 200 million women worldwide do not have adequate access to family planning services. According to the World Health Organisation, \"Family planning is key to slowing unsustainable population growth and the resulting negative impacts on the economy, environment, and national and regional development efforts\".\n\nHowever, there are many who believe that the world has more than enough resources to sustain its population. Instead, these theorists point to unequal distribution of resources and under- or unutilized arable land as the cause for malnutrition problems. For example, Amartya Sen advocates that, \"no matter how a famine is caused, methods of breaking it call for a large supply of food in the public distribution system. This applies not only to organizing rationing and control, but also to undertaking work programmes and other methods of increasing purchasing power for those hit by shifts in exchange entitlements in a general inflationary situation.\" \n\nOne suggested policy framework to resolve access issues is termed food sovereignty—the right of peoples to define their own food, agriculture, livestock, and fisheries systems, in contrast to having food largely subjected to international market forces. Food First is one of the primary think tanks working to build support for food sovereignty. Neoliberals advocate for an increasing role of the free market.\n\nAnother possible long term solution would be to increase access to health facilities to rural parts of the world. These facilities could monitor undernourished children, act as supplemental food distribution centers, and provide education on dietary needs. These types of facilities have already proven very successful in countries such as Peru and Ghana.\n\nAs of 2016 is estimated that about 821,000 deaths of children less than five years old could be prevented globally per year through more widespread breastfeeding. In addition to reducing infant death, breast milk feeding provides an important source of micronutrients, clinically proven to bolster the immune system of children, and provide long-term defenses against non-communicable and allergic diseases. Breastfeeding has also been shown to improve cognitive abilities in children, with a strong correlation to individual educational achievements. As previously noted, lack of proper breastfeeding is a major factor in child mortality rates, and a primary determinant of disease development for children. The medical community recommends exclusively breastfeeding infants for 6 months, with nutritional whole food supplementation and continued breastfeeding up to 2 years or older for overall optimal health outcomes. Exclusive breastfeeding is defined as only giving an infant breast milk for six months as a source of food and nutrition. This means no other liquids, including water or semi-solid foods.\n\nBreastfeeding is noted as one of the most cost effective medical interventions for providing beneficial child health. While there are considerable differences within developed and developing countries: income, employment, social norms, and access to healthcare were found to be universal determinants of whether a mother breast or formula fed their children. Community based healthcare workers have helped alleviate financial barriers faced by newly made mothers, and provided a viable alternative to traditional and expensive hospital based medical care. Recent studies based upon surveys conducted from 1995-2010 shows exclusive breastfeeding rates have gone up globally, from 33% to 39%. Despite the growth rates, medical professionals acknowledge the need for improvement given the importance of exclusive breastfeeding.\n\nThe EndingHunger campaign is an online communication campaign aimed at raising awareness of the hunger problem. It has many worked through viral videos depicting celebrities voicing their anger about the large number of hungry people in the world.\n\nFood security and global malnutrition has long been a topic of international concern, with one of the first official global documents addressing it being the 1948 Universal Declaration of Human Rights(UDHR). Within this document it stated that access to food was part of an adequate right to a standard of living. The Right to food was asserted in the International Covenant on Economic, Social and Cultural Rights, a treaty adopted by the United Nations General Assembly on December 16, 1966. The Right to food is a human right for people to feed themselves in dignity, be free from hunger, food insecurity, and malnutrition. As of 2018, the treaty has been signed by 166 countries, by signing states agreed to take steps to the maximum of their available resources to achieve the right to adequate food.\n\nHowever, after the 1966 International Covenant the global concern for the access to sufficient food only became more present, leading to the first ever World Food Conference that was held in 1974 in Rome, Italy. The Universal Declaration on the Eradication of Hunger and Malnutrition was a UN resolution adopted November 16, 1974 by all 135 countries that attended the 1974 World Food Conference. This non-legally binding document set forth certain aspirations for countries to follow to sufficiently take action on the global food problem. Ultimately this document outline and provided guidance as to how the international community as one could work towards fighting and solving the growing global issue of malnutrition and hunger.\n\nAdoption of the right to food was included in the Additional Protocol to the American Convention on Human Rights in the area of Economic, Social, and Cultural Rights, this 1978 document was adopted by many countries in the Americas, the purpose of the document is, \"to consolidate in this hemisphere, within the framework of democratic institutions, a system of personal liberty and social justice based on respect for the essential rights of man.\"\n\nThe next document in the timeline of global inititaves for malnutrition was the 1996 Rome Declaration on World Food Security, organized by the Food and Agriculture Organization. This document reaffirmed the right to have access to safe and nutritous food by everyone, also considering that everyone gets sufficient food, and set the goals for all nations to improve their commitment to food security by halfing their amount of undernourished people by 2015. In 2004 the Food and Agriculture Organization adopted the Right to Food Guidelines, which offered states a framework of how to increase the right to food on a national basis.\n\nIn April 2012, the Food Assistance Convention was signed, the world's first legally binding international agreement on food aid. The May 2012 Copenhagen Consensus recommended that efforts to combat hunger and malnutrition should be the first priority for politicians and private sector philanthropists looking to maximize the effectiveness of aid spending. They put this ahead of other priorities, like the fight against malaria and AIDS.\n\nThe main global policy to reduce hunger and poverty are the Sustainable Development Goals, approved through the UN in 2015. In particular Goal 2: Zero hunger sets globally agreed targets to end hunger, achieve food security and improved nutrition and promote sustainable agriculture. The partnership Compact2025, led by IFPRI with the involvement of UN organisations, NGOs and private foundations develops and disseminates evidence-based advice to politicians and other decision-makers aimed at ending hunger and undernutrition in the coming 10 years, by 2025.\n\nIn June 2015, the European Union and the Bill & Melinda Gates Foundation have launched a partnership to combat undernutrition especially in children. The program will initiatilly be implemented in Bangladesh, Burundi, Ethiopia, Kenya, Laos and Niger and will help these countries to improve information and analysis about nutrition so they can develop effective national nutrition policies.\n\nThe Food and Agriculture Organization of the UN has created a partnership that will act through the African Union's CAADP framework aiming to end hunger in Africa by 2025. It includes different interventions including support for improved food production, a strengthening of social protection and integration of the right to food into national legislation.\n\nThe EndingHunger campaign is an online communication campaign aimed at raising awareness of the hunger problem. It has many worked through viral videos depicting celebrities voicing their anger about the large number of hungry people in the world.\n\nIn response to child malnutrition, the Bangladeshi government recommends ten steps for treating severe malnutrition. They are to prevent or treat dehydration, low blood sugar, low body temperature, infection, correct electrolyte imbalances and micronutrient deficiencies, start feeding cautiously, achieve catch-up growth, provide psychological support, and prepare for discharge and follow-up after recovery.\n\nAmong those who are hospitalized, nutritional support improves protein, calorie intake and weight.\n\nThe evidence for benefit of supplementary feeding is poor. This is due to the small amount of research done on this treatment.\n\nSpecially formulated foods do however appear useful in those from the developing world with moderate acute malnutrition. In young children with severe acute malnutrition it is unclear if ready-to-use therapeutic food differs from a normal diet. They may have some benefits in humanitarian emergencies as they can be eaten directly from the packet, do not require refrigeration or mixing with clean water, and can be stored for years.\n\nIn those who are severely malnourished, feeding too much too quickly can result in refeeding syndrome. This can result regardless of route of feeding and can present itself a couple of days after eating with heart failure, dysrhythmias and confusion that can result in death.\n\nManufacturers are trying to fortify everyday foods with micronutrients that can be sold to consumers such as wheat flour for Beladi bread in Egypt or fish sauce in Vietnam and the iodization of salt.\n\nFor example, flour has been fortified with iron, zinc, folic acid and other B vitamins such as thiamine, riboflavin, niacin and vitamin B12.\n\nTreating malnutrition, mostly through fortifying foods with micronutrients (vitamins and minerals), improves lives at a lower cost and shorter time than other forms of aid, according to the World Bank. The Copenhagen Consensus, which look at a variety of development proposals, ranked micronutrient supplements as number one.\n\nIn those with diarrhea, once an initial four-hour rehydration period is completed, zinc supplementation is recommended. Daily zinc increases the chances of reducing the severity and duration of the diarrhea, and continuing with daily zinc for ten to fourteen days makes diarrhea less likely recur in the next two to three months.\n\nIn addition, malnourished children need both potassium and magnesium. This can be obtained by following the above recommendations for the dehydrated child to continue eating within two to three hours of starting rehydration, and including foods rich in potassium as above. Low blood potassium is worsened when base (as in Ringer's/Hartmann's) is given to treat acidosis without simultaneously providing potassium. As above, available home products such as salted and unsalted cereal water, salted and unsalted vegetable broth can be given early during the course of a child's diarrhea along with continued eating. Vitamin A, potassium, magnesium, and zinc should be added with other vitamins and minerals if available.\n\nFor a malnourished child with diarrhea from any cause, this should include foods rich in potassium such as bananas, green coconut water, and unsweetened fresh fruit juice.\n\nThe World Health Organization (WHO) recommends rehydrating a severely undernourished child who has diarrhea relatively slowly. The preferred method is with fluids by mouth using a drink called oral rehydration solution (ORS). The oral rehydration solution is both slightly sweet and slightly salty and the one recommended in those with severe undernutrition should have half the usual sodium and greater potassium. Fluids by nasogastric tube may be use in those who do not drink. Intravenous fluids are recommended only in those who have significant dehydration due to their potential complications. These complications include congestive heart failure. Over time, ORS developed into ORT, or oral rehydration therapy, which focused on increasing fluids by supplying salts, carbohydrates, and water. This switch from type of fluid to amount of fluid was crucial in order to prevent dehydration from diarrhea.\n\nBreast feeding and eating should resume as soon as possible. Drinks such as soft drinks, fruit juices, or sweetened teas are not recommended as they contain too much sugar and may worsen diarrhea. Broad spectrum antibiotics are recommended in all severely undernourished children with diarrhea requiring admission to hospital.\n\nTo prevent dehydration readily available fluids, preferably with a modest amount of sugars and salt such as vegetable broth or salted rice water, may be used. The drinking of additional clean water is also recommended. Once dehydration develops oral rehydration solutions are preferred. As much of these drinks as the person wants can be given, unless there are signs of swelling. If vomiting occurs, fluids can be paused for 5–10 minutes and then restarting more slowly. Vomiting rarely prevents rehydration as fluid are still absorbed and the vomiting rarely last long. A severely malnourished child with what appears to be dehydration but who has not had diarrhea should be treated as if they have an infection.\n\nFor babies a dropper or syringe without the needle can be used to put small amounts of fluid into the mouth; for children under 2, a teaspoon every one to two minutes; and for older children and adults, frequent sips directly from a cup. After the first two hours, rehydration should be continued at the same or slower rate, determined by how much fluid the child wants and any ongoing diarrheal loses. After the first two hours of rehydration it is recommended that to alternate between rehydration and food.\n\nIn 2003, WHO and UNICEF recommended a reduced-osmolarity ORS which still treats dehydration but also reduced stool volume and vomiting. Reduced-osmolarity ORS is the current standard ORS with reasonably wide availability. For general use, one packet of ORS (glucose sugar, salt, potassium chloride, and trisodium citrate) is added to one liter of water; however, for malnourished children it is recommended that one packet of ORS be added to two liters of water along with an extra 50 grams of sucrose sugar and some stock potassium solution.\n\nMalnourished children have an excess of body sodium. Recommendations for home remedies agree with one liter of water (34 oz.) and 6 teaspoons sugar and disagree regarding whether it is then one teaspoon of salt added or only 1/2, with perhaps most sources recommending 1/2 teaspoon of added salt to one liter water.\n\nHypoglycemia, whether known or suspected, can be treated with a mixture of sugar and water. If the child is conscious, the initial dose of sugar and water can be given by mouth. If the child is unconscious, give glucose by intravenous or nasogastric tube. If seizures occur after despite glucose, rectal diazepam is recommended. Blood sugar levels should be re-checked on two hour intervals.\n\nHypothermia can occur. To prevent or treat this, the child can be kept warm with covering including of the head or by direct skin-to-skin contact with the mother or father and then covering both parent and child. Prolonged bathing or prolonged medical exams should be avoided. Warming methods are usually most important at night.\n\nThe figures provided in this section on epidemiology all refer to \"undernutrition\" even if the term malnutrition is used which, by definition, could also apply to too much nutrition.\n\nThe Global Hunger Index (GHI) is a multidimensional statistical tool used to describe the state of countries’ hunger situation. The GHI measures progress and failures in the global fight against hunger. The GHI is updated once a year. The data from the 2015 report shows that Hunger levels have dropped 27% since 2000. Fifty two countries remain at serious or alarming levels. In addition to the latest statistics on Hunger and Food Security, the GHI also features different special topics each year. The 2015 report include an article on conflict and food security.\n\nThere were 815 million undernourished people in the world in 2017. This was 176 million fewer people than in 1990 when it was 991 million undernourished people. This is despite the world's farmers producing enough food to feed around 12 billion people – almost double the current world population.\n\nMalnutrition, as of 2010, was the cause of 1.4% of all disability adjusted life years.\n\nMortality due to malnutrition accounted for 58 percent of the total mortality in 2006: \"In the world, approximately 62 million people, all causes of death combined, die each year. One in twelve people worldwide is malnourished and according to the Save the Children 2012 report, one in four of the world’s children are chronically malnourished. In 2006, more than 36 million died of hunger or diseases due to deficiencies in micronutrients\".\n\nIn 2010 protein-energy malnutrition resulted in 600,000 deaths down from 883,000 deaths in 1990. Other nutritional deficiencies, which include iodine deficiency and iron deficiency anemia, result in another 84,000 deaths. In 2010 malnutrition caused about 1.5 million deaths in women and children.\n\nAccording to the World Health Organization, malnutrition is the biggest contributor to child mortality, present in half of all cases. Six million children die of hunger every year. Underweight births and intrauterine growth restrictions cause 2.2 million child deaths a year. Poor or non-existent breastfeeding causes another 1.4 million. Other deficiencies, such as lack of vitamin A or zinc, for example, account for 1 million. Malnutrition in the first two years is irreversible. Malnourished children grow up with worse health and lower education achievement. Their own children tend to be smaller. Malnutrition was previously seen as something that exacerbates the problems of diseases such as measles, pneumonia and diarrhea, but malnutrition actually causes diseases, and can be fatal in its own right.\n\nThroughout history, portions of the world's population have often experienced sustained periods of hunger. In many cases, this resulted from food supply disruptions caused by war, plagues, or adverse weather. For the first few decades after World War II, technological progress and enhanced political cooperation suggested it might be possible to substantially reduce the number of people suffering from hunger. While progress was uneven, by 2000 the threat of extreme hunger subsided for many of the world's people. According to the WFP some statistics are that, \"Some 795 million people in the world do not have enough food to lead a healthy active life. That's about one in nine people on earth. The vast majority of the world's hungry people live in developing countries, where 12.9 percent of the population is undernourished.\"\n\nUntil 2006, the average international price of food had been largely stable for several decades. In the closing months of 2006, however, prices began to rise rapidly. By 2008, rice had tripled in price in some regions, and this severely affected developing countries. Food prices fell in early 2009, but rose to another record high in 2011, and have since decreased slightly. The 2008 worldwide financial crisis further increased the number of people suffering from hunger, including dramatic increases even in advanced economies such as Great Britain, the Eurozone and the United States.\n\nThe Millennium Development Goals included a commitment to a further 50% reduction in the proportion of the world's population who have extreme hunger by 2015. As of 2012, this target appeared difficult to achieve, due in part to persistent inflation in food prices. However, in late 2012 the UN's Food and Agriculture Organization (FAO) stated it is still possible to hit the target with sufficient effort. In 2013, the \"FAO\" estimated that 842 million people are undernourished (12% of the global population). Malnutrition is a cause of death for more than 3.1 million children under 5 every year. UNICEF estimates 300 million children go to bed hungry each night; and that 8000 children under the age of 5 are estimated to die of malnutrition every day.\n\nThroughout history, the need to aid those suffering from hunger has been commonly, though not universally, recognized.\n\nThe philosopher Simone Weil wrote that feeding the hungry when you have resources to do so is the most obvious of all human obligations. She says that as far back as Ancient Egypt, many believed that people had to show they had helped the hungry in order to justify themselves in the afterlife. Weil writes that Social progress is commonly held to be first of all, \"...a transition to a state of human society in which people will not suffer from hunger.\" Social historian Karl Polanyi wrote that before markets became the world's dominant form of economic organization in the 19th century, most human societies would either starve all together or not at all, because communities would invariably share their food.\n\nFrom the first age of globalization, which began in the 19th century, it became more common for people to consider problems like hunger in global terms. However, as early globalization largely coincided with the high peak of influence for classical liberalism, there was relatively little call for politicians to address world hunger.\n\nIn the late nineteenth and early twentieth century, the view that politicians ought not to intervene against hunger was increasingly challenged by campaigning journalists, with some academics and politicians also calling for or organizing intervention against world hunger, such as U.S. President Woodrow Wilson.\n\nHunger as an academic and social topic came to prominence during the Great Depression. As many individuals struggled for food, the same agricultural industries were suddenly producing large surpluses as means of increased production to counter the drop in demand from the European markets. This increased output was meant to ease the growing debt levels, however domestic demand could not keep up with prices. Instead, what is often called \"the paradox of want amid plenty,\" agricultural surpluses and large demand simply did not fit together, causing the Hoover administration to buy large amounts of product, such as grain, to stabilize prices. Initially refusing to further compromise the distressed price levels, political pressure from starving families across the country forced Congress to reconsider. With large deposits of grain already wasting away in government possession, the only political move left was to begin a process of donations to the hungry from the Farm Board, a federal oversight created in 1929 to promote the sale and stabilization of agricultural products. Instead of hunger being a reason for the allocation of large grain surpluses, waste became the eventual driving force.\n\nAfter World War II, a new international politico-economic order came into being, which was later described as Embedded liberalism.\n\nFor at least the first decade after the war, the United States, by far the period's most dominant national actor, was strongly supportive of efforts to tackle world hunger and to promote international development. It heavily funded the United Nation's development programmes, and later the efforts of other multilateral organizations like the International Monetary Fund (IMF) and the World Bank (WB).\n\nThe newly established United Nations became a leading player in co-ordinating the global fight against hunger. The UN has three agencies that work to promote food security and agricultural development: the Food and Agriculture Organization (FAO), the World Food Programme (WFP) and the International Fund for Agricultural Development (IFAD). FAO is the world's agricultural knowledge agency, providing policy and technical assistance to developing countries to promote food security, nutrition and sustainable agricultural production, particularly in rural areas.\n\nWFP's key mission is to deliver food into the hands of the hungry poor. The agency steps in during emergencies and uses food to aid recovery after emergencies. Its longer term approaches to hunger helps the transition from recovery to development. IFAD, with its knowledge of rural poverty and exclusive focus on poor rural people, designs and implements programmes to help those people access the assets, services and opportunities they need to overcome poverty.\n\nFollowing successful post WWII reconstruction of Germany and Japan, the IMF and WB began to turn their attention to the developing world. A great many civil society actors were also active in trying to combat hunger, especially after the late 1970s when global media began to bring the plight of starving people in places like Ethiopia to wider attention. Most significant of all, especially in the late 1960s and 70s, the Green revolution helped improved agricultural technology propagate throughout the world.\n\nThe United States began to change its approach to the problem of world hunger from about the mid 1950s. Influential members of the administration became less enthusiastic about methods they saw as promoting an over reliance on the state, as they feared that might assist the spread of communism. Despite this view, during the 1960s postwar era hunger within the United States was overshadowed by hunger in Europe and Asia. John F. Kennedy did in fact use Executive Order to double the amount of commodities available from the surplus commodity program as well as initiated the pilot Food Stamp Program which later became permanent in 1964.\n\nBy the 1980s, the previous consensus in favour of moderate government intervention had been displaced across the western world. The IMF and World Bank in particular began to promote market-based solutions. In cases where countries became dependent on the IMF, they sometimes forced national governments to prioritize debt repayments and sharply cut public services. This sometimes had a negative effect on efforts to combat hunger.\n\nOrganizations such as Food First raised the issue of food sovereignty and claimed that every country on earth (with the possible minor exceptions of some city-states) has sufficient agricultural capacity to feed its own people, but that the \"free trade\" economic order, which from the late 1970s to about 2008 had been associated with such institutions as the IMF and World Bank, had prevented this from happening.\n\nThe World Bank itself claimed it was part of the solution to hunger, asserting that the best way for countries to break the cycle of poverty and hunger was to build export-led economies that provide the financial means to buy foodstuffs on the world market. However, in the early 21st century the World Bank and IMF became less dogmatic about promoting free market reforms. They increasingly returned to the view that government intervention does have a role to play, and that it can be advisable for governments to support food security with policies favourable to domestic agriculture, even for countries that do not have a Comparative advantage in that area. As of 2012, the World Bank remains active in helping governments to intervene against hunger.\n\nUntil at least the 1980s—and, to an extent, the 1990s—the dominant academic view concerning world hunger was that it was a problem of demand exceeding supply. Proposed solutions often focused on boosting food production, and sometimes on birth control. There were exceptions to this, even as early as the 1940s, Lord Boyd-Orr, the first head of the UN's FAO, had perceived hunger as largely a problem of distribution, and drew up comprehensive plans to correct this. Few agreed with him at the time, however, and he resigned after failing to secure support for his plans from the US and Great Britain. In 1998, Amartya Sen won a Nobel Prize in part for demonstrating that hunger in modern times is not typically the product of a lack of food. Rather, hunger usually arises from food distribution problems, or from governmental policies in the developed and developing world. It has since been broadly accepted that world hunger results from issues with the distribution as well as the production of food. Sen's 1981 essay \"Poverty and Famines: An Essay on Entitlement and Deprivation\" played a prominent part in forging the new consensus.\n\nIn 2007 and 2008, rapidly increasing food prices caused a \"global food crisis\", increasing the numbers suffering from hunger by over a hundred million. Food riots erupted in several dozen countries; in at least two cases, Haiti and Madagascar, this led to the toppling of governments. A second \"global food crisis\" unfolded due to the spike in food prices of late 2010 and early 2011. Fewer food riots occurred, due in part to greater availability of food stock piles for relief. However, several analysts argue the food crisis was one of the causes of the Arab Spring.\nAs of 2008 roughly $300 million of aid went to basic nutrition each year, less than $2 for each child below two in the 20 worst affected countries. In contrast, at that time HIV/AIDS, which caused fewer deaths than child malnutrition, received $2.2 billion—$67 per person with HIV in all countries. In 2008 UN estimated that ending world hunger could cost about 30 billion.\n\nThe International Crops Research Institute for the Semi-Arid Tropics (ICRISAT), a member of the CGIAR consortium, partners with farmers, governments, researchers and NGOs to help farmers grow nutritious crops, such as chickpea, groundnut, pigeonpea, millet and sorghum. This helps their communities have more balanced diets and become more resilient to pests and drought. The Harnessing Opportunities for Productivity Enhancement of Sorghum and Millets in Sub-Saharan Africa and the Indian-Subcontinent (HOPE) project, for example, is increasing yields of finger millet in Tanzania by encouraging farmers to grow improved varieties. Finger millet is very high in calcium, rich in iron and fiber, and has a better energy content than other cereals. These characteristics make it ideal for feeding to infants and the elderly.\n\nSome organizations have begun working with teachers, policymakers, and managed food service contractors to mandate improved nutritional content and increased nutritional resources in school cafeterias from primary to university-level institutions. Health and nutrition have been proven to have close links with overall educational success.\n\nIn the early 21st century, there was relatively little awareness of hunger from leaders of advanced nations such as those that form the G8. Prior to 2009, efforts to fight hunger were mainly undertaken by governments of the worst affected countries, by civil society actors, and by multilateral and regional organizations. In 2009, Pope Benedict published his third encyclical, Caritas in Veritate, which emphasised the importance of fighting against hunger. The encyclical was intentionally published immediately before the July 2009 G8 Summit to maximise its influence on that event. At the Summit, which took place at L'Aquila in central Italy, the \"L'Aquila Food Security Initiative\" was launched, with a total of US$22 billion committed to combat hunger.\n\nFood prices did fall sharply in 2009 and early 2010, though analysts credit this much more to farmers increasing production in response to the 2008 spike in prices, than to the fruits of enhanced government action. However, since the 2009 G8 summit, the fight against hunger has remained a high-profile issue among the leaders of the worlds major nations, and was a prominent part of the agenda for the 2012 G-20 summit. \n\nIn April 2012, the Food Assistance Convention was signed, the world's first legally binding international agreement on food aid. The May 2012 Copenhagen Consensus recommended that efforts to combat hunger and malnutrition should be the first priority for politicians and private sector philanthropists looking to maximize the effectiveness of aid spending. They put this ahead of other priorities, like the fight against malaria and AIDS. Also in May 2012, U.S. President Barack Obama launched a \"new alliance for food security and nutrition\"—a broad partnership between private sector, governmental and civil society actors—that aimed to \"...achieve sustained and inclusive agricultural growth and raise 50 million people out of poverty over the next 10 years.\" The UK's prime minister David Cameron held a hunger summit on 12 August, the last day of the 2012 Summer Olympics.\n\nThe fight against hunger has also been joined by an increased number of regular people. While folk throughout the world had long contributed to efforts to alleviate hunger in the developing world, there has recently been a rapid increase in the numbers involved in tackling domestic hunger even within the economically advanced nations of the Global North.\n\nThis had happened much earlier in North America than it did in Europe. In the US, the Reagan administration scaled back welfare the early 1980s, leading to a vast increase of charity sector efforts to help Americans unable to buy enough to eat. According to a 1992 survey of 1000 randomly selected US voters, 77% of Americans had contributed to efforts to feed the hungry, either by volunteering for various hunger relief agencies such as food banks and soup kitchens, or by donating cash or food. \nEurope, with its more generous welfare system, had little awareness of domestic hunger until the food price inflation that began in late 2006, and especially as austerity-imposed welfare cuts began to take effect in 2010. Various surveys reported that upwards of 10% of Europe's population had begun to suffer from food insecurity. Especially since 2011, there has been a substantial increase in grass roots efforts to help the hungry by means of food banks, within both the UK and continental Europe. \n\nBy July 2012, the 2012 US drought had already caused a rapid increase in the price of grain and soy, with a knock on effect on the price of meat. As well as affecting hungry people in the US, this caused prices to rise on the global markets; the US is the world's biggest exporter of food. This led to much talk of a possible third 21st century global food crisis. The \"Financial Times\" reported that the BRICS may not be as badly affected as they were in the earlier crises of 2008 and 2011. However, smaller developing countries that must import a substantial portion of their food could be hard hit. The UN and G20 has begun contingency planning so as to be ready to intervene if a third global crisis breaks out.\nBy August 2013 however, concerns had been allayed, with above average grain harvests expected from major exporters, including Brazil, Ukraine and the U.S. 2014 also saw a good worldwide harvest, leading to speculation that grain prices could soon begin to fall.\nIn an April 2013 summit held in Dublin concerning Hunger, Nutrition, Climate Justice, and the post 2015 MDG framwework for global justice, Ireland's President Higgins said that only 10% of deaths from hunger are due to armed conflict and natural disasters, with ongoing hunger being both the \"greatest ethical failure of the current global system\" and the \"greatest ethical challenge facing the global community.\"\n$4.15 billion of new commitments were made to tackle hunger at a June 2013 Hunger Summit held in London, hosted by the governments of Britain and Brazil, together with The Children's Investment Fund Foundation.\n\nUndernutrition is an important determinant of maternal and child health, accounting for more than a third of child deaths and more than 10 percent of the total global disease burden according to 2008 studies.\n\nThe World Health Organization estimates that malnutrition accounts for 54 percent of child mortality worldwide, about 1 million children. Another estimate also by WHO states that childhood underweight is the cause for about 35% of all deaths of children under the age of five years worldwide.\n\nAs underweight children are more vulnerable to almost all infectious diseases, the \"indirect\" disease burden of malnutrition is estimated to be an order of magnitude higher than the disease burden of the \"direct\" effects of malnutrition. The combination of direct and indirect deaths from malnutrition caused by unsafe water, sanitation and hygiene (WASH) practices is estimated to lead to 860,000 deaths per year in children under five years of age.\n\nOlder sources sometimes claim this phenomenon is unique to developing countries, due to greater sexual inequality. More recent findings suggested that mothers often miss meals in advanced economies too. For example, a 2012 study undertaken by Netmums in the UK found that one in five mothers sometimes misses out on food to save their children from hunger.\n\nIn several periods and regions, gender has also been an important factor determining whether or not victims of hunger would make suitable examples for generating enthusiasm for hunger relief efforts. James Vernon, in his \"Hunger: A Modern History\", wrote that in Britain before the 20th century, it was generally only women and children suffering from hunger who could arouse compassion. Men who failed to provide for themselves and their families were often regarded with contempt. This changed after World War I, where thousands of men who had proved their manliness in combat found themselves unable to secure employment. Similarly, female gender could be advantageous for those wishing to advocate for hunger relief, with Vernon writing that being a woman helped Emily Hobhouse draw the plight of hungry people to wider attention during the Second Boer War.\nResearchers from the Centre for World Food Studies in 2003 found that the gap between levels of undernutrition in men and women is generally small, but that the gap varies from region to region and from country to country. These small-scale studies showed that female undernutrition prevalence rates exceeded male undernutrition prevalence rates in South/Southeast Asia and Latin America and were lower in Sub-Saharan Africa. Datasets for Ethiopia and Zimbabwe reported undernutrition rates between 1.5 and 2 times higher in men than in women; however, in India and Pakistan, datasets rates of undernutrition were 1.5-2 times higher in women than in men. Intra-country variation also occurs, with frequent high gaps between regional undernutrition rates. Gender inequality in nutrition in some countries such as India is present in all stages of life.\n\nStudies on nutrition concerning gender bias within households look at patterns of food allocation, and one study from 2003 suggested that women often receive a lower share of food requirements than men. Gender discrimination, gender roles, and social norms affecting women can lead to early marriage and childbearing, close birth spacing, and undernutrition, all of which contribute to malnourished mothers.\n\nWithin the household, there may be differences in levels of malnutrition between men and women, and these differences have been shown to vary significantly from one region to another, with problem areas showing relative deprivation of women. Samples of 1000 women in India in 2008 demonstrated that malnutrition in women is associated with poverty, lack of development and awareness, and illiteracy. The same study showed that gender discrimination in households can prevent a woman's access to sufficient food and healthcare. How socialization affects the health of women in Bangladesh, Najma Rivzi explains in an article about a research program on this topic. In some cases, such as in parts of Kenya in 2006, rates of malnutrition in pregnant women were even higher than rates in children.\n\nWomen in some societies are traditionally given less food than men since men are perceived to have heavier workloads. Household chores and agricultural tasks can in fact be very arduous and require additional energy and nutrients; however, physical activity, which largely determines energy requirements, is difficult to estimate.\n\nWomen have unique nutritional requirements, and in some cases need more nutrients than men; for example, women need twice as much calcium as men.\n\nDuring pregnancy and breastfeeding, women must ingest enough nutrients for themselves and their child, so they need significantly more protein and calories during these periods, as well as more vitamins and minerals (especially iron, iodine, calcium, folic acid, and vitamins A, C, and K). In 2001 the FAO of the UN reported that iron deficiency afflicted 43 percent of women in developing countries and increased the risk of death during childbirth. A 2008 review of interventions estimated that universal supplementation with calcium, iron, and folic acid during pregnancy could prevent 105,000 maternal deaths (23.6 percent of all maternal deaths). Malnutrition has been found to affect three quarters of UK women aged 16-49 indicated by them having less folic acid than the WHO recommended levels.\n\nFrequent pregnancies with short intervals between them and long periods of breastfeeding add an additional nutritional burden.\n\nAccording to the FAO, women are often responsible for preparing food and have the chance to educate their children about beneficial food and health habits, giving mothers another chance to improve the nutrition of their children.\n\nMalnutrition and being underweight are more common in the elderly than in adults of other ages. If elderly people are healthy and active, the aging process alone does not usually cause malnutrition. However, changes in body composition, organ functions, adequate energy intake and ability to eat or access food are associated with aging, and may contribute to malnutrition. Sadness or depression can play a role, causing changes in appetite, digestion, energy level, weight, and well-being. A study on the relationship between malnutrition and other conditions in the elderly found that malnutrition in the elderly can result from gastrointestinal and endocrine system disorders, loss of taste and smell, decreased appetite and inadequate dietary intake. Poor dental health, ill-fitting dentures, or chewing and swallowing problems can make eating difficult. As a result of these factors, malnutrition is seen to develop more easily in the elderly.\n\nRates of malnutrition tend to increase with age with less than 10 percent of the \"young\" elderly (up to age 75) malnourished, while 30 to 65 percent of the elderly in home care, long-term care facilities, or acute hospitals are malnourished. Many elderly people require assistance in eating, which may contribute to malnutrition. However, the mortality rate due to undernourishment may be reduced. Because of this, one of the main requirements of elderly care is to provide an adequate diet and all essential nutrients. Providing the different nutrients such as protein and energy keeps even small but consistent weight gain.\n\nIn Australia malnutrition or risk of malnutrition occurs in 80 percent of elderly people presented to hospitals for admission. Malnutrition and weight loss can contribute to sarcopenia with loss of lean body mass and muscle function. Abdominal obesity or weight loss coupled with sarcopenia lead to immobility, skeletal disorders, insulin resistance, hypertension, atherosclerosis, and metabolic disorders. A paper from the \"Journal of the American Dietetic Association\" noted that routine nutrition screenings represent one way to detect and therefore decrease the prevalence of malnutrition in the elderly.\n\n\n"}
{"id": "778440", "url": "https://en.wikipedia.org/wiki?curid=778440", "title": "Medical identification tag", "text": "Medical identification tag\n\nA medical identification tag is a small emblem or tag worn on a bracelet, neck chain, or on the clothing bearing a message that the wearer has an important medical condition that might require immediate attention. The tag is often made out of stainless steel or sterling silver. The intention is to alert a paramedic, physician, emergency department personnel or other first responders (emergency medical services, community first responder, Emergency medical responder) of the condition even if the wearer is not conscious enough, old enough, or too injured to explain. A wallet card with the same information may be used instead of or along with a tag, and a stick-on medical ID tag may be added or used alone.\n\nA type of medic identification alert is the USB medical alert tag, essentially a USB flash drive with capacity to store a great deal of emergency information, including contacts and medical conditions. This information is accessible by any computer with a USB port. However, the practical effectiveness of such a system is limited in many cases by medical computer systems that restrict the use of USB devices which may carry malware. It is also possible that a device carried by an unconscious person may not be their own, or not be up to date, with concomitant risks to health and legal liability of medical personnel.\n\nAnother new type of medic identification alert is QR code based medical alert stickers. The QR code on the sticker links to a web service that contains the individual's emergency information. The information is accessed by any first responder or emergency personnel by scanning the QR code by using a smartphone. Since a web service is used to store the information there is normally no limitation of how much information that can be stored.\n\nTypical conditions warranting wearing of such a tag are:\n\nIn addition to mention of the relevant medical condition(s), the tag may have a telephone number that medical personnel can call for more information, for example that of physician, care-giver or next of kin. Where applicable and provided, the wearer's national health service user number can enable access to a more detailed case history. Basically, the medical information tag, engraved with the wearer's personal medical problem or history, speak for the wearer when the wearer can't. Incidentally and where the symptoms can mislead, such a tag may also be useful as evidence of such a condition to law enforcement personnel.\n\nThere are various types of medical ID available. The most common form of medical ID is jewelry which provides a logo or inscription indicating a particular medical condition. These medical identification tags can be made out of stainless steel (usually classified as 316L and known as surgical stainless steel), sterling silver or gold. If found by emergency personnel the inscription provides an indication of your special medical needs. Tags are available with pre-engraved conditions or can be custom engraved with your specific medical histories and have the benefit of that all information is self-contained and does not require any form of technology to view in case of an emergency.\n\nAnother type of medical ID jewelry indicates membership in a medical information organization such as the MedicAlert Foundation, and American Medical ID. Such medical ID jewelry includes a member identification number and a toll-free number for medical emergency personnel to contact the organization and obtain full information about the wearer's medical conditions, treatment, and history. These organizations maintain a database of medical information on their members and can provide it to medical personnel when requested.\n\nThe newest technology allows the user to carry stickers with an NFC Tag. A similar technology allows the user to carry stickers with a QR code. By scanning the NFC Tag or the QR code with a smartphone, you will reach the stored medical alert information. Apple's IOS 8 operating system includes the facility for a mobile phone to contain the owner's medical emergency information.\n\nSilicone bracelets, preprinted with a general medical condition or allergy, are also popular. The lack of personalization may be a deterrent. Recently patients have begun to \"tattoo\" their medical condition on their wrist or arm. Although a permanent tattoo might be considered, a temporary tattoo works as well. Other items include stick on tags that stick onto a driver's license, wallet, or cell phone which are practical for the person who does not want to carry something extra advertising their medical condition.\n\nAnother type of medical jewelry is a pendant or wrist strap containing a wireless alert button, also known as a panic button, worn in the home as part of a wireless medical alert system. This type of medical jewelry sends a signal to a dialing console which contacts a medical alarm monitoring service or directly dials first responders when an emergency occurs.\n\nDevices marked \"ICE\" which can hold a significant amount of data and are readable by a computer are sold, typically USB flash drives with password-protected data entry providing read-only access to emergency medical data. However, it has been pointed out by a staff nurse with experience in trauma and critical care that such devices are worse than useless, at least in most situations in the UK, as medical computer systems are designed not to accept USB storage devices due to the risk of computer viruses. Additionally, there is no guarantee that ICE information even pertains to an unconscious person carrying it; using incorrect information can lead to patient harm and legal liability.\n\n"}
{"id": "39407203", "url": "https://en.wikipedia.org/wiki?curid=39407203", "title": "Mental health in Southeast Africa", "text": "Mental health in Southeast Africa\n\nMental health in Southeast Africa is a concern, where mental illness is prevalent. Mental health issues in Africa are often viewed as the \"silent crisis\" since they are often given lower priority in a region of Africa where international aid is focused on communicable diseases and malnutrition. Each country in Southeast Africa is consistently confronted with barriers that make mental health policies a challenge to implement, including the lack of policy, social and cultural barriers, the role of traditional medicine, HIV/AIDS, and the stigma surrounding mental health issues.\n\nProfessor Anyang' Nyong'o wrote a letter on June 23, 2011 to the Kenyan National Commission of Human Rights, stating, \"Currently there is a very big gap existing between the mental health needs of Kenyans and the existing mental health services at all levels of the health care services delivery system.\"\n\nAlthough there is a lack of attention to mental health in the healthcare systems, it is a major issue in Southeast African countries. Efforts have been made in some countries to allocate funding to mental health initiatives. Uganda's policies are a prime example of a successful effort to improve mental health in Southeast Africa. In 2006–2007, after undertaking an initial situational analysis of Uganda's mental health system, a new mental health policy was created. The vision for the project in Uganda is to eliminate, mental health issues, neurological disorders, and substance use from the population. From this vision, guiding principles were implemented, key priority areas were identified, policy objectives were selected, and the Uganda Ministry of Health began to make mental health a priority. The World Health Organization (WHO) continues to work with the developing nations of Africa to implement strategies to improve mental health situations and treatment in these countries. The few countries that have mental health policies in place, were created prior to 1990, and are in dire need of updating. Uganda had achieved great success with their new mental health policies and is working in close cooperation with the WHO to finalize their health-care policy. The WHO is working to move other countries' mental health policies forward.\n\nThe Southeast African Great Lakes region has a long-standing tribal history of traditional medicine and practices. Although some slight progress has been made with the availability of mental health resources in these areas, still many people in rural Southeast Africa utilize traditional methods to treat mental illness. According to Vikram Patel, a global mental health expert and professor at the London School of Hygiene and Tropical Medicine, the stigmatization of mental illness is due to traditional and cultural methods of dealing with these situations and can only be changed through improved support, education, and greater awareness. In traditional Southeast Africa, when there is a situation in which there is a mental health concern, the patient is usually treated without their consent. For an example, if the patient is violent or destructive, he is sedated by the traditional healer or by family members in order to commence therapy, and people are often isolated or treated against their will in order to prevent patients from hurting themselves or others. In a CNN series, \"African Voices\", leading psychiatrist Frank Njenga, states that when a patient has some form of illness such as depression or schizophrenia, community members and traditional healers often look at it as being possessed by demons or witchcraft.\n\nSome mental illnesses have been linked to an imbalance of chemicals in the brain called neurotransmitters. These neurotransmitters help nerve cells in the brain communicate. If these chemicals are out of balance or are not working properly, messages may not properly transmit through the brain, leading to symptoms of mental illness. In addition, defects in or Traumatic Brain Injury (TBI) to areas of the brain have also been linked to some mental conditions.\n\nMany socioeconomic factors may contribute to mental health. Such as, disease, political unrest, violence, and any kind of addiction. These may all play a role in the high incident of mental illness in Southeast Africa. Adequate care may not be available to sufferers, due to the stigma that surrounds people dealing with mental illness. The reversal of this stigma, is due to the lack of educational resources . In Southeast Africa many people have developed post-traumatic stress disorder relating to genocide, civil war, tribal clashes, and refugee situations. Particularly in Rwanda and Uganda, post-traumatic stress disorder affects a significant number of the population, due to recent conflict, genocide, and crimes against humanity.\n\n"}
{"id": "516838", "url": "https://en.wikipedia.org/wiki?curid=516838", "title": "Micro-g environment", "text": "Micro-g environment\n\nThe term micro-g environment (also µg, often referred to by the term microgravity) is more or less a synonym for \"weightlessness\" and \"zero-g\", but indicates that g-forces are not quite zero—just very small. The symbol for microgravity, \"µg\", was used on the insignias of Space Shuttle flights STS-87 and STS-107, because these flights were devoted to microgravity research in low Earth orbit.\n\nA \"stationary\" micro-g environment would require travelling far enough into deep space so as to reduce the effect of gravity by attenuation to almost zero. This is the simplest in conception, but requires traveling an enormous distance, rendering it most impractical. For example, to reduce the gravity of the Earth by a factor of one million, one needs to be at a distance of 6 million kilometers from the Earth, but to reduce the gravity of the Sun to this amount one has to be at a distance of 3.7 billion kilometers. (On Earth the gravity due to the rest of the Milky Way is already attenuated by a factor greater than one million, so we do not need to move away further from its center). Thus it is not impossible, but it has only been achieved so far by four interstellar probes (Voyager 1 and 2 of the Voyager program, and Pioneer 10 and 11 of the Pioneer program) and they did not return to Earth. To reduce the gravity to one thousandth of that on Earth's surface, one needs to be at a distance of 200,000 km.\n\nAt a distance relatively close to Earth (less than 3000 km), gravity is only slightly reduced. As an object orbits a body such as the Earth, gravity is still attracting objects towards the Earth and the object is accelerated downward at almost 1g. Because the objects are typically moving laterally with respect to the surface at such immense speeds, the object will not lose altitude because of the curvature of the Earth. When viewed from an orbiting observer, other close objects in space appear to be floating because everything is being pulled towards Earth at the same speed, but also moving forward as the Earth's surface \"falls\" away below. All these objects are in free fall, not zero gravity.\n\nCompare the gravitational potential at some of these locations.\n\nWhat remains is a micro-g environment moving in free fall, i.e. there are no forces other than gravity acting on the people or objects in this environment. To prevent air drag making the free fall less perfect, objects and people can free-fall in a capsule that itself, while not necessarily in free fall, is accelerated as in free fall. This can be done by applying a force to compensate for air drag. Alternatively free fall can be carried out in space, or in a vacuum tower or shaft.\n\nTwo cases can be distinguished: Temporary micro-g, where after some time the Earth's surface is or would be reached, and indefinite micro-g.\n\nA temporary micro-g environment exists in a drop tube (in a tower or shaft), a sub-orbital spaceflight, e.g. with a sounding rocket, and in an airplane such as used by NASA's Reduced Gravity Research Program, aka the Vomit Comet, and by the Zero Gravity Corporation. A temporary micro-g environment is applied for training of astronauts, for some experiments, for filming movies, and for recreational purposes.\n\nA micro-g environment for an indefinite time, while also possible in a spaceship going to infinity in a parabolic or hyperbolic orbit, is most practical in an Earth orbit. This is the environment commonly experienced in the International Space Station, Space Shuttle, etc. While this scenario is the most suitable for scientific experimentation and commercial exploitation, it is still quite expensive to operate in, mostly due to launch costs.\n\nObjects in orbit are not perfectly weightless due to several effects:\n\n\nIn a shot tower (now obsolete), molten metal (such as lead or steel) was dripped through a sieve into free fall. With sufficient height (several hundred feet), the metal would be solid enough to resist impact (usually in a water bath) at the bottom of the tower. While the shot may have been slightly deformed by its passage through the air and by impact at the bottom, this method produced metal spheres of sufficient roundness to be used directly in shotgun shells or to be refined by further processing for applications requiring higher accuracy.\n\nWhile not yet a commercial application, there has been interest in growing crystals in micro-g, as in a space station or automated artificial satellite, in an attempt to reduce crystal lattice defects. Such defect-free crystals may prove useful for certain microelectronic applications and also to produce crystals for subsequent X-ray crystallography.\n\nSpace Motion Sickness (SMS) is thought to be a subtype of motion sickness that plagues nearly half of all astronauts who venture into space. SMS, along with facial stuffiness from headward shifts of fluids, headaches, and back pain, is part of a broader complex of symptoms that comprise Space Adaptation Syndrome (SAS). SMS was first described in 1961 during the second orbit of the fourth manned spaceflight when the Cosmonaut, Gherman Titov aboard the Vostok 2, described feeling disoriented with physical complaints mostly consistent with motion sickness. It is one of the most studied physiological problems of spaceflight but continues to pose a significant difficulty for many astronauts. In some instances, it can be so debilitating that astronauts must sit out from their scheduled occupational duties in space – including missing out on a spacewalk they have spent months training to perform. In most cases, however, astronauts will work through the symptoms even with degradation in their performance.\n\nDespite their experiences in some of the most rigorous and demanding physical maneuvers on earth, even the most seasoned astronauts may be affected by SMS, resulting in symptoms of severe nausea, projectile vomiting, fatigue, malaise (feeling sick), and headache. These symptoms may occur so abruptly and without any warning that space travelers may vomit suddenly without time to contain the emesis, resulting in strong odors and liquid within the cabin which may affect other astronauts. Symptoms typically last anywhere from one to three days upon entering weightlessness, but may recur upon reentry to Earth’s gravity or even shortly after landing. SMS differs from terrestrial motion sickness in that sweating and pallor are typically minimal or absent and gastrointestinal findings usually demonstrate absent bowel sounds indicating reduced gastrointestinal motility.\n\nEven when the nausea and vomiting resolve, some central nervous system symptoms may persist which may degrade the astronaut’s performance. Graybiel and Knepton proposed the term “sopite syndrome” to describe symptoms of lethargy and drowsiness associated with motion sickness in 1976. Since then, their definition has been revised to include “…a symptom complex that develops as a result of exposure to real or apparent motion and is characterized by excessive drowsiness, lassitude, lethargy, mild depression, and reduced ability to focus on an assigned task.” Together, these symptoms may pose a substantial threat (albeit temporary) to the astronaut who must remain attentive to life and death issues at all times.\n\nSMS is most commonly thought to be a disorder of the vestibular system that occurs when sensory information from the visual system (sight) and the proprioceptive system (posture, position of the body) conflicts with misperceived information from the semicircular canals and the otoliths within the inner ear. This is known as the ‘neural mismatch theory’ and was first suggested in 1975 by Reason and Brand. Alternatively, the fluid shift hypothesis suggests that weightlessness reduces the hydrostatic pressure on the lower body causing fluids to shift toward the head from the rest of the body. These fluid shifts are thought to increase cerebrospinal fluid pressure (causing back aches), intracranial pressure (causing headaches), and inner ear fluid pressure (causing vestibular dysfunction).\n\nDespite a multitude of studies searching for a solution to the problem of SMS, it remains an ongoing problem for space travel. Most non-pharmacological countermeasures such as training and other physical maneuvers have offered minimal benefit. Thornton and Bonato noted, “Pre- and inflight adaptive efforts, some of them mandatory and most of them onerous, have been, for the most part, operational failures.” To date, the most common intervention is promethazine, an injectable antihistamine with antiemetic properties, but sedation can be a problematic side effect. Other common pharmacological options include metaclopromide, as well as oral and transdermal application of scopolamine, but drowsiness and sedation are common side effects for these medications as well.\n\nIn the space (or microgravity) environment the effects of unloading varies significantly among individuals, with sex differences compounding the variability. Differences in mission duration, and the small sample size of astronauts participating in the same mission also adds to the variability to the musculoskeletal disorders that are seen in space. In addition to muscle loss, microgravity leads to increased bone resorption, decreased bone mineral density, and increased fracture risks. Bone resorption leads to increased urinary levels of calcium, which can subsequently lead to an increased risk of nephrolithiasis.\n\nIn the first two weeks that the muscles are unloaded from carrying the weight of the human frame during space flight, whole muscle atrophy begins. Postural muscles contain more slow fibers, and are more prone to atrophy than non-postural muscle groups. The loss of muscle mass occurs because of imbalances in protein synthesis and breakdown. The loss of muscle mass is also accompanied by a loss of muscle strength, which was observed after only 2–5 days of spaceflight during the Soyuz-3 and Soyuz-8 missions. Decreases in the generation of contractile forces and whole muscle power have also been found in response to microgravity.\n\nTo counter the effects of microgravity on the musculoskeletal system, aerobic exercise is recommended. This often takes the form of in-flight cycling. A more effective regimen includes resistive exercises or the use of a penguin suit (contains sewn-in elastic bands to maintain a stretch load on antigravity muscles), centrifugation, and vibration. Centrifugation recreates Earth’s gravitational force on the space station, in order to prevent muscle atrophy. Centrifugation can be performed with centrifuges or by cycling along the inner wall of the space station. Whole body vibration has been found to reduce bone resorption through mechanisms that are unclear. Vibration can be delivered using exercise devices that use vertical displacements juxtaposed to a fulcrum, or by using a plate that oscillates on a vertical axis. The use of beta-2 adrenergic agonists to increase muscle mass, and the use of essential amino acids in conjunction with resistive exercises have been proposed as pharmacologic means of combating muscle atrophy in space.\n\nMicrogravity can also lead to height increases; in 2018 a Japanese astronout reported growing 2 centimetres in just three weeks of micro-gravity on board the International Space Station.\n\nNext to the skeletal and muscular system, the cardiovascular system is less strained in weightlessness than on Earth and is de-conditioned during longer periods spent in space. In a regular environment, gravity exerts a downward force, setting up a vertical hydrostatic gradient. When standing, some 'excess' fluid resides in vessels and tissues of the legs. In a micro-g environment, with the loss of a hydrostatic gradient, some fluid quickly redistributes toward the chest and upper body; sensed as 'overload' of circulating blood volume. In the micro-g environment, the newly sensed excess blood volume is adjusted by expelling excess fluid into tissues and cells (12-15% volume reduction) and red blood cells are adjusted downward to maintain a normal concentration (relative anemia). In the absence of gravity, venous blood will rush to the right atrium because the force of gravity is no longer pulling the blood down into the vessels of the legs and abdomen, resulting in increased stroke volume. These fluid shifts become more dangerous upon returning to a regular gravity environment as the body will attempt to adapt to the reintroduction of gravity. The reintroduction of gravity again will pull the fluid downward, but now there would be a deficit in both circulating fluid and red blood cells. The decrease in cardiac filling pressure and stroke volume during the orthostatic stress due to a decreased blood volume is what causes orthostatic intolerance. Orthostatic intolerance can result in temporary loss of consciousness and posture, due to the lack of pressure and stroke volume. More chronic orthostatic intolerance can result in additional symptoms such as nausea, sleep problems, and other vasomotor symptoms as well.\n\nMany studies on the physiological effects of weightlessness on the cardiovascular system are done in parabolic flights. It is one of the only feasible options to combine with human experiments, making parabolic flights the only way to investigate the true effects of the micro-g environment on a body without traveling into space. Parabolic flight studies have provided a broad range of results regarding changes in the cardiovascular system in a micro-g environment. Parabolic flight studies have increased the understanding of orthostatic intolerance and decreased peripheral blood flow suffered by Astronauts returning to Earth. Due to the loss of blood to pump, the heart can atrophy in a micro-g environment. A weakened heart can result in low blood volume, low blood pressure and affect the body's ability to send oxygen to the brain without the individual becoming dizzy. Heart rhythm disturbances have also been seen among astronauts, but it is not clear whether this was due to pre-existing conditions of effects of a micro-g environment. One current countermeasure includes drinking a salt solution, which increases the viscosity of blood and would subsequently increase blood pressure which would mitigate post micro-g environment orthostatic intolerance. Another countermeasure includes administration of midodrine, which is a selective alpha-1 adrenergic agonist. Midodrine produces arterial and venous constriction resulting in an increase in blood pressure by baroreceptor reflexes.\n\nSpace Motion Sickness can lead to degraded astronaut performance. SMS threatens operational requirements, reduces situational awareness, and threatens the safety of those exposed to micro-g environments. Lost muscle mass leads to difficulty with movement, especially when astronauts return to earth. This can pose a safety issue if the need for emergency egress were to arise. Loss of muscle power makes it extremely difficult, if not impossible, for astronauts to climb through emergency egress hatches or create unconventional exit spaces in the case of a crash upon landing. Additionally, bone resorption and inadequate hydration in space can lead to the formation of kidney stones, and subsequent sudden incapacitation due to pain. If this were to occur during critical phases of flight, a capsule crash leading to worker injury and/or death could result. Short-term and long-term health effects have been seen in the cardiovascular system from exposure to the micro-g environment that would limit those exposed after they return to Earth or a regular gravity environment. Steps need to be taken to ensure proper precautions are taken into consideration when dealing a micro-g environment for worker safety. Orthostatic intolerance can lead to temporary loss of consciousness due to the lack of pressure and stroke volume. This loss of consciousness inhibits and endangers those affected and can lead to deadly consequences.\n\n\n"}
{"id": "55223808", "url": "https://en.wikipedia.org/wiki?curid=55223808", "title": "Miscarriage and grief", "text": "Miscarriage and grief\n\nMiscarriage and grief are both an event and subsequent process of grieving that develops in response to a miscarriage. Almost all those experiencing a miscarriage experience grief. This event is often considered to be identical to the loss of a child and has been described as traumatic. \"Devastation\" is another descriptor of miscarriage. Grief differs from the emotion sadness. Sadness is an emotion along with grief, on the other hand, is a response to the loss a of the bond or affection was formed and is a process rather than one single emotional response. Grief is not equivalent to depression. Grief also has physical, cognitive, behavioral, social, cultural, and philosophical dimensions. Bereavement and mourning refer to the ongoing state of loss, and grief is the reaction to that loss. Emotional responses may be bitterness, anxiety, anger, surprise, fear, and disgust and blaming others; these responses may persist for months. Self-esteem can be diminished as another response to miscarriage. Not only does miscarriage tend to be a traumatic event, women describe their treatment afterwards to be worse than the miscarriage itself.\n\nA miscarriage can often be \"heart-breaking\". A miscarriage can effect the women, husband, partner, siblings, grandparents, the whole family system and friends. Almost all those experiencing a miscarriage go through a grieving process. Serious emotional impact is usually experienced immediately after the miscarriage. Some may go through the same loss when an ectopic pregnancy is terminated. In some, the realization of the loss can take weeks. Providing family support to those experiencing the loss can be challenging because some find comfort in talking about the miscarriage while others may find the event painful to discuss. The father of the baby can have the same sense of loss. Expressing feelings of grief and loss can sometimes be harder for men. Some women are able to begin planning their next pregnancy after a few weeks of having the miscarriage. For others, planning another pregnancy can be difficult. Organizations exist that provide information and counselling to help those who have had a miscarriage. Some women have a higher risk of developing prolonged grief and complicated grief than others.\n\nMiscarriage has an emotional effect and can also lead to psychological disorders. One discorder that can develop is primary maternal preoccupation. This is defined as a \" ...'special psychiatric condition' in which the pregnant woman identifies with her baby, highlights the crisis a woman faces when the baby with whom she is preoccupied and identified dies...\" Grieving manifests itself differently for each woman after miscarriage. It may often go unrecognized. The grief that follows a miscarriage resembles, but is not the same as, the grief experienced after the loss of a family member. Disbelief, depression, anger, and yearning, are described as being a part of the normal grieving process. These reactions remain from three to nine months after the loss. Forty-one percent of parents experience a normal, expected decline in grief in the first two years while 59% were delayed in the resolution of their grief.\n\nGrieving can create feelings of loneliness.\nThis grieving has been called a type of psychological trauma. Other serious consequences can develop including depression, anxiety disorder, post-traumatic stress disorder, and somatoform disorder. These responses all are associated with grieving after a miscarriage. Some women are able to complete the grieving process a few weeks after the miscarriage and start anticipating their next pregnancy. Planning another pregnancy is traumatic for others. The impact of a miscarriage can be \"crippling\" psychologically. Anger can be directed toward those who have had successful pregnancies and children. A woman can grieve the \"loss of a future child\" and question her own role as a mother. They may blame themselves or their partner for the miscarriage.\n\nUnsuccessful attempts to become pregnant through in vitro fertilization (IVF) can also illicit a similar grief response in women. Those experiencing a late miscarriage may have more significant distress compared to those who have experienced a miscarriage in the first trimester. Even depression can occur.\n\n\"Women today...are aught in a unique historical moment: technology encourages them to form emotional attachments to their pregnancies, but society has not developed traditions to cushion the shock when those attachmets are shattered.\"\n\nDescriptions of the miscarriage are expressed in non-clinical terms by those who have experienced the event.\n\nMiscarriage has been found to be a traumatic event and a major loss for women. Pregnancy loss, including induced abortion is a risk factor for mental illness. The impact of miscarriage can be underestimated. The trauma can be compounded if the miscarriage was accompanied by visible and relatively large amounts of blood loss.\nThe trauma of miscarriage can be compounded if the miscarriage was accompanied by visible and relatively large amounts of blood loss.\nBipolar disorders are associated with miscarriage. Depression and bilpolar disorder becomes evident after a miscarriage in 43% of women. Some women are more likely to experience complicated and prolonged grief than others.\n\nWomen experiencing miscarriage are at risk for grief reactions, anxiety or depression. Obsessiveness regarding the miscarriage can develop. Primary maternal preoccupation is also considered a consequence of miscarriage. This condition can occur if a woman who develop a close bond \"with her baby\" experiences the loss of the pregnancy.\n\nDifferent grieving \"styles\" can exist and vary between individuals. There can be a complete avoidance of dealing with the memories of the miscarriage and there can be an \"obsessive\" concentration on an event in the miscarriage. This is in contrast with the expected ability to \"reminisce about the loss of a loved one\". Complicated grief differs from the more common form of grief that occurs after a miscarriage. The grieving process associated with other events such as the loss of a spouse or parent is expected to decline in predictable and steady rate. This not true for those experiencing grief after a miscarriage because only 41% follow the expected decline in grief while most, 59% do not fit this pattern.\n\nMiscarriage is associated with post traumatic stress disorder.\nRisks for developing PTSD after miscarriage are: emotional pain, expressions of emotion, and low levels of social support. Even if relatively low levels of stress occur after the miscarriage, symptoms of PTSD including flashbacks, intrusive thoughts, dissociation and hyperarousal can later develop. The effects of stress can complicate miscarriage. Miscarriage is a stressful event and because stress is a risk factor for subsequent miscarriage, its presence can become part of cycle that continues. Lower stress levels are associated with more favorable outcomes in future pregnancies while higher stress levels increase the risk.\n\nPhysical recovery from miscarriage can have an effect on emotional disturbances. The body has to recover from the sudden pregnancy loss. In some instances ffatigue is present. Insomnia can be a problem. The miscarriage is very upsetting to the family and can generate very strong emotions. Some women may feel that the miscarriage occurred because they somehow had caused it. Others may blame the father or partner for the miscarriage. Coping with a miscarriage can very greatly between women and families. Some find it difficult to talk about the miscarriage. The narratives of women tend to coincide with quantified and measurable effects. Some women engage in activities that are believed to aid in recovery such as therapy, religion and art.\nCounseling can be offered but effective interventions to assist in recovery have been difficult to identify due to the reports of efficacy and ineffective counseling. Comparisons are hard to make. Despite the lack of studies that describe effective interventions for those with grief after a miscarriage, some clinicians still offer counselling and follow-up to help women recover and adapt to the loss.\n\nRecommendations to help recover from the event include:\n\nGenerally, the impact of experiencing miscarriage is underestimated. Other methods used to promote recovery are be relaxation techniques, guided imagery, and \"thought-stopping\". Even Gestalt role-playing has been used. Some women can \"emotionally relocate the child\", redefine a relationship \"with the missing child\", and engage in \"continuing the bond\" to incorporate the loss into their life experiences.\n\nWomen who have miscarried report that they were dissatisfied with the care they received from physicians and nurses. One observer highlights the insensitivity of some health care providers when they approach the grieving mother \"...by playing down her emotion as somehow an irrational response...\" Clinicians may not recognize the psychological impact of the miscarriage and can \"expect parents to move on with their lives.\"\n\nSince the experiences of women can vary so widely, sensitive nursing care afterward is appropriate.\n\nOne emotional response to miscarriage is the strong apprehension that can develop anticipating a subsequent pregnancy. Procreation abilities may also be questioned by the woman. Significant distress can develop in the other children in the family when they think a sibling has died. They may regard this as a baby they did not get to meet. They can also experience grief and guilt find it difficult to express these emotions to their parents. The siblings may feel a need to act as if everything is the same and that they are unaffected in an attempt to protect their parents from their own feelings. Children can also need help, understanding and the ability to make sense of the event.\n\nRituals that recognize the loss can be important in coping. Family and friends often conduct a memorial or burial service. Hospitals also can provide support and help memorialize the event. Depending on locale others desire to have a private ceremony. The religious faith of the woman and family impacts the grief process. Conversely, the lack of recognition that the miscarriage has occurred by family and friends can be troubling and add to the trauma of the event.\nGrieving after the loss of a child through miscarriage in other cultures can vary from western culture. An individual’s culture plays a large role in determining an inappropriate pattern of grief, and it is appropriate to take into account cultural norms before reaching a complicated grief diagnosis. There are cultural differences in emotional levels, how these are expressed and how long they are expressed. External symptoms of grief differ in non-Western cultures, presenting increased somatization. Narratives by Swedish women include their own perception of losing a child. Investigations describe their grief over their miscarriage: \"When miscarriage occurs it is not a gore, an embryo, or a fetus they lose, it is their child. They feel that they are the cause of the miscarriage through something they have done, eaten, or thought. They feel abandonment and they grieve for their profound loss; they are actually in bereavement.\" Native American women have cut their long hair following the death of a family member. The narratives of women tend to coincide with quantified and measurable effects. In women who are induced to have an abortion, an identical grieving process can occur. The emotional responses to a spontaneous abortion (miscarriage) and an elective abortion are sometimes identical. Spanish women experience grief in much the same way in the rest of Western culture. Some women find online forums helpful.\n\n\n\n"}
{"id": "55326583", "url": "https://en.wikipedia.org/wiki?curid=55326583", "title": "Miscarriage and mental illness", "text": "Miscarriage and mental illness\n\nMental illness can be a consequence of miscarriage or early pregnancy loss. Even though women can develop long-term psychiatric symptoms after a miscarriage, acknowledging the potential of mental illness is not usually considered. A mental illness can develop in women who have experienced one or more miscarriages after the event or even years later. Some data suggest that men and women can be affected up to 15 years after the loss. Though recognized as a public health problem, studies investigating the mental health status of women following miscarriage are still lacking. Posttraumatic stress disorder (PTSD) can develop in women who have experienced a miscarriage. Risks for developing PTSD after miscarriage include emotional pain, expressions of emotion, and low levels of social support. Even if relatively low levels of stress occur after the miscarriage, symptoms of PTSD including flashbacks, intrusive thoughts, dissociation and hyperarousal can later develop. Clinical depression also is associated with miscarriage. Past responses by clinicians have been to prescribe sedatives.\n\nRecurring miscarriage may increase the incidence of intrusive thoughts in women and their partners.\n\nMiscarriage has an emotional effect and can also lead to psychological disorders. One disorder that can develop is primary maternal preoccupation. This psychological trauma can develop as a response to early pregnancy loss. Anxiety can also develop as a result of a miscarriage. Women describe the medical treatment that they receive contributed their distress. Even if relatively low levels of stress occur after the miscarriage, symptoms of PTSD including flashbacks, dissociation and hyperarousal can later develop. Clinical depression also is associated with miscarriage.\n\nIntrusive thoughts can develop after the loss. Panic disorder and obsessive thoughts may also develop as a response to a miscarriage. Men may experience pain and psychological effects but react by adopting “compensatory behaviours” such as increasing consumption of alcohol. Because men can consider their role to be supportive, they may not have their loss recognized.\n\nWomen who have had clinical depression before the miscarriage are more likely to develop PTSD. Posttraumatic stress disorder is associated with miscarriage along with other traumatic events associated with pregnancy. Those who experience recurrent miscarriage (>3) have a greater risk of developing PTSD than those who have experienced miscarriage once. An association between the gender of the infant lost through miscarriage exists whereby there is an increased chance of developing PTSD if the infant was a male. Knowing the cause of the miscarriage does not reduce the risk of developing PTSD. Finding a 'meaning' for the loss reduces the risk of developing PTSD. A negative outlook regarding the world in general is corrlated with increased levels of PTSD. Poor self-esteem is also related to developing PTSD after the loss. If memories of the loss are considered intense, risk for PTSD is increased. There are concerns that PTSD in mothers may have a negative impact on children born after the event.\n\nThough the development of PTSD in women and families after the loss has been identified, the presence of PTSD in a woman who is pregnant is detrimental. Women with PTSD are thought to be at a higher risk of prenatal loss, perinatal loss, pregnancy complications, ectopic pregnancy, preterm birth and growth abnormalities in the fetus.\n\nPTSD in a mother is suspected to increase the risk of autism, hypertension, cardiovascular diseases, and type 2 diabetes in a child.\n\n43% of women who miscarry are found to have depression, anxiety disorders, and obsessive-compulsive disorder. \n\nComplicated grief is an atypical response to a miscarriage. It differs from the more common form of grief that occurs after a miscarriage. The grieving process associated with other events such as the loss of a spouse or parent is expected to decline in a predictable and steady rate. This not true for those experiencing grief after a miscarriage because only 41% follow the expected decline in grief while most (59%) do not fit this pattern.\n\nCognitive Behavior Therapy has been found to be helpful if it is begun immediately after the loss.\n\nA woman who miscarries has a 2.5 times greater risk for depression than those who have not. In the US, estimates of PTSD related to miscarriage are thought to be 150,000–200,000 acute and 24,000–32,000 chronic PTSD cases. PTSD in fathers may be significant but remains unaddressed.\n\n"}
{"id": "79449", "url": "https://en.wikipedia.org/wiki?curid=79449", "title": "Multiple birth", "text": "Multiple birth\n\nA multiple birth is the culmination of one multiple pregnancy, wherein the mother delivers two or more offspring. A term most applicable to placental species, multiple births occur in most kinds of mammals, with varying frequencies. Such births are often named according to the number of offspring, as in \"twins\" and \"triplets\". In non-humans, the whole group may also be referred to as a \"litter\", and multiple births may be more common than single births. Multiple births in humans are the exception and can be exceptionally rare in the largest mammals.\n\nEach fertilized egg (zygote) may produce a single embryo, or it may split into two or more embryos, each carrying the same genetic material. Fetuses resulting from different zygotes are called \"fraternal\" and share only 50% of their genetic material, as ordinary full siblings from separate births do. Fetuses resulting from the same zygote share 100% of their genetic material and hence are called \"identical\". Identical twins are always the same sex, except in cases of Klinefelter syndrome (also known as XXY syndrome and 47,XXY syndrome).\n\nA multiple pregnancy may be the result of the fertilization of a single egg that then splits to create identical fetuses, or it may be the result of the fertilization of multiple eggs that create fraternal fetuses, or it may be a combination of these two. A multiple pregnancy from a single zygote is called \"monozygotic\", from two zygotes is called \"dizygotic\", or from three or more zygotes is called \"polyzygotic\".\n\nSimilarly, the siblings themselves from a multiple birth may be referred to as monozygotic if they are identical or as polyzygotic if they are fraternal.\n\nTerms used for the number of offspring in a multiple birth:\n\nTerms used for multiple births or the genetic relationships of their offspring:\n\nMultiple pregnancies are also classified by how the fetuses are surrounded by one or more placentas and amniotic sacs; the placentas are referenced as \"chorions\":\n\nIn humans, the average length of pregnancy (two weeks fewer than gestation) is 38 weeks with a single fetus. This average decreases for each additional fetus: to thirty-six weeks for twin births, thirty-two weeks for triplets, and thirty weeks for quadruplets. With the decreasing gestation time, the risks from immaturity at birth and subsequent viability increase with the size of the sibling group. Only as of the twentieth century have more than four all survived infancy.\n\nRecent history has also seen increasing numbers of multiple births. In the United States, it has been estimated that by 2011, 36% of twin births and 78% of triplet and higher-order births resulted from conception by assisted reproductive technology.\n\nTwins are by far the most common form of multiple births in humans. The U.S. Centers for Disease Control and Prevention report more than 132,000 sets of twins out of 3.9 million births of all kinds each year, about 3.4%, or 1 in 30.\n\nIdentical triplets come from a monozygotic pregnancy, three fetuses from one egg. The most common set, strictly fraternal triplets, comes from a polyzygotic pregnancy of three eggs. Between these types, triplets that include an identical (monozygotic) pair of siblings plus a fraternal sibling are the result of a dizygotic pregnancy, where one zygote divides into two fetuses and the other doesn't.\n\nTriplets are far less common than twins, according to the U.S. Centers for Disease Control and Prevention, accounting for only about 4300 sets in 3.9 million births, just a little more than 0.1%, or 1 in 1000. According to the American Society of Reproductive Medicine, only about 10% of these are identical triplets: about 1 in ten thousand. Nevertheless, only 4 sets of identical triplets were reported in the U.S. during 2015, about one in a million. According to Victor Khouzami, Chairman of Obstetrics at Greater Baltimore Medical Center, \"No one really knows the incidence\".\n\nIdentical triplets or quadruplets are very rare and result when the original fertilized egg splits and then one of the resultant cells splits again (for triplets) or, even more rarely, a further split occurs (for quadruplets). The odds of having identical triplets is unclear. News articles and other non-scientific organizations give odds from one in 60,000 to one in 200 million pregnancies.\n\nQuadruplets are much rarer than twins or triplets. As of 2007, there were approximately 3500 sets recorded worldwide. Quadruplet births are becoming increasingly common due to fertility treatments. There are around 70 sets of all-identical quadruplets worldwide. Many sets of quadruplets contain a mixture of identical and fraternal siblings, such as three identical and one fraternal, two identical and two fraternal, or two pairs of identicals. One famous set of identical quadruplets was the Genain quadruplets, all of whom developed schizophrenia.\nQuadruplets are sometimes referred to as \"quads\" in Britain.\n\nQuintuplets occur naturally in 1 in 55,000,000 births. The first quintuplets known to survive infancy were the all-female Canadian Dionne Quintuplets, born in 1934. Quintuplets are sometimes referred to as \"quins\" in the UK and \"quints\" in North America.\n\nBorn in Liverpool, England on 18 November 1983, the Walton sextuplets were the world's first all-female surviving sextuplets, and the world's fourth known set of surviving sextuplets. Another well-known set of sextuplets is the Gosselin sextuplets, born May 10, 2004 in Hershey, Pennsylvania. Reality television shows called Jon & Kate Plus 8 and later Kate Plus 8 have chronicled the lives of these sextuplets.\n\nIn 1997, the McCaughey septuplets were born in Carlisle, Iowa. Multiple births of as many as eight babies have been born alive, the first set on record to the Chukwu family in Texas in 1998; one died and seven survived. In 2009, a second set, the Suleman octuplets, were born in Bellflower, California. As of 2014, all were still alive shortly before their fifth birthday.\n\nThere have been a few sets of nonuplets (9) in which a few babies were born alive, though none lived longer than a few days. There have been cases of human pregnancies that started out with ten, eleven, twelve or fifteen fetuses, but no instances of live births. The pregnancies of the 10, 11 and 15 fetuses have all resulted from fertility medications and assisted reproductive technology (ART). However, there has been one documented case when 12 fetuses were conceived naturally.\n\nThe frequency of \"N\" multiple births from natural pregnancies has been given as approximately 1:89 (\"Hellin's law\") and as about 1:80. This gives:\n\nNorth American dizygotic twinning occurs about once in 83 conceptions, and triplets about once in 8000 conceptions. US figures for 2010 were:\n\nHuman multiple births can occur either naturally (the woman ovulates multiple eggs or the fertilized egg splits into two) or as the result of infertility treatments such as IVF (several embryos are often transferred to compensate for lower quality) or fertility drugs (which can cause multiple eggs to mature in one ovulatory cycle).\n\nFor reasons that are not yet known, the older a woman is, the more likely she is to have a multiple birth naturally. It is theorized that this is due to the higher level of follicle-stimulating hormone that older women sometimes have as their ovaries respond more slowly to FSH stimulation.\n\nThe number of multiple births has increased over the last decade. For example, in Canada between 1979 and 1999, the number of multiple birth babies increased 35%. Before the advent of ovulation-stimulating drugs, triplets were quite rare (approximately 1 in 8000 births) and higher-order births much rarer still. Much of the increase can probably be attributed to the impact of fertility treatments, such as in-vitro fertilization. Younger patients who undergo treatment with fertility medication containing artificial FSH, followed by intrauterine insemination, are particularly at risk for multiple births of higher order.\nCertain factors appear to increase the likelihood that a woman will naturally conceive multiples. These include:\n\nThe increasing use of fertility drugs and consequent increased rate of multiple births has made the phenomenon of multiples more frequent and hence more visible. In 2004 the birth of sextuplets, six children, to Pennsylvania couple Kate and Jon Gosselin helped them to launch their television series, originally \"Jon & Kate Plus 8\" and (following their divorce) \"Kate Plus 8\", which became the highest-rated show on the \"TLC\" network.\n\nBabies born from multiple-birth pregnancies are much more likely to result in premature birth than those from single pregnancies. 51% of twins and 91% of triplets are born preterm, compared to 9.4% in singletons. 14% of twins and 41% of triplets are even born \"very preterm\", compared to 1.7% in singletons.\n\nAt present, there is no way to stop twins being born early. In women with single pregnancies drugs called betamimetics can relax the muscles of the uterus and delay birth. Giving betamimetics can give more time to give steroids, for the baby’s lung development, or to transfer the mother to a hospital with a special care unit.\n\nHowever, there is not enough evidence to say whether or not women with twin pregnancies should be given oral betamimetics to reduce the risk of preterm birth. In some studies betamimetics have reduced the rate of preterm labour in twin pregnancies however the studies are too small to draw any solid conclusions. Likewise, putting a stitch in the neck of the womb (a cervical suture) to prevent premature birth has not been shown to work in women carrying more than one baby due to the small sample sizes in the studies.\n\nThe preterm births also result in multiples tending to have a lower birth weight compared to singletons.\n\nSome evidence indicates that only 1.10% of singletons are born with a very low birth weight and 10.12% twins and 31.88% triplets were found to be born with very low birth weight. This study was conducted by looking at the statistics from the U.S. Natality Files (5).\n\nAmong the exceptions are the Kupresak triplets of Mississauga, Ontario, Canada; their combined weight at birth in 2008, of 17 lbs, 2.7 ounces, set a world record.\n\nCerebral palsy is more common among multiple births than single births, being 2.3 per 1,000 survivors in singletons, 13 in twins, and 45 in triplets in North West England. This is likely a side effect of premature birth and low birth weight.\n\nMultiples may be monochorionic, sharing the same chorion, with resultant risk of twin-to-twin transfusion syndrome. Monochorionic multiples may even be monoamniotic, sharing the same amniotic sac, resulting in risk of umbilical cord compression and nuchal cord. In very rare cases, there may be conjoined twins, possibly impairing function of internal organs.\n\nMultiples are also known to have a higher mortality rate. It is more common for multiple births to be stillborn, while for singletons the risk is not as high. A literary review on multiple pregnancies shows a study done on one set each of septuplets and octuplets, two sets of sextuplets, 8 sets of quintuplets, 17 sets of quadruplets, and 228 sets of triplets. By doing this study, Hammond found that the mean gestational age (how many weeks when birthed) at birth was 33.4 weeks for triplets and 31 weeks for quadruplets. This shows that stillbirth happens usually 3–5 weeks before the woman reaches full term and also that for sextuplets or higher it almost always ends in death of the fetuses.\nThough multiples are at a greater risk of being stillborn, there is inconclusive evidence whether the actual mortality rate is higher in multiples than in singletons.\n\nToday many multiple pregnancies are the result of in vitro fertilisation (IVF). In a study in 1997 of 2,173 embryo transfers performed as part of in vitro fertilisation (IVF), 34% were successfully delivered pregnancies. The overall multiple pregnancy rate was 31.3% (24.7% twins, 5.8% triplets, and .08% quadruplets). Because IVFs are producing more multiples, a number of efforts are being made to reduce the risk of multiple births- specifically triplets or more. Medical practitioners are doing this by limiting the number of embryos per embryo transfer to one or two. That way, the risks for the mother and fetuses are decreased.\n\nThe appropriate number of embryos to be transferred depends on the age of the woman, whether it is the first, second or third full IVF cycle attempt and whether there are top-quality embryos available. According to a guideline from The National Institute for Health and Care Excellence (NICE) in 2013, the number of embryos transferred in a cycle should be chosen as in following table:\n\nAlso, it is recommended to use single embryo transfer in all situations if a top-quality blastocyst is available.\n\nBed rest has not been found to change outcomes and therefore is not generally recommended outside of a research study.\n\nSelective reduction is the practice of reducing the number of fetuses in a multiple pregnancy; it is also called \"multifetal reduction\". \n\nThe procedure generally takes two days; the first day for testing in order to select which fetuses to kill, and the second day for the procedure itself, in which potassium chloride is injected into the heart of each selected fetus under the guidance of ultrasound imaging. Risks of the procedure include bleeding requiring transfusion, rupture of the uterus, retained placenta, infection, a miscarriage, and prelabor rupture of membranes. Each of these appears to be rare.\n\nSelective reduction was developed in the mid-1980s, as people in the field of assisted reproductive technology became aware of the risks that multiple pregnancies carried for the mother and for the fetuses.\n\nWomen with a multiple pregnancy are usually seen more regularly by midwives or doctors than those with singleton pregnancies because of the higher risks of complications. However, there is currently no evidence to suggest that specialised antenatal services produce better outcomes for mother or babies than ‘normal’ antenatal care.\n\nAs preterm birth is such a risk for women with multiple pregnancies, it has been suggested that these women should be encouraged to follow a high-calorie diet to increase the birth weights of the babies. Evidence around this subject is not yet good enough to advise women to do this because the long term effects of the high-calorie diets on the mother are not known.\n\nA study in 2013 involving 106 participating centers in 25 countries came to the conclusion that, in a twin pregnancy of a gestational age between 32 weeks 0 days and 38 weeks 6 days, and the first twin is in cephalic presentation, planned Cesarean section does not significantly decrease or increase the risk of fetal or neonatal death or serious neonatal disability, as compared with planned vaginal delivery. In this study, 44% of the women planned for vaginal delivery still ended up having Cesarean section for unplanned reasons such as pregnancy complications. In comparison, it has been estimated that 75% of twin pregnancies in the United States were delivered by Cesarean section in 2008. Also in comparison, the rate of Cesarean section for all pregnancies in the general population varies between 40% and 14%.\n\nFetal position (the way the babies are lying in the womb) usually determines if they are delivered by caesarean section or vaginally. A review of good quality research on this subject found that if the twin that will be born first (i.e. is lowest in the womb) is head down there is no good evidence that caesarean section will be safer than a vaginal birth for the mother or babies.\n\nMonoamniotic twins (twins that form after the splitting of a fertilised egg and share the same amniotic fluid sac) are at more risk of complications than twins that have their own sacs. There is also insufficient evidence around whether to deliver the babies early by caesarean section or to wait for labour to start naturally while running checks on the babies’ wellbeing. The birth of this type of twins should therefore be decided with the mother and her family and should take into account the need for good neonatal care services.\n\nCesarean delivery is needed when first twin is in non cephalic presentation or when it is a monoamniotic twin pregnancy.\n\nMultiple-birth infants are usually admitted to neonatal intensive care immediately after being born. The records for all the triplet pregnancies managed and delivered from 1992-1996 were looked over to see what the neonatal statistics were. Kaufman found from reviewing these files that during a five-year period, 55 triplet pregnancies, which is 165 babies, were delivered. Of the 165 babies 149 were admitted to neonatal intensive care after the delivery.\n\nA study by the U.S. Agency for Healthcare Research and Quality found that, in 2011, pregnant women covered by private insurance in the United States were older and more likely to have multiple gestation than women covered by Medicaid.\n\nCertain cultures consider multiple births a portent of either good or evil.\n\nMayan culture saw twins as a blessing, and was fascinated by the idea of two bodies looking alike. The Mayans used to believe that twins were one soul that had fragmented.\n\nIn Ancient Rome, the legend of the twin brothers who founded the city (Romulus and Remus) made the birth of twin boys a blessing, while twin girls were seen as an unlucky burden, since both would have to be provided with an expensive dowry at about the same time.\n\nIn Greek mythology, fraternal twins Castor and Polydeuces, and Heracles and Iphicles, are sons of two different fathers. One of the twins (Polydeuces, Heracles) is the illegitimate son of the god Zeus; his brother is the son of their mother's mortal husband. A similar pair of twin sisters are Helen (of Troy) and Clytemnestra (who are also sisters of Castor and Polydeuces). The theme occurs in other mythologies as well, and is called \"superfecundation\".\n\nIn certain medieval European chivalric romances, such as Marie de France's \"Le Fresne\", a woman cites a multiple birth (often to a lower-class woman) as proof of adultery on her part; while this may reflect a widespread belief, it is invariably treated as malicious slander, to be justly punished by the accuser having a multiple birth of her own, and the events of the romance are triggered by her attempt to hide one or more of the children. A similar effect occurs in the Knight of the Swan romance, in the Beatrix variants of the Swan-Children; her taunt is punished by giving birth to seven children at once, and her wicked mother-in-law returns her taunt before exposing the children.\n\n\n"}
{"id": "40840374", "url": "https://en.wikipedia.org/wiki?curid=40840374", "title": "National Treatment Purchase Fund", "text": "National Treatment Purchase Fund\n\nThe National Treatment Purchase Fund (NTPF) is an Irish government body which was established to decrease waiting lists in the Irish public healthcare system. \n\nThe Fund was established under Statutory Instrument 179 - National Treatment Purchase Fund (Establishment) Order, 2004, and the Nursing Homes Support Scheme Act (2009). The NTPF reduced waiting times for procedures from between 2 and 5 years in 2002 to an average of 2.4 months in 2009.\n\n"}
{"id": "58424663", "url": "https://en.wikipedia.org/wiki?curid=58424663", "title": "Notifiable diseases in the United Kingdom", "text": "Notifiable diseases in the United Kingdom\n\nA notifiable disease is one which that has to be reported to the government authorities as required by law. In the United Kingdom, notification of infectious diseases is a statutory duty for registered medical practitioners and laboratories, under the Public Health (Control of Disease) Act 1984 and the Health Protection (Notification) Regulations 2010. The registered medical practitioners shall notify such diseases in a proper form within 3 days, or notify verbally via phone within 24 hours depending on the urgency of the situation.\n\nThe diseases notifiable to local authority under the Health Protection(Notification) Regulations 2010 are:\n\nThe causative organisms which the laboratories shall notify to the proper authority under the Health Protection (Notification) Regulations 2010 are:\n\n\n"}
{"id": "16063653", "url": "https://en.wikipedia.org/wiki?curid=16063653", "title": "Nutrition transition", "text": "Nutrition transition\n\nNutrition transition is the shift in dietary consumption and energy expenditure that coincides with economic, demographic, and epidemiological changes. Specifically the term is used for the transition of developing countries from traditional diets high in cereal and fiber to more Western pattern diets high in sugars, fat, and animal-source food.\n\nThe nutrition transition model was first proposed in 1993 by Barry Popkin, and is the most cited framework in literature regarding the nutrition transition, although it has been subject to some criticism for being overly simplified. Popkin posits that two other historic transitions affect and are affected by nutritional transition. The first is the demographic transition, whereby a pattern of high fertility and high mortality transforms to one of low fertility and low mortality. Secondly, an epidemiological transition occurs, wherein a shift from a pattern of high prevalence of infectious diseases associated with malnutrition, and with periodic famine and poor environmental sanitation, to a pattern of high prevalence of chronic and degenerative diseases associated with urban-industrial lifestyles is shown. These concurrent and dynamically influenced transitions share an emphasis on the ways in which populations move from one pattern to the next. \nPopkin used five broad patterns to help summarize the nutrition transition model. While these patterns largely appear chronological, it is important to note that they are not restricted to certain periods of human history and still characterize certain geographic and socioeconomic subpopulations. The first pattern is that of collecting food, a characterization of hunter-gatherers, whose diets were high in carbohydrates and low in fat, especially saturated fat. The second pattern is defined by famine, a marked scarcity and reduced variation of the food supply. The third pattern is one of receding famine. Fruits, vegetables, and animal protein consumption increases, and starchy staples become less important in the diet. The fourth pattern is one of degenerative diseases onset by a diet high in total fat, cholesterol, sugar, and other refined carbohydrates and low in polyunsaturated fatty acids and fiber. This pattern is often accompanied by an increasingly sedentary lifestyle. The fifth pattern, and most recently emerging pattern, is characterized by a behavioral change reflective of a desire to prevent or delay degenerative diseases. Recent and rapid changes seen in developing countries from the second and third pattern to the fourth is the common focus of nutrition transition research and desire for policy that would emphasize a healthier overall diet characterizes the shift from the fourth to the fifth pattern.\n\nThe nutrition transition has much of its roots in economic factors related to the development of a nation or subpopulations within a nation. It was once believed that current nutrition transition was endemic only to industrialized nations like the United States, but increasing research has indicated that not only is nutrition transition occurring most rapidly in low- and middle-income developing countries, the stress of its effects stands to burden the poorest populations of these countries the most as well.\nThis shift is attributable to many causes. Globalization has played a large role in altering the access and availability of foods in formerly undeveloped nations. Demographic shifts from rural to urban areas are central to this as well as the liberalization of food markets, global food marketing, and the emergence of transnational food companies in developing countries. All these forces of globalization are creating lifestyle changes that contribute to the nutrition transition. Technological advancements are making previously arduous labor less difficult and thus altering energy expenditure that would have helped offset the caloric increases in the diet. Daily tasks and leisure are also affected by technological advancements and contributing to greater rates of inactivity. The aforementioned increases in calorie are due to increased consumption of edible oils, animal-source foods, caloric sweeteners, accompanied by reduced consumption of grains and fruits and vegetables. These changes play into human biological preferences seen across the world. Socioeconomic factors also play an important role as do cultural values tied to appearance and status.\n\nThe current nutrition transition seen in the emerging markets of Asia, Latin America, the Middle East, North Africa, and urban areas of sub-Saharan Africa is largely a product of globalization. International food trade, investment, commercialization and marketing are drastically impacting the availability of and access to energy-dense, but nutrient-deficient foods causing the aforementioned shift from traditional diet. Another byproduct of globalization has been a marked demographic transition in these countries from rural areas to urban ones. Urban populations are more susceptible to current trends in nutrition transition because of the improved transportation, commercial food distribution and marketing, less labor-intensive-occupations, and changes in household eating habits and structure.\nThe liberalization of food markets has had a drastic effect on consumption patterns across the globe. Liberalization and commercialization of domestic agricultural markets are opening up food trading since this is needed to compete in the world market. This had led to changes in the types of food produced, and increases in amounts of food imported into developing countries, which affects the relative availability and prices of different foods. \nFood demand is being shaped by increases in income and urbanization. As these rapidly developing nations continue to accrue high incomes per capita, their food spending is increasing as well. They elect to use these higher incomes on more calorically-dense foods that are sweeter and higher in fats. For example, in China, for the same extra dollar of income, an average Chinese person is purchasing higher calorie food today than that person would have done for the same extra yuan in 1990. Rapid urbanization has also shaped food demand globally. The demographic transition from rural areas to urban populations is a well documented byproduct of globalization and technological advancements. This is because agro-food systems have replaced local subsistence farming in many rural areas. \nThe supply of food is directly sculpted by increasing demand in these areas with growing income. Urbanization is increasing access to new foods and therefore altering the supply chain. This is why transnational food companies have grown so rapidly over the past few decades. These companies are making processed and fast foods much cheaper and more widely available through the growth of transnational supermarkets and chain restaurants. Food is not only easier to obtain in urban areas; it is also cheaper and less time-consuming to acquire which creates an imbalance between energy intake and output. Their advertising and promotional strategies have a strong effect on consumer choices and desire. Foreign direct investment is also stimulating processed food sales in these supermarkets by lowering prices and creating incentives for advertising and promotion. A large proportion of this advertising is for energy-dense processed foods and is being directed at children and youth.\nTechnological and transportation advancements are reducing the barriers that once limited global food trade. These techniques are critical to facilitating the production and distribution needed in a global market. Better preservation techniques are helping to reduce waste which contributes to lower prices for consumers. Technology is creating higher yields which also reduce prices.\n\nThe forces of globalization are strongly influencing many lifestyle changes in developing countries. Major changes in economic structures from agrarian economies to industrialized economies are reducing physical activity levels in occupations around the world. Even in agricultural work, gas-powered technologies are helping reduce the energy expenditure needed to perform pertinent farming tasks. These reduced activity levels are not just seen in the workplace, but in homes as well. Daily tasks that were once laborious engagements are now much easier with the help of technological advancements, with examples being appliances such as washing machines, refrigerators, and stoves. Also, recent leaps in the efficiency of food production (canning, refrigeration, freezing, and packaging being a few of the most notable) and improvements in cookware, such as the introduction of improved metal stoves which use fossil fuels and microwave ovens, have helped reduce domestic efforts greatly.\n\nLeisure is being greatly impacted as well. Activities such as playing sports outside are being replaced with television watching and computer games. Decreasing physical leisure activities can also be contributed to urbanization wherein access to fields needed to play such games as soccer are not available due to such dense populations and their subsequent demand for land. Other important lifestyle changes fueling the nutrition transition relate to the composition of diets. These dietary shifts have been mentioned previously several times but deserve greater scrutiny. Diets rich in legumes, other vegetables, and coarse grains are disappearing in all regions and countries. Taking their place are diets characterized by fat-rich edible and vegetable oils, cheap animal-source foods high in fat and protein, and artificially sweetened foods high in sugar and refined carbohydrates. Consumption of caloric beverages such as soda represented 21% of all calorie intake in Mexico from 1996 to 2002. Processes of globalization that have influenced food markets have made these products much cheaper, flavorful, and easier to produce which has in turn driven up their demand. So while globalization and the accompanying economic development has created higher levels of food security for developing countries, the ongoing trend of eating in a more Western fashion has caused increased rates of adverse health and childhood obesity.\n\nThe desires for these new diets and lifestyles are very understandable from a biological and psychosocial perspective. For example, humans have an innate preference for sweets dating back to hunter-gatherer populations. These sweets signaled a good source of energy for hunter-gatherers that were not food secure. This same concept also relates to human predisposition for energy-dense fatty foods. These foods were needed to sustain long journeys and provided a safety net for times of famine. Humans also desire to eliminate physical exertion. This can explain the shift to more sedentary lifestyles from occupational, domestic, and leisurely activities that were previously much more physical taxing. \nSocioeconomic and cultural influences also contribute to lifestyle changes associated with nutrition transition. The transfer of tastes by means of tourism and open food trade has introduced developing nations to foods previously enjoyed only by industrialized countries. Global food advertising and promotion has only further cemented these dietary changes. Additionally some cultures view obese body types in high regard as they relate them to power, beauty and affluence. Several studies suggest that socioeconomic status contributes greatly to nutrition transition wherein there is a lack of healthy food alternatives completely or a lack of affordable healthy food alternatives.\n\nWhile increased food security is a major benefit of global nutrition transition, there are a myriad of coinciding negative health and economic consequences. Rates of obesity are soaring across the world and recent trends suggest that incidences of overnutrition in coming decades will overtake that of undernutrition in the developing world. As well there will be a marked epidemiological shift from infectious disease to degenerative, noncommunicable disease, NCDs in these countries. As it stands now these countries face a unique paradox in having to deal with both over- and undernutrition, a dual burden of malnutrition, that will inevitably be accompanied by both infectious and noncommunicable diseases, a dual burden of disease. The economic impact will be enormous as well. In addition to reduced productivity, the health systems of these countries stand to face a tremendous burden.\n\nThe foremost health outcome of the global nutrition transition will be an increased prevalence of obesity across the world. Obesity prevalence in developing countries increased from 2.3% in 1988 to 19.6% in 1998. Incidences are highest among women and children, indicating health inequities across global populations. Obesity is strongly linked to degenerative, NCDs such as coronary heart disease, diabetes, stroke, and hypertension. WHO estimates place NCDs as the principal global cause of morbidity and mortality, and global prevalence of chronic diseases is projected to increase substantially over the next 2 decades in developing countries. Between 1990 and 2020, mortality from cardiovascular diseases, CVDs, in developing countries is expected to increase 120% for women and 137% for men compared to 29 and 49% respectively in industrialized countries. In many of the countries facing epidemics of overnutrition, there is still widespread undernutrition.\n\nDeemed as a developmental challenge of epidemic proportions, the double burden of disease (DBD) is an emerging global health challenge, that exists predominately in low-to-middle income countries. More specifically, the DBD refers to the dual burden of communicable and non-communicable diseases(NCD). Today, over 90 per cent of the world’s disease burden occurs in developing regions, and most are attributed to communicable diseases. Communicable diseases are infectious diseases that “can be passed between people through proximity, social contact or intimate contact.” Common diseases in this category include whooping cough or tuberculosis, HIV/AIDs, malaria, influenza (the flu), and mumps. \nAs low-to-middle income countries continue to develop, the types of diseases that affecting populations within these countries shifts primarily from infectious diseases, such as diarrhea and pneumonia, to primarily non-communicable diseases, such as cardiovascular disease, cancer and obesity. This shift is increasingly being referred to as the risk transition. Thus, as globalization and the proliferation of pre-packaged foods continues, traditional diets and lifestyles are changing in many developing countries. As such, it is becoming increasingly common to see low-to-middle income countries battle with century old issues such as food insecurity and under nutrition, in addition to emerging health epidemics such as chronic heart disease, hypertension, stroke, and diabetes. Diseases once characteristic of industrialized nations, are increasingly becoming health challenges of epidemic proportions in many low-to-middle income countries.\n\nThe economic impact of these rising rates and dual burdens of disease looks to be tremendous. Disability, decreased quality of life, greater use of health care facilities, and increased absenteeism are strong associated with obesity. With inadequate resources, poorly construed health systems, and a general lack of expertise to address the burden of infective diseases, the disease burden for low-to-middle countries is exacerbated by the rising rate of non-communicable diseases. This is often attributed to the fact that these countries by nature have ill-health systems that possess inadequate resources to detect and prevent many non-communicable diseases.” Social constructs within these countries often amplify the risk of the double burden, as inequality, gender, and other social determinants often have a role to play in disparate access and allocation of health services and resources. If current trends are maintained, the World Health Organization predicts that low-and-middle income countries will be unable to support the burden of disease within the foreseeable future.\n\nCountries worldwide have made several, varied efforts to address the consequences of the nutrition transition. These policies target the food environment, governance, food system, or education and can be generally classified into the following categories:\n\nNutrition education intends to facilitate healthy behavioral changes, at the individual level. Dietary guidelines, specifically, promote public awareness of nutritional needs. Over 60 countries in the Global North and South have established national dietary guidelines.\n\nNutrition labeling for food packages and in restaurants may encourage consumers to choose healthier foods. Nutrition labeling has been emphasized as important in influencing food choices and potentially reducing the intake of fat, sugar, and sodium.\n\nSchools are viewed as a primary target of intervention for implementing nutrition-related policies. Children and adolescents are particularly vulnerable to exposure to unhealthy foods before, during, and after school. Children are more susceptible to developing early obesity and are likely to remain obese throughout adulthood. School policies are varied and specific to the political, economic, and social climates of a place. They can focus on increasing nutritional standards, promoting active lifestyles, regulating school meal programs, and banning the sale of certain foods and beverages in and around schools.\n\nFood marketing, via several media outlets - television, the Internet, packaging, popular culture - has been an effective strategy for influencing and changing consumers' food choices, particularly among children. Several studies have indicated the association between exposure to food advertising and food choices and beliefs. The impact of advertising has led to support for government level regulation of food marketing. Countries have implemented voluntary or mandatory restrictions on advertisements of unhealthy food products. Food companies are also urged to implement responsible food marketing strategies. Efforts by corporations should reverse drivers of food consumption, including convenience, low cost, good taste, and nutritional knowledge. Recommendations include downsizing packaging, reducing serving sizes, and recreating formulas to decrease caloric content.\n\nStemming from the success of taxation of tobacco products in reducing tobacco usage, policy makers and researchers have adopted a parallel approach for reducing obesity. The WHO supported economic policies as a method of influencing food prices and promoting healthy eating in public spaces (cite, 2008). Tax policies, in the form of sin taxes or Pigovian taxes, generally target unhealthy food and drink products, including the \"fat tax,\" \"junk food tax,\" and of particular popularity, the tax on sugar-sweetened beverages (SSBs). Taxation is intended to combat obesity by increasing the price of SSBs and unhealthy foods and in turn, reducing their consumption, as well as generating revenue that may be used towards obesity prevention programs or promotion of fruit and vegetable consumption. However, the effectiveness of taxation remains under scrutiny - economists argue that taxes are inefficient for combating obesity and can result in greater losses for consumers.\n\nThe literature suggests that it may be ideal for governments to adopt a holistic policy approach to address the obesity epidemic, given the associated social conditions. \"Policy package\" recommendations have been a supported framework for preventing obesity and diet-related non-communicable diseases worldwide because they are adaptable to country-specific circumstances. For example, the NOURISHING framework summarizes key avenues for action and policy but is flexible to suit a range of national and local contexts. The World Health Organization has called for governments to have multi-faceted interventions, focusing on food security, food safety, healthy lifestyle, and nutrition. Given the scope of the pandemic but the diverse place-based trends and risk factors, appropriate and adequate intervention calls for policy change across multiple levels - population and individual - and the need for international collaboration. At the same time, evaluations of programs and initiatives on their impact on obesity are necessary to both enhance efficacy of existing interventions and provide a foundation for future interventions.\n\nCase studies for individual nations are plentiful. The BRICS countries are specifically studied in great depth because of their rapidly transitioning economies, but more slowly developing nations are well studied too.\n\nCase studies in the United States and United Kingdom are particularly bountiful.\n\nReports based in Latin America, Asia, the Middle East, North Africa, and developed areas of sub-Saharan Africa can be found in a wide range of academic literature.\n\nWorldwide, Aboriginal populations have experienced radical changes in diet. Traditional diets and food intakes have been replaced by diets consisting of foods high in fat, sugar and salt. This change in diet is related to the life-style changes during the last century: for example, Hunter-gatherer communities became more settled, and traditional food gathering methods changed. The nutrition transition has been linked to increased rates of non-communicable diseases amongst Aboriginal populations. Industrialization introduced a less complicated way to access food; a protein rich diet was replaced by white bread, processed food and sugary beverages.\n\nTraditional food of First Nations included burbot filet (or muscle) and moose liver. Food consumption provided essential fats (i.e., fatty acids) and proteins that played a key medicinal role in the prevention and reduction of obesity and obesity-related diseases.\n\n"}
{"id": "47746937", "url": "https://en.wikipedia.org/wiki?curid=47746937", "title": "Occupational safety and health in Tanzania", "text": "Occupational safety and health in Tanzania\n\nTanzania has a number of laws and regulations that govern occupational safety and health (OSH) protections for workers. The International Labour Organization reports that due to insufficient statistics and consistent reporting, it is impossible to determine the number of workplace accidents that occur in the country. \n\nThe first law in Tanzania that related to workers health and safety was the Factories Ordinances Cap. 297 of 1950, that provided for occupational health and safety standards for workers in factories. As most of Tanzania's workforce was employed in the agricultural sector, this ordinance left most workers in the country unprotected. This law was replaced in 2003 with the new \"Occupational Health and Safety Act No. 5 of 2003\", which covers workers in all sectors, including within the public sector and in local government authorities.\n\nIn addition to the general act, the country has also adopted a number of laws providing for protection in individual industries that are perceived to be more hazardous that usual. Some of these laws include:\nThe \"Employment and Labour Relation Act No. 6 of 2004\" also contains provisions for OSH, although it is primarily concerned with industrial relations matters rather than worker safety.\n\nIn 2008 another important step was made; The establishment of Workers' Compensation Fund through Workers Compensation Act No. 20 of 2008 with objectives of providing for compensation to employees for disablement of death caused by or result from injuries or diseases sustained or contracted in the course of employment; to establish Fund for administration and regulation of workers compensation and to provide for related matter.\n\nStatus of occupational accidents and injuries varies considerably between different sources. It is estimated that in mining and quarry sector, the injury rate is 17 per 1000 workers whereas the manufacturing sector is responsible for 10.1% of total occupational accidents, 9.6% of fatalities, 12.2% of partial disabilities and about 7.4% of temporary disability and the injury rate is 9.9 per 1,000 workers.\n\nReport from National Audit office (NAO) showed that construction/building industry had highest Fatality rate of 23.7% followed by Transport and mining/quarrying that had 20.6% and 20.5 respectively (table 1 below). Injuries in transport sector is another life-threatening risk that continues to claim lives of people especially motorcyclist and public transport (buses). however the major challenge in these information is validity and reliability as the reporting and data keeping system in Tanzania is not well coordinated.\n\nTable 1: Fatality Rate sectorwise\nSource: NAO report- Performance Audit Report on the Management of Occupational Health and Safety in Tanzania, 2013.\n\nThe presented information may be challenged by several other factors as reporting system is not well functional. There were a total of 6,599 registered workplaces equivalent to 24% of eligible workplaces. This lack of reliable information is a challenge to the authority dealing with OSH matters in Tanzania.\n\n"}
{"id": "58696917", "url": "https://en.wikipedia.org/wiki?curid=58696917", "title": "PSYCHLOPS", "text": "PSYCHLOPS\n\nPSYCHLOPS (psychological outcome profiles) is a type of psychological testing, a tool used in primary care to measure mental health outcomes and as a quality of life measure.\n\nIt is also one intervention that has been adapted for use in children and adopted by the World Health Organization (WHO) in response to a demand for guidance on psychological interventions for people exposed to adversity, including humanitarian disasters.\n\nPSYCHLOPS is a mental health outcome-measure tool used in primary care or community care settings and is self-completed. It measures mental health problems, quality of life, social functioning and wellbeing. Others include the Beck Depression Inventory, the CORE-OM, Generalized Anxiety Disorder 7 (GAD-7) and the Patient Health Questionnaire (PHQ-9), amongst others. \n\nBy comparing scores before, during and after completion of talking therapy, the measure enables calculation of a change score. The change score indicates the change (improvement or deterioration) in perceived severity of problems during and following a course of talking therapy. PSYCHLOPS is a patient-generated outcome measure which means that measurement is based on items (issues) selected by patients rather than pre-specified items. By gathering information on patient-generated items, PSYCHLOPS contains a free text record (qualitative data) of the patient’s own perception of their problems which can be used for qualitative research.\n\nIt is also one intervention that has been adopted by the WHO in response to a demand for guidance on psychological interventions for people exposed to adversity. This includes people caught up in humanitarian crises and incorporates common mental health problems in a variety of countries, cultures and settings. WHO has included it in two of their mental health programmes, Programme Management Plus (PM+)and Self Help Plus (SH+), both of which use locally trained mental health workers to administer mental health support in countries affected by conflict and war.\n\nPSYCHLOPS has been adapted for use in children. Unlike a checklist, \"PSYCHLOPS Kids\" allows children to identify their worries themselves.\nFor children taking part in drama therapy, they have the potential to recognise the impact of their therapy. The reliance on the support of school staff has been reported as one limitation.\n\nPSYCHLOPS is completed by the individual person. A specific sequence of questions is included in the questionnaire which address the respondent’s greatest and least worries. These concerns are then scored. Subsequently, the answers and scores can be used to evaluate their perception of the change in their problems as a result of counselling or other talking therapy. It uses a six-point scale rather than the seven used in its sister questionnaire, MYMOP.\n\nThe procedure encompasses answering four questions on one sheet, with the first question being “Choose the problem that troubles you most?” This is followed by scoring it on a six-point scoring range and recording the duration of the problem. An outcome score is calculated using the scores of each of the before, during and after questionnaires.\n\nPSYCHLOPS was designed by an academic British general practitioner and was adapted from the self-report inventory, MYMOP (Measure Yourself Medical Outcome Profile), another outcome measure first published in 1996 and mainly used for people who present with physical, emotional, or social symptoms. \n\nThere has been debate as to whether to identify and measure individualized outcomes that are unique for each patient (idiographic approach) or to use standardized measures (nomothetic approach). Both measures have their advantages and disadvantages.\n\nAs a patient-generated measure, PSYCHLOPS appears a more sensitive (responsive) measure of change following a mental health intervention than some other standardised measures.\n\n\n"}
{"id": "24255", "url": "https://en.wikipedia.org/wiki?curid=24255", "title": "Pandemic", "text": "Pandemic\n\nA pandemic (from Greek πᾶν \"pan\" \"all\" and δῆμος \"demos\" \"people\") is an epidemic of infectious disease that has spread across a large region; for instance multiple continents, or even worldwide.\n\nA widespread endemic disease that is stable in terms of how many people are getting sick from it is not a pandemic. Further, flu pandemics generally exclude recurrences of seasonal flu. Throughout history, there have been a number of pandemics, such as smallpox and tuberculosis. One of the most devastating pandemics was the Black Death, which killed over 75 million people in 1350. The most recent pandemics include the HIV pandemic as well as the 1918 and 2009 H1N1 pandemics.\n\nA pandemic is an epidemic occurring on a scale which crosses international boundaries, usually affecting a large number of people. Pandemics can also occur in important agricultural organisms (livestock, crop plants, fish, tree species) or in other organisms.\n\nThe World Health Organization (WHO) has a six-stage classification that describes the process by which a novel influenza virus moves from the first few infections in humans through to a pandemic. This starts with the virus mostly infecting animals, with a few cases where animals infect people, then moves through the stage where the virus begins to spread directly between people, and ends with a pandemic when infections from the new virus have spread worldwide and it will be out of control until we stop it.\n\nA disease or condition is not a pandemic merely because it is widespread or kills many people; it must also be infectious. For instance, cancer is responsible for many deaths but is not considered a pandemic because the disease is not infectious or contagious.\n\nIn a virtual press conference in May 2009 on the influenza pandemic, Dr Keiji Fukuda, Assistant Director-General \"ad interim\" for Health Security and Environment, WHO said \"An easy way to think about pandemic … is to say: a pandemic is a global outbreak. Then you might ask yourself: 'What is a global outbreak'? Global outbreak means that we see both spread of the agent … and then we see disease activities in addition to the spread of the virus.\"\n\nIn planning for a possible influenza pandemic, the WHO published a document on pandemic preparedness guidance in 1999, revised in 2005 and in February 2009, defining phases and appropriate actions for each phase in an aide memoir entitled \"WHO pandemic phase descriptions and main actions by phase\". The 2009 revision, including definitions of a pandemic and the phases leading to its declaration, were finalized in February 2009. The pandemic H1N1 2009 virus was neither on the horizon at that time nor mentioned in the document. All versions of this document refer to influenza. The phases are defined by the spread of the disease; virulence and mortality are not mentioned in the current WHO definition, although these factors have previously been included.\n\nHIV originated in Africa, and spread to the United States via Haiti between 1966 and 1972. AIDS is currently a pandemic, with infection rates as high as 25% in southern and eastern Africa. In 2006, the HIV prevalence rate among pregnant women in South Africa was 29.1%. Effective education about safer sexual practices and bloodborne infection precautions training have helped to slow down infection rates in several African countries sponsoring national education programs. Infection rates are rising again in Asia and the Americas. The AIDS death toll in Africa may reach 90–100 million by 2025.\n\nThere have been a number of significant pandemics recorded in human history, generally zoonoses which came about with domestication of animals, such as influenza and tuberculosis. There have been a number of particularly significant epidemics that deserve mention above the \"mere\" destruction of cities:\n\n\nEncounters between European explorers and populations in the rest of the world often introduced local epidemics of extraordinary virulence. Disease killed part of the native population of the Canary Islands in the 16th century (Guanches). Half the native population of Hispaniola in 1518 was killed by smallpox. Smallpox also ravaged Mexico in the 1520s, killing 150,000 in Tenochtitlán alone, including the emperor, and Peru in the 1530s, aiding the European conquerors. Measles killed a further two million Mexican natives in the 17th century. In 1618–1619, smallpox wiped out 90% of the Massachusetts Bay Native Americans. During the 1770s, smallpox killed at least 30% of the Pacific Northwest Native Americans. Smallpox epidemics in 1780–1782 and 1837–1838 brought devastation and drastic depopulation among the Plains Indians. Some believe that the death of up to 95% of the Native American population of the New World was caused by Old World diseases such as smallpox, measles, and influenza. Over the centuries, the Europeans had developed high degrees of immunity to these diseases, while the indigenous peoples had no such immunity.\n\nSmallpox devastated the native population of Australia, killing around 50% of Indigenous Australians in the early years of British colonisation. It also killed many New Zealand Māori. As late as 1848–49, as many as 40,000 out of 150,000 Hawaiians are estimated to have died of measles, whooping cough and influenza. Introduced diseases, notably smallpox, nearly wiped out the native population of Easter Island. In 1875, measles killed over 40,000 Fijians, approximately one-third of the population. The disease devastated the Andamanese population. Ainu population decreased drastically in the 19th century, due in large part\nto infectious diseases brought by Japanese settlers pouring into Hokkaido.\n\nResearchers concluded that syphilis was carried from the New World to Europe after Columbus' voyages. The findings suggested Europeans could have carried the nonvenereal tropical bacteria home, where the organisms may have mutated into a more deadly form in the different conditions of Europe. The disease was more frequently fatal than it is today. Syphilis was a major killer in Europe during the Renaissance. Between 1602 and 1796, the Dutch East India Company sent almost a million Europeans to work in Asia. Ultimately, only less than one-third made their way back to Europe. The majority died of diseases. Disease killed more British soldiers in India than war.\n\nAs early as 1803, the Spanish Crown organized a mission (the Balmis expedition) to transport the smallpox vaccine to the Spanish colonies, and establish mass vaccination programs there. By 1832, the federal government of the United States established a smallpox vaccination program for Native Americans. From the beginning of the 20th century onwards, the elimination or control of disease in tropical countries became a driving force for all colonial powers. The sleeping sickness epidemic in Africa was arrested due to mobile teams systematically screening millions of people at risk. In the 20th century, the world saw the biggest increase in its population in human history due to lessening of the mortality rate in many countries due to medical advances. The world population has grown from 1.6 billion in 1900 to an estimated 7 billion today.\n\nSince it became widespread in the 19th century, cholera has killed tens of millions of people.\n\n. Influenza A (H3N2) viruses still circulate today.\n\nTyphus is sometimes called \"camp fever\" because of its pattern of flaring up in times of strife. (It is also known as \"gaol fever\" and \"ship fever\", for its habits of spreading wildly in cramped quarters, such as jails and ships.) Emerging during the Crusades, it had its first impact in Europe in 1489, in Spain. During fighting between the Christian Spaniards and the Muslims in Granada, the Spanish lost 3,000 to war casualties, and 20,000 to typhus. In 1528, the French lost 18,000 troops in Italy, and lost supremacy in Italy to the Spanish. In 1542, 30,000 soldiers died of typhus while fighting the Ottomans in the Balkans.\n\nDuring the Thirty Years' War (1618–1648), about 8 million Germans were killed by bubonic plague and typhus. The disease also played a major role in the destruction of Napoleon's \"Grande Armée\" in Russia in 1812. During the retreat from Moscow, more French military personnel died of typhus than were killed by the Russians. Of the 450,000 soldiers who crossed the Neman on 25 June 1812, fewer than 40,000 returned. More military personnel were killed from 1500–1914 by typhus than from military action. In early 1813, Napoleon raised a new army of 500,000 to replace his Russian losses. In the campaign of that year, over 219,000 of Napoleon's soldiers died of typhus. Typhus played a major factor in the Irish Potato Famine. During World War I, typhus epidemics killed over 150,000 in Serbia. There were about 25 million infections and 3 million deaths from epidemic typhus\nin Russia from 1918 to 1922. Typhus also killed numerous prisoners in the Nazi concentration camps and Soviet prisoner of war camps during World War II. More than 3.5 million Soviet POWs died out of the 5.7 million in Nazi custody.\n\nSmallpox was a contagious disease caused by the variola virus. The disease killed an estimated 400,000 Europeans per year during the closing years of the 18th century. During the 20th century, it is estimated that smallpox was responsible for 300–500 million deaths. As recently as the early 1950s, an estimated 50 million cases of smallpox occurred in the world each year. After successful vaccination campaigns throughout the 19th and 20th centuries, the WHO certified the eradication of smallpox in December 1979. To this day, smallpox is the only human infectious disease to have been completely eradicated, and one of two infectious viruses ever to be eradicated.\n\nHistorically, measles was prevalent throughout the world, as it is highly contagious. According to the U.S. National Immunization Program, 90% of people were infected with measles by age 15. Before the vaccine was introduced in 1963, there were an estimated 3–4 million\ncases in the U.S. each year. Measles killed around 200 million people worldwide over the last 150 years. In 2000 alone, measles killed some 777,000 worldwide out of 40 million cases globally.\n\nMeasles is an endemic disease, meaning that it has been continually present in a community, and many people develop resistance. In populations that have not been exposed to measles, exposure to a new disease can be devastating. In 1529, a measles outbreak in Cuba killed two-thirds of the natives who had previously survived smallpox. The disease had ravaged Mexico, Central America, and the Inca civilization.\n\nOne-third of the world's current population has been infected with \"Mycobacterium tuberculosis\", and new infections occur at a rate of one per second. About 5–10% of these latent infections will eventually progress to active disease, which, if left untreated, kills more than half of its victims. Annually, 8 million people become ill with tuberculosis, and 2 million people die from the disease worldwide. In the 19th century, tuberculosis killed an estimated one-quarter of the adult population of Europe; by 1918, one in six deaths in France were still caused by tuberculosis. During the 20th century, tuberculosis killed approximately 100 million people. TB is still one of the most important health problems in the developing world.\n\nLeprosy, also known as Hansen's disease, is caused by a bacillus, \"Mycobacterium leprae\". It is a chronic disease with an incubation period of up to five years. Since 1985, 15 million people worldwide have been cured of leprosy.\n\nHistorically, leprosy has affected people since at least 600 BC. Leprosy outbreaks began to occur in Western Europe around 1000 AD. Numerous \"leprosaria\", or leper hospitals, sprang up in the Middle Ages; Matthew Paris estimated that in the early 13th century, there were 19,000 of them across Europe.\n\nMalaria is widespread in tropical and subtropical regions, including parts of the Americas, Asia, and Africa. Each year, there are approximately 350–500 million cases of malaria. Drug resistance poses a growing problem in the treatment of malaria in the 21st century, since resistance is now common against all classes of antimalarial drugs, except for the artemisinins.\n\nMalaria was once common in most of Europe and North America, where it is now for all purposes non-existent. Malaria may have contributed to the decline of the Roman Empire. The disease became known as \"Roman fever\". \"Plasmodium falciparum\" became a real threat to colonists and indigenous people alike when it was introduced into the Americas along with the slave trade. Malaria devastated the Jamestown colony and regularly ravaged the South and Midwest of the United States. By 1830, it had reached the Pacific Northwest. During the American Civil War, there were over 1.2 million cases of malaria among\nsoldiers of both sides. The southern U.S. continued to be afflicted with millions of cases of malaria into the 1930s.\n\nYellow fever has been a source of several devastating epidemics. Cities as far north as New York, Philadelphia, and Boston were hit with epidemics. In 1793, one of the largest yellow fever epidemics in U.S. history killed as many as\n5,000 people in Philadelphia—roughly 10% of the population. About half of the residents had fled the city, including President George Washington. In colonial times, West Africa became known as \"the white man's grave\" because of malaria and yellow fever.\n\nViral hemorrhagic fevers such as Ebola virus disease, Lassa fever virus, Rift Valley fever, Marburg virus and Bolivian hemorrhagic fever are highly contagious and deadly diseases, with the theoretical potential to become pandemics. Their ability to spread efficiently enough to cause a pandemic is limited, however, as transmission of these viruses requires close contact with the infected vector, and the vector only has a short time before death or serious illness. Furthermore, the short time between a vector becoming infectious and the onset of symptoms allows medical professionals to quickly quarantine vectors, and prevent them from carrying the pathogen elsewhere. Genetic mutations could occur, which could elevate their potential for causing widespread harm; thus close observation by contagious disease specialists is merited.\n\nAntibiotic-resistant microorganisms, sometimes referred to as \"superbugs\", may contribute to the re-emergence of diseases which are currently well controlled. For example, cases of tuberculosis that are resistant to traditionally effective treatments remain a cause of great concern to health professionals. Every year, nearly half a million new cases of multidrug-resistant tuberculosis (MDR-TB) are estimated to occur worldwide. China and India have the highest rate of multidrug-resistant TB. The World Health Organization (WHO) reports that approximately 50 million people worldwide are infected with MDR TB, with 79 percent of those cases resistant to three or more antibiotics. In 2005, 124 cases of MDR TB were reported in the United States. Extensively drug-resistant tuberculosis (XDR TB) was identified in Africa in 2006, and subsequently discovered to exist in 49 countries, including the United States. There are about 40,000 new cases of XDR-TB per year, the WHO estimates.\n\nIn the past 20 years, common bacteria including \"Staphylococcus aureus\", \"Serratia marcescens\" and Enterococcus, have developed resistance to various antibiotics such as vancomycin, as well as whole classes of antibiotics, such as the aminoglycosides and cephalosporins. Antibiotic-resistant organisms have become an important cause of healthcare-associated (nosocomial) infections (HAI). In addition, infections caused by community-acquired strains of methicillin-resistant \"Staphylococcus aureus\" (MRSA) in otherwise healthy individuals have become more frequent in recent years. \n\nIn 2003 the Italian physician Carlo Urbani (1956–2003) was the first to identify severe acute respiratory syndrome (SARS) as a new and dangerously contagious disease, although he became infected and died. It is caused by a coronavirus dubbed SARS-CoV. Rapid action by national and international health authorities such as the World Health Organization helped to slow transmission and eventually broke the chain of transmission, which ended the localized epidemics before they could become a pandemic. However, the disease has not been eradicated. It could re-emerge. This warrants monitoring and reporting of suspicious cases of atypical pneumonia.\n\nWild aquatic birds are the natural hosts for a range of influenza A viruses. Occasionally, viruses are transmitted from these species to other species, and may then cause outbreaks in domestic poultry or, rarely, in humans.\n\nIn February 2004, avian influenza virus was detected in birds in Vietnam, increasing fears of the emergence of new variant strains. It is feared that if the avian influenza virus combines with a human influenza virus (in a bird or a human), the new subtype created could be both highly contagious and highly lethal in humans. Such a subtype could cause a global influenza pandemic, similar to the Spanish Flu, or the lower mortality pandemics such as the Asian Flu and the Hong Kong Flu.\n\nFrom October 2004 to February 2005, some 3,700 test kits of the 1957 Asian Flu virus were accidentally spread around the world from a lab in the US.\n\nIn May 2005, scientists urgently called upon nations to prepare for a global influenza pandemic that could strike as much as 20% of the world's population.\n\nIn October 2005, cases of the avian flu (the deadly strain H5N1) were identified in Turkey. EU Health Commissioner Markos Kyprianou said: \"We have received now confirmation that the virus found in Turkey is an avian flu H5N1 virus. There is a direct relationship with viruses found in Russia, Mongolia and China.\" Cases of bird flu were also identified shortly thereafter in Romania, and then Greece. Possible cases of the virus have also been found in Croatia, Bulgaria and the United Kingdom.\n\nBy November 2007, numerous confirmed cases of the H5N1 strain had been identified across Europe. However, by the end of October, only 59 people had died as a result of H5N1, which was atypical of previous influenza pandemics.\n\nAvian flu cannot yet be categorized as a \"pandemic\", because the virus cannot yet cause sustained and efficient human-to-human transmission. Cases so far are recognized to have been transmitted from bird to human, but as of December 2006, there have been very few (if any) cases of proven human-to-human transmission. Regular influenza viruses establish infection by attaching to receptors in the throat and lungs, but the avian influenza virus can only attach to receptors located deep in the lungs of humans, requiring close, prolonged contact with infected patients, and thus limiting person-to-person transmission.\n\nAn outbreak of Zika virus began in 2015 and strongly intensified throughout the start of 2016, with over 1.5 million cases across more than a dozen countries in the Americas. The World Health Organisation warned that Zika had the potential to become an explosive global pandemic if the outbreak was not controlled.\n\nIn 2016, the Commission on a Global Health Risk Framework for the Future estimated that pandemic disease events would cost the global economy over $6 trillion in the 21st century - over $60 billion per year. The same report also recommended spending $4.5 billion annually on global prevention and response capabilities to reduce the threat posed by pandemic events.\n\nIn 1346, the bodies of Mongol warriors who had died of plague were thrown over the walls of the besieged Crimean city of Kaffa (now Theodosia). After a protracted siege, during which the Mongol army under Jani Beg was suffering the disease, they catapulted the infected corpses over the city walls to infect the inhabitants. It has been speculated that this operation may have been responsible for the arrival of the Black Death in Europe.\n\nThe Native American population was devastated after contact with the Old World by introduction of many fatal diseases. In a well documented case of germ warfare involving British commander Jeffery Amherst and Swiss-British officer Colonel Henry Bouquet, their correspondence included a proposal and agreement to give smallpox-infected blankets to Indians in order to \"Extirpate this Execrable Race\". During the siege of Fort Pitt late in the French and Indian War, as recorded in his journal by sundries trader and militia Captain, William Trent, on June 24, 1763, dignitaries from the Delaware tribe met with Fort Pitt officials, warned them of \"great numbers of Indians\" coming to attack the fort, and pleaded with them to leave the fort while there was still time. The commander of the fort refused to abandon the fort. Instead, the British gave as gifts two blankets, one silk handkerchief and one linen from the smallpox hospital to two Delaware Indian dignitaries. A devastating smallpox epidemic plagued Native American tribes in the Ohio Valley and Great Lakes area through 1763 and 1764, but the effectiveness of individual instances of biological warfare remains unknown. After extensive review of surviving documentary evidence, historian Francis Jennings concluded the attempt at biological warfare was \"unquestionably effective at Fort Pitt\"; Smallpox after Pontiac's Rebellion killed 400,000–500,000 (possibly even up to 1.5 million) Native Americans.\n\nDuring the Sino-Japanese War (1937–1945), Unit 731 of the Imperial Japanese Army conducted human experimentation on thousands, mostly Chinese. In military campaigns, the Japanese army used biological weapons on Chinese soldiers and civilians. Plague fleas, infected clothing, and infected supplies encased in bombs were dropped on various targets. The resulting cholera, anthrax, and plague were estimated to have killed around 400,000 Chinese civilians.\n\nDiseases considered for or known to be used as a weapon include anthrax, ebola, Marburg virus, plague, cholera, typhus, Rocky Mountain spotted fever, tularemia, brucellosis, Q fever, machupo, Coccidioides mycosis, Glanders, Melioidosis, Shigella, Psittacosis, Japanese B encephalitis, Rift Valley fever, yellow fever, and smallpox.\n\nSpores of weaponized anthrax were accidentally released from a military facility near the Soviet closed city of Sverdlovsk in 1979. The Sverdlovsk anthrax leak is sometimes called \"biological Chernobyl\". In January 2009, an Al-Qaeda training camp in Algeria was reportedly wiped out by the plague, killing approximately 40 Islamic extremists. Some experts said that the group was developing biological weapons, however, a couple of days later the Algerian Health Ministry flatly denied this rumour stating \"No case of plague of any type has been recorded in any region of\nAlgeria since 2003\".\n\nPandemics appear in multiple fiction works. A common use is in disaster films, where the protagonists must avoid the effects of the plague, for example zombies.\n\nLiterature\n\nFilm\n\nTelevision\n\nGames\n\nNotes\nFurther reading\n\n"}
{"id": "12459821", "url": "https://en.wikipedia.org/wiki?curid=12459821", "title": "Pelvic girdle pain", "text": "Pelvic girdle pain\n\nPelvic girdle pain (abbreviated PGP) is a pregnancy discomfort that causes pain, instability and limitation of mobility and functioning in any of the three pelvic joints. PGP has a long history of recognition, mentioned by Hippocrates and later described in medical literature by Snelling.\n\nPrior to the 20th century, specialists of pregnancy-related PGP used varying terminologies. It is now referred to as Pregnancy Related Pelvic Girdle Pain that may incorporate the following conditions:\n\n\n\"\"The classification between hormonal and mechanical pelvic girdle instability is no longer used. For treatment and/or prognosis it makes no difference whether the complaints started during pregnancy or after childbirth.\" Mens (2005)\"\n\nA combination of postural changes,baby, unstable pelvic joints under the influence of pregnancy hormones, and changes in the centre of gravity can all add to the varying degrees of pain or discomfort. In some cases it can come on suddenly or following a fall, sudden abduction of the thighs (opening too wide too quickly) or an action that has strained the joint.\n\nPGP can begin as early as the first trimester of pregnancy. Pain is usually felt low down over the symphyseal joint, and this area may be extremely tender to the touch. Pain may also be felt in the hips, groin and lower abdomen and can radiate down the inner thighs. Women suffering from PGP may begin to waddle or shuffle, and may be aware of an audible clicking sound coming from the pelvis. PGP can develop slowly during pregnancy, gradually gaining in severity as the pregnancy progresses.\n\nDuring pregnancy and postpartum, the symphyseal gap can be felt moving or straining when walking, climbing stairs or turning over in bed; these activities can be difficult or even impossible. The pain may remain static, e.g., in one place such as the front of the pelvis, producing the feeling of having been kicked; in other cases it may start in one area and move to other areas. It is also possible that a woman may experience a combination of symptoms.\n\nAny weight bearing activity has the potential of aggravating an already unstable pelvis, producing symptoms that may limit the ability of the woman to carry out many daily activities. She may experience pain involving movements such as dressing, getting in and out of the bath, rolling in bed, climbing the stairs or sexual activity. Pain may also be present when lifting, carrying, pushing or pulling.\n\nThe symptoms (and their severity) experienced by women with PGP vary, but include:\n\n\nThe severity and instability of the pelvis can be measured on a three level scale.\n\nPelvic type 1:The pelvic ligaments support the pelvis sufficiently. Even when the\nmuscles are used incorrectly, no complaints will occur when performing everyday activities. This is the most common situation in persons who have never been pregnant, who have never been in an accident, and who are not hypermobile.\"\n\nPelvic type 2:The ligaments alone do not support the joint sufficiently. A\ncoordinated use of muscles around the joint will compensate for ligament weakness. In case the muscles around the joint do not function, the patient will experience pain and weakness when performing everyday activities. This type often occurs after giving birth to a child weighing 3000 grams or more, in cases of hypermobility, and sometimes after an accident involving the pelvis. Type 2 is the most common form of pelvic instability. Treatment is based on learning how to use the muscles around the pelvis more efficiently.\n\nPelvic type 3:The ligaments do not support the joint sufficiently. This is a serious\nsituation whereby the muscles around the joint are unable to compensate for ligament weakness. This type of pelvic instability usually only occurs after an accident, or occasionally after a (small) accident in combination with giving birth. Sometimes a small accident occurring long before giving birth is forgotten so that the pelvic instability is attributed only to the childbirth. Although the difference between Type 2 and 3 is often difficult to establish, in case of doubt an exercise program may help the patient. However, if Pelvic Type 3 has been diagnosed then invasive treatment is the only option: in this case parts of the pelvis are screwed together. (Mens 2005)\n\nPGP in pregnancy seriously interferes with participation in society and activities of daily life; the average sick leave due to posterior pelvic pain during pregnancy is 7 to 12 weeks.\n\nIn some cases women with PGP may also experience emotional problems such as anxiety over the cause of pain, resentment, anger, lack of self-esteem, frustration and depression; she is three times more likely to suffer postpartum depressive symptoms. Other psychosocial risk factors associated with woman experiencing PGP include higher level of stress, low job satisfaction and poorer relationship with spouse.\n\nSometimes there is no obvious explanation for the cause of PGP but usually there is a combination of factors such as:\n\n\nPregnancy related Pelvic Girdle Pain (PGP) can be either specific (trauma or injury to pelvic joints or genetical i.e. connective tissue disease) and non-specific. PGP disorder is complex and multi-factorial and likely to be also represented by a series of sub-groups driven by pain varying from peripheral or central nervous system,\naltered laxity/stiffness of muscles, laxity to injury of tendinous/ligamentous structures to ‘mal-adaptive’ body mechanics.\n\nPregnancy begins the physiological changes through a pattern of hormonal secretion and signal transduction thus initiating the remodelling of soft tissues, cartilage and ligaments. Over time, the ligaments could be stretched either by injury or excess strain and in turn may cause PGP.\n\nThe pelvis is the largest bony part of the skeleton and contains three joints: the pubic symphysis, and two sacroiliac joints. A highly durable network of ligaments surrounds these joints giving them tremendous strength.\n\nThe pubic symphysis has a fibrocartilage joint which may contain a fluid filled cavity and is avascular; it is supported by the superior and arcuate ligaments. The sacroiliac joints are synovial, but their movement is restricted throughout life and they are progressively obliterated by adhesions. The nature of the bony pelvic ring with its three joints determines that no one joint can move independently of the other two.\n\nRelaxin is a hormone produced mainly by the corpus luteum of the ovary and breast, in both pregnant and non-pregnant females. During pregnancy it is also produced by the placenta, chorion, and decidua. The body produces relaxin during menstruation that rises to a peak within approximately 14 days of ovulation and then declines. In pregnant cycles, rather than subsiding, relaxin secretion continues to rise during the first trimester and then again in the final weeks. During pregnancy relaxin has a diverse range of effects, including the production and remodelling of collagen thus increasing the elasticity of muscles, tendons, ligaments and tissues of the birth canal in view of delivery.\n\nAlthough relaxin's main cellular action in pregnancy is to remodel collagen by biosynthesis (thus facilitating the changes of connective tissue) it does not seem to generate musculoskeletal problems. European Research has determined that relaxin levels are not a predictor of PGP during pregnancy.\n\nThe pregnant woman has a different pattern of \"gait\". The step lengthens as the pregnancy progresses due to weight gain and changes in posture. Both the length and height of the footstep shortens with PGP. Sometimes the foot can turn inwards due to the rotation of the hips when the pelvic joints are unstable. On average, a woman's foot can grow by a half size or more during pregnancy. Pregnancy hormones that are released to adapt the bodily changes also remodel the ligaments in the foot. In addition, the increased body weight of pregnancy, fluid retention and weight gain lowers the arches, further adding to the foot's length and width. There is an increase of load on the lateral side of the foot and the hind foot. These changes may also be responsible for the musculoskeletal complaints of lower limb pain in pregnant women.\n\nDuring the motion of walking, an upward movement of the pelvis, one side then the other, is required to let the leg follow through. The faster or longer each step the pelvis adjusts accordingly. The flexibility within the knees, ankles and hips are stabilized by the pelvis. Normal gait tends to minimize displacement of centre of gravity whereas abnormal gait through pelvic instability tends to amplify displacement. During pregnancy there may be an increased demand placed on hip abductor, hip extensor, and ankle plantar flexor muscles during walking. To avoid pain on weight bearing structures a very short stance phase and limp occurs on the injured side(s), this is called Antalgic Gait.\n\nA number of treatments have some evidence for benefits include an exercise program. Paracetamol (acetaminophen) has not been found effective but is safe. NSAIDs are sometimes effective but should not be used after 30 weeks of pregnancy. There is tentative evidence for acupuncture.\n\nSome pelvic joint trauma will not respond to conservative type treatments and orthopedic surgery might become the only option to stabilize the joints.\n\nFor most women, PGP resolves in weeks after delivery but for some it can last for years resulting in a reduced tolerance for weight bearing activities. PGP can take from 11 weeks, 6 months or even up to 2 years postpartum to subside. However, some research supports that the average time to complete recovery is 6.25 years, and the more severe the case is, the longer recovery period.\n\nOverall, about 45% of all pregnant women and 25% of all women postpartum suffer from PGP. During pregnancy, serious pain occurs in about 25%, and severe disability in about 8% of patients. After pregnancy, problems are serious in about 7%. There is no correlation between age, culture, nationality and numbers of pregnancies that determine a higher incidence of PGP.\n\nIf a woman experiences PGP during one pregnancy, she is more likely to experience it in subsequent pregnancies; but the severity cannot be determined.\n\n"}
{"id": "3232045", "url": "https://en.wikipedia.org/wiki?curid=3232045", "title": "Proximate", "text": "Proximate\n\nProximates are used in the analysis of biological materials as a decomposition of a human-consumable good into its major constituents. They are a good approximation of the contents of packaged comestible goods and serve as a cost-effective and easy verification of nutritional panels. This means that testing can be used to verify lots, but cannot be used to validate a food processor or food processing facility; instead, a nutritional assay must be conducted on the product to qualify said producers. Nutritional panels in the United States are regulated by the FDA and must undergo rigorous testing to ensure the exact and precise content of nutrients. This should prevent food processors from making unfounded claims to the public.\n\nIn industry, the standard proximates are:\nAnalytically, four of the five constituents are obtained via chemical reactions and experiments. The fifth constituent, carbohydrates, are calculated based on the determination of the four others. Proximates should nearly always account for 100% of a food product; any deviation from 100% displays the resolution of the chemical test, as small variations in the way each test is performed accumulate or overlap the compositional make-up.\n\nThere are additional ingredients that may fall under the category of one of the five constituents. Carbohydrates, for example, include but are not limited to:\nWhereas ash includes but is not limited to:\nAlthough proximates do not give the entire nutritional assay, they are an inexpensive way to track deviations in the quality of foods.\n"}
{"id": "1121947", "url": "https://en.wikipedia.org/wiki?curid=1121947", "title": "Rete tubular ectasia", "text": "Rete tubular ectasia\n\nRete tubular ectasia is a benign condition, usually found in older men, involving numerous small, tubular cystic structures within the rete testis.\n\nThe formation of cysts in the rete testis is associated with the obstruction of the efferent ducts, which connect the rete testis with the head of the epididymis. They are often bilateral.\n\nThe condition can be detected with sonography. It is commonly associated with epididymal abnormalities, such as spermatocele, epididymal cyst, and epididymitis. The condition shares a common location with cystic dysplasia of the testis and intratesticular cysts. Unlike cystic neoplasms, they don't present specific tumor markers.\n\nTypically none is required, but they can be treated surgically if symptomatic.\n"}
{"id": "52975690", "url": "https://en.wikipedia.org/wiki?curid=52975690", "title": "Robert Kezaala", "text": "Robert Kezaala\n\nDr. Robert Kezaala is a medical doctor, epidemiologist, scholar and public health leader in the field of immunization and health emergencies. Currently he is serving as a Senior Health Advisor and team lead for Accelerated Immunization Initiatives: measles, rubella, epidemic meningitis and yellow fever control and Immunization in Emergencies at the United Nations Children’s Fund.\n\nDr. Kezaala received his medical degree from Makerere University in Kampala, Uganda. He also holds an MPH from the Royal Tropical Institute (KIT) in Amsterdam, Netherlands majoring in epidemiology and health planning.\n\nDr. Kezaala has over 30 years of professional experience in public health including 24 years at international level. In the late 1980s, Dr. Kezaala practiced as Medical Officer in Karamoja province in the northeast of Uganda with recognized work in immunization and Tuberculosis control. From 1992 to 1993, Dr Kezaala worked with UNDP in multi-sectoral HIV/AIDS control where he managed the collaborative program that supported Uganda government efforts to address the AIDS epidemic. Thereafter, until 1998, he worked with the International Federation of Red Cross and Red Crescent Societies (IFRC) as Regional Health Delegate for Eastern and Southern Africa, managing a variety of health interventions including HIV/AIDS control, community water and sanitation and refugee health and emergency response when he led IFRC's initial health response in Goma during the 1994 Rwanda crisis. Subsequently, he joined the World Health Organization (WHO), where he worked for 14 years, first as epidemiologist and Team Lead for WHO-EPI in Ethiopia. From 2001 to 2005, Dr. Kezaala headed Measles Control for the Africa Region of WHO, when the Africa region registered a reduction in measles mortality by 70%. He spent the next seven years serving as a medical officer with the Polio Eradication Initiative at the WHO headquarters in Geneva, Switzerland. While here, he worked in country support across the globe, including Chad, Pakistan, the Horn of Africa and served as the outbreak response manager for the 2010 Polio outbreak that affected Tajikistan, Kazakhstan and Russia. During the stint in GPEI, Dr Kezaala developed the Short Interval Additional Dose (SIAD) tactical approach that has since become a standard for Polio outbreak response. In 2012, Dr Kezaala served as WHO liaison officer to the US Centers for Disease Control (CDC) in setting up the Emergency Operations Centre (EOC) for CDC's global Polio Eradication Initiative. Since June 2012, Dr Kezaala has served as Senior Health Advisor at the UNICEF headquarters in New York in charge of the Accelerated Immunization Initiatives - responsible for Measles and Rubella control, Yellow Fever, epidemic Meningitis and immunization in emergency settings. In 2016, he was instrumental as liaison officer to WHO in the response to the central Africa Yellow Fever outbreak that affected Angola and the Democratic Republic of Congo. \n\nNumerous news outlets and reports such as U.S. News, CNN, TV2Africa, allAfrica have quoted Dr. Kezaala. He is also a thought leader in topics related to public health, vaccines, health diplomacy and on Uganda.\n"}
{"id": "23964266", "url": "https://en.wikipedia.org/wiki?curid=23964266", "title": "Sertoli cell nodule", "text": "Sertoli cell nodule\n\nA Sertoli cell nodule, also Pick's adenoma, testicular tubular adenoma and tubular adenoma of the testis, is a benign proliferation of Sertoli cells that arises in association with cryptorchidism (undescended testis). They are not composed of a clonal cell population, i.e. neoplastic; thus, technically, they should not be called an \"adenoma\".\n\nSertoli cell nodules are unencapsulated nodules that consist of:\n\n\n"}
{"id": "33213655", "url": "https://en.wikipedia.org/wiki?curid=33213655", "title": "Specialty Society Relative Value Scale Update Committee", "text": "Specialty Society Relative Value Scale Update Committee\n\nThe Specialty Society Relative Value Scale Update Committee or Relative Value Update Committee (RUC, pronounced \"ruck\") is a volunteer group of 31 physicians who have made highly influential recommendations on how to value a physician's work when computing health care prices in the United States' public health insurance program Medicare.\n\nBefore the 1992 implementation of the Medicare fee schedule, physician payments were made under the \"usual, customary and reasonable\" payment model (a \"charge-based\" payment system). Physician services were largely considered to be misvalued under this system, with evaluation and management services being undervalued and procedures overvalued. Third-party payers (public and private health insurance) advocated an improved model to replace the UCR fees, which had been associated with stark examples of specialists making significantly higher sums of money than primary care physicians.\n\nWith reference to the research of William Hsiao and colleagues, the Omnibus Budget Reconciliation Act of 1989 was passed with the legislative intent of reducing the payment disparity between primary care and other specialties through use of the resource-based relative value scale (RBRVS). Beginning in 2000, all three components of the Medicare RBRVS, physician work, practice expense and malpractice expense are resource-based as required by Section 1848(c) of the Social Security Act.\n\nRUC was established in 1991 by the American Medical Association (AMA) and medical specialist groups. The AMA sponsors RUC \"both as an exercise of 'its First Amendment rights to petition the Federal Government' and for 'monitoring economic trends ... related to the CPT [Current Procedures and Terminology] development process\".\n\nRUC is highly influential because it \"de facto\" sets Medicare valuations of physician work relative value units (RVUs) of Current Procedural Terminology (CPT) codes. (The Centers for Medicare and Medicaid Services (CMS) is the \"de jure\" work RVU determining body.) On average, physician work RVUs make up slightly more than half of the value in a Medicare payment. Historically, CMS has accepted RUC recommendations more than 90% of the time. Health economist Uwe Reinhardt characterized the CMS as slavishly accepting RUC recommendations. The physician work RVU values accepted by CMS also influence private health insurance reimbursement.\n\nIn 2002, a RUC update of values raised concerns that the process, which was initiated by medical speciality groups, unfairly cut primary care physician pay.\n\nIn a 2010 \"Archives of Internal Medicine\" publication written before the major health care reform legislation passed Congress—the Patient Protection and Affordable Care Act (PPACA)—Federman \"et al\". wrote:\n\nCritics have pointed out that many RUC members may have significant conflicts of interest because of their financial relationships.\n\nIn 2013 a report in the Washington \"Post\" highlighted how time seemed to bend in the system of time values assigned to various procedures. A Florida practice performing an average twelve colonoscopies and four other procedures a day in 2012 would be considered to take the physically impossible 26 hours in a nine- to 10-hour day. In other examples: In Florida and Pennsylvania surgery centers in some specialties, \"more than one in five doctors would have to have been working more than 12 hours on average on a single day — much longer than the 10 hours or so a typical surgery center is open\"; and \"Florida records show 78 doctors — gastroenterologists, ophthalmologists, orthopedic surgeons and others — who performed at least 24 hours worth of procedures on an average workday\". RUC chairperson Levy said in the report, \"None of us believe the numbers are fine-tuned... We do believe we get them right with respect to each other\" while emphasizing that the \"voting people around that table can be really harsh\". Researcher Hsiao of the original Harvard study said the \"current set of values 'seems to be distorted... The AMA fought very hard to take over this updating process. I said this had to be done by an impartial group of people. This is highly political'\". Looking at the time between 2003 and 2013, \"the AMA and Medicare have increased the work values for 68 percent of the 5,700 codes analyzed by The Post, while decreasing them for only 10 percent\" and while technology is argued, again with colonoscopies as an example, to be reducing actual time spent. Looked at another way, \"Medicare spending on physician fees per patient grew 58 percent between 2001 and 2011, mostly because doctors increased the number of procedures performed but also because the price of those procedures rose\". Finally, there was an indication in the report that the acceptance rate of the AMA's values by Medicare \"has fallen in recent years from 90 percent [or higher] to about 70 percent\" but the federal agency has far fewer people – \"six to eight\" – monitoring the process than the AMA has operating it.\n\nThe RUC bears the brunt of the inherent problems with regulation and government price-setting. In a follow-up to The Washington Post report, Bloomberg notes: \"There is no system of payment-setting that will not ultimately rely on information from self-interested parties, just as there is no system of financial regulation that can be designed without talking to bankers, or a system of education reform that can be put in place without asking teachers and principals how things work now.\" \n\nThe Independent Payment Advisory Board passed in the PPACA. It could bypass RUC to cut payments to relatively highly compensated specialists, such as dermatologists.\n\nThe current membership of the Relative Value Scale Update Committee (RUC) is as follows:\n\n"}
{"id": "43148573", "url": "https://en.wikipedia.org/wiki?curid=43148573", "title": "Teach-back method", "text": "Teach-back method\n\nThe teach-back method, also called the \"show-me\" method, is a communication confirmation method used by healthcare providers to confirm whether a patient (or care takers) understands what is being explained to them. If a patient understands, they are able to \"teach-back\" the information accurately. This is a communication method intended to improve health literacy.\n\nThere can be a significant gap in the perception of how much a patient needs information, or how effective a provider's communication is. This can be due to various reasons such as a patient not understanding medical terminology, not feeling comfortable asking questions or even cognitive impairment. Not only does the teach-back method help providers understand the patient's needs in understanding their care, it also allows providers to evaluate their communication skills. Case studies led by the National Quality Forum on the informed consent processes of various hospitals found that those that effectively used the teach-back method benefited in areas of quality, patient safety, risk management and cost/efficiency.\n\nThe National Quality Forum describes the practice as follows:\nWho should use the method→ Any healthcare providers. E.g. physicians, nurses, healthcare professionals\nWhat should patients teach-back→Information about their diagnosis, treatment plan, medications, risks and benefit of treatment, etc.\nWhen to ask for teach-back→ Early in the care process\nWhy is it important→Many patients have difficulty understanding medical information.\nHow→When asked to teach-back, patients should be able to clearly describe or explain the information provided to them.\n\nDepending on the patient's successful or unsuccessful teach-back, the provider will clarify or modify the information and reassess the teach-back to confirm the patient's comprehension and understanding.\n\nThe cycle of reassessing and teaching back to confirm comprehension has been found to improve knowledge retention and lower readmission rates in heart failure patients.\n\nBeyond healthcare literacy, the teach-back method can be utilized in academic and professional settings as well. Teachers often create feedback loops in which the instructor asks the student to share what they heard, and promote peer to peer coaching where students explain what they just learned to other students. Retention is also most positively impacted in participatory learning environments, when students participate in group discussions, practice by doing, and teaching others.\n"}
{"id": "35528221", "url": "https://en.wikipedia.org/wiki?curid=35528221", "title": "The Task Force for Global Health", "text": "The Task Force for Global Health\n\nThe Task Force for Global Health is an international, nonprofit organization that works to improve health of people most in need, primarily in developing countries. Founded in 1984 by global health pioneer Dr. William Foege, The Task Force consists of eight programs focused on neglected tropical diseases, vaccines, field epidemiology, public health informatics, and health workforce development. Those programs include the African Health Workforce Project, the Center for Vaccine Equity, Children Without Worms, International Trachoma Initiative, Mectizan Donation Program, Neglected Tropical Diseases Support Center, Public Health Informatics Institute, and TEPHINET. The Task Force works in partnership with ministries of health and hundreds of organizations, including major pharmaceutical companies that donate billions of dollars annually in essential medicines. Major funders include the Bill & Melinda Gates Foundation, CDC, WHO, Robert Wood Johnson Foundation, de Beaumont Foundation, United States Agency for International Development, Sightsavers, Pfizer, Merck & Co., Johnson & Johnson, and GlaxoSmithKline. The Task Force is affiliated with Emory University, headquartered in Decatur, Georgia, a town in metro Atlanta, and has regional offices in Guatemala and Ethiopia. The Task Force currently supports work in 154 countries.\n\nThe Task Force for Global Health received the 2016 Conrad N. Hilton Humanitarian Prize during a symposium titled \"The Future of Humanitarian Action\" held at the Waldorf Astoria in New York City. The Task Force was selected by a panel of independent international jurors for having made \"contributions to improving the health of people living in extreme poverty.\".\n\nThe organization was co-founded by global health pioneer and former CDC Director, Dr. William Foege and two of his former CDC colleagues, Carol Walters and Bill Watson. It was founded in 1984 as The Task Force for Child Survival. The Task Force was initially launched to foster collaboration among leading health and development agencies. Under the leadership of Dr. Foege, The Task Force brought together World Bank Group, The Rockefeller Foundation, The United Nations Development Programme, WHO, and UNICEF, WHO to raise childhood immunization rates. In 1984, only 20 percent of children worldwide were receiving immunizations and 12,000 children, mostly in the world's poorest countries, were dying every day. By 1990, The Task Force had raised childhood immunization rates to 80 percent globally. Since then, The Task Force has used its strength and credibility in collaboration to positively affect a broad range of health issues affecting the world's poor. In 2015, The Task Force celebrated 30 years of contributions to global health at an event that included World Bank Group President Jim Yong Kim.\n\nThe Task Force works with hundreds of partners to control and eliminate neglected tropical diseases and increase access to medicines and vaccines for multi-drug-resistant tuberculosis, poliomyelitis, influenza, and cholera. Beginning with the Mectizan Donation Program, The Task Force is credited with working with the pharmaceutical industry to donate billions of dollars annually in essential medicines for the control and elimination of neglected tropical diseases.\n\nCollaboration, health equity, and social justice are the cornerstones of all Task Force programs. The organization is a major partner in the global effort to eliminate three neglected tropical diseases by 2025—blinding trachoma, river blindness, and lymphatic filariasis—which collectively threaten hundreds of millions of people each year with blindness, disfigurement, and death.\n\nInformation and laboratory technologies are vital tools in The Task Force's work to control and eliminate diseases and increase access to quality health care for people in developing countries. Using a smartphone-based data collection system, The Task Force maps the prevalence of neglected tropical diseases (NTDs) to determine where interventions should be implemented. It uses portable molecular technology and tablet-based systems to detect and diagnose NTDs within populations. It also uses diverse technologies to help developing countries manage their healthcare workforce in order to meet the health needs of their populations.\n\nIn 2015, The Task Force's Public Health Informatics Institute was named a partner on a new Bill & Melinda Gates Foundation-funded initiative called the Child Health and Mortality Prevention Surveillance Program (CHAMPS) that aims to understand and ultimately address the causes of death for children under 5 in developing countries.\n\nIn 2016, the Task Force helped launch a program called Digital Bridge which is developing an electronic case reporting system to improve information exchange between the public health and healthcare sectors.\n\nIn 2016, The Task Force reached an agreement to purchase a DeKalb County government building in downtown Decatur for a larger headquarters. The Task Force has begun examining how it might help address the growing epidemic of non-communicable diseases (NCDs) in developing countries.\n"}
{"id": "25384450", "url": "https://en.wikipedia.org/wiki?curid=25384450", "title": "Uni Health", "text": "Uni Health\n\nUni Research Health is a department in Uni Research, one of the largest research companies in Norway. Research Director of Uni Research Health is Professor Hege R. Eriksen.\n\nUni research health has approximately 125 employees, most of them located in Bergen, Norway.\n\nThe research and educational activities of Uni Health are concentrated in the following research units:\n\nCentre for Child and Adolescent Mental Health Research\nChild Protection Research Unit\nDental Biomaterials: Adverse Reaction Unit\nGAMUT (the Grieg Academy Music Therapy Research Centre)\nHEMIL Centre (Research Centre for Health Promotion)\nNational Centre for Emergency Primary Health Care\nOccupational and Environmental Medicine\nResearch Unit for General Practice in Bergen\nResearch Centre for Sick Leave and Rehabilitation\nStress, Health and Rehabilitation (formerly the Research Unit of the Norwegian Network for Back Pain)\n\n"}
{"id": "584416", "url": "https://en.wikipedia.org/wiki?curid=584416", "title": "Uterine contraction", "text": "Uterine contraction\n\nA uterine contraction is a muscle contraction of the uterine smooth muscle.\n\nThe uterus frequently contracts throughout the entire menstrual cycle, and these contractions have been termed \"endometrial waves\" or \"contractile waves\". These appear to involve only the sub-endometrial layer of the myometrium. In the early follicular phase, these contractions occur once or twice per minute and last 10–15 seconds with a low amplitude of usually 30 mmHg. The frequency increases to 3–4 per minute towards ovulation. During the luteal phase, the frequency and amplitude decrease, possibly to facilitate any implantation.\n\nIf implantation does not occur, the frequency remains low, but the amplitude increases dramatically to between 50 and 200 mmHg producing labor-like contractions at the time of menstruation. These contractions are sometimes termed \"menstrual cramps\", although that term is often used for menstrual pain in general. These contractions may be uncomfortable or even painful, but they are generally significantly less painful than contractions during labour. A hot water bottle or exercising has been found to help.\n\nA shift in the myosin expression of the uterine smooth muscle has been hypothesized to avail for changes in the directions of uterine contractions that are seen during the menstrual cycle.\n\nA contraction refers specifically to the motion of the uterus as part of the process of childbirth. Contractions, and labour in general, is one condition that releases the hormone oxytocin into the body. Contractions become longer as labour intensifies.\n\nPrior to actual labour, women may experience Braxton Hicks contractions, sometimes known as \"false labour.\"\n\nSince every pregnancy is different, a doctor, midwife or other competent professional should always be consulted before any action is taken to reduce the pain. Some popular methods may be harmful to the mother and/or the baby, or may actually worsen the pain or lengthen the labour.\n\nUterine contractions during childbirth can be monitored by cardiotocography, in which a device is fixated to the skin of the mother or directly to the fetal scalp. The pressure required to flatten a section of the uterine wall correlates with the internal pressure, thereby providing an estimate of it. \n\nA type of monitoring technology under development at Drexel University embeds conductive threads in the knitted fabric of a bellyband. When the fibers stretch in response to a contraction, the threads function like an antenna, and send the signals they pick up to an embedded RFID (radio frequency identification device) chip that reports the data.\n\nThe uterus and vagina contract during female orgasm. These contractions may not be noticed by all women; pregnant women are more likely to notice these contractions by late 2nd and 3rd trimesters.\n\n"}
{"id": "9777312", "url": "https://en.wikipedia.org/wiki?curid=9777312", "title": "Vaginal intraepithelial neoplasia", "text": "Vaginal intraepithelial neoplasia\n\nVaginal intraepithelial neoplasia (VAIN) is a condition that describes premalignant histological findings in the vagina characterized by dysplastic changes.\n\nThe disorder is rare and generally has no symptoms. VAIN can be detected by the presence of abnormal cells in a Papanicolaou test (Pap smear).\n\nLike cervical intraepithelial neoplasia, VAIN comes in three stages, VAIN 1, 2, and 3. In VAIN 1, a third of the thickness of the cells in the vaginal skin are abnormal, while in VAIN 3, the full thickness is affected. VAIN 3 is also known as carcinoma in-situ, or stage 0 vaginal cancer.\n\nInfection with certain types of the human papillomavirus (\"high-risk types\") may be associated with up to 80% of cases of VAIN. Vaccinating girls with HPV vaccine before initial sexual contact has been shown to reduce incidence of VAIN.\n"}
{"id": "45269905", "url": "https://en.wikipedia.org/wiki?curid=45269905", "title": "Women &amp; Therapy", "text": "Women &amp; Therapy\n\nWomen & Therapy is a quarterly peer-reviewed academic journal covering behavioral science, feminist psychology, mental health, psychological science, and psychotherapy. It was established in 1982 and is published by Taylor & Francis. The editor-in-chief is Ellyn Kaschak (San Jose State University).\n\nThe journal is abstracted and indexed in:\n\nAccording to the \"Journal Citation Reports\", the journal has a 2015 impact factor of 0.229, ranking it 34th out of 41 journals in the category \"Women's Studies\".\n\n"}
