{"id": "1841309", "url": "https://en.wikipedia.org/wiki?curid=1841309", "title": "10/90 gap", "text": "10/90 gap\n\nThe 10/90 gap is the term adopted by the Global Forum for Health Research to highlight the finding by the Commission on Health Research for Development in 1990, that less than 10% of worldwide resources devoted to health research were put towards health in Developing Countries, where over 90% of all preventable deaths worldwide occurred. Every year, the spread of disease suffered in both rich and poor countries converges. According to the World Health Organization (WHO), the most prevalent diseases consist of cardiovascular disease, cancer and diabetes. These diseases now account for 45% of the global health burden and is the culprit for up to 85% of deaths in low-income countries. The 10/90 Gap focuses on joining organisations together to reduce these statistics.\n\nA substantial portion of diseases, most prevalent in impoverished or developing countries, are those which are preventable and/or treatable. The World Health Organization (2004) stated in their world health report that an estimated eight million individuals die prematurely, from diseases and conditions that can be cured, every year. These deaths contribute to approximately one third of all human deaths in the world, each year. Table 1 lists several of these curable diseases.\n\nTable 1: Causes of avoidable deaths.\n\nGlobal health organisations across the globe have explicitly made research into health their priority. In 2000, World Health Organization established the Commission on Macroeconomics and Health, who in their 2001 report, verified the relationship between poverty and disease and discussed the benefit of investment on the economic climates of developing countries. Possible strategies that can be implanted to help reduce the 10/90 gap are, policies prioritizing funding for health research, also developing capacities of credible public and private health research institutions. Also activities of international NGOs to undertake research aimed at resolving the gap, and the setting up of research based mechanisms to ensure access to new effective products for the treatment and prevention of poverty-related diseases. However, given the number of diseases that are preventable, other factors that are blocking the access of patients to these products, such as cost of treatments to the individual, also need to be addressed, rather that just focusing on developing new drugs. There is also a need to build the primary health care sector in developing countries. It has been shown that early detection and effective management of disease can be provided by appropriately-trained, non-physician, healthcare workers.\n\nThe gap between financial needs and financial means in low income countries can only be filled by donations. The average health expenditure per capita in western countries is estimated at $947 compared to $20 per capita in low income countries. To assist the worlds poorest, health expenditure between $44–60 per capita is placed as a target. This target is achieved by the contribution of 0.1% of advanced western countries gross national production (GNP) to fund global health, this recommendation was set by the WHO in 2001 and is estimated to save 8 million lives per year.\n\nAmong other efforts to address this issue are recent proposals for a Global Research and Development (R&D) treaty, and the creation of the nonprofit pharmaceutical company OneWorld Health which develops new and affordable medicines for neglected diseases. A treaty creates a system that allows health research to come from contributions by all and therefore the sharing of benefits for all, improving the fairness and sustainability of global research and development. In a positive development for the closing of the gap, in the 2012 London Declaration on Neglected Tropical Diseases many parties, including governments, funding organisations and drug companies, agreed to work towards the eradication of neglected tropical diseases.\n\nDespite the findings of the World Health Organization, and the disparity of curable diseases throughout developing and developed countries, some people believe the 10/90 gap is a myth. They criticise the fact that simple medicines which stop curable and treatable conditions such as diarrhoea and malaria are available to these countries. The issue is not with the medicine, but with the ability for the affected to access it. Access to medicine can be inhibited by many circumstances such as poverty, strict government regime and inadequate healthcare systems and infrastructure.\n\n\n"}
{"id": "39993576", "url": "https://en.wikipedia.org/wiki?curid=39993576", "title": "5-SPICE framework", "text": "5-SPICE framework\n\nThe 5-SPICE framework is an instrument designed for global health practitioners to guide discussions about community health worker (CHW) projects.\n\nThe 5-SPICE framework was developed by clinicians and researchers from Partners In Health, Harvard Medical School, and Brigham and Women’s Hospital in Boston, MA. The framework lays out a model for integrating community health workers into public health systems, learning from the experiences Partners In Health and partner organizations at their project sites in resource-poor settings around the world. 5-SPICE draws upon experiences from Haiti, Rwanda, Lesotho, Liberia, Nepal, Mali, and elsewhere, where CHWs have been employed to improve patient outcomes and overcome personnel shortages. The framework allows for all stakeholders in a community health program to participate in discussions and analyses to strengthen the impact of CHWs.\n\n5-SPICE allows for all stakeholders in a community health program to participate in discussions and analyses to strengthen the impact of community health workers.\n\nThe name 5-SPICE is derived from Chinese cuisine emphasizing the balance between inputs and elements. The five main elements form an acronym:\n\nThese elements are not a static list, but a way to holistically analyze how core programmatic elements affect each other in the field. In the Freirean tradition of awareness, the 5-SPICE model emphasizes facilitated discussion and contemplation among stakeholders, particularly CHWs, to maximize program outputs. Ultimately, the 5-SPICE framework allows program implementers to study the relationship between the health system and the local community.\n\nOther CHW program frameworks exist, such as the CHW Assessment and Improvement Matrix (AIM) developed by the USAID-funded Health Care Improvement (HCI) project. 5-SPICE complements these other frameworks by providing an acronym that condenses the many elements discussed in other frameworks into an easy-to-remember heuristic, allowing for more effective and efficient assessments that are exploratory rather than prescriptive.\n\nThe 5-SPICE framework was first formally introduced in an April 2013 publication in Global Health Action, in an article entitled “5-SPICE: the application of an original framework for community health worker program design, quality improvement and research agenda setting.” The framework was subsequently presented at the 2013 Consortium of Universities for Global Health Conference, and the 2013 Swedish Society of Medicine’s annual conference, Global Health—Beyond 2015. The article has also been accepted for presentation at the 2013 Annual Meeting of the American Public Health Association.\n"}
{"id": "37764225", "url": "https://en.wikipedia.org/wiki?curid=37764225", "title": "California Healthy Families Program", "text": "California Healthy Families Program\n\nThe California Healthy Families Program (HFP) is the California implementation of the federal Children's Health Insurance Program (CHIP) that provides low cost insurance offering health, dental, and vision coverage to children who do not have insurance today and do not qualify for no-cost Medi-Cal.\n\nAs a federal program it is administered by the U.S. Department of Health and Human Services (HHS) and as a California program it is administered by the California Managed Risk Medical Insurance Board (MRMIB).\n\nAs a result of the 2012-2013 budget deal, nearly 900,000 children will be moved from the HFP into Medi-Cal beginning in 2013.\n\n"}
{"id": "23804755", "url": "https://en.wikipedia.org/wiki?curid=23804755", "title": "Capitation (healthcare)", "text": "Capitation (healthcare)\n\nCapitation is a payment arrangement for health care service providers such as physicians, physician assistants or nurse practitioners. It pays a physician or group of physicians a set amount for each enrolled person assigned to them, per period of time, whether or not that person seeks care. These providers generally are contracted with a type of health maintenance organization (HMO) known as an independent practice association (IPA), which enlists the providers to care for HMO-enrolled patients. The amount of remuneration is based on the average expected health care utilization of that patient, with greater payment for patients with significant medical history.\n\nPrimary capitation is a relationship between a managed care organization (MCO) and primary care physician (PCP), in which the PCP is paid directly by the MCO for those enrolled members who have selected the physician as their provider.\n\nSecondary capitation is a relationship arranged by the MCO between a PCP and a secondary or specialist provider, such as an X-ray facility or ancillary facility such as a durable medical equipment supplier whose secondary provider is also paid capitation based on that PCP’s enrolled membership.\n\nGlobal capitation is a relationship based on a provider who provides services and is reimbursed per-member per-month (PMPM) for the entire network population.\n\nUnder capitation, physicians are given an incentive to consider the cost of treatment. Pure capitation pays a set fee per patient, regardless of their degree of infirmity, giving physicians an incentive to avoid the most costly patients.\n\nProviders who work under such plans focus on preventive health care, as there is a greater financial reward in the prevention of illness than in the treatment of the ill. Such plans divert providers from the use of expensive treatment options.\n\nThe financial risks providers accept in capitation are traditional insurance risks. Provider revenues are fixed, and each enrolled patient makes a claims against the full resources of the provider. In exchange for the fixed payment, physicians essentially become the enrolled clients' insurers, who resolve their patients' claims at the point of care and assume the responsibility for their unknown future health care costs. Large providers tend to manage the risk better than do smaller providers because they are better prepared for variations in service demand and costs, but even large providers are inefficient risk managers in comparison to large insurers. Providers tend to be small in comparison to insurers and so are more like individual consumers, whose annual costs as a percentage of their annual cash flow vary far more than do those of large insurers. For example, a capitated eye care program for 25,000 patients is more viable than a capitated eye program for 10,000 patients. The smaller the roster of patients, the greater the variation in annual costs and the more likely that the costs may exceed the resources of the provider. In very small capitation portfolios, a small number of costly patients can dramatically affect a provider's overall costs and increase the provider's risk of insolvency.\n\nPhysicians and other health care providers lack the necessary actuarial, underwriting, accounting and finance skills for insurance risk management, but their most severe problem is the greater variation in their estimates of the average patient cost, which leaves them at a financial disadvantage as compared to insurers whose estimates are far more accurate. Because their risks are a function of portfolio size, providers can reduce their risks only by increasing the numbers of patients they carry on their rosters, but their inefficiency relative to that of the insurers' is far greater than can be mitigated by these increases. To manage risk as efficiently as an insurer, a provider would have to assume 100% of the insurer's portfolio. HMOs and insurers manage their costs better than risk-assuming healthcare providers and cannot make risk-adjusted capitation payments without sacrificing profitability. Risk-transferring entities will enter into such agreements only if they can maintain the levels of profits they achieve by retaining risks.\n\nProviders cannot afford reinsurance, which would further deplete their inadequate capitation payments, as the reinsurer's expected loss costs, expenses, profits and risk loads must be paid by the providers. The goal of reinsurance is to offload risk and reward to the reinsurer in return for more stable operating results, but the provider's additional costs make that impractical. Reinsurance assumes that the insurance-risk-transferring entities do not create inefficiencies when they shift insurance risks to providers.\n\nWithout any induced inefficiencies, providers would be able to pass on a portion of their risk premiums to reinsurers, but the premiums that providers would have to receive would exceed the premiums that risk-transferring entities could charge in competitive insurance markets. Reinsurers are wary of contracting with physicians, as they believe that if providers think they can collect more than they pay in premiums, they would tend to revert to the same excesses encouraged by fee-for-service payment systems.\n\n\n"}
{"id": "29963762", "url": "https://en.wikipedia.org/wiki?curid=29963762", "title": "Cardiovascular fitness", "text": "Cardiovascular fitness\n\nCardiovascular Endurance is the ability of the heart and lungs to supply oxygen-rich blood to the working muscle tissues and the ability of the muscles to use oxygen to produce energy for movement. This type of fitness is a health-related component of physical fitness that is brought about by sustained physical activity. A person's ability to deliver oxygen to the working muscles is affected by many physiological parameters, including heart rate, stroke volume, cardiac output, and maximal oxygen consumption.\n\nUnderstanding the relationship between cardiorespiratory endurance training and other categories of conditioning requires a review of changes that occur with increased aerobic, or anaerobic capacity. As aerobic/anaerobic capacity increases, general metabolism rises, muscle metabolism is enhanced, haemoglobin rises, buffers in the bloodstream increase, venous return is improved, stroke volume is improved, and the blood bed becomes more able to adapt readily to varying demands. Each of these results of cardiovascular fitness/cardiorespiratory conditioning will have a direct positive effect on muscular endurance, and an indirect effect on strength and flexibility.\n\nTo facilitate optimal delivery of oxygen to the working muscles, the person needs to train or participate in activities that will build up the energy stores needed for sport. This is referred to as metabolic training. Metabolic training is generally divided into two types: aerobic and anaerobic. A 2005 Cochrane review demonstrated that physical activity interventions are effective for increasing cardiovascular fitness.\n"}
{"id": "52410293", "url": "https://en.wikipedia.org/wiki?curid=52410293", "title": "Cytotoxic hazard symbol", "text": "Cytotoxic hazard symbol\n\nThe Cytotoxic hazard symbol is a hazard symbol consisting of a triangle with the letter \"C\" inside it. It is used to label biomedical waste bags and containers.\n\nCytotoxic waste, the by-product of cytotoxic drug therapy administered to patients (such as chemotherapy), typically includes all drug administrative equipment (e.g. needles, syringes, dripsets etc.) as well as all gowns and body fluids/waste from patients undergoing such treatment.\n\n"}
{"id": "56325961", "url": "https://en.wikipedia.org/wiki?curid=56325961", "title": "Ernest Madu", "text": "Ernest Madu\n\nDr. Ernest Madu (born February 18, 1960) is a nuclear cardiologist whose work focuses on providing affordable public healthcare in low-resource nations. As founder, chairman, and CEO of the Heart Institute of the Caribbean (HIC), Dr. Madu’s research concentrates on the management and health effects of globalization in susceptible populations. Dr. Richard Chazal, President of the American College of Cardiology, noted that Dr. Madu has made \"great contributions to the College and to the international cardiovascular community.\" While TED notes that “Dr. Madu’s research on noninvasive evaluation of coronary artery disease in obese individuals has become a standard evaluating tool” in the world of medicine.\n\nExtramural honors and awards include:\n\n"}
{"id": "34902186", "url": "https://en.wikipedia.org/wiki?curid=34902186", "title": "Euro health consumer index", "text": "Euro health consumer index\n\nEuro Health Consumer Index (EHCI) is a comparison of European health care systems based on waiting times, results, and generosity. The information is presented as a graphic index. EHCI was produced 2005–2009 and 2012–2016 by Health Consumer Powerhouse. The 2014 ranking included 37 countries measured by 48 indicators. It claims to measure the \"consumer friendliness\" of healthcare systems. It does not claim to measure which European state has the best healthcare system, but it does produce specialist Indexes on Diabetes, Cardiac Care, HIV, Headache and Hepatitis. \n\nIn 2006 France was the champion with 768 points out of 1000. In the 2015 results the same performance would have given the 13th position among 35 countries because of the widespread improvements in standards.\n\nWhereas no bias in favour of any health system was alleged, the index was criticised in the British Medical Journal by Martin McKee and others from the European Observatory on Health Systems and Policies in February 2016. Points they made included: \n\n"}
{"id": "32181324", "url": "https://en.wikipedia.org/wiki?curid=32181324", "title": "Evaluation &amp; the Health Professions", "text": "Evaluation &amp; the Health Professions\n\nEvaluation & the Health Professions is a peer-reviewed public health journal that covers all aspects of the evaluation of health care. The founding editors-in-chief were R. Barker Bausell and Carolyn F. Waltz (University of Maryland, Baltimore), and the current one is Steve Sussman (University of Southern California). The journal was established in 1978 and is published by Sage Publications.\n\nThe journal is abstracted and indexed in Scopus, and the Social Sciences Citation Index. According to the \"Journal Citation Reports\", its 2014 impact factor is 1.909, ranking it 21st out of 71 journals in the category \"Health Policy & Services\" and 36th out of 89 journals in the category \"Health Care Sciences & Services\".\n"}
{"id": "44674023", "url": "https://en.wikipedia.org/wiki?curid=44674023", "title": "Fitkid", "text": "Fitkid\n\nFitkid (also \"FitKid\", \"Fit Kid\" or \"Fit-Kid\") is a type of children's sport combining gymnastics, dance and acrobatics. It originated in Europe in the 1990s, and is meant to engage children 8–18 years of age in fun individual and group exercise, free of the extremes of more strenuous aerobics or traditional gymnastics. International competitions are organized regularly by the \"International FitKid Division\", in addition to national competitions. A routine stands of four types of elements: strength, flexibility, acrobatics, and jumps. All elements are graded by difficulty from A to H. A elements being worth 0.1 points, B 0.2, C 0.3, and so on until H which is worth 0.8. Routines get evaluated in three categories: technique, artistry, and content (execution of elements). Competition is divided by age groups (I. to IX. (or Senior in A category)), and categories ( which are based on knowledge). There three categories (from hardest to easiest) A, B, and C, also called Dance. In each categories there are group and individual routines. In C Category (here group routines are also called Dance Show) team member can go from 4 to 8. In A and B category there are duos, small groups (3-4), and big groups (5-6). In C category, individually 4 elements have to be performed ( one from each type), A or B difficulty, and in groups, two are optional. In B category individually 8 elements need to be performed ( two from each type) A-D difficulty ( but max. two elements can be D), and in groups 4 (same as C category individual). In A category team and individual, 8 elements need to be performed (same as B category) and an acrobatic connection is optional. Skills in A category can be any difficulty. \n\n"}
{"id": "38853402", "url": "https://en.wikipedia.org/wiki?curid=38853402", "title": "Health care finance in the United States", "text": "Health care finance in the United States\n\nHealth care finance in the United States discusses how Americans obtain and pay for their healthcare, and why U.S. healthcare costs are the highest in the world based on various measures, without better results.\n\nThe American system is a mix of public and private insurance. The government provides insurance coverage for approximately 53 million elderly via Medicare, 62 million lower-income persons via Medicaid, and 15 million military veterans via the Veteran's Administration. About 178 million employed by companies receive subsidized health insurance through their employer, while 52 million other persons directly purchase insurance either via the subsidized marketplace exchanges developed as part of the Affordable Care Act or directly from insurers. The private sector delivers healthcare services, with the exception of the Veteran's Administration, where doctors are employed by the government.\n\nThe Centers for Medicare and Medicaid (CMS) reported that U.S. health care costs rose 5.8% to reach $3.2 trillion in 2015, or $9,990 per person. As measured by CMS, the share of the U.S. economy devoted to health care spending was 17.8% GDP in 2015, up from 17.4% in 2014. Increases were driven by the coverage expansion that began in 2014 as a result of the Affordable Care Act (i.e., more persons demanding healthcare or more healthcare units consumed) as well as higher healthcare prices per unit.\n\nU.S. healthcare costs are considerably higher than other countries as a share of GDP, among other measures. According to the OECD, U.S. healthcare costs in 2015 were 16.9% GDP, over 5% GDP higher than the next most expensive OECD country. A gap of 5% GDP represents $1 trillion, about $3,000 per person relative to the next most expensive country. In other words, the U.S. would have to cut healthcare costs by roughly one-third to be competitive with the next most expensive country.\n\nReasons for higher costs than other countries including higher administrative costs, spending more for the same services (i.e., higher prices per unit), receiving more medical care (units) per capita than other countries, cost variation across hospital regions without different results, higher levels of per-capita income, and less active government intervention to reduce costs. Spending is highly concentrated among sicker patients. The Institute of Medicine reported in September 2012 that approximately $750B per year in U.S. health care costs are avoidable or wasted. This included: unnecessary services ($210 billion annually); inefficient delivery of care ($130 billion); excess administrative costs ($190 billion); inflated prices ($105 billion); prevention failures ($55 billion), and fraud ($75 billion).\n\nDespite this spending, the quality of health care overall is low by OECD measures. The Commonwealth Fund ranked the United States last in the quality of health care among similar countries.\n\nThe percentage of persons without health insurance (the \"uninsured\") fell from 13.3% in 2013 to 8.8% in 2016, due primarily to the Affordable Care Act. The number uninsured fell from 41.8 million in 2013 to 28.0 million in 2016, a decline of 13.8 million. The number of persons with insurance (public or private) rose from 271.6 million in 2013 to 292.3 million in 2016, an increase of 20.7 million. In 2016, approximately 68% were covered by private plans, while 37% were covered by government plans; these do not add to 100% because some persons have both.\n\nAmong those whose employer pays for health insurance, the employee may be required to contribute part of the cost of this insurance, while the employer usually chooses the insurance company and, for large groups, negotiates with the insurance company. The government subsidizes the employer-based insurance by excluding premiums paid by employers from the employees income. This subsidy tax expenditure reduced federal tax revenue by $248 billion in 2013, or 1.5% GDP.\n\nThe non-partisan Congressional Budget Office (CBO) reported in March 2017 that healthcare cost inflation and an aging population are primary drivers of increasing budget deficits over time, as outlays (spending) continue to rise faster than revenues relative to GDP. CBO forecast that spending on major healthcare programs (including Medicare and Medicaid) would rise from 5.5% GDP in 2017 to 9.2% GDP by 2047.\n\nThe Centers for Medicare and Medicaid (CMS) reported that U.S. health care costs rose to 17.8% GDP in 2015, up from 17.4% in 2014. Increases were driven by the coverage expansion that began in 2014 as a result of the Affordable Care Act (i.e., more persons demanding healthcare or more healthcare units consumed) as well as higher healthcare prices per unit.\n\nU.S. healthcare costs are considerably higher than other countries as a share of GDP, among other measures. According to the OECD, U.S. healthcare costs in 2015 were 16.9% GDP, over 5% GDP higher than the next most expensive OECD country. A gap of 5% GDP represents $1 trillion, about $3,000 per person relative to the next most expensive country. In other words, the U.S. would have to cut healthcare costs by roughly one-third ($1 trillion or $3,000 per person on average) to be competitive with the next most expensive country.\n\nThe Centers for Medicare and Medicaid (CMS) reported that U.S. health care costs rose 5.8% to reach $3.2 trillion in 2015, or $9,990 per person.\n\nThe Office of the Actuary (OACT) of the Centers for Medicare and Medicaid Services publishes data on total health care spending in the United States, including both historical levels and future projections. In 2007, the U.S. spent $2.26 trillion on health care, or $7,439 per person, up from $2.1 trillion, or $7,026 per capita, the previous year. Spending in 2006 represented 16% of GDP, an increase of 6.7% over 2004 spending. Growth in spending is projected to average 6.7% annually over the period 2007 through 2017.\n\nIn 2009, the United States federal, state and local governments, corporations and individuals, together spent $2.5 trillion, $8,047 per person, on health care. This amount represented 17.3% of the GDP, up from 16.2% in 2008. Health insurance costs are rising faster than wages or inflation, and medical causes were cited by about half of bankruptcy filers in the United States in 2001.\n\nThe Centers for Medicare and Medicaid Services reported in 2013 that the rate of increase in annual healthcare costs has fallen since 2002. However, costs relative to GDP and per capita continue to rise. Per capita cost increases have averaged 5.4% since 2000.\n\nAccording to Federal Reserve data, healthcare annual inflation rates have declined in recent decades:\n\nWhile this inflation rate has declined, it has generally remained above the rate of economic growth, resulting in a steady increase of health expenditures relative to GDP from 6% in 1970 to nearly 18% in 2015.\n\nU.S. healthcare costs in 2015 were 16.9% GDP according to the OECD, over 5% GDP higher than the next most expensive OECD country. With U.S. GDP of $19 trillion, healthcare costs were about $3.2 trillion, or about $10,000 per person in a country of 320 million people. A gap of 5% GDP represents $1 trillion, about $3,000 per person relative to the next most expensive country. In other words, the U.S. would have to cut healthcare costs by roughly one-third to be competitive with the next most expensive country.\n\nOne analysis of international spending levels in the year 2000 found that while the U.S. spends more on health care than other countries in the Organisation for Economic Co-operation and Development (OECD), the use of health care services in the U.S. is below the OECD median by most measures. The authors of the study concluded that the prices paid for health care services are much higher in the U.S.\n\nSpending is highly concentrated among a relatively few patients. The Kaiser Family Foundation reported that the concentration of health care spending in the U.S. in 2010 was as follows:\n\nOther studies have found similar results using AHRQ analysis. Relative to the overall population, those who remained in the top 10% of spenders between 2008 and 2009 were more likely to be in fair or poor health, elderly, female, non-Hispanic whites and those with public-only coverage. Those who remained in the bottom half of spenders were more likely to be in excellent health, children and young adults, men, Hispanics, and the uninsured. These patterns were stable through the 1970s and 1980s, and some data suggest that they may have been typical of the mid-to-early 20th century as well.\n\nAn earlier study by AHRQ the found significant persistence in the level of health care spending from year to year. Of the 1% of the population with the highest health care spending in 2002, 24.3% maintained their ranking in the top 1% in 2003. Of the 5% with the highest spending in 2002, 34% maintained that ranking in 2003. Individuals over age 45 were disproportionately represented among those who were in the top decile of spending for both years.\n\nSeniors spend, on average, far more on health care costs than either working-age adults or children. The pattern of spending by age was stable for most ages from 1987 through 2004, with the exception of spending for seniors age 85 and over. Spending for this group grew less rapidly than that of other groups over this period.\n\nThe 2008 edition of the Dartmouth Atlas of Health Care found that providing Medicare beneficiaries with severe chronic illnesses with more intense health care in the last two years of life—increased spending, more tests, more procedures and longer hospital stays—is not associated with better patient outcomes. There are significant geographic variations in the level of care provided to chronically ill patients, only 4% of which are explained by differences in the number of severely ill people in an area. Most of the differences are explained by differences in the amount of \"supply-sensitive\" care available in an area. Acute hospital care accounts for over half (55%) of the spending for Medicare beneficiaries in the last two years of life, and differences in the volume of services provided is more significant than differences in price. The researchers found no evidence of \"substitution\" of care, where increased use of hospital care would reduce outpatient spending (or vice versa).\n\nHealthcare spending in the U.S. was distributed as follows by type of service in 2014: Hospital care 32%; physician and clinical services 20%; prescription drugs 10%; and all other, including many categories individually making up less than 7% of spending. These first three categories accounted for 62% of spending.\n\nThis distribution is relatively stable; in 2008, 31% went to hospital care, 21% to physician/clinical services, 10% to pharmaceuticals, 4% to dental, 6% to nursing homes, 3% to home health care, 3% for other retail products, 3% for government public health activities, 7% to administrative costs, 7% to investment, and 6% to other professional services (physical therapists, optometrists, etc.).\n\nAccording to a report from the Agency for Healthcare Research and Quality (AHRQ), aggregate U.S. hospital costs in 2011 were $387.3 billion—a 63% increase since 1997 (inflation adjusted). Costs per stay increased 47% since 1997, averaging $10,000 in 2011.\n\nAn estimated 178 million persons under 65 obtain their insurance through their employer. Firms are often \"self-insured\", meaning they reimburse the insurance companies that pay the medical claims on behalf of their employees. Employers may use a stop-loss, meaning they pay the insurance company a premium to cover very expensive individual claims (e.g., the firm is self-insured up to a threshold for individual workers). Workers pay a share of their costs to their employers for coverage, basically a premium deducted from their paychecks. Workers also have deductibles and out-of-pocket costs. The structure of the insurance plan may also include a Health savings account or HSA, which enable workers to save money tax-free for health expenses.\n\nThe Kaiser Family Foundation reported that employer-based health insurance premiums for a family of four averaged $18,765 in 2017, up 3% from the prior year, although there was considerable variation around this average. For single coverage, the premium costs averaged $6,690, up 4% from the previous year. The typical worker contributed $5,714 on average towards their coverage, with the employer providing the remainder.\n\nDeductibles have been rising much faster than premiums in recent years. For example, deductibles rose 12% in 2016, four times faster than premiums. From 2011 to 2016, deductibles rose 63% for single coverage, versus 19% for single coverage premiums. During that time, worker earnings rose 11%. The average annual deductible is around $1,500. For employers with fewer than 200 employees, 65% of employees are now in \"high-deductible plans\" which averaged $2,000.\n\nOne consequence of employer-based coverage (as opposed to single-payer or government-funded via individual taxes) is that employers facing increasing healthcare costs offset the expense by either paying relatively less or hiring fewer workers. Since health insurance benefits paid by employers are not treated as income to employees, the government foregoes a sizable amount of tax revenue each year. This subsidy or tax expenditure was estimated by CBO at $281 billion in 2017. On March 1, 2010, billionaire investor Warren Buffett said that the high costs paid by U.S. companies for their employees' health care put them at a competitive disadvantage. He compared the roughly 17% of GDP spent by the U.S. on health care with the 9% of GDP spent by much of the rest of the world, noted that the U.S. has fewer doctors and nurses per person, and said, \"[t]hat kind of a cost, compared with the rest of the world, is like a tapeworm eating at our economic body.\"\n\nAn estimated 12 million persons obtained their insurance from insurance companies in 2016 via online marketplaces (federal or state) developed as part of the Affordable Care Act, also known as \"Obamacare.\" This insurance is federally subsidized through a premium tax credit, which varies based on the level of income of the individual. The credit is typically applied by the insurance company to lower the monthly premium payment. The post-subsidy premium cost is capped as a percentage of income, meaning as premiums rise the subsidies rise. Approximately 10 million persons on the exchanges are eligible for subsidies. An estimated 80% of persons obtaining coverage under the ACA can get it for less than $75 per month after subsidies, if they choose the lowest-cost \"bronze\" plan. The average cost for the \"second-lowest cost silver plan\" (the benchmark plan and one of the most popular) was $208/month after subsidy for a 40-year-old male non-smoker in 2017.\n\nPresident Trump's decision in November 2017 to end the cost sharing reductions subsidy, a second type of subsidy used to reduce deductibles and co-payments, was expected to increase premiums dramatically, thereby increasing the premium tax credits as well to maintain after-subsidy costs to participants at the same percentage of income. In other words, the after-subsidy cost would not rise for those with premium tax credit subsidies. Those obtaining their insurance via the exchanges without subsidies would pay up to 20 percentage points more for insurance. CBO also estimated a $200 billion increase in the budget deficit over a decade due to Trump's decision.\n\nThe CBO estimated that ending or not enforcing the individual mandate (which requires those without health insurance to pay a penalty) would increase the uninsured by 13 million by 2027, reducing the budget deficit by $338 billion over 10 years as subsidies fall. CBO also estimated that ending the mandate would encourage healthier people to drop out of the marketplaces, thus raising premiums by up to 10%.\n\nMedicaid is a joint federal and state program that helps with medical costs for about 74 million people (as of 2017) with limited income and resources. Medicaid also offers benefits not normally covered by Medicare, like nursing home care and personal care services. Medicaid is the largest source of funding for medical and health-related services for people with low income in the United States, providing free health insurance to low-income and disabled people. It is a means-tested program that is jointly funded by the state and federal governments and managed by the states, with each state currently having broad leeway to determine who is eligible for its implementation of the program. States are not required to participate in the program, although all have since 1982. Medicaid recipients must be U.S. citizens or legal permanent residents, and may include low-income adults, their children, and people with certain disabilities. Poverty alone does not necessarily qualify someone for Medicaid. The Federal Medical Assistance Percentage (FMAP), the percent of Medicaid program costs covered by the federal government, ranges from 50% for higher-income states to 75% for states with lower per-capita incomes.\n\nThe Affordable Care Act (\"Obamacare\") significantly expanded both eligibility for and federal funding of Medicaid starting in 2014, with an additional 11 million covered by 2016. Under the law as written, all U.S. citizens and legal residents with income up to 133% of the poverty line, including adults without dependent children, would qualify for coverage in any state that participated in the Medicaid program. However, the United States Supreme Court ruled in National Federation of Independent Business v. Sebelius that states do not have to agree to this expansion to continue to receive previously established levels of Medicaid funding, and 19 Republican-controlled states have chosen to continue with pre-ACA funding levels and eligibility standards. Expanding Medicaid in these 19 states would expand coverage for up to four million people.\n\nThe CBO reported in October 2017 that the federal government spent $375 billion on Medicaid in fiscal year 2017, an increase of $7 billion or 2% over 2016. The increase was primarily driven by more persons covered due to the ACA.\n\nMedicare covered 57 million people mainly aged 65 and over as of September 2016. Enrollees pay little in premiums but have deductibles for hospital stays. The program is funded partially by the FICA payroll tax and partially by the general fund (other tax revenues). The CBO reported in October 2017 that adjusted for timing differences, Medicare spending rose by $22 billion (4%) in fiscal year 2017 to $595 billion, reflecting growth in both the number of beneficiaries and in the average benefit payment. Medicare average spending per-enrollee was $10,986 in 2014 across the U.S., with states ranging from $8,238 in Montana to $12,614 in New Jersey.\n\nThe reasons for higher U.S. healthcare costs relative to other countries and over time are debated by experts.\n\nThere are many reasons why U.S. healthcare costs are higher than other OECD countries:\n\nIn December 2011, the outgoing Administrator of the Centers for Medicare & Medicaid Services, Dr. Donald Berwick, asserted that 20% to 30% of health care spending is waste. He listed five causes for the waste: (1) overtreatment of patients, (2) the failure to coordinate care, (3) the administrative complexity of the health care system, (4) burdensome rules and (5) fraud.\n\nThe Institute of Medicine reported in September 2012 that approximately $750B per year in U.S. health care costs are avoidable or wasted. This included: unnecessary services ($210 billion annually); inefficient delivery of care ($130 billion); excess administrative costs ($190 billion); inflated prices ($105 billion); prevention failures ($55 billion), and fraud ($75 billion).\n\nThe Congressional Budget Office analyzed the reasons for healthcare cost inflation over time, reporting in 2008 that: \"Although many factors contributed to the growth, most analysts have concluded that the bulk of the long-term rise resulted from the health care system's use of new medical services that were made possible by technological advances...\" In summarizing several studies, CBO reported the following drove the indicated share of the increase (shown as a range across three studies) from 1940 to 1990:\n\nSeveral studies have attempted to explain the reduction in the rate of annual increase following the Great Recession of 2007-2009. Reasons include, among others:\n\nIn September 2008 \"The Wall Street Journal\" reported that consumers were reducing their health care spending in response to the current economic slow-down. Both the number of prescriptions filled and the number of office visits dropped between 2007 and 2008. In one survey, 22% of consumers reported going to the doctor less often, and 11% reported buying fewer prescription drugs.\n\nThe Health and Human Services Department expects that the health share of GDP will continue its historical upward trend, reaching 19.6% of GDP by 2024.\n\nThe non-partisan Congressional Budget Office (CBO) reported in March 2017 that healthcare cost inflation and an aging population are primary drivers of increasing budget deficits over time, as outlays (spending) continue to rise faster than revenues relative to GDP. CBO forecast that spending on major healthcare programs (including Medicare and Medicaid) would rise from 5.5% GDP in 2017 to 9.2% GDP by 2047.\n\nThe Medicare Trustees provide an annual report of the program's finances. The forecasts from 2009 and 2015 differ materially, mainly due to changes in the projected rate of healthcare cost increases, which have moderated considerably. Rather than rising to nearly 12% GDP over the forecast period (through 2080) as forecast in 2009, the 2015 forecast has Medicare costs rising to 6% GDP, comparable to the Social Security program.\n\nThe increase in healthcare costs is one of the primary drivers of long-term budget deficits. The long-term budget situation has considerably improved in the 2015 forecast versus the 2009 forecast per the Trustees Report.\n\nDoctors and hospitals are generally funded by payments from patients and insurance plans in return for services rendered (fee-for-service or FFS). In the FFS payment model, each service provided is billed as an individual item, which creates an incentive to provide more services (e.g., more tests, more expensive procedures, and more medicines). This is in contrast to bundled payments, in which the amount the insurer will pay to the service providers is bundled per episode (e.g., for a heart attack patient, a total amount will be paid to the network providing the care for say 180 days). Bundling on a per patient basis (rather than per-episode) was referred to in the 1990s as a \"capitated payment\" but is now described as an accountable care organization. Bundling provides an incentive to lower costs, which requires offsetting measures and incentives for quality of care. Several best-practice healthcare systems, such as the Kaiser and Mayo health systems, use bundled payments.\n\nAmong those whose employer pays for health insurance, the employee may be required to contribute part of the cost of this insurance, while the employer usually chooses the insurance company and, for large groups, negotiates with the insurance company. In 2004, private insurance paid for 36% of personal health expenditures, private out-of-pocket 15%, federal government 34%, state and local governments 11%, and other private funds 4%. Due to \"a dishonest and inefficient system\" that sometimes inflates bills to ten times the actual cost, even insured patients can be billed more than the real cost of their care.\n\nInsurance for dental and vision care (except for visits to ophthalmologists, which are covered by regular health insurance) is usually sold separately. Prescription drugs are often handled differently from medical services, including by the government programs. Major federal laws regulating the insurance industry include COBRA and HIPAA.\n\nIndividuals with private or government insurance are limited to medical facilities which accept the particular type of medical insurance they carry. Visits to facilities outside the insurance program's \"network\" are usually either not covered or the patient must bear more of the cost. Hospitals negotiate with insurance programs to set reimbursement rates; some rates for government insurance programs are set by law. The sum paid to a doctor for a service rendered to an insured patient is generally less than that paid \"out of pocket\" by an uninsured patient. In return for this discount, the insurance company includes the doctor as part of their \"network\", which means more patients are eligible for lowest-cost treatment there. The negotiated rate may not cover the cost of the service, but providers (hospitals and doctors) can refuse to accept a given type of insurance, including Medicare and Medicaid. Low reimbursement rates have generated complaints from providers, and some patients with government insurance have difficulty finding nearby providers for certain types of medical services.\n\nCharity care for those who cannot pay is sometimes available, and is usually funded by non-profit foundations, religious orders, government subsidies, or services donated by the employees. Massachusetts and New Jersey have programs where the state will pay for health care when the patient cannot afford to do so. The City and County of San Francisco is also implementing a citywide health care program for all uninsured residents, limited to those whose incomes and net worth are below an eligibility threshold. Some cities and counties operate or provide subsidies to private facilities open to all regardless of the ability to pay. Means testing is applied, and some patients of limited means may be charged for the services they use.\n\nThe Emergency Medical Treatment and Active Labor Act requires virtually all hospitals to accept all patients, regardless of the ability to pay, for emergency room care. The act does not provide access to non-emergency room care for patients who cannot afford to pay for health care, nor does it provide the benefit of preventive care and the continuity of a primary care physician. Emergency health care is generally more expensive than an urgent care clinic or a doctor's office visit, especially if a condition has worsened due to putting off needed care. Emergency rooms are typically at, near, or over capacity. Long wait times have become a problem nationally, and in urban areas some ERs are put on \"diversion\" on a regular basis, meaning that ambulances are directed to bring patients elsewhere.\n\nMost Americans under age 65 (59.3%) receive their health insurance coverage through an employer (which includes both private as well as civilian public-sector employers) under group coverage, although this percentage is declining. Costs for employer-paid health insurance are rising rapidly: since 2001, premiums for family coverage have increased 78%, while wages have risen 19% and inflation has risen 17%, according to a 2007 study by the Kaiser Family Foundation. Workers with employer-sponsored insurance also contribute; in 2007, the average percentage of premium paid by covered workers is 16% for single coverage and 28% for family coverage. In addition to their premium contributions, most covered workers face additional payments when they use health care services, in the form of deductibles and copayments.\n\nJust less than 9% of the population purchases individual health care insurance. Insurance payments are a form of cost-sharing and risk management where each individual or their employer pays predictable monthly premiums. This cost-spreading mechanism often picks up much of the cost of health care, but individuals must often pay up-front a minimum part of the total cost (a \"deductible\"), or a small part of the cost of every procedure (a copayment). Private insurance accounts for 35% of total health spending in the United States, by far the largest share among OECD countries. Beside the United States, Canada and France are the two other OECD countries where private insurance represents more than 10% of total health spending.\n\nProvider networks can be used to reduce costs by negotiating favorable fees from providers, selecting cost effective providers, and creating financial incentives for providers to practice more efficiently. A survey issued in 2009 by America's Health Insurance Plans found that patients going to out-of-network providers are sometimes charged extremely high fees.\n\nDefying many analysts' expectations, PPOs have gained market share at the expense of HMOs over the past decade.\n\nJust as the more loosely managed PPOs have edged out HMOs, HMOs themselves have also evolved towards less tightly managed models. The first HMOs in the U.S., such as Kaiser Permanente in Oakland, California, and the Health Insurance Plan (HIP) in New York, were \"staff-model\" HMOs, which owned their own health care facilities and employed the doctors and other health care professionals who staffed them. The name health maintenance organization stems from the idea that the HMO would make it its job to maintain the enrollee's health, rather than merely to treat illnesses. In accordance with this mission, managed care organizations typically cover preventive health care. Within the tightly integrated staff-model HMO, the HMO can develop and disseminate guidelines on cost-effective care, while the enrollee's primary care doctor can act as patient advocate and care coordinator, helping the patient negotiate the complex health care system. Despite a substantial body of research demonstrating that many staff-model HMOs deliver high-quality and cost-effective care, they have steadily lost market share. They have been replaced by more loosely managed networks of providers with whom health plans have negotiated discounted fees. It is common today for a physician or hospital to have contracts with a dozen or more health plans, each with different referral networks, contracts with different diagnostic facilities, and different practice guidelines.\n\nGovernment programs directly cover 27.8% of the population (83 million), including the elderly, disabled, children, veterans, and some of the poor, and federal law mandates public access to emergency services regardless of ability to pay. Public spending accounts for between 45% and 56.1% of U.S. health care spending. Per-capita spending on health care by the U.S. government placed it among the top ten highest spenders among United Nations member countries in 2004.\n\nHowever, all government-funded healthcare programs exist only in the form of statutory law, and accordingly can be amended or revoked like any other statute. There is no constitutional right to healthcare. The U.S. Supreme Court explained in 1977 that \"the Constitution imposes no obligation on the States to pay ... any of the medical expenses of indigents.\"\n\nGovernment funded programs include:\n\nThe exemption of employer-sponsored health benefits from federal income and payroll taxes distorts the health care market. The U.S. government, unlike some other countries, does not treat employer funded health care benefits as a taxable benefit in kind to the employee. The value of the lost tax revenue from a benefits in kind tax is an estimated $150 billion a year. Some regard this as being disadvantageous to people who have to buy insurance in the individual market which must be paid from income received after tax.\n\nHealth insurance benefits are an attractive way for employers to increase the salary of employees as they are nontaxable. As a result, 65% of the non-elderly population and over 90% of the privately insured non-elderly population receives health insurance at the workplace. Additionally, most economists agree that this tax shelter increases individual demand for health insurance, leading some to claim that it is largely responsible for the rise in health care spending.\n\nIn addition the government allows full tax shelter at the highest marginal rate to investors in health savings accounts (HSAs). Some have argued that this tax incentive adds little value to national health care as a whole because the most wealthy in society tend also to be the most healthy. Also it has been argued, HSAs segregate the insurance pools into those for the wealthy and those for the less wealthy which thereby makes equivalent insurance cheaper for the rich and more expensive for the poor. However, one advantage of health insurance accounts is that funds can only be used towards certain HSA qualified expenses, including medicine, doctor's fees, and Medicare Parts A and B. Funds cannot be used towards expenses such as cosmetic surgery.\n\nThere are also various state and local programs for the poor. In 2007, Medicaid provided health care coverage for 39.6 million low-income Americans (although Medicaid covers approximately 40% of America's poor), and Medicare provided health care coverage for 41.4 million elderly and disabled Americans. Enrollment in Medicare is expected to reach 77 million by 2031, when the baby boom generation is fully enrolled.\n\nIt has been reported that the number of physicians accepting Medicaid has decreased in recent years due to relatively high administrative costs and low reimbursements. In 1997, the federal government also created the State Children's Health Insurance Program (SCHIP), a joint federal-state program to insure children in families that earn too much to qualify for Medicaid but cannot afford health insurance. SCHIP covered 6.6 million children in 2006, but the program is already facing funding shortfalls in many states. The government has also mandated access to emergency care regardless of insurance status and ability to pay through the Emergency Medical Treatment and Labor Act (EMTALA), passed in 1986, but EMTALA is an unfunded mandate.\n\nThe percentage of persons without health insurance (the \"uninsured\") fell from 13.3% in 2013 to 8.8% in 2016, due primarily to the Affordable Care Act. The number uninsured fell from 41.8 million in 2013 to 28.0 million in 2016, a decline of 13.8 million. The number of persons with insurance (public or private) rose from 271.6 million in 2013 to 292.3 million in 2016, an increase of 20.7 million. In 2016, approximately 68% were covered by private plans, while 37% were covered by government plans; these do not add to 100% because some persons have both.\n\nSome Americans do not qualify for government-provided health insurance, are not provided health insurance by an employer, and are unable to afford, cannot qualify for, or choose not to purchase, private health insurance. When charity or \"uncompensated\" care is not available, they sometimes simply go without needed medical treatment. This problem has become a source of considerable political controversy on a national level. The uninsured still receive emergency care and thus if they are unable to afford it, they impose costs on others who pay higher premiums and deductibles to cover these expenses indirectly. Estimates for 2008 reported that the uninsured would spend $30 billion for healthcare and receive $56 billion in uncompensated care, and that if everyone were covered by insurance then overall costs would increase by $123 billion. A 2003 Institute of Medicine (IOM) report estimated total cost of health care provided to the uninsured at $98.9 billion in 2001, including $26.4 billion in out-of-pocket spending by the uninsured, with $34.5 billion in \"free\" \"uncompensated\" care covered by government subsidies of $30.6 billion to hospitals and clinics and $5.1 billion in donated services by physicians.\n\nA 2003 study in \"Health Affairs\" estimated that uninsured people in the U.S. received approximately $35 billion in uncompensated care in 2001. The study noted that this amount per capita was half what the average insured person received. The study found that various levels of government finance most uncompensated care, spending about $30.6 billion on payments and programs to serve the uninsured and covering as much as 80–85% of uncompensated care costs through grants and other direct payments, tax appropriations, and Medicare and Medicaid payment add-ons. Most of this money comes from the federal government, followed by state and local tax appropriations for hospitals. Another study by the same authors in the same year estimated the additional annual cost of covering the uninsured (in 2001 dollars) at $34 billion (for public coverage) and $69 billion (for private coverage). These estimates represent an increase in total health care spending of 3–6% and would raise health care's share of GDP by less than one percentage point, the study concluded. Another study published in the same journal in 2004 estimated that the value of health forgone each year because of uninsurance was $65–$130 billion and concluded that this figure constituted \"a lower-bound estimate of economic losses resulting from the present level of uninsurance nationally.\"\n\nNumerous publicly funded health care programs help to provide for the elderly, disabled, military service families and veterans, children, and the poor, and federal law ensures public access to emergency services regardless of ability to pay; however, a system of universal health care has not been implemented nationwide. However, as the OECD has pointed out, the total U.S. public expenditure for this limited population would, in most other OECD countries, be enough for the government to provide primary health insurance for the entire population. Although the federal Medicare program and the federal-state Medicaid programs possess some monopsonistic purchasing power, the highly fragmented buy side of the U.S. health system is relatively weak by international standards, and in some areas, some suppliers such as large hospital groups have a virtual monopoly on the supply side. In most OECD countries, there is a high degree of public ownership and public finance. The resulting economy of scale in providing health care services appears to enable a much tighter grip on costs. The U.S., as a matter of oft-stated public policy, largely does not regulate prices of services from private providers, assuming the private sector to do it better.\n\nMassachusetts has adopted a universal health care system through the Massachusetts 2006 Health Reform Statute. It mandates that all residents who can afford to do so purchase health insurance, provides subsidized insurance plans so that nearly everyone can afford health insurance, and provides a \"Health Safety Net Fund\" to pay for necessary treatment for those who cannot find affordable health insurance or are not eligible.\n\nIn July 2009, Connecticut passed into law a plan called SustiNet, with the goal of achieving health care coverage of 98% of its residents by 2014.\n\nPrimary cost reduction opportunities correspond to the causes described above. These include:\n\nIncreased spending on disease prevention is often suggested as a way of reducing health care spending. Whether prevention saves or costs money depends on the intervention. Childhood vaccinations, or contraceptives save much more than they cost. Research suggests that in many cases prevention does not produce significant long-term cost savings. Some interventions may be cost-effective by providing health benefits, while others are not cost-effective. Preventive care is typically provided to many people who would never become ill, and for those who would have become ill is partially offset by the health care costs during additional years of life. On the other hand, research conducted by Novartis argues that the countries that have excelled in getting the highest value for healthcare spending are the ones who have invested more in prevention, early diagnosis and treatment. The trick is to avoid getting patients to hospital, which is where highest healthcare dollars are being consumed. Not all preventive measures have good ROI (EG. Global vaccination campaign for a rare infectious diseases). However, preventive measures such as diet, exercises and reduction of tobacco intake would have broad impact on many diseases and will offer good return of investment.\n\n"}
{"id": "32890301", "url": "https://en.wikipedia.org/wiki?curid=32890301", "title": "Health crisis", "text": "Health crisis\n\nA health crisis or public health crisis is a difficult situation or complex health system that affects humans in one or more geographic areas (mainly occurred in natural hazards), from a particular locality to encompass the entire planet. Health crises generally have significant impacts on community health, loss of life, and on the economy. They may result from disease, industrial processes or poor policy.\n\nIts severity is often measured by the number of people affected by its geographical extent, or the disease or death of the pathogenic process which it originates.\n\nGenerally there are three key components in health crises:\n\n\n\n\n\n\n"}
{"id": "1810614", "url": "https://en.wikipedia.org/wiki?curid=1810614", "title": "Health equity", "text": "Health equity\n\nHealth equity refers to the study and causes of differences in the quality of health and healthcare across different populations. Health equity is different from health equality, as it refers only to the absence of disparities in controllable or remediable aspects of health. It is not possible to work towards complete equality in health, as there are some factors of health that are beyond human influence. Inequity implies some kind of social injustice. Thus, if one population dies younger than another because of genetic differences, a non-remediable/controllable factor, we tend to say that there is a health inequality. On the other hand, if a population has a lower life expectancy due to lack of access to medications, the situation would be classified as a health inequity. These inequities may include differences in the \"presence of disease, health outcomes, or access to health care\" between populations with a different race, ethnicity, sexual orientation or socioeconomic status.\n\nHealth equity falls into two major categories: horizontal equity, the equal treatment of individuals or groups in the same circumstances; and vertical equity, the principle that individuals who are unequal should be treated differently according to their level of need. Disparities in the quality of health across populations are well-documented globally in both developed and developing nations. The importance of equitable access to healthcare has been cited as crucial to achieving many of the Millennium Development Goals.\n\nSocioeconomic status is both a strong predictor of health, and a key factor underlying health inequities across populations. Poor socioeconomic status has the capacity to profoundly limit the capabilities of an individual or population, manifesting itself through deficiencies in both financial and social capital. It is clear how a lack of financial capital can compromise the capacity to maintain good health. In the UK, prior to the institution of the NHS reforms in the early 2000s, it was shown that income was an important determinant of access to healthcare resources. Because one's job or career is a primary conduit for both financial and social capital, work is an important, yet under represented, factor in health inequities research and prevention efforts. Maintenance of good health through the utilization of proper healthcare resources can be quite costly and therefore unaffordable to certain populations.\n\nIn China, for instance, the collapse of the Cooperative Medical System left many of the rural poor uninsured and unable to access the resources necessary to maintain good health. Increases in the cost of medical treatment made healthcare increasingly unaffordable for these populations. This issue was further perpetuated by the rising income inequality in the Chinese population. Poor Chinese were often unable to undergo necessary hospitalization and failed to complete treatment regimens, resulting in poorer health outcomes.\n\nSimilarly, in Tanzania, it was demonstrated that wealthier families were far more likely to bring their children to a healthcare provider: a significant step towards stronger healthcare. Some scholars have noted that unequal income distribution itself can be a cause of poorer health for a society as a result of \"underinvestment in social goods, such as public education and health care; disruption of social cohesion and the erosion of social capital\".\n\nThe role of socioeconomic status in health equity extends beyond simple monetary restrictions on an individual's purchasing power. In fact, social capital plays a significant role in the health of individuals and their communities. It has been shown that those who are better connected to the resources provided by the individuals and communities around them (those with more social capital) live longer lives. The segregation of communities on the basis of income occurs in nations worldwide and has a significant impact on quality of health as a result of a decrease in social capital for those trapped in poor neighborhoods. Social interventions, which seek to improve healthcare by enhancing the social resources of a community, are therefore an effective component of campaigns to improve a community's health. A 1998 epidemiological study\" \"showed that community healthcare approaches fared far better than individual approaches in the prevention of heart disease mortality.\n\nUnconditional cash transfers for reducing poverty used by some programs in the developing world appear to lead to a reduction in the likelihood of being sick.<ref name=\"doi10.1002/14651858.CD011135.pub2\"></ref> Such evidence can guide resource allocations to effective interventions.\n\nEducation is an important factor in healthcare utilization, though it is closely intertwined with economic status. An individual may not go to a medical professional or seek care if they don’t know the ills of their failure to do so, or the value of proper treatment. In Tajikistan, since the nation gained its independence, the likelihood of giving birth at home has increased rapidly among women with lower educational status. Education also has a significant impact on the quality of prenatal and maternal healthcare. Mothers with primary education consulted a doctor during pregnancy at significantly lower rates (72%) when compared to those with a secondary education (77%), technical training (88%) or a higher education (100%). There is also evidence for a correlation between socioeconomic status and health literacy; one study showed that wealthier Tanzanian families were more likely to recognize disease in their children than those that were coming from lower income backgrounds.\n\nFor some populations, access to healthcare and health resources is physically limited, resulting in health inequities. For instance, an individual might be physically incapable of traveling the distances required to reach healthcare services, or long distances can make seeking regular care unappealing despite the potential benefits. Costa Rica, for example, has demonstrable health spatial inequities with 12–14% of the population living in areas where healthcare is inaccessible. Inequity has decreased in some areas of the nation as a result of the work of healthcare reform programs, however those regions not served by the programs have experienced a slight increase in inequity.\n\nChina experienced a serious decrease in spatial health equity following the Chinese economic revolution in the 1980s as a result of the degradation of the Cooperative Medical System (CMS). The CMS provided an infrastructure for the delivery of healthcare to rural locations, as well as a framework to provide funding based upon communal contributions and government subsidies. In its absence, there was a significant decrease in the quantity of healthcare professionals (35.9%), as well as functioning clinics (from 71% to 55% of villages over 14 years) in rural areas, resulting in inequitable healthcare for rural populations. The significant poverty experienced by rural workers (some earning less than 1 USD per day) further limits access to healthcare, and results in malnutrition and poor general hygiene, compounding the loss of healthcare resources. The loss of the CMS has had noticeable impacts on life expectancy, with rural regions such as areas of Western China experiencing significantly lower life expectancies.\n\nSimilarly, populations in rural Tajikistan experience spatial health inequities. A study by Jane Falkingham noted that physical access to healthcare was one of the primary factors influencing quality of maternal healthcare. Further, many women in rural areas of the country did not have adequate access to healthcare resources, resulting in poor maternal and neonatal care. These rural women were, for instance, far more likely to give birth in their homes without medical oversight.\n\nAlong with the socioeconomic factor of health disparities, race is another key factor. The United States historically had large disparities in health and access to adequate healthcare between races, and current evidence supports the notion that these racially centered disparities continue to exist and are a significant social health issue. The disparities in access to adequate healthcare include differences in the quality of care based on race and overall insurance coverage based on race. A 2002 study in the \"Journal of the American Medical Association\" identifies race as a significant determinant in the level of quality of care, with blacks receiving lower quality care than their white counterparts. This is in part because members of ethnic minorities such as African Americans are either earning low incomes, or living below the poverty line. In a 2007 Census Bureau, African American families made an average of $33,916, while their white counterparts made an average of $54,920. Due to a lack of affordable health care, the African American death rate reveals that African Americans have a higher rate of dying from treatable or preventable causes. According to a study conducted in 2005 by the Office of Minority Health—a U.S. Department of Health—African American men were 30% more likely than white men to die from heart disease. Also African American women were 34% more likely to die from breast cancer than their white counterparts.\n\nThere are also considerable racial disparities in access to insurance coverage, with ethnic minorities generally having less insurance coverage than non-ethnic minorities. For example, Hispanic Americans tend to have less insurance coverage than white Americans and as a result receive less regular medical care. The level of insurance coverage is directly correlated with access to healthcare including preventative and ambulatory care. A 2010 study on racial and ethnic disparities in health done by the Institute of Medicine has shown that the aforementioned disparities cannot solely be accounted for in terms of certain demographic characteristics like: insurance status, household income, education, age, geographic location and quality of living conditions. Even when the researchers corrected for these factors, the disparities persist. Slavery has contributed to disparate health outcomes for generations of African Americans in the United States.\n\nEthnic health inequities also appear in nations across the African continent. A survey of the child mortality of major ethnic groups across 11 African nations (Central African Republic, Côte d'Ivoire, Ghana, Kenya, Mali, Namibia, Niger, Rwanda, Senegal, Uganda, and Zambia) was published in 2000 by the WHO. The study described the presence of significant ethnic parities in the child mortality rates among children younger than 5 years old, as well as in education and vaccine use. In South Africa, the legacy of apartheid still manifests itself as a differential access to social services, including healthcare based upon race and social class, and the resultant health inequities. Further, evidence suggests systematic disregard of indigenous populations in a number of countries. The Pygmys of Congo, for instance, are excluded from government health programs, discriminated against during public health campaigns, and receive poorer overall healthcare.\n\nIn a survey of five European countries (Sweden, Switzerland, the UK, Italy, and France), a 1995 survey noted that only Sweden provided access to translators for 100% of those who needed it, while the other countries lacked this service potentially compromising healthcare to non-native populations. Given that non-natives composed a considerable section of these nations (6%, 17%, 3%, 1%, and 6% respectively), this could have significant detrimental effects on the health equity of the nation. In France, an older study noted significant differences in access to healthcare between native French populations, and non-French/migrant populations based upon health expenditure; however this was not fully independent of poorer economic and working conditions experienced by these populations.\n\nA 1996 study of race-based health inequity in Australia revealed that Aborigines experienced higher rates of mortality than non-Aborigine populations. Aborigine populations experienced 10 times greater mortality in the 30–40 age range; 2.5 times greater infant mortality rate, and 3 times greater age standardized mortality rate. Rates of diarrheal diseases and tuberculosis are also significantly greater in this population (16 and 15 times greater respectively), which is indicative of the poor healthcare of this ethnic group. At this point in time, the parities in life expectancy at birth between indigenous and non-indigenous peoples were highest in Australia, when compared to the US, Canada and New Zealand. In South America, indigenous populations faced similarly poor health outcomes with maternal and infant mortality rates that were significantly higher (up to 3 to 4 times greater) than the national average. The same pattern of poor indigenous healthcare continues in India, where indigenous groups were shown to experience greater mortality at most stages of life, even when corrected for environmental effects.\n\nSexuality is a basis of health discrimination and inequity throughout the world. Homosexual, bisexual, transgender, and gender-variant populations around the world experience a range of health problems related to their sexuality and gender identity, some of which are complicated further by limited research.\n\nIn spite of recent advances, LGBT populations in China, India, and Chile continue to face significant discrimination and barriers to care. The World Health Organization (WHO) recognizes that there is inadequate research data about the effects of LGBT discrimination on morbidity and mortality rates in the patient population. In addition, retrospective epidemiological studies on LGBT populations are difficult to conduct as a result of the practice that sexual orientation is not noted on death certificates. WHO has proposed that more research about the LGBT patient population is needed  for improved understanding of its  unique health needs and barriers to accessing care.\n\nRecognizing the need for LGBT healthcare research, the Director of the National Institute on Minority Health and Health Disparities (NIMHD) at the U.S. Department of Health and Human Services designated sexual and gender minorities (SGMs) as a health disparity population for NIH research in October 2016. For the purposes of this designation, the Director defines SGM as \"encompass[ing] lesbian, gay, bisexual, and transgender populations, as well as those whose sexual orientation, gender identity and expressions, or reproductive development varies from traditional, societal, cultural, or physiological norms\". This designation has prioritized research into the extent, cause, and potential mitigation of health disparities among SGM populations within the larger LGBT community.\n\nWhile many aspects of LGBT health disparities are heretofore uninvestigated, at this stage, it is known that one of the main forms of healthcare discrimination  LGBT individuals face is discrimination from healthcare workers or institutions themselves. A systematic literature review of publications in English and Portuguese from 2004–2014 demonstrate significant difficulties in accessing care secondary to discrimination and homophobia from healthcare professionals. This discrimination can take the form of verbal abuse, disrespectful conduct, refusal of care, the withholding of health information,  inadequate treatment, and outright violence. In a study analyzing the quality of healthcare for South African men who have sex with men (MSM), researchers interviewed a cohort of individuals about their health experiences, finding that MSM who identified as homosexual felt their access to healthcare was limited due to an inability to find clinics employing healthcare workers who did not discriminate against their sexuality. They also reportedly faced \"homophobic verbal harassment from healthcare workers when presenting for STI treatment\". Further, MSM who did not feel comfortable disclosing their sexual activity to healthcare workers failed to identify as homosexuals, which limited the quality of the treatment they received.\n\nAdditionally, members of the LGBT community contend with health care disparities due, in part, to lack of provider training and awareness of the population’s healthcare needs. Studies regarding patient-provider communication in the LGBT patient community show that providers themselves report a significant lack of awareness regarding the health issues LGBT-identifying patients face. As a component of this fact, medical schools do not focus much attention on LGBT health issues in their curriculum; the LGBT-related topics that are discussed tend to be limited to HIV/AIDS, sexual orientation, and gender identity.\n\nAmong LGBT-identifying individuals, transgender individuals face especially significant barriers to treatment. Many countries still do not have legal recognition of transgender or non-binary gender individuals leading to placement in mis-gendered hospital wards and medical discrimination. Seventeen European states mandate sterilization of individuals who seek recognition of a gender identity that diverges from their birth gender. In addition to many of the same barriers as the rest of the LGBT community, a WHO bulletin points out that globally, transgender individuals often also face a higher disease burden. A 2010 survey of transgender and gender-variant people in the United States revealed that transgender individuals faced a significant level of discrimination. The survey indicated that 19% of individuals experienced a healthcare worker refusing care because of their gender, 28% faced harassment from a healthcare worker, 2% encountered violence, and 50% saw a doctor who was not able or qualified to provide transgender-sensitive care. In Kuwait, there have been reports of transgender individuals being reported to legal authorities by medical professionals, preventing safe access to care. An updated version of the U.S. survey from 2015 showed little change in terms of healthcare experiences for transgender and gender variant individuals. The updated survey revealed that 23% of individuals reported not seeking necessary medical care out of fear of discrimination, and 33% of individuals who had been to a doctor within a year of taking the survey reported negative encounters with medical professionals related to their transgender status.\n\nThe stigmatization represented particularly in the transgender population  creates a health disparity for LGBT individuals with regard to mental health. The LGBT community is at increased risk for psychosocial distress, mental health complications, suicidality, homelessness, and substance abuse, often complicated by access-based under-utilization or fear of health services. Transgender and gender-variant individuals have been found to experience higher rates of mental health disparity than LGB individuals. According to the 2015 U.S. Transgender Survey, for example, 39% of respondents reported serious psychological distress, compared to 5% of the general population.\n\nThese mental health facts are informed by a history of anti-LGBT bias in health care. The Diagnostic and Statistical Manual of Mental Disorders (DSM) listed homosexuality as a disorder until 1973; transgender status was listed as a disorder until 2012. This was amended in 2013 with the DSM-5 when \"gender identity disorder\" was replaced with \"gender dysphoria\", reflecting that simply identifying as transgender is not itself pathological and that the diagnosis is instead for the distress a transgender person may experience as a result of the discordance between assigned gender and gender identity.\n\nLGBT health issues have received disproportionately low levels of medical research, leading to difficulties in assessing appropriate strategies for LGBT treatment. For instance, a review of medical literature regarding LGBT patients revealed that there are significant gaps in the medical understanding of cervical cancer in lesbian and bisexual individuals it is unclear whether its prevalence in this community is a result of probability or some other preventable cause. For example, LGBT people report poorer cancer care experiences. It is incorrectly assumed that LGBT women have a lower incidence of cervical cancer than their heterosexual counterparts, resulting in lower rates of screening.  Such findings illustrate the need for continued research focused on the circumstances and needs of LGBT individuals and the inclusion in policy frameworks of sexual orientation and gender identity as social determinants of health.\n\nA June 2017 review sponsored by the European commission as part of a larger project to identify and diminish health inequities, found that LGB are at higher risk of some cancers and that LGBTI were at higher risk of mental illness, and that these risks were not adequately addressed. The causes of health inequities were, according to the review, \"i) cultural and social norms that preference and prioritise heterosexuality; ii) minority stress associated with sexual orientation, gender identity and sex characteristics; iii) victimisation; iv) discrimination (individual and institutional), and; v) stigma.\"\n\nBoth gender and sex are significant factors that influence health. Sex is characterized by female and male biological differences in regards to gene expression, hormonal concentration, and anatomical characteristics. Gender is an expression of behavior and lifestyle choices. Both sex and gender inform each other, and it is important to note that differences between the two genders influence disease manifestation and associated healthcare approaches. Understanding how the interaction of sex and gender contributes to disparity in the context of health allows providers to ensure quality outcomes for patients. This interaction is complicated by the difficulty of distinguishing between sex and gender given their intertwined nature; sex modifies gender, and gender can modify sex, thereby impacting health.  Sex and gender can both be considered sources of health disparity; both contribute to men and women’s susceptibility to various health conditions, including cardiovascular disease and autoimmune disorders.\n\nAs sex and gender are inextricably linked in day-to-day life, their union is apparent in medicine. Gender and sex are both components of health disparity in the male population. In non-Western regions, males tend to have a health advantage over women due to gender discrimination, evidenced by infanticide, early marriage, and domestic abuse for females. In most regions of the world, the mortality rate is higher for adult men than for adult women; for example, adult men suffer from fatal illnesses with more frequency than females. The leading causes of the higher male death rate are accidents, injuries, violence, and cardiovascular diseases. In a number of countries, males also face a heightened risk of mortality as a result of behavior and greater propensity for violence.\n\nPhysicians tend to offer invasive procedures to male patients more than female patients. Furthermore, men are more likely to smoke than women and experience smoking-related health complications later in life as a result; this trend is also observed in regard to other substances, such as marijuana, in Jamaica, where the rate of use is 2–3 times more for men than women. Lastly, men are more likely to have severe chronic conditions and a lower life expectancy than women in the United States.\n\nGender and sex are also components of health disparity in the female population. The 2012 World Development Report (WDR) noted that women in developing nations experience greater mortality rates than men in developing nations. Additionally, women in developing countries have a much higher risk of maternal death than those in developed countries. The highest risk of dying during childbirth is 1 in 6 in Afghanistan and Sierra Leone, compared to nearly 1 in 30,000 in Sweden—a disparity that is much greater than that for neonatal or child mortality.\n\nWhile women in the United States tend to live longer than men, they generally are of lower socioeconomic status (SES) and therefore have more barriers to accessing healthcare. Being of lower SES also tends to increase societal pressures, which can lead to higher rates of depression and chronic stress and, in turn, negatively impact health. Women are also more likely than men to suffer from sexual or intimate-partner violence both in the United States and worldwide. In Europe, women who grew up in poverty are more likely to have lower muscle strength and higher disability in old age.\n\nWomen have better access to healthcare in the United States than they do in many other places in the world. In one population study conducted in Harlem, New York, 86% of women reported having privatized or publicly assisted health insurance, while only 74% of men reported having any health insurance. This trend is representative of the general population of the United States.\n\nIn addition, women's pain tends to be treated less seriously and initially ignored by clinicians when compared to their treatment of men's pain complaints. Historically, women have not been included in the design or practice of clinical trials, which has slowed the understanding of women's reactions to medications and created a research gap. This has led to post-approval adverse events among women, resulting in several drugs being pulled from the market. However, the clinical research industry is aware of the problem, and has made progress in correcting it.\n\nHealth disparities are also due in part to cultural factors that involve practices based not only on sex, but also gender status. For example, in China, health disparities have distinguished medical treatment for men and women due to the cultural phenomenon of preference for male children. Recently, gender-based disparities have decreased as females have begun to receive higher-quality care. Additionally, a girl’s chances of survival are impacted by the presence of a male sibling; while girls do have the same chance of survival as boys if they are the oldest girl, they have a higher probability of being aborted or dying young if they have an older sister.\n\nIn India, gender-based health inequities are apparent in early childhood. Many families provide better nutrition for boys in the interest of maximizing future productivity given that boys are generally seen as breadwinners. In addition, boys receive better care than girls and are hospitalized at a greater rate. The magnitude of these disparities increases with the severity of poverty in a given population.\n\nAdditionally, the cultural practice of female genital mutilation (FGM) is known to impact women's health, though is difficult to know the worldwide extent of this practice. While generally thought of as a Sub-Saharan African practice, it may have roots in the Middle East as well. The estimated 3 million girls who are subjected to FGM each year potentially suffer both immediate and lifelong negative effects. Immediately following FGM, girls commonly experience excessive bleeding and urine retention. Long-term consequences include urinary tract infections, bacterial vaginosis, pain during intercourse, and difficulties in childbirth that include prolonged labor, vaginal tears, and excessive bleeding. Women who have undergone FGM also have higher rates of post-traumatic stress disorder (PTSD) and herpes simplex virus 2 (HSV2) than women who have not.\n\nMinority populations have increased exposure to environmental hazards that include lack of neighborhood resources, structural and community factors as well as residential segregation that result in a cycle of disease and stress. The environment that surrounds us can influence individual behaviors and lead to poor health choices and therefore outcomes. Minority neighborhoods have been continuously noted to have more fast food chains and fewer grocery stores than predominantly white neighborhoods. These food deserts affect a family’s ability to have easy access to nutritious food for their children. This lack of nutritious food extends beyond the household into the schools that have a variety of vending machines and deliver over processed foods. These environmental condition have social ramifications and in the first time in US history is it projected that the current generation will live shorter lives than their predecessors will.\n\nIn addition, minority neighborhoods have various health hazards that result from living close to highways and toxic waste factories or general dilapidated structures and streets. These environmental conditions create varying degrees of health risk from noise pollution, to carcinogenic toxic exposures from asbestos and radon that result in increase chronic disease, morbidity, and mortality. The quality of residential environment such as damaged housing has been shown to increase the risk of adverse birth outcomes, which is reflective of a communities health. Housing conditions can create varying degrees of health risk that lead to complications of birth and long-term consequences in the aging population. In addition, occupational hazards can add to the detrimental effects of poor housing conditions. It has been reported that a greater number of minorities work in jobs that have higher rates of exposure to toxic chemical, dust and fumes.\n\nRacial segregation is another environmental factor that occurs through the discriminatory action of those organizations and working individuals within the real estate industry, whether in the housing markets or rentals. Even though residential segregation is noted in all minority groups, blacks tend to be segregated regardless of income level when compared to Latinos and Asians. Thus, segregation results in minorities clustering in poor neighborhoods that have limited employment, medical care, and educational resources, which is associated with high rates of criminal behavior. In addition, segregation affects the health of individual residents because the environment is not conducive to physical exercise due to unsafe neighborhoods that lack recreational facilities and have nonexistent park space. Racial and ethnic discrimination adds an additional element to the environment that individuals have to interact with daily. Individuals that reported discrimination have been shown to have an increase risk of hypertension in addition to other physiological stress related affects. The high magnitude of environmental, structural, socioeconomic stressors leads to further compromise on the psychological and physical being, which leads to poor health and disease.\n\nIndividuals living in rural areas, especially poor rural areas, have access to fewer health care resources. Although 20 percent of the U.S. population lives in rural areas, only 9 percent of physicians practice in rural settings. Individuals in rural areas typically must travel longer distances for care, experience long waiting times at clinics, or are unable to obtain the necessary health care they need in a timely manner. Rural areas characterized by a largely Hispanic population average 5.3 physicians per 10,000 residents compared with 8.7 physicians per 10,000 residents in nonrural areas. Financial barriers to access, including lack of health insurance, are also common among the urban poor.\n\nReasons for disparities in access to health care are many, but can include the following:\n\n\nHealth disparities in the quality of care exist and are based on language and ethnicity/race which includes:\n\nCommunication is critical for the delivery of appropriate and effective treatment and care, regardless of a patient’s race, and miscommunication can lead to incorrect diagnosis, improper use of medications, and failure to receive follow-up care. The patient provider relationship is dependent on the ability of both individuals to effectively communicate. Language and culture both play a significant role in communication during a medical visit. Among the patient population, minorities face greater difficulty in communicating with their physicians. Patients when surveyed responded that 19% of the time they have problems communicating with their providers which included understanding doctor, feeling doctor listened, and had questions but did not ask. In contrast, the Hispanic population had the largest problem communicating with their provider, 33% of the time. Communication has been linked to health outcomes, as communication improves so does patient satisfaction which leads to improved compliance and then to improved health outcomes. Quality of care is impacted as a result of an inability to communicate with health care providers. Language plays a pivotal role in communication and efforts need to be taken to ensure excellent communication between patient and provider. Among limited English proficient patients in the United States, the linguistic barrier is even greater. Less than half of non-English speakers who say they need an interpreter during clinical visits report having one. The absence of interpreters during a clinical visit adds to the communication barrier. Furthermore, inability of providers to communicate with limited English proficient patients leads to more diagnostic procedures, more invasive procedures, and over prescribing of medications. Poor communication contributes to poor medical compliance and health outcomes. Many health-related settings provide interpreter services for their limited English proficient patients. This has been helpful when providers do not speak the same language as the patient. However, there is mounting evidence that patients need to communicate with a language concordant physician (not simply an interpreter) to receive the best medical care, bond with the physician, and be satisfied with the care experience. Having patient-physician language discordant pairs (i.e. Spanish-speaking patient with an English-speaking physician) may also lead to greater medical expenditures and thus higher costs to the organization. Additional communication problems result from a decrease or lack of cultural competence by providers. It is important for providers to be cognizant of patients’ health beliefs and practices without being judgmental or reacting. Understanding a patients’ view of health and disease is important for diagnosis and treatment. So providers need to assess patients’ health beliefs and practices to improve quality of care. Patient health decisions can be influenced by religious beliefs, mistrust of Western medicine, and familial and hierarchical roles, all of which a white provider may not be familiar with. Other type of communication problems are seen in LGBT health care with the spoken heterosexist (conscious or unconscious) attitude on LGBT patients, lack of understanding on issues like having no sex with men (lesbians, gynecologic examinations) and other issues.\n\nProvider discrimination occurs when health care providers either unconsciously or consciously treat certain racial and ethnic patients differently from other patients. This may be due to stereotypes that providers may have towards ethnic/racial groups. Doctors are more likely to ascribe negative racial stereotypes to their minority patients. This may occur regardless of consideration for education, income, and personality characteristics. Two types of stereotypes may be involved, automatic stereotypes or goal modified stereotypes. Automated stereotyping is when stereotypes are automatically activated and influence judgments/behaviors outside of consciousness. Goal modified stereotype is a more conscious process, done when specific needs of clinician arise (time constraints, filling in gaps in information needed) to make a complex decisions. Physicians are unaware of their implicit biases. Some research suggests that ethnic minorities are less likely than whites to receive a kidney transplant once on dialysis or to receive pain medication for bone fractures. Critics question this research and say further studies are needed to determine how doctors and patients make their treatment decisions. Others argue that certain diseases cluster by ethnicity and that clinical decision making does not always reflect these differences.\n\nAccording to the 2009 National Healthcare Disparities Report, uninsured Americans are less likely to receive preventive services in health care. For example, minorities are not regularly screened for colon cancer and the death rate for colon cancer has increased among African Americans and Hispanic populations. Furthermore, limited English proficient patients are also less likely to receive preventive health services such as mammograms. Studies have shown that use of professional interpreters have significantly reduced disparities in the rates of fecal occult testing, flu immunizations and pap smears. In the UK, Public Health England, a universal service free at the point of use, which forms part of the NHS, offers regular screening to any member of the population considered to be in an at-risk group (such as individuals over 45) for major disease (such as colon cancer, or diabetic-retinopathy).\n\nThere are a multitude of strategies for achieving health equity and reducing disparities outlined in scholarly texts, some examples include:\n\n\nHealth inequality is the term used in a number of countries to refer to those instances whereby the health of two demographic groups (not necessarily ethnic or racial groups) differs despite comparative access to health care services. Such examples include higher rates of morbidity and mortality for those in lower occupational classes than those in higher occupational classes, and the increased likelihood of those from ethnic minorities being diagnosed with a mental health disorder. In Canada, the issue was brought to public attention by the LaLonde report.\n\nIn UK, the Black Report was produced in 1980 to highlight inequalities. On 11 February 2010, Sir Michael Marmot, an epidemiologist at University College London, published the \"Fair Society, Healthy Lives\" report on the relationship between health and poverty. Marmot described his findings as illustrating a \"social gradient in health\": the life expectancy for the poorest is seven years shorter than for the most wealthy, and the poor are more likely to have a disability. In its report on this study, \"The Economist\" argued that the material causes of this contextual health inequality include unhealthful lifestyles - smoking remains more common, and obesity is increasing fastest, amongst the poor in Britain.\n\nIn June 2018, the European Commission launched the Joint Action Health Equity in Europe. Forty-nine participants from 25 European Union Member States will work together to address health inequalities and the underlying social determinants of health across Europe. Under the coordination of the Italian Institute of Public Health, the Joint Action aims to achieve greater equity in health in Europe across all social groups while reducing the inter-country heterogeneity in tackling health inequalities.\n\nPoor health outcomes appear to be an effect of economic inequality across a population. Nations and regions with greater economic inequality show poorer outcomes in life expectancy, mental health, drug abuse, obesity, educational performance, teenage birthrates, and ill health due to violence. On an international level, there is a positive correlation between developed countries with high economic equality and longevity. This is unrelated to average income per capita in wealthy nations. Economic gain only impacts life expectancy to a great degree in countries in which the mean per capita annual income is less than approximately $25,000.\nThe United States shows exceptionally low health outcomes for a developed country, despite having the highest national healthcare expenditure in the world. The US ranks 31st in life expectancy. Americans have a lower life expectancy than their European counterparts, even when factors such as race, income, diet, smoking, and education are controlled for.\n\nRelative inequality negatively affects health on an international, national, and institutional levels. The patterns seen internationally hold true between more and less economically equal states in the United States. The patterns seen internationally hold true between more and less economically equal states in the United States, that is, more equal states show more desirable health outcomes. Importantly, inequality can have a negative health impact on members of lower echelons of institutions. The Whitehall I and II studies looked at the rates of cardiovascular disease and other health risks in British civil servants and found that, even when lifestyle factors were controlled for, members of lower status in the institution showed increased mortality and morbidity on a sliding downward scale from their higher status counterparts.\nThe negative aspects of inequality are spread across the population. For example, when comparing the United States (a more unequal nation) to England (a less unequal nation), the US shows higher rates of diabetes, hypertension, cancer, lung disease, and heart disease across all income levels. This is also true of the difference between mortality across all occupational classes in highly equal Sweden as compared to less-equal England.\n\n\n\n\n[[Category:Determinants of health]]\n[[Category:Healthcare quality]]\n[[Category:Health economics]]\n[[Category:Social inequality]]\n[[Category:Social problems in medicine]]\n[[Category:Public health]]"}
{"id": "1465997", "url": "https://en.wikipedia.org/wiki?curid=1465997", "title": "Health psychology", "text": "Health psychology\n\nHealth psychology is the study of psychological and behavioral processes in health, illness, and healthcare. It is concerned with understanding how psychological, behavioral, and cultural factors contribute to physical health and illness. Psychological factors can affect health directly. For example, chronically occurring environmental stressors affecting the hypothalamic–pituitary–adrenal axis, cumulatively, can harm health. Behavioral factors can also affect a person's health. For example, certain behaviors can, over time, harm (smoking or consuming excessive amounts of alcohol) or enhance health (engaging in exercise). Health psychologists take a biopsychosocial approach. In other words, health psychologists understand health to be the product not only of biological processes (e.g., a virus, tumor, etc.) but also of psychological (e.g., thoughts and beliefs), behavioral (e.g., habits), and social processes (e.g., socioeconomic status and ethnicity).\n\nBy understanding psychological factors that influence health, and constructively applying that knowledge, health psychologists can improve health by working directly with individual patients or indirectly in large-scale public health programs. In addition, health psychologists can help train other healthcare professionals (e.g., physicians and nurses) to take advantage of the knowledge the discipline has generated, when treating patients. Health psychologists work in a variety of settings: alongside other medical professionals in hospitals and clinics, in public health departments working on large-scale behavior change and health promotion programs, and in universities and medical schools where they teach and conduct research.\n\nAlthough its early beginnings can be traced to the field of clinical psychology, four different divisions within health psychology and one related field, occupational health psychology (OHP), have developed over time. The four divisions include clinical health psychology, public health psychology, community health psychology, and critical health psychology. Professional organizations for the field of health psychology include Division 38 of the American Psychological Association (APA), the Division of Health Psychology of the British Psychological Society (BPS), and the European Health Psychology Society. Advanced credentialing in the US as a clinical health psychologist is provided through the American Board of Professional Psychology.\n\nRecent advances in psychological, medical, and physiological research have led to a new way of thinking about health and illness. This conceptualization, which has been labeled the biopsychosocial model, views health and illness as the product of a combination of factors including biological characteristics (e.g., genetic predisposition), behavioral factors (e.g., lifestyle, stress, health beliefs), and social conditions (e.g., cultural influences, family relationships, social support).\n\nPsychologists who strive to understand how biological, behavioral, and social factors influence health and illness are called health psychologists. Health psychologists use their knowledge of psychology and health to promote general well-being and understand physical illness. They are specially trained to help people deal with the psychological and emotional aspects of health and illness. Health psychologists work with many different health care professionals (e.g., physicians, dentists, nurses, physician's assistants, dietitians, social workers, pharmacists, physical and occupational therapists, and chaplains) to conduct research and provide clinical assessments and treatment services. Many health psychologists focus on prevention research and interventions designed to promote healthier lifestyles and try to find ways to encourage people to improve their health. For example, they may help people to lose weight or stop smoking. Health psychologists also use their skills to try to improve the healthcare system. For example, they may advise doctors about better ways to communicate with their patients. \nHealth psychologists work in many different settings including the UK's National Health Service (NHS), private practice, universities, communities, schools and organizations. While many health psychologists provide clinical services as part of their duties, others function in non-clinical roles, primarily involving teaching and research. Leading journals include \"Health Psychology\", the \"Journal of Health Psychology\", the \"British Journal of Health Psychology\", and \"Applied Psychology: Health and Well-Being\". Health psychologists can work with people on a one-to-one basis, in groups, as a family, or at a larger population level.\n\n\n\n\n\nHealth psychology, like other areas of applied psychology, is both a theoretical and applied field. Health psychologists employ diverse research methods. These methods include controlled randomized experiments, quasi-experiments, longitudinal studies, time-series designs, cross-sectional studies, case-control studies, qualitative research as well as action research. Health psychologists study a broad range of variables including cardiovascular disease, (cardiac psychology), smoking habits, the relation of religious beliefs to health, alcohol use, social support, living conditions, emotional state, social class, and more. Some health psychologists treat individuals with sleep problems, headaches, alcohol problems, etc. Other health psychologists work to empower community members by helping community members gain control over their health and improve quality of life of entire communities.\n\nPsychological factors in health had been studied since the early 20th century by disciplines such as psychosomatic medicine and later behavioral medicine, but these were primarily branches of medicine, not psychology. Health psychology began to emerge as a distinct discipline of psychology in the United States in the 1970s. In the mid-20th century there was a growing understanding in medicine of the effect of behavior on health. For example, the Alameda County Study, which began in the 1960s, showed that people who ate regular meals (e.g., breakfast), maintained a healthy weight, received adequate sleep, did not smoke, drank little alcohol, and exercised regularly were in better health and lived longer. In addition, psychologists and other scientists were discovering relationships between psychological processes and physiological ones. These discoveries include a better understanding of the impact of psychosocial stress on the cardiovascular and immune systems, and the early finding that the functioning of the immune system could be altered by learning.\n\nPsychologists have been working in medical settings for many years (in the UK sometimes the field was termed medical psychology). Medical psychology, however, was a relatively small field, primarily aimed at helping patients adjust to illness. In 1969, William Schofield prepared a report for the APA entitled \"The Role of Psychology in the Delivery of Health Services\". While there were exceptions, he found that the psychological research of the time frequently regarded mental health and physical health as separate, and devoted very little attention to psychology's impact upon physical health. One of the few psychologists working in this area at the time, Schofield proposed new forms of education and training for future psychologists. The APA, responding to his proposal, in 1973 established a task force to consider how psychologists could (a) help people to manage their health-related behaviors, (b) help patients manage their physical health problems, and (c) train healthcare staff to work more effectively with patients.\n\nLed by Joseph Matarazzo, in 1977, APA added a division devoted to health psychology. At the first divisional conference, Matarazzo delivered a speech that played an important role in defining health psychology. He defined the new field in this way, \"Health psychology is the aggregate of the specific educational, scientific and professional contributions of the discipline of psychology to the promotion and maintenance of health, the prevention and treatment of illness, the identification of diagnostic and etiologic correlates of health, illness and related dysfunction, and the analysis and improvement of the healthcare system and health policy formation.\" In the 1980s, similar organizations were established elsewhere. In 1986, the BPS established a Division of Health Psychology. The European Health Psychology Society was also established in 1986. Similar organizations were established in other countries, including Australia and Japan. Universities began to develop doctoral level training programs in health psychology. In the US, post-doctoral level health psychology training programs were established for individuals who completed a doctoral degree in clinical psychology.\n\nA number of relevant trends coincided with the emergence of health psychology, including:\n\nIn the UK, the BPS’s reconsideration of the role of the Medical Section prompted the emergence of health psychology as a distinct field. Marie Johnston and John Weinman argued in a letter to the \"BPS Bulletin\" that there was a great need for a Health Psychology Section. In December 1986 the section was established at the BPS London Conference, with Marie Johnston as chair. At the Annual BPS Conference in 1993 a review of \"Current Trends in Health Psychology\" was organized, and a definition of health psychology as \"the study of psychological and behavioural processes in health, illness and healthcare\" was proposed.\nThe Health Psychology Section became a Special Group in 1993 and was awarded divisional status within the UK in 1997. The awarding of divisional status meant that the individual training needs and professional practice of health psychologists were recognized, and members were able to obtain chartered status with the BPS. The BPS went on to regulate training and practice in health psychology until the regulation of professional standards and qualifications was taken over by statutory registration with the Health Professions Council in 2010.\n\nHealth psychologists conduct research to identify behaviors and experiences that promote health, give rise to illness, and influence the effectiveness of health care. They also recommend ways to improve health care policy. Health psychologists have worked on developing ways to reduce smoking and improve daily nutrition in order to promote health and prevent illness. They have also studied the association between illness and individual characteristics. For example, health psychology has found a relation between the personality characteristics of thrill seeking, impulsiveness, hostility/anger, emotional instability, and depression, on one hand, and high-risk driving, on the other.\n\nHealth psychology is also concerned with contextual factors, including economic, cultural, community, social, and lifestyle factors that influence health. The biopsychosocial model can help in understanding the relation between contextual factors and biology in affecting health. Physical addiction impedes smoking cessation. Some research suggests that seductive advertising also contributes to psychological dependency on tobacco, although other research has found no relationship between media exposure and smoking in youth. OHP research indicates that people in jobs that combine little decision latitude with a high psychological workload are at increased risk for cardiovascular disease. Other OHP research reveals a relation between unemployment and elevations in blood pressure. Epidemiologic research documents a relation between social class and cardiovascular disease.\n\nHealth psychologists also aim to change health behaviors for the dual purpose of helping people stay healthy and helping patients adhere to disease treatment regimens (also see health action process approach). Health psychologists employ cognitive behavior therapy and applied behavior analysis (also see behavior modification) for that purpose.\n\nHealth psychologists promote health through behavioral change, as mentioned above; however, they attempt to prevent illness in other ways as well. Health psychologists try to help people to lead a healthy life by developing and running programmes which can help people to make changes in their lives such as stopping smoking, reducing the amount of alcohol they consume, eating more healthily, and exercising regularly. Campaigns informed by health psychology have targeted tobacco use. Those least able to afford tobacco products consume them most. Tobacco provides individuals with a way of controlling aversive emotional states accompanying daily experiences of stress that characterize the lives of deprived and vulnerable individuals. Practitioners emphasize education and effective communication as a part of illness prevention because many people do not recognize, or minimize, the risk of illness present in their lives. Moreover, many individuals are often unable to apply their knowledge of health practices owing to everyday pressures and stresses. A common example of population-based attempts to motivate the smoking public to reduce its dependence on cigarettes is anti-smoking campaigns.\n\nHealth psychologists help to promote health and well-being by preventing illness. Some illnesses can be more effectively treated if caught early. Health psychologists have worked to understand why some people do not seek early screenings or immunizations, and have used that knowledge to develop ways to encourage people to have early health checks for illnesses such as cancer and heart disease. Health psychologists are also finding ways to help people to avoid risky behaviors (e.g., engaging in unprotected sex) and encourage health-enhancing behaviors (e.g., regular tooth brushing or hand washing).\n\nHealth psychologists also aim at educating health professionals, including physicians and nurses, in communicating effectively with patients in ways that overcome barriers to understanding, remembering, and implementing effective strategies for reducing exposures to risk factors and making health-enhancing behavior changes.\n\nThere is also evidence from OHP that stress-reduction interventions at the workplace can be effective. For example, Kompier and his colleagues have shown that a number of interventions aimed at reducing stress in bus drivers has had beneficial effects for employees and bus companies.\n\nHealth psychologists investigate how disease affects individuals' psychological well-being. An individual who becomes seriously ill or injured faces many different practical stressors. These stressors include problems meeting medical and other bills, problems obtaining proper care when home from the hospital, obstacles to caring for dependents, the experience of having one's sense of self-reliance compromised, gaining a new, unwanted identity as that of a sick person, and so on. These stressors can lead to depression, reduced self-esteem, etc.\n\nHealth psychology also concerns itself with bettering the lives of individuals with terminal illness. When there is little hope of recovery, health psychologist therapists can improve the quality of life of the patient by helping the patient recover at least some of his or her psychological well-being. Health psychologists are also concerned with providing therapeutic services for the bereaved.\n\nCritical health psychologists explore how health policy can influence inequities, inequalities and social injustice . These avenues of research expand the scope of health psychology beyond the level of individual health to an examination of the social and economic determinants of health both within and between regions and nations. The individualism of mainstream health psychology has been critiqued and deconstructed by critical health psychologists using qualitative methods that zero in on the health experience.\n\nLike psychologists in the other main psychology disciplines, health psychologists have advanced knowledge of research methods. Health psychologists apply this knowledge to conduct research on a variety of questions. For example, health psychologists carry out research to answer questions such as: \n\nHealth psychologists can also be responsible for training other health professionals on how to deliver interventions to help promote healthy eating, stopping smoking, weight loss, etc. Health psychologists also train other health professionals in communication skills such as how to break bad news or support behavior change for the purpose of improving adherence to treatment.\n\nHealth psychologists aid the process of communication between physicians and patients during medical consultations. There are many problems in this process, with patients showing a considerable lack of understanding of many medical terms, particularly anatomical terms (e.g., intestines). One area of research on this topic involves \"doctor-centered\" or \"patient-centered\" consultations. Doctor-centered consultations are generally directive, with the patient answering questions and playing less of a role in decision-making. Although this style is preferred by elderly people and others, many people dislike the sense of hierarchy or ignorance that it inspires. They prefer patient-centered consultations, which focus on the patient's needs, involve the doctor listening to the patient completely before making a decision, and involving the patient in the process of choosing treatment and finding a diagnosis.\n\nHealth psychologists engage in research and practice aimed at getting people to follow medical advice and adhere to their treatment regimens. Patients often forget to take their pills or consciously opt not to take their prescribed medications because of side effects. Failing to take prescribed medication is costly and wastes millions of usable medicines that could otherwise help other people. Estimated adherence rates are difficult to measure (see below); there is, however, evidence that adherence could be improved by tailoring treatment programs to individuals' daily lives. Additionally, traditional cognitive-behavioural therapies have been adapted for people suffering from chronic illnesses and comorbid psychological distress to include modules that encourage, support and reinforce adherence to medical advice as part of the larger treatment approach.\n\nHealth psychologists have identified a number of ways of measuring patients' adherence to medical regimens:\n\nHealth psychology attempts to find treatments to reduce or eliminate pain, as well as understand pain anomalies such as episodic analgesia, causalgia, neuralgia, and phantom limb pain. Although the task of measuring and describing pain has been problematic, the development of the McGill Pain Questionnaire has helped make progress in this area. Treatments for pain involve patient-administered analgesia, acupuncture (found to be effective in reducing pain for osteoarthritis of the knee), biofeedback, and cognitive behavior therapy.\n\nBelow are some examples of the types of positions held by health psychologists within applied settings such as the UK's NHS and private practice.\n\nIn the UK, health psychologists are registered by the Health Professions Council (HPC) and have trained to a level to be eligible for full membership of the Division of Health Psychology within the BPS. Registered health psychologists who are chartered with the BPS will have undertaken a minimum of six years of training and will have specialized in health psychology for a minimum of three years. Health psychologists in training must have completed BPS stage 1 training and be registered with the BPS Stage 2 training route or with a BPS-accredited university doctoral health psychology program. Once qualified, health psychologists can work in a range of settings, for example the NHS, universities, schools, private healthcare, and research and charitable organizations. A health psychologist in training might be working within applied settings while working towards registration and chartered status. A health psychologist will have demonstrated competencies in all of the following areas:\nAll qualified health psychologists must also engage in and record their continuing professional development (CPD) for psychology each year throughout their career.\n\n\n\n"}
{"id": "40634960", "url": "https://en.wikipedia.org/wiki?curid=40634960", "title": "Hermaphrodites with Attitude", "text": "Hermaphrodites with Attitude\n\nHermaphrodites with Attitude was a newsletter edited by Cheryl Chase and published by the Intersex Society of North America (ISNA) between 1994 and 2005. The full archives are available online. In 2008, ISNA transferred its remaining funds, assets, and copyrights to Accord Alliance and then closed.\n\n\"Hermaphrodites with Attitude\" was published on thirteen occasions over an eleven-year period. The first issue appeared in Winter 1994, comprising 6 pages of articles, analysis and case studies, including articles by people with lived experience, activists, physicianss, and academics. It was distributed to subscribers in five countries and 14 States of the United States.\n\nThe newsletter provided a voice for intersex activists for the first time, becoming a resource for intersex people and academics. The title of the newsletter appears in the title of multiple articles describing the intersex movement, and was also displayed on banners at the first public demonstration by intersex people and allies, outside a pediatric conference in Boston, on October 26, 1996.\n\nIn the early part of the 21st-century, Intersex Society of North America (ISNA) took on staff for the first time and began to engage closely with the Lawson Wilkins Pediatric Endocrine Society, establishing a North American Task Force on Intersex. These developments were stated in the newsletter's first issue of the 21st-century, in February 2001, which also marked a change in name to \"ISNA News\".\n\nThis shift in name of the newsletter reflected a significant shift in the goals of ISNA. Initially, Emi Koyama states that \"not only did intersex activists appropriate the medical label \"intersex\" as part of their identities, they also liberally used the word \"hermaphrodite\", which is now considered offensive, for example by naming the newsletter of Intersex Society of North America \"Hermaphrodites With Attitude\" and demonstrating under that name.\" Koyama argues that the intersex movement could not succeed with that label in addressing peer support needs, while identity politics drew in a different set of goals and interests. ISNA goals shifted to eradicate nomenclature based on hermaphroditism that was stated to be stigmatizing to intersex individuals, as well as potentially panic-inducing to parents of intersex children.\n\nThe suggested solution put forth by ISNA was to restructure the system of intersex taxonomy and nomenclature to not include the words 'hermaphrodite', 'hermaphroditism', 'sex reversal', or other similar terms.<ref name=\"http://www.isna.org/node/979\"></ref> This \"standard division of many intersex types into true hermaphroditism, male pseudohermaphroditism, and female pseudohermaphroditism\" was described by ISNA and its advocates as confusing and clinically problematic, and a replacement term, disorders of sex development was proposed by Alice Dreger, Cheryl Chase and others in 2005.\n\nISNA itself folded in 2008, following publication of a clinical paper and new clinical standards that adopted the term disorders of sex development to replace not only hermaphroditism and associated terms, but also the term intersex, in medical settings. ISNA gave a statement saying that \"at present, the new standard of care exists as little more than ideals on paper, thus falling short of its aim[s]\" to fulfill its goals. The ISNA decided its best course of action was to \"support a new organization with a mission to promote integrated, comprehensive approaches to care that enhance the overall health and well-being of persons [who are intersex] and their families.\" The ISNA transferred all of its remaining funds, assets, and copyrights to Accord Alliance and then closed.\n\nISNA has been survived or succeeded by several intersex civil society organizations, including the AIS Support Group USA (now called AISDSD), the Intersex Initiative, Bodies Like Ours, Organisation Intersex International, (now the Intersex Campaign for Equality) and Advocates for Informed Choice (now interACT).\n\n\n"}
{"id": "147918", "url": "https://en.wikipedia.org/wiki?curid=147918", "title": "Industrial robot", "text": "Industrial robot\n\nAn industrial robot is a robot system used for manufacturing. Industrial robots are automated, programmable and capable of movement on two or more axes.\n\nTypical applications of robots include welding, painting, assembly, pick and place for printed circuit boards, packaging and labeling, palletizing, product inspection, and testing; all accomplished with high endurance, speed, and precision. They can assist in material handling.\n\nIn the year 2015, an estimated 1.64 million industrial robots were in operation worldwide according to International Federation of Robotics (IFR).\n\nThe most commonly used robot configurations are articulated robots, SCARA robots, delta robots and cartesian coordinate robots, (gantry robots or x-y-z robots). In the context of general robotics, most types of robots would fall into the category of robotic arms (inherent in the use of the word \"manipulator\" in ISO standard 1738).\nRobots exhibit varying degrees of autonomy: \n\nThe earliest known industrial robot, conforming to the ISO definition was completed by \n\"Bill\" Griffith P. Taylor in 1937 and published in Meccano Magazine, March 1938. The crane-like device was built almost entirely using Meccano parts, and powered by a single electric motor. Five axes of movement were possible, including \"grab\" and \"grab rotation\". Automation was achieved using punched paper tape to energise solenoids, which would facilitate the movement of the crane's control levers. The robot could stack wooden blocks in pre-programmed patterns. The number of motor revolutions required for each desired movement was first plotted on graph paper. This information was then transferred to the paper tape, which was also driven by the robot's single motor. Chris Shute built a complete replica of the robot in 1997.\nGeorge Devol applied for the first robotics patents in 1954 (granted in 1961). The first company to produce a robot was Unimation, founded by Devol and Joseph F. Engelberger in 1956. Unimation robots were also called \"programmable transfer machines\" since their main use at first was to transfer objects from one point to another, less than a dozen feet or so apart. They used hydraulic actuators and were programmed in \"joint coordinates\", i.e. the angles of the various joints were stored during a teaching phase and replayed in operation. They were accurate to within 1/10,000 of an inch (note: although accuracy is not an appropriate measure for robots, usually evaluated in terms of repeatability - see later). Unimation later licensed their technology to Kawasaki Heavy Industries and GKN, manufacturing Unimates in Japan and England respectively. For some time Unimation's only competitor was Cincinnati Milacron Inc. of Ohio. This changed radically in the late 1970s when several big Japanese conglomerates began producing similar industrial robots.\n\nIn 1969 Victor Scheinman at Stanford University invented the Stanford arm, an all-electric, 6-axis articulated robot designed to permit an arm solution. This allowed it accurately to follow arbitrary paths in space and widened the potential use of the robot to more sophisticated applications such as assembly and welding. Scheinman then designed a second arm for the MIT AI Lab, called the \"MIT arm.\" Scheinman, after receiving a fellowship from Unimation to develop his designs, sold those designs to Unimation who further developed them with support from General Motors and later marketed it as the Programmable Universal Machine for Assembly (PUMA).\n\nIndustrial robotics took off quite quickly in Europe, with both ABB Robotics and KUKA Robotics bringing robots to the market in 1973. ABB Robotics (formerly ASEA) introduced IRB 6, among the world's first \"commercially available\" all electric micro-processor controlled robot. The first two IRB 6 robots were sold to Magnusson in Sweden for grinding and polishing pipe bends and were installed in production in January 1974. Also in 1973 KUKA Robotics built its first robot, known as FAMULUS, also one of the first articulated robots to have six electromechanically driven axes.\n\nInterest in robotics increased in the late 1970s and many US companies entered the field, including large firms like General Electric, and General Motors (which formed joint venture FANUC Robotics with FANUC LTD of Japan). U.S. startup companies included Automatix and Adept Technology, Inc. At the height of the robot boom in 1984, Unimation was acquired by Westinghouse Electric Corporation for 107 million U.S. dollars. Westinghouse sold Unimation to Stäubli Faverges SCA of France in 1988, which is still making articulated robots for general industrial and cleanroom applications and even bought the robotic division of Bosch in late 2004.\n\nOnly a few non-Japanese companies ultimately managed to survive in this market, the major ones being: Adept Technology, Stäub, the Swedish-Swiss company ABB Asea Brown Boveri, the German company KUKA Robotics and the Italian company Comau.\n\nAccuracy and repeatability are different measures. Repeatability is usually the most important criterion for a robot and is similar to the concept of 'precision' in measurement—see accuracy and precision. ISO 9283 sets out a method whereby both accuracy and repeatability can be measured. Typically a robot is sent to a taught position a number of times and the error is measured at each return to the position after visiting 4 other positions. Repeatability is then quantified using the standard deviation of those samples in all three dimensions. A typical robot can, of course make a positional error exceeding that and that could be a problem for the process. Moreover, the repeatability is different in different parts of the working envelope and also changes with speed and payload. ISO 9283 specifies that accuracy and repeatability should be measured at maximum speed and at maximum payload. But this results in pessimistic values whereas the robot could be much more accurate and repeatable at light loads and speeds.\nRepeatability in an industrial process is also subject to the accuracy of the end effector, for example a gripper, and even to the design of the 'fingers' that match the gripper to the object being grasped. For example, if a robot picks a screw by its head, the screw could be at a random angle. A subsequent attempt to insert the screw into a hole could easily fail. These and similar scenarios can be improved with 'lead-ins' e.g. by making the entrance to the hole tapered.\n\nThe setup or programming of motions and sequences for an industrial robot is typically taught by linking the robot controller to a laptop, desktop computer or (internal or Internet) network.\n\nA robot and a collection of machines or peripherals is referred to as a workcell, or cell. A typical cell might contain a parts feeder, a molding machine and a robot. The various machines are 'integrated' and controlled by a single computer or PLC. How the robot interacts with other machines in the cell must be programmed, both with regard to their positions in the cell and synchronizing with them.\n\n\"Software:\" The computer is installed with corresponding interface software. The use of a computer greatly simplifies the programming process. Specialized robot software is run either in the robot controller or in the computer or both depending on the system design.\n\nThere are two basic entities that need to be taught (or programmed): positional data and procedure. For example, in a task to move a screw from a feeder to a hole the positions of the feeder and the hole must first be taught or programmed. Secondly the procedure to get the screw from the feeder to the hole must be programmed along with any I/O involved, for example a signal to indicate when the screw is in the feeder ready to be picked up. The purpose of the robot software is to facilitate both these programming tasks.\n\nTeaching the robot positions may be achieved a number of ways:\n\n\"Positional commands\" The robot can be directed to the required position using a GUI or text based commands in which the required X-Y-Z position may be specified and edited.\n\n\"Teach pendant:\" Robot positions can be taught via a teach pendant. This is a handheld control and programming unit. The common features of such units are the ability to manually send the robot to a desired position, or \"inch\" or \"jog\" to adjust a position. They also have a means to change the speed since a low speed is usually required for careful positioning, or while test-running through a new or modified routine. A large emergency stop button is usually included as well. Typically once the robot has been programmed there is no more use for the teach pendant.\n\n\"Lead-by-the-nose:\" this is a technique offered by many robot manufacturers. In this method, one user holds the robot's manipulator, while another person enters a command which de-energizes the robot causing it to go into limp. The user then moves the robot by hand to the required positions and/or along a required path while the software logs these positions into memory. The program can later run the robot to these positions or along the taught path. This technique is popular for tasks such as paint spraying.\n\n\"Offline programming\" is where the entire cell, the robot and all the machines or instruments in the workspace are mapped graphically. The robot can then be moved on screen and the process simulated. A robotics simulator is used to create embedded applications for a robot, without depending on the physical operation of the robot arm and end effector. The advantages of robotics simulation is that it saves time in the design of robotics applications. It can also increase the level of safety associated with robotic equipment since various \"what if\" scenarios can be tried and tested before the system is activated.[8] Robot simulation software provides a platform to teach, test, run, and debug programs that have been written in a variety of programming languages. \n\n\"Robot simulation\" tools allow for robotics programs to be conveniently written and debugged off-line with the final version of the program tested on an actual robot. The ability to preview the behavior of a robotic system in a virtual world allows for a variety of mechanisms, devices, configurations and controllers to be tried and tested before being applied to a \"real world\" system. Robotics simulators have the ability to provide real-time computing of the simulated motion of an industrial robot using both geometric modeling and kinematics modeling.\n\n\"Others\" In addition, machine operators often use user interface devices, typically touchscreen units, which serve as the operator control panel. The operator can switch from program to program, make adjustments within a program and also operate a host of peripheral devices that may be integrated within the same robotic system. These include end effectors, feeders that supply components to the robot, conveyor belts, emergency stop controls, machine vision systems, safety interlock systems, bar code printers and an almost infinite array of other industrial devices which are accessed and controlled via the operator control panel.\n\nThe teach pendant or PC is usually disconnected after programming and the robot then runs on the program that has been installed in its controller. However a computer is often used to 'supervise' the robot and any peripherals, or to provide additional storage for access to numerous complex paths and routines.\n\nThe most essential robot peripheral is the end effector, or end-of-arm-tooling (EOT). Common examples of end effectors include welding devices (such as MIG-welding guns, spot-welders, etc.), spray guns and also grinding and deburring devices (such as pneumatic disk or belt grinders, burrs, etc.), and grippers (devices that can grasp an object, usually electromechanical or pneumatic). Other common means of picking up objects is by vacuum or magnets. End effectors are frequently highly complex, made to match the handled product and often capable of picking up an array of products at one time. They may utilize various sensors to aid the robot system in locating, handling, and positioning products.\n\nFor a given robot the only parameters necessary to completely locate the end effector (gripper, welding torch, etc.) of the robot are the angles of each of the joints or displacements of the linear axes (or combinations of the two for robot formats such as SCARA). However, there are many different ways to define the points. The most common and most convenient way of defining a point is to specify a Cartesian coordinate for it, i.e. the position of the 'end effector' in mm in the X, Y and Z directions relative to the robot's origin. In addition, depending on the types of joints a particular robot may have, the orientation of the end effector in yaw, pitch, and roll and the location of the tool point relative to the robot's faceplate must also be specified. For a jointed arm these coordinates must be converted to joint angles by the robot controller and such conversions are known as Cartesian Transformations which may need to be performed iteratively or recursively for a multiple axis robot. The mathematics of the relationship between joint angles and actual spatial coordinates is called kinematics. See robot control\n\nPositioning by Cartesian coordinates may be done by entering the coordinates into the system or by using a teach pendant which moves the robot in X-Y-Z directions. It is much easier for a human operator to visualize motions up/down, left/right, etc. than to move each joint one at a time. When the desired position is reached it is then defined in some way particular to the robot software in use, e.g. P1 - P5 below.\n\nMost articulated robots perform by storing a series of positions in memory, and moving to them at various times in their programming sequence. For example, a robot which is moving items from one place to another might have a simple 'pick and place' program similar to the following:\n\n\"Define points P1–P5:\"\n\n\n\"Define program:\"\n\n\nFor examples of how this would look in popular robot languages see industrial robot programming.\n\nThe American National Standard for Industrial Robots and Robot Systems — Safety Requirements (ANSI/RIA R15.06-1999) defines a singularity as “a condition caused by the collinear alignment of two or more robot axes resulting in unpredictable robot motion and velocities.” It is most common in robot arms that utilize a “triple-roll wrist”. This is a wrist about which the three axes of the wrist, controlling yaw, pitch, and roll, all pass through a common point. An example of a wrist singularity is when the path through which the robot is traveling causes the first and third axes of the robot’s wrist (i.e. robot's axes 4 and 6) to line up. The second wrist axis then attempts to spin 180° in zero time to maintain the orientation of the end effector. Another common term for this singularity is a “wrist flip”. The result of a singularity can be quite dramatic and can have adverse effects on the robot arm, the end effector, and the process. Some industrial robot manufacturers have attempted to side-step the situation by slightly altering the robot’s path to prevent this condition. Another method is to slow the robot’s travel speed, thus reducing the speed required for the wrist to make the transition. The ANSI/RIA has mandated that robot manufacturers shall make the user aware of singularities if they occur while the system is being manually manipulated.\n\nA second type of singularity in wrist-partitioned vertically articulated six-axis robots occurs when the wrist center lies on a cylinder that is centered about axis 1 and with radius equal to the distance between axes 1 and 4. This is called a shoulder singularity. Some robot manufacturers also mention alignment singularities, where axes 1 and 6 become coincident. This is simply a sub-case of shoulder singularities. When the robot passes close to a shoulder singularity, joint 1 spins very fast.\n\nThe third and last type of singularity in wrist-partitioned vertically articulated six-axis robots occurs when the wrist's center lies in the same plane as axes 2 and 3.\n\nSingularities are closely related to the phenomena of gimbal lock, which has a similar root cause of axes becoming lined up.\n\nA video illustrating these three types of singular configurations is available here.\n\nAccording to the International Federation of Robotics (IFR) study \"World Robotics 2018\", there were about 2,097,500 operational industrial robots by the end of 2017. This number is estimated to reach 3,788,000 by the end of 2021. For the year 2017 the IFR estimates the worldwide sales of industrial robots with US$16.2 billion. Including the cost of software, peripherals and systems engineering, the annual turnover for robot systems is estimated to be US$48.0 billion in 2017.\n\nChina is the largest industrial robot market, with 137,900 units sold in 2017. Japan had the largest operational stock of industrial robots, with 286,554 at the end of 2015. The biggest customer of industrial robots is automotive industry with 33% market share, then electrical/electronics industry with 32%, metal and machinery industry with 12%, rubber and plastics industry with 5%, food industry with 3%. In textiles, apparel and leather industry, 1,580 units are operational.\n\nEstimated worldwide annual supply of industrial robots (in units):\nThe International Federation of Robotics has predicted a worldwide increase in adoption of industrial robots and they estimated 1.7 million new robot installations in factories worldwide by 2020 [IFR 2017]. Rapid advances in automation technologies (e.g. fixed robots, collaborative and mobile robots, and exoskeletons) have the potential to improve work conditions but also to introduce workplace hazards in manufacturing workplaces. Despite the lack of occupational surveillance data on injuries associated specifically with robots, researchers from the US National Institute for Occupational Safety and Health (NIOSH) identified 61 robot-related deaths between 1992 and 2015 using keyword searches of the Bureau of Labor Statistics (BLS) Census of Fatal Occupational Injuries research database (see info from Center for Occupational Robotics Research). Using data from the Bureau of Labor Statistics, NIOSH and its state partners have investigated 4 robot-related fatalities under the Fatality Assessment and Control Evaluation Program. In addition the Occupational Safety and Health Administration (OSHA) has investigated dozens of robot-related deaths and injuries, which can be reviewed at OSHA Accident Search page. Injuries and fatalities could increase over time because of the increasing number of collaborative and co-existing robots, powered exoskeletons, and autonomous vehicles into the work environment.\n\nSafety standards are being developed by the Robotic Industries Association (RIA) in conjunction with the American National Standards Institute (ANSI). On October 5, 2017, OSHA, NIOSH and RIA signed an alliance to work together to enhance technical expertise, identify and help address potential workplace hazards associated with traditional industrial robots and the emerging technology of human-robot collaboration installations and systems, and help identify needed research to reduce workplace hazards. On October 16 NIOSH launched the Center for Occupational Robotics Research to \"provide scientific leadership to guide the development and use of occupational robots that enhance worker safety, health, and wellbeing.\" So far, the research needs identified by NIOSH and its partners include: tracking and preventing injuries and fatalities, intervention and dissemination strategies to promote safe machine control and maintenance procedures, and on translating effective evidence-based interventions into workplace practice.\n\n\n\n"}
{"id": "33334901", "url": "https://en.wikipedia.org/wiki?curid=33334901", "title": "Institut national de recherche et de sécurité", "text": "Institut national de recherche et de sécurité\n\nThe French National Research and Safety Institute for the Prevention of Occupational Accidents and Diseases (French: \"Institut national de recherche et de sécurité\", INRS) is a French association. It works under the auspices of the Caisse nationale de l’assurance maladie des travailleurs salariés (National Health Insurance Fund). Its board is composed of equal parts of representatives employers and representatives of the unions.\n\nThe main tasks of the INRS are:\n\nIt produces and distributes many information media such as magazines (Travail et sécurité), forms with the professional world. It also has a role of expertise and training to improve safety conditions.\n\n\n"}
{"id": "57857420", "url": "https://en.wikipedia.org/wiki?curid=57857420", "title": "Katharina Sophia Volz", "text": "Katharina Sophia Volz\n\nKatharina Sophia Volz is a medical researcher and entrepreneur. She is the founder and Chief Executive Officer of OccamzRazor, a start-up in San Francisco that looks to identify cures for Parkinson's Disease using machine learning.\n\nVolz was born in Ulm. She began to study molecular biology at the University of Graz, carrying out research in the United States. She worked at Harvard Medical School, Ohio State University and Stanford University. Volz was the first student from Germany's Biotechnology High School to be accepted at Harvard University.\n\nIn 2012, Volz became the first PhD student working on stem cell biology and regenerative medicine at Stanford University. She completed her PhD in 2.5 years. She was the first person to identify that the stem cells required to form coronary arteries. Cadriac pericytes turn into smooth muscle cells in response to increased blood flow. Her finding was covered by the Guardian (newspaper) and IFL Science. She worked with Kristy Red-Horse and Irving Weissman.\n\nVolz is one of UNESCO's trusted speakers and spoke at the 2015 Women Entrepreneurship Summit. She joined the Howard Hughes Medical Institute in 2011, where she worked with cardiologist Reza Ardehali. Volz was listed in Forbes 30 Under 30 in 2017. Volz was a participant at the 2018 Science Foo Camp.\n\nVolz is the founder and Chief Executive Officer of OccamzRazor, a group of researchers looking to cure degenerative diseases using machine learning. OccamzRazor is supported by The Michael J. Fox Foundation, Stanford University and MIT Media Lab. They will find data in biomedical publications, patents and genomic data sets and combine it with deep learning.\n"}
{"id": "43312937", "url": "https://en.wikipedia.org/wiki?curid=43312937", "title": "Late preterm infant", "text": "Late preterm infant\n\nLate preterm infants are infants born at a gestational age between weeks and weeks. They have higher morbidity and mortality rates than term infants (gestational age ≥37 weeks) due to their relative physiologic and metabolic immaturity, even though they are often the size and weight of some term infants. \"Late preterm\" has replaced \"near term\" to describe this group of infants, since near term incorrectly implies that these infants are \"almost term\" and only require routine neonatal care.\n\nIn 2005, late-preterm births accounted for more than 70% of all preterm births (<37 weeks’ gestation), or approximately 377,000 infants. In fact, much of the increase in the preterm birth rate in recent years can be attributed to increases in late-preterm births.\n\nSeveral important factors that may predispose late-preterm infants to medical conditions associated with immaturity:\n\nAt 34–35 weeks, the brain weight is only about that of a full-term baby. This may lead to an increased risk of:\n\nLate preterm infants have immature gastrointestinal function and feeding difficulties that predispose them to in increase in enterohepatic circulation, decreased stool frequency, dehydration, and hyperbilirubinemia. Feeding during the birth hospitalization may be transiently successful, but not sustained after discharge. Feeding difficulties are associated with relatively low oromotor tone, function, and neural maturation also predispose these infants to dehydration and hyperbilirubinemia.\n\nLate Preterm Infants have an increased risk of being underweight and stunted at 12 and 24 months of age versus term infants.\n\nProper nutrition is essential for normal growth, optimal neurologic and cognitive development, immune protection, and long term health.\n\nThe last trimester of pregnancy the fetus is expressing active amino acid transport, calcium, lipid transfer, and glucose facilitated diffusion. Delivery of the premature infant requires higher energy expenditure, but with inadequate intake the infant will have negative nitrogen balance. There are higher needs for Calcium, Phosphorus, and Vitamin D.\n\nFor every 10 kcal/kg increase in energy intake in the first week of life, there is a 4.6 points increase in MDI (Mental Development Index) at 18 months. For every 1 g/kg increase in protein intake in the first week of life, 8.2 point increase in MDI at 18 months.\n\n\nFactors such as hemodynamic stability, severe IUGR, respiratory, abdominal exam, whether feeding cues are present, and stable glucose could all effect the timing of nutrition. Some preterm infants will be NPO (nil per os). If infants are unable to start oral or enteral intake intravenous fluids may begin with amino acids or total parenteral nutrition.\n\nAccording to the American Academy of Pediatrics section on breastfeeding recommendations are all infants should receive human milk.\n\nUse caution when fortifying single nutrients to prevent alteration of protein/energy ratio. Center for Disease Control (CDC) recommends that sterile formulas and fortifiers be used when mom is not available. Powdered formula and HMF may be contaminants. Start with the mom's diet during breastfeeding. Mom should be eating adequate calories, protein, B vitamins and DHA.\n\nColostrum production can range from 26-56 mL the first day to 113-185 mL for day two. Although colostrum production is not voluminous, it can still meet the needs of the newborn.\n\n\n\n"}
{"id": "29605965", "url": "https://en.wikipedia.org/wiki?curid=29605965", "title": "Life satisfaction", "text": "Life satisfaction\n\nLife satisfaction (LS) is the way in which people show their emotions, feelings (moods) and how they feel about their directions and options for the future. It is a measure of well-being assessed in terms of mood, satisfaction with relationships, achieved goals, self-concepts, and self-perceived ability to cope with one's daily life. Life satisfaction involves a favorable attitude towards one's life rather than an assessment of current feelings. Life satisfaction has been measured in relation to economic standing, degree of education, experiences, residence, among many other topics.\n\nLife satisfaction is a key part of subjective wellbeing.\n\nOne of the primary concepts of personality is the Big Five factor model. This model illustrates what some researchers believe to be the building blocks of every individual's personality. This model considers the dimensions of openness to experience, conscientiousness, extraversion, agreeableness, and neuroticism. In a study carried out by Deneve and Cooper in 1998, multiple studies were analyzed with certain personality questionnaires that linked subjective well-being and personality measures. They found that neuroticism was the strongest predictor of life satisfaction. Neuroticism is also linked to people who have difficulty making up their mind, and is common in people who suffer from mental illness. The personality factor \"openness to experience\" is positively correlated with life satisfaction. Apart from the personality dimensions studied in the Big Five model, the trait chronotype has been related to life satisfaction; morning-oriented people (\"larks\") showed higher life satisfaction than evening-oriented individuals (\"owls\").\n\nMore frequent socialization can also contribute to overall well-being. Social support via others has been shown to affect the well-being of adults and the overall health of those individuals. Therefore, people who tend to communicate, and who are considered to be more open to others would have a higher-level of life satisfaction.\n\nHeritability has been shown to have an effect on how one is ranked in terms of life satisfaction. Heritability plays a role in both personality and individual experiences. Research suggests that heritability can influence life satisfaction to some degree. This study found that there were no individual differences between males and females in terms of the heritability of life-satisfaction, however the personality elements that were affected by heritability did seem to have an effect on their overall life-satisfaction.\n\nIt has been further suggested that being able to independently deal with negative emotions can influence long-term life-satisfaction. Having a personality capable of properly dealing with emotions like anger, angst, or hate can be beneficial when dealing with similar things later in life. People who are more easy-going tend to deal with their negative emotions differently than someone who is up-tight. These individual differences can influence the way people deal with problems in the present and how they deal with similar situations in the future.\n\nThe Satisfaction with Life Scale (SWLS) is a single scale that is used by UNESCO, the CIA, the New Economics Foundation, the WHO, the Veenhoven Database, the Latinbarometer, the Afrobarometer, and the UNHDR to measure how one views his or her self-esteem, well-being and overall happiness with life. Previous modeling showed that positive views and life satisfaction were completely mediated by the concept of self-esteem, together with the different ways in which ideas and events are perceived by people. Several studies found that self-esteem plays a definite role in influencing life satisfaction. By knowing yourself and your worth, you are driven to think in a positive way. There is also a homeostatic model that supports these findings.\n\nA person's mood and outlook on life can also influence his or her perception of his or her own life satisfaction. There are two kinds of emotions that may influence how people perceive their lives. Hope and optimism both consist of cognitive processes that are usually oriented towards the reaching of goals and the perception of those goals. Additionally, optimism is linked to higher life satisfaction, whereas pessimism is related to symptoms in depression.\n\nAccording to Seligman, the happier people are, the less they focus on the negative aspects of their lives. Happier people also have a greater tendency to like other people, which promotes a happier environment. This correlates to a higher level of the person's satisfaction with his or her life, due to the notion that constructiveness with others can positively influence life satisfaction. However, others have found that life satisfaction is compatible with profoundly negative emotional states like depression.\n\nLife-review therapy using Autobiographical Retrieval Practice for older adults with depressive symptoms, in a study carried out by Serrano JP, Latorre JM, Gatz M, and Montanes J, Department of Psychology at Universidad de Castilla-La Mancha, demonstrated that, with increased specificity of memories, individuals show decreased depression and hopelessness and increased life satisfaction. The test was designed to measure participants' ability to recall a specific memory in response to a cue word, while being timed.\nThirty cue words; including five words classified as 'positive' (e.g., \"funny, lucky, passionate, happy, hopeful),\" five as 'negative' \"(unsuccessful, unhappy, sad, abandoned, gloomy),\" and five as 'neutral' \"(work, city, home, shoes, family);\" were presented orally in a fixed, alternating order to each member of a focus group. To ensure that the participants understood the instructions, examples were provided of both 'general' memories (e.g., summers in the city) and 'specific' memories (e.g., the day I got married). For each cue word, participants were asked to share a memory evoked by that word. The memory had to be of an event that should have occurred only once, at a particular time and place, and lasted no longer than a day. \nIf the person could not recall a memory within 30 seconds, then that cue instance was not counted. Two psychologists served as raters and independently scored the responses of each participant. Each memory was tagged either as 'specific' – if the recalled event lasted no more than one day – or, otherwise, as 'general'. The raters were not informed regarding the hypotheses of the study, the experimental (control) group's membership, nor the content of the pretest or post-test.\n\nThe psychologists Yuval Palgi and Dov Shmotkin (2009) studied people who were primarily in their nineties. This subject group was found to have thought highly of their past and present. But generally, the group thought lower of their future. These people were very satisfied with their life up until the point they were surveyed but knew that the end was near and so were not quite as hopeful for the future. Intelligence is also a factor because life satisfaction grows as people become older; as they grow older, they become wiser and more knowledgeable, so they begin to see that life will be better and understand the important things in life more.\n\nIt has been recorded that adolescents seem to have a lower level of life satisfaction than their older counterparts. This could be because many decisions are imminent, and an adolescent could be facing them for the first time in his or her life. Although many adolescents have insecurities about many aspects of their lives, satisfaction with friends stayed at a consistent level. This is hypothesized to be due to the amount one can identify with those in one's age group over other age groups. In this same study, researchers found that satisfaction with family decreased. This could be because more rules and regulations are typically implemented by parental figures, and adolescents tend to demonize those in control of them. Also, it was found that life satisfaction in terms of sexuality increased. This is because at this age many adolescents reach sexual maturation, which can encourage them to find verification and satisfaction in the idea of a sexual partnership.\n\nIt has been suggested that there are several factors that contribute towards our level of life satisfaction. Experiences that are both acute events (e.g., death of\na loved one) and chronic, daily experiences (e.g., ongoing family discord) influence self-reports of life satisfaction. The book “Happier” by Harvard lecturer Tal Ben-Shahar argues that happiness should be one's ultimate goal, the primary factor in evaluating alternative choices. As the subtitle implies, Happier recommends for us to pursue immediate joyful experience in ways that contributes to more long-term, meaningful satisfaction. Furthermore, Ben-Shahar argues that pursuing genuine self-motivated goals, rather than just instant pleasure or selflessness in service of long-delayed enjoyment, results in an optimal combination of short- and long-term happiness.\n\nDifferences in experience can greatly shape the way that we observe and engage with the world around us. It can influence the way we speak to people, the way we act in public, and our general outlook. These experiences which shape the way we think about our surroundings affect our life-satisfaction. Someone who has the tendency to see the world in a more negative light, may have a completely different level of satisfaction than someone who is constantly admiring the beauty of his or her surroundings. People who engage with more stress on average tend to have higher levels of stress can contribute to higher levels of self-report life satisfaction, as long as those who understand how to deal with their stress in a positive way.\n\nA recent study analyzes time-dependent rhythms in happiness comparing life satisfaction by weekdays (weekend neurosis), days of the month (negative effects towards the end of the month) and year with gender and education and outlining the differences observed. Primarily within the winter months of the year, an onset of depression can affect us, which is called seasonal affective disorder (SAD). It is recurrent, beginning in the fall or winter months, and remitting in the spring or summer. It is said that those who experience this disorder usually have a history of major depressive or bipolar disorder, which may be hereditary, having a family member affected as well.\n\nSeasonal affective disorder is hypothesized to be caused by the diminishing of the exposure to environmental light which can lead to changes in levels of the neurotransmitter chemical seratonin. Diminishing active seratonin levels increases depressive symptoms. There are currently a few treatment therapies in order to help with seasonal affective disorder. The first line of therapy is light therapy. Light therapy involves exposure to bright, white light that mimics outdoor light, counteracting the presumed cause of SAD. Due to the shifts in one's neurochemical levels, antidepressants are another form of therapy. Other than light therapy and antidepressants, there are several alternatives which involve agomelatine, melatonin, psychological interventions, as well as diet and lifestyle changes.\n\nResearch has found that the onset of SAD typically occurs between the ages of 20–30 years, but most affected people do not seek medical help. This could be due to the stigma of mental health issues. Many are afraid to state they are suffering and would rather hide it. As a society we should push forward towards greater acceptance and gain knowledge in order to solve these issues.\n\nIt is proposed that overall life satisfaction comes from within an individual based on the individual's personal values and what he or she holds important. For some it is family, for others it is love, and for others it is money or other material items; either way, it varies from one person to another. Economic materialism can be considered a value. Previous research found that materialistic individuals were predominantly male, and that materialistic people also reported a lower life satisfaction level than their non-materialistic counterparts. The same is true of people who value money over helping other people; this is because the money they have can buy them the assets they deem valuable. Materialistic people are less satisfied with life because they constantly want more and more belongings, and once those belongings are obtained they lose value, which in turn causes these people to want more belongings and the cycle continues. If these materialistic individuals do not have enough money to satisfy their craving for more items, they become more dissatisfied. This has been referred to as a hedonic treadmill. Individuals reporting a high value on traditions and religion reported a higher level of life satisfaction. This is also true for reported routine churchgoers and people who pray frequently. Other individuals that reported higher levels of life satisfaction were people who valued creativity, and people who valued respect for and from others – two more qualities seemingly not related to material goods. Because hard times come around and often people count on their peers and family to help them through, it is no surprise that a higher life satisfaction level was reported of people who had social support, whether it be friends, family, or church. The people who personally valued material items were found to be less satisfied overall in life as opposed to people who attached a higher amount of value with interpersonal relationships. accordance with the findings above, it is also fair to say that the notion of how one values themselves plays a part in how someone considers their own life. People who take pride in themselves by staying mentally and physically fit have higher levels of life satisfaction purely due to the content of their day. These values come together in determining how somebody sees themselves in light of others.\n\nDefining culture by reference to deeply engrained societal values and beliefs. Culture affects the subjective well-being. Well-being includes both general life satisfaction, and the relative balance of positive affect verses negative affect in daily life. Culture directs the attention to different sources of information for making the life satisfaction judgments, thus affecting subjective well-being appraisal.\n\nIndividualistic cultures direct attention to inner states and feelings (such as positive or negative affects), while in collectivistic cultures the attention is directed to outer sources (i.e. adhering to social norms or fulfilling one's duties). Indeed, Suh et al. (1998) found that the correlation between life satisfaction and the prevalence of positive affect is higher in individualistic cultures, whereas in collectivistic cultures affect and adhering to norms are equally important for life satisfaction. Most of modern western societies, such as the United States and European countries are directed towards individualism, while the eastern societies like China and Japan, are directed towards collectivism. Those of a collectivistic culture emphasize deeply on the unity one has with their families. They put others' needs before their individual desires. An individualistic culture is geared towards one's own personal achievements and it signals a strong sense of competition. They are expected to carry their own weight and rely on themselves. The United States is said to be one of the most individualistic countries, and on the other hand Korea and Japan are some of the most collectivistic countries. However both groups have their flaws. With an individualistic approach, one is inclined in possibly experiencing loneliness. Meanwhile, those in a collectivist culture, may be prone to having a dismay of rejection.\n\nLife satisfaction can also be looked at in a new one as influenced by a family. Family life satisfaction is a pertinent topic as everyone's family influences them in some way and most strive to have high levels of satisfaction in life as well as within their own family. As discussed by Gary L. Bowen in his article, \"Family Life Satisfaction: A Value Based Approach\" he examines how family life satisfaction is enhanced by the ability of family members to jointly realize their family-related values in behavior (459). It is important to examine family life satisfaction from all members of the family from a \"perceived\" perspective and an \"ideal\" perspective. Greater life satisfaction within a family increases through communication and understanding each member's attitudes and perceptions. A family can make all the difference for someone's life satisfaction.\n\nIn the article \"Family System Characteristics, Parental Behaviors, and Adolescent Life Satisfaction\" by Carolyn S. Henry, adolescent life satisfaction has much different origins than the life satisfaction of adults. An adolescent's life satisfaction is heavily influenced by his or her family's dynamic and characteristics. Family bonding, family flexibility, parental support are all huge factors into the adolescent's life satisfaction. The more bonding, flexibility, and support there is within a family the higher the adolescent's life satisfaction. Results of this study also revealed that adolescents living in a single-parent family home had significantly lower life satisfaction that adolescents in a two-parent home. An adolescent's age is extremely important in terms of life satisfaction coming from their family (Henry).\n\nFamily also relates to life satisfaction in a very different way: a woman's decision to have children or not. In the \"Relationship between Information Search in the Childbearing Decision and Life Satisfaction for Parents and Nonparents\" article by Carole K. Holahan, reveals that childless women have much higher life satisfaction than women with children. Women who consciously decided not to have children overall had very high life satisfaction. It was found that most of the life satisfaction came from careers instead of children. On the other hand, women who did have children had high life satisfaction which depended on the reasons and decision making for having children. These are just generalizations and life satisfaction comes from many different sources which are unique and different for every person. Life satisfaction can shift all the time from events, situations, family and friend implications and many different things that all must be taken into consideration.\n\nOn the other hand, life satisfaction is also affected by parenthood and couples introducing children into their relationship. Research has shown that adults with children are less happy (McLanahan & Adams 1987) due to less life satisfaction, less marital satisfaction, more anxiety and more depression.\n\nA satisfying career is an important component of life satisfaction. Doing something meaningful in a productive capacity contributes to one's feeling of life satisfaction. This notion of accomplishment is related to a person’s drive. Need for accomplishment is an essential part of becoming a fully functional person, and if someone feels accomplished they would be more able to see bright sides in their life; thus improving their life satisfaction\n\nInternationally, the salary one earns is important – income levels show a moderate correlation with individual evaluations of life satisfaction. However, in developed nations, the connection is weak and disappears for the most part when individuals earn enough money to meet basic needs (Kahneman & Deaton 2010; Diener et al., 2010; Myers and Diener, 1995).\n\nLife satisfaction is one component of subjective well-being, along with affective balance..\n\n\n\n"}
{"id": "199249", "url": "https://en.wikipedia.org/wiki?curid=199249", "title": "Live birth (human)", "text": "Live birth (human)\n\nIn human reproduction, a live birth occurs when a fetus, whatever its gestational age, exits the maternal body and subsequently shows any sign of life, such as voluntary movement, heartbeat, or pulsation of the umbilical cord, for however brief a time and regardless of whether the umbilical cord or placenta are intact.\n\nThis definition of the term \"live birth\" was created by the World Health Organization in 1950 and is chiefly used for public health and statistical purposes. Whether the birth is vaginal or by Caesarean section, and whether the neonate is ultimately viable, is not relevant to this statistical definition. However, the term \"live birth\" was in common use long before 1950.\n\nIn the United States, the term \"born alive\" is defined by federal statute. In the United States, live births are recorded on a U.S. Standard Certificate of Live Birth, also known as a birth certificate. The United States recorded 3.95 million live births in 2016.\n\nSome women have chosen to release online videos of the live births of their infants. There is one case report of a woman having a live birth derived from a frozen embryo obtained before the woman began cancer treatment.\n\n\n"}
{"id": "41163867", "url": "https://en.wikipedia.org/wiki?curid=41163867", "title": "Marriage and health", "text": "Marriage and health\n\nMarriage and health are closely related. Married people experience lower morbidity and mortality across such diverse health threats as cancer, heart attacks, and surgery. There are gender differences in these effects which may be partially due to men's and women's relative status. Most research on marriage and health has focused on heterosexual couples, and more work is needed to clarify the health effects on same-sex marriage. Simply being married, as well as the quality of one's marriage, has been linked to diverse measures of health. Research has examined the social-cognitive, emotional, behavioral and biological processes involved in these links.\n\nBeyond marriage, social relationships more broadly have a powerful impact on health. A meta-analysis of 148 studies found that those with stronger social relationships had a 50% lower risk of all-cause mortality. Conversely, loneliness is associated with increased risk for cardiovascular disease, and all-cause mortality. Little work has directly compared the health impacts of marriage compared to those of non-romantic relationships, such as connections with friends or colleagues. However, there are several reasons why marriage may exert a greater health impact than other relationships, even other cohabiting relationships: married couples spend time together during a wide variety of activities, such as eating, leisure, housekeeping, child-care and sleep. Spouses also share resources and investments such as joint finances or home-ownership. Relative to other relationships, the increased interdependence of marriage serves as a source for more intense support.\n\nRomantic couples who live together, but are unmarried, may represent a middle ground in health benefits between those who are married, and those who self-identify as single. However, people live together without getting married for many different reasons; cohabitation may serve as a prelude to marriage. Selection factors of race, ethnicity, and social-economic status predispose certain groups to cohabit unmarried, and these factors also affect the health benefits of marriage and cohabitation.\n\nMost research on marriage and health has studied heterosexual couples. Same-sex and opposite-sex couples share many similarities. Both begin marriage with high levels of relationship satisfaction, followed by later declines, and both argue with similar frequency about similar issues.\n\nHowever, same-sex couples resolve conflicts more effectively, and distribute household labor more fairly compared to their heterosexual counterparts. Same-sex marriage remains illegal in many countries, and in many parts of the United States (where much research on marriage and health has been conducted). In these regions same-sex couples are not granted the institutional protections of marriage or its accompanying legal barriers to relationship dissolution. Moreover, same-sex couples are more likely to experience discrimination against their sexual orientation, contributing to problems with mental health and relationship quality.\n\nThe health-protective effect of marriage is stronger for men than women. Marital status — the simple fact of being married — confers more health benefits to men than women. Women's health is more strongly impacted than men's by marital conflict or satisfaction, such that unhappily married women do not enjoy better health relative to their single counterparts. Laboratory studies indicate that women have stronger physiological reactions than men in response to marital conflict.\n\nThese gender differences may be partially due to men's and women's relative status in a relationship. Research in humans and animals suggests subordinate status is linked to greater physiological reactions to social stress. Indeed, subordinate spouses show greater physiological reactions to arguments with their partner. Both husbands and wives show stronger physiological reactions to arguments when making demands for change from their partner. Women's heightened physiological reactions to marital conflict may be due to their relative subordinate position in marriage.\n\nResearch on the links between marriage and health has measured diverse outcomes. These are broadly categorized as clinical endpoints, surrogate endpoints, and biological mediators. Clinical endpoints are variables which affect how people feel, function, and survive. They are recognized as important outcomes by health care providers and patients, for instance being hospitalized, or having a heart-attack.\n\nSurrogate endpoints and biological mediators are types of biomarkers—objective indicators of normal or pathological physiological processes. Surrogate endpoints serve to substitute for clinical endpoints. They are expected to predict clinical endpoints, based on scientific evidence. For example, elevated blood pressure has been found to predict cardiovascular disease.\n\nBiological mediators reflect short-term sources of stress which affect health outcomes through repeated or persistent activation. These processes do not have a sufficient evidence base linking them to clinical endpoints in order be elevated to the class of surrogate endpoints. Examples include changes in hormone levels, or immune measures.\n\nThe health benefits of marriage are a result of both selection and protection effects. People with better health, more resources, and less stress are more likely to marry, and marriage brings resources, and social support. The health benefits of marriage persist even after controlling for selection effects, indicating that being married is protective of health.\n\nResearch on marriage and health is part of the broader study of the benefits of social relationships. Social ties provide people with a sense of identity, purpose, belonging and support. Two main models describe how social support influences health.\n\nThe main-effects model proposes that social support is good for one's health, regardless of whether or not one is under stress. The stress-buffering model proposes that social support acts as a buffer against the negative effects of stress occurring outside the relationship. Both models have received empirical support, depending on how social support is conceptualized and measured. Marriage should be a strong source of social support in both models.\n\nThere are several interesting social norms between couples that are relevant towards forming health-related habits. Couples have a higher chance of accomplishing a goal when they collaborate, as opposed to achieving the same goal as individuals. In addition, couples' habits play an important role in influencing the health habits of their children. As a result, shared activities among couples can help develop stronger relationships that can lead to other health and long-term benefits. There are several interesting social norms between couples that are relevant towards forming health-related habits. Examples include:\n\nThere exists several studies specifically examining the effect of spousal engagement on exercise program adherence. For example, one study examined healthy couples' behavior based on a 12-month fitness program where researchers tracked the behaviors between 30 married-singles vs. 32 married pairs. Their results reveal statistically significant differences:  at the end of the study, 6.3% of the married-pairs had dropped out of the program, compared to 43% of the married singles. These findings are consistent with previous research, and they point to the notion that social support in the form of \"spousal participation\" exerts beneficial effect on adherence towards fitness programs, or in general, most types of health-related joint endeavor.\n\nWhile simply being married is associated on average with better health, the health impacts of marriage are affected by marital quality. High marital quality is typically characterized as high self-reported satisfaction with the relationship, generally positive attitudes toward one's spouse, and low levels of hostile and negative behavior. Conversely, low marital quality is characterized as low self-reported satisfaction with the relationship, generally negative attitudes toward one's spouse, and high levels of hostile and negative behavior. A troubled marriage is a significant source of stress, and limits one's ability to seek support from other relationships. Unmarried people are, on average, happier than those unhappily married. A meta-analysis of 126 studies found that greater marital quality is related to better health, with effect sizes comparable to those of health behaviors such as diet and exercise. Explanations for the links between marital quality and health focus on social-cognitive and emotional processes, health behaviors, and a bidirectional association with mental illness.\n\nPeople in happy marriages may think about their relationship differently from people in troubled marriages. Unhappily married people often hold their partner responsible for negative behaviors, but attribute positive behavior to other factors—for example, \"she came home late because she doesn't want to spend time with me; she came home early because her boss told her to.\" Blaming one's partner for their negative behavior is associated with prolonged elevations of the stress hormone cortisol after an argument. Spouses in troubled marriages are also likely to misattribute their partners' communication as criticism. However, the links between these social-cognitive processes and health remain understudied.\n\nHigher levels of negative emotions and less effective emotional disclosure may be involved in linking marital quality and health. People in troubled marriages experience more negative emotions, particularly hostility. Negative emotions have been linked to elevated blood pressure and heart rate, and to increased levels of stress hormones, which may lead to ill health. Emotional disclosure often occurs in well-functioning marriages, and is linked to a host of health benefits, including fewer physician visits and missed work days. However, people in troubled marriages are less skillful in emotional disclosure.\n\nHealth behaviors such as diet, exercise and substance use, may also affect the interplay of marital quality and health. The health behaviors of married couples converge over time, such that couples who have been married many years have similar behavior. One explanation is that spouses influence or control one another's health behaviors. A spouse's positive control techniques, such as modeling a healthy behavior, increase their partner's intentions to improve health behaviors, whereas negative control techniques, such as inducing fear, do not affect intentions. Marital support may increase the psychological resources—such as self-efficacy, and self-regulation—needed to improve one's health behaviors.\n\nMarital problems predict the onset of mental illness, including anxiety, mood, and substance use disorders. Much research has focused on depression, showing a bidirectional connection with marital conflict. Marital distress interacts with existing susceptibility, increasing risk for depression. Conversely, depressive behavior such as excessive reassurance-seeking can be burdensome for one's spouse, who may respond with criticism or rejection. The links between depression and ill health are well established; depression is associated with immune system dysregulation, and poor health behaviors, such as lack of exercise, poor sleep and diet, and increased substance abuse.\n\nDysregulation of the cardiovascular, neuroendocrine and immune systems is implicated in the links between marital quality and health.\n\nMarital conflict, and seeking change from one's spouse evokes a cardiovascular reaction, increasing heart rate and blood pressure. Couples who are more hostile during arguments have stronger cardiovascular reactions. Heightened cardiovascular reactions are associated with increased risk for cardiovascular disease.\n\nHormones produced by the sympathetic-adrenal-medullary axis (SAM) and hypothalamic-pituitary-adrenal axis (HPA) have wide-ranging effects across the body. Both axes have been implicated in the links between psychological factors and physical health. SAM activity can be measured by levels of circulating catecholamines—epinephrine and norepinephrine. Negative interactions with one's spouse have been linked to elevated catecholamine levels, both during and after conflict.\n\nDaily fluctuations in the level of cortisol—a stress hormone—are an important marker of health; flatter slopes of cortisol change throughout the day are strongly associated with cardiovascular disease and related mortality. Lower marital satisfaction has been linked with flatter cortisol slopes across the day, lower waking levels of cortisol, and higher overall cortisol levels. However a meta-analysis found no relationship between marital quality and cortisol slopes.\n\nLow marital satisfaction, and hostility during arguments with one's spouse are associated with increased inflammation. Inflammation is part of a healthy response to injury and infection, however chronic and persistent inflammation damages the surrounding tissue. Chronic inflammation is implicated as a central mechanism linking psychosocial factors and diseases such as atherosclerosis and cancer. Beyond inflammation, lower marital quality is also related to poorer functioning of the adaptive immune system. Marital dissatisfaction and hostility during arguments with one's spouse are related to poorer ability to control the Epstein-Barr virus, a latent virus which infects most adults.\n"}
{"id": "778440", "url": "https://en.wikipedia.org/wiki?curid=778440", "title": "Medical identification tag", "text": "Medical identification tag\n\nA medical identification tag is a small emblem or tag worn on a bracelet, neck chain, or on the clothing bearing a message that the wearer has an important medical condition that might require immediate attention. The tag is often made out of stainless steel or sterling silver. The intention is to alert a paramedic, physician, emergency department personnel or other first responders (emergency medical services, community first responder, Emergency medical responder) of the condition even if the wearer is not conscious enough, old enough, or too injured to explain. A wallet card with the same information may be used instead of or along with a tag, and a stick-on medical ID tag may be added or used alone.\n\nA type of medic identification alert is the USB medical alert tag, essentially a USB flash drive with capacity to store a great deal of emergency information, including contacts and medical conditions. This information is accessible by any computer with a USB port. However, the practical effectiveness of such a system is limited in many cases by medical computer systems that restrict the use of USB devices which may carry malware. It is also possible that a device carried by an unconscious person may not be their own, or not be up to date, with concomitant risks to health and legal liability of medical personnel.\n\nAnother new type of medic identification alert is QR code based medical alert stickers. The QR code on the sticker links to a web service that contains the individual's emergency information. The information is accessed by any first responder or emergency personnel by scanning the QR code by using a smartphone. Since a web service is used to store the information there is normally no limitation of how much information that can be stored.\n\nTypical conditions warranting wearing of such a tag are:\n\nIn addition to mention of the relevant medical condition(s), the tag may have a telephone number that medical personnel can call for more information, for example that of physician, care-giver or next of kin. Where applicable and provided, the wearer's national health service user number can enable access to a more detailed case history. Basically, the medical information tag, engraved with the wearer's personal medical problem or history, speak for the wearer when the wearer can't. Incidentally and where the symptoms can mislead, such a tag may also be useful as evidence of such a condition to law enforcement personnel.\n\nThere are various types of medical ID available. The most common form of medical ID is jewelry which provides a logo or inscription indicating a particular medical condition. These medical identification tags can be made out of stainless steel (usually classified as 316L and known as surgical stainless steel), sterling silver or gold. If found by emergency personnel the inscription provides an indication of your special medical needs. Tags are available with pre-engraved conditions or can be custom engraved with your specific medical histories and have the benefit of that all information is self-contained and does not require any form of technology to view in case of an emergency.\n\nAnother type of medical ID jewelry indicates membership in a medical information organization such as the MedicAlert Foundation, and American Medical ID. Such medical ID jewelry includes a member identification number and a toll-free number for medical emergency personnel to contact the organization and obtain full information about the wearer's medical conditions, treatment, and history. These organizations maintain a database of medical information on their members and can provide it to medical personnel when requested.\n\nThe newest technology allows the user to carry stickers with an NFC Tag. A similar technology allows the user to carry stickers with a QR code. By scanning the NFC Tag or the QR code with a smartphone, you will reach the stored medical alert information. Apple's IOS 8 operating system includes the facility for a mobile phone to contain the owner's medical emergency information.\n\nSilicone bracelets, preprinted with a general medical condition or allergy, are also popular. The lack of personalization may be a deterrent. Recently patients have begun to \"tattoo\" their medical condition on their wrist or arm. Although a permanent tattoo might be considered, a temporary tattoo works as well. Other items include stick on tags that stick onto a driver's license, wallet, or cell phone which are practical for the person who does not want to carry something extra advertising their medical condition.\n\nAnother type of medical jewelry is a pendant or wrist strap containing a wireless alert button, also known as a panic button, worn in the home as part of a wireless medical alert system. This type of medical jewelry sends a signal to a dialing console which contacts a medical alarm monitoring service or directly dials first responders when an emergency occurs.\n\nDevices marked \"ICE\" which can hold a significant amount of data and are readable by a computer are sold, typically USB flash drives with password-protected data entry providing read-only access to emergency medical data. However, it has been pointed out by a staff nurse with experience in trauma and critical care that such devices are worse than useless, at least in most situations in the UK, as medical computer systems are designed not to accept USB storage devices due to the risk of computer viruses. Additionally, there is no guarantee that ICE information even pertains to an unconscious person carrying it; using incorrect information can lead to patient harm and legal liability.\n\n"}
{"id": "3940866", "url": "https://en.wikipedia.org/wiki?curid=3940866", "title": "Medical transcription", "text": "Medical transcription\n\nMedical transcription, also known as MT, is an allied health profession dealing with the process of transcribing voice-recorded medical reports that are dictated by physicians, nurses and other healthcare practitioners. Medical reports can be voice files, notes taken during a lecture, or other spoken material. These are dictated over the phone or uploaded digitally via the Internet or through smart phone apps.\n\nMedical records have been kept since humans began writing, as attested by ancient cave writings. Medical transcription as it is currently known has existed since the beginning of the 20th century, when standardization of medical data became critical to research. At that time, medical stenographers replaced physicians as the recorders of medical information, taking doctors' dictation in shorthand. With the creation of audio recording devices, it became possible for physicians and their transcriptions to work asynchronously, thus beginning the profession of healthcare documentation as we currently know it.\n\nOver the years, transcription equipment has changed from manual typewriters to electric typewriters to word processors to computers and from plastic disks and magnetic belts to cassettes and endless loops and digital recordings. Today, speech recognition (SR), also known as continuous speech recognition (CSR), is increasingly being used, with medical transcriptions and or \"editors\" providing supplemental editorial services, although there are occasional instances where SR fully replaces the MT. Natural-language processing takes \"automatic\" transcription a step further, providing an interpretive function that speech recognition alone does not provide (although Ms do).\n\nIn the past, these medical reports consisted of very abbreviated handwritten notes that were added in the patient's file for interpretation by the primary physician responsible for the treatment. Ultimately, this mess of handwritten notes and typed reports were consolidated into a single patient file and physically stored along with thousands of other patient records in a wall of filing cabinets in the medical records department. Whenever the need arose to review the records of a specific patient, the patient's file would be retrieved from the filing cabinet and delivered to the requesting physician. To enhance this manual process, many medical record documents were produced in duplicate or triplicate by means of carbon copy.\n\nIn recent years, medical records have changed considerably. Although many physicians and hospitals still maintain paper records, there is a drive for electronic records. Filing cabinets are giving way to desktop computers connected to powerful servers, where patient records are processed and archived digitally. This digital format allows for immediate remote access by any physician who is authorized to review the patient information. Reports are stored electronically and printed selectively as the need arises. Many MTs now utilize personal computers with electronic references and use the Internet not only for web resources but also as a working platform. Technology has gotten so sophisticated that MT services and MT departments work closely with programmers and information systems (IS) staff to stream in voice and accomplish seamless data transfers through network interfaces. In fact, many healthcare providers today are enjoying the benefits of handheld PCs or personal data assistants (PDAs) and are now utilizing software on them.\n\nMedical transcription is part of the healthcare industry that renders and edits doctor dictated reports, procedures, and notes in an electronic format in order to create files representing the treatment history of patients. Health practitioners dictate what they have done after performing procedures on patients and MT’s transcribe the oral dictation and/or edit reports that have gone through speech recognition software.\n\nPertinent up-to-date, confidential patient information is converted to a written text document by a medical transcriptionist (MT). This text may be printed and placed in the patient's record and/or retained only in its electronic format. Medical transcription can be performed by MTs who are employees in a hospital or who work at home as telecommuting employees for the hospital; by MTs working as telecommuting employees or independent contractors for an outsourced service that performs the work offsite under contract to a hospital, clinic, physician group or other healthcare provider; or by MTs working directly for the providers of service (doctors or their group practices) either onsite or telecommuting as employees or contractors. Hospital facilities often prefer electronic storage of medical records due to the sheer volume of hospital patients and the accompanying paperwork. The electronic storage in their database gives immediate access to subsequent departments or providers regarding the patient's care to date, notation of previous or present medications, notification of allergies, and establishes a history on the patient to facilitate healthcare delivery regardless of geographical distance or location.\n\nThe term transcript or \"report\" as it is more commonly called, is used as the name of the document (electronic or physical hard copy) which results from the medical transcription process, normally in reference to the healthcare professional's specific encounter with a patient on a specific date of service. This report is referred to by many as a \"medical record\". Each specific transcribed record or report, with its own specific date of service, is then merged and becomes part of the larger patient record commonly known as the patient's medical history. This record is often called the patient's chart in a hospital setting.\n\nMedical transcription encompasses the MT, performing document typing and formatting functions according to an established criterion or format, transcribing the spoken word of the patient's care information into a written, easily readable form. MT requires correct spelling of all terms and words, (occasionally) correcting medical terminology or dictation errors. MTs also edit the transcribed documents, print or return the completed documents in a timely fashion. All transcription reports must comply with medico-legal concerns, policies and procedures, and laws under patient confidentiality.\n\nIn transcribing directly for a doctor or a group of physicians, there are specific formats and report types used, dependent on that doctor's speciality of practice, although history and physical exams or consults are mainly utilized. In most of the off-hospital sites, independent medical practices perform consultations as a second opinion, pre-surgical exams, and as IMEs (Independent Medical Examinations) for liability insurance or disability claims. Some private practice family doctors choose not to utilize a medical transcriptionist, preferring to keep their patient's records in a handwritten format, although this is not true of all family practitioners.\n\nCurrently, a growing number of medical providers send their dictation by digital voice files, utilizing a method of transcription called speech or voice recognition. Speech recognition is still a nascent technology that loses much in translation. For dictators to utilize the software, they must first train the program to recognize their spoken words. Dictation is read into the database and the program continuously \"learns\" the spoken words and phrases.\n\nPoor speech habits and other problems such as heavy foreign accents and mumbling complicate the process for both the MT and the recognition software. An MT can \"flag\" such a report as unintelligible, but the recognition software will transcribe the unintelligible word(s) from the existing database of \"learned\" language. The result is often a \"word salad\" or missing text. Thresholds can be set to reject a bad report and return it for standard dictation, but these settings are arbitrary. Below a set percentage rate, the word salad passes for actual dictation. The MT simultaneously listens, reads and \"edits\" the correct version. Every word must be confirmed in this process. The downside of the technology is when the time spent in this process cancels out the benefits. The quality of recognition can range from excellent to poor, with whole words and sentences missing from the report. Not infrequently, negative contractions and the word \"not\" is dropped altogether. These flaws trigger concerns that the present technology could have adverse effects on patient care. Control over quality can also be reduced when providers choose a server-based program from a vendor Application Service Provider (ASP).\n\nDownward adjustments in MT pay rates for voice recognition are controversial. Understandably, a client will seek optimum savings to offset any net costs. Yet vendors that overstate the gains in productivity do harm to MTs paid by the line. Despite the new editing skills required of MTs, significant reductions in compensation for voice recognition have been reported. Reputable industry sources put the field average for increased productivity in the range of 30%-50%; yet this is still dependent on several other factors involved in the methodology. Metrics supplied by vendors that can be \"used\" in compensation decisions should be scientifically supported.\n\nOperationally, speech recognition technology (SRT) is an interdependent, collaborative effort. It is a mistake to treat it as compatible with the same organizational paradigm as standard dictation, a largely \"stand-alone\" system. The new software supplants an MT's former ability to realize immediate time-savings from programming tools such as macros and other word/format expanders. Requests for client/vendor format corrections delay those savings. If remote MTs cancel each other out with disparate style choices, they and the recognition engine may be trapped in a seesaw battle over control. Voice recognition managers should take care to ensure that the impositions on MT autonomy are not so onerous as to outweigh its benefits.\n\nMedical transcription is still the primary mechanism for a physician to clearly communicate with other healthcare providers who access the patient record, to advise them on the state of the patient's health and past/current treatment, and to assure continuity of care. More recently, following Federal and State Disability Act changes, a written report (IME) became a requirement for documentation of a medical bill or an application for Workers' Compensation (or continuation thereof) insurance benefits based on requirements of Federal and State agencies.\n\nAn individual who performs medical transcription is known as a \"medical transcriber\" (\"MT\") or a \"Medical Language Specialist\" (\"MLS\"). The equipment used is called a \"medical transcriber,\" e.g., a cassette player with foot controls operated by the MT for report playback and transcription.\n\nEducation and training can be obtained through certificate or diploma programs, distance learning, and/or on-the-job training offered in some hospitals, although there are countries currently employing transcriptionists that require 18 months to 2 years of specialized MT training. Working in medical transcription leads to a mastery in medical terminology and editing, ability to listen and type simultaneously, utilization of playback controls on the transcriber (machine), and use of foot pedal to play and adjust dictations – all while maintaining a steady rhythm of execution. Medical transcription training normally includes coursework in medical terminology, anatomy, editing, grammar and punctuation, typing, medical record types and formats, and healthcare documentation.\n\nWhile medical transcription does not mandate registration or certification, individual MTs may seek out registration/certification for personal or professional reasons. Obtaining a certificate from a medical transcription training program does not entitle an MT to use the title of Certified Medical Transcriptionist. A Certified Healthcare Documentation Specialist (CHDS) credential can be earned by passing a certification examination conducted solely by the Association for Healthcare Documentation Integrity (AHDI), formerly the American Association for Medical Transcription (AAMT), as the credentialing designation they created. AHDI also offers the credential of Registered Healthcare Documentation Specialist (RHDS). According to AHDI, RHDS is an entry-level credential while the CHDS is an advanced level. AHDI maintains a list of approved medical transcription schools. Generally, certified medical transcriptionists earn more than their non-certified counterparts. It is also notable that training through an educational program that is approved by AHDI will increase the chances of an MT getting certified and getting hired.\n\nThere is a great degree of internal debate about which training program best prepares an MT for industry work. Yet, whether one has learned medical transcription from an online course, community college, high school night course, or on-the-job training in a doctor's office or hospital, a knowledgeable MT is highly valued. In lieu of these AHDI certification credentials, MTs who can consistently and accurately transcribe multiple document work-types and return reports within a reasonable turnaround-time (TAT) are sought after. TATs set by the service provider or agreed to by the transcriptionist should be reasonable but consistent with the need to return the document to the patient's record in a timely manner.\n\nWhile most medical transcription agencies prefer candidates with a minimum of one year experience, formal instruction is not a requirement, and there is no mandatory test. Some hospitals require nothing more than a diploma for employment as a medical transcriptionist. The average pay range for an in-house MT in a hospital setting is $8/hr. However, MT/HDS can expect to receive anywhere from $12-$19 an hour, depending if they are working in a union or non-union environment. This large gap is because the job is based on production, so the more an MT can produce, the more they can make.\n\nOn March 7, 2006, the MT occupation became an eligible U.S. Department of Labor Apprenticeship, a 2-year program focusing on acute care facility (hospital) work. In May 2004, a pilot program for Vermont residents was initiated, with 737 applicants for only 20 classroom pilot-program openings. The objective was to train the applicants as MTs in a shorter time period. (See Vermont HITECH for pilot program established by the Federal Government Health and Human Services Commission).\n\nExperience that is directly related to the duties and responsibilities specified, and dependent on the employer (working directly for a physician or in hospital facility).\n\n\n\nWhen the patient visits a doctor, the latter spends time with the former discussing his medical problems, including history and/or problems. The doctor performs a physical examination and may request various laboratory or diagnostic studies; will make a diagnosis or differential diagnoses, then decides on a plan of treatment for the patient, which is discussed and explained to the patient, with instructions provided. After the patient leaves the office, the doctor uses a voice-recording device to record the information about the patient encounter. This information may be recorded into a hand-held cassette recorder or into a regular telephone, dialed into a central server located in the hospital or transcription service office, which will 'hold' the report for the transcriptionist. This report is then accessed by a medical transcriptionist, it is clearly received as a voice file or cassette recording, who then listens to the dictation and transcribes it into the required format for the medical record, and of which this medical record is considered a legal document. The next time the patient visits the doctor, the doctor will call for the medical record or the patient's entire chart, which will contain all reports from previous encounters. The doctor can on occasion refill the patient's medications after seeing only the medical record, although doctors prefer to not refill prescriptions without seeing the patient to establish if anything has changed.\n\nIt is very important to have a properly formatted, edited, and reviewed medical transcription document. If a medical transcriptionist accidentally typed a wrong medication or the wrong diagnosis, the patient could be at risk if the doctor (or his designee) did not review the document for accuracy. Both the doctor and the medical transcriptionist play an important role to make sure the transcribed dictation is correct and accurate. The doctor should speak slowly and concisely, especially when dictating medications or details of diseases and conditions. The medical transcriptionist must possess hearing acuity, medical knowledge, and good reading comprehension in addition to checking references when in doubt.\n\nHowever, some doctors do not review their transcribed reports for accuracy, and the computer attaches an electronic signature with the disclaimer that a report is \"dictated but not read\". This electronic signature is readily acceptable in a legal sense. The transcriptionist is bound to transcribe verbatim (exactly what is said) and make no changes, but has the option to flag any report inconsistencies. On some occasions, the doctors do not speak clearly, or voice files are garbled. Some doctors are, unfortunately, time-challenged and need to dictate their reports quickly (as in ER Reports). In addition, there are many regional or national accents and (mis)pronunciations of words the MT must contend with. It is imperative and a large part of the job of the Transcriptionist to look up the correct spelling of complex medical terms, medications, obvious dosage or dictation errors, and when in doubt should \"flag\" a report. A \"flag\" on a report requires the dictator (or his designee) to fill in a blank on a finished report, which has been returned to him, before it is considered complete. Transcriptionists are never, ever permitted to guess, or 'just put in anything' in a report transcription. Furthermore, medicine is constantly changing. New equipment, new medical devices, and new medications come on the market on a daily basis, and the Medical Transcriptionist needs to be creative and to tenaciously research (quickly) to find these new words. An MT needs to have access to, or keep on memory, an up-to-date library to quickly facilitate the insertion of a correctly spelled device.\n\nMedical transcription editing is the process of listening to a voice-recorded file and comparing that to the transcribed report of that audio file, correcting errors as needed. Although speech recognition technology has become better at understanding human language, editing is still needed to ensure better accuracy. Medical transcription editing is also performed on medical reports transcribed by medical transcriptionists.\n\nRecent advances in speech recognition technology have shifted the job responsibilities of medical transcriptionists from not only transcribing but also editing. Editing has always been a part of the medical transcriptionist job, however now editing is a larger requirement as reports are more often being transcribed electronically. With different accents, articulations, and pronunciations, speech recognition technology can still have problems deciphering words. This is where the medical transcriptionist editor steps in. Medical transcription editors will compare and correct the transcribed file to the voice-recorded audio file. The job is similar to medical transcription as editing will use a foot pedal and the education and training requirements are mostly the same.\n\nEducation and training requirements for medical transcription editors is very similar to that of medical transcriptionists. Many medical transcription editors start out as medical transcriptionists and transition to editing. Several of the AHDI-approved medical transcription schools have seen the need for medical transcription editing training and have incorporated editing in their training programs. Quality training will be the key to your success as a Medical Transcription / Healthcare Documentation Specialist. It is also very important to get work experience while training to ensure employers will be willing to hire freshly graduated students. Students who receive 'real world' training are much better suited for the medical transcription industry, than those who do not.\n\nDue to the increasing demand to document medical records, countries have started to outsource the services of medical transcription. The global medical transcription services market was valued at USD 41.4 Billion in 2012 and is expected to grow at a CAGR of 5.6% from 2013 to 2019, to reach an estimated value of USD 60.6 Billion in 2019. The main reason for outsourcing is stated to be the cost advantage due to cheap labor in developing countries, and their currency rates as compared to the U.S. dollar. Drivers that Influence Outsourcing to Medical Transcription Partners.\n\nThere is a volatile controversy on whether medical transcription work should be outsourced, mainly due to three reasons:\n\n\n"}
{"id": "19391", "url": "https://en.wikipedia.org/wiki?curid=19391", "title": "Midwifery", "text": "Midwifery\n\nMidwifery is the health science and health profession that deals with pregnancy, childbirth, and the postpartum period (including care of the newborn), in addition to the sexual and reproductive health of women throughout their lives. In many countries, midwifery is a medical profession (special for its independent and direct specialized education; should not be confused with the medical specialty, which depends on a previous general training). A professional in midwifery is known as a midwife.\n\nA 2013 Cochrane review concluded that \"most women should be offered midwifery-led continuity models of care and women should be encouraged to ask for this option although caution should be exercised in applying this advice to women with substantial medical or obstetric complications.\" The review found that midwifery-led care was associated with a reduction in the use of epidurals, with fewer episiotomies or instrumental births, and a decreased risk of losing the baby before 24 weeks' gestation. However, midwifery-led care was also associated with a longer mean length of labor as measured in hours.\n\nTrimester means \"3 months.\" A normal pregnancy lasts about 9 months and has 3 trimesters.\n\nFirst trimester screening varies by country. Women are typically offered a Pap smear and urine analysis (UA), and blood tests including a complete blood count (CBC), blood typing (including Rh screen), syphilis, hepatitis, HIV, and rubella testing. Additionally, women may have chlamydia testing via a urine sample, and women considered at high risk are screened for Sickle Cell disease and Thalassemia. Women must consent to all tests before they are carried out. The woman's blood pressure, height and weight are measured. Her past pregnancies and family, social, and medical history are discussed. Women may have an ultrasound scan during the first trimester which may be used to help find the estimated due date. Some women may have genetic testing, such as screening for Down's Syndrome. Diet, exercise, and discomforts such as morning sickness are discussed.\nThe mother visits the midwife monthly or more often during the second trimester. The mother's partner and/or the labor coach may accompany her. The midwife will discuss pregnancy issues such as fatigue, heartburn, varicose veins, and other common problems such as back pain.\nBlood pressure and weight are monitored and the midwife measures the mother's abdomen to see if the baby is growing as expected. Lab tests such as a UA, CBC, and glucose tolerance test are done if the midwife feels they are necessary.\n\nIn the third trimester the midwife will see the mother every two weeks until week 36 and every week after that. Weight, blood pressure, and abdominal measurements will continue to be done. Lab tests such as a CDC and UA may be done with additional testing done for at-risk pregnancies. The midwife palpates the woman's abdomen to establish the lie, presentation and position of the fetus and later, the engagement. A pelvic exam may be done to see if the mother's cervix is dilating. The midwife and the mother discuss birthing options and write a birth care plan.\n\nMidwives are qualified to assist with a normal vaginal delivery while more complicated deliveries are handled by a health care provider who has had further training. Childbirth is divided into four stages. \n\nFollowing the birth, if the mother had an episiotomy or a tearing of the perineum, it is stitched. The midwife does regular assessments for uterine contraction, fundal height, and vaginal bleeding. Throughout labor and delivery the mother's vital signs (temperature, blood pressure, and pulse) are closely monitored and her fluid intake and output are measured. The midwife also monitors the baby's pulse rate, palpates the mother's abdomen to monitor the baby's position, and does vaginal checks as needed. If the birth deviates from the norm at any stage, the midwife requests assist from a more highly trained health care provider.\n\nUntil the last century most women have used both the upright position and alternative positions to give birth. The lithotomy position was not used until the advent of forceps in the seventeenth century and since then childbirth has progressively moved from a woman supported experience in the home to a medical intervention within the hospital.\nThere are significant advantages to assuming an upright position in labor and birth, such as stronger and more efficient uterine contractions aiding cervical dilatation, increased pelvic inlet and outlet diameters and improved uterine contractility. Upright positions in the second stage include sitting, squatting, kneeling, and being on hands and knees.\n\nFor women who have a hospital birth, the minimum hospital stay is six hours. Women who leave before this do so against medical advice. Women may choose when to leave the hospital. Full postnatal assessments are conducted daily whilst inpatient, or more frequently if needed. A postnatal assessment includes the woman's observations, general well being, breasts (either a discussion and assistance with breastfeeding or a discussion about lactation suppression), abdominal palpation (if she has not had a caesarean section) to check for involution of the uterus, or a check of her caesarean wound (the dressing doesn't need to be removed for this), a check of her perineum, particularly if she tore or had stitches, reviewing her lochia, ensuring she has passed urine and had her bowels open and checking for signs and symptoms of a DVT. The baby is also checked for jaundice, signs of adequate feeding, or other concerns. The baby has a nursery exam between six and seventy two hours of birth to check for conditions such as heart defects, hip problems, or eye problems.\n\nIn the community, the community midwife sees the woman at least until day ten. This does not mean she sees the woman and baby daily, but she cannot discharge them from her care until day ten at the earliest. Postnatal checks include neonatal screening test (NST, or heel prick test) around day five. The baby is weighed and the midwife plans visits according to the health and needs of mother and baby. They are discharged to the care of the health visitor.\n\nAt birth, the baby receives an Apgar score at, at the least, one minute and five minutes of age. This is a score out of 10 that assesses the baby on five different areas—each worth between 0 and 2 points. These areas are: colour, respiratory effort, tone, heart rate, and response to stimuli. The midwife checks the baby for any obvious problems, weighs the baby, and measure head circumference. The midwife ensures the cord has been clamped securely and the baby has the appropriate name tags on (if in hospital). Babies lengths are not routinely measured. The midwife performs these checks as close to the mother as possible and returns the baby to the mother quickly. Skin-to-skin is encouraged, as this regulates the baby's heart rate, breathing, oxygen saturation, and temperature—and promotes bonding and breastfeeding.\n\nIn some countries, such as Chile, the midwife is the professional who can direct neonatal intensive care units. This is an advantage for these professionals, because this professionals can use the knowledge in perinatology to bring a high quality care of the newborn, with medical or surgical conditions.\n\nMidwifery-led continuity of care is where one or more midwives have the primary responsibility for the continuity of care for childbearing women, with a multidisciplinary network of consultation and referral with other health care providers. This is different from \"medical-led care\" where an obstetrician or family physician is primarily responsible. In \"shared-care\" models, responsibility may be shared between a midwife, an obstetrician and/or a family physician. The midwife plays a very unique role is part of very intimate situations with the mother. For this reason, many say that the most important thing to look for in a midwife is comfortability with them, as one will go to them with every question or problem.\n\nAccording to a Cochrane review of public health systems in Australia, Canada, Ireland, New Zealand and the United Kingdom, \"most women should be offered midwifery-led continuity models of care and women should be encouraged to ask for this option although caution should be exercised in applying this advice to women with substantial medical or obstetric complications.\" Midwifery-led care has effects including the following:\nThere was no difference in the number of Caesarean sections. All trials in the Cochrane review included licensed midwives, and none included lay or traditional midwives. Also, no trial included out of hospital birth.\n\nIn ancient Egypt, midwifery was a recognized female occupation, as attested by the Ebers Papyrus which dates from 1900 to 1550 BCE. Five columns of this papyrus deal with obstetrics and gynecology, especially concerning the acceleration of parturition (the action or process of giving birth to offspring) and the birth prognosis of the newborn. The Westcar papyrus, dated to 1700 BCE, includes instructions for calculating the expected date of confinement and describes different styles of birth chairs. Bas reliefs in the royal birth rooms at Luxor and other temples also attest to the heavy presence of midwifery in this culture.\n\nMidwifery in Greco-Roman antiquity covered a wide range of women, including old women who continued folk medical traditions in the villages of the Roman Empire, trained midwives who garnered their knowledge from a variety of sources, and highly trained women who were considered physicians. However, there were certain characteristics desired in a “good” midwife, as described by the physician Soranus of Ephesus in the 2nd century. He states in his work, \"Gynecology\", that “a suitable person will be literate, with her wits about her, possessed of a good memory, loving work, respectable and generally not unduly handicapped as regards her senses [i.e., sight, smell, hearing], sound of limb, robust, and, according to some people, endowed with long slim fingers and short nails at her fingertips.” Soranus also recommends that the midwife be of sympathetic disposition (although she need not have borne a child herself) and that she keep her hands soft for the comfort of both mother and child. Pliny, another physician from this time, valued nobility and a quiet and inconspicuous disposition in a midwife. There appears to have been three “grades” of midwives present: The first was technically proficient; the second may have read some of the texts on obstetrics and gynecology; but the third was highly trained and reasonably considered a medical specialist with a concentration in midwifery.\n\nAgnodice or Agnodike (Gr. Ἀγνοδίκη) was the earliest historical, and likely apocryphal, midwife mentioned among the ancient Greeks.\n\nMidwives were known by many different titles in antiquity, ranging from \"iatrinē\" (Gr. nurse), \"maia\" (Gr., midwife), \"obstetrix\" (Lat., obstetrician), and \"medica\" (Lat., doctor). It appears as though midwifery was treated differently in the Eastern end of the Mediterranean basin as opposed to the West. In the East, some women advanced beyond the profession of midwife (\"maia\") to that of gynaecologist (\"iatros gynaikeios\", translated as \"women's doctor\"), for which formal training was required. Also, there were some gynecological tracts circulating in the medical and educated circles of the East that were written by women with Greek names, although these women were few in number. Based on these facts, it would appear that midwifery in the East was a respectable profession in which respectable women could earn their livelihoods and enough esteem to publish works read and cited by male physicians. In fact, a number of Roman legal provisions strongly suggest that midwives enjoyed status and remuneration comparable to that of male doctors. One example of such a midwife is Salpe of Lemnos, who wrote on women’s diseases and was mentioned several times in the works of Pliny.\n\nHowever, in the Roman West, information about practicing midwives comes mainly from funerary epitaphs. Two hypotheses are suggested by looking at a small sample of these epitaphs. The first is the midwifery was not a profession to which freeborn women of families that had enjoyed free status of several generations were attracted; therefore it seems that most midwives were of servile origin. Second, since most of these funeral epitaphs describe the women as freed, it can be proposed that midwives were generally valued enough, and earned enough income, to be able to gain their freedom. It is not known from these epitaphs how certain slave women were selected for training as midwives. Slave girls may have been apprenticed, and it is most likely that mothers taught their daughters.\n\nThe actual duties of the midwife in antiquity consisted mainly of assisting in the birthing process, although they may also have helped with other medical problems relating to women when needed. Often, the midwife would call for the assistance of a physician when a more difficult birth was anticipated. In many cases the midwife brought along two or three assistants. In antiquity, it was believed by both midwives and physicians that a normal delivery was made easier when a woman sat upright. Therefore, during parturition, midwives brought a stool to the home where the delivery was to take place. In the seat of the birthstool was a crescent-shaped hole through which the baby would be delivered. The birthstool or chair often had armrests for the mother to grasp during the delivery. Most birthstools or chairs had backs which the patient could press against, but Soranus suggests that in some cases the chairs were backless and an assistant would stand behind the mother to support her. The midwife sat facing the mother, encouraging and supporting her through the birth, perhaps offering instruction on breathing and pushing, sometimes massaging her vaginal opening, and supporting her perineum during the delivery of the baby. The assistants may have helped by pushing downwards on the top of the mother's abdomen.\n\nFinally, the midwife received the infant, placed it in pieces of cloth, cut the umbilical cord, and cleansed the baby. The child was sprinkled with “fine and powdery salt, or natron or aphronitre” to soak up the birth residue, rinsed, and then powdered and rinsed again. Next, the midwives cleared away any and all mucus present from the nose, mouth, ears, or anus. Midwives were encouraged by Soranus to put olive oil in the baby’s eyes to cleanse away any birth residue, and to place a piece of wool soaked in olive oil over the umbilical cord. After the delivery, the midwife made the initial call on whether or not an infant was healthy and fit to rear. She inspected the newborn for congenital deformities and testing its cry to hear whether or not it was robust and hearty. Ultimately, midwives made a determination about the chances for an infant’s survival and likely recommended that a newborn with any severe deformities be exposed.\n\nA 2nd-century terracotta relief from the Ostian tomb of Scribonia Attice, wife of physician-surgeon M. Ulpius Amerimnus, details a childbirth scene. Scribonia was a midwife and the relief shows her in the midst of a delivery. A patient sits in the birth chair, gripping the handles and the midwife’s assistant stands behind her providing support. Scribonia sits on a low stool in front of the woman, modestly looking away while also assisting the delivery by dilating and massaging the vagina, as encouraged by Soranus.\n\nThe services of a midwife were not inexpensive; this fact that suggests poorer women who could not afford the services of a professional midwife often had to make do with female relatives. Many wealthier families had their own midwives. However, the vast majority of women in the Greco-Roman world very likely received their maternity care from hired midwives. They may have been highly trained or possessed only a rudimentary knowledge of obstetrics. Also, many families had a choice of whether or not they wanted to employ a midwife who practiced the traditional folk medicine or the newer methods of professional parturition. Like a lot of other factors in antiquity, quality gynecological care often depended heavily on the socioeconomic status of the patient.\n\nFrom the 18th century, a conflict between surgeons and midwives arose, as medical men began to assert that their modern scientific techniques were better for mothers and infants than the folk medicine practiced by midwives. \nAs doctors and medical associations pushed for a legal monopoly on obstetrical care, midwifery became outlawed or heavily regulated throughout the United States and Canada.. In Northern Europe and Russia the situation was a little easier - in Imperial Russia at the Duchy of Estonia, Professor Christian Friedrich Deutsch established a midwifery school for women at the University of Dorpat in 1811, which existed until World War I. It was the predecessor for the Tartu Health Care College. Training lasted for 7 months and in the end a certificate for practice was issued to the female students. Despite accusations that midwives were \"incompetent and ignorant\", some argued that poorly trained surgeons were far more of a danger to pregnant women. The argument that surgeons were more dangerous than midwives lasted until the study of bacteriology became popular in the early 1900s. Women began to feel safer in the setting of the hospitals with the amount of aid and the ease of birth that they experienced with doctors. “Physicians trained in the new century found a great contrast between their hospital and obstetrics practice in women’s homes where they could not maintain sterile conditions or have trained help.” German social scientists Gunnar Heinsohn and Otto Steiger theorize that midwifery became a target of persecution and repression by public authorities because midwives possessed highly specialized knowledge and skills regarding not only assisting birth, but also contraception and abortion.\n\nAt late 20th century, midwives were already recognized as highly trained and specialized professionals in obstetrics. However, at the beginning of the 21st century, the medical perception of pregnancy and childbirth as potentially pathological and dangerous still dominates Western culture. Midwives who work in hospital settings also have been influenced by this view, although by and large they are trained to view birth as a normal and healthy process. While midwives play a much larger role in the care of pregnant mothers in Europe than in America, the medicalized model of birth still has influence in those countries, even though the World Health Organization recommends a natural, normal and humanized birth.\n\nThe midwifery model of pregnancy and childbirth as a normal and healthy process plays a much larger role in Sweden and the Netherlands than the rest of Europe, however. Swedish midwives stand out, since they administer 80 percent of prenatal care and more than 80 percent of family planning services in Sweden. Midwives in Sweden attend all normal births in public hospitals and Swedish women tend to have fewer interventions in hospitals than American women. The Dutch infant mortality rate in 1992 was the tenth-lowest rate in the world, at 6.3 deaths per thousand births, while the United States ranked twenty-second. Midwives in the Netherlands and Sweden owe a great deal of their success to supportive government policies.\n\nNotes\nBibliography\n\n\n"}
{"id": "28328131", "url": "https://en.wikipedia.org/wiki?curid=28328131", "title": "Multi-barrier approach", "text": "Multi-barrier approach\n\nThe Multi-barrier approach is a key paradigm for ensuring safe drinking water in jurisdictions such as Ontario, elsewhere in Canada, and New Zealand. It is defined as, An integrated system of procedures, processes and tools that collectively prevent or reduce the contamination of drinking water from source to tap in order to reduce risks to public health.\n\nIn Part 2 of his report on the Walkerton Tragedy, Justice Dennis O'Connor discusses five elements of the multi-barrier approach: \n\nThe holistic perspective of the multi-barrier approach also incorporates many players into the goal of keeping drinking water safe. These include managers, researchers, regulators, legislators, and the public.\n\n\n"}
{"id": "525804", "url": "https://en.wikipedia.org/wiki?curid=525804", "title": "Natural childbirth", "text": "Natural childbirth\n\nNatural childbirth is childbirth without routine medical interventions, particularly anesthesia. Natural childbirth arose in opposition to the techno-medical model of childbirth that has recently gained popularity in industrialized societies. Natural childbirth attempts to minimize medical intervention, particularly the use of anesthetic medications and surgical interventions such as episiotomies, forceps and ventouse deliveries and caesarean sections. Natural childbirth may occur during a physician or midwife attended hospital birth, a midwife attended homebirth, or an unassisted birth. The term \"natural childbirth\" was coined by obstetrician Grantly Dick-Read upon publication of his book \"Natural Childbirth\" in the 1930s, which was followed by the 1942 \"Childbirth Without Fear\".\n\nHistorically, most women gave birth at home without emergency medical care available. The \"natural\" rate of maternal mortality—meaning without surgical or pharmaceutical intervention—has been estimated at 1,500 per 100,000 births. In the United States circa 1900, before the introduction and improvement of modern medical technologies, there were about 700 maternal deaths per 100,000 births (.7%).\n\nAt the onset of the Industrial Revolution, giving birth at home became more difficult due to congested living spaces and dirty living conditions. This drove urban and lower class women to newly available hospitals, while wealthy and middle-class women continued to labor at home. In the early 1900s there was an increasing availability of hospitals, and more women began going into the hospital for labor and delivery. In the United States, the middle classes were especially receptive to the medicalization of childbirth, which promised a safer and less painful labor. The ability to birth without pain was part of the early feminist movement. The use of childbirth drugs began in 1847 when Scottish obstetrician James Young Simpson introduced chloroform as an anesthetic during labor, but only the most rich and powerful women (such as Queen Victoria) had access. In the late 1800s, feminists in the United States and United Kingdom began to demand drugs for pain relief during childbirth.\n\nThe term \"natural childbirth\" was coined by obstetrician Grantly Dick-Read upon publication of his book \"Natural Childbirth\" in the 1930s. In 1942 Grantly Dick-Read published \"Revelation of Childbirth\" (which was later retitled \"Childbirth without Fear\"), advocating natural childbirth, which became an international bestseller. The Lamaze method of natural childbirth gained popularity in the United States after Marjorie Karmel wrote about her experiences in her 1959 book \"Thank You, Dr. Lamaze\", and with the formation of the American Society for Psychoprophylaxis in Obstetrics (currently Lamaze International) by Marjorie Karmel and Elisabeth Bing. Later, physicians Michel Odent and Frederick Leboyer and midwives such as Ina May Gaskin promoted birthing centers, water birth, and homebirth as alternatives to the hospital model. The Bradley method of natural childbirth (also known as \"husband-coached childbirth\"), a method of natural childbirth developed in 1947 by Robert A. Bradley, M.D., was popularized by his book \"Husband-Coached Childbirth\", first published in 1965.\n\nMany women consider natural birth empowering. Studies show that skin-to-skin contact between a mother and her newborn immediately after birth is beneficial for both mother and baby. A review done by the World Health Organization found that skin-to-skin contact between mothers and babies after birth reduces crying, improves mother-infant interaction, and helps mothers to breastfeed successfully. They recommend that neonates be allowed to bond with the mother during their first two hours after birth, the period that they tend to be more alert than in the following hours of early life.\n\nInstead of medical interventions, a variety of non-invasive methods are employed during natural childbirth to ease the mother's pain. Many of these techniques stress the importance of \"a mind-body connection,\" which the techno-medical model of birth does not. These techniques include hydrotherapy, massage, relaxation therapy, hypnosis, breathing exercises, acupressure for labor, transcutaneous electrical nerve stimulation (TENS), vocalization, visualization, mindfulness and water birth. Other approaches include movement, walking, and different positions (for example, using a birthing ball), hot and cold therapy (for example, using hot compresses and/or cold packs), and receiving one-on-one labor support like that provided by a midwife or doula. However, natural childbirth proponents maintain that pain is a natural and necessary part of the labor process, and should not automatically be regarded as entirely negative. In contrast to the pain of injury and disease, they believe that the pain of childbirth is a sign that the female body is functioning as it is meant to.\n\nBirth positions favored in natural childbirth—including squatting, hands and knees, or suspension in water—contrast with the lithotomy position (woman in hospital bed on her back with legs in stirrups), which has consistently been shown to slow and complicate labor.\n\nMethods to reduce tearing during natural childbirth (instead of an episiotomy) include managing the perineum with counter-pressure, hot compresses, and pushing the baby out slowly.\n\nSome women take birth education classes such as Lamaze or the Bradley Method to prepare for a natural childbirth. Several books are also available with information to help women prepare. A midwife or doula may include preparation for a natural birth as part of the prenatal care services. However, a study published in 2009 suggests that preparation alone is not enough to ensure an intervention-free outcome.\n\nIn the U.S in 2007: 93% of mothers used electronic fetal monitoring; 63% used\nepidurals; 55% had their membranes ruptured; 53% received oxytocin to stimulate labor progress; and 52% received episiotomies.\n\nSome studies argue that the push towards 'natural childbirth' in Western countries is exaggerated, and can harm women.\n\n\n"}
{"id": "66575", "url": "https://en.wikipedia.org/wiki?curid=66575", "title": "Nutrient", "text": "Nutrient\n\nA nutrient is a substance used by an organism to survive, grow, and reproduce. The requirement for dietary nutrient intake applies to animals, plants, fungi, and protists. Nutrients can be incorporated into cells for metabolic purposes or excreted by cells to create non-cellular structures, such as hair, scales, feathers, or exoskeletons. Some nutrients can be metabolically converted to smaller molecules in the process of releasing energy, such as for carbohydrates, lipids, proteins, and fermentation products (ethanol or vinegar), leading to end-products of water and carbon dioxide. All organisms require water. Essential nutrients for animals are the energy sources, some of the amino acids that are combined to create proteins, a subset of fatty acids, vitamins and certain minerals. Plants require more diverse minerals absorbed through roots, plus carbon dioxide and oxygen absorbed through leaves. Fungi live on dead or living organic matter and meet nutrient needs from their host.\n\nDifferent types of organism have different essential nutrients. Ascorbic acid (vitamin C) is essential, meaning it must be consumed in sufficient amounts, to humans and some other animal species, but not to all animals and not to plants, which are able to synthesize it. Nutrients may be organic or inorganic: organic compounds include most compounds containing carbon, while all other chemicals are inorganic. Inorganic nutrients include nutrients such as iron, selenium, and zinc, while organic nutrients include, among many others, energy-providing compounds and vitamins.\n\nA classification used primarily to describe nutrient needs of animals divides nutrients into macronutrients and micronutrients. Consumed in relatively large amounts (grams or ounces), macronutrients (carbohydrates, fats, proteins, water) are used primarily to generate energy or to incorporate into tissues for growth and repair. Micronutrients are needed in smaller amounts (milligrams or micrograms); they have subtle biochemical and physiological roles in cellular processes, like vascular functions or nerve conduction. Inadequate amounts of essential nutrients, or diseases that interfere with absorption, result in a deficiency state that compromises growth, survival and reproduction. Consumer advisories for dietary nutrient intakes, such as the United States Dietary Reference Intake, are based on deficiency outcomes and provide macronutrient and micronutrient guides for both lower and upper limits of intake. In many countries, macronutrients and micronutrients in significant content are required by regulations to be displayed on food product labels. Nutrients in larger quantities than the body needs may have harmful effects. Edible plants also contain thousands of compounds generally called phytochemicals which have unknown effects on disease or health, including a diverse class with non-nutrient status called polyphenols, which remain poorly understood as of 2017.\n\nPlant nutrients consist of more than a dozen minerals absorbed through roots, plus carbon dioxide and oxygen absorbed or released through leaves. All organisms obtain all their nutrients from the surrounding environment.\n\nPlants absorb carbon, hydrogen and oxygen from air. These three, in the form of water and carbon dioxide. Other nutrients are absorbed from soil (exceptions include some parasitic or carnivorous plants). Counting these, there are 17 important nutrients for plants: the macronutrients nitrogen (N), phosphorus (P), potassium (K), calcium (Ca), sulfur (S), magnesium (Mg), carbon (C), oxygen(O) and hydrogen (H), and the micronutrients iron (Fe), boron (B), chlorine (Cl), manganese (Mn), zinc (Zn), copper (Cu), molybdenum (Mo) and nickel (Ni). In addition to carbon, hydrogen and oxygen, nitrogen, phosphorus, and sulfur are also needed in relatively large quantities. Together, the \"Big Six\" are the elemental macronutrients for all organisms.\nThey are sourced from inorganic matter (for example, carbon dioxide, water, nitrates, phosphates, sulfates, and diatomic molecules of nitrogen and, especially, oxygen) and organic matter (carbohydrates, lipids, proteins).\n\nMacronutrients are defined in several ways.\n\nMacronutrients provide energy:\n\n\nFat has an energy content of 9kcal/g (~37.7 kJ/g) and proteins and carbohydrates 4kcal/g (~16.7 kJ/g).\n\nMicronutrients support metabolism.\n\n\nAn essential nutrient is a nutrient required for normal physiological function that cannot be synthesized in the body – either at all or in sufficient quantities – and thus must be obtained from a dietary source. Apart from water, which is universally required for the maintenance of homeostasis in mammals, essential nutrients are indispensable for various cellular metabolic processes and maintaining tissue and organ function. In the case of humans, there are nine amino acids, two fatty acids, thirteen vitamins and fifteen minerals that are considered essential nutrients. In addition, there are several molecules that are considered conditionally essential nutrients since they are indispensable in certain developmental and pathological states.\n\nAn essential amino acid is an amino acid that is required by an organism but cannot be synthesized \"de novo\" by it, and therefore must be supplied in its diet. Out of the twenty standard protein-producing amino acids, nine cannot be endogenously synthesized by humans: phenylalanine, valine, threonine, tryptophan, methionine, leucine, isoleucine, lysine, and histidine.\n\nEssential fatty acids (EFAs) are fatty acids that humans and other animals must ingest because the body requires them for good health but cannot synthesize them. Only two fatty acids are known to be essential for humans: alpha-linolenic acid (an omega-3 fatty acid) and linoleic acid (an omega-6 fatty acid).\n\nVitamins are organic molecules essential for an organism that are not classified as amino acids or fatty acids. They commonly function as enzymatic cofactors, metabolic regulators or antioxidants. Humans require thirteen vitamins in their diet, most of which are actually groups of related molecules (e.g. vitamin E includes tocopherols and tocotrienols): vitamins A, C, D, E, K, thiamine (B), riboflavin (B), niacin (B), pantothenic acid (B), vitamin B (e.g., pyridoxine), biotin (B), folate (B), and cobalamin (B). The requirement for vitamin D is conditional, as people who get sufficient exposure to ultraviolet light, either from the sun or an artificial source, synthesize vitamin D in the skin.\n\nMinerals are the exogenous chemical elements indispensable for life. Although the four elements: carbon, hydrogen, oxygen, and nitrogen, are essential for life, they are so plentiful in food and drink that these are not considered nutrients and there are no recommended intakes for these as minerals. The need for nitrogen is addressed by requirements set for protein, which is composed of nitrogen-containing amino acids. Sulfur is essential, but again does not have a recommended intake. Instead, recommended intakes are identified for the sulfur-containing amino acids methionine and cysteine.\n\nThe essential nutrient elements for humans, listed in order of Recommended Dietary Allowance (expressed as a mass), are potassium, chlorine, sodium, calcium, phosphorus, magnesium, iron, zinc, manganese, copper, iodine, chromium, molybdenum, selenium and cobalt (the last as a component of vitamin B). There are other minerals which are essential for some plants and animals, but may or may not be essential for humans, such as boron and silicon.\n\nConditionally essential nutrients are certain organic molecules that can normally be synthesized by an organism, but under certain conditions in insufficient quantities. In humans, such conditions include premature birth, limited nutrient intake, rapid growth, and certain disease states. Choline, inositol, taurine, arginine, glutamine and nucleotides are classified as conditionally essential and are particularly important in neonatal diet and metabolism.\n\nNon-essential nutrients are substances within foods that can have a significant impact on health; these substances can be beneficial or toxic. For example, dietary fiber is not absorbed in the human digestive tract, but is important in maintaining the bulk of a bowel movement to avoid constipation. A subset of dietary fiber, soluble fiber, can be metabolized by bacteria residing in the large intestine. Soluble fiber is marketed as serving a prebiotic functionpromoting \"healthy\" intestinal bacteria. Bacterial metabolism of soluble fiber also produces short-chain fatty acids like butyric acid, which may be absorbed into intestinal cells as a source of calories.\n\nEthanol (CHOH) supplies calories. For spirits (vodka, gin, rum, etc.) a standard serving in the United States is , which at 40%ethanol (80proof) would be 14 grams and 98 calories. At 50%alcohol, 17.5grams and 122.5calories. Wine and beer contain a similar amount of ethanol in servings of and , respectively, but these beverages also contain non-ethanol calories. A 5-ounce serving of wine contains 100 to 130 calories. A 12-ounce serving of beer contains 95 to 200 calories. According to the U.S. Department of Agriculture, based on NHANES 2013–2014 surveys, women ages 20 and up consume on average 6.8grams of alcohol per day and men consume on average 15.5 grams per day. Ignoring the non-alcohol contribution of those beverages, the average ethanol calorie contributions are 48 and 108 cal/day, respectively. Alcoholic beverages are considered empty calorie foods because, other than calories, they contribute no essential nutrients.\n\nBy definition, phytochemicals include all nutritional and non-nutritional components of edible plants. Included as nutritional constituents are provitamin A carotenoids, whereas those without nutrient status are diverse polyphenols, flavonoids, resveratrol, and lignans – often claimed to have antioxidant effects – that are present in numerous plant foods. A number of phytochemical compounds are under preliminary research for their potential effects on human diseases and health. However, the qualification for nutrient status of compounds with poorly defined properties \"in vivo\" is that they must first be defined with a Dietary Reference Intake level to enable accurate food labeling, a condition not established for most phytochemicals that are claimed to be antioxidant nutrients.\n\n\"See Vitamin, Mineral (nutrient), Protein (nutrient)\"\n\nAn inadequate amount of a nutrient is a deficiency. Deficiencies can be due to a number of causes including an inadequacy in nutrient intake, called a dietary deficiency, or any of several conditions that interfere with the utilization of a nutrient within an organism. Some of the conditions that can interfere with nutrient utilization include problems with nutrient absorption, substances that cause a greater than normal need for a nutrient, conditions that cause nutrient destruction, and conditions that cause greater nutrient excretion. Nutrient toxicity occurs when excess consumption of a nutrient does harm to an organism.\n\nIn the United States and Canada, recommended dietary intake levels of essential nutrients are based on the minimum level that \"will maintain a defined level of nutriture in an individual\", a definition somewhat different from that used by the World Health Organization and Food and Agriculture Organization of a \"basal requirement to indicate the level of intake needed to prevent pathologically relevant and clinically detectable signs of a dietary inadequacy\".\n\nIn setting human nutrient guidelines, government organizations do not necessarily agree on amounts needed to avoid deficiency or maximum amounts to avoid the risk of toxicity. For example, for vitamin C, recommended intakes range from 40 mg/day in India to 155 mg/day for the European Union. The table below shows U.S. Estimated Average Requirements (EARs) and Recommended Dietary Allowances (RDAs) for vitamins and minerals, PRIs for the European Union (same concept as RDAs), followed by what three government organizations deem to be the safe upper intake. RDAs are set higher than EARs to cover people with higher than average needs. Adequate Intakes (AIs) are set when there is not sufficient information to establish EARs and RDAs. Governments are slow to revise information of this nature. For the U.S. values, with the exception of calcium and vitamin D, all of the data date to 1997-2004.\n\n\nEAR U.S. Estimated Average Requirements.\n\nRDA U.S. Recommended Dietary Allowances; higher for adults than for children, and may be even higher for women who are pregnant or lactating.\n\nAI U.S. Adequate Intake; AIs established when there is not sufficient information to set EARs and RDAs.\n\nPRI Population Reference Intake is European Union equivalent of RDA; higher for adults than for children, and may be even higher for women who are pregnant or lactating. For Thiamin and Niacin the PRIs are expressed as amounts per MJ of calories consumed. MJ = megajoule = 239 food calories.\n\nUpper Limit Tolerable upper intake levels.\n\nND ULs have not been determined.\n\nNE EARs, PRIs or AIs have not yet been established or will not be (EU does not consider chromium an essential nutrient).\n\n"}
{"id": "34730803", "url": "https://en.wikipedia.org/wiki?curid=34730803", "title": "Nutrition and cognition", "text": "Nutrition and cognition\n\nRelatively speaking, the brain consumes an immense amount of energy in comparison to the rest of the body. The mechanisms involved in the transfer of energy from foods to neurons are likely to be fundamental to the control of brain function. Human bodily processes, including the brain, all require both macronutrients, as well as micronutrients.\n\nInsufficient intake of selected vitamins, or certain metabolic disorders, may affect cognitive processes by disrupting the nutrient-dependent processes within the body that are associated with the management of energy in neurons, which can subsequently affect synaptic plasticity, or the ability to encode new memories.\n\nCholine is an essential nutrient and its primary function within the human body is the synthesis of cellular membranes, although it serves other functions as well. It is a precursor molecule to the neurotransmitter Acetylcholine which serves a wide range of functions including motor control and memory. Choline itself has also been shown to have additional health benefits in relation to memory and choline deficiencies may be related to some liver and neurological disorders. Because of its role in cellular synthesis, choline is an important nutrient during the prenatal and early postnatal development of offspring as it contributes heavily to the development of the brain. Despite the wide range of foods that choline is found in, studies have shown that the mean choline intake of men, women and children are below the Adequate Intake levels. Women, especially pregnant or lactating women, older people, and infants, are especially at risk for choline deficiency.\n\nB vitamins, also known as the B-complex, are an interrelated group of nutrients which often co-occur in food. The complex consists of: thiamine (B), riboflavin (B), niacin (B), pantothenic acid (B), pyridoxin (B), folic acid (B), cobalamin (B), and biotin. B vitamins are not synthesized in the body, and thus need to be obtained from food. B-complex vitamins are water-soluble vitamins, which means that they are not stored within the body. In consequence, the B vitamins need ongoing replenishment. It is possible to identify broad cognitive effects of certain B vitamins, as they are involved in many significant metabolic processes within the brain.\n\nThis vitamin is important for the facilitation of glucose use, thus ensuring the production of energy for the brain, and normal functioning of the nervous system, muscles and heart. Thiamine is found throughout mammalian nervous tissue, including the brain and spinal cord. Metabolism and coenzyme function of the vitamin suggest a distinctive function for thiamine within the nervous system. The brain retains its thiamine content in the face of a vitamin-deficient diet with great tenacity, as it is the last of all nervous tissues studied to become depleted.\n\nLack of thiamin causes the disease known as beriberi. There are two forms of beriberi: \"wet\", and \"dry\". Dry beriberi is also known as cerebral beriberi and characterized by peripheral neuropathy. Thiamine deficiency has been reported in up to 80% of alcoholic patients due to inadequate nutritional intake, reduced absorption, and impaired utilization of thiamine. Clinical signs of B deficiency include mental changes such as apathy, decrease in short-term memory, confusion, and irritability; also increased rates of depression, dementia, falls, and fractures in old age.\n\nThe lingering symptoms of neuropathy associated with cerebral beriberi are known as Korsakoff's syndrome, or the chronic phase of Wernicke-Korsakoff's. Wernicke encephalopathy is characterized by ocular abnormalities, ataxia of gait, a global state of confusion, and neuropathy. The state of confusion associated with Wernicke's may consist of apathy, inattention, spatial disorientation, inability to concentrate, and mental sluggishness or restlessness. Clinical diagnosis of Wernicke's disease cannot be made without evidence of ocular disturbance, yet these criteria may be too rigid. Korsakoff's syndrome likely represents a variation in the clinical manifestation of Wernicke encephalophathy, as they both share similar pathological origin. It is often characterized by confabulation, disorientation, and profound amnesia. Characteristics of the neuropathology are varied, but generally consist of bilaterally symmetrical midline lesions of brainstem areas, including the mammillary bodies, thalamus, periaqueductal region, hypothalamus, and the cerebellar vermis. Immediate treatment of Wernicke encephalopathy involves the administration of intravenous thiamine, followed with long-term treatment and prevention of the disorder through oral thiamine supplements, alcohol abstinence, and a balanced diet. Improvements in brain functioning of chronic alcoholics may occur with abstinence-related treatment, involving the discontinuation of alcohol consumption and improved nutrition.\n\nVitamin B, also known as niacin, includes both nicotinamide as well as nicotinic acid, both of which function in many biological oxidization and reduction reactions within the body. Niacin is involved in the synthesis of fatty acids and cholesterol, known mediators of brain biochemistry, and in effect, of cognitive function. Pellagra is a niacin deficiency disease. Pellagra is classically characterized by four 4 \"D's\": diarrhea, dermatitis, dementia, and death. Neuropsychiatric manifestations of pellagra include headache, irritability, poor concentration, anxiety, hallucinations, stupor, apathy, psychomotor unrest, photophobia, tremor, ataxia, spastic paresis, fatigue, and depression. Symptoms of fatigue and insomnia may progress to encephalophathy characterized by confusion, memory loss, and psychosis. Those afflicted with pellagra may undergo pathological alterations in the nervous system. Findings may include demylenation and degeneration of various affected parts of the brain, spinal cord, and peripheral nerves.\n\nOral nicotinamide has been promoted as an over-the-counter drug for the treatment of Alzheimer's dementia. Conversely, no clinically significant effect has been found for the drug, as nicotinamide administration has not been found to promote memory functions in patients with mild to moderate dementia of either Alzheimers', vascular, or fronto-temporal types. This evidence suggests that nicotinamide may treat dementia as related to pellegra, but administration does not effectively treat other types of dementia. Though treatment with niacin does little to alter the effects of Alzheimer's dementia, niacin intake from foods is inversely associated with the disease.\n\nFolate and vitamin B play a vital role in the synthesis of S-adenosylmethionine, which is of key importance in the maintenance and repair of all cells, including neurons. In addition, folate has been linked to the maintenance of adequate brain levels of cofactors necessary for chemicals reactions that lead to the synthesis of serotonin and catecholamine neurotransmitters. Concentrations of blood plasma folate and homocysteine concentrations are inversely related, such that an increase in dietary folate decreases homocysteine concentration. Thus, dietary intake of folate is a major determinant of homocysteine levels within the body. The relationship between folate and B is so interdependent that deficiency in either vitamin can result in megaloblastic anemia, characterized by organic mental change.\n\nThe link between levels of folate and altered mental function is not large, but is sufficient enough to suggest a causal association. Deficiency in folate can cause an elevation of homocysteine within the blood, as the clearance of homocysteine requires enzymatic action dependent on folate, and to a lesser extent, vitamins B and B. Elevated homocysteine has been associated with increased risk of vascular events, as well as dementia.\n\nDifferences lie in the presentation of megaloblastic anemia induced by either folate or B deficiency. Megaloblastic anemia related to deficiency in B generally results in peripheral neuropathy, whereas folate-related anemia often results in affective, or mood disorders. Neurological effects are not often associated with folate-related megaloblastic anemia, although demyelinating disorders may eventually present. In one study, mood disturbances were recorded for the majority of patients presenting with megaloblastic anemia in the absence of B deficiency. In addition, folate concentrations within blood plasma have been found to be lower in patients with both unipolar and bipolar depressive disorders when compared with control groups. In addition, depressive groups with low folate concentrations responded less well to standard antidepressant therapy than did those with normal levels within plasma. However, replication of these findings are less robust.\n\nIntake of the vitamin has been linked to deficits in learning and memory, particularly within the elderly population. Elderly people deficient in folate may present with deficits in free recall and recognition, which suggests that levels of folate may be related to efficacy of episodic memory. Lack of adequate folate may produce a form of dementia considered to be reversible with administration of the vitamin. Indeed, there is a degree of improvement in memory associated with folate treatment. In a 3-year longitudinal study of men and women aged 50–70 years with elevated homocysteine plasma concentration, researchers found that a daily oral folic acid supplementation of 800μg resulted in an increase in folate levels and a decrease in homocysteine levels within blood plasma. In addition to these results, improvements of memory, and information-processing speed, as well as slight improvements of sensorimotor speed were observed, which suggests there is a link between homocysteine and cognitive performance. However, while the amount of cognitive improvement after treatment with folate is correlated with the severity of folate deficiency, the severity of cognitive decline is independent of the severity of folate deficiency. This suggests that the dementia observed may not be entirely related to levels folate, as there could be additional factors that were not accounted for which might have an effect.\n\nBecause neurulation may be completed before pregnancy is recognized, it is recommended that women capable of becoming pregnant take about 400μg of folic acid from fortified foods, supplements, or a combination of the two in order to reduce the risk of neural tube defects. These major anomalies in the nervous system can be reduced by 85% with systematic folate supplementation occurring before the onset of pregnancy. The incidence of Alzheimer's and other cognitive diseases has been loosely connected to deficiencies in folate. It is recommended for the elderly to consume folate through food, fortified or not, and supplements in order to reduce risk of developing the disease.\n\nAlso known as cobalamin, B is important for the maintenance of neurological function and psychiatric health. B deficiency, also known as hypocobalaminemia, often results from complications involving absorption into the body. An assortment of neurological effects can be observed in 75–90% of individuals of any age with clinically observable B deficiency. Cobalamin deficiency manifestations are apparent in the abnormalities of the spinal cord, peripheral nerves, optic nerves, and cerebrum. These abnormalities involve a progressive degeneration of myelin, and may be expressed behaviorally through reports of sensory disturbances in the extremities, or motor disturbances, such as gait ataxia. Combined myelopathy and neuropathy are prevalent within a large percentage of cases. Cognitive changes may range from loss of concentration to memory loss, disorientation, and dementia. All of these symptoms may present with or without additional mood changes. Mental symptoms are extremely variable, and include mild disorders of mood, mental slowness, and memory defect. Memory defect encompasses symptoms of confusion, severe agitation and depression, delusions and paranoid behavior, visual and auditory hallucinations, dysphasia, violent maniacal behavior and epilepsy. It has been suggested that mental symptoms could be related to a decrease in cerebral metabolism, as caused by the state of deficiency.\n\nMild to moderate cases of pernicious anemia may show poor concentration. In severe cases of pernicious anemia, individuals may present with various cognitive problems such as dementia, and memory loss. It is not always easy to determine whether B deficiency is present, especially within older adults. Patients may present with violent behavior or more subtle personality changes. They may also present with vague complaints, such as fatigue or memory loss, that may be attributed to normative aging processes. Cognitive symptoms may mimic behavior in Alzheimer's and other dementias as well.\n\nPatients deficient in B despite normal absorption functionality may be treated through oral administration of at least 6 µg/day of the vitamin in pill form. Patients who suffer from irreversible causes of deficiency, such as pernicious anemia or old age, will need lifelong treatment with pharmacological doses of B. Strategy for treatment is dependent on the patient's level of deficiency as well as their level of cognitive functioning. Treatment for those with severe deficiency involves 1000 µg of B administered intramuscularly daily for one week, weekly for one month, then monthly for the rest of the patients life. The progression of neurological manifestations of cobalamin deficiency is generally gradual. As a result, early diagnosis is important or else irreversible damage may occur. Patients who become demented usually show little to no cognitive improvement with the administration of B. There is risk that folic acid administered to those with B deficiency may mask anemic symptoms without solving the issue at hand. In this case, patients would still be at risk for neurological deficits associated with B deficiency-related anemia, which are not associated with anemia related to folate deficiency.\n\nVitamin A is an essential nutrient for mammals which takes form in either retinol or the provitamin beta-Carotene. It helps regulation of cell division, cell function, genetic regulation, helps enhance the immune system, and is required for brain function, chemical balance, growth and development of the central nervous system and vision.\n\nFoods that are rich in omega-3 fatty acids have been shown to decrease risk of getting Alzheimer's disease. Omega-3 fatty acids, primarily docosahexaenoic acid (DHA), which is the most prevalent omega-3 fatty acid found in neurons, have been studied extensively for use in possible prevention and therapy of Alzheimer's disease. Some studies (cross-sectional) suggest that reduced intake or low brain levels of DHA are associated with earlier development of cognitive deficits or development of dementia, including Alzheimer's disease. Several clinical trials suggest that omega-3 fatty acid supplementation does not have significant effects in the treatment of Alzheimer's disease—which in turn may suggest that the protective benefits of omega-3 fatty acid supplementation could depend on the scope of the disease and other confounding factors. A diet that is rich in antioxidants will also help get rid of free radicals in the body, which could be a cause for Alzheimer's. The buildup of Beta Amyloid plaques, a marker highly associated with Alzheimer's disease, generates cell damaging free radicals. Therefore, the role of antioxidants as protectants against Alzheimer's disease has become a hot topic of study. Simple dietary modification, towards fewer highly processed carbohydrates and relatively more fats ad cholesterol, is likely a protective measure against Alzheimer's disease.\n\nAdditionally, folic acid has also been found to improve the memory of older people. There is some evidence that deficiency in folic acid may increase the risk of dementia, especially Alzheimer's disease and vascular dementia, but there is debate about whether it lowers risk of cognitive impairment in the older population. Folic acid supplementation is shown to lower blood homocysteine levels, while folic acid deficiency can lead to a condition of high levels of homocysteine (Hcy) in the bloodstream called hyperhomocysteinemia (HHcy). HHcy is related to several vascular diseases such as coronary artery disease, peripheral vascular disease, and stroke.\n\n"}
{"id": "27024757", "url": "https://en.wikipedia.org/wiki?curid=27024757", "title": "Nutritional neuroscience", "text": "Nutritional neuroscience\n\nNutritional neuroscience is the scientific discipline that studies the effects various components of the diet such as minerals, vitamins, protein, carbohydrates, fats, dietary supplements, synthetic hormones, and food additives have on neurochemistry, neurobiology, behavior, and cognition.\n\nRecent research on nutritional mechanisms and their effect on the brain show they are involved in almost every facet of neurological functioning including alterations in neurogenesis, neurotrophic factors, neural pathways and neuroplasticity, throughout the life cycle.\n\nRelatively speaking, the brain consumes an immense amount of energy in comparison to the rest of the body. The human brain is approximately 2% of the human body mass and uses 20–25% of the total energy expenditure. Therefore, mechanisms involved in the transfer of energy from foods to neurons are likely to be fundamental to the control of brain function. Insufficient intake of selected vitamins, or certain metabolic disorders, affect cognitive processes by disrupting the nutrient-dependent processes within the body that are associated with the management of energy in neurons, which can subsequently affect neurotransmission, synaptic plasticity, and cell survival.\n\nDeficiency or excess of essential minerals (e.g. iron, zinc, copper, and magnesium) can disrupt brain development and neurophysiology to affect behavior. Furthermore, minerals have been implicated in the pathophysiology of neurodegenerative diseases including Alzheimer's dementia.\n\nIron is essential for several critical metabolic enzymes and a deficiency of this mineral can disrupt brain development. For, example chronic marginal iron affects dopamine metabolism and myelin fatty acid composition and behavior in mice. In rats a marginal iron deficiency that does not cause anemia disrupted axon growth in the auditory nerve affecting auditory brainstem latency without major changes in myelination. In rhesus macaques, prenatal iron deficiency disrupts emotional behavior and polymorphisms that reduce the expression of monoamine oxidase interact with gestational iron deficiency to exacerbate the response to a stressful situation leading to increased aggressiveness. Inexpensive and effective iron supplementation is an available preventative strategy recommended by the World Health Organization. However, iron supplementation can exacerbate malaria infection. Therefore, individuals receiving iron supplementation in malaria-endemic areas must be carefully monitored.\n\nZinc is essential for the structure and function of thousands of proteins critical for the function of every cell. Zinc can also serve as a neurotransmitter in the brain, thus a deficiency of this mineral can clearly disrupt development as well as neurophysiology. For example, zinc deficiency during early development impairs neurogenesis leading to memory impairments. However, zinc deficiency later in life can disrupt appetite and cause depression like behavior. However, it is important to consider copper intake relative to zinc supplementation because excess zinc can disrupt copper absorption.\n\nConservative estimates suggest that 25% of the world's population is at risk of zinc deficiency.\n\nHypozincemia is usually a nutritional deficiency, but can also be associated with malabsorption, diarrhea, acrodermatitis enteropathica, chronic liver disease, chronic renal disease, sickle-cell disease, diabetes, malignancy, pyroluria, and other chronic illnesses. It can also occur after bariatric surgery, heavy metal exposure and tartrazine. \n\nZinc deficiency is typically the result of inadequate dietary intake of zinc, disease states that promote zinc losses, or physiological states that require increased zinc. Populations that consume primarily plant based diets that are low in bioavailable zinc often have zinc deficiencies. Diseases or conditions that involve intestinal malabsorption promote zinc losses. Fecal losses of zinc caused by diarrhea are one contributing factor, often common in developing countries. Changes in intestinal tract absorbability and permeability due, in part, to viral, protozoal, and bacteria pathogens may also encourage fecal losses of zinc. Physiological states that require increased zinc include periods of growth in infants and children as well as in mothers during pregnancy.\n\nZinc deficiency may cause a decrease in appetite which can degenerate into anorexia or anorexia nervosa. Appetite disorders, in turn, cause malnutrition and, notably, inadequate zinc intake. Anorexia itself is a cause of zinc deficiency, thus leading to a vicious cycle: the worsening of anorexia worsens the zinc deficiency. A 1994 randomized, double-blind, placebo-controlled trial showed that zinc (14 mg per day) doubled the rate of body mass increase in the treatment of anorexia nervosa.\n\nCognitive and motor function may also be impaired in zinc deficient children. Zinc deficiency can interfere with many organ systems especially when it occurs during a time of rapid growth and development when nutritional needs are high, such as during infancy. In animal studies, rats who were deprived of zinc during early fetal development exhibited increased emotionality, poor memory, and abnormal response to stress which interfered with performance in learning situations. Zinc deprivation in monkeys showed that zinc deficient animals were emotionally less mature, and also had cognitive deficits indicated by their difficulty in retaining previously learned problems and in learning new problems. Human observational studies show weaker results. Low maternal zinc status has been associated with less attention during the neonatal period and worse motor functioning. In some studies, supplementation has been associated with motor development in very low birth weight infants and more vigorous and functional activity in infants and toddlers.\n\nPlasma zinc level has been associated with many psychological disorders. However, the nature of this relationship remains unclear in most instances. An increasing amount of evidence suggests that zinc deficiency could play a causal role in the etiology of depression. Indeed, zinc supplementation has been reported to improve measures of depression in randomized double blind placebo controlled trials.\n\nCopper is important for the function of many enzymes in the brain. Notably, dopamine β-mono-oxygenase is affected by copper deficiency leading to increased dopamine and decreased norepinephrine levels. Both copper deficiency and toxicity can interfere with brain development and function.\n\nThe neurodegenerative syndrome of copper deficiency has been recognized for some time in ruminant animals, in which it is commonly known as \"swayback\". The disease involves a nutritional deficiency in the trace element copper. Copper is ubiquitous and daily requirement is low making acquired copper deficiency very rare. Copper deficiency can manifest in parallel with vitamin B12 and other nutritional deficiencies. The most common cause of copper deficiency is a remote gastrointestinal surgery, such as gastric bypass surgery, due to malabsorption of copper, or zinc toxicity. On the other hand, Menkes disease is a genetic disorder of copper deficiency involving a wide variety of symptoms that is often fatal.\n\nCopper deficiency can cause a wide variety of neurological problems including, myelopathy, peripheral neuropathy, and optic neuropathy.\n\nSufferers typically present difficulty walking (gait difficulty) caused by sensory ataxia (irregular muscle coordination) due to dorsal column dysfunction or degeneration of the spinal cord (myelopathy). Patients with ataxic gait have problems balancing and display an unstable wide walk. They often feel tremors in their torso, causing side way jerks and lunges.\n\nIn brain MRI, there is often an increased T2 signalling at the posterior columns of the spinal cord in patients with myelopathy caused by copper deficiency. T2 signalling is often an indicator of some kind of neurodegeneration. There are some changes in the spinal cord MRI involving the thoracic cord, the cervical cord or sometimes both. Copper deficiency myelopathy is often compared to subacute combined degeneration (SCD). Subacute combined degeneration is also a degeneration of the spinal cord, but instead vitamin B12 deficiency is the cause of the spinal degeneration. SCD also has the same high T2 signalling intensities in the posterior column as copper deficient patient in MRI imaging.\n\nAnother common symptom of copper deficiency is peripheral neuropathy, which is numbness or tingling that can start in the extremities and can sometimes progress radially inward towards the torso. In an Advances in Clinical Neuroscience & Rehabilitation (ACNR) published case report, a 69-year-old patient had progressively worsened neurological symptoms. These symptoms included diminished upper limb reflexes with abnormal lower limb reflexes, sensation to light touch and pin prick was diminished above the waist, vibration sensation was lost in the sternum, and markedly reduced proprioception or sensation about the self's orientation. Many people suffering from the neurological effects of copper deficiency complain about very similar or identical symptoms as the patient. This numbness and tingling poses danger for the elderly because it increases their risk of falling and injuring themselves. Peripheral neuropathy can become very disabling leaving some patients dependent on wheel chairs or walking canes for mobility if there is lack of correct diagnosis. Rarely can copper deficiency cause major disabling symptoms. The deficiency will have to be present for an extensive amount of time until such disabling conditions manifest.\n\nSome patients suffering from copper deficiency have shown signs of vision and color loss. The vision is usually lost in the peripheral views of the eye. The bilateral vision loss is usually very gradual. An optical coherence tomography (OCT) shows some nerve fiber layer loss in most patients, suggesting the vision loss and color vision loss was secondary to optic neuropathy or neurodegeneration.\n\nCopper toxicity can occur from excessive supplement use, eating acid foods cooked in uncoated copper cookware, exposure to excess copper in drinking water, or as the result of an inherited metabolic disorder in the case of Wilson's disease. A significant portion of the toxicity of copper comes from its ability to accept and donate single electrons as it changes oxidation state. This catalyzes the production of very reactive radical ions, such as hydroxyl radical in a manner similar to Fenton chemistry. This catalytic activity of copper is used by the enzymes with which it is associated, thus is only toxic when unsequestered and unmediated. This increase in unmediated reactive radicals is generally termed oxidative stress, and is an active area of research in a variety of diseases where copper may play an important but more subtle role than in acute toxicity.\n\nSome of the effects of aging may be associated with excess copper. In addition, studies have found that people with mental illnesses, such as schizophrenia, had heightened levels of copper in their systems. However, it is unknown at this stage whether the copper contributes to the mental illness, whether the body attempts to store more copper in response to the illness, or whether the high levels of copper are the result of the mental illness.\n\nElevated free copper levels exist in Alzheimer's disease. Copper and zinc are known to bind to amyloid beta proteins in Alzheimer's disease. This bound form is thought to mediate the production of reactive oxygen species in the brain. A preliminary clinical trial suggests that zinc supplementation may be able to decrease copper levels and slow degeneration in Alzheimer's disease.\n\nManganese is a component of some enzymes and stimulates the development and activity of other enzymes. Manganese superoxide dismutase (MnSOD) is the principal antioxidant in mitochondria. Several enzymes activated by manganese contribute to the metabolism of carbohydrates, amino acids, and cholesterol.\n\nDeficiency of manganese causes skeletal deformation in animals and inhibits the production of collagen in wound healing. On the other hand, manganese toxicity is associated with neurological complications.\n\nManganese poisoning is a toxic condition resulting from chronic exposure to manganese and first identified in 1837 by James Couper.\n\nChronic exposure to excessive Mn levels can lead to a variety of psychiatric and motor disturbances, termed manganism. Generally, exposure to ambient Mn air concentrations in excess of 5 mg Mn/m3 can lead to Mn-induced symptoms.\n\nIn initial stages of manganism, neurological symptoms consist of reduced response speed, irritability, mood changes, and compulsive behaviors. Upon protracted exposure symptoms are more prominent and resemble those of idiopathic Parkinson's disease, as which it is often misdiagnosed, although there are particular differences in both the symptoms (nature of tremors, for example), response to drugs such as levodopa, and affected portion of the basal ganglia. Symptoms are also similar to Lou Gehrig's disease and multiple sclerosis.\n\nManganism has become an active issue in workplace safety as it has been the subject of numerous product liability lawsuits against manufacturers of arc welding supplies. In these lawsuits, welders have accused the manufacturers of failing to provide adequate warning that their products could cause welding fumes to contain dangerously high manganese concentrations that could lead welders to develop manganism. Companies employing welders are also being sued, for what colloquially is known as \"welders' disease\". However, studies fail to show any link between employment as a welder and manganism (or other neurological problems).\n\nManganism is also documented in reports of illicit methcathinone manufacturing. This is due to manganese being a byproduct of methcathinone synthesis if potassium permanganate is used as an oxidiser. Symptoms include apathy, bradykinesia, gait disorder with postural instability, and spastic-hypokinetic dysarthria. Another street drug sometimes contaminated with manganese is the so-called \"Bazooka\", prepared by free-base methods from cocaine using manganese carbonate.\n\nReports also mention such sources as contaminated drinking water, and fuel additive methylcyclopentadienyl manganese tricarbonyl (MMT), which on combustion becomes partially converted into manganese phosphates and sulfate that go airborne with the exhaust, and manganese ethylene-bis-dithiocarbamate (Maneb), a pesticide.\n\nManganese may affect liver function, but the threshold of acute toxicity is very high. On the other hand, more than 95% of manganese is eliminated by biliary excretion. Any existing liver damage may slow this process, increasing its concentration in blood plasma. The exact neurotoxic mechanism of manganese is uncertain but there are clues pointing at the interaction of manganese with iron, zinc, aluminum, and copper. Based on a number of studies, disturbed iron metabolism could underlie the neurotoxic action of manganese.\n\nIt participates in Fenton reactions and could thus induce oxidative damage, a hypothesis corroborated by the evidence from studies of affected welders. A study of the exposed workers showed that they have significantly fewer children. This may indicate that long-term accumulation of manganese affects fertility. Pregnant animals repeatedly receiving high doses of manganese bore malformed offspring significantly more often compared to controls. Manganism mimics Schizophrenia. It is found in large quantities in paint and steelmaking.\n\nThe current mainstay of manganism treatment is levodopa and chelation with EDTA. Both have limited and at best transient efficacy. Replenishing the deficit of dopamine with levodopa has been shown to initially improve extrapyramidal symptoms, but the response to treatment goes down after 2 or 3 years, with worsening condition of the same patients noted even after 10 years since last exposure to manganese. Enhanced excretion of manganese prompted by chelation therapy brings its blood levels down but the symptoms remain largely unchanged, raising questions about efficacy of this form of treatment.\n\nIncreased ferroportin protein expression in human embryonic kidney (HEK293) cells is associated with decreased intracellular Mn concentration and attenuated cytotoxicity, characterized by the reversal of Mn-reduced glutamate uptake and diminished lactate dehydrogenase (LDH) leakage.\n\nThe Red River Delta near Hanoi has high levels of manganese or arsenic in the water. Approximately 65 percent of the region’s wells contain high levels of arsenic, manganese, selenium and barium. This was also published in the Proceedings of the National Academy of Sciences.\n\nMagnesium is necessary for the function of many metabolic enzymes and also serves as a key regulator of calcium channels involved in neurotransmission (e.g. NMDA receptor). Magnesium supplementation facilitates nerve regeneration after injury. Although unpolished grains contain magnesium, phytic acid in grains can inhibit its absorption. Leafy greens are an excellent source of magnesium.\n\nDeficiency or excess intake of many vitamins can affect the brain contributing to developmental and degenerative diseases.\n\nVitamin A is an essential nutrient for mammals which takes form in either retinol or the provitamin beta-Carotene. It helps regulation of cell division, cell function, genetic regulation, helps enhance the immune system, and is required for brain function, chemical balance, growth and development of the central nervous system and vision.\n\nIn an experiment by Chongqing Medical University pregnant rats were either plentiful in vitamin A or were of a vitamin A deficiency (VAD) due to their diet. The offspring of these rats were then tested in a water maze at 8 weeks old and it was found the VAD offspring had a harder time finishing the maze which helps show that these rats, even while having a deficiency from \"in utero\", have more problems with learning memory. Young rats in a separate study by the same university also showed impaired long-term potentiation in the hippocampus when they were VAD which shows neuronal impairment. When the patient is VAD for too long, the effects of the damage to the hippocampus can be irreversible.\n\nVitamin A affects spatial memory most of the time because the size of the nuclei in hippocampal neurons are reduced by approximately 70% when there is a deficiency which affects a person's abilities for higher cognitive function. In a study by the University of Cagliari, Italy, VAD rats had more trouble learning a Radial arm maze than rats who had normal levels of the vitamin. The healthy rats were able to correctly solve the maze within the 15-day training period and other rats that were once deficient but had vitamin A restored to normal levels were also able to solve it. Here it was found that the retinoid receptors which help transport vitamin A were of normal function.\n\nEating foods high in vitamin A or taking dietary supplements, retinol or retinal will prevent a deficiency. The foods highest in vitamin A are any pigmented fruits and vegetables and leafy green vegetables also provide beta-Carotene.\nThere can be symptoms of fat loss and a reduction of any weight gain that would be considered normal for an individual, especially developmental weight gains such as in infants which would occur if the infant was deprived of vitamin A in utero and/or if it was deprived postnatal for an extensive period of time. The deficiency can also cause conditions such as blindness or night blindness, also known as nyctalopia. Night blindness is due to the inability to regenerate rhodopsin in the rods which is needed in dim light in order to see properly.\nA treatment of supplements of retinoic acid which is a part of vitamin A can help replenish levels and help bring learning to normal, but after 39 weeks this is ineffective even if the treatment is daily because it will not bring the retinoid hypo-signalling back to normal.\n\nZinc is needed to maintain normal vitamin A levels in blood plasma. It also helps vitamin A become metabolized by the liver. However evidence suggests that when someone is deficient in both vitamin A and zinc, memory is more improved when just vitamin A is increased than when just zinc is increased. Of course memory has the largest improvement when both are increased. When one of these nutrients is not balanced, the other is most likely to be affected because they rely on each other for proper functioning in learning.\n\nVitamin B, also known as thiamine, is a coenzyme essential for the metabolism of carbohydrates. This vitamin is important for the facilitation of glucose use, thus ensuring the production of energy for the brain, and normal functioning of the nervous system, muscles, and heart.\n\nThiamine is found in all living tissues, and is uniformly distributed throughout mammalian nervous tissue, including the brain and spinal cord. Metabolism and coenzyme function of the vitamin suggest a distinctive function for thiamin within the nervous system.\n\nThe brain retains its thiamine content in the face of a vitamin-deficient diet with great tenacity, as it is the last of all nervous tissues studied to become depleted. A 50% reduction of thiamine stores in rats becomes apparent after only 4 days of being put on a thiamine-deficient diet. However, polyneuritic signs do not begin to appear until about 4 or 5 weeks have passed. Similar results have been found in human subjects.\n\nThe body has only small stores of B; accordingly, there is risk of deficiency if the level of intake is reduced only for a few weeks. Thiamin deficiency during critical periods of early development can disrupts neurogenesis in animal models. Lack of thiamin later in life causes the disease known as beriberi. There are two forms of beriberi: \"wet\", and \"dry\". Dry beriberi is also known as cerebral beriberi. Characteristics of wet beriberi include prominent edema and cardiac involvement, whereas dry beriberi is mainly characterized by a polyneuritis. Severe thiamin deficiency can also result in acute neurodegeneration leading to peripheral neuropathy and memory loss.\n\nIn industrialized nations, thiamine deficiency is a clinically significant problem in individuals with chronic alcoholism or other disorders that interfere with normal ingestion of food. Thiamine deficiency within developed nations tends to manifest as Wernicke–Korsakoff syndrome. Chronic alcoholism can disrupt thiamin absorption and thiamin deficiency contributes to neurodegeneration and memory loss in alcoholics known as Wernicke's encephalopathy. Individuals with chronic alcoholism may fall short on minimum daily requirements of thiamine in part due to anorexia, erratic eating habits, lack of available food, or a combination of any of these factors. Thiamine deficiency has been reported in up to 80% of alcoholic patients due to inadequate nutritional intake, reduced absorption, and impaired utilization of thiamine. Alcohol, in combination with its metabolite acetaldehyde, interacts with thiamine utilization at the molecular level during transport, diphosphorylation, and modification processes. For this reason, chronic alcoholics may have insufficient thiamine for maintenance of normal brain function, even with seemingly adequate dietary intake.\n\nClinical signs of B deficiency include mental changes such as apathy, decrease in short-term memory, confusion, and irritability. Moderate deficiency in thiamine may reduce growth in young populations, in increase chronic illness in both young and middle-aged adults. In addition, moderate deficiency of thiamine may increase rates of depression, dementia, falls, and fractures in old age.\n\nThe lingering symptoms of neuropathy associated with cerebral beriberi are known as Korsakoff's syndrome, or the chronic phase of Wernicke-Korsakoff's. Wernicke encephalopathy is a neurological disorder resulting from a deficiency in thiamine, sharing the same predominant features of cerebral beriberi, as characterized by ocular abnormalities, ataxia of gait, a global state of confusion, and neuropathy. The state of confusion associated with Wernicke's may consist of apathy, inattention, spatial disorientation, inability to concentrate, and mental sluggishness or restlessness. Clinical diagnosis of Wernicke's disease cannot be made without evidence of ocular disturbance, yet these criteria may be too rigid. Korsakoff's likely represents a variation in the clinical manifestation of Wernicke encephalophathy, as they both share similar pathological origin.\n\nKorsakoff's syndrome is often characterized by confabulation, disorientation, and profound amnesia. Characteristics of the neuropathology are varied, but generally consist of bilaterally symmetrical midline lesions of brainstem areas, including the mammillary bodies, thalamus, periaqueductal region, hypothalamus, and the cerebellar vermis.\n\nImmediate treatment of Wernicke encephalopathy involves the administration of intravenous thiamine, followed with long-term treatment and prevention of the disorder through oral thiamine supplements, alcohol abstinence, and a balanced diet. Improvements in brain functioning of chronic alcoholics may occur with abstinence-related treatment, involving the discontinuation of alcohol consumption and improved nutrition. Wernicke's encephalopathy is life-threatening if left untreated. However, a rapid reversal of symptoms may result from prompt administration of thiamine.\n\nFortification of flour is practiced in some countries to replace the thiamine lost during processing. However, this method has been criticized for missing the target population of chronic alcoholics, who are most at risk for deficiency. Alternative solutions have suggested the fortification of alcoholic beverages with thiamine.\n\nIngesting a diet rich in thiamine may stave off the adverse effects of deficiency. Foods providing rich sources of thiamine include unrefined grain products, ready-to-eat cereals, meat (especially pork), dairy products, peanuts, legumes, fruits and eggs.\n\nVitamin B, also known as niacin, includes both nicotinamide as well as nicotinic acid, both of which function in many biological oxidization and reduction reactions within the body. These functions include the biochemical degradation of carbohydrates, fats and proteins. Niacin is also involved in the synthesis of fatty acids and cholesterol, which are known mediators of brain biochemistry, and in effect, of cognitive function.\n\nSufficient niacin intake is either obtained from diet, or synthesized from the amino acid tryptophan.\n\nSevere niacin deficiency typically manifests itself as the disease pellagra. Synthesis of B from tryptophan involves vitamin B and B, so deficiencies in either of these nutrients can lead to niacin deficiency. An excess of leucine, an essential amino acid, in the diet can also interfere with tryptophan conversion and subsequently result in a B deficiency.\n\nPellagra is most common to populations within developing countries in which corn is the dietary staple. The disease has virtually disappeared from industrialized countries, yet still appears in India and parts of China and Africa. This is in part due to the bound form of niacin that unprocessed corn contains, which is not readily absorbed into the human body. The processes involved in making corn tortillas, can release the bound niacin into a more absorbable form. Pellagra is not problematic in countries which traditionally prepare their corn in this way, but is a problem in other countries where unprocessed corn is main source of caloric intake.\n\nThough pellagra predominantly occurs in developing countries, sporadic cases of pellagra may be observed within industrialized nations, primarily in chronic alcoholics and patients living with functional absorption complications.\n\nPellagra is classically characterized by four 4 \"D's\": diarrhea, dermatitis, dementia, and death. Neuropsychiatric manifestations of pellagra include headache, irritability, poor concentration, anxiety, hallucinations, stupor, apathy, psychomotor unrest, photophobia, tremor, ataxia, spastic paresis, fatigue, and depression. Symptoms of fatigue and insomnia may progress to encephalopathy characterized by confusion, memory loss, and psychosis.\n\nThose afflicted with pellagra may undergo pathological alterations in the nervous system. Findings may include demylenation and degeneration of various affected parts of the brain, spinal cord, and peripheral nerves.\n\nPrognosis of deficiency is excellent with treatment. Without, pellagra will gradually progress and lead to death within 4–5 years, often a result of malnutrition from prolonged diarrhea, or complications as caused by concurrent infections or neurological symptoms. Symptoms of pellagra can be cured with exogenous administration of nicotinic acid or nicotinamide.\n\nFlushing occurs in many patients treated therapeutically with nicotinic acid, and as a result, nicotinamide holds more clinical value as it is not associated with the same uncomfortable flushing. The adult dose of nicotinamide is 100 mg taken orally every 6 hours until resolution of major acute symptoms, followed with oral administration of 50 mg every 8–12 hours until skin lesions heal. For children, treatment involves oral ingestion of 10–15 mg of nicotinamide, depending on weight, every 6 hours until signs and symptoms are resolved. Severe cases require 1 gram every 3–4 hours, administered parenterally.\n\nOral nicotinamide has been promoted as an over-the-counter drug for the treatment of Alzheimer's dementia. Conversely, no clinically significant effect has been found for the drug, as nicotinamide administration has not been found to promote memory functions in patients with mild to moderate dementia of either Alzheimer's, vascular, or fronto-temporal types. This evidence suggests that nicotinamide may treat dementia as related to pellagra, but administration does not effectively treat other types of dementia.\n\nThe best method of prevention is to each food rich in B. Generally, this involves the intake of a protein-rich diet. Foods that contain high concentrations of niacin in the free form include beans and organ meat, as well as enriched grain and cereal products. While niacin is present in corn and other grains, the bioavailability of the nutrient is much less than it is in protein-rich sources. Different methods of processing corn may result in a higher degree of bioavailability of the vitamin.\n\nThough treatment with niacin does little to alter the effects of Alzheimer's dementia, niacin intake from foods is inversely associated with the disease.\n\nFolate deficiency can disrupt neurulation and neurogenesis. Maternal folic acid intake around the time of conception prevents neural tube defects. Furthermore, folic acid intake was recently associated with incidence of autism. Enriched white flour is fortified with folic acid in the United States and many other countries. However the European Union does not have mandatory folic acid fortification. Although the protective effects of folic acid are well documented, there remains legitimate concern that fortification could lead to toxic levels in a subset of the population. For example, elevated levels of folic acid can interact with vitamin B12 deficiency to cause neurodegeneration. Furthermore, folic acid and iron can interact to exacerbate malaria.\n\nFolic acid is the most oxidized and stable form of folate, and can also be referred to as vitamin B. It rarely occurs naturally in foods, but it is the form used in vitamin supplements as well as fortified food products.\n\nFolate coenzymes are involved in numerous conversion processes within the body, including DNA synthesis and amino acid interconversions. Folate and vitamin B play a vital role in the synthesis of S-adenosylmethionine, which is of key importance in the maintenance and repairment of all cells, including neurons. In addition, folate has been linked to the maintenance of adequate brain levels of cofactors necessary for chemicals reactions that lead to the synthesis of serotonin and catecholamine neurotransmitters.\n\nFolate has a major, but indirect role in activities which help to direct gene expression and cell proliferation. These activities occur at a greatly increased rate during pregnancy, and depend on adequate levels of folate within blood plasma.\n\nConcentrations of blood plasma folate and homocysteine concentrations are inversely related, such that an increase in dietary folate decreases homocysteine concentration. Thus, dietary intake of folate is a major determinant of homocysteine levels within the body.\n\nAutoantibodies against folate receptor alpha have been found in up to 75% of children with autism.\n\nFolate deficiency most commonly arises from insufficient folate intake from the diet, but may also stem from inefficient absorption or metabolic utilization of folate, usually a result of genetic variation. The relationship between folate and B is so interdependent that deficiency in either vitamin can result in megaloblastic anemia, characterized by organic mental change.\n\nThe process of neural tube transformation into structures that will eventually develop into the central nervous system is known as neurulation, the success of which is dependent on the presence of folate within the body. This process begins in the human approximately 21 days after conception, and is completed by 28 days. Thus, a woman may not even be aware of her pregnancy by the time the process of neurulation is complete, potentially causing severe consequences in the development of the fetus.\n\nFunctional problems in the absorption and utilization of vitamins may also play a role in folate deficiencies within the elderly.\n\nThe link between levels of folate and altered mental function is not large, but is sufficient enough to suggest a causal association. Deficiency in folate can cause an elevation of homocysteine within the blood, as the clearance of homocysteine requires enzymatic action dependent on folate, and to a lesser extent, vitamins B and B. Elevated homocysteine has been associated with increased risk of vascular events, as well as dementia.\n\nDifferences lie in the presentation of megaloblastic anemia induced by either folate or B deficiency. Megaloblastic anemia related to deficiency in B generally results in peripheral neuropathy, whereas folate-related anemia often results in affective, or mood disorders. Neurological effects are not often associated with folate-related megaloblastic anemia, although demyelinating disorders may eventually present. In one study, mood disturbances were recorded for the majority of patients presenting with megaloblastic anemia in the absence of B deficiency. In addition, folate concentrations within blood plasma have been found to be lower in patients with both unipolar and bipolar depressive disorders when compared with control groups. In addition, depressive groups with low folate concentrations responded less well to standard antidepressant therapy than did those with normal levels within plasma. However, replication of these findings are less robust.\n\nThe role of folic acid during pregnancy is vital to normal development of the nervous system in the fetus. A deficiency in folate levels of a pregnant woman could potentially result in neural tube disorder, a debilitating condition in which the tubes of the central nervous system do not fuse entirely. NTDs are not to be confused with spina bifida, which does not involve neural elements. Neural tube defects can present in a number of ways as a result of the improper closure at various points of the neural tube. The clinical spectrum of the disorder includes encephalocele, craniorachischisis, and anencephaly. In addition, these defects can also be classified as open, if neural tissue is exposed or covered only by membrane, or can be classified as closed, if the tissue is covered by normal skin.\n\nIntake of the vitamin has been linked to deficits in learning and memory, particularly within the elderly population. Elderly people deficient in folate may present with deficits in free recall and recognition, which suggests that levels of folate may be related to efficacy of episodic memory.\n\nLack of adequate folate may produce a form of dementia considered to be reversible with administration of the vitamin. Indeed, there is a degree of improvement in memory associated with folate treatment. In a 3-year longitudinal study of men and women aged 50–70 years with elevated homocysteine plasma concentration, researchers found that a daily oral folic acid supplementation of 800μg resulted in an increase in folate levels and a decrease in homocysteine levels within blood plasma. In addition to these results, improvements of memory, and information-processing speed, as well as slight improvements of sensorimotor speed were observed, which suggests there is a link between homocysteine and cognitive performance.\n\nHowever, while the amount of cognitive improvement after treatment with folate is correlated with the severity of folate deficiency, the severity of cognitive decline is independent of the severity of folate deficiency. This suggests that the dementia observed may not be entirely related to levels of folate, as there could be additional factors that were not accounted for which might have an effect.\n\nBecause neurulation may be completed before pregnancy is recognized, it is recommended that women capable of becoming pregnant take about 400μg of folic acid from fortified foods, supplements, or a combination of the two in order to reduce the risk of neural tube defects. These major anomalies in the nervous system can be reduced by 85% with systematic folate supplementation occurring before the onset of pregnancy.\n\nThe incidence of Alzheimer's and other cognitive diseases has been loosely connected to deficiencies in folate. It is recommended for the elderly to consume folate through food, fortified or not, and supplements in order to reduce risk of developing the disease.\nGood sources of folate include liver, ready-to-eat breakfast cereals, beans, asparagus, spinach, broccoli, and orange juice.\n\nCholine is an important methyl donor involved in one-carbon metabolism that also becomes incorporated into phospholipids and the neurotransmitter acetylcholine. Because of its role in cellular synthesis, choline is an important nutrient during the prenatal and early postnatal development of offspring as it contributes heavily to the development of the brain. A study found that rats that were given supplements of choline in utero or in the weeks following birth had superior memories. The changes appeared to be a result of physical changes to the hippocampus, the area of the brain responsible for memory. Furthermore, choline can reduce some of the deleterious effects of folate deficiency on neurogenesis.\n\nWhile choline during development is important, adult levels of choline are also important. Choline has been shown to increase the synthesis and release of acetylcholine from neurons, which in turn increases memory. A double-blind study was conducted using normal college students (no neurological disorders). Results showed that twenty-five grams of phosphatidylcholine (another form of choline) created a significant improvement in explicit memory, measured by a serial learning task, however this improvement may be attributed to the improvement of slow learners. Another study found that a single ten-gram oral dose of choline, given to normal volunteers (again, without neurological disorders) significantly decreased the number of trials needed to master a serial-learning word test. This increase in memory is particularly beneficial to memory loss suffered by old age. A study conducted on rats who, like humans, suffer from an age-related loss of memory were tested on how choline affected memory. The results showed that rats who had a chronic low-choline diet showed greater memory loss then their same-age control counterparts, while rats who had choline-enriched diets showed a diminished memory loss compared to both the choline-low diet and control rat groups. Furthermore, young rats who were choline-deficient performed as poorly on memory tasks as older rats while older rats that were given choline supplements performed as well as three-month-old rats.\n\nDespite the wide range of foods that choline is found in, studies have shown that the mean choline intake of men, women and children are below the Adequate Intake levels. It is important to note that not enough choline is naturally produced by the body, so diet is an important factor. People who consume a large quantity of alcohol may be at an increased risk for choline deficiency. Sex and age also plays a role, with premenopausal females being less sensitive to choline deficiency than either males or postmenopausal females. This has been theorized to be a result of premenopausal women having an increased ability to synthesize choline in some form, which has been confirmed in studies on rats. In such instances of deficiency, choline supplements or (if able) dietary changes may be beneficial. Good sources of choline include liver, milk, eggs and peanuts. There is further evidence to suggest that choline supplements can be used to treat people who suffer from neurological disorders as well we memory defects. Oral doses of CDP-choline (another form of choline) given to elderly subjects with memory deficits, but without dementia, for four weeks showed improved memory in free recall tasks, but not in recognition tests. In a second study, patients with early Alzheimer-type dementia were treated with twenty-give gram doses of phosphatidylcholine every day for six months. Slightly improvements were shown in memory tests compared to the placebo control group. It should be noted however that other studies conducted did not find any such improvement.\n\nAlso known as cobalamin, B is an essential vitamin necessary for normal blood formation. It is also important for the maintenance of neurological function and psychiatric health. The absorption of B into the body requires adequate amounts of intrinsic factor, the glycoprotein produced in the parietal cells of the stomach lining. A functioning small intestine is also necessary for the proper metabolism of the vitamin, as absorption occurs within the ileum.\n\nB is produced in the digestive tracts of all animals, including humans. Thus, animal-origin food is the only natural food source of vitamin B However, synthesis of B occurs in the large intestine, which is past the point of absorption that occurs within the small intestine. As such, vitamin B must be obtained through diet.\n\nUnlike other B vitamins which are not stored in the body, B is stored in the liver. Because of this, it may take 5–10 years before a sudden dietary B deficiency will become apparent in a previously healthy adult. B deficiency, also known as hypocobalaminemia, often results from complications involving absorption into the body.\n\nB deficiency is often associated with pernicious anemia, as it is the most common cause. Pernicious anemia results from an autoimmune disorder which destroys the cells that produce intrinsic factor within the stomach lining, thereby hindering B absorption. B absorption is important for the subsequent absorption of iron, thus, people with pernicious anemia often present with typical symptoms of anemia, such as pale skin, dizziness, and fatigue.\n\nAmong those at highest risk for B deficiency are the elderly population, as 10-15% of people aged 60+ may present with some form of hypocobalaminemia. High rates of deficiency in the elderly commonly results from the decrease of functional absorption of B, as production of intrinsic factor declines with age. However, pernicious anemia is the most common cause of B deficiency in North American and European populations.\n\nThose afflicted with various gastrointestinal diseases may also be at risk for deficiency as a result of malabsorption. These diseases may affect production of intrinsic factor in the stomach, or of pancreatic bile. Diseases that involve disorders of the small intestine, such as celiac disease, Crohn's disease and ileitis, may also reduce B absorption. For example, people with celiac disease may damage the microvilli within their small intestines through the consumption of gluten, thereby inhibiting absorption of B as well as other nutrients.\n\nA diet low in B, whether voluntary or not, can also cause symptoms of hypocobalaminemia. Many rich sources of B come from animal meats and by-products. Populations in developing countries may not have access to these foods on a consistent basis, and as a result may become deficient in B. In addition, vegans, and to a lesser extent vegetarians, are at risk for consuming a diet low in cobalamin as they voluntarily abstain from animal sources of B. A combination of these two scenarios may increase prevalence of cobalamin deficit. For instance, B deficiency is problematic in India, where the majority of the population is vegetarian and the scarcity of meat consumption is common for omnivores as well.\n\nAn assortment of neurological effects can be observed in 75-90% of individuals of any age with clinically observable B deficiency. Cobalamin deficiency manifestations are apparent in the abnormalities of the spinal cord, peripheral nerves, optic nerves, and cerebrum. These abnormalities involve a progressive degeneration of myelin, and may be expressed behaviourally through reports of sensory disturbances in the extremities, or motor disturbances, such as gait ataxia. Combined myelopathy and neuropathy are prevalent within a large percentage of cases. Cognitive changes may range from loss of concentration to memory loss, disorientation, and dementia. All of these symptoms may present with or without additional mood changes. Mental symptoms are extremely variable, and include mild disorders of mood, mental slowness, and memory defect. Memory defect encompasses symptoms of confusion, severe agitation and depression, delusions and paranoid behaviour, visual and auditory hallucinations, urinary and fecal incontinence in the absence of overt spinal lesions, dysphasia, violent maniacal behaviour, and epilepsy. It has been suggested that mental symptoms could be related to a decrease in cerebral metabolism, as caused by the state of deficiency. All of these symptoms may present with or without additional mood changes.\n\nMild to moderate cases of pernicious anemia may show symptoms of bleeding gums, headache, poor concentration, shortness of breath, and weakness. In severe cases of pernicious anemia, individuals may present with various cognitive problems such as dementia, and memory loss.\n\nIt is not always easy to determine whether B deficiency is present, especially within older adults. Patients may present with violent behaviour or more subtle personality changes. They may also present with vague complaints, such as fatigue or memory loss, that may be attributed to normative aging processes. Cognitive symptoms may mimic behaviour in Alzheimer's and other dementias as well. Tests must be run on individuals presenting with such signs to confirm or negate cobalamin deficiency within the blood.\n\nPatients deficient in B despite normal absorption functionality may be treated through oral administration of at least 6 mg of the vitamin in pill form. Patients who suffer from irreversible causes of deficiency, such as pernicious anemia or old age, will need lifelong treatment with pharmacological doses of B. Strategy for treatment is dependent on the patient's level of deficiency as well as their level of cognitive functioning. Treatment for those with severe deficiency involves 1000 mg of B administered intramuscularly daily for one week, weekly for one month, then monthly for the rest of the patient's life. Daily oral supplementation of B mega-doses may be sufficient in reliable patients, but it is imperative that the supplementation be continued on a lifelong basis as relapse may occur otherwise.\n\nThe progression of neurological manifestations of cobalamin deficiency is generally gradual. As a result, early diagnosis is important or else irreversible damage may occur. Patients who become demented usually show little to no cognitive improvement with the administration of B.\n\nA deficiency in folate may produce anemia similar to the anemia resulting from B deficiency. There is risk that folic acid administered to those with B deficiency may mask anemic symptoms without solving the issue at hand. In this case, patients would still be at risk for neurological deficits associated with B deficiency-related anemia, which are not associated with anemia related to folate deficiency.\n\nIn addition to meeting intake requirements through food consumption, supplementation of diet with vitamin B is seen as a viable preventative measure for deficiency. It has been recommended for the elderly to supplement 50 mcg a day in order to prevent deficit from occurring.\n\nAnimal protein products are a good source of B, particularly organ meats such as kidney or liver. Other good sources are fish, eggs, and dairy products. It is suggested that vegans, who consume no animal meat or by-products, supplement their diet with B. While there are foods fortified with B available, some may be mislabelled in an attempt to boost their nutritional claims. Products of fermentation, such as algae extracts and sea vegetables, may be labelled as sources of B, but actually contain B analogues which compete for the absorption of the nutrient itself. In order to get adequate amounts of the vitamin, orally administered pills or fortified foods such as cereals and soy milk, are recommended for vegans.\n\nVitamin D is an essential regulator of the vitamin D receptor that controls gene transcription during development. The vitamin D receptor is strongly expressed in the substantia nigra. Accordingly, vitamin D deficiency can disrupt neurogenesis leading to altered dopamine signaling and increased exploratory behavior in rats. This is considered a rodent model of the schizophrenia phenotype and vitamin D deficiency has been proposed as an explanation for the increased incidence of schizophrenia among children that were conceived during winter months. A Finnish study found that vitamin D supplement use is associated with reduced risk of schizophrenia.\n\nFatty acids are necessary for the synthesis of cell membranes neurotransmitters and other signaling molecules. While excessive fat intake can be harmful, deficiency of essential fatty acids can disrupt neurodevelopment and synaptic plasticity.\n\nConsuming large amounts of saturated fat can negatively affect the brain. Eating foods with saturated fats elevates the level of cholesterol and triglycerides in the body. Studies have shown that high levels of triglycerides strongly link with mood problems such as depression, hostility and aggression. This may occur because high triglyceride levels decrease the amount of oxygen that blood can carry to the brain. The American Heart Association recommends that people consume no more than 16g of saturated fat daily. Common sources of saturated fat are meat and dairy products.\n\nThere are two kinds of essential fatty acids that people must consume (omega-3 and omega-6). Many academics recommend a balanced amount of omega-3s and omega-6s. However, some estimate that Americans consume twenty times more omega-6s than omega-3s. There is a theory that an imbalance of essential fatty acids may lead to mental disorders such as depression, hyperactivity and schizophrenia, but it still lacking evidences. An omega-3 deficient diet increases omega-6 levels in the brain disrupting endocannabinoid signaling in the prefrontal cortex and nucleus accumbens contributing to anxiety and depression-like behaviors in mice. Sources of omega-3 include flax seeds, chia seeds, walnuts, sea vegetables, green leafy vegetables, and cold water fish. Sources of omega-6 include walnuts, hazelnuts; sunflower, safflower, corn, and sesame oils.\n\nWhile cholesterol is essential for membranes and steroid hormones, excess cholesterol affects blood flow impairing cognitive function in vascular dementia.\n\nStudies have shown that learning and memory improve after consuming carbohydrates. There are two kinds of carbohydrates people consume: simple and complex. Simple carbohydrates are often found in processed foods and release sugar into the bloodstream quickly after consumption. Complex carbohydrates are digested more slowly and therefore cause sugar to be released into the bloodstream more slowly. Good sources of complex carbohydrates are whole-grain breads, pasta, brown rice, oatmeal, and potatoes. It is recommended that people consume more complex carbohydrates because consuming complex carbohydrates will cause the level of sugar in the bloodstream to be more stable, which will cause less stress hormones to be released. Consuming simple carbohydrates may cause the levels of sugar in the bloodstream to rise and fall, which can cause mood swings.\n\nThe ketone body beta-hydroxybutyrate is a fuel source for the brain during times of fasting when blood glucose levels fall. Although the mechanism is not understood, it is well established that eating a diet low in carbohydrates can be therapeutic for children with epilepsy. This is likely a result of ketone bodies providing an alternative fuel source to glucose for neuronal function. Furthermore, a ketogenic diet can be beneficial for dementia patients. Medium-chain triglycerides can stimulate ketone synthesis and coconut oil is a rich source of medium chain triglycerides that several anecdotal reports suggest can improve cognitive function in Alzheimer's type dementia patients.\n\nWhen protein is consumed, it is broken down into amino acids. These amino acids are used to produce many things like neurotransmitters, enzymes, hormones, and chromosomes. Proteins known as complete proteins contain all eight of the essential amino acids. Meat, cheese, eggs, and yogurt are all examples of complete proteins. Incomplete proteins contain only some of the eight essential amino acids and it is recommended that people consume a combination of these proteins. Examples of incomplete proteins include nuts, seeds, legumes, and grains. When animals are fed a diet deficient in essential amino acids, uncharged tRNAs accumulate in the anterior piriform cortex signaling diet rejection [105]. The body normally interconverts amino acids to maintain homeostasis, but muscle protein can be catabolized to release amino acids during conditions of amino acid deficiency. Disruption of amino acid metabolism can affect brain development and neurophysiology to affect behavior. For example, fetal protein deficiency decreases the number of neurons in the CA1 region of the hippocampus.\n\nGlutamate is a proteinogenic amino acid and neurotransmitter, though it is perhaps publicly best known in its sodium salt form: monosodium glutamate (MSG). It is also a flavor on its own, producing the umami or savory flavor found in many fermented foods such as cheese. As an amino acid it acts as a source of fuel for various cellular functions and as a neurotransmitter. Glutamate operates as an excitatory neurotransmitter and is released when a nerve impulse excites a glutamate producing cell. This in turn binds to neurons with glutamate receptors, stimulating them.\n\nGlutamate is a nutrient that is extremely difficult to be deficient in, as, being an amino acid, it is found in every food that contains protein. Additionally it is found, as previously mentioned, in fermented foods and in foods containing monosodium glutamate. As such, good sources of glutamate include meat, fish, dairy products and a wide array of other foods. Glutamate is also absorbed extremely efficiently by the intestines. However, there are instances of glutamate deficiency occurring, but only in cases where genetic disorders are present. One such example is \"Glutamate formiminotransferase deficiency\" and can cause either minor or profound physical and intellectual disabilities, depending on the severity of the condition. This disorder is extremely rare however, as only twenty people have so far been identified with this condition. Glutamate, while critically important in the body also acts as an excitotoxin in high concentrations not normally found outside of laboratory conditions, although it can occur following brain injury or spinal cord injury.\n\nL-Phenylalanine is biologically converted into L-tyrosine, another one of the DNA-encoded amino acids, and beta-phenethylamine. L-tyrosine in turn is converted into L-DOPA, which is further converted into dopamine, norepinephrine (noradrenaline), and epinephrine (adrenaline). The latter three are known as the catecholamines. Phenethylamine is further converted into N-methylphenethylamine. Phenylalanine uses the same active transport channel as tryptophan to cross the blood–brain barrier, and, in large quantities, interferes with the production of serotonin.\n\nToxic levels of phenylalanine accumulate in the brains of patients with phenylketonuria leading to severe brain damage and mental retardation. To prevent brain damage, these individuals can restrict dietary phenylalanine intake by avoiding protein and supplementing their diet with essential amino acids.\n\n"}
{"id": "13115986", "url": "https://en.wikipedia.org/wiki?curid=13115986", "title": "Nutritional rating systems", "text": "Nutritional rating systems\n\nNutritional rating systems are methods of ranking or rating food products or food categories to communicate the nutritional value of food in a simplified manner to a target audience. Rating systems are developed by governments, nonprofit organizations, or private institutions and companies.\n\nThe methods may use point systems to rank or rate foods for general nutritional value or they may rate specific food attributes such as cholesterol content. Graphics or other symbols may be used to communicate the ratings to the target audience.\n\nNutritional rating systems differ from nutritional labeling in that they attempt to simplify food choices, rather than listing specific amounts of nutrients or ingredients. Dietary guidelines are similar to nutritional rating systems in that they attempt to simplify the communication of nutritional information, however, they do not rate individual food products.\n\nGlycemic index is a ranking of how quickly food is metabolized into glucose when digested. It compares available carbohydrates gram for gram in individual foods, providing a numerical, evidence-based index of postprandial (post-meal) glycemia. The concept was invented by Dr. David J. Jenkins and colleagues in 1981 at the University of Toronto.\n\nThe glycemic load (GL) of food is a number that estimates how much the food will raise a person's blood glucose level after eating it.\n\nGuiding Stars is patented food rating system that rates food based on nutrient density using a scientific algorithm. Foods are credited for vitamins, minerals, dietary fibre, whole grains, and Omega-3 fatty acids, and debited for saturated fats, trans fats, added sodium (salt) and added sugar. Rated foods are marked with tags indicating one, two or three stars. The program first launched a Hannaford Supermarkets in 2006, and is currently found in about 1,900 supermarkets in the US and Canada. Guiding Stars has also expanded into public schools, colleges and hospitals.\n\nThe evidence-based proprietary algorithm is based on the dietary guidelines and recommendations of regulatory and health organizations including the US Food and Drug Administration, the US Department of Agriculture, and the World Health Organization. It was developed by a scientific advisory panel composed of experts in nutrition and health from Dartmouth College, Harvard University, Tufts University, University of North Carolina and other colleges.\n\nNutripoints is a system for rating foods on a numerical scale for their overall nutritional value. The method is based on an analysis of 26 positive (such as vitamins, minerals, protein, fiber) and negative factors (such as cholesterol, saturated fat, sugar, sodium) compared to the calories in the food. The overall Nutripoint score of the food is the result. The higher the value, the more nutrition per calorie (nutrient density) and the least negative factors in the food. Thus, the higher the Nutripoint score, the better the food for overall health. The system rates 3600+ foods including basic foods like apples and oranges, fast-foods, and brand-name foods.\n\nNutripoints was developed by Dr. Roy E. Vartabedian (a Doctor of Public Health) in the 1980s and was released to the general public in 1990 with his book, \"Nutripoints,\" published in 13 countries and 10 languages worldwide. The food rating system is part of an overall program designed to help people measure, balance, and upgrade their diet's nutritional quality for overall health improvement and well-being.\n\nThe Nutrition iQ program is a joint venture of the Joslin Clinic and multi-banner supermarket operator Supervalu. The labeling system consists of color-coded tags denoting a food product's superior status with respect to attributes such as vitamin and mineral content, fiber content, 100% juice content, Omega-3 or low saturated fat content, whole grain content, calcium content, protein content, low or reduced sodium content and low or reduced caloric content. The first phase of the program launched in 2009 covering center store food products, with coverage of fresh food departments rolling out in 2011.\n\nWeight Watchers developed the POINTS Food System for use with their Flex Plan. Healthy weight control is the primary objective of the system. The system is designed to allow customers to eat any food while tracking the number of points for each food consumed. Members try to keep to their POINTS Target, a number of points for a given time frame. The daily POINTS Target is personalized based on members' height, weight and other factors, such as gender. A weekly allowance for points is also established to provide for special occasions, mistakes, etc.\n\nDeveloped by Adam Drewnowski, University of Washington, NNR \"is based on mean percentage daily values (DVs) for 14 nutrients in 2000 kcal food, can be used to assign nutrient density values to foods within and across food groups. Use of the NNR score allows consumers to identify and select nutrient-dense foods while permitting some flexibility where the discretionary calories are concerned.\"\n\nDeveloped by ReViVer, a nutritionally-oriented restaurant in New York City, the ReViVer Score expresses the nutrient density of menu items from a variety of fast food and casual restaurants, based on the amount of 10 nutrients (Vitamins A, C, and E, folate, calcium, magnesium, potassium, iron, fiber, and omega-3 fats) per calorie. A score of 100 indicates that the meal provides at least 100% of the recommended daily intake for all ten nutrients, proportionate to the energy (calorie) content of the meal. The score allows consumers to compare the nutritional quality of various restaurant offerings with similar calorie content.\n\nDeveloped by Efficiency Is Everything, a Data Science website. This expresses the amount of micronutrients in produce per the caloric value based on 40+ vitamins and minerals. Based off a sum of each nutrient vs that nutrients maximum value per calorie, Spinach, Parsely, Romaine Lettuce, and turnip greens top the list. This allows consumers to compare the nutritional quality of various produce.\n\nLaunched late in 2009, the Smart Choices Program (SCP) was a rating system developed by a coalition of companies from the food industry. The criteria for rating food products used 18 different attributes, however, the system had varying levels of acceptability based on 16 types of food which allowed for wide discretion in the selection of foods to include in the program. The program was discontinued in October 2009 after sharp criticism for including products such as \"Froot Loops\", \"Lucky Charms\" and \"Frosted Flakes\" as Smart Choices. As a consequence of the backlash from the program, General Mills announced on December 10, 2009 that it would reduce the amount of sugar in many of its cereal brands.\n\nOn August 19, 2009, the FDA wrote a letter to SCP manager, saying: \"FDA and FSIS would be concerned if any FOP labeling systems used criteria that were not stringent enough to protect consumers against misleading claims; were inconsistent with the Dietary Guidelines for Americans; or had the effect of encouraging consumers to choose highly processed foods and refined grains instead of fruits, vegetables, and whole grains.\" SCP was suspended in 2009 after the FDA's announcement that they will be addressing both on front-of- package and on-shelf systems. SCP Chair Mike Hughes said: \"It is more appropriate to postpone active operations and channel our information and learnings to the agency to support their initiative.\"\n\nSmart Choices Program \"was developed by a coalition of scientists and nutrition educators, experts with experience with dietary guidelines, public health organizations, and food manufacturers in response to consumer confusion over multiple front-of-the-package systems based on different criteria. Representatives from different government organizations acted as observers. The process of developing the program was facilitated by the nonprofit Keystone Center, an organization that develops consensus solutions to complex health and social policy changes. The nutrition criteria for receiving the SCP icon are specific for product category by indicating \"smarter\" products within that category. A calorie indicator noting calories per serving and servings per package accompanies the SCP icon to remind consumers that calories do count, even for smarter food choices. For a product to qualify, it first has to be below the threshold for \"nutrients to limit\" and then (in most cases) it must be above the threshold for one or more nutrients or food groups to encourage. The criteria are based on the 2005 Dietary Guidelines and other consensus science and are transparent and available on the SCP website.\"\n"}
{"id": "14220245", "url": "https://en.wikipedia.org/wiki?curid=14220245", "title": "Occupational rehabilitation", "text": "Occupational rehabilitation\n\nOccupational rehabilitation is the science and practices of returning injured workers to a level of work activity that is appropriate to their functional and cognitive capacity, both of which are influenced by the severity of a worker's injuries.\n\n\nMany workers have an increased risk of developing common mental disorders (CMDs) in the workplace due to job stressors such as job insecurity, bullying or psychological harassment, low social support at work, employee perceptions of fairness in the workplace, and an imbalance between job demands and rewards. These CMDs may include anxiety disorders, alcohol dependence, addiction-related disorders, suicidal ideation, and depression \n\nA symptom of CMDs is having disorganized and deteriorated habits. Therefore, during work rehabilitation, occupational therapists and/or other rehabilitation professionals often use a graded environment, intentionally eliminating barriers to increase individuals’ performance and self-esteem. An integrative approach, based on the three key disciplines of medicine, public health, and psychology, is being utilized by occupational therapists to reduce job stressors and improve the psychological well-being of employees with CMDs. The purpose of an integrative approach is to prevent further harm to the employee and to learn how to manage the illness through health promotion, occupational psychology, positive psychology management, psychiatry, and occupational medicine.\n\nCognitive work hardening programs administered by occupational therapists using the Canadian Model of Client-Centered Enablement (CMCE) improve return to work outcomes of employees suffering from depression. Cognitive work hardening incorporates meaningful occupations or work tasks that are graded to fit individual needs within an environment that is supportive in order to improve self-worth. Cognitive work hardening programs are individualized to promote interpersonal communication and coping skills within a real-life work setting.\n\nThe Stimulating Healthy Participation and Relapse Prevention (SHARP) approach is used for individuals with CMDs who experience many sick absences from work. The SHARP approach encompasses five steps including: listing positive and negative situations encountered in the workplace; solutions to negative situations or problems; support need for solutions; planning how to implement solutions; and evaluation of implementation \n"}
{"id": "49986510", "url": "https://en.wikipedia.org/wiki?curid=49986510", "title": "Paula method", "text": "Paula method\n\nThe Paula Method is a proposed alternative to Kegel exercises. The idea is that by strengthening your sphincter muscles (eye muscle: orbicularis oculi and mouth muscle: orbicularis oris), the contractions would also strengthen the sphincter muscles in the pelvic floor. Evidence to support its use is lacking.\n"}
{"id": "1479978", "url": "https://en.wikipedia.org/wiki?curid=1479978", "title": "Phytic acid", "text": "Phytic acid\n\nPhytic acid (known as inositol hexakisphosphate (IP6), inositol polyphosphate, or phytate when in salt form) is the phosphate ester of inositol. It contains six phosphate groups. At physiological pH, these phosphates are partially ionized. The resulting anion is a colorless species that has significant nutritional role as the principal storage form of phosphorus in many plant tissues, especially bran and seeds. It is also present in many legumes, cereals, and grains. Phytic acid has a strong binding affinity to the dietary minerals, calcium, iron, and zinc, inhibiting their absorption.\n\nCatabolites of phytic acid are called lower inositol polyphosphates. Examples are inositol penta- (IP5), tetra- (IP4), and triphosphate (IP3).\n\nPhosphorus and inositol in phytate form are not, in general, bioavailable to nonruminant animals because these animals lack the digestive enzyme phytase required to hydrolyze (break) the inositol-phosphate linkages. Ruminants are readily able to digest phytate because of the phytase produced by rumen microorganisms.\n\nIn most commercial agriculture, non-ruminant livestock, such as swine, fowl, and fish, are fed mainly grains, such as maize, legumes, and soybeans. Because phytate from these grains and beans is unavailable for absorption, the unabsorbed phytate passes through the gastrointestinal tract, elevating the amount of phosphorus in the manure. Excess phosphorus excretion can lead to environmental problems, such as eutrophication. The use of sprouted grains will reduce the quantity of phytic acids in feed, with no significant reduction of nutritional value.\n\nAlso, viable low-phytic acid mutant lines have been developed in several crop species in which the seeds have drastically reduced levels of phytic acid and concomitant increases in inorganic phosphorus. However, germination problems have reportedly hindered the use of these cultivars thus far. This may be due to phytic acid's critical role in both phosphorus and metal ion storage. Phytate variants also have the potential to be used in soil remediation, to immobilize uranium, nickel and other inorganic contaminants.\n\nAlthough indigestible for many animals, phytic acid and its metabolites as they occur in seeds and grains have several important roles for the seedling plant.\n\nMost notably, phytic acid functions as a phosphorus store, as an energy store, as a source of cations and as a source of myoinositol (a cell wall precursor). Phytic acid is the principal storage form of phosphorus in plant seeds.\n\nIn animal cells, myoinositol polyphosphates are ubiquitous, and phytic acid (myoinositol hexakisphosphate) is the most abundant, with its concentration ranging from 10 to 100 µM in mammalian cells, depending on cell type and developmental stage.\n\nThis compound is not obtained from the animal diet, but must be synthesized inside the cell from phosphate and inositol (which in turn is produced from glucose, usually in the kidneys). The interaction of intracellular phytic acid with specific intracellular proteins has been investigated \"in vitro\", and these interactions have been found to result in the inhibition or potentiation of the physiological activities of those proteins. The best evidence from these studies suggests an intracellular role for phytic acid as a cofactor in DNA repair by nonhomologous end-joining. Other studies using yeast mutants have also suggested intracellular phytic acid may be involved in mRNA export from the nucleus to the cytosol.\n\nInositol hexaphosphate facilitates the formation of the six-helix bundle and assembly of the immature HIV-1 Gag lattice. IP6 makes ionic contacts with two rings of lysine residues at the centre of the Gag hexamer. Proteolytic cleavage then unmasks an alternative binding site, where IP6 interaction promotes the assembly of the mature capsid lattice. These studies identify IP6 as a naturally occurring small molecule that promotes both assembly and maturation of HIV-1.\n\nPhytic acid was discovered in 1903. Phytic acid, mostly as phytate in the form of phytin, is found within the hulls of seeds, including nuts, grains and pulses. In-home food preparation techniques can break down the phytic acid in all of these foods. Simply cooking the food will reduce the phytic acid to some degree. More effective methods are soaking in an acid medium, sprouting and lactic acid fermentation such as in sourdough and pickling. No detectable phytate (less than 0.02% of wet weight) was observed in vegetables such as scallion and cabbage leaves or in fruits such as apples, oranges, bananas, or pears.\n\nAs a food additive, phytic acid is used as the preservative, E391.\n\nChestnuts contain 47 mg of phytic acid for 100g.\n\nPhytic acid has a strong binding affinity to the dietary minerals, calcium, iron, and zinc, inhibiting their absorption. Phytochemicals like polyphenols and tannins also influence the binding. When iron and zinc bind to phytic acid, they form insoluble precipitates and are far less absorbable in the intestines. This process can therefore contribute to iron and zinc deficiencies in people whose diets rely on these foods for their mineral intake, such as those in developing countries and vegetarians.\n\nBecause phytic acid can affect the absorption of iron, it has been proposed that \"dephytinization should be considered as a major strategy to improve iron nutrition during the weaning period\".\n\n"}
{"id": "45236241", "url": "https://en.wikipedia.org/wiki?curid=45236241", "title": "PositiveSingles", "text": "PositiveSingles\n\nPositiveSingles is a free friendship, social and dating website that specifically caters to people who are living with sexually transmitted diseases (STD). Its services are mainly provided in North America and Europe. PositiveSingles was founded in 2001 and its headquarters are in Vaughan, Ontario. The company is privately held.\n\nLaunched in Mountain View, California in 2001, PositiveSingles claims to be \"The best, largest, completely anonymous and most trusted online dating site for people with Herpes, HPV, HIV / AIDS, Hepatitis, Chlamydia, Gonorrhea, Syphilis and other STD\" on its homepage.\n\nThe slogan of PositiveSingles is \"\"Stay Positive! Find Love, Support & Hope\" in contrast to its previous slogan \"We Are Not Alone!\"\"\n\nPositiveSingles was founded in 2001 as a free STD dating site. Membership was divided into guest and gold membership a year later.\n\nIn 2007, STD Q & A (live counselor) was added on PositiveSingles, where every member can anonymously (if they choose to) ask questions about STDs and STD counselor will reply to each one. Members can put forward their own advice as well. Any information provided by users will be kept confidential.\n\nIn July, 2010, PositiveSingles added a feature named STD Legal Information. This feature was made on account of the particularity of the special group. PositiveSingles members can obtain legal information regarding their STD status as well as learn how to protect themselves from legal encounters.\n\nIn 2012, PositiveSingles launched its apps for iOS and Android devices, which provide full functionality as the website.\n\nPositiveSingles charges no fee when users register, search or browse profiles, send winks to another user. However, gold membership (i.e., paid) is required to use some of its premium features.\n\n"}
{"id": "83896", "url": "https://en.wikipedia.org/wiki?curid=83896", "title": "Priapism", "text": "Priapism\n\nPriapism is a condition in which a penis remains erect for hours in the absence of stimulation or after stimulation has ended. There are three types: ischemic (low-flow), nonischemic (high-flow), and recurrent ischemic (intermittent). Most cases are ischemic. Ischemic priapism is generally painful while nonischemic priapism is not. In ischemic priapism, most of the penis is hard; however, the glans penis is not. In nonischemic priapism, the entire penis is only somewhat hard. Very rarely, clitoral priapism occurs in women.\nSickle cell disease is the most common cause of ischemic priapism. Other causes include medications such as antipsychotics, SSRIs, and blood thinners, as well as drugs such as cocaine and cannabis. Ischemic priapism occurs when blood does not adequately drain from the penis. Nonischemic priapism is typically due to a connection forming between an artery and the corpus cavernosum or disruption of the parasympathetic nervous system resulting in increased arterial flow. Nonischemic priapism may occur following trauma to the penis or a spinal cord injury. Diagnosis may be supported by blood gas analysis of blood aspirated from the penis or ultrasound.\nTreatment depends on the type. Ischemic priapism is typically treated with a nerve block of the penis followed by aspiration of blood from the corpora cavernosa. If this is not sufficient, the corpus cavernosum may be irrigated with cold normal saline or injected with phenylephrine. Nonischemic priapism is often treated with cold packs and compression. Surgery may be done if usual measures are not effective. In ischemic priapism, the risk of permanent scarring of the penis begins to increase after four hours and definitely occurs after 48 hours. Priapism occurs in about 1 in 20,000 to 1 in 100,000 males per year.\n\nPriapism is classified into three groups: ischemic (low-flow), nonischemic (high-flow), and recurrent ischemic. The majority of cases (19 out of 20) are ischemic in nature.\n\nSome sources give a duration of four hours as a definition of priapism, but others give six: \"The duration of a normal erection before it is classifiable as priapism is still controversial. Ongoing penile erections for more than 6 hours can be classified as priapism.\"\n\nPriapism in females (continued, painful erection of the clitoris) is significantly rarer than priapism in men, and is known as clitoral priapism or clitorism. It is associated with persistent genital arousal disorder (PGAD). Only a few case reports of women experiencing clitoral priapism exist.\n\nBecause ischemic priapism causes the blood to remain in the penis for unusually long periods of time, the blood becomes deprived of oxygen and can cause damage to the penile tissue itself. Should the penile tissue become damaged, it can result in erectile dysfunction or disfigurement of the penis. In extreme cases, if the penis develops severe vascular disease, the priapism can result in penile gangrene.\n\nPriapism may be associated with haematological disorders, especially sickle-cell disease, sickle-cell trait, and other conditions such as leukemia, thalassemia, and Fabry's disease, and neurologic disorders such as spinal cord lesions and spinal cord trauma (priapism has been reported in people who have been hanged; see death erection).\n\nPriapism may also be associated with glucose-6-phosphate dehydrogenase deficiency, which leads to decreased NADPH levels. NADPH is a co-factor involved in the formation of nitric oxide, which may result in priapism.\n\nSickle cell disease often presents special treatment obstacles. Hyperbaric oxygen therapy has also been used with success in some patients. Priapism is also found to occur in extreme cases of rabies. Priapism also occurs due to encephalitic rabies.\n\nPriapism can also be caused by reactions to medications. The most common medications that cause priapism are intra-cavernous injections for treatment of erectile dysfunction (papaverine, alprostadil). Other groups reported are antihypertensives, antipsychotics (e.g., chlorpromazine, clozapine), antidepressants (most notably trazodone), anti-convulsant and mood stabilizer drugs such as sodium valproate, anticoagulants, cantharides (\"Spanish Fly\") and recreational drugs (alcohol, heroin and cocaine). Priapism is also known to occur from bites of the Brazilian wandering spider and the black widow spider.\n\nThe mechanisms are poorly understood but involve complex neurological and vascular factors.\n\nThe diagnosis is often based on the history of the condition and the physical exam.\n\nBlood gas testing the blood from the cavernosa of the penis can help in the diagnosis. If the low flow type of priapism is present the blood typically has a low pH while if the high flow type is present the pH is typically normal. Color doppler ultrasound may also help differentiate the two. Testing a person to make sure they do not have a hemoglobinopathy may also be reasonable.\n\nMedical evaluation is recommended for erections that last for longer than four hours. Pain can often be reduced with a dorsal penile nerve block or penile ring block. For those with nonischemic priapism cold packs and pressure to the area may be sufficient.\n\nFor those with ischemic priapism the initial treatment is typically aspiration of blood from the corpus cavernosum. This is done on either side. If this is not sufficiently effective then cold normal saline may be injected and removed.\n\nIf aspiration is not sufficient a small dose of phenylephrine may be injected into the corpus cavernosum. Side effects of phenylephrine may include high blood pressure, slow heart rate, and arrhythmia. If this medication is used, it is recommended that people be monitored for at least an hour after. For those with recurrent ischemic priapism diethylstilbestrol (DES) or terbutaline may be tried.\n\nDistal shunts, such as the Winter's, involve puncturing the glans (the distal part of the penis) into one of the cavernosa, where the old, stagnant blood is held. This causes the blood to leave the penis and return to the circulation. This procedure can be performed by a urologist at the bedside. Winter's shunts are often the first invasive technique used, especially in hematologically induced priapism, as it is relatively simple and repeatable.\n\nProximal shunts, such as the Quackel's, are more involved and entail operative dissection in the perineum to where the corpora meet the spongiosum, making an incision in both, and suturing both openings together. Shunts created between corpora cavernosa and saphenous vein called Grayhack shunt can be done though this technique is rarely used.\n\nAs the complication rates with prolonged priapism are high, early penile prosthesis implantation may be considered. As well as allowing early resumption of sexual activity, early implantation can avoid the formation of dense fibrosis and hence a shortened penis.\n\nIn sickle-cell anemia treatment is initially with intravenous fluids, pain medication, and oxygen therapy. The typical treatment of priapism may be carried out as well. Blood transfusions are not usually recommended as part of the initial treatment but if other treatments are not effective exchange transfusion may be done.\n\nPersistent semi-erections or intermittent states of prolonged erections have historically been sometimes called semi-priapism.\n\nThe name comes from the Greek god Priapus (), a fertility god often represented with a disproportionately large and permanent erection.\n\n"}
{"id": "40014144", "url": "https://en.wikipedia.org/wiki?curid=40014144", "title": "Ramboll Environ", "text": "Ramboll Environ\n\nENVIRON was a privately held, international environmental, safety and health sciences consulting firm headquartered in Arlington, Virginia. ENVIRON had operations across more than 90 offices in 21 countries, with more than 1,500 consultants when it was acquired in December 2014 by Danish-based Ramboll\n\nIn a transition period, legacy ENVIRON was rebranded as Ramboll Environ, Inc. Since January 1 2018, ENVIRON no longer exists as a separate business entity, and is now part of the Water and Environment & Health divisions of Ramboll.\n\nThe firm was founded as ENVIRON in Washington, D.C., in 1982.\n\nIn 1996 the firm acquired EAG, a consulting firm in the United Kingdom. In 2003 ENVIRON merged with Applied Epidemiology, Inc., a provider of epidemiological consulting services in occupational health, environmental health and injury, and disability research. ENVIRON then merged in 2005 with The ADVENT Group, an engineering consulting firm specializing in industrial wastewater management and related areas, and developer of the patented ADVENT Integral System (AIS).\n\nIn 2007 ENVIRON acquired Boelter Associates, Inc., a consulting firm with expertise in industrial hygiene and building forensics. In 2009, the firm acquired Brazilian consultancy ARQUIPÉLAGO Engenharia Ambiental Ltda.(Arquipélago).\n\nENVIRON acquired Mexican firm Hicks Environmental in February 2014, expanding into Mexico with an office in Monterrey. In March 2014, ENVIRON established operations in Yangon, Myanmar, becoming the first global environmental consultancy in the country.\n\nThe Danish company Ramboll acquired ENVIRON 2014-12-16. January 1 2018, legacy Environ was included in the Water and Environment & Health divisions of Ramboll, and Environ no longer exists as a separate business entity.\nThe firm offered various environmental, health and safety, sustainability and health sciences services, including air quality and climate change management, regulatory compliance assistance, due diligence, remedial design and engineering, ecology and sediment management, and risk and exposure assessment and management.\n\nIn its April 28, 2014 edition, Engineering News-Record ranked ENVIRON Holdings, Inc., as the 27th largest pure design firm; the 35th largest design firm in international markets; and the 49th largest overall U.S. design firm.\n\nENVIRON provided assistance in assessing and mitigating potential environmental risks. Key services included: air quality management; climate change and energy management; due diligence; ecology and sediment management; risk assessment and management; and site investigation and remediation.\n\nThe firm provided assistance so that industrial facilities are regulatory compliant, managed potential liabilities, and assessed environmental, health, and safety risks when making an acquisition. Key services included: Building Performance and Property Loss Consulting, Compliance Assistance, Dose Reconstruction, Occupational Health and Safety; and Site Investigation and Remediation.\n\nENVIRON assessed and addressed potential human health risks associated with environmental, residential and workplace exposures and product-related exposures. This work included product safety and stewardship, industrial hygiene and safety, risk assessment and management, and industrial chemical safety evaluations.\n\nAt the request of the Hong Kong Environmental Protection Department the company developed a comprehensive air quality model system to address air quality concerns, including ozone, particulate matter, and other pollutants due to local sources, as well as transport from China and other areas in Asia.\n\nENVIRON was engaged to deliver a wide range of social and environmental services to international standards related to a 13,000-km-long fibre optic submarine telecommunications cable system running the entire length of the Indian Ocean coast of Africa, from South Africa to Egypt.\n\nIn the United States, the firm worked with the Marathon Petroleum Company at its major gulf coast refinery in Garyville, Louisiana, to develop VOC BioTreat™, a patent-pending biotreatment solution to reduce volatile organic compound emissions at oil and chemical refineries. The company asserts that the system achieves regulatory compliance and cost savings of an order of magnitude.\n\nTheir studies uncovered the chemical mechanism that established a link between certain types of gypsum wallboard (Chinese drywall) and corrosive effects on copper and silver components in residential properties across the southeastern United States. ENVIRON scientists were the first to recognize and publish findings about the progressive nature of the corrosion.\n\nBP Remediation Management awarded ENVIRON four framework agreements for a range of strategic worldwide environmental services. Two agreements call for the firm to provide a broad spectrum of environmental services for upstream, refining and marketing, shipping and remediation that include regulatory compliance; multimedia modeling; environmental, social and health impact assessments; natural resource management; due diligence support; and sustainability services around the world. Two additional agreements focus on global inland and offshore oil spill preparedness and response planning.\n\nThe company and another prominent engineering and scientific firm, Exponent, were employed by Georgia-Pacific to conduct research intended for its defense in an asbestos-related lawsuit. Controversially, the dissemination of the research was controlled by Georgia-Pacific and is subject to ongoing litigation.\n\nAn environmental impact assessment (EIA) prepared for the proposed expansion of the Kemira Chemicals production plant in Sastamala, Finland received an honorary award from the Finnish National EIA Association (YVA ry) for “exceptionally good assessment of risks and the potential environmental impacts of disturbances and emergency situations”.\n\nThe U.S. Environmental Protection Agency USEPA selected Principal Dr. Carl Adams to join its task force for the Wastewater Treatment and Related Modeling Technical Workshop. During the opening panel of the first task force meeting, Dr. Adams spoke on Zero Discharge of Water from Hydrofracturing Activities, a unique concept he has developed to recycle 100 percent of all wastewaters related to hydraulic fracturing activities, including spills and rainwater. The concept involves no contact with air, ground and surface water or soils.\n\nIn January 2013, the Climate Change Business Journal awarded ENVIRON its Gold Medal Business Achievement Award in Energy and Carbon Management.\n\nThe Port of Los Angeles, an ENVIRON client, received the Environmental Campaign of the Year Award in 2012 given by Containerisation International, an international shipping publication.\n\nDP World awarded ENVIRON the Golden Dhow Award for its environmental consultancy services to the London Gateway super port.\n\nENVIRON also assisted a client in Milan, Italy that received the first award for Leadership in Energy and Environmental Design (LEED) Platinum certification, the U.S. Green Building Council’s highest certification level.\n\nIn 2011, the American Academy of Environmental Engineers Excellence in Environmental Engineering Competition awarded ENVIRON its Grand Prize, Research for ENVIRON’s American patent-pending VOC BioTreat™ Technology.\n\nAlso in 2011, the firm won the UK's Edie Environmental Excellence Award: Best Consultancy for Due Diligence.\n\nENVIRON won the Chemical Industry Awards REACH Service Provider Award 2010.\n\nRamboll Environ professionals published more than 20 articles in peer-reviewed and scientific publications since 2013.\n\nPrincipal Doug Daugherty co-authored a chapter in “Responsible Mining: Case Studies in Managing Social & Environmental Risks in the Developed World”, a trade book published by the Society for Mining, Metallurgy and Exploration (SME) in January 2015. The chapter details Molycorp’s Mountain Pass rare-earth resource and processing facility in southeastern California as it aims to become one of the largest rare-earth production centers in the world in a sustainable way that minimize environmental impacts.\n\nPrincipal P. Robinan Gentry co-authored “Evaluation of gene expression changes in human primary uroepithelial cells following 24-Hr. exposures to inorganic arsenic and its methylated metabolites,” in the publication Environmental and Molecular Mutagenesis. This publication was honored with the Editor’s Choice.\n\nFour Principals have contributed to The EU Environmental Liability: A Commentary. Written for lawyers and others, this Oxford University Press publication examines the legal, administrative, scientific, and economic issues relating to the directive.\n\nIn 2011, Dr. Joseph V. Rodricks, a co-founder and Principal of ENVIRON International, prepared a chapter on exposure science appearing in the third edition of the National Academies of Science’s Reference Manual on Scientific Evidence. He has also written for the International Journal of Toxicology.\n"}
{"id": "77668", "url": "https://en.wikipedia.org/wiki?curid=77668", "title": "Scar", "text": "Scar\n\nA scar is an area of fibrous tissue that replaces normal skin after an injury. Scars result from the biological process of wound repair in the skin, as well as in other organs and tissues of the body. Thus, scarring is a natural part of the healing process. With the exception of very minor lesions, every wound (e.g., after accident, disease, or surgery) results in some degree of scarring. An exception to this are animals with complete regeneration, which regrow tissue without scar formation.\n\nScar tissue is composed of the same protein (collagen) as the tissue that it replaces, but the fiber composition of the protein is different; instead of a random basketweave formation of the collagen fibers found in normal tissue, in fibrosis the collagen cross-links and forms a pronounced alignment in a single direction. This collagen scar tissue alignment is usually of inferior functional quality to the normal collagen randomised alignment. For example, scars in the skin are less resistant to ultraviolet radiation, and sweat glands and hair follicles do not grow back within scar tissues. A myocardial infarction, commonly known as a heart attack, causes scar formation in the heart muscle, which leads to loss of muscular power and possibly heart failure. However, there are some tissues (e.g. bone) that can heal without any structural or functional deterioration.\n\nAll scarring is composed of the same collagen as the tissue it has replaced, but the composition of the scar tissue, compared to the normal tissue, is different. Scar tissue also lacks elasticity unlike normal tissue which distributes fiber elasticity. Scars differ in the amounts of collagen overexpressed. Labels have been applied to the differences in overexpression. Two of the most common types are hypertrophic and keloid scarring, both of which experience excessive stiff collagen bundled growth overextending the tissue, blocking off regeneration of tissues. Another form is atrophic scarring (sunken scarring), which also has an overexpression of collagen blocking regeneration. This scar type is sunken, because the collagen bundles do not overextend the tissue. Stretch marks (striae) are regarded as scars by some.\n\nHigh melanin levels and either African or Asian ancestry may make adverse scarring more noticeable.\n\nHypertrophic scars occur when the body overproduces collagen, which causes the scar to be raised above the surrounding skin. Hypertrophic scars take the form of a red raised lump on the skin. They usually occur within 4 to 8 weeks following wound infection or wound closure with excess tension and/or other traumatic skin injuries.\n\nKeloid scars are a more serious form of excessive scarring, because they can grow indefinitely into large, tumorous (although benign) neoplasms.\n\nHypertrophic scars are often distinguished from keloid scars by their lack of growth outside the original wound area, but this commonly taught distinction can lead to confusion.\n\nKeloid scars can occur on anyone, but they are most common in dark-skinned people. They can be caused by surgery, accident, acne or, sometimes, body piercings. In some people, keloid scars form spontaneously. Although they can be a cosmetic problem, keloid scars are only inert masses of collagen and therefore completely harmless and not cancerous. However, they can be itchy or painful in some individuals. They tend to be most common on the shoulders and chest. Hypertrophic scars and keloids tend to be more common in wounds closed by secondary intention. Surgical removal of keloid is risky and may excerbate the condition and worsening of the keloid.\n\nAn atrophic scar takes the form of a sunken recess in the skin, which has a pitted appearance. These are caused when underlying structures supporting the skin, such as fat or muscle, are lost. This type of scarring is often associated with acne, chickenpox, other diseases (especially \"Staphylococcus\" infection), surgery, certain insect and spider bites, or accidents. It can also be caused by a genetic connective tissue disorder, such as Ehlers–Danlos syndrome.\n\nStretch marks (technically called \"striae\") are also a form of scarring. These are caused when the skin is stretched rapidly (for instance during pregnancy,<ref name=\"doi10.1002/14651858.CD000066\"></ref> significant weight gain, or adolescent growth spurts), or when skin is put under tension during the healing process, (usually near joints). This type of scar usually improves in appearance after a few years.\n\nElevated corticosteroid levels are implicated in striae development.\n\nHumans and other placental mammals have an umbilical scar (commonly referred to as to a navel) which starts to heal when the umbilical cord is cut after birth. Egg-laying animals have an umbilical scar which, depending on the species, may remain visible for life or disappear within a few days after birth.\n\nA scar is the product of the body's repair mechanism after tissue injury. If a wound heals quickly within two weeks with new formation of skin, minimal collagen will be deposited and no scar will form. When the extracellular matrix senses elevated mechanical stress loading, tissue will scar, and scars can be limited by stress shielding wounds. Small full thickness wounds under 2mm reepithilize fast and heal scar free. Deep second-degree burns heal with scarring and hair loss. Sweat glands do not form in scar tissue, which impairs the regulation of body temperature. Elastic fibers are generally not detected in scar tissue younger than 3 months old. In scars rete pegs are lost; through a lack of rete pegs scars tend to shear easier than normal tissue.\n\nThe endometrium, the inner lining of the uterus, is the only adult tissue to undergo rapid cyclic shedding and regeneration without scarring; shedding and restoring roughly inside a 7-day window on a monthly basis. All other adult tissues, upon rapid shedding or injury, can scar.\n\nProlonged inflammation, as well as the fibroblast proliferation can occur. Redness that often follows an injury to the skin is not a scar, and is generally not permanent (see wound healing). The time it takes for this redness to dissipate may, however, range from a few days to, in some serious and rare cases, a few years. \n\nScars form differently based on the location of the injury on the body and the age of the person who was injured. \n\nThe worse the initial damage is, the worse the scar will generally be. \n\nSkin scars occur when the dermis (the deep, thick layer of skin) is damaged. Most skin scars are flat and leave a trace of the original injury that caused them. \n\nWounds allowed to heal secondarily tend to scar worse than wounds from primary closure.\n\nAny injury does not become a scar until the wound has completely healed; this can take many months, or years in the worst pathological cases, such as keloids. To begin to patch the damage, a clot is created; the clot is the beginning process that results in a provisional matrix. In the process, the first layer is a provisional matrix and is not scar. Over time, the wounded body tissue then overexpresses collagen inside the provisional matrix to create a collagen matrix. This collagen overexpression continues and crosslinks the fiber arrangement inside the collagen matrix, making the collagen dense. This densely packed collagen, morphing into an inelastic whitish collagen scar wall, blocks off cell communication and regeneration; as a result, the new tissue generated will have a different texture and quality than the surrounding unwounded tissue. This prolonged collagen-producing process results in a fortuna scar.\n\nThe scarring is created by fibroblast proliferation, a process that begins with a reaction to the clot.\n\nTo mend the damage, fibroblasts slowly form the collagen scar. The fibroblast proliferation is circular and cyclically, the fibroblast proliferation lays down thick, whitish collagen inside the provisional and collagen matrix, resulting in the abundant production of packed collagen on the fibers giving scars their uneven texture. Over time, the fibroblasts continue to crawl around the matrix, adjusting more fibers and, in the process, the scarring settles and becomes stiff. This fibroblast proliferation also contracts the tissue. In unwounded tissue, these fibers are not overexpressed with thick collagen and do not contract.\n\nThe fibroblast involved in scarring and contraction is the myofibroblast, which is a specialized contractile fibroblast. These cells express α-smooth muscle actin (α-SMA).\n\nThe myofibroblasts are absent in the first trimester in the embryonic stage where damage heals scar free; in small incisional or excision wounds less than 2 mm that also heal without scarring; and in adult unwounded tissues where the fibroblast in itself is arrested; however, the myofibroblast is found in massive numbers in adult wound healing which heals with a scar.\n\nThe myofibroblasts make up a high proportion of the fibroblasts proliferating in the postembryonic wound at the onset of healing. In the rat model, for instance, myofibroblasts can constitute up to 70% of the fibroblasts, and is responsible for fibrosis on tissue.\nGenerally, the myofibroblasts disappear from the wound within 30 days, but can stay around in pathological cases in hypertrophy, such as keloids. Myofibroblasts have plasticity and in mice can be transformed into fat cells, instead of scar tissue, via the regeneration of hair follicles.\n\nEarly and effective treatment of acne scarring can prevent severe acne and the scarring that often follows. no prescription drugs for the treatment or prevention of scars were available.\n\nChemical peels are chemicals which destroy the epidermis in a controlled manner, leading to exfoliation and the alleviation of certain skin conditions, including superficial acne scars. Various chemicals can be used depending upon the depth of the peel, and caution should be used, particularly for dark-skinned individuals and those individuals susceptible to keloid formation or with active infections.\n\nFiller injections of collagen can be used to raise atrophic scars to the level of surrounding skin. Risks vary based upon the filler used, and can include further disfigurement and allergic reaction.\n\nNonablative lasers, such as the 585 nm pulsed dye laser, 1064 nm and 1320 nm , or the 1540 nm are used as laser therapy for hypertrophic scars and keloids. For burn scars they improve the appearance.\n\nAblative lasers such as the carbon dioxide laser (CO) or offer the best results for atrophic and acne scars. Like dermabrasion, ablative lasers work by destroying the epidermis to a certain depth. Healing times for ablative therapy are much longer and the risk profile is greater compared to nonablative therapy; however, nonablative therapy offers only minor improvements in cosmetic appearance of atrophic and acne scars.\n\nLow-dose, superficial radiotherapy is sometimes used to prevent recurrence of severe keloid and hypertrophic scarring. It is thought to be effective despite a lack of clinical trials, but only used in extreme cases due to the perceived risk of long-term side effects.\n\nSilicone scar treatments are commonly used in preventing scar formation and improving existing scar appearance. A meta-study by the Cochrane collaboration found weak evidence that silicone gel sheeting helps prevent scarring. However, the studies examining it were of poor quality and susceptible to bias.\n\nPressure dressings are commonly used in managing burn and hypertrophic scars, although supporting evidence is lacking. Care providers commonly report improvements, however, and pressure therapy has been effective in treating ear keloids. The general acceptance of the treatment as effective may prevent it from being further studied in clinical trials.\n\nA long-term course of corticosteroid injections into the scar may help flatten and soften the appearance of keloid or hypertrophic scars.\n\nTopical steroids are ineffective. However, clobetasol propionate can be used as an alternative treatment for keloid scars.\n\nScar revision is a process of cutting the scar tissue out. After the excision, the new wound is usually closed up to heal by primary intention, instead of secondary intention. Deeper cuts need a multilayered closure to heal optimally, otherwise depressed or dented scars can result.\n\nSurgical excision of hypertrophic or keloid scars is often associated to other methods, such as pressotherapy or silicone gel sheeting. Lone excision of keloid scars, however, shows a recurrence rate close to 45%. A clinical study is currently ongoing to assess the benefits of a treatment combining surgery and laser-assisted healing in hypertrophic or keloid scars.\n\n\"Subcision\" is a process used to treat deep rolling scars left behind by acne or other skin diseases. It is also used to lessen the appearance of severe glabella lines, though its effectiveness in this application is debatable. Essentially the process involves separating the skin tissue in the affected area from the deeper scar tissue. This allows the blood to pool under the affected area, eventually causing the deep rolling scar to level off with the rest of the skin area. Once the skin has leveled, treatments such as laser resurfacing, microdermabrasion or chemical peels can be used to smooth out the scarred tissue. \n\nResearch shows the use of vitamin E and onion extract (sold as Mederma) as treatments for scars is ineffective. Vitamin E causes contact dermatitis in up to 33% of users and in some cases it may worsen scar appearance and could cause minor skin irritations, but Vitamin C and some of its esters fade the dark pigment associated with some scars.\n\n\nThe permanence of scarring has led to its intentional use as a form of body art within some cultures and subcultures. These forms of ritual and non-ritual scarring practices can be found in many groups and cultures around the world.\n\nFirst attested in English in the late 14th century, the word \"scar\" derives from a conflation of Old French \"escharre\", from Late Latin \"eschara\", which is the latinisation of the Greek ἐσχάρα (\"eskhara\"), meaning \"hearth, fireplace\", but in medicine \"scab, eschar on a wound caused by burning or otherwise, and Middle English \"skar\" (\"cut, crack, incision\"), which is from Old Norse \"skarð\" (\"notch, gap\"). . The conflation helped to form the English meaning. Compare Scarborough for evolution of \"skarð\" to \"scar\".\n\nAn intradermal injection of transforming growth factor beta 3 (TGFβ3) is being tested. The results of three trials already completed were published in the \"Lancet\" along with an editorial commentary.\n\nA study implicated the protein ribosomal s6 kinase (RSK) in the formation of scar tissue and found the introduction of a chemical to counteract RSK could halt the formation of cirrhosis. This treatment also has the potential to reduce or even prevent altogether other types of scarring.\n\nResearch has also implicated osteopontin in scarring.\n\n"}
{"id": "1566626", "url": "https://en.wikipedia.org/wiki?curid=1566626", "title": "Sensory garden", "text": "Sensory garden\n\nA sensory garden is a self-contained garden area that allows visitors to enjoy a wide variety of sensory experiences. Sensory gardens are designed to provide opportunities to stimulate the senses, both individually and in combination, in ways that users may not usually encounter.\n\nSensory gardens have a wide range of educational and recreational applications. They can be used in the education of special-needs students, including people with autism. As a form of horticultural therapy, they may be helpful in the care of people with dementia. \n\nSensory gardens can be designed in such a way as to be accessible and enjoyable for both disabled and non-disabled users. A sensory garden, for example, may contain features accessible to the disabled individual such as: scented and edible plants, sculptures and sculpted handrails, water features designed to make sound and play over the hands, textured touch-pads, magnifying-glass screens, braille and audio induction loop descriptions. Depending on the user group, other provisions may integrate sound and music more centrally to combine the play needs of younger users with their sensory needs.\n\nMany sensory gardens devote themselves to providing experience for multiple senses; those specialising in scent are sometimes called scented gardens, those specialising in music/sound are sound gardens where the equipment doubles up to provides an enhanced opportunity for strategic developmental, learning and educational outcomes.\n\nSensory Gardens usually have an enhanced infrastructure to permit wheelchair access and meet other accessibility concerns; the design and layout provides a stimulating journey through the senses, heightening awareness, and bringing positive learning experiences.\n\n"}
{"id": "10016622", "url": "https://en.wikipedia.org/wiki?curid=10016622", "title": "Spermarche", "text": "Spermarche\n\nSpermarche—also known as semenarche—is the beginning of development of sperm in boys' testicles at puberty. It is the counterpart of menarche in girls. Depending on their upbringing, cultural differences, and prior sexual knowledge, boys may have different reactions to spermarche, ranging from fear to excitement. Spermarche is one of the first events in the life of a male leading to sexual maturity. It occurs at the time when the secondary sexual characteristics are just beginning to develop. The age when spermarche occurs is not easy to determine. However, researchers have tried to determine the age in various populations by taking urine samples of boys and determining the presence of spermatozoa. The presence of sperm in urine is referred to as \"spermaturia\". From various sources, it appears that spermarche occurs between 13 and 15 years of age in most cases.\n\nIn one study, boys were asked the circumstances in which their first ejaculation occurred. Most commonly this occurred via a nocturnal emission, with a significant number experiencing semenarche via masturbation. Less commonly, the first ejaculation occurred during sexual intercourse with a partner.\n\n"}
{"id": "23184143", "url": "https://en.wikipedia.org/wiki?curid=23184143", "title": "Sugary drink tax", "text": "Sugary drink tax\n\nA sugary drink tax or soda tax is a tax or surcharge designed to reduce consumption of drinks with added sugar. Drinks covered under a soda tax often include carbonated soft drinks, sports drinks and energy drinks.\n\nThe tax is a matter of public debate in many countries and beverage producers like Coca-Cola often oppose it. Advocates such as national medical associations and the World Health Organization promote the tax as an example of Pigovian taxation, aimed to discourage unhealthy diets and offset the growing economic costs of obesity.\n\nType II diabetes is a growing health concern in many developed and developing countries around the world, with 1.6 million deaths directly due to this disease in 2015 alone. Unlike sugar from food, the sugar from drinks enters the body so quickly that it can overload the pancreas and the liver, leading to diabetes and heart disease over time. A 2010 study said that consuming one to two sugary drinks a day increases your risk of developing diabetes by 26%.\n\nHeart disease is responsible for 31% of all global deaths and although one sugary drink has minimal effects on the heart, consuming sugary drinks daily are associated with long term consequences. A study found that men, for every added serving per day of sugar-sweetened beverages, each serving was associated with a 19% increased risk of developing heart disease. Another study also found increased risks for heart disease in women who drank sugary drinks daily.\n\nObesity is also a global public and health policy concern, with the percentage of overweight and obese people in many developed and middle income countries rising rapidly. Consumption of added sugar in sugar-sweetened beverages has been positively correlated with high calorie intake, and through it, with excess weight and obesity. The addition of one sugar-sweetened beverage per day to the normal US diet can amount to 15 pounds of weight gain over the course of 1 year. Added sugar is a common feature of many processed and convenience foods such as breakfast cereals, chocolate, ice cream, cookies, yogurts and drinks produced by retailers. The ubiquity of sugar-sweetened beverages and their appeal to younger consumers has made their consumption a subject of particular concern by public health professionals. In both the United States and the United Kingdom, sugar sweetened drinks are the top calorie source in teenager's diets.\n\nTrends indicate that traditional soda consumption is declining in many developed economies, but growing rapidly in middle income economies such as Vietnam and India. In the United States, the single biggest market for carbonated soft drinks, consumers annual average per capita purchase of soda was 154 liters.\n\nDenmark began taxing soft drinks and juices in the 1930s. More recently, Finland reintroduced an earlier soft drink tax in 2011, while Hungary taxes sugary drinks as part of its 2011 public health product tax, which covers all food products with unhealthy levels of sugar. France introduced a targeted sugar tax on soft drinks in 2012. At a national level similar measures have also been announced in Mexico in 2013 and in the United Kingdom in 2016. In November 2014, Berkeley, California was the first city in the U.S. to pass a targeted tax on surgary drinks.\n\nProponents of soda taxes cite the success of tobacco taxes worldwide when explaining why they think a soda tax will work to lower soda consumption. Where the main concern with tobacco is cancer, the main concerns with soda are diabetes and obesity. The tactics used to oppose soda taxes by soda companies mimic those of tobacco companies, including funding research that downplays the health risks of its products.\n\nThe U.S. Department of Health and Human Services reports that a targeted tax on sugar in soda could generate $14.9 billion in the first year alone. The Congressional Budget Office (CBO) estimates that three-cent-per-ounce tax would generate over $24 billion over four years. Some tax measures call for using the revenue collected to pay for relevant health needs: improving diet, increasing physical activity, obesity prevention, nutrition education, advancing healthcare reform, etc. Another area to which the revenue raised by a soda tax might go, as suggested by Mike Rayner of the United Kingdom, is to subsidize healthier foods like fruits and vegetables.\n\nThe imposition of a sugar tax means that sellers of sugary drinks would have to increase the price of their goods by an amount P2 from the original price X, and then take on the rest of the tax themselves (P1) in the form of lower profit per unit sold. The tax burden on consumers (P2) makes it more expensive for consumers to buy sugary drinks and hence a higher proportion of their incomes would have to be spent to buy the same amount of sugary drinks. This decreases the equilibrium quantity of sugary drinks that will be sold. Whether the sugary drinks tax is imposed on the seller or consumer, in both cases the tax burden is shared between both.\n\nThe way that the tax burden is divided upon the consumer and seller depends on the price elasticity for sugary drinks. The tax burden will fall more on sellers when the price elasticity of demand is greater than the price elasticity of supply while on buyers when the price elasticity of supply is greater than the price elasticity of demand. The price elasticity for sugary drinks is different from country to country. For instance, the price elasticity of demand for sugary drinks was found to be -1.37 in Chile while -1.16 in Mexico. Hence if both of those results were realistic and the price elasticity of supply would be the same for both, the tax burden on consumers would be higher in Mexico than in Chile.\n\nThe reasons for a sugar tax are the negative externalities of consuming sugar. As over-consumption of sugar causes health problems (external costs) such as obesity, type 2 diabetes and other diseases, and lost productivity, the third party impacted by this is the ‘public health system’ that will need to deal with those issues. More demand for health services leads to higher costs for health care and hence this increased stress on the public health system is a negative consumption externality of sugar consumption.\n\nIn economics terms, the marginal social benefit (MSB) of sugar consumption is less than the marginal private benefit (MB). This can also be illustrated in the following equation. MSB = MB – Marginal External Cost (MXC). This is the case due to the fact that consumers think only of the benefit of sugar consumption to them (MB) and not the negative externalities to third parties (MXC) and so want to consume at the unregulated market equilibrium to maximize their utility. This means that there is overconsumption of sugar and a welfare loss is created.\n\nThe sugary drinks tax, a Pigovian tax, is a way to correct the negative externality by regulating the consumption of sugary drinks. Without a sugary drink tax, taxpayer money is used to pay for higher health care costs incurred from high consumption of sugar. Although this solution corrects the negative consumption externality, taxpayers that consume sugary drinks moderately and hence do not contribute to higher health care costs, still need to pay for this negative externality. Hence a sugary drinks tax may be a more appropriate solution as tax revenue that is collected from the sugar tax can be used to create childhood nutrition programs or obesity-prevention programs. This is a solution that could also correct the negative externality of sugar consumption as well as is a way to make the parties that cause the negative externality pay their fair share.\n\nThe Australian Beverages Council announced in June 2018 that the industry would cut sugar content by 10% by 2020, and by another 10% by 2025. This was seen as an attempt to stave off a sugar tax. There were no plans to reduce the sugar content in the high sugar drinks. The plan is primarily to increase consumption of low-sugar or no-sugar drinks. sales of Coca-Cola Amatil's fizzy drinks have fallen 8.1% by volume from 2016 to 2018. The Australian Medical Association continued to press for a sugar tax.\n\nA 2016 proposal for a 20% sugary drink tax, campaigned by Educar Consumidores, was turned down by the Colombian legislature despite popular support for it. Soda is often less expensive than bottled water in Colombia.\n\nDenmark instituted a soft drink tax in the 1930s (it amounted to 1.64 Danish krone per liter), but announced in 2013 that they were going to abolish it along with an equally unpopular fat tax, with the goal of creating jobs and helping the local economy. Critics claimed that the taxes were notably ineffective; to avoid the fat and sugar taxes, local retailers had complained that Danes simply went to Sweden and Germany, where prices were lower to buy butter, ice cream and soda. Denmark repealed the fat tax in January 2013 and repealed the tax on soft drinks in 2014.\n\nFrance first introduced a targeted tax on sugary drinks at a national level in 2012; following introduction, soft drinks are estimated to be up to 3.5% more expensive. Analysis by the market research firm Canadean found that sales of soft drinks declined in the year following the introduction of the tax, following several years of annual growth. However, the tax applies to both drinks with added sugars and drinks with artificial sweeteners, possibly limiting its effects on the healthfulness of soda products.\n\nA 2016 study by Mazzochi has shown that the sugary drinks tax saw a 19 euro-cent per liter increase in price of non-pure fruit juices, a 16 euro-cent per liter increase for diet sodas and little impact on regular soft drinks prices. The study also estimated that the quantity consumed of the taxed drinks has decreased by 9 centiliters per week per person after the tax has been implemented.\n\nHungary's tax, which came into effect in September 2011, is a 4-cent tax on foods and drinks that contain large quantities of sugar and salt, such as soft drinks, confectionery, salty snacks, condiments, and fruit jams. In 2016, the tax has resulted in a 22% reduction in energy drink consumption and 19% of people reduced their intake of sugary soft drinks.\n\nSoda tax introduced on May 1st 2018. The tax will see 30 cent per litre added to the price of popular sweetened drinks containing more than 8g of sugar per 100ml.\n\nIn September 2013, Mexico's president Enrique Peña Nieto, on his fiscal bill package, proposed a 10% tax on all soft drinks, especially carbonated drinks, with the intention of reducing the number of patients with diabetes and other cardiovascular diseases in Mexico, which has one of the world's highest rates of obesity. According to Mexican government data, in 2011, the treatment for each patient with diabetes cost the Mexican public health care system (the largest of Latin America) around 708 USD per year, with a total cost of 778,427,475 USD in 2010, and with each patient paying only 30 MXN (around 2.31 USD).\n\nIn September 2013, soda companies launched a media campaign to discourage the Mexican Chamber of Deputies and Senate from approving the 10% soda tax. They argued that such measure would not help reduce the obesity in Mexico and would leave hundreds of Mexicans working in the sugar cane industry jobless. They also publicly accused New York City Mayor Michael Bloomberg of orchestrating the controversial bill from overseas. In late October 2013, the Mexican Senate approved a 1 MXN per litre tax (around 0.08 USD) on sodas, along with a 5% tax on junk food.\n\nResearch has shown that Mexico's sugary drinks tax reduced soft drink consumption. According to a 2016 study published in \"BMJ\", annual sales of sodas in Mexico declined 6% in 2014 after the introduction of the soda tax. Monthly sales figures for December 2014 were down 12% on the previous two years. Households with the fewest resources had an average reduction in purchases of 9% in 2014, increasing to 17% by December. Furthermore, purchases of water and non-taxed beverages increased by about 4% on average. Whether the imposition of the tax and the resulting 6% decline in sales of soft drinks will have any measurable impact on long-term obesity or diabetes trends in Mexico has yet to be determined. The authors of the study urged the Mexican authorities to double the tax to further reduce consumption.\n\nA 2016 study published in \"PLoS Medicine\" suggested that a 10% excise tax on soda \"could prevent 189,300 new cases of Type 2 diabetes, 20,400 strokes and heart attacks, and 18,900 deaths among adults 35 to 94 years old\" over a ten-year period. The study also included that \"the reductions in diabetes alone could yield savings in projected healthcare costs of $983 million.\"\n\nA 2017 study in the \"Journal of Nutrition\" found a 6.3% reduction in soft drink consumption, with the greatest reductions \"among lower-income households, residents living in urban areas, and households with children. We also found a 16.2% increase in water purchases that was higher in low- and middle-income households, in urban areas, and among households with adults only.\"\n\nNorway has had a generalized sugar tax measure on refined sugar products since 1922, introduced to boost state income rather than reducing sugar consumption. Non-alcoholic beverages have since been separated from the general tax, and in 2017, the tax for sugary drinks was set to 3.34 kroner per litre.\n\nIn January 2018, the Norwegian government increased the sugar tax level by 83% for general sugar-containing ready-to-eat products, and 42% for beverages. The sugar tax per litre was bumped up to 4.75 kroner, and applies to beverages which are either naturally or artificially sweetened.\n\nIn the taxation reform law dubbed as the Tax Reform for Acceleration and Inclusion Law (TRAIN) signed by Philippine President Rodrigo Duterte in December 2017. It includes taxation on sugar-sweetened drinks which will be implemented the following year, as an effort to increase revenue and to fight obesity. Drinks with caloric and non-caloric sweeteners will be taxed ₱6.00 per liter, while those using high-fructose corn syrup, a cheap sugar substitute, will be taxed at ₱12 per liter.\n\nExempted from the sugar tax are all kinds of milk, whether powdered or in liquid form, ground and 3-in-1 coffee packs, and 100-percent natural fruit and vegetable juices, meal replacements and medically indicated drinks, as well as beverages sweetened with stevia or coco sugar. These drinks, especially 3-in-1 coffee drinks which are popular especially among lower-income families, are to be taxed as initially proposed by the House of Representatives version of the bill, but were exempted in the Senate version.\n\nSouth Africa proposed a sugar-sweetened beverages tax in the 2016 South African national government budget. South Africa introduced a sugar tax on 1 April 2018. The levy was fixed at 2.1 cents per gram of sugar, for each gram above 4g per 100ml of sweetened beverage. The levy excludes fruit juices.\n\nOn October 2017, the United Arab Emirates introduced a 50% tax on soft drinks and a 100% tax on energy drinks, to curb unhealthy consumption of sugary drinks that can lead to diabetes; it also added a 100% tax on cigarettes.\n\nIn the 2016 United Kingdom budget, the UK Government announced the introduction of a sugar tax, officially named the \"Soft Drinks Industry Levy\". The tax came into effect on 6 April 2018. Beverage manufacturers are taxed according to the volume of sugar-sweetened beverages they produce or import. The tax is imposed at the point of production or importation, in two bands. Drinks with total sugar content above 5g per 100 millilitres are taxed at 18p per litre and drinks above 8g per 100 millilitres at 24p per litre. The measure is estimated to generate an additional £1 billion a year in tax revenue which will be spent on funding for sport in UK schools.\n\nIt was proposed that pure fruit juices, milk-based drinks and the smallest producers would not be taxed. For other beverages there was an expectation that some manufacturers would reduce sugar content in order to avoid the taxation. Indeed, manufacturer A.G. Barr significantly cut sugar content in their primary product Irn-Bru in advance of the tax.\n\nNotable research on effect of excess sugar in modern diets in the United Kingdom includes the work of Professor John Yudkin with his book called, \"Pure, White and Deadly: The Problem of Sugar\" first published in 1972. With regard to a proposed tax on sugar-sweetened beverages, a study published in the British Medical Journal on 31 October 2013, postulated that a 20% tax on sugar-sweetened beverages would reduce obesity in the United Kingdom rates by about 1.3%, and concluded that taxing sugar-sweetened beverages was \"a promising population measure to target population obesity, particularly among younger adults.\"\n\nThe tax has been criticised on several grounds, including its likely efficacy and its narrow base. UK Member of Parliament Will Quince as, \"patronising, regressive and the nanny state at its worst.\" In addition a study by the University of Glasgow, which sampled 132,000 adults, found that focusing on sugar in isolation misleads consumers as reducing fat intake is also crucial to reducing obesity.\n\nFrom an opposing standpoint, Professor Robert Lustig of the University of California, San Francisco School of Medicine, has argued that the UK tax measure may not go far enough and that, \"juice should be taxed the same way as soda because from a metabolic standpoint juice is the same as soda.\" Campaigners have since called for the soft drinks tax to be extended to include confectionery and sweets to help tackle childhood obesity.\n\nThe United States does not have a nationwide soda tax, but a few of its cities have passed their own tax and the U.S. has seen a growing debate around taxing soda in various cities, states and even in Congress in recent years. A few states impose excise taxes on bottled soft drinks or on wholesalers, manufacturers, or distributors of soft drinks.\n\nMedical costs related to obesity in the United States alone were estimated to be $147 billion a year in 2009. In the same year, the American Heart Association reported that the soft drinks and sugar sweetened beverages are the largest contributors of added sugars in Americans' diets. Added sugars are sugars and syrups added to foods during processing or preparation and sugars and syrups added after preparation. Excessive intake of added sugars, as opposed to naturally occurring sugars, is implicated in the rise in obesity.\n\nPhiladelphia and Berkeley are the first two cities to pass a tax on sugary drinks in the U.S. Berkeley's tax of 1 cent/oz of sugary drink has seen a decline in soda consumption by more than 20 percent. Philadelphia's tax of 1.5 cents/oz took effect on January 1, 2017.\n\nThe Measure D soda tax was approved by 76% of Berkeley voters on 4 November 2014, and took effect on 1 January 2015 as the first such tax in the United States. The measure imposes a tax of one cent per ounce on the distributors of specified sugar-sweetened beverages such as soda, sports drinks, energy drinks, and sweetened ice teas but excluding milk-based beverages, meal replacement drink, diet sodas, fruit juice, and alcohol. The revenue generated will enter the general fund of the City of Berkeley. A similar measure in neighboring San Francisco received 54% of the vote, but fell short of the supermajority required to pass. In August 2015, researchers found that average prices for beverages covered under the law rose by less than half of the tax amount. For Coke and Pepsi, 22 percent of the tax was passed on to consumers, with the balance paid by vendors. UC Berkeley researchers found a higher pass-through rate for the tax: 47% of the tax was passed-through to higher prices of sugar-sweetened beverages overall with 69% being passed-through to higher soda prices. In August 2016, a UC Berkeley study showed a 21% drop in the drinking of soda and sugary beverages in low-income neighborhoods in its city.\n\nA study from 2016 compared the changing intake of sugar sweetened beverages and water in Berkeley versus San Francisco and Oakland (which did not have a sugary drink tax passed) before and after Berkeley passed its sugary drink tax. This analysis showed a 26% decrease of soda consumption in Berkeley and 10% increase in San Francisco and Oakland while water intake increased by 63% in Berkeley and 19% in the two neighboring cities. A 2017 before and after study has concluded that one year after the tax was introduced in Berkeley, sugary drink sales decreased by 9.6% when compared to a scenario where the tax was not in place. This same study was also able to show that overall consumer spending did not increase, contradicting the argument of opponents of the Sugary Drink Tax. Another 2017 study results were that purchases of healthier drinks went up and sales of sugary drinks went down, without overall grocery bills increasing or the local food sector losing money.\n\nDemocratic Philadelphia mayor Jim Kenney proposed a citywide soda tax that would raise the price of soda at three cents per ounce. At the time, it was the biggest soda tax proposal in the United States. Kenney promoted using tax revenue to fund universal pre-K, jobs, and development projects, which he predicted would raise $400 million over five years, all the while reducing sugar intake by decreasing the demand for sugary beverages. Kenney's soda tax proposal was brought to the national spotlight and divided key members of the Democratic Party. Presidential hopeful Bernie Sanders argued in an op-ed that the tax would hurt the poor. His opponent, Hillary Clinton, on the other hand, said that she was \"very supportive\" of the idea. The American Beverage Association (ABA), funded by soda companies and distributors, ran local television, radio, and newspaper advertisements against the idea, claiming that the tax would disproportionately hurt the poor. The ABA spent $10.6 million in 2016 in its effort against the tax. The American Medical Association, American Heart Association, and other medical and public health groups support the tax.\n\nThe Philadelphia City Council approved a 1.5-cents-per-ounce tax on 16 June 2016. As part of the compromise legislation that passed, the tax is also imposed on artificially sweetened beverages, such as diet soda. The law became effective on 1 January 2017. It was reported after two months of the tax that Philadelphia supermarkets and beverage distributors are planning layoffs because sugary beverage sales are down between 30 and 50 percent.\n\nAfter the tax took effect, Kenney said it was \"wrong\" and \"misleading\" for businesses to pass the tax on to their customers in the form of higher soda prices. In February 2017, soda manufacturers and retailers announced sales declines of 30-50% in Philadelphia and announced job cuts and layoffs. Kenny characterized the layoffs as evidence of greed among manufacturers. In the first four months of the soda tax $25.6 million was collected, which is lower than predicted. The revenue is intended to pay for a pre-K program (49% of tax revenue), government employee benefits and city programs (20%), and rebuilding city parks and recreation centers. A recent study from 2017 found that Philadelphia's tax has decreased sugary beverage consumption in impoverished youth by 1.3 drinks/week. Langellier et al. also found that when paired with the pre-K program, attendance increases significantly, a finding that is likely to have longer term positive effects than a sugary drink tax alone.\n\nIn March 2017, Pepsi laid off between 80 and 100 employees at two distribution plants in Philadelphia and one plant in nearby Wilmington, Delaware. The company blamed the layoffs on the tax, an assertion rejected by the city government.\n\nIn September 2016, the American Beverage Association, Philadelphia business owners, and other plaintiffs filed a lawsuit against the soda tax, alleging that the tax violated the \"Tax Uniformity Clause\" of the state constitution. The legal challenge was dismissed by the Court of Common Pleas in December 2016, and in June 2017 the Commonwealth Court of Pennsylvania (in a 5-2 decision) affirmed that ruling. The ABA is appealing the decision to the Pennsylvania Supreme Court.\n\nA one-cent-per-ounce soda tax (Prop V) passed with over 61% of the vote on 8 November 2016 and applies to distributors of sugary beverages on 1 January 2018. Exemptions for the tax include infant formulas, milk products, supplements, drinks used for medical reasons, and 100% fruit and vegetable juices. The soda industry spent almost $20 million in its unsuccessful push to defeat the soda tax initiative, a record-breaking amount for a San Francisco ballot initiative.\n\nIn 2014, the first referendum on a soda tax, Proposition E, was voted down by San Francisco; the 2014 referendum received the support of 55 percent of voters, short of the two-thirds required for a referendum directing money to a specific item (the referendum proposed directing the revenue raised to children's physical education and nutrition programs, and in San Francisco such earmarking requires a two-thirds vote to pass). In that referendum campaign, the soda industry spent about $10 million in opposition to the proposed tax.\n\nA one-cent-per-ounce soda tax (Measure HH) passed with over 60% of the vote on 8 November 2016. The tax went into effect on 1 July 2017.\n\nA one-cent-per-ounce soda tax (Prop O1) passed with over 70% of the vote on 8 November 2016. The tax went into effect on April 1, 2017\n\nA two-cents-per-ounce soda tax (Measure 2H) passed with 54% of the vote on 8 November 2016. The tax took effect on July 1, 2017, and revenue will be spent on health promotion, general wellness programs and chronic disease prevention that improve health equity, and other health programs especially for residents with low income and those most affected by chronic disease linked to sugary drink consumption. The tax is exempted at the University of Colorado, Boulder, campus as school officials survey what types of drinks students wish to have. The University was not aware it would be involved in the soda tax, and would have to pay as much as $1 million a year to purchase it.\n\nA one-cent-per-ounce soda tax passed on November 10, 2016, by a 9-8 vote, with Cook County Board of Commissioners President Toni Preckwinkle breaking the 8-8 tie. Cook County includes Chicago and has a population of nearly 5.2 million. This was the most populous jurisdiction with a soda tax in the U.S. The campaign to introduce the tax was heavily funded by Mike Bloomberg.\n\nOn June 30, 2017, a Cook County judge granted a temporary restraining order filed by the Illinois Retail Merchants Association and several Cook County-based grocers that prohibited the tax from being put into effect until at least July 12. The tax eventually went into effect on August 2. Due to a conflict with the Supplemental Nutrition Assistance Program, this soda tax did not apply to any soda purchases made with food stamps, which were used by over 870,000 people. Controversially, the tax affected diet drinks but not sugar-packed fruit juices.\n\nOn October 10, 2017, the Board of Commissioners voted to repeal the tax in a 15-1 vote. The tax stayed in effect up until December 1. The tax was highly unpopular and seen mainly as an attempt to plug the county’s $1.8 billion budget deficit, rather than a public health measure.\n\nThe Coalition for Healthy Kids and Education is currently campaigning to get a soda tax on the May 2018 ballot. Their aim is to implement a 1.15 cents per ounce tax on sugary drinks. There are 18,000 signatures required by December 15, 2017 in order for the tax to be voted on in May.\n\nOn June 5, 2017, Seattle's City Council voted 7-1 to pass a 1.75 cents per ounce tax on sugary drinks; the tax does not include diet soda drinks or fruit drinks and it started on January 1, 2018. After the tax was implemented, people were surprised that the tax made a case (24 cans) of Coke become $7.35 more expensive when compared to a case of Diet Coke or Coke Zero. The $15 million Seattle assumes will be collected from the tax will be used for programs that give access to more fruits and vegetables for low-income families, adding education programs and studying the tax on how it impacts behavior. Seattle collected $4 million in the first four months of the tax.\n\nCoca-Cola has been under fire since 2015 when emails revealed that funding for scientific studies sought to influence research to be more favorable to soda. Research funded by soda companies are 34 times more likely to find soda has no significant health impacts on obesity or diabetes.\n\nTaxing soda can lead to a reduction in overall consumption, according to a scientific study published in the \"Archives of Internal Medicine\" in March 2010. The study found that a 10 percent tax on soda led to a 7 percent reduction in calories from soft drinks. These researchers believe that an 18 percent tax on these foods could cut daily intake by 56 calories per person, resulting in a weight loss of 5 pounds (2 kg) per person per year. The study followed 5,115 young adults ages 18 to 30 from 1985 to 2006.\n\nA 2010 study published in the medical journal \"Health Affairs\" found that if taxes were about 18 cents on the dollar, they would make a significant difference in consumption.\n\nResearch from Duke University and the National University of Singapore released in December 2010 tested larger taxes and determined that a 20 percent and 40 percent taxes on sugar-sweetened beverages would largely not affect calorie intake because people switch to untaxed, but equally caloric, beverages. Kelly Brownell, a proponent of soda taxes, reacted by stating that “[t]he fact is that nobody has been able to see how people will really respond under these conditions.” Similarly, a 2010 study concluded that while people would drink less soda as a result of a soda tax, they would also compensate for this reduction by switching to other high-calorie beverages. In response to these arguments, the American Public Health Association released a statement in 2012 in which they argued that \"Even if individuals switch to 100% juice or chocolate milk, this would be an improvement, as those beverages contribute some nutrients to the diet.\"\n\nA 2011 study in the journal \"Preventive Medicine\" concluded that \"a modest tax on sugar-sweetened beverages could both raise significant revenues and improve public health by reducing obesity\". It has been used by the Rudd Center for Food Policy and Obesity at Yale to estimate revenue from a soda tax, depending on the state, year and tax rate.\n\nA 2012 study by Y. Claire Wang, also in the journal \"Health Affairs\", estimates that a penny per ounce tax on sugared beverages could prevent 2.4 million cases of diabetes per year, 8,000 strokes, and 26,000 premature deaths over 10 years.\n\nIn 2012, just before the city of Richmond began voting on a soda tax, a study was presented at a conference held by the American Public Health Association regarding the potential effects of such a tax in California. The study concluded that, given that soda's price elasticity is such that taxing it would reduce consumption by 10–20 percent, that this reduction \"...is projected to reduce diabetes incidence by 2.9–5.6% and CHD by 0.6–1.2%.\"\n\nA 2013 study in the \"American Journal of Agricultural Economics\" concluded that a 0.5-cent-per-ounce tax on soft drinks would reduce consumption, but \"increase sodium and fat intakes as a result of product substitution,\" in line with the Duke University study mentioned above.\n\nA 2014 study published in the \"American Journal of Public Health\" concluded that Sugar-Sweetened Beverages (SSBs) don’t have a negative impact on employment. Even though job losses in the taxed industry occurred, they were offset by new employment in other sectors of the economy.\n\nA 2016 modelling study estimated that a 20% tax on SSBs would decrease the consumption of SSBs in Australia by 12.6%. The tax could decline the prevalence of obesity in the Australian population, which could lead to gains in health-adjusted life years. The results showed an increase of 7.6 days in full health for a 20-24-year-old male and a 3.7 day increase in longevity for their female peers.\n\nThere have been a number of proposed taxes on sugary beverages, including:\n\n\nA 2016 poll by Morning Consult-Vox finds Americans split on their support of a soda tax. Attitudes seem to have shifted a lot since 2013 when a poll concluded that \"respondents were opposed to government taxes on sugary drinks and candy by a more than 2-to-1 margin.\" In California, however, support for a tax has been high for a few years. According to a Field Poll conducted in 2012, \"Nearly 3 out of 5 California voters would support a special fee on soft drinks to fight childhood obesity.\" \nSupport for a soda tax in New York was higher when pollsters say the money will go towards health care. A Quinnipiac University poll released in April 2010 found that New Yorkers opposed a state tax on soda of one penny per ounce by a 35-point margin, but opposition dropped to a margin of one point when respondents were told the money would go towards health care. A Thompson Reuters poll released in the same month found that 51 percent of Americans opposed a soda tax, while 33 percent supported one.\n\nFighting the creation of soft drink taxes, the American Beverage Association, the largest U.S. trade organization for soft drink bottlers, has spent considerable money lobbying Congress. The Association's annual lobbying spending rose from about $391,000 to more than $690,000 from 2003 to 2008. And, in the 2010 election cycle, its lobbying grew to $8.67 million. These funds helped to pay for 25 lobbyists at seven different lobbying firms.\n\nAn industry group called \"Americans Against Food Taxes,\" backed by juice maker Welch's, soft drink maker PepsiCo Inc, the American Beverage Association, the Corn Refiners Association, McDonald's Corporation and Burger King Holdings Inc used national advertising and conducted lobbying to oppose these taxes. The group has characterized the soda tax as a regressive tax, which would unfairly burden the poor\n\n\nIsland nations and territories have been successful in passing soda taxes. Just like with tobacco taxes, smaller communities are often the first to pass a new type of tax.\n\nBarbados passed a soda tax in September 2015, applied as an excise of 10%.\n\nFiji has an import tax and an excise tax on soda.\n\nFrench Polynesia implemented taxes on soft drinks in 2002.\n\nMauritius passed a soda tax in 2013.\n\nNauru implemented a soda tax in 2007.\n\nSamoa passed a soda tax in 1984.\n\nIn March 2014, the government of the island of St Helena, a British Overseas Territory in the South Atlantic, announced that it would be introducing an additional import duty of 75 pence per litre on sugar-sweetened carbonated drinks with more than 15 grams of sugar per litre. The measure was introduced in May 2014 as part of a number of measures to tackle obesity on the island and the resulting high incidence of type 2 diabetes.\n\nTonga has a soda tax.\n\n\n"}
{"id": "28752034", "url": "https://en.wikipedia.org/wiki?curid=28752034", "title": "Telepathology", "text": "Telepathology\n\nTelepathology is the practice of pathology at a distance. It uses telecommunications technology to facilitate the transfer of image-rich pathology data between distant locations for the purposes of diagnosis, education, and research. Performance of telepathology requires that a pathologist selects the video images for analysis and the rendering of diagnoses. The use of \"television microscopy\", the forerunner of telepathology, did not require that a pathologist have physical or virtual \"hands-on\" involvement in the selection of microscopic fields-of-view for analysis and diagnosis.\n\nAn academic pathologist, Ronald S. Weinstein, M.D., coined the term \"telepathology\" in 1986. In a medical journal editorial, Weinstein outlined the actions that would be needed to create remote pathology diagnostic services. He and his collaborators published the first scientific paper on robotic telepathology. Weinstein was also granted the first U.S. patents for robotic telepathology systems and telepathology diagnostic networks. Weinstein is known to many as the \"father of telepathology\". In Norway, Eide and Nordrum implemented the first sustainable clinical telepathology service in 1989; this is still in operation decades later. A number of clinical telepathology services have benefited many thousands of patients in North America, Europe, and Asia.\n\nTelepathology has been successfully used for many applications, including the rendering of histopathology tissue diagnoses at a distance. Although digital pathology imaging, including virtual microscopy, is the mode of choice for telepathology services in developed countries, analog telepathology imaging is still used for patient services in some developing countries.\n\nTelepathology systems are divided into three major types: static image-based systems, real-time systems, and virtual slide systems.\n\nStatic image systems have the benefit of being the most reasonably priced and usable systems. They have the significant drawback in only being able to capture a selected subset of microscopic fields for off-site evaluation.\n\nReal-time robotic microscopy systems and virtual slides allow a consultant pathologist the opportunity to evaluate histopathology slides in their entirety, from a distance. With real-time systems, the consultant actively operates a robotically controlled motorized microscope located at a distant site—changing focus, illumination, magnification, and field of view—at will. Either an analog video camera or a digital video camera can be used for robotic microscopy. Another form of real-time microscopy involves utilizing a high resolution video camera mounted on a path lab microscope to send live digital video of a slide to a large computer monitor at the pathologist's remote location via encrypted store-and-forward software. An echo-cancelling microphone at each end of the video conference allows the pathologist to communicate with the person moving the slide under the microscope.\n\nVirtual slide systems utilize automated digital slide scanners that create a digital image file of an entire glass slide (whole slide image). This file is stored on a computer server and can be navigated at a distance, over the Internet, using a browser. Digital imaging is required for virtual microscopy.\n\nWhile real-time and virtual slide systems offer higher diagnostic accuracy when compared with static-image telepathology, there are drawbacks to each. Real-time systems perform best on local area networks (LANs), but performance may suffer if employed during periods of high network traffic or using the Internet proper as a backbone. Expense is an issue with real-time systems and virtual slide systems as they can be costly. Virtual slide telepathology is emerging as the technology of choice for telepathology services. However, high throughput virtual slide scanners (those producing one virtual slide or more per minute) are currently expensive. Also, virtual slide digital files are relatively large, often exceeding one gigabyte in size. Storing and simultaneously retrieving large numbers of telepathology whole-slide image files can be cumbersome, introducing their own workflow challenges in the clinical laboratory.\n\nTelepathology is currently being used for a wide spectrum of clinical applications including diagnosing of frozen section specimens, primary histopathology diagnoses, second opinion diagnoses, subspecialty pathology expert diagnoses, education, compentency assessment, and research. Benefits of telepathology include providing immediate access to off-site pathologists for rapid frozen section diagnoses. Another benefit can be gaining direct access to subspecialty pathologists such as a renal pathologist, a neuropathologist, or a dermatopathologist, for immediate consultations.\n\nCanada Health Infoway is the organization responsible for the implementation of telepathology in Canada. Canada Health Infoway is a federal non-profit which provides funding for improving digital health infrastructure.\n\nCanada Health Infoway has targeted funding of $1.2 million CAD to the Telepathology Solution for the province of British Columbia. The system is designed to connect all pathologists within the province. The long-term expectations are improvement to patient care and safety through access to pathology expertise, improved timeliness of results and quality of service.\n\nIn Ontario, the University Health Network (UHN) hospitals are the primary drivers of the development of telepathology. The three northern Ontario communities of Timmins, Sault Ste. Marie and Kapuskasing have several community hospitals virtually linked to UHN pathologists via the Internet 24 hours a day.\n\n\n\n"}
{"id": "13311819", "url": "https://en.wikipedia.org/wiki?curid=13311819", "title": "Therapy", "text": "Therapy\n\nTherapy (often abbreviated tx, Tx, or T) is the attempted remediation of a health problem, usually following a diagnosis. In the medical field, it is usually synonymous with treatment (also abbreviated tx or T). Among psychologists and other mental health professionals, including psychiatrists, psychiatric nurse practitioners, counselors, and clinical social workers, the term may refer specifically to psychotherapy (sometimes dubbed 'talking therapy'). The English word \"therapy\" comes via Latin \"therapīa\" from and literally means \"curing\" or \"healing\".\n\nAs a rule, each therapy has indications and contraindications.\n\nThe words care, therapy, treatment, and intervention overlap in a semantic field, and thus they can be synonymous depending on context. Moving rightward through that order, the connotative level of holism decreases and the level of specificity (to concrete instances) increases. Thus, in health care contexts (where its senses are always noncount), the word \"care\" tends to imply a broad idea of everything done to protect or improve someone's health (for example, as in the terms \"preventive care\" and \"primary care\", which connote ongoing action), although it sometimes implies a narrower idea (for example, in the simplest cases of wound care or postanesthesia care, a few particular steps are sufficient, and the patient's interaction with that provider is soon finished). In contrast, the word \"intervention\" tends to be specific and concrete, and thus the word is often countable; for example, one instance of cardiac catheterization is one intervention performed, and coronary care (noncount) can require a series of interventions (count). At the extreme, the piling on of such countable interventions amounts to interventionism, a flawed model of care lacking holistic circumspection—merely treating discrete problems (in billable increments) rather than maintaining health. \"Therapy\" and \"treatment\", in the middle of the semantic field, can connote either the holism of \"care\" or the discreteness of \"intervention\", with context conveying the intent in each use. Accordingly, they can be used in both noncount and count senses (for example, \"therapy for chronic kidney disease can involve several dialysis treatments per week\").\n\nThe words \"aceology\" and \"iamatology\" are obscure and obsolete synonyms referring to the study of therapies.\n\nLevels of care classify health care into categories of chronology, priority, or intensity, as follows:\n\nTreatment decisions often follow formal or informal algorithmic guidelines. Treatment options can often be ranked or prioritized into lines of therapy: first-line therapy, second-line therapy, third-line therapy, and so on. First-line therapy (sometimes called induction therapy, primary therapy, or front-line therapy) is the first therapy that will be tried. Its priority over other options is usually either: (1) formally recommended on the basis of clinical trial evidence for its best-available combination of efficacy, safety, and tolerability or (2) chosen based on the clinical experience of the physician. If a first-line therapy either fails to resolve the issue or produces intolerable side effects, additional (second-line) therapies may be substituted or added to the treatment regimen, followed by third-line therapies, and so on.\n\nAn example of a context in which the formalization of treatment algorithms and the ranking of lines of therapy is very extensive is chemotherapy regimens. Because of the great difficulty in successfully treating some forms of cancer, one line after another may be tried. In oncology the count of therapy lines may reach 10 or even 20.\n\nOften multiple therapies may be tried simultaneously (combination therapy or polytherapy). Thus combination chemotherapy is also called polychemotherapy, whereas chemotherapy with one agent at a time is called single-agent therapy or monotherapy.\n\nAdjuvant therapy is therapy given in addition to the primary, main, or initial treatment, but simultaneously (as opposed to second-line therapy). Neoadjuvant therapy is therapy that is begun before the main therapy. Thus one can consider surgical excision of a tumor as the first-line therapy for a certain type and stage of cancer even though radiotherapy is used before it; the radiotherapy is neoadjuvant (chronologically first but not primary in the sense of the main event). Premedication is conceptually not far from this, but the words are not interchangeable; cytotoxic drugs to put a tumor \"on the ropes\" before surgery delivers the \"knockout punch\" are called neoadjuvant chemotherapy, not premedication, whereas things like anesthetics or prophylactic antibiotics before dental surgery are called premedication.\n\nStep therapy or stepladder therapy is a specific type of prioritization by lines of therapy. It is controversial in American health care because unlike conventional decision-making about what constitutes first-line, second-line, and third-line therapy, which in the U.S. reflects safety and efficacy first and cost only according to the patient's wishes, step therapy attempts to mix cost containment by someone other than the patient (third-party payers) into the algorithm. Therapy freedom and the negotiation between individual and group rights are involved.\n\nTreatments can be classified according to the method of treatment:\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "50166890", "url": "https://en.wikipedia.org/wiki?curid=50166890", "title": "Timeline of global health", "text": "Timeline of global health\n\nThis page is a timeline of global health, including major conferences, interventions, cures, and crises.\n\nDuring this pre-WWII era, there are three big trends that operate separately, but sometimes affect each other in development and outcomes.\n\nFirst, a trend of urbanization (fueled by the Industrial Revolution) as well as greater global trade and migration leads to new challenges, including those in urban sanitation and infectious diseases/pandemics. Six global cholera pandemics happen in this period because of increased commerce and migration.\n\nSecond, there is a lot of development on the underlying theory of disease, advancements in vaccine and antibiotic development, and a variety of experimental large-scale eradication and control programs. One big example: the germ theory of diseases begins to become accepted and popularized starting around 1850. Another big example is the development of the smallpox vaccine by Edward Jenner in 1796. Systematic eradication and control efforts include the Rockefeller Sanitary Commission and efforts to eradicate smallpox. Antitoxins and vaccines for numerous diseases including cholera and tuberculosis are developed during this period, building on a trend of greater understanding of and control over microorganisms.\n\nA third theme during this era is the formation of various preliminary international alliances and conferences, including the International Sanitary Conferences, Pan American Health Organization, Office International d'Hygiène Publique, and the League of Nations Health Committee. This is closely intertwined with the other two trends. For instance, the cholera pandemics mentioned above, as well as the growing scientific understanding of the germ theory of disease, are both key impetuses for the International Sanitary Conferences.\n\nFollowing the end of World War II, the first batch of big organizations, both international and national (with international cooperation), including the United Nations and World Health Organization (WHO), form. Beginning with the United Nations Relief and Rehabilitation Administration for relief of victims of war in 1943, there is a big push to begin creating large scale health initiatives, non-governmental organizations, and worldwide global health programs by the United Nations to improve quality of life around the world. UNICEF, the World Health Organization, as well as the UNRRA are all part of United Nations efforts to benefit global health beginning with developing countries. These various programs aim to aid in economic endeavors by providing loans, direct disease prevention programs, health education, etc.\n\nAfter wrapping up complications caused by the end of the war, there is an international energy put in into eradication, beginning with the complete smallpox eradication in 1979. There is greater dissatisfaction with WHO for its focus on disease/infection control at the expense of trying to improve general living conditions, as well as disappointment at its low budget and staffing. This atmosphere spurs other organizations to provide their own forms of aid. The Alma Ata Declaration and selective primary healthcare are created to express urgent action by all governments and citizens to protect and promote the health of all people equally. More organizations form following these new active attitudes toward global health, including the International Agency for Research on Cancer and the Doctors Without Borders organization. Publications like the WHO Model List of Essential Medicines highlight basic medicines required by most adults and children to survive, and set priorities for healthcare fund allocation in developing countries. Generally, there is more buy-in for the idea that direct, targeted efforts to address healthcare could be worthwhile and benefit many countries.\n\nCertain specific efforts increase in efficiency and productivity, including improvement in maternal and child health and a focus on HIV/AIDS, tuberculosis, and malaria (the 'Big Three') in developing countries. During this time period, the child survival revolution (CSR), which helps reduce child mortality in the developing world, and GOBI-FFF are both advocated by James P. Grant. The World Summit for Children also takes place, becoming one of the largest ever gathering of heads of states and government to commit a set of goals to improve the well-being of children. Finally, HIV/AIDS becomes the focus of many governmental and non-governmental organizations, leading to the formation of the Global Programme on AIDS (GPA) by efforts of the World Health Organization. However, these health organizations also make significant advancements to tuberculosis treatments, including the DOTS strategy and the formation of the Stop TB Partnership.\n\nUN's Millennium Development Goals establishes health care as an important goal (not just combating infectious diseases). Later in 2015, the Sustainable Development Goals build on the MDGs to outline the objectives that will transform our world by ending poverty, helping the environment, and improving health and education. More specific disease-targeting organizations are created primarily to fund healthcare plans in developing countries, including the President's Emergency Plan for AIDS Relief and The Global Fund to Fight AIDS, Tuberculosis and Malaria. These organizations (especially the WHO) adopt new strategies and initiatives, including the 3 by 5 Initiative to widen the access to antiretroviral treatment, the WHO Framework Convention on Tobacco Control, etc. Private large donors such as the Bill & Melinda Gates Foundation begin to play an important role in shaping the funding landscape and direction of efforts in global health.\n\nThe following events are selected for inclusion in the timeline:\n\n\nWe do \"not\" include:\n\n\n\n\n\n"}
{"id": "55761001", "url": "https://en.wikipedia.org/wiki?curid=55761001", "title": "Total Diet Study", "text": "Total Diet Study\n\nThe Total Diet Study is the US FDA´s ongoing annual assessment of U.S. consumers' average dietary intake of about 800 contaminants and nutrients since 1961.\n\nThe Total Diet Study is the US FDA´s ongoing annual assessment of U.S. consumers' average dietary intake of about 800 contaminants and nutrients. To this effort its Center for Food Safety and Applied Nutrition at College Park, Maryland has been buying, preparing, and analyzing about \"280 kinds of foods and beverages from representative areas of the country, four times a year\".\nTo make estimates as realistic as possible, foods are bought from the same places \"as consumers do\", in each of four regions of the country (North Central, West, South, and Northeast). The purchases or market baskets are identical as much as possible. It is bought in supermarkets, grocery stores, and fast-food restaurants in three cities per region, which change every year.\n\nThe FDA Kansas City District Laboratory in Lenexa, Kansas is the central lab for the country and prepares and analyzes each kind of food, combining samples from the 3 cities of a region to make one composite sample. \nFoods are prepared \"as consumers typically would\", which includes washing, peeling, cooking. Radionuclide analyses are only done from two market baskets per year at FDA's Engineering and Analytical Center in Winchester, Massachusetts.\n\nWith the analysis, FDA estimates the average consumption by the entire U.S. population, and split up by age and gender (from age 14 years onward) each year. The food list is adjusted about every 10 years to account for food trends, changing patterns of what people eat. Likewise data on how much of those foods consumers eat may be adjusted.\nFDA states the purpose as tracking food trends and trends in the consumption of contaminants and nutrients in the average American diet, and developing interventions to reduce risks, as needed in its food safety and nutrition programs.\n\nFood categories include baby foods, beverages, dairy products, eggs, food mixtures, fruits and fruit juice, grain products, legumes & nuts & seeds, meat & poultry & fish, fats and oils, sweets, vegetables.\nA Food/Analyte Matrix displays all the specific foods and analytes.\nDetermination methods for analysis including Strontium-90, a beta emitter and for Gamma emitters are published .\n\nIn 1961, the US FDA began a program to monitor radioactive contamination of foods.\nOver time, it expanded to include pesticides, industrial and other toxic chemicals, and nutrients.\nData are published annually, and electronic data since 1991 are available. \n\nIn 1991, the market baskets were updated, and the food list was chosen from the USDA 1987-88 \"Nationwide Food Consumption Survey\".\n\nThe 2003 food-list update used a food list from the USDA's 1994-96, 1998 \"Continuing Survey of Food Intakes by Individuals\" (CSFII) published in 2000.\n\nIn 2014, FDA updated methods of measuring elements in foods to detect and differentiate them at lower levels.\nIn 2016 the FDA has been criticized for not following their own compliance program guidance manual to follow-up on unusual analytical findings. Perchlorate levels in the 2016 update had been far greater than in 2004-2006. Upon a FOIA request, the Center for Food Science and Applied Nutrition could not find any records or communications of the coordinator investigating the possible cause of high levels of perchlorate in bologna, collard greens and salami lunchmeat nor receipts to identify brand and lot \n"}
{"id": "4212753", "url": "https://en.wikipedia.org/wiki?curid=4212753", "title": "University of Washington Department of Global Health", "text": "University of Washington Department of Global Health\n\nThe University of Washington Department of Global Health is a department jointly run by the schools of Medicine and Public Health at the University of Washington in Seattle, Washington. Its aim is to provide a multidisciplinary venue to address issues of global health at the university.\n\nThe department was begun with funding supplied by the Bill & Melinda Gates Foundation.\n\nThe Department of Global Health was launched in January 2007 with support from the Bill & Melinda Gates Foundation, the state of Washington, and the University of Washington, with a mandate to harness the extraordinary expertise, energy, and creativity of faculty across all 17 UW schools and colleges to create one of the most comprehensive academic global health programs in the world.[1]\n\nThe pioneering work of UW researchers in sexually transmitted diseases in the 1970s and 1980s paved the way for the University's leading role in HIV/AID research and training, and, now, global health.\n\nThe Department is housed in both the School of Medicine and School of Public Health and has formed linkages across campus and throughout the world to help address not only infectious diseases but a host of pressing global health issues, including health metrics and evaluation; the health of women, children, and adolescents; health system strengthening and implementation science; climate change and health; global trauma and violence prevention; and global medicines safety with a cross-cutting focus on social justice and equity. The Center for Integrated Health of Women, Children, and Adolescents, for example, includes at least 22 collaborations (13 on campus and partnerships in Kenya, Ethiopia, Mexico, and Peru). And the initiative on Climate Change and Global Health involves more than 25 collaborations on campus and beyond.\n\nIts closely affiliated centers also include the Institute for Health Metrics and Evaluation (IHME), the International Training and Education Center for Health (I-TECH), the International Clinical Research Center (ICRFC), the Center for AIDS Research (CFAR), Health Alliance International (HaI), and the Global Health Resource Center (GHRC).\n\nThe Department has strong ties in Kenya, Peru, Mozambique, and Ethiopia. But worldwide, the Department works with nearly 250 collaborating organizations, including universities and hospitals, NGOs, government agencies, and ministries of health.\n\nQuote from Howard Frumkin MD, MPH, Dean of the UW School of Public Health: \"Global health represents the best of academic health sciences -- transdisciplinary systems thinking, cross-cultural sensitivity, rigorous scientific research, hands-on participatory training, effective service delivery with impact empirically measured, and sustainable collaboration. This Department is outstanding. \"\n\nMaster's Level: The Department offers several global health tracks for a master of Public Health degree: General; Leadership, Policy, and Management; Health Metrics and Evaluation; Peace Corps; Epidemiology; and concurrent degrees.\n\nDoctoral Programs: The Department offers a doctoral program in Pathobiology. A doctoral program in global health with emphases on health metrics and evaluation and implementation science is in development.\n\nFellowship Programs: The Department offers post-bachelor and post-graduate fellowship programs with the Institute for Health Metrics and Evaluation.\n\nCertificate Programs: The Department has a certificate program in Global Health and a program in AIDS and STIs. Medical students also can take a Global Health Pathway.\n\nUndergraduate Programs: An undergraduate minor in global health was launched in January 2011.\n\nJim Yong Kim, formerly of Partners in Health and the WHO HIV/AIDS program, was originally a candidate for director of the department, but was not selected. A controversial second selection process involving three new candidates took place in late 2005 and early 2006. The process was criticized for not being open, and there was concern among the student body and faculty about the chosen chair. Some feared that the department would be too heavily oriented towards biomedical research and biotechnology (e.g. vaccine development) and would neglect the broader issues of public health, such as social justice, health disparities, prevention, promotion, human resources in health, and public policy. Some also feared that the areas of education and service would be sacrificed for a research agenda, and pointed to the fact that one of the first steps in implementing the department was the leasing of a large facility off campus in Seattle's South Lake Union neighborhood - an area being developed as a biotechnology hub.\n\nThose within the process argued that planning was open and that the department would be multidisciplinary and would live up to its stated vision of taking a broad approach to global health. They also noted that while the department would have some facilities off campus, it would be primarily based at the university; the deans of the schools committed to finding on-campus space to house the department's administrative offices, although much of the lab space was planned to be located in a building in the Eastlake neighborhood.\n\nIn Spring, 2006, Michael Merson of Yale University was offered the position of director. However, in July of that year it was announced that Merson had been appointed direct of Duke University's Global Health Institute.\n\nOn September 8, University of Washington announced King K. Holmes, MD, PhD, a world leader in AIDS and infectious disease research and training, to become the first chair of the University of Washington's new Department of Global Health. His leadership in global health research and training, and experience as a public-health practitioner, will serve Holmes well in leading the Department of Global Health, according to Paul G. Ramsey, dean of the School of Medicine.\n\n\n"}
{"id": "38754765", "url": "https://en.wikipedia.org/wiki?curid=38754765", "title": "Violence against men", "text": "Violence against men\n\nViolence against men (VAM), consists of violent acts that are disproportionately or exclusively committed against men. Men are overrepresented as both victims and perpetrators of violence. Sexual violence against men is treated differently in any given society from that committed against women, and may be unrecognized by international law.\n\nStudies of social attitudes show violence is perceived as more or less serious depending on the gender of victim and perpetrator. According to a study in the publication \"Aggressive Behavior\", violence against women was about a third more likely to be reported by third parties to the police regardless of the gender of the attacker, although the most likely to be reported gender combination was a male perpetrator and female victim. The use of stereotypes by law enforcement is a recognised issue, and international law scholar Solange Mouthaan argues that, in conflict scenarios, sexual violence against men has been ignored in favor of a focus on sexual violence against women and children. One explanation for this difference in focus is the physical power that men hold over women making people more likely to condemn violence with this gender configuration. The concept of male survivors of violence go against social perceptions of the male gender role, leading to low recognition and few legal provisions. Often there is no legal framework for a woman to be prosecuted when committing violent offenses against a man.\n\nRichard Felson challenges the assumption that violence against women is different from violence against men. The same motives play a role in almost all violence, regardless of gender: to gain control or retribution and to promote or defend self-image.\n\nWriting for TIME, Cathy Young criticised the feminist movement for not doing enough to challenge double standards in the treatment of male victims of physical abuse and sexual assault.\n\nIn 2013 editor-in-chief of the journal \"Partner Abuse\", John Hamel, set up the Domestic Violence Research Group to create the \"Partner Abuse State of Knowledge Project (PASK)\". PASK found parity in rates of both perpetration and victimisation for men and women.\n\nMen who are victims of domestic violence are at times reluctant to report it or to seek help. According to some commentators there is also a paradigm that only males perpetrate domestic violence and are never victims. Shamita Das Dasgupta and Erin Pizzey are amongst those who argue that, as with other forms of violence against men, intimate partner violence is generally less recognized in society when the victims are men. Violence of women against men in relationships is often 'trivialized' due to the supposed weaker physique of women; in such cases the use of dangerous objects and weapons is omitted. Research since the 1990s has identified issues of perceived and actual bias when police are involved, with the male victim being negated even whilst injured.\n\nAccording to the journalist Martin Daubney \"...there remains a theory that men under report their experiences [of violence by women against men] due to a culture of masculine expectations. The official figure in the United Kingdom, for example, is about 50% of the number of acts of violence by men against women, but there are indications that only about 10% of male victims of female violence report the incidents to the authorities, mainly due to taboos and fears of misunderstanding created by a culture of masculine expectations. By comparison 1.9 million people aged 16-59 told the Crime Survey for England and Wales (year ending March 2017), that they were victims of domestic violence and 79% did not report their partner or ex-partner. Of the 1.9 million, approximately 1.2 million were female and 713,000 were male. A Canadian report found that men were 22% more likely to report being victims of spousal violence in their current relationship than women. Researchers Stemple and Meyer report that sexual violence by women against men is often understudied or unrecognized.\n\nNon-therapeutic male circumcision is considered, by several groups, to be a form of violence against young men and boys. The International Criminal Court considers forced circumcision to be an \"inhumane act\". Some court decisions have found it to be a violation of a child's rights. In certain countries, such as Australia, Bangladesh, Canada, Indonesia, Pakistan, the Philippines, South Korea, Turkey and the United States, newborn baby males are routinely circumcised without the child's consent. As well, the Jewish and Muslim faiths circumcise boys at a young age. It is also practiced in Coptic Christianity and the Ethiopian Orthodox Church.\n\nAny cutting whatsoever of a female's genitals, also known as female genital mutilation, has been banned in most Western countries, starting in Sweden in 1982 and the United States in 1997. When Sweden outlawed it in 1982, it became the first Western country to do so. Several former colonial powers, including Belgium, Britain, France and the Netherlands, followed suit, either with new laws or by making clear that it was covered by existing legislation.\n\nAlthough a 2012 court ruling in Germany put the practice of male cutting under question, calling circumcision \"grievous bodily harm,\" the German parliament passed a law to keep circumcision of boys legal. As of 2016, cutting of boys' foreskins is still legal worldwide.\n\nIn situations of structural violence that include war and genocide, men and boys are frequently singled out and killed. The murder of targets by sex during the Kosovo War, estimates of civilian male victims of mass killings suggest that they made up more than 90% of all civilian casualties.\n\nNon-combatant men and boys have been and continue to be the most frequent targets of mass killing and genocidal slaughter, as well as a host of lesser atrocities and abuses. Gendercide Watch, an independent human rights group, documents multiple gendercides aimed at males (adult and children): The Anfal Campaign, \n(Iraqi Kurdistan), 1988 – Armenian Genocide (1915-17) – Rwanda, 1994. Forced conscription can also be considered gender-based violence against men.\n\nIn armed conflict, sexual violence is committed by men against men as psychological warfare in order to demoralize the enemy. The practice is ancient, and was recorded as taking place during the Crusades. Castration is used as a means of physical torture with strong psychological effects, namely the loss of the ability to procreate and the loss of the status of a full man. International criminal law does not consider gender based sexual violence against men a separate type of offense and treats it as war crimes or torture. The culture of silence around this issue often leaves men with no support.\n\nIn 2012, a UNHCR report stated that \"SGBV (sexual and gender based violence) against men and boys has generally been mentioned as a footnote in reports\". In one study, less than 3% of organizations that address rape as a weapon of war, mention men or provide services to male victims. It was noted in 1990 that the English language is \"bereft of terms and phrases which accurately describe male rape\".\n\nIn the U.S., crime statistics from the 1976 onwards show that men make up the majority of the homicide perpetrators regardless if the victim is female or male. Men are also over-represented as victims in homicide involving both male and female offenders. According to the Bureau of Justice Statistics, women who kill men are most likely to kill acquaintances, spouses or boyfriends while men are more likely to kill strangers. In many cases, women kill men due to being victims of intimate partner violence, however it should be noted that this research was conducted on women on death row, a sample size of approximately 97 during the last 100 years.\n\n"}
{"id": "39327248", "url": "https://en.wikipedia.org/wiki?curid=39327248", "title": "Well-woman examination", "text": "Well-woman examination\n\nA well woman examination is an exam offered to women to review elements of their reproductive health. It is recommended once a year for most women. The exam includes a breast examination, a pelvic examination and a pap smear but may also include other procedures. Hospitals employ strict policies relating to the provision of consent by the patient, the availability of chaperones at the examination, and the absence of other parties.\n\nMost healthcare providers also allow the patient to specify if they have any preferences towards the examiner's gender.\n\nThe well-woman examination by a medical professional is recommended at least once a year to women over 18 years old and/or women who are sexually active. Its importance lies in identifying potential early health problems. The most important tests included in an examination is the breast exam, pelvic exam and the pap test, although some doctors consider other tests in the examination, including measurement of blood pressure, HIV testing, and other laboratory tests such as urinalysis, CBC (Complete blood count) and testing for other sexually transmitted diseases. The procedure is important also to detect certain cancers, especially breast and cervical cancer.\n\nThe breast examination begins with a visual inspection. With the patient in a prone or seated position, the medical professional will look at both breasts to check the color, symmetry, dimensions according to age, lean body mass, the physiological (pregnancy and lactation) and race, looking for abnormalities, such as bulges and shrinkage. One of these abnormalities is changed in the areola or nipple. If it is flattened or retracted (umbilicated), it is necessary to consider the possibility of a cancerous lesion which has caused the malformation.\n\nNext, the breasts are palpated, again with the patient lying or sitting. The patient has to lift the arm and put one hand behind her head. With this position, the entire gland is palpated. It is also important to examine the armpits, because of masses that may be found there. The test is executed pressing the gland with two or three fingers against the chest wall, making a radial route or by quadrants. The nipple is also squeezed check for secretions, such as secretion of milk (galactorrhea), serous, blood or purulent secretions. If a node is detected, it is necessary to determine its place, size, shape, edges, consistency and sensitivity.\n\nIn addition to the yearly check by a professional, women over the age of 18 should also perform this examination monthly.\nIt is important because regular and comprehensive examinations of the breasts can be used to find breast changes that occur between every clinical examination and detect early breast cancer. This auto examination should to be performed seven days after the onset of the menstrual period. If a woman finds a lump or notice any changes in her breast, she should seek medical attention promptly.\n\nA mammogram or mammography is a special x-ray of the breasts. They are the procedure most likely to detect early breast cancer in asymptomatic women. Mammograms can show tumors long before they are large enough to palpate. They are recommended for women who have symptoms of breast cancer or who are at increased risk of developing the disease. They are performed with the patient standing, the breast pressed between two plastic plates, as the image is taken. The interpretation has to be performed by a specialist.\n\nBreast ultrasound is a complementary study of mammography. In many women the tissue that makes up the breast is very dense, representing fibrous tissue and glandular tissue, which produces milk during lactation. This limits the radiologist interpreting the study, so, in these cases, the ultrasound is helpful, since this is capable of distinguishing tumors in women with dense breast tissue, where identification is otherwise difficult. Additionally, it is advisable to follow up a mammogram that shows indications of tumors with an ultrasound, to confirm, before more invasive procedures are undertaken.\n\nThe pelvic exam is part of the physical examination of the pelvic area of a woman, which generally also includes the taking of a sample for a pap smear. This test includes three parts. These are the general inspection of the external genitalia, bimanual examination, and inspection of the vaginal canal using a speculum.\n\nThe patient is placed in a supine position on a special examination table, which has two protrusions called \"stirrups.\" With the feet in these stirrups, the legs are placed in a position such that the medical professional can access the pelvic area. The external genitalia is examined first, looking for abnormalities like lesions, ulcers, warts and color changes. The elements of this exam include the vulva, which contains the mons pubis, of which there are two longitudinal folds of skin forming the labia majora; then the labia minora and hair follicles. The clitoral hood is also checked.\n\nThe purpose of this exam is to palpate organs which cannot be seen with visual inspection. The index and middle finger are inserted into the vagina. This maneuver allows the doctor to check the orientation, length and width of the vagina. Next, the cervix and vaginal fornices are palpated, to check position, size, consistency, mobility and sensibility. The other hand is placed in the pubis pressing it to feel the uterus between both hands. The most important characteristics to examine are the size of the uterus, presence of nodes or agglomerations, consistency, size, tilt, and mobility. With this technique, the ovaries are also palpable.\n\nThe speculum examination is recommended for only women over 21 years old, irrespective of her sexual activity. The speculum is an instrument made of metal or plastic and is constructed with two flaps. Its purpose is to separate and widen the vaginal opening and keep it open. This allows direct observation by the physician into the vaginal canal with the help of a lamp or a mirror.\n\nThere are different types of speculums used within the different characteristics of each patient such as age, sex life, and other factors. The first step is to open the vaginal opening with two fingers at the vulvo-perineal angle, then separate the fingers slightly and press down, then insert the speculum arranging the width of the tip of the flaps in anteroposterior. Then the speculum is moved into the vagina at an angle of 45°, following the natural contour of the posterior vaginal wall. When the speculum is in place, the fingers are removed and the device is rotated so that the flaps are horizontal. The flaps are then separated and locked into place when the cervical neck is completely visible.\n\nThe pap smear is a screening test to test for abnormalities such as cervical cancer and human papillomavirus infections, which require early treatment. To be viable, the patient should not be menstruating and had not used a vaginal contraceptive in the prior 24–48 hours. The procedure begins with the scraping of cells from the cervix and the uterine fornix, done during the speculum examination as there is access to the cervix. The scraping is done with a spatula, cervical brush or swab. Some women experience temporary bleeding from this procedure. The scrapings are placed on a slide, covered with a fixative for later examination under a microscope to determine if they are normal or abnormal.\n"}
