{"id": "18534749", "url": "https://en.wikipedia.org/wiki?curid=18534749", "title": "Advanced Life Support in Obstetrics", "text": "Advanced Life Support in Obstetrics\n\nALSO aims to decrease morbidity and mortality for both the mother and baby. ALSO does this by incorporating both didactic and practical hands on workstations with lifelike mannequins. Topics include assisted vaginal delivery, Doppler fetal monitoring, fetal dystocias, neonatal resuscitation, management premature labor, management of postpartum hemorrhage, along with forceps and vacuum-assisted delivery. Participants must pass a written test as well as a practical hands-on case management of a birth (mega-delivery) incorporating many elements learned throughout the course.\n\nALSO helps serve the same function as advanced trauma life support (ATLS) and advanced cardiac life support (ACLS) to help keep physicians who work with rural or underserved populations up to date on evidence-based medicine and curriculum.\n"}
{"id": "9125999", "url": "https://en.wikipedia.org/wiki?curid=9125999", "title": "Blood sugar regulation", "text": "Blood sugar regulation\n\nBlood sugar regulation is the process by which the levels of blood sugar, primarily glucose, are maintained by the body within a narrow range. This tight regulation is referred to as glucose homeostasis. Insulin, which lowers blood sugar, and glucagon, which raises it, are the most well known of the hormones involved, but more recent discoveries of other glucoregulatory hormones have expanded the understanding of this process.\n\nBlood sugar levels are regulated by negative feedback in order to keep the body in balance. The levels of glucose in the blood are monitored by many tissues, but the cells in the pancreatic islets are among the most well understood and important.\n\nIf the blood glucose level falls to dangerous levels (as during very heavy exercise or lack of food for extended periods), the alpha cells of the pancreas release glucagon, a hormone whose effects on liver cells act to increase blood glucose levels. They convert glycogen into glucose (this process is called glycogenolysis). The glucose is released into the bloodstream, increasing blood sugar. Hypoglycemia, the state of having low blood sugar, is treated by restoring the blood glucose level to normal by the ingestion or administration of dextrose or carbohydrate foods. It is often self-diagnosed and self-medicated orally by the ingestion of balanced meals. In more severe circumstances, it is treated by injection or infusion of glucagon.\n\nWhen levels of blood sugar rise, whether as a result of glycogen conversion, or from digestion of a meal, a different hormone is released from beta cells found in the Islets of Langerhans in the pancreas. This hormone, insulin, causes the liver to convert more glucose into glycogen (this process is called glycogenesis), and to force about 2/3 of body cells (primarily muscle and fat tissue cells) to take up glucose from the blood through the GLUT4 transporter, thus decreasing blood sugar. When insulin binds to the receptors on the cell surface, vesicles containing the GLUT4 transporters come to the plasma membrane and fuse together by the process of endocytosis, thus enabling a facilitated diffusion of glucose into the cell. As soon as the glucose enters the cell, it is phosphorylated into Glucose-6-Phosphate in order to preserve the concentration gradient so glucose will continue to enter the cell. Insulin also provides signals to several other body systems, and is the chief regulator of metabolic control in humans.\n\nThere are also several other causes for an increase in blood sugar levels. Among them are the 'stress' hormones such as epinephrine (also known as adrenaline), several of the steroids, infections, trauma, and of course, the ingestion of food.\n\nDiabetes mellitus type 1 is caused by insufficient or non-existent production of insulin, while type 2 is primarily due to a decreased response to insulin in the tissues of the body (insulin resistance). Both types of diabetes, if untreated, result in too much glucose remaining in the blood (hyperglycemia) and many of the same complications. Also, too much insulin and/or exercise without enough corresponding food intake in diabetics can result in low blood sugar (hypoglycemia).\n\nSome edible mushrooms are noted for the ability to lower blood sugar levels including lingzhi, maitake \"Agaricus blazei\" as well as some others.\n\nSome minerals play roles in glucose regulation: see Chromium in glucose metabolism for example.\n"}
{"id": "33680836", "url": "https://en.wikipedia.org/wiki?curid=33680836", "title": "Calvin C.J. Sia", "text": "Calvin C.J. Sia\n\nCalvin C.J. Sia (born Calvin Chia Jung Sia on June 3, 1927) is a primary care pediatrician from Hawaii who developed innovative programs to improve the quality of medical care for children in the United States and Asia. Two particular programs have been implemented throughout America: the Medical Home concept for primary care that has been promoted by the American Academy of Pediatrics and the federal Emergency Medical Services for Children program administered by the U.S. Department of Health and Human Services’ Health Resources and Services Administration, Maternal and Child Health Bureau. His Medical Home model for pediatric care and early childhood development began to take root in several Asian countries in 2003.\n\nSia is also creator of Hawaii Healthy Start Home Visiting Program to prevent child abuse and neglect and co-founder of Hawaii's Zero to Three program and Healthy and Ready to Learn Center. The Hawaii Healthy Start program, which targets expecting and new parents who may be at risk of abusing or neglecting their children, became the model for the Healthy Families America home visiting program that the United States Department of Justice's Office of Justice Programs identified in 2010 as a \"promising\" approach to child abuse prevention. The Healthy and Ready to Learn Center was a three-year pilot project to initiate training and health delivery services in an integrated system of care, with pediatric residents and graduate students in social work and early childhood education working as a team.\n\nIn addition, Sia spearheaded the creation of the Variety School for learning disabled children, a Honolulu-based educational institution for children ages 5 through 13. Sia retired from his Honolulu-based medical practice in 1996, after almost 40 years of treating patients, but continues to promote Medical Home and community pediatrics as professor of Pediatrics at the University of Hawaii John A. Burns School of Medicine. Although he retired as chairman of the American Medical Association Section Council on Pediatrics in 2007, a post he assumed in 1983, Sia continues to play a national role as an emeritus member of the executive committee of the National Center for Medical Home Implementation Project Advisory Committee, an organization he formerly served as chairman.\n\nSia is a 1945 graduate of Punahou School in Honolulu and a graduate of Dartmouth College in 1950. He received his medical degree at Western Reserve University School of Medicine in 1955 and did a general rotating internship as a lieutenant in the U.S. Army Medical Corps at William Beaumont Army Hospital in El Paso, Texas from 1955-1956. Sia then served his pediatric residency under Dr. Irvine McQuarrie at Kauikeolani Children's Hospital in Honolulu, and obtained his license to practice medicine in Hawaii in 1958. He was certified by the American Board of Pediatrics in 1960 and recertified in 1987. The University of Hawaii awarded Sia an honorary Doctor of Humane Letters degree in 1992.\n\nAs a young practicing pediatrician, Sia joined the early cadre of American Academy of Pediatrics consultants for Head Start and Parent Child Centers in Hawaii in the 1960s and developed a strong interest in prenatal, neonatal, and postnatal causes of physical and mental disabilities in children. In a paper he presented in 1964 to the Hawaii Academy of Sciences on advances in neonatology, Sia cited progress in the care of premature babies but also noted that \"completeness\" of the first physical exam and the education of nurses to be on the alert for early signs of disabilities were possible ways to save newborns with previously lethal birth defects. He concluded by observing, \"One of the basic problems will be in solving the causes and prevention of prematurity.\"\n\nInspired by one of his mentors, Dr. Robert E. Cooke, the Johns Hopkins pediatrician behind the creation of the Hopkins hospital's Kennedy Institute for Handicapped Children, Sia helped establish Hawaii's Variety School for Learning Disabilities in 1967 and served as chairman of its board of directors for many years. Sia broadened the scope of his community work to address all children with special health care needs. In the early 1970s, he invited Dr. C. Henry Kempe, founder of the Denver-based National Center for the Prevention and Treatment of Child Abuse and Neglect, and Dr. Ray E. Helfer of Michigan—two pioneers in the identification and treatment of child abuse—to help him and a small group of child advocates develop a plan to prevent and treat child abuse and neglect in the islands. That effort netted one of the first 12 demonstration grant awards by the newly created National Center on Child Abuse and Neglect in 1975, with $1 million going to establish the first Hawaii Family Stress Center. The center, later renamed the Hawaii Family Support Center, established several child abuse and neglect programs on Oahu, including a home-visiting program based on Kempe's effective use of \"lay therapists.\" These were home visitors from the community, properly trained and supervised by public health nurses and social workers who could earn the trust of at-risk families and focus on family strengths to reduce environmental risk and prevent child abuse and neglect. The center's goal was to identify vulnerable families before their day-to-day stresses, isolation, and lack of parenting knowledge and good role models gave rise to abusive and neglectful behavior.\n\nThe center's operations coincided with an effort launched by Dr. Vince L. Hutchins and Dr. Merle McPherson of the Maternal and Child Health Bureau in 1977 to revise and update the\nmission of the federal agency's Title V and companion \"crippled children's\" programs to address child development and the prevention of developmental, behavioral and psychosocial problems. McPherson took note of Sia's call for a continuous system of care originating with the primary care pediatrician. The AAP collaborated in this effort by asking each state’s AAP chapter to develop a Child Health Plan that set priorities for using MCHB block grants. Sia spearheaded the Hawaii planning effort, bringing together representatives from the Hawaii AAP Chapter, the UH medical school, the Hawaii Medical Association, and Kapiolani Medical Center for Women and Children. Armed with anecdotal evidence showing home visitors were able to promote\neffective parenting and ultimately improve outcomes, the group wrote a plan that incorporated a coordinated system of care that emphasized wellness and prevention for\nchildren, especially those with special needs.\n\nThis was the birth of the Medical Home concept for primary care, to which Sia attached the slogan, “Every Child Deserves a Medical Home.” Under this idea, which the American Academy of Pediatrics adopted as a policy statement in 1992, the medical care of all infants, children and adolescents should be accessible, continuous, comprehensive, family-centered, coordinated, compassionate, and culturally effective. It should be delivered or directed by well-trained physicians who provide primary care and help to manage and facilitate essentially all aspects of pediatric care. The physician should be known to the child and family and should be able to develop a partnership of mutual responsibility and trust with them. As Sia and his co-authors of a 2006 monograph on the Medical Home noted, this new model broadens the traditional focus on acute care to include prevention and well care at one end of the continuum and chronic care management of children with special health care needs at the other. One expert observed, for example, that for a child born with spina bifida, Sia's Medical Home model would have the family and its health care provider compose a list of specialists and therapists who would be caring for the child and a timeline of anticipated surgeries and interventions. The aim would be to have as few emergencies and unanticipated events as possible.\n\nAs the lead author of an often-cited article published by the journal Pediatrics in May 2004, Sia traced the development of the Medical Home concept.\n\nBy 1984, Sia had begun to implement the Medical Home concept in Hawaii. As chairman of an ad hoc state legislative task force on child abuse, he persuaded Hawaii lawmakers to authorize the Hawaii Healthy Start Home Visiting Program for the prevention of child abuse and neglect. This state-funded pilot program, carried out by Hawaii Family Support Center in collaboration with the Hawaii Department of Health, focused on a neighborhood in the Ewa community on Oahu, a community with relatively high rates of child abuse and neglect. A year later, he spearheaded the Hawaii Medical Association's effort to obtain a grant from the U.S. Maternal and Child Health Bureau, under the Special Projects of Regional and National Significance (SPRANS) initiative, to train primary care physicians to provide a \"Medical Home\" for all children with special health care needs. The demonstration project—which sought to help first-time families give their newborn children the best start in life—was so successful it was expanded from a small part of Oahu to other areas of Hawaii, and as word of the demonstrated positive outcomes spread, Hawaii’s Healthy Start became a model for parenting education programs nationwide. In the early 1990s, Healthy Families America and the National Healthy Start Association began to standardize and credential programs to ensure effectiveness and research-based practices. Across the United States, according to the MCHB, the home visiting program has shown that it can reduce child maltreatment and increase children’s readiness for school.\n\nMeanwhile, Sia launched the Hawaii Early Intervention Program for infants and toddlers in 1986 and also became actively involved with Hawaii’s Early Intervention Coordinating Council for Zero to Three, placing this under Hawaii’s Department of Health instead of the Department of Education. The focus of this effort was to support the Medical Home system of care with prevention and early intervention programs.\n\nAt a June 1987 conference called by Surgeon General C. Everett Koop and sponsored by the AAP and MCHB to address children with special needs, Sia and his delegation from Hawaii made a presentation of the Medical Home concept. Koop appeared to embrace it by issuing a report that endorsed a system of family-centered, community-based, coordinated care for children with special needs. This was followed in 1989 by the first National Medical Home Conference, which drew 26 AAP state chapters to Hawaii for presentations organized by Sia and MCHB officials on how to train pediatricians in the Medical Home system of care. This led to consultations to introduce the Medical Home training program to interdisciplinary teams of pediatricians, families, and other health care–related professionals in Florida, Minnesota, Nebraska, Pennsylvania, Washington and other states.\n\nThe pace of activity prompted Sia to close his private medical practice in 1996 so he could devote his time as principal investigator on various early childhood grant projects promoting the Medical Home and its integrated system of care. He launched several initiatives with a MCHB Health Education Collaboration grant in support of interprofessional training in early childhood, a Carnegie Corporation of New York Starting Points planning grant in early childhood, and Consuelo Foundation of Hawaii's Healthy and Ready to Learn grant–all with the emphasis on integrating the continuum of care of the Medical Home with other health, family, and community services from a holistic approach. The MCHB funding enabled him to travel across the country to promote the Medical Home concept to various\ncommunities, state AAP chapters, family advocacy groups and state Title V maternal and child health officers.\n\nA three-year pilot project creating a Healthy and Ready to Learn Center in Hawaii began in 1992 and helped gauge the effectiveness of Sia's family-centered interprofessional collaboration approach. Lessons learned from this project were subsequently adopted by the Office of Children and Youth of the Governor's Office of Hawaii with Sia as Co-Principal Investigator. The Carnegie Corp. Starting Points grant then was assumed by the Good Beginnings Alliance in Hawaii.\n\nSia, serving as chairman of the American Medical Association's Section Council on Pediatrics and other AMA- and AAP-related posts, used those platforms and his network of contacts with other groups to help introduce the Medical Home concept into the care of adults as well as children, although his primary focus has remained on pediatric care. In 2007, the AAP, American Academy of Family Physicians, American Academy of Pediatrics, American College of Physicians and the American Osteopathic Association adopted the Joint Principles of the Patient-Centered Medical Home that set a standard definition of a Medical Home. A year later, the AMA adopted the principles, which have since received support from over 700 member organizations of the Patient Centered Primary Care Collaborative, including primary care and specialty care societies, all major health plans and consumer organizations. In addition, the term Medical Home now regularly shows up in the literature of parent groups such as Family Voices, in family practice journals and on the websites of state public health and medical agencies.\n\nBeginning in 2000, Sia expanded his efforts related to early child development and the Medical Home to Asia. In 2003, he created the Asia-US Partnership, a think tank based at the University of Hawaii medical school whose mission is to improve child health in Asia and the United States through cross-cultural\nexchanges with leaders in pediatrics. That same year, Sia initiated and chaired the first of several AUSP Early Child Development and Primary Care conferences, bringing together pediatric and early childhood development experts from Asia and the United States to translate the science of early child development into policy and action. Participants have come from China (Beijing, Shanghai and Hong Kong), the Philippines, Singapore and Thailand and the United States. According to conference reports, these international exchanges have stimulated translation of the science on early child development and primary care into action programs in the broad areas of advocacy, service delivery, research, and training among the Asian early childhood professionals leadership. Sia has continued to serve as co-chairman of these events, including the sixth international conference, held in the Philippines capital of Manila, in May 2011. After hosting the earliest AUSP conferences in Hawaii, Sia decided to move the 2009 event to Shanghai and tapped a team of Chinese doctors to serve as conference host, signaling what he called a new phase of activity aimed at developing greater shared leadership and stronger \"country teams.\"\n\nWhile planting the seeds of the Medical Home concept in Hawaii, Sia embarked on a related advocacy campaign focused on emergency care for children. In 1979, as president of the Hawaii Medical Association, Sia urged members of the American Academy of Pediatrics to develop multifaceted Emergency Medical Services programs designed to decrease disability and death in children. By January 1981, AAP's Executive Board had approved formation of a Section on Emergency Medicine, with Sia as one of its seven charter members. He along with José B. Lee then-executive officer of the Hawaii Medical Association Emergency Medical Services Program began working closely with Senator Daniel Inouye, whom he happened to meet on a flight to Washington, D.C., to create a National Emergency Medical Services for Children System (EMSC) demonstration grant program to address acute injuries, illnesses and other childhood crises. The program was launched after the October 1984 enactment of EMSC legislation (Public Law 98-555), a bipartisan measure sponsored by Inouye and Republican Senators Orrin Hatch of Utah and Lowell Weicker of Connecticut and endorsed by Surgeon General C. Everett Koop. States receiving these demonstration grants established an emergency medical care service system for children that upgraded training and equipment for first responders and emergency departments to treat children. Hawaii ultimately received a grant to initiate its own emergency care system for children, which improved care coordination with the primary care physician. EMSC is now an established statewide system of care for children in all 50 states and territories.\n\nSeveral national and state organizations have recognized Sia for developing innovative and responsive family-centered grassroots services. Among the awards he has received are these:\n\n\nSia was born in Beijing, China to Dr. Richard Ho Ping Sia, a physician and former Rockefeller Institute researcher in infectious diseases whose work laid the groundwork for the Avery–MacLeod–McCarty experiment on DNA and bacterial transformation, and Mary Li Sia, a Honolulu-born author of several Chinese cookbooks. His mother's parents were Kong Tai Heong and Li Khai Fai, doctors who worked on the 1899 plague outbreak. Sia and his older sister Sylvia and younger sister Julia, all United States citizens by birth, grew up in Hawaii, where the family settled in 1939 after living under Japanese occupation in Beijing for nearly two years.\n\nSia married Katherine Li in 1951. Sia has three sons, Richard H.P. Sia, a journalist; Jeffrey H.K. Sia, a Honolulu-based attorney and former president of the Hawaii State Bar Association; and Dr. Michael H.T. Sia, a pediatrician and chairman of Pediatrics at Kapiolani Medical Center for Women and Children; and six grandchildren.\n\n"}
{"id": "51491107", "url": "https://en.wikipedia.org/wiki?curid=51491107", "title": "Circle of Health International", "text": "Circle of Health International\n\nCircle of Health International, known as COHI for short, is a US based non-governmental organization founded in 2004 with the mission to work with women and their communities with a community based approach in times of crisis. As of 2016, COHI has responded to eighteen humanitarian emergencies and served over three million women globally. COHI has worked with midwives and public health professionals in Sri Lanka, Louisiana, Tibet, Tanzania, Israel, the Philippines, Palestine, Jordan, Syria, Oklahoma, Nicaragua, Sudan, Haiti, and Afghanistan.\n\nAs of 2016 COHI supports maternal and child health clinics in Haiti, midwives in an indigenous women's forum in Nicaragua, midwifery students and sexual health advocates in Nepal, a clinic for refugees in the Rio Grande Valley on the Mexico/US border, and works with survivors of human trafficking globally. COHI is also engaged in Austin's social enterprise community through a program, known as the COHI Cloth Network, to address women's poverty through income generation initiatives.\n\nIn 2004, COHI partnered with a local host organization, Tibetan Healing Fund, in Tongren, also known as Repkong, Eastern Tibet to aid with the training of midwives in order to create a more sustainable maternal health care system.\n\nIn 2004, COHI and their partners worked with Israeli and Palestinian woman to address midwifery and gender based violence (GBV). Conducted an assessment based on three main categories\n\n\nThe assessment included in depth interviews and recommendations for each sector of Israel and the West Bank's diverse populations: religious and secular Jews, immigrant populations (Ethiopian, Russian, Congolese), the Bedouin, and the Palestinian populations of Israel and the West Bank, both Muslim and Christian\n\nThe results from the assessment were used to advocate for the needs of Israeli and Palestinian women such as better postpartum care.\n\n"}
{"id": "4857593", "url": "https://en.wikipedia.org/wiki?curid=4857593", "title": "Clinical research", "text": "Clinical research\n\nClinical research is a branch of healthcare science that determines the safety and effectiveness (efficacy) of medications, devices, diagnostic products and treatment regimens intended for human use. These may be used for prevention, treatment, diagnosis or for relieving symptoms of a disease. Clinical research is different from clinical practice. In clinical practice established treatments are used, while in clinical research evidence is collected to establish a treatment.\n\nThe term \"clinical research\" refers to the entire bibliography of a drug/device/biologic, in fact any test article from its inception in the lab to its introduction to the consumer market and beyond. Once the promising candidate or the molecule is identified in the lab, it is subjected to pre-clinical studies or animal studies where different aspects of the test article (including its safety toxicity if applicable and efficacy, if possible at this early stage) are studied.\n\nIn the United States, when a test article is unapproved or not yet cleared by the Food and Drug Administration (FDA), or when an approved or cleared test article is used in a way that may significantly increase the risks (or decreases the acceptability of the risks), the data obtained from the pre-clinical studies or other supporting evidence, case studies of off label use, etc. are submitted in support of an Investigational New Drug (IND) application to the FDA for review prior to conducting studies that involve even one human and a test article if the results are intended to be submitted to or held for inspection by the FDA at any time in the future (in the case of an already approved test article, if intended to submit or hold for inspection by the FDA in support of a change in labeling or advertising). Where devices are concerned the submission to the FDA would be for an Investigational Device Exemption (IDE) application if the device is a significant risk device or is not in some way exempt from prior submission to the FDA. In addition, clinical research may require Institutional Review Board (IRB) or Research Ethics Board (REB) and possibly other institutional committee reviews, Privacy Board, Conflict of Interest Committee, Radiation Safety Committee, Radioactive Drug Research Committee, etc. approval whether or not the research requires prior submission to the FDA. Clinical research review criteria will depend on which federal regulations the research is subject to (e.g., (Department of Health and Human Services (DHHS) if federally funded, FDA as already discussed) and will depend on which regulations the institutions subscribe to, in addition to any more stringent criteria added by the institution possibly in response to state or local laws/policies or accreditation entity recommendations. This additional layer of review (IRB/REB in particular) is critical to the protection of human subjects especially when you consider that often research subject to the FDA regulation for prior submission is allowed to proceed, by those same FDA regulations, 30 days after submission to the FDA unless specifically notified by the FDA not to initiate the study.\n\nClinical research is often conducted at academic medical centers and affiliated research study sites. These centers and sites provide the prestige of the academic institution as well as access to larger metropolitan areas, providing a larger pool of medical participants. These academic medical centers often have their internal Institutional Review Boards that oversee the ethical conduct of medical research.\n\nThe clinical research ecosystem involves a complex network of sites, pharmaceutical companies and academic research institutions. This has led to a growing field of technologies used for managing the data and operational factors of clinical research. Clinical research management is often aided by eClinical systems to help automate the management and conducting of clinical trials.\n\nIn the European Union, the European Medicines Agency (EMA) acts in a similar fashion for studies conducted in their region. These human studies are conducted in four phases in research subjects that give consent to participate in the clinical trials.\n\nClinical trials involving new drugs are commonly classified into four phases. Each phase of the drug approval process is treated as a separate clinical trial. The drug-development process will normally proceed through all four phases over many years. If the drug successfully passes through Phases I, II, and III, it will usually be approved by the national regulatory authority for use in the general population. Phase IV are 'post-approval' studies.\n\nPhase I includes 20 to 100 healthy volunteers or individuals with the disease/condition. This study typically lasts several months and its purpose is safety and dosage. Phase II includes larger number of individual participants ranging 100-300, and phase III includes Approximately 1000-3000 participants to collect more data about the drug. 70% of drugs advance to the next phase.\n\nBefore pharmaceutical companies start clinical trials on a drug, they conduct extensive pre-clinical studies.\n\n\n"}
{"id": "6762402", "url": "https://en.wikipedia.org/wiki?curid=6762402", "title": "Component causes", "text": "Component causes\n\nA component cause of a disease is an event required for the disease to develop.\nGiven a disease or medical condition, there is a causality chain of events from the first event to the appearance of the clinical disease\nA cause of a disease event is an event that preceded the disease event in a disease causal chain. Without this antecedent event the disease event either would not have occurred at all or would not have occurred until some later time. However, no specific event is sufficient by itself to produce disease. Hence such an event is a component of a sufficient cause.\n\n\n"}
{"id": "4118664", "url": "https://en.wikipedia.org/wiki?curid=4118664", "title": "Dental assistant", "text": "Dental assistant\n\nDental assistants (also known as dental nurses) are members of the dental team. They may support a dental operator (such as a dentist or other treating dental auxiliary) in providing more efficient dental treatment. \"Dental assistants\" are distinguished from other groups of dental auxiliaries (such as dental therapists, dental hygienists and dental technicians) by differing training, roles and patient scopes.\n\nThe first dental assistant in modern history is believed to trace back to 1885. Dr Edmund Kells, a pioneering dentist operating from New Orleans, enlisted the help of his wife, Malvina Cueria to assist him during dental procedures. The dental field was initially dominated by males, but after this first addition of a female, it was then acceptable for women to seek dental treatment without their husbands. This led to dental assistants of that era also being known as \"Ladies in Attendance\". Thanks to the addition of women to dentistry, the profession flourished with more and more women seeking treatment and more patients overall receiving care.\n\nIt was not until almost four decades later that in 1923 the first dental assistant association was founded by Juliette Southard, named the American Dental Assistant Association and it is still in practise now. It began with just five members, now reaching more than 10,000.\n\nThe dental assistant's role is often thought to be to support the dental operator by typically passing instruments during clinical procedures. However, in fact, their role extends much further to include: providing patients help with their oral hygiene skills, preparing the patient for treatment, sterilising instruments, assisting during general anaesthetic dental procedures, positioning suction devices, exposing dental radiographs, taking dental impressions, recording patient notes and administration roles such as scheduling appointments.\n\nIt was customary for oral health care workers and dental assistants in the 1980s to practice oral health care without wearing gloves, masks or eye protection. This was at a crucial time due to the human immunodeficiency virus (HIV) spreading rapidly at a global rate. However, in 2018 gloves, masks and eye protection have become part of the standard infection control guidelines which has been implemented in all oral health care settings as a means of preventing the spread of infectious disease. Infection control in oral health care not only protects the patient but it also protects the oral health care workers. This includes: dentists, dental specialists, oral health therapists, dental hygienists and dental assistants.\n\nDental assistants play a crucial role in maintaining high levels of infection control in the oral health care setting. The dental assistant is the major link between oral health care workers and the patient. To perform infection control responsibilities well, the dental assistant must have the appropriate education, training and work experience. Examples of infection control protocols that the dental assistant needs to follow in an oral health setting include:\n\nHand hygiene aims to reduce the number of microorganisms on hands. Antimicrobial agents such as alcohol-based hand rub or antimicrobial soap and water are effective agents to remove most antimicrobial bacteria on hands in dental settings.\n\nGloves, gown and eye protection are essential barrier protection items that enable the dental assistant to reduce the transmission of infectious diseases to themselves, other dental co-workers and patients. Gloves and masks need to be disposed after each patient, eyewear can be disposable or reusable and gowns need to be replaced if visibly soiled. Lastly, footwear must include leather closed toe shoes; this minimises the risk of sharps injury.\n\nIt is crucial to wear sterile gloves when the dental assistant is assisting in surgery with the oral health care clinician. Hand hygiene using antimicrobial soap and water is imperial to maintaining adequate infection control in this setting, as this minimises the risk of infection.\n\nIt is crucial that sharp instruments which include: needles, scalers, scalpels, burs, orthodontics bands and endodontic files need to be handled with care and appropriate techniques to minimise any potential sharps injury. Sharps also need to be disposed accordingly into the sharps containers, separate from other disposable bins. The dental assistant needs to be aware of what is required to go into the sharps containers and what is not. This minimises the chance of spreading infectious diseases.\n\nIt is imperative that when performing change over, that the dental assistant is able to distinguish the difference between medical related waste compared to contaminated waste. Contaminated waste needs to be placed in a leak proof thick yellow bag with a biohazard symbol label.\n\nThe dental assistant should put on new gloves when cleaning working surfaces during the changeover between patients. They must also be able to distinguish between clean and dirty zones as this minimises the spread of infectious diseases between co-workers and patients. Additionally, plastic barriers are placed on: instruments such as; hand pieces connected to the chair, overhead lights, amalgamators, x-ray machines, mixing materials and other miscellaneous dental instruments, materials or appliances. One of the roles that the dental assistant participates in is changing these plastic barriers after each patient has left that surgery. This ensures that the surgery is set up ready for the next patient.\n\n\nDental assistants play a large role in ensuring that the reusable dental instruments used daily in dental practices are sterilised adequately after each use. Sterilisation is an essential part of the infection control protocol. This can be defined as free of all life forms where the elimination of considerable number of the most heat resistant spores (bacterial and mycotic) is the basic criteria sterilisation. Sterilisation process consists of\n\n\nDisinfectant is also one of the major roles of dental assistant where chemicals applied to inanimate surfaces such as lab tops, counter tops, headrests, light handles, etc. This is to make sure that germicide and/or microbiostatic are achieved. Antiseptic chemical agents similar to disinfectants but they may be applied safely to living tissue, is another task for dental assistant where Alcohol is the most commonly used.\n\nDental assistants make a difference in the community by participating in health promotion activities and programs. These programs may take place at schools, preschools, immunisation events or at maternal health clinics. Dental operators may also be supported by dental assistants during pre-school or school screenings.\n\nDental assistants can extend their scope to provide oral health promotion to patients in Australia by completing the Certificate IV in Dental Assisting (Oral Health Promotion). The dental assistant will have the ability to implement an individualised oral health promotion program and oral hygiene program. After the appropriate training the dental assistant may; \n\n\nDental assistants help other dental and allied health professionals in health promotion. These dental assistants implement oral health programs by providing resources and presentation promoting oral health messages to several target groups and community settings. These settings include:\n\n\nDental assistant can educate the community and schools by advising on:\n\n\nCurrently in Australia, dental assistants are not required to be registered under the Dental Board of Australia. However, dental assistants who have attained their certificate IV in dental assisting – Dental Radiography must hold a current license with the relative state or territory radiation authority. Dental assistants that decide to take on further study into their certificate IV in dental assisting - dental radiography, have an advantage of exposing patients to radiation also known as an x-ray, with regards to oral health care. The dental assistant will take training to further their practical and theoretical skills relevant to the role of the dental radiographer. \nUpon successful completion of the training program dependent on the course structure, the dental assistant may:\n\nExpose intra-oral radiographs that ensures an optimum radiographic image.\n\n\nLooking to the future of dentistry and oral health, the roles for dental assistants are ever-changing and expanding. With the increase in an ageing population, it will become more and more commonplace for dental assistants to be employed to support dental operators with providing oral health promotion and treatment within residential care facilities.\n\nThe number of newly graduated dentists and dental auxiliaries is increasing and with that the need for dental assistants. According to the Bureau of Labor Statistics of America the rate of employed dental assistants will likely increase by 18% in the ten years between 2014 and 2024. With an increase in dental assistants comes the possibility of extension in the dental assistant roles and scope. As seen in some states of the United States of America, dental assistants are able to perform some simple fillings with the supervision of a practising dentist. By allowing dental assistants to extend their scope alongside the appropriate training, the workload of the other members of the dental team is lessened and increases efficiency of the dental clinic management. This may have the potential to reach other countries in the future as the need for dental and oral health treatments increase.\n\nDental assistant roles can vary depending on different country qualifications and regulations. Below are examples of dental assisting roles which the dental assistant is able to perform, respective to that country.\n\nAccording to the Australian Government, Department of Health, in 2006 there were 15,381 registered dental assistants supporting dental operators. Of those, 171 were Indigenous.\n\nIn Australia Dental Assistants should have the following skills:\n\n\nDental Assistants work as part of a wider dental team, primarily with Dentists, but also with Dental Specialists, Oral Health Therapists, Dental Therapists, Dental Technitions, Dental Hygienists and Dental Prosthetists.\n\nTasks include:\n\n\n\nVICTORIA\n\nNEW SOUTH WALES\n\nQUEENSLAND\n\nNORTHERN TERRITORY\n\nWESTERN AUSTRALIA\n\nSOUTH AUSTRALIA\n\nTASMANIA\nAustralian Dental assistants perform limited and restricted duties and are not permitted to perform any of the following:\n\n\nDental Assisting is not a registered profession in Australia and as such training courses are not mandatory, although those with nationally recognised qualifications will enjoy the benefits of higher wages and better employment opportunities.\n\nThere is no formal training required of entry level dental assistants in order to commence or undertake work in private practice in Australia. Most dental assistants gain practical experience at a place of employment although there are vocational qualifications which are nationally recognised and highly recommended for increasing a person's job prospects, remuneration, and professional development.\n\nThe National Vocational Qualification HLT35015 Certificate III in Dental Assisting is the entry level of vocational training for dental assisting while HLT45015 Certificate IV in Dental Assisting are suitable for those who seek to further their skills and duties and elect units from particular streams such as dental radiography, oral health promotion, practice administration, general anaesthesia and conscious sedation. These formal qualifications can be offered only by registered training organisations such as TAFE and professional associations while Certificate III in Dental Assisting may also be offered as a traineeship in most States of Australia and as a School-based Traineeship for years 11 and 12 in some States.\n\nCurrently dental assistants are not required to be registered under the Dental Board of Australia or with any State and Territory Boards since dental assisting is not a registered profession. Dental Assistants who have attained a Certificate IV in Dental Assisting – Dental Radiography and are required to operate dental radiography apparatus as part of their job role, must hold a current license with the relevant state or territory Radiation Authority.\n\nDental assistants are strongly encouraged to have current vaccinations for Hepatitis B, and Tetanus along with the normal childhood vaccination recommendations (Measles, mumps, varicella, polio) and influenza. Many state and territory public health care facilities and training providers will require students and workers to present evidence of Hepatitis B immunity and the results of a criminal history check prior to commencing clinical placement. Most private dental clinics will also require employees to have current vaccination records and may also require workers to undertake annual influenza vaccinations.\n\nAccording to Occupational Employment Statistics, in the USA in 2017 there are a total of 337,160 Dental Assistants: they all should have the following personal qualities:\n\n\nUnlike Australia, in the USA dental assisting is a registered profession represented by the American Dental Assistants Association (ADAA) and members should possess both front desk and chairside skills.\n\nRoutine duties include:\n\n\nExtended duties may include:\n\n\nIn some U.S. states, dental assistants can work in the field without a college degree, while in other states, dental assistants must be licensed or registered/\n\nDental assistants can receive formal education through academic programs at community colleges, vocational schools, career colleges, technical institutes, universities and dental schools with most programs needing only 8 to 11 months to complete.\n\nThe Commission on Dental Accreditation of the American Dental Association accredits dental assisting school programs, of which there are over 200 in the United States.\n\nTo become a Certified Dental Assistant, or CDA, dental assistants must take the DANB (Dental Assisting National Board) CDA examination after they have completed an accredited dental assisting program, while those who have been trained on the job or have graduated from non-accredited programs are eligible to take the national certification examination after they have completed two years of full-time work experience as dental assistants. Some dentists are willing to pay a dental assistant-in-training that has a good attitude and work ethic.\n\nIn the USA the Dental Assisting National Board offers three nationally recognised certifications, namely:\n\n\nExpanded duties dental assistants or expanded functions dental assistants, as they are known in some states, may work one on one with the patient performing restorations after the doctor has removed decay Ideally, a dental assistant should have both administrative and clinical skills although it's still acceptable to have one or the other.\n\nDuties may also include seating and preparing the patient, charting, mixing dental materials, providing patient education and post-operative instructions. They also keep track with inventory control and ordering supplies.\n\nIn the UK, Registered Dental Nurses are prohibited from carrying out any form of direct dental treatment on the patient, including teeth whitening procedures, under the GDC scope of practice. Dental nurses found to be carrying out dental procedures are liable to be removed from the statutory GDC register.\n\nDuties include:\n\n\nThose with additional training or skills developed during their careers can undertake expanded duties that may include:\n\n\nEntry level working as a trainee dental nurse does not require any qualification, but progression to qualified dental nurse requires completion of a formal course of study, either part or full-time, approved by the General Dental Council. A minimum 2 GCSEs (C grade or above) in English language and maths or a science subject are usually required for part-time courses while full-time courses may require evidence of A-level or AS-level study. A level 3 apprenticeship in dental nursing is an alternative pathway to gaining the required qualifications in dental nursing.\n\nIn Ireland dental assistants have the following tasks:\n\n\nSkills Required\n\n\nIn the Republic of Ireland, it is often dental nurses (and teeth whitening technicians) who carry out teeth whitening procedures rather than dentists.\n\nThis practice mainly occurs in clinics focusing solely on laser teeth whitening. In Ireland, registration as a dental nurse with The Irish Dental Council is voluntary; however, nurses who are registered and who carry out teeth whitening may face disciplinary action if caught.\n\n"}
{"id": "50500819", "url": "https://en.wikipedia.org/wiki?curid=50500819", "title": "Digestible Indispensable Amino Acid Score", "text": "Digestible Indispensable Amino Acid Score\n\nDigestible Indispensable Amino Acid Score (DIAAS) is a protein quality method, proposed in March 2013 by the Food and Agriculture Organization to replace the current protein ranking standard, the Protein Digestibility Corrected Amino Acid Score (PDCAAS).\n"}
{"id": "1547057", "url": "https://en.wikipedia.org/wiki?curid=1547057", "title": "EHealth", "text": "EHealth\n\neHealth (also written e-health) is a relatively recent healthcare practice supported by electronic processes and communication, dating back to at least 1999. Usage of the term varies. A study in 2005 found 51 unique definitions. Some argue that it is interchangeable with health informatics with a broad definition covering electronic/digital processes in health while others use it in the narrower sense of healthcare practice using the Internet. It can also include health applications and links on mobile phones, referred to as mHealth or m-Health. \nSince about 2011, the increasing recognition of the need for better cyber-security and regulation may result in the need for these specialized resources to develop safer eHealth solutions that can withstand these growing threats.\n\nThe term can encompass a range of services or systems that are at the edge of medicine/healthcare and information technology, including:\n\n\nSeveral authors have noted the variable usage in the term; from being specific to the use of the Internet in healthcare to being generally around any use of computers in healthcare. Various authors have considered the evolution of the term and its usage and how this maps to changes in health informatics and healthcare generally. Oh \"et al.\", in a 2005 systematic review of the term's usage, offered the definition of eHealth as a set of technological themes in health today, more specifically based on commerce, activities, stakeholders, outcomes, locations, or perspectives. One thing that all sources seem to agree on is that e-Health initiatives do not originate with the patient, though the patient may be a member of a patient organization that seeks to do this, as in the e-Patient movement.\n\neHealth literacy is defined as “the ability to seek, find, understand and appraise health information from electronic sources and apply knowledge gained to addressing or solving a health problem. According to this definition, eHealth literacy encompasses six types of literacy: traditional (literacy and numeracy), information, media, health, computer, and scientific. Of these, media and computer literacies are unique to the Internet context, with eHealth media literacy being the awareness of media bias or perspective, the ability to discern both explicit and implicit meaning from media messages, and to derive meaning from media messages. The literature includes other definitions of perceived media capability or efficacy, but these were not specific to health information on the Internet. Having the composite skills of eHealth literacy allows health consumers to achieve positive outcomes from using the Internet for health purposes. eHealth literacy has the potential to both protect consumers from harm and empower them to fully participate in informed health-related decision making. People with high levels of eHealth literacy are also more aware of the risk of encountering unreliable information on the Internet On the other hand, the extension of digital resources to the health domain in the form of eHealth literacy can also create new gaps between health consumers. eHealth literacy hinges not on the mere access to technology, but rather on the skill to apply the accessed knowledge.\n\nOne of the factors blocking the use of e-Health tools from widespread acceptance is the concern about privacy issues regarding patient records, most specifically the EPR (Electronic patient record). This main concern has to do with the confidentiality of the data. There is also concern about non-confidential data however. Each medical practise has its own jargon and diagnostic tools. To standardize the exchange of information, various coding schemes may be used in combination with international medical standards. Systems that deal with these transfers are often referred to as Health Information Exchange (HIE). Of the forms of e-Health already mentioned, there are roughly two types; front-end data exchange and back-end exchange.\n\nFront-end exchange typically involves the patient, while back-end exchange does not. A common example of a rather simple front-end exchange is a patient sending a photo taken by mobile phone of a healing wound and sending it by email to the family doctor for control. Such an actions may avoid the cost of an expensive visit to the hospital.\n\nA common example of a back-end exchange is when a patient on vacation visits a doctor who then may request access to the patient's health records, such as medicine prescriptions, x-ray photographs, or blood test results. Such an action may reveal allergies or other prior conditions that are relevant to the visit.\n\nSuccessful e-Health initiatives such as e-Diabetes have shown that for data exchange to be facilitated either at the front-end or the back-end, a common thesaurus is needed for terms of reference. Various medical practises in chronic patient care (such as for diabetic patients) already have a well defined set of terms and actions, which makes standard communication exchange easier, whether the exchange is initiated by the patient or the caregiver.\n\nIn general, explanatory diagnostic information (such as the standard ICD-10) may be exchanged insecurely, and private information (such as personal information from the patient) must be secured. E-health manages both flows of information, while ensuring the quality of the data exchange.\n\nPatients living with long term conditions (also called Chronic conditions) over time often acquire a high level of knowledge about the processes involved in their own care, and often develop a routine in coping with their condition. For these types of routine patients, front-end e-Health solutions tend to be relatively easy to implement.\n\nE-mental health is frequently used to refer to internet based interventions and support for mental health conditions. However, it can also refer to the use of information and communication technologies that also includes the use of social media, landline and mobile phones. E-mental health services can include information; peer support services, computer and internet based programs, virtual applications and games as well as real time interaction with trained clinicians. Programs can also be delivered using telephones and interactive voice response (IVR).\n\nMental disorders includes a range of conditions such as alcohol and drug use disorders, mood disorders such as depression, dementia and Alzheimer's disease, delusional disorders such as schizophrenia and anxiety disorders. The majority of e-mental health interventions have focused on the treatment of depression and anxiety. There are, however, programs also for problems as diverse as smoking cessation, gambling, and post-disaster mental health.\n\nE-mental health has a number of advantages such as being low cost, easily accessible and providing anonymity to users. However, there are also a number of disadvantages such as concerns regarding treatment credibility, user privacy and confidentiality. Online security involves the implementation of appropriate safeguards to protect user privacy and confidentiality. This includes appropriate collection and handling of user data, the protection of data from unauthorized access and modification and the safe storage of data.\n\nE-mental health has been gaining momentum in the academic research as well as practical arenas in a wide variety of disciplines such as psychology, clinical social work, family and marriage therapy, and mental health counseling. Testifying to this momentum, the E-Mental Health movement has its own international organization, the International Society for Mental Health Online. However, e-Mental health implementation into clinical practice and healthcare systems remains limited and fragmented.\n\nThere are at least five programs currently available to treat anxiety and depression. Several programs have been identified by the UK National Institute for Health and Care Excellence as cost effective for use in primary care. These include \"Fearfighter\", a text based cognitive behavioral therapy program to treat people with phobias, and \"Beating the Blues\", an interactive text, cartoon and video CBT program for anxiety and depression. Two programs have been supported for use in primary care by the Australian Government. The first is \"Anxiety Online\", a text based program for the anxiety, depressive and eating disorders, and the second is \"THIS WAY UP\", a set of interactive text, cartoon and video programs for the anxiety and depressive disorders. Another is \"iFightDepression®\" a multilingual, free to use, web-based tool for self-management of less severe forms of depression, for use under guidance of a GP or psychotherapist.\n\nThere are a number of online programs relating to smoking cessation. \"QuitCoach\" is a personalised quit plan based on the users response to questions regarding giving up smoking and tailored individually each time the user logs into the site. \"Freedom From Smoking\" takes users through lessons that are grouped into modules that provide information and assignments to complete. The modules guide participants through steps such as preparing to quit smoking, stopping smoking and preventing relapse.\n\nOther internet programs have been developed specifically as part of research into treatment for specific disorders. For example, an online self-directed therapy for problem gambling was developed to specifically test this as a method of treatment. All participants were given access to a website. The treatment group was provided with behavioural and cognitive strategies to reduce or quit gambling. This was presented in the form of a workbook which encouraged participants to self-monitor their gambling by maintaining an online log of gambling and gambling urges. Participants could also use a smartphone application to collect self-monitoring information. Finally participants could also choose to receive motivational email or text reminders of their progress and goals.\n\nAn internet based intervention was also developed for use after Hurricane Ike in 2009. During this study, 1,249 disaster-affected adults were randomly recruited to take part in the intervention. Participants were given a structured interview then invited to access the web intervention using a unique password. Access to the website was provided for a four-month period. As participants accessed the site they were randomly assigned to either the intervention. those assigned to the intervention were provided with modules consisting of information regarding effective coping strategies to manage mental health and health risk behaviour.\n\nCybermedicine is the use of the Internet to deliver medical services, such as medical consultations and drug prescriptions. It is the successor to telemedicine, wherein doctors would consult and treat patients remotely via telephone or fax.\n\nCybermedicine is already being used in small projects where images are transmitted from a primary care setting to a medical specialist, who comments on the case and suggests which intervention might benefit the patient. A field that lends itself to this approach is dermatology, where images of an eruption are communicated to a hospital specialist who determines if referral is necessary.\n\nThe field has also expanded to include online \"ask the doctor\" services that allow patients direct, paid access to consultations (with varying degrees of depth) with medical professionals (examples include Bundoo.com, Doctor Spring, Teladoc, and Ask The Doctor).\n\nA Cyber Doctor, known in the UK as a Cyber Physician, is a medical professional who does consultation via the internet, treating virtual patients, who may never meet face to face. This is a new area of medicine which has been utilized by the armed forces and teaching hospitals offering online consultation to patients before making their decision to travel for unique medical treatment only offered at a particular medical facility.\n\nSelf-monitoring is the use of sensors or tools which are readily available to the general public to track and record personal data. The sensors are usually wearable devices and the tools are digitally available through mobile device applications. \nSelf-monitoring devices were created for the purpose of allowing personal data to be instantly available to the individual to be analyzed. As of now, fitness and health monitoring are the most popular applications for self-monitoring devices. The biggest benefit to self-monitoring devices is the elimination of the necessity for third party hospitals to run tests, which are both expensive and lengthy. These devices are an important advancement in the field of personal health management.\n\nSelf-monitoring healthcare devices exist in many forms. An example is the Nike+ FuelBand, which is a modified version of the original pedometer. This device is wearable on the wrist and allows one to set a personal goal for a daily energy burn. It records the calories burned and the number of steps taken for each day while simultaneously functioning as a watch. To add to the ease of the user interface, it includes both numeric and visual indicators of whether or not the individual has achieved his or her daily goal. Finally, it is also synced to an iPhone app which allows for tracking and sharing of personal record and achievements.\n\nOther monitoring devices have more medical relevance. A well-known device of this type is the blood glucose monitor. The use of this device is restricted to diabetic patients and allows users to measure the blood glucose levels in their body. It is extremely quantitative and the results are available instantaneously. However, this device is not as independent of a self-monitoring device as the Nike+ Fuelband because it requires some patient education before use. One needs to be able to make connections between the levels of glucose and the effect of diet and exercise. In addition, the users must also understand how the treatment should be adjusted based on the results. In other words, the results are not just static measurements.\n\nThe demand for self-monitoring health devices is skyrocketing, as wireless health technologies have become especially popular in the last few years. In fact, it is expected that by 2016, self-monitoring health devices will account for 80% of wireless medical devices. The key selling point for these devices is the mobility of information for consumers. The accessibility of mobile devices such as smartphones and tablets has increased significantly within the past decade. This has made it easier for users to access real-time information in a number of peripheral devices.\n\nThere are still many future improvements for self-monitoring healthcare devices. Although most of these wearable devices have been excellent at providing direct data to the individual user, the biggest task which remains at hand is how to effectively use this data. Although the blood glucose monitor allows the user to take action based on the results, measurements such as the pulse rate, EKG signals, and calories do not necessarily serve to actively guide an individual's personal healthcare management. Consumers are interested in qualitative feedback in addition to the quantitative measurements recorded by the devices.\n\nKnowledge of the socio-economic performance of eHealth is limited, and findings from evaluations are often challenging to transfer to other settings. Socio-economic evaluations of some narrow types of mHealth can rely on health economic methodologies, but larger scale eHealth may have too many variables, and tortuous, intangible cause and effect links may need a wider approach.\n\neHealth in general, and telemedicine in particular, is a vital resource to remote regions of emerging and developing countries but is often difficult to establish because of the lack of communications infrastructure. For example, in Benin, hospitals often can become inaccessible due to flooding during the rainy season and across Africa, the low population density, along with severe weather conditions and the difficult financial situation in many African states, has meant that the majority of the African people are badly disadvantaged in medical care. In many regions there is not only a significant lack of facilities and trained health professionals, but also no access to eHealth because there is also no internet access in remote villages, or even a reliable electricity supply.\n\nInternet connectivity, and the benefits of eHealth, can be brought to these regions using satellite broadband technology, and satellite is often the only solution where terrestrial access may be limited, or poor quality, and one that can provide a fast connection over a vast coverage area.\n\n\n\n"}
{"id": "56686863", "url": "https://en.wikipedia.org/wiki?curid=56686863", "title": "Fresh food", "text": "Fresh food\n\nFresh food is food which has not been preserved and has not spoiled yet. For vegetables and fruits, this means that they have been recently harvested and treated properly postharvest; for meat, it has recently been slaughtered and butchered; for fish, it has been recently caught or harvested and kept cold.\n\nDairy products are fresh and will spoil quickly. Thus, fresh cheese is cheese which has not been dried or salted for aging. Soured cream may be considered \"fresh\" (crème fraîche). \n\nFresh food has not been dried, smoked, salted, frozen, canned, pickled, or otherwise preserved.\n\nFreshly cooked food has not been held cold or warm, reheated, for a long time; or made from leftovers.\n"}
{"id": "13312969", "url": "https://en.wikipedia.org/wiki?curid=13312969", "title": "Global mental health", "text": "Global mental health\n\nGlobal mental health is the international perspective on different aspects of mental health. It is 'the area of study, research and practice that places a priority on improving mental health and achieving equity in mental health for all people worldwide'. There is a growing body of criticism of the global mental health movement, and has been widely criticised as a neo-colonial or \"missionary\" project and as primarily a front for pharmaceutical companies seeking new clients for psychiatric drugs.\n\nIn theory, taking into account cultural differences and country-specific conditions, it deals with the epidemiology of mental disorders in different countries, their treatment options, mental health education, political and financial aspects, the structure of mental health care systems, human resources in mental health, and human rights issues among others.\n\nThe overall aim of the field of global mental health is to strengthen mental health all over the world by providing information about the mental health situation in all countries, and identifying mental health care needs in order to develop cost-effective interventions to meet those specific needs.\n\nMental, neurological, and substance use disorders make a substantial contribution to the global burden of disease (GBD). This is a global measure of so-called disability-adjusted life years (DALY's) assigned to a certain disease/disorder, which is a sum of the years lived with disability and years of life lost due to this disease within the total population. Neuropsychiatric conditions account for 14% of the global burden of disease. Among non-communicable diseases, they account for 28% of the DALY's – more than cardiovascular disease or cancer. However, it is estimated that the real contribution of mental disorders to the global burden of disease is even higher, due to the complex interactions and co-morbidity of physical and mental illness.\n\nAround the world, almost one million people die due to suicide every year, and it is the third leading cause of death among young people. The most important causes of disability due to health-related conditions worldwide include unipolar depression, alcoholism, schizophrenia, bipolar depression and dementia. In low- and middle-income countries, these conditions represent a total of 19.1% of all disability related to health conditions.\n\nIt is estimated that one in four people in the world will be affected by mental or neurological disorders at some point in their lives. Although many effective interventions for the treatment of mental disorders are known, and awareness of the need for treatment of people with mental disorders has risen, the proportion of those who need mental health care but who do not receive it remains very high. This so-called \"treatment gap\" is estimated to reach between 76–85% for low- and middle-income countries, and 35–50% for high-income countries.\n\nDespite the acknowledged need, for the most part there have not been substantial changes in mental health care delivery during the past years. Main reasons for this problem are public health priorities, lack of a mental health policy and legislation in many countries, a lack of resources – financial and human resources – as well as inefficient resource allocation.\n\nIn 2011, the World Health Organization estimated a shortage of 1.18 million mental health professionals, including 55,000 psychiatrists, 628,000\nnurses in mental health settings, and 493,000 psychosocial care providers needed to treat mental disorders in 144 low- and middle-income countries. The annual wage bill to remove this health workforce shortage was estimated at about US$4.4 billion.\n\nInformation and evidence about cost-effective interventions to provide better mental health care are available. Although most of the research (80%) has been carried out in high-income countries, there is also strong evidence from low- and middle-income countries that pharmacological and psychosocial interventions are effective ways to treat mental disorders, with the strongest evidence for depression, schizophrenia, bipolar disorder and hazardous alcohol use.\n\nRecommendations to strengthen mental health systems around the world have been first mentioned in the WHO's \"World Health Report 2001\", which focused on mental health:\n\n\nBased on the data of 12 countries, assessed by the \"WHO Assessment Instrument for Mental Health Systems\" (WHO-AIMS), the costs of scaling up mental health services by providing a core treatment package for schizophrenia, bipolar affective disorder, depressive episodes and hazardous alcohol use have been estimated. Structural changes in mental health systems according to the WHO recommendations have been taken into account.\n\nFor most countries, this model suggests an initial period of investment of US$0.30 – 0.50 per person per year. The total expenditure on mental health would have to rise at least ten-fold in low-income countries. In those countries, additional financial resources will be needed, while in middle- and high-income countries the main challenge will be the reallocation of resources within the health system to provide better mental health service.\n\nPrevention is beginning to appear in mental health strategies, including the 2004 WHO report \"Prevention of Mental Disorders\", the 2008 EU \"Pact for Mental Health\" and the 2011 US National Prevention Strategy. NIMH or the National Institute of Mental Health has over 400 grants.\n\nTwo of WHO's core programmes for mental health are WHO MIND (Mental health improvements for Nations Development) and Mental Health Gap Action Programme (mhGAP).\n\nWHO MIND focuses on 5 areas of action to ensure concrete changes in people's daily lives. These are:\n\nMental Health Gap Action Programme (mhGAP) is WHO’s action plan to scale up services for mental, neurological and substance use disorders for countries especially with low and lower middle incomes. The aim of mhGAP is to build partnerships for collective action and to reinforce the commitment of governments, international organizations and other stakeholders.\n\nThe mhGAP Intervention Guide (mhGAP-IG) was launched in October 2010. It is a technical tool for the management of mental, neurological and substance use disorders in non-specialist health settings. The priority conditions included are: depression, psychosis, bipolar disorders, epilepsy, developmental and behavioural disorders in children and adolescents, dementia, alcohol use disorders, drug use disorders, self-harm/suicide and other significant emotional or medically unexplained complaints.\n\nOne of the most prominent critics of the Movement for Global Mental Health has been China Mills, author of the book \"Decolonizing Global Mental Health: The Psychiatrization of the Majority World\".\n\nMills writes that:\n\nAnother prominent critic is Ethan Watters, author of \"Crazy Like Us: The Globalization of the American Psyche\".\n\n\n"}
{"id": "12739077", "url": "https://en.wikipedia.org/wiki?curid=12739077", "title": "Healing of periapical lesions", "text": "Healing of periapical lesions\n\nApical periodontitis is typically the body’s defense response to the threat of microbial invasion from the root canal. Primary among the members of the host defense mechanism is the polymorphonuclear leukocyte, otherwise known as the neutrophil. The task of the neutrophil is to locate and destroy microbes that intrude into the body – anywhere in the body – and they represent the hallmark of acute inflammation.\n\nIn response to tissue injury, neutrophils leave the circulatory system in great numbers and gather at the site of tissue injury. They are drawn to the site by chemotaxis, following a concentration gradient of chemotactic molecules until they reach the site of greatest concentration: the site of injury and microbial presence. Once there, the antimicrobial action of superoxide and hydrogen peroxide, derived from the metabolic processes of the neutrophils, act to combat the microbial invasion. While primarily mobilized to kill the invading microorganisms, the neutrophils actually cause a significant amount of host tissue damage as well. Although the neutrophils themselves rarely remain alive for more than a few days, the excessive accumulation of dead neutrophils and the enzymes they released is a major cause of tissue breakdown in the acute phases of apical periodontitis.\n\nSoon after inflammation has been initiated, macrophages enter the scene and, if not controlled by the initial ambush of neutrophils and their tactics, the microbial invasion is faced with a second strike consisting of these leukocytes, along with lymphocytes. Together, the cells of this second strike compose the bulk of the apical periodontitis lesion and serve an important role in the subsequent chronic phase of inflammation of apical periodontitis, as they can live for many months. Some researchers posit that it must not be macrophages that are involved, as they could not appropriately discriminate between the varied array of opsonized entities as necessary, and that, in reality, the properties ascribed to the macrophage in the initiation phase of the inflammatory response actually belong to the lymphatic dendritic cell. It is unclear, however, if the latter is a distinct population of cells or if it is merely a particularly specialized strain of macrophage.\n\nWhen infections such as these occur elsewhere in the body, the host defense system, able to travel the body via the circulatory system, is, more often than not, capable of appropriately gaining access to the site of infection in order to mount a proper and successful retaliation. Dental pulp, which is a richly vascularized and innervated tissue, is enclosed by tissues, such as dentin, which are incapable of expanding. It has terminal blood flow and possesses only small-gauge circulatory access at the apical foramen. All of these characteristics severely constrain the defensive capacity of the pulp tissue when faced with the different aggressions to which it may be subjected. As a result, necrotic tissue located within the pulp chamber and canals provide nutrients for pathogenic bacteria to grow and form a periapical lesion; the infected tooth serves as a biochemically and physiologically ideal location for bacterial growth and maturation, and, in essence, acts as a refuge from which bacterial reinforcements can mobilize to the periapical lesion. It is this concept that serves as the basis for conventional endodontic therapy; both chemical and mechanical debridement procedures are essential in effectively disrupting and removing the microbial ecosystem that is associated with the disease process. Thus, whenever a pulp is removed and the canal treated and filled in a manner that is compatible with or favorable to a physiologic reaction, we may expect a satisfactory percentage of endodontic success.\n\nIn 1890, W.D. Miller, considered the father of oral microbiology, was the first to associate pulpal disease with the presence of bacteria. This was confirmed by Kakehashi, who, in 1965, proved that bacteria were the cause of pulpal and periradicular disease in studies using animal models; pulpal exposures were initiated in both normal and germ-free rats, and while no pathologic changes were exhibited in the mouths of the germ-free rats, introduction of the normal oral microbial flora produced pulpal necrosis and led to periradicular lesion formation in the normal rats. The germ-free rats healed regardless of the severity of pulpal exposure, demonstrating that the presence or absence of bacteria was the determinant for pulpal and periapical disease.\n\nMoreover, it has since been discovered that endodontic infections are polymicrobial. In fact, the bacteria present within endodontic infections are thoroughly similar to the bacteria that are involved in periodontal disease. It has also been shown that certain enzymes produced by bacteria are detrimental to the host, and can work in concert with the destructive capability of the enzymes released by dying neutrophils. Recent studies have revealed that the gene for collagenases could be detected in stains of \"Porphyromonas gingivalis\", one of the many endodontic infective agents that are also involved in periodontal disease.\n\nAdditionally, it has been proven that a positive correlation exists between the number of bacteria in an infected root canal and the size of the resultant periradicular radiolucency.\n\nIn attempting to resolve a periapical lesion of endodontic origin, it is essential to be conscious of these principles in order to effectively combat the infection. Without proper consideration for the causes, the pulpal and periapical infection cannot be suitably treated, for effective patient management requires the correct diagnosis and removal of the cause of the infection of endodontic origin to correct the associated periapical lesion. Because periapical disease is almost inevitably preceded by pulp disease, proper chemomechanical debridement of the infected root canals, together with incision and drainage of associated periradicular swellings, will usually allow for rapid improvement in patient signs and symptoms. The same end can be accomplished by extracting the involved tooth.\n\nAlthough periapical changes will be in response to pulpal changes the majority of the time, it is still important to determine the disease process sequence. When the disease process is of pulpal origin, the pulpal infection and necrosis may drain not only through the apical foramen, but also through an accessory canal, which may present radiographically as a periradicular or furcation radiolucency. This may further lead to furcal involvement through loss of clinical attachment and alveolar bone. A cursory clinical examination and radiographic analysis can easily lead the clinician off the right course and pulpal involvement might be overlooked when the tooth is asymptomatic. Similarly, a periodontal abscess may very well appear to be pulpal in origin, when in fact it is not. Notwithstanding the tissue of origin, though, when it is determined that there is a pulpal involvement to the periodontal lesion, the endodontic infection should be controlled prior to beginning definitive management of the periodontal lesion, especially when regenerative or bone grafting techniques are planned.\n\nTo achieve healing of the periapical lesion, one must obtain and maintain a decontaminated root canal system. \"System\" is to be emphasized, because the root canal system does not merely consist of tapering cone-shaped canals from orifice to apex, but rather, can and often is an intricate labyrinth of canals that diverge and weave to form an elaborate web of anastomosing passages. It is precisely because of this reality that “it is important to appreciate that files produce shape, but it is essential to understand that irrigants clean [the] root canal system. Copious amounts of sodium hypochlorite are necessary to utterly dissolve all remnants of pulp tissue as well as completely destroy all microorganisms. The tooth stability does not undergo major changes after surgery comparated with the initial value which was determined before establishing any kind of treatment.\n\nMany authoritative clinicians and researchers advise completing endodontic therapy as soon as possible, especially in situations necessitating incision and drainage, in order to remove the cause of infection without delay. Recent studies have shown, however, that intracanal application of certain medicaments prior to the completion of endodontic therapy may produce highly favorable results when followed by conventional therapy, even when the periapical area is very large. The use of chlorhexidine gluconate and calcium hydroxide for infection control was shown to lead to substantial healing of a large periapical lesion.\n\nThe traditional thought that it is necessary to complete endodontic therapy as quickly as possible may be related only to the initial steps of therapy, namely, a thorough instrumentation, thus ensuring a proper biomechanical preparation. While completion of the procedure with immediate obturation might secure the decontaminated root canal system, delaying this step in order to allow for application of medicaments has been shown to be beneficial. Periodic application and renewal of calcium hydroxide over a year’s time (four applications over a 12-month period), has been shown to represent a nonsurgical approach to resolving even extensive inflammatory periapical lesions.\n\nThe use of adjunctive antibiotics is usually uncalled for when proper debridement procedures can be executed in a conventional periapical lesion of endodontic origin; however, they can be centrally important to the treatment of a progressive or persistent infection. It has been proposed, however, that disinfection of the root canal by means of an antibacterial agent, such as propolis or otosporin, can lead to improved healing by reducing and controlling pulpal and periapical inflammatory reactions. This would, in turn, promote the healing process as well as provide for better control, prevention and reduction of post-treatment pain and discomfort.\n\nThere are a number of active biologic mediators that have been implicated in promoting apical resorption. Matrix metalloproteinases (MMPs), which are endogenous zinc-dependent catabolic enzymes, are primarily responsible for the degradation of much of the tissue matrices built on such architecturally important substances as collagen and proteoglycan core proteins. Their biologic activities have been extensively researched and reviewed, and their importance in the pathogenesis of apical periodontitis is obvious. Furthermore, concentrations of IgG antibodies have been found to be nearly five times higher in lesions of apical periodontitis than in uninflamed oral mucosa.\n\nProstaglandins, specifically PGE2 and PGI2, are important in inflammation and have been implicated in promoting apical resorption. This is because neutrophils, which are rich sources of PGE2, are present when the majority of rapid bone loss occurs during the initial stages of apical periodontitis. It has been illustrated clinically that parenteral administration of indomethacin, an inhibitor of cyclooxygenase, can act to suppress resorption of apical hard tissue.\n\nThe predominant mechanism of bone resorption in a periapical lesion, as in the rest of the body, is the performed by osteoclasts. In the periapical lesion, mediators that are normally produced primarily only by osteoblasts are released by many other cells as well, overstimulating proosteoclasts. As a result, these begin to proliferate and several cells fuse to form multinucleated giant cells capable of spreading over the infected, injured site and cause resorption of the periapical alveolar bone.\n\nIt is possible that after conventional endodontic therapy has been completed, little to no resolution of the periapical lesion occurs over a considerable amount of time; there is a great deal of current research that discusses the possible reasons for this outcome and suggests possible treatment options. For example, it has been demonstrated that there is an association between nonresolving apical periodontitis lesions and the presence of cholesterol clefts within the periapical lesion; in fact, an incidence of up to 44% has been reported! It was shown that macrophages joined to form multinucleated giant cells and then produced a well-circumscribed area of tissue reaction, not unlike the granulomatous tissue reactions of a typical foreign body reaction, and persisted for up to 8 months. Similarly, endodontic materials as well as food debris may also be the cause of persistent periapical lesions. It was once shown that contaminated gutta percha resulted in a persistent periapical lesion for more than a decade!\n\nFor retreatment of a non-healing lesion, there is really no magical method that can be employed; the course of action is merely to achieve what was supposed to have been achieved the first time. Keeping in mind the notion that endodontic retreatment is a problem-solving exploit will substantially increase its success.\n\nAfter endodontic therapy has been executed, or re-executed, successfully, and the canals can no longer provide a nutrient-rich habitat for microbes, the issue of bone healing comes into focus. Ostensibly, then, for regeneration to occur, the root canal system must have been decontaminated and further access to microbial invasion must be prohibited. Regeneration of the bone has been demonstrated to occur, on average, at a rate of 3.2 mm² per month, and studies suggest that 71% of lesions have achieved complete resolution one year post-operatively.\n\nSituations in which a surgical form of retreatment had been selected and in which apical resolution has still not occurred may still benefit from additional surgical intervention. A comparison of the outcome of periradicular surgery in teeth that had previously undergone surgical treatment versus teeth that were undergoing a surgical procedure for the first time showed that, after 5 years, 86% of surgically treated teeth healed with complete bone filling of the surgical cavity while only 59% of resurgically treated teeth healed with complete bone filling. It has thus been demonstrated that surgical retreatment of teeth previously treated with surgery is a valid alternative to extraction.\n\nHowever, a combination of three antibiotics (metronidazole, ciprofloxacin, and minocycline) in a paste has been used successfully to treat these lesions non-surgically.\n"}
{"id": "4279531", "url": "https://en.wikipedia.org/wiki?curid=4279531", "title": "Health impact assessment", "text": "Health impact assessment\n\nHealth impact assessment (HIA) is defined as \"a combination of procedures, methods, and tools by\nwhich a policy, program, or project may be judged as to its potential effects on the\nhealth of a population, and the distribution of those effects within the population.\" \n\nHIA is intended to produce a set of evidence-based recommendations to inform decision-making . HIA seeks to maximise the positive health impacts and minimise the negative health impacts of proposed policies, programs or projects.\n\nThe procedures of HIA are similar to those used in other forms of impact assessment, such as environmental impact assessment or social impact assessment. HIA is usually described as following the steps listed, though many practitioners break these into sub-steps or label them differently:\n\n\nThe main objective of HIA is to apply existing knowledge and evidence about health impacts, to specific social and community contexts, to develop evidence-based recommendations that inform decision-making in order to protect and improve community health and wellbeing. Because of financial and time constraints, HIAs do not generally involve new research or the generation of original scientific knowledge. However, the findings of HIAs, especially where these have been monitored and evaluated over time, can be used to inform other HIAs in contexts that are similar. An HIA's recommendations may focus on both design and operational aspects of a proposal.\n\nHIA has also been identified as a mechanism by which potential health inequalities can be identified and redressed prior to the implementation of proposed policy, program or project .\n\nA number of manuals and guidelines for HIA's use have been developed (see further reading).\n\nThe proposition that policies, programs and projects have the potential to change the determinants of health underpins HIA's use. Changes to health determinants then leads to changes in health outcomes or the health status of individuals and communities. The determinants of health are largely environmental and social, so that there are many overlaps with environmental impact assessment and social impact assessment.\n\nThree forms of HIA exist:\n\n\nIt has been suggested that HIAs can be prospective (done before a proposal is implemented), concurrent (done while the proposal is being implemented) or retrospective (done after a proposal has been implemented) . This remains controversial, however, with a number of HIA practitioners suggesting that concurrent HIA is better regarded as a monitoring activity and that retrospective HIA is more akin to evaluation with a health focus, rather than being assessment per se . Prospective HIA is preferred as it allows the maximum practical opportunity to influence decision-making and subsequent health impacts.\n\nHIA practitioners can be found in the private and public sectors, but are relatively few in number. There are no universally accepted competency frameworks or certification processes. It is suggested that a lead practitioner should have extensive education and training in a health related field, experience of participating in HIAs, and have attended an HIA training course. It has been suggested and widely accepted that merely having a medical or health degree should not be regarded as an indication of competency.\n\nThe International Association for Impact Assessment has an active health section.\n\nA HIA People Directory can be found on the HIA GATEWAY.\n\nHIA is currently being used or developed around the world, most notably in Europe, North America, Australia, New Zealand, Africa and Thailand.\n\nThe new safeguard policies and standards of the International Finance Corporation (IFC), part of the World Bank, were established in 2006. These contain a requirement for health impact assessment in large projects. The standards have been accepted by most of the leading lending banks who are parties to the Equator Principles. Health impact assessments are becoming routine in many large development projects in both public and private sectors of developing countries. There is also a long history of health impact assessment in the water resource development sector - large dams and irrigation systems.\n\nThe 13th International HIA Conference was held in Geneva in 2013.\n\nThe 12th International HIA Conference was held in Québec City in 2012.\n\n\n\n\"This page uses . References are sorted alphabetically by author surname.\"\n\n\n\n\n\n\"This page uses . Further reading categories are sorted alphabetically; citations are sorted by year (newest to oldest), then alphabetically by author surname within years. If citations are included in the references section they are not listed in the further reading section.\"\n\n\n\n\n\n\n\"This page uses . External links are sorted alphabetically.\"\n"}
{"id": "57944950", "url": "https://en.wikipedia.org/wiki?curid=57944950", "title": "Health of Donald Trump", "text": "Health of Donald Trump\n\nSince the early days of Donald Trump's presidential campaign, his physical and mental health have been a subject of public debate. Trump was seventy years old when he took office, surpassing Ronald Reagan as the oldest person to assume the presidency. Comments on his age, weight and lifestyle have raised questions about his physical health. In addition, numerous public figures, media sources, and mental health professionals have speculated that Donald Trump may have mental health challenges, ranging from narcissistic personality disorder to some form of dementia. Trump and his supporters have denied these allegations, and have contested the authority and motives of persons making such claims.\n\nIn December 2015, Trump's personal physician, Harold Bornstein, released a superlative-laden letter of health praising Trump for \"extraordinary physical strength and stamina\". Bornstein later said that Trump himself had dictated the contents. A followup medical report showed Trump's blood pressure, liver and thyroid functions to be in normal ranges, and that he takes a statin.\n\nIn January 2018, Trump was examined by White House physician Ronny Jackson, who stated that he was in excellent health, although his weight and cholesterol level were higher than recommended, and that his cardiac assessment revealed no medical issues. Several outside cardiologists commented that Trump's weight, lifestyle and LDL cholesterol ought to have raised serious concerns about his cardiac health.\n\nTrump does not drink alcohol; this decision arose in part from watching his older brother Fred Jr. suffer from alcoholism that contributed to his early death in 1981. He also said that he has never smoked cigarettes or consumed drugs, including marijuana.\n\nOn May 1, 2018, Bornstein told NBC News that three Trump representatives had \"raided\" his office on February 3, 2017, taking all of Trump's medical records. He identified two of the men as Trump's longtime bodyguard Keith Schiller and the Trump Organization's chief legal officer Alan Garten. Two days earlier, Bornstein had told a reporter that Trump took a prescription hair growth medicine, Propecia, after which Trump cut ties with him.\n\nAs early as November 2015, \"Vanity Fair\" reported the opinion of a number of mental health experts that Trump had \"[t]extbook narcissistic personality disorder\". Bornstein's letter of December 2015, drafted in response to questions about the health of presidential candidates, did not address Trump's mental health, while claiming that he would be \"the healthiest individual ever elected to the presidency\". Bornstein disclosed in 2018 that Trump had dictated this letter over the telephone. He said: \"Mr. Trump dictated the letter and I would tell him what he couldn't put in there.\" In August 2016, Bornstein stated that Trump's \"health is excellent, especially his mental health.\"\n\nIn February 2016, presidential candidate Jeb Bush speculated that Trump had mental health issues, stating \"I'm not a psychiatrist or a psychologist, but the guy needs therapy\". In early 2017, psychologist John Gartner collected more than 25,000 signatures of mental health professionals on a petition, directed to the national opposition leader, Democratic Senator Chuck Schumer. At the end of April 2017 he was able to forward the petition with more than 41,000 signatures of mental healthcare professionals to Washington D.C.. The core of the petition stated: \"My professional judgement is that Donald Trump manifests a serious mental illness that renders him psychologically incapable of competently discharging the duties of President of the United States. And I respectfully request him [sic] be removed from office, according to article 3 of the 25th Amendment, and so on\". Gartner asserted that Trump's mental handicaps are a mix of 1. narcissism, 2. paranoia; 3. sociopathy; and 4. a dash of sadism.\n\nOn April 14, 2017, Representatives Jamie Raskin and Earl Blumenauer introduced the Oversight Commission on Presidential Capacity Act. The bill would replace the Cabinet as the body that, together with the Vice President, determines whether to invoke Section 4 of the Twenty-fifth Amendment to the United States Constitution, which permits removal of a President who is mentally incapacitated. Under the bill, an eleven-member commission, including four psychiatrists, would conduct an examination of the President when directed to do so by a concurrent resolution of the Congress. Blumenauer stated:\n\nIn January 2018, North Korea's leader Kim Jong-un and Trump publicly exchanged claims about their respective \"nuclear buttons\". In reaction, Richard W. Painter, a former adviser to President George W. Bush, deemed Trump \"psychologically unfit\" and supported transferring his powers to Vice President Pence under the 25th Amendment. In April 2018 \"Vanity Fair\" reported that Trump's advisers \"worry about his mental health\" when he is outside the controls available in the White House environment.\n\nIn September 2017, Jeanne Suk Gerson wrote in \"The New Yorker\": \"A strange consensus does appear to be forming around Trump's mental state,\" including Democrats and Republicans who doubt Trump's fitness for office. Journalist Bill Moyers interviewed psychiatrist Robert Jay Lifton and said that Trump \"makes increasingly bizarre statements that are contradicted by irrefutable evidence to the contrary\". Lifton replied, \"He doesn’t have clear contact with reality, though I’m not sure it qualifies as a \"bona fide\" delusion.\" As an example, Lifton said, when Trump claimed that former president Barack Obama was born in Kenya, \"he was manipulating that lie as well as undoubtedly believing it in part.\"\n\nIn April 2017 forensic psychiatrist Bandy X. Lee hosted a meeting at Yale University medical school regarding the ethics of discussing Trump's mental health. In October 2017, Lee published \"The Dangerous Case of Donald Trump\", containing essays from 27 psychologists, psychiatrists, and mental health professionals on the \"clear and present danger\" that Trump's mental health poses to the \"nation and individual well being\". They argued that the president's issues affected the mental health of the United States population, and that he placed the country at grave risk of war because of his pathological traits. They defined Trump's behavior in terms of psychiatric diseases, such as narcissistic personality disorder. Carlos Lozada, writing for \"The Washington Post\", considered these conclusions \"compelling\", but also noted that the book contributors were writing from their own political perspective, as other mental health professionals differ. Lee and others contend that Trump's presidency represents an emergency allowing, or even requiring, psychiatrists to take exception from the APA's Goldwater rule, which holds that it is unethical for psychiatrists to give a professional opinion about public figures without having examined them in person, and without their consent.\n\nThe letter titled I Am Part of the Resistance Inside the Trump Administration published by the New York Times declares most if not all those working for Trump see him as mentally unfit for his job.\n\nIt has been asserted that Trump has signs of some degree of an early stage of dementia, constituting an inability to consistently remember facts, or respond appropriately to circumstances of his surroundings.\n\nOn several occasions, Trump has been reported as appearing to have mental lapses. In March 2017, Trump forgot to sign two executive orders before leaving a signing ceremony for those orders. On July 5, 2017, Trump, after deplaning from Air Force One, appeared confused and wandered away from his waiting limousine. On October 12, 2017, Trump initially forgot to sign the Trumpcare executive order before leaving the signing ceremony, but was ushered back to the table by Vice President Mike Pence to complete this step.\n\nIn July 2018, during a press conference with Russian President Vladimir Putin at the 2018 Russia–United States summit in Helsinki, Trump made a statement interpreted by observers as indicating his inclination to accept Putin's denial of Russian interference in the 2016 United States elections, saying \"President Putin says it's not Russia. I don't see any reason why it would be\". Following criticism of this remark, Trump made a statement asserting that this was the result of a mental lapse, stating \"In a key sentence in my remarks, I said the word 'would' instead of 'wouldn't'\". Former Republican governor of New Jersey, Christine Todd Whitman wrote of this assertion that if Trump did make this error, \"it demonstrates his inability to articulate accurately U.S. foreign policy at the highest level, for the highest stakes\".\n\nTrump has dismissed questions regarding his mental health, stating that he is a \"very stable genius\". As evidence of his mental capacities, he pointed to his business success, his victory over Republican competitors, and his election to the presidency against Hillary Clinton.\n\nTrump and others have asserted that such questions are evidence of a problem with his critics, labeling them as suffering from a \"Trump derangement syndrome\" (TDS). For example, commenting on the book \"The Dangerous Case of Donald Trump\", RealClearPolitics writer Carl M. Cannon argued that the foreword by psychiatrist Robert Jay Lifton \"offers the melodramatic view that clinicians who don't warn the world about Donald Trump's shortcomings are akin to Nazi doctors who worked at Auschwitz. At the risk of practicing medicine without a license, I'd suggest that this historical comparison is \"de facto\" evidence of TDS – and paranoid grandiosity\".\n\nIn response to speculation about his cognitive abilities, Trump was administered the Montreal Cognitive Assessment (MoCA) at his own request as part of his January 2018 health checkup. He received a score of 30/30, indicating a normal level of cognitive function. Critics have contended that the MoCA test is too basic to diagnose the problems asserted.\n\n"}
{"id": "25614856", "url": "https://en.wikipedia.org/wiki?curid=25614856", "title": "Health of Robert E. Howard", "text": "Health of Robert E. Howard\n\nThe health of American author Robert E. Howard (1906–1936), especially his mental health, has been the focus of the biographical and critical analysis of his life. In terms of physical health, Howard had a weak heart which he treated by taking digitalis. The precise nature of Howard's mental health has been much debated, both during his life and following his suicide. Three main points of view exists. Some have declared that Howard suffered from an Oedipus complex or similar mental disorder. Another viewpoint is that Howard suffered from major depressive disorder. The third view is that Howard had no disorders and his suicide was a common reaction to stress.\n\nHoward had a weak heart, which was treated by taking digitalis. The first indication of this problem came when, while working with an oil-field surveyor in the period before he became a successful author, Howard passed out in the heat. A later diagnosis confirmed that his heart had a mild tendency to race under stress and he was told that a heavy blow to his chest could be fatal.\n\nOn December 29, 1933 Howard was involved in a traffic accident. Driving back from a football game in Brownwood with three friends in the mist and rain, he crashed into a flag pole in Ranger. A piece of windshield cut his neck close to the jugular vein and he was forced heavily into the steering column. This impact to his chest could have been lethal with his heart problems but he escaped with just bruised ribs. None of the other passengers were seriously injured either and one escaped injury altogether. The town helped to pay for repairs to the car and the flag pole was eventually removed following further accidents.\n\nIn the years since his suicide, there has been a lot of speculation about Robert E. Howard's mental health. Even during his life, others in Cross Plains thought of him as crazy or odd. Some have suggested that he had an Oedipus complex, others have found evidence for clinical depression, and others still have diagnosed him as being mentally healthy. His act of suicide is often the basis of these opinions. Almost all speculation is in the form of amateur-psychoanalysis from people with no qualifications in the field.\n\nThe people of Cross Plains considered Howard to be an odd person and possibly \"crazy\". Cross Plains was mainly used to blue-collar agricultural and oil field work, as well as professions such as a teacher or doctor; they were not used to some who only wrote all day. From their perspective, Howard did not seem to be doing any work at all. His neighbors even complained about the noise he made typing, as well as loudly reading his own narration as he did so, throughout the day.\n\nHoward displayed eccentric behavior, such as: having his pants hemmed short so he would not trip in a potential fight; carrying a gun in his car in case of enemies; shouting stories as he wrote them; dressing oddly for the time and place, including a large moustache and sombrero; shadow-boxing as he walked down the street. At the time, Novalyne Price told her roommate, \"He’s trying to tell people he’s a writer and writers have a right to be odd. Since they think he’s crazy, anyway, he’ll show them just how crazy he can be.\" In the wake of his break-up with Novalyne Price, Howard's behavior became increasing eccentric. On one occasion, having taken Novalyne to a drug store for a soda, he loudly and publicly re-told his story \"Red Nails.\" He changed his appearance and grew a large, drooping mustache. Later he began to wear about town a black sombrero with red bandana and black vaquero pants.\n\nDr. Charles Gramlich's opinion on Howard's behavior is that these are either normal acts or just eccentricities, which does not extend to having a mental disorder. The carrying of gun is, in Gramlich's opinion, normal for Texas, especially in the 1930s when the Wild West was within recent, living memory. The story about needing the gun in case of \"enemies,\" given to E. Hoffmann Price, may have just been a tall tale invented by Howard for his friend. Novalyne Price considered Howard's talk of enemies to be part of an act. Finn notes that highwaymen had operated in the area during the oil boom and this was the reason Howard gave to Price for the gun. The shadow-boxing and shouting out of stories as they were written were due to Howard being a writer and simply getting caught up in a story. Other acts were intentionally eccentric as a reaction to the criticism from other townspeople. He was expressing his anger to this criticism by acting in a contrary manner.\n\nHoward maintained in correspondence with other writers that he was a failure and a hack. This was despite being commercially successful, sought after by publishers and receiving fan mail for his work. When sending condolences to August Derleth in May 1936, the month before his suicide, Howard wrote \"Death to the old is inevitable, and yet somehow I often feel that it is a greater tragedy than death to the young...I don't want to live to be old. I want to die when my time comes, quickly and suddenly, in the full tide of my strength.\" E. Hoffmann Price visited Howard in early 1934. His impression on leaving was \"Bob lived in a dream world people by enemies, and by peers and other folks who downgraded him.\"\n\nHoward suffered from nightmares and sleep walking into his early twenties, probably as a result of stress. In spring 1926, Howard went to Brownwood to see Tevis Clyde Smith. In the night Clyde Smith was woken by Howard's scream; he saw him wrestle with a large shape and fall through a closed window. The family found him wandering outside in a daze. Clyde Smith talked to him until he went back to sleep, on prior instructions, and then woke him. Howard had apparently dreamed that he saw a newspaper with the headline \"Axe Murderer Slays Three.\"\n\nIn 1930, Howard went to a hospital in Temple complaining of a varicocele, gas in the stomach and an abnormally small penis. The working diagnosis at the time was sexual neurasthenia but the symptoms may instead point towards neurotic depressive disorder. The doctor concluded that \"We do not think there is anything wrong with Robert. We can find no varicocele of any consequence, and his organs are normally developed and he tests out good in every respect. His trouble, in our judgment, is due to his thinking there is something wrong. After he has dispelled this thought from his mind he will be in fine shape.\"\n\nBiographer Mark Finn suggests that Howard picked up on his mother's depression. She was dying of tuberculosis and may have suffered a miscarriage in October 1907. Howard's father's job as a country doctor required him to make long journeys away from home, which would often mean him spending the night elsewhere. This frequently left Howard alone with his mother and later put him in the role of primary caregiver. The situation may have been exacerbated by frequent moves during his youth, which prevented Howard making many friends of his own during his early years, and the gradual breakdown of his parents' marriage. His life was not his own and he developed a hatred of authority and any control placed over him. He rebelled by boxing and drinking but, nevertheless, he felt duty-bound to remain and look after his mother. Even after he became a success as a writer, he continued to live with his parents; although this was not unusual at the time. Finn writes that \"Robert was clinically depressed by any definition applicable, and had been for many years. Whether the cause of the depression was a chemical imbalance, an untenable situation at home, or a combination of the two is not important.\" His suicide, once his duty to his mother was done, may have been an act of finally asserting control over his own life.\n\nL. Sprague de Camp wrote in \"The Science Fiction Handbook\" (1953) that \"the neurotic Howard suffered from an Oedipean devotion to his mother and... from delusions of persecution.\" Faced with refutations from Glenn Lord, de Camp eventually stopped making claims about an Oedipus Complex but maintained that Howard was insane, especially due to Howard's suicide.\n\nA lot of the speculations about Howard's mental state appear to originate in the first, flawed, biography of Howard, \"Dark Valley Destiny\" by L. Sprague de Camp and others. These tend to be from a Freudian perspective and suggest an Oedipal attraction between Robert and his mother Hester, based on the facts that Howard took care of his dying mother, that his bedroom had a window through to his parents' bedroom and that he killed himself on the day she was going to die herself. Gramlich points out that Dr. Isaac Howard's job forced him to take house calls and be away on business while Robert E. Howard's job as a writer meant he was at home most of the time; so, naturally, Robert became the primary caregiver. Howard's bedroom was a converted porch and the window was a pre-existing part of the building.\n\nDr. Charles Gramlich, a professor of psychology and an author of fantasy fiction, believes that Howard had no mental disorders and that amateur psychoanalysis has only come to such a conclusion through cherry-picking of evidence from Howard's life. Burke concurs that almost all speculation is in the form of amateur psychoanalysis from people like L. Sprague de Camp with no qualifications in the field.\n\nConfidants such as Tevis Clyde Smith and Novalyne Price Ellis found Howard to be an agreeable companion most of the time, full of life and good humor — but always with an underlying simmering melancholy. Smith noted that Howard first mentioned suicide in October 1923 when a classmate, Roy Guthrie, committed suicide. It affected Howard and he increasingly defended the right to do so in later years. Howard may have considered suicide as early as 1925. Howard first talked of his own death in 1928 when his dog Patch was dying. When Novalyne Price was admitted to hospital in 1935, her doctor, a friend of Howard's father, asked her if Howard had ever talked about not wanting to live after his mother died.\n\nRegarding Howard's suicide, Gramlich believes it was nothing to do with any long-term mental abnormality; it was a common reaction to the strain he was under at the time. While Howard did talk of suicide during his life, statistics show that one out of three teenagers contemplate suicide and the details of Howard's are normal (white, single, from the south of the United States, with a gun). At the moment of Howard's death, he was mentally and physically exhausted with little available support: he was caring for his dying mother as her condition got worse; he was not being paid the money owed to him by \"Weird Tales\", at a time when he needed it for his mother's healthcare bills; he was working increasingly harder to make the money through other markets; his relationship with Novalyne Price had recently broken down; Tevis Clyde Smith had recently married and moved away; he did not have a strong relationship with his father. Gramlich ends by saying \"He wasn't crazy; he was just a very good writer.\"\n\nDavid Hayles wrote in the Times: \"Maybe, at the end of it all, Howard felt that he had done what he needed to do. The prolific writer, whose 160-plus published stories were full of men facing death on their own terms, wanted to do the same. In a letter to the fantasy writer August Derleth he stated: 'I don’t want to live to be old. I want to die when my time comes, quickly and suddenly, in the full tide of my strength and health.' Youthful bravado perhaps, but he was true to his word. He wrote about men who didn’t age — his heroes were immortal. In bowing out in his prime, so was Robert E. Howard.\"\n\n"}
{"id": "49604", "url": "https://en.wikipedia.org/wiki?curid=49604", "title": "Hearing loss", "text": "Hearing loss\n\nHearing loss, also known as hearing impairment, is a partial or total inability to hear. A deaf person has little to no hearing. Hearing loss may occur in one or both ears. In children, hearing problems can affect the ability to learn spoken language and in adults it can create difficulties with social interaction and at work. In some people, particularly older people, hearing loss can result in loneliness. Hearing loss can be temporary or permanent.\nHearing loss may be caused by a number of factors, including: genetics, ageing, exposure to noise, some infections, birth complications, trauma to the ear, and certain medications or toxins. A common condition that results in hearing loss is chronic ear infections. Certain infections during pregnancy, such as syphilis and rubella, may also cause hearing loss in the child. Hearing loss is diagnosed when hearing testing finds that a person is unable to hear 25 decibels in at least one ear. Testing for poor hearing is recommended for all newborns. Hearing loss can be categorized as mild (25 to 40 dB), moderate (41 to 55 dB), moderate-severe (56 to 70 dB), severe (71 to 90 dB), or profound (greater than 90 dB). There are three main types of hearing loss: conductive hearing loss, sensorineural hearing loss, and mixed hearing loss.\nAbout half of hearing loss globally is preventable through public health measures. Such practices include immunization, proper care around pregnancy, avoiding loud noise, and avoiding certain medications. The World Health Organization recommends that young people limit the use of personal audio players to an hour a day in an effort to limit exposure to noise. Early identification and support are particularly important in children. For many hearing aids, sign language, cochlear implants and subtitles are useful. Lip reading is another useful skill some develop. Access to hearing aids, however, is limited in many areas of the world.\nAs of 2013 hearing loss affects about 1.1 billion people to some degree. It causes disability in 5% (360 to 538 million) and moderate to severe disability in 124 million people. Of those with moderate to severe disability 108 million live in low and middle income countries. Of those with hearing loss it began in 65 million during childhood. Those who use sign language and are members of Deaf culture see themselves as having a difference rather than an illness. Most members of Deaf culture oppose attempts to cure deafness and some within this community view cochlear implants with concern as they have the potential to eliminate their culture. The term hearing impairment is often viewed negatively as it emphasises what people cannot do.\n\nUse of the terms \"hearing impaired\", \"deaf-mute\", or \"deaf and dumb\" to describe deaf and hard of hearing people is discouraged by advocacy organizations as they are offensive to many deaf and hard of hearing people.\n\nHuman hearing extends in frequency from 20–20,000 Hz, and in amplitude from 0 dB to 130 dB or more. 0 dB does not represent absence of sound, but rather the softest sound an average unimpaired human ear can hear; some people can hear down to −5 or even −10 dB. 130 dB represents the threshold of pain. But the ear does not hear all frequencies equally well; hearing sensitivity peaks around 3000 Hz. There are many qualities of human hearing besides frequency range and intensity that can't easily be measured quantitatively. But for many practical purposes, normal hearing is defined by a frequency versus intensity graph, or audiogram, charting sensitivity thresholds of hearing at defined frequencies. Because of the cumulative impact of age and exposure to noise and other acoustic insults, 'typical' hearing may not be normal.\n\n\nHearing loss is sensory, but may have accompanying symptoms:\n\nThere may also be accompanying secondary symptoms:\n\nHearing loss has multiple causes, including ageing, genetics, perinatal problems and acquired causes like noise and disease. For some kinds of hearing loss the cause may be classified as of unknown cause.\n\nThere is a progressive loss of ability to hear high frequencies with aging known as presbycusis. For men, this can start as early as 25 and women at 30. Although genetically variable it is a normal concomitant of ageing and is distinct from hearing losses caused by noise exposure, toxins or disease agents. Common conditions that can increase the risk of hearing loss in elderly people are high blood pressure, diabetes or the use of certain medications harmful to the ear. While everyone loses hearing with age, the amount and type of hearing loss is variable.\n\nNoise exposure is the cause of approximately half of all cases of hearing loss, causing some degree of problems in 5% of the population globally.\nThe National Institute for Occupational Safety and Health (NIOSH) recognizes that the majority of hearing loss is not due to age, but due to noise exposure. By correcting for age in assessing hearing, one tends to overestimate the hearing loss due to noise for some and underestimate it for others.\n\nHearing loss due to noise may be temporary, called a 'temporary threshold shift', a reduced sensitivity to sound over a wide frequency range resulting from exposure to a brief but very loud noise like a gunshot, firecracker, jet engine, jackhammer, etc. or to exposure to loud sound over a few hours such as during a pop concert or nightclub session. Recovery of hearing is usually within 24 hours, but may take up to a week. Both constant exposure to loud sounds (85 dB(A) or above) and one-time exposure to extremely loud sounds (120 dB(A) or above) may cause permanent hearing loss.\n\nNoise-induced hearing loss (NIHL) typically manifests as elevated hearing thresholds (i.e. less sensitivity or muting) between 3000 and 6000  Hz, centred at 4000  Hz. As noise damage progresses, damage spreads to affect lower and higher frequencies. On an audiogram, the resulting configuration has a distinctive notch, called a 'noise' notch. As ageing and other effects contribute to higher frequency loss (6–8 kHz on an audiogram), this notch may be obscured and entirely disappear.\n\nVarious governmental, industry and standards organizations set noise standards.\n\nThe U.S. Environmental Protection Agency has identified the level of 70 dB(A) (40% louder to twice as loud as normal conversation; typical level of TV, radio, stereo; city street noise) for 24‑hour exposure as the level necessary to protect the public from hearing loss and other disruptive effects from noise, such as sleep disturbance, stress-related problems, learning detriment, etc. Noise levels are typically in the 65 to 75 dB (A) range for those living near airports of freeways and may result in hearing damage if sufficient time is spent outdoors.\n\nLouder sounds cause damage in a shorter period of time. Estimation of a \"safe\" duration of exposure is possible using an \"exchange rate\" of 3 dB. As 3 dB represents a doubling of the intensity of sound, duration of exposure must be cut in half to maintain the same energy dose. For workplace noise regulation, the \"safe\" daily exposure amount at 85 dB A, known as an exposure action value, is 8 hours, while the \"safe\" exposure at 91 dB(A) is only 2 hours. \nDifferent standards use exposure action values between 80dBA and 90dBA. Note that for some people, sound may be damaging at even lower levels than 85 dB A. Exposures to other ototoxins (such as pesticides, some medications including chemotherapy agents, solvents, etc.) can lead to greater susceptibility to noise damage, as well as causing its own damage. This is called a \"synergistic\" interaction. Since noise damage is cumulative over long periods of time, persons who are exposed to non-workplace noise, like recreational activities or environmental noise, may have compounding damage from all sources.\n\nSome national and international organizations and agencies use an exchange rate of 4 dB or 5 dB. While these exchange rates may indicate a wider zone of comfort or safety, they can significantly underestimate the damage caused by loud noise. For example, at 100 dB (nightclub music level), a 3 dB exchange rate would limit exposure to 15 minutes; the 5 dB exchange rate allows an hour.\n\nMany people are unaware of the presence of environmental sound at damaging levels, or of the level at which sound becomes harmful. Common sources of damaging noise levels include car stereos, children's toys, motor vehicles, crowds, lawn and maintenance equipment, power tools, gun use, musical instruments, and even hair dryers. Noise damage is cumulative; all sources of damage must be considered to assess risk. If one is exposed to loud sound (including music) at high levels or for extended durations (85 dB A or greater), then hearing loss will occur. Sound intensity (sound energy, or propensity to cause damage to the ears) increases dramatically with proximity according to an inverse square law: halving the distance to the sound quadruples the sound intensity.\n\nIn the USA, 12.5% of children aged 6–19 years have permanent hearing damage from excessive noise exposure. The World Health Organization estimates that half of those between 12 and 35 are at risk from using personal audio devices that are too loud.\n\nHearing loss due to noise has been described as primarily a condition of modern society. In preindustrial times, humans had far less exposure to loud sounds. Studies of primitive peoples indicate that much of what has been attributed to age-related hearing loss may be long term cumulative damage from all sources, especially noise. People living in preindustrial societies have considerably less hearing loss than similar populations living in modern society. Among primitive people who have migrated into modern society, hearing loss is proportional to the number of years spent in modern society. Military service in World War II, the Korean War, and the Vietnam War, has likely also caused hearing loss in large numbers of men from those generations, though proving that hearing loss was a direct result of military service is problematic without entry and exit audiograms.\n\nHearing loss in adolescents may be caused by loud noise from toys, music by headphones, and concerts or events. In 2017, the Centers for Disease Control and Prevention brought their researchers together with experts from the World Health Organization and academia to examine the risk of hearing loss from excessive noise exposure in and outside the workplace in different age groups, as well as actions being taken to reduce the burden of the condition. A summary report was published in 2018.\n\nHearing loss can be inherited. Around 75–80% of all these cases are inherited by recessive genes, 20–25% are inherited by dominant genes, 1–2% are inherited by X-linked patterns, and fewer than 1% are inherited by mitochondrial inheritance.\n\nWhen looking at the genetics of deafness, there are 2 different forms, syndromic and nonsyndromic. Syndromic deafness occurs when there are other signs or medical problems aside from deafness in an individual. This accounts for around 30% of deaf individuals who are deaf from a genetic standpoint. Nonsyndromic deafness occurs when there are no other signs or medical problems associated with an individual other than deafness. From a genetic standpoint, this accounts for the other 70% of cases, and represents the majority of hereditary hearing loss. Syndromic cases occur with diseases such as Usher syndrome, Stickler syndrome, Waardenburg syndrome, Alport's syndrome, and neurofibromatosis type 2. These are diseases that have deafness as one of the symptoms or as a common feature associated with it. Many of the genetic mutations giving rise to syndromic deafness have been identified. In nonsyndromic cases, where deafness is the only finding, it is more difficult to identify the genetic mutation although some have been discovered.\n\n\n\n\nSome medications may reversibly affect hearing. These medications are considered ototoxic. This includes loop diuretics such as furosemide and bumetanide, non-steroidal anti-inflammatory drugs (NSAIDs) both over-the-counter (aspirin, ibuprofen, naproxen) as well as prescription (celecoxib, diclofenac, etc.), paracetamol, quinine, and macrolide antibiotics. The link between NSAIDs and hearing loss tends to be greater in women, especially those who take ibuprofen six or more times a week. Others may cause permanent hearing loss. The most important group is the aminoglycosides (main member gentamicin) and platinum based chemotherapeutics such as cisplatin and carboplatin.\n\nOn October 18, 2007, the U.S. Food and Drug Administration (FDA) announced that a warning about possible sudden hearing loss would be added to drug labels of PDE5 inhibitors, which are used for erectile dysfunction.\n\nAudiologic monitoring for ototoxicity allows for the (1) early detection of changes to hearing status presumably attributed to a drug/treatment regime so that changes in the drug regimen may be considered, and (2) audiologic intervention when handicapping hearing impairment has occurred.\n\nCo-administration of anti-oxidants and ototoxic medications may limit the extent of the ototoxic damage\n\nIn addition to medications, hearing loss can also result from specific chemicals in the environment: metals, such as lead; solvents, such as toluene (found in crude oil, gasoline and automobile exhaust, for example); and asphyxiants. Combined with noise, these ototoxic chemicals have an additive effect on a person’s hearing loss.\n\nHearing loss due to chemicals starts in the high frequency range and is irreversible. It damages the cochlea with lesions and degrades central portions of the auditory system. For some ototoxic chemical exposures, particularly styrene, the risk of hearing loss can be higher than being exposed to noise alone. The effects is greatest when the combined exposure include impulse noise. \nA 2018 informational bulletin by the US Occupational Safety and Health Administration (OSHA) and the National Institute for Occupational Safety and Health (NIOSH) introduces the issue, provides examples of ototoxic chemicals, lists the industries and occupations at risk and provides prevention information.\n\nThere can be damage either to the ear itself or to the brain centers that process the aural information conveyed by the ears. People who sustain head injury are especially vulnerable to hearing loss or tinnitus, either temporary or permanent.\n\nSound waves reach the outer ear and are conducted down the ear canal to the eardrum, causing it to vibrate. The vibrations are transferred by the 3 tiny ear bones of the middle ear to the fluid in the inner ear. The fluid moves hair cells (stereocilia), and their movement generates nerve impulses which are then taken to the brain by the cochlear nerve. The auditory nerve takes the impulses to the brainstem, which sends the impulses to the midbrain. Finally, the signal goes to the auditory cortex of the temporal lobe to be interpreted as sound.\n\nHearing loss is most commonly caused by long-term exposure to loud noises, from recreation or from work, that damage the hair cells, which do not grow back on their own.\n\nOlder people may lose their hearing from long exposure to noise, changes in the inner ear, changes in the middle ear, or from changes along the nerves from the ear to the brain.\n\nIdentification of a hearing loss is usually conducted by a general practitioner medical doctor, otolaryngologist, certified and licensed audiologist, school or industrial audiometrist, or other audiometric technician. Diagnosis of the cause of a hearing loss is carried out by a specialist physician (audiovestibular physician) or otorhinolaryngologist.\n\nA case history (usually a written form, with questionnaire) can provide valuable information about the context of the hearing loss, and indicate what kind of diagnostic procedures to employ. Case history will include such items as:\n\n\nIn case of infection or inflammation, blood or other body fluids may be submitted for laboratory analysis.\n\nHearing loss is generally measured by playing generated or recorded sounds, and determining whether the person can hear them. Hearing sensitivity varies according to the frequency of sounds. To take this into account, hearing sensitivity can be measured for a range of frequencies and plotted on an audiogram.\n\nAnother method for quantifying hearing loss is a speech-in-noise test. As the name implies, a speech-in-noise test gives an indication of how well one can understand speech in a noisy environment. A person with a hearing loss will often be less able to understand speech, especially in noisy conditions. This is especially true for people who have a sensorineural loss – which is by far the most common type of hearing loss. As such, speech-in-noise tests can provide valuable information about a person's hearing ability, and can be used to detect the presence of a sensorineural hearing loss. A recently developed digit-triple speech-in-noise test may be a more efficient screening test.\n\nOtoacoustic emissions test is an objective hearing test that may be administered to toddlers and children too young to cooperate in a conventional hearing test. The test is also useful in older children and adults and is an important measure in diagnosing auditory neuropathy described above.\n\nAuditory brainstem response testing is an electrophysiological test used to test for hearing deficits caused by pathology within the ear, the cochlear nerve and also within the brainstem. This test can be used to identify delay in the conduction of neural impulses due to tumours or inflammation but can also be an objective test of hearing thresholds. Other electrophysiological tests, such as cortical evoked responses, can look at the hearing pathway up to the level of the auditory cortex.\n\nMRI and CT scans can be useful to identify the pathology of many causes of hearing loss. They are only needed in selected cases.\n\nHearing loss is categorized by type, severity, and configuration. Furthermore, a hearing loss may exist in only one ear (unilateral) or in both ears (bilateral). Hearing loss can be temporary or permanent, sudden or progressive.\n\nThe severity of a hearing loss is ranked according to ranges of nominal thresholds in which a sound must be so it can be detected by an individual. It is measured in decibels of hearing loss, or dB HL. The measurement of hearing loss in an individual is conducted over several frequencies, mostly 500 Hz, 1000 Hz, 2000 Hz and 4000 Hz. The hearing loss of the individual is the average of the hearing loss values over the different frequencies. Hearing loss can be ranked differently according to different organisations; and so, in different countries different systems are in use.\n\nHearing loss may be ranked as slight, mild, moderate, moderately severe, severe or profound as defined below:\n\nThe 'Audiometric Classifications of Hearing Impairment' according to the International Bureau Audiophonology (BIAP) in Belgium is as follows:\n\nHearing loss may affect one or both ears. If both ears are affected, then one ear may be more affected than the other. Thus it is possible, for example, to have normal hearing in one ear and none at all in the other, or to have mild hearing loss in one ear and moderate hearing loss in the other.\n\nFor certain legal purposes such as insurance claims, hearing loss is described in terms of percentages. Given that hearing loss can vary by frequency and that audiograms are plotted with a logarithmic scale, the idea of a percentage of hearing loss is somewhat arbitrary, but where decibels of loss are converted via a legally recognized formula, it is possible to calculate a standardized \"percentage of hearing loss\", which is suitable for legal purposes only.\n\nThere are four main types of hearing loss, conductive hearing loss, sensorineural hearing loss, central deafness and combinations of conductive and sensorineural hearing losses which is called mixed hearing loss. An additional problem which is increasingly recognised is auditory processing disorder which is not a hearing loss as such but a difficulty perceiving sound.\n\nConductive hearing loss is present when the sound is not reaching the inner ear, the cochlea. This can be due to external ear canal malformation, dysfunction of the eardrum or malfunction of the bones of the middle ear. The eardrum may show defects from small to total resulting in hearing loss of different degree. Scar tissue after ear infections may also make the eardrum dysfunction as well as when it is retracted and adherent to the medial part of the middle ear.\n\nDysfunction of the three small bones of the middle ear – malleus, incus, and stapes – may cause conductive hearing loss. The mobility of the ossicles may be impaired for different reasons including a boney disorder of the ossicles called otosclerosis and disruption of the ossicular chain due to trauma, infection or ankylosis may also cause hearing loss.\n\nSensorineural hearing loss is one caused by dysfunction of the inner ear, the cochlea or the nerve that transmits the impulses from the cochlea to the hearing centre in the brain. The most common reason for sensorineural hearing loss is damage to the hair cells in the cochlea. Depending on the definition it could be estimated that more than 50% of the population over the age of 70 has impaired hearing.\n\nDamage to the brain can lead to a central deafness. The peripheral ear and the auditory nerve may function well but the central connections are damaged by tumour, trauma or other disease and the patient is unable to process speech information.\n\nMixed hearing loss is a combination of conductive and sensorineural hearing loss. Chronic ear infection (a fairly common diagnosis) can cause a defective ear drum or middle-ear ossicle damages, or both. In addition to the conductive loss, a sensory component may be present.\n\n\nThis is not an actual hearing loss but gives rise to significant difficulties in hearing. One kind of auditory processing disorder is King-Kopetzky syndrome, which is characterized by an inability to process out background noise in noisy environments despite normal performance on traditional hearing tests. An auditory processing disorders is sometimes linked to language disorders in persons of all ages.\n\nThe shape of an audiogram shows the relative configuration of the hearing loss, such as a Carhart notch for otosclerosis, 'noise' notch for noise-induced damage, high frequency rolloff for presbycusis, or a flat audiogram for conductive hearing loss. In conjunction with speech audiometry, it may indicate central auditory processing disorder, or the presence of a schwannoma or other tumor.\nThere are four general configurations of hearing loss:\n\n1. Flat: thresholds essentially equal across test frequencies.\n\n2. Sloping: lower (better) thresholds in low-frequency regions and higher (poorer) thresholds in high-frequency regions.\n\n3. Rising: higher (poorer) thresholds in low-frequency regions and lower (better) thresholds in higher-frequency regions.\n\n4. Trough-shaped (\"cookie-bite\" or \"U\" shaped): greatest hearing loss in the mid-frequency range, with lower (better) thresholds in low- and high-frequency regions.\n\nPeople with unilateral hearing loss or single-sided deafness (SSD) have difficulty in:\n\n\nIn quiet conditions, speech discrimination is approximately the same for normal hearing and those with unilateral deafness; however, in noisy environments speech discrimination varies individually and ranges from mild to severe.\n\nOne reason for the hearing problems these patients often experience is due to the head shadow effect. Newborn children with no hearing on one side but one normal ear could still have problems. Speech development could be delayed and difficulties to concentrate in school are common. More children with unilateral hearing loss have to repeat classes than their peers. Taking part in social activities could be a problem. Early aiding is therefore of utmost importance.\n\nIt is estimated that half of cases of hearing loss are preventable. About 60% of hearing loss in children under the age of 15 can be avoided. A number of preventative strategies are effective including: immunization against rubella to prevent congenital rubella syndrome, immunization against \"H. influenza\" and \"S. pneumoniae\" to reduce cases of meningitis, and avoiding or protecting against excessive noise exposure. The World Health Organization also recommends immunization against measles, mumps, and meningitis, efforts to prevent premature birth, and avoidance of certain medication as prevention.\n\nNoise exposure is the most significant risk factor for noise-induced hearing loss that can be prevented. Different programs exist for specific populations such as school-age children, adolescents and workers. Education regarding noise exposure increases the use of hearing protectors. The use of antioxidants is being studied for the prevention of noise-induced hearing loss, particularly for scenarios in which noise exposure cannot be reduced, such as during military operations.\n\nNoise is widely recognized as an occupational hazard. In the United States, the National Institute for Occupational Safety and Health (NIOSH) and the Occupational Safety and Health Administration (OSHA) work together to provide standards and enforcement on workplace noise levels. The hierarchy of hazard controls demonstrates the different levels of controls to reduce or eliminate exposure to noise and prevent hearing loss, including engineering controls and personal protective equipment (PPE). Other programs and initiative have been created to prevent hearing loss in the workplace. For example, the Safe-in-Sound Award was created to recognize organizations that can demonstrate results of successful noise control and other interventions. Additionally, the Buy Quiet program was created to encourage employers to purchase quieter machinery and tools. By purchasing less noisy power tools like those found on the NIOSH Power Tools Database and limiting exposure to ototoxic chemicals, great strides can be made in preventing hearing loss.\n\nCompanies can also provide personal hearing protector devices tailored to both the worker and type of employment. Some hearing protectors universally block out all noise, and some allow for certain noises to be heard. Workers are more likely to wear hearing protector devices when they are properly fitted.\n\nOften interventions to prevent noise-induced hearing loss have many components. A 2017 Cochrane review found that stricter legislation might reduce noise levels. Providing workers with information on their noise exposure levels was not shown to decrease exposure to noise. Ear protection, if used correctly, can reduce noise to safer levels, but often, providing them is not sufficient to prevent hearing loss. Engineering noise out and other solutions such as proper maintenance of equipment can lead to noise reduction, but further field studies on resulting noise exposures following such interventions are needed. Other possible solutions include improved enforcement of existing legislation and better implementation of well-designed prevention programmes, which have not yet been proven conclusively to be effective. The conclusion of the Cochrane Review was that further research could modify what is now regarding the effectiveness of the evaluated interventions.\n\nThe United States Preventive Services Task Force recommends screening for all newborns.\n\nThe American Academy of Pediatrics advises that children should have their hearing tested several times throughout their schooling:\nWhile the American College of Physicians indicated that there is not enough evidence to determine the utility of screening in adults over 50 years old who do not have any symptoms, the American Language, Speech Pathology and Hearing Association recommends that adults should be screened at least every decade through age 50 and at 3-year intervals thereafter, to minimize the detrimental effects of the untreated condition on quality of life. For the same reason, the US Office of Disease Prevention and Health Promotion included as one of Healthy People 2020 objectives: to increase the proportion of persons who have had a hearing examination.\n\nTreatment depends on the specific cause if known as well as the extent, type and configuration of the hearing loss. Most hearing loss, that resulting from age and noise, is progressive and irreversible, and there are currently no approved or recommended treatments; management is by hearing aid. A few specific kinds of hearing loss are amenable to surgical treatment. In other cases, treatment is addressed to underlying pathologies, but any hearing loss incurred may be permanent.\n\nThere are a number of devices that can improve hearing in those who are deaf or hard of hearing or allow people with these conditions to manage better in their lives.\n\nHearing aids are devices that work to improve the hearing and speech comprehension of those with hearing loss. They work by magnifying the sound vibrations in the ear so that one can understand what is being said around them. Hearing aids have been shown to have a large beneficial effect in helping adults with mild to moderate hearing loss take part in everyday situations, and a smaller beneficial effect in improving physical, social, emotional and mental well-being in these people. Some people feel as if they cannot live without one because they say it is the only thing that keeps them engaged with the public. Conversely, there are many people who choose not to wear their hearing aids for a multitude of reasons. Up to 40% of adults with hearing aids for hearing loss fail to use them, or do not use them to their full effect. There are a number of reasons for this, stemming from factors such as: the aid amplifying background noises instead of the sounds they intended to hear; issues with comfort, care, or maintenance of the device; aesthetic factors; financial factors; and personal preference for quietness.\n\nThere is little evidence that interventions to encourage the regular use of hearing aids, (e.g. improving the information given to people about how to use hearing aids), increase daily hours of hearing aid use, and there is currently no agreed set of outcome measures for assessing this type of intervention.\n\nMany deaf and hard of hearing individuals use assistive devices in their daily lives:\n\nA wireless device has two main components: a transmitter and a receiver. The transmitter broadcasts the captured sound, and the receiver detects the broadcast audio and enables the incoming audio stream to be connected to accommodations such as hearing aids or captioning systems.\n\nThree types of wireless systems are commonly used: FM, audio induction loop, and InfraRed. Each system has advantages and benefits for particular uses. FM systems can be battery operated or plugged into an electrical outlet. FM system produce an analog audio signal, meaning they have extremely high fidelity. Many FM systems are very small in size, allowing them to be used in mobile situations. The audio induction loop permits the listener with hearing loss to be free of wearing a receiver provided that the listener has a hearing aid or cochlear implant processor with an accessory called a \"telecoil\". If the listener does not have a telecoil, then he or she must carry a receiver with an earpiece. As with FM systems, the infrared (IR) system also requires a receiver to be worn or carried by the listener. An advantage of IR wireless systems is that people in adjoining rooms cannot listen in on conversations, making it useful for situations where privacy and confidentiality are required. Another way to achieve confidentiality is to use a hardwired amplifier, which contains or is connected to a microphone and transmits no signal beyond the earpiece plugged directly into it.\n\nThere is no treatment, surgical or otherwise, for hearing loss due to the most common causes (age, noise, and genetic defects). For a few specific conditions, surgical intervention can provide a remedy:\n\nSurgical and implantable hearing aids are an alternative to conventional external hearing aids. \nIf the ear is dry and not infected, an air conduction aid could be tried; if the ear is draining, a direct bone condition hearing aid is often the best solution. If the conductive part of the hearing loss is more than 30–35 dB, an air conduction device could have problems overcoming this gap. A bone-anchored hearing aid could, in this situation, be a good option.\nThe active bone conduction hearing implant Bonebridge is also an option. This implant is invisible under the intact skin and therefore minimises the risk of skin irritations.\n\nCochlear implants improve outcomes in people with hearing loss in either one or both ears. They work by artificial stimulation of the cochlear nerve by providing an electric impulse substitution for the firing of hair cells. They are expensive, and require programming along with extensive training for effectiveness.\n\nCochlear implants as well as bone conduction implants can help with single sided deafness.\nMiddle ear implants or bone conduction implants can help with conductive hearing loss.\n\nPeople with cochlear implants are at a higher risk for bacterial meningitis. Thus, meningitis vaccination is recommended. People who have hearing loss, especially those who develop a hearing problem in childhood or old age, may need support and technical adaptations as part of the rehabilitation process. Recent research shows variations in efficacy but some studies show that if implanted at a very young age, some profoundly impaired children can acquire effective hearing and speech, particularly if supported by appropriate rehabilitation.\n\nFor a classroom setting, children with hearing loss often benefit from direct instruction and communication. One option for students is to attend a school for the Deaf, where they will have access to the language, communication, and education. Another option is to have the child attend a mainstream program, with special accommodation such as providing favorable seating for the child. Having the student sit as close to the teacher as possible improves the student's ability to hear the teacher's voice and to more easily read the teacher's lips. When lecturing, teachers can help the student by facing them and by limiting unnecessary noise in the classroom. In particular, the teacher can avoid talking when their back is turned to the classroom, such as while writing on a whiteboard.\n\nSome other approaches for classroom accommodations include pairing deaf or hard of hearing students with hearing students. This allows the deaf or hard of hearing student to ask the hearing student questions about concepts that they have not understood. The use of CART (Communication Access Real Time) systems, where an individual types a captioning of what the teacher is saying, is also beneficial. The student views this captioning on their computer. Automated captioning systems are also becoming a popular option. In an automated system, software, instead of a person, is used to generate the captioning. Unlike CART systems, automated systems generally do not require an Internet connection and thus they can be used anywhere and anytime. Another advantage of automated systems over CART is that they are much lower in cost. However, automated systems are generally designed to only transcribe what the teacher is saying and to not transcribe what other students say. An automated system works best for situations where just the teacher is speaking, whereas a CART system will be preferred for situations where there is a lot of classroom discussion.\n\nFor those students who are completely deaf, one of the most common interventions is having the child communicate with others through an interpreter using sign language.\n\nGlobally, hearing loss affects about 10% of the population to some degree. It caused moderate to severe disability in 124.2 million people as of 2004 (107.9 million of whom are in low and middle income countries). Of these 65 million acquired the condition during childhood. At birth ~3 per 1000 in developed countries and more than 6 per 1000 in developing countries have hearing problems.\n\nHearing loss increases with age. In those between 20 and 35 rates of hearing loss are 3% while in those 44 to 55 it is 11% and in those 65 to 85 it is 43%.\n\nA 2017 report by the World Health Organization estimated the costs of unaddressed hearing loss and the cost-effectiveness of interventions, for the health-care sector, for the education sector and as broad societal costs. Globally, the annual cost of unaddressed hearing loss was estimated to be in the range of $750–790 billion international dollars.\n\nData from the United States in 2011-2012 found that rates of hearing loss has declined among adults aged 20 to 69 years, when compared with the results from an earlier time period (1999-2004). It also found that adult hearing loss is associated with increasing age, sex, race/ethnicity, educational level, and noise exposure.\n\nNearly one in four adults had audiometric results suggesting noise-induced hearing loss. Almost one in four adults who reported excellent or good hearing had a similar pattern (5.5% on both sides and 18% on one side). Among people who reported exposure to loud noise at work, almost one third had such changes.\n\nAbbé Charles-Michel de l'Épée opened the first school for the deaf in Paris at the deaf school. The American Thomas Gallaudet witnessed a demonstration of deaf teaching skills from Épée's successor Abbé Sicard and two of the school's deaf faculty members, Laurent Clerc and Jean Massieu; accompanied by Clerc, he returned to the United States, where in 1817 they founded American School for the Deaf in Hartford, Connecticut. American Sign Language (ASL) started to evolve from primarily French Sign Language (LSF), and other outside influences.\n\n\"Post-lingual deafness\" is hearing loss that is sustained after the acquisition of language, which can occur due to disease, trauma, or as a side-effect of a medicine. Typically, hearing loss is gradual and often detected by family and friends of affected individuals long before the patients themselves will acknowledge the disability. Post-lingual deafness is far more common than pre-lingual deafness. Those who lose their hearing later in life, such as in late adolescence or adulthood, face their own challenges, living with the adaptations that allow them to live independently.\n\n\"Prelingual deafness\" is hearing loss that is sustained before the acquisition of language, which can occur due to a congenital condition or through hearing loss in early infancy. It is believed that prelingual deafness impairs an individual's ability to acquire a \"spoken\" language, but some deaf children can acquire spoken language through speech immersion along with support from sign language and hearing aids or cochlear implants. Non-signing parents of deaf babies usually go with oral approach without the support of sign language because prelingual hearing loss is acquired via either disease or trauma rather than genetically inherited, so families with deaf children nearly always lack previous experience with sign language. Unfortunately, this brings on the risk of language deprivation for the deaf baby because the deaf baby wouldn't have a language if the child is unable to acquire spoken language successfully. Deaf babies born into signing families rarely have delays in language development since they do meet language milestones, but in sign language in lieu of spoken language.\n\nThere has been considerable controversy within the culturally deaf community over cochlear implants. For the most part, there is little objection to those who lost their hearing later in life, or culturally deaf adults choosing to be fitted with a cochlear implant.\n\nMany in the deaf community strongly object to a deaf child being fitted with a cochlear implant (often on the advice of an audiologist); new parents may not have sufficient information on raising deaf children and placed in an oral-only program that emphasizes the ability to speak and listen over other forms of communication such as sign language or total communication. Many Deaf people view cochlear implants and other hearing devices as confusing to one's identity. A Deaf person will never be a hearing person and therefore would be trying to fit into a way of living that is not their own. Other concerns include loss of Deaf culture and identity and limitations on hearing restoration.\n\nJack Gannon, a professor at Gallaudet University, said this about Deaf culture: \"Deaf culture is a set of learned behaviors and perceptions that shape the values and norms of deaf people based on their shared or common experiences.\" Some doctors believe that being deaf makes a person more social. Bill Vicar, from ASL University, shared his experiences as a deaf person, \"[deaf people] tend to congregate around the kitchen table rather than the living room sofa… our good-byes take nearly forever, and our hellos often consist of serious hugs. When two of us meet for the first time we tend to exchange detailed biographies.\" Deaf culture is not about contemplating what deaf people cannot do and how to fix their problems, an approach known as the \"pathological view of the deaf.\" Instead deaf people celebrate what they can do. There is a strong sense of unity between deaf people as they share their experiences of suffering through a similar struggle. This celebration creates a unity between even deaf strangers. Bill Vicars expresses the power of this bond when stating, \"if given the chance to become hearing most [deaf people] would choose to remain deaf.\"\n\nThe United States-based National Association of the Deaf has a statement on its website regarding cochlear implants. The NAD asserts that the choice to implant is up to the individual (or the parents), yet strongly advocates a fully informed decision in all aspects of a cochlear implant. Much of the negative reaction to cochlear implants stems from the medical viewpoint that deafness is a condition that needs to be \"cured,\" while the Deaf community instead regards deafness a defining cultural characteristic.\n\nMany other assistive devices are more acceptable to the Deaf community, including but not limited to, hearing aids, closed captioning, email and the Internet, text telephones, and video relay services.\n\nSign languages convey meaning through manual communication and body language instead of acoustically conveyed sound patterns. This involves the simultaneous combination of hand shapes, orientation and movement of the hands, arms or body, and facial expressions to express a speaker's thoughts. \"Sign languages are based on the idea that vision is the most useful tool a deaf person has to communicate and receive information\".\n\nThose who are deaf (by either state or federal standards) have access to a free and appropriate public education. If a child does qualify as being deaf or hard of hearing and receives an individualized education plan, the IEP team must consider, \"the child's language and communication needs. The IEP must include opportunities for direct communication with peers and professionals. It must also include the student’s academic level, and finally must include the students full range of needs\"\n\nIn part, the Department of Education defines deafness as \"… a hearing impairment that is so severe that the child is impaired in processing linguistic information through hearing, with or without amplification ….\" Hearing impairment is defined as \"… an impairment in hearing, whether permanent or fluctuating, that adversely affects a child's educational performance but that is not included under the definition of deafness ….\"\n\nIn a residential school where all the children use the same communication system (whether it is a school using ASL, Total Communication or Oralism), students will be able to interact normally with other students, without having to worry about being criticized. An argument supporting inclusion, on the other hand, exposes the student to people who are not just like them, preparing them for adult life. Through interacting, children with hearing disabilities can expose themselves to other cultures which in the future may be beneficial for them when it comes to finding jobs and living on their own in a society where their disability may put them in the minority. These are some reasons why a person may or may not want to put their child in an inclusion classroom.\n\nThe communication limitations between people who are deaf and their hearing family members can often cause difficulties in family relationships, and affect the strength of relationships among individual family members. It was found that most people who are deaf have hearing parents, which means that the channel that the child and parents communicate through can be very different, often affecting their relationship in a negative way. If a parent communicates best verbally, and their child communicates best using sign language, this could result in ineffective communication between parents and children. Ineffective communication can potentially lead to fights caused by misunderstanding, less willingness to talk about life events and issues, and an overall weaker relationship. Even if individuals in the family made an effort to learn deaf communication techniques such as sign language, a deaf family member often will feel excluded from casual banter; such as the exchange of daily events and news at the dinner table. It is often difficult for people who are deaf to follow these conversations due to the fast-paced and overlapping nature of these exchanges. This can cause a deaf individual to become frustrated and take part in less family conversations. This can potentially result in weaker relationships between the hearing individual and their immediate family members. This communication barrier can have a particularly negative effect on relationships with extended family members as well. Communication between a deaf individual and their extended family members can be very difficult due to the gap in verbal and non-verbal communication. This can cause the individuals to feel frustrated and unwilling to put effort into communicating effectively. The lack of effort put into communicating can result in anger, miscommunication, and unwillingness to build a strong relationship.\n\nPeople who have hearing loss can often experience many difficulties as a result of communication barriers among them and other hearing individuals in the community. Some major areas that can be impacted by this are involvement in extracurricular activities and social relationships. For young people, extracurricular activities are vehicles for physical, emotional, social, and intellectual development. However, it is often the case that communication barriers between people who are deaf and their hearing peers and coaches/club advisors limit them from getting involved. These communication barriers make it difficult for someone with a hearing loss to understand directions, take advice, collaborate, and form bonding relationships with other team or club members. As a result, extracurricular activities such as sports teams, clubs, and volunteering are often not as enjoyable and beneficial for individuals who have hearing loss, and they may engage in them less often. A lack of community involvement through extracurricular activities may also limit the individual’s social network. In general, it can be difficult for someone who is deaf to develop and maintain friendships with their hearing peers due to the communication gap that they experience. They can often miss the jokes, informal banter, and \"messing around\" that is associated with the formation of many friendships among young people. Conversations between people who are deaf and their hearing peers can often be limited and short due to their differences in communication methods and lack of knowledge on how to overcome these differences. Deaf individuals can often experience rejection by hearing peers who are not willing to make an effort to find their way around communication difficulties. Patience and motivation to overcome such communication barriers is required by both the deaf or hard of hearing and hearing individuals in order to establish and maintain good friendships.\n\nMany people tend to forget about the difficulties that deaf children encounter, as they view the deaf child differently from a deaf adult. Deaf children grow up being unable to fully communicate with their parents, siblings and other family members. Examples include being unable to tell their family what they have learned, what they did, asking for help, or even simply being unable to interact in daily conversation. Deaf children have to learn sign language and to read lips at a young age, however they cannot communicate with others using it unless the others are educated in sign language as well. Children who are deaf or hard of hearing are faced with many complications while growing up, for example some children have to wear hearing aids and others require assistance from sign language (ASL) interpreters. The interpreters help them to communicate with other individuals until they develop the skills they need to efficiently communicate on their own. Although growing up for deaf children may entitle more difficulties than for other children, there are many support groups that allow deaf children to interact with other children. This is where they develop friendships. There are also classes for young children to learn sign language in an environment that has other children in their same situation and around their same age. These groups and classes can be very beneficial in providing the child with the proper knowledge and not to mention the societal interactions that they need in order to live a healthy, young, playful and carefree life that any child deserves.\n\nThere are three typical adjustment patterns adopted by adults with hearing loss. The first one is to remain withdrawn into your own self. This provides a sense of safety and familiarity which can be a comforting way to lead your life. The second is to act \"as if\" one does not even have hearing loss. A positive attitude will help people to live a life with no barriers and thus, engage in optimal interaction. The final and third pattern is for the person to accept their hearing loss as a part of them without undervaluing oneself. This means understanding that one is forced to live life with this disability, however it is not the only thing that constitutes life’s meaning. Furthermore, many feel as if their inability to hear others during conversation is their fault. It's important that these individuals learn how to become more assertive individuals who do not lack fear when it comes to asking someone to repeat something or to speak a little louder. Although there is much fatigue and frustration that is produced from one’s inability to hear, it is important to learn from personal experiences in order to improve on one’s communication skills. In essence, these patterns will help adults with hearing loss deal with the communication barriers that are present.\n\nIn most instances, people who are deaf find themselves working with hearing colleagues, where they can often be cut off from the communication going on around them. Interpreters can be provided for meetings and workshops, however are seldom provided for everyday work interactions. Communication of important information needed for jobs typically comes in the form of written or verbal summaries, which do not convey subtle meanings such as tone of voice, side conversations during group discussions, and body language. This can result in confusion and misunderstanding for the worker who is deaf, therefore making it harder to do their job effectively. Additionally, deaf workers can be unintentionally left out of professional networks, informal gatherings, and casual conversations among their collogues. Information about informal rules and organizational culture in the workplace is often communicated though these types of interactions, which puts the worker who is deaf at a professional and personal disadvantage. This could sever their job performance due to lack of access to information and therefore, reduce their opportunity to form relationships with their co-workers. Additionally, these communication barriers can all affect a deaf person’s career development. Since being able to effectively communicate with one's co-workers and other people relevant to one's job is essential to managerial positions, people with hearing loss can often be denied such opportunities.\n\nTo avoid these situations in the workplace, individuals can take full-time or part-time sign language courses. In this way, they can become better able to communicate with the deaf and hard of hearing. Such courses teach the American Sign Language (ASL) language as most North Americans use this particular language to communicate. It is a visual language made up of specific gestures (signs), hand shapes, and facial expressions that contain their own unique grammatical rules and sentence structures By completing sign language courses, it ensures that deaf individuals feel a part of the workplace and have the ability to communicate with their co-workers and employer in the manner as other hearing employees do.\n\nNot only can communication barriers between deaf and hearing people affect family relationships, work, and school, but they can also have a very significant effect on a deaf individual’s physical and mental health care. As a result of poor communication between the health care professional and the deaf or hard of hearing patient, many patients report that they are not properly informed about their disease and prognosis. \n\nThis lack of or poor communication could also lead to other issues such as misdiagnosis, poor assessments, mistreatment, and even possibly harm to patients. Poor communication in this setting is often the result of health care providers having the misconception that all people who are deaf or hard of hearing have the same type of hearing loss, and require the same type of communication methods. In reality, there are many different types and range of hearing loss, and in order to communicate effectively a health care provider needs to understand that each individual with hearing loss has unique needs. This affects how individuals have been educated to communicate, as some communication methods work better depending on an individual’s severity of hearing loss. For example, assuming every deaf or hard of hearing patient knows American Sign Language would be incorrect because there are different types of sign language, each varying in signs and meanings. A patient could have been educated to use cued speech which is entirely different from ASL. Therefore, in order to communicate effectively, a health care provider needs to understand that each individual has unique needs when communicating.\n\nAlthough there are specific laws and rules to govern communication between health care professionals and people who are deaf, they are not always followed due to the health care professional’s insufficient knowledge of communication techniques. This lack of knowledge can lead them to make assumptions about communicating with someone who is deaf, which can in turn cause them to use an unsuitable form of communication. \nActs in countries such as the Americans with Disabilities Act (ADA) state that all health care providers are required to provide reasonable communication accommodations when caring for patients who are deaf. These accommodations could include qualified sign language interpreters, CDIs, and technology such as Internet interpretation services. A qualified sign language interpreter will enhance communication between a deaf individual and a health care professional by interpreting not only a health professional’s verbal communication, but also their non-verbal such as expressions, perceptions, and body language. A Certified Deaf Interpreter (CDI) is a sign language interpreter who is also a member of the Deaf community. They accompany a sign language interpreter and are useful for communication with deaf individuals who also have language or cognitive deficits. A CDI will transform what the health care professional communicates into basic, simple language. This method takes much longer, however it can also be more effective than other techniques. Internet interpretation services are convenient and less costly, but can potentially pose significant risks. They involve the use of a sign language interpreter over a video device rather than directly in the room. This can often be an inaccurate form of communication because the interpreter may not be licensed, is often unfamiliar with the patient and their signs, and can lack knowledge of medical terminology.\n\nAside from utilizing interpreters, healthcare professionals can improve their communication with deaf or hard of hearing patients by educating themselves on common misconceptions and proper practices depending on the patient’s needs. For example, a common misconception is that exaggerating words and speaking loudly will help the patient understand more clearly. However, many individuals with hearing loss depend on lip-reading to identify words. Exaggerated pronunciation and a raised voice can distort the lips, making it even more difficult to understand. Another common mistake health care professionals make are the use of single words rather than full sentences. Although language should be kept simple and short, keeping context is important because certain homophonous words are difficult to distinguish by lip-reading. Health care professionals can further improve their own communication with their patients by eliminating any background noise and positioning themselves in a way where their face is clearly visible to the patient, and suitably lit. The healthcare professional should know how to use body language and facial expressions to properly communicate different feelings.\n\nA 2005 study achieved successful regrowth of cochlea cells in guinea pigs. However, the regrowth of cochlear hair cells does not imply the restoration of hearing sensitivity, as the sensory cells may or may not make connections with neurons that carry the signals from hair cells to the brain. A 2008 study has shown that gene therapy targeting Atoh1 can cause hair cell growth and attract neuronal processes in embryonic mice. Some hope that a similar treatment will one day ameliorate hearing loss in humans.\n\nRecent research, reported in 2012 achieved growth of cochlear nerve cells resulting in hearing improvements in gerbils, using stem cells. Also reported in 2013 was regrowth of hair cells in deaf adult mice using a drug intervention resulting in hearing improvement. The Hearing Health Foundation in the US has embarked on a project called the Hearing Restoration Project. Also Action on Hearing Loss in the UK is also aiming to restore hearing.\n\nResearchers reported in 2015 that genetically deaf mice which were treated with TMC1 gene therapy recovered some of their hearing. In 2017, additional studies were performed to treat Usher syndrome and here, a recombinant adeno-associated virus seemed to outperform the older vectors.\n\nBesides research studies seeking to improve hearing, such as the ones listed above, research studies on the deaf have also been carried out in order to understand more about audition. Pijil and Shwarz (2005) conducted their study on the deaf who lost their hearing later in life and, hence, used cochlear implants to hear. They discovered further evidence for rate coding of pitch, a system that codes for information for frequencies by the rate that neurons fire in the auditory system, especially for lower frequencies as they are coded by the frequencies that neurons fire from the basilar membrane in a synchronous manner. Their results showed that the subjects could identify different pitches that were proportional to the frequency stimulated by a single electrode. The lower frequencies were detected when the basilar membrane was stimulated, providing even further evidence for rate coding.\n\n"}
{"id": "93827", "url": "https://en.wikipedia.org/wiki?curid=93827", "title": "Human nutrition", "text": "Human nutrition\n\nHuman nutrition deals with the provision of essential nutrients in food that are necessary to support human life and health. Poor nutrition is a chronic problem often linked to poverty, food security or a poor understanding of nutrition and dietary practices. Malnutrition and its consequences are large contributors to deaths and disabilities worldwide. Good nutrition helps children grow physically, promotes human biological development and helps in the eradication of poverty.\nThe human body contains chemical compounds, such as water, carbohydrates, amino acids (in proteins), fatty acids (in lipids), and nucleic acids (DNA and RNA). These compounds are composed of elements such as carbon, hydrogen, oxygen, nitrogen, phosphorus. Any study done to determine nutritional status must take into account the state of the body before and after experiments, as well as the chemical composition of the whole diet and of all the materials excreted and eliminated from the body (including urine and feces). Comparing food to waste material can help determine the specific compounds and elements absorbed and metabolized by the body. The effects of nutrients may only be discernible over an extended period of time, during which all food and waste must be analyzed. The number of variables involved in such experiments is high, making nutritional studies time-consuming and expensive, which explains why the science of human nutrition is still slowly evolving.\n\nThe seven major classes of nutrients are carbohydrates, fats, fiber, minerals, proteins, vitamins, and water. These nutrient classes are categorized as either macronutrients or micronutrients (needed in small quantities). The macronutrients are carbohydrates, fats, fiber, proteins, and water. The micronutrients are minerals and vitamins.\n\nThe macronutrients (excluding fiber and water) provide structural material (amino acids from which proteins are built, and lipids from which cell membranes and some signaling molecules are built), and energy. Some of the structural material can also be used to generate energy internally, and in either case it is measured in Joules or kilocalories (often called \"Calories\" and written with a capital 'C' to distinguish them from little 'c' calories). Carbohydrates and proteins provide 17 kJ approximately (4 kcal) of energy per gram, while fats provide 37 kJ (9 kcal) per gram, though the net energy from either depends on such factors as absorption and digestive effort, which vary substantially from instance to instance.\n\nVitamins, minerals, fiber, and water do not provide energy, but are required for other reasons. A third class of dietary material, fiber (i.e., nondigestible material such as cellulose), seems also to be required, for both mechanical and biochemical reasons, though the exact reasons remain unclear. For all age groups, males need to consume higher amounts of macronutrients than females. In general, intakes increase with age until the second or third decade of life.\n\nMolecules of carbohydrates and fats consist of carbon, hydrogen, and oxygen atoms. Carbohydrates range from simple monosaccharides (glucose, fructose, galactose) to complex polysaccharides (starch). Fats are triglycerides, made of assorted fatty acid monomers bound to a glycerol backbone. Some fatty acids, but not all, are essential in the diet: they cannot be synthesized in the body. Protein molecules contain nitrogen atoms in addition to carbon, oxygen, and hydrogen. The fundamental components of protein are nitrogen-containing amino acids, some of which are essential in the sense that humans cannot make them internally. Some of the amino acids are convertible (with the expenditure of energy) to glucose and can be used for energy production just as ordinary glucose. By breaking down existing protein, some glucose can be produced internally; the remaining amino acids are discarded, primarily as urea in urine. This occurs naturally when atrophy takes place, or during periods of starvation.\n\nCarbohydrates may be classified as monosaccharides, disaccharides or polysaccharides depending on the number of monomer (sugar) units they contain. They are a diverse group of substances, with a range of chemical, physical and physiological properties. They make up a large part of foods such as rice, noodles, bread, and other grain-based products, but they are not an essential nutrient, meaning a human does not need to eat carbohydrates.\n\nMonosaccharides contain one sugar unit, disaccharides two, and polysaccharides three or more. Monosaccharides include glucose, fructose and galactose. Disaccharides include sucrose, lactose, and maltose; purified sucrose, for instance, is used as table sugar. Polysaccharides, which include starch and glycogen, are often referred to as 'complex' carbohydrates because they are typically long multiple-branched chains of sugar units. The difference is that complex carbohydrates take longer to digest and absorb since their sugar units must be separated from the chain before absorption. The spike in blood glucose levels after ingestion of simple sugars is thought to be related to some of the heart and vascular diseases, which have become more common in recent times. Simple sugars form a greater part of modern diets than in the past, perhaps leading to more cardiovascular disease. The degree of causation is still not clear.\n\nSimple carbohydrates are absorbed quickly, and therefore raise blood-sugar levels more rapidly than other nutrients. However, the most important plant carbohydrate nutrient, starch, varies in its absorption. Gelatinized starch (starch heated for a few minutes in the presence of water) is far more digestible than plain starch, and starch which has been divided into fine particles is also more absorbable during digestion. The increased effort and decreased availability reduces the available energy from starchy foods substantially and can be seen experimentally in rats and anecdotally in humans. Additionally, up to a third of dietary starch may be unavailable due to mechanical or chemical difficulty.\n\nA molecule of dietary fat typically consists of several fatty acids (containing long chains of carbon and hydrogen atoms), bonded to a glycerol. They are typically found as triglycerides (three fatty acids attached to one glycerol backbone). Fats may be classified as saturated or unsaturated depending on the chemical structure of the fatty acids involved. Saturated fats have all of the carbon atoms in their fatty acid chains bonded to hydrogen atoms, whereas unsaturated fats have some of these carbon atoms double-bonded, so their molecules have relatively fewer hydrogen atoms than a saturated fatty acid of the same length. Unsaturated fats may be further classified as monounsaturated (one double-bond) or polyunsaturated (many double-bonds). Furthermore, depending on the location of the double-bond in the fatty acid chain, unsaturated fatty acids are classified as omega-3 or omega-6 fatty acids. Trans fats are a type of unsaturated fat with \"trans\"-isomer bonds; these are rare in nature and in foods from natural sources; they are typically created in an industrial process called (partial) hydrogenation.\n\nMany studies have shown that consumption of unsaturated fats, particularly monounsaturated fats, is associated with better health in humans. Saturated fats, typically from animal sources, are next in order of preference, while trans fats are associated with a variety of disease and should be avoided. Saturated and some trans fats are typically solid at room temperature (such as butter or lard), while unsaturated fats are typically liquids (such as olive oil or flaxseed oil). Trans fats are very rare in nature, but have properties useful in the food processing industry, such as rancidity resistance.\n\nMost fatty acids are not essential, meaning the body can produce them as needed, generally from other fatty acids and always by expending energy to do so. However, in humans, at least two fatty acids are essential and must be included in the diet. An appropriate balance of essential fatty acids – omega-3 and omega-6 fatty acids – seems also important for health, though definitive experimental demonstration has been elusive. Both of these \"omega\" long-chain polyunsaturated fatty acids are substrates for a class of eicosanoids known as prostaglandins, which have roles throughout the human body. They are hormones, in some respects. The omega-3 eicosapentaenoic acid (EPA), which can be made in the human body from the omega-3 essential fatty acid alpha-linolenic acid (LNA), or taken in through marine food sources, serves as a building block for series 3 prostaglandins (e.g. weakly inflammatory PGE3). The omega-6 dihomo-gamma-linolenic acid (DGLA) serves as a building block for series 1 prostaglandins (e.g. anti-inflammatory PGE1), whereas arachidonic acid (AA) serves as a building block for series 2 prostaglandins (e.g., pro-inflammatory PGE 2). Both DGLA and AA can be made from the omega-6 linoleic acid (LA) in the human body, or can be taken in directly through food. An appropriately balanced intake of omega-3 and omega-6 partly determines the relative production of different prostaglandins: one reason a balance between omega-3 and omega-6 is believed important for cardiovascular health. In industrialized societies, people typically consume large amounts of processed vegetable oils, which have reduced amounts of the essential fatty acids along with too much of omega-6 fatty acids relative to omega-3 fatty acids.\n\nDietary fiber is a carbohydrate, specifically a polysaccharide, which is incompletely absorbed in humans and in some animals. Like all carbohydrates, when it is metabolized, it can produce four Calories (kilocalories) of energy per gram, but in most circumstances, it accounts for less than that because of its limited absorption and digestibility. The two subcategories are insoluble and soluble fiber. Insoluble dietary fiber consists mainly of cellulose, a large carbohydrate polymer that is indigestible by humans, because humans do not have the required enzymes to break it down, and the human digestive system does not harbor enough of the types of microbes that can do so. Soluble dietary fiber comprises a variety of oligosaccharides, waxes, esters, resistant starches, and other carbohydrates that dissolve or gelatinize in water. Many of these soluble fibers can be fermented or partially fermented by microbes in the human digestive system to produce short-chain fatty acids which are absorbed and therefore introduce some caloric content.\n\nWhole grains, beans and other legumes, fruits (especially plums, prunes, and figs), and vegetables are good sources of dietary fiber. Fiber is important to digestive health and is thought to reduce the risk of colon cancer. For mechanical reasons, fiber can help in alleviating both constipation and diarrhea. Fiber provides bulk to the intestinal contents, and insoluble fiber especially stimulates peristalsis – the rhythmic muscular contractions of the intestines which move digesta along the digestive tract. Some soluble fibers produce a solution of high viscosity; this is essentially a gel, which slows the movement of food through the intestines. Additionally, fiber, perhaps especially that from whole grains, may help lessen insulin spikes and reduce the risk of type 2 diabetes.\n\nProteins are the basis of many animal body structures (e.g. muscles, skin, and hair) and form the enzymes which catalyse chemical reactions throughout the body. Each protein molecule is composed of amino acids which contain nitrogen and sometimes sulphur (these components are responsible for the distinctive smell of burning protein, such as the keratin in hair). The body requires amino acids to produce new proteins (protein retention) and to replace damaged proteins (maintenance). Amino acids are soluble in the digestive juices within the small intestine, where they are absorbed into the blood. Once absorbed, they cannot be stored in the body, so they are either metabolized as required or excreted in the urine.\n\nProteins consist of amino acids in different proportions. The most important aspect and defining characteristic of protein from a nutritional standpoint is its amino acid composition. Amino acids which an animal cannot synthesize on its own from smaller molecules are deemed essential. The synthesis of some amino acids can be limited under special pathophysiological conditions, such as prematurity in the infant or individuals in severe catabolic distress, and those are called conditionally essential.\n\nA vegetarian diet can adequately supply protein, support pregnancy, childhood and athletic endeavors, and lower the risk of cardiovascular disease and cancer.\n\nDietary minerals are the chemical elements required by living organisms, other than the four elements carbon, hydrogen, nitrogen, and oxygen that are present in nearly all organic molecules. The term \"mineral\" is archaic, since the intent is to describe simply the less common elements in the diet. Some are heavier than the four just mentioned – including several metals, which often occur as ions in the body. Some dietitians recommend that these be supplied from foods in which they occur naturally, or at least as complex compounds, or sometimes even from natural inorganic sources (such as calcium carbonate from ground oyster shells). Some are absorbed much more readily in the ionic forms found in such sources. On the other hand, minerals are often artificially added to the diet as supplements; the most well-known is likely iodine in iodized salt which prevents goiter.\n\ninclude the following:\n\nMany elements are required in smaller amounts (microgram quantities), usually because they play a catalytic role in enzymes. Some trace mineral elements (RDA < 200 mg/day) are, in alphabetical order:\n\nAs with the minerals discussed above, some vitamins are recognized as essential nutrients, necessary in the diet for good health. (Vitamin D is the exception: it can alternatively be synthesized in the skin, in the presence of UVB radiation.) Certain vitamin-like compounds that are recommended in the diet, such as carnitine, are thought useful for survival and health, but these are not \"essential\" dietary nutrients because the human body has some capacity to produce them from other compounds. Moreover, thousands of different phytochemicals have recently been discovered in food (particularly in fresh vegetables), which may have desirable properties including antioxidant activity (see below); experimental demonstration has been suggestive but inconclusive. Other essential nutrients not classed as vitamins include essential amino acids (see above), essential fatty acids (see above), and the minerals discussed in the preceding section.\n\nVitamin deficiencies may result in disease conditions: goiter, scurvy, osteoporosis, impaired immune system, disorders of cell metabolism, certain forms of cancer, symptoms of premature aging, and poor psychological health (including eating disorders), among many others.\n\nMalnutrition refers to insufficient, excessive, or imbalanced consumption of nutrients. In developed countries, the diseases of malnutrition are most often associated with nutritional imbalances or excessive consumption.\nAlthough there are more people in the world who are malnourished due to excessive consumption, according to the United Nations World Health Organization, the greatest challenge in developing nations today is not starvation, but insufficient nutrition – the lack of nutrients necessary for the growth and maintenance of vital functions. The causes of malnutrition are directly linked to inadequate macronutrient consumption and disease, and are indirectly linked to factors like “household food security, maternal and child care, health services, and the environment.” \n\nResearch indicates that improving the awareness of nutritious meal choices and establishing long-term habits of healthy eating has a positive effect on a cognitive and spatial memory capacity, potentially increasing a student's potential to process and retain academic information. \n\nSome organizations have begun working with teachers, policymakers, and managed foodservice contractors to mandate improved nutritional content and increased nutritional resources in school cafeterias from primary to university level institutions. Health and nutrition have been proven to have close links with overall educational success. Currently less than 10% of American college students report that they eat the recommended five servings of fruit and vegetables daily. Better nutrition has been shown to affect both cognitive and spatial memory performance; a study showed those with higher blood sugar levels performed better on certain memory tests. In another study, those who consumed yogurt performed better on thinking tasks when compared to those who consumed caffeine free diet soda or confections. Nutritional deficiencies have been shown to have a negative effect on learning behavior in mice as far back as 1951.\"Better learning performance is associated with diet induced effects on learning and memory ability\".\n\n\nNutritional supplement treatment may be appropriate for major depression, bipolar disorder, schizophrenia, and obsessive compulsive disorder, the four most common mental disorders in developed countries. It is because Lakhan and Vieira mentioned that the supplements possess amino acids that may change into neurotransmitters and improve mental disorders. Supplements that have been studied most for mood elevation and stabilization include eicosapentaenoic acid and docosahexaenoic acid (each of which are an omega-3 fatty acid contained in fish oil, but not in flaxseed oil), vitamin B, folic acid, and inositol.\n\nCancer has become common in developing countries. According to a study by the International Agency for Research on Cancer, \"In the developing world, cancers of the liver, stomach and esophagus were more common, often linked to consumption of carcinogenic preserved foods, such as smoked or salted food, and parasitic infections that attack organs.\" Lung cancer rates are rising rapidly in poorer nations because of increased use of tobacco. Developed countries \"tended to have cancers linked to affluence or a 'Western lifestyle' – cancers of the colon, rectum, breast and prostate – that can be caused by obesity, lack of exercise, diet and age.\"\n\nA comprehensive worldwide report, \"\", compiled by the World Cancer Research Fund and the American Institute for Cancer Research, reports that there is a significant relation between lifestyle (including food consumption) and cancer prevention. The same report recommends eating mostly foods of plant origin and aiming to meet nutritional needs through diet alone, while limiting consumption of energy-dense foods, red meat, alcoholic drinks and salt and avoiding sugary drinks, processed meat and moldy cereals (grains) or pulses (legumes). Protein consumption leads to an increase in IGF-1, which plays a role in cancer development.\n\nSeveral lines of evidence indicate lifestyle-induced hyperinsulinemia and reduced insulin function (i.e. insulin resistance) as decisive factors in many disease states. For example, hyperinsulinemia and insulin resistance are strongly linked to chronic inflammation, which in turn is strongly linked to a variety of adverse developments such as arterial microinjuries and clot formation (i.e. heart disease) and exaggerated cell division (i.e. cancer). Hyperinsulinemia and insulin resistance (the so-called metabolic syndrome) are characterized by a combination of abdominal obesity, elevated blood sugar, elevated blood pressure, elevated blood triglycerides, and reduced HDL cholesterol.\n\nObesity can unfavourably alter hormonal and metabolic status via resistance to the hormone leptin, and a vicious cycle may occur in which insulin/leptin resistance and obesity aggravate one another. The vicious cycle is putatively fuelled by continuously high insulin/leptin stimulation and fat storage, as a result of high intake of strongly insulin/leptin stimulating foods and energy. Both insulin and leptin normally function as satiety signals to the hypothalamus in the brain; however, insulin/leptin resistance may reduce this signal and therefore allow continued overfeeding despite large body fat stores.\n\nThere is a debate about how and to what extent different dietary factors – such as intake of processed carbohydrates, total protein, fat, and carbohydrate intake, intake of saturated and trans fatty acids, and low intake of vitamins/minerals – contribute to the development of insulin and leptin resistance. Evidence indicates that diets possibly protective against metabolic syndrome include low saturated and trans fat intake and foods rich in dietary fiber, such as high consumption of fruits and vegetables and moderate intake of low-fat dairy products.\n\nThe challenges facing global nutrition are disease, child malnutrition, obesity, and vitamin deficiency.\n\nThe most common non-infectious diseases worldwide, that contribute most to the global mortality rate, are cardiovascular diseases, various cancers, diabetes, and chronic respiratory problems, all of which are linked to poor nutrition. Nutrition and diet are closely associated with the leading causes of death, including cardiovascular disease and cancer. Obesity and high sodium intake can contribute to ischemic heart disease, while consumption of fruits and vegetables can decrease the risk of developing cancer.\n\nFoodborne and infectious diseases can result in malnutrition, and malnutrition exacerbates infectious disease. Poor nutrition leaves children and adults more susceptible to contracting life-threatening diseases such as diarrheal infections and respiratory infections. According to the WHO, in 2011, 6.9 million children died of infectious diseases like pneumonia, diarrhea, malaria, and neonatal conditions, of which at least one third were associated with undernutrition.\n\nAccording to UNICEF, in 2011, 101 million children across the globe were underweight and one in four children, 165 million, were stunted in growth. Simultaneously, there are 43 million children under five who are overweight or obese. Nearly 20 million children under 5 suffer from severe acute malnutrition, a life-threatening condition requiring urgent treatment. According to estimations at UNICEF, hunger will be responsible for 5.6 million deaths of children under the age of five this year. These all represent significant public health emergencies. This is because proper maternal and child nutrition has immense consequences for survival, acute and chronic disease incidence, normal growth, and economic productivity of individuals.\n\nChildhood malnutrition is common and contributes to the global burden of disease. Childhood is a particularly important time to achieve good nutrition status, because poor nutrition has the capability to lock a child in a vicious cycle of disease susceptibility and recurring sickness, which threatens cognitive and social development. Undernutrition and bias in access to food and health services leaves children less likely to attend or perform well in school.\n\nUNICEF defines undernutrition “as the outcome of insufficient food intake (hunger) and repeated infectious diseases. Under nutrition includes being underweight for one’s age, too short for one’s age (stunted), dangerously thin (wasted), and deficient in vitamins and minerals (micronutrient malnutrient). Under nutrition causes 53% of deaths of children under five across the world. It has been estimated that undernutrition is the underlying cause for 35% of child deaths. The Maternal and Child Nutrition Study Group estimate that under nutrition, “including fetal growth restriction, stunting, wasting, deficiencies of vitamin A and zinc along with suboptimum breastfeeding- is a cause of 3.1 million child deaths and infant mortality, or 45% of all child deaths in 2011”.\n\nWhen humans are undernourished, they no longer maintain normal bodily functions, such as growth, resistance to infection, or have satisfactory performance in school or work. Major causes of under nutrition in young children include lack of proper breast feeding for infants and illnesses such as diarrhea, pneumonia, malaria, and HIV/AIDS. According to UNICEF 146 million children across the globe, that one out of four under the age of five, are underweight. The amount of underweight children has decreased since 1990, from 33 percent to 28 percent between 1990 and 2004. Underweight and stunted children are more susceptible to infection, more likely to fall behind in school, more likely to become overweight and develop non-infectious diseases, and ultimately earn less than their non-stunted coworkers. Therefore, undernutrition can accumulate deficiencies in health which results in less productive individuals and societies \n\nMany children are born with the inherent disadvantage of low birth weight, often caused by intrauterine growth restriction and poor maternal nutrition, which results in worse growth, development, and health throughout the course of their lifetime. Children born at low birthweight (less than 5.5 pounds or 2.5 kg), are less likely to be healthy and are more susceptible to disease and early death. Those born at low birthweight also are likely to have a depressed immune system, which can increase their chances of heart disease and diabetes later on in life. Because 96% of low birthweight occurs in the developing world, low birthweight is associated with being born to a mother in poverty with poor nutritional status that has had to perform demanding labor.\n\nStunting and other forms of undernutrition reduces a child’s chance of survival and hinders their optimal growth and health. Stunting has demonstrated association with poor brain development, which reduces cognitive ability, academic performance, and eventually earning potential. Important determinants of stunting include the quality and frequency of infant and child feeding, infectious disease susceptibility, and the mother’s nutrition and health status. Undernourished mothers are more likely to birth stunted children, perpetuating a cycle of undernutrition and poverty. Stunted children are more likely to develop obesity and chronic diseases upon reaching adulthood. Therefore, malnutrition resulting in stunting can further worsen the obesity epidemic, especially in low and middle income countries. This creates even new economic and social challenges for vulnerable impoverished groups.\n\nData on global and regional food supply shows that consumption rose from 2011-2012 in all regions. Diets became more diverse, with a decrease in consumption of cereals and roots and an increase in fruits, vegetables, and meat products. However, this increase masks the discrepancies between nations, where Africa, in particular, saw a decrease in food consumption over the same years. This information is derived from food balance sheets that reflect national food supplies, however, this does not necessarily reflect the distribution of micro and macronutrients. Often inequality in food access leaves distribution which uneven, resulting in undernourishment for some and obesity for others.\n\nUndernourishment, or hunger, according to the FAO, is dietary intake below the minimum daily energy requirement. The amount of undernourishment is calculated utilizing the average amount of food available for consumption, the size of the population, the relative disparities in access to the food, and the minimum calories required for each individual. According to FAO, 868 million people (12% of the global population) were undernourished in 2012. This has decreased across the world since 1990, in all regions except for Africa, where undernourishment has steadily increased. However, the rates of decrease are not sufficient to meet the first Millennium Development Goal of halving hunger between 1990 and 2015. The global financial, economic, and food price crisis in 2008 drove many people to hunger, especially women and children. The spike in food prices prevented many people from escaping poverty, because the poor spend a larger proportion of their income on food and farmers are net consumers of food. High food prices cause consumers to have less purchasing power and to substitute more-nutritious foods with low-cost alternatives.\n\nMalnutrition in industrialized nations is primarily due to excess calories and non-nutritious carbohydrates, which has contributed to the obesity epidemic affecting both developed and some developing nations. In 2008, 35% of adults above the age of 20 years were overweight (BMI 25 kg/m), a prevalence that has doubled worldwide between 1980 and 2008. Also 10% of men and 14% of women were obese, with a BMI greater than 30. Rates of overweight and obesity vary across the globe, with the highest prevalence in the Americas, followed by European nations, where over 50% of the population is overweight or obese.\n\nObesity is more prevalent amongst high income and higher middle income groups than lower divisions of income. Women are more likely than men to be obese, where the rate of obesity in women doubled from 8% to 14% between 1980 and 2008. Being overweight as a child has become an increasingly important indicator for later development of obesity and non-infectious diseases such as heart disease. In several western European nations, the prevalence of overweight and obese children rose by 10% from 1980 to 1990, a rate that has begun to accelerate recently.\n\nVitamins and minerals are essential to the proper functioning and maintenance of the human body. Globally, particularly in developing nations, deficiencies in Iodine, Iron, and Zinc among others are said to impair human health when these minerals are not ingested in an adequate quantity. There are 20 trace elements and minerals that are essential in small quantities to body function and overall human health.\n\nIron deficiency is the most common inadequate nutrient worldwide, affecting approximately 2 billion people. Globally, anemia affects 1.6 billion people, and represents a public health emergency in children under five and mothers. The World Health Organization estimates that there exists 469 million women of reproductive age and approximately 600 million preschool and school-age children worldwide who are anemic. Anemia, especially iron-deficient anemia, is a critical problem for cognitive developments in children, and its presence leads to maternal deaths and poor brain and motor development in children. The development of anemia affects mothers and children more because infants and children have higher iron requirements for growth. Health consequences for iron deficiency in young children include increased perinatal mortality, delayed mental and physical development, negative behavioral consequences, reduced auditory and visual function, and impaired physical performance. The harm caused by iron deficiency during child development cannot be reversed and result in reduced academic performance, poor physical work capacity, and decreased productivity in adulthood. Mothers are also very susceptible to iron-deficient anemia because women lose iron during menstruation, and rarely supplement it in their diet. Maternal iron deficiency anemia increases the chances of maternal mortality, contributing to at least 18% of maternal deaths in low and middle income countries.\n\nVitamin A plays an essential role in developing the immune system in children, therefore, it is considered an essential micronutrient that can greatly affect health. However, because of the expense of testing for deficiencies, many developing nations have not been able to fully detect and address vitamin A deficiency, leaving vitamin A deficiency considered a silent hunger. According to estimates, subclinical vitamin A deficiency, characterized by low retinol levels, affects 190 million pre-school children and 19 million mothers worldwide.\nThe WHO estimates that 5.2 million of these children under 5 are affected by night blindness, which is considered clinical vitamin A deficiency. Severe vitamin A deficiency (VAD) for developing children can result in visual impairments, anemia and weakened immunity, and increase their risk of morbidity and mortality from infectious disease. This also presents a problem for women, with WHO estimating that 9.8 million women are affected by night blindness. Clinical vitamin A deficiency is particularly common among pregnant women, with prevalence rates as high as 9.8% in South-East Asia.\n\nEstimates say that 28.5% of the global population is iodine deficient, representing 1.88 billion individuals. Although salt iodization programs have reduced the prevalence of iodine deficiency, this is still a public health concern in 32 nations. Moderate deficiencies are common in Europe and Africa, and over consumption is common in the Americas. Iodine-deficient diets can interfere with adequate thyroid hormone production, which is responsible for normal growth in the brain and nervous system. This ultimately leads to poor school performance and impaired intellectual capabilities.\n\nImprovement of breast feeding practices, like early initiation and exclusive breast feeding for the first two years of life, could save the lives of 1.5 million children annually. Nutrition interventions targeted at infants aged 0–5 months first encourages early initiation of breastfeeding. Though the relationship between early initiation of breast feeding and improved health outcomes has not been formally established, a recent study in Ghana suggests a causal relationship between early initiation and reduced infection-caused neo-natal deaths. Also, experts promote exclusive breastfeeding, rather than using formula, which has shown to promote optimal growth, development, and health of infants. Exclusive breastfeeding often indicates nutritional status because infants that consume breast milk are more likely to receive all adequate nourishment and nutrients that will aid their developing body and immune system. This leaves children less likely to contract diarrheal diseases and respiratory infections.\n\nBesides the quality and frequency of breastfeeding, the nutritional status of mothers affects infant health. When mothers do not receive proper nutrition, it threatens the wellness and potential of their children. Well-nourished women are less likely to experience risks of birth and are more likely to deliver children who will develop well physically and mentally. Maternal undernutrition increases the chances of low-birth weight, which can increase the risk of infections and asphyxia in fetuses, increasing the probability of neonatal deaths. Growth failure during intrauterine conditions, associated with improper mother nutrition, can contribute to lifelong health complications. Approximately 13 million children are born with intrauterine growth restriction annually.\n\nAccording to UNICEF, South Asia has the highest levels of underweight children under five, followed by sub-Saharan Africans nations, with Industrialized countries and Latin nations having the lowest rates.\n\nIn the United States, 2% of children are underweight, with under 1% stunted and 6% are wasting.\n\nIn the US, dietitians are registered (RD) or licensed (LD) with the Commission for Dietetic Registration and the American Dietetic Association, and are only able to use the title \"dietitian,\" as described by the business and professions codes of each respective state, when they have met specific educational and experiential prerequisites and passed a national registration or licensure examination, respectively. In California, registered dietitians must abide by the Anyone may call themselves a nutritionist, including unqualified dietitians, as this term is unregulated. Some states, such as the State of Florida, have begun to include the title \"nutritionist\" in state licensure requirements. Most governments provide guidance on nutrition, and some also impose mandatory disclosure/labeling requirements for processed food manufacturers and restaurants to assist consumers in complying with such guidance..\n\nIn the US, nutritional standards and recommendations are established jointly by the US Department of Agriculture and US Department of Health and Human Services. Dietary and physical activity guidelines from the USDA are presented in the concept of a plate of food which in 2011 superseded the MyPyramid food pyramid that had replaced the Four Food Groups. The Senate committee currently responsible for oversight of the USDA is the \"Agriculture, Nutrition and Forestry Committee\". Committee hearings are often televised on C-SPAN. The U.S. Department of Health and Human Services provides a sample week-long menu which fulfills the nutritional recommendations of the government. Canada's Food Guide is another governmental recommendation..\n\nAccording to UNICEF, the Commonwealth of Independent States has the lowest rates of stunting and wasting, at 14 percent and 3 percent. The nations of Estonia, Finland, Iceland, Lithuania and Sweden have the lowest prevalence of low birthweight children in the world- at 4%. Proper prenatal nutrition is responsible for this small prevalence of low birthweight infants. However, low birthweight rates are increasing, due to the use of fertility drugs, resulting in multiple births, women bearing children at an older age, and the advancement of technology allowing more pre-term infants to survive. Industrialized nations more often face malnutrition in the form of over-nutrition from excess calories and non-nutritious carbohydrates, which has contributed greatly to the public health epidemic of obesity. Disparities, according to gender, geographic location and socio-economic position, both within and between countries, represent the biggest threat to child nutrition in industrialized countries. These disparities are a direct product of social inequalities and social inequalities are rising throughout the industrialized world, particularly in Europe.\n\nSouth Asia has the highest percentage and number of underweight children under five in the world, at approximately 78 million children. Patterns of stunting and wasting are similar, where 44% have not reached optimal height and 15% are wasted, rates much higher than any other regions. This region of the world has extremely high rates of child underweight- 46% of its child population under five is underweight. India, Bangladesh, and Pakistan alone account for half the globe’s underweight child population. South Asian nations have made progress towards the MDGs, considering the rate has decreased from 53% since 1990, however, a 1.7% decrease of underweight prevalence per year will not be sufficient to meet the 2015 goal. Some nations, such as Afghanistan, Bangladesh, and Sri Lanka, on the other hand, have made significant improvements, all decreasing their prevalence by half in ten years. While India and Pakistan have made modest improvements, Nepal has made no significant improvement in underweight child prevalence. Other forms of undernutrition have continued to persist with high resistance to improvement, such as the prevalence of stunting and wasting, which has not changed significantly in the past 10 years. Causes of this poor nutrition include energy-insufficient diets, poor sanitation conditions, and the gender disparities in educational and social status. Girls and women face discrimination especially in nutrition status, where South Asia is the only region in the world where girls are more likely to be underweight than boys. In South Asia, 60% of children in the lowest quintile are underweight, compared to only 26% in the highest quintile, and the rate of reduction of underweight is slower amongst the poorest.\n\nThe Eastern and Southern African nations have shown no improvement since 1990 in the rate of underweight children under five. They have also made no progress in halving hunger by 2015, the most prevalent Millennium Development Goal. This is due primarily to the prevalence of famine, declined agricultural productivity, food emergencies, drought, conflict, and increased poverty. This, along with HIV/AIDS, has inhibited the nutrition development of nations such as Lesotho, Malawi, Mozambique, Swaziland, Zambia and Zimbabwe. Botswana has made remarkable achievements in reducing underweight prevalence, dropping 4% in 4 years, despite its place as the second leader in HIV prevalence amongst adults in the globe. South Africa, the wealthiest nation in this region, has the second lowest proportion of underweight children at 12%, but has been steadily increasing in underweight prevalence since 1995. Almost half of Ethiopian children are underweight, and along with Nigeria, they account for almost one-third of the underweight under five in all of Sub-Saharan Africa.\n\nWest/Central Africa has the highest rate of children under five underweight in the world. Of the countries in this region, the Congo has the lowest rate at 14%, while the nations of Democratic Republic of the Congo, Ghana, Guinea, Mali, Nigeria, Senegal and Togo are improving slowly. In Gambia, rates decreased from 26% to 17% in four years, and their coverage of vitamin A supplementation reaches 91% of vulnerable populations. This region has the next highest proportion of wasted children, with 10% of the population under five not at optimal weight. Little improvement has been made between the years of 1990 and 2004 in reducing the rates of underweight children under five, whose rate stayed approximately the same. Sierra Leone has the highest child under five mortality rate in the world, due predominantly to its extreme infant mortality rate, at 238 deaths per 1000 live births. Other contributing factors include the high rate of low birthweight children (23%) and low levels of exclusive breast feeding (4%). Anemia is prevalent in these nations, with unacceptable rates of iron deficient anemia. The nutritional status of children is further indicated by its high rate of child wasting - 10%. Wasting is a significant problem in Sahelian countries – Burkina Faso, Chad, Mali, Mauritania and Niger – where rates fall between 11% and 19% of under fives, affecting more than 1 million children.\n\nSix countries in the Middle East and North Africa region are on target to meet goals for reducing underweight children by 2015, and 12 countries have prevalence rates below 10%. However, the nutrition of children in the region as a whole has degraded for the past ten years due to the increasing portion of underweight children in three populous nations – Iraq, Sudan, and Yemen. Forty six percent of all children in Yemen are underweight, a percentage that has worsened by 4% since 1990. In Yemen, 53% of children under five are stunted and 32% are born at low birth weight. Sudan has an underweight prevalence of 41%, and the highest proportion of wasted children in the region at 16%. One percent of households in Sudan consume iodized salt. Iraq has also seen an increase in child underweight since 1990. Djibouti, Jordan, the Occupied Palestinian Territory (OPT), Oman, the Syrian Arab Republic and Tunisia are all projected to meet minimum nutrition goals, with OPT, Syrian AR, and Tunisia the fastest improving regions. This region demonstrates that undernutrition does not always improve with economic prosperity, where the United Arab Emirates, for example, despite being a wealthy nation, has similar child death rates due to malnutrition to those seen in Yemen.\n\nThe East Asia/Pacific region has reached its goals on nutrition, in part due to the improvements contributed by China, the region’s most populous country. China has reduced its underweight prevalence from 19 percent to 8 percent between 1990 and 2002. China played the largest role in the world in decreasing the rate of children under five underweight between 1990 and 2004, halving the prevalence. This reduction of underweight prevalence has aided in the lowering of the under 5 mortality rate from 49 to 31 of 1000. They also have a low birthweight rate at 4%, a rate comparable to industrialized countries, and over 90% of households receive adequate iodized salts. However, large disparities exist between children in rural and urban areas, where 5 provinces in China leave 1.5 million children iodine deficient and susceptible to diseases. Singapore, Vietnam, Malaysia, and Indonesia are all projected to reach nutrition MDGs. Singapore has the lowest under five mortality rate of any nation, besides Iceland, in the world, at 3%. Cambodia has the highest rate of child mortality in the region (141 per 1,000 live births), while still its proportion of underweight children increased by 5 percent to 45% in 2000. Further nutrient indicators show that only 12 per cent of Cambodian babies are exclusively breastfed and only 14 per cent of households consume iodized salt.\n\nThis region has undergone the fastest progress in decreasing poor nutrition status of children in the world. The Latin American region has reduced underweight children prevalence by 3.8% every year between 1990 and 2004, with a current rate of 7% underweight. They also have the lowest rate of child mortality in the developing world, with only 31 per 1000 deaths, and the highest iodine consumption. Cuba has seen improvement from 9 to 4 percent underweight under 5 between 1996 and 2004. The prevalence has also decreased in the Dominican Republic, Jamaica, Peru, and Chile. Chile has a rate of underweight under 5, at merely 1%. The most populous nations, Brazil and Mexico, mostly have relatively low rates of underweight under 5, with only 6% and 8%. Guatemala has the highest percentage of underweight and stunted children in the region, with rates above 45%. There are disparities amongst different populations in this region. For example, children in rural areas have twice the prevalence of underweight at 13%, compared to urban areas at 5%.\n\nOccurring throughout the world, lack of proper nutrition is both a consequence and cause of poverty. Impoverished individuals are less likely to have access to nutritious food and to escape from poverty than those who have healthy diets. Disparities in socioeconomic status, both between and within nations, provide the largest threat to child nutrition in industrialized nations, where social inequality is on the rise. According to UNICEF, children living in the poorest households are twice as likely to be underweight as those in the richest. Those in the lowest wealth quintile and whose mothers have the least education demonstrate the highest rates of child mortality and stunting. Throughout the developing world, socioeconomic inequality in childhood malnutrition is more severe than in upper income brackets, regardless of the general rate of malnutrition. Concurrently, the greatest increase in childhood obesity has been seen in the lower middle income bracket.\n\nAccording to UNICEF, children in rural locations are more than twice as likely to be underweight as compared to children under five in urban areas. In Latin American/Caribbean nations, “Children living in rural areas in Bolivia, Honduras, Mexico and Nicaragua are more than twice as likely to be underweight as children living in urban areas. That likelihood doubles to four times in Peru.” \n\nIn the United States, the incidence of low birthweight is on the rise among all populations, but particularly among minorities.\n\nAccording to UNICEF, boys and girls have almost identical rates as underweight children under age 5 across the world, except in South Asia.\n\nNutrition directly influences progress towards meeting the Millennium Goals of eradicating hunger and poverty through health and education. Therefore, nutrition interventions take a multi-faceted approach to improve the nutrition status of various populations. Policy and programming must target both individual behavioral changes and policy approaches to public health. While most nutrition interventions focus on delivery through the health-sector, non-health sector interventions targeting agriculture, water and sanitation, and education are important as well. Global nutrition micro-nutrient deficiencies often receive large-scale solution approaches by deploying large governmental and non-governmental organizations. For example, in 1990, iodine deficiency was particularly prevalent, with one in five households, or 1.7 billion people, not consuming adequate iodine, leaving them at risk to develop associated diseases. Therefore, a global campaign to iodize salt to eliminate iodine deficiency successfully boosted the rate to 69% of households in the world consuming adequate amounts of iodine.\n\nEmergencies and crises often exacerbate undernutrition, due to the aftermath of crises that include food insecurity, poor health resources, unhealthy environments, and poor healthcare practices. Therefore, the repercussions of natural disasters and other emergencies can exponentially increase the rates of macro and micronutrient deficiencies in populations. Disaster relief interventions often take a multi-faceted public health approach. UNICEF’s programming targeting nutrition services amongst disaster settings include nutrition assessments, measles immunization, vitamin A supplementation, provision of fortified foods and micronutrient supplements, support for breastfeeding and complementary feeding for infants and young children, and therapeutic and supplementary feeding. For example, during Nigeria’s food crisis of 2005, 300,000 children received therapeutic nutrition feeding programs through the collaboration of UNICEF, the Niger government, the World Food Programme, and 24 NGOs utilizing community and facility based feeding schemes.\n\nInterventions aimed at pregnant women, infants, and children take a behavioral and program-based approach. Behavioral intervention objectives include promoting proper breast-feeding, the immediate initiation of breastfeeding, and its continuation through 2 years and beyond. UNICEF recognizes that to promote these behaviors, healthful environments must be established conducive to promoting these behaviors, like healthy hospital environments, skilled health workers, support in the public and workplace, and removing negative influences. Finally, other interventions include provisions of adequate micro and macro nutrients such as iron, anemia, and vitamin A supplements and vitamin-fortified foods and ready-to-use products. Programs addressing micro-nutrient deficiencies, such as those aimed at anemia, have attempted to provide iron supplementation to pregnant and lactating women. However, because supplementation often occurs too late, these programs have had little effect. Interventions such as women’s nutrition, early and exclusive breastfeeding, appropriate complementary food and micronutrient supplementation have proven to reduce stunting and other manifestations of undernutrition. A Cochrane review of community-based maternal health packages showed that this community-based approach improved the initiation of breastfeeding within one hour of birth. Some programs have had adverse effects. One example is the “Formula for Oil” relief program in Iraq, which resulted in the replacement of breastfeeding for formula, which has negatively affected infant nutrition.\n\nIn April 2010, the World Bank and the IMF released a policy briefing entitled “Scaling up Nutrition (SUN): A Framework for action” that represented a partnered effort to address the Lancet’s Series on under nutrition, and the goals it set out for improving under nutrition. They emphasized the 1000 days after birth as the prime window for effective nutrition intervention, encouraging programming that was cost-effective and showed significant cognitive improvement in populations, as well as enhanced productivity and economic growth. This document was labeled the SUN framework, and was launched by the UN General Assembly in 2010 as a road map encouraging the coherence of stakeholders like governments, academia, UN system organizations and foundations in working towards reducing under nutrition. The SUN framework has initiated a transformation in global nutrition- calling for country-based nutrition programs, increasing evidence based and cost–effective interventions, and “integrating nutrition within national strategies for gender equality, agriculture, food security, social protection, education, water supply, sanitation, and health care”. Government often plays a role in implementing nutrition programs through policy. For instance, several East Asian nations have enacted legislation to increase iodization of salt to increase household consumption. Political commitment in the form of evidence-based effective national policies and programs, trained skilled community nutrition workers, and effective communication and advocacy can all work to decrease malnutrition. Market and industrial production can play a role as well. For example, in the Philippines, improved production and market availability of iodized salt increased household consumption. While most nutrition interventions are delivered directly through governments and health services, other sectors, such as agriculture, water and sanitation, and education, are vital for nutrition promotion as well.\n\nNutrition is taught in schools in many countries. In England and Wales the Personal and Social Education and Food Technology curricula include nutrition, stressing the importance of a balanced diet and teaching how to read nutrition labels on packaging. In many schools a Nutrition class will fall within the Family and Consumer Science or Health departments. In some American schools, students are required to take a certain number of FCS or Health related classes. Nutrition is offered at many schools, and if it is not a class of its own, nutrition is included in other FCS or Health classes such as: Life Skills, Independent Living, Single Survival, Freshmen Connection, Health etc. In many Nutrition classes, students learn about the food groups, the food pyramid, Daily Recommended Allowances, calories, vitamins, minerals, malnutrition, physical activity, healthy food choices and how to live a healthy life.\n\nA 1985 US National Research Council report entitled \"Nutrition Education in US Medical Schools\" concluded that nutrition education in medical schools was inadequate. Only 20% of the schools surveyed taught nutrition as a separate, required course. A 2006 survey found that this number had risen to 30%.\n\nThe protein requirement for each individual differs, as do opinions about whether and to what extent physically active people require more protein. The 2005 Recommended Dietary Allowances (RDA), aimed at the general healthy adult population, provide for an intake of 0.8 grams of protein per kilogram of body weight. A review panel stating that \"no additional dietary protein is suggested for healthy adults undertaking resistance or endurance exercise.\"\n\nThe main fuel used by the body during exercise is carbohydrates, which is stored in muscle as glycogen – a form of sugar. During exercise, muscle glycogen reserves can be used up, especially when activities last longer than 90 min. Because the amount of glycogen stored in the body is limited, it is important for athletes participating in endurance sports such as marathons to consume carbohydrates during their events.\n\nAdequate nutrition is essential for the growth of children from infancy right through until adolescence. Some nutrients are specifically required for growth on top of nutrients required for normal body maintenance, in particular calcium and iron.\n\nMalnutrition in general is higher among the elderly, but has different aspects in developed and undeveloped countries.\n\nHumans have evolved as omnivorous hunter-gatherers over the past 250,000 years. The diet of early modern humans varied significantly depending on location and climate. The diet in the tropics tended to depend more heavily on plant foods, while the diet at higher latitudes tended more towards animal products. Analyses of postcranial and cranial remains of humans and animals from the Neolithic, along with detailed bone-modification studies, have shown that cannibalism also occurred among prehistoric humans.\n\nAgriculture developed about 10,000 years ago in multiple locations throughout the world, providing grains (such as wheat, rice and maize) and potatoes; and originating staples such as bread and pasta dough, and tortillas. Farming also provided milk and dairy products, and sharply increased the availability of meats and the diversity of vegetables. The importance of food purity was recognized when bulk storage led to infestation and contamination risks. Cooking developed as an often ritualistic activity,\ndue to efficiency and reliability concerns requiring adherence to strict recipes and procedures, and in response to demands for food purity and consistency.\n\nAround 3000 BC the Vedic texts made mention of scientific research on nutrition. The Bible's Book of Daniel recounts first recorded nutritional experiment. During an invasion of Judah, King Nebuchadnezzar of Babylon captured Daniel and his friends. Selected as court servants, they were to share in the king's fine foods and wine. But they objected, preferring vegetables (pulses) and water in accordance with their Jewish dietary restrictions. The king's chief steward reluctantly agreed to a trial. Daniel and his friends received their diet for 10 days. On comparison with the king's men, they appeared healthier, and were allowed to continue with their diet. Around 475 BC, Anaxagoras stated that food is absorbed by the human body and therefore contained \"homeomerics\" (generative components), suggesting the existence of nutrients. Around 400 BC, Hippocrates said: \"Let food be your medicine and medicine be your food.\"\n\nThe 16th-century scientist and artist Leonardo da Vinci (1452–1519) compared metabolism to a burning candle. In 1747 Dr. James Lind, a physician in the British navy, performed the first attested scientific nutrition experiment, discovering that lime juice saved sailors who had been at sea for years from scurvy, a deadly and painful bleeding disorder. The discovery was ignored for forty years, but after about 1850 British sailors became known as \"limeys\". (Scientists would not identify the essential vitamin C within lime juice until the 1930s.)\n\nAround 1770 Antoine Lavoisier, the \"Father of Nutrition and Chemistry\", discovered the details of metabolism, demonstrating that the oxidation of food is the source of body heat. In 1790 George Fordyce recognized calcium as necessary for fowl survival. In the early 19th century, the elements carbon, nitrogen, hydrogen and oxygen were recognized as the primary components of food, and methods to measure their proportions were developed.\n\nIn 1816 François Magendie discovered that dogs fed only carbohydrates and fat lost their body protein and died in a few weeks, but dogs also fed protein survived, identifying protein as an essential dietary component. In 1840, Justus Liebig discovered the chemical makeup of carbohydrates (sugars), fats (fatty acids) and proteins (amino acids). In the 1860s Claude Bernard discovered that body fat can be synthesized from carbohydrate and protein, showing that the energy in blood glucose can be stored as fat or as glycogen. In the early 1880s Kanehiro Takaki observed that Japanese sailors (whose diets consisted almost entirely of white rice) developed beriberi (or endemic neuritis, a disease causing heart problems and paralysis), but British sailors and Japanese naval officers did not. Adding various types of vegetables and meats to the diets of Japanese sailors prevented the disease.\n\nIn 1896 Eugen Baumann observed iodine in thyroid glands. In 1897, Christiaan Eijkman worked with natives of Java, who also suffered from beriberi. Eijkman observed that chickens fed the native diet of white rice developed the symptoms of beriberi, but remained healthy when fed unprocessed brown rice with the outer bran intact. Eijkman cured the natives by feeding them brown rice, demonstrating that food can cure disease. Over two decades later, nutritionists learned that the outer rice bran contains vitamin B.\n\nIn the early 20th century Carl von Voit and Max Rubner independently measured caloric energy expenditure in different species of animals, applying principles of physics in nutrition. In 1906, Wilcock and Hopkins showed that the amino acid tryptophan was necessary for the survival of rats. He fed them a special mixture of food containing all the nutrients he believed were essential for survival, but the rats died. A second group of rats to which he also fed an amount of milk containing vitamins. Gowland Hopkins recognized \"accessory food factors\" other than calories, protein and minerals, as organic materials essential to health but which the body cannot synthesize. In 1907 Stephen M. Babcock and Edwin B. Hart conducted the single-grain experiment. This experiment ran through 1911.\n\nIn 1912 Casimir Funk coined the term vitamin to label a vital factor in the diet: from the words \"vital\" and \"amine,\" because these unknown substances preventing scurvy, beriberi, and pellagra, were thought then to derive from ammonia. The vitamins were studied in the first half of the 20th century. In 1913 Elmer McCollum discovered the first vitamins, fat-soluble vitamin A and water-soluble vitamin B (in 1915; later identified as a complex of several water-soluble vitamins) and named vitamin C as the then-unknown substance preventing scurvy. Lafayette Mendel (1872-1935) and Thomas Osborne (1859–1929) also performed pioneering work on vitamins A and B. In 1919 Sir Edward Mellanby incorrectly identified rickets as a vitamin A deficiency, because he could cure it in dogs with cod-liver oil. In 1922 McCollum destroyed the vitamin A in cod liver oil but found it still cured rickets, thus identifying vitamin D. Also in 1922, H.M. Evans and L.S. Bishop discovered vitamin E as essential for rat pregnancy, and originally called it \"food factor X\" until 1925.\n\nIn 1925 Hart discovered that iron absorption requires trace amounts of copper. In 1927 Adolf Otto Reinhold Windaus synthesized vitamin D, for which he won the Nobel Prize in Chemistry in 1928. In 1928 Albert Szent-Györgyi isolated ascorbic acid, and in 1932 proved that it is vitamin C by preventing scurvy. In 1935 he synthesized it, and in 1937 won a Nobel Prize for his efforts. Szent-Györgyi concurrently elucidated much of the citric acid cycle. In the 1930s William Cumming Rose identified essential amino acids, necessary protein components which the body cannot synthesize. In 1935 Eric Underwood and Hedley Marston independently discovered the necessity of cobalt. In 1936 Eugene Floyd Dubois showed that work and school performance relate to caloric intake. In 1938 Erhard Fernholz discovered the chemical structure of vitamin E. It was synthesised by Paul Karrer (1889–1971).\n\nFrom 1940 rationing in the United Kingdom – during and after World War II – took place according to nutritional principles drawn up by Elsie Widdowson and others. In 1941 the National Research Council established the first Recommended Dietary Allowances (RDAs). In 1992 the U.S. Department of Agriculture introduced the Food Guide Pyramid. In 2002 a Natural Justice study showed a relation between nutrition and violent behavior. In 2005 a study found that in addition to bad nutrition, adenovirus may cause obesity.\n\n\n\n"}
{"id": "4011165", "url": "https://en.wikipedia.org/wiki?curid=4011165", "title": "Information continuity", "text": "Information continuity\n\nIn the healthcare industry, information continuity is the process by which information relevant to a patient's care is made available to both the patient and the provider at the right place and the right time, to facilitate ongoing health care management and continuity of care.\n\nThis is an extension of the concept of \"Continuity of Care,\" which is defined by the American Academy of Family Physicians in their Continuity of Care definition as \"the process by which the patient and the physician are cooperatively involved in ongoing health care management toward the goal of high quality, cost-effective medical care.\"\n\nThere is a non-Information Technology reference to \"Informational continuity\" — the use of information on past events and personal circumstances to make current care appropriate for each individual. This exists with \"Management continuity\" and \"Relational continuity.\"\n\nInformation continuity in the information technology sense may exist alongside physical care continuity, such as when a medical chart arrives with a patient to the hospital. Information continuity may also be separate, such as when a patient's electronic records are sent to a treating physician before the patient arrives at a care site.\n\nCreating information continuity in health care typically involves the use of health information technology to link systems using standards. Information continuity will become more and more important as patients in health care systems expect that their treating physicians have all of their medical information across the health care spectrum.\n\nThis use of this term in health information technology initiated at Seattle, Washington, at the Group Health Cooperative non-profit care system to describe activities including data sharing, allergy and medication reconciliation, and interfacing of data between health care institutions.\n\n"}
{"id": "21548279", "url": "https://en.wikipedia.org/wiki?curid=21548279", "title": "James Lind Alliance", "text": "James Lind Alliance\n\nThe James Lind Alliance is a British non-profit making initiative, established in 2004. It was established to bring patients, carers and clinicians together, in Priority Setting Partnerships, to identify and prioritise the unanswered questions about treatments that they agree are most important. The intention is to ensure that those who fund health research are aware of what matters to patients, carers and clinicians.\n\nFrom 1 April 2013, the secretariat of the James Lind Alliance has been hosted by the National Institute for Health Research Evaluation, Trials and Studies Coordinating Centre (NETSCC). Its work includes recruiting and training JLA Advisers, coordinating Priority Setting Partnerships, communications and liaising with other parts of the National Institute for Health Research (NIHR). In 2016 the James Lind Alliance was granted with the Societal Award of the Foundation Federation of Dutch Medical Scientific Societies (Federa) for their initiative to bring patients into partnerships for research priorities \n\nResearch on the effects of treatments often overlooks the shared interests of patients and clinicians. As a result, questions they both consider important are not addressed. The pharmaceutical and medical technology industries and academia play essential roles in developing new treatments, but their priorities are not necessarily the same as those of patients and clinicians. For this reason many areas of potentially valuable research are neglected. Bringing patients and clinicians together to prioritise questions in treatment uncertainty is thought to be rare.\n\nThe James Lind Alliance brings together patients and patient representatives, carers and clinicians, as individuals or represented by groups, to form Priority Setting Partnerships, focusing on specific health conditions. For example, the Asthma Priority Setting Partnership was led by Asthma UK and the British Thoracic Society, while the Urinary Incontinence Priority Setting Partnership was led by the Bladder & Bowel Foundation and the Cochrane Incontinence Group , part of the Cochrane Collaboration.\n\nPriority Setting Partnerships work together to gather uncertainties about the effects of treatments. The uncertainties are all checked to ensure they cannot be answered by existing knowledge, research or sources of information.\n\nThe uncertainties then go through a process of prioritisation, which aims to culminate in a top ten list of priorities for research, shared by patients, carers and clinicians. To date, the process has been completed for over 40 health areas including Asthma, Urinary Incontinence, Anaesthetics and Perioperative Care, Vitiligo, Intensive Care, Prostate Cancer, Cystic Fibrosis and Surgery for Common Shoulder Problems.\n\nThe Alliance is named after a pioneer of clinical trials, James Lind (1716–1794). Two hundred and fifty years ago, there were many conflicting ideas and unanswered questions about how to treat the deadly disease scurvy. Lind, a Scottish naval surgeon, decided to confront this uncertainty by treating his patients within a clinical trial comparing six of the proposed remedies. His trial showed that oranges and lemons were dramatically better than the other supposed treatments.\n\n"}
{"id": "12612736", "url": "https://en.wikipedia.org/wiki?curid=12612736", "title": "Lactivism", "text": "Lactivism\n\nLactivism (a portmanteau of \"lactation\" and \"activism\") is the doctrine or practice of vigorous action or involvement as a means of achieving a breastfeeding culture, sometimes by demonstrations, protests, etc. of breastfeeding. Supporters, referred to as \"lactivists\", seek to protest the violation of International Code of Marketing of Breast-milk Substitutes by formula companies and industry. \n\nOne form that lactivism takes is the staging of a \"nurse-in\" (a play on \"sit-in\"), which involves women gathering in public to nurse their children, usually to protest incidents in which a nursing mother was asked to cover up or leave a location because she was nursing.\n\nDuring nurse-ins, nursing mothers often wear clothing with the International Breastfeeding Symbol on it, to show their solidarity.\n\nAnother form of lactivism is acting as support for mothers that wish to nurse. Lactivists provide information and share resources on successful nursing.\n\nMany lactivists choose to breastfeed their children over bottle feeding, seeing this as the natural way to provide nutrition. It is claimed that breastfeeding provides a bonding experience superior to that or bottle feeding. Lactivists may also argue that bottle feeding is costlier than breastfeeding as it requires a multitude of items, and the money saved from breastfeeding can be spent on other useful items for the child. Multiple health organizations recommend breast milk as the primary source of nutrition for babies, including the American Academy of Pediatrics, the American Medical Association, and the World Health Organization.\n\n\n"}
{"id": "23281432", "url": "https://en.wikipedia.org/wiki?curid=23281432", "title": "List of current youth hearing conservation programs", "text": "List of current youth hearing conservation programs\n\n\n"}
{"id": "23811783", "url": "https://en.wikipedia.org/wiki?curid=23811783", "title": "Mandated choice", "text": "Mandated choice\n\nMandated choice or mandatory choice is an approach to public policy questions in which people are required by law to state in advance whether or not they are willing to engage in a particular action. The approach contrasts with \"opt-in\" and \"opt-out\" (\"presumed consent\") models of policy formation. The approach has most frequently been applied to cadaveric organ donation, but has increasingly been considered for advance directives as well. One bioethicist, in advocating for a mandatory choice model for living wills, argues that \"while all Americans should have a right to decide how they want their lives to end, it does not follow that they should be able to avoid confronting such a choice.\"\n\nOne of the first considerations of mandated choice appeared in Great Britain's Gore Report, a 1989-1990 study funded by the British Department of Health. From 2011 all those applying for or renewing driving licences online in the UK are required to state whether they wished to donate their organs.\n\nThe American Medical Association endorsed a mandated choice model for organ donation in 1994.\n\nIt has been suggested that individuals could be compelled to choose as part of tax returns, driver's licence applications, and/or state benefits claims.\n\nA 1992 survey found that 90% of American college students favored a mandated choice model for organ donation, compared with only 60% who favored presumed consent. However, Texas implemented such a program, requiring drivers to make a choice on organ donation when obtaining licenses, and found that 80% of drivers declined to donate.\n\nChouhan and Draper propose a modified scheme of mandated choice, in which though all patients are given a choice whether to donate they are actively encouraged to do so.\n\n"}
{"id": "41815264", "url": "https://en.wikipedia.org/wiki?curid=41815264", "title": "Mental illness portrayed in media", "text": "Mental illness portrayed in media\n\nMental illnesses, also known as psychiatric disorders, can be poorly portrayed in terms of factual accuracy. In different forms of entertainment, such as movies, television shows, books, magazines, and news, those living with mental illness are sometimes shown to be stereotypically violent and unpredictable, unlike how many of those with mental illnesses truly are. Due to the potential for inaccurate portrayals of people with mental illnesses, some people believe that those with mental illnesses should be shunned away from society, locked away in mental institutions, heavily medicated, or a combination of the three. However, not only are those with these disorders able to function normally in society, but they can also lead highly successful jobs and careers, as well as make important contributions to society.\n\nIn 2012, India Knight wrote a column in London's \"The Sunday Times\" about depression. Fellow columnist, Alastair Campbell of The Huffington Post was ashamed to read her article mentioning how \"'everybody gets depressed'\" and also saying \"there is no stigma in depression\".\n\nIn his article, Campbell discusses the wrongfulness in her word choice. By saying everyone gets depressed is showing that she is a part of that world that either does not accept depression is a disease or they do not believe it is a disease.\n\nCampbell claims that Knight's article reinforces the reality that there is still stigma and taboo surrounding depression. He goes on to explain how even in the medical profession, people are afraid to mention to their employers that they have it simply because they would not fully understand like they would understand a physical illness such as the flu.\n\nEnding his article, Campbell mentions the fight to bring awareness and understanding to mental illness and describing Knight's article as, \"unhelpful, potentially damaging and certainly showed we still have quite a way to go.\"\n\nSchizophrenics are often portrayed as dangerous, violent, and as criminals despite the fact that the large majority of them are not.\n\n\nThroughout the world of television mental illnesses have been showcased throughout the years within many programs; for example, the hit television show on the A&E network \"Hoarders\", starts off with showcasing one or two individuals on their Obsessive compulsive disorder. Each individual would work with a psychologist or psychiatrist, professional organizer, or an “extreme cleaning specialist” which are individuals who specialize in treatment for this exact compulsive disorder. Mental illness and treatments using the media as a platform stated in “Issues of Mental Health (p.593) “The role of documentary shows like Hoarders in the change of classification is unclear. However, some believe the rise in awareness caused by them was a significant contributing factor. The article also stated that with the rise of “Hoarder” becoming a “buzzword” it began to command significant amount of professional attention.\n\n\"Intervention\", another program on the A&E network, also focuses on mental illness but, in this program it introduces the aspect of substance abuse. This program, like Hoarders follows the story on either one or two individuals who suffer from substance dependence and we are then taken into their day-to-day lives living with this dependence. Later the individual with the addiction is then given an ultimatum in which they decide the future of their well-being. For example, they would either go to rehabilitation or risk losing family, friends, shelter and in most cases financial assistance. The documentary style television program also brought in celebrity subjects to draw more attention to how important and powerful an intervention can effect anyone. What this show educates the viewers about was the intervention process. Being introduced to the intervention process and the way to properly handle an individual with addiction. This television program also eased the stigma on therapy; but more specifically the stigma on the effectiveness of interventions.\n\nChildren's television programs contain references to mental illnesses. A study conducted on a variety of New Zealand Children's television shows showed that a mental illness reference appeared in 59 out of 128 episodes studied. 159 mental illness references where contained in the 59 episodes.The 159 references consisted of vocabulary and character descriptions. The terms \"mad\", \"crazy', and \"losing your mind\" were above the three most common vocabulary references. Character descriptions consisted of disfigured facial features (teeth, noses, etc.) as well as disfigured extremities (feet, fingers, etc.). \n\nStudies have shown that social media can have an impact on how mental illness is portrayed and also can be linked to its development, according to Patrick Robinson. Social media has been known to create stigmas towards mental health that are not being made towards physical health. He conducts a study on Twitter and discovers that things such as OCD, anxiety, or depression are used in a way that takes away the seriousness from the mental illness. One example of how social media can have an effect on mental health is how people use Instagram and other forms of media to compare themselves to others' posts. \"Facebook depression\" leads to problems such as reclusiveness which can negatively damage one's health by creating feelings of loneliness and low self-esteem among young people\" . Traditional mainstream media portrays unrealistic, idealized, and sexualized body types for women and also men. Ladies having a heavier body type are less likely to be portrayed in a romantic situation. This depiction can lead to the body shaming of females who do not have the 'ideal' body type. \n\nThe following list of statistics was obtained from studies done in the United Kingdom.\n\n"}
{"id": "19391", "url": "https://en.wikipedia.org/wiki?curid=19391", "title": "Midwifery", "text": "Midwifery\n\nMidwifery is the health science and health profession that deals with pregnancy, childbirth, and the postpartum period (including care of the newborn), in addition to the sexual and reproductive health of women throughout their lives. In many countries, midwifery is a medical profession (special for its independent and direct specialized education; should not be confused with the medical specialty, which depends on a previous general training). A professional in midwifery is known as a midwife.\n\nA 2013 Cochrane review concluded that \"most women should be offered midwifery-led continuity models of care and women should be encouraged to ask for this option although caution should be exercised in applying this advice to women with substantial medical or obstetric complications.\" The review found that midwifery-led care was associated with a reduction in the use of epidurals, with fewer episiotomies or instrumental births, and a decreased risk of losing the baby before 24 weeks' gestation. However, midwifery-led care was also associated with a longer mean length of labor as measured in hours.\n\nTrimester means \"3 months.\" A normal pregnancy lasts about 9 months and has 3 trimesters.\n\nFirst trimester screening varies by country. Women are typically offered a Pap smear and urine analysis (UA), and blood tests including a complete blood count (CBC), blood typing (including Rh screen), syphilis, hepatitis, HIV, and rubella testing. Additionally, women may have chlamydia testing via a urine sample, and women considered at high risk are screened for Sickle Cell disease and Thalassemia. Women must consent to all tests before they are carried out. The woman's blood pressure, height and weight are measured. Her past pregnancies and family, social, and medical history are discussed. Women may have an ultrasound scan during the first trimester which may be used to help find the estimated due date. Some women may have genetic testing, such as screening for Down's Syndrome. Diet, exercise, and discomforts such as morning sickness are discussed.\nThe mother visits the midwife monthly or more often during the second trimester. The mother's partner and/or the labor coach may accompany her. The midwife will discuss pregnancy issues such as fatigue, heartburn, varicose veins, and other common problems such as back pain.\nBlood pressure and weight are monitored and the midwife measures the mother's abdomen to see if the baby is growing as expected. Lab tests such as a UA, CBC, and glucose tolerance test are done if the midwife feels they are necessary.\n\nIn the third trimester the midwife will see the mother every two weeks until week 36 and every week after that. Weight, blood pressure, and abdominal measurements will continue to be done. Lab tests such as a CDC and UA may be done with additional testing done for at-risk pregnancies. The midwife palpates the woman's abdomen to establish the lie, presentation and position of the fetus and later, the engagement. A pelvic exam may be done to see if the mother's cervix is dilating. The midwife and the mother discuss birthing options and write a birth care plan.\n\nMidwives are qualified to assist with a normal vaginal delivery while more complicated deliveries are handled by a health care provider who has had further training. Childbirth is divided into four stages. \n\nFollowing the birth, if the mother had an episiotomy or a tearing of the perineum, it is stitched. The midwife does regular assessments for uterine contraction, fundal height, and vaginal bleeding. Throughout labor and delivery the mother's vital signs (temperature, blood pressure, and pulse) are closely monitored and her fluid intake and output are measured. The midwife also monitors the baby's pulse rate, palpates the mother's abdomen to monitor the baby's position, and does vaginal checks as needed. If the birth deviates from the norm at any stage, the midwife requests assist from a more highly trained health care provider.\n\nUntil the last century most women have used both the upright position and alternative positions to give birth. The lithotomy position was not used until the advent of forceps in the seventeenth century and since then childbirth has progressively moved from a woman supported experience in the home to a medical intervention within the hospital.\nThere are significant advantages to assuming an upright position in labor and birth, such as stronger and more efficient uterine contractions aiding cervical dilatation, increased pelvic inlet and outlet diameters and improved uterine contractility. Upright positions in the second stage include sitting, squatting, kneeling, and being on hands and knees.\n\nFor women who have a hospital birth, the minimum hospital stay is six hours. Women who leave before this do so against medical advice. Women may choose when to leave the hospital. Full postnatal assessments are conducted daily whilst inpatient, or more frequently if needed. A postnatal assessment includes the woman's observations, general well being, breasts (either a discussion and assistance with breastfeeding or a discussion about lactation suppression), abdominal palpation (if she has not had a caesarean section) to check for involution of the uterus, or a check of her caesarean wound (the dressing doesn't need to be removed for this), a check of her perineum, particularly if she tore or had stitches, reviewing her lochia, ensuring she has passed urine and had her bowels open and checking for signs and symptoms of a DVT. The baby is also checked for jaundice, signs of adequate feeding, or other concerns. The baby has a nursery exam between six and seventy two hours of birth to check for conditions such as heart defects, hip problems, or eye problems.\n\nIn the community, the community midwife sees the woman at least until day ten. This does not mean she sees the woman and baby daily, but she cannot discharge them from her care until day ten at the earliest. Postnatal checks include neonatal screening test (NST, or heel prick test) around day five. The baby is weighed and the midwife plans visits according to the health and needs of mother and baby. They are discharged to the care of the health visitor.\n\nAt birth, the baby receives an Apgar score at, at the least, one minute and five minutes of age. This is a score out of 10 that assesses the baby on five different areas—each worth between 0 and 2 points. These areas are: colour, respiratory effort, tone, heart rate, and response to stimuli. The midwife checks the baby for any obvious problems, weighs the baby, and measure head circumference. The midwife ensures the cord has been clamped securely and the baby has the appropriate name tags on (if in hospital). Babies lengths are not routinely measured. The midwife performs these checks as close to the mother as possible and returns the baby to the mother quickly. Skin-to-skin is encouraged, as this regulates the baby's heart rate, breathing, oxygen saturation, and temperature—and promotes bonding and breastfeeding.\n\nIn some countries, such as Chile, the midwife is the professional who can direct neonatal intensive care units. This is an advantage for these professionals, because this professionals can use the knowledge in perinatology to bring a high quality care of the newborn, with medical or surgical conditions.\n\nMidwifery-led continuity of care is where one or more midwives have the primary responsibility for the continuity of care for childbearing women, with a multidisciplinary network of consultation and referral with other health care providers. This is different from \"medical-led care\" where an obstetrician or family physician is primarily responsible. In \"shared-care\" models, responsibility may be shared between a midwife, an obstetrician and/or a family physician. The midwife plays a very unique role is part of very intimate situations with the mother. For this reason, many say that the most important thing to look for in a midwife is comfortability with them, as one will go to them with every question or problem.\n\nAccording to a Cochrane review of public health systems in Australia, Canada, Ireland, New Zealand and the United Kingdom, \"most women should be offered midwifery-led continuity models of care and women should be encouraged to ask for this option although caution should be exercised in applying this advice to women with substantial medical or obstetric complications.\" Midwifery-led care has effects including the following:\nThere was no difference in the number of Caesarean sections. All trials in the Cochrane review included licensed midwives, and none included lay or traditional midwives. Also, no trial included out of hospital birth.\n\nIn ancient Egypt, midwifery was a recognized female occupation, as attested by the Ebers Papyrus which dates from 1900 to 1550 BCE. Five columns of this papyrus deal with obstetrics and gynecology, especially concerning the acceleration of parturition (the action or process of giving birth to offspring) and the birth prognosis of the newborn. The Westcar papyrus, dated to 1700 BCE, includes instructions for calculating the expected date of confinement and describes different styles of birth chairs. Bas reliefs in the royal birth rooms at Luxor and other temples also attest to the heavy presence of midwifery in this culture.\n\nMidwifery in Greco-Roman antiquity covered a wide range of women, including old women who continued folk medical traditions in the villages of the Roman Empire, trained midwives who garnered their knowledge from a variety of sources, and highly trained women who were considered physicians. However, there were certain characteristics desired in a “good” midwife, as described by the physician Soranus of Ephesus in the 2nd century. He states in his work, \"Gynecology\", that “a suitable person will be literate, with her wits about her, possessed of a good memory, loving work, respectable and generally not unduly handicapped as regards her senses [i.e., sight, smell, hearing], sound of limb, robust, and, according to some people, endowed with long slim fingers and short nails at her fingertips.” Soranus also recommends that the midwife be of sympathetic disposition (although she need not have borne a child herself) and that she keep her hands soft for the comfort of both mother and child. Pliny, another physician from this time, valued nobility and a quiet and inconspicuous disposition in a midwife. There appears to have been three “grades” of midwives present: The first was technically proficient; the second may have read some of the texts on obstetrics and gynecology; but the third was highly trained and reasonably considered a medical specialist with a concentration in midwifery.\n\nAgnodice or Agnodike (Gr. Ἀγνοδίκη) was the earliest historical, and likely apocryphal, midwife mentioned among the ancient Greeks.\n\nMidwives were known by many different titles in antiquity, ranging from \"iatrinē\" (Gr. nurse), \"maia\" (Gr., midwife), \"obstetrix\" (Lat., obstetrician), and \"medica\" (Lat., doctor). It appears as though midwifery was treated differently in the Eastern end of the Mediterranean basin as opposed to the West. In the East, some women advanced beyond the profession of midwife (\"maia\") to that of gynaecologist (\"iatros gynaikeios\", translated as \"women's doctor\"), for which formal training was required. Also, there were some gynecological tracts circulating in the medical and educated circles of the East that were written by women with Greek names, although these women were few in number. Based on these facts, it would appear that midwifery in the East was a respectable profession in which respectable women could earn their livelihoods and enough esteem to publish works read and cited by male physicians. In fact, a number of Roman legal provisions strongly suggest that midwives enjoyed status and remuneration comparable to that of male doctors. One example of such a midwife is Salpe of Lemnos, who wrote on women’s diseases and was mentioned several times in the works of Pliny.\n\nHowever, in the Roman West, information about practicing midwives comes mainly from funerary epitaphs. Two hypotheses are suggested by looking at a small sample of these epitaphs. The first is the midwifery was not a profession to which freeborn women of families that had enjoyed free status of several generations were attracted; therefore it seems that most midwives were of servile origin. Second, since most of these funeral epitaphs describe the women as freed, it can be proposed that midwives were generally valued enough, and earned enough income, to be able to gain their freedom. It is not known from these epitaphs how certain slave women were selected for training as midwives. Slave girls may have been apprenticed, and it is most likely that mothers taught their daughters.\n\nThe actual duties of the midwife in antiquity consisted mainly of assisting in the birthing process, although they may also have helped with other medical problems relating to women when needed. Often, the midwife would call for the assistance of a physician when a more difficult birth was anticipated. In many cases the midwife brought along two or three assistants. In antiquity, it was believed by both midwives and physicians that a normal delivery was made easier when a woman sat upright. Therefore, during parturition, midwives brought a stool to the home where the delivery was to take place. In the seat of the birthstool was a crescent-shaped hole through which the baby would be delivered. The birthstool or chair often had armrests for the mother to grasp during the delivery. Most birthstools or chairs had backs which the patient could press against, but Soranus suggests that in some cases the chairs were backless and an assistant would stand behind the mother to support her. The midwife sat facing the mother, encouraging and supporting her through the birth, perhaps offering instruction on breathing and pushing, sometimes massaging her vaginal opening, and supporting her perineum during the delivery of the baby. The assistants may have helped by pushing downwards on the top of the mother's abdomen.\n\nFinally, the midwife received the infant, placed it in pieces of cloth, cut the umbilical cord, and cleansed the baby. The child was sprinkled with “fine and powdery salt, or natron or aphronitre” to soak up the birth residue, rinsed, and then powdered and rinsed again. Next, the midwives cleared away any and all mucus present from the nose, mouth, ears, or anus. Midwives were encouraged by Soranus to put olive oil in the baby’s eyes to cleanse away any birth residue, and to place a piece of wool soaked in olive oil over the umbilical cord. After the delivery, the midwife made the initial call on whether or not an infant was healthy and fit to rear. She inspected the newborn for congenital deformities and testing its cry to hear whether or not it was robust and hearty. Ultimately, midwives made a determination about the chances for an infant’s survival and likely recommended that a newborn with any severe deformities be exposed.\n\nA 2nd-century terracotta relief from the Ostian tomb of Scribonia Attice, wife of physician-surgeon M. Ulpius Amerimnus, details a childbirth scene. Scribonia was a midwife and the relief shows her in the midst of a delivery. A patient sits in the birth chair, gripping the handles and the midwife’s assistant stands behind her providing support. Scribonia sits on a low stool in front of the woman, modestly looking away while also assisting the delivery by dilating and massaging the vagina, as encouraged by Soranus.\n\nThe services of a midwife were not inexpensive; this fact that suggests poorer women who could not afford the services of a professional midwife often had to make do with female relatives. Many wealthier families had their own midwives. However, the vast majority of women in the Greco-Roman world very likely received their maternity care from hired midwives. They may have been highly trained or possessed only a rudimentary knowledge of obstetrics. Also, many families had a choice of whether or not they wanted to employ a midwife who practiced the traditional folk medicine or the newer methods of professional parturition. Like a lot of other factors in antiquity, quality gynecological care often depended heavily on the socioeconomic status of the patient.\n\nFrom the 18th century, a conflict between surgeons and midwives arose, as medical men began to assert that their modern scientific techniques were better for mothers and infants than the folk medicine practiced by midwives. \nAs doctors and medical associations pushed for a legal monopoly on obstetrical care, midwifery became outlawed or heavily regulated throughout the United States and Canada.. In Northern Europe and Russia the situation was a little easier - in Imperial Russia at the Duchy of Estonia, Professor Christian Friedrich Deutsch established a midwifery school for women at the University of Dorpat in 1811, which existed until World War I. It was the predecessor for the Tartu Health Care College. Training lasted for 7 months and in the end a certificate for practice was issued to the female students. Despite accusations that midwives were \"incompetent and ignorant\", some argued that poorly trained surgeons were far more of a danger to pregnant women. The argument that surgeons were more dangerous than midwives lasted until the study of bacteriology became popular in the early 1900s. Women began to feel safer in the setting of the hospitals with the amount of aid and the ease of birth that they experienced with doctors. “Physicians trained in the new century found a great contrast between their hospital and obstetrics practice in women’s homes where they could not maintain sterile conditions or have trained help.” German social scientists Gunnar Heinsohn and Otto Steiger theorize that midwifery became a target of persecution and repression by public authorities because midwives possessed highly specialized knowledge and skills regarding not only assisting birth, but also contraception and abortion.\n\nAt late 20th century, midwives were already recognized as highly trained and specialized professionals in obstetrics. However, at the beginning of the 21st century, the medical perception of pregnancy and childbirth as potentially pathological and dangerous still dominates Western culture. Midwives who work in hospital settings also have been influenced by this view, although by and large they are trained to view birth as a normal and healthy process. While midwives play a much larger role in the care of pregnant mothers in Europe than in America, the medicalized model of birth still has influence in those countries, even though the World Health Organization recommends a natural, normal and humanized birth.\n\nThe midwifery model of pregnancy and childbirth as a normal and healthy process plays a much larger role in Sweden and the Netherlands than the rest of Europe, however. Swedish midwives stand out, since they administer 80 percent of prenatal care and more than 80 percent of family planning services in Sweden. Midwives in Sweden attend all normal births in public hospitals and Swedish women tend to have fewer interventions in hospitals than American women. The Dutch infant mortality rate in 1992 was the tenth-lowest rate in the world, at 6.3 deaths per thousand births, while the United States ranked twenty-second. Midwives in the Netherlands and Sweden owe a great deal of their success to supportive government policies.\n\nNotes\nBibliography\n\n\n"}
{"id": "25678466", "url": "https://en.wikipedia.org/wiki?curid=25678466", "title": "Monthly nurse", "text": "Monthly nurse\n\nA monthly nurse is a woman who looks after a mother and her baby during the postpartum or postnatal period.\n\nHistorically, women were confined to their beds or their homes for extensive periods after giving birth; care was provided either by her female relatives (mother or mother-in-law), or, for those who could afford it, by the monthly nurse. These weeks were called confinement or lying-in, and ended with the re-introduction of the mother to the community in the Christian ceremony of the churching of women. The term \"monthly nurse\" was most common in 18th and 19th century England.\n\nThe job still exists, although it now might be described as \"postnatal doula\" or \"maternity nurse\" or \"newborn care specialist\" - all specialist sorts of nannies. A modern version of this rest period has evolved, to give maximum support to the new mother, especially if she is recovering from a difficult labour and delivery. It is especially popular in China and its diaspora, where postpartum confinement is known as \"sitting the month\".\n\nFrom long ago, the delivery of children and care of the mothers was a profession often handed down from mother to daughter, with the daughter spending many years as the pupil or apprentice. The Church supported that by a system of licensing, which required midwives to swear to certain rules relating to contraception, abortion and concealment of births and also to deliver the newborn infants for baptism or, in extreme cases, to perform the ceremony themselves.\n\nIn the mid-18th century the legal status of midwives was withdrawn and the responsibility for delivery was vested in the surgeon. The work of the nurse element had to be covered, as \"who was to look after the baby?\" Clearly, the first thought that would naturally occur to a mother was that the best person to look after her baby was a woman who had had one herself. Often, the task was allotted to motherly or grandmotherly hands and, from that requirement for postnatal care, the monthly nurse originated. \"The Nursing Record\" reported that \"there was little or no attempt at knowledge or instruction, and we know as a fact that ignorance, prejudice and neglect resulted in a goodly crop of errors, wrongs, and woes as regards the hapless infant\".\n\nThe term \"monthly nurse\" is one that is frequently used to describe the nurse who cares for lying-in cases, certainly because such a nurse frequently remains with the patient for four weeks. The term \"monthly\" is somewhat inaccurate, as there is no reason for the nurse's services to be dispensed with after ten days or retained for much longer, but it is entirely a matter of arrangement.\n\n\"The Nursing Record\" reported that \"nurses who attend the 'artisan' classes in their confinements as a rule pay a visit daily for ten days and then give up the case, as few working class mothers can afford to lie up for longer\".\n\nA monthly nurse could earn more than a midwife, as the monthly nurse was employed for periods between 10 days and often much longer and might attend several women on a part time basis. She often \"lived in\". The midwife's only duty was perceived as \"being trained to assist the parturient woman while nature does her own work and able to call upon a surgeon who could step in where nature fails and skill and science are required\". Many certified midwives transferred to the ranks of monthly nurses to benefit from an increased income.\n\nAlthough 'registration' was not available for women to act as midwives or monthly nurses a system of 'certification' was in being in the late 19th century and continued into the early 20th century. To qualify, a candidate monthly nurse would attend a course in a lying-in hospital for four or five weeks and a midwife for up to three months. The prospective midwives and monthly nurses, as a rule, paid their own charges in respect of hospital expenses and then entered practice on their own responsibility. In 1893, a Miss Gosling reported that \"although the certificated monthly nurse could be relied upon as being trustworthy and efficient, there were a number of women who attend lectures for a short time and through one cause or another fail to pass their examination and obtain a certificate nevertheless enter a 'Nurses Home' or open one for themselves\".\n\nAs might be expected rogue institutions issued certificates and diplomas “for a price”. Another that reporting on a lying in hospital and signed herself a ‘victim of the system’ said that she “witnessed the first phase of the system which turns out yearly hundreds of midwives and monthly nurses on an unsuspecting public. These would be nurses represented almost every grade of the lower classes and every degree of lack of education, and one woman, I remember could not write. Personally I found many to be dishonest, untruthful, indescribably dirty in their habits and persons, utterly unprincipled, shockingly coarse and deficient intelligence, and with not the faintest idea of discipline”’\n\nIn the late 19th century, reformers were calling not only for registration and recognition of the profession of midwife but also for the two functions of midwife and monthly nurse to be amalgamated: \"The work of midwives lies, for the most part, amongst the poor and the poor lying-in woman needs not only to be delivered, but to be visited for some ten days subsequent to her confinement\". The registration of midwives was opposed by members of the House of Lords and Parliament for many years, who argued that the delivery of infants was the responsibility of trained doctors and to allow women to do the job, even in straightforward cases, would take away doctors' income. It was not until the Midwives Act 1902, following 12 years of representation by women, that midwives were \"registered\", but it would still take several years for it to be accepted. The professional training and formal qualification of midwives, and eventually, the postnatal care offered by the National Health Service, saw the end of the monthly nurse.\n\n\n"}
{"id": "57070169", "url": "https://en.wikipedia.org/wiki?curid=57070169", "title": "Notifiable diseases in USA", "text": "Notifiable diseases in USA\n\nIn USA, the National Notifiable Disease Surveillance System (NNDSS) is responsible for sharing information regarding notifiable diseases. As of 2017, the following are the notifiable diseases in United States of America as mandated by the Centers for Disease Control and Prevention :\n\n"}
{"id": "29829829", "url": "https://en.wikipedia.org/wiki?curid=29829829", "title": "Nutritional challenges of HIV/AIDS", "text": "Nutritional challenges of HIV/AIDS\n\nPeople living with HIV/AIDS face increased challenges in maintaining proper nutrition. Despite developments in medical treatment, nutrition remains a key component in managing this condition. The challenges that those living with HIV/AIDS face can be the result of the viral infection itself or from the effects of anti-HIV therapy (HAART).\n\nSome of the side effects from HAART that may affect how the body absorbs and utilizes nutrients include fatigue, nausea, and poor appetite.\nAs well, the nutritional needs of people with HIV/AIDS are greater due to their immune system fighting off opportunistic infections that do not normally cause disease in people with healthy immune systems. Medication along with proper nutrition is a major component of maintaining good health and quality of life for people living with HIV/AIDS.\n\nMonitoring caloric intake is important in ensuring that energy needs are met. For people with HIV/AIDS, energy requirements often increase in order to maintain their regular body weight. A classification system revised by the Centers for Disease Control and Prevention (CDC), categorizes HIV-infection into 3 clinical stages and addresses the suggested caloric requirements for each stage.\n\nThe World Health Organization (WHO) issued consultative recommendations regarding nutrient requirements in HIV/AIDS. A generally healthy diet was promoted. For HIV-infected adults, the WHO recommended micronutrient intake comes from a good diet at RDA levels; higher intake of vitamin A, zinc, and iron can produce adverse effects in HIV positive adults, and these were not recommended unless there is documented deficiency.\n\nDespite the WHO recommendations, recent reviews have highlighted the absence of a simple consensus regarding the effects of multivitamins or micronutrient and nutrient supplementation on HIV positive individuals. This is partly due to a lack of strong scientific evidence.\n\nSome studies have looked into the use of implementing daily multivitamins into the diet regimens of HIV/AIDS patients. One study done in Tanzania involved a trial group with one thousand HIV positive pregnant women. Findings showed that daily multivitamins benefited both the mothers and their babies. After four years, the multivitamins were found to reduce the women’s risk of AIDS and death by approximately 30%. Another trial in Thailand revealed that the use of multivitamins led to fewer deaths, but only among people in advanced stages of HIV. However, not all studies have provided a positive correlation. A small trial done in Zambia found no benefits from multivitamins after one month of use.\n\nRegarding individual vitamin and mineral supplementation, research shows mixed results. Vitamin A supplementation has been shown to reduce mortality and morbidity rates among African children suffering from HIV. The World Health Organization (WHO) recommends vitamin A supplements for all young children 6 to 59 months old that are at high risk of vitamin A deficiency every 4 to 6 months. In contrast, a trial from Tanzania found that the use of vitamin A supplements increased the risk of mother-to-child transmission by 40%. With the inconsistency of these results, scientists have not reached a consensus regarding Vitamin A supplementation and its possible benefits for HIV/AIDS patients. Other vitamins to be taken by HIV-infected adults are vitamins C and E.\n\nEvidence for supplementation with selenium is mixed with some tentative evidence of benefit. There is some evidence that vitamin A supplementation in children reduces mortality and improves growth. For nutritionally compromised pregnant and lactating women, a multivitamin supplementation has improved outcomes for both mothers and children.\n\nThus, further research is required to determine the relationship between supplements and HIV/AIDS in order to develop effective nutritional interventions.\n\nStatistics show that HIV/AIDS is most prevalent in the Sub-Saharan African region. And according to the hunger map of 2010, undernourishment is most prevalent in Asia-Pacific and, once again, in Sub-Saharan Africa. Some of the reasons as to why there is a correlation between malnutrition and the presence of HIV/AIDS are listed below.\n\nFood security is present \"when all people at all times have access to sufficient, safe, nutritious food to maintain a healthy and active life\", as defined by the World Food Summit of 1996. It is set on the basis of food availability, food access, and proper food use. The difficulty for some people suffering from HIV and AIDS involves how they must obtain food security because the virus increases fatigue, compromising their ability to work in order to provide food and food preparation. This impact is even greater on those living in poverty in rural areas, where providing food is largely based on farming and other household chores.\n\nIn Malawi, the AIDS epidemic and its effect on food security for 65 rural households was studied. Findings revealed that illness and death as a result of AIDS compromised household food security. 24 out of 65 households had negative food security indexes, meaning that they did not have enough flour and/or corn to meet their daily caloric requirements. Sharing of food and resources was substantially decreased to those families that were affected with HIV/AIDS, which proved especially detrimental in families that no longer had a prime adult. With the removal of a prime adult, these families were deprived of productive labor resulting in serious production deficiencies. The study concludes that family members victimized by AIDS led to fracturing of interpersonal ties with other families which created food insecurity in a community based on a rural subsistence economy.\n\nMalnutrition can be used as a measure of food insecurity and it has the most impact on those living in Sub-Saharan Africa and Asia-Pacific, where poverty and malnutrition is most prevalent in the world. It is also in Sub-Saharan Africa where the cases of HIV/AIDS is most prevalent. An individual whose body is already compromised with HIV has an immune system even less effective at defending against infections when the body is malnourished. Anti-retroviral drugs are now being distributed to people in these areas but when given to those who are undernourished, the medicine’s effectiveness is decreased and toxicity is increased. Malnutrition accelerates the onset of the disease and give rise to repeated illnesses because of their weakened immune systems. Consequently, HIV and malnutrition provide a cyclic form of feedback for each other, with worsening conditions of malnutrition being linked to a more rapid onset of HIV.\n\n\"For other uses, see food safety\"\n\nPeople living with AIDS have impaired immune systems and therefore are more susceptible to infections and diseases due to foodborne pathogens. Food safety includes food handling, food preparation and food storage, all to be dealt with carefully to ensure safety from food-borne bacteria. Those that are more prevalent in people with AIDS include Salmonella, which is the most common cause of illness, being 100 times more prevalent in AIDS patients than healthy individuals. Another example is Listeriosis caused by \"listeria monocytogenes\", with severe and often fatal consequences when encountered by a persons with AIDS. Simple measures can be used to increase food safety and prevent food-borne illnesses for those with affected with HIV/AIDS. Washing your hands, the food about to be prepared, kitchen utensils and kitchen surfaces is effective against bacterial growth. Keeping raw meat and cooked meat separate and cooking foods thoroughly, using a food thermometer to be sure. And lastly, storing leftover foods in the refrigerator within two hours to ensure minimal risk of food-borne illnesses.\n\nThere are some areas of research that have either not been explored or researched in-depth that are related to the area of HIV/AIDS treatment.\n\nWith food security also having an impact in rural areas affected by HIV/AIDS, another area of research involves agriculture in subsistence economies. More specifically, in these areas where households affected by HIV/AIDS may suffer from food insecurity, focuses for future research may include looking into ways to improve agricultural practices in order to enhance the household food production of families where one or more adults suffer from HIV/AIDS.\n"}
{"id": "52965", "url": "https://en.wikipedia.org/wiki?curid=52965", "title": "Obstetrics", "text": "Obstetrics\n\nObstetrics is the field of study concentrated on pregnancy, childbirth, and the postpartum period. As a medical specialty, obstetrics is combined with gynecology under the discipline known as obstetrics and gynecology (OB/GYN) which is a surgical field. \n\nPrenatal care is important in screening for various complications of pregnancy. This includes routine office visits with physical exams and routine lab tests:\n\n\nGenetic screening for Down syndrome (trisomy 21) and trisomy 18, the national standard in the United States, is rapidly evolving away from the AFP-Quad screen for Down syndrome, done typically in the second trimester at 16–18 weeks. The newer integrated screen (formerly called F.A.S.T.E.R for First And Second Trimester Early Results) can be done at 10 plus weeks to 13 plus weeks with an ultrasound of the fetal neck (thicker nuchal skin correlates with higher risk of Down syndrome being present) and two chemicals (analytes) PAPP-A and βHCG (pregnancy hormone level itself). It gives an accurate risk profile very early. A second blood screen at 15 to 20 weeks refines the risk more accurately. The cost is higher than an \"AFP-quad\" screen due to the ultrasound and second blood test, but it is quoted to have a 93% pick up rate as opposed to 88% for the standard AFP/QS. This is an evolving standard of care in the United States.\n\n\nMost doctors do a sugar load in a drink form of 50 grams of glucose in cola, lime or orange and draw blood an hour later (plus or minus 5 minutes) ; the standard modified criteria have been lowered to 135 since the late 1980s\n\nObstetric ultrasonography is routinely used for dating the gestational age of a pregnancy from the size of the fetus, determine the number of fetuses and placentae, evaluate for an ectopic pregnancy and first trimester bleeding, the most accurate dating being in first trimester before the growth of the foetus has been significantly influenced by other factors. Ultrasound is also used for detecting congenital anomalies (or other foetal anomalies) and determining the biophysical profiles (BPP), which are generally easier to detect in the second trimester when the foetal structures are larger and more developed. Specialised ultrasound equipment can also evaluate the blood flow velocity in the umbilical cord, looking to detect a decrease/absence/reversal or diastolic blood flow in the umbilical artery.\n\nX-rays and computerized tomography (CT) are not used, especially in the first trimester, due to the ionizing radiation, which has teratogenic effects on the foetus. No effects of magnetic resonance imaging (MRI) on the foetus have been demonstrated, but this technique is too expensive for routine observation. Instead, obstetric ultrasonography is the imaging method of choice in the first trimester and throughout the pregnancy, because it emits no radiation, is portable, and allows for realtime imaging.\n\nThe safety of frequent ultrasound scanning has not be confirmed. Despite this, increasing numbers of women are choosing to have additional scans for no medical purpose, such as gender scans, 3D and 4D scans. A normal gestation would reveal a gestational sac, yolk sac, and fetal pole. The gestational age can be assessed by evaluating the mean gestational sac diameter (MGD) before week 6, and the crown-rump length after week 6. Multiple gestation is evaluated by the number of placentae and amniotic sacs present.\n\nOther tools used for assessment include:\n\nA pregnant woman may have intercurrent diseases, that is, other diseases or conditions (not directly caused by the pregnancy) that may become worse or be a potential risk to the pregnancy.\n\n\nInduction is a method of artificially or prematurely stimulating labour in a woman. Reasons to induce can include pre-eclampsia, foetal distress, placental malfunction, intrauterine growth retardation and failure to progress through labour increasing the risk of infection and foetal distresses.\n\nInduction may be achieved via several methods:\n\nDuring labour, the obstetrician carries out the following tasks:\n\nThe main emergencies include:\n\n\nPostnatal care is care provided to the mother following parturition.\n\nA woman in the Western world who is delivering in a hospital may leave the hospital as soon as she is medically stable and chooses to leave, which can be as early as a few hours postpartum, though the average for spontaneous vaginal delivery (SVD) is 1–2 days, and the average caesarean section postnatal stay is 3–4 days.\n\nDuring this time the mother is monitored for bleeding, bowel and bladder function, and baby care. The infant's health is also monitored.\n\nCertain things must be kept in mind as the physician proceeds with the post-natal care.\n\nPrior to the 18th century, caring for pregnant women in Europe was confined exclusively to women, and rigorously excluded men. The expectant mother would invite close female friends and family members to her home to keep her company during childbirth. Skilled midwives managed all aspects of the labour and delivery. The presence of physicians and surgeons was very rare and only occurred if a serious complication had taken place and the midwife had exhausted all measures at her disposal. Calling a surgeon was very much a last resort and having men deliver women in this era was seen as offending female modesty.\n\nPrior to the 18th and 19th centuries, midwifery was well established but obstetrics was not recognized as a specific medical specialty. However, the subject matter and interest in the female reproductive system and sexual practice can be traced back to Ancient Egypt and Ancient Greece. Soranus of Ephesus sometimes is called the most important figure in ancient gynecology. Living in the late first century A.D. and early second century, he studied anatomy and had opinions and techniques on abortion, contraception –most notably \"coitus interruptus\"– and birth complications. After his death, techniques and works of gynecology declined; very little of his works were recorded and survived to the late 18th century when gynecology and obstetrics reemerged as a medical specialism.\n\nThe 18th century marked the beginning of many advances in European midwifery, based on better knowledge of the physiology of pregnancy and labour. By the end of the century, medical professionals began to understand the anatomy of the uterus and the physiological changes that take place during labour. The introduction of forceps in childbirth also took place at this time. All these medical advances in obstetrics were a lever for the introduction of men into an arena previously managed and run by women—midwifery.\n\nThe addition of the male-midwife (or man-midwife) is historically a significant change to the profession of obstetrics. In the 18th century medical men began to train in area of childbirth and believed with their advanced knowledge in anatomy that childbirth could be improved. In France these male-midwives were referred to as \"accoucheurs\", a title later used all over Europe. The founding of lying-in hospitals also contributed to the medicalization and male-dominance of obstetrics. These early maternity hospitals were establishments where women would come to have their babies delivered, as opposed to the practice since time immemorial of the midwife attending the home of the woman in labour. This institution provided male-midwives with endless patients to practice their techniques on and was a way for these men to demonstrate their knowledge.\n\nMany midwives of the time bitterly opposed the involvement of men in childbirth. Some male practitioners also opposed the involvement of medical men like themselves in midwifery and even went as far as to say that male-midwives only undertook midwifery solely for perverse erotic satisfaction. The accoucheurs argued that their involvement in midwifery was to improve the process of childbirth. These men also believed that obstetrics would forge ahead and continue to strengthen.\n\n18th century physicians expected that obstetrics would continue to grow, but the opposite happened. Obstetrics entered a stage of stagnation in the 19th century, which lasted until about the 1880s. The central explanation for the lack of advancement during this time was the rejection of obstetrics by the medical community. The 19th century marked an era of medical reform in Europe and increased regulation over the profession. Major European institutions such as The College of Physicians and Surgeons considered delivering babies ungentlemanly work and refused to have anything to do with childbirth as a whole. Even when Medical Act 1858 was introduced, which stated that medical students could qualify as doctors, midwifery was entirely ignored. This made it nearly impossible to pursue an education in midwifery and also have the recognition of being a doctor or surgeon. Obstetrics was pushed to the side.\n\nBy the late 19th century, the foundation of modern-day obstetrics and midwifery began developing. Delivery of babies by doctors became popular and readily accepted, but midwives continued to play a role in childbirth. Midwifery also changed during this era due to increased regulation and the eventual need for midwives to become certified. Many European countries by the late 19th century were monitoring the training of midwives and issued certification based on competency. Midwives were no longer uneducated in the formal sense.\n\nAs midwifery began to develop, so did the profession of obstetrics near the end of the century. Childbirth was no longer unjustifiably despised by the medical community as it once had been at the beginning of the century. But obstetrics was underdeveloped compared to other medical specialites. Many male physicians would deliver children but very few would have referred to themselves as obstetricians. The end of the 19th century did mark a significant accomplishment in the profession with the advancements in asepsis and anaesthesia, which paved the way for the mainstream introduction and later success of the Caesarean Section.\n\nBefore the 1880s mortality rates in lying-hospitals would reach unacceptably high levels and became an area of public concern. Much of these maternal deaths were due to puerperal fever, then known as childbed fever. In the 1800s Dr. Ignaz Semmelweis noticed that women giving birth at home had a much lower incidence of childbed fever than those giving birth by physicians in lying-hospitals. His investigation discovered that washing hands with an antiseptic solution before a delivery reduced childbed fever fatalities by 90%. So it was concluded that it was physicians who had been spreading disease from one labouring mother to the next. Despite the publication of this information, doctors still would not wash. It was not until the 20th century when advancements in aseptic technique and the understanding of disease would play a significant role in the decrease of maternal mortality rates among many populations.\n\nThe development of obstetrics as a practice for accredited doctors happened at the turn of the 18th century and thus was very differently developed in Europe and in the Americas due to the independence of many countries in the Americas from European powers. “Unlike in Europe and the British Isles, where midwifery laws were national, in America, midwifery laws were local and varied widely”.\n\nGynaecology and Obstetrics gained attention in the American medical field at the end of the nineteenth century through the development of such procedures as the ovariotomy. These procedures then were shared with European surgeons who replicated the surgeries. This was a period when antiseptic, aseptic or anaesthetic measures were just being introduced to surgical and observational procedures and without these procedures surgeries were dangerous and often fatal. Following are two surgeons noted for their contributions to these fields include Ephraim McDowell and James Marion Sims.\n\nEphraim McDowell developed a surgical practice in 1795 and performed the first ovariotomy in 1809 on a 47-year-old widow who then lived on for thirty-one more years. He had attempted to share this with John Bell whom he had practiced under who had retired to Italy. Bell was said to have died without seeing the document but it was published by an associate in \"Extractions of Diseased Ovaria\" in 1825. By the mid-century the surgery was both successfully and unsuccessfully being performed. Pennsylvanian surgeons the Attlee brothers made this procedure very routine for a total of 465 surgeries–John Attlee performed 64 successfully of 78 while his brother William reported 387– between the years of 1843 and 1883. By the middle of the nineteenth century this procedure was successfully performed in Europe by English surgeons Sir Spencer Wells and Charles Clay as well as French surgeons Eugène Koeberlé, Auguste Nélaton and Jules Péan.\n\nJ. Marion Sims was the surgeon responsible for being the first treating a vesicovaginal fistula –a condition linked to many caused mainly by prolonged pressing of the feotus against the pelvis or other causes such as rape, hysterectomy, or other operations– and also having been doctor to many European royals and the 20th President of the United States James A. Garfield after he had been shot. Sims does have a controversial medical past. Under the beliefs at the time about pain and the prejudice towards African people, he had practiced his surgical skills and developed skills on slaves. These women were the first patients of modern gynecology. One of the women he operated on was named Anarcha Westcott, the woman he first treated for a fistula.\n\nWomen and men inhabited very different roles in natal care up to the 18th century. The role of a physician was exclusively held by men who went to university, an overly male institution, who would theorize anatomy and the process of reproduction based on theological teaching and philosophy. Many beliefs about the female body and menstruation in the 17th and 18th centuries were inaccurate; clearly resulting from the lack of literature about the practice. Many of the theories of what caused menstruation prevailed from Hippocratic philosophy. Midwives of this time were those assisted in the birth and care of both born and unborn children, and as the name suggests, this position was held mainly by women.\n\nDuring the birth of a child, men were rarely present. Women from the neighbourhood or family would join in on the process of birth and assist in many different ways. The one position where men would help with the birth of a child would be in the sitting position, usually when performed on the side of a bed to support the mother.\n\nMen were introduced into the field of obstetrics in the nineteenth century and resulted in a change of the focus of this profession. Gynaecology directly resulted as a new and separate field of study from obstetrics and focused on the curing of illness and indispositions of female sexual organs. This had some relevance to some conditions as menopause, uterine and cervical problems, and childbirth could leave the mother in need of extensive surgery to repair tissue. But, there was also a large blame of the uterus for completely unrelated conditions. This led to many social consequences of the nineteenth century.\n\n"}
{"id": "50799575", "url": "https://en.wikipedia.org/wiki?curid=50799575", "title": "Occupational epidemiology", "text": "Occupational epidemiology\n\nOccupational epidemiology is a subdiscipline of epidemiology that focuses on investigations of workers and the workplace. Occupational epidemiologic studies examine health outcomes among workers, and their potential association with conditions in the workplace including noise, chemicals, heat, or radiation, or work organization such as schedules.\n\nThe need for evidence to inform occupational safety regulations, workers' compensation programs, and safety legislation motivated the development of public health policy, occupational epidemiology methods, and surveillance mechanisms. Occupational epidemiological research can inform risk assessments; development of standards and other risk management activities; and estimates of the co-benefits and co-harms of policies designed to reduce risk factors or conditions that can affect human health. Occupational epidemiology methods are common to methods used in environmental epidemiology.\n\nOccupational hazards have long been recognized. For example, Hippocrates recommended other physicians consider patients' vocational backgrounds when diagnosing and treating disease, and Bernardino Ramazzini in 1700 outlined many occupational diseases in his book \"De Morbis Artificum\". There are several examples from the 19th century onwards of hazard recognition proceeding to systematic epidemiology studies. In one example, premature mortality was reported among gold and silver miners in the Erz Mountains in Germany as early as the 16th century. It was initially though to be the result of consumption, but it was subsequently determined to be silicosis, and studies from 1879 through the 1930s uncovered the association of miners' deaths with lung cancer and nonmalignant respiratory diseases. Other examples include cancer among chimney sweeps, asbestos-related diseases, and the variety of occupational diseases found among factory workers in the early 1900s.\n\nOccupational health risks were initially observed by case series reports of apparent disease excesses or clusters. Although the case series approach provided a good indicator of occupational hazards, they are not adequate on their own to assess a wide spectrum of health outcomes that may not be closely related to workplace exposure. The development of retrospective, cohort design allowed for a more comprehensive study of the cases. Desire to improve the cost-efficiency of studies led to the use of case-control studies. Other methods later used in occupational epidemiology include cross-sectional and longitudinal studies.\n\nTypically occupational epidemiological investigations begin with the observation of an unusual number of cases of disease among a group of workers. When the investigation does not go further than what is referred to as identifying a disease cluster, the study is referred to as a case series report.\n\nIn a cohort design study, a population, or cohort, of workers is compared to a control group that was not exposed to the workplace hazards being investigated. This type of study is the most accepted in the scientific community because it most closely follows experimental strategy and observes the entire population rather than a sample. In a prospective cohort study, the group examined at the time of the study is compared to a follow up with the same group in the future. The historical cohort study design begins with defining a cohort at a time in the past and following the cohort over historical time.\n\nCase-control studies compare the past exposure of cases with the disease to the past exposure of cases that did not have the disease. Because cohort studies require the entire population, case-control studies are a more cost-effective approach, using only the sample of workers with the disease to compare to a control.\n\nA typical cross-sectional study involves the comparison of varying degrees of exposure and the prevalence of disease, symptoms, or physiological status. The main advantage of cross-sectional studies is that they allow collection of data on conditions which would not be recorded normally because other study designs focus on severe states of disease. This is also the biggest shortcoming of this study type because by using prevalence rather than incidence it cannot be used to make a causal inference.\n\nBy contributing to reduction in exposure, occupational epidemiology helps reduce health risks among workers. Using occupational epidemiological methods can also have benefits for society at large. For example, recommendations for exposure limits to benzene developed by the Expert Panel on Air Quality Standards were based on occupational epidemiology.\n\nUsing meta-analysis, many occupational epidemiology studies can be synthesized in order to help set occupational exposure limits and make other kinds of policy decisions. This can also can be applied in health risk assessments, which is a method of predicting health risk based on hypothetical exposure conditions.\n\n\n"}
{"id": "18079605", "url": "https://en.wikipedia.org/wiki?curid=18079605", "title": "Occupational health psychology", "text": "Occupational health psychology\n\nOccupational health psychology (OHP) is an interdisciplinary area of psychology that is concerned with the health and safety of workers. OHP addresses a number of major topic areas including the impact of occupational stressors on physical and mental health, the impact of involuntary unemployment on physical and mental health, work-family balance, workplace violence and other forms of mistreatment, accidents and safety, and interventions designed to improve/protect worker health. OHP emerged from two distinct disciplines within applied psychology, namely, health psychology and industrial and organizational psychology, as well as occupational medicine. OHP has also been informed by other disciplines including industrial sociology, industrial engineering, and economics, as well as preventive medicine and public health. OHP is concerned with the relationship of psychosocial workplace factors to the development, maintenance, and promotion of workers' health and that of their families. Thus the field's focus is work-related factors that can lead to injury, disease, and distress.\n\nThe Industrial Revolution prompted thinkers, such as Karl Marx with his theory of alienation, to concern themselves with the nature of work and its impact on workers. Taylor's (1911) \"Principles of Scientific Management\" as well as Mayo’s research in the late 1920s and early 1930s on workers at the Hawthorne Western Electric plant helped to inject the impact of work on workers into the subject matter psychology addresses. About the time Taylorism arose, Hartness reconsidered worker-machine interaction and its impact on worker psychology. The creation in 1948 of the Institute for Social Research (ISR) at the University of Michigan was important because of its research on occupational stress and employee health.\n\nResearch in the U.K. by Trist and Bamforth (1951) suggested the reduction in autonomy that accompanied organizational changes in English coal mining operations adversely affected worker morale. Arthur Kornhauser’s work in the early 1960s on the mental health of automobile workers in Michigan also contributed to the development of the field. \nA 1971 study by Gardell examined the impact of work organization on mental health in Swedish pulp and paper mill workers and engineers. Research on the impact of unemployment on mental health was conducted at the University of Sheffield’s Institute of Work Psychology. In 1970 Kasl and Cobb documented the impact of unemployment on blood pressure in U.S. factory workers.\n\nA number of individuals are associated with the creation of the term “occupational health psychology” or \"occupational health psychologist.\" They include Ferguson (1977), Feldman (1985), Everly (1986), and Raymond, Wood, and Patrick (1990). In 1988, in response to a dramatic increase in the number of stress-related worker compensation claims in the U.S., the National Institute for Occupational Safety and Health (NIOSH) \"recognized stress-related psychological disorders as a leading occupational health risk\" (p. 201). When this change was coupled with an increased recognition of the impact of stress on a range of problems in the workplace, NIOSH found that their stress-related programs were significantly increasing in prominence. In 1990, Raymond et al. argued that the time has come for doctoral-level psychologists to get interdisciplinary OHP training, integrating health psychology with public health, because creating healthy workplaces should be a goal for the field.\n\nEstablished in 1987, \"Work & Stress\" is the first and \"longest established journal in the fast developing discipline that is occupational health psychology\" (p. 1). Three years later, the American Psychological Association (APA) and NIOSH jointly organized the first international Work, Stress, and Health conference in Washington, DC. The conference has since become a biannual OHP meeting. In 1996, the first issue of the \"Journal of Occupational Health Psychology\" was published by APA. That same year, the International Commission on Occupational Health created the Work Organisation and Psychosocial Factors (ICOH-WOPS) scientific committee, which focused primarily on OHP. In 1999, the European Academy of Occupational Health Psychology (EA-OHP) was established at the first European Workshop on Occupational Health Psychology in Lund, Sweden. That workshop is considered to be the first EA-OHP conference, the first of a continuing series of conferences EA-OHP organizes and devotes to OHP research and practice.\n\nIn 2000 the informal International Coordinating Group for Occupational Health Psychology (ICGOHP) was founded for the purpose of facilitating OHP-related research, education, and practice as well as coordinating international conference scheduling. Also in 2000, \"Work & Stress\" became associated with the EA-OHP. In 2005, the Society for Occupational Health Psychology (SOHP) was established in the United States. In 2008, SOHP joined with APA and NIOSH in co-sponsoring the Work, Stress, and Health conferences. In addition, EA-OHP and SOHP began to coordinate biennial conferences schedules such that the organizations' conferences would take place on alternate years, minimizing scheduling conflicts. In 2017, SOHP and Springer began to publish an OHP-related journal \"Occupational Health Science\".\n\nThe main purpose of OHP research is to understand how working conditions affect worker health, use that knowledge to design interventions to protect and improve worker health, and evaluate the effectiveness of such interventions. The research methods used in OHP are similar to those used in other branches of psychology.\n\nSelf-report survey methodology is the most used approach in OHP research. Cross-sectional designs are commonly used; case-control designs have been employed much less frequently. Longitudinal designs including prospective cohort studies and experience sampling studies can examine relationships over time. OHP-related research devoted to evaluating health-promoting workplace interventions has relied on quasi-experimental designs and, less commonly, experimental approaches.\n\nStatistical methods commonly used in other areas of psychology are also used in OHP-related research. Statistical methods used include structural equation modeling and hierarchical linear modeling (HLM; also known as multilevel modeling). HLM can better adjust for similarities between employees and is especially well suited to evaluating the lagged impact of work stressors on health outcomes; in this research context HLM can help minimize censoring and is well-suited to experience sampling studies. Meta-analyses have been used to aggregate data (modern approaches to meta-analyses rely on HLM), and draw conclusions across multiple studies.\n\nQualitative research methods include interviews, focus groups, and self-reported, written descriptions of stressful incidents at work. First-hand observation of workers on the job has also been used, as has participant observation.\n\nThree influential theoretical models in OHP research are the demand-control-support, demand-resources, and effort-reward imbalance models.\n\nThe most influential model in OHP research has been the original demand-control model. According to the model, the combination of low levels of work-related decision latitude (i.e., autonomy and control over the job) combined with high workloads (high levels of work demands) can be particularly harmful to workers (they can lead to \"job strain,\" a term representing the combination of low decision latitude and high workload leading to poorer mental or physical health). The model suggests not only that these two job factors are related to poorer health but that high levels of decision latitude on the job will buffer or reduce the adverse health impact of high levels of demands. Research has clearly supported the idea that decision latitude and demands relate to strains, but research findings about buffering have been mixed with only some studies providing support. The demand-control model asserts that job control can come in two broad forms: ‘skill discretion’ and ‘decision authority’. Skill discretion refers to the level of skill and creativity required on the job and the flexibility an employee is permitted in deciding what skills to use (e.g. opportunity to use skills, similar to job variety). Decision authority refers to employees being able to make decisions about their work (e.g., having autonomy). These two forms of job control are traditionally assessed together in a composite measure of decision latitude; there is, however, some evidence that the two types of job control may not be similarly related to health outcomes.\n\nAbout a decade after Karasek first introduced the demand-control model, Johnson, Hall, and Theorell (1989), in the context of research on heart disease, extended the model to include social isolation. Johnson et al. labeled the combination of high levels of demands, low levels of control, and low levels of coworker support “iso-strain.” The resulting expanded model has been labeled the demand–control–support (DCS) model. Research that followed the development of this model has suggested that one or more of the components of the DCS model (high psychological workload, low control, and lack of social support), if not the exact combination represented by iso-strain, have adverse effects of physical and mental health.\n\nAn alternative model, the job demands-resources (JD-R) model, grew out of the DCS model. In the JD-R model, the category of demands (workload) remains more or less the same as in the DCS model although the JD-R model more specifically includes physical demands. Resources, however, are defined as job-relevant features that help workers achieve work-related goals, lessen job demands, or stimulate personal growth. Control and support as per the DCS model are subsumed under resources. Resources can be external (provided by the organization) or internal (part of a worker's personal make-up). In addition to control and support, resources encompassed by the model can also include physical equipment, software, performance feedback from supervisors, the worker's own coping strategies, etc. There has not, however, been as much research on the JD-R model as there has been on the constituents of the DC or DCS model.\n\nAfter the DCS model, the, perhaps, second most influential model in OHP research has been the effort-reward imbalance (ERI) model. It links job demands to the rewards employees receive for the job. That model holds that high work-related effort coupled with low control over job-related intrinsic (e.g., recognition) and extrinsic (e.g., pay) rewards triggers high levels of activation in neurohormonal pathways that, cumulatively, are thought to exert adverse effects on mental and physical health.\n\nA number of work-related, psychosocial factors have been linked to cardiovascular disease (CVD).\n\nResearch has identified health-behavioral and biological factors that are related to increased risk for CVD. These risk factors include smoking, obesity, low density lipoprotein (the \"bad\" cholesterol), lack of exercise, and blood pressure. Psychosocial working conditions are also risk factors for CVD. In a case-control study involving two large U.S. data sets, Murphy (1991) found that hazardous work situations, jobs that required vigilance and responsibility for others, and work that required attention to devices were related to increased risk for cardiovascular disability. These included jobs in transportation (e.g., air traffic controllers, airline pilots, bus drivers, locomotive engineers, truck drivers), preschool teachers, and craftsmen. Among 30 studies involving men and women, most have found an association between workplace stressors and CVD.\n\nFredikson, Sundin, and Frankenhaeuser (1985) found that reactions to psychological stressors include increased activity in the brain axes which play an important role in the regulation of blood pressure, particularly ambulatory blood pressure. A meta-analysis and systematic review involving 29 samples linked job strain to elevated ambulatory blood pressure. Belkić et al. (2000) found that many of the 30 studies covered in their review revealed that decision latitude and psychological workload exerted independent effects on CVD; two studies found synergistic effects, consistent with the strictest version of the demand-control model. A review of 17 longitudinal studies having reasonably high internal validity found that 8 showed a significant relation between the combination of low levels of decision latitude and high workload (the job strain condition) and CVD and 3 more showed a nonsignificant relation. The findings, however, were clearer for men than for women, on whom data were more sparse. Fishta and Backé's review-of-reviews also links work-related psychosocial stress to elevated risk of CVD in men. In a massive (\"n\" > 197,000) longitudinal study that combined data from 13 independent studies, Kivimäki et al. (2012) found that, controlling for other risk factors, the combination of high levels of demands and low control at baseline increased the risk of CVD in initially healthy workers by between 20 and 30% over a follow-up period that averaged 7.5 years. In this study the effects were similar for men and women. Meta-analytic research also links job strain (the combination of high demands and low control) to stroke.\n\nThere is evidence that, consistent with the ERI model, high work-related effort coupled with low control over job-related rewards adversely affects cardiovascular health. At least five studies of men have linked effort-reward imbalance with CVD. Another large study links ERI to the incidence of coronary disease.\n\nThere is evidence from a prospective study that job-related burnout, controlling for traditional risk factors, such as smoking and hypertension, increases the risk of coronary heart disease over the course of the next three and a half years in workers who were initially disease-free.\n\nResearch has suggested that job loss adversely affects cardiovascular health as well as health in general.\n\nMusculoskeletal disorders (MSDs) involve injury and pain to the joints and muscles of the body. Approximately 2.5 million workers in the US suffer from MSDs, which is the third most common cause of disability and early retirement for American workers. In Europe MSDs are the most often reported workplace health problem. The development of musculoskelelatal problems cannot be solely explained in the basis of biomechanical factors (e.g., repetitive motion) although such factors are important contributors. There has been evidence that psychosocial workplace factors (e.g., job strain) also contribute to the development of musculoskeletal problems. Systematic reviews and meta-analyses of high-quality longitudinal studies have indicated that psychosocial working conditions (e.g., supportive coworkers, monotonous work) are related to the development of MSDs.\n\nThere are many forms of workplace mistreatment ranging from relatively minor discourtesies to serious cases of bullying and violence.\n\nWorkplace incivility has been defined as \"low-intensity deviant behavior with ambiguous intent to harm the target...Uncivil behaviors are characteristically rude and discourteous, displaying a lack of regard for others\" (p. 457). Incivility is distinct from violence. Examples of workplace incivility include insulting comments, denigration of the target's work, spreading false rumors, social isolation, etc. A summary of research conducted in Europe suggests that workplace incivility is common there. In research on more than 1000 U.S. civil service workers, more than 70% of the sample experienced workplace incivility in the past five years. Compared to men, women were more exposed to incivility; incivility was associated with psychological distress and reduced job satisfaction.\n\nAbusive supervision is the extent to which a supervisor engages in a pattern of behavior that harms subordinates.\n\nAlthough definitions of workplace bullying vary, it involves a repeated pattern of harmful behaviors directed towards an individual by one or more others who have more power than the target. Workplace bullying is sometimes termed \"mobbing\".\n\nSexual harassment is behavior that denigrates or mistreats an individual due to his or her gender, creates an offensive workplace, and interferes with an individual being able to do the job.\n\nWorkplace violence is a significant health hazard for employees, both physically and psychologically.\n\nMost workplace assaults are nonfatal, with an annual physical assault rate of 6% in the U.S. Assaultive behavior in the workplace often produces injury, psychological distress, and economic loss. One study of California workers found a rate of 72.9 non-fatal, officially documented assaults per 100,000 workers per year, with workers in the education, retail, and health care sectors subject to excess risk. A Minnesota workers' compensation study found that women workers had a twofold higher risk of being injured in an assault than men, and health and social service workers, transit workers, and members of the education sector were at high risk for injury compared to workers in other economic sectors. A West Virginia workers' compensation study found that workers in the health care sector and, to a lesser extent, the education sector were at elevated risk for assault-related injury. Another workers' compensation study found that excessively high rates of assault-related injury in schools, healthcare, and, to a lesser extent, banking. In addition to the physical injury that results from being a victim of workplace violence, individuals who witness such violence without being directly victimized are at increased risk for experiencing adverse psychological effects, including high levels of distress and arousal, as found in a study of Los Angeles teachers.\n\nIn 1996 there were 927 work-associated homicides in the United States, in a labor force that numbered approximately 132,616,000. The rate works out to be about 7 homicides per million workers for the one year. Men are more likely to be victims of workplace homicide than women.\n\nResearch has found that psychosocial workplace factors are among the risk factors for a number of categories of mental disorder.\n\nWorkplace factors can contribute to alcohol abuse and dependence of employees. Rates of abuse can vary by occupation, with high rates in the construction and transportation industries as well as among waiters and waitresses. Within the transportation sector, heavy truck drivers and material movers were shown to be at especially high risk. A prospective study of ECA subjects who were followed one year after the initial interviews provided data on newly incident cases of alcohol abuse and dependence. The study found that workers in jobs that combined low control with high physical demands were at increased risk of developing alcohol problems although the findings were confined to men.\n\nUsing data from the ECA study, Eaton, Anthony, Mandel, and Garrison (1990) found that members of three occupational groups, lawyers, secretaries, and special education teachers (but not other types of teachers) showed elevated rates of \"DSM-III\" major depression, adjusting for social demographic factors. The ECA study involved representative samples of American adults from five geographical areas, providing relatively unbiased estimates of the risk of mental disorder by occupation; however, because the data were cross-sectional, no conclusions bearing on cause-and-effect relations are warranted. Evidence from a Canadian prospective study indicated that individuals in the highest quartile of occupational stress (high-strain jobs as per the demand-control model) are at increased risk of experiencing an episode of major depression. A literature review and meta-analysis links high demands, low control, and low support to clinical depression. A meta-analysis that pooled the results of 11 well-designed longitudinal studies indicated that a number of facets of the psychosocial work environment (e.g., low decision latitude, high psychological workload, lack of social support at work, effort-reward imbalance, and job insecurity) increase the risk of common mental disorders such as depression.\n\nDepending on the diagnosis, severity and individual, and the job itself, personality disorders can be associated with difficulty coping with work or the workplace, potentially leading to problems with others by interfering with interpersonal relationships. Indirect effects also play a role; for example, impaired educational progress or complications outside of work, such as substance abuse and co-morbid mental disorders, can plague sufferers. However, personality disorders can also bring about above-average work abilities by increasing competitive drive or causing the sufferer to exploit his or her co-workers.\n\nIn a case-control study, Link, Dohrenwend, and Skodol found that, compared to depressed and well control subjects, schizophrenic patients were more likely to have had jobs, prior to their first episode of the disorder, that exposed them to “noisesome” work characteristics (e.g., noise, humidity, heat, cold, etc.). The jobs tended to be of higher status than other blue collar jobs, suggesting that downward drift in already-affected individuals does not account for the finding. One explanation involving a diathesis-stress model suggests that the job-related stressors helped precipitate the first episode in already-vulnerable individuals. There is some supporting evidence from the (ECA) study.\n\nLongitudinal studies have suggested adverse working conditions can contribute to the development of psychological distress. Psychological distress refers to negative affect, without the individuals necessarily meeting criteria for a psychiatric disorder. Psychological distress is often expressed in affective (depressive), psychophysical or psychosomatic (e.g., headaches, stomach aches, etc.), and anxiety symptoms. The relation of adverse working conditions to psychological distress is thus an important avenue of research. Job satisfaction is also related to negative health outcomes. A literature review and meta-analysis of high-quality longitudinal studies link high demands, low control, and low support to psychological symptoms.\n\nParkes (1982) studied the relation of working conditions to psychological distress in British student nurses. She found that in this \"natural experiment,\" student nurses experienced higher levels of distress and lower levels of job satisfaction in medical wards than in surgical wards; compared to surgical wards, medical wards make greater affective demands on the nurses. In another study, Frese (1985) concluded that objective working conditions (e.g., noise, ambiguities, conflicts) give rise to subjective stress and psychosomatic symptoms in blue collar German workers. In addition to the above studies, a number of other well-controlled longitudinal studies have implicated work stressors in the development of psychological distress and reduced job satisfaction.\n\nA comprehensive meta-analysis involving 86 studies indicated that involuntary job loss is linked to increased psychological distress. The impact of involuntary unemployment was comparatively weaker in countries that had greater income equality and better social safety nets. The research evidence also indicates that poorer mental health slightly, but significantly, increases the risk of later job loss.\n\nSome OHP research is concerned with (a) understanding the impact of economic crises on individuals' physical and mental health and well-being and (b) calling attention to personal and organizational means for ameliorating the impact of the crisis. Economic insecurity contributes, at least partly, to psychological distress and work-family conflict. Ongoing job insecurity, even in the absence of job loss, is related to higher levels of depressive symptoms, psychological distress, and worse overall health.\n\nEmployees must balance their working lives with their home lives. Work–family conflict is a situation in which the demands of work conflict with the demands of family or vice versa, making it difficult to adequately do both, giving rise to distress. Although more research has been conducted on work-family conflict, there is also the phenomenon of work-family enhancement, which occurs when positive effects carry over from one domain into the other.\n\nA number of stress management interventions have emerged that have shown demonstrable effects in reducing job stress. Cognitive behavioral interventions have tended to have greatest impact on stress reduction.\n\nOHP interventions often concern both the health of the individual and the health of the organization. Adkins (1999) described the development of one such intervention, an organizational health center (OHC) at a California industrial complex. The OHC helped to improve both organizational and individual health as well as help workers manage job stress. Innovations included labor-management partnerships, suicide risk reduction, conflict mediation, and occupational mental health support. OHC practitioners also coordinated their services with previously underutilized local community services in the same city, thus reducing redundancy in service delivery.\n\nHugentobler, Israel, and Schurman (1992) detailed a different, multi-layered intervention in a mid-sized Michigan manufacturing plant. The hub of the intervention was the Stress and Wellness Committee (SWC) which solicited ideas from workers on ways to improve both their well-being and productivity. Innovations the SWC developed included improvements that ensured two-way communication between workers and management and reduction in stress resulting from diminished conflict over issues of quantity versus quality. Both the interventions described by Adkins and Hugentobler et al. had a positive impact on productivity.\n\nCurrently there are efforts under way at NIOSH to help reduce the incidence of preventable disorders (e.g., sleep apnea) among heavy-truck and tractor-trailer drivers and, concomitantly, the life-threatening accidents to which the disorders lead, improve the health and safety of workers who are assigned to shift work or who work long hours, and reduce the incidence of falls among iron workers.\n\nThe Mental Health Advisory Teams of the United States Army employ OHP-related interventions with combat troops. OHP also has a role to play in interventions aimed at helping first responders.\n\nSchmitt (2007) described three different modestly scaled OHP-related interventions that helped workers abstain from smoking, exercise more frequently, and shed weight. Other OHP interventions include a campaign to improve the rates of hand washing, an effort to get workers to walk more often, and a drive to get employees to be more compliant with regard to taking prescribed medicines. The interventions tended reduce organization health-care costs.\n\nOrganizations can play a role in the health behavior of employees by providing resources to encourage healthy behavior in areas of exercise, nutrition, and smoking cessation.\n\nAlthough the dimensions of the problem of workplace violence vary by economic sector, one sector, education, has had some limited success in introducing programmatic, psychologically-based efforts to reduce the level of violence. Research suggests that there continue to be difficulties in successfully \"screening out applicants [for jobs] who may be prone to engaging in aggressive behavior,\" suggesting that aggression-prevention training of existing employees may be an alternative to screening. Only a small number of studies evaluating the effectiveness of training programs to reduce workplace violence currently exist.\n\nBecause many companies have implemented worker safety and health measures in a fragmented way, a new approach to worker safety and health has emerged in response, driven by efforts advanced by NIOSH. NIOSH trademarked that approach, naming it Total Worker Health. Total Worker Health involves the coordination of evidence-based (a) health promotion practices at the level of the individual worker and (b) umbrella-like health and safety practices at the level of the organizational unit. Research findings indicate that this two-pronged approach is effective in preventing work-related illness and injury.\n\nPsychological factors are an important factor in occupational accidents that can lead to injury and death of employees. An important influence on the incidence of accidents is the organization's safety climate that is employees' shared beliefs about how supervisors reward and support safety behavior.\n\n"}
{"id": "501376", "url": "https://en.wikipedia.org/wiki?curid=501376", "title": "Oligosaccharide", "text": "Oligosaccharide\n\nAn oligosaccharide (/ˌɑlɪgoʊˈsækəˌɹaɪd/; from the Greek ὀλίγος \"olígos\", \"a few\", and σάκχαρ \"sácchar\", \"sugar\") is a saccharide polymer containing a small number (typically three to ten) of monosaccharides (simple sugars). Oligosaccharides can have many functions including cell recognition and cell binding. For example, glycolipids have an important role in the immune response.\n\nThey are normally present as glycans: oligosaccharide chains linked to lipids or to compatible amino acid side chains in proteins, by \"N\"- or \"O\"-glygosidic bonds. N-linked oligosaccharides are always pentasaccharides attached to asparagine via a beta linkage to the amine nitrogen of the side chain. Alternately, O-linked oligosaccharides are generally attached to threonine or serine on the alcohol group of the side chain. Not all natural oligosaccharides occur as components of glycoproteins or glycolipids. Some, such as the raffinose series, occur as storage or transport carbohydrates in plants. Others, such as maltodextrins or cellodextrins, result from the microbial breakdown of larger polysaccharides such as starch or cellulose.\n\nIn biology, glycosylation is the process by which a carbohydrate is covalently attached to an organic molecule, creating structures such as glycoproteins and glycolipids.\n\nN-linked glycosylation involves oligosaccharide attachment to asparagine via a beta linkage to the amine nitrogen of the side chain. The process of N-linked glycosylation occurs cotranslationally, or concurrently while the proteins is being translated. Since it is added cotranslationally, it is believed that N-linked glycosylation helps determine the folding of polypeptides due to the hydrophilic nature of sugars. All N-linked Oligosaccharides are pentasaccharides: five monosaccharides long.\n\nIn N-glycosylation for eukaryotes, the oligosaccharide substrate is assembled right at the membrane of the endoplasmatic reticulum. For prokaryotes, this process occurs at the plasma membrane. In both cases, the acceptor substrate is an asparagine residue. The asparagine residue linked to an N-linked oligosaccharide usually occurs in the sequence Asn-X-Ser/Thr, where X can be any amino acid except for proline, although it is rare to see Asp, Glu, Leu, or Trp in this position.\n\nOligosaccharides that participate in O-linked glycosylation are attached to threonine or serine on the hydroxyl group of the side chain. O-linked glycosylation occurs in the golgi apparatus, where monosaccharide units are added to a complete polypeptide chain. Cell surface proteins and extracellular proteins are O-glycosylated. Glycosylation sites in O-linked oligosaccharides are determined by the secondary and tertiary structures of the polypeptide, which dictate where glycosyltransferases will add sugars.\n\nGlycoproteins and glycolipids are by definition covalently bonded to carbohydrates. They are very abundant on the surface of the cell, and their interactions contribute to the overall stability of the cell.\n\nGlycoproteins have distinct Oligosaccharide structures which have significant effects on many of their properties, affecting critical functions such as antigenicity, solubility, and resistance to proteases. Glycoproteins are relevant as cell-surface receptors, cell-adhesion molecules, immunoglobulins, and tumor antigens.\n\nGlycolipids are important for cell recognition, and are important for modulating the function of membrane proteins that act as receptors. Glycolipids are lipid molecules bound to oligosaccharides, generally present in the lipid bilayer. Additionally, they can serve as receptors for cellular recognition and cell signaling. The head of the oligosaccharide serves as a binding partner in receptor activity. The binding mechanisms of receptors to the oligosaccharides depends on the composition of the oligosaccharides that are exposed or presented above the surface of the membrane. There is great diversity in the binding mechanisms of glycolipids, which is what makes them such an important target for pathogens as a site for interaction and entrance. For example, the chaperone activity of glycolipids has been studied for its relevance to HIV infection.\n\nAll cells are coated in either glycoproteins or glycolipids, both of which help determine cell types. Lectins, or proteins that bind carbohydrates, can recognize specific oligosaccharides and provide useful information for cell recognition based on oligosaccharide binding.\n\nAn important example of oligosaccharide cell recognition is the role of glycolipids in determining blood types. The various blood types are distinguished by the glycan modification present on the surface of blood cells. These can be visualized using mass spectrometry. The oligosaccharides found on the A, B, and H antigen occur on the non-reducing ends of the oligosaccharide. The H antigen (which indicates an O blood type) serves as a precursor for the A and B antigen. Therefore, a person with A blood type will have the A antigen and H antigen present on the glycolipids of the red blood cell plasma membrane. A person with B blood type will have the B and H antigen present. A person with AB blood type will have A, B, and H antigens present. And finally, a person with O blood type will only have the H antigen present. This means all blood types have the H antigen, which explains why the O blood type is known as the \"universal donor\".\n\nMany cells produce specific carbohydrate-binding proteins known as lectins, which mediate cell adhesion with oligosaccharides. Selectins - a family of lectins - mediate certain cell-cell adhesion processes, including those of leukocytes to endothelial cells. In an immune response, endothelial cells can express certain selectins transiently in response to damage or injury to the cells. In response, a reciprocal selectin-oligosaccharide interaction will occur between the two molecules which allows the white blood cell to help eliminate the infection or damage. Protein-Carbohydrate bonding is often mediated by hydrogen bonding and van der Waals forces.\n\nFructo-oligosaccharides (FOS), which are found in many vegetables, are short chains of fructose molecules. They differ from inulin, which has a much higher degree of polymerization than FOS and is therefore a polysaccharide, but like inulin, they are considered soluble dietary fibre. Galactooligosaccharides (GOS), which also occur naturally, consist of short chains of galactose molecules. These compounds cannot be digested in the human small intestine, and instead pass through to the large intestine, where they promote the growth of \"Bifidobacteria\", which are beneficial to gut health.\n\nMannan oligosaccharides (MOS) are widely used in animal feed to improve gastrointestinal health. They are normally obtained from the yeast cell walls of \"Saccharomyces cerevisiae\". Mannan oligosaccharides differ from other oligosaccharides in that they are not fermentable and their primary mode of actions include agglutination of type-1 fimbria pathogens and immunomodulation\n\nOligosaccharides are a component of fibre from plant tissue. FOS and inulin are present in Jerusalem artichoke, burdock, chicory, leeks, onions, and asparagus. Inulin is a significant part of the daily diet of most of the world’s population. FOS can also be synthesized by enzymes of the fungus \"Aspergillus niger\" acting on sucrose. GOS is naturally found in soybeans and can be synthesized from lactose. FOS, GOS, and inulin are also sold as nutritional supplements.\n\n"}
{"id": "40504763", "url": "https://en.wikipedia.org/wiki?curid=40504763", "title": "Option grid", "text": "Option grid\n\nOption Grid is the name for a tool for patients and providers to use together when they are discussing and deciding what best to do about possible options, either treatments or tests. The grid is published in the form of a summary table to enable comparisons between multiple potential treatments or options. The grids do this by using questions that patients frequently ask (FAQs), and are designed for use in face-to-face clinical encounters or to be given to patients to read for a few minutes, ahead of a conversation with a provider.\n\nThe key to the grids is the use of frequently asked questions (FAQs) that relate to the most common or most important concerns of patients. It is important to choose these FAQs carefully and to limit them to those that can be considered briefly. These FAQs are based on evidence where possible, and final versions are developed by teams of patients, clinicians, and editors. All Grids are written at a reading level of 10–12 years, in accordance with the plain English campaign guides. The evidence summaries upon which Option Grids are based are available for public review at the official Option Grid website.\n\nA number of option grids exist including:\n\n\n\n\n\n\n\n\n\n\n\nThe Option Grid Collaborative is a not-for-profit group of over 90 people, made up of patient representatives, medical experts, and clinicians involved in supporting shared decision making via the creation of Option Grids. The Collaborative welcomes new members who wish to create new Option Grids according to the group’s agreed-upon process, which involves a thorough review of best available evidence and user testing process. Interested members should visit the Official Website in the links below for more information about how to get involved. Collaborative members receive support and guidance for Option Grid development from Dartmouth College throughout the process.\n\nThe Collaborative operates under a Creative Commons license, which is a type of public copyright license that enables authors to give others the right to collaborate and build upon their work according to guidelines specified by the author. The Collaborative’s specific license, , allows others to download the group’s work and share it so long as they credit the source, do not make changes, and do not use it commercially.\n\n\n"}
{"id": "33287428", "url": "https://en.wikipedia.org/wiki?curid=33287428", "title": "Person-centred thinking", "text": "Person-centred thinking\n\nPerson-centred thinking is a set of values, skills and tools used in Person Centred Planning and in the personalisation of services used by people who need supports provided by social or health care.\n\nPerson-centred thinking is described by the UK Department of Health as \"the foundation for person centred planning\"\nThe British Institute of Learning Disabilities advocates Person centred thinking suggesting that such tools \"can be really helpful in assisting the process of getting to know a person really well\".\n\nA major piece of research into the impact of person centred planning found that the prevalence of person-centred thinking in services was an important condition for services having the capacity and systems for delivering person centred results.\n\nPerson-centred thinking tools in common use include one-page profiles, 'working/not working', 'important to/important for', 'good day/bad day', communication charts, 'doughnut' of staff roles and responsibilities, relationship circles, learning logs and person-centred reviews. \n\nThese tools provide an agenda which a person and the people who know that person best can think together, focussing on what is important to that person, how they wish to live, and then introduce changes that will move towards those aspirations. \nPractitioners of person centred thinking suggest that it's possible to build up from one or two pieces of person centred thinking, piece by piece based on the area of the person's life that they feel is most important to consider next, this process of building gradually creates a collection of person centred information, equivalent to a 'full' person centred plan, and more importantly a range of co-produced actions. Where a person has a specific budget allocated to pay for their social care, this portfolio of person centred thinking tools (a 'person centred description') can also be extremely useful as a basis to write a support plan, which explains how they will use this budget to meet what is important to and for them.\n\nPerson centred thinking was first developed by a group of people practicing Essential Lifestyle Planning in the US (now known as 'The Learning Community for Person Centred Practices). These planners realised that while many plans were being written, many of these were not leading to real change in peoples lives. This group decided that person centred planning was something a small group of people needed to know, but that for planning to work, a much wider group of people need skills around listening for what's important to people and how to best support people. Michael Smull who is a leading figure in this group describes Person centred thinking as \"something that virtually everyone who touches a person needs to know\" because change was most powerful when all staff were using person centred thinking tools in their roles, rather than relying solely on person centred planning facilitators to create plans. To enable these tools to be shared at scale, Essential Lifestyle Planning was 'deconstructed' into a range of person centred thinking tools, that enable staff to participate with the person in a cycle of listening, learning and action by building up a set of tools recording this process to form a person centred description of the person.\n\nPerson Centred Thinking, like other person centred approaches is based in the values of independence and rights, coproduction, choice and control and inclusive and competent communities.\n\n"}
{"id": "53058654", "url": "https://en.wikipedia.org/wiki?curid=53058654", "title": "Preherpetic neuralgia", "text": "Preherpetic neuralgia\n\nPreherpetic neuralgia is a form of nerve pain (neuralgia) specifically associated with a Shingles (herpes zoster) viral infection. This nerve pain often precedes visible indications of a Shingles infection and consequently can be a key early indicator of a need to begin preventative anti-viral drug therapy. Pain associated with Shingles can be extremely difficult to treat whereas the source is related to the virus attacking the nervous system itself. Pain symptoms can last months or years beyond any outward sign of viral infection and can be quite severe. The combination of extreme pain severity and longevity can contribute to chronic depression and even suicide.\n\n\n"}
{"id": "50611106", "url": "https://en.wikipedia.org/wiki?curid=50611106", "title": "Protein digestibility", "text": "Protein digestibility\n\nProtein digestibility refers to how well a given protein is digested. Along with the amino acid score, protein digestibility determines the values for PDCAAS and DIAAS.\n"}
{"id": "50500706", "url": "https://en.wikipedia.org/wiki?curid=50500706", "title": "Protein quality", "text": "Protein quality\n\nProtein quality is the digestibility and quantity of essential amino acids for providing the proteins in correct ratios for human consumption. There are various methods that rank the quality of different types of protein, some of which are outdated and no longer in use, or not considered as useful as they once were thought to be. The Protein Digestibility Corrected Amino Acid Score (PDCAAS), which was recommended by the Food and Agriculture Organization of the United Nations (FAO), became the industry standard in 1993. FAO has recently recommended the newer Digestible Indispensable Amino Acid Score (DIAAS) to supersede PDCAAS. The dairy industry is in favor of this, because while PDCAAS truncates all protein types that exceed the essential amino acid (EAA) requirements to 1.0, DIAAS allows a higher than 1.0 ranking: while for example both soy protein isolate and whey isolate are ranked 1.0 according to PDCAAS, in the DIAAS system, whey has a higher score than soy.\n\nThe main limitations of PDCAAS is that it doesn't take into account anti-nutrient factors like phytic acid and trypsin inhibitors, which limit the absorption of protein among other nutrients. For this reason, DIAAS is promoted as the superior method and preferable over the PDCAAS. Other older methods like BV, PER, NPU and nitrogen balance may not reveal much about the amino acid profile and digestibility of the protein source in question, but can still be considered useful in that they determine other aspects of protein quality not taken into account by PDCAAS and DIAAS.\n\nBelow follows a table that compares various proteins based on their rankings. Some of these results may differ and vary significantly depending on if it is soybeans or soy protein isolate, and so on. For example, while soybeans have a PDCAAS score of 0.91, many soy protein isolates (though not all) typically get a PDCAAS score of 1.0. Likewise, the amino acid profile may differ from crop to crop depending on the soil, and between different breeds of soy. Generally speaking, however, soybeans rarely outperform whey protein isolate in PDCAAS rankings.\n\nBelow follows a table that compares the complete amino acid profiles of various proteins. The amino acid score is based on the prevalence of the essential amino acids and depends on if they reach sufficient quantity. PDCAAS scores do not take into account the quantity of the non-essential amino acids. \n\n<nowiki>*</nowiki>Semi-essential, under certain conditions\n\n<nowiki>**</nowiki>Branched-chain amino acid (BCAA)\n"}
{"id": "427499", "url": "https://en.wikipedia.org/wiki?curid=427499", "title": "Proteinogenic amino acid", "text": "Proteinogenic amino acid\n\nProteinogenic amino acids are amino acids that are incorporated biosynthetically into proteins during translation. The word \"proteinogenic\" means \"protein creating\". Throughout known life, there are 22 genetically encoded (proteinogenic) amino acids, 20 in the standard genetic code and an additional 2 that can be incorporated by special translation mechanisms.\n\nIn contrast, non-proteinogenic amino acids are amino acids that are either not incorporated into proteins (like GABA, -DOPA, or triiodothyronine), misincorporated in place of a genetically encoded amino acid, or not produced directly and in isolation by standard cellular machinery (like hydroxyproline). The latter often results from post-translational modification of proteins. Some non-proteinogenic amino acids are incorporated into nonribosomal peptides which are synthesized by non-ribosomal peptide synthetases.\n\nBoth eukaryotes and prokaryotes can incorporate selenocysteine into their proteins via a nucleotide sequence known as a SECIS element, which directs the cell to translate a nearby UGA codon as selenocysteine (UGA is normally a stop codon). In some methanogenic prokaryotes, the UAG codon (normally a stop codon) can also be translated to pyrrolysine.\n\nIn eukaryotes, there are only 21 proteinogenic amino acids, the 20 of the standard genetic code, plus selenocysteine. Humans can synthesize 12 of these from each other or from other molecules of intermediary metabolism. The other nine must be consumed (usually as their protein derivatives), and so they are called essential amino acids. The essential amino acids are histidine, isoleucine, leucine, lysine, methionine, phenylalanine, threonine, tryptophan, and valine (i.e. H, I, L, K, M, F, T, W, V).\n\nThe proteinogenic amino acids have been found to be related to the set of amino acids that can be recognized by ribozyme autoaminoacylation systems. Thus, non-proteinogenic amino acids would have been excluded by the contingent evolutionary success of nucleotide-based life forms. Other reasons have been offered to explain why certain specific non-proteinogenic amino acids are not generally incorporated into proteins; for example, ornithine and homoserine cyclize against the peptide backbone and fragment the protein with relatively short half-lives, while others are toxic because they can be mistakenly incorporated into proteins, such as the arginine analog canavanine.\n\nThe following illustrates the structures and abbreviations of the 21 amino acids that are directly encoded for protein synthesis by the genetic code of eukaryotes. The structures given below are standard chemical structures, not the typical zwitterion forms that exist in aqueous solutions.\n\nIUPAC/IUBMB now also recommends standard abbreviations for the following two amino acids:\n\nFollowing is a table listing the one-letter symbols, the three-letter symbols, and the chemical properties of the side chains of the standard amino acids. The masses listed are based on weighted averages of the elemental isotopes at their natural abundances. Forming a peptide bond results in elimination of a molecule of water, so the mass of an amino acid unit within a protein chain is reduced by 18.01524 Da.\n\nGeneral chemical properties\n\n§: Values for Asp, Cys, Glu, His, Lys & Tyr were determined using the amino acid residue placed centrally in an alanine pentapeptide. The value for Arg is from Pace \"et al.\" (2009). The value for Sec is from Byun & Kang (2011).\n\nN.D.: The pKa value of Pyrrolysine has not been reported.\n\nNote: The pKa value of an amino-acid residue in a small peptide is typically slightly different when it is inside a protein. Protein pKa calculations are sometimes used to calculate the change in the pKa value of an amino-acid residue in this situation.\n\n† The stop codon is not an amino acid, but is included for completeness.\n†† UAG and UGA do not always act as stop codons (see above).\n‡ An essential amino acid cannot be synthesized in humans and must, therefore, be supplied in the diet. Conditionally essential amino acids are not normally required in the diet, but must be supplied exogenously to specific populations that do not synthesize it in adequate amounts.\n& Occurrence of amino acids is based on 135 Archaea, 3775 Bacteria, 614 Eukaryota proteomes and human proteome (21 006 proteins) respectively.\n\nIn mass spectrometry of peptides and proteins, knowledge of the masses of the residues is useful. The mass of the peptide or protein is the sum of the residue masses plus the mass of water (Monoisotopic mass = 18.01056 Da; average mass = 18.0153 Da). The residue masses are calculated from the tabulated chemical formulas and atomic weights. In mass spectrometry, ions may also include one or more protons (Monoisotopic mass = 1.00728 Da; average mass = 1.0074 Da).\n§ Monoisotopic mass\n\nThe table below lists the abundance of amino acids in \"E.coli\" cells and the metabolic cost (ATP) for synthesis the amino acids. Negative numbers indicate the metabolic processes are energy favorable and do not cost net ATP of the cell. The abundance of amino acids includes amino acids in free form and in polymerization form (proteins).\n\nAmino acids can be classified according to the properties of their main products: \n\nThe proteinogenic set used by known life on Earth appears to be arbitrarily selected by evolution, according to current knowledge, from many hundreds of possible alpha-type amino acids. Xenobiology studies hypothetical life forms that could be constructed using alternative sets using expanded genetic codes. Miller-type experiments on artificial abiogenesis show that alpha-type amino acids predominate in water-based 'primordial soups', but beta-type amino acids dominate when less water is present. Both alpha- and beta-based sets could form the basis for alternative protein constructions and life forms.\n\n\n"}
{"id": "463734", "url": "https://en.wikipedia.org/wiki?curid=463734", "title": "Public health", "text": "Public health\n\nPublic health is \"the science and art of preventing disease, prolonging life and promoting human health through organized efforts and informed choices of society, organizations, public and private, communities and individuals\". Analyzing the health of a population and the threats is the basis for public health. The \"public\" in question can be as small as a handful of people, an entire village or it can be as large as several continents, in the case of a pandemic. \"Health\" takes into account physical, mental and social well-being. It is not merely the absence of disease or infirmity, according to the World Health Organization. Public health is interdisciplinary. For example, epidemiology, biostatistics and health services are all relevant. Environmental health, community health, behavioral health, health economics, public policy, mental health and occupational safety, gender issues in health, sexual and reproductive health are other important subfields.\n\nPublic health aims to improve the quality of life through prevention and treatment of disease, including mental health. This is done through the surveillance of cases and health indicators, and through the promotion of healthy behaviors. Common public health initiatives include promoting handwashing and breastfeeding, delivery of vaccinations, suicide prevention and distribution of condoms to control the spread of sexually transmitted diseases.\n\nModern public health practice requires multidisciplinary teams of public health workers and professionals. Teams might include epidemiologists, biostatisticians, medical assistants, public health nurses, midwives, medical microbiologists, economists, sociologists, geneticists and data managers. Depending on the need environmental health officers or public health inspectors, bioethicists, and even veterinarians, gender experts, sexual and reproductive health specialists might be called on.\n\nAccess to health care and public health initiatives are difficult challenges in developing countries. Public health infrastructures are still forming in those countries.\n\nThe focus of a public health intervention is to prevent and manage diseases, injuries and other health conditions through surveillance of cases and the promotion of healthy behaviors, communities and environments. Many diseases are preventable through simple, nonmedical methods. For example, research has shown that the simple act of handwashing with soap can prevent the spread of many contagious diseases. In other cases, treating a disease or controlling a pathogen can be vital to preventing its spread to others, either during an outbreak of infectious disease or through contamination of food or water supplies. Public health communications programs, vaccination programs and distribution of condoms are examples of common preventive public health measures. Measures such as these have contributed greatly to the health of populations and increases in life expectancy.\n\nPublic health plays an important role in disease prevention efforts in both the developing world and in developed countries through local health systems and non-governmental organizations. The World Health Organization (WHO) is the international agency that coordinates and acts on global public health issues. Most countries have their own government public health agencies, sometimes known as ministries of health, to respond to domestic health issues. For example, in the United States, the front line of public health initiatives are state and local health departments. The United States Public Health Service (PHS), led by the Surgeon General of the United States, and the Centers for Disease Control and Prevention, headquartered in Atlanta, are involved with several international health activities, in addition to their national duties. In Canada, the Public Health Agency of Canada is the national agency responsible for public health, emergency preparedness and response, and infectious and chronic disease control and prevention. The Public health system in India is managed by the Ministry of Health & Family Welfare of the government of India with state-owned health care facilities.\n\nMost governments recognize the importance of public health programs in reducing the incidence of disease, disability, and the effects of aging and other physical and mental health conditions. However, public health generally receives significantly less government funding compared with medicine. Public health programs providing vaccinations have made strides in promoting health, including the eradication of smallpox, a disease that plagued humanity for thousands of years.\nThe World Health Organization (WHO) identifies core functions of public health programs including:\n\nIn particular, public health surveillance programs can:\n\nPublic health surveillance has led to the identification and prioritization of many public health issues facing the world today, including HIV/AIDS, diabetes, waterborne diseases, zoonotic diseases, and antibiotic resistance leading to the reemergence of infectious diseases such as tuberculosis. Antibiotic resistance, also known as drug resistance, was the theme of World Health Day 2011. Although the prioritization of pressing public health issues is important, Laurie Garrett argues that there are following consequences. When foreign aid is funnelled into disease-specific programs, the importance of public health in general is disregarded. This public health problem of stovepiping is thought to create a lack of funds to combat other existing diseases in a given country.\n\nFor example, the WHO reports that at least 220 million people worldwide suffer from diabetes. Its incidence is increasing rapidly, and it is projected that the number of diabetes deaths will double by the year 2030. In a June 2010 editorial in the medical journal \"The Lancet\", the authors opined that \"The fact that type 2 diabetes, a largely preventable disorder, has reached epidemic proportion is a public health humiliation.\" The risk of type 2 diabetes is closely linked with the growing problem of obesity. The WHO’s latest estimates as of June 2016 highlighted that globally approximately 1.9 billion adults were overweight in 2014, and 41 million children under the age of five were overweight in 2014. The United States is the leading country with 30.6% of its population being obese. Mexico follows behind with 24.2% and the United Kingdom with 23%. Once considered a problem in high-income countries, it is now on the rise in low-income countries, especially in urban settings. Many public health programs are increasingly dedicating attention and resources to the issue of obesity, with objectives to address the underlying causes including healthy diet and physical exercise.\n\nSome programs and policies associated with public health promotion and prevention can be controversial. One such example is programs focusing on the prevention of HIV transmission through safe sex campaigns and needle-exchange programmes. Another is the control of tobacco smoking. Changing smoking behavior requires long-term strategies, unlike the fight against communicable diseases, which usually takes a shorter period for effects to be observed. Many nations have implemented major initiatives to cut smoking, such as increased taxation and bans on smoking in some or all public places. Proponents argue by presenting evidence that smoking is one of the major killers, and that therefore governments have a duty to reduce the death rate, both through limiting passive (second-hand) smoking and by providing fewer opportunities for people to smoke. Opponents say that this undermines individual freedom and personal responsibility, and worry that the state may be emboldened to remove more and more choice in the name of better population health overall.\n\nSimultaneously, while communicable diseases have historically ranged uppermost as a global health priority, non-communicable diseases and the underlying behavior-related risk factors have been at the bottom. This is changing, however, as illustrated by the United Nations hosting its first General Assembly Special Summit on the issue of non-communicable diseases in September 2011.\n\nMany health problems are due to maladaptive personal behaviors. From an evolutionary psychology perspective, over consumption of novel substances that are harmful is due to the activation of an evolved reward system for substances such as drugs, tobacco, alcohol, refined salt, fat, and carbohydrates. New technologies such as modern transportation also cause reduced physical activity. Research has found that behavior is more effectively changed by taking evolutionary motivations into consideration instead of only presenting information about health effects. The marketing industry has long known the importance of associating products with high status and attractiveness to others. Films are increasingly being recognized as a public health tool. In fact, film festivals and competitions have been established to specifically promote films about health. Conversely, it has been argued that emphasizing the harmful and undesirable effects of tobacco smoking on other persons and imposing smoking bans in public places have been particularly effective in reducing tobacco smoking.\n\nAs well as seeking to improve population health through the implementation of specific population-level interventions, public health contributes to medical care by identifying and assessing population needs for health care services, including:\n\nTo improve public health, one important strategy is to promote modern medicine and scientific neutrality to drive the public health policy and campaign, which is recommended by Armanda Solorzana, through a case study of the Rockefeller Foundation's hookworm campaign in Mexico in the 1920s. Soloranza argues that public health policy can't concern only politics or economics. Political concerns can lead government officials to hide the real numbers of people affected by disease in their regions, such as upcoming elections. Therefore, scientific neutrality in making public health policy is critical; it can ensure treatment needs are met regardless of political and economic conditions.\n\nThe history of public health care clearly shows the global effort to improve health care for all. However, in modern-day medicine, real, measurable change has not been clearly seen, and critics argue that this lack of improvement is due to ineffective methods that are being implemented. As argued by Paul E. Farmer, structural interventions could possibly have a large impact, and yet there are numerous problems as to why this strategy has yet to be incorporated into the health system. One of the main reasons that he suggests could be the fact that physicians are not properly trained to carry out structural interventions, meaning that the ground level health care professionals cannot implement these improvements. While structural interventions can not be the only area for improvement, the lack of coordination between socioeconomic factors and health care for the poor could be counterproductive, and end up causing greater inequity between the health care services received by the rich and by the poor. Unless health care is no longer treated as a commodity, global public health will ultimately not be achieved. This being the case, without changing the way in which health care is delivered to those who have less access to it, the universal goal of public health care cannot be achieved.\n\nAnother reason why measurable changes may not be noticed in public health is because agencies themselves may not be measuring their programs' efficacy. Perrault et al. analyzed over 4,000 published objectives from Community Health Improvement Plans (CHIPs) of 280 local accredited and non-accredited public health agencies in the U.S., and found that the majority of objectives - around two-thirds - were focused on achieving agency outputs (e.g., developing communication plans, installing sidewalks, disseminating data to the community). Only about one-third focused on seeking measurable changes in the populations they serve (i.e., changing people's knowledge, attitudes, behaviors). What this research showcases is that if agencies are only focused on accomplishing tasks (i.e., outputs) and do not have a focus on measuring actual changes in their populations with the activities they perform, it should not be surprising when measurable changes are not reported. Perrault et al. advocate for public health agencies to work with those in the discipline of Health Communication to craft objectives that are measurable outcomes, and to assist agencies in developing tools and methods to be able to track more proximal changes in their target populations (e.g., knowledge and attitude shifts) that may be influenced by the activities the agencies are performing.\n\n\"Public Health 2.0\" is a movement within public health that aims to make the field more accessible to the general public and more user-driven. The term is used in three senses. In the first sense, \"Public Health 2.0\" is similar to \"Health 2.0\" and describes the ways in which traditional public health practitioners and institutions are reaching out (or could reach out) to the public through social media and health blogs.\n\nIn the second sense, \"Public Health 2.0\" describes public health research that uses data gathered from social networking sites, search engine queries, cell phones, or other technologies. A recent example is the proposal of statistical framework that utilizes online user-generated content (from social media or search engine queries) to estimate the impact of an influenza vaccination campaign in the UK.\n\nIn the third sense, \"Public Health 2.0\" is used to describe public health activities that are completely user-driven. An example is the collection and sharing of information about environmental radiation levels after the March 2011 tsunami in Japan. In all cases, Public Health 2.0 draws on ideas from Web 2.0, such as crowdsourcing, information sharing, and user-centred design. While many individual healthcare providers have started making their own personal contributions to \"Public Health 2.0\" through personal blogs, social profiles, and websites, other larger organizations, such as the American Heart Association (AHA) and United Medical Education (UME), have a larger team of employees centered around online driven health education, research, and training. These private organizations recognize the need for free and easy to access health materials often building libraries of educational articles.\n\nThere is a great disparity in access to health care and public health initiatives between developed nations and developing nations. In the developing world, public health infrastructures are still forming. There may not be enough trained health workers, monetary resources or, in some cases, sufficient knowledge to provide even a basic level of medical care and disease prevention. As a result, a large majority of disease and mortality in the developing world results from and contributes to extreme poverty. For example, many African governments spend less than US$10 per person per year on health care, while, in the United States, the federal government spent approximately US$4,500 per capita in 2000. However, expenditures on health care should not be confused with spending on public health. Public health measures may not generally be considered \"health care\" in the strictest sense. For example, mandating the use of seat belts in cars can save countless lives and contribute to the health of a population, but typically money spent enforcing this rule would not count as money spent on health care.\n\nLarge parts of the developing world remained plagued by largely preventable or treatable infectious diseases. In addition to this however, many developing countries are also experiencing an epidemiological shift and polarization in which populations are now experiencing more of the effects of chronic diseases as life expectancy increases with, the poorer communities being heavily affected by both chronic and infectious diseases. Another major public health concern in the developing world is poor maternal and child health, exacerbated by malnutrition and poverty. The WHO reports that a lack of exclusive breastfeeding during the first six months of life contributes to over a million avoidable child deaths each year. Intermittent preventive therapy aimed at treating and preventing malaria episodes among pregnant women and young children is one public health measure in endemic countries.\n\nEach day brings new front-page headlines about public health: emerging infectious diseases such as SARS, rapidly making its way from China (see Public health in China) to Canada, the United States and other geographically distant countries; reducing inequities in health care access through publicly funded health insurance programs; the HIV/AIDS pandemic and its spread from certain high-risk groups to the general population in many countries, such as in South Africa; the increase of childhood obesity and the concomitant increase in type II diabetes among children; the social, economic and health effects of adolescent pregnancy; and the public health challenges related to natural disasters such as the 2004 Indian Ocean tsunami, 2005's Hurricane Katrina in the United States and the 2010 Haiti earthquake.\n\nSince the 1980s, the growing field of population health has broadened the focus of public health from individual behaviors and risk factors to population-level issues such as inequality, poverty, and education. Modern public health is often concerned with addressing determinants of health across a population. There is a recognition that our health is affected by many factors including where we live, genetics, our income, our educational status and our social relationships; these are known as \"social determinants of health\". The upstream drivers such as environment, education, employment, income, food security, housing, social inclusion and many others effect the distribution of health between and within populations and are often shaped by policy. A social gradient in health runs through society. The poorest generally suffer the worst health, but even the middle classes will generally have worse health outcomes than those of a higher social stratum. The new public health advocates for population-based policies that improve health in an equitable manner.\n\nHealth aid to developing countries is an important source of public health funding for many developing countries. Health aid to developing countries has shown a significant increase after World War II as concerns over the spread of disease as a result of globalization increased and the HIV/AIDS epidemic in sub-Saharan Africa surfaced. From 1990 to 2010, total health aid from developed countries increased from 5.5 billion to 26.87 billion with wealthy countries continuously donating billions of dollars every year with the goal of improving population health. Some efforts, however, receive a significantly larger proportion of funds such as HIV which received an increase in funds of over $6 billion dollars between 2000 and 2010 which was more than twice the increase seen in any other sector during those years. Health aid has seen an expansion through multiple channels including private philanthropy, non-governmental organizations, private foundations such as the Bill & Melinda Gates Foundation, bilateral donors, and multilateral donors such as the World Bank or UNICEF. In 2009 health aid from the OECD amounted to $12.47 billion which amounted to 11.4% of its total bilateral aid. In 2009, Multilateral donors were found to spend 15.3% of their total aid on bettering public healthcare. Recent data, however, shows that international health aid has plateaued and may begin to decrease.\n\nDebates exist questioning the efficacy of international health aid. Proponents of aid claim that health aid from wealthy countries is necessary in order for developing countries to escape the poverty trap. Opponents of health aid claim that international health aid actually disrupts developing countries' course of development, causes dependence on aid, and in many cases the aid fails to reach its recipients. For example, recently, health aid was funneled towards initiatives such as financing new technologies like antiretroviral medication, insecticide-treated mosquito nets, and new vaccines. The positive impacts of these initiatives can be seen in the eradication of smallpox and polio; however, critics claim that misuse or misplacement of funds may cause many of these efforts to never come into fruition.\n\nEconomic modeling based on the Institute for Health Metrics and Evaluation and the World Health Organization has shown a link between international health aid in developing countries and a reduction in adult mortality rates. However, a 2014-2016 study suggests that a potential confounding variable for this outcome is the possibility that aid was directed at countries once they were already on track for improvement. That same study, however, also suggests that 1 billion dollars in health aid was associated with 364,000 fewer deaths occurring between ages 0 and 5 in 2011.\n\nTo address current and future challenges in addressing health issues in the world, the United Nations have developed the Sustainable Development Goals building off of the Millennium Development Goals of 2000 to be completed by 2030. These goals in their entirety encompass the entire spectrum of development across nations, however Goals 1-6 directly address health disparities, primarily in developing countries. These six goals address key issues in global public health: Poverty, Hunger and food security, Health, Education, Gender equality and women's empowerment, and water and sanitation. Public health officials can use these goals to set their own agenda and plan for smaller scale initiatives for their organizations. These goals hope to lessen the burden of disease and inequality faced by developing countries and lead to a healthier future.\n\nThe links between the various sustainable development goals and public health are numerous and well established:\n\nThe U.S. Global Health Initiative was created in 2009 by President Obama in an attempt to have a more holistic, comprehensive approach to improving global health as opposed to previous, disease-specific interventions. The Global Health Initiative is a six-year plan, \"to develop a comprehensive U.S. government strategy for global health, building on the President's Emergency Plan for AIDS Relief (PEPFAR) to combat HIV as well as U.S. efforts to address tuberculosis (TB) and malaria, and augmenting the focus on other global health priorities, including neglected tropical diseases (NTDs), maternal, newborn and child health (MNCH), family planning and reproductive health (FP/RH), nutrition, and health systems strengthening (HSS)\". The GHI programs are being implemented in more than 80 countries around the world and works closely with the United States Agency for International Development, the Centers for Disease Control and Prevention, the United States Deputy Secretary of State.\n\nThere are seven core principles:\n\n\nThe aid effectiveness agenda is a useful tool for measuring the impact of these large scale programs such as The Global Fund to Fight AIDS, Tuberculosis and Malaria and the Global Alliance for Vaccines and Immunization (GAVI) which have been successful in achieving rapid and visible results. The Global Fund claims that its efforts have provided antiretroviral treatment for over three million people worldwide. GAVI claims that its vaccination programs have prevented over 5 million deaths since it began in 2000.\n\nEducation and training of public health professionals is available throughout the world in Schools of Public Health, Medical Schools, Veterinary Schools, Schools of Nursing, and Schools of Public Affairs. The training typically requires a university degree with a focus on core disciplines of biostatistics, epidemiology, health services administration, health policy, health education, behavioral science, gender issues, sexual and reproductive health, public health nutrition and environmental and occupational health. In the global context, the field of public health education has evolved enormously in recent decades, supported by institutions such as the World Health Organization and the World Bank, among others. Operational structures are formulated by strategic principles, with educational and career pathways guided by competency frameworks, all requiring modulation according to local, national and global realities. It is critically important for the health of populations that nations assess their public health human resource needs and develop their ability to deliver this capacity, and not depend on other countries to supply it.\n\nIn the United States, the Welch-Rose Report of 1915 has been viewed as the basis for the critical movement in the history of the institutional schism between public health and medicine because it led to the establishment of schools of public health supported by the Rockefeller Foundation. The report was authored by William Welch, founding dean of the Johns Hopkins Bloomberg School of Public Health, and Wickliffe Rose of the Rockefeller Foundation. The report focused more on research than practical education. Some have blamed the Rockefeller Foundation's 1916 decision to support the establishment of schools of public health for creating the schism between public health and medicine and legitimizing the rift between medicine's laboratory investigation of the mechanisms of disease and public health's nonclinical concern with environmental and social influences on health and wellness.\n\nEven though schools of public health had already been established in Canada, Europe and North Africa, the United States had still maintained the traditional system of housing faculties of public health within their medical institutions. A $25,000 donation from businessman Samuel Zemurray instituted the School of Public Health and Tropical Medicine at Tulane University in 1912 conferring its first doctor of public health degree in 1914. The Yale School of Public Health was founded by Charles-Edward Avory Winslow in 1915. The Johns Hopkins School of Hygiene and Public Health became an independent, degree-granting institution for research and training in public health, and the largest public health training facility in the United States, when it was founded in 1916. By 1922, schools of public health were established at Columbia and Harvard on the Hopkins model. By 1999 there were twenty nine schools of public health in the US, enrolling around fifteen thousand students.\n\nOver the years, the types of students and training provided have also changed. In the beginning, students who enrolled in public health schools typically had already obtained a medical degree; public health school training was largely a second degree for medical professionals. However, in 1978, 69% of American students enrolled in public health schools had only a bachelor's degree.\n\nSchools of public health offer a variety of degrees which generally fall into two categories: professional or academic. The two major postgraduate degrees are the Master of Public Health (MPH) or the Master of Science in Public Health (MSPH). Doctoral studies in this field include Doctor of Public Health (DrPH) and Doctor of Philosophy (PhD) in a subspeciality of greater Public Health disciplines. DrPH is regarded as a professional degree and PhD as more of an academic degree.\n\nProfessional degrees are oriented towards practice in public health settings. The Master of Public Health, Doctor of Public Health, Doctor of Health Science (DHSc) and the Master of Health Care Administration are examples of degrees which are geared towards people who want careers as practitioners of public health in health departments, managed care and community-based organizations, hospitals and consulting firms, among others. Master of Public Health degrees broadly fall into two categories, those that put more emphasis on an understanding of epidemiology and statistics as the scientific basis of public health practice and those that include a more eclectic range of methodologies. A Master of Science of Public Health is similar to an MPH but is considered an academic degree (as opposed to a professional degree) and places more emphasis on scientific methods and research. The same distinction can be made between the DrPH and the DHSc. The DrPH is considered a professional degree and the DHSc is an academic degree.\n\nAcademic degrees are more oriented towards those with interests in the scientific basis of public health and preventive medicine who wish to pursue careers in research, university teaching in graduate programs, policy analysis and development, and other high-level public health positions. Examples of academic degrees are the Master of Science, Doctor of Philosophy, Doctor of Science (ScD), and Doctor of Health Science (DHSc). The doctoral programs are distinct from the MPH and other professional programs by the addition of advanced coursework and the nature and scope of a dissertation research project.\n\nIn the United States, the Association of Schools of Public Health represents Council on Education for Public Health (CEPH) accredited schools of public health. Delta Omega is the honor society for graduate studies in public health. The society was founded in 1924 at the Johns Hopkins School of Hygiene and Public Health. Currently, there are approximately 68 chapters throughout the United States and Puerto Rico.\n\nPublic health has early roots in antiquity. From the beginnings of human civilization, it was recognized that polluted water and lack of proper waste disposal spread communicable diseases (theory of miasma). Early religions attempted to regulate behavior that specifically related to health, from types of food eaten, to regulating certain indulgent behaviors, such as drinking alcohol or sexual relations. Leaders were responsible for the health of their subjects to ensure social stability, prosperity, and maintain order.\n\nBy Roman times, it was well understood that proper diversion of human waste was a necessary tenet of public health in urban areas. The ancient Chinese medical doctors developed the practice of variolation following a smallpox epidemic around 1000 BC. An individual without the disease could gain some measure of immunity against it by inhaling the dried crusts that formed around lesions of infected individuals. Also, children were protected by inoculating a scratch on their forearms with the pus from a lesion.\n\nIn 1485 the Republic of Venice established a permanent Venetian Magistrate for Health comprising supervisors of health with special attention to the prevention of the spread of epidemics in the territory from abroad. The three supervisors were initially appointed by the Venetian Senate. In 1537 it was assumed by the Grand Council, and in 1556 added two judges, with the task of control, on behalf of the Republic, the efforts of the supervisors.\n\nHowever, according to Michel Foucault, the plague model of governmentality was later controverted by the cholera model. A Cholera pandemic devastated Europe between 1829 and 1851, and was first fought by the use of what Foucault called \"social medicine\", which focused on flux, circulation of air, location of cemeteries, etc. All those concerns, born of the miasma theory of disease, were mixed with urbanistic concerns for the management of populations, which Foucault designated as the concept of \"biopower\". The German conceptualized this in the \"Polizeiwissenschaft\" (\"Police science\").\n\nThe 18th century saw rapid growth in voluntary hospitals in England. The latter part of the century brought the establishment of the basic pattern of improvements in public health over the next two centuries: a social evil was identified, private philanthropists brought attention to it, and changing public opinion led to government action.\nThe practice of vaccination became prevalent in the 1800s, following the pioneering work of Edward Jenner in treating smallpox. James Lind's discovery of the causes of scurvy amongst sailors and its mitigation via the introduction of fruit on lengthy voyages was published in 1754 and led to the adoption of this idea by the Royal Navy. Efforts were also made to promulgate health matters to the broader public; in 1752 the British physician Sir John Pringle published \"Observations on the Diseases of the Army in Camp and Garrison\", in which he advocated for the importance of adequate ventilation in the military barracks and the provision of latrines for the soldiers.\n\nWith the onset of the Industrial Revolution, living standards amongst the working population began to worsen, with cramped and unsanitary urban conditions. In the first four decades of the 19th century alone, London's population doubled and even greater growth rates were recorded in the new industrial towns, such as Leeds and Manchester. This rapid urbanisation exacerbated the spread of disease in the large conurbations that built up around the workhouses and factories. These settlements were cramped and primitive with no organized sanitation. Disease was inevitable and its incubation in these areas was encouraged by the poor lifestyle of the inhabitants. Unavailable housing led to the rapid growth of slums and the per capita death rate began to rise alarmingly, almost doubling in Birmingham and Liverpool. Thomas Malthus warned of the dangers of overpopulation in 1798. His ideas, as well as those of Jeremy Bentham, became very influential in government circles in the early years of the 19th century.\n\nThe first attempts at sanitary reform and the establishment of public health institutions were made in the 1840s. Thomas Southwood Smith, physician at the London Fever Hospital, began to write papers on the importance of public health, and was one of the first physicians brought in to give evidence before the Poor Law Commission in the 1830s, along with Neil Arnott and James Phillips Kay. Smith advised the government on the importance of quarantine and sanitary improvement for limiting the spread of infectious diseases such as cholera and yellow fever.\n\nThe Poor Law Commission reported in 1838 that \"the expenditures necessary to the adoption and maintenance of measures of prevention would ultimately amount to less than the cost of the disease now constantly engendered\". It recommended the implementation of large scale government engineering projects to alleviate the conditions that allowed for the propagation of disease. The Health of Towns Association was formed in Exeter on 11 December 1844, and vigorously campaigned for the development of public health in the United Kingdom. Its formation followed the 1843 establishment of the Health of Towns Commission, chaired by Sir Edwin Chadwick, which produced a series of reports on poor and insanitary conditions in British cities.\n\nThese national and local movements led to the Public Health Act, finally passed in 1848. It aimed to improve the sanitary condition of towns and populous places in England and Wales by placing the supply of water, sewerage, drainage, cleansing and paving under a single local body with the General Board of Health as a central authority. The Act was passed by the Liberal government of Lord John Russell, in response to the urging of Edwin Chadwick. Chadwick's seminal report on \"The Sanitary Condition of the Labouring Population\" was published in 1842 and was followed up with a supplementary report a year later.\n\nVaccination for various diseases was made compulsory in the United Kingdom in 1851, and by 1871 legislation required a comprehensive system of registration run by appointed vaccination officers.\n\nFurther interventions were made by a series of subsequent Public Health Acts, notably the 1875 Act. Reforms included latrinization, the building of sewers, the regular collection of garbage followed by incineration or disposal in a landfill, the provision of clean water and the draining of standing water to prevent the breeding of mosquitoes.\n\nThe Infectious Disease (Notification) Act 1889 mandated the reporting of infectious diseases to the local sanitary authority, which could then pursue measures such as the removal of the patient to hospital and the disinfection of homes and properties.\n\nIn the United States, the first public health organization based on a state health department and local boards of health was founded in New York City in 1866.\n\nThe science of epidemiology was founded by John Snow's identification of a polluted public water well as the source of an 1854 cholera outbreak in London. Dr. Snow believed in the germ theory of disease as opposed to the prevailing miasma theory. He first publicized his theory in an essay, \"On the Mode of Communication of Cholera\", in 1849, followed by a more detailed treatise in 1855 incorporating the results of his investigation of the role of the water supply in the Soho epidemic of 1854.\n\nBy talking to local residents (with the help of Reverend Henry Whitehead), he identified the source of the outbreak as the public water pump on Broad Street (now Broadwick Street). Although Snow's chemical and microscope examination of a water sample from the Broad Street pump did not conclusively prove its danger, his studies of the pattern of the disease were convincing enough to persuade the local council to disable the well pump by removing its handle.\n\nSnow later used a dot map to illustrate the cluster of cholera cases around the pump. He also used statistics to illustrate the connection between the quality of the water source and cholera cases. He showed that the Southwark and Vauxhall Waterworks Company was taking water from sewage-polluted sections of the Thames and delivering the water to homes, leading to an increased incidence of cholera. Snow's study was a major event in the history of public health and geography. It is regarded as the founding event of the science of epidemiology.\n\nWith the pioneering work in bacteriology of French chemist Louis Pasteur and German scientist Robert Koch, methods for isolating the bacteria responsible for a given disease and vaccines for remedy were developed at the turn of the 20th century. British physician Ronald Ross identified the mosquito as the carrier of malaria and laid the foundations for combating the disease. Joseph Lister revolutionized surgery by the introduction of antiseptic surgery to eliminate infection. French epidemiologist Paul-Louis Simond proved that plague was carried by fleas on the back of rats, and Cuban scientist Carlos J. Finlay and U.S. Americans Walter Reed and James Carroll demonstrated that mosquitoes carry the virus responsible for yellow fever. Brazilian scientist Carlos Chagas identified a tropical disease and its vector.\n\nWith onset of the epidemiological transition and as the prevalence of infectious diseases decreased through the 20th century, public health began to put more focus on chronic diseases such as cancer and heart disease. Previous efforts in many developed countries had already led to dramatic reductions in the infant mortality rate using preventative methods. In Britain, the infant mortality rate fell from over 15% in 1870 to 7% by 1930.\n\nFrance 1871-1914 followed well behind Bismarckian Germany, as well as Great Britain, in developing the welfare state including public health. Tuberculosis was the most dreaded disease of the day, especially striking young people in their 20s. Germany set up vigorous measures of public hygiene and public sanatoria, but France let private physicians handle the problem, which left it with a much higher death rate. The French medical profession jealously guarded its prerogatives, and public health activists were not as well organized or as influential as in Germany, Britain or the United States. For example, there was a long battle over a public health law which began in the 1880s as a campaign to reorganize the nation's health services, to require the registration of infectious diseases, to mandate quarantines, and to improve the deficient health and housing legislation of 1850. However the reformers met opposition from bureaucrats, politicians, and physicians. Because it was so threatening to so many interests, the proposal was debated and postponed for 20 years before becoming law in 1902. Success finally came when the government realized that contagious diseases had a national security impact in weakening military recruits, and keeping the population growth rate well below Germany's.\n\nModern public health began developing in the 19th century, as a response to advances in science that led to the understanding of, the source and spread of disease. As the knowledge of contagious diseases increased, means to control them and prevent infection were soon developed. Once it became understood that these strategies would require community-wide participation, disease control began being viewed as a public responsibility. Various organizations and agencies were then created to implement these disease preventing strategies.\n\nMost of the Public health activity in the United States took place at the municipal level before the mid-20th century. There was some activity at the national and state level as well.\n\nIn the administration of the second president of the United States John Adams, the Congress authorized the creation of hospitals for mariners. As the U.S. expanded, the scope of the governmental health agency expanded.\n\nIn the United States, public health worker Sara Josephine Baker, M.D. established many programs to help the poor in New York City keep their infants healthy, leading teams of nurses into the crowded neighborhoods of Hell's Kitchen and teaching mothers how to dress, feed, and bathe their babies.\n\nAnother key pioneer of public health in the U.S. was Lillian Wald, who founded the Henry Street Settlement house in New York. The Visiting Nurse Service of New York was a significant organization for bringing health care to the urban poor.\n\nDramatic increases in average life span in the late 19th century and 20th century, is widely credited to public health achievements, such as vaccination programs and control of many infectious diseases including polio, diphtheria, yellow fever and smallpox; effective health and safety policies such as road traffic safety and occupational safety; improved family planning; tobacco control measures; and programs designed to decrease non-communicable diseases by acting on known risk factors such as a person's background, lifestyle and environment.\n\nAnother major public health improvement was the decline in the \"urban penalty\" brought about by improvements in sanitation. These improvements included chlorination of drinking water, filtration and sewage treatment which led to the decline in deaths caused by infectious waterborne diseases such as cholera and intestinal diseases.\nThe federal Office of Indian Affairs (OIA) operated a large-scale field nursing program. Field nurses targeted native women for health education, emphasizing personal hygiene and infant care and nutrition.\n\nPublic health issues were important for the Spanish empire during the colonial era. Epidemic disease was the main factor in the decline of indigenous populations in the era immediately following the sixteenth-century conquest era and was a problem during the colonial era. The Spanish crown took steps in eighteenth-century Mexico to bring in regulations to make populations healthier.\n\nIn the late nineteenth century, Mexico was in the process of modernization, and public health issues were again tackled from a scientific point of view. Even during the Mexican Revolution (1910–20), public health was an important concern, with a text on hygiene published in 1916. During the Mexican Revolution, feminist and trained nurse Elena Arizmendi Mejia founded the Neutral White Cross, treating wounded soldiers no matter for what faction they fought.\n\nIn the post-revolutionary period after 1920, improved public health was a revolutionary goal of the Mexican government.\nThe Mexican state promoted the health of the Mexican population, with most resources going to cities. Concern about disease conditions and social impediments to the improvement of Mexicans' health were important in the formation of the Mexican Society for Eugenics. The movement flourished from the 1920s to the 1940s. Mexico was not alone in Latin America or the world in promoting eugenics. Government campaigns against disease and alcoholism were also seen as promoting public health.\n\nThe Mexican Social Security Institute was established in 1943, during the administration of President Manuel Avila Camacho to deal with public health, pensions, and social security.\n\nSince the 1959 Cuban Revolution the Cuban government has devoted extensive resources to the improvement of health conditions for its entire population via universal access to health care. Infant mortality has plummeted. Cuban medical internationalism as a policy has seen the Cuban government sent doctors as a form of aid and export to countries in need in Latin America, especially Venezuela, as well as Oceania and Africa countries.\n\nPublic health was important elsewhere in Latin America in consolidating state power and integrating marginalized populations into the nation-state. In Colombia, public health was a means for creating and implementing ideas of citizenship. In Bolivia, a similar push came after their 1952 revolution.\n\nThough curable and preventative, malaria remains a huge public health problem and is the third leading cause of death in Ghana. In the absence of a vaccine, mosquito control, or access to anti-malaria medication, public health methods become the main strategy for reducing the prevalence and severity of malaria. These methods include reducing breeding sites, screening doors and windows, insecticide sprays, prompt treatment following infection, and usage of insecticide treated mosquito nets. Distribution and sale of insecticide-treated mosquito nets is a common, cost-effective anti-malaria public health intervention; however, barriers to use exist including cost, hosehold and family organization, access to resources, and social and behavioral determinants which have not only been shown to affect malaria prevalence rates but also mosquito net use.\n\n"}
{"id": "55198067", "url": "https://en.wikipedia.org/wiki?curid=55198067", "title": "Relapsing–remitting", "text": "Relapsing–remitting\n\nRelapsing–remitting is a medical term referring to a presentation of disease symptoms that become worse over time (relapsing), followed by periods of less severe symptoms that do not completely cease (remitting). The term is sometimes applied to groupings of presentations of particular diseases that display this pattern, such as:\n\n"}
{"id": "26196692", "url": "https://en.wikipedia.org/wiki?curid=26196692", "title": "Remote therapy", "text": "Remote therapy\n\nRemote therapy, sometimes called telemental health applications or Internet-based psychotherapy, is a form of psychotherapy or related psychological practice in which a trained psychotherapist meets with a client or patient via telephone, cellular phone, the internet or other electronic media in place of or in addition to conventional face-to-face psychotherapy.\n\nInitially, it was primarily intended a substitution for conventional, face-to-face therapy in which a client or patient is required to visit a psychotherapists office. Increasingly, however, academics are studying the use of electronic media in treatment to explore the possibility of providing novel and potentially more effective therapies.\n\nAfter reviewing thirteen relevant studies, the authors of a meta-analytic review of psychotherapy mediated by remote communications technology concluded that:\n\nRemote therapy has the potential to overcome some of the barriers to conventional psychological therapy services. Telephone-based interventions are a particularly popular research focus and as a means of therapeutic communication may confer specific advantages in terms of their widespread availability and ease of operation. However, the available evidence is limited in quantity and quality. More rigorous trials are required to confirm these preliminary estimates of effectiveness.\n\nDespite the absence of complete study, remote therapy has enjoyed growing popularity as a replacement for traditional therapy and innovative practice made possible by electronic medium. Examples include:\n\n"}
{"id": "1826989", "url": "https://en.wikipedia.org/wiki?curid=1826989", "title": "Self-healing", "text": "Self-healing\n\nSelf-healing refers to the process of recovery (generally from psychological disturbances, trauma, etc.), motivated by and directed by the patient, guided often only by instinct. Such a process encounters mixed fortunes due to its amateur nature, although self-motivation is a major asset. The value of self-healing lies in its ability to be tailored to the unique experience and requirements of the individual. The process can be helped and accelerated with introspection techniques such as Meditation. \n\nHistorically, communities of Color in the United States and around the World have attempted to use different modes (i.e. scholarship, art, and community gathering) to create self-healing as a way to combat the daily trauma of living in a racialized society.\n\nFrantz Fanon wrote about the subjectivity and objectivity paradox inherent in Blackness in his book Black Skin, White Masks. He describes feeling \"infinite\" but being subjected to the standards reserved for someone who is crippled. At the end of the fifth chapter, he writes that he weeps at the \"crossroads between Nothingness and Infinity.\" His scholarship is meant to provide a lesson to White readers: Black people are also human beings. However, his intention in writing the fifth chapter was no doubt a hope to heal himself in offering up his struggle so that those who mistreated him and his people could reach a place of understanding. In sharing his personal trauma, he offered a way for Black people to connect over shared struggle and commiserate, but also brought his plight to the attentions of a White audience, who, if they could empathize, could lighten the racial load of the Black people around them and could hopefully, someday, ease Fanon's own mental load.\n\nLangston Hughes is an example of how Black people have used art to self-heal from racial trauma. His poem, \"Theme for English B\" details his struggle with completing a writing assignment about truth for a class. He is only able to complete the assignment when he acknowledges the stratified differences between him, his other classmates, and his professor. As he writes, \"I am the only colored student in my class.\" His poem is a way to package his trauma so that he can use it for something constructive in the hopes that it will ultimately heal some of his pain.\n\nHarriet's Apothecary is a NYC-based organization that seeks to create self-healing for communities of color through different healing based events. Their work takes place across the US. They host vendors, and offer reiki-healing, massages, food, and yoga among many other different \"stations\" as a way to combat racial trauma. They do community building work to address poverty (because racism in the United States has left communities of color in disproportionate levels of poverty compared to their White counterparts). To gain entrance to most, if not all of their events, their policy is pay-what-you-can.\n\nSelf-healing is the ultimate phase of Gestalt Therapy.\n\nSelf-healing may refer to automatic, homeostatic processes of the body that are controlled by physiological mechanisms inherent in the organism. Disorders of the spirit and the absence of faith can be self-healed.\n\nIn a figurative sense, self-healing properties can be ascribed to systems or processes, which by nature or design tend to correct any disturbances brought into them. Such as the regeneration of the skin after a cut or scrape, or of an entire limb. The injured party (the living body) repairs the damaged part by itself.\n\nBeyond the innate restorative capacities of the physical body, there are many factors of psychological nature that can influence self-healing. Hippocrates, considered by many to be the father of medical treatment, observed: \"The physician must be ready, not only to do his duty himself, but also to secure the co-operation of the patient, of the attendants and of externals.\"\n— Hippocrates.\n\nSelf-healing may also be achieved through deliberately applied psychological mechanisms. These approaches may improve the psychological and physical conditions of a person. Research confirms that this can be achieved through numerous mechanisms, including relaxation, breathing exercises, fitness exercises, imagery, Meditation, Yoga, qigong, t'ai chi, biofeedback, and various forms of psychotherapy, among other approaches.\n\nVarieties of mechanisms for self-healing have been proposed, including:\n\n\nAnother phrase that often includes self-healing is self-help. In 2013 Kathryn Schulz examined it as \"an $11 billion industry\".\n\nTwelve-step programs support individuals recovering from dysfunctional families and addictive/compulsive behaviors.\n\n"}
{"id": "25010568", "url": "https://en.wikipedia.org/wiki?curid=25010568", "title": "Sex Roles (journal)", "text": "Sex Roles (journal)\n\nSex Roles is a peer-reviewed scientific journal published by Springer. Articles appearing in \"Sex Roles\" are written from a feminist perspective, and topics span gender role socialization, gendered perceptions and behaviors, gender stereotypes, body image, violence against women, gender issues in employment and work environments, sexual orientation and identity, and methodological issues in gender research. The Editor-in-Chief is Janice D. Yoder.\n\n\"Sex Roles\" is abstracted/indexed in:\n\nAccording to the \"Journal Citation Reports\", the journal has a 2016 impact factor of 1.954, ranking it 1st out of 41 journals in the category \"Women's Studies\" and 11th out of 62 journals in the category, \"Social Psychology\".\n\n"}
{"id": "11196670", "url": "https://en.wikipedia.org/wiki?curid=11196670", "title": "Strengths and Difficulties Questionnaire", "text": "Strengths and Difficulties Questionnaire\n\nThe Strengths and Difficulties Questionnaire (SDQ) is a self-report inventory behavioral screening questionnaire for children and adolescents ages 2 through 17 years old, developed by United Kingdom child psychiatrist Robert N. Goodman. The SDQ is available online and has been translated into more than 80 languages, including Spanish, Chinese, Russian, and Portuguese. Overall, the SDQ has been proven to have satisfactory construct and concurrent validity.\n\nThere are three versions of the SDQ: a \"short form\", a \"longer form\" with impact supplement, and a \"follow-up form\" designed for use after a behavioral intervention. The questionnaire takes 3–10 minutes to complete.\n\nThe SDQ scoring site allows one to score paper copies of a parent, teacher and/or self-report and generates a brief report. Each of the five scales of the SDQ are scored from 0-10, and one can add up four of these (emotional, conduct, hyperactivity and peer problems) to create a total difficulty score (range 0-40). One can also add the emotional and peer items together to get an internalising problems score (range 0-20) and add the conduct and hyperactivity questions together to get an externalising score (range 0-20).\n\n\nOne can also use the 5 scales of the SDQ either as dimensions (range 0-10) or else categorised into three- or four-categories in a similar was as is done for the total difficulty score. Alternatively, one can combine scales such that emotional+peer problems scales form a larger internalising scale (range 0-20) and the conduct + hyperactivity scales form a larger externalising scale (range 0=20). Research suggests that in low-risk groups (e.g. a general population sample) the latter approach may be preferable, as there is less discrimination between the finer scales for lower scores (e.g. a child with a score of 2 on the hyperactivity scale is at increased risk of hyperactivity and behavioural disorder to a roughly equal degree). By contrast in high-risk samples (e.g. a clinic setting) the 5 finer scales may come into their own, as there is much more discrimination between them at higher scores (e.g. a child with a score of 7 on the hyperactivity scale is specifically at greater risk of a hyperactivity disorder, to a greater degree than they are at increased risk of a behavioural disorder)\n\nThe PhenX Toolkit uses SDQ as its child protocol for General Psychiatric Assessment and for Broad Psychopathology.. The SDQ is implemented in numerous electronic health record systems. \n\nThe Strengths and Difficulties Questionnaires, whether in English or in translation, are copyrighted documents that may not be modified in any way. Paper versions of the SDQ can be downloaded from the SDQ website in a wide variety of languages, and can be printed or photocopied without charge by individuals or non-profit organizations provided those organisations are not making any charge to families. No one except Youth in Mind is authorized to create or distribute electronic versions for any purpose - individuals or organisations wishing to create electronic versions (e.g. for a computer-administered survey) need to contact Youth in Mind and may need to pay a licence fee. It is also illegal to make or distribute unauthorised translations of the SDQ - individuals wishing to translate the SDQ to a new language should contact Youth in Mind to discuss arranging a full process of translation, back-translation and authorisation.\n\n"}
{"id": "26875658", "url": "https://en.wikipedia.org/wiki?curid=26875658", "title": "Studies on intercessory prayer", "text": "Studies on intercessory prayer\n\nSome religions claim that praying for somebody who is sick can have positive effects on the health of the person being prayed for.\n\nMeta-studies of the literature in the field have been performed showing evidence only for no effect or a potentially small effect. For instance, a 2006 meta analysis on 14 studies concluded that there is \"no discernible effect\" while a 2007 systemic review of intercessory prayer reported inconclusive results, noting that 7 of 17 studies had \"small, but significant, effect sizes\" but the review noted that the most methodologically rigorous studies failed to produce significant findings.\n\nIn comparison to other fields that have been scientifically studied, carefully monitored studies of prayer are relatively few. The field remains tiny, with about $5 million spent worldwide on such research. If and when more studies of prayer are done, the issue of prayer's efficacy may be further clarified.\n\nThe third party studies discussed here have all been performed using Christian prayers. Some have reported null results, some have reported correlations between prayer and health, and some have reported contradictory results in which beneficiaries of prayer had worsened health outcomes. The parameters used within the study designs have varied, for instance, daily or weekly prayers, whether to provide patient photographs, with full or partial names, measuring levels of belief in prayer, and whether patients underwent surgery.\n\nIn 1872, the Victorian scientist Francis Galton made the first statistical analysis of third-party prayer. He hypothesized, partly as satire, that if prayer were effective, members of the British Royal Family would live longer than average, given that thousands prayed for their well-being every Sunday, and he prayed over randomized plots of land to see whether the plants would grow any faster, and found no correlation in either case.\n\nA 1988 study by Randolph C. Byrd used 393 patients at the San Francisco General Hospital coronary care unit (CCU). Measuring 29 health outcomes using three-level (good, intermediate, or bad) scoring, the prayer group suffered fewer newly diagnosed ailments on only six of them. Byrd concluded that \"Based on these data there seemed to be an effect, and that effect was presumed to be beneficial\", and that \"intercessory prayer to the Judeo-Christian God has a beneficial therapeutic effect in patients admitted to a CCU.\" The reaction from the scientific community concerning this study was mixed. Several reviewers considered Byrd’s study to be well-designed and well-executed, while others remained skeptical. A criticism of Byrd's study, which also applies to most other studies, is the fact that he did not limit prayers by the friends and family of patients, hence it is unclear which prayers, if any, may have been measured.\n\nThe Byrd study had an inconsistent pattern of only six positive outcomes amongst 26 specific problem conditions. A systematic review suggested this indicates possible Type I errors.\n\nA 1999 follow-up by William S. Harris et al. attempted to replicate Byrd's findings under stricter experimental conditions, noting that the original research was not completely blinded and was limited to only \"prayer-receptive\" individuals (57 of the 450 patients invited to participate in the study refused to give consent \"for personal reasons or religious convictions\"). Using a different, continuous weighted scoring system – which admittedly was, like Byrd's scoring, \"an unvalidated measure of CCU outcomes\" – Harris et al. concluded that \"supplementary, remote, blinded, intercessory prayer produced a measurable improvement in the medical outcomes of critically ill patients\", and suggested that \"prayer be an\neffective adjunct to standard medical care.\" However, when they applied Byrd’s scores to their data, they could not document an effect of prayer using his scoring method. Critics have suggested that both Byrd's and Harris's results can be explained by chance. Richard P. Sloan compared the Byrd and Harris studies with the sharpshooter fallacy, \"searching through the data until a significant effect is found, then drawing the bull's-eye.\"\n\nA 1997 study by O'Laoire measured the effects on the agents performing daily prayers and reported benefits not only for the beneficiaries, but also for the agents, and the benefit levels correlated with the belief levels of agents and beneficiaries in some cases. The study measured anxiety and depression. This study used beneficiary names as well as photographs.\n\nIn 1998 Fred Sicher et al. performed a small scale double-blind randomized study of 40 patients with advanced AIDS. The patients were in category C-3 with CD4 cell counts below 200 and each had at least one case of AIDS-defining illness. The patients were randomly assigned to receive distant intercessory healing or none at all. The intercession took place by people in different parts of the United States who never had any contact with the patients. Both patients and physicians were blind to who received or did not receive intercession. Six months later the prayer group had significantly fewer AIDS illnesses, less frequent doctor visits, and fewer days in the hospital. However, CD4 counts and scores on other physiological tests had no significant variation between the two groups of patients.\n\nA 2001 double-blind study at the Mayo Clinic randomized 799 discharged coronary surgery patients into a control group and an intercessory prayer group, which received prayers at least once a week from 5 intercessors per patient. Analyzing \"primary end points\" (death, cardiac arrest, rehospitalization, etc.) after 26 weeks, the researchers concluded \"intercessory prayer had no significant effect on medical outcomes after hospitalization in a coronary care unit.\"\n\nIn 2001 the \"Journal of Reproductive Medicine\" published an experimental study by three Columbia University researchers indicating that prayer for women undergoing \"in vitro\" fertilization-embryo transfer (IVF-ET) resulted in a success rate (50%) of pregnancy double that of women who did not receive prayer. Columbia University issued a news release saying that the study had been carefully designed to eliminate bias. The most vocal skeptic was Bruce Flamm, a clinical professor of gynecology and obstetrics at the University of California at Irvine, who found that the experimental procedures were flawed. One of the study's authors, Cha, responded to criticism of the study in the November 2004 issue of JRM. In December 2001, the U.S. Department of Health and Human Services' (DHHS) office for Human Research Protections (OHRP) confirmed a report by the Columbia University Health Sciences Division that one of the study’s authors, Rogerio Lobo, only learned of the study six to twelve months after the study was completed, and that he had only provided editorial assistance. The name of Columbia University and Lobo were retracted from the study.\n\nA 2001 study by Leonard Leibovici used records of 3,393 patients who had developed blood infections at the Rabin Medical Center between 1990 and 1996 to study \"retroactive\" intercessory prayer. To compound the alleged miraculous power of prayer itself, the prayers were performed \"after\" the patients had already left the hospital. All 3,393 patients were those in the hospital between 1990 and 1996, and the prayers were conducted in 2000. Two of the outcomes, length of stay in the hospital and duration of fever, were found to be significantly improved in the intervention group, implying that prayer can even change events \"in the past.\" However, the \"mortality rate was lower in the intervention group, but the difference between the groups was not significant.\" Leibovici concluded that \"Remote, retroactive intercessory prayer was associated with a shorter stay in hospital and a shorter duration of fever in patients with a bloodstream infection.\" Leibovici goes on to note that in the past, people knew the way to prevent diseases (he cites scurvy) without understanding why it worked. In saying so, he suggests that if prayer truly does have a positive effect on patients in hospital, then there may be a naturalist explanation for it that we do not yet understand. After many scientists and scholars criticized this retroactive study, Leibovici stated in 2002 that the \"article has nothing to do with religion. I believe that prayer is a real comfort and help to a believer. I do not believe it should be tested in controlled trials.\" The study has been summarised as being \"intended lightheartedly to illustrate the importance of asking research questions that fit with scientific models.\"\n\nIn 2003, Larry Dossey, the executive editor of the journal \"\" and an advocate of faith healing co-authored a paper responding to Leibovici which discussed possible mechanisms to explain the results reported. Olshansky and Dossey invoked quantum mechanics to explain not only the benefits of intercessory prayer, but also how it might operate retroactively, drawing strong criticisms from physicist Victor Stenger and physician Jeffrey Bishop. The observer effect is regularly used to suggest that conscious control of physical reality is predicted by quantum mechanics, but this misconception \"can be traced to a misinterpretation of wave-particle duality.\" In relation to backwards causality, Stenger noted that \"the results of some quantum experiments may be interpreted as evidence for events in the future affecting events in the past at the quantum level, [but] no theoretical basis exists for applying this notion on the macroscopic scale of human experience.\" He concluded that while \"the atoms in biological systems are quantum in nature ... their collective behaviour does not exhibit any quantum effects. ... What is more, even if the brain were a quantum system, that would not imply that it can break the laws of physics any more than electrons or photons, which are inarguably quanta.\" One further point which illustrates that Dossey and Olshansky do not understand the physics they are using is seen in their invocation of quantum nonlocality in explaining backward causation, stating that \"[r]etroactive prayer may be less absurd than [Leibovici] supposes, in the light of the discovery of non-local phenomena.\" Unfortunately, the two are mutually incompatible: in allowing reverse causality in a model, the phenomenon of nonlocality ceases. Dossey has written in \"Explore\" about coining the term \"nonlocal mind\" in 1987, though quantum nonlocality goes back to a 1935 paper by Einstein, Podolsky, and Rosen. Olshansky and Dossey defended their work from various critics in the \"British Medical Journal's rapid response section\n\nOlensky and Dossey are not alone amongst alternative medicine proponents in having missed the point which Leibovici was making. In 2004, Stephen Wright described the Olshansky and Dossey contribution as a \"thoughtful essay,\" and it was praised by an editorial in the \"Journal of Alternative and Complementary Medicine\" the same year. In 2005, Olshansky and Dossey's work was included in a critical review published in \"Explore\" which concluded that \"Religious activity may improve health outcomes.\" Their work was also defended in the \"British Medical Journal\" itself in 2004. Dossey authored an \"Explore\" paper defending experiments on the medical effects of prayer in 2005.\n\nA 2005 MANTRA (Monitoring and Actualisation of Noetic Trainings) II study conducted a three-year clinical trial led by Duke University comparing intercessory prayer and MIT (Music, Imagery, and Touch) therapies for 748 cardiology patients. The study is regarded as the first time rigorous scientific protocols were applied on a large scale to assess the feasibility of intercessory prayer and other healing practices. The study produced null results and the authors concluded, \"Neither masked prayer nor MIT therapy significantly improved clinical outcome after elective catheterization or percutaneous coronary intervention.\" Neither study specified whether photographs were used or whether belief levels were measured in the agents or those performing the prayers.\n\nHarvard professor Herbert Benson performed a \"Study of the Therapeutic Effects of Intercessory Prayer (STEP)\" in 2006. The STEP, commonly called the \"Templeton Foundation prayer study\" or \"Great Prayer Experiment\", used 1,802 coronary artery bypass surgery patients at six hospitals. Using double-blind protocols, patients were randomized into three groups, individual prayer receptiveness was not measured. The members of the experimental and control Groups 1 and 2 were informed they might or might not receive prayers, and only Group 1 received prayers. Group 3, which served as a test for possible psychosomatic effects, was informed they would receive prayers and subsequently did. Unlike some other studies, STEP attempted to standardize the prayer method. Only first names and last initial for patients were provided and no photographs were supplied. The congregations of three Christian churches who prayed for the patients \"were allowed to pray in their own manner, but they were instructed to include the following phrase in their prayers: \"for a successful surgery with a quick, healthy recovery and no complications\". Some participants complained that this mechanical way they were told to pray as part of the experiment was unusual for them. Complications of surgery occurred in 52 percent of those who received prayer (Group 1), 51 percent of those who did not receive it (Group 2), and 59 percent of patients who knew they would receive prayers (Group 3). There were no statistically significant differences in major complications or thirty-day mortality. In \"The God Delusion,\" evolutionary biologist Richard Dawkins wrote, \"It seems more probable that those patients who knew they were being prayed for suffered additional stress in consequence: performance anxiety', as the experimenters put it. Dr Charles Bethea, one of the researchers, said, \"It may have made them uncertain, wondering am I so sick they had to call in their prayer team?'\" Study co-author Jeffery Dusek stated that: \"Each study builds on others, and STEP advanced the design beyond what had been previously done. The findings, however, could well be due to the study limitations.\" Team leader Benson stated that STEP was not the last word on the effects of intercessory prayer and that questions raised by the study will require additional answers.\n\nA meta-analysis of several studies related to distant intercessory healing was published in the \"Annals of Internal Medicine\" in 2000. The authors analyzed 23 trials of 2,774 patients. Five of the trials were for prayer as the distant healing method, 11 were with noncontact touch, and 7 were other forms. Of these trials, 13 showed statistically significant beneficial treatment results, 9 showed no effect, and 1 showed a negative result. The authors concluded that it is difficult to draw conclusions regarding distant healing and suggested further studies.\n\nA 2003 levels of evidence review found \"some\" evidence for the hypothesis that \"Being prayed for improves physical recovery from acute illness\". It concluded that although \"a number of studies\" have tested this hypothesis, \"only three have sufficient rigor for review here\" (Byrd 1988, Harris et al. 1999, and Sicher et al. 1998). In all three, \"the strongest findings were for the variables that were evaluated most subjectively. This raises concerns about the possible inadvertent unmasking of the outcomes assessors. Moreover, the absence of a clearly plausible biological mechanism by which such a treatment could influence hard medical outcome results in the inclination to be skeptical of results.\" This 2003 review was performed before the 2005 MANTRA study and the 2006 STEP project, neither of which were conclusive in establishing the efficacy of prayer.\n\nVarious broader meta-studies of the literature in the field have been performed showing evidence only for no effect or a potentially small effect. For instance, a 2006 meta analysis on 14 studies concluded that \"There is no scientifically discernable effect for intercessory prayer as assessed in controlled studies\". However, a 2007 systemic review of 17 intercessory prayer studies found \"small, but significant, effect sizes for the use of intercessory prayer\" in 7 studies, but \"prayer was unassociated with positive improvement in the condition of client\" in the other 10, concluding that based upon the American Psychology Association's Division 12 (clinical psychology) criteria for evidence-based practice, intercessory prayer \"must be classified as an experimental intervention.\" The review noted that the most methodologically rigorous studies had failed to produce significant findings.\n\n\n"}
{"id": "23184143", "url": "https://en.wikipedia.org/wiki?curid=23184143", "title": "Sugary drink tax", "text": "Sugary drink tax\n\nA sugary drink tax or soda tax is a tax or surcharge designed to reduce consumption of drinks with added sugar. Drinks covered under a soda tax often include carbonated soft drinks, sports drinks and energy drinks.\n\nThe tax is a matter of public debate in many countries and beverage producers like Coca-Cola often oppose it. Advocates such as national medical associations and the World Health Organization promote the tax as an example of Pigovian taxation, aimed to discourage unhealthy diets and offset the growing economic costs of obesity.\n\nType II diabetes is a growing health concern in many developed and developing countries around the world, with 1.6 million deaths directly due to this disease in 2015 alone. Unlike sugar from food, the sugar from drinks enters the body so quickly that it can overload the pancreas and the liver, leading to diabetes and heart disease over time. A 2010 study said that consuming one to two sugary drinks a day increases your risk of developing diabetes by 26%.\n\nHeart disease is responsible for 31% of all global deaths and although one sugary drink has minimal effects on the heart, consuming sugary drinks daily are associated with long term consequences. A study found that men, for every added serving per day of sugar-sweetened beverages, each serving was associated with a 19% increased risk of developing heart disease. Another study also found increased risks for heart disease in women who drank sugary drinks daily.\n\nObesity is also a global public and health policy concern, with the percentage of overweight and obese people in many developed and middle income countries rising rapidly. Consumption of added sugar in sugar-sweetened beverages has been positively correlated with high calorie intake, and through it, with excess weight and obesity. The addition of one sugar-sweetened beverage per day to the normal US diet can amount to 15 pounds of weight gain over the course of 1 year. Added sugar is a common feature of many processed and convenience foods such as breakfast cereals, chocolate, ice cream, cookies, yogurts and drinks produced by retailers. The ubiquity of sugar-sweetened beverages and their appeal to younger consumers has made their consumption a subject of particular concern by public health professionals. In both the United States and the United Kingdom, sugar sweetened drinks are the top calorie source in teenager's diets.\n\nTrends indicate that traditional soda consumption is declining in many developed economies, but growing rapidly in middle income economies such as Vietnam and India. In the United States, the single biggest market for carbonated soft drinks, consumers annual average per capita purchase of soda was 154 liters.\n\nDenmark began taxing soft drinks and juices in the 1930s. More recently, Finland reintroduced an earlier soft drink tax in 2011, while Hungary taxes sugary drinks as part of its 2011 public health product tax, which covers all food products with unhealthy levels of sugar. France introduced a targeted sugar tax on soft drinks in 2012. At a national level similar measures have also been announced in Mexico in 2013 and in the United Kingdom in 2016. In November 2014, Berkeley, California was the first city in the U.S. to pass a targeted tax on surgary drinks.\n\nProponents of soda taxes cite the success of tobacco taxes worldwide when explaining why they think a soda tax will work to lower soda consumption. Where the main concern with tobacco is cancer, the main concerns with soda are diabetes and obesity. The tactics used to oppose soda taxes by soda companies mimic those of tobacco companies, including funding research that downplays the health risks of its products.\n\nThe U.S. Department of Health and Human Services reports that a targeted tax on sugar in soda could generate $14.9 billion in the first year alone. The Congressional Budget Office (CBO) estimates that three-cent-per-ounce tax would generate over $24 billion over four years. Some tax measures call for using the revenue collected to pay for relevant health needs: improving diet, increasing physical activity, obesity prevention, nutrition education, advancing healthcare reform, etc. Another area to which the revenue raised by a soda tax might go, as suggested by Mike Rayner of the United Kingdom, is to subsidize healthier foods like fruits and vegetables.\n\nThe imposition of a sugar tax means that sellers of sugary drinks would have to increase the price of their goods by an amount P2 from the original price X, and then take on the rest of the tax themselves (P1) in the form of lower profit per unit sold. The tax burden on consumers (P2) makes it more expensive for consumers to buy sugary drinks and hence a higher proportion of their incomes would have to be spent to buy the same amount of sugary drinks. This decreases the equilibrium quantity of sugary drinks that will be sold. Whether the sugary drinks tax is imposed on the seller or consumer, in both cases the tax burden is shared between both.\n\nThe way that the tax burden is divided upon the consumer and seller depends on the price elasticity for sugary drinks. The tax burden will fall more on sellers when the price elasticity of demand is greater than the price elasticity of supply while on buyers when the price elasticity of supply is greater than the price elasticity of demand. The price elasticity for sugary drinks is different from country to country. For instance, the price elasticity of demand for sugary drinks was found to be -1.37 in Chile while -1.16 in Mexico. Hence if both of those results were realistic and the price elasticity of supply would be the same for both, the tax burden on consumers would be higher in Mexico than in Chile.\n\nThe reasons for a sugar tax are the negative externalities of consuming sugar. As over-consumption of sugar causes health problems (external costs) such as obesity, type 2 diabetes and other diseases, and lost productivity, the third party impacted by this is the ‘public health system’ that will need to deal with those issues. More demand for health services leads to higher costs for health care and hence this increased stress on the public health system is a negative consumption externality of sugar consumption.\n\nIn economics terms, the marginal social benefit (MSB) of sugar consumption is less than the marginal private benefit (MB). This can also be illustrated in the following equation. MSB = MB – Marginal External Cost (MXC). This is the case due to the fact that consumers think only of the benefit of sugar consumption to them (MB) and not the negative externalities to third parties (MXC) and so want to consume at the unregulated market equilibrium to maximize their utility. This means that there is overconsumption of sugar and a welfare loss is created.\n\nThe sugary drinks tax, a Pigovian tax, is a way to correct the negative externality by regulating the consumption of sugary drinks. Without a sugary drink tax, taxpayer money is used to pay for higher health care costs incurred from high consumption of sugar. Although this solution corrects the negative consumption externality, taxpayers that consume sugary drinks moderately and hence do not contribute to higher health care costs, still need to pay for this negative externality. Hence a sugary drinks tax may be a more appropriate solution as tax revenue that is collected from the sugar tax can be used to create childhood nutrition programs or obesity-prevention programs. This is a solution that could also correct the negative externality of sugar consumption as well as is a way to make the parties that cause the negative externality pay their fair share.\n\nThe Australian Beverages Council announced in June 2018 that the industry would cut sugar content by 10% by 2020, and by another 10% by 2025. This was seen as an attempt to stave off a sugar tax. There were no plans to reduce the sugar content in the high sugar drinks. The plan is primarily to increase consumption of low-sugar or no-sugar drinks. sales of Coca-Cola Amatil's fizzy drinks have fallen 8.1% by volume from 2016 to 2018. The Australian Medical Association continued to press for a sugar tax.\n\nA 2016 proposal for a 20% sugary drink tax, campaigned by Educar Consumidores, was turned down by the Colombian legislature despite popular support for it. Soda is often less expensive than bottled water in Colombia.\n\nDenmark instituted a soft drink tax in the 1930s (it amounted to 1.64 Danish krone per liter), but announced in 2013 that they were going to abolish it along with an equally unpopular fat tax, with the goal of creating jobs and helping the local economy. Critics claimed that the taxes were notably ineffective; to avoid the fat and sugar taxes, local retailers had complained that Danes simply went to Sweden and Germany, where prices were lower to buy butter, ice cream and soda. Denmark repealed the fat tax in January 2013 and repealed the tax on soft drinks in 2014.\n\nFrance first introduced a targeted tax on sugary drinks at a national level in 2012; following introduction, soft drinks are estimated to be up to 3.5% more expensive. Analysis by the market research firm Canadean found that sales of soft drinks declined in the year following the introduction of the tax, following several years of annual growth. However, the tax applies to both drinks with added sugars and drinks with artificial sweeteners, possibly limiting its effects on the healthfulness of soda products.\n\nA 2016 study by Mazzochi has shown that the sugary drinks tax saw a 19 euro-cent per liter increase in price of non-pure fruit juices, a 16 euro-cent per liter increase for diet sodas and little impact on regular soft drinks prices. The study also estimated that the quantity consumed of the taxed drinks has decreased by 9 centiliters per week per person after the tax has been implemented.\n\nHungary's tax, which came into effect in September 2011, is a 4-cent tax on foods and drinks that contain large quantities of sugar and salt, such as soft drinks, confectionery, salty snacks, condiments, and fruit jams. In 2016, the tax has resulted in a 22% reduction in energy drink consumption and 19% of people reduced their intake of sugary soft drinks.\n\nSoda tax introduced on May 1st 2018. The tax will see 30 cent per litre added to the price of popular sweetened drinks containing more than 8g of sugar per 100ml.\n\nIn September 2013, Mexico's president Enrique Peña Nieto, on his fiscal bill package, proposed a 10% tax on all soft drinks, especially carbonated drinks, with the intention of reducing the number of patients with diabetes and other cardiovascular diseases in Mexico, which has one of the world's highest rates of obesity. According to Mexican government data, in 2011, the treatment for each patient with diabetes cost the Mexican public health care system (the largest of Latin America) around 708 USD per year, with a total cost of 778,427,475 USD in 2010, and with each patient paying only 30 MXN (around 2.31 USD).\n\nIn September 2013, soda companies launched a media campaign to discourage the Mexican Chamber of Deputies and Senate from approving the 10% soda tax. They argued that such measure would not help reduce the obesity in Mexico and would leave hundreds of Mexicans working in the sugar cane industry jobless. They also publicly accused New York City Mayor Michael Bloomberg of orchestrating the controversial bill from overseas. In late October 2013, the Mexican Senate approved a 1 MXN per litre tax (around 0.08 USD) on sodas, along with a 5% tax on junk food.\n\nResearch has shown that Mexico's sugary drinks tax reduced soft drink consumption. According to a 2016 study published in \"BMJ\", annual sales of sodas in Mexico declined 6% in 2014 after the introduction of the soda tax. Monthly sales figures for December 2014 were down 12% on the previous two years. Households with the fewest resources had an average reduction in purchases of 9% in 2014, increasing to 17% by December. Furthermore, purchases of water and non-taxed beverages increased by about 4% on average. Whether the imposition of the tax and the resulting 6% decline in sales of soft drinks will have any measurable impact on long-term obesity or diabetes trends in Mexico has yet to be determined. The authors of the study urged the Mexican authorities to double the tax to further reduce consumption.\n\nA 2016 study published in \"PLoS Medicine\" suggested that a 10% excise tax on soda \"could prevent 189,300 new cases of Type 2 diabetes, 20,400 strokes and heart attacks, and 18,900 deaths among adults 35 to 94 years old\" over a ten-year period. The study also included that \"the reductions in diabetes alone could yield savings in projected healthcare costs of $983 million.\"\n\nA 2017 study in the \"Journal of Nutrition\" found a 6.3% reduction in soft drink consumption, with the greatest reductions \"among lower-income households, residents living in urban areas, and households with children. We also found a 16.2% increase in water purchases that was higher in low- and middle-income households, in urban areas, and among households with adults only.\"\n\nNorway has had a generalized sugar tax measure on refined sugar products since 1922, introduced to boost state income rather than reducing sugar consumption. Non-alcoholic beverages have since been separated from the general tax, and in 2017, the tax for sugary drinks was set to 3.34 kroner per litre.\n\nIn January 2018, the Norwegian government increased the sugar tax level by 83% for general sugar-containing ready-to-eat products, and 42% for beverages. The sugar tax per litre was bumped up to 4.75 kroner, and applies to beverages which are either naturally or artificially sweetened.\n\nIn the taxation reform law dubbed as the Tax Reform for Acceleration and Inclusion Law (TRAIN) signed by Philippine President Rodrigo Duterte in December 2017. It includes taxation on sugar-sweetened drinks which will be implemented the following year, as an effort to increase revenue and to fight obesity. Drinks with caloric and non-caloric sweeteners will be taxed ₱6.00 per liter, while those using high-fructose corn syrup, a cheap sugar substitute, will be taxed at ₱12 per liter.\n\nExempted from the sugar tax are all kinds of milk, whether powdered or in liquid form, ground and 3-in-1 coffee packs, and 100-percent natural fruit and vegetable juices, meal replacements and medically indicated drinks, as well as beverages sweetened with stevia or coco sugar. These drinks, especially 3-in-1 coffee drinks which are popular especially among lower-income families, are to be taxed as initially proposed by the House of Representatives version of the bill, but were exempted in the Senate version.\n\nSouth Africa proposed a sugar-sweetened beverages tax in the 2016 South African national government budget. South Africa introduced a sugar tax on 1 April 2018. The levy was fixed at 2.1 cents per gram of sugar, for each gram above 4g per 100ml of sweetened beverage. The levy excludes fruit juices.\n\nOn October 2017, the United Arab Emirates introduced a 50% tax on soft drinks and a 100% tax on energy drinks, to curb unhealthy consumption of sugary drinks that can lead to diabetes; it also added a 100% tax on cigarettes.\n\nIn the 2016 United Kingdom budget, the UK Government announced the introduction of a sugar tax, officially named the \"Soft Drinks Industry Levy\". The tax came into effect on 6 April 2018. Beverage manufacturers are taxed according to the volume of sugar-sweetened beverages they produce or import. The tax is imposed at the point of production or importation, in two bands. Drinks with total sugar content above 5g per 100 millilitres are taxed at 18p per litre and drinks above 8g per 100 millilitres at 24p per litre. The measure is estimated to generate an additional £1 billion a year in tax revenue which will be spent on funding for sport in UK schools.\n\nIt was proposed that pure fruit juices, milk-based drinks and the smallest producers would not be taxed. For other beverages there was an expectation that some manufacturers would reduce sugar content in order to avoid the taxation. Indeed, manufacturer A.G. Barr significantly cut sugar content in their primary product Irn-Bru in advance of the tax.\n\nNotable research on effect of excess sugar in modern diets in the United Kingdom includes the work of Professor John Yudkin with his book called, \"Pure, White and Deadly: The Problem of Sugar\" first published in 1972. With regard to a proposed tax on sugar-sweetened beverages, a study published in the British Medical Journal on 31 October 2013, postulated that a 20% tax on sugar-sweetened beverages would reduce obesity in the United Kingdom rates by about 1.3%, and concluded that taxing sugar-sweetened beverages was \"a promising population measure to target population obesity, particularly among younger adults.\"\n\nThe tax has been criticised on several grounds, including its likely efficacy and its narrow base. UK Member of Parliament Will Quince as, \"patronising, regressive and the nanny state at its worst.\" In addition a study by the University of Glasgow, which sampled 132,000 adults, found that focusing on sugar in isolation misleads consumers as reducing fat intake is also crucial to reducing obesity.\n\nFrom an opposing standpoint, Professor Robert Lustig of the University of California, San Francisco School of Medicine, has argued that the UK tax measure may not go far enough and that, \"juice should be taxed the same way as soda because from a metabolic standpoint juice is the same as soda.\" Campaigners have since called for the soft drinks tax to be extended to include confectionery and sweets to help tackle childhood obesity.\n\nThe United States does not have a nationwide soda tax, but a few of its cities have passed their own tax and the U.S. has seen a growing debate around taxing soda in various cities, states and even in Congress in recent years. A few states impose excise taxes on bottled soft drinks or on wholesalers, manufacturers, or distributors of soft drinks.\n\nMedical costs related to obesity in the United States alone were estimated to be $147 billion a year in 2009. In the same year, the American Heart Association reported that the soft drinks and sugar sweetened beverages are the largest contributors of added sugars in Americans' diets. Added sugars are sugars and syrups added to foods during processing or preparation and sugars and syrups added after preparation. Excessive intake of added sugars, as opposed to naturally occurring sugars, is implicated in the rise in obesity.\n\nPhiladelphia and Berkeley are the first two cities to pass a tax on sugary drinks in the U.S. Berkeley's tax of 1 cent/oz of sugary drink has seen a decline in soda consumption by more than 20 percent. Philadelphia's tax of 1.5 cents/oz took effect on January 1, 2017.\n\nThe Measure D soda tax was approved by 76% of Berkeley voters on 4 November 2014, and took effect on 1 January 2015 as the first such tax in the United States. The measure imposes a tax of one cent per ounce on the distributors of specified sugar-sweetened beverages such as soda, sports drinks, energy drinks, and sweetened ice teas but excluding milk-based beverages, meal replacement drink, diet sodas, fruit juice, and alcohol. The revenue generated will enter the general fund of the City of Berkeley. A similar measure in neighboring San Francisco received 54% of the vote, but fell short of the supermajority required to pass. In August 2015, researchers found that average prices for beverages covered under the law rose by less than half of the tax amount. For Coke and Pepsi, 22 percent of the tax was passed on to consumers, with the balance paid by vendors. UC Berkeley researchers found a higher pass-through rate for the tax: 47% of the tax was passed-through to higher prices of sugar-sweetened beverages overall with 69% being passed-through to higher soda prices. In August 2016, a UC Berkeley study showed a 21% drop in the drinking of soda and sugary beverages in low-income neighborhoods in its city.\n\nA study from 2016 compared the changing intake of sugar sweetened beverages and water in Berkeley versus San Francisco and Oakland (which did not have a sugary drink tax passed) before and after Berkeley passed its sugary drink tax. This analysis showed a 26% decrease of soda consumption in Berkeley and 10% increase in San Francisco and Oakland while water intake increased by 63% in Berkeley and 19% in the two neighboring cities. A 2017 before and after study has concluded that one year after the tax was introduced in Berkeley, sugary drink sales decreased by 9.6% when compared to a scenario where the tax was not in place. This same study was also able to show that overall consumer spending did not increase, contradicting the argument of opponents of the Sugary Drink Tax. Another 2017 study results were that purchases of healthier drinks went up and sales of sugary drinks went down, without overall grocery bills increasing or the local food sector losing money.\n\nDemocratic Philadelphia mayor Jim Kenney proposed a citywide soda tax that would raise the price of soda at three cents per ounce. At the time, it was the biggest soda tax proposal in the United States. Kenney promoted using tax revenue to fund universal pre-K, jobs, and development projects, which he predicted would raise $400 million over five years, all the while reducing sugar intake by decreasing the demand for sugary beverages. Kenney's soda tax proposal was brought to the national spotlight and divided key members of the Democratic Party. Presidential hopeful Bernie Sanders argued in an op-ed that the tax would hurt the poor. His opponent, Hillary Clinton, on the other hand, said that she was \"very supportive\" of the idea. The American Beverage Association (ABA), funded by soda companies and distributors, ran local television, radio, and newspaper advertisements against the idea, claiming that the tax would disproportionately hurt the poor. The ABA spent $10.6 million in 2016 in its effort against the tax. The American Medical Association, American Heart Association, and other medical and public health groups support the tax.\n\nThe Philadelphia City Council approved a 1.5-cents-per-ounce tax on 16 June 2016. As part of the compromise legislation that passed, the tax is also imposed on artificially sweetened beverages, such as diet soda. The law became effective on 1 January 2017. It was reported after two months of the tax that Philadelphia supermarkets and beverage distributors are planning layoffs because sugary beverage sales are down between 30 and 50 percent.\n\nAfter the tax took effect, Kenney said it was \"wrong\" and \"misleading\" for businesses to pass the tax on to their customers in the form of higher soda prices. In February 2017, soda manufacturers and retailers announced sales declines of 30-50% in Philadelphia and announced job cuts and layoffs. Kenny characterized the layoffs as evidence of greed among manufacturers. In the first four months of the soda tax $25.6 million was collected, which is lower than predicted. The revenue is intended to pay for a pre-K program (49% of tax revenue), government employee benefits and city programs (20%), and rebuilding city parks and recreation centers. A recent study from 2017 found that Philadelphia's tax has decreased sugary beverage consumption in impoverished youth by 1.3 drinks/week. Langellier et al. also found that when paired with the pre-K program, attendance increases significantly, a finding that is likely to have longer term positive effects than a sugary drink tax alone.\n\nIn March 2017, Pepsi laid off between 80 and 100 employees at two distribution plants in Philadelphia and one plant in nearby Wilmington, Delaware. The company blamed the layoffs on the tax, an assertion rejected by the city government.\n\nIn September 2016, the American Beverage Association, Philadelphia business owners, and other plaintiffs filed a lawsuit against the soda tax, alleging that the tax violated the \"Tax Uniformity Clause\" of the state constitution. The legal challenge was dismissed by the Court of Common Pleas in December 2016, and in June 2017 the Commonwealth Court of Pennsylvania (in a 5-2 decision) affirmed that ruling. The ABA is appealing the decision to the Pennsylvania Supreme Court.\n\nA one-cent-per-ounce soda tax (Prop V) passed with over 61% of the vote on 8 November 2016 and applies to distributors of sugary beverages on 1 January 2018. Exemptions for the tax include infant formulas, milk products, supplements, drinks used for medical reasons, and 100% fruit and vegetable juices. The soda industry spent almost $20 million in its unsuccessful push to defeat the soda tax initiative, a record-breaking amount for a San Francisco ballot initiative.\n\nIn 2014, the first referendum on a soda tax, Proposition E, was voted down by San Francisco; the 2014 referendum received the support of 55 percent of voters, short of the two-thirds required for a referendum directing money to a specific item (the referendum proposed directing the revenue raised to children's physical education and nutrition programs, and in San Francisco such earmarking requires a two-thirds vote to pass). In that referendum campaign, the soda industry spent about $10 million in opposition to the proposed tax.\n\nA one-cent-per-ounce soda tax (Measure HH) passed with over 60% of the vote on 8 November 2016. The tax went into effect on 1 July 2017.\n\nA one-cent-per-ounce soda tax (Prop O1) passed with over 70% of the vote on 8 November 2016. The tax went into effect on April 1, 2017\n\nA two-cents-per-ounce soda tax (Measure 2H) passed with 54% of the vote on 8 November 2016. The tax took effect on July 1, 2017, and revenue will be spent on health promotion, general wellness programs and chronic disease prevention that improve health equity, and other health programs especially for residents with low income and those most affected by chronic disease linked to sugary drink consumption. The tax is exempted at the University of Colorado, Boulder, campus as school officials survey what types of drinks students wish to have. The University was not aware it would be involved in the soda tax, and would have to pay as much as $1 million a year to purchase it.\n\nA one-cent-per-ounce soda tax passed on November 10, 2016, by a 9-8 vote, with Cook County Board of Commissioners President Toni Preckwinkle breaking the 8-8 tie. Cook County includes Chicago and has a population of nearly 5.2 million. This was the most populous jurisdiction with a soda tax in the U.S. The campaign to introduce the tax was heavily funded by Mike Bloomberg.\n\nOn June 30, 2017, a Cook County judge granted a temporary restraining order filed by the Illinois Retail Merchants Association and several Cook County-based grocers that prohibited the tax from being put into effect until at least July 12. The tax eventually went into effect on August 2. Due to a conflict with the Supplemental Nutrition Assistance Program, this soda tax did not apply to any soda purchases made with food stamps, which were used by over 870,000 people. Controversially, the tax affected diet drinks but not sugar-packed fruit juices.\n\nOn October 10, 2017, the Board of Commissioners voted to repeal the tax in a 15-1 vote. The tax stayed in effect up until December 1. The tax was highly unpopular and seen mainly as an attempt to plug the county’s $1.8 billion budget deficit, rather than a public health measure.\n\nThe Coalition for Healthy Kids and Education is currently campaigning to get a soda tax on the May 2018 ballot. Their aim is to implement a 1.15 cents per ounce tax on sugary drinks. There are 18,000 signatures required by December 15, 2017 in order for the tax to be voted on in May.\n\nOn June 5, 2017, Seattle's City Council voted 7-1 to pass a 1.75 cents per ounce tax on sugary drinks; the tax does not include diet soda drinks or fruit drinks and it started on January 1, 2018. After the tax was implemented, people were surprised that the tax made a case (24 cans) of Coke become $7.35 more expensive when compared to a case of Diet Coke or Coke Zero. The $15 million Seattle assumes will be collected from the tax will be used for programs that give access to more fruits and vegetables for low-income families, adding education programs and studying the tax on how it impacts behavior. Seattle collected $4 million in the first four months of the tax.\n\nCoca-Cola has been under fire since 2015 when emails revealed that funding for scientific studies sought to influence research to be more favorable to soda. Research funded by soda companies are 34 times more likely to find soda has no significant health impacts on obesity or diabetes.\n\nTaxing soda can lead to a reduction in overall consumption, according to a scientific study published in the \"Archives of Internal Medicine\" in March 2010. The study found that a 10 percent tax on soda led to a 7 percent reduction in calories from soft drinks. These researchers believe that an 18 percent tax on these foods could cut daily intake by 56 calories per person, resulting in a weight loss of 5 pounds (2 kg) per person per year. The study followed 5,115 young adults ages 18 to 30 from 1985 to 2006.\n\nA 2010 study published in the medical journal \"Health Affairs\" found that if taxes were about 18 cents on the dollar, they would make a significant difference in consumption.\n\nResearch from Duke University and the National University of Singapore released in December 2010 tested larger taxes and determined that a 20 percent and 40 percent taxes on sugar-sweetened beverages would largely not affect calorie intake because people switch to untaxed, but equally caloric, beverages. Kelly Brownell, a proponent of soda taxes, reacted by stating that “[t]he fact is that nobody has been able to see how people will really respond under these conditions.” Similarly, a 2010 study concluded that while people would drink less soda as a result of a soda tax, they would also compensate for this reduction by switching to other high-calorie beverages. In response to these arguments, the American Public Health Association released a statement in 2012 in which they argued that \"Even if individuals switch to 100% juice or chocolate milk, this would be an improvement, as those beverages contribute some nutrients to the diet.\"\n\nA 2011 study in the journal \"Preventive Medicine\" concluded that \"a modest tax on sugar-sweetened beverages could both raise significant revenues and improve public health by reducing obesity\". It has been used by the Rudd Center for Food Policy and Obesity at Yale to estimate revenue from a soda tax, depending on the state, year and tax rate.\n\nA 2012 study by Y. Claire Wang, also in the journal \"Health Affairs\", estimates that a penny per ounce tax on sugared beverages could prevent 2.4 million cases of diabetes per year, 8,000 strokes, and 26,000 premature deaths over 10 years.\n\nIn 2012, just before the city of Richmond began voting on a soda tax, a study was presented at a conference held by the American Public Health Association regarding the potential effects of such a tax in California. The study concluded that, given that soda's price elasticity is such that taxing it would reduce consumption by 10–20 percent, that this reduction \"...is projected to reduce diabetes incidence by 2.9–5.6% and CHD by 0.6–1.2%.\"\n\nA 2013 study in the \"American Journal of Agricultural Economics\" concluded that a 0.5-cent-per-ounce tax on soft drinks would reduce consumption, but \"increase sodium and fat intakes as a result of product substitution,\" in line with the Duke University study mentioned above.\n\nA 2014 study published in the \"American Journal of Public Health\" concluded that Sugar-Sweetened Beverages (SSBs) don’t have a negative impact on employment. Even though job losses in the taxed industry occurred, they were offset by new employment in other sectors of the economy.\n\nA 2016 modelling study estimated that a 20% tax on SSBs would decrease the consumption of SSBs in Australia by 12.6%. The tax could decline the prevalence of obesity in the Australian population, which could lead to gains in health-adjusted life years. The results showed an increase of 7.6 days in full health for a 20-24-year-old male and a 3.7 day increase in longevity for their female peers.\n\nThere have been a number of proposed taxes on sugary beverages, including:\n\n\nA 2016 poll by Morning Consult-Vox finds Americans split on their support of a soda tax. Attitudes seem to have shifted a lot since 2013 when a poll concluded that \"respondents were opposed to government taxes on sugary drinks and candy by a more than 2-to-1 margin.\" In California, however, support for a tax has been high for a few years. According to a Field Poll conducted in 2012, \"Nearly 3 out of 5 California voters would support a special fee on soft drinks to fight childhood obesity.\" \nSupport for a soda tax in New York was higher when pollsters say the money will go towards health care. A Quinnipiac University poll released in April 2010 found that New Yorkers opposed a state tax on soda of one penny per ounce by a 35-point margin, but opposition dropped to a margin of one point when respondents were told the money would go towards health care. A Thompson Reuters poll released in the same month found that 51 percent of Americans opposed a soda tax, while 33 percent supported one.\n\nFighting the creation of soft drink taxes, the American Beverage Association, the largest U.S. trade organization for soft drink bottlers, has spent considerable money lobbying Congress. The Association's annual lobbying spending rose from about $391,000 to more than $690,000 from 2003 to 2008. And, in the 2010 election cycle, its lobbying grew to $8.67 million. These funds helped to pay for 25 lobbyists at seven different lobbying firms.\n\nAn industry group called \"Americans Against Food Taxes,\" backed by juice maker Welch's, soft drink maker PepsiCo Inc, the American Beverage Association, the Corn Refiners Association, McDonald's Corporation and Burger King Holdings Inc used national advertising and conducted lobbying to oppose these taxes. The group has characterized the soda tax as a regressive tax, which would unfairly burden the poor\n\n\nIsland nations and territories have been successful in passing soda taxes. Just like with tobacco taxes, smaller communities are often the first to pass a new type of tax.\n\nBarbados passed a soda tax in September 2015, applied as an excise of 10%.\n\nFiji has an import tax and an excise tax on soda.\n\nFrench Polynesia implemented taxes on soft drinks in 2002.\n\nMauritius passed a soda tax in 2013.\n\nNauru implemented a soda tax in 2007.\n\nSamoa passed a soda tax in 1984.\n\nIn March 2014, the government of the island of St Helena, a British Overseas Territory in the South Atlantic, announced that it would be introducing an additional import duty of 75 pence per litre on sugar-sweetened carbonated drinks with more than 15 grams of sugar per litre. The measure was introduced in May 2014 as part of a number of measures to tackle obesity on the island and the resulting high incidence of type 2 diabetes.\n\nTonga has a soda tax.\n\n\n"}
{"id": "20928420", "url": "https://en.wikipedia.org/wiki?curid=20928420", "title": "Translational research informatics", "text": "Translational research informatics\n\nTranslational research informatics (TRI) is a sister domain to or a sub-domain of biomedical informatics or medical informatics concerned with the application of informatics theory and methods to translational research. There is some overlap with the related domain of clinical research informatics, but TRI is more concerned with enabling multi-disciplinary research to accelerate clinical outcomes, with clinical trials often being the natural step beyond translational research. \n\nTranslational research as defined by the National Institutes of Health includes two areas of translation. One is the process of applying discoveries generated during research in the laboratory, and in preclinical studies, to the development of trials and studies in humans. The second area of translation concerns research aimed at enhancing the adoption of best practices in the community. Cost-effectiveness of prevention and treatment strategies is also an important part of translational research.\n\nTranslational research informatics can be described as \"an integrated software solution to manage the: (i) logistics, (ii) data integration, and (iii) collaboration, required by translational investigators and their supporting institutions\". It is the class of informatics systems that sits between and often interoperates with: (i) health information technology/electronic medical record systems, (ii) CTMS/clinical research informatics, and (iii) statistical analysis and data mining. \n\nTranslational research informatics is relatively new, with most CTSA awardee academic medical centers actively acquiring and integrating systems to enable the end-to-end TRI requirements. One advanced TRI system is being implemented at the Windber Research Institute in collaboration with GenoLogics and InforSense. Translational Research Informatics systems are expected to rapidly develop and evolve over the next couple of years.\n\nFurther discussion of this domain can be found at the Clinical Research Informatics Wiki (CRI Wiki), a wiki dedicated to issues in clinical and translational research informatics.\n\n"}
{"id": "584416", "url": "https://en.wikipedia.org/wiki?curid=584416", "title": "Uterine contraction", "text": "Uterine contraction\n\nA uterine contraction is a muscle contraction of the uterine smooth muscle.\n\nThe uterus frequently contracts throughout the entire menstrual cycle, and these contractions have been termed \"endometrial waves\" or \"contractile waves\". These appear to involve only the sub-endometrial layer of the myometrium. In the early follicular phase, these contractions occur once or twice per minute and last 10–15 seconds with a low amplitude of usually 30 mmHg. The frequency increases to 3–4 per minute towards ovulation. During the luteal phase, the frequency and amplitude decrease, possibly to facilitate any implantation.\n\nIf implantation does not occur, the frequency remains low, but the amplitude increases dramatically to between 50 and 200 mmHg producing labor-like contractions at the time of menstruation. These contractions are sometimes termed \"menstrual cramps\", although that term is often used for menstrual pain in general. These contractions may be uncomfortable or even painful, but they are generally significantly less painful than contractions during labour. A hot water bottle or exercising has been found to help.\n\nA shift in the myosin expression of the uterine smooth muscle has been hypothesized to avail for changes in the directions of uterine contractions that are seen during the menstrual cycle.\n\nA contraction refers specifically to the motion of the uterus as part of the process of childbirth. Contractions, and labour in general, is one condition that releases the hormone oxytocin into the body. Contractions become longer as labour intensifies.\n\nPrior to actual labour, women may experience Braxton Hicks contractions, sometimes known as \"false labour.\"\n\nSince every pregnancy is different, a doctor, midwife or other competent professional should always be consulted before any action is taken to reduce the pain. Some popular methods may be harmful to the mother and/or the baby, or may actually worsen the pain or lengthen the labour.\n\nUterine contractions during childbirth can be monitored by cardiotocography, in which a device is fixated to the skin of the mother or directly to the fetal scalp. The pressure required to flatten a section of the uterine wall correlates with the internal pressure, thereby providing an estimate of it. \n\nA type of monitoring technology under development at Drexel University embeds conductive threads in the knitted fabric of a bellyband. When the fibers stretch in response to a contraction, the threads function like an antenna, and send the signals they pick up to an embedded RFID (radio frequency identification device) chip that reports the data.\n\nThe uterus and vagina contract during female orgasm. These contractions may not be noticed by all women; pregnant women are more likely to notice these contractions by late 2nd and 3rd trimesters.\n\n"}
{"id": "13587755", "url": "https://en.wikipedia.org/wiki?curid=13587755", "title": "Young worker safety and health", "text": "Young worker safety and health\n\nAround the world, nearly 250 million children, about one in every six children, ages 5 through 17, are involved in child labor. Children can be found in almost any economic sector. However, at a global level, most of them work in agriculture (70%). Approximately 2.4 million adolescents aged 16 to 17 years worked in the U.S. in 2006. Official employment statistics are not available for younger adolescents who are also known to work, especially in agricultural settings.\n\nIn 2006, 30 youth under 18 died from work-related injuries in the U.S. In 2003, an estimated 54,800 work-related injuries and illnesses among youth less than 18 years of age were treated in hospital emergency departments. The National Institute for Occupational Safety and Health reports that only one-third of work-related injuries are seen in emergency departments, therefore it is likely that approximately 160,000 youth sustain work-related injuries and illnesses each year. The highest number of teen worker fatalities occur in agricultural work and the retail trades, according to recent data. Across Europe, 18- to 24-year-olds are at least 50% more likely to be injured in the workplace than more experienced workers.\n\nBecause of their biologic, social, and economic characteristics, young workers have unique and substantial risks for work-related injuries and illnesses. Besides retail and agriculture, other areas of high risk for work-related injuries include construction and work activities involving motor vehicles and mobile machinery. Although safety requirements and child labor laws prohibit or restrict teen employment in certain kinds of industries and occupations, young workers may yet face risks on the job because an employer or a young employee may not be aware of applicable laws and may not be aware that a hazard exists, because the young employee may lack experience, or because there is inadequate training or supervision.\n\nChild labor is the employment of children under an age determined by law or custom. This practice is considered exploitative by many countries and international organizations. Child labor was utilized to varying extents through most of history, but entered public dispute with the beginning of universal schooling, with changes in working conditions during industrialization, and with the emergence of the concepts of workers' and children's rights. Child labor is still common in some places. Even after passing child labor legislation, developing countries in particular still feature informal economies made up of child workers who are more exposed to human rights violations in the workplace.\n\nThere are roughly 250,000 children who work on U.S. farms, that do not live on the farm, and another 30 million children who visit farms annually. About every three days a child dies on a U.S. farm. In 2004, most youth work fatalities occurred in the agriculture sector. About two thirds of these fatalities could be attributed to transportation accidents. According to the \"Occupational injuries among young workers\" report, most of these transportation accidents occurred either by truck or by tractor. Tractor accidents alone counted for 1/4 of the youth worker fatalities from 1993 to 2002. For workers 13 years old and younger, agricultural fatalities accounted for 42 out of the 49 total fatalities for that age group between 1998 and 2002. There are fewer child labor restrictions on family-owned or operated farms; workers as young as 13 can legally operate tractors on farms if their family owns it or operates it.\n\nExperiences and exposures across different sectors of agriculture vary greatly. For example, children who worked on tobacco farms experienced a number of negative health effects including nausea, heat exhaustion, and burning eyes.\n\nBetween the years 1998 and 2002, construction accidents accounted for 18% of youth worker fatalities, making construction the second most deadly industry for a young worker to be employed in during that time period. A significant number of young worker fatalities in this sector resulted from work that the young worker was not legally authorized to do. For example, 16-year-old workers accounted for almost 20% of the young worker fatalities in the construction sector between 1998 and 2002, even though workers 16 and younger are legally unauthorized to enter a construction site; if they do work for the construction industry, they can only work in an office or sales department. Additionally, Hispanic and Latino young workers made up 35% of the young worker fatalities in the construction sector. Hispanics make up 14.4% of the United States' general population.\n\nThe second highest number of workplace fatalities between 1993 and 1997 among workers younger than age 18 occurred in the retail trades (e.g., restaurants and retail stores). Between 1992 and 2000, 63% of these deaths were due to assaults and violent acts, most of which were homicides. Homicide associated with robbery is the probable cause for one fourth to one half of all young worker fatalities in retail trades. Handling cash, working alone or in small numbers, and working in the late evening and early morning hours may contribute to workplace homicides [NIOSH 1996a].\n\nIn 1998, more than half of all work-related nonfatal injuries to young workers occurred in retail trades, more than 60% of which were eating and drinking establishments. Cuts in retail trades were the most common type of injury treated in emergency departments, followed by burns in eating and drinking establishments and bruises, scrapes, and scratches in other retail settings. Common hazards in restaurants include using knives to prepare food, handling hot grease from fryers, working near hot surfaces, and slipping on wet or greasy floors.\nIn addition, certain types of machinery prohibited for use by young workers under current child labor laws are commonly found in retail establishments—including food slicers, paper balers, forklifts, dough and batter mixers, and bread cutting machines. Young workers may choose to operate unfamiliar machinery to prove responsibility, independence, or maturity, or they may be instructed to do so by an employer who is unaware of child labor laws or chooses to disregard them.\n\nThe Occupational Safety and Health Administration (OSHA) within the U.S. Department of Labor (DOL) is the Federal agency with primary responsibility for setting and enforcing standards to promote safe and healthful working conditions for all workers. Employers are responsible for becoming familiar with standards applicable to their establishments and for ensuring a safe working environment.\n\nThe U.S. Public Health Service has a Healthy People 2020 objective to reduce emergency department injury rates among young workers, ages 15 through 19, to 4.9 injuries/100 full-time equivalents by 2020. The rate in 2007 was 5.3 injuries/100 full-time equivalents.\n\nThe National Institute for Occupational Safety and Health (NIOSH) within the U.S. Centers for Disease Control and Prevention plays a lead role in efforts to reduce injuries and illnesses among working youth by conducting and supporting science to guide prevention efforts, disseminating findings, and working with others in collaborative outreach.\n\nThe NIOSH funds the National Children's Center for Rural and Agricultural Health and Safety.\n\nA workplace may be fully compliant with OSHA regulations and yet may place young workers at risk of injury or illness if applicable federal and state child labor laws are not followed. One study estimated that more than three-fourths of employers of young workers were unfamiliar with child labor laws. Lack of awareness of occupational safety and health laws by young workers, adults, and employers has been identified as a major obstacle to preventing injury and illness in young workers. The primary Federal law governing the employment of workers under age 18 is the Fair Labor Standards Act (FLSA) of 1938, which is enforced by the Wage and Hour Division of the Employment Standards Administration within DOL. Child labor provisions of the FLSA are designed to protect the educational opportunities of minors and prohibit their employment in jobs that pose safety or health risks. The FLSA does not cover all young workers. The FLSA applies to an entire business enterprise if the enterprise has annual gross revenues of $500,000 or more. Child Labor Regulation No. 3 restricts hours and specifies allowable employment activities for workers aged 14 and 15.\n\nStates may also have their own child labor laws that are stricter than federal laws. If a state child labor law is less protective than federal law, or if no applicable state law exists, Federal child labor laws apply.\n\n\n"}
