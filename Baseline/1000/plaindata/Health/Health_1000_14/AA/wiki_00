{"id": "37958263", "url": "https://en.wikipedia.org/wiki?curid=37958263", "title": "2009 flu pandemic timeline summary", "text": "2009 flu pandemic timeline summary\n\nThis article covers the chronology of the 2009 novel influenza A (H1N1) pandemic. Flag icons denote the first announcements of confirmed cases by the respective nation-states, their first deaths (and other major events such as their first intergenerational cases, cases of zoonosis, and the start of national vaccination campaigns), and relevant sessions and announcements of the World Health Organization (WHO), the European Union (and its agency the European Centre for Disease Prevention and Control),\nand the U.S. Centers for Disease Control (CDC).\n\nUnless otherwise noted, references to terms like S-OIV, H1N1 and such, all refer to this new A(H1N1) strain and not to sundry other strains of H1N1 which are endemic in humans, birds and pigs.\n\n"}
{"id": "39993576", "url": "https://en.wikipedia.org/wiki?curid=39993576", "title": "5-SPICE framework", "text": "5-SPICE framework\n\nThe 5-SPICE framework is an instrument designed for global health practitioners to guide discussions about community health worker (CHW) projects.\n\nThe 5-SPICE framework was developed by clinicians and researchers from Partners In Health, Harvard Medical School, and Brigham and Women’s Hospital in Boston, MA. The framework lays out a model for integrating community health workers into public health systems, learning from the experiences Partners In Health and partner organizations at their project sites in resource-poor settings around the world. 5-SPICE draws upon experiences from Haiti, Rwanda, Lesotho, Liberia, Nepal, Mali, and elsewhere, where CHWs have been employed to improve patient outcomes and overcome personnel shortages. The framework allows for all stakeholders in a community health program to participate in discussions and analyses to strengthen the impact of CHWs.\n\n5-SPICE allows for all stakeholders in a community health program to participate in discussions and analyses to strengthen the impact of community health workers.\n\nThe name 5-SPICE is derived from Chinese cuisine emphasizing the balance between inputs and elements. The five main elements form an acronym:\n\nThese elements are not a static list, but a way to holistically analyze how core programmatic elements affect each other in the field. In the Freirean tradition of awareness, the 5-SPICE model emphasizes facilitated discussion and contemplation among stakeholders, particularly CHWs, to maximize program outputs. Ultimately, the 5-SPICE framework allows program implementers to study the relationship between the health system and the local community.\n\nOther CHW program frameworks exist, such as the CHW Assessment and Improvement Matrix (AIM) developed by the USAID-funded Health Care Improvement (HCI) project. 5-SPICE complements these other frameworks by providing an acronym that condenses the many elements discussed in other frameworks into an easy-to-remember heuristic, allowing for more effective and efficient assessments that are exploratory rather than prescriptive.\n\nThe 5-SPICE framework was first formally introduced in an April 2013 publication in Global Health Action, in an article entitled “5-SPICE: the application of an original framework for community health worker program design, quality improvement and research agenda setting.” The framework was subsequently presented at the 2013 Consortium of Universities for Global Health Conference, and the 2013 Swedish Society of Medicine’s annual conference, Global Health—Beyond 2015. The article has also been accepted for presentation at the 2013 Annual Meeting of the American Public Health Association.\n"}
{"id": "43686632", "url": "https://en.wikipedia.org/wiki?curid=43686632", "title": "Australian Longitudinal Study on Women's Health", "text": "Australian Longitudinal Study on Women's Health\n\nThe Australian Longitudinal Study on Women’s Health (ALSWH), also known as Women’s Health Australia or Women’s Health of Australia (WHoA!), is an ongoing population-based survey examining the health of over 50,000 Australian women. The study is funded by the Australian Government Department of Health and is a collaborative endeavour conducted by staff and investigators at The University of Newcastle and The University of Queensland. The current Director is Professor Gita Mishra, the previous Director was Annette Dobson\n\nIn 1996, women in three age groups (born 1973-78, 1946–51 and 1921–26) were randomly selected from the national Medicare database. These women were sent an invitation by mail to participate in the study. Over 40,000 women responded and agreed to participate. These women represented 2-3% of women in their age groups living in urban, rural and remote areas of Australia at that time.\n\nIn 2012, Women’s Health of Australia (WHoA!) was launched. WHoA! was developed to recruit an additional cohort of women aged 18–23. Over 17,000 women were recruited using social media.\nThe primary method of data collection employed by ALSWH is self-completed surveys. ALSWH also includes a qualitative data set, where participants are asked to write anything they feel is important to their health and wellbeing in a free response space at the end of the survey.\n\nThe ALSWH study assesses:\n\nAs well as completing the main surveys, participants are occasionally invited to participate in substudies that aim to examine selected issues in more depth. A wide range of topics has been covered, including:\n\nALSWH has approval to anonymously link survey data with a number of national and state-based administrative datasets. Currently these approvals include the National Death Index, Medicare data, state-based cancer registries, perinatal data collections and hospital admission datasets. The aim of linking to these datasets is to provide new insights into the health and health service use of Australian women, taking into account individual level differences in socioeconomic, behavioural, environmental and other risk factors. In the context of a changing health system, it is important to be able to monitor and evaluate the impact and outcomes of health service policy and practice.\n\nThe longitudinal study design ensures that the same women are followed over many years so researchers can observe changes in their health, clarify cause-and-effect relationships, and assess the effects of changes in policy and practice. ALSWH can provide information about the health of women across the lifespan. This information has assisted federal and state governments to plan for the future and to develop and evaluate health policy and practice relevant to Australian women. The study was used extensively in the 2010 National Women’s Health Policy.\n\nALSWH has collaborated with numerous investigators, universities and organisations around the world. The findings from ALSWH have been reported in over 100 reports to the government, and been included in more than 400 academic publications.\n\n"}
{"id": "1591856", "url": "https://en.wikipedia.org/wiki?curid=1591856", "title": "Cervical dilation", "text": "Cervical dilation\n\nCervical dilation (or cervical dilatation) is the opening of the cervix, the entrance to the uterus, during childbirth, miscarriage, induced abortion, or gynecological surgery. Cervical dilation may occur naturally, or may be induced by surgical or medical means.\n\nIn the later stages of pregnancy, the cervix may already have opened up to 1–3 cm (or more in rarer circumstances), but during labor, repeated uterine contractions lead to further widening of the cervix to about 6 centimeters. From that point, pressure from the presenting part (head in vertex births or bottom in breech births), along with uterine contractions, will dilate the cervix to 10 centimeters, which is \"complete.\" Cervical dilation is accompanied by effacement, the thinning of the cervix.\n\nGeneral guidelines for cervical dilation:\n\nDuring pregnancy, the os (opening) of the cervix is blocked by a thick plug of mucus to prevent bacteria from entering the uterus. During dilation, this plug is loosened. It may come out as one piece, or as thick mucus discharge from the vagina. When this occurs, it is an indication that the cervix is beginning to dilate, although not all women will notice this mucus plug being released.\n\nBloody show is another indication that the cervix is dilating. Bloody show usually comes along with the mucus plug, and may continue throughout labor, making the mucus tinged pink, red or brown. Fresh, red blood is usually not associated with dilation, but rather serious complications such as placental abruption, or placenta previa. Red blood in small quantities often also follows an exam.\n\nThe pain experienced during dilation is similar to that of menstruation (although markedly more intense), as period pains are thought to be due to the passing of endometrium through the cervix. Most of the pain during labor is caused by the uterus contracting to dilate the cervix.\n\nProstaglandins (P2 and PGE2) contribute to cervical ripening and dilation. The body produces these hormones naturally. Sometimes prostaglandins in synthesized forms are applied directly to the cervix to induce labor. In women who have had a previous cesarean section, the American College of Obstetricians and Gynecologists issued a bulletin that misoprostol never be used for this purpose. ACOG's findings conclude that the collagen softening properties of misoprostol could be absorbed through the cervix and vaginal vault up into the low transverse scar of a typical cesarean section, and significantly increase the risk of uterine rupture. Prostaglandins are also present in human semen, and sexual intercourse is commonly recommended for promoting the onset of labor, although the limited data available makes the effectiveness of this method uncertain.\n\nOther means of natural cervical ripening include nipple stimulation, which produces oxytocin, a hormone which is necessary for uterine contractions. Nipple stimulation can be performed manually, by use of a breast pump, or by suckling. Henci Goer, in her comprehensive book, The Thinking Woman's Guide to a Better Birth, details how this practice was researched in two separate studies of 100 and 200 women in the mid nineteen-eighties. Women were assigned randomly to two groups. In one group, nipples were stimulated for one-hour sessions, three times per day. In the other group, women were to avoid any form of nipple stimulation or sexual intercourse. The researchers concluded in both studies that nipple stimulation could indeed ripen the cervix and in some cases induce uterine contractions. Goer further notes that in the smaller study, an external fetal monitor was used, and no uterine hyperstimulation was noted.\n\nCervical dilation may be induced mechanically by placing devices inside the cervix that will expand while in place. A balloon catheter may be used. Other products include laminaria stick (made of dried seaweed) or synthetic hygroscopic materials, which expand when placed in a moist environment.\n\nIn hysteroscopy, the diameter of the hysteroscope is generally too large to conveniently pass the cervix directly, thereby necessitating cervical dilation to be performed prior to insertion. Cervical dilation can be performed by temporarily stretching the cervix with a series of \"(cervical) dilators\" of increasing diameter. Misoprostol prior to hysteroscopy for cervical dilation appears to facilitate an easier and uncomplicated procedure only in premenopausal women.\n"}
{"id": "3141410", "url": "https://en.wikipedia.org/wiki?curid=3141410", "title": "Child mortality", "text": "Child mortality\n\nChild mortality, also known as child death, refers to the death of children under the age of 14 and encompasses neonatal mortality, under-5 mortality, and mortality of children aged 5–14. Many child deaths go unreported for a variety of reasons, including lack of death registration and lack of data on child migrants. Without accurate data on child deaths, we cannot fully discover and combat the greatest risks to a child's life.\n\nReduction of child mortality is reflected in several of the United Nations' Sustainable Development Goals. Rapid progress has resulted in a significant decline in preventable child deaths since 1990, with the global under-5 mortality rate declining by over half between 1990 and 2016. While in 1990, 12.6 million children under age five died, in 2016 that number fell to 5.6 million children. However, despite advances, there are still 15,000 under-five deaths per day from largely preventable causes. About 80 per cent of these occur in sub-Saharan Africa and South Asia, and just 6 countries account for half of all under-five deaths: India, Nigeria, Pakistan, the Democratic Republic of the Congo, Ethiopia and China. 45% of these children died during the first 28 days of life.\n\nChild mortality refers to number of child deaths under the age of 5 per 1000 live births. However, the child mortality could be simplified into more specific terms such as prenatal, perinatal, Neonatal, infancy and under 5. Prenatal: child death before the birth, Perinatal: child death before one week of birth, Neonatal: child death before 28 days of birth, Infancy: child death before 1st birthday, and child mortality under 5 refer to any deaths from birth to the 5th birthday.\n\nPerinatal mortality rate: Number of child deaths within first week of birth/ total number of birth\n\nNeonatal mortality rate: number of child deaths within first 28 days of life/ total number of birth\n\nInfancy mortality rate: number of child deaths within first 12 months of life/ total number of birth\n\nUnder 5 mortality rates: number of child deaths within 5th birthday/ total number of birth\n\nThe leading causes of death of children under five include:\n\n\nThere is variation of child mortality around the world; countries that are in the second or third stage of the Demographic Transition Mode (DTM) have higher rates of child mortality than countries in the fourth or fifth state of the DTM. Chad infant mortality is about 96 per 1,000 live births. And developed country such as Japan infant mortality is about 2.2 per 1,000 live births. In 2010, there were estimated to 7.6 million child deaths around the world and most of it occurred in less developed countries and 4.7 million died from infection and disorder. Child mortality isn’t only caused by infection and disorder, it is also caused by premature birth, birth defect, new born infection, birth complication, and disease like malaria, sepsis, and diarrhea. In less developed countries, malnutrition is the main source of child mortality. Pneumonia, diarrhea and malaria together are the cause of 1 out of every 3 child deaths before the age of 5 and nearly half of under-five deaths globally are attributable to under nutrition.\n\nChild survival is a field of public health concerned with reducing child mortality. Child survival \ninterventions are designed to address the most common causes of child deaths that occur, which include diarrhea, pneumonia, malaria, and neonatal conditions. Of the portion of children under the age of 5 alone, an estimated 5.6 million children die each year mostly from such preventable causes.\n\nThe child survival strategies and interventions are in line with the fourth Millennium Development Goals (MDGs) which focused on reducing child mortality by 2/3 of children under five before the year 2015. In 2015, the MDGs were replaced with the Sustainable Development Goals (SDGs), which aim to end these deaths by 2030. In order to achieve SDG targets, progress must be accelerated in more than 1/4 of all countries (most of which are in sub-Saharan Africa) in order to achieve targets for under-5 mortality, and in 60 countries (many in sub-Saharan Africa and South Asia) to achieve targets for neonatal mortality. Without accelerated progress, 60 million children under age 5 will die between 2017 and 2030, about half of which would be newborns.\n\nTwo-thirds of child deaths are preventable. Most of the children who die each year could be saved by low-tech, evidence-based, cost-effective measures such as vaccines, antibiotics, micronutrient supplementation, insecticide-treated bed nets, improved family care and breastfeeding practices, and oral rehydration therapy. Empowering women, removing financial and social barriers to accessing basic services, developing innovations that make the supply of critical services more available to the poor and increasing local accountability of health systems are policy interventions that have allowed health systems to improve equity and reduce mortality.\n\nIn developing countries, child mortality rates related to respiratory and diarrheal diseases can be reduced by introducing simple behavioral changes, such as handwashing with soap. This simple action can reduce the rate of mortality from these diseases by almost 50 per cent.\n\nProven, cost-effective interventions can save the lives of millions of children per year. The UN Vaccine division as of 2014 supported 36% of the world's children in order to best improve their survival chances, yet still, low-cost immunization interventions do not reach 30 million children despite success in reducing polio, tetanus, and measles. Measles and tetanus still kill more than 1 million children under 5 each year. Vitamin A supplementation costs only $0.02 cents for each capsule and given 2-3 times a year will prevent blindness and death. Although vitamin A supplementation has been shown to reduce all-cause mortality by 12 to 24 per cent, only 70 per cent of targeted children were reached in 2015. Between 250,000 and 500,000 children become blind every year, with 70 percent of them dying within 12 months. Oral rehydration therapy (ORT) is an effective treatment for lost liquids through diarrhea; yet only 4 in 10 (44 per cent) of children ill with diarrhea are treated with ORT.\n\nEssential newborn care - including immunizing mothers against tetanus, ensuring clean delivery practices in a hygienic birthing environment, drying and wrapping the baby immediately after birth, providing necessary warmth, and promoting immediate and continued breastfeeding, immunization, and treatment of infections with antibiotics - could save the lives of 3 million newborns annually. Improved sanitation and access to clean drinking water can reduce childhood infections and diarrhea. Over 30% of the world's population does not have access to basic sanitation, and 844 million people use unsafe sources of drinking water.\n\nAgencies promoting and implementing child survival activities worldwide include UNICEF and non-governmental organizations; major child survival donors worldwide include the World Bank, the British Government's Department for International Development, the Canadian International Development Agency and the United States Agency for International Development. In the United States, most non-governmental child survival agencies belong to the CORE Group, a coalition working, through collaborative action, to save the lives of young children in the world's poorest countries.\n\nChild mortality has been dropping as each country reaches a high stage of DTM. From 2000 to 2010, child mortality has dropped from 9.6 million to 7.6 million. In order to reduce child mortality rates, there needs to be better education, higher standards of healthcare and more caution in childbearing. Child mortality could be reduced by attendance of professionals at birth and by breastfeeding and through access to clean water, sanitation, and immunization. In 2016, the world average was 41 (4.1%), down from 93 (9.3%) in 1990. This is equivalent to 5.6 million children less than five years old dying in 2016.\n\nHuge disparities in under-5 mortality rates exist. Globally, the risk of a child dying in the country with the highest under-5 mortality rate is about 60 times higher than in the country with the lowest under-5 mortality rate. Sub-Saharan Africa remains the region with the highest under-5 mortality rates in the world: All six countries with rates above 100 deaths per 1,000 live births are in sub-Saharan Africa.\n\nFurthermore, approximately 80% of under-5 deaths occur in only two regions: sub-Saharan Africa and South Asia. 6 countries account for half of the global under-5 deaths, namely, India, Nigeria, Pakistan, the Democratic Republic of the Congo, Ethiopia and China. India and Nigeria alone account for almost a third (32 per cent) of the global under-five deaths.\n\nLikewise, there are disparities between wealthy and poor households in developing countries. According to a Save the Children paper, children from the poorest households in India are three times more likely to die before their fifth birthday than those from the richest households.\n\nThe child survival rate of nations varies with factors such as fertility rate and income distribution; the change in distribution shows a strong correlation between child survival and income distribution as well as fertility rate, where increasing child survival allows the average income to increase as well as the average fertility rate to decrease.\n\n\n"}
{"id": "13522147", "url": "https://en.wikipedia.org/wiki?curid=13522147", "title": "Clinical coder", "text": "Clinical coder\n\nA clinical coder – also known as clinical coding officer, diagnostic coder, medical coder, nosologist or medical records technician – is a health information professional whose main duties are to analyse clinical statements and assign standard codes using a classification system. The data produced are an integral part of health information management, and are used by local and national governments, private healthcare organizations and international agencies for various purposes, including medical and health services research, epidemiological studies, health resource allocation, case mix management, public health programming, medical billing, and public education.\n\nFor example, a clinical coder may use a set of published codes on medical diagnoses and procedures, such as the International Classification of Diseases (ICD) or the Common Coding System for Healthcare Procedures (HCPCS), for reporting to the health insurance provider of the recipient of the care. The use of standard codes allows insurance providers to map equivalencies across different service providers who may use different terminologies or abbreviations in their written claims forms, and be used to justify reimbursement of fees and expenses. The codes may cover topics related to diagnoses, procedures, pharmaceuticals or topography. The medical notes may also be divided into specialities for example cardiology, gastroenterology, nephrology, neurology , pulmonology or orthopedic care.\n\nA clinical coder therefore requires a good knowledge of medical terminology, anatomy and physiology, a basic knowledge of clinical procedures and diseases and injuries and other conditions, medical illustrations, clinical documentation (such as medical or surgical reports and patient charts), legal and ethical aspects of health information, health data standards, classification conventions, and computer- or paper-based data management, usually as obtained through formal education and/or on-the-job training.\n\nThe basic task of a clinical coder is to classify medical and health care concepts using a standardised classification. Most clinical coders are employed in coding inpatient episodes of care. However, mortality events, outpatient episodes, general practitioner visits and population health studies can all be coded.\n\nClinical coding has three key phases: a) Abstraction; b) Assignment; and c) Review.\n\nThe abstraction phase involves reading the entire record of the health encounter and analysing the information to determine what condition(s) the patient had, what caused it and how it was treated. The information comes from a variety of sources within the medical record, such as clinical notes, laboratory and radiology results, and operation notes.\n\nThe assignment phase has two parts: finding the appropriate code(s) from the classification for the abstraction; and entering the code into the system being used to collect the coded data.\n\nReviewing the code set produced from the assignment phase is very important. Clinical coder must ask themselves, \"does this code set fairly represent what happened to this patient in this health encounter at this facility?\" By doing this, clinical coders are checking that they have covered everything that they must, but not used extraneous codes. For health encounters that are funded through a case mix mechanism, the clinical coder will also review the diagnosis-related group (DRG) to ensure that it does fairly represent the health encounter.\n\nClinical coders may have different competency levels depending on the specific tasks and employment setting.\n\nAn entry level coder has completed (or nearly completed) an introductory training program in using clinical classifications. Depending on the country; this program may be in the form of a certificate, or even a degree; which has to be earned before the trainee is allowed to start coding. All trainee coders will have some form of continuous, on-the-job training; often being overseen by a more senior coder. \n\nAn intermediate level coder has acquired the skills necessary to code many cases independently. Coders at this level are also able to code cases with incomplete information. They have a good understanding of anatomy and physiology along with disease processes. Intermediate level coders have their work audited periodically by an Advanced coder.\n\nAdvanced level and senior coders are authorized to code all cases including the most complex. Advanced coders will usually be credentialed and will have several years of experience. An advanced coder is also able to train entry-level coders.\n\nA nosologist understands how the classification is underpinned. Nosologists consult nationally and internationally to resolve issues in the classification and are viewed as experts who can not only code, but design and deliver education, assist in the development of the classification and the rules for using it.\n\nNosologists are usually expert in more than one classification, including morbidity, mortality and casemix. In some countries the term \"nosologist\" is used as a catch-all term for all levels.\n\nClinical coders may use many different classifications, which fall into two main groupings: statistical classifications and nomenclatures.\n\nA statistical classification, such as ICD-10 or DSM-5, will bring together similar clinical concepts, and group them into one category. This allows the number of categories to be limited so that the classification does not become too big, but still allows statistical analysis. An example of this is in ICD-10 at code I47.1. The code title (or rubric) is Supraventricular tachycardia. However, there are several other clinical concepts that are also classified here. Amongst them are paroxysmal atrial tachycardia, paroxysmal junctional tachycardia, auricular tachycardia and nodal tachycardia.\n\nWith a nomenclature, for example SNOMED CT, there is a separate listing and code for every clinical concept. So, in the tachycardia example above, each type and clinical term for tachycardia would have its own code listed. This makes nomenclatures unwieldy for compiling health statistics.\n\nIn some countries, clinical coders may seek voluntary certification or accreditation through assessments conducted by professional associations, health authorities or, in some instances, universities. The options available to the coder will depend on the country, and, occasionally, even between states within a country. \n\nClinical coders start as trainees, and there are no conversion courses for coders immigrating to the United Kingdom.\n\nThe National Clinical Coding Qualification (NCCQ) is an exam for experienced coders, and is recognised by the four health agencies of the UK.\nIn England, a novice coder will complete the national standards course written by NHS Digital within 6 months of being in post. They will then start working towards the NCCQ. \n\nThree years after passing the NCCQ, two further professional qualifications are made available to the coder in the form of NHS Digital's clinical coding auditor and trainer programmes.\n\nIn 2015, National Services Scotland, in collaboration with Health Boards, launched the Certificate of Technical Competence (CTC) in Clinical Coding (Scotland). Awarded by the Institute of Health Records & Information Management (IHRIM); the aims of the certificate include supporting staff new to clinical coding, and providing a standardised framework of clinical coding training across NHS Scotland.\n\nThe NCCQ is a recognized coding qualification in Scotland.\n\nAs of 2016; the typical qualification for an entry-level medical coder in the United States is completion of a diploma or certificate, or, where they are offered, an associate degree. The diploma, certificate, or degree will usually always include an Internet-based and/or in-person internship, at some form of a medical office or facility, at the conclusion. Some form of on-the-job training, or at least oversight, is also usually provided in the first months on the job, until the coder can earn an intermediate or advanced level of certification and accumulate time on the job. For further academic training, a baccalaureate or master's degree in medical information technology, or a related field, can be earned by those who wish to advance to a supervisory or academic role. That option would be recommended for those wishing to teach medical billing or coding at a college or university, community college, or technical or vocational institute, or who wish to become heads of medical billing and coding departments, especially if the doctor's office or clinic, or other facility (among other working options, a medical school or hospital, a skilled nursing facility or other nursing home, a psychiatric facility, an assisted or independent living facility, a rehabilitation facility, a rest home or domiciliary or boarding house, etc.) is very large and receives complex cases, such as a referral facility or a Level I trauma teaching hospital center. A nosologist (medical coding expert) in the U.S. will usually be certified by either AHIMA or the AAPC (often both) at their highest level of certification and specialty inpatient and/or outpatient certification (pediatrics, obstetrics/gynecology, gerontology, oncology are among those offered by AHIMA and/or the AAPC), have at least 3-5 years of intermediate experience beyond entry-level certification and employment, and often holds an associate, bachelor's, or graduate degree. \n\nThe AAPC offers the following entry-level certifications in the U.S.: Certified Professional Coder (CPC); which tests on most areas of medical coding, and also the Certified Inpatient Coder (CIC) and Certified Outpatient Coder (COC). Also in the American Health Information Management Association (AHIMA) offers the entry-level Certified Coding Associate (CCA); which is, like the AAPC's CPC, a wide-ranging introductory test. \n\nSome U.S. states, though decidedly not the majority, as it is a very recent trend, now mandate or at least strongly encourage certification or a degree from a college- or at the minimum, some evidence of competency beyond the record of on the job training- and/or from either the AAPC or AHIMA, to be employed. Some states have registries of medical coders, though these can be voluntary listings- which is, for those few who do, most often the case- and so not mandatory. This trend was accelerated in part by the passage of HIPAA (which enforces among other things, patient privacy and access to and the form of medical records) and the Affordable Care Act (U.S. President Barack Obama's health care reform law); and similar changes in other developed and developing countries, many of which, especially in the Western developed countries, and beyond, use the ICD-10 for diagnostic medical coding, which is a quite complex system of codes. The change to more regulation and training has also been driven by the need to create accurate, detailed, and secure medical records- especially patient charts, bills, and claim form submissions, that can be recorded efficiently in an electronic era of medical records where they need to be carefully shared between different providers or institutions of care, which was encouraged and later required by legislation and institutional policy.\n\nIn many countries clinical coders are accommodated for by both professional bodies specific to coding, and organisations who represent the health information management profession as a whole.\n\n\n\n\nIHRIM are the awarding body for the National Clinical Coding Qualification (NCCQ).\n\nThere are several associations that medical coders in the United States may join, including:\n\nThe AHIMA and AAPC societies' accredited programs will generally train medical coders at a sufficient level to work in their respective states. Some medical coders elect to be certified by both societies. \n\nAHIMA maintains a list of accredited medical coding certificate (and health information management associate, bachelor's, and graduate programs, through a link on the AHIMA accredited programs page, to CAHIIM) here.\n\n\n"}
{"id": "24712166", "url": "https://en.wikipedia.org/wiki?curid=24712166", "title": "Closed system drug transfer device", "text": "Closed system drug transfer device\n\nA closed system drug transfer device or \"CSTD\" is a drug transfer device that mechanically prohibits the transfer of environmental contaminants into a system and the escape of hazardous drug or vapor concentrations outside the system. Open versus closed systems are commonly applied in medical devices to maintain the sterility of a fluid pathway. CSTDs work by preventing the uncontrolled inflow and outflow of contaminants and drugs, preserving the quality of solution to be infused into a patient. Theoretically, CSTDs should enable complete protection to healthcare workers in managing hazardous drugs, but possibly due to improper handling or incomplete product design, contaminants can still be detected despite use of CSTDs.\n\nHazardous drugs are often used for patients suffering from cancer. For example, chemotherapy agents are routinely used in the treatment of cancer. However, chemotherapy can be dangerous to a person even if they don't have cancer, as chemotherapy often indiscriminately affects both healthy and cancerous cells. For the healthcare worker tasked with preparing hazardous medications like chemotherapy, manipulation of these agents presents a substantial risk; for example, it may negatively affect their fertility, increase their risk of developing certain cancers themselves, or have unwelcome effects on fetuses. As an addition to standard safe handling practices, CSTDs are devices that are designed to additionally limit exposure of hazardous drugs to the personnel that manipulate them.\n\nInvestment and interests in CSTDs continue to grow over the past decade as concerns of Occupational Safety and Health (OSH), together with increased awareness of drug risks have pushed the market to explore better options for handling hazardous materials. A Cochrane review found \"no evidence for or against adding CSTD to safe handling of hazardous medicines\" based on a review of 23 studies, but acknowledged that the studies did not use randomised controlled trials nor evaluate the value of treatment. CSTDs used in this study include PhaSeal, Tevadaptor and SpikeSwan. It remains that new solutions to increasing safety of handling hazardous drugs have to be developed. Conceptually, through operating in a closed system, CSTDs should significantly reduce risks to nurses. However, the robustness of product design and extent of proper usage by nurses affects the efficacy of the CSTD in achieving OSH.\n\nThe first FDA-approved CSTD was in 1998, called PhaSeal. Since that time, many other CSTD products have been developed in the United States. MD Anderson hospital was the first hospital in the United States to widely implement CSTD technology.\n\nThe definition of a closed system drug transfer device was first published in an alert warning released by the American National Institute for Occupational Safety and Health (NIOSH). This warning was issued in relation to studies that showed a correlation between working with or near hazardous drugs in a health care environment and the increased risk of developing skin rashes, infertility, miscarriage and infant birth defects, as well as the possibility of developing leukemia and other forms of cancer. This NIOSH alert recommended that a closed system drug transfer device be used whenever hazardous drugs were to be handled.\n\nNIOSH, in response to the need for a working model as to what a \"closed system\" and what a \"closed system drug transfer device\" was, provided the following definition:\n\n\nThe NIOSH definition is the only definition that includes drug vapors. NIOSH considers the containment of vapor extremely important, such that in September 2015, NIOSH issued a Testing Protocol to assess the effectiveness of closed systems. NIOSH developed and tested 5 CSTDs to assess its \"closeness.\" 2 of the 5 CSTDs tested passed. This protocol is aimed as being a test standard for CSTDs to assess the \"closeness\" of each system.\n\nISOPP, the International Society of Oncology Pharmacy Practitioners, splits the definition of a closed system into two different categories:\n\n\nIn September 2015, the CDC issued a proposal for a testing protocol designed to test various CSTDs on the market today to help healthcare decision makers decipher between various systems available on the market today. This protocol had an initial close date of November 2015 but was extended until March 2016. Findings from \"Fred Massoomi's Response to Docket No. CDC-2015-0075, NIOSH-288\" include:\n\nCommercially available CSTD products include the following:\n"}
{"id": "240766", "url": "https://en.wikipedia.org/wiki?curid=240766", "title": "Cochrane Library", "text": "Cochrane Library\n\nThe Cochrane Library (named after Archie Cochrane) is a collection of databases in medicine and other healthcare specialties provided by Cochrane and other organizations. At its core is the collection of Cochrane Reviews, a database of systematic reviews and meta-analyses which summarize and interpret the results of medical research. The Cochrane Library aims to make the results of well-conducted controlled trials readily available and is a key resource in evidence-based medicine.\n\nThe Cochrane Library is a subscription-based database, originally published by Update Software and now published by John Wiley & Sons, Ltd. as part of Wiley Online Library. In many countries, including parts of Canada, the United Kingdom, Ireland, the Scandinavian countries, New Zealand, Australia, India, South Africa, and Poland, it has been made available free to all residents by \"national provision\" (typically a government or Department of Health pays for the license). There are also arrangements for free access in much of Latin America and in \"low-income countries\", typically via HINARI. All countries have free access to two-page abstracts of all Cochrane Reviews and to short plain-language summaries of selected articles.\n\nCochrane Reviews appear to be relatively underused in the United States, presumably because public access is limited (the state of Wyoming is an exception, having paid for a licence to enable free access to Cochrane Reviews for all residents of Wyoming).\n\nThe Cochrane Library consists of the following databases:\n\nThe Cochrane Reviews, CENTRAL, Methodology Reviews and Methodology Register are produced by the Cochrane Collaboration. DARE, HTA and NHS EED are compiled and maintained by the Centre for Reviews and Dissemination.\n\nThe Cochrane reviews take the format of full-length methodological studies. Cochrane researchers will perform searches of medical databases including MEDLINE, PubMed and EMBASE; a continually updated database of trials called the Cochrane Central Register of Controlled Trials (CENTRAL); hand searching, where researchers look through entire libraries of scientific journals by hand and; reference checking of obtained articles in order to identify studies that are relevant to the question they are attempting to answer. The quality of each study is carefully assessed using predefined criteria and evidence of weak methodology or the possibility that a study may have been affected by bias is reported in the review.\n\nCochrane researchers then apply statistical analysis to compare the data of the trials. This creates a review of studies, or systematic review, giving a comprehensive view of the efficacy of a particular medical intervention. Finished reviews are available as a full report with diagrams, in condensed form or as a plain language summary, in order to provide for every reader of the review.\n\nAccording to \"Journal Citation Reports\", \"The Cochrane Database of Systematic Reviews\" has a 2016 impact factor of 6.264, ranking 14th out of 154 in the category \"Medicine, General & Internal category\". Reviews are abstracted and indexed in the following bibliographic databases: Science Citation Index Expanded, Scopus, CINAHL, EMBASE, MEDLINE.\n\nThe Cochrane Library Feedback tool allows users to provide comments on and feedback of Cochrane Reviews and Protocols in The Cochrane Library. If accepted, the feedback will be published in a scrolling list of comments in reverse chronological order, with the most recent submission at the top of the page. The Collaboration has a procedure for the event of serious error, an event which has only occurred once in its history.\nAnnual colloquia have been conducted by Cochrane since 1993. From 1994 onwards, Cochrane maintains a database of posters and presentations of past colloquia. From 2009 onwards, Cochrane published the abstracts of those colloquia as supplements to the \"Cochrane Database of Systematic Reviews\". From 2010 to 2016, an annual newsletter related to Cochrane methodology called \"Cochrane Methods\" (), was published as an annual supplement.\n\n"}
{"id": "5978246", "url": "https://en.wikipedia.org/wiki?curid=5978246", "title": "Complete protein", "text": "Complete protein\n\nA complete protein or whole protein is a food source of protein that contains an adequate proportion of each of the nine essential amino acids necessary in the human diet. Examples of single-source complete proteins are red meat, poultry, fish, eggs, milk, cheese, yogurt, soybeans and quinoa. The concept does not include whether or not the food source is high in total protein, or any other information about that food's nutritious value.\n\nIt was once thought that plant sources of protein are deficient in one or more amino acids, and so vegetarian diets had to specifically combine foods during meals, which would create a complete protein. However, the most recent position of the Academy of Nutrition and Dietetics is that protein from a variety of plant foods eaten during the course of a day supplies enough essential amino acids when caloric requirements are met. Normal physiological functioning of the body is possible if one obtains enough protein and sufficient amounts of each amino acid from a plant-based diet. In fact, the highest PDCAAS scores are not given to commonly eaten meat products, but rather to animal-derived vegetarian foods like milk and eggs and the vegan food soy protein isolate.\n\nThe following table lists the optimal profile of the nine essential amino acids in the human diet, which comprises complete protein, as recommended by the Institute of Medicine's Food and Nutrition Board:\n\nThe second column in the following table shows the amino acid requirements of adults as recommended by the World Health Organization calculated for a adult. Recommended Daily Intake is based on per day, which could be appropriate for a adult.\n\n\n"}
{"id": "7694809", "url": "https://en.wikipedia.org/wiki?curid=7694809", "title": "Comprehensive sex education", "text": "Comprehensive sex education\n\nComprehensive sex education (CSE) is a sex education instruction method based on-curriculum that aims to give students the knowledge, attitudes, skills and values to make appropriate and healthy choices in their sexual lives. The intention is that this understanding will prevent students from contracting sexually transmitted infections in the future, including HIV and HPV. CSE is also designed with the intention of reducing teenage and unwanted pregnancies, as well as lowering rates of domestic and sexual violence, thus contributing to a healthier society, both physically and mentally.\n\nComprehensive sex education ultimately promotes sexual abstinence as the safest sexual choice for young people. However, CSE curriculums and teachers are still committed to teaching students about topics connected to future sexual activity, such as age of consent, safe sex, contraception such as: birth control, abortion, and use of condoms. This also includes discussions which promote safe behaviors, such as communicating with partners and seeking testing for sexually transmitted infections. Additionally, comprehensive sex education curricula may include discussions surrounding pregnancy outcomes such as parenting, adoption, and abortion. The most widely agreed benefit of using comprehensive sex education over abstinence-only sex education is that CSE acknowledges the student population will be sexually active in their future. By acknowledging this, CSE can encourage students to plan ahead to make the healthiest possible sexual decisions. This ideology of arming students to most successfully survive their future sexual experiences underlies the majority of topics within CSE, including condoms, contraception, and refusal skills.\n\nStudies have found that comprehensive sex education is more effective than receiving no instruction and/or those who receive abstinence-only instruction. Acknowledging that people may engage in premarital sex rather than ignoring it (which abstinence-only is often criticized for) allows educators to give the students the necessary information to safely navigate their future sexual lives. \n\nCSE advocates argue that promoting abstinence without accompanied information regarding safe sex practices is a disregard of reality, and is ultimately putting the student at risk. For example, programs funded under AEGP are reviewed for compliance with the 8 standards (listed below in \"Abstinence Education Grant Program (AGEP) Requirements), but are not screened for medical accuracy. Therefore, critics believe that students under these educational programs are put at a disadvantage because it prevents them from making informed choices about their sexual health. Additionally, under these AEGP programs, health educators have referred to those that engage in sex, especially females, as \"dirty\" and \"used.\" They have also used phrases such as \"stay like a new toothbrush, wrapped up and unused\" and \"chewed-up gum\" to teach abstinence. Under a CSE model, language would be more sensitive.\n\nThere is clear evidence that CSE has a positive impact on sexual and reproductive health (SRH), notably in contributing to reducing STIs, HIV and unintended pregnancy. Sexuality education does not hasten sexual activity but has a positive impact on safer sexual behaviours and can delay sexual debut. A 2014 review of school-based sexuality education programmes has demonstrated increased HIV knowledge, increased self-efficacy related to condom use and refusing sex, increased contraception and condom use, a reduced number of sexual partners and later initiation of first sexual intercourse. A Cochrane review of 41 randomized controlled trials in Europe, the United States, Nigeria and Mexico also confirmed that CSE prevents unintended adolescent pregnancies. CSE is very beneficial in regards to teen pregnancy because studies show that, teen pregnancy and childbearing have a significant negative impact on high school success and completion, as well as future job prospects. A study in Kenya, involving more than 6,000 students who had received sexuality education led to delayed sexual initiation, and increased condom use among those who were sexually active once these students reached secondary school compared to more than 6,000 students who did not receive sexuality education. CSE also reduces the frequency of sex and the number of partners which in turn also reduces the rates of sexually transmitted infections.\n\nUNAIDS and the African Union have recognized CSE’s impact on increasing condom use, voluntary HIV testing and reducing pregnancy among adolescent girls and have included comprehensive, age-appropriate sexuality education as one of the key recommendations to fast track the HIV response and end the AIDS epidemic among young women and girls in Africa.\n\nAs the field of sexuality education develops, there is increasing focus on addressing gender, power relations and human rights in order to improve the impact on SRH outcomes. Integrating content on gender and rights makes sexuality education even more effective. A review of 22 curriculum- based sexuality education programmes found that 80 per cent of programmes that addressed gender or power relations were associated with a significant decrease in pregnancy, childbearing or STIs. These programmes were five times as effective as those programmes that did not address gender or power. CSE empowers young people to reflect critically on their environment and behaviours, and promotes gender equality and equitable social norms, which are important contributing factors for improving health outcomes, including HIV infection rates. The impact of CSE also increases when delivered together with efforts to expand access to a full range of high- quality, youth-friendly services and commodities, particularly in relation to contraceptive choice.\n\nA global review of evidence in the education sector also found that teaching sexuality education builds confidence, a necessary skill for delaying the age that young people first engage in sexual intercourse, and for using contraception, including condoms. CSE has a demonstrated impact on improving knowledge, self-esteem, changing attitudes, gender and social norms, and building self-efficacy.\n\nWhile CSE implementation is on the rise in the United States, it remains difficult for state officials to regulate what is and is not taught in the classroom. This is due in large part to the undefinability of CSE; CSE has the potential to comprise such a wide range of sexual information, and over-all focus varies widely between curriculums. Educators have also accused CSE as fundamentally operating as a form of \"abstinence-plus,\" due to the reality that CSE often involves minimal body related information and excessive promotions of abstinence. \"So-called Comprehensive Sex Ed\" says Sharon Lamb, a professor at the University of Massachusetts Boston, \"has been made less comprehensive as curricula are revised to meet current federal, state, and local requirements.\"\n\nThe term \"comprehensive\" is also often misleading because some comprehensive programs do not show the holistic picture of human sexuality. LGBTQIA+ advocates have long been critical of the ways in which comprehensive sex education generally promotes marriage as the end goal for students. Even when curriculums claim to be inclusive of LGBT experiences, they often promote heteronormative lifestyles as \"normal.\" Inclusion of LGBT identities and health topics is necessary for LGBTQIA+ students to feel safe and seen in their sex ed classrooms. When these students do not have access to or an interest in marriage they are practically erased from the CSE narrative. \n\nA cross sectional study done in New York City analyzed the sexual behaviors of high school girls. Studies found that, \"high school girls who identified as LGBTQIA+ were more likely to report substance use such as: alcohol, marijuana, cocaine, heroin, meth, ecstasy and prescription drugs. They also had higher rates of contemplating and/or attempting suicide.\" Another study found that \"the LGBTQIA+ youth accesses health information online five times more than the heterosexual population, and these rates are even higher for LGBTQIA+ youth that identify as a person of color which stems from the fact that they lack health resources. \n\nIn fact, as of May 2018, only 12 states require discussion of sexual orientation and of these, only 9 states require that discussion of sexual orientation be inclusive. Additionally, several states have passed legislation that bans teachers from discussing gay and transgender issues, such as sexual health and HIV/AIDS awareness, in a positive light. Furthermore, three states require that teachers only portray LGBTQIA+ people in a negative light.\n\nIn a Canada, a federal report showed that LGBTQIA+ community has less access to health services and faces more comprehensive health challenges compared to the general population. As a result of lack of support for the LGBTQIA+ population, the Comprehensive Health Education Workers (CHEW) Project emerged in October 2014. Their goal is to educate the LGBTQIA+ community about topics such sexual and gender identity, sexually transmitted infections (STIs), healthy social relationships, and depression. They do this though workshops, arts‐based projects, and one‐on‐one meetings. \n\nMany people regard health education as a moral or religious issue, and therefore should be taught not in schools. \"Before the late 1800s, delivering sex education in the United States and Canada was primarily seen as a parent’s responsibility. Today, programs under the Sexuality Information and Education Council of the United States (SIECUS) begin comprehensive sex education in pre-kindergarten, which many people believe is not age appropriate. Many of those in favor of abstinence only education, usually fear any type of sexual education that encourages sexual behavior at a young age.\n\nAlthough CSE is seen as the polar opposite of abstinence only education, some critics believe that they are very similar. They both aim at preventing STIs and teen pregnancy. The only way in which they differ is through their primary goal. Abstinence only education aims at reducing premarital sex while comprehensive sex education acknowledges that premarital sex may happen and therefore seeks to reduce the unintended consequences of premarital sex through education. Studies in developing counties surrounding sex education have also shown that comprehensive sex education is not needed. General education, such as literacy skills, was seen to delay sexual initiation and reduced the likelihood of pregnancy. Therefore, some people believe general education is of more importance.\n\nAlthough there is no federal mandate that requires states to teach sexual education, there is federal funding available to assist with sexual education programs.\n\nHistorically, funding for abstinence education has always been favored over CSE. In 1996, during Bill Clinton's presidency, legislation was passed to promote abstinence in education programs. Under Title V Section 510 of the Social Security Act, the Abstinence Education Grant Program (AGEP), was passed. AEGP has always been renewed before its expiration date, and each time funds gradually increase from fifty million dollars per year to seventy-five and as high as $6.75 million per state grant in 2015. The way the funds are disbursed are based on the proportion of low-income children in each state. So far, thirty-six states have been given AEGP funds.\n\nPart of Section 510(b) of Title V of the Social Security Act, contains the \"A-H guidelines,\" which are the eight criteria that programs must abide by order to be eligible to receive federal funding. They are as follows:\n\nA. Has as its exclusive purpose teaching the social, psychological, and health gains to be realized by abstaining from sexual activity;\n\nB. Teaches abstinence from sexual activity outside marriage as the expected standard for all school-age children;\n\nC. Teaches that abstinence from sexual activity is the only certain way to avoid out-of-wedlock pregnancy, sexually transmitted diseases, and other associated health problems;D. Teaches that a mutually faithful, monogamous relationship in the context of marriage is the expected standard of sexual activity;E. Teaches that sexual activity outside the context of marriage is likely to have harmful psychological and physical effects;\n\nF. Teaches that bearing children out of wedlock is likely to have harmful consequences for the child, the child's parents, and society;G. Teaches young people how to reject sexual advances and how alcohol and drug use increase vulnerability to sexual advances; andH. Teaches the importance of attaining self-sufficiency before engaging in sexual activity;\n\nIn addition to abiding by these 8 conditions, AGEP compliant programs cannot discuss contraception, STIs, or methods for protecting against STIs, except only when describing failure rates.\n\nMore recently legislation has pushed for funding that goes beyond abstinence only education. In 2010, President Obama introduced the Teen Pregnancy Prevention Program (TPP), which provides a total of $114.5 million annually to sex education programs that are \"medically accurate and age-appropriate.\" TPP falls under a subsection of United States Department of Health and Human Services (\"HHS\") which is overseen by the Office of Adolescent Health. Funding for TPP is dispersed if \"they emulate specific evidence-based programs promulgated under TPP.\" \n\nIn January 2016, the California Healthy Youth Act, amended the California Comprehensive Sexual Health and HIV/AIDS Prevention Education Act to include minority groups and expand health education. Before it authorized schools to provide comprehensive sex education and required that all materials are made accessible to students with a variety of needs. It also focused solely on marital relationships. It now mandates that schools provide comprehensive sex education and states that \"materials cannot be biased and must be appropriate for students of all races, genders, sexual orientations, and ethnic and cultural backgrounds, as well as those with disabilities and English language learners.\" Additionally, education must now include \"instruction about forming healthy and respectful committed relationships,\" regardless if marital status. Furthermore, it is now required to have discussions about all FDA-approved contraceptive methods in preventing pregnancy, including the morning after pill.\n\nIn conclusion now requires that all sex education programs promulgated in the state should\n\n\nSome critics state that young people’s access to CSE is grounded in internationally recognized human rights, which require governments to guarantee the overall protection of health, well-being and dignity, as per the Universal Declaration on Human Rights, and specifically to guarantee the provision of unbiased, scientifically accurate sexuality education.\n\nThese rights are protected by internationally ratified treaties, and lack of access to sexual and reproductive health (SRH) education remains a barrier to complying with the obligations to ensure the rights to life, health, non-discrimination and information, a view that has been supported by the Statements of the Committee on the Rights of the Child, the Convention on the Elimination of all Forms of Discrimination Against Women (CEDAW) Committee, and the Committee on Economic, Social and Cultural Rights.\n\nThe commitment of individual states to realizing these rights has been reaffirmed by the international community, in particular the Commission on Population and Development (CPD), which – in its resolutions 2009/12 and 2012/13 – called on governments to provide young people with comprehensive education on human sexuality, SRH and gender equality.\n\nOther analysis show that comprehensive sex education is not an international right nor a human right because it not clearly stated in either a treaty nor custom. By international law, states are required to provide access to information and education about reproductive health, but this does not require a sex education curriculum. It may take different forms such as mandating that local school districts create a system for providing information to students, or mandating that health clinics and practitioners dispense information to patients.\n\nAs CSE gains momentum and interest at international, regional and national levels, governments are increasingly putting in place measures to scale-up their delivery of some form of life skills-based sexuality education, as well as seeking guidance on best practice, particularly regarding placement within the school curriculum. Sexuality education may be delivered as a stand-alone subject or integrated across relevant subjects within the school curricula. These options have direct implications for implementation, including teacher training, the ease of evaluating and revising curricula, the likelihood of curricula being delivered, and the methods through which it is delivered.\n\nWithin countries, choices about implementing integrated or stand-alone sexuality education are typically linked to national policies and overall organization of the curricula. The evidence base on the effectiveness of stand-alone vs. integrated sexuality education programming is still limited. However, there are discernible differences for policy-makers to consider when deciding the position of CSE within the curriculum.\n\nAs a stand-alone subject, sexuality education is set apart from the rest of the curriculum, whether on its own or within a broader stand-alone health and life skills curriculum. This makes it more vulnerable to potentially being sacrificed due to time and budget constraints, since school curricula are typically overcrowded.\n\nHowever, a stand-alone curriculum also presents opportunities for specialized teacher training pathways, and the use of non-formal teaching methodologies that aim to build learners’ critical thinking skills. The pedagogical approaches promoted through sexuality education – such as learner-centred methodologies, development of skills and values, group learning and peer engagement – are increasingly being recognized as transformative approaches that impact on learning and education more widely. As a standalone subject, it is also significantly easier to monitor, which is crucial in terms of evaluating the effectiveness of programming, and revising curricula where it is not delivering the desired learning outcomes.\n\nWhen sexuality education is integrated or infused, it is mainstreamed across a number of subject areas, such as biology, social studies, home economics or religious studies. While this model may reduce pressure on an overcrowded curriculum, it is difficult to monitor or evaluate, and may limit teaching methodologies to traditional approaches.\n\nApart from the different teaching methods, termiology also differs. Abortion, homosexuality, abstinence have connotations and definitions that vary state. For example, the word \"abstinence\" may refer to disengaging from all forms of sexual activities until marriage or may refer to only disengaging from sexual intercourse. Furthermore, the degree of sexual activity that \"abstinence\" connotates is often unclear, because sexual behavior that is not sexual intercourse may or may not be included in its definition. As a result, students are left confused about what activities are risky and teachers do not know what they can and cannot teach.\n\nThe term \"comprehensive,\" is also falls on spectrum, therefore can be considered an umbrella term. CSE means something radical for some institutions while it can mean something moderate and even conservative for others.\n\nAccording to the Sexuality Information and Education Council of the United States (SIECUS), the guidelines for comprehensive sexuality education are as follows:\n\nJust as teaching methods and curricula vary by state, excusal from sex education also varies by state. States may have with an opt out or opt in produce. In some states, students can opt out of receiving sexual education without specifying a particular reason. In other states, students can only opt out for religious or moral reasons. In an opt-in provision, parents must actively agree to allow their children to receive sex education prior to the start of the sexual education.\n\nSince 1997, the amount of sexual content on TV has nearly doubled in the United States. Additionally, a study done in 2008 showed that nearly 40% of popular music lyrics contained sexual references which were often sexually degrading. These lyrics were also often accompanied with mentions of other risk behaviors, such as substance use and violence. \n\nTeens (ages 13-15) in the United States, use entertainment media as their top source for education in regards to sexuality and sexual health. Additionally, a study found that 15-19 year olds in the U.S use media far more than parents or schools to obtain information about birth control. Some studies have found that, \"very few teen television shows mention any of the responsibilities or risks (e.g., using contraception, pregnancy, STIs) associated with sex and almost none of the shows with sexual content include precaution, prevention, or negative outcomes as the primary theme.\" Television shows 16 and Pregnant and its spin-off, Teen Mom, which first aired on MTV in 2009 received major disapproval from some parents as they thought the shows glamorized teen pregnancy and motherhood. However, 16 and Pregnant actually led to a 4.3 percent reduction in teen pregnancy, mostly as a result of increased contraceptive use. In contrast, other data shows that exposure to high levels of sexual content on the television causes adolescents to have twice the risk of becoming pregnant in the following 3 years, compared to those who were exposed to low levels.\n\nThe film Mean Girls, directed by Mark Waters shed light on the state sex education in some parts of the United States. In the film the health instructor states, \"At your age, you're going to have a lot of urges. You're going to want to take off your clothes and touch each other. But if you do touch each other, you will get chlamydia and die.\" This line is meant to be satirical, but it illustrates common flaws within sex education in the U.S. It depicts simplistic descriptions of sexual activity and implementation of fear without any legitimate basis.\n\nComprehensive sex education is the main topic in the documentary \"The Education of Shelby Knox\" released in 2005 about Lubbock, Texas, which has one of the highest teen pregnancy and STD rates in the nation. The \"solution\" to which is a strict abstinence-only sex education curriculum in the public schools and a conservative preacher who urges kids to pledge abstinence until marriage.\n\nIn 2013, \"How to Lose Your Virginity\" was released, a documentary that questioned the effectiveness of the abstinence-only sex education movement and observed how sexuality continues to define a young woman's morality and self-worth. The meaning and necessity of virginity as a social construct is also examined through narration and interviews with notable sexuality experts, such as former Surgeon General Dr. Joycelyn Elders, \"Scarleteen\" creator and editor Heather Corinna, historian Hanne Blank, author Jessica Valenti, and comprehensive sex education advocate Shelby Knox.\n\nNot only have films portrayed sex education, but so has social media. Platforms such as YouTube, Facebook, Vine, and others are used as a tool to uplift the narratives of marginalized communities such as persons of color and LGBTQIA+ persons in hopes to \"strengthen sexual health equity for all.\"\n\nAs a result of the mass amount of sex content in media, media literacy education (MLE) has emerged. It was created to address the influence of unhealthy media messages on risky health decisions, such as intention to use substances, body image issues, and eating disorders. A study analyzed the effectiveness of a teacher-led MLE program, called Media Aware Sexual Health (MASH), which provides students with accurate health information and teaches them how to apply that information to critical analysis of media messages. This comprehensive sex education resulted in increased intentions to talk to a parent, partner and medical professional prior to sexual activity, and intentions for condom use.\n\nDue to knowledge gaps in most sex education curricula for teens, free online resources like Sex, Etc. and teensource.org have been created to promote comprehensive, inclusive sex education for teenagers.\n\n"}
{"id": "1864519", "url": "https://en.wikipedia.org/wiki?curid=1864519", "title": "Condom fatigue", "text": "Condom fatigue\n\nCondom fatigue is a term used by medical professionals and safer sex educators to refer to the phenomenon of decreased condom use. It can also be used to describe a general weariness of and decreased effectiveness of safer sex messages. This is sometimes called \"prevention fatigue\".\n\nThe term has particularly been used to describe men who have sex with men, though the term applies to people of all genders and sexual orientations. Condom fatigue has been partially blamed for an increase in HIV infection rates, though this has not been substantiated in any study.\n\nCondom fatigue is not a universal phenomenon. In Germany, condom use between new sexual partners has increased between 1994 and 2010 from 65% to 87%.\n\nHIV infection is increasing at a rate of 12% annually among 13–24-year-old American men who have sex with men. Experts attribute this to \"AIDS fatigue\" among younger people who have no memory of the worst phase of the epidemic in the 1980s and early 1990s, as well as \"condom fatigue\" among those who have grown tired of and disillusioned with the unrelenting safer sex message. The increase may also be because of new treatments.\n\n"}
{"id": "2393025", "url": "https://en.wikipedia.org/wiki?curid=2393025", "title": "Coriaria thymifolia", "text": "Coriaria thymifolia\n\nCoriaria thymifolia, known as shanshi, is a plant found in the northern Andes. The plant is often mistaken for blueberries, however unlike blueberries, the fruit of the Shanshi is toxic and has hallucinogenic properties.\n"}
{"id": "56771693", "url": "https://en.wikipedia.org/wiki?curid=56771693", "title": "DISCERN", "text": "DISCERN\n\nDISCERN is an online instrument designed to measure the quality of health information on the Internet. It was originally developed by Deborah Charnock, Sasha Shepperd, Gill Needham, and Robert Gann, who reported on its development and validation in a February 1999 paper. The DISCERN website was launched in May 1999. Despite originally being developed using printed materials, it has also been shown to be effective at evaluating the quality of online medical information, as well as for printed information.\n"}
{"id": "10588046", "url": "https://en.wikipedia.org/wiki?curid=10588046", "title": "Diet and cancer", "text": "Diet and cancer\n\nDietary factors are recognized as having a significant effect on the risk of cancers, with different dietary elements both increasing and reducing risk. Diet and obesity may be related to up to 30-35% of cancer deaths, while physical inactivity appears to be related to 7% risk of cancer occurrence. One review in 2011 suggested that total caloric intake influences cancer incidence and possibly progression.\n\nWhile many dietary recommendations have been proposed to reduce the risk of cancer, few have significant supporting scientific evidence. Obesity and drinking alcohol are confirmed causes of cancer. Lowering the drinking of beverages sweetened with sugar is recommended as a measure to address obesity. A diet low in fruits and vegetables and high in red meat has been implicated but not confirmed, and the effect may be small for well-nourished people who maintain a healthy weight. \n\nSome specific foods are linked to specific cancers. Studies have linked eating red or processed meat to an increased risk of breast cancer, colon cancer, prostate cancer, and pancreatic cancer, which may be partially explained by the presence of carcinogens in foods cooked at high temperatures. Aflatoxin B1, a frequent food contaminate, causes liver cancer, but drinking coffee is associated with a reduced risk. Betel nut chewing causes oral cancer. Pickled vegetables are directly linked to increased risks of several cancers. The differences in dietary practices may partly explain differences in cancer incidence in different countries. For example, stomach cancer is more common in Japan due to its high-salt diet and colon cancer is more common in the United States. Immigrant communities tend to develop the risk of their new country, often within one generation, suggesting a substantial link between diet and cancer.\n\nDietary recommendations for cancer prevention typically include weight management and eating \"mainly vegetables, fruit, whole grains and fish, and a reduced intake of red meat, animal fat, and refined sugar.\"\n\nA number of diets and diet-based regimes are claimed to be useful against cancer. Popular types of \"anti-cancer\" diet include the Breuss diet, Gerson therapy, the Budwig protocol and the macrobiotic diet. None of these diets has been found to be effective, and some of them have been found to be harmful.\n\nNutritional epidemiologists use multivariate statistics, such as principal components analysis and factor analysis, to measure how patterns of dietary behavior influence the risk of developing cancer. (The most well-studied dietary pattern is the mediterranean diet.) Based on their dietary pattern score, epidemiologists categorize people into quantiles. To estimate the influence of dietary behavior on risk of cancer, they measure the association between quantiles and the distribution of cancer prevalence (in case-control studies) and cancer incidence (in longitudinal studies). They usually include other variables in their statistical model to account for the other differences between people with and without cancer (confounders). For breast cancer, there is a replicated trend for women with a more \"prudent or healthy\" diet, i.e. higher in fruits and vegetables, to have a lower risk of cancer. A \"drinker dietary pattern\" is also associated with higher breast cancer risk, while the association is inconsistent between a more westernized diet and elevated risk of breast cancer. Pickled foods are linked with cancer.\n\nAlcohol is associated with an increased risk of a number of cancers. 3.6% of all cancer cases and 3.5% of cancer deaths worldwide are attributable to drinking of alcohol. Breast cancer in women is linked with alcohol intake. Alcohol also increases the risk of cancers of the mouth, esophagus, pharynx and larynx, colorectal cancer, liver cancer, stomach and ovaries. The International Agency for Research on Cancer (Centre International de Recherche sur le Cancer) of the World Health Organization has classified alcohol as a Group 1 carcinogen. Its evaluation states, \"There is sufficient evidence for the carcinogenicity of alcoholic beverages in humans. …Alcoholic beverages are carcinogenic to humans (Group 1).\"\n\nOn October 26, 2015, the International Agency for Research on Cancer of the World Health Organization reported that eating processed meat (e.g., bacon, ham, hot dogs, sausages) or red meat was linked to some cancers.\n\nThe evidence on the effect of dietary fiber on the risk of colon cancer is mixed with some types of evidence showing a benefit and others not. While eating fruit and vegetables has a benefit, it has less benefit on reducing cancer than once thought.\n\nA 2014 study found fruit but not vegetables protected against upper gastrointestinal tract cancer. While fruit, vegetable and fiber protected against colorectal cancer and fiber protected against liver cancer.\n\nFlavonoids (specifically flavonoids such as the catechins) are \"the most common group of polyphenolic compounds in the human diet and are found ubiquitously in plants.\" While some studies have suggested flavonoids may have a role in cancer prevention, others have been inconclusive or suggested they may be harmful.\n\nAccording to Cancer Research UK, \"there is currently no evidence that any type of mushroom or mushroom extract can prevent or cure cancer\", although research into some species continues.\n\nAccording to the American Cancer Society, although laboratory research has shown the possibility of some connection between soybeans and cancer, as yet there is no conclusive evidence about the anti-cancer effect of soy on human beings.\n\nLaboratory experiments have found that turmeric might have an anti-cancer effect. Although trials are ongoing, large doses would need to be taken for any effect. It is not known what, in any, positive effect turmeric has for human beings with cancer.\n\nAlthough green tea has been promoted for its anti-cancer effect, research into it has produced mixed results; it is not known if it helps people prevent or treat cancer. A review of all published studies by the US Food and Drug Administration in 2011 concluded it is very unlikely that green tea prevents any kind of cancer in humans.\n\nResveratrol has shown anti-cancer activity in laboratory experiments, but , there is no evidence of an effect on cancer in humans.\n\nVitamin D supplements have been widely marketed on the internet and elsewhere for their claimed anti-cancer properties. There is however insufficient evidence to recommend that vitamin D be prescribed for people with cancer, although there is some evidence that hypovitaminosis D may be associated with a worse outcome for some cancers. A 2014 systematic review by the Cochrane Collaboration found, \"no firm evidence that vitamin D supplementation decreases or increases cancer occurrence in predominantly elderly community-dwelling women.\"\n\nAlthough numerous cellular mechanisms are involved in food intake, many investigations over the past decades have pointed out defects in the methionine metabolic pathway as cause of carcinogenesis. For instance, deficiencies of the main dietary sources of methyl donors, methionine and choline, lead to the formation of liver cancer in rodents. Methionine is an essential amino acid that must be provided by dietary intake of proteins or methyl donors (choline and betaine found in beef, eggs and some vegetables). Assimilated methionine is transformed in S-adenosyl methionine (SAM) which is a key metabolite for polyamine synthesis, e.g. spermidine, and cysteine formation (see the figure on the right). Methionine breakdown products are also recycled back into methionine by homocysteine remethylation and methylthioadenosine (MTA) conversion (see the figure on the right). Vitamins B, B, folic acid and choline are essential cofactors for these reactions. SAM is the substrate for methylation reactions catalyzed by DNA, RNA and protein methyltransferases. \n\nThe products of these reactions are methylated DNA, RNA or proteins and S-adenosylhomocysteine (SAH). SAH has a negative feedback on its own production as an inhibitor of methyltransferase enzymes. Therefore, SAM:SAH ratio directly regulates cellular methylation, whereas levels of vitamins B, B, folic acid and choline regulates indirectly the methylation state via the methionine metabolism cycle. A near ubiquitous feature of cancer is a maladaption of the methionine metabolic pathway in response to genetic or environmental conditions resulting in depletion of SAM and/or SAM-dependent methylation. Whether it is deficiency in enzymes such as methylthioadenosine phosphorylase, methionine-dependency of cancer cells, high levels of polyamine synthesis in cancer, or induction of cancer through a diet deprived of extrinsic methyl donors or enhanced in methylation inhibitors, tumor formation is strongly correlated with a decrease in levels of SAM in mice, rats and humans.\n\nAccording to a 2012 review, the effect of methionine restriction on cancer has yet to be studied directly in humans and \"there is still insufficient knowledge to give reliable nutritional advice\".\n\nMultiple oncogenic signaling pathways have been involved in the processes of cancer cell invasion and metastasis. Among these signaling pathways, Wnt and Hedgehog signaling pathways are involved in the embryonic development, in the biology of cancer stem cells (CSCs) and in the acquisition of epithelial to mesenchymal transition (EMT).\n\n"}
{"id": "900035", "url": "https://en.wikipedia.org/wiki?curid=900035", "title": "Dipsomania", "text": "Dipsomania\n\nDipsomania is a historical term describing a medical condition involving an uncontrollable craving for alcohol. In the 19th century, the term dipsomania was used to refer to a variety of alcohol-related problems, most of which are known today as alcoholism. Dipsomania is occasionally still used to describe a particular condition of periodic, compulsive bouts of alcohol intake. The idea of dipsomania is important for its historical role in promoting a disease theory of chronic drunkenness. The word comes from Greek \"dipso\" (= thirst) and \"mania\".\n\nIt is still mentioned in the WHO ICD-10 classification as an alternative description for Alcohol Dependence Syndrome, episodic use F10.26\n\nThe term was coined by the German physician Christoph Wilhelm Hufeland in 1819, when, in a preface to an influential book by German-Russian doctor C. von Brühl-Cramer, he translated Brühl-Cramer's term \"trunksucht\" as \"dipsomania\".\n\nDue to the influence of Brühl-Cramer's pioneering work, dipsomania became popular in medical circles throughout the 19th century. Political scientist Mariana Valverde describes dipsomania as \"the most medical\" of the many terms used to describe habitual drunkenness in the 19th century. Along with terms such as \"inebriety\", the idea of dipsomania was used as part of an effort of medical professionals and reformers to change attitudes about habitual drunkenness from being a criminally punishable vice to being a medically treatable disease. As historian Roy MacLeod wrote about this dipsomania reform movement, it \"illuminates certain features of the gradual transformation taking place in national attitudes towards the prevention and cure of social illnesses during the last quarter of the 19th century.\"\n\nAlthough \"dipsomania\" was used in a variety of somewhat contradictory ways by different individuals, by the late 19th century the term was usually used to describe a periodic or acute condition, in contrast to chronic drunkenness. In his 1893 book \"Clinical Lessons on Mental Diseases: The Mental State of Dipsomania\", Magnan characterized dipsomania as a crisis lasting from one day to two weeks, and consisting of a rapid and huge ingestion of alcohol or whatever other strong, excitatory liquid was available. Magnan further described dipsomania as solitary alcohol abuse, with loss of all other interests, and these crises recurred at indeterminate intervals, separated by periods when the subject was generally sober.\n\nOver time, the term dipsomania became less common, replaced by newer ideas and terms concerning chronic and acute drunkenness and alcoholism.\n\n"}
{"id": "17746102", "url": "https://en.wikipedia.org/wiki?curid=17746102", "title": "Donald Hunter (physician)", "text": "Donald Hunter (physician)\n\nDonald Hunter CBE FRCP (11 February 1898 – 11 December 1978) was a British physician and author of a classic text on occupational medicine, \"The Diseases of Occupations\".\n\nHunter was born in the East End of London. His father was George Hunter, a deputy engineer in the General Post Office. He entered The London Hospital in 1915 but left in World War I to become a surgeon probationer RNVR in \"HMS Faulkner\" in the Dover Patrol. After the war, he returned to The London Hospital and qualified in 1920.\n\nFollowing a series of house appointments, he became first assistant to Lord Dawson of Penn (1864–1945) and then was appointed Assistant Physician to The London Hospital in 1927. He completed his MD degree in 1922 and FRCP in 1929.\n\nHunter became curator to the Medical School Museum in 1933, and during his 30 years in office collected specimens said to ‘represent almost all that is worth having in morbid anatomy’. He later became Director of the Medical Research Council’s Department for Research in Industrial Medicine at The London Hospital. In 1925 he married Mathilda Bugnion, daughter of a Lutheran Pastor from Lausanne, Switzerland. They had four children, two of whom became doctors.\n\nIn 1935 he gave a series of lectures to the Derby Medical Society on occupational diseases. These lectures were followed by other publications, often jointly with colleagues. These all culminated in his 1955 book \"The Diseases of Occupations\" which became a classic \"as soon as it was published\". It was said that he wrote all 1000 pages by getting up at 5 a.m. for 18 months. It has been criticized as \"perhaps lacking in balance\" and being \"sometimes irrelevant\" but \"always entertaining and gaining immeasurably from its readability and its impact on the medical profession.\"\n\nThere followed a Penguin Book \"Health in Industry\" published in 1959. Hunter was founder Editor of the British Journal of Industrial Medicine.\n\nHunter's memory is preserved not only by his book but also by a Donald Hunter Room at the Royal College of Physicians. There is also a Donald Hunter House in London which provides accommodation for students.\n\nDonald Hunter CBE MD FRCP, Geoffrey O Storey. \"Journal of Medical Biography\", Vol.15, No.3, pp. 153–157\n"}
{"id": "7742604", "url": "https://en.wikipedia.org/wiki?curid=7742604", "title": "E-patient", "text": "E-patient\n\nAn e-patient is a health consumer who participates fully in his/her medical care, primarily by gathering information about medical conditions that impact them and their families, using the Internet and other digital tools. The term encompasses those who seek guidance for their own ailments and the friends and family members who go online on their behalf. E-patients report two effects of their health research: \"better health information and services, and different, but not always better, relationships with their doctors.\"\n\nE-patients are active in their care and demonstrate the power of the Participatory Medicine or Health 2.0 / Medicine 2.0. model of care. The \"e\" can stand for \"electronic\" but has also been used to refer to other terms, such as \"equipped\", \"enabled\", \"empowered\" and \"expert\".\n\nThe current state of knowledge on the impact of e-patients on the healthcare system and the quality of care received indicates:\nA 2011 study of European e-patients found that they tended to be \"inquisitive and autonomous\" and that they noted that the number of e-patients in Europe appeared to be rising. A 2012 study found that e-patients uploading videos about their health experienced a loss of privacy, but also positive benefits from social support. Furthermore, a 2017 study utilizing social network analysis found that when e-patients are included in health care conferences, they increase information flow, expand propagation, and deepen engagement in the conversation of Tweets when compared to both physicians and researchers while only making up 1.4% of the stakeholder mix.\n\n\n"}
{"id": "28999061", "url": "https://en.wikipedia.org/wiki?curid=28999061", "title": "Forced circumcision", "text": "Forced circumcision\n\nForced circumcision refers to circumcision of males who have not given their consent to the procedure. In a biblical context, the term is used especially in relation to Paul the Apostle and his polemics against the forced circumcision of gentile Christians. Among adults, forced circumcisions have occurred in a wide range of situations, most notably in the compulsory conversion of non-Muslims to Islam and the forced circumcision of Teso, Turkana and Luo men in Kenya, as well as the abduction of South African teenage boys to so-called circumcision schools (\"bush schools\"). In South Africa, custom allows uncircumcised Xhosa-speaking men past the age of circumcision (i.e., 25 years or older) to be overpowered by other men and forcibly circumcised.\n\n1 Maccabees relates the story of how Mattathias (ca. 166 BC) forcibly circumcised the sons of Jewish parents who had abandoned the rite. Forced circumcision of Gentiles by Jews is attested from the second century BC onwards. In 125 BC John Hyrcanus conquered Edom, which the Romans called Idumea; and the Idumeans were converted to Judaism. As reported by Josephus, circumcision was required of the Idumeans as a token of their acceptance of Judaism:\n\nScholars disagree on the interpretation of the sources. For example, Steven Weitzman believes the Idumeans were forcibly circumcised for political, not religious, reasons. According to Shaye J. D. Cohen, \"Ptolemy's claim, that the Idumaeans were compelled to be circumcised and to adopt Jewish ways, is a simplified account of what these urban Idumaeans experienced.\"\nDuring the short reign of Hyrcanus' eldest son, Aristobulus I (104-103 BC), the Hasmoneans gained control of Galilee. In this case, too, sources indicate that the residents were subjected to forced circumcision. Archaeological evidence suggests that, during this period, Gentiles fled from Galilee to avoid being forcibly circumcised.\n\nGreeks and Romans regarded circumcision as a mutilation of the male genitalia, but the practice is little discussed in Roman literary sources until the second century of the Christian era. There was a circumcision controversy in Early Christianity but this was resolved at the Council of Jerusalem c.50 which made it clear that circumcision of gentile converts to Christianity was not required. Josephus (who changed his allegiance from the Jews to the Roman Flavians) reports that two Roman officers who had taken refuge with Galileans during the war with Rome (early 67 AD) were put under pressure to convert to Judaism. Josephus, declaring that \"every one should worship God in accordance with the dictates of his own conscience,\" claims to have saved the two Gentiles from forced circumcision. After the First Roman-Jewish War, a head tax, the Fiscus Judaicus, was levied against all Jews. According to Suetonius, Domitian (c.90) also applied this tax to those who were circumcisied, even if they claimed they were not Jews. Titus Flavius Clemens (consul) was put to death in 95 for adopting Jewish customs. In 96 Nerva relaxed the Jewish tax as applying only to those who professed to be Jews. Sometime between 128 and 132 AD, the emperor Hadrian seems to have temporarily banned circumcision, on pain of death. Antoninus Pius exempted Jews from the ban, as well as Egyptian priests, and Origen (d. \"ca.\" 253) says that in his time only Jews were permitted to practice circumcision. Legislation under Constantine, the first Christian emperor, freed any slave who was subjected to circumcision; in the year 339, circumcising a slave became punishable by death.\n\nAlthough Greco-Roman writers view circumcision as an identifying characteristic of Jews, they believed the practice to have originated in Egypt, and recorded it among peoples they identified as Arab, Syrian, Phoenician, Colchian, and Ethiopian; circumcision was a marker of \"the Other\". Diaspora Jews might circumcise their male slaves as well as adult male converts and Jewish male infants. According to Catherine Hezser, it is an open question whether Jews of late antiquity refrained from forcibly circumcising their Gentile slaves and whether Romans avoided selling their slaves to Jews in reaction to the prohibition. The Mishnah (compiled about 200 AD) is silent on this point, whereas the Mekhilta de-Rabbi Ishmael (written at the end of the fourth century or later) suggests that Jews might indeed possess uncircumcised slaves.\n\nForced conversions, involving forced circumcision, are echoed in a vast body of scholarly literature spanning the entire history of Islam. Scholars conclude that, during the Islamic conquest of the Middle East and North Africa, forced conversion to Islam through violence or threat of violence did not play a key role. However, taxes and regulations requiring the holders of prestigious positions to become Muslims have been regarded as a form of forced conversion.\n\nIn the aftermath of the 1780 Battle of Pollilur, 7,000 British soldiers were held imprisoned by Haidar Ali and Tipu Sultan in the fortress of Seringapatnam. Of these, more than 300 were forcibly circumcised. Cromwell Massey, who kept a secret diary during his captivity, wrote: \"I lost with the foreskin of my yard all those benefits of a Christian and Englishman which were and ever shall be my greatest glory.\" Adolescent captives were, in addition to being circumcised, made to wear female clothes. James Bristow, a teenage artilleryman, revenged himself by circumcising dogs, believing that this would harm the religious feelings of the Muslim warders. The prospect of punishment did not deter him, because \"compelling us to undergo an abhorred operation [was] so base and barbarous an act of aggression, that it was impossible to reflect on it with temper.\" James Scurry, also a prisoner of war, confirms in his book, \"The Captivity, Sufferings, and Escape of James Scurry\" (1824), that English soldiers, Mangalorean Catholics, and other prisoners were forcibly circumcised.\nIn 1784, when Tipu returned from Mangalore, he brought back tens of thousands of Mangalorean Catholics from Kanara and subjected them to forced circumcision. A Hindi idiom 'Mar-mar ke Musalman bana' (meaning 'make Muslim by repeated beating') can be traced to originate from forcible conversion of Hindus.\n\nAccording to Kativa Daiya, during the 1947 partition of India \"[f]orced circumcision, shaving facial and head hair (for Sikh men), and shaving off the Hindu Brahmin's traditional, short, plaited hair (on an otherwise bald head) were routine Muslim conversion tactics for men and boys.\" \"Asia News\" reported in 2004 that the Justice and Peace Commission of Lahor spoke out against young non-Muslim men in Pakistan being converted and circumcised against their will. In 2005, the \"Gulf Times\" discussed a case of forced circumcision of Nepalese boys in Mumbai in the context of sex trade in large Indian cities.\n\nIraqi Mandaeans, residing almost exclusively in Baghdad and Basra, do not circumcise. However, their religious sensitivity on this issue has not prevented hostile rulers from subjecting Mandaean men and boys to forced circumcision. Mandaean communities, especially after the invasion of Iraq, have been subject to \"murder, kidnapping, rape, forced conversion, forced circumcision and destruction of religious property.\"\n\nIn Iraq in 2003, shortly after the fall of the Saddam regime, the thirty-five families who made up the Mandean community in Fallujia were ordered at gunpoint to adopt Islam; the men were forcibly circumcised.\n\nIn 2007 the US Committee on International Religious Freedom heard testimony reporting: \"Forced conversion is happening in an alarming degree. Boys are being kidnapped, forcibly circumcised - a major sin in the Mandaean religion - and forcibly converted to Islam.\"\n\nIn 2014, after the exodus of the Christians from Mosul and the Yazidi's from Mount Sinjar, it was reported that forced circumcisions were taking place conducted by the Islamic State \n\nThere are accounts of Christian boys abducted and forcibly circumcised even in the nineteenth century. In 1829, 9 year old Greek boy Alexandros Kitos and other young boys were kidnapped by Ottoman soldiers and sold into slavery in Egypt, all were circumcised against their will.\n\nIt is well established that, before and during the Armenian Genocide, forced conversions (involving forced circumcisions) of Armenian boys and men were frequent. \"In many cases young Armenian children were spared from deportation by local Turks who took them from their families. The children were coerced into denouncing Christianity and becoming Muslims, and were then given new Turkish names. For Armenian boys the forced conversion meant they each had to endure painful circumcision as required by Islamic custom.\"\n\nDuring the Istanbul Pogrom in September 1955, \"many Greek men, including at least one priest, were subjected to forced circumcision.\" As a result of the pogrom, the Greek minority eventually emigrated from Turkey. In 2002 there was a report that non-Muslim army recruits in Turkey had been threatened with forced circumcision. Cases are documented where Syro-Orthodox men serving in the Turkish military forces have been threatened with forced circumcision. In 1991, a young Christian Turk, fleeing from forced circumcision in the Turkish military forces, was granted asylum in Germany.\n\nThe Yazidi (not all of whom are circumcised) in Turkey have for years been subjected to direct state persecution, including compulsory religious instruction at school, forced conversion, forced circumcision, and mistreatment during military service. In 1999 there was a report of the forced circumcision of Yedizi men in Turkish Kurdistan.\n\nJohn Rawlins had sailed for 23 years without incident when, in 1621, he and his crew were kidnapped by pirates from the Barbary Coast of North Africa. Rawlins later reported that, after being taken to Algiers, two younger men were \"by force and torment ... compelled ... to turn Turks,\" which means that they were forcibly circumcised. By organizing a successful mutiny, he was able to return home in 1622.\nThe Portuguese Friar Jaono dos Sanctos claimed that, annually in Algiers in the 1620s, more than nine hundred Christian slaves were converted to Islam, \"besides about fifty boys yearly circumcised against their wills.\"\n\nThousands of Christians were forcibly circumcised in the Moluccas from December 1999 to January 2001. The Sydney Morning Herald reported in detail on this, stating that \"almost all\" of 3,928 villagers forced to convert to Islam were circumcised. Razors and knives were reused, causing infections. One of those circumcised, Kostantinus Idi, reported: \"I could not escape,\" he said. \"One of them held up my foreskin between pieces of wood while another cut me with a razor ...the third man held my head back, ready to pour water down my throat if I screamed. But I couldn't help but scream and he poured the water. I kept screaming aloud and vomited. I couldn't stand the pain.\" He further reported that one of the clerics urinated on his wound, saying it would stop infection. The Sydney Morning Herald reported that the forced conversions and forced circumcisions had been condemned by moderate Muslim leaders who said they were contrary to Islamic teachings. The local governor had also investigated the incidents.\n\nMarco Polo, in his \"Travels\", relates how a Christian king of Ethiopia took revenge on the Sultan of Aden, who had forcibly circumcised a bishop.\n\nIn Kenya, most tribes circumcise. Luo men from Western Kenya are a significant exception, for which reason they have regularly been subjected to forced circumcision. In August 2002, following a violent incident in Butere/Mumias District, a district commissioner instructed the police to \"crack down on traditional surgeons involved in forcible circumcision.\"\n\nIn November 2005, the Kenyan Human Rights Commission announced that it would seek prosecutions against politicians for inciting such violence. In one instance, a cabinet minister had said, \"Those who are not circumcised should be taken for a circumcision ceremony.\" The Commission said this amounted to an incitement to violence.\n\nIn late January 2008, a disputed election in which circumcision became an issue between President Mwai Kibaki, a Kikuyu and opposition candidate Raila Odinga, a Luo, \"the fact that Odinga was uncircumcised became an issue: He was seen by some Kikuyus as a 'child' unfit to rule because he had not passed through circumcision and initiation.\" Post-election violence reportedly \"focused on tribal animosities\", and included several cases of forced circumcision. AFP reported one Kenyan man's experience: \"A group of eight men with pangas (machetes) entered. They asked for my ID [to determine what tribe he belonged to] They slashed me and they circumcised me by force. I screamed a lot and cried for help...' He complained that police left him in a pool of blood, taking weapons left behind by the Kikuyu gang.\n\nIn September 2010, at Malaba, West Kenya, a 21-year-old Teso man was lured to a hotel, drugged, smeared with fermented millet flour and was being led away by several Bukusu to be circumcised when the police intervened. The Teso man, who agreed to a medical circumcision, condemned the Bukusu youths for trying to impose their culture on the Teso. Three weeks previously, village neighbours in Aedomoru sub location in Teso north armed themselves with clubs and prevented a 35-year-old man from being forcibly circumcised.\n\nIn 1999, a woman who was feared throughout the Vaal Triangle district of South Africa, controlled a gang of kidnappers that abducted young people, forcibly circumcising the boys and extorting ransoms from their parents for their release. A local police officer said as many as 10 teenagers had been snatched every day.\n\nIn 2004, a 22-year-old Rastafari convert was seized by relatives and forcibly circumcised by group of Xhosa tribal elders and relatives.\n\nIn December 2004, 45-year-old Nceba Cekiso was caught and circumcised against his will. The report in the \"Cape Argus\" noted, \"Xhosa culture allows people to forcibly circumcise boys deemed to be past the age of initiation... Forcing people do undergo the ancient ritual ... has, in recent times, caused concern among human rights organisations... (In) one instance two Rastafarians objected to the procedure on religious grounds. The incident has sparked a debate on whether or not traditionalists should still be allowed to force people against their will into the bush to undergo initiation.\n\nDespite being medically circumcised, a Christian Xhosa was forcibly recircumcised by his father and community leaders in 2007. He laid a charge of unfair discrimination on the grounds of his religious beliefs, seeking an apology from his father and the Congress of Traditional Leaders of South Africa. In the settlement that was reached, and which was made an order of the Equality Court, the Congress of Traditional Leaders accepted the right of adult males to choose whether to attend traditional circumcision schools according to their religious beliefs. It apologised for the comments made by its former chairman encouraging the ostracism of teenagers who refused to undergo traditional circumcision. The judge declared, \"What is important in terms of the Constitution and law is that no one can be forced to submit to circumcision without his consent.\"\n\nAccording to South African newspapers, the subsequent trial became \"a landmark case around forced circumcision.\" In October 2009, the Bhisho Equality Court (High Court) ruled that, in South Africa, circumcision is unlawful unless done with the full consent of the initiate. According to Thembela Kepe, traditional leaders allege that the ban on forced circumcision is \"a violation of cultural rights enshrined in the Constitution.\"\n\nThere is ample evidence that, for years, Christians of Khartoum and elsewhere in Sudan have been forcefully converted to Islam, and that Christian men and boys have been forcibly circumcised. Examples of Dinka boys having been forcibly circumcised in the 1990s and 2000s are known from the context of traditional slavery, still endemic in Sudan.\n\nIn 1885, Kabaka Mwanga ordered the murders of Bishop James Hannington and many local Christians. During the following period, Islamization led to several Christians being forcibly circumcised.\n\nAs discussed by anthropologist Suzette Heald and other scholars, the Gisu (alternatively, Bagishu) of Uganda \"take pride in not tolerating uncircumcised men.\" For this reason, in Gisu society, any boy or man who has been able to escape ritual circumcision (called \"imbalu\") faces the prospect of being forcibly circumcised. Voice of America, referring to the same practice, reports: \"Among the Bagishu, uncircumcised men are treated with contempt; they are not allowed in society and in most cases they are seen as failing to get local women for marriage. This is supported by all the Bagishu including women who often report uncircumcised men to tribal elders. It's considered traditional that no male is to escape the ritual regardless of where he lives, what he does or what kind of security he has.\"\nIn 2004 a father of seven was seized and forcibly circumcised after his wife told Bagishu tribal circumcisers that he was uncircumcised. A local official said the authorities could not intervene in a cultural ritual. Other forced circumcisions occurred in September 2006 and June 2008. In all these cases, family members of the victims approved of the forced circumcision. Other tribal groups in Uganda and the Ugandan Foundation for Human Rights Initiative regard forced circumcision as a human rights abuse. The Ugandan Government and the President of the Ugandan Law Society condemned the incident, but the victim refused to press charges.\n\nTraditional circumcision is still practised in some tribal areas of Australia. Linguist and anthropologist Peter Sutton, commenting on forced circumcision and the absence of law enforcement in remote settlements, claims that Australian law has been applied in a patchy way: \"Involuntary circumcision has long been widely accepted as being de facto outside the scope of Australian law.\" Late in 1996, 34-year-old Irwin Brookdale was drinking with a group of Australian Aborigines on the banks of a river in far north Queensland. After he passed out, a woman in the group felt down his pants, found that he was not circumcised and called on her companions to \"make a man out of him.\" They attempted to circumcise him with a broken beer bottle. Brookdale ended up in hospital, one of his assailants was convicted of unlawful wounding and Brookdale was awarded A$10,000 compensation for nervous shock.\n\nThe breakup of Yugoslavia, according to Milica Z. Bookman, \"was extremely violent, producing some two million refugees, over 100,000 killed, and evidence of gang rape, impaling, dismemberment and forced circumcision.\"\n\nThe US Department of State reported that Muslim and Mujahedin irregular troops \"had routinely performed crude, disfiguring, nonmedical circumcisions on Bosnian Serb soldiers.\" One 18-year-old Bosnian Serb soldier \"was so brutally circumcised that eventually the entire organ required amputation.\"\n\n\n"}
{"id": "3284724", "url": "https://en.wikipedia.org/wiki?curid=3284724", "title": "Gargling", "text": "Gargling\n\nGargling (same root as 'gurgle') is the act of bubbling liquid in the mouth. Vibration caused by the muscles in the throat and back of the mouth cause the liquid to bubble and flurry around inside the mouth cavity.\n\nA traditional home remedy of gargling warm saltwater is sometimes recommended to soothe a sore throat. \n\nA study in Japan has shown that gargling water a few times a day will lower the chance of upper respiratory infections such as colds, though some medical authorities are skeptical.\n\n "}
{"id": "13152", "url": "https://en.wikipedia.org/wiki?curid=13152", "title": "Gluten", "text": "Gluten\n\nGluten (from Latin \"gluten\", \"glue\") is a composite of storage proteins termed prolamins and glutelins that is stored together with starch in the endosperm (which nourishes the embryonic plant during germination) of various cereal (grass) grains. It is found in wheat, barley, rye, [malt] and related species and hybrids (such as spelt, khorasan, emmer, einkorn, triticale, kamut, etc.), as well as products derived from these grains (such as breads and malts). Glutens, and most especially the Triticeae glutens, are appreciated for their viscoelastic properties, which give dough its elasticity, helping it rise and keep its shape and often leaving the final product with a chewy texture.\n\nWheat, barley, rye and oat prolamins are respectively known as gliadins, hordeins, secalins and avenins; these protein classes are often collectively referred to as gluten. Wheat glutelins are called glutenin. True gluten is limited to the grains listed above. The storage proteins in maize and rice are sometimes called glutens, but they differ from true gluten.\nIn a small part of the general human population, gluten can trigger adverse autoimmune reactions responsible for a broad spectrum of gluten-related disorders, including coeliac disease, non-coeliac gluten sensitivity, gluten ataxia and dermatitis herpetiformis. People with these conditions often practice gluten-free diets. The occurrence of oat avenin toxicity depends on the oat cultivar consumed, because the immunoreactivities of toxic prolamins are different among oat varieties. Also, many oat products are cross-contaminated with other gluten-containing cereals.\n\nGluten is a protein complex that accounts for 75–85% of the total protein in bread wheat. In home or restaurant cooking, gluten is prepared from flour by kneading the flour under water, agglomerating the gluten into an elastic network known as a dough, and then washing out the starch. Starch granules disperse in low-temperature water, and the dispersed starch is sedimented and dried. If a saline solution is used instead of water, a purer protein is obtained, with certain harmless impurities departing the solution with the starch. Where starch is the prime product, cold water is the favored solvent because the impurities depart from the gluten.\n\nIn industrial production, a slurry of wheat flour is kneaded vigorously by machinery until the gluten agglomerates into a mass. This mass is collected by centrifugation, then transported through several stages integrated in a continuous process. About 65% of the water in the wet gluten is removed by means of a screw press; the remainder is sprayed through an atomizer nozzle into a drying chamber, where it remains at an elevated temperature for a short time to allow the water to evaporate without denaturing the gluten. The process yields a flour-like powder with a 7% moisture content, which is air cooled and pneumatically transported to a receiving vessel. In the final step, the processed gluten is sifted and milled to produce a uniform product.\n\nGluten forms when glutenin molecules cross-link via disulfide bonds to form a submicroscopic network attached to gliadin, which contributes viscosity (thickness) and extensibility to the mix. If this dough is leavened with yeast, fermentation produces carbon dioxide bubbles, which, trapped by the gluten network, cause the dough to rise. Baking coagulates the gluten, which, along with starch, stabilizes the shape of the final product. Gluten content has been implicated as a factor in the staling of bread, possibly because it binds water through hydration.\n\nThe formation of gluten affects the texture of the baked goods. Gluten's attainable elasticity is proportional to its content of glutenins with low molecular weights, as this portion contains the preponderance of the sulfur atoms responsible for the cross-linking in the gluten network.\nFurther refining of the gluten leads to chewier doughs such as those found in pizza and bagels, while less refining yields tender baked goods such as pastry products.\n\nGenerally, bread flours are high in gluten (hard wheat); pastry flours have a lower gluten content. Kneading promotes the formation of gluten strands and cross-links, creating baked products that are chewier (as opposed to more brittle or crumbly). The \"chewiness\" increases as the dough is kneaded for longer times. An increased moisture content in the dough enhances gluten development, and very wet doughs left to rise for a long time require no kneading (see no-knead bread). Shortening inhibits formation of cross-links and is used, along with diminished water and less kneading, when a tender and flaky product, such as a pie crust, is desired.\n\nThe strength and elasticity of gluten in flour is measured in the baking industry using a farinograph. This gives the baker a measurement of quality for different varieties of flours when developing recipes for various baked goods.\n\nGluten, when dried, milled and added to ordinary flour dough, may help improve the dough's ability to increase in volume. The resulting mixture also increases the bread's structural stability and chewiness. Gluten-added dough must be worked vigorously to induce it to rise to its full capacity; an automatic bread machine or food processor may be required for high-gluten kneading. Generally, higher gluten levels are associated with higher overall protein content.\n\nGluten, especially wheat gluten, is often the basis for imitation meats resembling beef, chicken, duck (see mock duck), fish and pork. When cooked in broth, gluten absorbs some of the surrounding liquid (including the flavor) and becomes firm to the bite. This use of gluten is a popular means of adding supplemental protein and variety to many vegetarian diets.\n\nGluten is often present in beer and soy sauce, and can be used as a stabilizing agent in more unexpected food products, such as ice cream and ketchup. Foods of this kind may therefore present problems for a small number of consumers because the hidden gluten constitutes a hazard for people with celiac disease and gluten sensitivities. The protein content of some pet foods may also be enhanced by adding gluten.\n\nGluten is also used in cosmetics, hair products and other dermatological preparations. \n\n\"Gluten-related disorders\" is the umbrella term for all diseases triggered by gluten, which include celiac disease (CD), non-celiac gluten sensitivity (NCGS), wheat allergy, gluten ataxia and dermatitis herpetiformis (DH). Currently, their incidence is increasing in most geographic areas of the world. This can possibly be explained by the growing westernization of diets, the increasing use of wheat-based foods included in the Mediterranean diet, the progressive replacement of rice by wheat in many countries in Asia, the Middle East, and North Africa, the development in recent years of new types of wheat with a higher amount of cytotoxic gluten peptides, and the higher content of gluten in bread and bakery products due to the reduction of dough fermentation time.\n\nCeliac disease (CD) is a chronic, multiple-organ autoimmune disorder primarily affecting the small intestine caused by the ingestion of wheat, barley, rye, oats, and derivatives, that appears in genetically predisposed people of all ages. CD is not only a gastrointestinal disease, because it may involve several organs and cause an extensive variety of non-gastrointestinal symptoms, and most importantly, it may be apparently asymptomatic. Many asymptomatic people actually are not, but have become accustomed to living with a chronic bad health status as if it were normal, and they are able to recognize that they actually had symptoms related to celiac disease after starting the gluten-free diet and improvement is evident, in contrast to the situation prior to the diet. Added difficulties for diagnosis are the fact that serological markers (anti-tissue transglutaminase [TG2]) are not always present and many people may have minor mucosal lesions, without atrophy of the intestinal villi.\n\nCD affects approximately 1–2% of the general population, but most cases remain unrecognized, undiagnosed and untreated, and at risk for serious long-term health complications. People may suffer severe disease symptoms and be subjected to extensive investigations for many years, before a proper diagnosis is achieved. Untreated CD may cause malabsorption, reduced quality of life, iron deficiency, osteoporosis, an increased risk of intestinal lymphomas, and greater mortality. CD is associated with some other autoimmune diseases, such as diabetes mellitus type 1, thyroiditis, gluten ataxia, psoriasis, vitiligo, autoimmune hepatitis, dermatitis herpetiformis, primary sclerosing cholangitis, and more.\n\nCD with \"classic symptoms\", which include gastrointestinal manifestations such as chronic diarrhea and abdominal distention, malabsorption, loss of appetite, and impaired growth, is currently the least common presentation form of the disease and affects predominantly small children generally younger than two years of age.\n\nCD with \"non-classic symptoms\" is the most common clinical type and occurs in older children (over 2 years old), adolescents, and adults. It is characterized by milder or even absent gastrointestinal symptoms and a wide spectrum of non-intestinal manifestations that can involve any organ of the body, and very frequently may be completely asymptomatic both in children (at least in 43% of the cases) and adults.\n\nNon-celiac gluten sensitivity (NCGS) is described as a condition of multiple symptoms that improves when switching to a gluten-free diet, after celiac disease and wheat allergy are excluded. Recognized since 2010, it is included among gluten-related disorders, but its pathogenesis is not yet well understood. NCGS is the most common syndrome of gluten intolerance, with a prevalence estimated to be 6-10%. NCGS is becoming a more common diagnosis, but its true prevalence is difficult to determine because many people self-diagnose and start the gluten-free diet, without having previously tested for celiac disease or doctor's recommendations. People with NCGS remain habitually in a \"no man's land\", without being recognized by the specialists and lacking the adequate medical care and treatment. Most of these people have a long history of health complaints and unsuccessful consultations with numerous physicians, and this is the reason why the majority of them end up resorting to a gluten-free diet and a self-diagnosis of gluten sensitivity.\n\nPeople with NCGS may develop gastrointestinal symptoms, which resemble those of irritable bowel syndrome or wheat allergy, or a wide variety of non-gastrointestinal symptoms, such as headache, chronic fatigue, fibromyalgia, atopic diseases, allergies, neurological diseases, or psychiatric disorders, among others.\n\nBesides gluten, additional components present in wheat, rye, barley, oats, and their derivatives, including other proteins called ATIs and short-chain carbohydrates known as FODMAPs, may cause NCGS symptoms. The effects of FODMAPs are only limited to gastrointestinal mild discomfort. ATIs may cause toll-like receptor 4 (TLR4)-mediated intestinal inflammation in humans.\n\nPeople can also experience adverse effects of wheat as result of a wheat allergy. As with most allergies, a wheat allergy causes the immune system to abnormally respond to a component of wheat that it treats as a threatening foreign body. This immune response is often time-limited and does not cause lasting harm to body tissues. Wheat allergy and celiac disease are different disorders. Gastrointestinal symptoms of wheat allergy are similar to those of celiac disease and non-celiac gluten sensitivity, but there is a different interval between exposure to wheat and onset of symptoms. An allergic reaction to wheat has a fast onset (from minutes to hours) after the consumption of food containing wheat and could include anaphylaxis.\n\nGluten ataxia is an autoimmune disease triggered by the ingestion of gluten. With gluten ataxia, damage takes place in the cerebellum, the balance center of the brain that controls coordination and complex movements like walking, speaking and swallowing, with loss of Purkinje cells. People with gluten ataxia usually present gait abnormality or incoordination and tremor of the upper limbs. Gaze-evoked nystagmus and other ocular signs of cerebellar dysfunction are common. Myoclonus, palatal tremor, and opsoclonus-myoclonus may also appear.\n\nEarly diagnosis and treatment with a gluten-free diet can improve ataxia and prevent its progression. The effectiveness of the treatment depends on the elapsed time from the onset of the ataxia until diagnosis, because the death of neurons in the cerebellum as a result of gluten exposure is irreversible.\n\nGluten ataxia accounts for 40% of ataxias of unknown origin and 15% of all ataxias. Less than 10% of people with gluten ataxia present any gastrointestinal symptom, yet about 40% have intestinal damage.\n\nThe \"Codex Alimentarius\" international standards for food labeling has a standard relating to the labeling of products as \"gluten-free\". It only applies to foods that would normally contain gluten.\n\nBy law in Brazil, all food products must display labels clearly indicating whether or not they contain gluten.\n\nThe Canadian Celiac Association estimates that one in 133 Canadians experiences adverse symptoms from gluten in celiac disease. Labels for all food products sold in Canada must clearly identify the presence of gluten if it is present at a level greater than 20 parts per million.\n\nIn the European Union, all prepackaged foods and non-prepacked foods from a restaurant, take-out food wrapped just before sale, or unpackaged food served in institutions must be identified if gluten-free. \"Gluten-free\" is defined as 20 parts per million of gluten or less and \"very low gluten\" is 100 parts per million of gluten or less; only foods with cereal ingredients processed to remove gluten can claim \"very low gluten\" on labels.\n\nAll foods containing gluten as an ingredient must be labelled accordingly as gluten is defined as one of the 14 recognised EU allergens.\n\nIn the United States, gluten is not listed on labels unless added as a standalone ingredient. Wheat or other allergens are listed after the ingredient line. The US Food and Drug Administration (FDA) has historically classified gluten as \"generally recognized as safe\" (GRAS). In August 2013, the FDA issued a final ruling, effective August 2014, that defined the term \"gluten-free\" for voluntary use in the labeling of foods as meaning that the amount of gluten contained in the food is below 20 parts per million.\n\n\n"}
{"id": "53050024", "url": "https://en.wikipedia.org/wiki?curid=53050024", "title": "Granny dumping", "text": "Granny dumping\n\nGranny dumping (informal) is a term that was introduced in the early 1980s by professionals in the medical and social work fields. Granny dumping is defined by the \"Oxford English Dictionary\" as \"the abandonment of an elderly person in a public place such as a hospital or nursing home, especially by a relative\". It may be carried out by family members who are unable or unwilling to continue providing care due to financial problems, burnout, lack of resources (such as home health or assisted living options), or stress.\n\nThe phenomenon is not new. A practice, known as \"ubasute\", had allegedly existed in Japan centuries ago when senile elders were brought to mountaintops by poor citizens who were unable to look after them. The widespread economic and demographic problems facing Japan have seen it on the rise with relatives dropping off seniors at hospitals or charities. 70,000 (both male and female equally) elderly Americans were estimated to have been abandoned in 1992 in a report issued by the American College of Emergency Physicians. In this same study, ACEP received informal surveys from 169 hospital Emergency Departments and report an average of 8 \"granny dumping\" abandonments per week. According to the New York Times, 1 in 5 people are now caring for an elderly parent and people are spending more time than ever caring for an elderly parent than their own children. Social workers have said that this may be the result of millions of people who are near the breaking point of looking after their elderly parents who are in poor health.\n\nIn the US, granny dumping is more likely to happen in states such as Florida, Texas and California where there are large populations of retirement communities. Congress has attempted to step in by mandating to emergency departments requiring them to see all patients. However, Medicaid is covering less and less of medical bills through reimbursement (in 1989 it was 78% but that number is decreasing) and reduced eligibility. In some cases, the hospitals may not want to take the risk of having a patient who cannot pay so they will attempt to transfer their care to another hospital. According to the Consolidated Omnibus Budget Reconciliation Act of 1985 set into place by Ronald Reagan, a hospital can transfer at the patient's request or providers must sign a document providing why they believe a patient's care should be better served at another facility. With 40% of revenue coming from Medicaid and Medicare a hospital must earn 8 cents per dollar to compensate for the loss of 7 cents per Medicaid/Medicare patients. Hospitals had to pay an additional 2 billion dollars to private payers to cover costs for Medicare/Medicaid patients in 1989.\n\nIncidents of granny dumping can happen before long weekends and may peak before Christmas when families head off on holidays. Caregivers in both Australia and New Zealand report that old people without acute medical problems are dropped off at hospitals. As a result, hospitals and care facilities have to carry an extra burden on their limited resources.\n"}
{"id": "49604", "url": "https://en.wikipedia.org/wiki?curid=49604", "title": "Hearing loss", "text": "Hearing loss\n\nHearing loss, also known as hearing impairment, is a partial or total inability to hear. A deaf person has little to no hearing. Hearing loss may occur in one or both ears. In children, hearing problems can affect the ability to learn spoken language and in adults it can create difficulties with social interaction and at work. In some people, particularly older people, hearing loss can result in loneliness. Hearing loss can be temporary or permanent.\nHearing loss may be caused by a number of factors, including: genetics, ageing, exposure to noise, some infections, birth complications, trauma to the ear, and certain medications or toxins. A common condition that results in hearing loss is chronic ear infections. Certain infections during pregnancy, such as syphilis and rubella, may also cause hearing loss in the child. Hearing loss is diagnosed when hearing testing finds that a person is unable to hear 25 decibels in at least one ear. Testing for poor hearing is recommended for all newborns. Hearing loss can be categorized as mild (25 to 40 dB), moderate (41 to 55 dB), moderate-severe (56 to 70 dB), severe (71 to 90 dB), or profound (greater than 90 dB). There are three main types of hearing loss: conductive hearing loss, sensorineural hearing loss, and mixed hearing loss.\nAbout half of hearing loss globally is preventable through public health measures. Such practices include immunization, proper care around pregnancy, avoiding loud noise, and avoiding certain medications. The World Health Organization recommends that young people limit the use of personal audio players to an hour a day in an effort to limit exposure to noise. Early identification and support are particularly important in children. For many hearing aids, sign language, cochlear implants and subtitles are useful. Lip reading is another useful skill some develop. Access to hearing aids, however, is limited in many areas of the world.\nAs of 2013 hearing loss affects about 1.1 billion people to some degree. It causes disability in 5% (360 to 538 million) and moderate to severe disability in 124 million people. Of those with moderate to severe disability 108 million live in low and middle income countries. Of those with hearing loss it began in 65 million during childhood. Those who use sign language and are members of Deaf culture see themselves as having a difference rather than an illness. Most members of Deaf culture oppose attempts to cure deafness and some within this community view cochlear implants with concern as they have the potential to eliminate their culture. The term hearing impairment is often viewed negatively as it emphasises what people cannot do.\n\nUse of the terms \"hearing impaired\", \"deaf-mute\", or \"deaf and dumb\" to describe deaf and hard of hearing people is discouraged by advocacy organizations as they are offensive to many deaf and hard of hearing people.\n\nHuman hearing extends in frequency from 20–20,000 Hz, and in amplitude from 0 dB to 130 dB or more. 0 dB does not represent absence of sound, but rather the softest sound an average unimpaired human ear can hear; some people can hear down to −5 or even −10 dB. 130 dB represents the threshold of pain. But the ear does not hear all frequencies equally well; hearing sensitivity peaks around 3000 Hz. There are many qualities of human hearing besides frequency range and intensity that can't easily be measured quantitatively. But for many practical purposes, normal hearing is defined by a frequency versus intensity graph, or audiogram, charting sensitivity thresholds of hearing at defined frequencies. Because of the cumulative impact of age and exposure to noise and other acoustic insults, 'typical' hearing may not be normal.\n\n\nHearing loss is sensory, but may have accompanying symptoms:\n\nThere may also be accompanying secondary symptoms:\n\nHearing loss has multiple causes, including ageing, genetics, perinatal problems and acquired causes like noise and disease. For some kinds of hearing loss the cause may be classified as of unknown cause.\n\nThere is a progressive loss of ability to hear high frequencies with aging known as presbycusis. For men, this can start as early as 25 and women at 30. Although genetically variable it is a normal concomitant of ageing and is distinct from hearing losses caused by noise exposure, toxins or disease agents. Common conditions that can increase the risk of hearing loss in elderly people are high blood pressure, diabetes or the use of certain medications harmful to the ear. While everyone loses hearing with age, the amount and type of hearing loss is variable.\n\nNoise exposure is the cause of approximately half of all cases of hearing loss, causing some degree of problems in 5% of the population globally.\nThe National Institute for Occupational Safety and Health (NIOSH) recognizes that the majority of hearing loss is not due to age, but due to noise exposure. By correcting for age in assessing hearing, one tends to overestimate the hearing loss due to noise for some and underestimate it for others.\n\nHearing loss due to noise may be temporary, called a 'temporary threshold shift', a reduced sensitivity to sound over a wide frequency range resulting from exposure to a brief but very loud noise like a gunshot, firecracker, jet engine, jackhammer, etc. or to exposure to loud sound over a few hours such as during a pop concert or nightclub session. Recovery of hearing is usually within 24 hours, but may take up to a week. Both constant exposure to loud sounds (85 dB(A) or above) and one-time exposure to extremely loud sounds (120 dB(A) or above) may cause permanent hearing loss.\n\nNoise-induced hearing loss (NIHL) typically manifests as elevated hearing thresholds (i.e. less sensitivity or muting) between 3000 and 6000  Hz, centred at 4000  Hz. As noise damage progresses, damage spreads to affect lower and higher frequencies. On an audiogram, the resulting configuration has a distinctive notch, called a 'noise' notch. As ageing and other effects contribute to higher frequency loss (6–8 kHz on an audiogram), this notch may be obscured and entirely disappear.\n\nVarious governmental, industry and standards organizations set noise standards.\n\nThe U.S. Environmental Protection Agency has identified the level of 70 dB(A) (40% louder to twice as loud as normal conversation; typical level of TV, radio, stereo; city street noise) for 24‑hour exposure as the level necessary to protect the public from hearing loss and other disruptive effects from noise, such as sleep disturbance, stress-related problems, learning detriment, etc. Noise levels are typically in the 65 to 75 dB (A) range for those living near airports of freeways and may result in hearing damage if sufficient time is spent outdoors.\n\nLouder sounds cause damage in a shorter period of time. Estimation of a \"safe\" duration of exposure is possible using an \"exchange rate\" of 3 dB. As 3 dB represents a doubling of the intensity of sound, duration of exposure must be cut in half to maintain the same energy dose. For workplace noise regulation, the \"safe\" daily exposure amount at 85 dB A, known as an exposure action value, is 8 hours, while the \"safe\" exposure at 91 dB(A) is only 2 hours. \nDifferent standards use exposure action values between 80dBA and 90dBA. Note that for some people, sound may be damaging at even lower levels than 85 dB A. Exposures to other ototoxins (such as pesticides, some medications including chemotherapy agents, solvents, etc.) can lead to greater susceptibility to noise damage, as well as causing its own damage. This is called a \"synergistic\" interaction. Since noise damage is cumulative over long periods of time, persons who are exposed to non-workplace noise, like recreational activities or environmental noise, may have compounding damage from all sources.\n\nSome national and international organizations and agencies use an exchange rate of 4 dB or 5 dB. While these exchange rates may indicate a wider zone of comfort or safety, they can significantly underestimate the damage caused by loud noise. For example, at 100 dB (nightclub music level), a 3 dB exchange rate would limit exposure to 15 minutes; the 5 dB exchange rate allows an hour.\n\nMany people are unaware of the presence of environmental sound at damaging levels, or of the level at which sound becomes harmful. Common sources of damaging noise levels include car stereos, children's toys, motor vehicles, crowds, lawn and maintenance equipment, power tools, gun use, musical instruments, and even hair dryers. Noise damage is cumulative; all sources of damage must be considered to assess risk. If one is exposed to loud sound (including music) at high levels or for extended durations (85 dB A or greater), then hearing loss will occur. Sound intensity (sound energy, or propensity to cause damage to the ears) increases dramatically with proximity according to an inverse square law: halving the distance to the sound quadruples the sound intensity.\n\nIn the USA, 12.5% of children aged 6–19 years have permanent hearing damage from excessive noise exposure. The World Health Organization estimates that half of those between 12 and 35 are at risk from using personal audio devices that are too loud.\n\nHearing loss due to noise has been described as primarily a condition of modern society. In preindustrial times, humans had far less exposure to loud sounds. Studies of primitive peoples indicate that much of what has been attributed to age-related hearing loss may be long term cumulative damage from all sources, especially noise. People living in preindustrial societies have considerably less hearing loss than similar populations living in modern society. Among primitive people who have migrated into modern society, hearing loss is proportional to the number of years spent in modern society. Military service in World War II, the Korean War, and the Vietnam War, has likely also caused hearing loss in large numbers of men from those generations, though proving that hearing loss was a direct result of military service is problematic without entry and exit audiograms.\n\nHearing loss in adolescents may be caused by loud noise from toys, music by headphones, and concerts or events. In 2017, the Centers for Disease Control and Prevention brought their researchers together with experts from the World Health Organization and academia to examine the risk of hearing loss from excessive noise exposure in and outside the workplace in different age groups, as well as actions being taken to reduce the burden of the condition. A summary report was published in 2018.\n\nHearing loss can be inherited. Around 75–80% of all these cases are inherited by recessive genes, 20–25% are inherited by dominant genes, 1–2% are inherited by X-linked patterns, and fewer than 1% are inherited by mitochondrial inheritance.\n\nWhen looking at the genetics of deafness, there are 2 different forms, syndromic and nonsyndromic. Syndromic deafness occurs when there are other signs or medical problems aside from deafness in an individual. This accounts for around 30% of deaf individuals who are deaf from a genetic standpoint. Nonsyndromic deafness occurs when there are no other signs or medical problems associated with an individual other than deafness. From a genetic standpoint, this accounts for the other 70% of cases, and represents the majority of hereditary hearing loss. Syndromic cases occur with diseases such as Usher syndrome, Stickler syndrome, Waardenburg syndrome, Alport's syndrome, and neurofibromatosis type 2. These are diseases that have deafness as one of the symptoms or as a common feature associated with it. Many of the genetic mutations giving rise to syndromic deafness have been identified. In nonsyndromic cases, where deafness is the only finding, it is more difficult to identify the genetic mutation although some have been discovered.\n\n\n\n\nSome medications may reversibly affect hearing. These medications are considered ototoxic. This includes loop diuretics such as furosemide and bumetanide, non-steroidal anti-inflammatory drugs (NSAIDs) both over-the-counter (aspirin, ibuprofen, naproxen) as well as prescription (celecoxib, diclofenac, etc.), paracetamol, quinine, and macrolide antibiotics. The link between NSAIDs and hearing loss tends to be greater in women, especially those who take ibuprofen six or more times a week. Others may cause permanent hearing loss. The most important group is the aminoglycosides (main member gentamicin) and platinum based chemotherapeutics such as cisplatin and carboplatin.\n\nOn October 18, 2007, the U.S. Food and Drug Administration (FDA) announced that a warning about possible sudden hearing loss would be added to drug labels of PDE5 inhibitors, which are used for erectile dysfunction.\n\nAudiologic monitoring for ototoxicity allows for the (1) early detection of changes to hearing status presumably attributed to a drug/treatment regime so that changes in the drug regimen may be considered, and (2) audiologic intervention when handicapping hearing impairment has occurred.\n\nCo-administration of anti-oxidants and ototoxic medications may limit the extent of the ototoxic damage\n\nIn addition to medications, hearing loss can also result from specific chemicals in the environment: metals, such as lead; solvents, such as toluene (found in crude oil, gasoline and automobile exhaust, for example); and asphyxiants. Combined with noise, these ototoxic chemicals have an additive effect on a person’s hearing loss.\n\nHearing loss due to chemicals starts in the high frequency range and is irreversible. It damages the cochlea with lesions and degrades central portions of the auditory system. For some ototoxic chemical exposures, particularly styrene, the risk of hearing loss can be higher than being exposed to noise alone. The effects is greatest when the combined exposure include impulse noise. \nA 2018 informational bulletin by the US Occupational Safety and Health Administration (OSHA) and the National Institute for Occupational Safety and Health (NIOSH) introduces the issue, provides examples of ototoxic chemicals, lists the industries and occupations at risk and provides prevention information.\n\nThere can be damage either to the ear itself or to the brain centers that process the aural information conveyed by the ears. People who sustain head injury are especially vulnerable to hearing loss or tinnitus, either temporary or permanent.\n\nSound waves reach the outer ear and are conducted down the ear canal to the eardrum, causing it to vibrate. The vibrations are transferred by the 3 tiny ear bones of the middle ear to the fluid in the inner ear. The fluid moves hair cells (stereocilia), and their movement generates nerve impulses which are then taken to the brain by the cochlear nerve. The auditory nerve takes the impulses to the brainstem, which sends the impulses to the midbrain. Finally, the signal goes to the auditory cortex of the temporal lobe to be interpreted as sound.\n\nHearing loss is most commonly caused by long-term exposure to loud noises, from recreation or from work, that damage the hair cells, which do not grow back on their own.\n\nOlder people may lose their hearing from long exposure to noise, changes in the inner ear, changes in the middle ear, or from changes along the nerves from the ear to the brain.\n\nIdentification of a hearing loss is usually conducted by a general practitioner medical doctor, otolaryngologist, certified and licensed audiologist, school or industrial audiometrist, or other audiometric technician. Diagnosis of the cause of a hearing loss is carried out by a specialist physician (audiovestibular physician) or otorhinolaryngologist.\n\nA case history (usually a written form, with questionnaire) can provide valuable information about the context of the hearing loss, and indicate what kind of diagnostic procedures to employ. Case history will include such items as:\n\n\nIn case of infection or inflammation, blood or other body fluids may be submitted for laboratory analysis.\n\nHearing loss is generally measured by playing generated or recorded sounds, and determining whether the person can hear them. Hearing sensitivity varies according to the frequency of sounds. To take this into account, hearing sensitivity can be measured for a range of frequencies and plotted on an audiogram.\n\nAnother method for quantifying hearing loss is a speech-in-noise test. As the name implies, a speech-in-noise test gives an indication of how well one can understand speech in a noisy environment. A person with a hearing loss will often be less able to understand speech, especially in noisy conditions. This is especially true for people who have a sensorineural loss – which is by far the most common type of hearing loss. As such, speech-in-noise tests can provide valuable information about a person's hearing ability, and can be used to detect the presence of a sensorineural hearing loss. A recently developed digit-triple speech-in-noise test may be a more efficient screening test.\n\nOtoacoustic emissions test is an objective hearing test that may be administered to toddlers and children too young to cooperate in a conventional hearing test. The test is also useful in older children and adults and is an important measure in diagnosing auditory neuropathy described above.\n\nAuditory brainstem response testing is an electrophysiological test used to test for hearing deficits caused by pathology within the ear, the cochlear nerve and also within the brainstem. This test can be used to identify delay in the conduction of neural impulses due to tumours or inflammation but can also be an objective test of hearing thresholds. Other electrophysiological tests, such as cortical evoked responses, can look at the hearing pathway up to the level of the auditory cortex.\n\nMRI and CT scans can be useful to identify the pathology of many causes of hearing loss. They are only needed in selected cases.\n\nHearing loss is categorized by type, severity, and configuration. Furthermore, a hearing loss may exist in only one ear (unilateral) or in both ears (bilateral). Hearing loss can be temporary or permanent, sudden or progressive.\n\nThe severity of a hearing loss is ranked according to ranges of nominal thresholds in which a sound must be so it can be detected by an individual. It is measured in decibels of hearing loss, or dB HL. The measurement of hearing loss in an individual is conducted over several frequencies, mostly 500 Hz, 1000 Hz, 2000 Hz and 4000 Hz. The hearing loss of the individual is the average of the hearing loss values over the different frequencies. Hearing loss can be ranked differently according to different organisations; and so, in different countries different systems are in use.\n\nHearing loss may be ranked as slight, mild, moderate, moderately severe, severe or profound as defined below:\n\nThe 'Audiometric Classifications of Hearing Impairment' according to the International Bureau Audiophonology (BIAP) in Belgium is as follows:\n\nHearing loss may affect one or both ears. If both ears are affected, then one ear may be more affected than the other. Thus it is possible, for example, to have normal hearing in one ear and none at all in the other, or to have mild hearing loss in one ear and moderate hearing loss in the other.\n\nFor certain legal purposes such as insurance claims, hearing loss is described in terms of percentages. Given that hearing loss can vary by frequency and that audiograms are plotted with a logarithmic scale, the idea of a percentage of hearing loss is somewhat arbitrary, but where decibels of loss are converted via a legally recognized formula, it is possible to calculate a standardized \"percentage of hearing loss\", which is suitable for legal purposes only.\n\nThere are four main types of hearing loss, conductive hearing loss, sensorineural hearing loss, central deafness and combinations of conductive and sensorineural hearing losses which is called mixed hearing loss. An additional problem which is increasingly recognised is auditory processing disorder which is not a hearing loss as such but a difficulty perceiving sound.\n\nConductive hearing loss is present when the sound is not reaching the inner ear, the cochlea. This can be due to external ear canal malformation, dysfunction of the eardrum or malfunction of the bones of the middle ear. The eardrum may show defects from small to total resulting in hearing loss of different degree. Scar tissue after ear infections may also make the eardrum dysfunction as well as when it is retracted and adherent to the medial part of the middle ear.\n\nDysfunction of the three small bones of the middle ear – malleus, incus, and stapes – may cause conductive hearing loss. The mobility of the ossicles may be impaired for different reasons including a boney disorder of the ossicles called otosclerosis and disruption of the ossicular chain due to trauma, infection or ankylosis may also cause hearing loss.\n\nSensorineural hearing loss is one caused by dysfunction of the inner ear, the cochlea or the nerve that transmits the impulses from the cochlea to the hearing centre in the brain. The most common reason for sensorineural hearing loss is damage to the hair cells in the cochlea. Depending on the definition it could be estimated that more than 50% of the population over the age of 70 has impaired hearing.\n\nDamage to the brain can lead to a central deafness. The peripheral ear and the auditory nerve may function well but the central connections are damaged by tumour, trauma or other disease and the patient is unable to process speech information.\n\nMixed hearing loss is a combination of conductive and sensorineural hearing loss. Chronic ear infection (a fairly common diagnosis) can cause a defective ear drum or middle-ear ossicle damages, or both. In addition to the conductive loss, a sensory component may be present.\n\n\nThis is not an actual hearing loss but gives rise to significant difficulties in hearing. One kind of auditory processing disorder is King-Kopetzky syndrome, which is characterized by an inability to process out background noise in noisy environments despite normal performance on traditional hearing tests. An auditory processing disorders is sometimes linked to language disorders in persons of all ages.\n\nThe shape of an audiogram shows the relative configuration of the hearing loss, such as a Carhart notch for otosclerosis, 'noise' notch for noise-induced damage, high frequency rolloff for presbycusis, or a flat audiogram for conductive hearing loss. In conjunction with speech audiometry, it may indicate central auditory processing disorder, or the presence of a schwannoma or other tumor.\nThere are four general configurations of hearing loss:\n\n1. Flat: thresholds essentially equal across test frequencies.\n\n2. Sloping: lower (better) thresholds in low-frequency regions and higher (poorer) thresholds in high-frequency regions.\n\n3. Rising: higher (poorer) thresholds in low-frequency regions and lower (better) thresholds in higher-frequency regions.\n\n4. Trough-shaped (\"cookie-bite\" or \"U\" shaped): greatest hearing loss in the mid-frequency range, with lower (better) thresholds in low- and high-frequency regions.\n\nPeople with unilateral hearing loss or single-sided deafness (SSD) have difficulty in:\n\n\nIn quiet conditions, speech discrimination is approximately the same for normal hearing and those with unilateral deafness; however, in noisy environments speech discrimination varies individually and ranges from mild to severe.\n\nOne reason for the hearing problems these patients often experience is due to the head shadow effect. Newborn children with no hearing on one side but one normal ear could still have problems. Speech development could be delayed and difficulties to concentrate in school are common. More children with unilateral hearing loss have to repeat classes than their peers. Taking part in social activities could be a problem. Early aiding is therefore of utmost importance.\n\nIt is estimated that half of cases of hearing loss are preventable. About 60% of hearing loss in children under the age of 15 can be avoided. A number of preventative strategies are effective including: immunization against rubella to prevent congenital rubella syndrome, immunization against \"H. influenza\" and \"S. pneumoniae\" to reduce cases of meningitis, and avoiding or protecting against excessive noise exposure. The World Health Organization also recommends immunization against measles, mumps, and meningitis, efforts to prevent premature birth, and avoidance of certain medication as prevention.\n\nNoise exposure is the most significant risk factor for noise-induced hearing loss that can be prevented. Different programs exist for specific populations such as school-age children, adolescents and workers. Education regarding noise exposure increases the use of hearing protectors. The use of antioxidants is being studied for the prevention of noise-induced hearing loss, particularly for scenarios in which noise exposure cannot be reduced, such as during military operations.\n\nNoise is widely recognized as an occupational hazard. In the United States, the National Institute for Occupational Safety and Health (NIOSH) and the Occupational Safety and Health Administration (OSHA) work together to provide standards and enforcement on workplace noise levels. The hierarchy of hazard controls demonstrates the different levels of controls to reduce or eliminate exposure to noise and prevent hearing loss, including engineering controls and personal protective equipment (PPE). Other programs and initiative have been created to prevent hearing loss in the workplace. For example, the Safe-in-Sound Award was created to recognize organizations that can demonstrate results of successful noise control and other interventions. Additionally, the Buy Quiet program was created to encourage employers to purchase quieter machinery and tools. By purchasing less noisy power tools like those found on the NIOSH Power Tools Database and limiting exposure to ototoxic chemicals, great strides can be made in preventing hearing loss.\n\nCompanies can also provide personal hearing protector devices tailored to both the worker and type of employment. Some hearing protectors universally block out all noise, and some allow for certain noises to be heard. Workers are more likely to wear hearing protector devices when they are properly fitted.\n\nOften interventions to prevent noise-induced hearing loss have many components. A 2017 Cochrane review found that stricter legislation might reduce noise levels. Providing workers with information on their noise exposure levels was not shown to decrease exposure to noise. Ear protection, if used correctly, can reduce noise to safer levels, but often, providing them is not sufficient to prevent hearing loss. Engineering noise out and other solutions such as proper maintenance of equipment can lead to noise reduction, but further field studies on resulting noise exposures following such interventions are needed. Other possible solutions include improved enforcement of existing legislation and better implementation of well-designed prevention programmes, which have not yet been proven conclusively to be effective. The conclusion of the Cochrane Review was that further research could modify what is now regarding the effectiveness of the evaluated interventions.\n\nThe United States Preventive Services Task Force recommends screening for all newborns.\n\nThe American Academy of Pediatrics advises that children should have their hearing tested several times throughout their schooling:\nWhile the American College of Physicians indicated that there is not enough evidence to determine the utility of screening in adults over 50 years old who do not have any symptoms, the American Language, Speech Pathology and Hearing Association recommends that adults should be screened at least every decade through age 50 and at 3-year intervals thereafter, to minimize the detrimental effects of the untreated condition on quality of life. For the same reason, the US Office of Disease Prevention and Health Promotion included as one of Healthy People 2020 objectives: to increase the proportion of persons who have had a hearing examination.\n\nTreatment depends on the specific cause if known as well as the extent, type and configuration of the hearing loss. Most hearing loss, that resulting from age and noise, is progressive and irreversible, and there are currently no approved or recommended treatments; management is by hearing aid. A few specific kinds of hearing loss are amenable to surgical treatment. In other cases, treatment is addressed to underlying pathologies, but any hearing loss incurred may be permanent.\n\nThere are a number of devices that can improve hearing in those who are deaf or hard of hearing or allow people with these conditions to manage better in their lives.\n\nHearing aids are devices that work to improve the hearing and speech comprehension of those with hearing loss. They work by magnifying the sound vibrations in the ear so that one can understand what is being said around them. Hearing aids have been shown to have a large beneficial effect in helping adults with mild to moderate hearing loss take part in everyday situations, and a smaller beneficial effect in improving physical, social, emotional and mental well-being in these people. Some people feel as if they cannot live without one because they say it is the only thing that keeps them engaged with the public. Conversely, there are many people who choose not to wear their hearing aids for a multitude of reasons. Up to 40% of adults with hearing aids for hearing loss fail to use them, or do not use them to their full effect. There are a number of reasons for this, stemming from factors such as: the aid amplifying background noises instead of the sounds they intended to hear; issues with comfort, care, or maintenance of the device; aesthetic factors; financial factors; and personal preference for quietness.\n\nThere is little evidence that interventions to encourage the regular use of hearing aids, (e.g. improving the information given to people about how to use hearing aids), increase daily hours of hearing aid use, and there is currently no agreed set of outcome measures for assessing this type of intervention.\n\nMany deaf and hard of hearing individuals use assistive devices in their daily lives:\n\nA wireless device has two main components: a transmitter and a receiver. The transmitter broadcasts the captured sound, and the receiver detects the broadcast audio and enables the incoming audio stream to be connected to accommodations such as hearing aids or captioning systems.\n\nThree types of wireless systems are commonly used: FM, audio induction loop, and InfraRed. Each system has advantages and benefits for particular uses. FM systems can be battery operated or plugged into an electrical outlet. FM system produce an analog audio signal, meaning they have extremely high fidelity. Many FM systems are very small in size, allowing them to be used in mobile situations. The audio induction loop permits the listener with hearing loss to be free of wearing a receiver provided that the listener has a hearing aid or cochlear implant processor with an accessory called a \"telecoil\". If the listener does not have a telecoil, then he or she must carry a receiver with an earpiece. As with FM systems, the infrared (IR) system also requires a receiver to be worn or carried by the listener. An advantage of IR wireless systems is that people in adjoining rooms cannot listen in on conversations, making it useful for situations where privacy and confidentiality are required. Another way to achieve confidentiality is to use a hardwired amplifier, which contains or is connected to a microphone and transmits no signal beyond the earpiece plugged directly into it.\n\nThere is no treatment, surgical or otherwise, for hearing loss due to the most common causes (age, noise, and genetic defects). For a few specific conditions, surgical intervention can provide a remedy:\n\nSurgical and implantable hearing aids are an alternative to conventional external hearing aids. \nIf the ear is dry and not infected, an air conduction aid could be tried; if the ear is draining, a direct bone condition hearing aid is often the best solution. If the conductive part of the hearing loss is more than 30–35 dB, an air conduction device could have problems overcoming this gap. A bone-anchored hearing aid could, in this situation, be a good option.\nThe active bone conduction hearing implant Bonebridge is also an option. This implant is invisible under the intact skin and therefore minimises the risk of skin irritations.\n\nCochlear implants improve outcomes in people with hearing loss in either one or both ears. They work by artificial stimulation of the cochlear nerve by providing an electric impulse substitution for the firing of hair cells. They are expensive, and require programming along with extensive training for effectiveness.\n\nCochlear implants as well as bone conduction implants can help with single sided deafness.\nMiddle ear implants or bone conduction implants can help with conductive hearing loss.\n\nPeople with cochlear implants are at a higher risk for bacterial meningitis. Thus, meningitis vaccination is recommended. People who have hearing loss, especially those who develop a hearing problem in childhood or old age, may need support and technical adaptations as part of the rehabilitation process. Recent research shows variations in efficacy but some studies show that if implanted at a very young age, some profoundly impaired children can acquire effective hearing and speech, particularly if supported by appropriate rehabilitation.\n\nFor a classroom setting, children with hearing loss often benefit from direct instruction and communication. One option for students is to attend a school for the Deaf, where they will have access to the language, communication, and education. Another option is to have the child attend a mainstream program, with special accommodation such as providing favorable seating for the child. Having the student sit as close to the teacher as possible improves the student's ability to hear the teacher's voice and to more easily read the teacher's lips. When lecturing, teachers can help the student by facing them and by limiting unnecessary noise in the classroom. In particular, the teacher can avoid talking when their back is turned to the classroom, such as while writing on a whiteboard.\n\nSome other approaches for classroom accommodations include pairing deaf or hard of hearing students with hearing students. This allows the deaf or hard of hearing student to ask the hearing student questions about concepts that they have not understood. The use of CART (Communication Access Real Time) systems, where an individual types a captioning of what the teacher is saying, is also beneficial. The student views this captioning on their computer. Automated captioning systems are also becoming a popular option. In an automated system, software, instead of a person, is used to generate the captioning. Unlike CART systems, automated systems generally do not require an Internet connection and thus they can be used anywhere and anytime. Another advantage of automated systems over CART is that they are much lower in cost. However, automated systems are generally designed to only transcribe what the teacher is saying and to not transcribe what other students say. An automated system works best for situations where just the teacher is speaking, whereas a CART system will be preferred for situations where there is a lot of classroom discussion.\n\nFor those students who are completely deaf, one of the most common interventions is having the child communicate with others through an interpreter using sign language.\n\nGlobally, hearing loss affects about 10% of the population to some degree. It caused moderate to severe disability in 124.2 million people as of 2004 (107.9 million of whom are in low and middle income countries). Of these 65 million acquired the condition during childhood. At birth ~3 per 1000 in developed countries and more than 6 per 1000 in developing countries have hearing problems.\n\nHearing loss increases with age. In those between 20 and 35 rates of hearing loss are 3% while in those 44 to 55 it is 11% and in those 65 to 85 it is 43%.\n\nA 2017 report by the World Health Organization estimated the costs of unaddressed hearing loss and the cost-effectiveness of interventions, for the health-care sector, for the education sector and as broad societal costs. Globally, the annual cost of unaddressed hearing loss was estimated to be in the range of $750–790 billion international dollars.\n\nData from the United States in 2011-2012 found that rates of hearing loss has declined among adults aged 20 to 69 years, when compared with the results from an earlier time period (1999-2004). It also found that adult hearing loss is associated with increasing age, sex, race/ethnicity, educational level, and noise exposure.\n\nNearly one in four adults had audiometric results suggesting noise-induced hearing loss. Almost one in four adults who reported excellent or good hearing had a similar pattern (5.5% on both sides and 18% on one side). Among people who reported exposure to loud noise at work, almost one third had such changes.\n\nAbbé Charles-Michel de l'Épée opened the first school for the deaf in Paris at the deaf school. The American Thomas Gallaudet witnessed a demonstration of deaf teaching skills from Épée's successor Abbé Sicard and two of the school's deaf faculty members, Laurent Clerc and Jean Massieu; accompanied by Clerc, he returned to the United States, where in 1817 they founded American School for the Deaf in Hartford, Connecticut. American Sign Language (ASL) started to evolve from primarily French Sign Language (LSF), and other outside influences.\n\n\"Post-lingual deafness\" is hearing loss that is sustained after the acquisition of language, which can occur due to disease, trauma, or as a side-effect of a medicine. Typically, hearing loss is gradual and often detected by family and friends of affected individuals long before the patients themselves will acknowledge the disability. Post-lingual deafness is far more common than pre-lingual deafness. Those who lose their hearing later in life, such as in late adolescence or adulthood, face their own challenges, living with the adaptations that allow them to live independently.\n\n\"Prelingual deafness\" is hearing loss that is sustained before the acquisition of language, which can occur due to a congenital condition or through hearing loss in early infancy. It is believed that prelingual deafness impairs an individual's ability to acquire a \"spoken\" language, but some deaf children can acquire spoken language through speech immersion along with support from sign language and hearing aids or cochlear implants. Non-signing parents of deaf babies usually go with oral approach without the support of sign language because prelingual hearing loss is acquired via either disease or trauma rather than genetically inherited, so families with deaf children nearly always lack previous experience with sign language. Unfortunately, this brings on the risk of language deprivation for the deaf baby because the deaf baby wouldn't have a language if the child is unable to acquire spoken language successfully. Deaf babies born into signing families rarely have delays in language development since they do meet language milestones, but in sign language in lieu of spoken language.\n\nThere has been considerable controversy within the culturally deaf community over cochlear implants. For the most part, there is little objection to those who lost their hearing later in life, or culturally deaf adults choosing to be fitted with a cochlear implant.\n\nMany in the deaf community strongly object to a deaf child being fitted with a cochlear implant (often on the advice of an audiologist); new parents may not have sufficient information on raising deaf children and placed in an oral-only program that emphasizes the ability to speak and listen over other forms of communication such as sign language or total communication. Many Deaf people view cochlear implants and other hearing devices as confusing to one's identity. A Deaf person will never be a hearing person and therefore would be trying to fit into a way of living that is not their own. Other concerns include loss of Deaf culture and identity and limitations on hearing restoration.\n\nJack Gannon, a professor at Gallaudet University, said this about Deaf culture: \"Deaf culture is a set of learned behaviors and perceptions that shape the values and norms of deaf people based on their shared or common experiences.\" Some doctors believe that being deaf makes a person more social. Bill Vicar, from ASL University, shared his experiences as a deaf person, \"[deaf people] tend to congregate around the kitchen table rather than the living room sofa… our good-byes take nearly forever, and our hellos often consist of serious hugs. When two of us meet for the first time we tend to exchange detailed biographies.\" Deaf culture is not about contemplating what deaf people cannot do and how to fix their problems, an approach known as the \"pathological view of the deaf.\" Instead deaf people celebrate what they can do. There is a strong sense of unity between deaf people as they share their experiences of suffering through a similar struggle. This celebration creates a unity between even deaf strangers. Bill Vicars expresses the power of this bond when stating, \"if given the chance to become hearing most [deaf people] would choose to remain deaf.\"\n\nThe United States-based National Association of the Deaf has a statement on its website regarding cochlear implants. The NAD asserts that the choice to implant is up to the individual (or the parents), yet strongly advocates a fully informed decision in all aspects of a cochlear implant. Much of the negative reaction to cochlear implants stems from the medical viewpoint that deafness is a condition that needs to be \"cured,\" while the Deaf community instead regards deafness a defining cultural characteristic.\n\nMany other assistive devices are more acceptable to the Deaf community, including but not limited to, hearing aids, closed captioning, email and the Internet, text telephones, and video relay services.\n\nSign languages convey meaning through manual communication and body language instead of acoustically conveyed sound patterns. This involves the simultaneous combination of hand shapes, orientation and movement of the hands, arms or body, and facial expressions to express a speaker's thoughts. \"Sign languages are based on the idea that vision is the most useful tool a deaf person has to communicate and receive information\".\n\nThose who are deaf (by either state or federal standards) have access to a free and appropriate public education. If a child does qualify as being deaf or hard of hearing and receives an individualized education plan, the IEP team must consider, \"the child's language and communication needs. The IEP must include opportunities for direct communication with peers and professionals. It must also include the student’s academic level, and finally must include the students full range of needs\"\n\nIn part, the Department of Education defines deafness as \"… a hearing impairment that is so severe that the child is impaired in processing linguistic information through hearing, with or without amplification ….\" Hearing impairment is defined as \"… an impairment in hearing, whether permanent or fluctuating, that adversely affects a child's educational performance but that is not included under the definition of deafness ….\"\n\nIn a residential school where all the children use the same communication system (whether it is a school using ASL, Total Communication or Oralism), students will be able to interact normally with other students, without having to worry about being criticized. An argument supporting inclusion, on the other hand, exposes the student to people who are not just like them, preparing them for adult life. Through interacting, children with hearing disabilities can expose themselves to other cultures which in the future may be beneficial for them when it comes to finding jobs and living on their own in a society where their disability may put them in the minority. These are some reasons why a person may or may not want to put their child in an inclusion classroom.\n\nThe communication limitations between people who are deaf and their hearing family members can often cause difficulties in family relationships, and affect the strength of relationships among individual family members. It was found that most people who are deaf have hearing parents, which means that the channel that the child and parents communicate through can be very different, often affecting their relationship in a negative way. If a parent communicates best verbally, and their child communicates best using sign language, this could result in ineffective communication between parents and children. Ineffective communication can potentially lead to fights caused by misunderstanding, less willingness to talk about life events and issues, and an overall weaker relationship. Even if individuals in the family made an effort to learn deaf communication techniques such as sign language, a deaf family member often will feel excluded from casual banter; such as the exchange of daily events and news at the dinner table. It is often difficult for people who are deaf to follow these conversations due to the fast-paced and overlapping nature of these exchanges. This can cause a deaf individual to become frustrated and take part in less family conversations. This can potentially result in weaker relationships between the hearing individual and their immediate family members. This communication barrier can have a particularly negative effect on relationships with extended family members as well. Communication between a deaf individual and their extended family members can be very difficult due to the gap in verbal and non-verbal communication. This can cause the individuals to feel frustrated and unwilling to put effort into communicating effectively. The lack of effort put into communicating can result in anger, miscommunication, and unwillingness to build a strong relationship.\n\nPeople who have hearing loss can often experience many difficulties as a result of communication barriers among them and other hearing individuals in the community. Some major areas that can be impacted by this are involvement in extracurricular activities and social relationships. For young people, extracurricular activities are vehicles for physical, emotional, social, and intellectual development. However, it is often the case that communication barriers between people who are deaf and their hearing peers and coaches/club advisors limit them from getting involved. These communication barriers make it difficult for someone with a hearing loss to understand directions, take advice, collaborate, and form bonding relationships with other team or club members. As a result, extracurricular activities such as sports teams, clubs, and volunteering are often not as enjoyable and beneficial for individuals who have hearing loss, and they may engage in them less often. A lack of community involvement through extracurricular activities may also limit the individual’s social network. In general, it can be difficult for someone who is deaf to develop and maintain friendships with their hearing peers due to the communication gap that they experience. They can often miss the jokes, informal banter, and \"messing around\" that is associated with the formation of many friendships among young people. Conversations between people who are deaf and their hearing peers can often be limited and short due to their differences in communication methods and lack of knowledge on how to overcome these differences. Deaf individuals can often experience rejection by hearing peers who are not willing to make an effort to find their way around communication difficulties. Patience and motivation to overcome such communication barriers is required by both the deaf or hard of hearing and hearing individuals in order to establish and maintain good friendships.\n\nMany people tend to forget about the difficulties that deaf children encounter, as they view the deaf child differently from a deaf adult. Deaf children grow up being unable to fully communicate with their parents, siblings and other family members. Examples include being unable to tell their family what they have learned, what they did, asking for help, or even simply being unable to interact in daily conversation. Deaf children have to learn sign language and to read lips at a young age, however they cannot communicate with others using it unless the others are educated in sign language as well. Children who are deaf or hard of hearing are faced with many complications while growing up, for example some children have to wear hearing aids and others require assistance from sign language (ASL) interpreters. The interpreters help them to communicate with other individuals until they develop the skills they need to efficiently communicate on their own. Although growing up for deaf children may entitle more difficulties than for other children, there are many support groups that allow deaf children to interact with other children. This is where they develop friendships. There are also classes for young children to learn sign language in an environment that has other children in their same situation and around their same age. These groups and classes can be very beneficial in providing the child with the proper knowledge and not to mention the societal interactions that they need in order to live a healthy, young, playful and carefree life that any child deserves.\n\nThere are three typical adjustment patterns adopted by adults with hearing loss. The first one is to remain withdrawn into your own self. This provides a sense of safety and familiarity which can be a comforting way to lead your life. The second is to act \"as if\" one does not even have hearing loss. A positive attitude will help people to live a life with no barriers and thus, engage in optimal interaction. The final and third pattern is for the person to accept their hearing loss as a part of them without undervaluing oneself. This means understanding that one is forced to live life with this disability, however it is not the only thing that constitutes life’s meaning. Furthermore, many feel as if their inability to hear others during conversation is their fault. It's important that these individuals learn how to become more assertive individuals who do not lack fear when it comes to asking someone to repeat something or to speak a little louder. Although there is much fatigue and frustration that is produced from one’s inability to hear, it is important to learn from personal experiences in order to improve on one’s communication skills. In essence, these patterns will help adults with hearing loss deal with the communication barriers that are present.\n\nIn most instances, people who are deaf find themselves working with hearing colleagues, where they can often be cut off from the communication going on around them. Interpreters can be provided for meetings and workshops, however are seldom provided for everyday work interactions. Communication of important information needed for jobs typically comes in the form of written or verbal summaries, which do not convey subtle meanings such as tone of voice, side conversations during group discussions, and body language. This can result in confusion and misunderstanding for the worker who is deaf, therefore making it harder to do their job effectively. Additionally, deaf workers can be unintentionally left out of professional networks, informal gatherings, and casual conversations among their collogues. Information about informal rules and organizational culture in the workplace is often communicated though these types of interactions, which puts the worker who is deaf at a professional and personal disadvantage. This could sever their job performance due to lack of access to information and therefore, reduce their opportunity to form relationships with their co-workers. Additionally, these communication barriers can all affect a deaf person’s career development. Since being able to effectively communicate with one's co-workers and other people relevant to one's job is essential to managerial positions, people with hearing loss can often be denied such opportunities.\n\nTo avoid these situations in the workplace, individuals can take full-time or part-time sign language courses. In this way, they can become better able to communicate with the deaf and hard of hearing. Such courses teach the American Sign Language (ASL) language as most North Americans use this particular language to communicate. It is a visual language made up of specific gestures (signs), hand shapes, and facial expressions that contain their own unique grammatical rules and sentence structures By completing sign language courses, it ensures that deaf individuals feel a part of the workplace and have the ability to communicate with their co-workers and employer in the manner as other hearing employees do.\n\nNot only can communication barriers between deaf and hearing people affect family relationships, work, and school, but they can also have a very significant effect on a deaf individual’s physical and mental health care. As a result of poor communication between the health care professional and the deaf or hard of hearing patient, many patients report that they are not properly informed about their disease and prognosis. \n\nThis lack of or poor communication could also lead to other issues such as misdiagnosis, poor assessments, mistreatment, and even possibly harm to patients. Poor communication in this setting is often the result of health care providers having the misconception that all people who are deaf or hard of hearing have the same type of hearing loss, and require the same type of communication methods. In reality, there are many different types and range of hearing loss, and in order to communicate effectively a health care provider needs to understand that each individual with hearing loss has unique needs. This affects how individuals have been educated to communicate, as some communication methods work better depending on an individual’s severity of hearing loss. For example, assuming every deaf or hard of hearing patient knows American Sign Language would be incorrect because there are different types of sign language, each varying in signs and meanings. A patient could have been educated to use cued speech which is entirely different from ASL. Therefore, in order to communicate effectively, a health care provider needs to understand that each individual has unique needs when communicating.\n\nAlthough there are specific laws and rules to govern communication between health care professionals and people who are deaf, they are not always followed due to the health care professional’s insufficient knowledge of communication techniques. This lack of knowledge can lead them to make assumptions about communicating with someone who is deaf, which can in turn cause them to use an unsuitable form of communication. \nActs in countries such as the Americans with Disabilities Act (ADA) state that all health care providers are required to provide reasonable communication accommodations when caring for patients who are deaf. These accommodations could include qualified sign language interpreters, CDIs, and technology such as Internet interpretation services. A qualified sign language interpreter will enhance communication between a deaf individual and a health care professional by interpreting not only a health professional’s verbal communication, but also their non-verbal such as expressions, perceptions, and body language. A Certified Deaf Interpreter (CDI) is a sign language interpreter who is also a member of the Deaf community. They accompany a sign language interpreter and are useful for communication with deaf individuals who also have language or cognitive deficits. A CDI will transform what the health care professional communicates into basic, simple language. This method takes much longer, however it can also be more effective than other techniques. Internet interpretation services are convenient and less costly, but can potentially pose significant risks. They involve the use of a sign language interpreter over a video device rather than directly in the room. This can often be an inaccurate form of communication because the interpreter may not be licensed, is often unfamiliar with the patient and their signs, and can lack knowledge of medical terminology.\n\nAside from utilizing interpreters, healthcare professionals can improve their communication with deaf or hard of hearing patients by educating themselves on common misconceptions and proper practices depending on the patient’s needs. For example, a common misconception is that exaggerating words and speaking loudly will help the patient understand more clearly. However, many individuals with hearing loss depend on lip-reading to identify words. Exaggerated pronunciation and a raised voice can distort the lips, making it even more difficult to understand. Another common mistake health care professionals make are the use of single words rather than full sentences. Although language should be kept simple and short, keeping context is important because certain homophonous words are difficult to distinguish by lip-reading. Health care professionals can further improve their own communication with their patients by eliminating any background noise and positioning themselves in a way where their face is clearly visible to the patient, and suitably lit. The healthcare professional should know how to use body language and facial expressions to properly communicate different feelings.\n\nA 2005 study achieved successful regrowth of cochlea cells in guinea pigs. However, the regrowth of cochlear hair cells does not imply the restoration of hearing sensitivity, as the sensory cells may or may not make connections with neurons that carry the signals from hair cells to the brain. A 2008 study has shown that gene therapy targeting Atoh1 can cause hair cell growth and attract neuronal processes in embryonic mice. Some hope that a similar treatment will one day ameliorate hearing loss in humans.\n\nRecent research, reported in 2012 achieved growth of cochlear nerve cells resulting in hearing improvements in gerbils, using stem cells. Also reported in 2013 was regrowth of hair cells in deaf adult mice using a drug intervention resulting in hearing improvement. The Hearing Health Foundation in the US has embarked on a project called the Hearing Restoration Project. Also Action on Hearing Loss in the UK is also aiming to restore hearing.\n\nResearchers reported in 2015 that genetically deaf mice which were treated with TMC1 gene therapy recovered some of their hearing. In 2017, additional studies were performed to treat Usher syndrome and here, a recombinant adeno-associated virus seemed to outperform the older vectors.\n\nBesides research studies seeking to improve hearing, such as the ones listed above, research studies on the deaf have also been carried out in order to understand more about audition. Pijil and Shwarz (2005) conducted their study on the deaf who lost their hearing later in life and, hence, used cochlear implants to hear. They discovered further evidence for rate coding of pitch, a system that codes for information for frequencies by the rate that neurons fire in the auditory system, especially for lower frequencies as they are coded by the frequencies that neurons fire from the basilar membrane in a synchronous manner. Their results showed that the subjects could identify different pitches that were proportional to the frequency stimulated by a single electrode. The lower frequencies were detected when the basilar membrane was stimulated, providing even further evidence for rate coding.\n\n"}
{"id": "39127332", "url": "https://en.wikipedia.org/wiki?curid=39127332", "title": "High-altitude adaptation in humans", "text": "High-altitude adaptation in humans\n\nHigh-altitude adaptation in humans is an instance of evolutionary modification in certain human populations, including those of Tibet in Asia, the Andes of the Americas, and Ethiopia in Africa, who have acquired the ability to survive at extremely high altitudes. This adaptation means irreversible, long-term physiological responses to high-altitude environments, associated with heritable behavioural and genetic changes.\n\nWhile the rest of the human population would suffer serious health consequences, the indigenous inhabitants of these regions thrive well in the highest parts of the world. These people have undergone extensive physiological and genetic changes, particularly in the regulatory systems of oxygen respiration and blood circulation, when compared to the general lowland population.\n\nThis special adaptation is now recognised as an example of natural selection in action. The adaptation account of the Tibetans has become the fastest case of human evolution in the scientific record, as it is estimated to have occurred in less than 3,000 years.\n\nModern humans dispersed from Africa less than 100,000 years ago, and eventually colonized the rest of the world, including the harshest environments of extreme cold and high mountains. The abundance of oxygen in the atmosphere is inversely related to altitude from the sea level; hence, the highest mountain ranges of the world are considered unsuitable for human habitation.\n\nNevertheless, around 140 million people, just under 2% of the world's human population, live permanently at high altitudes, that is, at heights above in South America, East Africa, and South Asia. These populations have done so for millennia without apparent complications. The overwhelming majority, over 98% of humans from other parts of the world, normally suffer symptoms of altitude sickness in these regions, often resulting in life-threatening trauma and even death.\n\nStudies on the detail biological mechanism have revealed that adaptation of the Tibetans, Andeans and Ethiopians is indeed an observable instance of the process of natural selection in acting on favourable characters such as enhanced respiratory mechanisms in humans.\n\nHumans are naturally adapted to lowland environment where oxygen is abundant. When people from the general lowlands go to altitudes above , with atmospheric pressure 74% of normal they experience mountain sickness, which is a type of hypoxia, a clinical syndrome of severe lack of oxygen. Complications include fatigue, dizziness, breathlessness, headaches, insomnia, malaise, nausea, vomiting, body pain, loss of appetite, ear-ringing, blistering and purpling of the hands and feet, and dilated veins.\n\nThe sickness is compounded by related symptoms such as cerebral oedema (swelling of brain) and pulmonary oedema (fluid accumulation in lungs). For several days, they breathe excessively and burn extra energy even when the body is relaxed. The heart rate then gradually decreases. Hypoxia, in fact, is one of the principal causes of death among mountaineers. In women, pregnancy can be severely affected, such as development of high blood pressure, called preeclampsia, which causes premature labour, low birth weight of babies, and often complicated with profuse bleeding, seizures, and death of the mother.\n\nMore than 140 million people worldwide are estimated to live at an elevation higher than above sea level, of which 13 million are in Ethiopia, 1.7 million in Tibet (total of 78 million in Asia), 35 million in the South American Andes, and 0.3 million in Colorado Rocky Mountains. Certain natives of Tibet, Ethiopia, and the Andes have been living at these high altitudes for generations and are protected from hypoxia as a consequence of genetic adaptation. It is estimated that at , every lungful of air only has 60% of the oxygen molecules that people at sea level have. At elevations above , lack of oxygen becomes seriously lethal. That is, these highlanders are constantly exposed to an intolerably low oxygen environment, yet they live without any debilitating problems. Basically, the shared adaptation is the ability to maintain relatively low levels of haemoglobin, which is the chemical complex for transporting oxygen in the blood. One of the best documented effects of high altitude is a progressive reduction in birth weight. It has been known that women of long-resident high-altitude population are not affected. These women are known to give birth to heavier-weight infants than women of lowland inhabitants. This is particularly true among Tibetan babies, whose average birth weight is 294-650 (~470) g heavier than the surrounding Chinese population; and their blood-oxygen level is considerably higher.\n\nThe first scientific investigations of high-altitude adaptation was done by A. Roberto Frisancho of the University of Michigan in the late 1960s among the Quechua people of Peru. Paul T. Baker, Penn State University, (in the Department of Anthropology) also conducted a considerable amount of research into human adaptation to high altitudes, and mentored students who continued this research. One of these students went on to conduct research on high altitude adaptation among the Tibetans in the early 1980s through to today, anthropologist Cynthia Beall of the Case Western Reserve University.\n\nScientists started to notice the extraordinary physical performance of Tibetans since the beginning of Himalayan climbing era in the early 20th century. The hypothesis of a possible evolutionary genetic adaptation makes sense. The Tibetan plateau has an average elevation of above sea level, and covering more than 2.5 million km, it is the highest and largest plateau in the world. In 1990, it was estimated that 4,594,188 Tibetans live on the plateau, with 53% living at an altitude over . Fairly large numbers (about 600,000) live at an altitude exceeding in the Chantong-Qingnan area. Where the Tibetan highlanders live, the oxygen level is only about 60% of that at sea level. The Tibetans, who have been living in this region for 3,000 years, do not exhibit the elevated haemoglobin concentrations to cope with oxygen deficiency as observed in other populations who have moved temporarily or permanently at high altitudes. Instead, the Tibetans inhale more air with each breath and breathe more rapidly than either sea-level populations or Andeans. Tibetans have better oxygenation at birth, enlarged lung volumes throughout life, and a higher capacity for exercise. They show a sustained increase in cerebral blood flow, lower haemoglobin concentration, and less susceptibility to chronic mountain sickness than other populations, due to their longer history of high-altitude habitation.\n\nGeneral people can develop short-term tolerance with careful physical preparation and systematic monitoring of movements, but the biological changes are quite temporary and reversible when they return to lowlands. Moreover, unlike lowland people who only experience increased breathing for a few days after entering high altitudes, Tibetans retain this rapid breathing and elevated lung-capacity throughout their lifetime. This enables them to inhale larger amounts of air per unit of time to compensate for low oxygen levels. In addition, they have high levels (mostly double) of nitric oxide in their blood, when compared to lowlanders, and this probably helps their blood vessels dilate for enhanced blood circulation. Further, their haemoglobin level is not significantly different (average 15.6 g/dl in males and 14.2 g/dl in females), from those of people living at low altitude. (Normally, mountaineers experience >2 g/dl increase in Hb level at Mt. Everest base camp in two weeks.) In this way they are able to evade both the effects of hypoxia and mountain sickness throughout life. Even when they climbed the highest summits like Mt. Everest, they showed regular oxygen uptake, greater ventilation, more brisk hypoxic ventilatory responses, larger lung volumes, greater diffusing capacities, constant body weight and a better quality of sleep, compared to people from the lowland.\n\nIn contrast to the Tibetans, the Andean highlanders, who have been living at high-altitudes for no more than 11,000 years, show different pattern of haemoglobin adaptation. Their haemoglobin concentration is higher compared to those of lowlander population, which also happens to lowlanders moving to high altitude. When they spend some weeks in the lowland their haemoglobin drops to average of other people. This shows only temporary and reversible acclimatisation. However, in contrast to lowland people, they do have increased oxygen level in their haemoglobin, that is, more oxygen per blood volume than other people. This confers an ability to carry more oxygen in each red blood cell, making a more effective transport of oxygen in their body, while their breathing is essentially at the same rate. This enables them to overcome hypoxia and normally reproduce without risk of death for the mother or baby. The Andean highlanders are known from the 16th-century missionaries that their reproduction had always been normal, without any effect in the giving birth or the risk for early pregnancy loss, which are common to hypoxic stress. They have developmentally acquired enlarged residual lung volume and its associated increase in alveolar area, which are supplemented with increased tissue thickness and moderate increase in red blood cells. Though the physical growth in body size is delayed, growth in lung volumes is accelerated. An incomplete adaptation such as elevated haemoglobin levels still leaves them at risk for mountain sickness with old age. \n\nAmong the Quechua people of the Altiplano, there is a significant variation in \"NOS3\" (the gene encoding endothelial nitric oxide synthase, eNOS), which is associated with higher levels of nitric oxide in high altitude. Nuñoa children of Quechua ancestry exhibit higher blood-oxygen content (91.3) and lower heart rate (84.8) than their counterpart school children of different ethnicity, who have an average of 89.9 blood-oxygen and 88-91 heart rate. High-altitude born and bred females of Quechua origins have comparatively enlarged lung volume for increased respiration. \n\nBlood profile comparisons show that among the Andeans, Aymaran highlanders are better adapted to highlands than the Quechuas. Among the Bolivian Aymara people, the resting ventilation and hypoxic ventilatory response were quite low (roughly 1.5 times lower), in contrast to those of the Tibetans. The intrapopulation genetic variation was relatively less among the Aymara people. Moreover, unlike the Tibetans, the blood haemoglobin level is quite normal among Aymarans, with an average of 19.2 g/dl for males and 17.8 g/dl for females. Among the different native highlander populations, the underlying physiological responses to adaptation are quite different. For example, among four quantitative features, such as are resting ventilation, hypoxic ventilatory response, oxygen saturation, and haemoglobin concentration, the levels of variations are significantly different between the Tibetans and the Aymaras. The Andeans, in general are the most poorly adapted, as particularly shown by their frequent mountain sickness and loss of adaptative characters when they move to lowlands.\n\nThe peoples of the Ethiopian highlands also live at extremely high altitudes, around to . Highland Ethiopians exhibit elevated haemoglobin levels, like Andeans and lowlander peoples at high altitudes, but do not exhibit the Andean’s increased in oxygen-content of haemoglobin. Among healthy individuals, the average haemoglobin concentrations are 15.9 and 15.0 g/dl for males and females respectively (which is lower than normal, almost similar to the Tibetans), and an average oxygen saturation of haemoglobin is 95.3% (which is higher than average, like the Andeans). Additionally, Ethiopian highlanders do not exhibit any significant change in blood circulation of the brain, which has been observed among the Peruvian highlanders (and attributed to their frequent altitude-related illnesses). Yet, similar to the Andeans and Tibetans, the Ethiopian highlanders are immune to the extreme dangers posed by high-altitude environment, and their pattern of adaptation is definitely unique from that of other highland peoples.\n\nThe underlying molecular evolution of high-altitude adaptation has been explored and understood fairly recently. Depending on the geographical and environmental pressures, high-altitude adaptation involves different genetic patterns, some of which\nhave evolved quite recently. For example, Tibetan adaptations became prevalent in the past 3,000 years, a rapid example of recent human evolution. At the turn of the 21st century, it was reported that the genetic make-up of the respiratory components of the Tibetan and the Ethiopian populations are significantly different.\n\nSubstantial evidence in Tibetan highlanders suggests that variation in haemoglobin and blood-oxygen levels are adaptive as Darwinian fitness. It has been documented that Tibetan women with a high likelihood of possessing one to two alleles for high blood-oxygen content (which is odd for normal women) had more surviving children; the higher the oxygen capacity, the lower the infant mortality. In 2010, for the first time, the genes responsible for the unique adaptive traits were identified following genome sequences of 50 Tibetans and 40 Han Chinese from Beijing. Initially, the strongest signal of natural selection detected was a transcription factor involved in response to hypoxia, called endothelial Per-Arnt-Sim (PAS) domain protein 1 (\"EPAS1\"). It was found that one single-nucleotide polymorphism (SNP) at \"EPAS1\" shows a 78% frequency difference between Tibetan and mainland Chinese samples, representing the fastest genetic change observed in any human gene to date. Hence, Tibetan adaptation to high altitude becomes the fastest process of phenotypically observable evolution in humans, which is estimated to occur in less than 3,000 years ago, when the Tibetans split up from the mainland Chinese population. Mutations in \"EPAS1\", at higher frequency in Tibetans than their Han neighbours, correlate with decreased haemoglobin concentrations among the Tibetans, which is the hallmark of their adaptation to hypoxia. Simultaneously, two genes, egl nine homolog 1 (\"EGLN1\") (which inhibits haemoglobin production under high oxygen concentration) and peroxisome proliferator-activated receptor alpha (\"PPARA\"), were also identified to be positively selected in relation to decreased haemoglobin nature in the Tibetans.\n\nSimilarly, the Sherpas, known for their Himalayan hardiness, exhibit similar patterns in the \"EPAS1\" gene, which further fortifies that the gene is under selection for adaptation to the high-altitude life of Tibetans. A study in 2014 indicates that the mutant \"EPAS1\" gene could have been inherited from archaic hominins, the Denisovans. \"EPAS1\" and \"EGLN1\" are definitely the major genes for unique adaptive traits when compared with those of the Chinese and Japanese. Comparative genome analysis in 2014 revealed that the Tibetans inherited an equal mixture of genomes from the Nepalese-Sherpas and Hans, and they acquired the adaptive genes from the sherpa-lineage. Further, the population split was estimated to occur around 20,000 to 40,000 years ago, a range of which support archaeological, mitochondria DNA and Y chromosome evidence for an initial colonisation of the Tibetan plateau around 30,000 years ago.\n\nThe genes (\"EPAS1\", \"EGLN1\", and \"PPARA\") function in concert with another gene named hypoxia inducible factors (\"HIF\"), which in turn is a principal regulator of red blood cell production (erythropoiesis) in response to oxygen metabolism. The genes are associated not only with decreased haemoglobin levels, but also in regulating energy metabolism. \"EPAS1\" is significantly associated with increased lactate concentration (the product of anaerobic glycolysis), and \"PPARA\" is correlated with decrease in the activity of fatty acid oxidation. \"EGLN1\" codes for an enzyme, prolyl hydroxylase 2 (PHD2), involved in erythropoiesis. Among the Tibetans, mutation in \"EGLN1\" (specifically at position 12, where cytosine is replaced with guanine; and at 380, where G is replaced with C) results in mutant PHD2 (aspartic acid at position 4 becomes glutamine, and cysteine at 127 becomes serine) and this mutation inhibits erythropoiesis. The mutation is estimated to occur about 8,000 years ago. Further, the Tibetans are enriched for genes in the disease class of human reproduction (such as genes from the \"DAZ\", \"BPY2\", \"CDY\", and \"HLA-DQ\" and \"HLA-DR\" gene clusters) and biological process categories of response to DNA damage stimulus and DNA repair (such as \"RAD51\", \"RAD52\", and \"MRE11A\"), which are related to the adaptive traits of high infant birth weight and darker skin tone and are most likely due to recent local adaptation.\n\nThe patterns of genetic adaptation among the Andeans are largely distinct from those of the Tibetan, with both populations showing evidence of positive natural selection in different genes or gene regions. However, \"EGLN1\" appears to be the principal signature of evolution, as it shows evidence of positive selection in both Tibetans and Andeans. Even then, the pattern of variation for this gene differs between the two populations. Among the Andeans, there are no significant associations between \"EPAS1\" or \"EGLN1\" SNP genotypes and haemoglobin concentration, which has been the characteristic of the Tibetans. The whole genome sequences of 20 Andeans (half of them having chronic mountain sickness) revealed that two genes, SENP1 (an erythropoiesis regulator) and ANP32D (an oncogene) play vital roles in their weak adaptation to hypoxia.\n\nThe adaptive mechanism of Ethiopian highlanders is quite different. This is probably because their migration to the highland was relatively early; for example, the Amhara have inhabited altitudes above for at least 5,000 years and altitudes around to for more than 70,000 years. Genomic analysis of two ethnic groups, Amhara and Oromo, revealed that gene variations associated with haemoglobin difference among Tibetans or other variants at the same gene location do not influence the adaptation in Ethiopians. Identification of specific genes further reveals that several candidate genes are involved in Ethiopians, including \"CBARA1\", \"VAV3\", \"ARNT2\" and \"THRB\". Two of these genes (\"THRB\" and \"ARNT2\") are known to play a role in the HIF-1 pathway, a pathway implicated in previous work reported in Tibetan and Andean studies. This supports the concept that adaptation to high altitude arose independently among different highlanders as a result of convergent evolution.\n\n\n"}
{"id": "17094565", "url": "https://en.wikipedia.org/wiki?curid=17094565", "title": "Indicative limit value", "text": "Indicative limit value\n\nIn the law of the European Union, indicative limit values, more exactly indicative occupational exposure limit values (IOELVs), are human exposure limits to hazardous substances specified by the Council of the European Union based on expert research and advice.\n\nThey are not binding on member states but must be taken into consideration in setting national occupational exposure limits. Some member states have pre-existing national limits lower than the IOELV and are not required to revise these upwards. In practice, most member states adopt the IOELV but there are some variances upwards and downwards.\n\nA system of IOELVs was first introduced in 1980 by directive 80/1107/EEC but the first list of 27 substances was not created until directive 91/322/EC in 1991. Member states had until 31 December 1993 to implement national limits. This first list was amended by directive 2006/15/EC in 2006 which transferred 10 of the 27 to a different regulatory regime. A second list was defined in directive 96/94/EEC but this was repealed by directive 2000/39/EC.\n\nIn 1998, directive 80/1107/EEC was repealed and replaced by a new regime under the chemical agents directive (directive 98/24/EC). The directive defines occupational exposure limit value as \"the limit of the time-weighted average of the concentration of a chemical agent in the air within the breathing zone of a worker over a specified reference period.\" Article 3 of the directive led to the creation of the Scientific Committee on Occupational Exposure Limit Values to advise the European Commission. There have subsequently been two directives establishing further lists of IOELVs: 2000/39/EC and 2006/15/EC. , the IOELVs under directive 91/322/EC remain in force but under review. A third list of IOELVs, now under directive 98/24/EC, is expected in the first half of 2008.\n\n"}
{"id": "45266571", "url": "https://en.wikipedia.org/wiki?curid=45266571", "title": "Laboratory safety", "text": "Laboratory safety\n\nMany laboratories contain significant risks, and the prevention of laboratory accidents requires great care and constant vigilance. Examples of risk factors include high voltages, high and low pressures and temperatures, corrosive and toxic chemicals, and biohazards including infective organisms and their toxins.\n\nMeasures to protect against laboratory accidents include safety training and enforcement of laboratory safety policies, safety review of experimental designs, the use of personal protective equipment, and the use of the buddy system for particularly risky operations.\n\nIn many countries, laboratory work is subject by health and safety legislation. In some cases, laboratory activities can also present environmental health risks, for example, the accidental or deliberate discharge of toxic or infective material from the laboratory into the environment.\n\nHazardous chemicals present physical and/or health threats to workers in clinical, industrial, and academic laboratories. Laboratory chemicals include cancer-causing agents (carcinogens), toxins (e.g., those affecting the liver, kidney, and nervous system), irritants, corrosives, sensitizers, as well as agents that act on the blood system or damage the lungs, skin, eyes, or mucous membranes.\n\nMany laboratory workers encounter daily exposure to biological hazards. These hazards are present in various sources throughout the laboratory such as blood and body fluids, culture specimens, body tissue and cadavers, and laboratory animals, as well as other workers.\n\nThese are federally regulated biological agents (e.g., viruses, bacteria, fungi, and prions) and toxins that have the potential to pose a severe threat to public health and safety, to animal or plant health, or to animal or plant products.\n\n\nBesides exposure to chemicals and biological agents, laboratory workers can also be exposed to a number of physical hazards. Some of the common physical hazards that they may encounter include the following: ergonomic, ionizing radiation, non-ionizing\nradiation and noise hazards.\n\nLaboratory workers are at risk for repetitive motion injuries during routine laboratory procedures such as pipetting, working at microscopes, operating microtomes, using cell counters and keyboarding at computer workstations. Repetitive motion injuries develop over time and occur when muscles and joints are stressed, tendons are inflamed, nerves are pinched and the flow of blood is restricted. Standing and working in awkward positions in front of laboratory hoods/biological safety cabinets can also present ergonomic problems.\n\nIonizing radiation sources are found in a wide range of occupational settings, including laboratories. These radiation sources can pose a considerable health risk to affected workers if not properly controlled. Any laboratory possessing or using radioactive isotopes must be licensed by the Nuclear Regulatory Commission (NRC) and/or by a state agency that has been approved by the NRC, 10 CFR 31.11 and 10 CFR 35.12.\n\nThe fundamental objectives of radiation protection measures are:\n\n\n\nWorkers should be trained to recognize the potential for exposure to burns or cuts that can occur from handling or sorting hot sterilized items or sharp instruments when removing them from autoclaves/sterilizers or from steam lines that service the autoclaves.\n\nCentrifuges, due to the high speed at which they operate, have great potential for injuring users if not operated properly. Unbalanced centrifuge rotors can result in injury, even death. Sample container breakage can generate aerosols that may be harmful if inhaled.\nThe majority of all centrifuge accidents are the result of user error.\n\nLaboratory standard for compressed gas\n\nWithin laboratories, compressed gases are usually supplied either through fixed piped gas systems or individual cylinders of gases. Compressed gases can be toxic, flammable, oxidizing, corrosive, or inert. Leakage of any of these gases can be hazardous.\n\n\nCryogens, substances used to produce very low temperatures [below -153 °C (-243 °F)], such as liquid nitrogen (LN) which has a boiling point of -196 °C (-321 °F), are commonly used in laboratories.\n\nAlthough not a cryogen, solid carbon dioxide or dry ice which converts directly to carbon dioxide gas at -78 °C (-109 °F) is also often used in laboratories. Shipments packed with dry ice, samples preserved with liquid nitrogen, and in some cases, techniques that use cryogenic liquids, such as cryogenic grinding of samples, present potential hazards in the laboratory.\n\nHand protection is required to guard against the hazard of touching cold surfaces. It is recommended that Cryogen Safety Gloves be used by the worker.\n\nEye protection is required at all times when working with cryogenic fluids. When pouring a cryogen, working with a wide-mouth Dewar flask or around the exhaust of cold boil-off gas, use of a full face shield is recommended.\n\nPersonal protective equipment or PPE are equipments worn to prevent against exposure of hazardous substances. Although, PPE does not eliminate the risks of hazards but it helps protect the user from the exposure. To make a workplace safer, it should provide instructions and training of how to use and choose proper PPE in different situations.\n\nPPE includes: \n\nIn the laboratory, there is the potential for workers to be exposed to electrical hazards including electric shock, electrocutions, fires and explosions. Damaged electrical cords can lead to possible shocks or electrocutions. A flexible electrical cord may be damaged by door or window edges, by staples and fastenings, by equipment rolling over it, or simply by aging.\n\nThe potential for possible electrocution or electric shock or contact with electrical hazards can result from a number of factors, including the following:\n\nFire is the most common serious hazard that one faces in a typical laboratory. While proper procedures and training can minimize the chances of an accidental fire, laboratory workers should still be prepared to deal with a fire emergency should it occur. In dealing with a laboratory fire, all containers of infectious materials should be placed into autoclaves, incubators, refrigerators, or freezers for containment.\n\nSmall bench-top fires in laboratory spaces are not uncommon. Large laboratory fires are rare. However, the risk of severe injury or death is significant because fuel load and hazard levels in labs are typically very high. Laboratories, especially those using solvents in any quantity, have the potential for flash fires, explosion, rapid spread of fire, and high toxicity of products of combustion (heat, smoke, and flame)\n\n\n"}
{"id": "52683089", "url": "https://en.wikipedia.org/wiki?curid=52683089", "title": "List of CAHIIM-accredited HIM Programs in the United States", "text": "List of CAHIIM-accredited HIM Programs in the United States\n\nThe Commission on Accreditation for Health Informatics and Information Management Education (CAHIIM) has given full accreditation to the following list of Health information management (HIM) programs in the United States.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "23666772", "url": "https://en.wikipedia.org/wiki?curid=23666772", "title": "List of food contamination incidents", "text": "List of food contamination incidents\n\nFood may be accidentally or deliberately contaminated by microbiological, chemical or physical hazards.\n\nIn contrast to microbiologically caused foodborne illness, the link between exposure and effect of chemical hazards in foods is usually complicated by cumulative low doses and the delay between exposure and the onset of symptoms. Chemical hazards include environmental contaminants, food ingredients (such as iodine), heavy metals, mycotoxins, natural toxins, improper storage, processing contaminants, and veterinary medicines.\n\nIncidents have occurred because of poor harvesting or storage of grain, use of banned veterinary products, industrial discharges, human error and deliberate adulteration and fraud.\n\nAn \"incident\" of chemical food contamination may be defined as an episodic occurrence of adverse health effects in humans (or animals that might be consumed by humans) following high exposure to particular chemicals, or instances where episodically high concentrations of chemical hazards were detected in the food chain, and traced back to a particular event.\n\nInformation on the impacts of these incidents is fragmentary and unsystematic, ranging from thousands of dollars to meet the cost of monitoring analysis, to many millions of dollars due to court prosecutions, bankruptcy, product disposal, compensation for revenue loss, damage to brand or reputation, or loss of life.\n\n\n\n\n\n\n\n"}
{"id": "1888454", "url": "https://en.wikipedia.org/wiki?curid=1888454", "title": "List of mammalian gestation durations", "text": "List of mammalian gestation durations\n\n"}
{"id": "1377246", "url": "https://en.wikipedia.org/wiki?curid=1377246", "title": "Lochia", "text": "Lochia\n\nIn the field of obstetrics, lochia is the vaginal discharge after giving birth (puerperium) containing blood, mucus, and uterine tissue. Lochia discharge typically continues for 4 to 6 weeks after childbirth, which is known as the postpartum period.\n\nIt is sterile for the first 2 \ndays, but not so by the third or fourth day, as the uterus begins to be colonized by vaginal commensals such as non-hemolytic streptococci and \"E. coli\".\n\nIt progresses through three stages:\n\nNote: Ambulant patients < 2 weeks, Moribund Patients > 2 weeks, Preterm Delivery < 2 weeks, Twin Delivery > 2 weeks.\n\nIn general, lochia has an odor similar to that of normal menstrual fluid. Any offensive odor or change to a greenish color indicates contamination by organisms such as chlamydia or saprophytic.\n\nLochia that is retained within the uterus is known as lochiostasis or lochioschesis, and can result in lochiometra (distention of the uterus - pushing it out of shape). Lochiorrhea describes an excessive flow of lochia and can indicate infection.\n"}
{"id": "47456362", "url": "https://en.wikipedia.org/wiki?curid=47456362", "title": "Midwife", "text": "Midwife\n\nA midwife is a health professional specializing in pregnancy, childbirth, postpartum, women's sexual and reproductive health (including annual gynecological exams, family planning, menopausal care and others), and newborn care. Their education and training in midwifery equips them to recognise the variations of normal progress of labor, and understand how to deal with deviations from normal. They may intervene in high risk situations such as breech births, twin births and births where the baby is in a posterior position, using non-invasive techniques. When a pregnant woman requires care beyond the midwife's scope of practice, they refer women to obstetricians or perinatologists, who are medical specialists in complications related to pregnancy and birth, including surgical and instrumental deliveries. In many parts of the world, these professions work in tandem to provide care to childbearing women. In others, only the midwife is available to provide care, and in yet other countries many women elect to utilize obstetricians primarily over midwives.\n\nMany developing countries are investing money and training for midwives as these services are needed all over the world. Some primary care services are currently lacking due to the shortage of money being funded for these resources.\n\nAccording to the definition of the International Confederation of Midwives, which has also been adopted by the World Health Organization and the International Federation of Gynecology and Obstetrics:\n\nThe word derives from Old English \"mid\", \"with\" and \"wif\", \"woman\", and thus originally meant \"with-woman\", that is, the person who is \"with\" the \"mother\" (woman) at childbirth. The word is used to refer to both male and female midwives.\n\nThe midwife is recognised as a responsible and accountable professional who works in partnership with women to give the necessary support, care and advice during pregnancy, labour and the postpartum period, to conduct births on the midwife’s own responsibility and to provide care for the newborn and the infant. This care includes preventative measures, the promotion of normal birth, the detection of complications in mother and child, the accessing of medical\ncare or other appropriate assistance and the carrying out of emergency measures.\n\nThe midwife has an important task in health counselling and education, not only for the woman, but also within the family and the community. This work should involve antenatal education and preparation for parenthood and may extend to women’s health, sexual or reproductive health and child care.\n\nA midwife may practise in any setting including the home, community, hospitals, clinics or health units.\n\nEducation, training and regulation\n\nThe undergraduate midwifery programs are three-year full-time university programs leading to a bachelor's degree in midwifery (Bachelor of Midwifery) with additional one-year full-time programs leading to an honours bachelor's degree in midwifery (Bachelor of Midwifery (Honours)). The postgraduate midwifery programs (for registered midwives) lead to master's degrees in midwifery (Master in Midwifery, Master in Midwifery (Research), MSc Midwifery). There are also postgraduate midwifery programs (for registered nurses who wish to become midwives) leading to a bachelor's degree or equivalent qualification in midwifery (Bachelor of Midwifery, Graduate Diploma in Midwifery).\n\nMidwives in Australia must be registered with the Australian Health Practitioner Regulation Agency in order to practice midwifery and use the title 'midwife' or 'registered midwife'.\n\nPractice\n\nMidwives work in a number of settings including hospitals, birthing centres, community centres and women’s homes. They may be employed by health services or organisations, or self-employed as privately practising midwives. All midwives are expected to work within a defined scope of practice and conform to ongoing regulatory requirements that ensure they are safe and autonomous practitioners.\n\nProfessional associations/colleges\n\nMidwifery was reintroduced as a regulated profession in most of Canada's ten provinces in the 1990s. Prior to this legalization, some midwives had practiced in a legal \"grey area\" in some provinces. In 1981, a midwife in BC was charged with practicing without a medical license.\n\nAfter several decades of intensive political lobbying by midwives and consumers, fully integrated, regulated and publicly funded midwifery is now part of the health system in the provinces of British Columbia (regulated since 1995), Alberta (regulated since 2000, fully funded since 2009) Saskatchewan (regulated since 1999), Manitoba (regulated since 1997), Ontario (regulated since 1991), Quebec (regulated since 1999), and Nova Scotia (regulated since 2006), and in the Northwest Territories (regulated since 2003) and Nunavut(regulated since 2008). In 2010, Midwifery legislation was proclaimed in New Brunswick and Newfoundland and Labrador. Only Prince Edward Island and Yukon have no legislation in place for the practice of midwifery.\n\nThe undergraduate midwifery programs are four-year full-time university programs leading to bachelor's degrees in midwifery (B.H.Sc. in Midwifery, Bachelor of Midwifery). \n\nIn British Columbia, the program is offered at the University of British Columbia. Mount Royal University in Calgary, Alberta offers a Bachelor of Midwifery program. In Ontario, the Midwifery Education Program (MEP) is offered by a consortium of McMaster University, Ryerson University and Laurentian University. In Manitoba the program is offered by University College of the North. In Quebec, the program is offered at the Université du Québec à Trois-Rivières. In northern Quebec and Nunavut, Inuit women are being educated to be midwives in their own communities. \n\nThere are also \"bridging programs\" for internationally educated midwives at Ryerson University in Ontario and at the University of British Columbia. A federally funded pilot project called the Multi-jurisdictional Midwifery Bridging Program has been offered in Western Canada in the past, but funding was discontinued when they expanded their midwifery program. A new program was reinstated through the University of British Columbia in 2016 called the Internationally Educated Midwives Bridging Program IEMBP.\n\nMidwives in Canada must be registered, after a process of assessment by the provincial regulatory bodies, in order to practice midwifery and use the title 'midwife', 'registered midwife' or, the French-language equivalent, 'sage femme'.\n\nFrom the original 'alternative' style of midwifery in the 1960s and 1970s, midwifery practice is offered in a variety of ways within regulated provinces: midwives offer continuity of care within small group practices, choice of birthplace, and a focus on the woman as the primary decision-maker in her maternity care. When women or their newborns experience complications, midwives will work in consultation with an appropriate specialist. Registered midwives have access to appropriate diagnostics like blood tests and ultrasounds and can prescribe some medications. Founding principles of the Canadian model of midwifery include informed choice, choice of birthplace, continuity of care from a small group of midwives and respect for the woman as the primary decision maker. Midwives typically have hospital privileges and support women's right to choose where she will have her baby.\n\nThe legal recognition of midwifery has brought midwives into the mainstream of health care with universal funding for services, hospital privileges, rights to prescribe medications commonly needed during pregnancy, birth and postpartum, and rights to order blood work and ultrasounds for their own clients and full consultation access to physicians. To protect the tenets of midwifery and support midwives to provide woman-centered care, the regulatory bodies and professional associations have legislation and standards in place to provide protection, particularly for choice of birth place, informed choice and continuity of care. All regulated midwives have malpractice insurance. Any unregulated person who provides care with 'restricted acts' in regulated provinces or territories is practicing midwifery without a license and is subject to investigation and prosecution.\n\nPrior to legislative changes, very few Canadian women had access to midwifery care, in part because it was not funded by the health care system. Legalizing midwifery has made midwifery services available to a wide and diverse population of women and in many communities the number of available midwives does not meet the growing demand for services. Midwifery services are free to women living in provinces and territories with regulated midwifery.\n\nThe national professional associations in Canada is the Canadian Association of Midwives (CAM).\n\nOn 16 March 1995, the BC government announced the approval of regulations governing midwifery and establishing the College of Midwives of BC. In 1996, the Health Professional Council released a draft of Bylaws for the College of Midwives of BC which received Cabinet approval on 13 April 1997. In 1998, midwives were officially registered with the College of Midwives of BC.\n\nIn BC, midwives are primary care providers for women in all stages of pregnancy, from prenatal to six weeks postpartum. Midwives also care for newborns. To see the approximate proportion of women whose primary birth attendant was a midwife in British Columbia see, \"What Mothers Say: The Canadian Maternity Experiences Survey. Public Health Agency of Canada. Ottawa, 2009, p. 115.\n\nMidwives in BC can deliver natural births in hospitals or homes. If a complication arises in a pregnancy, labour, birth or postpartum, a midwife will consult with a specialist such as an obstetrician or paediatrician.\nCore competencies and restricted activities are included in the BC Health Professions Act Midwives Regulation.\nAs of April 2009, the scope of practice for midwives allows them to prescribe certain prescription drugs, use acupuncture for pain relief, assist a surgeon in a caesarean section delivery and to perform a vacuum extraction delivery. These specialized practices require additional education and certification.\n\nAs of November 2015, the College of Midwives of British Columbia reported 247 General, 2 Temporary, 46 Non-practicing Registrant midwives. There were 2 midwives per 100,000 people in BC in 2006.\n\nA midwife must be registered with the College of Midwives of BC in order to practice. To continue licensure midwives must maintain regular recertification in neonatal resuscitation and management of maternal emergencies, maintain the minimum volume of clinical care (40 women), participate in peer case reviews and continuing education activities.\n\nThe University of British Columbia (UBC) has a four-year Bachelor of Midwifery program. The UBC midwifery program is poised to double in size thanks to an increase in government funding. Graduation of students will increase to 20 per year.\n\nIn terms of proffesional associations, BC has both the College of Midwives of BC and the Midwives Association of BC\n\nEducation, training and regulation\n\nThe undergraduate midwifery programs are five-year full-time university programs (four years in midwifery schools after a first year of medical studies common with Medicine, Odontology and Pharmacy) leading to an accredited master's degree in midwifery (Diplôme d'Etat de Sage-Femme).\n\nMidwives in France must be registered with the Ordre des sages-femmes in order to practice midwifery and use the title 'sage-femme'.\n\nPractice\n\nMidwives (\"sage-femmes\" \"wise women\" or \"maïeuticien\"/\"maïeuticienne\") are independent practitioners, specialists in birth and women's medicine.\n\nProfessional associations/colleges\n\nEducation, training and regulation\n\nThe undergraduate midwifery programs are four-year full-time university programs, with an internship in the final year, leading to an honours bachelor's degree in midwifery (BSc (Hons) Midwifery). The postgraduate midwifery programs (for registered midwives) lead to master's degrees in midwifery (MSc Midwifery, MSc Midwifery Practice). There are also postgraduate midwifery programs (for registered general nurses who wish to become midwives) leading to a qualification in midwifery (Higher Diploma in Midwifery).\n\nMidwives must be registered with the Nursing and Midwifery Board of Ireland (NMBI) in order to practice midwifery and use the title 'midwife' or 'registered midwife'. \n\nEducation, training and regulation\n\nMidwifery was first regulated in 1868. Today midwives in Japan are regulated under the Act on Public Health Nurse, Midwife and Nurse (No. 203) established in 1948. Japanese midwives must pass a national certification exam. On 1 March 2003 the Japanese name of midwife officially converted to unisex name. Still, only women can take the national exam to be midwives.\n\nProfessional associations/colleges\n\nWhen a 16-year civil war ended in 1992, Mozambique's health care system was devastated and one in ten women were dying in childbirth. There were only 18 obstetricians for a population of 19 million. In 2004, Mozambique introduced a new health care initiative to train midwives in emergency obstetric care in an attempt to guarantee access to quality medical care during pregnancy and childbirth. The newly introduced midwives system now perform major surgeries including Cesareans and hysterectomies. \n\nAs the figures now stand, Mozambique is one of the few countries on track to achieve the MDG of reducing the maternal death rate by 75% by 2015.\n\nEducation, training and regulation\n\nThe undergraduate midwifery programs are four-year full-time university programs leading to a bachelor's degree in midwifery (HBO-bachelor Verloskunde). There are four colleges for midwifery in the Netherlands: in Amsterdam, Groningen, Rotterdam and Maastricht. Midwives are called \"vroedvrouw\" (knowledge woman), \"vroedmeester\" (knowledge master, male), or \"verloskundige\" (deliverance experts) in Dutch.\n\nPractice\n\nMidwives are independent specialists in physiologic birth. In the Netherlands, home birth is still a common practice, although rates have been declining during the past decades. Between 2005-2008, 29% of babies were delivered at home. This figure fell to 23% delivered at home between 2007-2010 according to Midwifery in the Netherlands, a 2012 pamphlet by The Royal Dutch Organization for Midwives. In 2014 it has dropped further to 13.4%.perined.nl/jaarboek2104.pdf.\n\nMidwives are generally organized as private practices, some of those are hospital-based. In-hospital outpatient childbirth is available in most hospitals. In this case, a woman's own midwife delivers the baby at the delivery room of a hospital, without intervention of an obstetrician. In all settings, midwives will transfer care to an obstetrician in case of a complicated childbirth or need for emergency intervention.\n\nApart from childbirth and immediate postpartum care, midwives are the first line of care in pregnancy control and education of mothers-to-be. Typical information that is given to mothers includes information about food, alcohol, life style, travel, hobbies, sex, etc. Some midwifery practices give additional care in the form of preconceptional care and help with fertility problems.\n\nAll care by midwives is legal and it is totally reimbursed by all insurance companies. This includes prenatal care, childbirth (by midwives or obstetricians, at home or in the hospital), as well as postpartum/postnatal care for mother and baby at home.\n\nProfessional associations/colleges\n\nMidwifery is a regulated profession with no connection to Nursing. Midwifery is a profession with a distinct body of knowledge and its own scope of practice, code of ethics and standards of practice. The midwifery profession has knowledge, skills and abilities to provide a primary complete maternity service to childbearing women on its own responsibility.\n\nEducation, training and regulation\n\nThe undergraduate midwifery programmes are three-year full-time (three trimesters per year) tertiary programmes leading to a bachelor's degree in midwifery (Bachelor of Midwifery or Bachelor of Health Science (Midwifery)). \nThese programmes are offered by Otago Polytechnic in Dunedin, Ara Institute of Canterbury (formally CPIT) in Christchurch, Waikato Institute of Technology in Hamilton and Auckland University of Technology (AUT) in Auckland. Several schools have satellite programmes such as Otago with a programme in Southland, Wanaka, Wellington, Palmerston North, Wanganui, and Wairarapa - and AUT with student cohorts in various sites in the upper North Island. The postgraduate midwifery programmes (for registered midwives) lead to postgraduate degrees or equivalent qualifications in midwifery (Postgraduate Certificate in Midwifery, Postgraduate Diploma in Midwifery, Master of Midwifery, PhD Professional Doctorate).\n\nThe Midwifery First Year of Practice Programme (MFYP) is a compulsory national programme for all New Zealand registered midwifery graduates, irrespective of work setting. The New Zealand College of Midwives (the NZCOM) is contracted by the funder, Health Workforce New Zealand (HWNZ), to provide the programme nationally in accordance with the programme specification.\n\nMidwives in New Zealand must be registered with the Midwifery Council of New Zealand in order to practice midwifery and use the title 'midwife'.\n\nPractice\n\nWomen may choose a midwife, a General Practitioner or an Obstetrician to provide their maternity care. About 78 percent choose a midwife (8 percent GP, 8 percent Obstetrician, 6 percent unknown). Midwives provide maternity care from early pregnancy to 6 weeks postpartum. The midwifery scope of practise covers normal pregnancy and birth. The midwife will either consult or transfer care where there is a departure from a normal pregnancy. Antenatal care is normally provided in clinics, and postnatal care is initially provided in the woman’s home. Birth can be in the home, a primary birthing unit, or a hospital. Midwifery care is fully funded by the Government. (GP care may be fully funded. Private obstetric care will incur a fee in addition to the government funding.)\n\nProfessional associations/colleges\n\nIncrease in midwifery education has led to advances in impoverished countries. In Somalia, 1 in 14 women die while giving birth. Senior reproductive and maternal health adviser at UNFPA, Achu Lordfred claims, “the severe shortage of skilled health personnel with obstetric and midwifery skills means the most have their babies delivered by traditional birth attendants. But, when complications arise, these women either die or develop debilitating conditions, such as obstetric fistula, or lose their babies.” UNFPA is striving to change these odds by opening seven midwifery schools and training 125 midwives so far.\n\nEducation, training and regulation\n\nTraining includes aspects of midwifery, general nursing, community nursing and psychiatry, and can be achieved as either a four-year degree or a four-year diploma.\n\nThe midwifery profession is regulated under the Nursing Act, Act No 3 of 2005. The South African Nursing Council (SANC) is the regulatory body of midwifery in South Africa. \n\nProfessional associations/colleges\n\nEducation, training and regulation\n\nThere are different levels of education for midwives: \n\nMidwives must be licensed by the Tanzania Nursing and Midwifery Council (TNMC) in order to practice as a 'registered midwife' or 'enrolled midwife'. TNMC ensure the quality midwifery education output, develop and reviews various guidelines and standards on midwifery professionals and monitor their implementation, monitor and evaluate midwifery education programs and approve such programs to meet the Council and international requirements. Also it establish standards of proficiencies for midwifery education.\n\nProfessional associations/colleges\n\nEducation, training and regulation\n\nThe undergraduate midwifery programs are three-year full-time university programs leading to honours bachelor's degrees in midwifery: BSc (Hons) Midwifery, Bachelor of Midwifery (Hons). The postgraduate midwifery programs (for registered midwives) lead to master's degrees in midwifery (MSc Midwifery, MSc Advanced Practice Midwifery). There are also undergraduate and postgraduate midwifery programs (for graduates with a relevant degree who wish to become midwives) leading to degrees or equivalent qualifications in midwifery (BSc (Hons) Midwifery, Bachelor of Midwifery (Hons), Graduate Diploma in Midwifery, Postgraduate Diploma in Midwifery, MSc Midwifery).\nMidwifery training consists of classroom-based learning provided by select universities in conjunction with hospital- and community-based training placements at NHS Trusts.\n\nMidwifery students do not pay tuition fees and are eligible for additional financial support while training. Funding varies depending on which country within the UK the student is located in: students are eligible for NHS bursaries in addition to a grant of 1,000 pounds a year, and neither has to be repaid. Shortened-course students, who are already registered adult nurses, have different funding arrangements, are employed by the local NHS Trust via the Strategic Health Authority (SHA), and are paid salaries. This varies between universities and SHAs, with some students being paid their pre-training salaries, while others are employed as a Band 5 and still others are paid a proportion of a Band 5 salary.\n\nMidwives must be registered with the Nursing and Midwifery Council in order to practice midwifery and use the title 'midwife' or 'registered midwife', and must also have a Supervisor of Midwives through their local supervising authority.\n\nPractice\n\nMidwives are practitioners in their own right in the United Kingdom. They take responsibility for the antenatal, intrapartum and postnatal care of women up until 28 days after the birth, or as required thereafter. Midwives are the lead health care professional attending the majority of births, whether at home, in a midwife-led unit or in a hospital (although most births in the UK occur in hospitals).\n\nIn December 2014 the National Institute for Health and Care Excellence updated its guidance regarding where women should give birth. The new guidance states that midwife-led units are safer than hospitals for women having straightforward (low risk) pregnancies. Its updated guidance also confirms that home birth is as safe as birth in a midwife-led unit or a traditional labour ward for the babies of low-risk pregnant women who have already had at least one child previously.\n\nMany midwives also work in the community. The role of community midwives includes making initial appointments with pregnant women, managing clinics, undertaking postnatal care in the home and attending home births. A community midwife typically has a pager, is responsible for a particular area and can be contacted by ambulance control when needed. Sometimes they are paged to help out in a hospital when there are insufficient midwives available.\n\nMost midwives work within the National Health Service, providing both hospital and community care, but a significant proportion work independently, providing total care for their clients within a community setting. However, recent government proposals to require insurance for all health professionals is threatening independent midwifery in England.\n\nMidwives are at all times responsible for the women they are caring for. They must know when to refer complications to medical staff, act as the women's advocate, and ensure that mothers retain choice and control over childbirth.\n\nMost practising midwives in the United Kingdom are female: men account for less than 0.5 per cent of midwives on the register of the NMC.\n\nProfessional associations/colleges\n\nEducation, training and regulation\n\nThere are accredited midwifery programs which lead to different professional midwifery credentials:\n\n\nAccording to each US state, a midwife must be licensed and/or authorized to practice midwifery.\n\nPractice\n\nMidwives work with women and their families in many settings. They generally support and encourage natural childbirth in all practice settings. Laws regarding who can practice midwifery and in what circumstances vary from state to state. Many states have birthing centers where a midwife may work individually or as a group, which provides additional clinical opportunities for student midwives.\n\nCPMs provide on-going care throughout pregnancy and continuous, hands-on care during labor, birth and the immediate postpartum period. They practice as autonomous health professionals working in a network of relationships with other maternity care professionals who can provide consultation and collaboration when needed. Although qualified to practice in any setting, they have particular expertise in providing care in homes and free-standing birth centers, and own or work in over half of the birth centers in the U.S. today.\n\nCNMs and CMs work in a variety of settings including private practices, hospitals, birth centers, health clinics, and home birth services. They supervise not only pregnancy, delivery and postpartum period care for women and their newborns, but also provide gynecological care for all women and have autonomy and prescriptive authority in most states. They manage women's healthcare from puberty through post-menopause. With appropriate training, they can also first assist in cesarean (operative) deliveries and perform limited ultrasound examinations. It is possible for CNMs/CMs practice independently of physicians, establishing themselves as health care providers in the community of their choice.\n\nProfessional associations/colleges\n\nMen rarely practice midwifery for cultural and historical reasons. In ancient Greece, midwives were required by law to have given birth themselves, which prevented men from joining their ranks. In 17th century Europe, some barber surgeons, all of whom were male, specialized in births, especially births requiring the use of surgical instruments. This eventually developed into a professional split, with women serving as midwives and men becoming obstetricians. Men who work as midwives are called \"midwives\" (or \"male midwives\", if it is necessary to identify them further) or \"accoucheurs\"; the term \"midhusband\" (based on a misunderstanding of the etymology of \"midwife\") is occasionally encountered, mostly as a joke. In previous centuries, they were called \"man-midwives\" in English.\n\nWilliam Smellie is credited with innovations on the shape of forceps. This invention corresponds with the development towards obstetrics. He advised male midwives to wear dresses in order to reduce controversy over having a man present at birth.\n\nAs of the 21st century, most developed countries allow men to train as midwives. However, it remains very rare. In the United Kingdom, even after the passing of the Sex Discrimination Act 1975, the Royal College of Midwives barred men from the profession until 1983. As of March 2016, there were between 113 and 137 registered male midwives, representing 0.6% of all practising midwives in the UK.\n\nIn the US, there remain a small, stable or minimally declining number of male midwives with full scope training (CNMs/CMs), comprising approximately 1% of the membership of the American College of Nurse-Midwives.\n\nIn some Southeast Asian cultures, some or even most of the traditional midwives are men.\n\n\n\nNotes\n\n"}
{"id": "42558734", "url": "https://en.wikipedia.org/wiki?curid=42558734", "title": "National Survey of Family Growth", "text": "National Survey of Family Growth\n\nThe National Survey of Family Growth (NSFG) is a survey conducted by the National Center for Health Statistics division of the Centers for Disease Control and Prevention to understand trends related to fertility, family structure, and demographics in the United States.\n\nThe National Survey of Family Growth is conducted in five-year cycles. In each cycle, surveys are administered via personal interviews with people at homes. The interviewees generally comprise only the civilian, non-institutionalized population.\n\nThe cycles so far have been:\n\n\nWhile Cycles 1-5 surveyed only women, Cycle 6 and later surveyed both men and women and used households as the unit of analysis. Cycle 6 surveyed 12,571 respondents 15–44 years of age: 7,643 females and 4,928 males. The 2006-2010 NSFG surveyed 22,682 interviews: over 10,000 interviews with men and more than 12,000 interviews with women. For Cycle 6 onward, the surveys were conducted in person by female interviewers who are hired and managed by the University of Michigan Institute for Social Research. The survey samples are intended to be nationally representative but not necessarily representative at subnational levels (such as individual states, ethnicities, or religions).\n\nFor the survey cycles that have been completed, data is available both in the form of portable document format summaries and as full data files. In addition, program statements are available in SAS, SPSS, and STATA.\n\nKey statistics are also browsable online.\n\nNSFG data is also mirrored on the website of the University of Michigan's Inter-university Consortium for Political and Social Research.\n\nThe NSFG website claims that the NSFG is used as follows:\n\n\nThe NSFG website claims that the NSFG has been cited in \"more than 600 journal articles, NCHS reports, and book chapters shown in our publication lists.\" The research citing the NSFG is concentrated more on topics related to family planning, contraception, abortion, and fertility.\n\n"}
{"id": "64073", "url": "https://en.wikipedia.org/wiki?curid=64073", "title": "New World Syndrome", "text": "New World Syndrome\n\nNew World Syndrome is a set of non-communicable diseases brought on by consumption of junk food and a sedentary lifestyle, especially common to the indigenous peoples of the \"New World\" (i.e. of the Americas). Indigenous peoples of Oceania and Circumpolar peoples, and perhaps other populations of Asiatic origin are similarly affected and perhaps genetically predisposed. It is characterized by obesity, heart disease, diabetes, hypertension, and shortened life span.\n\nNew World Syndrome is linked to a change from a traditional diet and exercise to a Western diet and a sedentary lifestyle. Along with the lack of money. Traditional occupations of indigenous people—such as fishing, farming, and hunting—tended to involve constant activity, whereas modern office jobs do not. The introduction of modern transportation such as automobiles also decreased physical exertion. Meanwhile, Western foods which are rich in fat, salt, sugar, and refined starches are also imported into countries. The amount of carbohydrates in diets increases.\n\n\nRelated:\n\n"}
{"id": "27024757", "url": "https://en.wikipedia.org/wiki?curid=27024757", "title": "Nutritional neuroscience", "text": "Nutritional neuroscience\n\nNutritional neuroscience is the scientific discipline that studies the effects various components of the diet such as minerals, vitamins, protein, carbohydrates, fats, dietary supplements, synthetic hormones, and food additives have on neurochemistry, neurobiology, behavior, and cognition.\n\nRecent research on nutritional mechanisms and their effect on the brain show they are involved in almost every facet of neurological functioning including alterations in neurogenesis, neurotrophic factors, neural pathways and neuroplasticity, throughout the life cycle.\n\nRelatively speaking, the brain consumes an immense amount of energy in comparison to the rest of the body. The human brain is approximately 2% of the human body mass and uses 20–25% of the total energy expenditure. Therefore, mechanisms involved in the transfer of energy from foods to neurons are likely to be fundamental to the control of brain function. Insufficient intake of selected vitamins, or certain metabolic disorders, affect cognitive processes by disrupting the nutrient-dependent processes within the body that are associated with the management of energy in neurons, which can subsequently affect neurotransmission, synaptic plasticity, and cell survival.\n\nDeficiency or excess of essential minerals (e.g. iron, zinc, copper, and magnesium) can disrupt brain development and neurophysiology to affect behavior. Furthermore, minerals have been implicated in the pathophysiology of neurodegenerative diseases including Alzheimer's dementia.\n\nIron is essential for several critical metabolic enzymes and a deficiency of this mineral can disrupt brain development. For, example chronic marginal iron affects dopamine metabolism and myelin fatty acid composition and behavior in mice. In rats a marginal iron deficiency that does not cause anemia disrupted axon growth in the auditory nerve affecting auditory brainstem latency without major changes in myelination. In rhesus macaques, prenatal iron deficiency disrupts emotional behavior and polymorphisms that reduce the expression of monoamine oxidase interact with gestational iron deficiency to exacerbate the response to a stressful situation leading to increased aggressiveness. Inexpensive and effective iron supplementation is an available preventative strategy recommended by the World Health Organization. However, iron supplementation can exacerbate malaria infection. Therefore, individuals receiving iron supplementation in malaria-endemic areas must be carefully monitored.\n\nZinc is essential for the structure and function of thousands of proteins critical for the function of every cell. Zinc can also serve as a neurotransmitter in the brain, thus a deficiency of this mineral can clearly disrupt development as well as neurophysiology. For example, zinc deficiency during early development impairs neurogenesis leading to memory impairments. However, zinc deficiency later in life can disrupt appetite and cause depression like behavior. However, it is important to consider copper intake relative to zinc supplementation because excess zinc can disrupt copper absorption.\n\nConservative estimates suggest that 25% of the world's population is at risk of zinc deficiency.\n\nHypozincemia is usually a nutritional deficiency, but can also be associated with malabsorption, diarrhea, acrodermatitis enteropathica, chronic liver disease, chronic renal disease, sickle-cell disease, diabetes, malignancy, pyroluria, and other chronic illnesses. It can also occur after bariatric surgery, heavy metal exposure and tartrazine. \n\nZinc deficiency is typically the result of inadequate dietary intake of zinc, disease states that promote zinc losses, or physiological states that require increased zinc. Populations that consume primarily plant based diets that are low in bioavailable zinc often have zinc deficiencies. Diseases or conditions that involve intestinal malabsorption promote zinc losses. Fecal losses of zinc caused by diarrhea are one contributing factor, often common in developing countries. Changes in intestinal tract absorbability and permeability due, in part, to viral, protozoal, and bacteria pathogens may also encourage fecal losses of zinc. Physiological states that require increased zinc include periods of growth in infants and children as well as in mothers during pregnancy.\n\nZinc deficiency may cause a decrease in appetite which can degenerate into anorexia or anorexia nervosa. Appetite disorders, in turn, cause malnutrition and, notably, inadequate zinc intake. Anorexia itself is a cause of zinc deficiency, thus leading to a vicious cycle: the worsening of anorexia worsens the zinc deficiency. A 1994 randomized, double-blind, placebo-controlled trial showed that zinc (14 mg per day) doubled the rate of body mass increase in the treatment of anorexia nervosa.\n\nCognitive and motor function may also be impaired in zinc deficient children. Zinc deficiency can interfere with many organ systems especially when it occurs during a time of rapid growth and development when nutritional needs are high, such as during infancy. In animal studies, rats who were deprived of zinc during early fetal development exhibited increased emotionality, poor memory, and abnormal response to stress which interfered with performance in learning situations. Zinc deprivation in monkeys showed that zinc deficient animals were emotionally less mature, and also had cognitive deficits indicated by their difficulty in retaining previously learned problems and in learning new problems. Human observational studies show weaker results. Low maternal zinc status has been associated with less attention during the neonatal period and worse motor functioning. In some studies, supplementation has been associated with motor development in very low birth weight infants and more vigorous and functional activity in infants and toddlers.\n\nPlasma zinc level has been associated with many psychological disorders. However, the nature of this relationship remains unclear in most instances. An increasing amount of evidence suggests that zinc deficiency could play a causal role in the etiology of depression. Indeed, zinc supplementation has been reported to improve measures of depression in randomized double blind placebo controlled trials.\n\nCopper is important for the function of many enzymes in the brain. Notably, dopamine β-mono-oxygenase is affected by copper deficiency leading to increased dopamine and decreased norepinephrine levels. Both copper deficiency and toxicity can interfere with brain development and function.\n\nThe neurodegenerative syndrome of copper deficiency has been recognized for some time in ruminant animals, in which it is commonly known as \"swayback\". The disease involves a nutritional deficiency in the trace element copper. Copper is ubiquitous and daily requirement is low making acquired copper deficiency very rare. Copper deficiency can manifest in parallel with vitamin B12 and other nutritional deficiencies. The most common cause of copper deficiency is a remote gastrointestinal surgery, such as gastric bypass surgery, due to malabsorption of copper, or zinc toxicity. On the other hand, Menkes disease is a genetic disorder of copper deficiency involving a wide variety of symptoms that is often fatal.\n\nCopper deficiency can cause a wide variety of neurological problems including, myelopathy, peripheral neuropathy, and optic neuropathy.\n\nSufferers typically present difficulty walking (gait difficulty) caused by sensory ataxia (irregular muscle coordination) due to dorsal column dysfunction or degeneration of the spinal cord (myelopathy). Patients with ataxic gait have problems balancing and display an unstable wide walk. They often feel tremors in their torso, causing side way jerks and lunges.\n\nIn brain MRI, there is often an increased T2 signalling at the posterior columns of the spinal cord in patients with myelopathy caused by copper deficiency. T2 signalling is often an indicator of some kind of neurodegeneration. There are some changes in the spinal cord MRI involving the thoracic cord, the cervical cord or sometimes both. Copper deficiency myelopathy is often compared to subacute combined degeneration (SCD). Subacute combined degeneration is also a degeneration of the spinal cord, but instead vitamin B12 deficiency is the cause of the spinal degeneration. SCD also has the same high T2 signalling intensities in the posterior column as copper deficient patient in MRI imaging.\n\nAnother common symptom of copper deficiency is peripheral neuropathy, which is numbness or tingling that can start in the extremities and can sometimes progress radially inward towards the torso. In an Advances in Clinical Neuroscience & Rehabilitation (ACNR) published case report, a 69-year-old patient had progressively worsened neurological symptoms. These symptoms included diminished upper limb reflexes with abnormal lower limb reflexes, sensation to light touch and pin prick was diminished above the waist, vibration sensation was lost in the sternum, and markedly reduced proprioception or sensation about the self's orientation. Many people suffering from the neurological effects of copper deficiency complain about very similar or identical symptoms as the patient. This numbness and tingling poses danger for the elderly because it increases their risk of falling and injuring themselves. Peripheral neuropathy can become very disabling leaving some patients dependent on wheel chairs or walking canes for mobility if there is lack of correct diagnosis. Rarely can copper deficiency cause major disabling symptoms. The deficiency will have to be present for an extensive amount of time until such disabling conditions manifest.\n\nSome patients suffering from copper deficiency have shown signs of vision and color loss. The vision is usually lost in the peripheral views of the eye. The bilateral vision loss is usually very gradual. An optical coherence tomography (OCT) shows some nerve fiber layer loss in most patients, suggesting the vision loss and color vision loss was secondary to optic neuropathy or neurodegeneration.\n\nCopper toxicity can occur from excessive supplement use, eating acid foods cooked in uncoated copper cookware, exposure to excess copper in drinking water, or as the result of an inherited metabolic disorder in the case of Wilson's disease. A significant portion of the toxicity of copper comes from its ability to accept and donate single electrons as it changes oxidation state. This catalyzes the production of very reactive radical ions, such as hydroxyl radical in a manner similar to Fenton chemistry. This catalytic activity of copper is used by the enzymes with which it is associated, thus is only toxic when unsequestered and unmediated. This increase in unmediated reactive radicals is generally termed oxidative stress, and is an active area of research in a variety of diseases where copper may play an important but more subtle role than in acute toxicity.\n\nSome of the effects of aging may be associated with excess copper. In addition, studies have found that people with mental illnesses, such as schizophrenia, had heightened levels of copper in their systems. However, it is unknown at this stage whether the copper contributes to the mental illness, whether the body attempts to store more copper in response to the illness, or whether the high levels of copper are the result of the mental illness.\n\nElevated free copper levels exist in Alzheimer's disease. Copper and zinc are known to bind to amyloid beta proteins in Alzheimer's disease. This bound form is thought to mediate the production of reactive oxygen species in the brain. A preliminary clinical trial suggests that zinc supplementation may be able to decrease copper levels and slow degeneration in Alzheimer's disease.\n\nManganese is a component of some enzymes and stimulates the development and activity of other enzymes. Manganese superoxide dismutase (MnSOD) is the principal antioxidant in mitochondria. Several enzymes activated by manganese contribute to the metabolism of carbohydrates, amino acids, and cholesterol.\n\nDeficiency of manganese causes skeletal deformation in animals and inhibits the production of collagen in wound healing. On the other hand, manganese toxicity is associated with neurological complications.\n\nManganese poisoning is a toxic condition resulting from chronic exposure to manganese and first identified in 1837 by James Couper.\n\nChronic exposure to excessive Mn levels can lead to a variety of psychiatric and motor disturbances, termed manganism. Generally, exposure to ambient Mn air concentrations in excess of 5 mg Mn/m3 can lead to Mn-induced symptoms.\n\nIn initial stages of manganism, neurological symptoms consist of reduced response speed, irritability, mood changes, and compulsive behaviors. Upon protracted exposure symptoms are more prominent and resemble those of idiopathic Parkinson's disease, as which it is often misdiagnosed, although there are particular differences in both the symptoms (nature of tremors, for example), response to drugs such as levodopa, and affected portion of the basal ganglia. Symptoms are also similar to Lou Gehrig's disease and multiple sclerosis.\n\nManganism has become an active issue in workplace safety as it has been the subject of numerous product liability lawsuits against manufacturers of arc welding supplies. In these lawsuits, welders have accused the manufacturers of failing to provide adequate warning that their products could cause welding fumes to contain dangerously high manganese concentrations that could lead welders to develop manganism. Companies employing welders are also being sued, for what colloquially is known as \"welders' disease\". However, studies fail to show any link between employment as a welder and manganism (or other neurological problems).\n\nManganism is also documented in reports of illicit methcathinone manufacturing. This is due to manganese being a byproduct of methcathinone synthesis if potassium permanganate is used as an oxidiser. Symptoms include apathy, bradykinesia, gait disorder with postural instability, and spastic-hypokinetic dysarthria. Another street drug sometimes contaminated with manganese is the so-called \"Bazooka\", prepared by free-base methods from cocaine using manganese carbonate.\n\nReports also mention such sources as contaminated drinking water, and fuel additive methylcyclopentadienyl manganese tricarbonyl (MMT), which on combustion becomes partially converted into manganese phosphates and sulfate that go airborne with the exhaust, and manganese ethylene-bis-dithiocarbamate (Maneb), a pesticide.\n\nManganese may affect liver function, but the threshold of acute toxicity is very high. On the other hand, more than 95% of manganese is eliminated by biliary excretion. Any existing liver damage may slow this process, increasing its concentration in blood plasma. The exact neurotoxic mechanism of manganese is uncertain but there are clues pointing at the interaction of manganese with iron, zinc, aluminum, and copper. Based on a number of studies, disturbed iron metabolism could underlie the neurotoxic action of manganese.\n\nIt participates in Fenton reactions and could thus induce oxidative damage, a hypothesis corroborated by the evidence from studies of affected welders. A study of the exposed workers showed that they have significantly fewer children. This may indicate that long-term accumulation of manganese affects fertility. Pregnant animals repeatedly receiving high doses of manganese bore malformed offspring significantly more often compared to controls. Manganism mimics Schizophrenia. It is found in large quantities in paint and steelmaking.\n\nThe current mainstay of manganism treatment is levodopa and chelation with EDTA. Both have limited and at best transient efficacy. Replenishing the deficit of dopamine with levodopa has been shown to initially improve extrapyramidal symptoms, but the response to treatment goes down after 2 or 3 years, with worsening condition of the same patients noted even after 10 years since last exposure to manganese. Enhanced excretion of manganese prompted by chelation therapy brings its blood levels down but the symptoms remain largely unchanged, raising questions about efficacy of this form of treatment.\n\nIncreased ferroportin protein expression in human embryonic kidney (HEK293) cells is associated with decreased intracellular Mn concentration and attenuated cytotoxicity, characterized by the reversal of Mn-reduced glutamate uptake and diminished lactate dehydrogenase (LDH) leakage.\n\nThe Red River Delta near Hanoi has high levels of manganese or arsenic in the water. Approximately 65 percent of the region’s wells contain high levels of arsenic, manganese, selenium and barium. This was also published in the Proceedings of the National Academy of Sciences.\n\nMagnesium is necessary for the function of many metabolic enzymes and also serves as a key regulator of calcium channels involved in neurotransmission (e.g. NMDA receptor). Magnesium supplementation facilitates nerve regeneration after injury. Although unpolished grains contain magnesium, phytic acid in grains can inhibit its absorption. Leafy greens are an excellent source of magnesium.\n\nDeficiency or excess intake of many vitamins can affect the brain contributing to developmental and degenerative diseases.\n\nVitamin A is an essential nutrient for mammals which takes form in either retinol or the provitamin beta-Carotene. It helps regulation of cell division, cell function, genetic regulation, helps enhance the immune system, and is required for brain function, chemical balance, growth and development of the central nervous system and vision.\n\nIn an experiment by Chongqing Medical University pregnant rats were either plentiful in vitamin A or were of a vitamin A deficiency (VAD) due to their diet. The offspring of these rats were then tested in a water maze at 8 weeks old and it was found the VAD offspring had a harder time finishing the maze which helps show that these rats, even while having a deficiency from \"in utero\", have more problems with learning memory. Young rats in a separate study by the same university also showed impaired long-term potentiation in the hippocampus when they were VAD which shows neuronal impairment. When the patient is VAD for too long, the effects of the damage to the hippocampus can be irreversible.\n\nVitamin A affects spatial memory most of the time because the size of the nuclei in hippocampal neurons are reduced by approximately 70% when there is a deficiency which affects a person's abilities for higher cognitive function. In a study by the University of Cagliari, Italy, VAD rats had more trouble learning a Radial arm maze than rats who had normal levels of the vitamin. The healthy rats were able to correctly solve the maze within the 15-day training period and other rats that were once deficient but had vitamin A restored to normal levels were also able to solve it. Here it was found that the retinoid receptors which help transport vitamin A were of normal function.\n\nEating foods high in vitamin A or taking dietary supplements, retinol or retinal will prevent a deficiency. The foods highest in vitamin A are any pigmented fruits and vegetables and leafy green vegetables also provide beta-Carotene.\nThere can be symptoms of fat loss and a reduction of any weight gain that would be considered normal for an individual, especially developmental weight gains such as in infants which would occur if the infant was deprived of vitamin A in utero and/or if it was deprived postnatal for an extensive period of time. The deficiency can also cause conditions such as blindness or night blindness, also known as nyctalopia. Night blindness is due to the inability to regenerate rhodopsin in the rods which is needed in dim light in order to see properly.\nA treatment of supplements of retinoic acid which is a part of vitamin A can help replenish levels and help bring learning to normal, but after 39 weeks this is ineffective even if the treatment is daily because it will not bring the retinoid hypo-signalling back to normal.\n\nZinc is needed to maintain normal vitamin A levels in blood plasma. It also helps vitamin A become metabolized by the liver. However evidence suggests that when someone is deficient in both vitamin A and zinc, memory is more improved when just vitamin A is increased than when just zinc is increased. Of course memory has the largest improvement when both are increased. When one of these nutrients is not balanced, the other is most likely to be affected because they rely on each other for proper functioning in learning.\n\nVitamin B, also known as thiamine, is a coenzyme essential for the metabolism of carbohydrates. This vitamin is important for the facilitation of glucose use, thus ensuring the production of energy for the brain, and normal functioning of the nervous system, muscles, and heart.\n\nThiamine is found in all living tissues, and is uniformly distributed throughout mammalian nervous tissue, including the brain and spinal cord. Metabolism and coenzyme function of the vitamin suggest a distinctive function for thiamin within the nervous system.\n\nThe brain retains its thiamine content in the face of a vitamin-deficient diet with great tenacity, as it is the last of all nervous tissues studied to become depleted. A 50% reduction of thiamine stores in rats becomes apparent after only 4 days of being put on a thiamine-deficient diet. However, polyneuritic signs do not begin to appear until about 4 or 5 weeks have passed. Similar results have been found in human subjects.\n\nThe body has only small stores of B; accordingly, there is risk of deficiency if the level of intake is reduced only for a few weeks. Thiamin deficiency during critical periods of early development can disrupts neurogenesis in animal models. Lack of thiamin later in life causes the disease known as beriberi. There are two forms of beriberi: \"wet\", and \"dry\". Dry beriberi is also known as cerebral beriberi. Characteristics of wet beriberi include prominent edema and cardiac involvement, whereas dry beriberi is mainly characterized by a polyneuritis. Severe thiamin deficiency can also result in acute neurodegeneration leading to peripheral neuropathy and memory loss.\n\nIn industrialized nations, thiamine deficiency is a clinically significant problem in individuals with chronic alcoholism or other disorders that interfere with normal ingestion of food. Thiamine deficiency within developed nations tends to manifest as Wernicke–Korsakoff syndrome. Chronic alcoholism can disrupt thiamin absorption and thiamin deficiency contributes to neurodegeneration and memory loss in alcoholics known as Wernicke's encephalopathy. Individuals with chronic alcoholism may fall short on minimum daily requirements of thiamine in part due to anorexia, erratic eating habits, lack of available food, or a combination of any of these factors. Thiamine deficiency has been reported in up to 80% of alcoholic patients due to inadequate nutritional intake, reduced absorption, and impaired utilization of thiamine. Alcohol, in combination with its metabolite acetaldehyde, interacts with thiamine utilization at the molecular level during transport, diphosphorylation, and modification processes. For this reason, chronic alcoholics may have insufficient thiamine for maintenance of normal brain function, even with seemingly adequate dietary intake.\n\nClinical signs of B deficiency include mental changes such as apathy, decrease in short-term memory, confusion, and irritability. Moderate deficiency in thiamine may reduce growth in young populations, in increase chronic illness in both young and middle-aged adults. In addition, moderate deficiency of thiamine may increase rates of depression, dementia, falls, and fractures in old age.\n\nThe lingering symptoms of neuropathy associated with cerebral beriberi are known as Korsakoff's syndrome, or the chronic phase of Wernicke-Korsakoff's. Wernicke encephalopathy is a neurological disorder resulting from a deficiency in thiamine, sharing the same predominant features of cerebral beriberi, as characterized by ocular abnormalities, ataxia of gait, a global state of confusion, and neuropathy. The state of confusion associated with Wernicke's may consist of apathy, inattention, spatial disorientation, inability to concentrate, and mental sluggishness or restlessness. Clinical diagnosis of Wernicke's disease cannot be made without evidence of ocular disturbance, yet these criteria may be too rigid. Korsakoff's likely represents a variation in the clinical manifestation of Wernicke encephalophathy, as they both share similar pathological origin.\n\nKorsakoff's syndrome is often characterized by confabulation, disorientation, and profound amnesia. Characteristics of the neuropathology are varied, but generally consist of bilaterally symmetrical midline lesions of brainstem areas, including the mammillary bodies, thalamus, periaqueductal region, hypothalamus, and the cerebellar vermis.\n\nImmediate treatment of Wernicke encephalopathy involves the administration of intravenous thiamine, followed with long-term treatment and prevention of the disorder through oral thiamine supplements, alcohol abstinence, and a balanced diet. Improvements in brain functioning of chronic alcoholics may occur with abstinence-related treatment, involving the discontinuation of alcohol consumption and improved nutrition. Wernicke's encephalopathy is life-threatening if left untreated. However, a rapid reversal of symptoms may result from prompt administration of thiamine.\n\nFortification of flour is practiced in some countries to replace the thiamine lost during processing. However, this method has been criticized for missing the target population of chronic alcoholics, who are most at risk for deficiency. Alternative solutions have suggested the fortification of alcoholic beverages with thiamine.\n\nIngesting a diet rich in thiamine may stave off the adverse effects of deficiency. Foods providing rich sources of thiamine include unrefined grain products, ready-to-eat cereals, meat (especially pork), dairy products, peanuts, legumes, fruits and eggs.\n\nVitamin B, also known as niacin, includes both nicotinamide as well as nicotinic acid, both of which function in many biological oxidization and reduction reactions within the body. These functions include the biochemical degradation of carbohydrates, fats and proteins. Niacin is also involved in the synthesis of fatty acids and cholesterol, which are known mediators of brain biochemistry, and in effect, of cognitive function.\n\nSufficient niacin intake is either obtained from diet, or synthesized from the amino acid tryptophan.\n\nSevere niacin deficiency typically manifests itself as the disease pellagra. Synthesis of B from tryptophan involves vitamin B and B, so deficiencies in either of these nutrients can lead to niacin deficiency. An excess of leucine, an essential amino acid, in the diet can also interfere with tryptophan conversion and subsequently result in a B deficiency.\n\nPellagra is most common to populations within developing countries in which corn is the dietary staple. The disease has virtually disappeared from industrialized countries, yet still appears in India and parts of China and Africa. This is in part due to the bound form of niacin that unprocessed corn contains, which is not readily absorbed into the human body. The processes involved in making corn tortillas, can release the bound niacin into a more absorbable form. Pellagra is not problematic in countries which traditionally prepare their corn in this way, but is a problem in other countries where unprocessed corn is main source of caloric intake.\n\nThough pellagra predominantly occurs in developing countries, sporadic cases of pellagra may be observed within industrialized nations, primarily in chronic alcoholics and patients living with functional absorption complications.\n\nPellagra is classically characterized by four 4 \"D's\": diarrhea, dermatitis, dementia, and death. Neuropsychiatric manifestations of pellagra include headache, irritability, poor concentration, anxiety, hallucinations, stupor, apathy, psychomotor unrest, photophobia, tremor, ataxia, spastic paresis, fatigue, and depression. Symptoms of fatigue and insomnia may progress to encephalopathy characterized by confusion, memory loss, and psychosis.\n\nThose afflicted with pellagra may undergo pathological alterations in the nervous system. Findings may include demylenation and degeneration of various affected parts of the brain, spinal cord, and peripheral nerves.\n\nPrognosis of deficiency is excellent with treatment. Without, pellagra will gradually progress and lead to death within 4–5 years, often a result of malnutrition from prolonged diarrhea, or complications as caused by concurrent infections or neurological symptoms. Symptoms of pellagra can be cured with exogenous administration of nicotinic acid or nicotinamide.\n\nFlushing occurs in many patients treated therapeutically with nicotinic acid, and as a result, nicotinamide holds more clinical value as it is not associated with the same uncomfortable flushing. The adult dose of nicotinamide is 100 mg taken orally every 6 hours until resolution of major acute symptoms, followed with oral administration of 50 mg every 8–12 hours until skin lesions heal. For children, treatment involves oral ingestion of 10–15 mg of nicotinamide, depending on weight, every 6 hours until signs and symptoms are resolved. Severe cases require 1 gram every 3–4 hours, administered parenterally.\n\nOral nicotinamide has been promoted as an over-the-counter drug for the treatment of Alzheimer's dementia. Conversely, no clinically significant effect has been found for the drug, as nicotinamide administration has not been found to promote memory functions in patients with mild to moderate dementia of either Alzheimer's, vascular, or fronto-temporal types. This evidence suggests that nicotinamide may treat dementia as related to pellagra, but administration does not effectively treat other types of dementia.\n\nThe best method of prevention is to each food rich in B. Generally, this involves the intake of a protein-rich diet. Foods that contain high concentrations of niacin in the free form include beans and organ meat, as well as enriched grain and cereal products. While niacin is present in corn and other grains, the bioavailability of the nutrient is much less than it is in protein-rich sources. Different methods of processing corn may result in a higher degree of bioavailability of the vitamin.\n\nThough treatment with niacin does little to alter the effects of Alzheimer's dementia, niacin intake from foods is inversely associated with the disease.\n\nFolate deficiency can disrupt neurulation and neurogenesis. Maternal folic acid intake around the time of conception prevents neural tube defects. Furthermore, folic acid intake was recently associated with incidence of autism. Enriched white flour is fortified with folic acid in the United States and many other countries. However the European Union does not have mandatory folic acid fortification. Although the protective effects of folic acid are well documented, there remains legitimate concern that fortification could lead to toxic levels in a subset of the population. For example, elevated levels of folic acid can interact with vitamin B12 deficiency to cause neurodegeneration. Furthermore, folic acid and iron can interact to exacerbate malaria.\n\nFolic acid is the most oxidized and stable form of folate, and can also be referred to as vitamin B. It rarely occurs naturally in foods, but it is the form used in vitamin supplements as well as fortified food products.\n\nFolate coenzymes are involved in numerous conversion processes within the body, including DNA synthesis and amino acid interconversions. Folate and vitamin B play a vital role in the synthesis of S-adenosylmethionine, which is of key importance in the maintenance and repairment of all cells, including neurons. In addition, folate has been linked to the maintenance of adequate brain levels of cofactors necessary for chemicals reactions that lead to the synthesis of serotonin and catecholamine neurotransmitters.\n\nFolate has a major, but indirect role in activities which help to direct gene expression and cell proliferation. These activities occur at a greatly increased rate during pregnancy, and depend on adequate levels of folate within blood plasma.\n\nConcentrations of blood plasma folate and homocysteine concentrations are inversely related, such that an increase in dietary folate decreases homocysteine concentration. Thus, dietary intake of folate is a major determinant of homocysteine levels within the body.\n\nAutoantibodies against folate receptor alpha have been found in up to 75% of children with autism.\n\nFolate deficiency most commonly arises from insufficient folate intake from the diet, but may also stem from inefficient absorption or metabolic utilization of folate, usually a result of genetic variation. The relationship between folate and B is so interdependent that deficiency in either vitamin can result in megaloblastic anemia, characterized by organic mental change.\n\nThe process of neural tube transformation into structures that will eventually develop into the central nervous system is known as neurulation, the success of which is dependent on the presence of folate within the body. This process begins in the human approximately 21 days after conception, and is completed by 28 days. Thus, a woman may not even be aware of her pregnancy by the time the process of neurulation is complete, potentially causing severe consequences in the development of the fetus.\n\nFunctional problems in the absorption and utilization of vitamins may also play a role in folate deficiencies within the elderly.\n\nThe link between levels of folate and altered mental function is not large, but is sufficient enough to suggest a causal association. Deficiency in folate can cause an elevation of homocysteine within the blood, as the clearance of homocysteine requires enzymatic action dependent on folate, and to a lesser extent, vitamins B and B. Elevated homocysteine has been associated with increased risk of vascular events, as well as dementia.\n\nDifferences lie in the presentation of megaloblastic anemia induced by either folate or B deficiency. Megaloblastic anemia related to deficiency in B generally results in peripheral neuropathy, whereas folate-related anemia often results in affective, or mood disorders. Neurological effects are not often associated with folate-related megaloblastic anemia, although demyelinating disorders may eventually present. In one study, mood disturbances were recorded for the majority of patients presenting with megaloblastic anemia in the absence of B deficiency. In addition, folate concentrations within blood plasma have been found to be lower in patients with both unipolar and bipolar depressive disorders when compared with control groups. In addition, depressive groups with low folate concentrations responded less well to standard antidepressant therapy than did those with normal levels within plasma. However, replication of these findings are less robust.\n\nThe role of folic acid during pregnancy is vital to normal development of the nervous system in the fetus. A deficiency in folate levels of a pregnant woman could potentially result in neural tube disorder, a debilitating condition in which the tubes of the central nervous system do not fuse entirely. NTDs are not to be confused with spina bifida, which does not involve neural elements. Neural tube defects can present in a number of ways as a result of the improper closure at various points of the neural tube. The clinical spectrum of the disorder includes encephalocele, craniorachischisis, and anencephaly. In addition, these defects can also be classified as open, if neural tissue is exposed or covered only by membrane, or can be classified as closed, if the tissue is covered by normal skin.\n\nIntake of the vitamin has been linked to deficits in learning and memory, particularly within the elderly population. Elderly people deficient in folate may present with deficits in free recall and recognition, which suggests that levels of folate may be related to efficacy of episodic memory.\n\nLack of adequate folate may produce a form of dementia considered to be reversible with administration of the vitamin. Indeed, there is a degree of improvement in memory associated with folate treatment. In a 3-year longitudinal study of men and women aged 50–70 years with elevated homocysteine plasma concentration, researchers found that a daily oral folic acid supplementation of 800μg resulted in an increase in folate levels and a decrease in homocysteine levels within blood plasma. In addition to these results, improvements of memory, and information-processing speed, as well as slight improvements of sensorimotor speed were observed, which suggests there is a link between homocysteine and cognitive performance.\n\nHowever, while the amount of cognitive improvement after treatment with folate is correlated with the severity of folate deficiency, the severity of cognitive decline is independent of the severity of folate deficiency. This suggests that the dementia observed may not be entirely related to levels of folate, as there could be additional factors that were not accounted for which might have an effect.\n\nBecause neurulation may be completed before pregnancy is recognized, it is recommended that women capable of becoming pregnant take about 400μg of folic acid from fortified foods, supplements, or a combination of the two in order to reduce the risk of neural tube defects. These major anomalies in the nervous system can be reduced by 85% with systematic folate supplementation occurring before the onset of pregnancy.\n\nThe incidence of Alzheimer's and other cognitive diseases has been loosely connected to deficiencies in folate. It is recommended for the elderly to consume folate through food, fortified or not, and supplements in order to reduce risk of developing the disease.\nGood sources of folate include liver, ready-to-eat breakfast cereals, beans, asparagus, spinach, broccoli, and orange juice.\n\nCholine is an important methyl donor involved in one-carbon metabolism that also becomes incorporated into phospholipids and the neurotransmitter acetylcholine. Because of its role in cellular synthesis, choline is an important nutrient during the prenatal and early postnatal development of offspring as it contributes heavily to the development of the brain. A study found that rats that were given supplements of choline in utero or in the weeks following birth had superior memories. The changes appeared to be a result of physical changes to the hippocampus, the area of the brain responsible for memory. Furthermore, choline can reduce some of the deleterious effects of folate deficiency on neurogenesis.\n\nWhile choline during development is important, adult levels of choline are also important. Choline has been shown to increase the synthesis and release of acetylcholine from neurons, which in turn increases memory. A double-blind study was conducted using normal college students (no neurological disorders). Results showed that twenty-five grams of phosphatidylcholine (another form of choline) created a significant improvement in explicit memory, measured by a serial learning task, however this improvement may be attributed to the improvement of slow learners. Another study found that a single ten-gram oral dose of choline, given to normal volunteers (again, without neurological disorders) significantly decreased the number of trials needed to master a serial-learning word test. This increase in memory is particularly beneficial to memory loss suffered by old age. A study conducted on rats who, like humans, suffer from an age-related loss of memory were tested on how choline affected memory. The results showed that rats who had a chronic low-choline diet showed greater memory loss then their same-age control counterparts, while rats who had choline-enriched diets showed a diminished memory loss compared to both the choline-low diet and control rat groups. Furthermore, young rats who were choline-deficient performed as poorly on memory tasks as older rats while older rats that were given choline supplements performed as well as three-month-old rats.\n\nDespite the wide range of foods that choline is found in, studies have shown that the mean choline intake of men, women and children are below the Adequate Intake levels. It is important to note that not enough choline is naturally produced by the body, so diet is an important factor. People who consume a large quantity of alcohol may be at an increased risk for choline deficiency. Sex and age also plays a role, with premenopausal females being less sensitive to choline deficiency than either males or postmenopausal females. This has been theorized to be a result of premenopausal women having an increased ability to synthesize choline in some form, which has been confirmed in studies on rats. In such instances of deficiency, choline supplements or (if able) dietary changes may be beneficial. Good sources of choline include liver, milk, eggs and peanuts. There is further evidence to suggest that choline supplements can be used to treat people who suffer from neurological disorders as well we memory defects. Oral doses of CDP-choline (another form of choline) given to elderly subjects with memory deficits, but without dementia, for four weeks showed improved memory in free recall tasks, but not in recognition tests. In a second study, patients with early Alzheimer-type dementia were treated with twenty-give gram doses of phosphatidylcholine every day for six months. Slightly improvements were shown in memory tests compared to the placebo control group. It should be noted however that other studies conducted did not find any such improvement.\n\nAlso known as cobalamin, B is an essential vitamin necessary for normal blood formation. It is also important for the maintenance of neurological function and psychiatric health. The absorption of B into the body requires adequate amounts of intrinsic factor, the glycoprotein produced in the parietal cells of the stomach lining. A functioning small intestine is also necessary for the proper metabolism of the vitamin, as absorption occurs within the ileum.\n\nB is produced in the digestive tracts of all animals, including humans. Thus, animal-origin food is the only natural food source of vitamin B However, synthesis of B occurs in the large intestine, which is past the point of absorption that occurs within the small intestine. As such, vitamin B must be obtained through diet.\n\nUnlike other B vitamins which are not stored in the body, B is stored in the liver. Because of this, it may take 5–10 years before a sudden dietary B deficiency will become apparent in a previously healthy adult. B deficiency, also known as hypocobalaminemia, often results from complications involving absorption into the body.\n\nB deficiency is often associated with pernicious anemia, as it is the most common cause. Pernicious anemia results from an autoimmune disorder which destroys the cells that produce intrinsic factor within the stomach lining, thereby hindering B absorption. B absorption is important for the subsequent absorption of iron, thus, people with pernicious anemia often present with typical symptoms of anemia, such as pale skin, dizziness, and fatigue.\n\nAmong those at highest risk for B deficiency are the elderly population, as 10-15% of people aged 60+ may present with some form of hypocobalaminemia. High rates of deficiency in the elderly commonly results from the decrease of functional absorption of B, as production of intrinsic factor declines with age. However, pernicious anemia is the most common cause of B deficiency in North American and European populations.\n\nThose afflicted with various gastrointestinal diseases may also be at risk for deficiency as a result of malabsorption. These diseases may affect production of intrinsic factor in the stomach, or of pancreatic bile. Diseases that involve disorders of the small intestine, such as celiac disease, Crohn's disease and ileitis, may also reduce B absorption. For example, people with celiac disease may damage the microvilli within their small intestines through the consumption of gluten, thereby inhibiting absorption of B as well as other nutrients.\n\nA diet low in B, whether voluntary or not, can also cause symptoms of hypocobalaminemia. Many rich sources of B come from animal meats and by-products. Populations in developing countries may not have access to these foods on a consistent basis, and as a result may become deficient in B. In addition, vegans, and to a lesser extent vegetarians, are at risk for consuming a diet low in cobalamin as they voluntarily abstain from animal sources of B. A combination of these two scenarios may increase prevalence of cobalamin deficit. For instance, B deficiency is problematic in India, where the majority of the population is vegetarian and the scarcity of meat consumption is common for omnivores as well.\n\nAn assortment of neurological effects can be observed in 75-90% of individuals of any age with clinically observable B deficiency. Cobalamin deficiency manifestations are apparent in the abnormalities of the spinal cord, peripheral nerves, optic nerves, and cerebrum. These abnormalities involve a progressive degeneration of myelin, and may be expressed behaviourally through reports of sensory disturbances in the extremities, or motor disturbances, such as gait ataxia. Combined myelopathy and neuropathy are prevalent within a large percentage of cases. Cognitive changes may range from loss of concentration to memory loss, disorientation, and dementia. All of these symptoms may present with or without additional mood changes. Mental symptoms are extremely variable, and include mild disorders of mood, mental slowness, and memory defect. Memory defect encompasses symptoms of confusion, severe agitation and depression, delusions and paranoid behaviour, visual and auditory hallucinations, urinary and fecal incontinence in the absence of overt spinal lesions, dysphasia, violent maniacal behaviour, and epilepsy. It has been suggested that mental symptoms could be related to a decrease in cerebral metabolism, as caused by the state of deficiency. All of these symptoms may present with or without additional mood changes.\n\nMild to moderate cases of pernicious anemia may show symptoms of bleeding gums, headache, poor concentration, shortness of breath, and weakness. In severe cases of pernicious anemia, individuals may present with various cognitive problems such as dementia, and memory loss.\n\nIt is not always easy to determine whether B deficiency is present, especially within older adults. Patients may present with violent behaviour or more subtle personality changes. They may also present with vague complaints, such as fatigue or memory loss, that may be attributed to normative aging processes. Cognitive symptoms may mimic behaviour in Alzheimer's and other dementias as well. Tests must be run on individuals presenting with such signs to confirm or negate cobalamin deficiency within the blood.\n\nPatients deficient in B despite normal absorption functionality may be treated through oral administration of at least 6 mg of the vitamin in pill form. Patients who suffer from irreversible causes of deficiency, such as pernicious anemia or old age, will need lifelong treatment with pharmacological doses of B. Strategy for treatment is dependent on the patient's level of deficiency as well as their level of cognitive functioning. Treatment for those with severe deficiency involves 1000 mg of B administered intramuscularly daily for one week, weekly for one month, then monthly for the rest of the patient's life. Daily oral supplementation of B mega-doses may be sufficient in reliable patients, but it is imperative that the supplementation be continued on a lifelong basis as relapse may occur otherwise.\n\nThe progression of neurological manifestations of cobalamin deficiency is generally gradual. As a result, early diagnosis is important or else irreversible damage may occur. Patients who become demented usually show little to no cognitive improvement with the administration of B.\n\nA deficiency in folate may produce anemia similar to the anemia resulting from B deficiency. There is risk that folic acid administered to those with B deficiency may mask anemic symptoms without solving the issue at hand. In this case, patients would still be at risk for neurological deficits associated with B deficiency-related anemia, which are not associated with anemia related to folate deficiency.\n\nIn addition to meeting intake requirements through food consumption, supplementation of diet with vitamin B is seen as a viable preventative measure for deficiency. It has been recommended for the elderly to supplement 50 mcg a day in order to prevent deficit from occurring.\n\nAnimal protein products are a good source of B, particularly organ meats such as kidney or liver. Other good sources are fish, eggs, and dairy products. It is suggested that vegans, who consume no animal meat or by-products, supplement their diet with B. While there are foods fortified with B available, some may be mislabelled in an attempt to boost their nutritional claims. Products of fermentation, such as algae extracts and sea vegetables, may be labelled as sources of B, but actually contain B analogues which compete for the absorption of the nutrient itself. In order to get adequate amounts of the vitamin, orally administered pills or fortified foods such as cereals and soy milk, are recommended for vegans.\n\nVitamin D is an essential regulator of the vitamin D receptor that controls gene transcription during development. The vitamin D receptor is strongly expressed in the substantia nigra. Accordingly, vitamin D deficiency can disrupt neurogenesis leading to altered dopamine signaling and increased exploratory behavior in rats. This is considered a rodent model of the schizophrenia phenotype and vitamin D deficiency has been proposed as an explanation for the increased incidence of schizophrenia among children that were conceived during winter months. A Finnish study found that vitamin D supplement use is associated with reduced risk of schizophrenia.\n\nFatty acids are necessary for the synthesis of cell membranes neurotransmitters and other signaling molecules. While excessive fat intake can be harmful, deficiency of essential fatty acids can disrupt neurodevelopment and synaptic plasticity.\n\nConsuming large amounts of saturated fat can negatively affect the brain. Eating foods with saturated fats elevates the level of cholesterol and triglycerides in the body. Studies have shown that high levels of triglycerides strongly link with mood problems such as depression, hostility and aggression. This may occur because high triglyceride levels decrease the amount of oxygen that blood can carry to the brain. The American Heart Association recommends that people consume no more than 16g of saturated fat daily. Common sources of saturated fat are meat and dairy products.\n\nThere are two kinds of essential fatty acids that people must consume (omega-3 and omega-6). Many academics recommend a balanced amount of omega-3s and omega-6s. However, some estimate that Americans consume twenty times more omega-6s than omega-3s. There is a theory that an imbalance of essential fatty acids may lead to mental disorders such as depression, hyperactivity and schizophrenia, but it still lacking evidences. An omega-3 deficient diet increases omega-6 levels in the brain disrupting endocannabinoid signaling in the prefrontal cortex and nucleus accumbens contributing to anxiety and depression-like behaviors in mice. Sources of omega-3 include flax seeds, chia seeds, walnuts, sea vegetables, green leafy vegetables, and cold water fish. Sources of omega-6 include walnuts, hazelnuts; sunflower, safflower, corn, and sesame oils.\n\nWhile cholesterol is essential for membranes and steroid hormones, excess cholesterol affects blood flow impairing cognitive function in vascular dementia.\n\nStudies have shown that learning and memory improve after consuming carbohydrates. There are two kinds of carbohydrates people consume: simple and complex. Simple carbohydrates are often found in processed foods and release sugar into the bloodstream quickly after consumption. Complex carbohydrates are digested more slowly and therefore cause sugar to be released into the bloodstream more slowly. Good sources of complex carbohydrates are whole-grain breads, pasta, brown rice, oatmeal, and potatoes. It is recommended that people consume more complex carbohydrates because consuming complex carbohydrates will cause the level of sugar in the bloodstream to be more stable, which will cause less stress hormones to be released. Consuming simple carbohydrates may cause the levels of sugar in the bloodstream to rise and fall, which can cause mood swings.\n\nThe ketone body beta-hydroxybutyrate is a fuel source for the brain during times of fasting when blood glucose levels fall. Although the mechanism is not understood, it is well established that eating a diet low in carbohydrates can be therapeutic for children with epilepsy. This is likely a result of ketone bodies providing an alternative fuel source to glucose for neuronal function. Furthermore, a ketogenic diet can be beneficial for dementia patients. Medium-chain triglycerides can stimulate ketone synthesis and coconut oil is a rich source of medium chain triglycerides that several anecdotal reports suggest can improve cognitive function in Alzheimer's type dementia patients.\n\nWhen protein is consumed, it is broken down into amino acids. These amino acids are used to produce many things like neurotransmitters, enzymes, hormones, and chromosomes. Proteins known as complete proteins contain all eight of the essential amino acids. Meat, cheese, eggs, and yogurt are all examples of complete proteins. Incomplete proteins contain only some of the eight essential amino acids and it is recommended that people consume a combination of these proteins. Examples of incomplete proteins include nuts, seeds, legumes, and grains. When animals are fed a diet deficient in essential amino acids, uncharged tRNAs accumulate in the anterior piriform cortex signaling diet rejection [105]. The body normally interconverts amino acids to maintain homeostasis, but muscle protein can be catabolized to release amino acids during conditions of amino acid deficiency. Disruption of amino acid metabolism can affect brain development and neurophysiology to affect behavior. For example, fetal protein deficiency decreases the number of neurons in the CA1 region of the hippocampus.\n\nGlutamate is a proteinogenic amino acid and neurotransmitter, though it is perhaps publicly best known in its sodium salt form: monosodium glutamate (MSG). It is also a flavor on its own, producing the umami or savory flavor found in many fermented foods such as cheese. As an amino acid it acts as a source of fuel for various cellular functions and as a neurotransmitter. Glutamate operates as an excitatory neurotransmitter and is released when a nerve impulse excites a glutamate producing cell. This in turn binds to neurons with glutamate receptors, stimulating them.\n\nGlutamate is a nutrient that is extremely difficult to be deficient in, as, being an amino acid, it is found in every food that contains protein. Additionally it is found, as previously mentioned, in fermented foods and in foods containing monosodium glutamate. As such, good sources of glutamate include meat, fish, dairy products and a wide array of other foods. Glutamate is also absorbed extremely efficiently by the intestines. However, there are instances of glutamate deficiency occurring, but only in cases where genetic disorders are present. One such example is \"Glutamate formiminotransferase deficiency\" and can cause either minor or profound physical and intellectual disabilities, depending on the severity of the condition. This disorder is extremely rare however, as only twenty people have so far been identified with this condition. Glutamate, while critically important in the body also acts as an excitotoxin in high concentrations not normally found outside of laboratory conditions, although it can occur following brain injury or spinal cord injury.\n\nL-Phenylalanine is biologically converted into L-tyrosine, another one of the DNA-encoded amino acids, and beta-phenethylamine. L-tyrosine in turn is converted into L-DOPA, which is further converted into dopamine, norepinephrine (noradrenaline), and epinephrine (adrenaline). The latter three are known as the catecholamines. Phenethylamine is further converted into N-methylphenethylamine. Phenylalanine uses the same active transport channel as tryptophan to cross the blood–brain barrier, and, in large quantities, interferes with the production of serotonin.\n\nToxic levels of phenylalanine accumulate in the brains of patients with phenylketonuria leading to severe brain damage and mental retardation. To prevent brain damage, these individuals can restrict dietary phenylalanine intake by avoiding protein and supplementing their diet with essential amino acids.\n\n"}
{"id": "395889", "url": "https://en.wikipedia.org/wiki?curid=395889", "title": "Orphan drug", "text": "Orphan drug\n\nAn orphan drug is a pharmaceutical agent that has been developed specifically to treat a rare medical condition, the condition itself being referred to as an orphan disease.\n\nThe assignment of orphan status to a disease and to any drugs developed to treat it is a matter of public policy in many countries and has resulted in medical breakthroughs that may not have otherwise been achieved due to the economics of drug research and development.\n\nIn the US and the EU, it is easier to gain marketing approval for an orphan drug, and there may be other financial incentives, such as an extended period of exclusivity, a time during which that company is the only one allowed to market the orphan drug—all intended to encourage the development of those drugs which might otherwise lack a sufficient profit motive and market to attract companies' research budgets and personnel.\n\nAn orphan drug is a pharmaceutical agent which has been developed to treat a rare medical condition, the condition itself being referred to as a rare disease. A rare disease, also referred to as an \"orphan disease\", is any disease which affects a small percentage of the population. Most rare diseases are genetic, and thus are present throughout the person's entire life, even if symptoms do not immediately appear. Many rare diseases appear early in life, and about 30 percent of children with rare diseases will die before reaching their fifth birthday. No single cutoff number has been agreed upon for which a disease is considered rare. A disease may be considered rare in one part of the world, or in a particular group of people, but still be common in another. As many as one-in-ten Americans suffers from rare disease.\n\n, there were 281 marketed orphan drugs and more than 400 orphan designated drugs in clinical trials. More than 60% of orphan drugs were biologics. The US dominated the development of orphan drugs with more than 300 orphan drugs in clinical trial process, followed by Europe. Cancer treatment was the indication in more than 30% for orphan drug clinical trials.\n\nAccording to Thomson Reuters in their 2012 publication \"The Economic Power of Orphan Drugs,\" there has been increased investing in orphan drug research and development partly due to the U. S. Orphan Drug Act of 1983 (ODA) and similar acts in other regions of the world and also driven by \"high-profile philanthropic funding.\"\n\nPer the trade journal Drug Discovery Today between 2001 and 2011 was the \"most productive period in the history of orphan drug development, in terms of average annual orphan drug designations and orphan drug approvals.\" For the same decade the compound annual growth rate (CAGR) of the orphan drugs was an \"impressive 25.8 percent, compared to only 20.1 percent for a matched control group of non-orphan drugs.\" \nBy 2012, the market for orphan drugs was worth USD$637 million compared to the USD$638 million matched control group of non-orphan drugs.\n\nBy 2012,\n\nAccording to a 2014 report, the orphan drug market has become increasingly lucrative for a number of reasons: The cost of clinical trials for orphan drugs is substantially lower than for other diseases —trial sizes are naturally much smaller than for more common diseases with larger numbers of patients. Small clinical trials and little competition place orphan agents at an advantage in regulatory review.\n\nTax incentives reduce the cost of development. On average the cost per patient for orphan drugs is \"six times that of non-orphan drugs, a clear indication of their pricing power\". The cost of per-person outlays are huge and are expected to increase with wider use of public subsidies.\n\nThe 2014 Orphan Drug report stated that the percentage of orphan drug sales as part of all prescription drug sales had been increasing at rapid rate. By 2020 the total was predicted to be $176 billion. Although orphan disease populations are the smallest, the cost of per-patient outlays have been the largest. There pressure on pharmaceuticals which \"represent the biggest budgetary drain\" has been predicted to increase particularly as more people with rare diseases will be eligible for subsidies —in the United States for example through the Affordable Care Act.\n\nOrphan drugs generally follow the same regulatory development path as any other pharmaceutical product, in which testing focuses on pharmacokinetics and pharmacodynamics, dosing, stability, safety and efficacy. However, some statistical burdens are lessened to maintain development momentum. For example, orphan drug regulations generally acknowledge the fact that it may not be possible to test 1,000 patients in a phase III clinical trial, as fewer than that number may be afflicted with the disease in question.\n\nSince the market for any drug with such a limited application scope would, by definition, be small and thus largely unprofitable for pharmaceutical companies, some governments have chosen to intervene to motivate a manufacturer to develop an orphan drug.\n\nThe intervention by government on behalf of orphan drug development can take a variety of forms:\n\nA 2015 study of \"34 key Canadian stakeholders including drug regulators, funders, scientists, policy experts, pharmaceutical industry representatives, and patient advocates\" investigated contributing factors to the growing interest of the pharmaceutical industry in \"niche markets\" such as orphan drugs.\n\nThe Orphan Drug Act (ODA) of January 1983, passed in the United States, with lobbying from the National Organization for Rare Disorders and many other organizations, is meant to encourage pharmaceutical companies to develop drugs for diseases that have a small market. Under the ODA drugs, vaccines, and diagnostic agents would qualify for orphan status if they were intended to treat a disease affecting less than 200,000 American citizens. Under the ODA orphan drug sponsors qualify for seven-year FDA-administered market Orphan Drug Exclusivity (ODE), \"tax credits of up to 50% of R&D costs, R&D grants, waived FDA fees, protocol assistance and may get clinical trial tax incentives.\n\nOrphan drug designation means that the sponsor qualifies for certain benefits but it does not mean the drug is safe and effective and legal to manufacture and market in the United States.\n\nIn 2002, the Rare Diseases Act was signed into law. It amended the Public Health Service Act to establish the Office of Rare Diseases. It also increased funding for the development of treatments for people with rare diseases.\n\nIn 2000, the European Union (EU) has enacted similar legislation, Regulation(EC) No 141/2000, which refers to drugs developed to treat rare diseases to as \"orphan medicinal products\". The EU's definition of an orphan condition is broader than that of the USA, in that it also covers some tropical diseases that are primarily found in developing nations. Orphan drug status granted by the European Commission gives marketing exclusivity in the EU for 10 years after approval. The EU's legislation is administered by the Committee on Orphan Medicinal Products of the European Medicines Agency (EMA).\n\nIn late 2007 the FDA and EMA agreed to use a common application process for both agencies to make it easier for manufacturers to apply for orphan drug status but, while continuing two separate approval processes.\n\nLegislation has been implemented by Japan, Singapore, and Australia that offers subsidies and other incentives to encourage the development of drugs that treat orphan diseases.\n\nUnder the ODA and EU legislation, many orphan drugs have been developed, including drugs to treat glioma, multiple myeloma, cystic fibrosis, phenylketonuria, snake venom poisoning, and idiopathic thrombocytopenic purpura.\n\nThe Pharmaceitical Executive opines, that the \"ODA is nearly universally acknowledged to be a success\".\nBefore the United States Congress enacted the ODA in 1983, only 38 drugs were approved in the USA specifically to treat orphan diseases. \nIn the USA, from January 1983 to June 2004, the Office of Orphan Products Development 249 orphan drugs received marketing authorization and granted 1,129 different orphan drug designations, compared to fewer than ten such products in the decade prior to 1983. \nFrom 1983 until May 2010, the FDA approved 353 orphan drugs and granted orphan designations to 2,116 compounds. \nAs of 2010, 200 of the roughly 7,000 officially designated orphan diseases have become treatable.\n\nCritics have questioned whether orphan drug legislation was the real cause of this increase, claiming that many of the new drugs were for disorders which were already being researched anyway, and would have had drugs developed regardless of the legislation, and whether the ODA has truly stimulated the production of non-profitable drugs; the act also has been criticised for allowing some pharmaceutical companies to make a large profit of drugs which have a small market but sell for a high price.\n\nWhile the European Medicines Agency grants orphan drugs market access to all member states, in practice, they only reach the market when a member state decides that its national health system will reimburse for the drug. For example, in 2008, 44 orphan drugs reached the market in the Netherlands, 35 in Belgium, and 28 in Sweden, while in 2007, 35 such drugs reached the market in France and 23 in Italy.\n\nThough not technically an orphan disease, the research and development into the treatment for AIDS has been heavily linked to the Orphan Drug Act. In the beginning of the AIDS epidemic the lack of treatment for the disease was often accredited to a believed lack of commercial base for a medication linked to HIV infection. This encouraged the FDA to use the Orphan Drug Act to help bolster research in this field, and by 1995 13 of the 19 drugs approved by the FDA to treat AIDS had received orphan drug designation, with 10 receiving marketing rights. These are in addition to the 70 designated orphan drugs designed to treat other HIV related illnesses.\n\nIn the 1980s, people with cystic fibrosis rarely lived beyond their early teens. Drugs like Pulmozyme and tobramycin, both developed with aid from the ODA, revolutionized treatment for cystic fibrosis patients by significantly improving their quality of life and extending their life expectancies. Now, cystic fibrosis patients often survive into their thirties and some into their fifties.\n\nThe 1985 Nobel Prize for medicine went to two researchers for their work related to familial hypercholesterolemia, which causes large and rapid increases in cholesterol levels. Their research led to the development of statin drugs which are now commonly used to treat high cholesterol.\n\nPenicillamine was developed to treat Wilson's disease, a rare hereditary disease that can lead to a fatal accumulation of copper in the body. This drug was later found to be effective in treating arthritis. Bis-choline tetrathiomolybdate is currently under investigation as a therapy against Wilson's disease.\n\nThe Center for Orphan Drug Research at the University of Minnesota College of Pharmacy helps small companies with insufficient in-house expertise and resources in drug synthesis, formulation, pharmacometrics, and bio-analysis.\nThe Keck Graduate Institute Center for Rare Disease Therapies (CRDT) in Claremont, California, supports projects to revive potential orphan drugs whose development has stalled by identifying barriers to commercialization, such as problems with formulation and bio-processing.\n\nNumerous advocacy groups such as the National Organization for Rare Disorders, Global Genes Project, Children's Rare Disease Network, Abetalipoproteinemia Collaboration Foundation, Zellweger Baby Support Network, and the Friedreich's Ataxia Research Alliance have been founded in order to advocate on behalf of patients suffering from rare diseases with a particular emphasis on diseases that afflict children.\n\nAccording to a 2015 report published by EvaluatePharma, the economics of orphan drugs mirrors the economics of the pharmaceutical market as a whole but has a few very large differences. The market for orphan drugs is by definition very small, but while the customer base is drastically smaller the cost of research and development is very much the same as for non orphan drugs. This, the producers have claimed, causes them to charge extremely high amounts for treatment sometimes as high as $700,000 a year, as in the case of Spinraza (Biogen), FDA approved in December 2016 for spinal muscular atrophy, placing a large amount of stress on insurance companies and patients. An analysis of 12 orphan drugs that were approved in the US between 1990 and 2000 estimated a price reduction of on average 50% upon loss of marketing exclusivity, with a range of price reductions from 14% to 95%.\n\nGovernments have implemented steps to reduce high research and development cost with subsidies and other forms of financial assistance. The largest assistance are tax breaks which can be as high as 50% of research and development costs. Orphan drug manufacturers are also able to take advantage of the small customer base to cut cost on clinical trials due to the small number of cases to have smaller trials which reduces cost. These smaller clinical trials also allow orphan drugs to move to market faster as the average time to receive FDA approval for an orphan drug is 10 months compared to 13 months for non-orphan drugs. This is especially true in the market for cancer drugs, as a 2011 study found that between 2004 and 2010 orphan drug trials were more likely to be smaller and less randomized than their non-orphan counterparts, but still had a higher FDA approval rate, with 15 orphan cancer drugs being approved, while only 12 non-orphan drugs were approved. This allows manufactures to get cost to the point that it is economically feasible to produce these treatments. The subsidies can total up to 30 million dollars per fiscal year in the United States alone .\n\nBy 2015, industry analysts and academic researchers agreed, that the sky-high price of orphan drugs, such as eculizumab, was not related to research, development and manufacturing costs. Their price is arbitrary and they have become more profitable than traditional medicines.\n\nBy 2007 the use of economic evaluation methods regarding public-funding of orphan drugs, using estimates of the incremental cost-effectiveness, for example, became more established internationally. The QALY has often been used in cost-utility analysis to calculate the ratio of cost to QALYs saved for a particular health care intervention. By 2008 the National Institute for Health and Care Excellence (NICE) in England and Wales, for example, operated with a threshold range of £20,000–£30,000 per Quality-adjusted life year (QALY). By 2005 doubts were raised about the use of economic evaluations in orphan drugs. By 2008 most of the orphan drugs appraised had cost-effectiveness thresholds \"well in excess of the ‘accepted’ level and would not be reimbursed according to conventional criteria.\" As early as 2005 McCabe et al argued that rarity should not have a premium and orphan drugs should be treated like other pharmaceuticals in general. Drummond et al argued that the social value of health technologies should also be included in the assessment along than the estimation of the incremental cost-effectiveness ratio.\n\nThe very large incentives given to pharmaceutical companies to produce orphan drugs have led to the impression that the financial support afforded to make these drugs possible is akin to abuse. Because drugs can be used to treat multiple conditions, companies can take drugs that were filed with their government agency as orphan drugs to receive financial assistance, and then market it to a wide population to increase their profit margin. For example AstraZeneca's cholesterol drug Crestor was filed as a treatment for the rare disease pediatric familial hypercholesterolemia. After the drug was approved for orphan drug designation, and AstraZeneca had received tax breaks and other advantages, AstraZeneca later applied and received FDA approval for the drug to be used to treat cholesterol in all diabetics.\n\nBy 2008 if an orphan drug cost more than £30,000 the NICE required other arguments for funding.\n\nIn 2015, NICE held consultations with \"patient groups, the Department of Health, companies, learned societies, charities and researchers\" regarding the appraisal of medicines and other technologies. There was a call for more research into new processes including the \n\n\n"}
{"id": "2613847", "url": "https://en.wikipedia.org/wiki?curid=2613847", "title": "Overnutrition", "text": "Overnutrition\n\nOvernutrition or hyperalimentation is a form of malnutrition in which the intake of nutrients is oversupplied. The amount of nutrients exceeds the amount required for normal growth, development, and metabolism. \n\nThe term can also refer to:\n\nFor mineral excess, see:\n\nOvernutrition may also refer to greater food consumption than appropriate, as well as other feeding procedures such as parenteral nutrition.\n\n"}
{"id": "5263684", "url": "https://en.wikipedia.org/wiki?curid=5263684", "title": "Paralytic illness of Franklin D. Roosevelt", "text": "Paralytic illness of Franklin D. Roosevelt\n\nThe paralytic illness of Franklin D. Roosevelt began in 1921 when the future President of the United States was 39 years old. His main symptoms were fever; symmetric, ascending paralysis; facial paralysis; bowel and bladder dysfunction; numbness and hyperesthesia; and a descending pattern of recovery. Roosevelt was left permanently paralyzed from the waist down. He was diagnosed with poliomyelitis at the time, but his symptoms are more consistent with Guillain–Barré syndrome (GBS) – an autoimmune neuropathy which Roosevelt's doctors failed to consider as a diagnostic possibility. In 1926, his belief in the benefits of hydrotherapy led him to found a rehabilitation center at Warm Springs, Georgia. He avoided being seen using his wheelchair in public, but his disability was well known and became a major part of his image. In 1938, he founded the National Foundation for Infantile Paralysis, leading to the development of polio vaccines.\n\nIn August 1921, 39-year-old Franklin D. Roosevelt, at the time a practicing lawyer in New York, joined his family at their vacation home at Campobello, a Canadian island off the coast of Maine. As former Assistant Secretary of the Navy, he had spent two weeks in mid-July in Washington, D.C., giving testimony to a Senate committee investigating a Navy scandal. On August 5, Roosevelt sailed up the New England coast with his friend and new employer, Van Lear Black, on Black's ocean-going yacht. Among those at Campobello when Roosevelt arrived were his wife Eleanor, their children, his political aide Louis Howe, Howe's wife, and their young son.\n\nOn August 10, after a day of strenuous activity, Roosevelt came down with an illness characterized by fever, ascending paralysis, facial paralysis, prolonged bowel and bladder dysfunction, and numbness and hypersensitivity of the skin. Most of the symptoms resolved themselves, but he was left permanently paralyzed from the waist down.\n\nRoosevelt came close to death from the illness. He faced many life-threatening medical problems including the possibility of respiratory failure, urinary tract infection, injury to the urethra or bladder, decubitus ulcers, clots in the leg veins, and malnutrition. Eleanor’s nursing care was responsible for Roosevelt’s survival.\n\nJuly 28: \nRoosevelt visited the Boy Scout Jamboree at Bear Mountain State Park.\n\nAugust 5–8: \nRoosevelt traveled to Campobello on board the yacht \"Sabolo\".\n\nAugust 9:\nRoosevelt fell into the cold waters of the Bay of Fundy. Later, arrived at Campobello.\n\nAugust 10:\nRoosevelt spent the day physically active. Afterward, he complained of chills, nausea, and pain in his lower back. He skipped dinner and went to bed. Chills lasted through the night. \n\nAugust 11:\nIn the morning, one of his legs felt weak. Roosevelt had fever.\nDr. Eben H. Bennet, a general practitioner in the nearby village of Lubec who had known the Roosevelts for years, visited Roosevelt and diagnosed a bad summer cold. By the evening, one leg was paralyzed, and the other had become weak. \n\nAugust 12:\nBoth legs were paralyzed. His temperature was 102 °F. Pain shot through his legs, feet and back.\nBennet suggested a consultation with Dr. William W. Keen, an eminent retired neurosurgeon vacationing nearby.\nRoosevelt's legs were numb. They then became painfully sensitive to touch. He could not pass urine. By evening, his hands were affected.\n\nAugust 13:\nRoosevelt was paralyzed from the chest down. On that day and the following, his hands, arms, and shoulders were weak. He had difficulty moving his bowels and required enemas.\nKeen made what Eleanor described as \"a most careful, thorough examination\".\n\nAugust 14:\nRoosevelt continued to be unable to pass urine for two weeks, and required catheterization. His fever continued for a total of six to seven days.\nKeen repeated his examination, a bending and prodding that Elliott later termed \"excruciating\" for his father.\nKeen diagnosed a clot of blood to the lower spinal cord, and prescribed massage of the leg muscles.\nEleanor and Howe began massaging Roosevelt's legs as instructed by Keen, bringing on agonizing pain.\n\nAugust 15:\nProstrate and mildly sedated, Roosevelt was occasionally delirious.\n\nAugust 19:\nFrederic Delano, Roosevelt's uncle, had received a letter from Louis Howe requesting to find a doctor to come see Roosevelt. Delano called his son-in-law, a physician, who recommended he speak to another physician, a Dr. Parker. Parker told Delano that the case sounded like infantile paralysis, and that the leading authorities on the disease were at the Harvard Infantile Paralysis Commission in Boston. Delano caught a train and arrived the next morning.\n\nAugust 20:\nDr. Samuel A. Levine was at his office when Delano telephoned Brigham Hospital on Saturday morning. Levine said the senior members of the Harvard Infantile Paralysis Commission, Dr. Lovett and Dr. Peabody, were out of town, but he would try to answer Delano's questions. After reviewing the messages Delano had received from Campobello, Levine thought Roosevelt was suffering from acute poliomyelitis. He urged that a lumbar puncture be done, with the goal of making a diagnosis, but mainly because Levine believed there could be acute benefit from the procedure. \nDelano phoned and wrote Eleanor the same day, advising her to stop massaging Roosevelt's legs, and to disregard Keen's advice: \"I think it would be very unwise to trust his diagnosis where the Inf. Paralysis can be determined by test of the spinal fluid.\"\nEleanor communicated with Keen, who \"very strenuously\" resisted the idea of poliomyelitis. Keen asked Lovett to visit Campobello.\n\nAugust 22:\nLovett met Levine for dinner. Lovett asked how to distinguish whether paralysis was caused by poliomyelitis or by a clot or lesion of the spinal cord.\n\nAugust 23:\nLovett left for Campobello.\n\nAugust 24:\nLovett saw Roosevelt and performed a \"more or less superficial\" examination since Roosevelt was highly sensitive to touch. The arms were weak; the bladder was paralyzed; the left thumb indicated atrophy. Roosevelt could not stand or walk, and Lovett documented \"scattered weakness, most marked in the hips\".\n\nAugust 25:\nRoosevelt's temperature was 100 °F. Both legs were paralyzed. His back muscles were weak. There was also weakness of the face and left hand. Pain in the legs and inability to urinate continued. \nAfter a brief conference with Keen, Lovett saw Roosevelt. Lovett informed him that the \"physical findings\" presented a \"perfectly clear\" diagnosis of poliomyelitis.\nLovett ordered an end to massage, which had no benefit and caused pain, and recommended a trained nurse to care for Roosevelt.\n\nSeptember 1:\nRoosevelt was still unable to urinate. His leg pain continued.\n\nSeptember 14:\nRoosevelt was transported to New York, by boat and train, a long and painful journey.\n\nSeptember 15:\nRoosevelt was admitted to Presbyterian Hospital in New York City for convalescence, under the care of Dr. George Draper, an expert on poliomyelitis and Roosevelt's personal physician. Lovett continued to consult from Boston.\nThere was pain in the legs, paralysis of the legs, muscle wasting in the lower lumbar area and the buttocks, weakness of the right triceps, and gross muscle twitching in both forearms.\n\nOctober 28:\nRoosevelt was transferred from Presbyterian Hospital to his house on East 65th Street. His chart still read \"not improving\".\n\nLater:\nRoosevelt exercised daily. His hamstrings tightened, and his legs were encased in plaster to straighten them by degrees.\nThere was gradual recovery, but he remained paralyzed from the waist down.\n\nAfter falling ill, Roosevelt was seen by four doctors. Eben Homer Bennet, the Roosevelt family doctor, diagnosed a heavy cold. William Keen, a retired neurosurgeon, thought Roosevelt had a blood clot. Robert Lovett, an expert on the orthopedic management of children paralyzed from poliomyelitis, diagnosed \"infantile paralysis\", as did George Draper, Roosevelt's personal physician.\n\nRoosevelt's physicians never mentioned Guillain–Barré syndrome (GBS) in their communications concerning Roosevelt's case, indicating that they were not aware of it as a diagnostic possibility. All reports before 1921 of what is now called GBS were by European physicians, in European journals. The result was that very few American physicians knew that GBS was a separate disease. For example, Lovett mistakenly believed that Landry’s ascending paralysis, now termed GBS, was one of the clinical presentations of paralytic polio. In 1921, an American physician would assume that if an individual developed a sudden, non-traumatic flaccid paralysis, it was due to paralytic polio. The concept of GBS as a separate disease was not widely accepted in the United States until after the Second World War.\n\nRoosevelt first traveled to Warm Springs, Georgia, on October 3, 1924. For many years to come Warm Springs would be where he would retreat in comfort for hydrotherapy. On April 29, 1926, he bought the center with the intention of making it into a rehabilitation center for polio patients. One of Roosevelt's primary goals was to get the American Orthopedic Association to endorse the resort, but he was rejected because there was no real progress in physical health.\n\nIn 1929 Roosevelt was elected Governor of New York and moved into the Governor's Mansion in Albany. Before he moved in, the mansion was made wheelchair-friendly with ramps and an elevator.\n\nRoosevelt won the Presidential election in 1932 in a landslide victory and became the first, and so far only, disabled person to be President of the United States. Before he moved into the White House, ramps were added to make it wheelchair-friendly. Any pictures of the President were taken at certain angles and at a distance.\n\nRoosevelt was totally and permanently paralyzed from the waist down, and unable to stand or walk without support. For the next few months, he confined himself to indoor pursuits, including resuming his lifelong hobby of stamp collecting. In December 1921, after he had recuperated for several months, a physiotherapist began working with him to determine the extent of the damage. He was able to perform small exercises on his own, moving one muscle and then another. He was fitted with heavy steel braces that locked at the knee and provided enough stability that he could stand with crutches. In 1922, at Springwood, he worked diligently to make his way across the room. He set himself the goal of getting down the long driveway, managing to do it once, but never trying again.\n\nIn October 1922, Roosevelt visited his law office at the Equitable Building, where a welcome-back luncheon had been arranged. The chauffeur assisting him failed to brace the tip of his left crutch and Roosevelt fell onto the highly polished lobby floor. Laughing, he asked two young men in the crowd of onlookers to help get him back on his feet. After the luncheon, he told friends it was a \"grand and glorious occasion\". He did not return to his office for two months.\n\nRoosevelt believed that warmth and exercise would help rebuild his legs. He bought a run-down 71-foot houseboat and, in February 1923, sailed to Florida with friends and a small crew. Eleanor found it dull and left, but Roosevelt sailed for weeks, fishing and spending time with a succession of friends who came to visit. He designed a pulley system that lowered him into the water to swim. In May 1923, Lovett documented no overall improvement over the preceding year, but Roosevelt would not accept his doctors' determination that further progress was unlikely. He tried a range of therapies and made two more voyages on his houseboat, but his efforts had no effect.\n\n\"Between 1925 and 1928, Franklin would spend more than half his time—116 of 208 weeks—away from home, struggling to find a way to regain his feet,\" wrote biographer Geoffrey Ward. \"Eleanor was with him just 4 of those 116 weeks, and his mother was with him for only 2. His children hardly saw him.\"\n\nRoosevelt lost the use of his legs and two inches of height, but the subsequent development of the rest of his body gave him a robust physique, and he enjoyed many years of excellent health. Jack Dempsey praised his upper-body musculature, and Roosevelt once landed a 237-pound shark after fighting it on his line for two hours.\n\nWith his physiotherapist at Warm Springs, Roosevelt laboriously taught himself to walk short distances while wearing iron braces on his hips and legs, by swiveling his torso. For this \"two-point walk\", he would grip the arm of a strong person with his left hand, and brace himself with a cane in his right.\n\nRoosevelt was the United States' first disabled president, so there has been discussion concerning how he handled his health. Disability advocate Hugh Gallagher posited that Roosevelt was desperate to appear able-bodied. When discussing Roosevelt's limited use of a wheelchair in public, Gallagher stated, \"This was not by accident. It was a strategy that served to minimize the extent of his handicap, to make it unnoticed when possible and palatable when it was noticed.\". Historian James Tobin argued that Roosevelt used his disability to his advantage. Tobin stated, \"But he could, instead, show himself to be something he had never been seen as before: a fighter and, and better yet, an underdog; not a man to pity, not a man to envy, but a man to cheer.\" Despite any controversy, Roosevelt has become an inspirational figure for disabled people around the world. The Roosevelt Memorial in Washington, D.C. includes a statue of Roosevelt in a wheelchair.\n\nRoosevelt was able to convince many people that he was getting better, which he believed was essential if he was to run for public office again. In private he used a wheelchair but was careful not to be seen in it in public, although he sometimes appeared on crutches. He usually appeared in public standing upright, while being supported on one side by an aide or one of his sons. For major speaking occasions, an especially solid lectern was placed on the stage so that he could support himself on it; as a result, in films of his speeches, Roosevelt can be observed using his head to make gestures because his hands were gripping the lectern. He might raise one hand to gesture, but his other hand held the podium.\n\nHis public appearances were choreographed to avoid the press covering his arrival and departure, which would have shown him getting into or out of a vehicle. When possible, his vehicle was driven into a building's parking garage for his arrivals and departures. On other occasions, the vehicle would be driven onto a ramp to avoid steps, which Roosevelt was unable to ascend. When that was not practical, the steps would be covered with a ramp with railings, with Roosevelt using his arms to pull himself upward. When traveling by train, Roosevelt often appeared on the rear platform of the presidential railroad car. When he boarded or disembarked, the private car was sometimes shunted to an area of the railroad yard away from the public for reasons of security and privacy. When Roosevelt's train used a ramp and the president was on a publicly known trip, he insisted on walking on the ramp no matter how difficult.\n\nJournalist John Gunther reported that in the 1930s he often met people in Europe, including world leaders, who were unaware of Roosevelt's paralysis. David Brinkley, who was a young White House reporter in World War II, stated that the Secret Service actively interfered with photographers who tried to take pictures of Roosevelt in a wheelchair or being moved about by others. The Secret Service commonly destroyed photographs they caught being taken in this manner; however, there were occasional exceptions. When Roosevelt addressed the Congress in person on March 1, 1945, about a month before his death, he made public reference to his disability for almost the first time in 20 years. \"I hope that you will pardon me for this unusual posture of sitting down,\" Roosevelt began, \"but I know you will realize that it makes it a lot easier for me not to have to carry about ten pounds of steel around on the bottom of my legs.\"\n\nOn January 3, 1938, Roosevelt founded the National Foundation for Infantile Paralysis, now known as the March of Dimes. Basil O'Connor, an attorney and close associate of Roosevelt, helped establish the foundation and was its president for more than three decades. The organization's annual fundraising campaign coincided with Roosevelt's birthday on January 30. The organization initially focused on the rehabilitation of victims of paralytic polio and supported the work of Jonas Salk and others that led to the development of polio vaccines. Today, the March of Dimes focuses on preventing premature births, congenital disabilities, and infant mortality.\n\nBecause he founded the March of Dimes, a dime was chosen to honor Roosevelt after his death. The Roosevelt dime was issued on January 30, 1946.\n\nRoosevelt's center at Warm Springs operates today as the Roosevelt Warm Springs Institute for Rehabilitation, a comprehensive rehabilitation facility operated by the state of Georgia. A center for post-polio treatment, it provides vocational rehabilitation, long-term acute care, and inpatient rehabilitation for amputees and people recovering from spinal cord injuries, brain damage, and stroke.\n\nA 2003 peer-reviewed study of Roosevelt's paralytic illness, using three diagnostic methods - pattern recognition, reconstructing the pathogenesis, and Bayesian analysis - favored a diagnosis of Guillain–Barré syndrome (GBS) over poliomyelitis. For the Bayesian analysis, the best estimate of the annual incidence of GBS was 1.3 per 100,000. For paralytic polio in Roosevelt's age group, an annual incidence of 1.0 per 100,000 was used. The paralytic polio rate was derived from the exceptionally severe polio epidemic that struck New York City in 1916, to tilt the odds in favor of polio. The prior probability of paralytic polio in Roosevelt’s age group in the United States in 1921 was likely much lower because paralytic polio was at one of its lowest ebbs in the Northeastern region of the country at that time. In July 1921, three cases were reported in New Jersey. By late August some 100 cases were reported in the state of New York. Based on the incidence rates for GBS and paralytic polio, and symptom probabilities from the medical literature, Roosevelt's symptoms were analyzed by Bayesian analysis to obtain posterior probabilities, as listed below.\n\nSix of eight symptoms favored GBS, with the posterior probability of GBS shown for each:\n\nTwo of eight symptoms favored polio, with the posterior probability of polio shown for each:\n\nUsing all eight symptoms in a Bayesian analysis, based on disease incidence rates and symptom probabilities from the medical literature, the overall probability that Roosevelt had GBS is over 99%.\n\nIt is possible Roosevelt was exposed to an infectious agent at the Boy Scout Jamboree in late July. The two-week interval before the onset of his neurological illness was in keeping with both the incubation period of poliomyelitis, and with exposure to an infectious agent leading to GBS. There are no reports that any scouts or personnel at the camp were ill around the time of Roosevelt’s visit. In 1912 and 1915, Roosevelt had illnesses compatible with \"Campylobacter jejuni\", a major causative agent of GBS.\n\nIt has been stated that Roosevelt may have been predisposed to paralytic polio by genetic inheritance. However, such a genetic predisposition has never been discovered. Several authors have stated that Roosevelt was more vulnerable to polio since he was raised on an isolated family estate and had little contact with other children until he entered Groton at age 14. However, Roosevelt was not a \"boy in a bubble\". He had many possible exposures to polioviruses before 1921. Most polio cases are asymptomatic or a mild illness. Yet those asymptomatic individuals can transmit the viral infection. When the prior probability of paralytic polio was artificially made 100-fold higher, to 99.4%, overall Bayesian analysis of Roosevelt's symptoms still greatly favored GBS (99.97% posterior probability). Roosevelt’s symptoms are typical of GBS, not of paralytic polio, so they overwhelm the initial prior probability.\n\nA 2014 book argued that a lumbar puncture was done, based on excerpts from an \"unpublished note\" by Dr. Samuel A. Levine of the Harvard Infantile Paralysis Commission. The book stated, \"Levine's private note indicates that Dr. Lovett did examine the cerebrospinal fluid and knew very well that a high level of white blood cells was consistent with poliomyelitis. . . . If Lovett had discovered a low white blood cell count, he would have doubted that poliomyelitis was the cause of Roosevelt's illness. Yet Lovett wrote George Draper that \"I thought [the diagnosis] was perfectly clear as far as the physical findings were concerned.\"\"\n\nA 2017 book finally published the note. Based on the full text, the note was not found to support the idea that a spinal tap was performed, because: \n1) The note, written many years after 1921, had many errors. \n2) Levine never saw Roosevelt. \n3) Levine did not mention who did the procedure, or the results. \n4) Lovett had made it clear he would not do the procedure. \n5) None of Roosevelt's physicians at Campobello were equipped to do the procedure. \n6) \"Physical findings\" means information gained from a physical examination of the patient. Lovett described many \"physical findings\": hyperesthesia, facial paralysis, etc. But neither he, any of Roosevelt's other physicians at Campobello, Roosevelt, Eleanor, nor any of Roosevelt's associates ever said that a spinal tap was done.\n\nThe 2017 book went on to say: \"Given the invasive nature of a spinal tap, and the difficulty that would have occurred doing the procedure on a patient in FDR’s condition, it seems highly unlikely that a direct observer would have failed to mention anything about it. . . . Even if a spinal tap had been performed, it would have been done at the earliest about 15 days after the onset of the neurological illness, around when Lovett first saw Roosevelt. . . . One should keep in mind that the classical distinction between paralytic polio and GBS, the concentrations of leukocytes and total protein in CSF, blurs after the first several days of the onset of paralysis in both diseases.\"\n\n\"In any event, there was no cure for either disease in 1921,\" wrote biographer Jonathan Alter. However, Levine mistakenly thought that the main benefit of a spinal tap, if done, would be to improve the outcome by lowering elevated CSF pressure. And even today, some authors mistakenly believe that Roosevelt's paralysis, assuming a polio diagnosis, could have been prevented with early intervention. However, there is no objective evidence that a spinal tap lessens the possibility of paralysis in polio. And it is unlikely Roosevelt's physicians would have tried human serum injections into the CSF, or that such injections would have helped. Lovett did not think the injections were useful, and there were alarming meningeal symptoms associated with them, probably secondary to the formation of antigen-antibody complexes. Concerning GBS, virtually all of the effective measures that are currently standard practice for the medical management of GBS were not developed until many decades after Roosevelt's 1921 illness, so Roosevelt's prognosis would not have improved even if GBS had been diagnosed.\n\n"}
{"id": "49886610", "url": "https://en.wikipedia.org/wiki?curid=49886610", "title": "Postpartum confinement", "text": "Postpartum confinement\n\nPostpartum confinement refers to a system for recovery following childbirth. This postpartum period (less commonly, puerperium or puerperal period) begins immediately after the birth, and lasts for a culturally variable length: typically for one month or 30 days, up to 40 days, two months or 100 days. This postnatal recuperation can include \"traditional health beliefs, taboos, rituals, and proscriptions.\" The practice used to be known as \"lying-in\", which, as the term suggests, centres around bed rest.\n\nThe custom is well-documented in China, where it is known as \"Sitting the month\". Japanese women know it as \"\"Sango no hidachi\" and Koreans as \"Samchilil\"\", which means \"thirty-one\". In Latin American countries it is called \"la cuarentena\", i.e. \"forty days\" (the source of the English word \"quarantine\"). In India it is called \"jaappa\" (also transliterated \"japa\").\n\nPostpartum confinement refers both to the mother and the baby. Human newborns are so underdeveloped that pediatricians such as Harvey Karp refer to the first three months as the \"fourth trimester\". The weeks of rest while the mother heals also protect the infant as it adjusts to the world, and both learn the skills of breastfeeding.\n\nA 2016 American book describes the difficulties of documenting these \"global grandmotherly customs\" but asserts that \"like a golden rope connecting women from one generation to the next, the protocol of caring for the new mother by unburdening her of responsibilities and ensuring she rests and eats shows up in wildly diverse places\".\n\nOne meta-review of studies concluded, \"There is little consistent evidence that confinement practices reduce postpartum depression.\"\n\nAlmost all countries have some form of maternity leave. Many countries encourage men to take some paternity leave, but even those that mandate that some of the shared parental leave must be used by the father (\"father's quota\") acknowledge that the mother needs time off work to recover from the childbirth and deal with the postpartum physiological changes.\n\n\"Sitting the month\": 坐月子 \"Zuò yuè zi\" in Mandarin or 坐月 \"Co5 Jyut2\" in Cantonese. The custom, going back to the year 960, is referred to as 'confinement' as women are advised to stay indoors for recovery from the trauma of birth and feed the newborn baby. Aspects of traditional Chinese medicine are included, with a special focus on eating foods considered to be nourishing for the body and help with the production of breastmilk. In Guangdong and neighboring regions, new mothers are barred from visitors until the baby is 12 days old, marked by a celebration called 'Twelve mornings' (known as 十二朝). From this day onwards, Cantonese families with a new baby usually share their joy through giving away food gifts, while some families mark the occasion by paying tribute to their ancestors. \n\nIn ancient China, women of certain ethnic groups in the South would resume work right after birth, and allow the men to practice postpartum confinement instead. (See Couvade.)\n\nDuring confinement, mothers are told not to expose their body to physical agents such cold weather or wind, as these are claimed to be detrimental to their recovery. Specifically, mothers were traditionally not allowed to have any contact with water (e.g. bathing or washing hair), to exert themselves by climbing stairs, to read books or cry, to sew, or to have sex. \n\nNowadays, however, new mothers may wash their hair or take a bath or shower infrequently during the postpartum period, but it is claimed to be important to dry their body immediately afterwards with a clean towel and their hair properly using a hair dryer. It is also claimed to be important for women to wrap up warm and minimize the amount of skin exposed, as it was believed that they may catch a cold during this vulnerable time.\n\nThe custom of confinement advises new mothers to choose energy and protein-rich foods to recover energy levels, help shrink the uterus and for the perineum to heal. This is also important for the production of breastmilk. Sometimes, new mothers only begin to consume special herbal foods after all the lochia is discharged.\n\nA common dish is pork knuckles with ginger and black vinegar as pork knuckles are believed to help replenish calcium levels in the women. Ginger is featured in many dishes, as it is believed that it can remove the 'wind' accumulated in the body during pregnancy. Meat-based soup broths are also commonly consumed to provide hydration and added nutrients. Although not entirely backed by scientific evidence, for example, fish and papaya soup is considered to help produce breastmilk.\n\nOther East Asian cultures, such as South Korean and Vietnamese, have their own versions of \"sitting the month\", combining prescribed foods with proscribed activities. Similar practices are popular among Japanese women called 産後の肥立ち \"\"Sango no hidachi\" and Korean women called 삼칠일 \"Samchilil\"\" for at least 21 days. The new mother is given special postnatal foods, such as seaweed soup in Korea. \"Samchilil\" is practiced in addition to other traditions encompassed in sanhujori, which is Korea's version of postnatal care. During this period of time that could extend beyond the 21 days, women followed principles that emphasize activities and foods that keep the body warm, rest and relaxation to maximize the body’s return to its normal state, maintaining cleanliness, eating nutritious foods, and peace of mind and heart. \n\nIn Thailand \"yu-fai\" (\"lie down by a fire\") treatment is traditional postpartum healing. Performed in an open area, it involves using smokeless tamarind wood, local herbs and massage.\n\nMost traditional Indians follow the 40-day confinement and recuperation period also known as the \"jaappa\" (in Hindi). A special diet to facilitate milk production and increase hemoglobin levels is followed. Sex is not allowed during this time. In Hindu culture, this time after childbirth was traditionally considered a period of relative impurity (\"asaucham\"), and a period of confinement of 10–40 days (known as \"purudu\") was recommended for the mother and the baby. During this period, she was exempted from usual household chores and religious rites. The father was purified by a ritual bath before visiting the mother in confinement. \n\nIn the event of a stillbirth, the period of impurity for both parents was 24 hours. \n\nMany Indian subcultures have their own traditions after birth. This birth period is called Virdi (Marathi), which lasts for 10 days after birth and includes complete abstinence from \"puja\" or temple visits.\n\nThe \"cuarantena\" (literally, forty days, also meaning quarantine) is practised in parts of Latin America, and amongst immigrant communities in the United States. It is described as \"intergenerational family ritual that facilitated adaptation to parenthood\", including some paternal role reversal.\n\nThe term used in English, now old-fashioned or archaic, was once used to name maternity hospitals, for example the General Lying-In Hospital in London. A 1932 Canadian publication refers to lying-in as ranging from two weeks to two months. These weeks ended with the re-introduction of the mother to the community in the Christian ceremony of the churching of women. \n\nLying-in features in Christian art, notably \"Birth of Jesus\" paintings. One of the gifts presented to the new mother in Renaisssance Florence was a \"desco da parto\", a special form of painted tray.\n\nTraditionally, women were taken care of by their elders: their mother, mother-in-law, sister, or aunt. The lying-in hospitals provided an institutional variation which gave women weeks of bedrest and a respite from household chores.\n\nIncreasingly, these older women are unavailable or unwilling to take on this role; given the lingering effects of the one-child policy, many older Chinese women had limited experience of newborn babies, having only had one themselves. Replacements for this familial help are commercial services, both in the home and at residential centres. \n\nAgencies provide specialist carers that come to the new parents' home. This job used to be known as the monthly nurse, as she came and lived with the family for a month. Now more common terms are maternity nurse, newborn care specialist, or confinement nanny; the worker is not a registered health care professional such as the word \"nurse\" usually implies in current English. In Indian English the role is called a \"\"japa\" maid\". A doula is best known as a birth companion, but some provide practical and emotional post-birth support. A lactation consultant and a health visitor are trained health professionals who may assist the new mother at this time. In the Netherlands, the in-home support is known as \"kraamzorg\".\n\nThe use of \"yue sao\", a specialist carer translated in Canada as \"postpartum doula\", is also very common in China. \"Yue Sao\" typically are live-in domestic helpers who care for both the new mother and baby for the first month after birth. Salaries as at 2017 vary from RMB8000 to RMB20000 per month depending on city and experience. They are described as \"mothering the mother\". Australian documentary-maker Aela Callan called them \"Chinese supermums\" but says they are colloquially known as \"confinement ladies\". \n\nCompanies have sprung up to offer extended postpartum care outside the home, sometimes in a hotel-like environment. Luxury options are big business. Private postpartum care centres were introduced to Korea in 1996 under the name of \"sanhujoriwon\". Within the Chinese tradition, specialist businesses such as Red Wall Confinement Centre charge up to $27,000 for one month. Birth tourism centres operating under the radar in the United States for Chinese women offer \"sitting the month\".\n\n\n"}
{"id": "12636929", "url": "https://en.wikipedia.org/wiki?curid=12636929", "title": "Pre-labor", "text": "Pre-labor\n\nPre-labor, also called \"prodromal labor\", consists of the early signs before labor starts. It is the body's preparation for real labor.\n\nProdromal labor has been misnamed as “false labor.\" Prodromal labor begins much as traditional labor but does not progress to the birth of the baby. Not everyone feels this stage of labor, though it does always occur. However, this does not mean that every woman will experience every symptom. The term is used to describe a cluster of physical changes that may take place in a pregnant woman before she goes into \"real\" labor, such as an increase in blood volume (sometimes resulting in edema), Braxton Hicks contractions, the presence of colostrum in the breasts, and the dislodging of the mucous plug that has sealed the cervix during the pregnancy.\n\nThe term \"false labor\" is sometimes used to describe a cluster of Braxton Hicks contractions that are mistaken for real labor.\n\nThe term \"false labor\" and \"false pains\" are sometimes considered equivalent.\n\n"}
{"id": "647286", "url": "https://en.wikipedia.org/wiki?curid=647286", "title": "Prenatal testing", "text": "Prenatal testing\n\nPrenatal testing consists of prenatal screening and prenatal diagnosis, which are aspects of prenatal care that focus on detecting problems with the pregnancy as early as possible. These may be anatomic and physiologic problems with the health of the zygote, embryo, or fetus, either before gestation even starts (as in preimplantation genetic diagnosis) or as early in gestation as practicable. Screening can detect problems such as neural tube defects, chromosome abnormalities, and gene mutations that would lead to genetic disorders and birth defects, such as spina bifida, cleft palate, Downs Syndrome, Tay–Sachs disease, sickle cell anemia, thalassemia, cystic fibrosis, muscular dystrophy, and fragile X syndrome. Some tests are designed to discover problems which primarily affect the health of the mother, such as PAPP-A to detect pre-eclampsia or glucose tolerance tests to diagnose gestational diabetes. Screening can also detect anatomical defects such as hydrocephalus, anencephaly, heart defects, and amniotic band syndrome.\n\nPrenatal screening focuses on finding problems among a large population with affordable and noninvasive methods. Prenatal diagnosis focuses on pursuing additional detailed information once a particular problem has been found, and can sometimes be more invasive. The most common screening procedures are routine ultrasounds, blood tests, and blood pressure measurement. Common diagnosis procedures include amniocentesis and chorionic villus sampling. In some cases, the tests are administered to determine if the fetus will be aborted, though physicians and patients also find it useful to diagnose high-risk pregnancies early so that delivery can be scheduled in a tertiary care hospital where the baby can receive appropriate care.\n\nThere are three purposes of prenatal diagnosis: (1) to enable timely medical or surgical treatment of a condition before or after birth, (2) to give the parents the chance to abort a fetus with the diagnosed condition, and (3) to give parents the chance to prepare psychologically, socially, financially, and medically for a baby with a health problem or disability, or for the likelihood of a stillbirth.\n\nHaving this information in advance of the birth means that healthcare staff as well as parents can better prepare themselves for the delivery of a child with a health problem. For example, Down Syndrome is associated with cardiac defects that may need intervention immediately upon birth.\n\nMany expectant parents would like to know the sex of their baby before birth. Methods include cell-free fetal DNA analysis, amniocentesis with karyotyping, and prenatal ultrasound. In some countries, health care providers are expected to withhold this information from parents, while in other countries they are expected to give this information.\n\nBecause of the miscarriage and fetal damage risks associated with amniocentesis and CVS procedures, many women prefer to first undergo screening so they can find out if the fetus' risk of birth defects is high enough to justify the risks of invasive testing. Since screening tests yield a risk score which represents the chance that the baby has the birth defect, the most common threshold for high-risk is 1:270. A risk score of 1:300 would therefore be considered low-risk by many physicians. However, the trade-off between risk of birth defect and risk of complications from invasive testing is relative and subjective; some parents may decide that even a 1:1000 risk of birth defects warrants an invasive test while others wouldn't opt for an invasive test even if they had a 1:10 risk score.\n\nACOG guidelines currently recommend that all pregnant women, regardless of age, be offered invasive testing to obtain a definitive diagnosis of certain birth defects. Therefore, most physicians offer diagnostic testing to all their patients, with or without prior screening and let the patient decide.\n\nThe following are some reasons why a patient might consider her risk of birth defects already to be high enough to warrant skipping screening and going straight for invasive testing.\n\n\nDiagnostic prenatal testing can be performed by invasive or non-invasive methods. An invasive method involves probes or needles being inserted into the uterus, e.g. amniocentesis, which can be done from about 14 weeks gestation, and usually up to about 20 weeks, and chorionic villus sampling, which can be done earlier (between 9.5 and 12.5 weeks gestation) but which may be slightly more risky to the fetus. One study comparing transabdominal chorionic villus sampling with second trimester amniocentesis found no significant difference in the total pregnancy loss between the two procedures. However, transcervical chorionic villus sampling carries a significantly higher risk, compared with a second trimester amniocentesis, of total pregnancy loss (relative risk 1.40; 95% confidence interval 1.09 to 1.81) and spontaneous miscarriage (9.4% risk; relative risk 1.50; 95% confidence interval 1.07 to 2.11).\n\nNon-invasive techniques include examinations of the woman's womb through ultrasonography and maternal serum screens (i.e. Alpha-fetoprotein). Blood tests for select trisomies (Down syndrome in the United States, Down and Edwards syndromes in China) based on detecting cell-free placental DNA present in maternal blood, also known as non-invasive prenatal testing (NIPT), have become available. If an elevated risk of chromosomal or genetic abnormality is indicated by a non-invasive screening test, a more invasive technique may be employed to gather more information. In the case of neural tube defects, a detailed ultrasound can non-invasively provide a definitive diagnosis.\n\nPrior to conception, couples may elect to have genetic testing done to determine the odds of conceiving a child with a known genetic anomaly. The most common in the caucasian population are:\nHundreds of additional conditions are known and more discovered on a regular basis. However the economic justification for population-wide testing of all known conditions is not well supported, particularly once the cost of possible false positive results and concomitant follow-up testing are taken into account. There are also ethical concerns related to this or any type of genetic testing.\n\nOne or both partners may be aware of other family members with these diseases. Testing prior to conception may alleviate concern, prepare the couple for the potential short- or long-term consequences of having a child with the disease, direct the couple toward adoption or foster parenting, or prompt for preimplantation genetic testing during \"in vitro\" fertilization. If a genetic disorder is found, professional genetic counseling is usually recommended owing to the host of ethical considerations related to subsequent decisions for the partners and potential impact on their extended families. Most, but not all, of these diseases follow Mendelian inheritance patterns. Fragile X syndrome is related to expansion of certain repeated DNA segments and may change generation-to-generation.\n\nAt early presentation of pregnancy at around 6 weeks, early dating ultrasound scan may be offered to help confirm the gestational age of the embryo and check for a single or twin pregnancy, but such a scan is unable detect common abnormalities. Details of prenatal screening and testing options may be provided.\n\nAround weeks 11-13, nuchal translucency scan (NT) may be offered which can be combined with blood tests for PAPP-A and beta-hCG, two serum markers that correlate with chromosomal abnormalities, -in what is called the First Trimester Combined Test. The results of the blood test are then combined with the NT ultrasound measurements, maternal age, and gestational age of the fetus to yield a risk score for Down Syndrome, Trisomy 18, and Trisomy 13. First Trimester Combined Test has a sensitivity (i.e. detection rate for abnormalities) of 82–87% and a false-positive rate around 5%.\n\nThe anomaly scan is performed between 18–22 weeks of gestational age. The International Society of Ultrasound in Obstetrics and Gynecology (ISUOG) recommends that this ultrasound is performed as a matter of routine prenatal care, to measure the fetus so that growth abnormalities can be recognized quickly later in pregnancy, and to assess for congenital malformations and multiple pregnancies (i.e. twins). The scan can detect anencephaly, open spina bifida, cleft lip, diaphragmatic hernia, gastrochisis, omphalocele, congenital heart defect, bilateral renal agenesis, osteochondrodysplasia, Edwards syndrome, and Patau syndrome.\n\nA second trimester Quad blood test may be taken (the Triple test is widely considered obsolete but in some states, such as Missouri, where Medicaid only covers the Triple test, that's what the patient typically gets). With \"integrated screening\", both a First Trimester Combined Test and a Triple/Quad test is performed, and a report is only produced after both tests have been analyzed. However patients may not wish to wait between these two sets of test. With \"sequential screening\", a first report is produced after the first trimester sample has been submitted, and a final report after the second sample. With \"contingent screening\", patients at very high or very low risks will get reports after the first trimester sample has been submitted. Only patients with \"moderate risk\" (risk score between 1:50 and 1:2000) will be asked to submit a second trimester sample, after which they will receive a report combining information from both serum samples and the NT measurement. The First Trimester Combined Test and the Triple/Quad test together have a sensitivity of 88–95% with a 5% false-positive rate for Down Syndrome, though they can also be analyzed in such a way as to offer a 90% sensitivity with a 2% false-positive rate. Finally for patients who do not receive an NT ultrasound in the 1st trimester may still receive a Serum Integrated test involving measuring PAPP-A serum levels in the 1st trimester and then doing a Quad test in the 2nd trimester. This offers an 85–88% sensitivity and 5% false-positive rate for Down Syndrome. Also, patient may skip 1st trimester screening entirely and receive only a 2nd trimester Quad test, with an 81% sensitivity for Down Syndrome and 5% false-positive rate.\n\nFirst trimester maternal serum screening can check levels of free β-hCG, PAPP-A, intact or beta hCG, or h-hCG in the woman's serum, and combine these with the measurement of nuchal translucency (NT). Some institutions also look for the presence of a fetal nasalbone on the ultrasound.\n\nSecond trimester maternal serum screening (AFP screening, triple screen, quad screen, or penta screen) can check levels of alpha fetoprotein, β-hCG, inhibin-A, estriol, and h-hCG (hyperglycosolated hCG) in the woman's serum.\n\nThe triple test measures serum levels of AFP, estriol, and beta-hCG, with a 70% sensitivity and 5% false-positive rate. It is complemented in some regions of the United States, as the \"Quad test\" (adding inhibin A to the panel, resulting in an 81% sensitivity and 5% false-positive rate for detecting Down syndrome when taken at 15–18 weeks of gestational age).\n\nThe biomarkers PAPP-A and β-hCG seem to be altered for pregnancies resulting from ICSI, causing a higher false-positive rate. Correction factors have been developed and\nshould be used when screening for Down’s syndrome in singleton pregnancies after ICSI, but in twin pregnancies such correction factors have not been fully elucidated. In vanishing twin pregnancies with a second gestational sac with a dead fetus, first trimester screening should be based solely on the maternal age and the nuchal translucency scan as biomarkers are altered in these cases.\n\nMeasurement of fetal proteins in maternal serum is a part of standard prenatal screening for fetal aneuploidy and neural tube defects. Computational predictive model shows that extensive and diverse feto-maternal protein trafficking occurs during pregnancy and can be readily detected non-invasively in maternal whole blood. This computational approach circumvented a major limitation, the abundance of maternal proteins interfering with the detection of fetal proteins, to fetal proteomic analysis of maternal blood. Entering fetal gene transcripts previously identified in maternal whole blood into a computational predictive model helped develop a comprehensive proteomic network of the term neonate. It also shows that the fetal proteins detected in pregnant woman’s blood originate from a diverse group of tissues and organs from the developing fetus. Development proteomic networks dominate the functional characterization of the predicted proteins, illustrating the potential clinical application of this technology as a way to monitor normal and abnormal fetal development.\n\nThe difference in methylation of specific DNA sequences between mother and fetus can be used to identify fetal-specific DNA in the blood circulation of the mother. In a study published in March 6, 2011 online issue of \"Nature\", using this non-invasive technique a group of investigators from Greece and UK achieved correct diagnosis of 14 trisomy 21 (Down Syndrome) and 26 normal cases. Using massive parallel sequencing, a study testing for trisomy 21 only, successfully detected 209 of 212 cases (98.6%) with 3 false-positives in 1,471 pregnancies (0.2%). With commercially available non-invasive (blood) testing for Down syndrome having become available to patients in the United States and already available in China, in October 2011, the International Society for Prenatal Diagnosis created some guidance. Based on its sensitivity and specificity, it constitutes an advanced screening test and that positive results require confirmation by an invasive test, and that while effective in the diagnosis of Down syndrome, it cannot assess half the abnormalities detected by invasive testing. The test is not recommended for general use until results from broader studies have been reported, but may be useful in high-risk patients in conjunction with genetic counseling.\n\nA study in 2012 found that the maternal plasma cell-free DNA test was also able to detect Trisomy 18 (Edwards syndrome) in 100% of the cases (59/59) at a false-positive rate of 0.28%, and Trisomy 13 (Patau syndrome) in 91.7% of the cases (11/12) at a false-positive rate of 0.97%. The test interpreted 99.1% of samples (1,971/1,988); among the 17 samples without an interpretation, three were Trisomy 18. The study stated that if z-score cutoffs for Trisomy 18 and 13 were raised slightly, the overall false-positive rates for the three aneuploidies could be as low as 0.1% (2/1,688) at an overall detection rate of 98.9% (280/283) for common aneuploidies (this includes all three trisomies- Down, Edwards and Patau).\n\nInterphase-fluorescence in situ hybridization (FISH), quantitative PCR and direct preparation of chromosomes from chorionic villi are all current methods being used that are the most effective for detecting fetal aneuploidy.\n\nDue to the detection of fetal cells and fetal DNA circulating in maternal blood, noninvasive diagnosis of fetal aneuploidy is becoming more promising. The development of a variety of screening methods for fetal aneuploidy and other chromosomal aberrations is now a prominent research area because of the discovery of circulating fetal nucleic acid in maternal blood plasma. However, the key problem is that circulating fetal nucleated cells comprise only three to six percent of maternal blood plasma DNA. Therefore, two effective approaches have been developed that can be used for the detection of fetal aneuploidy. The first involves the measuring of the allelic ratio of single nucleotide polymorphisms (SNPs) in the mRNA coding region in the placenta. The next approach is analyzing both maternal and fetal DNA and looking for differences in the DNA methylation patterns. \n\nRecently, it has been proposed that digital PCR can used for detection of fetal aneuploidy using fetal DNA and RNA found in maternal blood plasma. Research has shown that digital PCR can be used to differentiate between normal and aneuploid DNA using fetal DNA in the maternal blood plasma.\n\nA variation of the PCR technique called multiplex ligation-dependent probe amplification (MLPA), targeting DNA, has been successively applied for diagnosing fetal aneuploidy as a chromosome- or gene-specific assay.\n\nFetal cell DNA has been directly sequenced using shotgun sequencing technology. This DNA was obtained from the blood plasma of eighteen pregnant women. This was followed by mapping the chromosome using the quantification of fragments. This was done using advanced methods in DNA sequencing resulting in the parallel sequencing of the fetal DNA. The amount of sequence tags mapped to each chromosome was counted. If there was a surplus or deficiency in any of the chromosomes, this meant that there was a fetal aneuploid. Using this method of shotgun sequencing, the successful identification of trisomy 21 (Down syndrome), trisomy 18 (Edward syndrome), and trisomy 13 (Patau syndrome) was possible. This method of noninvasive diagnosis is now starting to be heavily used and researched further.\n\nFetal components in samples from maternal blood plasma can be analyzed by genome-wide techniques not only by total DNA, but also by methylated DNA immunoprecipitation (with tiling array), microRNA (such as with Megaplex) and total RNA (RNA-sequencing).\n\nResearch was conducted to determine how women felt about noninvasive diagnosis of fetal aneuploid using maternal blood. This study was conducted using surveys. It was reported that eighty-two percent of pregnant women and seventy-nine percent of female medical students view this type of diagnosis in a positive light, agreeing that it is important for prenatal care. Overall, women responded optimistically that this form of diagnosis will be available in the future.\n\nIn addition to the direct seeking of chromosomal abnormalities and spina bifida, the blood tests can suggest additional conditions:\n\n\nDisability rights activists and scholars have suggested a more critical view of prenatal testing and its implications for people with disabilities. They argue that there is pressure to abort fetuses that might be born with disabilities, and that these pressures rely on eugenics interests and ableist stereotypes. This selective abortion relies on the ideas that people with disabilities cannot live desirable lives,that they are \"defective,\" and that they are burdens, ignoring the fact that disability scholars argue that \"oppression is what's most disabling about disability.\" Marsha Saxton suggests that women should question whether or not they are relying on real, factual information about people with disabilities or on stereotypes if they decide to abort a fetus with a disability.\n\nIn some genetic conditions, for instance cystic fibrosis, an abnormality can only be detected if DNA is obtained from the fetus. Usually an invasive method is needed to do this.\n\nIf a genetic disease is detected, there is often no treatment that can help the fetus until it is born. However, in the US, there are prenatal surgeries for spina bifida fetus. Early diagnosis gives the parents time to research and discuss post-natal treatment and care, or in some cases, abortion. Genetic counselors are usually called upon to help families make informed decisions regarding results of prenatal diagnosis.\n\nUltrasound of a fetus, which is considered a screening test, can sometimes miss subtle abnormalities. For example, studies show that a detailed 2nd trimester ultrasound, also called a level 2 ultrasound, can detect about 97% of neural tube defects such as spina bifida . Ultrasound results may also show \"soft signs,\" such as an Echogenic intracardiac focus or a Choroid plexus cyst, which are usually normal, but can be associated with an increased risk for chromosome abnormalities.\n\nOther screening tests, such as the Quad test, can also have false positives and false negatives. Even when the Quad results are positive (or, to be more precise, when the Quad test yields a score that shows at least a 1 in 270 risk of abnormality), usually the pregnancy is normal, but additional diagnostic tests are offered. In fact, consider that Down Syndrome affects about 1:400 pregnancies; if you screened 4000 pregnancies with a Quad test, there would probably be 10 Down Syndrome pregnancies of which the Quad test, with its 80% sensitivity, would call 8 of them high-risk. The quad test would also tell 5% (~200) of the 3990 normal women that they are high-risk. Therefore, about 208 women would be told they are high-risk, but when they undergo an invasive test, only 8 (or 4% of the high risk pool) will be confirmed as positive and 200 (96%) will be told that their pregnancies are normal. Since amniocentesis has approximately a 0.5% chance of miscarriage, one of those 200 normal pregnancies might result in a miscarriage because of the invasive procedure. Meanwhile, of the 3792 women told they are low-risk by the Quad test, 2 of them will go on to deliver a baby with Down Syndrome. The Quad test is therefore said to have a 4% positive predictive value (PPV) because only 4% of women who are told they are \"high-risk\" by the screening test actually have an affected fetus. The other 96% of the women who are told they are \"high-risk\" find out that their pregnancy is normal.\n\nBy comparison, in the same 4000 women, a screening test that has a 99% sensitivity and a 0.5% false positive rate would detect all 10 positives while telling 20 normal women that they are positive. Therefore, 30 women would undergo a confirmatory invasive procedure and 10 of them (33%) would be confirmed as positive and 20 would be told that they have a normal pregnancy. Of the 3970 women told by the screen that they are negative, none of the women would have an affected pregnancy. Therefore, such a screen would have a 33% positive predictive value. It's still unfortunate that 20 false-positive women have had to undergo an invasive procedure to find out they have a normal pregnancy, but it's still better than 200 false-positives with the Quad test.\n\nThe real-world false-positive rate for the Quad test (as well as 1st Trimester Combined, Integrated, etc.) is greater than 5%. 5% was the rate quoted in the large clinical studies that were done by the best researchers and physicians, where all the ultrasounds were done by well-trained sonographers and the gestational age of the fetus was calculated as closely as possible. In the real world, where calculating gestational age may be a less precise art, the formulas that generate a patient's risk score are not as accurate and the false-positive rate can be higher, even 10%.\n\nBecause of the low accuracy of conventional screening tests, 5–10% of women, often those who are older, will opt for an invasive test even if they received a low-risk score from the screening. A patient who received a 1:330 risk score, while technically low-risk (since the cutoff for high-risk is commonly quoted as 1:270), might be more likely to still opt for a confirmatory invasive test. On the other hand, a patient who receives a 1:1000 risk score is more likely to feel assuaged that her pregnancy is normal.\n\nBoth false positives and false negatives will have a large impact on a couple when they are told the result, or when the child is born. Diagnostic tests, such as amniocentesis, are considered to be very accurate for the defects they check for, though even these tests are not perfect, with a reported 0.2% error rate (often due to rare abnormalities such as mosaic Down Syndrome where only some of the fetal/placental cells carry the genetic abnormality).\n\nA higher maternal serum AFP level indicates a greater risk for anencephaly and open spina bifida. This screening is 80% and 90% sensitive for spina bifida and anencephaly, respectively.\n\nAmniotic fluid acetylcholinesterase and AFP level are more sensitive and specific than AFP in predicting neural tube defects.\n\nMany maternal-fetal specialists do not bother to even do an AFP test on their patients because they do a detail ultrasound on all of them in the 2nd trimester, which has a 97% detection rate for neural tube defects such as anencephaly and open spina bifida. Performing tests to determine possible birth defects is mandatory in all U.S. states. Failure to detect issues early can have dangerous consequences on both the mother and the baby. OBGYNs may be held culpable. In one case a man who was born with spina fibia was awarded $2 million in settlement, apart from medical expenses, due to the OBGYN's negligence in conducting AFP tests.\n\nNo prenatal test can detect \"all\" forms of birth defects and abnormalities.\n\nAmniocentesis has become the standard of care for prenatal care visits for women who are \"at risk\" or over a certain age. The wide use of amniocentesis has been defined as consumeristic. and some argue that this can be in conflict with the right to privacy, Most obstetricians (depending on the country) offer patients the AFP triple test, HIV test, and ultrasounds routinely. However, almost all women meet with a genetic counselor before deciding whether to have prenatal diagnosis. It is the role of the genetic counselor to accurately inform women of the risks and benefits of prenatal diagnosis. Genetic counselors are trained to be non-directive and to support the patient's decision. Some doctors do advise women to have certain prenatal tests and the patient's partner may also influence the woman's decision.\n\nObstetricians have an ethical duty to properly inform patients of their options, specifically the availability of screening and diagnostic testing. Physicians have been successfully sued by women who gave birth to babies with abnormalities that could have been detected had they known about their screening options, though the plaintiff must also prove that she would have elected to terminate the pregnancy in the event of a positive finding. Also, physicians who fail to inform their patients of the risks of amniocentesis and CVS might be found guilty of negligence informed consent in the event that the patient sues after a procedure-related miscarriage or fetal damage.\n\nThere is a misconception that a physician only needs to do what other physicians typically do (i.e. standard of care). However, in the case of informed consent, the legal standard is more commonly defined as what a reasonable patient would elect to do if she is informed. So if a reasonable patient would want to be screened if only she is informed or if a reasonable patient would want to receive an amniocentesis if only she is informed of that option, then a physician is legally obligated to inform the patient of these options.\n\nAs newer, more accurate screening tests emerge, physicians may need to quickly get up to speed on the most recent data and start informing their patients of the existence of these tests. Failure to inform patients of the available of these more accurate screening tests might result in a wrongful birth or wrongful miscarriage lawsuit if the patient can demonstrate that she would have chosen the newer test, if she had known about it, to avoid the unfortunate outcome that resulted from receiving a conventional screening test or invasive procedure.\n\nResearchers have studied how disclosing aminocentesis or chorionic villous sampling (CVS) results on a fixed date versus a variable date (i.e. \"when available\") affects maternal anxiety. Systematic review of the relevant articles found no conclusive evidence to support issuing aminocentesis results as soon as they become available (in comparison to issuing results on a pre-defined fixed date). The researchers concluded that further studies evaluating the effect of different strategies for disclosing CVS results on maternal anxiety are needed.\n\n\n"}
{"id": "51179429", "url": "https://en.wikipedia.org/wiki?curid=51179429", "title": "Priority-setting in global health", "text": "Priority-setting in global health\n\nIn global health, priority-setting is a term used for the process and strategy of deciding which health interventions to carry out. Priority-setting can be conducted at the disease level (i.e. deciding which disease to alleviate), the overall strategy level (i.e. selective primary healthcare versus primary healthcare versus more general health systems strengthening), research level (i.e. which health research to carry out), or other levels.\n\nPriority-setting is the act of deciding which health interventions to carry out, and can occur at several levels of granularity. Priority-setting can occur at the following levels:\n\n\nSynonymous terms include \"prioritization in health care and health research\", \"priority determination\", \"health priorities\", and \"agenda-setting\".\n\nVarious metrics have been used to compare interventions. These include:\n\n\nPriority-setting can be done by various actors. These include:\n\n\nAccording to Devi Sridhar, professor of global health at the University of Edinburgh, \"the priorities of funding bodies largely dictate what health issues and diseases are studied\".\n\nAccording to Amanda Glassman et al., global-level priority-setting has occurred since at least the 1980s, though these efforts have only focused on a few aspects.\n\nThe following table is a timeline of organizations and programs working on priority-setting.\n\nRudan et al. says that priority-setting efforts have relied on \"consensus reached by panels of experts\" and as a result have not been systematic enough, and that this has \"often made it difficult to present the identified priorities to wider audiences as legitimate and fair\".\n\nGlassman et al. notes that criticisms of priority-setting include \"the weak data on which estimates of burden, cost, and effectiveness relied; the value judgments implicit in disability-adjusted life year age weighting and discounting decisions; and treatment of equity issues, as well as the political difficulties associated with translating a ground zero package into a public budget based on historical inputs\"; and the consideration of only health maximization at the expense of other objectives such as fairness.\n\nGlassman et al. also notes how there are more cost-effectiveness studies for LMICs (in the thousands), but that these are unlikely to be actually applied to priority-setting processes.\n\nJeremy Shiffman has said that some bodies such as the Institute for Health Metrics and Evaluation and \"The Lancet\" are prominent in priority-setting due to their dominion rather than data and analysis, and also notes that the process of creating the Sustainable Development Goals was not sufficiently transparent.\n\n"}
{"id": "788091", "url": "https://en.wikipedia.org/wiki?curid=788091", "title": "Psychological trauma", "text": "Psychological trauma\n\nPsychological trauma is a type of damage to the mind that occurs as a result of a distressing event. Trauma is often the result of an overwhelming amount of stress that exceeds one's ability to cope, or integrate the emotions involved with that experience. Trauma may result from a single distressing experience or recurring events of being overwhelmed that can be precipitated in weeks, years, or even decades as the person struggles to cope with the immediate circumstances, eventually leading to serious, long-term negative consequences.\n\nBecause trauma differs between individuals, according to their subjective experiences, people will react to similar traumatic events differently. In other words, not all people who experience a potentially traumatic event will actually become psychologically traumatized. However, it is possible for some people to develop post-traumatic stress disorder (PTSD) after being exposed to a major traumatic event.\nThis discrepancy in risk rate can be attributed to protective factors some individuals may have that enable them to cope with trauma; they are related to temperamental and environmental factors from among others. Some examples are mild exposure to stress early in life, resilience characteristics, and active seeking of help.\n\nThe Diagnostic and Statistical Manual of Mental Disorders (DSM-IV-TR) defines trauma as direct personal experience of an event that involves actual or threatened death or serious injury; threat to one's physical integrity, witnessing an event that involves the above experience, learning about unexpected or violent death, serious harm, or threat of death, or injury experienced by a family member or close associate. Memories associated with trauma are implicit, pre-verbal and cannot be recalled, but can be triggered by stimuli from the in vivo environment. The person's response to aversive details of traumatic event involve intense fear, helplessness or horror. In children it is manifested as disorganized or agitative behaviors.\n\nTrauma can be caused by a wide variety of events, but there are a few common aspects. There is frequently a violation of the person's core assumptions about the world and their human rights, putting the person in a state of extreme confusion and insecurity. This is seen when institutions depended upon for survival violate, humiliate, betray, or cause major losses or separations instead of evoking aspects like positive self worth, safe boundaries and personal freedom.\n\nPsychologically traumatic experiences often involve physical trauma that threatens one's survival and sense of security. Typical causes and dangers of psychological trauma include harassment, embarrassment, abandonment, abusive relationships, rejection, co-dependence, physical assault, sexual abuse, partner battery, employment discrimination, police brutality, judicial corruption and misconduct, bullying, paternalism, domestic violence, indoctrination, being the victim of an alcoholic parent, the threat or the witnessing of violence (particularly in childhood), life-threatening medical conditions, and medication-induced trauma. Catastrophic natural disasters such as earthquakes and volcanic eruptions, large scale transportation accidents, house or domestic fire, motor vehicle accident, mass interpersonal violence like war, terrorist attacks or other mass victimization like sex trafficking, being taken as a hostage or being kidnapped can also cause psychological trauma. Long-term exposure to situations such as extreme poverty or other forms of abuse, such as verbal abuse, exist independently of physical trauma but still generate psychological trauma.\n\nSome theories suggest childhood trauma can increase one's risk for mental disorders including post-traumatic stress disorder (PTSD), depression, and substance abuse. Childhood adversity is associated with neuroticism during adulthood. \nParts of the brain in a growing child are developing in a sequential and hierarchical order, from least complex to most complex. The brain's neurons are designed to change in response to the constant external signals and stimulation, receiving and storing new information. This allows the brain to continually respond to its surroundings and promote survival. Our five main sensory signals contribute to the developing brain structure and its function. \nInfants and children begin to create internal representations of their external environment, and in particular, key attachment relationships, shortly after birth. Violent and victimizing attachment figures impact infants' and young children's internal representations. The more frequent a specific pattern of brain neurons is activated, the more permanent the internal representation associated with the pattern becomes. This causes sensitization in the brain towards the specific neural network. Because of this sensitization, the neural pattern can be activated by decreasingly less external stimuli. \nChildhood abuse tends to have the most complications with long-term effects out of all forms of trauma because it occurs during the most sensitive and critical stages of psychological development. It could also lead to violent behavior, possibly as extreme as serial murder. For example, Hickey's Trauma-Control Model suggests that \"childhood trauma for serial murderers may serve as a triggering mechanism resulting in an individual's inability to cope with the stress of certain events.\"\n\nOften psychodynamic aspects of trauma are overlooked even by health professionals: \"If clinicians fail to look through a trauma lens and to conceptualize client problems as related possibly to current or past trauma, they may fail to see that trauma victims, young and old, organize much of their lives around repetitive patterns of reliving and warding off traumatic memories, reminders, and affects.\"\n\nPeople who go through these types of extremely traumatic experiences often have certain symptoms and problems afterward. The severity of these symptoms depends on the person, the type of trauma involved, and the emotional support they receive from others. The range of reactions to and symptoms of trauma can be wide and varied, and differ in severity from person to person. A traumatized individual may experience one or several of them.\n\nAfter a traumatic experience, a person may re-experience the trauma mentally and physically, hence trauma reminders, also called triggers, can be uncomfortable and even painful. Re-experiencing can damage people’s sense of safety, self, self-efficacy, as well as their ability to regulate emotions and navigate relationships. They may turn to psychoactive substances including alcohol to try to escape or dampen the feelings. These triggers cause flashbacks, which are dissociative experiences where the person feels as though the events are recurring. Flashbacks can range from distraction to complete dissociation or loss of awareness of the current context. Re-experiencing of symptoms is a sign that the body and mind are actively struggling to cope with the traumatic experience.\n\nTriggers and cues act as reminders of the trauma and can cause anxiety and other associated emotions. Often the person can be completely unaware of what these triggers are. In many cases this may lead a person suffering from traumatic disorders to engage in disruptive behaviors or self-destructive coping mechanisms, often without being fully aware of the nature or causes of their own actions. Panic attacks are an example of a psychosomatic response to such emotional triggers.\n\nConsequently, intense feelings of anger may frequently surface, sometimes in inappropriate or unexpected situations, as danger may always seem to be present due to re-experiencing past events. Upsetting memories such as images, thoughts, or flashbacks may haunt the person, and nightmares may be frequent. Insomnia may occur as lurking fears and insecurity keep the person vigilant and on the lookout for danger, both day and night. Trauma doesn't only cause changes in one's daily functions, but could also lead to morphological changes. Such epigenetic changes can be passed on to the next generation, thus making genetics one of the components of psychological trauma. However, some people are born with or later develop protective factors such as genetics and sex that help lower their risk of psychological trauma.\n\nThe person may not remember what actually happened, while emotions experienced during the trauma may be re-experienced without the person understanding why (see Repressed Memory). This can lead to the traumatic events being constantly experienced as if they were happening in the present, preventing the subject from gaining perspective on the experience. This can produce a pattern of prolonged periods of acute arousal punctuated by periods of physical and mental exhaustion. This can lead to mental health disorders like acute stress and anxiety disorder, traumatic grief, undifferentiated somatoform disorder, conversion disorders, brief psychotic disorder, borderline personality disorder, adjustment disorder, etc.\n\nIn time, emotional exhaustion may set in, leading to distraction, and clear thinking may be difficult or impossible. Emotional detachment, as well as dissociation or \"numbing out\" can frequently occur. Dissociating from the painful emotion includes numbing all emotion, and the person may seem emotionally flat, preoccupied, distant, or cold. Dissociation includes depersonalisation disorder, dissociative amnesia, dissociative fugue, dissociative identity disorder, etc. Exposure to and re-experiencing trauma can cause neurophysiological changes like slowed myelination, abnormalities in synaptic pruning, shrinking of the hippocampus, cognitive and affective impairment. This is significant in brain scan studies done regarding higher order function assessment with children and youth who were in vulnerable environments.\n\nSome traumatized people may feel permanently damaged when trauma symptoms do not go away and they do not believe their situation will improve. This can lead to feelings of despair, transient paranoid ideation, loss of self-esteem, profound emptiness, suicidality, and frequently depression. If important aspects of the person's self and world understanding have been violated, the person may call their own identity into question. Often despite their best efforts, traumatized parents may have difficulty assisting their child with emotion regulation, attribution of meaning, and containment of post-traumatic fear in the wake of the child's traumatization, leading to adverse consequences for the child. In such instances, seeking counselling in appropriate mental health services is in the best interests of both the child and the parent(s).\n\nTrauma can be caused by man-made, technological disasters and natural disasters, including war, abuse, violence, mechanized accidents (car, train, or plane crashes, etc.) or medical emergencies.\n\nResponses to psychological trauma:\nResponse to psychological trauma can be varied based on the type of trauma, as well as socio demographic and background factors.\nThere are several behavioral responses common towards stressors including the proactive, reactive, and passive responses. Proactive responses include attempts to address and correct a stressor before it has a noticeable effect on lifestyle. Reactive responses occur after the stress and possible trauma has occurred and are aimed more at correcting or minimizing the damage of a stressful event. A passive response is often characterized by an emotional numbness or ignorance of a stressor.\n\nThose who are able to be proactive can often overcome stressors and are more likely to be able to cope well with unexpected situations. On the other hand, those who are more reactive will often experience more noticeable effects from an unexpected stressor. In the case of those who are passive, victims of a stressful event are more likely to suffer from long-term traumatic effects and often enact no intentional coping actions. These observations may suggest that the level of trauma associated with a victim is related to such independent coping abilities.\n\nThere is also a distinction between trauma induced by recent situations and long-term trauma which may have been buried in the unconscious from past situations such as childhood abuse. Trauma is sometimes overcome through healing; in some cases this can be achieved by recreating or revisiting the origin of the trauma under more psychologically safe circumstances, such as with a therapist.\n\nFrench neurologist Jean-Martin Charcot argued in the 1890s that psychological trauma was the origin of all instances of the mental illness known as hysteria. Charcot's \"traumatic hysteria\" often manifested as a paralysis that followed a physical trauma, typically years later after what Charcot described as a period of \"incubation\".\nSigmund Freud, Charcot's student and the father of psychoanalysis, examined the concept of psychological trauma throughout his career. Jean Laplanche has given a general description of Freud's understanding of trauma, which varied significantly over the course of Freud's career: \"An event in the subject's life, defined by its intensity, by the subject's incapacity to respond adequately to it and by the upheaval and long-lasting effects that it brings about in the psychical organization\".\n\nThe French psychoanalyst Jacques Lacan claimed that what he called \"The Real\" had a traumatic quality external to symbolization. As an object of anxiety, Lacan maintained that The Real is \"the essential object which isn't an object any longer, but this something faced with which all words cease and all categories fail, the object of anxiety \"par excellence\"\".\n\nAll psychological traumas originate from stress, a physiological response to an unpleasant stimulus. Long term stress increases the risk of poor mental health and mental disorders, which can be attributed to secretion of glucocorticoids for a long period of time. Such prolonged exposure causes many physiological dysfunctions such as the suppression of the immune system and increase in blood pressure. Not only does it affect the body physiologically, but a morphological change in the hippocampus also takes place. Studies showed that extreme stress early in life can disrupt normal development of hippocampus and impact its functions in adulthood. Studies surely show a correlation between the size of hippocampus and one's susceptibility to stress disorders. In times of war, psychological trauma has been known as shell shock or combat stress reaction. Psychological trauma may cause an acute stress reaction which may lead to post-traumatic stress disorder (PTSD). PTSD emerged as the label for this condition after the Vietnam War in which many veterans returned to their respective countries demoralized, and sometimes, addicted to psychoactive substances. The symptoms of PTSD must persist for at least a month for diagnosis. The main symptoms of PTSD consist of four main categories: trauma (i.e. intense fear), reliving (i.e. flashbacks), avoidance behavior (i.e. emotional numbing), and hypervigilance (i.e. continuous scanning of the environment for danger). Research shows that about 60% of the US population reported as having experienced at least one traumatic symptom in their lives, but only a small proportion actually develops PTSD. There is a correlation between the risk of PTSD and whether or not the act was inflicted deliberately by the offender. Psychological trauma is treated with therapy and, if indicated, psychotropic medications.\n\nThe term \"continuous post traumatic stress disorder\" (CTSD) was introduced into the trauma literature by Gill Straker (1987). It was originally used by South African clinicians to describe the effects of exposure to frequent, high levels of violence usually associated with civil conflict and political repression. The term is also applicable to the effects of exposure to contexts in which gang violence and crime are endemic as well as to the effects of ongoing exposure to life threats in high-risk occupations such as police, fire and emergency services.\n\nAs one of the processes of treatment, confrontation with their sources of trauma plays a crucial role. While debriefing people immediately after a critical incident has not been shown to reduce incidence of PTSD, coming alongside people experiencing trauma in a supportive way has become standard practice.\n\nVicarious trauma affects workers who witnesses their clients' trauma. It is more likely to occur in situations where trauma related work is the norm rather than the exception. Listening with empathy to the clients generates feeling, and seeing oneself in clients' trauma may compound the risk for developing trauma symptoms. Trauma may also result if workers witness situations that happen in the course of their work (e.g. violence in the workplace, reviewing violent video tapes.) Risk increases with exposure and with the absence of help seeking protective factors and pre-preparation of preventive strategies.\n\nAs \"trauma\" adopted a more widely defined scope, traumatology as a field developed a more interdisciplinary approach. This is in part due to the field's diverse professional representation including: psychologists, medical professionals, and lawyers. As a result, findings in this field are adapted for various applications, from individual psychiatric treatments to sociological large-scale trauma management. However, novel fields require novel methodologies. While the field has adopted a number of diverse methodological approaches, many pose their own limitations in practical application.\n\nThe experience and outcomes of psychological trauma can be assessed in a number of ways. Within the context of a clinical interview, the risk of imminent danger to the self or others is important to address but is not the focus of assessment. In most cases, it will not be necessary to involve contacting emergency services (e.g., medical, psychiatric, law enforcement) to ensure the individuals safety; members of the individual's social support network are much more critical.\n\nUnderstanding and accepting the psychological state an individual is in is paramount. There are many misconceptions of what it means for a traumatized individual to be in psychological crisis. These are times when an individual is in inordinate amounts of pain and incapable of self-comfort. If treated humanely and respectfully the individual is less likely to resort to self harm. In these situations it is best to provide a supportive, caring environment and to communicate to the individual that no matter the circumstance, the individual will be taken seriously rather than being treated as delusional. It is vital for the assessor to understand that what is going on in the traumatized person's head is valid and real. If deemed appropriate, the assessing clinician may proceed by inquiring about both the traumatic event and the outcomes experienced (e.g., post-traumatic symptoms, dissociation, substance abuse, somatic symptoms, psychotic reactions). Such inquiry occurs within the context of established rapport and is completed in an empathic, sensitive, and supportive manner. The clinician may also inquire about possible relational disturbance, such as alertness to interpersonal danger, abandonment issues, and the need for self-protection via interpersonal control. Through discussion of interpersonal relationships, the clinician is better able to assess the individual's ability to enter and sustain a clinical relationship.\n\nDuring assessment, individuals may exhibit activation responses in which reminders of the traumatic event trigger sudden feelings (e.g., distress, anxiety, anger), memories, or thoughts relating to the event. Because individuals may not yet be capable of managing this distress, it is necessary to determine how the event can be discussed in such a way that will not \"retraumatize\" the individual. It is also important to take note of such responses, as these responses may aid the clinician in determining the intensity and severity of possible post traumatic stress as well as the ease with which responses are triggered. Further, it is important to note the presence of possible avoidance responses. Avoidance responses may involve the absence of expected activation or emotional reactivity as well as the use of avoidance mechanisms (e.g., substance use, effortful avoidance of cues associated with the event, dissociation).\n\nIn addition to monitoring activation and avoidance responses, clinicians carefully observe the individual's strengths or difficulties with affect regulation (i.e., affect tolerance and affect modulation). Such difficulties may be evidenced by mood swings, brief yet intense depressive episodes, or self-mutilation. The information gathered through observation of affect regulation will guide the clinician's decisions regarding the individual's readiness to partake in various therapeutic activities.\n\nThough assessment of psychological trauma may be conducted in an unstructured manner, assessment may also involve the use of a structured interview. Such interviews might include the Clinician-Administered PTSD Scale (CAPS; Blake et al., 1995), Acute Stress Disorder Interview (ASDI; Bryant, Harvey, Dang, & Sackville, 1998), Structured Interview for Disorders of Extreme Stress (SIDES; Pelcovitz et al., 1997), Structured Clinical Interview for DSM-IV Dissociative Disorders- Revised (SCID-D; Steinberg, 1994), and Brief Interview for post-traumatic Disorders (BIPD; Briere, 1998).\n\nLastly, assessment of psychological trauma might include the use of self-administered psychological tests. Individual scores on such tests are compared to normative data in order to determine how the individual's level of functioning compares to others in a sample representative of the general population. Psychological testing might include the use of generic tests (e.g., MMPI-2, MCMI-III, SCL-90-R) to assess non-trauma-specific symptoms as well as difficulties related to personality. In addition, psychological testing might include the use of trauma-specific tests to assess post-traumatic outcomes. Such tests might include the post-traumatic Stress Diagnostic Scale (PDS; Foa, 1995), Davidson Trauma Scale (DTS: Davidson et al., 1997), Detailed Assessment of post-traumatic Stress (DAPS; Briere, 2001), Trauma Symptom Inventory (TSI: Briere, 1995), Trauma Symptom Checklist for Children (TSCC; Briere, 1996), Traumatic Life Events Questionnaire (TLEQ: Kubany et al., 2000), and Trauma-related Guilt Inventory (TRGI: Kubany et al., 1996).\n\nChildren are assessed through activities and therapeutic relationship, some of the activities are play genogram, sand worlds, coloring feelings, Self and Kinetic family drawing, symbol work, dramatic-puppet play, story telling, Briere's TSCC, etc.\n\nA number of psychotherapy approaches have been designed with the treatment of trauma in mind—EMDR, progressive counting (PC), somatic experiencing, biofeedback, Internal Family Systems Therapy, and sensorimotor psychotherapy.\n\nThere is a large body of empirical support for the use of cognitive behavioral therapy for the treatment of trauma-related symptoms, including post-traumatic stress disorder. Institute of Medicine guidelines identify cognitive behavioral therapies as the most effective treatments for PTSD. Two of these cognitive behavioral therapies, prolonged exposure and cognitive processing therapy, are being disseminated nationally by the Department of Veterans Affairs for the treatment of PTSD. Seeking Safety is another type of cognitive behavioral therapy that focuses on learning safe coping skills for co-occurring PTSD and substance use problems. While some sources highlight Seeking Safety as effective with strong research support, others have suggested that it did not lead to improvements beyond usual treatment. Recent studies show that a combination of treatments involving dialectical behavior therapy (DBT), often used for borderline personality disorder, and exposure therapy is highly effective in treating psychological trauma. If, however, psychological trauma has caused dissociative disorders or complex PTSD, the trauma model approach (also known as phase-oriented treatment of structural dissociation) has been proven to work better than simple cognitive approach. Studies funded by pharmaceuticals have also shown that medications such as the new anti-depressants are effective when used in combination with other psychological approaches.\n\nTrauma therapy allows processing trauma-related memories and allows growth towards more adaptive psychological functioning. It helps to develop positive coping instead of negative coping and allows the individual to integrate upsetting-distressing material (thoughts, feelings and memories) and to resolve these internally. It also aids in growth of personal skills like resilience, ego regulation, empathy, etc.\n\nProcesses involved in trauma therapy are:\n\n\nA number of complementary approaches to trauma treatment have been implicated as well, including yoga and meditation. Trauma-sensitive yoga has been specifically developed for the purposes of use with traumatized individuals.\n\n\n"}
{"id": "2497276", "url": "https://en.wikipedia.org/wiki?curid=2497276", "title": "Public health surveillance", "text": "Public health surveillance\n\nPublic health surveillance (also epidemiological surveillance, clinical surveillance or syndromic surveillance) is, according to the World Health Organization (WHO), \"the continuous, systematic collection, analysis and interpretation of health-related data needed for the planning, implementation, and evaluation of public health practice.\" Public health surveillance may be used to \"serve as an early warning system for impending public health emergencies; document the impact of an intervention, or track progress towards specified goals; and monitor and clarify the epidemiology of health problems, to allow priorities to be set and to inform public health policy and strategies.\"\n\nPublic Health surveillance systems can be passive or active. A passive surveillance system consists of the regular, ongoing reporting of diseases and conditions by all health facilities in a given territory. An active surveillance system is one where health facilities are visited and health care providers and medical records are reviewed in order to identify a specific disease or condition. Passive surveillance systems are less time-consuming and less expensive to run but risk under-reporting of some diseases. Active surveillance systems are most appropriate for epidemics or where a disease has been targeted for elimination.\n\nTechniques of public health surveillance have been used in particular to study infectious diseases. Many large institutions, such as the WHO and the CDC, have created databases and modern computer systems (public health informatics) that can track and monitor emerging outbreaks of illnesses such as influenza, SARS, HIV, and even bioterrorism, such as the 2001 anthrax attacks in the United States.\n\nMany regions and countries have their own cancer registry, one function of which is to monitor the incidence of cancers to determine the prevalence and possible causes of these illnesses.\n\nOther illnesses such as one-time events like stroke and chronic conditions such as diabetes, as well as social problems such as domestic violence, are increasingly being integrated into epidemiologic databases called disease registries that are being used in the cost-benefit analysis in determining governmental funding for research and prevention.\n\nSystems that can automate the process of identifying adverse drug events, are currently being used, and are being compared to traditional written reports of such events. These systems intersect with the field of medical informatics, and are rapidly becoming adopted by hospitals and endorsed by institutions that oversee healthcare providers (such as JCAHO in the United States). Issues in regard to healthcare improvement are evolving around the surveillance of medication errors within institutions.\n\nSyndromic surveillance is the analysis of medical data to detect or anticipate disease outbreaks. According to a CDC definition, \"the term 'syndromic surveillance' applies to surveillance using health-related data that precede diagnosis and signal a sufficient probability of a case or an outbreak to warrant further public health response. Though historically syndromic surveillance has been utilized to target investigation of potential cases, its utility for detecting outbreaks associated with bioterrorism is increasingly being explored by public health officials.\"\n\nThe first indications of disease outbreak or bioterrorist attack may not be the definitive diagnosis of a physician or a lab.\n\nUsing a normal influenza outbreak as an example, once the outbreak begins to affect the population, some people may call in sick for work/school, others may visit their drug store and purchase medicine over the counter, others will visit their doctor's office and other's may have symptoms severe enough that they call the emergency telephone number or go to an emergency department.\n\nSyndromic surveillance systems monitor data from school absenteeism logs, emergency call systems, hospitals' over-the-counter drug sale records, Internet searches, and other data sources to detect unusual patterns. When a spike in activity is seen in any of the monitored systems disease epidemiologists and public health professionals are alerted that there may be an issue.\n\nAn early awareness and response to a bioterrorist attack could save many lives and potentially stop or slow the spread of the outbreak. The most effective syndromic surveillance systems automatically monitor these systems in real-time, do not require individuals to enter separate information (secondary data entry), include advanced analytical tools, aggregate data from multiple systems, across geo-political boundaries and include an automated alerting process.\n\nA syndromic surveillance system based on search queries was first proposed by Gunther Eysenbach, who began work on such a system in 2004.\nInspired by these early, encouraging experiences, Google launched Google Flu Trends in 2008. More flu-related searches are taken to indicate higher flu activity. The results closely match CDC data, and lead it by 1–2 weeks. The results appeared in Nature. More recently, a series of more advanced linear and nonlinear approaches to influenza modelling from Google search queries have been proposed. Extending Google's work researchers from the Intelligent Systems Laboratory (University of Bristol, UK) created Flu Detector; an online tool which based on Information Retrieval and Statistical Analysis methods uses the content of Twitter to nowcast flu rates in the UK.\n\nInfluenzanet is a syndromic surveillance system based on voluntary reports of symptoms via the internet. Residents of the participant countries are invited to provide regular reports on the presence or absence of flu related symptoms. The system has been in place and running since 2003 in the Netherlands and Belgium. The success of this first initiative led to the implementation of Gripenet in Portugal in 2005 followed by Italy in 2008 and Brasil, Mexico, and the United Kingdom in 2009.\n\nSome conditions, especially chronic diseases such as diabetes mellitus, are supposed to be routinely managed with frequent laboratory measurements. Since many laboratory results, at least in Europe and the US, are automatically processed by computerized laboratory information systems, the results are relatively easy to inexpensively collate in special purpose databases or disease registries. Unlike most syndromic surveillance systems, in which each record is assumed to be independent of the others, laboratory data in chronic conditions can be theoretically linked together at the individual patient level. If patient identifiers can be matched, a chronological record of each patient's laboratory results can be analyzed as well as aggregated to the population level.\n\nLaboratory registries allow for the analysis of the incidence and prevalence of the target condition as well as trends in the level of control. For instance, an NIH-funded program called the Vermedx Diabetes Information System maintained a registry of laboratory values of diabetic adults in Vermont and northern New York State in the US with several years of laboratory results on thousands of patients. The data included measures of blood sugar control (glycosolated hemoglobin A1C), cholesterol, and kidney function (serum creatinine and urine protein), and were used to monitor the quality of care at the patient, practice, and population levels. Since the data contained each patient's name and address, the system was also used to communicate directly with patients when the laboratory data indicated the need for attention. Out of control test results generated a letter to the patient suggesting they take action with their medical provider. Tests that were overdue generated reminders to have testing performed. The system also generated reminders and alerts with guideline-based advice for the practice as well as a periodic roster of each provider's patients and a report card summarizing the health status of the population. Clinical and economic evaluations of the system, including a large randomized clinical trial, demonstrated improvements in adherence to practice guidelines and reductions in the need for emergency department and hospital services as well as total costs per patient. The system has been commercialized and distributed to physicians, insurers, employers and others responsible for the care of chronically ill patients. It is now being expanded to other conditions such as chronic kidney disease.\n\nA similar system, The New York City A1C Registry, is in used to monitor the estimated 600,000 diabetic patients in New York City, although unlike the Vermont Diabetes Information System, there are no provisions for patients to have their data excluded from the NYC database. The NYC Department of Health and Mental Hygiene has linked additional patient services to the registry such as health information and improved access to health care services. As of early 2012, the registry contains over 10 million test results on 3.6 million individuals. Although intended to improve health outcomes and reduce the incidence of the complications of diabetes, a formal evaluation has not yet been done.\n\nIn May 2008, the City Council of San Antonio, Texas approved the deployment of an A1C registry for Bexar County. Authorized by the Texas Legislature and the state Health Department, the San Antonio Metropolitan Health District implemented the registry which drew results from all the major clinical laboratories in San Antonio. The program was discontinued in 2010 for lack of funds.\n\nLaboratory surveillance differs from population-wide surveillance in that it can only monitor patients who are already receiving medical treatment and therefore having lab tests done. For this reason, it does not identify patients who have never been tested. Therefore, it is more suitable for quality management and care improvement than for epidemiological monitoring of an entire population or catchment area.\n\n"}
{"id": "51597941", "url": "https://en.wikipedia.org/wiki?curid=51597941", "title": "Puppy nutrition", "text": "Puppy nutrition\n\nThe developmental life stage of dogs requires a specific intake of nutrients to ensure proper growth and development and to meet energy requirements. Despite the fact that puppies have different nutritional requirements compared to their adult counterparts, of the 652 breeders surveyed in the United States and Canada in 2012, 8.7% report feeding puppies commercial diets not intended for the developmental life stage of canines. Large and small dog breeds have even more specific nutrient requirements during growth, such as adjusted calcium to phosphorus ratio, and as such should receive a breed specific growth formula. Feeding diets formulated by a nutritionist for specific breed and life stage differences in nutrient requirements ensures a growing puppy will receive the proper nutrition associated with appropriate skeletal, neurological and immune development. This includes nutrients such as protein, fibre, essential fatty acids, calcium and vitamin E. It is therefore important to feed puppies a diet that meets the minimum and/or maximum requirements established by the National Research Council.\n\nThe nutritional requirements determined by the NRC are based on scientific evidence and used as the basis for nutritional adequacy in cats and dogs. However, these values are based on the assumption that the availability and digestibility of the nutrients are not variable, although in reality, this is not the case. The Association of American Feed Control Officials (AAFCO) also has recommended nutrient levels, but their values serve primarily as regulatory guidance. AAFCO bases their recommendations on feeding trials and are not necessarily supported by scientific evidence; however their nutritional adequacy statement on pet food bags is considered an important part of the label because their recommendations account for ingredient variability. Other agencies involved in pet food regulations include the FDA in the United States who directly regulates the sales of pet food, the FEDIAF in Europe and PFIAA in Australia who recommend regulatory requirements for the pet food industry, as well as others. When selecting puppy food, it is important to consult the labels and ensure products meet the standards of regulatory agencies of your respective country.\n\nYoung growing dogs require greater amounts of energy per unit body mass than fully grown adult dogs. From time of weaning until the puppy reaches 40% of the adult body weight, the optimal energy intake per unit body weight is twice that of an adult dog of the same breed. From 40% to 80% of adult body weight, energy requirements decrease to 1.6 times the adult requirement, and from 80% to the end of growth, this decreases further to 1.2 times the adult energy requirement. Because of this, it is important to ensure that puppy diets contain higher amounts of energy than adult dog foods. However, over nutrition associated with feeding \"ad libitum\" results in accelerated skeletal growth and weight gain leading to osteopenia, especially in large breed dogs. Therefore, appropriate energy requirements should be met, but not exceeded in the growing dog.\n\nOwners and puppy raisers should monitor their puppy’s weight gain by referencing a 9-point body condition score scale which is easily accessible on the internet, regularly weighing the puppy, and consulting with their veterinarian. Owners should also be mindful as to the energy level of their puppy; high energy level puppies will require more energy and may need to be fed extra amounts of food, just as lower energy puppies may require less food. The 9 point body condition score has been shown to be an effective method for estimating percent body fat in dogs. Therefore, by adhering to the 9 point body condition scale, and adjusting food intake accordingly, owners will be able to maintain their puppy at an appropriate weight throughout its growth. In addition to visual estimates of body condition, physical palpation of the pet can provide insight on general health and weight. Speaking to a licensed veterinarian can be very beneficial, as they can help develop these dietary and weight monitoring skills.\n\nAn important aspect of the composition of puppy food is the calcium and phosphorus content. Bone mineral is composed mainly of calcium, which functions in skeletal mineralization during growth. Puppies younger than 5 months are not able to adjust the absorption of calcium in response to intake, therefore an oversupply or undersupply can be harmful. When raised on diets deficient in calcium, pathological bone fractures can result. These fractures have been shown to occur in smaller breed puppies when their diet contained less than 0.33% calcium on a dry matter (DM) basis. Larger breeds present with fractures at a calcium level of less than 0.55%, thus requiring these breeds to have greater calcium requirements.\nThe ratio of calcium to phosphorus greatly impacts the retention of phosphorus. A ratio of 1.3:1 will allow for good phosphorus retention, but levels above a ratio of 2:1 will decrease phosphorus retention. Raising puppies on diets containing excess calcium and phosphorus will result in osteochondrosis which disturbs bone remodeling and cartilage and bone maturation. These changes are noticed in larger breed puppies at lower levels of excess than in smaller breeds. Because large breed puppies are more sensitive to deficient and excess calcium and phosphorus levels, it is important to ensure that their diets contain sufficient amounts of both. The minimum requirement laid out by the NRC is 8.0g/kg DM calcium (0.8%), with a 12g/kg DM (1.2%) recommended allowance. However, recommendations laid out by AAFCO consist of slightly higher calcium values; 1.2% to 1.8% on a DM basis, whereas phosphorus levels should be between 1.0% and 1.6%, with a ratio of calcium to phosphorus between 1:1 and 2:1. Many commonly used ingredients, such as corn, which are added in high amounts to commercial dog foods do not contain adequate amounts of calcium, and as such, calcium supplements are often added to the formulation to ensure proper amounts for development. In addition, a descriptive study of over-the-counter maintenance dog foods determined that 4/45 of the foods studied contained calcium concentrations that exceeded the AAFCO recommendation, but were labeled for all life stages. Therefore, it is important to feed a diet specifically recommended for the developmental life stage and consult the calcium and phosphorus levels in puppy foods.\n\nFat is a nutrient that provides more energy per gram than all other nutrients. Fat provides 9.4kcal/g of gross energy (GE) compared to protein and carbohydrate which only provide 5.56 and 4.15 kcal/g respectively. Due to this greater energy concentration and the higher energy demand of puppies, the higher fat content of canine development diets helps reach these increased energy requirements while also providing the essential fatty acids necessary for the growing dog. Omega-3 (n-3) fatty acids are an important component of puppy diets. Multiple studies suggest brain and retinal function are dependent on the level of n-3 polyunsaturated fatty acids acquired during in utero development and postnatal life. DHA is the main n-3 fatty acid and can be obtained as DHA itself from dietary sources such as fish and fish oils, or as the DHA precursor linolenic acid. With a high DHA intake, plasma lipid DHA content increases, aiding neurological development, as demonstrated by increases in memory and learning ability. Other benefits of n-3 and DHA include promotion of neurogenesis, increased neuron size and phospholipid synthesis, as well as protection against cell death and peroxidative brain damage by damaging free radicals in the rodent model and presumably across the mammalian kingdom. A deficit of dietary n-3 fatty acids leads to a reduction in brain DHA content by 50-80%, leading to cognitive deficits and increased n-6 fatty acid level which increases inflammation. Adequate intake and recommended allowance levels of 85g/kg (DM) of total fat (8.5%), and 0.5g/kg (DM) of omega 3 fatty acids (EPA+DHA) are laid out by the NRC. Both of these values are also laid out as recommendations by AAFCO. However, according to a study on the effects of supplementing DHA in puppy diets, diets containing 1.4% n-3 fatty acids, or 0.19% DHA resulted in optimal neurological function.\n\nAlong with DHA, vitamin E also supports the proper development of eyesight, reduces oxidative damage, and provides additional immune function support. During instances of immune system stimulation, such as vaccinations and infection, immune cells generate more tissue damaging free radicals. These free radicals are reduced by the action of antioxidants such as vitamin E, which promote the formation and maintenance of healthy immune cells. The adequate intake level laid out by the NRC consists of 24 mg/kg (36 IU/kg) of vitamin E, with a recommended allowance of 30 mg/kg (45 IU/kg). However, the regulatory guidance laid out by AAFCO states a vitamin E minimum recommendation of 50 IU/kg DM. Nonetheless, these values have been shown to be insufficient for immune protection; immune function is best optimized at a level of 500 IU/kg. In addition to prevention of free radical damage, this higher level of vitamin E greatly increases the number of memory CD4+ immune cells, aiding in a greater and a longer response to infection. Due to their still-developing immune system, puppies are more susceptible to infection than adult dogs, and proper levels of vitamin E, as indicated above, are needed in their diet.\n\nAlthough young growing dogs have immature gastrointestinal tracts, they do contain microflora which can ferment fibre and generate short chain fatty acids beneficial to gut health. Colonization and establishment of these bacterial populations happens over time, beginning immediately after birth. Even with the ability for fermentation, addition of fibre to the diet should be done carefully, as fibre will dilute other nutrients in the feed. There are different types of fibre, all are made of carbohydrates, and NRC classifies them into fermentable and non-fermentable carbohydrates. Fermentable carbohydrates are generally 3-10 monosaccharide units joined by glycosidic bonds which cannot be degraded by endogenous enzymes and require the fermentation processes of intestinal microflora for proper degradation. Non-fermentable fibres are carbohydrate structures which are not fully fermented even by intestinal microflora. Little research has been done on non-fermentable fibre, however it is known that it causes fecal bulking and reduced intestinal transit time. It has been shown that addition of non-fermentable fibre in concentrations of 22% or greater of the total diet will reduce growth, feed intake and feed efficiency.\n\nFermentable fibre is more beneficial for growing puppies, and it is often considered a prebiotic, meaning it supports the growth of beneficial microflora species in the intestinal tract. Fermentable fibre often contains fructo-oligosaccharides (FOS) and mannanoligosaccharides (MOS), both of which greatly stimulate gut health. FOS are fermented by beneficial bacteria such as bifidobacteria promoting their growth, while most pathogenic bacterial strains such as E. coli, C. perfringens and Salmonella are unable to ferment it. MOS has been shown to stimulate the immune system and increase resistance to pathogenic bacteria. Beet pulp is generally the standard fermentable fibre used in canine diets, however blends of fermentable fibre sources have been shown to be as effective at promoting gut health. Although fermentable fibre is extremely beneficial, it should still be added in moderate amounts as addition of fermentable fibre to a concentration of 14.1% or greater will result in lower digestibility of the feed.\n\nGrowing puppies require higher levels of protein than adult dogs of the same breed to promote proper growth and development. Protein should account for at least 25% of energy; however protein requirements also depend on the digestibility of the protein and age of the puppy. Amino acid and nitrogen requirements decrease between 10–14 weeks, indicating that different protein levels are often beneficial before and after 14 weeks of age. Before 14 weeks of age, a protein level of 250 g/kg of diet (25% of the diet) containing 4.0 kcal ME/g will result in optimal growth. After 14 weeks of age, protein requirements decrease to 200 g/kg of diet (20% of the diet) for optimal growth.\n\nProtein in excess of these levels is metabolized and leads to an increase of the glomerular filtration rate, and increased urea excretion in the urine, with no evidence of damage to the kidneys. However, addition of protein past these requirements is not recommended, although the NRC has not quantified a safe upper limit. Considering that the energy density of protein is similar to that of carbohydrates, an excess of protein in the diet may lead to fat deposition and weight gain. This excess weight can lead to abnormal joint development, potentially giving rise to joint issues in the future. Additionally, excess of some amino acids may also have detrimental impacts; for example, excess lysine will negatively antagonise arginine. The minimal requirement laid out by the NRC is 180g/kg (18%) DM of crude protein, with a recommended allowance of 225g/kg (22.5%) DM. With this in mind, it is also important to consider the quality and digestibility of the protein used.\n\nTo date, there is little information available about minimum requirements of dietary iodine for developing puppies. However, literature suggests that overdosage of iodine in puppies has deleterious effects on the thyroid gland. High dietary iodine may cause both hypothyroidism and changes in bone metabolism, considering that thyroid hormones are key factors in osteogenesis, as they stimulate osteoblasts, and promote collagen synthesis and mineralization of the osteoid. This is particularly important during juvenile development, as these hormones favor the action of growth hormone (GH) and its facilitator, insulin growth factor-1 (IGF-1) at the osteoplastic level. Hypothyroidism acquired at an early age presents a delay in the maturation and development of the ossification sites of newly-developing bone. It is therefore important to closely monitor the amount of iodine in puppy diets in order to prevent overdosage. The NRC states an adequate intake and recommended allowance of 900 μg/kg (DM) which translates into 220μg/1000 kcal. These values have been shown in multiple studies to be the recommended maximum allowance of iodine that would result in no abnormalities.\n\nCanine development diets are designed specifically to meet the energy demands and nutritional requirements for healthy growth. Proper nutrition is imperative to support development of bones, joints, muscles, and the immune system, and includes the addition of nutrients such as protein, essential fatty acids, calcium, fibre, vitamin E, and others. Ensuring optimal energy intake allows the diet to meet the high energy demands of puppies while avoiding over nutrition, over-accelerated growth, and unhealthy weight gain. Dietary fats also help to meet these high energy demands and provide the essential fatty acids necessary for brain, neuron, and retinal development and function. Since growing puppies require greater amounts of protein than their adult counterparts, it is important to include the appropriate amounts to support healthy development. Correct levels of both fermentable and non-fermentable fibre help to support GI and immune health. During growth, young dogs are more susceptible to infection, but the addition of proper levels of vitamin E to the diet reduces free radical oxidative damage and leads to an increase in immunity. Calcium and phosphorus, in the appropriate amounts and ratio, aid in proper bone and cartilage growth and maturation. It is imperative to be aware of the levels of other minerals as well, as the over-dosage of iodine can lead to hypothyroidism, and the delay in maturation and development of bones. It is therefore important to feed a diet that meets the nutrient minimum and/or maximum requirements established by the National Research Council in order to ensure optimal growth and development of young dogs of all breeds.\n"}
{"id": "9819863", "url": "https://en.wikipedia.org/wiki?curid=9819863", "title": "Race and health", "text": "Race and health\n\nRace and health refers to the relationship between individual health and one's race and ethnicity. Differences in health status, health outcomes, life expectancy, and many other indicators of health in different racial and ethnic groups are well documented, and referred to as racial health disparities. Some individuals in certain racial groups receive less care, have less access to resources, and live shorter lives in general. \n\nRace is a complex concept, and the two major competing theories of race use biological definitions and social construction to define racial difference. A \"race\" is also viewed as physically distinguishable populations that share a common ancestry. Although this relationship can vary depending on the definitions used, race is generally used in the context of health research as a fluid concept to group populations of people according to various factors that include but are not limited to ancestry, social identity, visible phenotype, and genetic makeup. \n\nDeterminants of health include environmental, social, and genetic factors, as well as the person's individual characteristics and behaviors. Race and medicine are related additionally by how environmental factors and physiological factors respond to each other. Additionally, how genetic needs coerce with environmental standings.\n\nHealth disparities refer to gaps in the quality of health and health care across racial and ethnic groups. The US Health Resources and Services Administration defines health disparities as \"population-specific differences in the presence of disease, health outcomes, or access to health care\". Health is measured through variables such as life expectancy and incidence of diseases.\n\nHow researchers view race is often linked to how we address racial disparities because the national administrator of health uses these research findings to implement policies.\n\nFor more information, see the Health equity page's section on \"Ethnic and racial disparities\".\n\nThere are various paradigms used to discuss race, including biological and social views. Definitions have changed throughout history to yield a modern understanding of race that is complex and fluid. Moreover, there is no one definition that stands, as there are many competing and interlocking ways to look at race. The terms race, genetic population, ethnicity, geographic population, and ancestry are used interchangeably in everyday discourse involving race.\n\nBiological definitions of race encompass essentialist and anti-essentialist views. The scientific community does not universally accept a single definition of race. Essentialism is a mode of thought that uses scientific data to argue that racial groups are genetically distinct populations. Essentialists describe \"races as groups of people who share certain innate, inherited biological traits, aka use of biological evidence to demonstrate racial differences\". As its counterpart, anti-essentialism uses biological evidence to demonstrate that \"race groupings do not reflect patterns of human biological variation, countering essentialist claims to the contrary\".\n\nSocial definitions are commonly constructionist. Racial groups are \"constructed\" from differing historical, political, and economic contexts, rather than corresponding to inherited, biological variations. Proponents of the constructionist view claim that biological definitions have been used to justify racism in the past and still have the potential to be used to encourage racist thinking in the future.\n\nSocial views also better explain the ambiguity of racial definitions. An individual may self-identify as one race based on one set of determinants (for example, phenotype, culture, ancestry) while society may ascribe the person otherwise based on external forces and discrete racial standards. Dominant racial conceptions influence how individuals label both themselves and others within society. Modern human populations are becoming more difficult to define within traditional racial boundaries due to racial admixture. Most scientific studies, applications, and government documents ask individuals to self-identify race from a limited assortment of common racial categories. The conflict between self-identification and societal ascription further complicates biomedical research and public health policies. However complex its sociological roots, race has real biological ramifications; the intersection of race, science, and society permeates everyday life and influences human health via genetics, access to medical care, diagnosis, and treatment.\n\nThere are a myriad of factors that affect health disparities between racial groups. Among these factors are genetic differences within racial populations, cultural mores, and social and environmental factors. For instance, in some populations where perceptions of race are linked with socioeconomic disparities and differences, the access to care can be considerably lesser. Dually, bias in healthcare systems themselves can perpetuate non-biologically founded higher instances of disease. A study conducted by the National Health Service checks program in the United Kingdom, which aims to increase diagnosis across demographics, noted that \"the reported lower screening in specific black and minority ethnic communities... may increase inequalities in health.\" In this specific case, the lack of attention to certain demographics can be seen as a cause of increased instances of disease from this lack of proper, equal preventative care. One must consider these external factors when evaluating statistics on the prevalence of disease in populations, even though genetic components can play a role in predispositions to contracting some illnesses. These linkages should be more closely evaluated than by ethnicity. Overall, health disparities, which include variations in life expectancy and prevalence of disease, due to the differences in health conditions across various ethnic populations that can be attributed to inequalities in living environment and access to healthcare may also play a role.\n\nThere are many single gene genetic disorders that differ in frequency between different populations due to the region, though many assume it to be solely based on race. Examples include:\n\n\nMany diseases differ in frequency between different populations. However, complex diseases are affected by multiple factors, both genetic and environmental. There is controversy over the extent to which some of these conditions are influenced by genes, and ongoing research aims to identify which genetic loci, if any, are linked to these diseases. \"Risk is the probability that an event will occur. In epidemiology, it is most often used to express the probability that a particular outcome will occur following a particular exposure.\" Different populations are considered \"high-risk\" or \"low-risk\" groups for various diseases due to the probability of that particular population being more exposed to certain risk factors. Beyond genetic factors, history and culture, as well as current environmental and social conditions, influence a certain populations' risk for specific diseases.\n\nGroups may differ in how a disease progresses. Black men who were diagnosed with HIV generally fared worse than their white and Hispanic counterparts. The percentage of men studied with very low CD4+ T-cell count, defined as fewer than 50 cells per microliter, at AIDS diagnosis was 24.1% for white men, 27.8% for Hispanic men, and 34.4% for Black men. Black men were also significantly less likely to be alive three years after diagnosis (80.6%) than Hispanic or white men, who had 85.2% and 84.5% survival rates, respectively. However, the reasons for these differences are not clear, and should not be understood as an essential difference between races, but rather as effects of social and environmental factors.\n\nGenetics have been proven to be a strong predictor for common diseases such as cancer, cardiovascular disease (CVD), diabetes, autoimmune disorders, and psychiatric illnesses, and geneticists have been able to determine that \"human genetic variation is geographically structured\". The different geographic regions correlate with different races, which is logical when looking at the \"Out of Africa\" theory and understanding how changes in genetics of a population acquire over time.\n\nExamples of diseases which are more common in certain racial groups are sickle-cell disease in African and Mediterranean populations, and cystic fibrosis and hemochromatosis in northern European populations. This information can be made available to people by their physicians, and is useful in prevention or early detection of these diseases. For instance, a baby with sickle cell disease should be more closely monitored throughout their life than a baby without it.\n\nWhile genetics certainly play a role in determining how susceptible a person is to specific diseases, environmental factors and lifestyle play a large role as well. For this reason, it is impossible to discern exactly what causes a person to acquire a disease, but it's clear that a mixture of all 3 factors are at play. Each person's health is unique, as they have different genetic compositions and life histories.\n\nThe science of 'ethnic health', or research into ethnicity and health, and the development of services which are culturally competent to meet the specific health care needs of minority ethnic groups, is still in its infancy. In the United States, the Office of Minority Health provides useful links and supports research and development relating to the needs of America's ethnic minorities. In the United Kingdom, the National Health Service established a specialist collection on Ethnicity & Health. This resource was supported by the National Institute for Health and Clinical Excellence (NICE) as part of the UK NHS Evidence initiative NHS Evidence. Similarly, there are growing numbers of resource and research centers which are seeking to provide this service for other national settings, such as Multicultural Mental Health Australia.\n\n\"Race-based medicine\" is the term for medicines that are targeted at specific ethnic clusters which are shown to have a propensity for a certain disorder. The first example of this in the U.S. was when BiDil, a medication for congestive heart failure, was licensed specifically for use in American patients that self-identify as black. Previous studies had shown that African American patients with congestive heart failure generally respond less effectively to traditional treatments than white patients with similar conditions.\n\nAfter two trials, BiDil was licensed exclusively for use in African American patients. Critics have argued that this particular licensing was unwarranted, since the trials did not in fact show that the drug was more effective in African Americans than in other groups, but merely that it was more effective in African Americans than other similar drugs. It was also only tested in African American males, but not in any other racial groups or among women. This peculiar trial and licensing procedure has prompted suggestions that the licensing was in fact used as a race-based advertising scheme.\n\nCritics are concerned that the trend of research on race-specific pharmaceutical treatments will result in inequitable access to pharmaceutical innovation and smaller minority groups may be ignored. This has led to a call for regulatory approaches to be put in place to ensure scientific validity of racial disparity in pharmacological treatment.\n\nAn alternative to \"race-based medicine\" is personalized medicine that involves identifying genetic, genomic (i.e., genomic sequencing), and clinical information—as opposed to using race as a proxy for these data—to better predict a patient's predisposition to certain diseases.\n\nA positive correlation between minorities and a socio economic status of being low income in industrialized and rural regions of the U.S. depict how low income communities tend to include more individuals that have a lower educational background, most importantly in health. Income status, diet, and education all construct a higher burden for low income minorities, to be conscious about their health. Research conducted by medical departments at universities in San diego, Miami, Pennsylvania, and North Carolina suggested that minorities in regions where lower socioeconomic status is common, there was a direct relationship with unhealthy diets and greater distance of supermarkets. Therefore, in areas where supermarkets are less accessible (food deserts) to impoverished areas, the more likely these groups are to purchase inexpensive fast food or just follow an unhealthy diet. As a result, because food deserts are more prevalent in low income communities, minorities that reside in these areas are more prone obesity, which can lead to diseases such as chronic kidney disease, hypertension, or diabetes.\n\nFurthermore, this can also occur when minorities living in rural areas undergoing urbanization, are introduced to fast food. A study done in Thailand focused on urbanized metropolitan areas, the students who participated in this study as were diagnosed as “non-obese” in their early life according to their BMI, however were increasingly at risk of developing Type 2 Diabetes, or obesity as adults, as opposed to young adults who lived in more rural areas during their early life. Therefore, early exposure to urbanized regions can encourage unhealthy eating due to widespread presence of inexpensive fast food. Different racial populations that originate from more rural areas and then immigrate to the urbanized metropolitan areas can develop a fixation for a more westernized diet; this change in lifestyle typically occurs due to loss of traditional values when adapting to a new environment. For example, a 2009 study named CYKIDS was based on children from Cyprus, a country east of the mediterranean sea, who were evaluated by the KIDMED index to test their adherence to a mediterranean diet after changing from rural residence to an urban residence. It was found that children in urban areas swapped their traditional dietary patterns for a diet favoring fast food.\n\nThe fact that every human has a unique genetic code is the key to techniques such as genetic fingerprinting. Versions of a trait, known as alleles, occur at different frequencies in different human populations; populations that are more geographically and ancestrally remote tend to differ more.\n\nA phenotype is the \"outward, physical manifestation\" of an organism.\" For humans, phenotypic differences are most readily seen via skin color, eye color, hair color, or height; however, any observable structure, function, or behavior can be considered part of a phenotype. A genotype is the \"internally coded, inheritable information\" carried by all living organisms. The human genome is encoded in DNA\n\nFor any trait of interest, observed differences among individuals \"may be due to differences in the genes\" coding for a trait or \"the result of variation in environmental condition\". This variability is due to gene-environment interactions that influence genetic expression patterns and trait heritability.\n\nFor humans, there is \"more genetic variation among individual people than between larger racial groups\". In general, an average of 80% of genetic variation exists within local populations, around 10% is between local populations within the same continent, and approximately 8% of variation occurs between large groups living on different continents. Studies have found evidence of genetic differences between populations, but the distribution of genetic variants within and among human populations is impossible to describe succinctly because of the difficulty of defining a \"population\", the clinal nature of variation, and heterogeneity across the genome. Thus, the racialization of science and medicine can lead to controversy when the term population and race are used interchangeably.\n\nGenes may be under strong selection in response to local diseases. For example, people who are duffy negative tend to have higher resistance to malaria. Most Africans are duffy negative and most non-Africans are duffy positive. A number of genetic diseases more prevalent in malaria-afflicted areas may provide some genetic resistance to malaria including sickle cell disease, thalassaemias, glucose-6-phosphate dehydrogenase, and possibly others.\n\nMany theories about the origin of the cystic fibrosis have suggested that it provides a heterozygote advantage by giving resistance to diseases earlier common in Europe.\n\nIn earlier research, a common theory was the \"common disease-common variant\" model. It argues that for common illnesses, the genetic contribution comes from the additive or multiplicative effects of gene variants that each one is common in the population. Each such gene variant is argued to cause only a small risk of disease and no single variant is enough to cause the disease. An individual must have many of these common gene variants in order for the risk of disease to be substantial.\n\nMore recent research indicates that the \"common disease-rare variant\" may be a better explanation for many common diseases. In this model, rare but higher-risk gene variants cause common diseases. This model may be relevant for diseases that reduces fertility. In contrast, for common genes associated with common disease to persist they must either have little effect during the reproductive period of life (like Alzheimer's disease) or provide some advantage in the original environment (like genes causing autoimmune diseases also providing resistance against infections). In either case varying frequencies of genes variants in different populations may be an explanation for health disparities. Genetic variants associated with Alzheimer's disease, deep venous thrombosis, Crohn disease, and type 2 diabetes appear to adhere to \"common disease-common variant\" model.\n\nGene flow and admixture can also have an effect on relationships between race and race-linked disorders. Multiple sclerosis, for example, is typically associated with people of European descent, but due to admixture African Americans have elevated levels of the disorder relative to Africans.\n\nSome diseases and physiological variables vary depending upon their admixture ratios. Examples include measures of insulin functioning and obesity.\n\nThe same gene variant, or group of gene variants, may produce different effects in different populations depending on differences in the gene variants, or groups of gene variants, they interact with. One example is the rate of progression to AIDS and death in HIV–infected patients. In Caucasians and Hispanics, HHC haplotypes were associated with disease retardation, particularly a delayed progression to death, while for African Americans, possession of HHC haplotypes was associated with disease acceleration. In contrast, while the disease-retarding effects of the CCR2-641 allele were found in African Americans, they were not found in Caucasians.\n\nThere is a controversy regarding race as a method for classifying humans. Different sources argue it is purely social construct or a biological reality reflecting average genetic group differences. New interest in human biological variation has resulted in a resurgence of the use of race in biomedicine.\n\nThe main impetus for this development is the possibility of improving the prevention and treatment of certain diseases by predicting hard-to-ascertain factors, such as genetically conditioned health factors, based on more easily ascertained characteristics such as phenotype and racial self-identification. Since medical judgment often involves decision making under uncertain conditions, many doctors consider it useful to take race into account when treating disease because diseases and treatment responses tend to cluster by geographic ancestry. The discovery that more diseases than previously thought correlate with racial identification have further sparked the interest in using race as a proxy for bio-geographical ancestry and genetic buildup.\n\nRace in medicine is used as an approximation for more specific genetic and environmental risk factors. Race is thus partly a surrogate for environmental factors such as differences in socioeconomic status that are known to affect health. It is also an imperfect surrogate for ancestral geographic regions and differences in gene frequencies between different ancestral populations and thus differences in genes that can affect health. This can give an approximation of probability for disease or for preferred treatment, although the approximation is less than perfect.\n\nTaking the example of sickle-cell disease, in an emergency room, knowing the geographic origin of a patient may help a doctor doing an initial diagnosis if a patient presents with symptoms compatible with this disease. This is unreliable evidence with the disease being present in many different groups as noted above with the trait also present in some Mediterranean European populations. Definitive diagnosis comes from examining the blood of the patient. In the US, screening for sickle cell anemia is done on all newborns regardless of race.\n\nThe continued use of racial categories has been criticized. Apart from the general controversy regarding race, some argue that the continued use of racial categories in health care and as risk factors could result in increased stereotyping and discrimination in society and health services. Some of those who are critical of race as a biological concept see race as socially meaningful group that is important to study epidemiologically in order to reduce disparities. For example, some racial groups are less likely than others to receive adequate treatment for osteoporosis, even after risk factors have been assessed. Since the 19th century, blacks have been thought to have thicker bones than whites have and to lose bone mass more slowly with age. In a recent study, African Americans were shown to be substantially less likely to receive prescription osteoporosis medications than Caucasians. Men were also significantly less likely to be treated compared with women. This discrepancy may be due to physicians' knowledge that, on average, African Americans are at lower risk for osteoporosis than Caucasians. It may be possible that these physicians generalize this data to high-risk African-Americans, leading them to fail to appropriately assess and manage these individuals' osteoporosis. On the other hand, some of those who are critical of race as a biological concept see race as socially meaningful group that is important to study epidemiologically in order to reduce disparities.\n\nDavid Williams (1994) argued, after an examination of articles in the journal \"Health Services Research\" during the 1966–90 period, that how race was determined and defined was seldom described. At a minimum, researchers should describe if race was assessed by self-report, proxy report, extraction from records, or direct observation. Race was also often used questionable, such as an indicator of socioeconomic status. Racial genetic explanations may be overemphasized, ignoring the interaction with and the role of the environment.\n\nThere is general agreement that a goal of health-related genetics should be to move past the weak surrogate relationships of racial health disparity and get to the root causes of health and disease. This includes research which strives to analyze human genetic variation in smaller groups than races across the world.\n\nOne such method is called ethnogenetic layering. It works by focusing on geographically identified microethnic groups. For example, in the Mississippi Delta region ethnogenetic layering might include such microethnic groups as the Cajun (as a subset of European Americans), the Creole and Black groups [with African origins in Senegambia, Central Africa and Bight of Benin] (as a subset of African Americans), and Choctaw, Houmas, Chickasaw, Coushatta, Caddo, Atakapa, Karankawa and Chitimacha peoples (as subsets of Native Americans).\n\nBetter still may be individual genetic assessment of relevant genes. As genotyping and sequencing have become more accessible and affordable, avenues for determining individual genetic makeup have opened dramatically. Even when such methods become commonly available, race will continue to be important when looking at groups instead of individuals such as in epidemiologic research.\n\nSome doctors and scientists such as geneticist Neil Risch argue that using self-identified race as a proxy for ancestry is necessary to be able to get a sufficiently broad sample of different ancestral populations, and in turn to be able to provide health care that is tailored to the needs of minority groups.\n\nOne area in which population categories can be important considerations in genetics research is in controlling for confounding between population genetic substructure, environmental exposures, and health outcomes. Association studies can produce spurious results if cases and controls have differing allele frequencies for genes that are not related to the disease being studied, although the magnitude of its problem in genetic association studies is subject to debate. Various techniques detect and account for population substructure, but these methods can be difficult to apply in practice.\n\nPopulation genetic substructure also can aid genetic association studies. For example, populations that represent recent mixtures of separated ancestral groups can exhibit longer-range linkage disequilibrium between susceptibility alleles and genetic markers than is the case for other populations. Genetic studies can use this disequilibrium to search for disease alleles with fewer markers than would be needed otherwise. Association studies also can take advantage of the contrasting experiences of racial or ethnic groups, including migrant groups, to search for interactions between particular alleles and environmental factors that might influence health.\n\nThe Human Genome Diversity Project has collected genetic samples from 52 indigenous populations.\n\nIn a report by the Institute of Medicine called Unequal Treatment, three major source categories are put forth as potential explanations for disparities in health care: patient-level variables, healthcare system-level factors, and care process-level variables.\n\nThere are many individual factors that could explain the established differences in health care between different racial and ethnic groups. First, attitudes and behaviors of minority patients are different. They are more likely to refuse recommended services, adhere poorly to treatment regimens, and delay seeking care, yet despite this, these behaviors and attitudes are unlikely to explain the differences in health care. In addition to behaviors and attitudes, biological based racial differences have been documented, but these also seem unlikely to explain the majority of observed disparities in care.\n\nHealth system-level factors include any aspects of health systems that can have different effects on patient outcomes. Some of these factors include different access to services, access to insurance or other means to pay for services, access to adequate language and interpretation services, and geographic availability of different services. Many studies assert that these factors explain portions of the existing disparities in health of racial and ethnic minorities in the United States when compared to their white counterparts.\n\nThree major mechanisms are suggested by the Institute of Medicine that may contribute to healthcare disparities from the provider's side: bias (or prejudice) against racial and ethnic minorities; greater clinical uncertainty when interacting with minority patients; and beliefs held by the provider about the behavior or health of minorities. Research in this area is new and ongoing.\n\n\nUnited States:\n\nGeneral:\n\n\n"}
{"id": "267488", "url": "https://en.wikipedia.org/wiki?curid=267488", "title": "Reference Daily Intake", "text": "Reference Daily Intake\n\nThe Reference Daily Intake (RDI) is the daily intake level of a nutrient that is considered to be sufficient to meet the requirements of 97–98% of healthy individuals in every demographic in the United States. While developed for the US population, it has been adopted by other countries, though not universally.\n\nThe RDI is used to determine the Daily Value (DV) of foods, which is printed on nutrition facts labels (as % DV) in the United States and Canada, and is regulated by the Food and Drug Administration (FDA) and Health Canada. The labels \"high\", \"rich in\", or \"excellent source of\" may be used for a food if it contains 20% or more of the RDI. The labels \"good source\", \"contains\", or \"provides\" may be used on a food if it contains between 10% and 20% of the RDI. \n\nThe Recommended Dietary Allowances (RDAs) were a set of nutrition recommendations that evolved into both the Dietary Reference Intake (DRI) system of nutrition recommendations (which still defines RDA values) and the RDIs used for food labelling. The first regulations governing U.S. nutrition labels specified a \"% U.S. RDA\" declaration based on the current RDA values, which had been published in 1968. Later, the % U.S. RDA was renamed the %DV and the RDA values that the %DVs were based on became the RDIs.\n\nThe RDAs (and later the RDA values within the DRI) were regularly revised to reflect the latest scientific information, but although the nutrition labeling regulations were occasionally updated, the existing RDI values were not changed, so that until 2016 many of the DVs used on nutrition facts labels were still based on the outdated RDAs from 1968. In 2016, the Food and Drug Administration published changes to the regulations including updated RDIs and DVs based primarily on the RDAs in the current DRI.\n\nDaily Values used by the FDA for the following macronutrients are Daily Reference Values.\n\nFDA issued a Final Rule on changes to facts panel on May 27, 2016. The new values were published in the Federal Register. New values can be used on labels now. The original deadline to be in compliance was July 28, 2018, but on September 29, 2017 the FDA released a proposed rule that extended the deadline to January 1, 2020 for large companies and January 1, 2021 for small companies. In the interim, products with old or new facts panel content will be on market shelves at same time.\n\nThe following table lists the old and new DVs based on a caloric intake of 2000 kcal (8400 kJ), for adults and children four or more years of age.\n\nFor vitamins and minerals, the old RDIs and new RDIs (old and new adult 100% Daily Values) are given in the following table, along with the more recent RDAs or AIs of the Dietary Reference Intakes (maximized over sex and age groups, excluding women who are pregnant or lactating):\n\nThe RDI is derived from the RDAs, which were first developed during World War II by Lydia J. Roberts, Hazel Stiebeling and Helen S. Mitchell, all part of a committee established by the U.S. National Academy of Sciences to investigate issues of nutrition that might \"affect national defense\" (Nestle, 35). The committee was renamed the Food and Nutrition Board in 1941, after which they began to deliberate on a set of recommendations of a standard daily allowance for each type of nutrient. The standards would be used for nutrition recommendations for the armed forces, for civilians, and for overseas population who might need food relief. Roberts, Stiebeling, and Mitchell surveyed all available data, created a tentative set of allowances for \"energy and eight nutrients\", and submitted them to experts for review (Nestle, 35). The final set of guidelines, called RDAs for Recommended Dietary Allowances, were accepted in 1941. The allowances were meant to provide superior nutrition for civilians and military personnel, so they included a \"margin of safety\". Because of food rationing during the war, the food guides created by government agencies to direct citizens' nutritional intake also took food availability into account.\n\nThe Food and Nutrition Board subsequently revised the RDAs every five to ten years. In 1973, the FDA introduced regulations to specify the format of nutrition labels when present, although the inclusion of such labels was largely voluntary, only being required if nutrition claims were made or if nutritional supplements were added to the food. The nutrition labels were to include percent U.S. RDA based on the 1968 RDAs in effect at the time. The RDAs continued to be updated (in 1974, 1980 and 1989) but the values specified for nutrition labelling remained unchanged.\n\nIn 1993 the FDA published new regulations mandating the inclusion of a nutrition facts label on most packaged foods. Originally the FDA had proposed replacing the percent U.S. RDAs with percent daily values based on the 1989 RDAs but the Dietary Supplement Act of 1992 prevented it from doing so. Instead it introduced the RDI to be the basis of the new daily values. The RDI consisted of the existing U.S. RDA values (still based on the 1968 RDAs as the FDA was not allowed to change them at the time) and new values for additional nutrients not included in the 1968 RDAs.\n\nIn 1997, at the suggestion of the Institute of Medicine of the National Academy, the RDAs became one part of a broader set of dietary guidelines called the Dietary Reference Intake used by both the United States and Canada. As part of the DRI, the RDAs continued to be updated.\n\nOn May 27, 2016, the FDA updated the regulations to change the RDI and Daily Values to reflect current scientific information. Until this time, the Daily Values were still largely based on the 1968 RDAs. The new regulations make several other changes to the nutrition facts label to facilitate consumer understanding of the calorie and nutrient contents of their foods, emphasizing nutrients of current concern, such as vitamin D and potassium. The revision to the regulations came into effect on 26 July 2016 and initially stipulated that larger manufacturers must comply within two years while smaller manufacturers had an additional year. On September 29, 2017 the FDA released a proposed rule that extended the deadline to January 1, 2020 for large companies and January 1, 2021 for small companies.\n\nIn 2010, the U.S. Institute of Medicine determined that the government should establish new consumption standards for salt to reduce the amount of sodium in the typical American diet below levels associated with higher risk of several cardiovascular diseases, yet maintain consumer preferences for salt-flavored food. The daily maximum for sodium in the United States had been above estimated minimums for decades. For instance, the National Research Council found that 500 milligrams of sodium per day (approximately 1,250 milligrams of table salt) is a safe minimum level. In the United Kingdom, the daily allowance for salt is 6 g (approximately 2.5 teaspoons, about the upper limit in the U.S.), an amount considered \"too high\".\n\nThe Institute of Medicine advisory stated (daily intake basis): \"Americans consume more than 3,400 milligrams of sodium – the amount in about 1.5 teaspoons of salt (8.7 g) – each day. The recommended maximum daily intake of sodium – the amount above which health problems appear – is 2,300 milligrams per day for adults, about 1 teaspoon of salt (5.9 g). The recommended adequate intake of sodium is 1,500 milligrams (3.9 g salt) per day, and people over 50 need even less.\"\n\n\n"}
{"id": "2550591", "url": "https://en.wikipedia.org/wiki?curid=2550591", "title": "Religion and drugs", "text": "Religion and drugs\n\nMany religions have expressed positions on what is acceptable to consume as a means of intoxication for spiritual, pleasure, or medicinal purposes. Psychoactive substances may also play a significant part in the development of religion and religious views as well as in rituals.\n\nIn the book \"Inside the Neolithic Mind\", the authors, archaeologists David Lewis-Williams and David Pearce argue that hallucinogenic drugs formed the basis of neolithic religion and rock art. Similar practices and images are found in some contemporary \"stone-age\" Indigenous peoples in South America using yaje.\n\nSome scholars have suggested that Ancient Greek mystery religions employed entheogen, such as the Kykeon central to the Eleusinian Mysteries, to induce a trance or dream state. Research conducted by John R. Hale, Jelle Zeilinga de Boer, Jeffrey P. Chanton and Henry A. Spiller suggests that the prophecies of the Delphic Oracle were uttered by Priestesses under the influence of gaseous vapors exuded from the ground. Their findings are published in \"Questioning the Delphic Oracle: Overview / An Intoxicating Tale\".\n\nIn Buddhism the Right View (' / ') can also be translated as \"right perspective\", \"right outlook\" or \"right understanding\", is the right way of looking at life, nature, and the world as they really are for us. It is to understand how our reality works. It acts as the reasoning with which someone starts practicing the path. It explains the reasons for our human existence, suffering, sickness, aging, death, the existence of greed, hatred, and delusion. Right view gives direction and efficacy to the other seven path factors. It begins with concepts and propositional knowledge, but through the practice of right concentration, it gradually becomes transmuted into wisdom, which can eradicate the fetters of the mind. An understanding of right view will inspire the person to lead a virtuous life in line with right view. In the Pāli and Chinese canons, it is explained thus:\n\nRight livelihood (\"samyag-ājīva\" / \"sammā-ājīva\"). This means that practitioners ought not to engage in trades or occupations which, either directly or indirectly, result in harm for other living beings. In the Chinese and Pali Canon, it is explained thus:\n\nMore concretely today interpretations include \"work and career need to be integrated into life as a Buddhist,\" it is also an ethical livelihood, \"wealth obtained through rightful means\" (Bhikku Basnagoda Rahula) – that means being honest and ethical in business dealings, not to cheat, lie or steal. As people are spending most of their time at work, it’s important to assess how our work affects our mind and heart. So important questions include \"How can work become meaningful? How can it be a support, not a hindrance, to spiritual practice — a place to deepen our awareness and kindness?\"\n\nThe five types of businesses that should not be undertaken:\n\n\nAccording to the fifth precept of the Pancasila, Buddhists are meant to refrain from any quantity of \"fermented or distilled beverages\" which would prevent mindfulness or cause heedlessness. In the Pali Tipitaka the precept is explicitly concerned with alcoholic beverages:\n\"I undertake the training rule to abstain from fermented drink that causes heedlessness.\"\n\n\"Surāmerayamajjapamādaṭṭhānā veramaṇī sikkhāpadaṃ samādiyāmi.\"\nHowever, caffeine and tea are permitted, even encouraged for monks of most traditions, as it is believed to promote wakefulness.\n\nGenerally speaking, the vast majority of Buddhists and Buddhist sects denounce and have historically frowned upon the use of any intoxicants by an individual who has taken the five precepts. Most Buddhists view the use and abuse of intoxicants to be a hindrance in the development of an enlightened mind. However, there are a few historical and doctrinal exceptions.\n\nMany modern Buddhist schools have strongly discouraged the use of psychoactive drugs of any kind; however, they may not be prohibited in all circumstances in all traditions. Some denominations of tantric or esoteric Buddhism especially exemplify the latter, often with the principle skillful means:\n\nFor example, as part of the ganachakra \"tsok\" ritual (as well as Homa, abhisheka and sometimes drubchen) some Tibetan Buddhists and Bönpos have been known to ingest small amounts of grain alcohol (called \"amrit\" or \"amrita\") as an offering. If a member is an alcoholic, or for some other reason does not wish to partake in the drinking of the alcoholic offering, then he or she may dip a finger in the alcohol and then flick it three times as part of the ceremony.\n\nAmrita is also possibly the same as, or at least in some sense a conceptual derivative of the ancient Hindu \"soma\". (The latter which historians often equate with \"Amanita muscaria\" or other \"Amanita\" psychoactive fungi.) Crowley (1996) states:\n\"Undoubtedly, the striking parallels between \"The legend about Chakdor\" and the Hindu legend of the origin of soma show that the Buddhist amrita and the Hindu soma were at one time understood to be identical. Moreover, the principal property of amrita is, to this day, perceived by Buddhists as being a species of inebriation, however symbolically this inebriation may be interpreted. Why else would beer (Tibetan \"chhang\", \"barley beer\") be used by yogins as a symbolic substitute for amrita [Ardussi]? Conversely, why else would the term bDud.rTsi be used as a poetic synonym for beer?\nConversely, in Tibetan and Sherpa lore there is a story about a monk who came across a woman who told him that he must either:\n\nThe monk thought to himself, \"well, surely if I kill the goat then I will be causing great suffering since a living being will die. If I sleep with the woman then I will have broken another great vow of a monk and will surely be lost to the ways of the world. Lastly, if I drink the beer then perhaps no great harm will come and I will only be intoxicated for a while, and most importantly I will only be hurting myself.\" (In the context of the story this instance is of particular importance to him because monks in the Mahayana and Vajrayana try to bring all sentient beings to enlightenment as part of their goal.)\n\nSo the monk drank the mug of beer and then he became very drunk. In his drunkenness he proceeded to kill the woman and sleep with the goat, breaking all three vows and, at least in his eyes, doing much harm in the world. The lesson of this story is meant to be that, at least according to the cultures from which it delineates, alcohol causes one to break all of one's vows, in a sense that one could say it is the cause of all other harmful deeds.\n\nThe Vajrayana teacher Drupon Thinley Ningpo Rinpoche has said that as part of the five precepts which a layperson takes upon taking refuge, that although they must refrain from taking intoxicants, they may drink enough so as they do not become drunk. Bhikkus and Bhikkunis (monks and nuns, respectively), on the other hand, who have taken the ten vows as part of taking refuge and becoming ordained, cannot imbibe any amount of alcohol or other drugs, other than pharmaceuticals taken as medicine.\n\nTenzin Gyatso, the 14th Dalai Lama of Tibet, is known as teetotaler and non-smoker.\n\nThere is some evidence regarding the use of deliriant \"Datura\" seeds (known as \"candabija\") in Dharmic rituals associated with many tantras – namely the \"Vajramahabhairava\", \"Samputa\", \"Mahakala\", \"Guhyasamaja\", \"Tara\" and \"Krsnayamari\" tantras – as well as cannabis and other entheogens in minority Vajrayana sanghas. Ronald M Davidson says that in Indian Vajrayana, Datura was:\n\n“employed as a narcotic paste or as wood in a fire ceremony and could be easily absorbed through the skin or the lungs. The seeds of this powerful narcotic, termed \"passion seeds\" (candabija), are the strongest elements and contain the alkaloids hyoscine, hyoscyamine, and atropine in forms that survive burning or boiling. In even moderate doses, datura can render a person virtually immobile with severe belladonna-like hallucinations.”\n\nIn the \"Profound Summarizing Notes on the Path Presented as the Three Continua\" a Sakya Lamdre text, by Jamyang Khyentse Wangchuk (1524-1568), the use of Datura in combination with other substances, is prescribed as part of a meditation practice meant to establish that \"All the phenomena included in apparent existence, samsara and nirvana, are not established outside of one's mind.\"\n\nIan Baker writes that Tibetan terma literature such as the Vima Nyingtik describes \"various concoctions of mind altering substances, including datura and oleander, which can be formed into pills or placed directly in the eyes to induce visions and illuminate hidden contents of the psyche.\"\n\nA book titled \"Zig Zag Zen: Buddhism and Psychedelics\" (2002), details the history of Buddhism and the use of psychedelic drugs, and includes essays by modern Buddhist teachers on the topic.\n\nZen Buddhism is known for stressing the precepts. In Japan, however, where Zen flourished historically, there are a number of examples of misconduct on the part of monks and laypeople alike. This often involved the use of alcohol, as sake drinking has and continues to be a well known aspect of Japanese culture.\n\nThe Japanese Zen monk and abbot, shakuhachi player and poet Ikkyu was known for his unconventional take on Zen Buddhism: His style of expressing dharma is sometimes deemed \"Red Thread Zen\" or \"Crazy Cloud Zen\" for its unorthodox characteristics. Ikkyu is considered both a heretic and saint in the Rinzai Zen tradition, and was known for his derogatory poetry, open alcoholism and for frequenting the services of prostitutes in brothels. He personally found no conflict between his lifestyle and Buddhism.\n\nThere are several koans (Zen riddles) referencing the drinking of sake (rice wine); for instance Mumonkan's tenth koan titled \"Seizei Is Utterly Destitute\":\n'Seizei said to Sozan, \"Seizei is utterly destitute. Will you give him support?\" Sozan called out: \"Seizei!\" Seizei responded, \"Yes sir?!\" Sozan said, \"You have finished three cups of the finest wine in China, and still you say you have not yet moistened your lips!\"'\nAnother monk, Gudo, is mentioned in a koan called \"Finding a Diamond on a Muddy Road\" buying a gallon of sake.\n\nJudaism maintains that people do not own their bodies – they belong to God. As a result, Jews are not permitted to harm, mutilate, destroy or take risks with their bodies, life or health with activities such as taking life-threatening drugs. However, there is no general prohibition against drugs in Judaism, as long as they don't interfere with one's ritual duties and don't cause definite harm, though most Rabbis generally prohibit drugs, in order to avoid social, legal and medical problems in their community.\n\nSpiritual use of various alcoholic beverages, sometimes in very large quantities, is common and well known. In some Jewish communities there is a tradition to get drunk on Purim until they forget the difference between the Hebrew phrases \"Cursed is Haman\" and \"Blessed is Mordechai\", which signified reaching the spiritual world Atzilut where all opposites unite. In many Jewish communities it is customary to drink on Simchat Torah as well. Drinking in small quantities as a mind-altering practice is commonly used during the Farbrengens of the Chabad Hasidim. A large body of Chabad literature refers to the spiritual dangers of drinking, but a few anecdotal references refer to the spirutal power of alcohol, when used for the sake of connecting to God and achieving brotherly love among fellows Jews. The Lubavitcher Rebbe forbade his Chassidim under the age of 40 to consume more than 4 small shots of hard liqueurs. Wine plays a prominent role in many Jewish rituals, most notably the kiddush.\nHasidic Jews often engage in a free ceremony called \"Tisch\" in which drinks such as Vodka are drunk in a group. Drinking is accompanied by singing and the study of the Torah.\n\nSome Hasidic Rabbis, e.g. the Ribnitzer Rebbe used to drink large amounts of Vodka on some special occasions, apparently as a powerful mind-altering method. The Ribnitzer Rebbe also practiced severe sleep deprivation, extremely long meditative prayers and a number of ascetic purification rituals. During his life in the USSR he used to immerse himself every day in ice water.\n\nThe spiritual use of caffeine and nicotine as stimulants is well known in the Hasidic communities. Many stories are told about miracles and spiritual journeys performed by the Baal Shem Tov and other famous Tzaddikim with the help of their smoking pipe. Some people suggest that, judging by the nature of these stories, the tobacco was sometimes mixed with strong mind-altering drugs.\n\nA popular Hasidic saying relates coffee to the Psalmic verse \"Hope in God\". The Hebrew word for \"hope\" (\"Kave\") sounds identical to the Yiddish word for coffee. Coffee is believed to have power to awaken the soul to the worship of God.\n\nSome Kabbalists, including Isaac of Acco and Abraham Abulafia, mention a method of \"philosophical meditation\", which involves drinking a cup of \"strong wine of Avicenna\", which would induce a trance and would help the adept to ponder over difficult philosophical questions. The exact recipe of this wine remains unknown; Avicenna refers in his works to the effects of opium and datura extracts.\n\nRabbi Aryeh Kaplan, a prominent researcher of Jewish meditations, suggested that some medieval Kabbalists may have used some psychedelic drugs Indeed, one can find in Kabbalistic medical manuals cryptic references to the hidden powers of mandrake, harmal and other psychoactive plants, though the exact usage of these powers is hard to decipher.\n\nAccording to Aryeh Kaplan, cannabis was an ingredient in the Holy anointing oil mentioned in various sacred Hebrew texts. The herb of interest is most commonly known as \"kaneh-bosem\" (קְנֵה-בֹשֶׂם) which is mentioned several times in the Old Testament as a bartering material, incense, and an ingredient in Holy anointing oil used by the high priest of the temple. Many Rastafarians, who use cannabis as a sacrament, identify as Jewish.\n\nAccording to Josephus, the head-dress of the Jewish High Priests' was modeled upon the capsule of the Hyoscyamus flower, which he calls \"Saccharus\". This Greek word for sugar stems from the Hebrew root that means \"intoxicating\".\n\nBenny Shanon, a psychology professor at the Hebrew University of Jerusalem, proposed that Moses may have been high on hallucinogenic mushrooms at the time he received the Ten Commandments.\n\nMany Christian denominations disapprove of the use of most illicit drugs. Many denominations permit the moderate use of socially and legally acceptable drugs like alcohol, caffeine and tobacco. Some Christian denominations permit smoking tobacco, while others disapprove of it. Many denominations do not have any official stance on drug use, some more-recent Christian denominations (e.g. Mormons, Seventh-day Adventists and Jehovah’s Witnesses) discourage or prohibit the use of any of these substances.\n\nBecause Jesus and many Biblical figures drank wine, most Christian denominations do not require teetotalism. In the Eucharist, wine represents (or among Christians who believe in some form of Real Presence, like the Catholic, Lutheran and Orthodox churches, actually \"is\") the blood of Christ. Lutherans believe in the real presence of the body and blood of Christ in the Eucharist, that the body and blood of Christ are \"truly and substantially present in, with and under the forms.\" of the consecrated bread and wine (the elements), so that communicants orally eat and drink the holy body and blood of Christ Himself as well as the bread and wine (cf. Augsburg Confession, Article 10) in this Sacrament. The Lutheran doctrine of the Real Presence is more accurately and formally known as \"the Sacramental Union.\" It has been inaccurately called \"consubstantiation\", a term which is specifically rejected by most Lutheran churches and theologians.\n\nOn the other hand, some Protestant Christian denominations, such as Baptists and Methodists associated with the temperance movement, encourage or require teetotalism. In some Protestant denomination, grape juice or non-alcoholic wine is used in place of wine to represent the blood of Christ.\nThe best known Western prohibition against alcohol happened in the United States in the 1920s, where concerned prohibitionists were worried about its dangerous side effects. However, the demand for alcohol remained and criminals stepped in and created the supply. The consequences of organized crime and the popular demand for alcohol, led to alcohol being legalized again.\n\nIslam prohibits all drugs that are not medically prescribed. Islam's prohibition of drugs stems from two concerns:\n\nThere are numerous verses in the Qur'an and hadith that ban \"khamr\" (intoxicants, including alcohol). Muhammad said:\nEvery intoxicant is like alcohol, and every (type of) alcohol is prohibited. (Muslim)\n\nThe second reason for banning drugs is that they are believed to have a harmful effect on the body. The Qur'an says,\n\n\"And make not your own hands contribute to your destruction.\" Surah, Al-Baqara, 2: 195\n\nThe Muslim nations of Turkey and Egypt were instrumental in banning opium, cocaine, and cannabis when the League of Nations committed to the 1925 International Convention relating to opium and other drugs (later the 1934 Dangerous Drugs Act). The primary goal was to ban opium and cocaine, but cannabis was added to the list, and it remained there largely unnoticed due to the much more heated debate over opium and coca. The 1925 Act has been the foundation upon which every subsequent policy in the United Nations has been founded. Cannabis use and abuse as an intoxicant was largely unknown in the West at that point, but Islamic leaders have been critical of it since the 13th century.\n\nO You who believe! Intoxicants and gambling, (dedication of) stones and (divination by) arrows are an abomination of Satan’s handiwork. Avoid (such abominations) that you may prosper. (5:90)\n\nSatan’s plan is to sow hatred and enmity amongst you with intoxicants and gambling, and to hamper you from the remembrance of Allah and from prayer. Will you not give up? (5:91)\n\nThere are no prohibitions in Islam on alcohol for scientific, industrial or automotive use.\n\nIn spite of these restrictions on substance use, tobacco and caffeine are still widely used throughout many Muslim nations.\n\nBahá'ís are forbidden to drink alcohol or to take drugs, unless prescribed by doctors. Accordingly, the sale and trafficking of such substances is also forbidden. Smoking is discouraged but not prohibited.\n\nMany Rastafari believe cannabis, which they call \"ganja,\" \"the herb,\" or \"Kaya,\" is a sacred gift of Jah. It may be used for spiritual purposes to commune with God, but should not be used profanely. The use of other drugs, however, including alcohol, is frowned upon. Many believe that the wine Jesus/Iyesus drank was not an alcoholic beverage, but simply the juice of grapes or other fruits.\n\nWhile some Rastafari suggest that the Bible may refer to marijuana, it is generally held by academics specializing in the archaeology and paleobotany of Ancient Israel, and those specializing in the lexicography of the Hebrew Bible, that cannabis is not documented or mentioned in early Judaism. Against this some popular writers have argued that there is evidence for religious use of cannabis in the Hebrew Bible, although this hypothesis and some of the specific case studies (e.g., John Allegro in relation to Qumran, 1970) have been \"widely dismissed as erroneous\" (Merlin, 2003). The primary advocate of a religious use of cannabis plant in early Judaism was Sula Benet (1967), who claimed that the plant \"kaneh bosm קְנֵה-בֹשֶׂם\" mentioned five times in the Hebrew Bible, and used in the holy anointing oil of the Book of Exodus, was in fact cannabis, although lexicons of Hebrew and dictionaries of plants of the Bible such as by Michael Zohary (1985), Hans Arne Jensen (2004) and James A. Duke (2010) and others identify the plant in question as either \"Acorus calamus\" or \"Cymbopogon citratus\".\n\nA \"reasoning\" is a simple event where the Rastas gather, smoke cannabis (\"ganja\"), and discuss. The person honored by being allowed to light the herb says a short sentence beforehand. The ganja is passed in a clockwise fashion (passing 'pon the lef' han' side) except in times of war, when it is passed counterclockwise. It is used to reason with Jah.\n\nA \"groundation\" (also spelled \"grounation\") or \"binghi\" is a holy day; the name \"binghi\" is derived from \"Nyabinghi\" (literally \"Nya\" meaning \"black\" and \"Binghi\" meaning \"victory\"). Binghis are marked by much dancing, singing, feasting, and the smoking of \"ganja\", and can last for several days.\n\n...thou shalt eat the herb of the field.\n\nGenesis 3.18\n\n...eat every herb of the land.\n\nExodus 10:12\n\nBetter is a dinner of herb where love is, than a stalled ox and hatred therewith.\n\nProverbs 15:17\n\nAccording to many Rastas, the illegality of cannabis in many nations is evidence of persecution of Rastafari. They are not surprised that it is illegal, viewing Cannabis as a powerful substance that opens people's minds to the truth – something the Babylon system, they reason, clearly does not want. Cannabis use is contrasted with the use of alcohol and other drugs, which they feel destroy the mind.\n\nAlcoholic drinks are commonly used during Asatru blots but non-alcoholic drinks can be substituted.\n\n"}
{"id": "1874234", "url": "https://en.wikipedia.org/wiki?curid=1874234", "title": "Species dysphoria", "text": "Species dysphoria\n\nSpecies dysphoria is the experience of dysphoria associated with the feeling that one's body is of the wrong species. Outside of psychological literature, the term is common within the otherkin and therian communities to describe their experiences.\n\nEarls and Lalumière (2009) describe it as \"the sense of being in the wrong [species'] body...a desire to be an animal\". A term that has also been used is \"transspecies\", described by Phaedra and Isaac Bonewits as \"people who believe themselves to be part animal, or animal souls that have been incarnated in human bodies\".\n\nSpecies dysphoria may include:\n\n\nMany find comfort in a form of transition, whether physical or social. There is little research in surgeries towards the end of looking like another species, amongst what does exist, a 2008 paper by Samuel Poore in \"The Journal of Hand Surgery - American Volume\" details how a wing similar to that of a flightless bird could be constructed from a human arm.\n\nIn a critical discussion of the work of Gerbasi \"et al.\" (2008), Fiona Probyn-Rapsey (2011) proposes that if \"Species Identity Disorder\" were to be treated, it may follow paths towards encouraging desistance, mirroring aims for desistance in the treatment of gender dysphoria in children; perhaps \"redirecting a child’s attention away from cross-dressing as an animal\" or \"limiting the influence of humanimal creatures like stuffed toys\". She proposes alternatively that treatments \"might involve counseling to learn to tolerate “atypical” humanimal development for those bothered by furries [with Species Identity Disorder]\".\n\nThough not all people with species dysphoria have gender dysphoria, and vice versa, overlap exists. Some people experience both gender dysphoria and species dysphoria and consider them to be related in that they believe them to be similar dysphoric experiences.\n\n\"Species dysphoria\" is informally used mainly in psychological literature to compare the experiences of some individuals to those in the transgender community.\n\nIn a 2008 study by Gerbasi \"et al.\", amongst other things pursuing the potential of a condition termed \"Species Identity Disorder\", 46% of people surveyed who identified as being in the furry fandom, answered \"yes\" to the question \"Do you consider yourself to be less than 100% human?\" and 41% answered \"yes\" to the question \"If you could become 0% human, would you?\"\n\nQuestions that Gerbasi states as being deliberately designed to draw parallels with gender dysphoria, specifying \"a persistent feeling of discomfort\" about the human body and the feeling that the person was the \"non-human species trapped in a human body\", were answered \"yes\" by 24% and 29% of respondents, respectively.\n\nThough not all people with species dysphoria are zoophilic, it has been proposed that there is comorbidity in some zoophiles by Beetz (2004), citing Miletski (2002) as evidence of this.\n\nMiletski (2002), in a study contained in their book, involving 67 male and nine female zoophiles, found that 16 (24%) of the men reported that it was ‘‘completely or mostly true’’ that they began having sex with animals because they ‘‘identiﬁed with the animal of [their] gender’’, 0 women reported this, with 1 woman reporting it as \"sometimes true\" against 14 of the men reporting as \"sometimes true\".\n\nA 2003 study by Williams and Weinberg surveyed 114 self identified zoophilic men and wrote that some admitted to \"believing they had animal characteristics or that they felt like they were an animal.\"\n\nIn 2007, Los Angeles artist Micha Cárdenas created \"Becoming Dragon\", a \"mixed-reality performance\" in which a virtual reality experience was created to allow a person to completely experience life through the eyes of a dragon avatar in the virtual world, Second Life. After the performance, Cárdenas reported that \"some of these people call themselves Otherkin, and feel deeply, truly, painfully that they were born as the wrong species, that they are foxes, dragons and horses. I would refer to them as transspecies.\"\n\nJ M Barrie's \"Peter Pan\" has been described as experiencing species dysphoria by Garber (1997).\n\nJean Dutourd's short novel \"Une Tête de Chien\" features a spaniel-headed human protagonist described by Giffney and Hird as suffering from species dysphoria.\n\n"}
{"id": "52549787", "url": "https://en.wikipedia.org/wiki?curid=52549787", "title": "Tampon tax", "text": "Tampon tax\n\nA tampon tax is a popular term used to call attention to the fact that tampons—and other feminine hygiene products—are subject to value-added tax, unlike the tax exemption status granted to other products considered basic necessities. Proponents of tax exemption argue that tampons, sanitary napkins, menstrual cups and comparable products constitute basic, unavoidable necessities for women and thus should be made tax exempt.\n\nProponents argue that feminine hygiene products serving the basic menstrual cycle should be classified alongside other unavoidable, tax exempt necessities, such as groceries and personal medical items. The BBC estimates that women—half of the global population—need to use feminine hygiene products for about a week each month for about 30 years. While sales tax policy varies across jurisdictions, these products were typically taxed at the same rate as non-essential goods, such as in the United States, while other countries, such as the United Kingdom and Ireland, reduced or eliminated their general consumption tax on sanitary products. When asked about equivalent exemptions for men, proponents argue that no male products, condoms included, are comparable to feminine hygiene products, since menstruation is biological and \"feminine hygiene is not a choice\". As the vast majority of consumers of feminine hygiene products are women, the increased cost has been criticized as being discriminatory against women. The tampon tax is not a special tax levied directly on feminine hygiene products.\n\nAfter the tax in Canada was removed mid-2015, women began protesting in other countries later that year. On July 21, 2018, India eliminated the Goods and Services Tax (GST) of 12% from sanitary napkins.\n\n\nThe United Kingdom has levied a value-added tax on sanitary products since it joined the European Economic Community in 1973. This rate was reduced to 5% specifically for sanitary products in 2000 with lobbying from Member of Parliament Dawn Primarolo saying that this reduction was \"about fairness, and doing what we can to lower the cost of a necessity.\" This is the lowest rate possible under the European Union's value added tax law, which as of 2015 does not allow zero rates. The UK Independence Party raised the issue in the 2015 general election with promises to withdraw from the European Union and allow the zero rate. Prime Minister David Cameron commented, when prompted, that the tampon tax campaign was \"long-standing\" and a complicated issue within the European Union. In England, one in ten women between 14 and 21 cannot afford menstrual management products.\n\nLaura Coryton led a \"Stop taxing periods, period\" campaign with an online petition to have the European Union remove the value-added tax for sanitary products. George Osborne mentioned the petition by name in his 2015 Autumn Statement pledge to end the tampon tax at the European Union level. The petition platform's CEO cited the campaign as an example of successful clicktivism, with over 320,000 signatures. In March 2016, Parliament created legislation to eliminate the tampon VAT. It was expected to go into effect by April 2018 but did not do so; several British women protested for it publicly while displaying blood stains from their periods. On the 3rd October 2018, new EU VAT rules that will allow the UK to stop taxing sanitary products were approved by the European Parliament.\n\nIn July 2017, a pilot program began in Scotland to have free sanitary products available at schools and food banks for women who cannot afford them. The pilot scheme was launched for six months in Aberdeen, Scotland, with £42,500 of funding from the devolved Scottish Government in order to address the growing scandal of \"period poverty\". It was believed 1,000 girls would benefit from the scheme, as there were reports of teenage girls using tissues, toilet roll, torn T-shirts and even newspaper as makeshift sanitary products, with some girls even skipping school altogether. It was decided to launch the scheme to improve attainment and school attendance, as well as improve confidence amongst teenage girls during their period; and Scotland is believed to be the first country in the world to give out free sanitary products as part of a government-sponsored initiative.\n\nFurther to this half year pilot program, Scotland's opposition Labour Party intends to introduce a bill to make this permanent. Scotland is the first country to ban period poverty.\n\nA study by the WHO and UNICEF showed that one out of five women in Scotland have been forced to improvise with items including toilet paper and old clothes due to the high cost of commercial products.\n\nIn the United States, almost all states tax \"tangible individual property\" but exempt non-luxury \"necessities\": groceries, prescriptions, prosthetics, agriculture supplies, and sometimes clothes—the exemptions vary between states. Five states do not have a state sales tax (Alaska, Delaware, Montana, New Hampshire, and Oregon), and , ten states specifically exempted essential hygiene products (Connecticut, Florida, Illinois, Maryland, Massachusetts, Minnesota, New Jersey, New York, Nevada, and Pennsylvania).\n\nIn the U.S., most states charge sales tax for women's pads and tampons. Ten states have dropped the tampon tax — Minnesota, Illinois, Nevada, Pennsylvania, New York, Massachusetts, Maryland, New Jersey, Connecticut and Florida, according to NPR and CNN. Seven other states have introduced such legislation, most recently Nebraska, Virginia and Arizona. In January 2018, California rejected a proposal to eliminate tampon tax.\n\nMany federal assistance programs such as SNAP (Supplemental Nutrition Assistance Program) and WIC (Women, Infants and Children) don’t allow the use of an EBT for products such as pads or tampons despite the products' classification as medical devices.  The IRS does not classify female products as medical devices, thus blocking women from buying them with pre-tax dollars in both flexible spending accounts and health savings accounts. \n\nThere has been some changes to the tampon taxes but most of these changes are state level or by city. On a smaller scale, individual cities have also changed their laws in favor of eliminating the tampon tax. \n\nCalifornia Assemblywoman Cristina Garcia reported that California women each pay roughly US$7 per month over 40 years, constituting US$20 million in annual taxes. Garcia and Ling Ling Chang proposed a bill to remove the tampon tax in early 2016. At this period, only a handful of the country's states exempted tampons, and several others had no state sales tax. Garcia held that women were taxed \"for being women\" and bore an economic burden for having no other choice but to buy these products. Garcia and Chang added that the tax was \"regulatory discrimination\" that disproportionately affected poor women and women of color, and that it likely persisted due to social taboos against discussing menstruation. Both houses of the California State Legislature voted to exempt tampons from taxation in June 2016, but the bill was vetoed by the state's governor, Jerry Brown, three months later.\n\nCalifornia Gov. Jerry Brown vetoed AB-1561 due to the potential loss of money in taxing feminine hygiene products. In response, Cristina Garcia co-authored AB-0479: Common Cents Tax Reform Act with Lorena Gonzalez Fletcher, which is a new measure outlining a solution to offset the feminine product and diaper tax exemption by increasing the tax on hard liquor.\n\nIn 2017, California State Legislature passed AB 10 requiring public middle schools and high schools where at least 40% of students meet the federal poverty level to stock half of the restrooms with free tampons and sanitary napkins. The law was passed in an effort to eliminate the cost burden and keep low-income students in schools during their menstrual cycle.\n\nCompanies involved in supplying the necessary feminine hygiene products (tampons and pads) for complete menstrual care in the restrooms of schools include WAXIE and Hospeco. They also supply various options for menstrual product dispensers that have a time delay mechanism to prevent products from being overused and/or abused.\n\nIn July 2016, New York State exempted feminine hygiene products from taxation, reducing the state's tax revenue by an estimated US$10 million annually. Connecticut and Illinois also removed their tax in 2016, with Florida following suit in 2017.\n\nA 2018 empirical study on New Jersey's 2005 tax break on menstrual products found that \"repealing tampon taxes removes an unequal tax burden and could make menstrual hygiene products more accessible for low-income consumers.\" The study utilized data from more than 16,000 purchases in 2004-2006 made in New Jersey, Delaware, Connecticut, Maryland, and Pennsylvania, using these latter nearby states as the control group. Through a differences-in-differences approach, they found that after the repeal, consumer prices on menstrual products decreased by 7.3%, relative to the control states. This was greater than the 6.9% sales tax, suggesting that the consumers benefitted from the tax break. Upon further analysis, the study also found that the decrease in consumer prices was greater for low-income consumers than high-income consumers (3.9% decrease versus 12.4% decrease). This suggests that low-income consumers received the most benefit from the tax break, while high-income consumers shared the benefit with producers of menstrual products.\n\nSupporters of the exemption of said taxes are calling their efforts \"menstrual equity\", explaining it as a social movement that strives for feminine products like tampons to be considered necessities. Things that are considered necessities, for example toilet paper, are not taxed. Activists are often being led by members of government. Councilwomen Julissa Ferreras-Copeland led a movement with a tampon tax pilot project ultimately providing free pads and tampons at a local high school in Queens, New York. Ferreras-Copeland's effort has now been expanded into 25 different schools around New York City. Other democrats including Ydanis Rodriguez and council speaker Melissa Mark-Viverito are advocating for state legislature to stop taxing sanitary products. \n\nFree the Tampon, an advocate for free menstrual products estimates that it would cost less than $5 a year per user to provide tampons and pads in restrooms at schools and businesses.\n\n"}
{"id": "50293718", "url": "https://en.wikipedia.org/wiki?curid=50293718", "title": "Timeline of deworming", "text": "Timeline of deworming\n\nThis is a timeline of deworming, and specifically mass deworming.\n\n"}
{"id": "6606537", "url": "https://en.wikipedia.org/wiki?curid=6606537", "title": "Tony Mills (physician)", "text": "Tony Mills (physician)\n\nTony Mills, is an American physician who specializes in treatment of HIV and AIDS. \nHe is one of the leading clinician in the fields of Men's Health and HIV disease. Mills has served as the primary care provider for over 2,000 patients, including approximately half living with HIV. Mills received both his undergraduate and medical degrees from Duke University. He completed an internship in Internal Medicine, a residency in Anesthesiology and a fellowship in Cardiovascular Anesthesiology, all at the University of California, San Francisco. Mills is a member of many professional societies including; the Infectious Disease Society of America, International AIDS Society, IAS-USA, and the American Academy of HIV Medicine. He is the executive director of SoCal Men's Medical Group, the clinical research director of Mills Clinical Research, and the president of the Men's Health Foundation.\n\nIn May 1998, he won the title of International Mister Leather, publicly coming out as HIV-positive one day later.\n\n\nMills graduated from Duke University School of Medicine and was awarded both the Stanley Sarnoff Fellowship Award in Cardiovascular Research and the Eugene Stead Research Award. He began his clinical practice in 1991 at Columbia Presbyterian Medical Center in New York City, concentrating on heart transplantation and cardiovascular research.\n\nIn 1994, Mills was named Chief of Pediatric Cardiac Anesthesiology at the University of Miami, where he was actively involved in both the recovery community and in the gay community as an advocate for people living with HIV.\n\nIn 1999, he moved to Los Angeles and opened a general medical practice specializing in HIV care. He was certified as an HIV specialist by the American Academy of HIV Medicine in 2000 and currently serves on both the California Board and the National Board of the AAHIVM. In 2002, Mills joined the Clinical Medicine Faculty at UCLA where he works actively with residents and fellows and is a frequent lecturer.\n\nHe is the current editor of \"HIV Treatment News\" and is a frequent contributor to other HIV-related publications.\n\nOn May 5, 1998, having earned the regional title of Mister Mid-Atlantic Leather, Mills entered and won the International Mister Leather contest, competing against 61 contestants from 7 countries.\n\nSince winning the contest, Mills has been featured in the documentaries \"Beyond Vanilla\" and \"Mr. Leather\". He has also been a model for the COLT Studio Group.\n"}
