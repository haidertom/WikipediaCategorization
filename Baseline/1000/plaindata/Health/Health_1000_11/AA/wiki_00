{"id": "166146", "url": "https://en.wikipedia.org/wiki?curid=166146", "title": "Advance healthcare directive", "text": "Advance healthcare directive\n\nAn advance healthcare directive, also known as living will, personal directive, advance directive, medical directive or advance decision, is a legal document in which a person specifies what actions should be taken for their health if they are no longer able to make decisions for themselves because of illness or incapacity. In the U.S. it has a legal status in itself, whereas in some countries it is legally persuasive without being a legal document.\n\nA living will is one form of advance directive, leaving instructions for treatment. Another form is a specific type of power of attorney or health care proxy, in which the person authorizes someone (an agent) to make decisions on their behalf when they are incapacitated. People are often encouraged to complete both documents to provide comprehensive guidance regarding their care, although they may be combined into a single form. An example of combination documents includes the Five Wishes in the United States. The term \"living will\" is also the commonly recognised vernacular in many countries, especially the U.K.\n\nAdvance directives were created in response to the increasing sophistication and prevalence of medical technology. Numerous studies have documented critical deficits in the medical care of the dying; it has been found to be unnecessarily prolonged, painful, expensive, and emotionally burdensome to both patients and their families.\n\nThe living will is the oldest form of advance directive. It was first proposed by an Illinois attorney, Luis Kutner, in a law journal in 1969. Kutner drew from existing estate law, by which an individual can control property affairs after death (i.e., when no longer available to speak for himself or herself) and devised a way for an individual to express his or her health care desires when no longer able to express current healthcare wishes. Because this form of \"will\" was to be used while an individual was still alive (but no longer able to make decisions) it was dubbed the \"living will\". In the U.S., The Patient Self-Determination Act (PSDA) went into effect in December 1991, and required healthcare providers (primarily hospitals, nursing homes and home health agencies) to give patients information about their rights to make advance directives under state law.\n\nA living will usually provides specific directives about the course of treatment healthcare providers and caregivers are to follow. In some cases a living will may forbid the use of various kinds of burdensome medical treatment. It may also be used to express wishes about the use or foregoing of food and water, if supplied via tubes or other medical devices. The living will is used only if the individual has become unable to give informed consent or refusal due to incapacity. A living will can be very specific or very general. An example of a statement sometimes found in a living will is: \"If I suffer an incurable, irreversible illness, disease, or condition and my attending physician determines that my condition is terminal, I direct that life-sustaining measures that would serve only to prolong my dying be withheld or discontinued.\"\n\nMore specific living wills may include information regarding an individual's desire for such services such as analgesia (pain relief), antibiotics, hydration, feeding, and the use of ventilators or cardiopulmonary resuscitation. However, studies have also shown that adults are more likely to complete these documents if they are written in everyday language and less focused on technical treatments.\n\nHowever, by the late 1980s, public advocacy groups became aware that many people remained unaware of advance directives and even fewer actually completed them. In part, this was seen as a failure of health care providers and medical organizations to promote and support the use of these documents. The public’s response was to press for further legislative support. The most recent result was the Patient Self-Determination Act of 1990, which attempted to address this awareness problem by requiring health care institutions to better promote and support the use of advance directives.\n\nLiving wills proved to be very popular, and by 2007, 41% of Americans had completed a living will. In response to public needs, state legislatures soon passed laws in support of living wills in virtually every state in the union.\n\nHowever, as living wills began to be better recognized, key deficits were soon discovered. Most living wills tended to be limited in scope and often failed to fully address presenting problems and needs. Further, many individuals wrote out their wishes in ways that might conflict with quality medical practice. Ultimately, it was determined that a living will alone might be insufficient to address many important health care decisions. This led to the development of what some have called \"second generation\" advance directives – the \"health care proxy appointment\" or \"medical power of attorney.\"\n\nLiving wills also reflect a moment in time, and may therefore need regular updating to ensure that the correct course of action can be chosen.\n\nAs before, the next generation advance directive was drawn from existing law – specifically from business law. Power of attorney statutes have existed in the United States since the days of \"common law\" (i.e., laws brought from England to the United States during the colonial period). These early powers of attorney allowed an individual to name someone to act in their stead. Drawing upon these laws, \"durable powers of attorney for health care\" and \"healthcare proxy appointment\" documents were created and codified in law, allowing an individual to appoint someone to make healthcare decisions in their behalf if they should ever be rendered incapable of making their wishes known. The appointed healthcare proxy has, in essence, the same rights to request or refuse treatment that the individual would have if still capable of making and communicating health care decisions.\nThe primary benefit of second-generation advance directives is that the appointed representative can make real-time decisions in actual circumstances, as opposed to advance decisions framed in hypothetical situations, as recorded in a living will. This new advance directive was heartily endorsed by the U.S. public, and supporting legislation soon followed in virtually all states.\n\nEventually, however, deficiencies in \"second-generation\" advance directives were also soon noted. Primarily, individuals faced problems similar to those that handicapped living wills – knowing what to tell the proxy decision-maker about one's wishes in a meaningful way. Studies found most of what appointed proxies are told is too vague for meaningful interpretation. In the absence of meaningful information, family and physician \"guesswork\" is found to be inaccurate as much as 76% of the time. While a study comparing next-of-kin decisions on behalf of an incapacitated person, (who later recovered) found that these surrogates chose correctly 68% of the time overall. This continuing problem led to the development of what might be called \"third generation\" advance directives.\n\nThird generation advance directives were designed to contain enriched content to assist individuals and their appointed agents, families, and physicians to better understand and honor their wishes. The first of the third-generation advance directives was the Values History by Doukas and McCullough, created at the Georgetown University School of Medicine, first published in 1988, and then more widely cited in an article in 1991. The Values History is a \"two-part advance directive instrument that elicits patient values about terminal medical care and therapy-specific directives.\" The goal of this advance directive is to move away from a focus on specific treatments and medical procedures to a focus on patient values and personal goals. Another values-based project was later published by Lambert, Gibson, and Nathanson at the Institute of Public Law, University of New Mexico School of Law in 1990. It continues to be made available via the Hospice and Palliative Care Federation. One persistent challenge of third generation-based values documents is to show a linkage between the elicited values and goals with medical care wishes, although studies have demonstrated that values regarding financial and psychological burden are strong motivators in not wanting a broad array of end-of-life therapies.\n\nThe next widely recognized third generation advance directive is the Medical Directive, created by Emanuel and Emanuel of Massachusetts General Hospital and Harvard Medical School. It is a six-page document that provides six case scenarios for advance medical decision-making. The scenarios are each associated with a roster of commonly considered medical procedures and interventions, allowing the individual to decide in advance which treatments are wanted or not wanted under the circumstances. Several criticisms regarding this advance directive have been expressed. Primarily, it prompts individuals to make medical treatment decisions, which they are typically not equipped to make.\n\nPerhaps the best known third-generation advance directive is the Five Wishes directive. This document was developed in collaboration with multiple experts with funding from the Robert Wood Johnson foundation, and is distributed by the organization Aging with Dignity. The document was endorsed by Mother Teresa of the Sisters of Calcutta and by the Chief Justice of the Florida state supreme court. The document meets statutory criteria in 42 states.\n\nThe most recent third-generation advance directive is the Lifecare Advance Directive. In creating this document, researchers reviewed more than 6,500 articles from medical, legal, sociological, and theological sources. The conclusion was that advance directives needed to be based more on \"health outcome states\" than on rosters of medical treatments and legal jargon. Building upon the insights gleaned from the literature review, an advance directive document created, tested in a study involving nearly 1,000 participants, and then comparison tested against other popular advance directive forms. The results indicated greater patient/proxy decision-making accuracy, and superior comprehensive content as compared with other documents tested. The primary criticism has been that it is very lengthy and tedious to complete.\n\nWhile some commentators suggest that any recording of one's wishes is problematic, the preponderance of experts recommend the completion of an advance directive document – especially one that includes both a living will and a proxy designation. While most of the public continue to rely upon their state's standard directive format, research demonstrates that many of these documents are too jargon laden and vague, confusing, and incomplete to adequately capture an individual's wishes, and that they focus too much on the needs of medical and legal practitioners to the exclusion of the needs of patients. Advance directive documents are increasingly available online.\n\nSome legal commentators have suggested that using a non-statutory advance directive will leave the user with a document that may not be honored. However, legal counsel for the Hastings Center for Bioethics refute this assertion. To make the best choice, individuals should consider reviewing several document styles to ensure that they complete the document that best meets their personal needs.\n\nNote: Some of the countries listed below are in the European Union, where legal systems may vary considerably from those in the U.S. \"Country reports on advance directives\" is a 2008 paper summarizing advance health care legislation in the European Union with a shorter summary for the U.S.; a 2009 paper also provides a European overview.\n\nThe laws regarding advance directives, powers of attorney, and enduring guardianships vary from state to state. In Queensland, for example, the concept of an advance health directive is defined in the \"Powers of attorney act of 1998\" and \"Guardianship and Administration act of 2000\". Tasmania has no specific legislation concerning advance healthcare directives.\n\nHealth Canada – Canada's federal health agency – has acknowledged the need for a greater investment in palliative and hospice care as the country faces a rapidly growing population of elderly and terminally ill citizens.\n\nMuch of the current focus in Canada is on advance care planning which involves encouraging individuals to reflect on and express their wishes for future care, including end-of-life care, before they become terminally ill or incapable of making decisions for themselves. A\nnumber of publicly funded initiatives exist to promote advance care planning and to encourage people to appoint \"substitute decision makers\" who make medical decisions and can give or withhold consent for medical procedures according to the patient's\npre-expressed wishes when the patient becomes incapable of doing so themselves\n\nIn 2008, The Advance Care Planning in Canada: A National Framework and Implementation Project was founded. The goal was to engage healthcare professionals and educate patients\nabout the importance of advance care planning and end of life care.\n\nPolling indicates that 96% of Canadians think that having a conversation with a loved one about planning for the end of life is important. However, the same polls show that only about 13% have actually done so, or have created an advance care plan for themselves.\nA 2014 Ipsos Reid Survey reveals that only about a third of Canadian doctors and nurses working in primary care feel comfortable discussing end of life issues with their patients. End-of-life issues in Canada have recently been highlighted due to the ongoing related debate about Physician assisted death in Canada. Former Federal Health Minister Rona Ambrose (July 15, 2013 to November 4, 2015) has stated: \"I think the starting point for me is that we still don't have the best elderly care and palliative care yet… So let's talk about making sure we have the best end-of-life care before we start talking about assisted suicide and euthanasia.\"\n\nIn England and Wales, people may make an advance directive or appoint a proxy under the Mental Capacity Act 2005. This is only for an advance refusal of treatment for when the person lacks mental capacity; to be legally binding, the advance decision must be specific about the treatment that is being refused and the circumstances in which the refusal will apply. To be valid, the person must have been competent and understood the decision when they signed the directive. Where the patient's advance decision relates to a refusal of life-prolonging treatment this must be recorded in writing and witnessed. Any advance refusal is legally binding providing that the patient is an adult, the patient was competent and properly informed when reaching the decision, it is clearly applicable to the present circumstances and there is no reason to believe that the patient has changed his or her mind. If an advance decision does not meet these criteria but appears to set out a clear indication of the patient's wishes, it will not be legally binding but should be taken into consideration in determining the patient's best interests. In June 2010, the Wealth Management Solicitors, Moore Blatch, announced that research showed demand for Living Wills had trebled in the two years previous, indicating the rising level of people concerned about the way in which their terminal illness will be managed. According to the British Government, every adult with mental capacity has the right to agree to or refuse medical treatment. In order to make their advance wishes clear, people can use a living will, which can include general statements about wishes, which are not legally binding, and specific refusals of treatment called \"advance decisions\" or \"advance directives\".\n\nOn 18 June 2009 the Bundestag passed a law on advance directives, applicable since 1 September 2009. Such law, based on the principle of the right of self-determination, provides for the assistance of a fiduciary and of the physician.\n\nOn 14 December 2017, Italian Senate officially approved a law on advance healthcare directive that came into force on 31 January 2018.\n\nControversy over end-of-life care emerged in Italy in 2006, when a terminally ill patient suffering from muscular dystrophy, Piergiorgio Welby, petitioned the courts for removal of his respirator. Debated in Parliament, no decision was reached. A doctor eventually honored Welby's wishes by removing the respirator under sedation. The physician was initially charged for violating Italy's laws against euthanasia, but was later cleared. Further debate ensued after the father of a 38-year-old woman, Eluana Englaro, petitioned the courts for permission to withdraw feeding tubes to allow her to die. Englaro had been in a coma for 17 years, following a car accident. After petitioning the courts for 10 years, authorization was granted and Englaro died in February 2009. In May 2008, apparently as a result of the recent Court of Cassation's holding in the case of Englaro, a guardianship judge in Modena, Italy used relatively new legislation to work around the lack of the advance directive legislation. The new law permitted a judicially appointed guardian (\"amministratore di sostegno\") to make decisions for an individual. Faced with a 70-year-old woman with end-stage Lou Gehrig's Disease who was petitioning the court (with the support of her family) to prevent any later use of a respirator, the judge appointed her husband as guardian with the specific duty to refuse any tracheotomy and/or respirator use if/when the patient became unable to refuse such treatment herself.\n\nIn the Netherlands, patients and potential patients can specify the circumstances under which they would want euthanasia for themselves. They do this by providing a written euthanasia directive. This helps establish the previously expressed wish of the patient even if the patient is no longer able to communicate. However, it is only one of the factors that is taken into account. Apart from the will in writing of the patients, at least two physicians, the second being totally unrelated to the first physician in a professional matter (e.g. working in another hospital, no prior knowledge of the medical case at hand), have to agree that the patient is terminally ill and that no hope for recovery exists. \n\nIn Switzerland, there are several organizations which take care of registering patient decrees, forms which are signed by the patients declaring that in case of permanent loss of judgement (e.g., inability to communicate or severe brain damage) all means of prolonging life shall be stopped. Family members and these organizations also keep proxies which entitle their holder to enforce such patient decrees. Establishing such decrees is relatively uncomplicated.\n\nHowever, in Switzerland, a patient decree has, as of November 2008, no legally binding effects, whether concerning civil or criminal aspects. Such a decree is today merely viewed as representing the supposed will of the person with the incapability. There is, however, a revision of the Swiss Civil Code under way that aims to change this situation (intended to be article 360 of the Swiss Civil Code) by making the patient decree a legally binding document.\nAggressive medical intervention leaves nearly two million Americans confined to nursing homes, and over 1.4 million Americans remain so medically frail as to survive only through the use of feeding tubes. Of U.S. deaths, 25–55% occur in health care facilities. As many as 30,000 persons are kept alive in comatose and permanently vegetative states.\n\nCost burdens to individuals and families are considerable. A national study found that: “In 20% of cases, a family member had to quit work;” 31% lost “all or most savings” (even though 96% had insurance); and “20% reported loss of [their] major source of income.” Yet, studies indicate that 70-95% of people would rather refuse aggressive medical treatment than have their lives medically prolonged in incompetent or other poor prognosis states.\n\nAs more and more Americans experienced the burdens and diminishing benefits of invasive and aggressive medical treatment in poor prognosis states – either directly (themselves) or through a loved one – pressure began to mount to devise ways to avoid the suffering and costs associated with treatments one did not want in personally untenable situations. The first formal response was the living will.\n\nIn the United States, all states recognize some form of living wills or the designation of a health care proxy. The term \"living will\" is not officially recognized under California law, but an advance health care directive or durable power of attorney may be used for the same purpose as a living will. A \"report card\" issued by the Robert Wood Johnson Foundation in 2002 concluded that only seven states deserved an \"A\" for meeting the standards of the model Uniform Rights of the Terminally Ill Act. Surveys show that one-third of Americans say they have had to make decisions about end-of-life care for a loved one.\n\nIn Pennsylvania on November 30, 2006, Governor Edward Rendell signed into law Act 169, that provides a comprehensive statutory framework governing advance health care directives and health care decision-making for incompetent patients. As a result, health care organizations make available a \"Combined Living Will & Health Care Power of Attorney Example Form from Pennsylvania Act 169 of 2006.\"\n\nSeveral states offer living will \"registries\" where citizens can file their living will so that they are more easily and readily accessible by doctors and other health care providers. However, in recent years some of these registries, such as the one run by the Washington State Department of Health, have been shuttered by the state government because of low enrollment, lack of funds, or both.\n\nOn July 28, 2009, Barack Obama became the first United States President to announce publicly that he had a living will, and to encourage others to do the same. He told an AARP town meeting, \"So I actually think it's a good idea to have a living will. I'd encourage everybody to get one. I have one; Michelle has one. And we hope we don't have to use it for a long time, but I think it's something that is sensible.\" The announcement followed controversy surrounding proposed health care legislation that included language that would permit the payment of doctors under Medicare to counsel patients regarding living wills, sometimes referred to as the \"infamous\" page 425. Shortly afterwards, bioethicist Jacob Appel issued a call to make living wills mandatory.\n\nIndian Supreme Court on March 9, 2018 permitted living wills and passive euthanasia. The country's apex court held that the right to a dignified life extends up to the point of having a dignified death.\n\n\n"}
{"id": "262772", "url": "https://en.wikipedia.org/wiki?curid=262772", "title": "Autoerotic fatality", "text": "Autoerotic fatality\n\nAutoerotic fatalities are accidental deaths that occur during sexual self-stimulation when an apparatus, device or prop that is being employed to enhance pleasure causes the death. Researchers only apply the term to unintentional deaths resulting from solitary sexual activity, not suicide or acts with a partner. The incidence of autoerotic fatalities in Western countries is around 0.5 per million inhabitants each year.\n\nAutoerotic asphyxia is the leading cause. 70 to 80% of autoerotic deaths are caused by hanging, while 10 to 30% are attributed to plastic bags or chemical use. Both of these lead to autoerotic asphyxia. 5 to 10% are related to electrocution, foreign body insertion, overdressing/body wrapping, or another atypical method. Specific causes include the use of chemicals such as amyl nitrite, GHB, or nitrous oxide, and props and tools such as knives, oversized dildos, ligatures or bags for asphyxiation, duct tape, electrical apparatus for shocks, water for self-immersion, fire-making equipment for self-immolation, or sharp, unhygienic or large fetishized objects. Male victims are much more likely to use a variety of devices during autoerotic behaviour than female victims.\n\nThe subject has been treated in two books, \"Autoerotic Fatalities\" by Hazelwood et al. (1983) and \"Autoerotic Asphyxiation: Forensic, Medical, and Social Aspects\" by Sheleg et al. (2006).\n\n\n\n\n"}
{"id": "1510082", "url": "https://en.wikipedia.org/wiki?curid=1510082", "title": "Bishop score", "text": "Bishop score\n\nBishop score, also Bishop's score, also known as cervix score is a pre-labor scoring system to assist in predicting whether induction of labor will be required. It has also been used to assess the likelihood of spontaneous preterm delivery. The Bishop Score was developed by Professor Emeritus of Obstetrics and Gynecology, Dr. Edward Bishop, and was first published in August 1964.\n\nThe total score is calculated by assessing the following five components on manual vaginal examination by a trained professional:\n\n\nThe Bishop score grades patients who would be most likely to achieve a successful induction. The duration of labor is inversely correlated with the Bishop score; a score that exceeds 8 describes the patient most likely to achieve a successful vaginal\nbirth. Bishop scores of less than 6 usually require that a cervical ripening method (pharmacologic or physical, such as a foley bulb) be used before other methods.\n\nThey can be remembered with the mnemonic: \"Call PEDS For Parturition\" = \"C\"ervical \"P\"osition, \"E\"ffacement, \"D\"ilation, \"S\"oftness; \"F\"etal \"S\"tation.\n\nThe examiner assigns a score to each component of 0 to 2 or 0 to 3. The highest possible score is 13 and the lowest possible score is 0.\n\nA Bishop's score 6 or less often indicates that induction (e.g., with controlled-release prostaglandin E2/prostin gel [Cervidil], intravaginal gel [Prostin], intracervical gel [Prepidil]) is unlikely to be successful. Some sources indicate that only a score of 8 or greater is reliably predictive of a successful induction.\n\nAccording to the Modified Bishop's pre-induction cervical scoring system, effacement has been replaced by cervical length in cm, with scores as follows: 0 for >3 cm, 1 for >2 cm, 2 for >1 cm, 3 for >0 cm. Cervical length may be easier and more accurate to measure and have less inter-examiner variability.\nAnother modification for the Bishop's score is the modifiers. Points are added or subtracted according to special circumstances as follows:\n\n"}
{"id": "43839116", "url": "https://en.wikipedia.org/wiki?curid=43839116", "title": "Caregiving by country", "text": "Caregiving by country\n\nCaregiving by country is the regional variation of caregiving practices as distinguished among countries.\n\nAccording to the Australian Bureau of Statistics 2001 paper on the health and well being of Carers, Carers save the Australian Federal Government over $30 billion a year, according to the same statistics there are over 300 000 Young Carers (Carers Australia states that a Young carer is any carer under the age of 25) with 1.5 million potential young carers, where potential is defined as a young person who lives in a household where there is at least one person who requires full-time care (Is disabled etc.). In 2015, carers provided around 1.9 billion hours of unpaid care. According to a new study by the University of Queensland, Australian carers are providing $13.2 billion worth of free mental health support to their friends and family members. This \"hidden workforce\" is an equivalent of 173,000 full-time mental health support workers.\n\nIn Australia they also have The Australian National Young Carers Action Team (ANYCAT) whose goal is to advocate on behalf of young carers (Being young carers themselves) each board member is the sole representative of their state or territory and represent as few as 75 000 Young Carers.\nIn Most states and Territories they have an ANYCAT equivalent team or Board. In Queensland this is called Young Carers Action Board Queensland (YCABQ).\n\nAccording to the National Bureau of Statistics of China 2011 report regarding China’s total population and structural changes, people belong to the age group of 60 years and above accounted for 184.99 million, which occupied 13.7 percent of the total national population at the end of 2011. The number has risen 0.47 percentage point comparing to the year of 2010. Of these, people age 65 years and above figured up 122.88 million that occupied 9.1 percent of the total and has increased 0.25 percentage points.\n\nAs a steadily increasing older population with a growing demand for long-term care, an issue of lacking of elderly care facilities as well as inadequate training for skilled caregivers has generated a social concern pertaining to elder care. According to the official Chinese media Xinhua, professionally qualified caregivers are in great request with approximately 10 million people needed to provide care for the Chinese aging population. However, the report also stated that only 300,000 people currently working as caregivers with less than 1/3 of them are trained properly.\n\nThere is no organized caregiver association in China. As a result, family members still construct the major source of caregiving in China especially in rural area where the quality of health services is a problem. A recent study aims to examine the effect of depression on family members of whom sons and daughters-in-law carry out main responsibilities in caring for elderly parents have indicated several findings, including:\n\n\nMore recently, another Caregiver Reaction Assessment was conducted in China with a purpose to analyze the reliability and validity among family caregivers of cancer patients. The study recruited 400 participants from the Second Affiliated Hospital of China Medical University in Shenyang with 312 family caregivers completed the survey. This study tried to measure caregiving experience from five aspects: disrupted schedule, financial problems, lack of family support, health problems, and self-esteem. The results indicated that the Chinese version of caregiving is profoundly influenced by the conception of Confucianism that emphasizes filial piety and guides the traditional caregiving ideology. Furthermore, the average age of adult children provide care is 46 years old of whom the majorities still work to support the family. As such, adult children sometimes bear significant physical and mental strain when their personal schedule and life activities are negatively impacted.\n\nWhen compared to all other countries Italy has the highest percentage of residents who are age 65 and older. The life expectancy for males is 76.7 and 82.9 for females, and it is predicted that by 2050 these numbers will reach 81 and 86 years, for men and women respectively. In addition, the country is faced with a rapidly growing older population and a shortage of younger individuals. Furthermore, the Minister of Labor and Social Policies of Italy stated that the family plays the most important role in the quality of life of elders, and it is the primary care provider for the older populations; however, the government also provides assistance to the family care giver by the following services:\nThe family plays an important role in providing care to its older parents but many children still have to work so that they can survive financially and as a result many caregivers are immigrant workers. These immigrant workers come in search of work from many parts of the world such as Moldavia, the Philippines, and Peru; however, the largest population of immigrant workers come from the Ukraine. Many of the caregivers work long hours and weeks, and also leave their own families behind and send them the money they make.\n\nTaiwan Today reports that currently in Taiwan only 72,000 seniors, about one third of those receiving long-term care, live in nursing institutes. The rest are taken care of by family members. Chen indicates that there were 600,000 family caregivers in Taiwan and they spent an average of 13.55 hours a day caregiving. 80 percent of these 600,000 family caregivers encountered limitations on social activities. 70 percent of them needed to take care of patients even when they did not feel well. He also mentions that 80 percent of family caregivers are women, and 50 percent of them are over 50 years old. Women are more likely to be regarded as natural caregivers because of gender expectations. Taiwanese females take care of children and spouses with chronicle illnesses to meet social norms and maintain filial piety. Among the family caregivers in Taiwan, some of them suffer from guilt, depression, nervousness, and injuries, sleeplessness, and hopelessness. Thus, it is a very essential task to promote the capabilities of family caregivers and improve their life quality.\n\nAccording to Carers UK, and based on the 2001 census around six million people in the UK provide care on an unpaid basis for a relative, friend or neighbour in need of support due to old age, disability, frailty or illness. The population of carers is dynamic: at least a third of all people will fulfil a caring role at some point in their lives.\n\nResearch has shown that becoming a carer can have many impacts on a person's life. These include financial costs, exclusion and discrimination at work, social isolation and poor health through stress and physical injury.\n\nAt least half of all carers are in full or part-time employment and some care for more than one person. Carers save the UK economy an estimated £87bn a year, and economic considerations form a key element in government policy to support carers.\n\nThe 2001 Census indicated that there are 175,000 young carers aged under 18 in the UK today. A poll commissioned by The Princess Royal Trust for Carers in 2004 indicates that the number of young carers could be much higher.\n\nPolicy and legislation in relation to caregivers living in Scotland is somewhat different from that in England, Wales and Ireland. Carers are defined by the Scottish Census as being \"individuals who look after, or give any help or support to family members, friends, neighbours and others because of long-term physical or mental ill health or disability or problems related to old age\" (Scotland’s Census Results Online [SCROL]). Estimates from the 2001 census put the numbers of carers in Scotland at 481,579. Of these, 175,969 are reported to provide more than 20 hours of care a week, and 24% provide more than 50 hours of care.\n\nCarers who provide care for 20 hours a week or more are regarded as being at the ‘heavy end’ of caring (Parker 1990). This assumes that they are the most involved carers, providing both personal and physical care, resulting in high levels of stress and most in need of support services. Many of these carers continue to provide care without support from social work or health services and because of this they remain hidden or invisible (Scottish Executive 2006, Cavaye 2006).\n\nCarers are viewed by the government as an important resource and in recent years have been given increasing recognition in health and social care policy. Since devolution in 1999 legislation and policy for caregivers has been developed by the former Scottish Executive (now Scottish Government).\n\nCarers in Scotland are regarded as ‘partners’ in the provision of care. As a result, support services provided to carers are regarded as part of the overall package of care to the person being looked after. This means that carers are not seen as service users and are therefore not responsible for the cost of any service provided. The exception to this is when a carer is looking after their partner; in that situation their income may be taken into account during a financial assessment.\n\nThis situation is different from that which exists in England where carers are viewed as services users in their own right and as such are liable for the cost of services provided. Yet, in many cases, it is not the carer who actually needs the service; it is the person being cared for who needs it because of their illness or disability.\n\nThe average life expectancy in the Ukraine is 61 for men and 72 for women, and as of 2005 Ukraine’s population is about 48 million of whom 15% are 65 and older. Older adults who cannot take care of themselves rely on family members, humanitarian organizations, government programs, or a combination of the three sources. According to the United Nations the Ukrainian Institute of Gerontology completed a study in 1998 which included 8,574 adults of retirement age, and the results demonstrated that older adults rely on older spouses, relatives or others, and departments of community social centers for care. In addition, 10 percent of the participants had an agreement with their caregiver(s) to trade inheritance of their property for the care that they were receiving. The United Nations also describes the very underfunded and understaffed situation of the country, which has effected the quality of services available to elders. For example, they mention that some 38,000 social workers provide care to about 500,000 older adults in need, and a total of 631 community centers and 131 social welfare units are available to provide support for the older population throughout the country. In addition to limited government and local resources many of those who could be caregivers to family members or others in the community choose to migrate to Italy.\n\nThe declining standards of living, economy, and production in the country of Ukraine could be contributing to the migration of Ukrainians to Italy and other countries abroad. It is estimated that there are close to 700,000 undocumented and approximately 195,000 legal working Ukrainian immigrants in Italy and a majority of them work as caregivers. Fedyuk mentions that many of those who move to Italy do not speak the language, work unregulated hours, may be underpaid, and usually are not prepared for all the physical and psychological requirements of being a caregiver. For instance, Fedyuk describes the working requirements of a 51-year-old caregiver who only had a 2-hour break during a single day and one day off a week. Also, there is mention of a 49-year-old woman who arrived to her new job and was not told the \"granny\" she would be caring for had an amputated arm and leg, was blind and could not speak. Those who migrate to Italy usually leave behind families and parents who require care themselves, and this puts further stress and pressure on the individual. Further research should be done in order to examine the extent of the consequences that may result to the caregiver, receiver of care, and those who are left behind in the cases of migrant Ukrainian workers.\n\nAccording to a November, 2007 survey on family caregiving, most family caregivers feel more positive about their experiences than they did just before they took on the responsibility, with significant differences in expectation prior to becoming a caregiver and the actual experience.\n\nThe survey also found that caregivers are often burdened by high out-of-pocket costs in caring for a spouse or parent, but:\n\nAccording to the Caregiver statistics fact sheet (2012) 43.5 million of adult family caregivers care for someone 50+ years of age and 14.9 million care for someone who has Alzheimer's disease or other dementia [Alzheimer's Association, 2011 Alzheimer's Disease Facts and Figures, Alzheimer's and Dementia, Vol. 7, Issue 2]. The number of male caregivers may be increasing and will continue to do so due to a variety of social demographic factors [Kramer, B. J. & E. H. Thompson, (eds.), \"Men as Caregivers,\" (New York:Prometheus Books, 2002]. Retrieved from.\n\nDellman-Jenkins, Blankemeyer and Pinkard found that young adults are increasingly becoming caregivers to their elderly relatives because of economic factors. A new population of caregivers to elderly are children and grandchildren, aged 40 years and younger, serving as the major source of support to an older relative(s). Retrieved from \n\nOther information about US caregivers:\nBy 2009, more than 61.6 million people provided unpaid care for a chronically ill, disabled or aged family member or friend at an estimated value of $450 billion.\n\n1.4 million children ages 8 to 18 provide care for an adult relative; 72% are caring for a parent or grandparent, although most are not the sole caregiver.\n\n30% of family caregivers caring for older individuals are themselves aged 65 or over; another 15% are between the ages of 45 to 54.\n\nLicensing and certification for caregiving can vary by state. In some states, the only licensing needed is by the company that employs the caregivers (unless it is from the state). In other states, an online certification course is recommended. Some states require in-classroom certification, and others require personal care licensing, which can be applied for online. Along with that, in some states, further certifications are required such as CPR training and first aid training.\n\nSome US states, such as California, have set out the responsibilities of the primary caregiver.\nOn February 27, 2004, the International Alliance of Carers Organizations (IACO) was launched by family caregiving organizations from Australia, the UK, Sweden, the Netherlands, and the U.S. The mission of the organization is threefold: \n\nIACO is headquartered in London. Initial IACO projects included promotion of a United Nations Day for Carers and a presentation on the IACO as part of a half-day workshop at the International Federation on Aging conference in Singapore on August 4, 2004. National family carer organizations in all countries are encouraged to join the alliance.\n\nCanadian Caregivers Association is a non-for-profit organization that was established to protect the rights of Canadian families and caregivers from all over Canada and warn them about malpractices in this business.\n\nEUROFAMCARE aims to provide a European review of the situation of family carers of elderly people in relation to the existence, familiarity, availability, use and acceptability of supporting services. \nIn 2003 six countries (Germany, Greece, Italy, Poland, Sweden, United Kingdom) formed a trans-European group, systematically representing the different types of welfare-states in Europe and started a comparative study. The Pan-European Group consists of 23 countries (including the six countries, which are represented by the members of the Consortium).\n\nThe last step is a feedback research action phase based both on the study results and on the pan-European expertise. A European Carers’ Charter in progress will be further developed by the new European network organization EUROCARERS in order to stimulate further activities both on national and European policy levels.\n\nEUROCARERS was formally launched in June 2007 to provide a united voice at European level and influence policy both nationally and within the European Union. Eurocarers currently comprises representatives of 18 organisations and research bodies from nine countries. Members have come together to influence policy within the European Institutions to ensure that the invaluable contribution of carers is recognised across Europe.\n\nEUFAMI - the European Federation of Families of People with Mental Illness - has been operating since 1992.\n\nThe Carers Association was founded in 1987 to represent family carers and advocate for carers rights. The national census of 2006 shows that there are 160,917 people who stated that they are carers and almost 41,000 of these carers are providing 43 or more hours of care each week. They estimate that carers provide 194 million hours of care a year to the value of about 2.5 billion Euros to the economy. Approximately 33,000 full-time carers qualify for the Carers Allowance from the government.\n\nThe Taiwan Association of Family Caregivers (TAFC) was built in 1996. It is the first non-profit organization to voice the rights for family caregivers in Taiwan. The association encourages each city to set up a chapter in order to provide family caregivers with services. The main association is in Taipei and there are ten chapters around Taiwan. The TAFC claims that (1) The government should share the burden of family carers; (2) The government should provide well-qualified respite services; (3) The government should provide family carers with training and support; (4) Labor policy should help employees fulfill their responsibilities as family caregivers.\n\nThe TAFC provides family caregivers with a consulting hotline, newsletters of the association, in-service courses for professional personnel, support groups, and counseling services for caregivers. The association has allied with the Welfare Organization of the Elderly, Taiwan Long-term Care Professional Association, and Taiwan Alzheimer's Disease Association to build a support network for family caregivers.\n\n\nThere is no known or formal caregiver association established in the Ukraine; however, there is an Institute of Gerontology which was established in 1958, and it has four main centers or associations which include:\nThis Institute has contributed research and publications which can be very helpful in assisting the older population of the country and the caregivers of this population.\n"}
{"id": "26639837", "url": "https://en.wikipedia.org/wiki?curid=26639837", "title": "Cellulite", "text": "Cellulite\n\nCellulite (also known as adiposis edematosa, dermopanniculosis deformans, status protrusus cutis, gynoid lipodystrophy, and orange peel syndrome) is the herniation of subcutaneous fat within fibrous connective tissue that manifests topographically as skin dimpling and nodularity, often on the pelvic region (specifically the buttocks), lower limbs, and abdomen. Cellulite occurs in most postpubescent females. A review gives a prevalence of 85 to 98% of women, indicating that it is physiologic rather than pathologic. It can result from a complex combination of factors ranging from hormones to heredity.\n\nThe causes of cellulite include changes in metabolism, physiology, diet and exercise habits, obesity, sex-specific dimorphic skin architecture, alteration of connective tissue structure, hormonal factors, genetic factors, the microcirculatory system, the extracellular matrix, and subtle inflammatory alterations.\n\nHormones play a dominant role in the formation of cellulite. Estrogen may be the important hormone in the development of cellulite. However, there has been no reliable clinical evidence to support such a claim. Other hormones, including insulin, the catecholamines adrenaline and noradrenaline, thyroid hormones, and prolactin, are all believed to participate in the development of cellulite.\n\nThere is a genetic element in individual susceptibility to cellulite. Researchers led by Enzo Emanuele have traced the genetic component of cellulite to particular polymorphisms in the angiotensin converting enzyme (ACE) and hypoxia-inducible factor 1A (HIF1a) genes.\n\nSeveral factors have been shown to affect the development of cellulite. Sex, race, biotype, distribution of subcutaneous fat, and predisposition to lymphatic and circulatory insufficiency have all been shown to contribute to cellulite.\n\nA high-stress lifestyle will cause an increase in the level of catecholamines, which have also been associated with the development of cellulite.\n\nCellulite is a multifactorial condition and can be resistant to the array of treatments currently available. Aside from 'topical' products (creams, ointments, etc) and injectables (eg collagenase), treatments for cellulite include non-invasive therapy such as mechanical suction or mechanical massage. Energy-based devices include radio frequency with deep penetration of the skin, ultrasound, laser and pulsed-light devices. Combinations of mechanical treatments and energy-based procedures are widely used. More invasive 'subcision' techniques utilise a needle-sized micro-scalpel to cut through the causative fibrous bands of connective tissue. Subcision procedures (manual, vacuum-assisted, or laser-assisted) are performed in specialist clinics with patients given local anaesthetic.\n\nCellulite is thought to occur in 80–90% of post-adolescent females.<ref name=\"doi 10.1111/j.1468-3083.2009.03556.x\"></ref> There appears to be a hormonal component to its presentation. Its existence as a real disorder has been challenged and the prevailing medical opinion is that it is merely the \"normal condition of many women\". It is rarely seen in males, but is more common in males with androgen-deficient states, such as Klinefelter's syndrome, hypogonadism, postcastration states and in those patients receiving estrogen therapy for prostate cancer. The cellulite becomes more severe as the androgen deficiency worsens in these males.\n\nThe term was first used in the 1920s by spa and beauty services to promote their services, and began appearing in English-language publications in the late 1960s, with the earliest reference in \"Vogue\" magazine, \"Like a swift migrating fish, the word \"cellulite\" has suddenly crossed the Atlantic.\"\n\n"}
{"id": "6580964", "url": "https://en.wikipedia.org/wiki?curid=6580964", "title": "Centre for Reviews and Dissemination", "text": "Centre for Reviews and Dissemination\n\nThe Centre for Reviews and Dissemination (CRD) is a health services research centre based at the University of York, England. CRD was established in January 1994, and aims to provide research-based information for evidence-based medicine. CRD carries out systematic reviews and meta-analyses of healthcare interventions, and disseminates the results of research to decision-makers in the NHS. \n\nCRD produces three databases:\n\n\nThese are freely available from the CRD database website and as part of the Cochrane Library.\n\nCRD also publishes a number of regular reports including \"Effective Health Care\" and \"Effectiveness Matters\".\n\nCRD is funded by the UK Department of Health's NHS Research and Development Programme, as well as from a number of other sources.\n\nCRD was established in 1994. Along with the UK Cochrane Centre, the \nCentre was originally created as part of the Information Systems \nStrategy of the NHS Research and Development Programme.\n\nThe original aims of the centre were:\n\n\nProfessor Trevor Sheldon established and directed CRD from 1994 to 1998. He was followed as director by Professor Jos Kleijnen from 1998 to 2005. The current director is Professor Lesley Stewart who took up appointment in 2006.\n\nThe Centre for Reviews and Dissemination (CRD) is part of the National Institute for Health Research (NIHR) and is a department of the University of York. CRD is one of the largest groups in the world engaged exclusively in evidence synthesis in the health field. The Centre comprises health researchers, medical information specialists, health economists and a dissemination team.\n\nCRD undertakes systematic reviews evaluating the research evidence on health and public health questions. The findings of CRD reviews are widely disseminated and have impacted on health care policy and practice, both in the UK and internationally.\n\nCRD produce the DARE, NHS EED and HTA databases which are used by health professionals, policy makers and researchers. \n\nCRD also undertake methods research and produce internationally accepted guidelines for undertaking systematic reviews.\n\nThe Centre's role in developing research evidence to support decision making in policy and practice was highlighted in the Cooksey report on UK health research funding and subsequently in the Government national research strategy Best Research for Best Health. CRD was also recognised by the Lancet as part of NHS R&D's most important contribution to the UK science base, namely building systematic review capacity and promulgating systematic reviews as a global public good.\n\nCRD receives core funding through the NIHR. This funding enables the centre to function as a national resource and provides the necessary infrastructure to respond to requests from policy makers and healthcare professionals, and to support the provision and promotion of the online databases (DARE, NHS EED and HTA ).\n\nIn addition to the core funding, the Centre has undertaken independent research for a number of different agencies including:\n\n\nCRD is one of seven independent academic centres currently undertaking reviews commissioned by NICE. CRD collaborate with the Centre for Health Economics at the University of York to undertake technology assessment reviews that inform NICE Technology Appraisals on the use of new and existing medicines and treatments within the NHS.\n\nCRD also has close links with the UK Cochrane Centre and contributes to the work of several of the Cochrane Collaboration’s Review and Methods groups. The Centre also has representation on the Steering and User groups of the Campbell Collaboration and is a member of the International Network of Agencies for Health Technology Assessment (INAHTA).\n\nCRD is a founder member of the Public Health Research Consortium (PHRC) with brings together researchers from 11 UK institutions. The PHRC aims to strengthen the evidence base for public health, with a strong emphasis on tackling socioeconomic inequalities in health. The centre is collaborating on a number of projects and is providing support for information retrieval and the knowledge transfer activities of the Consortium. \n\n"}
{"id": "20811415", "url": "https://en.wikipedia.org/wiki?curid=20811415", "title": "Cephalic presentation", "text": "Cephalic presentation\n\nA cephalic presentation or head presentation or head-first presentation is a situation at childbirth where the fetus is in a longitudinal lie and the head enters the pelvis first; the most common form of cephalic presentation is the vertex presentation where the occiput is the leading part (the part that first enters the birth canal). All other presentations are abnormal (malpresentations) which are either more difficult to deliver or not deliverable by natural means.\n\nThe movement of the fetus to cephalic presentation is called \"head engagement\". It occurs in the third trimester. In head engagement, the fetal head descends into the pelvic cavity so that only a small part (or none) of it can be felt abdominally. The perineum and cervix are further flattened and the head may be felt vaginally. Head engagement is known colloquially as the \"baby drop\", and in natural medicine as the \"lightening\" because of the release of pressure on the upper abdomen and renewed ease in breathing. However, it severely reduces bladder capacity, increases pressure on the pelvic floor and the rectum, and the mother may experience the perpetual sensation that the fetus will \"fall out\" at any moment.\n\nIn the vertex presentation the head is flexed and the occiput leads the way. This is the most common configuration and seen at term in 95% of singletons. If the head is extended, the face becomes the leading part. Face presentations account for less than 1% of presentations at term. In the sinicipital presentation the large fontanelle is the presenting part; with further labor the head will either flex or extend more so that in the end this presentation leads to a vertex or face presentation. In the brow presentation the head is slightly extended, but less than in the face presentation. The chin presentation is a variant of the face presentation with maximum extension of the head.\nNon-cephalic presentations are the breech presentation (3.5%) and the shoulder presentation (0.5%).\n\nThe vertex is the area of the vault bounded anteriorly by the anterior fontanelle and the coronal suture, posteriorly by the posterior fontanelle and the lambdoid suture and laterally by 2 lines passing through the parietal eminences.\n\nIn the vertex presentation the occiput typically is anterior and thus in an optimal position to negotiate the pelvic curve by extending the head. In an occiput posterior position, labor becomes prolonged and more operative interventions are deemed necessary. The prevalence of the persistent occiput posterior is given as 4.7%\n\nThe vertex presentations are further classified according to the position of the occiput, it being right, left, or transverse, and anterior or posterior: \n\nThe Occipito-Anterior position is ideal for birth - it means that the baby is lined up so as to fit through the pelvis as easily as possible. The baby is head down, facing the spine, with its back anterior. In this position, the baby's chin is tucked onto its chest, so that the smallest part of its head will be applied to the cervix first. The position is usually \"Left Occiput Anterior\", or LOA. Occasionally, the baby may be \"Right Occiput Anterior\", or ROA.\n\nFactors that predispose to face presentation are prematurity, macrosomia, anencephaly and other malformations, cephalopelvic disproportion, and polyhydramnios. In an uncomplicated face presentation duration of labor is not altered. Perinatal losses with face presentation occur with traumatic version and extraction and midforceps procedures Duff indicates that the prevalence of face presentations is about 1/500–600., while Benedetti et al. found it to be 1/1,250 term deliveries.\n\nFace presentations are classified according to the position of the chin (mentum):\n\nWhile some consider the brow presentation as an intermediate stage towards the face presentation, others disagree. Thus Bhal et al. indicated that both conditions are about equally common (1/994 face and 1/755 brow positions), and that prematurity was more common with face while postmaturity was more common with brow positions.\n\nThe Oskie presentation is similar to the Occipito-Anterior position, where the baby is head down, facing the spine, with back on the ventral side of the uterus; however, in this position, while the torso is aligned with the mother's longitudinal axis, the legs of the fetus are extended straight along the frontal axis of the mother, as if the baby is creating a right angle with its body. For the Oskie position to occur the baby's head must be far down the pelvis in order to allow room for leg extension, typically the arms are bent, tucked against the baby's body. There are no known complications for labor and delivery. This presentation is rare and is not well researched.\n\nThe piriform (pear-shaped) morphology of the uterus has been given as the major cause for the finding that most singletons favor the cephalic presentation at term. The fundus is larger and thus a fetus will adapt its position so that the bulkier and more movable podalic pole makes use of it, while the head moves to the opposite site. Factors that influence this positioning include the gestational age (earlier in gestation breech presentations are more common as the head is relatively bigger), size of the head, malformations, amount of amniotic fluid, presence of multiple gestations, presence of tumors, and others.\n\nTwo-thirds of all vertex presentations are LOA, possibly because of the asymmetry created by the descending colon that is on the left side of the pelvis .\n\nUsually performing the Leopold maneuvers will demonstrate the presentation and possibly the position of the fetus. Ultrasound examination delivers the precise diagnosis and may indicate possible causes of a malpresentation. On vaginal examination, the leading part of the fetus becomes identifiable after the amniotic sac has been broken and the head is descending in the pelvis.\n\nMany factors determine the optimal way to deliver a baby. A vertex presentation is the ideal situation for a vaginal birth, however, occiput posterior positions tend to proceed more slowly, often requiring an intervention in the form of forceps, vacuum extraction, or Cesarean section. In a large study, a majority of brow presentations were delivered by Cesarean section, however, because of 'postmaturity', factors other than labour dynamics may have played a role. Most face presentations can be delivered vaginally as long as the chin is anterior; there is no increase in fetal or maternal mortality. Mento-posterior positions cannot be delivered vaginally in most cases (unless rotated) and are candidates for Cesarean section in contemporary management.\n"}
{"id": "3340717", "url": "https://en.wikipedia.org/wiki?curid=3340717", "title": "Certified nurse midwife", "text": "Certified nurse midwife\n\nIn the United States, a certified nurse-midwife (CNM) is a midwife who exceeds the International Confederation of Midwives essential competencies for a midwife and is also an advanced practice registered nurse having completed registered nursing and midwifery education. CNMs provide care of women across their lifespan, including pregnancy and the postpartum period, and well woman care and birth control. Certified nurse midwives are exceptionally recognized by the International Confederation of Midwives as a type of midwives in the United States.\n\nThe American College of Nurse-Midwives accredits midwifery education programs and serves as the national specialty society for the nation's CNMs and Certified Midwives (CMs). CNMs in most states are required to \n\nCNMs function as primary healthcare providers for women and most often provide medical care for relatively healthy women, whose health and births are considered uncomplicated and not \"high risk,\" as well as their neonates. Often, women with high risk pregnancies can receive the benefits of midwifery care from a CNM in collaboration with a physician. CNMs may work closely or in collaboration with an obstetrician & gynecologist, who provides consultation and/or assistance to patients who develop complications or have complex medical histories or disease(s). CNMs provide health care for sexual health, as they also see women for routine exams and are able to initiate all types of contraception.\n\nCNMs practice in hospitals and private practice medical clinics and may also deliver babies in birthing centers and attend at-home births. Some work with academic institutions as professors. They are able to prescribe medications, treatments, medical devices, therapeutic and diagnostic measures. CNMs are able to provide medical care to women from puberty through menopause, including care for their newborn (neonatology), antepartum, intrapartum, postpartum and nonsurgical gynecological care. In some cases, CNMs may also provide care to the male partner, in areas of sexually transmitted diseases and reproductive health, of their female patients. In the United States, fewer than 1% of nurse midwives are men..\n\n"}
{"id": "9341141", "url": "https://en.wikipedia.org/wiki?curid=9341141", "title": "Clinic management system", "text": "Clinic management system\n\nClinic management systems are computer software products that coordinate and integrate all the inherent activities involved in the management and running of a healthcare facility. They must meet specified security, technology and functionality standards for managing electronic medical records and practice management information. Some systems maintain the complete Patient Information coupled with the electronic medical records (EMR), medical billing, EDI/HCFA claim sending and meeting the stipulated security, technology & functional standards.\n\nAside from general patient care, a clinic may also participate in running clinical trials. In order to manage the additional responsibilities in managing a clinical trial, a clinic may also use a CTMS or clinical data management system (CDMS) in addition to and combined with their EMR and billing systems.\n\nHealthcare computer system, commonly known as clinic management system, is created to computerize manual operations in clinics. The primary purpose is to digitize patient records so as to make data retrieval easy and efficient. Being in the digital form, patient data can be conveniently shared and accessed by multiple simultaneous users at different locations, resulting in smoother clinical operations and collaboration among clinicians. It also means that patient data can be easily backed up, and be protected for confidentiality and from tampering through access control. In addition, clinical tasks involving panel billing, inventory management and accounting are all made easy, and in some manner automated. Economically, clinics benefit from constant cost savings as a result of increased productivity and overall efficiency. Essentially, everyone in the clinic benefits from the system – doctor, nurse, clerk, administrator and the clinic owner – which makes their lives easy and removes unnecessary human errors from their daily activities.\n\n"}
{"id": "2021968", "url": "https://en.wikipedia.org/wiki?curid=2021968", "title": "Computerized physician order entry", "text": "Computerized physician order entry\n\nComputerized physician order entry (CPOE), sometimes referred to as computerized provider order entry or computerized provider order management (CPOM), is a process of electronic entry of medical practitioner instructions for the treatment of patients (particularly hospitalized patients) under his or her care.\n\nThe entered orders are communicated over a computer network to the medical staff or to the departments (pharmacy, laboratory, or radiology) responsible for fulfilling the order. CPOE reduces the time it takes to distribute and complete orders, while increasing efficiency by reducing transcription errors including preventing duplicate order entry, while simplifying inventory management and billing. \n\nCPOE is a form of patient management software.\n\nIn a graphical representation of an order sequence, specific data should be presented to CPOE system staff in cleartext, including: \n\n\nSome textual data can be reduced to simple graphics.\n\nCPOE systems use terminology familiar to medical and nursing staff, but there are different terms used to classify and concatenate orders. The following items are examples of additional terminology that a CPOE system programmer might need to know:\n\nThe application responding to, \"i.e.\", performing, a request for services (orders) or producing an observation. The filler can also originate requests for services (new orders), add additional services to existing orders, replace existing orders, put an order on hold, discontinue an order, release a held order, or cancel existing orders.\n\nA request for a service from one application to a second application. In some cases an application is allowed to place orders with itself.\n\nOne of several segments that can carry order information. Future ancillary specific segments may be defined in subsequent releases of the Standard if they become necessary.\n\nThe application or individual originating a request for services (order).\n\nA list of associated orders coming from a single location regarding a single patient.\n\nA grouping of orders used to standardize and expedite the ordering process for a common clinical scenario. (Typically, these orders are started, modified, and stopped by a licensed physician.)\n\nA grouping of orders used to standardize and automate a clinical process on behalf of a physician. (Typically, these orders are started, modified, and stopped by a nurse, pharmacist, or other licensed health professional.)\n\nFeatures of the ideal computerized physician order entry system (CPOE) include:\n\nIn the past, physicians have traditionally hand-written or verbally communicated orders for patient care, which are then transcribed by various individuals (such as unit clerks, nurses, and ancillary staff) before being carried out. Handwritten reports or notes, manual order entry, non-standard abbreviations and poor legibility lead to errors and injuries to patients, . A follow up IOM report in 2001 advised use of electronic medication ordering, with computer- and internet-based information systems to support clinical decisions. Prescribing errors are the largest identified source of preventable hospital medical error. A 2006 report by the Institute of Medicine estimated that a hospitalized patient is exposed to a medication error each day of his or her stay. While further studies have estimated that CPOE implementation at all nonrural hospitals in the United States could prevent over 500,000 serious medication errors each year. Studies of computerized physician order entry (CPOE) has yielded evidence that suggests the medication error rate can be reduced by 80%, and errors that have potential for serious harm or death for patients can be reduced by 55%, and other studies have also suggested benefits. Further, in 2005, CMS and CDC released a report that showed only 41 percent of prophylactic antibacterials were correctly stopped within 24 hours of completed surgery. The researchers conducted an analysis over an eight-month period, implementing a CPOE system designed to stop the administration of prophylactic antibacterials. Results showed CPOE significantly improved timely discontinuation of antibacterials from 38.8 percent of surgeries to 55.7 percent in the intervention hospital. CPOE/e-Prescribing systems can provide automatic dosing alerts (for example, letting the user know that the dose is too high and thus dangerous) and interaction checking (for example, telling the user that 2 medicines ordered taken together can cause health problems). In this way, specialists in pharmacy informatics work with the medical and nursing staffs at hospitals to improve the safety and effectiveness of medication use by utilizing CPOE systems.\n\nGenerally, CPOE is advantageous, as it leaves the trails of just better formatting retrospective information, similarly to traditional hospital information systems designs. The key advantage of providing information from the physician in charge of treatment for a single patient to the different roles involved in processing he treatise itself is widely innovative. This makes CPOE the primary tool for information transfer to the performing staff and lesser the tool for collecting action items for the accounting staff. However, the needs of proper accounting get served automatically upon feedback on completion of orders.\n\nCPOE is generally not suitable without reasonable training and tutoring respectively. As with other technical means, the system based communicating of information may be inaccessible or inoperable due to failures. That is not different to making use of an ordinary telephone or with conventional hospital information systems. Beyond, the information conveyed may be faulty or erratic. A concatenated validating of orders must be well organized. Errors lead to liability cases as with all professional treatment of patients.\n\nPrescriber and staff inexperience may cause slower entry of orders at first, use more staff time, and is slower than person-to-person communication in an emergency situation. Physician to nurse communication can worsen if each group works alone at their workstations.\n\nBut, in general, the options to reuse order sets anew with new patients lays the basic for substantial enhancement of the processing of services to the patients in the complex distribution of work amongst the roles involved. The basic concepts are defined with the clinical pathway approach. However, success does not occur by itself. The preparatory work has to be budgeted from the very beginning and has to be maintained all the time. Patterns of proper management from other service industry and from production industry may apply. However, the medical methodologies and nursing procedures do not get affected by the management approaches.\n\nCPOE presents several possible dangers by introducing new types of errors. Automation causes a false sense of security, a misconception that when technology suggests a course of action, errors are avoided. These factors contributed to an \"increased\" mortality rate in the Children's Hospital of Pittsburgh's Pediatric ICU when a CPOE systems was introduced. In other settings, shortcut or default selections can override non-standard medication regimens for elderly or underweight patients, resulting in toxic doses. Frequent alerts and warnings can interrupt work flow, causing these messages to be ignored or overridden due to alert fatigue. CPOE and automated drug dispensing was identified as a cause of error by 84% of over 500 health care facilities participating in a surveillance system by the United States Pharmacopoeia. Introducing CPOE to a complex medical environment requires ongoing changes in design to cope with unique patients and care settings, close supervision of overrides caused by automatic systems, and training, testing and re-training all users.\n\nCPOE systems can take years to install and configure. Despite ample evidence of the potential to reduce medication errors, adoption of this technology by doctors and hospitals in the United States has been slowed by resistance to changes in physician's practice patterns, costs and training time involved, and concern with interoperability and compliance with future national standards. According to a study by RAND Health, the US healthcare system could save more than 81 billion dollars annually, reduce adverse medical events and improve the quality of care if it were to widely adopt CPOE and other health information technology. As more hospitals become aware of the financial benefits of CPOE, and more physicians with a familiarity with computers enter practice, increased use of CPOE is predicted. Several high-profile failures of CPOE implementation have occurred, so a major effort must be focused on change management, including restructuring workflows, dealing with physicians' resistance to change, and creating a collaborative environment.\n\nAn early success with CPOE by the United States Department of Veterans Affairs (VA) is the Veterans Health Information Systems and Technology Architecture or VistA. A graphical user interface known as the Computerized Patient Record System (CPRS) allows health care providers to review and update a patient's record at any computer in the VA's over 1,000 healthcare facilities. CPRS includes the ability to place orders by CPOE, including medications, special procedures, x-rays, patient care nursing orders, diets and laboratory tests.\n\nThe world's first successful implementation of a CPOE system was at El Camino Hospital in Mountain View, California in the early 1970s. The Medical Information System (MIS) was originally developed by a software and hardware team at Lockheed in Sunnyvale, California, which became the TMIS group at Technicon Instruments Corporation. The MIS system used a light pen to allow physicians and nurses to quickly point and click items to be ordered.\n\n, one of the largest projects for a national EHR is by the National Health Service (NHS) in the United Kingdom. The goal of the NHS is to have 60,000,000 patients with a centralized electronic health record by 2010. The plan involves a gradual roll-out commencing May 2006, providing general practices in England access to the National Programme for IT (NPfIT). The NHS component, known as the \"Connecting for Health Programme\", includes office-based CPOE for medication prescribing and test ordering and retrieval, although some concerns have been raised about patient safety features.\n\nIn 2008, the Massachusetts Technology Collaborative and the New England Healthcare Institute (NEHI) published research showing that 1 in 10 patients admitted to a Massachusetts community hospital suffered a preventable medication error. The study argued that Massachusetts hospitals could prevent 55,000 adverse drug events per year and save $170 million annually if they fully implemented CPOE. The findings prompted the Commonwealth of Massachusetts to enact legislation requiring all hospitals to implement CPOE by 2012 as a condition of licensure.\n\nIn addition, the study also concludes that it would cost approximately $2.1 million to implement a CPOE system, and a cost of $435,000 to maintain it in the state of Massachusetts while it saves annually about $2.7 million per hospital. The hospitals will still see payback within 26 months through reducing hospitalizations generated by error. Despite the advantages and cost savings, the CPOE is still not well adapted by many hospitals in the US.\n\nThe Leapfrog’s 2008 Survey showed that most hospitals are still not complying with having a fully implemented, effective CPOE system. The CPOE requirement became more challenging to meet in 2008 because the Leapfrog introduced a new requirement: Hospitals must test their CPOE systems with Leapfrog’s CPOE Evaluation Tool. So the number of hospitals in the survey considered to be fully meeting the standard dropped to 7% in 2008 from 11% the previous year. Though the adoption rate seems very low in 2008, it is still an improvement from 2002 when only 2% of hospitals met this Leapfrog standard.\n\n\n"}
{"id": "12310114", "url": "https://en.wikipedia.org/wiki?curid=12310114", "title": "Control banding", "text": "Control banding\n\nControl banding is a qualitative or semi-quantitative risk assessment and management approach to promoting occupational health and safety. It is intended to minimize worker exposures to hazardous chemicals and other risk factors in the workplace and to help small businesses by providing an easy-to-understand, practical approach to controlling hazardous exposures at work. \n\nThe principle of control banding was first applied to dangerous chemicals, chemical mixtures, and fumes. The control banding process emphasizes the controls needed to prevent hazardous substances from causing harm to people at work. The greater the potential for harm, the greater the degree of control needed to manage the situation and make the risk “acceptable.” \n\nA single \"control\" technology or strategy is matched with a single \"band\", or range of exposures (e.g. 1-10 milligrams per cubic meter) for a particular class of chemicals (e.g. skin irritants, reproductive hazards). \n\nHere is an example of four control bands developed for inhalation hazards.\n\nIn the United Kingdom, the Health and Safety Executive (HSE) has developed a comprehensive control banding model called \"COSHH Essentials\" (COSHH stands for \"control of substances hazardous to health.)\"\n\nThe use of control banding strategies has become very popular in the pharmaceutical industry where early stage development compounds may have little or no toxicology data.\n\nControl banding is not without limitations and still requires professional knowledge and experience to verify that the control measures specified have been properly installed, maintained, and used. Controls should be validated prior to use by either using substance specific industrial hygiene methods or performing surrogate monitoring.\n\n\n"}
{"id": "6045553", "url": "https://en.wikipedia.org/wiki?curid=6045553", "title": "Convalescence", "text": "Convalescence\n\nConvalescence is the gradual recovery of health and strength after illness or injury. It refers to the later stage of an infectious disease or illness when the patient recovers and returns to previous health, but may continue to be a source of infection to others even if feeling better. In this sense, \"recovery\" can be considered a synonymous term. This also sometimes includes patient care after a major surgery, under which they are required to visit the doctor for regular check-ups.\n\nConvalescent care facilities are sometimes recognized by the acronym CCF.\n\n"}
{"id": "31559443", "url": "https://en.wikipedia.org/wiki?curid=31559443", "title": "Domestic violence and pregnancy", "text": "Domestic violence and pregnancy\n\nPregnancy when coupled with domestic violence is a form of intimate partner violence (IPV) where health risks may be amplified. Abuse during pregnancy, whether physical, verbal or emotional, produces many adverse physical and psychological effects for both the mother and fetus. Domestic violence during pregnancy is categorized as abusive behavior towards a pregnant woman, where the pattern of abuse can often change in terms of severity and frequency of violence. Abuse may be a long-standing problem in a relationship that continues after a woman becomes pregnant or it may commence in pregnancy. Although female-to-male partner violence occurs in these settings, the overwhelming form of domestic violence is perpetrated by men against women. Pregnancy provides a unique opportunity for healthcare workers to screen women for domestic violence though a recent review found that the best way in which to do this is unclear. Reducing domestic violence in pregnancy should improve outcomes for mothers and babies though more good quality studies are needed to work out effective ways of screening pregnant women.\n\nDomestic abuse can be triggered by pregnancy for a number of reasons. Pregnancy itself can be used a form of coercion and the phenomenon of preventing an intimate partner's reproductive choice is referred to as reproductive coercion. Studies on birth control sabotage performed by males against female partners have indicated a strong correlation between domestic violence and birth control sabotage. Pregnancy can also lead to a hiatus of domestic violence when the abuser does not want to harm the unborn child. The risk of domestic violence for pregnant women is greatest immediately after childbirth.\n\nDomestic violence can increase a woman's chances of becoming pregnant and the number of children she has, both because the woman may be coerced into sex and because she may be prevented from using birth control. A correlation has been shown between large families and domestic violence. Whereas previously it was thought that having many children and the resultant stress of large families increased likelihood domestic violence, it has been shown that the violence commonly predates the births.\n\nBirth control sabotage, or reproductive coercion, is a form of coercion where someone manipulates another person's use of birth control - weakening efforts to prevent an unwanted pregnancy. Replacing birth control pills with fakes, puncturing condoms, and threats and violence are examples of prevention of an individual's attempt to avoid pregnancy. Pregnancy-promoting behavior of abusive male partners is one method of domestic violence and is associated with unwanted pregnancy, particularly in adolescents. Reproductive coercion itself is a form of domestic violence because it results from unwanted sexual activity and hinders a woman’s ability to control her body. Forced pregnancy can also be a form of financial abuse when a woman becomes trapped in a relationship because the pregnancy has led to economic dependence for new mothers.\n\nUnintended pregnancies are 2 to 3 times more likely to be associated with abuse than intended pregnancies. Research among adolescent populations shows females who experience IPV use condoms at low rates and are fearful of negotiating the use of condoms. In a study of sexually experienced women 15-19 in Uganda, surveys found that fourteen percent of women’s first sexual intercourse had been coerced. Of those fourteen percent, the women were far more likely to be having unprotected sex without the use of modern contraceptives and to have had unintended pregnancies within the last six months compared to women who had not been sexually coerced. In Egypt, over 80% of rural women believe that beatings are sometimes justified and one of the most common reasons given as a just cause for beatings is refusing a man sex. This affects the ability of women to protect themselves from unwanted sexual contact and the consequences of sexual intercourse, such as pregnancy and sexually transmitted infections.\n\nA study conducted by the Center for Impact Research on young mothers classified birth control sabotage into two categories: verbal and behavioral. Verbal sabotage is verbal or emotional pressure not to use birth control, or pressure to become pregnant. Behavioral sabotage is the use of force to prevent the use of birth control, or to have unprotected sexual intercourse.\n\nIn most cases, domestic violence can be prompted by or intensified by pregnancy, but in some cases domestic violence ends during pregnancy because the abuser makes a conscious effort to not harm the fetus.\n\nDomestic violence does not always increase during pregnancy and can even lead to a hiatus in violence. This phenomenon can provide protection for both the woman and child. Because this can lead to decreased violence, some women use pregnancy as a means of protection against domestic abuse. Since abuse generally restarts after the pregnancy ends, women may get pregnant intentionally to prevent violence. However, since women who have been abused before getting pregnant are more likely to experience violence during pregnancy, this is not a reliable means of protection.\n\nAlthough pregnancy can be a protective period for some women, either in terms of a hiatus of pre-existing violence, for others it is a risk period during which abuse may begin or escalate. Women with violent partners have a hard time protecting themselves from unintended pregnancy and sexual violence can directly lead to pregnancy. Studies consistently indicate that domestic violence is more common in large families. However, international studies show that 25% of women are abused for the first time during pregnancy.\n\nIn one study conduct by Campbell \"et al.\", women were asked to speculate on why they thought they were abused during their pregnancies. The answers were categorized into four categories:\n\nThere are many dangerous effects that violence during pregnancy can cause for both the mother and child. A violent pregnancy is considered high risk because verbal, emotional, and physical abuse all lead to adverse health consequences for both the mother and fetus. Violence during pregnancy has been associated with miscarriage, late prenatal care, stillbirth, preterm birth, fetal injury (including bruising, broken and fractured bones, stab wounds and low birth weight. Violence during pregnancy also leads to additional risks for the mother such as increased mental health problems, suicide attempts, worsening of chronic illness, injury, substance abuse, anxiety, stress, chronic pain, and gynecological problems. Women battered during pregnancy were more frequently and severely beaten throughout the course of their relationship compared to women who were not abused during pregnancy. IPV also accounts for a large portion of maternal mortality. Homicide is the second leading cause of injury related deaths in pregnant and post-partum women in the United States and a study conducted in hospital in India found that 16% of all deaths during pregnancy were a result of partner violence. Studies have also found a correlation between domestic violence and increased use of abortion. Pregnant abused women are less likely to report abuse or leave their abuser because of added financial and housing security concerns.\n\nCertain women are more likely to be abused during pregnancy than others. Women who have been abused before getting pregnant are at higher risk of violence during pregnancy. Abuse is not restricted to a specific socio-economic or demographic group of women or to a specific period in a woman’s reproductive life.\n\nIn general, the rate of physical violence during pregnancy decreases as household income increases. Women whose total household income was less than $16,000 were much more likely to experience physical or sexual violence during pregnancy than women with a total household income over $16,000.\n\nPartner violence in a relationship increases the chances of unintended pregnancy. A Canadian study that outlined causes of physical abuse identified “social instability” (e.g. low age, unmarried, lower level of education, and unemployment) as a trigger for violence and used unplanned pregnancies as an example. This suggests that partner violence can lead to increased unintended pregnancies which, in turn, increases physical abuse. Younger women are statistically more susceptible to reproductive coercion and this may be due to less experience in relationships and, for minors, less access to doctor’s appointments and emergency contraception. Adolescents are especially at risk and teenage pregnancy is correlated with increased rates of domestic violence. Young women with older boyfriends are more likely to experience domestic violence. Women who experience physical violence from their husbands are less likely to use contraception and more likely to have an unwanted pregnancy.\n\nA study done on reporting rates of domestic violence concluded that a woman’s risk of physical and sexual violence during pregnancy is under-reported and underestimated. Each year, over 324,000 pregnant women are victims of domestic violence in the United States. A number of countries have sought to statistically estimate the number of adult women who have experienced domestic violence during pregnancy:\n\nIncidence rates are higher for teenagers. The incidence rate for low-income, teen mothers is as high as 38%.\n"}
{"id": "20611030", "url": "https://en.wikipedia.org/wiki?curid=20611030", "title": "Ejaculation", "text": "Ejaculation\n\nEjaculation is the discharge of semen (normally containing sperm) from the male reproductory tract, usually accompanied by orgasm. It is the final stage and natural objective of male sexual stimulation, and an essential component of natural conception. In rare cases, ejaculation occurs because of prostatic disease. Ejaculation may also occur spontaneously during sleep (a nocturnal emission or \"wet dream\"). \"Anejaculation\" is the condition of being unable to ejaculate. \"Dysejaculation\" is an ejaculation that is painful or uncomfortable. Retrograde ejaculation is the condition where semen travels backwards into the bladder rather than out the urethra.\n\nA usual precursor to ejaculation is the sexual arousal of the male, leading to the erection of the penis, though not every arousal nor erection leads to ejaculation. Penile sexual stimulation during masturbation or vaginal, anal, oral, or non-penetrative sexual activity may provide the necessary stimulus for a man to achieve orgasm and ejaculation. With regard to intravaginal ejaculation latency time, men typically reach orgasm 5–7 minutes after the start of penile-vaginal intercourse, taking into account their desires and those of their partners, but 10 minutes is also a common intravaginal ejaculation latency time. A prolonged stimulation either through foreplay (kissing, petting and direct stimulation of erogenous zones before penetration during intercourse) or stroking (during masturbation) leads to an adequate amount of arousal and production of pre-ejaculatory fluid. While the presence of sperm in pre-ejaculatory fluid is thought to be rare, sperm from an earlier ejaculation, still present in the urethra, may be picked up by pre-ejaculatory fluid. In addition, infectious agents (including HIV) can often be present in pre-ejaculate.\n\nPremature ejaculation is when ejaculation occurs before the desired time. If a man is unable to ejaculate in a timely manner after prolonged sexual stimulation, in spite of his desire to do so, it is called delayed ejaculation or anorgasmia. An orgasm that is not accompanied by ejaculation is known as a dry orgasm.\n\nWhen a man has achieved a sufficient level of stimulation, the orgasm and ejaculation begins. At that point, under the control of the sympathetic nervous system, semen containing sperm is produced (emission). The semen is ejected through the urethra with rhythmic contractions. These rhythmic contractions are part of the male orgasm. They are generated by the bulbospongiosus and pubococcygeus muscles under the control of a spinal reflex at the level of the spinal nerves S2–4 via the pudendal nerve. The typical male orgasm lasts several seconds.\n\nAfter the start of orgasm, pulses of semen begin to flow from the urethra, reach a peak discharge and then diminish in flow. The typical orgasm consists of 10 to 15 contractions, although the man is unlikely to be consciously aware of that many. Once the first contraction has taken place, ejaculation will continue to completion as an involuntary process. At this stage, ejaculation cannot be stopped. The rate of contractions gradually slows during the orgasm. Initial contractions occur at an average interval of 0.6 seconds with an increasing increment of 0.1 seconds per contraction. Contractions of most men proceed at regular rhythmic intervals for the duration of the orgasm. Many men also experience additional irregular contractions at the conclusion of the orgasm.\n\nEjaculation usually begins during the first or second contraction of orgasm. For most men, the first ejection of semen occurs during the second contraction, while the second is typically the largest expelling 40% or more of total semen discharge. After this peak, the magnitude of semen the penis emits diminishes as the contractions begin to lessen in intensity. The muscle contractions of the orgasm can continue after ejaculation with no additional semen discharge occurring. A small sample study of seven men showed an average of 7 spurts of semen followed by an average of 10 more contractions with no semen expelled. This study also found a high correlation between number of spurts of semen and total ejaculate volume, i.e., larger semen volumes resulted from additional pulses of semen rather than larger individual spurts.\n\nAlfred Kinsey measured the distance of ejaculation, in \"some hundreds\" of men. In three-quarters of men tested, ejaculate \"is propelled with so little force that the liquid is not carried more than a minute distance beyond the tip of the penis.\" In contrast to those test subjects, Kinsey noted \"In other males the semen may be propelled from a matter of some inches to a foot or two, or even as far as five or six and (rarely) eight feet\". Masters and Johnson report ejaculation distance to be no greater than . During the series of contractions that accompany ejaculation, semen is propelled from the urethra at , close to .\n\nMost men experience a refractory period immediately following an orgasm, during which time they are unable to achieve another erection, and a longer period again before they are capable of achieving another ejaculation. During this time a male feels a deep and often pleasurable sense of relaxation, usually felt in the groin and thighs. The duration of the refractory period varies considerably, even for a given individual. Age affects the recovery time, with younger men typically recovering faster than older men, though not universally so.\n\nWhereas some men may have refractory periods of 15 minutes or more, some men are able to experience sexual arousal immediately after ejaculation. A short recovery period may allow partners to continue sexual play relatively uninterrupted by ejaculation. Some men may experience their penis becoming hypersensitive to stimulation after ejaculation, which can make sexual stimulation unpleasant even while they may be sexually aroused.\n\nThere are men who are able to achieve multiple orgasms, with or without the typical sequence of ejaculation and refractory period. Some of those men report not noticing refractory periods, or are able to maintain erection by \"sustaining sexual activity with a full erection until they passed their refractory time for orgasm when they proceeded to have a second or third orgasm\".\n\nThe force and amount of semen that will be ejected during an ejaculation will vary widely between men and may contain between 0.1 and 10 milliliters (by way of comparison, note that a teaspoon is 5 ml and a tablespoon holds 15 ml). Adult semen volume is affected by the time that has passed since the previous ejaculation; larger semen volumes are seen with greater durations of abstinence. The duration of the stimulation leading up to the ejaculation can affect the volume. Abnormally low semen volume is known as hypospermia. One of the possible underlying causes of low volume or complete lack of semen is ejaculatory duct obstruction. It is normal for the amount of semen to diminish with age.\n\nThe number of sperm in an ejaculation also varies widely, depending on many factors, including the time since the last ejaculation, age, stress levels, and testosterone. Greater lengths of sexual stimulation immediately preceding ejaculation can result in higher concentrations of sperm. An unusually low sperm count, not the same as low semen volume, is known as oligospermia, and the absence of any sperm from the semen is termed azoospermia.\n\nThe first ejaculation in males often occurs about 12 months after the onset of puberty, generally through masturbation or nocturnal emission (wet dreams). This first semen volume is small. The typical ejaculation over the following three months produces less than 1 ml of semen. The semen produced during early puberty is also typically clear. After ejaculation this early semen remains jellylike and, unlike semen from mature males, fails to liquefy. A summary of semen development is shown in Table 1.\n\nMost first ejaculations (90 percent) lack sperm. Of the few early ejaculations that do contain sperm, the majority of sperm (97%) lack motion. The remaining sperm (3%) have abnormal motion.\n\nAs the male proceeds through puberty, the semen develops mature characteristics with increasing quantities of normal sperm. Semen produced 12 to 14 months after the first ejaculation liquefies after a short period of time. Within 24 months of the first ejaculation, the semen volume and the quantity and characteristics of the sperm match that of adult male semen.\nEjaculate is jellylike and fails to liquefy.\nMost samples liquefy. Some remain jellylike.\nEjaculate liquefies within an hour.\n\nThere is a central pattern generator in the spinal cord, made up of groups of spinal interneurons, that is involved in the rhythmic response of ejaculation. This is known as the \"spinal generator for ejaculation\".\n\nTo map the neuronal activation of the brain during the ejaculatory response, researchers have studied the expression of c-Fos, a proto-oncogene expressed in neurons in response to stimulation by hormones and neurotransmitters. Expression of c-Fos in the following areas has been observed:\n\nAlthough uncommon, some men can achieve ejaculations during masturbation without any manual stimulation. Such men usually do it by tensing and flexing their abdominal and buttocks muscles along with vigorous fantasising. Others may do it by relaxing the area around the penis, which may result in harder erections especially when hyperaroused.\n\nPerineum pressing results in an ejaculation which is purposefully held back by pressing on either the perineum or the urethra to force the seminal fluid to remain inside. In such a scenario, the seminal fluid stays inside the body and goes to the bladder. Some people do this to avoid making a mess by keeping all the semen inside. As a medical condition, it is called retrograde ejaculation.\n\nFor most men, no detrimental health effects have been determined from ejaculation itself or from frequent ejaculations, though sexual activity in general can have health or psychological consequences. A small fraction of men have a disease called postorgasmic illness syndrome (POIS), which causes severe muscle pain throughout the body and other symptoms immediately following ejaculation. The symptoms last for up to a week. Some doctors speculate that the frequency of POIS \"in the population may be greater than has been reported in the academic literature\", and that many POIS sufferers are undiagnosed.\n\nIt is not clear whether frequent ejaculation increases, reduces or has no effect on the risk of prostate cancer. Two large studies: \"Ejaculation Frequency and Subsequent Risk of Prostate Cancer\" and \"Sexual Factors and Prostate Cancer\" suggest that frequent ejaculation over a lifetime offers some protection against prostate cancer. The US study involving \"29,342 US men aged 46 to 81 years\" suggest that \"high ejaculation frequency was related to decreased risk of total prostate cancer\". An Australian study involving \"1,079 men with prostate cancer and 1,259 healthy men\" found that \"there is evidence that the more frequently men ejaculate between the ages of 20 and 50, the less likely they are to develop prostate cancer\":\n\nIn mammals and birds, multiple ejaculation is commonplace. During copulation, each side of a short-beaked echidna's penis is used alternately, with the other half being shut down between ejaculations.\n\nIn stallions, ejaculation is accompanied by a motion of the tail known as \"tail flagging\". When a male wolf ejaculates, his final pelvic thrust may be slightly prolonged. A male rhesus monkey usually ejaculates less than 15 seconds after sexual penetration. The first report and footage of spontaneous ejaculation in an aquatic mammal was recorded in a wild Indo-Pacific bottlenose dolphin near Mikura Island, Japan in 2012.\n\nIn horses, sheep, and cattle, ejaculation lasts for several seconds or fractions of a second, but in boars, it can last for 10–30 minutes or 5–10 minutes. Ejaculation in boars is stimulated when the spiral-shaped glans penis interlocks with the female's cervix. A mature boar can produce of semen during one ejaculation. In llamas and alpacas, ejaculation occurs continuously during copulation.\n\nThe semen of male dogs is ejaculated in three separate fractions. The third fraction is produced during the copulatory tie, and consists mainly of prostatic fluid.\n\n\n"}
{"id": "31987996", "url": "https://en.wikipedia.org/wiki?curid=31987996", "title": "First aid room", "text": "First aid room\n\nA first aid room or medical room is a room in an establishment (e.g. a school, factory, sports venue or airport) to which someone who is injured or taken ill on the premises can be taken for first aid and to await the arrival of professional emergency medical services.\n\nAccording to guidance issued in 1981 in the UK, a first aid room should be clearly signposted, easily accessible and contain:\n\nIn the United Kingdom a first aid room is required in some chemical factories, construction sites, and premises at a distance from medical services. In some cases the room may be used for other purposes when not required for first aid.\n\nA first aid station or first aid post is a manned first aid room - especially a temporary one at a large gathering of people or an emergency incident.\n\n"}
{"id": "52207602", "url": "https://en.wikipedia.org/wiki?curid=52207602", "title": "Global Outbreak Alert and Response Network", "text": "Global Outbreak Alert and Response Network\n\nThe Global Outbreak Alert and Response Network (GOARN) is a network composed of numerous technical and public health institutions, laboratories, NGOs, and other organizations that work to observe and respond to threatening epidemics. GOARN works closely with and under the World Health Organization (WHO), which is one of its most notable partners. Its goals are to: examine and study diseases, evaluate the risks that certain diseases pose, and improve international capability to deal with diseases.\n\nThe World Health Organization realized at the start of the 21st century that it did not have the resources required to adequately respond to and prevent epidemics around the world. Thus, a \"Framework for Global Outbreak and Response\" was created by the Department of Communicable Diseases Surveillance and Response, and Regional Offices. This framework was put then forth in a meeting in Geneva from April 26–28, 2000. In this meeting, which was attended by 121 representatives from 67 institutions, the decision was made to form GOARN to contribute resources, coordination, surveillance, and technical assistance towards combating diseases.\n\nIt was decided that GOARN would be directed by a steering committee made of 20 representatives of GOARN partners and an operational support team (OST) based in WHO. The steering committee oversees and plans the activities of GOARN, and the OST is composed of a minimum of 5–6 WHO staff. Task forces and groups were established to deal with specific issues. GOARN resources are primarily coordinated by the World Health Organization.\n\nThe WHO's guiding principles are to standardize \"epidemiological, laboratory, clinical management, research, communication, logistics, support, security, evacuation, and communication systems\" and coordinative international resources to support local efforts by GOARN partners to combat outbreaks. It also focuses on improving long term ability to provide technical assistance to affected areas.\n\nGOARN has grown to now have over 600 partners in the form of public health institutions, networks, laboratories, and United Nations and non-governmental organizations. Technical institutions, networks, and organizations that have the ability to improve GOARN's capabilities are eligible for partnership. Through its partners, GOARN is staffed by a variety of individuals who specialize in public health, such as \"doctors, nurses, infection control specialists, logisticians, laboratory specialists; communication, anthropology and social mobilization experts, emergency management and public health professionals among others.\"\n\nAs its biggest partner, WHO plays a large role in GOARN. Alongside coordinating its resources to combat outbreaks, WHO provides much of the staffing and assistance for GOARN, though as will be covered later, does not fund GOARN directly. Since the network is primarily led by the WHO, there is some uncertainty as to whether WHO should be considered a partner in GOARN or if the network should be considered a WHO initiative.\n\nAnother notable partner is the Center for Disease Control, which sends technical resources and staff to GOARN. The CDC also has a history of resource sharing and cooperation with WHO in order to combat disease.\n\nThe WHO does not directly fund GOARN. Instead, GOARN members and outside fundraising that is carried out each time there is a new incidence is used to support the GOARN response. The Nuclear Threat Initiative provides GOARN with US$500,000 as a revolving fund, meant to be used for quickly mobilizing response teams. This is known as the WHO-NTI Global Emergency Response Fund, and must be repaid after withdrawal. The GOARN is effective at operating from a fairly small budget.\n\nGOARN has responded to over 120 occurrences in 85 countries and has deployed over 2,300 experts into the field. Some examples of deployments are the SARS outbreak in Asia in 2003, Rift Valley fever, and the nipah virus around the Indian subcontinent.\n\nSince its creation, GOARN cooperated with various other organizations to control outbreaks and improve national capacity to respond to diseases. A brief history of GOARN's work against international diseases is as follows. In 2000–2003, GOARN primarily responded to outbreaks of diseases such as cholera, meningitis, and yellow fever in Africa. It supported field investigation and outbreak containment. In 2003, GOARN helped to deploy international teams and helped to coordinate the response against SARS. In 2004, the network was one of the first to deploy against H5N1 influenza. In April–July 2005, GOARN helped to control Marburg Hemorrhagic fever in Angola. It carried out some \"risk assessment and preparedness missions\" in 2006, along with some response to human bird flu. In 2008/2009, GOARN responded to cholera in Zimbabwe.\n\nGOARN played a role in containing the 2003 SARS outbreak in Asia. The network sent teams of experts in epidemiology, microbiology/virology, and infection control to Hanoi, Vietnam in March 14, 2003 and then Beijing China in March 25, 2003. GOARN assisted during this outbreak to not only study the outbreak and provide assistance, but also facilitate communication between the Department of Health (Hong Kong) and the WHO.\n\nEarliest signs of the outbreak in China were reported February 11–24, when multiple people were reported to contract the disease. WHO was notified February 28 and then directly notified GOARN March 13. The first members of a WHO/GOARN outbreak control team arrived in Hong Kong March 14, followed by another 5-person GOARN team 12 days later. This second team transitioned to Guangdong, where they investigated the earliest cases of SARS and conducted interviews with health staff. The outbreak was declared by WHO to be contained by July 5.\n\nWorldwide, GOARN carried out many of the operations for the initial response to SARS through the mobilization of field teams. Also through GOARN, the WHO developed many international networks to create tools and standards for containing the epidemic. These networks communicated data by teleconference and use of secure websites for sharing of information.\n\nBesides these networks and field teams, GOARN also assisted nations by directly providing assistance to affected areas and improving their capacity to respond to such threats in the future. GOARN's role in the outbreak was recognized by the World Health Assembly during the 56th Assembly in resolution WHA56.29.\n\nOn March 23, 2014, the first reports of Ebola in Guinea were reported by WHO's Regional Office in Africa. Five days later, the first GOARN team was sent to Guinea. This team found the situation to be quite severe and its findings were discussed in a press conference in Geneva April 8.\n\nIn the third week of April, WHO collaborated with GOARN to send a new medical team trained in infection prevention/control and intensive care to Guinea's principal hospital, Donka Hospital. Two weeks later, on May 5, WHO deployed experts, thirty three of whom were from GOARN, to West Africa to assist in the response to the outbreak. The outbreak was detected to have spread to Sierre Leone later in the month.\n\nOn June 23, a GOARN steering committee session sent a message to WHO requesting for WHO to lead the response more strongly because it was the only agency with the resources and staff to do so.\n\nOver the course of the outbreak, the network deployed 895 experts, including \"doctors, nurses, infection control specialists, logisticians, laboratory specialists; communication, anthropology and social mobilisation experts, emergency management and public health professionals.\" The network is still involved in the response to Ebola.\n\nIn Sierre Leone, GOARN has sent case management and laboratory experts from the International Centre for Diarrhoeal Disease Research, Bangladesh to help train the response capacity of health care and laboratory workers case management and diagnosis.\n\nIn Northern Iraq, the Syrian Civil War displaced many refugees into the Kurdistan. The refugee camps suffer from poor sanitation, which has led to cholera outbreaks in the region in 2007 and 2012. GOARN, as per the request of the Ministry of Health of Iraq for support and training in outbreak response, deployed a multidisciplinary team of six experts to the Northern Iraqian Dohuk and Erbil camps to assist with assessing the risk of cholera and other diseases as well as assisting MoH to prepare for response to the diseases.\n\nGOARN supported countries and various other outbreak control organizations to fight against the H1N1 outbreak in the US and Mexico. The GOARN alert and request for assistance started in Mexico April 24, 2009. Over the course of the outbreak, GOARN helped the Pan American Health Organization coordinate and exchange information with the CDC and Public Health Agency of Canada. It was provided with support and training from the Regional Office for the Western Pacific Response so that it could support regional offices in Manila and carry out field missions in Malaysia and Mongolia. The network carried out a joint training course with the Regional Office for the Eastern Mediterranean in Cairo.\n\nAll in all, GOARN carried out 188 missions to 27 countries to strengthen international coordination between these organizations and to improve international capacity to respond to threats. Its activities consisted of assessment of the situation, communication between partners, infection control, laboratory diagnostic, and transportation of specimens.\n\n\n"}
{"id": "38008898", "url": "https://en.wikipedia.org/wiki?curid=38008898", "title": "Global microbial identifier", "text": "Global microbial identifier\n\nThe genomic epidemiological database for global identification of microorganisms or global microbial identifier (GMI) is a platform for storing whole genome sequencing (WGS) data of microorganisms, for the identification of relevant genes and for the comparison of genomes to detect and track-and-trace infectious disease outbreaks and emerging pathogens. The database holds two types of information: 1) genomic information of microorganisms, linked to, 2) metadata of those microorganism such as epidemiological details. The database includes all genera of microorganisms: bacteria, viruses, parasites and fungi.\n\nFor genotyping of microorganisms for medical diagnosis, or other purposes, scientists may use a wide variety of DNA profiling techniques, such as PCR, PFGE and MLST. A complication of this broad variety of pre-WGS techniques is the difficulty to standardize between techniques, laboratories and microorganisms, which may be overcome using the complete DNA code of the genome generated by WGS techniques. For straight forward diagnostic identification the WGS information of a microbiological sample is fed into a global genomic database and compared using BLAST procedures to the genomes already present in the database. In addition, WGS data may be used to back calculate to the different pre-WGS genotyping methods, so previous collected valuable information is not lost. For the global microbial identifier the genomic information is coupled to a wide spectrum of metadata about the specific microbial clone and includes important clinical and epidemiological information such as the global finding place(s), treatment options and antimicrobial resistance, making it a general microbiological identification tool.This makes personalized treatment of microbial disease possible as well as real-time tracing systems for global surveillance of infectious diseases for food safety and serving human health.\n\nThe initiative for building the database arose in 2011 and when several preconditions were met : 1) WGS has become mature and serious alternative for other genotyping techniques, 2) the price of WGS has started falling dramatically and in some cases below the price of traditional identifications, 3) vast amounts of IT resources and a fast Internet have become available, and 4) there is the idea that via a cross sectoral and One Health approach infectious diseases may be better controlled.\n\nStarting the second millennium, many microbiological laboratories, as well as national health institutes, started genome sequencing projects for sequencing the infectious agents collections they had in their biobanks. Thereby generating private databases and sending model genomes to global nucleotide databases such as GenBank of the NCBI or the nucleotide database of the EMBL. This created a wealth of genomic information and independent databases for eukaryotic as well as prokaryotic genomes. The need to further integrate these databases and to harmonize data collection, and to link the genomic data to metadata for optimal prevention of infectious diseases, was generally recognized by the scientific community. In 2011, several infectious disease control centers and other organizations took the initiative of a series of international scientific- and policy-meetings, to develop a common platform and to better understand the potentials of an interactive microbiological genomic database. The first meeting was in Brussels, September 2011, followed by meetings in Washington (March 2012) and Copenhagen (February 2013). In addition to experts from around the globe, Intergovernmental Organizations have been included in the action, notably the World Health Organization (WHO) and the World Organization for Animal Health (OIE).\n\nA detailed roadmap for the development of the database was set up with the following general timeline:\n\nCurrent members:\n\n\nFormer members:\n\n\n\n"}
{"id": "33222575", "url": "https://en.wikipedia.org/wiki?curid=33222575", "title": "Harold Pollack", "text": "Harold Pollack\n\nHarold Pollack is an American professor at the University of Chicago who has been appointed to two Institute of Medicine committees. His research has focused on public health and health policy. At the University of Chicago, he has chaired the Center for Health Administration Studies. A special correspondent for the \"New Republic\" during 2009 and 2010, he writes frequently about public policy for a variety of national publications. Pollack is a frequent contributor to Healthinsurance.org, where he has conducted interviews with other prominent health policy bloggers, including Jonathan Cohn.\n\nPollack went to Princeton University for his undergraduate education. He received his masters and doctoral degrees in Public Policy from the Kennedy School of Government at Harvard University. After attending Harvard, he was a Robert Wood Johnson Foundation fellow at Yale University before teaching at the University of Michigan School of Public Health and then the University of Chicago.\n\n\n"}
{"id": "46810902", "url": "https://en.wikipedia.org/wiki?curid=46810902", "title": "Health Care for Women International", "text": "Health Care for Women International\n\nHealth Care for Women International is a monthly peer-reviewed healthcare journal covering health care and related topics that concern women around the globe.\n\nIt is the official journal for Women's Health Issues and it is published by Taylor & Francis. Its editor-in-chief is Eleanor Krassen Covan (University of North Carolina at Wilmington).\n\nThe journal was originally titled \"Issues in Health Care of Women\" (1978–1983).\n\nThe editor-in-chief from 1983 to 2001 was Phyllis Stern (University of Pennsylvania School of Nursing).\n\nAccording to the \"Journal Citation Reports\", the journal has a 2015 impact factor of 0.950, ranking it 20th out of 40 journals in the category \"Women's Studies\" and 115th out of 153 journals in the category \"Public, Environmental & Occupational Health\".\n\n\n"}
{"id": "49627903", "url": "https://en.wikipedia.org/wiki?curid=49627903", "title": "Health effects of coal ash", "text": "Health effects of coal ash\n\nCoal ash, also known as coal combustion residuals (CCRs), is the particulate residue that remains from burning coal. Depending on the chemical composition of the coal burned, this residue may contain toxic substances and pose a health risk to workers in coal-fired power plants. \n\nCoal ash is found in coal-fired power plants. Coal is burned in coal-fired plants to produce electricity. More specifically, the coal is pulverized and then burned to generate energy. The particles that remain after burning coal is called coal ash. The production of coal combustion produces many by-products of coal ash. Some of these by-products are boiler slag, flue gas desulfurization material, bottom ash, fly ash, scrubber residues, cenospheres and fluidized bed combustion ash. Depending on the coal that was burned, the chemical composition found in coal ash can vary. However, most coal ash will contain aluminum oxide (AlO), calcium oxide (CaO) and silicon dioxide (SiO). Regardless of the by-product produced, there are many toxic substances that are present in coal ash that can cause major health problems in humans. Some toxic constituents that are found in coal ash are arsenic, boron, cadmium, chromium, cobalt, copper, lead, mercury, molybdenum, selenium, thallium and uranium.\n\nIn the United States, approximately 44.6 percent of electricity is produced from over 450 coal-fired power plants. In 2012, approximately 110 million tons of coal ash was produced from the coal that was burned in the United States. However, more than half of the coal ash produced is dumped into surface impoundments (wet storage) or landfills (dry storage). Specifically, there are approximately 1,070 coal ash waste ponds and about 435 landfill sites located throughout the United States. The major problem of these disposal sites is the toxicity of coal ash escaping and causing harm to humans and the environment. When coal ash waste is not properly controlled, the toxic substances can affect drinking water, food and air.\n\nCoal ash contains many toxic substances that can negatively impact the human body. Employees working in coal-fired power plants or near coal ash waste sites are at major risk of inhaling coal ash dust. Coal ash dust is generally known as particulate matter (particle pollution) and the dust particles can harm the lungs when inhaled. Workers increase their risk of harmful side effects when they inhale the smallest coal ash dust particles. The smaller the coal ash dust particle, the deeper the particle will be inhaled into the lungs. As a result, the toxic particles can inflame the lungs causing severe damage to the body. Furthermore, coal ash dust can enter the body through the nose causing damage to the brain directly. However, regardless of particle entry, the toxicity from coal ash can cause harm to major body parts such as the brain, lungs, heart, liver, kidneys, stomach and intestines. Thus, individuals working near coal ash surface impoundments or landfills are at increased risk for many serious health problems.\n\nLead: The exposure of lead in coal ash can cause major damage to the nervous system. Lead exposure can lead to kidney disease, hearing impairment, high blood pressure, delays in development, swelling of the brain, hemoglobin damage, and male reproductive problems. Both low levels and high levels of lead exposure can cause harm to the human body.\n\nCadmium: When coal ash dust is inhaled, high levels of cadmium is absorbed into the body. More specifically, the lungs directly absorb cadmium into the bloodstream. When humans are exposed to cadmium over a long period of time, kidney disease and lung disease can occur. In addition, cadmium exposure can be associated with hypertension. Lastly, chronic exposure of cadmium can cause bone weakness which increases the risk of bone fractures and osteoporosis.\n\nChromium: The exposure of chromium (IV) in coal ash can cause lung cancer and asthma when inhaled. When coal ash waste pollutes drinking water, chromium (IV) can cause ulcers in the small intestine and stomach when ingested. Lastly, skin ulcers can also occur when the exposure chromium (IV) in coal ash comes in contact with the skin. \n\nArsenic: When high amounts of arsenic is inhaled or ingested through coal ash waste, diseases such as bladder cancer, skin cancer, kidney cancer and lung cancer can develop. Ultimately, exposure of arsenic over a long period of time can cause mortality. Furthermore, low levels of arsenic exposure can cause irregular heartbeats, nausea, diarrhea, vomiting, peripheral neuropathy and vision impairment. \n\nMercury: Chronic exposure of mercury from coal ash can cause harm to the nervous system. When mercury is inhaled or ingested various health effects can occur such as vision impairment, seizures, numbness, memory loss and sleeplessness.\n\nBoron: When coal ash dust is inhaled, the exposure of boron can cause discomfort in the throat, nose and eye. Moreover, when coal ash waste is ingested, boron exposure can be associated with kidney, liver, brain, and intestine impairment. \n\nMolybdenum: When molybdenum is inhaled from coal ash dust, discomfort of the nose, throat, skin and eye can occur. As a result, short-term molybdenum exposure can cause an increase of wheezing and coughing. Furthermore, chronic exposure of molybdenum can cause loss of appetite, tiredness, headaches and muscle soreness. \n\nThallium: The exposure of thallium in coal ash dust can cause peripheral neuropathy when inhaled. Furthermore, when coal ash is ingested, thallium exposure can cause diarrhea and vomiting. In addition, thallium exposure is also associated with heart, liver, lung and kidney complications. \n\nSilica: When silica is inhaled from coal ash dust, fetal lung disease or silicosis can develop. Furthermore, chronic exposure of silica can cause lung cancer. In addition, exposure to silica over a period of time can cause loss of appetite, poor oxygen circulation, breathing complications and fever.\n\nReuse of coal ash benefits the environment by reducing the production of greenhouse gas and reducing the need to use virgin materials. In addition, when coal ash is recycled, costs related to coal ash disposal sites are avoided. Thus, when coal ash is reused both the environment and economy are benefited. \n\nThere are two forms of coal ash recycling- “encapsulated” and “unencapsulated.\" When coal ash is bound to other materials it is encapsulated. For example, coal ash can be reused in making concrete, bricks and wallboards. On the other hand, unencapsulated use of coal ash is when the ash is not bound to other materials (loose particulate or sludge form). An example of unencapsulated coal ash is distributing the ash on icy roads in the winter. \n\nEven though reusing coal ash minimizes health effects in humans, health problems can still occur when coal ash is recycled. Specifically, workers drilling or cutting encapsulated coal ash increase their risk of inhaling coal ash dust. In addition, when unencapsulated coal ash is scattered on snowy streets in the winter, the loose ash can come in contact with ditches on the side of the road. As a result, the toxins from coal ash can “leach” into the water streams polluting above-ground waterways and eventually contaminating underground water supplies (drinking water). Therefore, both forms of recycled coal ash (encapsulated and unencapsulated) can cause serious health issues in humans.\n\nIn the United States, the only federal regulation regarding the disposal of coal ash is called “Disposal of Coal Combustion Residuals from Electric Utilities”, which was signed into law on December 19th, 2014. In addition, when coal ash is disposed into surface impoundments and landfills, coal ash is regulated as non-hazardous solid waste under the Resource Conservation and Recovery Act (RCRA). Thus, the requirements of the coal ash disposal law is regulated under subtitle D of the RCRA.\n\nIn order for this federal regulation to be effective, there are some major requirements that surface impoundments and landfill facilities must follow. This rule requires facilities to prevent and control coal ash dust from accumulating into the air. As a result, facilities must provide annual plans for coal ash dust control. Furthermore, there are location restrictions where new landfills and surface impoundments can be built. In addition, if regulations of coal ash dust control are not maintained, closure of the facility will occur under the federal law. The law also requires all coal ash waste facilities to create annual groundwater monitoring reports. Lastly, all coal ash waste surface impoundments and landfills must keep a written record of the federal regulations at the facility for five years. Ultimately, this recent federal regulation is trying to eliminate occupational health concerns and environmental health issues regarding coal ash toxicity.\n\n"}
{"id": "490201", "url": "https://en.wikipedia.org/wiki?curid=490201", "title": "Health insurance", "text": "Health insurance\n\nHealth insurance is insurance that covers the whole or a part of the risk of a person incurring medical expenses, spreading the risk over a large number of persons. By estimating the overall risk of health care and health system expenses over the risk pool, an insurer can develop a routine finance structure, such as a monthly premium or payroll tax, to provide the money to pay for the health care benefits specified in the insurance agreement. The benefit is administered by a central organization such as a government agency, private business, or not-for-profit entity.\n\nAccording to the Health Insurance Association of America, health insurance is defined as \"coverage that provides for the payments of benefits as a result of sickness or injury. It includes insurance for losses from accident, medical expense, disability, or accidental death and dismemberment\" (p. 225).\nA health insurance policy is:\n\nThe individual insured person's obligations may take several forms:\n\n\nPrescription drug plans are a form of insurance offered through some health insurance plans. In the U.S., the patient usually pays a copayment and the prescription drug insurance part or all of the balance for drugs covered in the formulary of the plan. Such plans are routinely part of national health insurance programs. For example, in the province of Quebec, Canada, prescription drug insurance is universally required as part of the public health insurance plan, but may be purchased and administered either through private or group plans, or through the public plan.\n\nSome, if not most, health care providers in the United States will agree to bill the insurance company if patients are willing to sign an agreement that they will be responsible for the amount that the insurance company doesn't pay. The insurance company pays out of network providers according to \"reasonable and customary\" charges, which may be less than the provider's usual fee. The provider may also have a separate contract with the insurer to accept what amounts to a discounted rate or capitation to the provider's standard charges. It generally costs the patient less to use an in-network provider.\n\nThe Commonwealth Fund, in its annual survey, \"Mirror, Mirror on the Wall\", compares the performance of the health care systems in Australia, New Zealand, the United Kingdom, Germany, Canada and the U.S. Its 2007 study found that, although the U.S. system is the most expensive, it consistently under-performs compared to the other countries. One difference between the U.S. and the other countries in the study is that the U.S. is the only country without universal health insurance coverage.\nThe Commonwealth Fund completed its thirteenth annual health policy survey in 2010. A study of the survey \"found significant differences in access, cost burdens, and problems with health insurance that are associated with insurance design\". Of the countries surveyed, the results indicated that people in the United States had more out-of-pocket expenses, more disputes with insurance companies than other countries, and more insurance payments denied; paperwork was also higher although Germany had similarly high levels of paperwork.\n\nThe Australian public health system is called Medicare, which provides free universal access to hospital treatment and subsidised out-of-hospital medical treatment. It is funded by a 2% tax levy on all taxpayers, an extra 1% levy on high income earners, as well as general revenue.\n\nThe private health system is funded by a number of private health insurance organizations. The largest of these is Medibank Private Limited, which was, until 2014, a government-owned entity, when it was privatized and listed on the Australian Stock Exchange.\n\nAustralian health funds can be either 'for profit' including Bupa and nib; 'mutual' including Australian Unity; or 'non-profit' including GMHBA, HCF and the HBF Health Fund (HBF). Some, such as Police Health, have membership restricted to particular groups, but the majority have open membership. Membership to most health funds is now also available through comparison websites like moneytime, Compare the Market, iSelect Ltd., Choosi, ComparingExpert and YouCompare. These comparison sites operate on a commission-basis by agreement with their participating health funds. The Private Health Insurance Ombudsman also operates a free website which allows consumers to search for and compare private health insurers' products, which includes information on price and level of cover.\n\nMost aspects of private health insurance in Australia are regulated by the \"Private Health Insurance Act 2007\". Complaints and reporting of the private health industry is carried out by an independent government agency, the Private Health Insurance Ombudsman. The ombudsman publishes an annual report that outlines the number and nature of complaints per health fund compared to their market share \n\nThe private health system in Australia operates on a \"community rating\" basis, whereby premiums do not vary solely because of a person's previous medical history, current state of health, or (generally speaking) their age (but see Lifetime Health Cover below). Balancing this are waiting periods, in particular for pre-existing conditions (usually referred to within the industry as PEA, which stands for \"pre-existing ailment\"). Funds are entitled to impose a waiting period of up to 12 months on benefits for any medical condition the signs and symptoms of which existed during the six months ending on the day the person first took out insurance. They are also entitled to impose a 12-month waiting period for benefits for treatment relating to an obstetric condition, and a 2-month waiting period for all other benefits when a person first takes out private insurance. Funds have the discretion to reduce or remove such waiting periods in individual cases. They are also free not to impose them to begin with, but this would place such a fund at risk of \"adverse selection\", attracting a disproportionate number of members from other funds, or from the pool of intending members who might otherwise have joined other funds. It would also attract people with existing medical conditions, who might not otherwise have taken out insurance at all because of the denial of benefits for 12 months due to the PEA Rule. The benefits paid out for these conditions would create pressure on premiums for all the fund's members, causing some to drop their membership, which would lead to further rises in premiums, and a vicious cycle of higher premiums-leaving members would ensue.\n\nThe Australian government has introduced a number of incentives to encourage adults to take out private hospital insurance. These include:\n\nAs per the Constitution of Canada, health care is mainly a provincial government responsibility in Canada (the main exceptions being federal government responsibility for services provided to aboriginal peoples covered by treaties, the Royal Canadian Mounted Police, the armed forces, and Members of Parliament). Consequently, each province administers its own health insurance program. The federal government influences health insurance by virtue of its fiscal powers – it transfers cash and tax points to the provinces to help cover the costs of the universal health insurance programs. Under the Canada Health Act, the federal government mandates and enforces the requirement that all people have free access to what are termed \"medically necessary services,\" defined primarily as care delivered by physicians or in hospitals, and the nursing component of long-term residential care. If provinces allow doctors or institutions to charge patients for medically necessary services, the federal government reduces its payments to the provinces by the amount of the prohibited charges. Collectively, the public provincial health insurance systems in Canada are frequently referred to as Medicare. This public insurance is tax-funded out of general government revenues, although British Columbia and Ontario levy a mandatory premium with flat rates for individuals and families to generate additional revenues - in essence, a surtax. Private health insurance is allowed, but in six provincial governments only for services that the public health plans do not cover (for example, semi-private or private rooms in hospitals and prescription drug plans). Four provinces allow insurance for services also mandated by the Canada Health Act, but in practice there is no market for it. All Canadians are free to use private insurance for elective medical services such as laser vision correction surgery, cosmetic surgery, and other non-basic medical procedures. Some 65% of Canadians have some form of supplementary private health insurance; many of them receive it through their employers. Private-sector services not paid for by the government account for nearly 30 percent of total health care spending.\n\nIn 2005, the Supreme Court of Canada ruled, in Chaoulli v. Quebec, that the province's prohibition on private insurance for health care already insured by the provincial plan violated the Quebec Charter of Rights and Freedoms, and in particular the sections dealing with the right to life and security, if there were unacceptably long wait times for treatment, as was alleged in this case. The ruling has not changed the overall pattern of health insurance across Canada, but has spurred on attempts to tackle the core issues of supply and demand and the impact of wait times. \n\nThe national system of health insurance was instituted in 1945, just after the end of the Second World War. It was a compromise between Gaullist and Communist representatives in the French parliament. The Conservative Gaullists were opposed to a state-run healthcare system, while the Communists were supportive of a complete nationalisation of health care along a British Beveridge model.\n\nThe resulting programme is profession-based: all people working are required to pay a portion of their income to a not-for-profit health insurance fund, which mutualises the risk of illness, and which reimburses medical expenses at varying rates. Children and spouses of insured people are eligible for benefits, as well. Each fund is free to manage its own budget, and used to reimburse medical expenses at the rate it saw fit, however following a number of reforms in recent years, the majority of funds provide the same level of reimbursement and benefits.\n\nThe government has two responsibilities in this system.\n\nToday, this system is more or less intact. All citizens and legal foreign residents of France are covered by one of these mandatory programs, which continue to be funded by worker participation. However, since 1945, a number of major changes have been introduced. Firstly, the different health care funds (there are five: General, Independent, Agricultural, Student, Public Servants) now all reimburse at the same rate. Secondly, since 2000, the government now provides health care to those who are not covered by a mandatory regime (those who have never worked and who are not students, meaning the very rich or the very poor). This regime, unlike the worker-financed ones, is financed via general taxation and reimburses at a higher rate than the profession-based system for those who cannot afford to make up the difference. Finally, to counter the rise in health care costs, the government has installed two plans, (in 2004 and 2006), which require insured people to declare a referring doctor in order to be fully reimbursed for specialist visits, and which installed a mandatory co-pay of €1 for a doctor visit, €0.50 for each box of medicine prescribed, and a fee of €16–18 per day for hospital stays and for expensive procedures.\n\nAn important element of the French insurance system is solidarity: the more ill a person becomes, the less the person pays. This means that for people with serious or chronic illnesses, the insurance system reimburses them 100% of expenses, and waives their co-pay charges.\n\nFinally, for fees that the mandatory system does not cover, there is a large range of private complementary insurance plans available. The market for these programs is very competitive, and often subsidised by the employer, which means that premiums are usually modest. 85% of French people benefit from complementary private health insurance.\n\nGermany has the world's oldest national social health insurance system, with origins dating back to Otto von Bismarck's Sickness Insurance Law of 1883.\n\nBeginning with 10% of blue-collar workers in 1885, mandatory insurance has expanded; in 2009, insurance was made mandatory on all citizens, with private health insurance for the self-employed or above an income threshold. As of 2016, 85% of the population is covered by the compulsory Statutory Health Insurance (SHI) (\"Gesetzliche Krankenversicherung\") (GKV), with the remainder covered by private insurance (\"Private Krankenversicherung (PKV).\" Germany's health care system was 77% government-funded and 23% privately funded as of 2004. They may also opt for private insurance, which is generally more expensive, but whose price may vary based on the individual's health status.\n\nReimbursement is on a fee-for-service basis, but the number of physicians allowed to accept Statutory Health Insurance in a given locale is regulated by the government and professional societies.\n\nCo payments were introduced in the 1980s in an attempt to prevent over utilization. The average length of hospital stay in Germany has decreased in recent years from 14 days to 9 days, still considerably longer than average stays in the United States (5 to 6 days). Part of the difference is that the chief consideration for hospital reimbursement is the number of hospital days as opposed to procedures or diagnosis. Drug costs have increased substantially, rising nearly 60% from 1991 through 2005. Despite attempts to contain costs, overall health care expenditures rose to 10.7% of GDP in 2005, comparable to other western European nations, but substantially less than that spent in the U.S. (nearly 16% of GDP).\n\nGermans are offered three kinds of social security insurance dealing with the physical status of a person and which are co-financed by employer and employee: health insurance, accident insurance, and long-term care insurance. Long-term care insurance (\"Gesetzliche Pflegeversicherung\") emerged in 1994, but it is not mandatory. Accident insurance (gesetzliche Unfallversicherung) is covered by the employer and basically covers all risks for commuting to work and at the workplace.\n\nThere are two major types of insurance programs available in Japan – Employees Health Insurance (健康保険 Kenkō-Hoken), and National Health Insurance (国民健康保険 Kokumin-Kenkō-Hoken). National Health insurance is designed for people who are not eligible to be members of any employment-based health insurance program. Although private health insurance is also available, all Japanese citizens, permanent residents, and non-Japanese with a visa lasting one year or longer are required to be enrolled in either National Health Insurance or Employees Health Insurance.\n\nIn 2006, a new system of health insurance came into force in the Netherlands. This new system avoids the two pitfalls of adverse selection and moral hazard associated with traditional forms of health insurance by using a combination of regulation and an insurance equalization pool. Moral hazard is avoided by mandating that insurance companies provide at least one policy which meets a government set minimum standard level of coverage, and all adult residents are obliged by law to purchase this coverage from an insurance company of their choice. All insurance companies receive funds from the equalization pool to help cover the cost of this government-mandated coverage. This pool is run by a regulator which collects salary-based contributions from employers, which make up about 50% of all health care funding, and funding from the government to cover people who cannot afford health care, which makes up an additional 5%.\n\nThe remaining 45% of health care funding comes from insurance premiums paid by the public, for which companies compete on price, though the variation between the various competing insurers is only about 5%. However, insurance companies are free to sell additional policies to provide coverage beyond the national minimum. These policies do not receive funding from the equalization pool, but cover additional treatments, such as dental procedures and physiotherapy, which are not paid for by the mandatory policy.\n\nFunding from the equalization pool is distributed to insurance companies for each person they insure under the required policy. However, high-risk individuals get more from the pool, and low-income persons and children under 18 have their insurance paid for entirely. Because of this, insurance companies no longer find insuring high risk individuals an unappealing proposition, avoiding the potential problem of adverse selection.\n\nInsurance companies are not allowed to have co-payments, caps, or deductibles, or to deny coverage to any person applying for a policy, or to charge anything other than their nationally set and published standard premiums. Therefore, every person buying insurance will pay the same price as everyone else buying the same policy, and every person will get at least the minimum level of coverage.\n\nSince 1974, New Zealand has had a system of universal no-fault health insurance for personal injuries through the Accident Compensation Corporation (ACC). The ACC scheme covers most of the costs of related to treatment of injuries acquired in New Zealand (including overseas visitors) regardless of how the injury occurred, and also covers lost income (at 80 percent of the employee's pre-injury income) and costs related to long-term rehabilitation, such as home and vehicle modifications for those seriously injured. Funding from the scheme comes from a combination of levies on employers' payroll (for work injuries), levies on an employee's taxable income (for non-work injuries to salary earners), levies on vehicle licensing fees and petrol (for motor vehicle accidents), and funds from the general taxation pool (for non-work injuries to children, senior citizens, unemployed people, overseas visitors, etc.)\n\nRwanda is one of a handful of low income countries that has implemented community-based health insurance schemes in order to reduce the financial barriers that prevent poor people from seeking and receiving needed health services. This scheme has helped reach 90% of the country's population with health care coverage.\n\nHealthcare in Switzerland is universal and is regulated by the Swiss Federal Law on Health Insurance. Health insurance is compulsory for all persons residing in Switzerland (within three months of taking up residence or being born in the country). It is therefore the same throughout the country and avoids double standards in healthcare. Insurers are required to offer this basic insurance to everyone, regardless of age or medical condition. They are not allowed to make a profit off this basic insurance, but can on supplemental plans.\n\nThe universal compulsory coverage provides for treatment in case of illness or accident and pregnancy. Health insurance covers the costs of medical treatment, medication and hospitalization of the insured. However, the insured person pays part of the costs up to a maximum, which can vary based on the individually chosen plan, premiums are then adjusted accordingly. The whole healthcare system is geared towards to the general goals of enhancing general public health and reducing costs while encouraging individual responsibility.\n\nThe Swiss healthcare system is a combination of public, subsidised private and totally private systems. Insurance premiums vary from insurance company to company, the excess level individually chosen (\"franchise\"), the place of residence of the insured person and the degree of supplementary benefit coverage chosen (complementary medicine, routine dental care, semi-private or private ward hospitalisation, etc.).\n\nThe insured person has full freedom of choice among the approximately 60 recognised healthcare providers competent to treat their condition (in their region) on the understanding that the costs are covered by the insurance up to the level of the official tariff. There is freedom of choice when selecting an insurance company to which one pays a premium, usually on a monthly basis. The insured person pays the insurance premium for the basic plan up to 8% of their personal income. If a premium is higher than this, the government gives the insured person a cash subsidy to pay for any additional premium.\n\nThe compulsory insurance can be supplemented by private \"complementary\" insurance policies that allow for coverage of some of the treatment categories not covered by the basic insurance or to improve the standard of room and service in case of hospitalisation. This can include complementary medicine, routine dental treatment and private ward hospitalisation, which are not covered by the compulsory insurance.\n\nAs far as the compulsory health insurance is concerned, the insurance companies cannot set any conditions relating to age, sex or state of health for coverage. Although the level of premium can vary from one company to another, they must be identical within the same company for all insured persons of the same age group and region, regardless of sex or state of health. This does not apply to complementary insurance, where premiums are risk-based.\n\nSwitzerland has an infant mortality rate of about 3.6 out of 1,000. The general life expectancy in 2012 was for men 80.5 years compared to 84.7 years for women. These are the world's best figures.\n\nThe UK's National Health Service (NHS) is a publicly funded healthcare system that provides coverage to everyone normally resident in the UK. It is not strictly an insurance system because (a) there are no premiums collected, (b) costs are not charged at the patient level and (c) costs are not pre-paid from a pool. However, it does achieve the main aim of insurance which is to spread financial risk arising from ill-health. The costs of running the NHS (est. £104 billion in 2007-8) are met directly from general taxation. The NHS provides the majority of health care in the UK, including primary care, in-patient care, long-term health care, ophthalmology, and dentistry.\n\nPrivate health care has continued parallel to the NHS, paid for largely by private insurance, but it is used by less than 8% of the population, and generally as a top-up to NHS services.\nThere are many treatments that the private sector does not provide. For example, health insurance on pregnancy is generally not covered or covered with restricting clauses. Typical exclusions for Bupa schemes (and many other insurers) include:\nageing, menopause and puberty; AIDS/HIV; allergies or allergic disorders; birth control, conception, sexual problems and sex changes; chronic conditions; complications from excluded or restricted conditions/ treatment; convalescence, rehabilitation and general nursing care ; cosmetic, reconstructive or weight loss treatment; deafness; dental/oral treatment (such as fillings, gum disease, jaw shrinkage, etc); dialysis; drugs and dressings for out-patient or take-home use† ; experimental drugs and treatment; eyesight; HRT and bone densitometry; learning difficulties, behavioural and developmental problems; overseas treatment and repatriation; physical aids and devices; pre-existing or special conditions; pregnancy and childbirth; screening and preventive treatment; sleep problems and disorders; speech disorders; temporary relief of symptoms.\n\nThere are a number of other companies in the United Kingdom which include, among others, ACE Limited, AXA, Aviva, Bupa, Groupama Healthcare, WPA and PruHealth. Similar exclusions apply, depending on the policy which is purchased.\n\nRecently (2009) the main representative body of British Medical physicians, the British Medical Association, adopted a policy statement expressing concerns about developments in the health insurance market in the UK. In its Annual Representative Meeting which had been agreed earlier by the Consultants Policy Group (i.e. Senior physicians)\nstating that the BMA was \"extremely concerned that the policies of some private healthcare insurance companies are preventing or restricting patients exercising choice about (i) the consultants who treat them; (ii) the hospital at which they are treated; (iii) making top up payments to cover any gap between the funding provided by their insurance company and the cost of their chosen private treatment.\" It went in to \"call on the BMA to publicise these concerns so that patients are fully informed when making choices about private healthcare insurance.\" The practice of insurance companies deciding which consultant a patient may see as opposed to GPs or patients is referred to as Open Referral. The NHS offers patients a choice of hospitals and consultants and does not charge for its services.\n\nThe private sector has been used to increase NHS capacity despite a large proportion of the British public opposing such involvement. According to the World Health Organization, government funding covered 86% of overall health care expenditures in the UK as of 2004, with private expenditures covering the remaining 14%.\n\nNearly one in three patients receiving NHS hospital treatment is privately insured and could have the cost paid for by their insurer. Some private schemes provide cash payments to patients who opt for NHS treatment, to deter use of private facilities. A report, by private health analysts Laing and Buisson, in November 2012, estimated that more than 250,000 operations were performed on patients with private medical insurance each year at a cost of £359 million. In addition, £609 million was spent on emergency medical or surgical treatment. Private medical insurance does not normally cover emergency treatment but subsequent recovery could be paid for if the patient were moved into a private patient unit.\n\nThe United States health care system relies heavily on private health insurance, which is the primary source of coverage for most Americans. about 61% of Americans had private health insurance according to the Centers for Disease Control and Prevention. The Agency for Healthcare Research and Quality (AHRQ) found that in 2011, private insurance was billed for 12.2 million U.S. inpatient hospital stays and incurred approximately $112.5 billion in aggregate inpatient hospital costs (29% of the total national aggregate costs). Public programs provide the primary source of coverage for most senior citizens and for low-income children and families who meet certain eligibility requirements. The primary public programs are Medicare, a federal social insurance program for seniors and certain disabled individuals; and Medicaid, funded jointly by the federal government and states but administered at the state level, which covers certain very low income children and their families. Together, Medicare and Medicaid accounted for approximately 63 percent of the national inpatient hospital costs in 2011. SCHIP is a federal-state partnership that serves certain children and families who do not qualify for Medicaid but who cannot afford private coverage. Other public programs include military health benefits provided through TRICARE and the Veterans Health Administration and benefits provided through the Indian Health Service. Some states have additional programs for low-income individuals.\n\nIn the late 1990s and early 2000s, health advocacy companies began to appear to help patients deal with the complexities of the healthcare system. The complexity of the healthcare system has resulted in a variety of problems for the American public. A study found that 62 percent of persons declaring bankruptcy in 2007 had unpaid medical expenses of $1000 or more, and in 92% of these cases the medical debts exceeded $5000. Nearly 80 percent who filed for bankruptcy had health insurance. The Medicare and Medicaid programs were estimated to soon account for 50 percent of all national health spending. These factors and many others fueled interest in an overhaul of the health care system in the United States. In 2010 President Obama signed into law the Patient Protection and Affordable Care Act. This Act includes an 'individual mandate' that every American must have medical insurance (or pay a fine). Health policy experts such as David Cutler and Jonathan Gruber, as well as the American medical insurance lobby group America's Health Insurance Plans, argued this provision was required in order to provide \"guaranteed issue\" and a \"community rating,\" which address unpopular features of America's health insurance system such as premium weightings, exclusions for pre-existing conditions, and the pre-screening of insurance applicants. During 26–28 March, the Supreme Court heard arguments regarding the validity of the Act. The Patient Protection and Affordable Care Act was determined to be constitutional on 28 June 2012. SCOTUS determined that Congress had the authority to apply the individual mandate within its taxing powers.\n\nIn the late 19th century, \"accident insurance\" began to be available, which operated much like modern disability insurance. This payment model continued until the start of the 20th century in some jurisdictions (like California), where all laws regulating health insurance actually referred to disability insurance.\n\nAccident insurance was first offered in the United States by the Franklin Health Assurance Company of Massachusetts. This firm, founded in 1850, offered insurance against injuries arising from railroad and steamboat accidents. Sixty organizations were offering accident insurance in the U.S. by 1866, but the industry consolidated rapidly soon thereafter. While there were earlier experiments, the origins of sickness coverage in the U.S. effectively date from 1890. The first employer-sponsored group disability policy was issued in 1911.\n\nBefore the development of medical expense insurance, patients were expected to pay health care costs out of their own pockets, under what is known as the fee-for-service business model. During the middle-to-late 20th century, traditional disability insurance evolved into modern health insurance programs. One major obstacle to this development was that early forms of comprehensive health insurance were enjoined by courts for violating the traditional ban on corporate practice of the professions by for-profit corporations. State legislatures had to intervene and expressly legalize health insurance as an exception to that traditional rule. Today, most comprehensive private health insurance programs cover the cost of routine, preventive, and emergency health care procedures, and most prescription drugs (but this is not always the case).\n\nHospital and medical expense policies were introduced during the first half of the 20th century. During the 1920s, individual hospitals began offering services to individuals on a pre-paid basis, eventually leading to the development of Blue Cross organizations. The predecessors of today's Health Maintenance Organizations (HMOs) originated beginning in 1929, through the 1930s and on during World War II.\n\nThe Employee Retirement Income Security Act of 1974 (ERISA) regulated the operation of a health benefit plan if an employer chooses to establish one, which is not required. The Consolidated Omnibus Budget Reconciliation Act of 1985 (COBRA) gives an ex-employee the right to continue coverage under an employer-sponsored group health benefit plan.\n\nThrough the 1990s, managed care insurance schemes including health maintenance organizations (HMO), preferred provider organizations, or point of service plans grew from about 25% US employees with employer-sponsored coverage to the vast majority. With managed care, insurers use various techniques to address costs and improve quality, including negotiation of prices (\"in-network\" providers), utilization management, and requirements for quality assurance such as being accredited by accreditation schemes such as the Joint Commission and the American Accreditation Healthcare Commission.\n\nEmployers and employees may have some choice in the details of plans, including health savings accounts, deductible, and coinsurance. As of 2015, a trend has emerged for employers to offer high-deductible plans, called consumer-driven healthcare plans which place more costs on employees; some employers will offer multiple plans to their employees.\n\n"}
{"id": "30551758", "url": "https://en.wikipedia.org/wiki?curid=30551758", "title": "Health network surveillance", "text": "Health network surveillance\n\nHealth network surveillance is a practice of health information management involving a combination of security, privacy and regulatory compliance with patient health information (PHI). Health network surveillance addresses the rapidly increasing trend of electronic health records (EHR) and its incompatibility with information security practices that ‘lock down’ access through methods such as: modern firewalls, intrusion detection and prevention devices, and anti-virus and end-point protections.\n\nIn contrast to restrictive security measures, health network surveillance runs in the background of networks through a combination of hardware and software devices that allow for real time monitoring that do not impede the day-to-day health care operations that make up healthcare systems and deliver essential services to patients and clients. Surveillance, in this context, means tracking the connections that are made between computers. These connections can be between computers within a health network or from a computer outside the health network. Effectively, this approach has the capacity to provide additional assurance that standard protective devices and approaches are working.\n\nGovernments at all levels have increased legislation and regulation of the ways health information should be handled, for both public and private health organizations in many countries. Major regulatory bodies and legislation in Canada and the United States include but are not limited to: the Health Insurance Portability and Accountability Act (HIPAA), the Personal Information and Electronic Documents Act (PIPEDA), the Personal Health Information Protection Act (PHIPA), International Organization for Standardization (ISO), PCI Security Standards Council, and Canada Health Infoway. Health network surveillance is able to address the increasingly complex legislation, regulations and policies imposed on health organizations in a way that restrictive security measures can only reduce the service levels of these organizations.\n\nHealth network surveillance also has a proactive impact by providing business intelligence and network monitoring that can improve a health organization’s efficiency and effectiveness through real time information that can support decision making about network architecture, business processes and resource allocation. Two approaches enable the development of health network surveillance tools. Commonly used flow measures based on a number of flow protocols available on the market use the capacity of routers and switches to provide data regarding the functioning of networks. The use of connection tracking works to record every connection between devices in a monitored network. There may be advantages in connection tracking techniques as they avoid sampling, produce more data in real time and put less load on the functioning of networks.\n"}
{"id": "435128", "url": "https://en.wikipedia.org/wiki?curid=435128", "title": "Home birth", "text": "Home birth\n\nA home birth is a birth that takes place in a residence rather than in a hospital or a birth centre. They may be attended by a midwife, or lay attendant with experience in managing home births. Home birth was, until the advent of modern medicine, the de facto method of delivery. Since the beginning of the 20th century, home birth rates have fallen in most developed countries, often to less than 1% of all births. Infant and mother mortality rates have also dropped drastically over the same time period and initially, assumptions were made that these findings were linked, as reflected in the UK Government's Peel Report (DoH 1970). Epidemiological work later identified there were no causal links, with improvements in mortality linked primarily to improvements in income and general health. Analysis which controlled for socioeconomic factors and for whether birth was planned and attended or unplanned and unattended identified that outcomes of planned home birth were positive. However, by this time, the view that birth should take place in hospital had become more normalised.\n\nWomen with access to high-quality medical care may choose home birth because they prefer the intimacy of a home and family-centered experience, or they desire to avoid a medically-centered experience typical of a hospital, among other reasons. Professionals attending home births can be obstetricians, certified or uncertified midwives, and doulas. In developing countries, where women may not be able to afford medical care or it may not be accessible to them, a home birth may be the only option available, and the woman may or may not be assisted by a professional attendant of any kind. In some cases, therefore, an unattended home birth may be unplanned (owing to lack of access to care or lack of easy access to a facility for birth) or chosen (often called [freebirth]). The latter tends to occur in women seeking to avoid repeat of previous traumatic birth experiences in facilities.\n\nMultiple studies have been performed concerning the safety of home births for both the child and the mother. Standard practices, licensing requirements and access to emergency hospital care differ between countries, and in countries like the US, between regions, making it difficult to compare studies across national borders. A 2014 US systematic review and meta-analysis of studies concluded that neonatal mortality rates were triple that of hospital births (Wax et al 2010), but the methodology has been subject to critical review on the basis of inclusion of studies with poor controls. A US-wide cohort study of planned home or hospital births from 2004-2009 concluded that there were no significant differences in perinatal outcomes but a lower intervention rate for mothers planning a home birth. A US registry-based study of all planned births in Oregon 2012-2013 (Snowden et al NEJM 2015) concluded that adverse outcome rates in low-risk women were low but births planned at home were associated with a higher rate of perinatal death and neonatal seizures when compared to planned birth in hospitals. However, this study relied on retrospective routine data and failed to control for differences in service provision, such as unlicensed midwives attending a proportion of home births and lack of integration of midwifery services in Oregon state). Conversely, a more recent study of integration of midwifery services in the US found that mortality rates and rates of preterm or low-birthweight births were lower in states with integrated midwifery services, in addition to lower rates of obstetric intervention and higher rates of physiological birth. A large-scale prospective cohort study of outcomes of births planned in home, obstetric hospital or midwifery unit settings in England found lower rates of intervention in all midwife-led settings and no differences in adverse neonatal outcome. However, although rates were very low overall, there was an increased risk of adverse perinatal outcomes for first births planned at home. Optimal outcomes in this study were found with freestanding midwifery units, which are units separate from a hospital with an obstetric unit, which are managed by midwives and intended primarily for the care of women with healthy pregnancies. \nVariations in study findings are likely to be associated with the inability to offer timely assistance to mothers with emergency procedures in case of complications during labour in settings with low levels of service integration or lack of universal access to care, as well as with widely varying licensing and training standards for birth attendants between different states and countries.\n\nHome births are either attended or unattended, planned or unplanned. Women are attended when they are assisted through labor and birth by a professional, usually a midwife, and rarely a general practitioner. Women who are unassisted or only attended by a lay person, perhaps their spouse, family, friend, or a non-professional birth attendant, are sometimes called freebirths. A \"planned\" home birth is a birth that occurs at home by intention. An \"unplanned\" home birth is one that occurs at home by necessity but not with intention. Reasons for unplanned home births include inability to travel to the hospital or birthing center due to conditions outside the control of the mother such as weather or road blockages or speed of birth progression.\n\nMany women choose home birth because delivering a baby in familiar surroundings is important to them. Others choose home birth because they dislike a hospital or birthing center environment, do not like a medically centered birthing experience, are concerned about exposing the infant to hospital-borne pathogens, or dislike the presence of strangers at the birth. Others prefer home birth because they feel it is more natural and less stressful. In a study published in the \"Journal of Midwifery and Women's Health\", women were asked, \"Why did you choose a home birth?\" The top five reasons given were safety, avoidance of unnecessary medical interventions common in hospital births, previous negative hospital experiences, more control, and a comfortable and familiar environment. One study found that women experience pain inherent in birth differently, and less negatively, in a home setting. In developing countries, where women may not be able to afford medical care or it may not be accessible to them, a home birth may be the only option available, and the woman may or may not be assisted by a professional attendant of any kind. Some women may not be able to have a safe birth at home, even with highly trained midwives. There are some medical conditions that can prevent a woman from qualifying for a home birth. These often include heart disease, renal disease, diabetes, preeclampsia, placenta previa, placenta abruption, antepartum hemorrhage after 20 weeks gestation, and active genital herpes. Prior cesarean deliveries can sometimes prevent a woman from qualifying for a home birth, though not always. It is important that a woman and her health care provider discuss the individual health risks prior to planning a home birth.\n\nHome birth was, until the advent of modern medicine, the \"de facto\" method of delivery. In many developed countries, home birth declined rapidly over the 20th century. In the United States there was a large shift towards hospital births beginning around 1900, when close to 100% of births were at home. Rates fell to 50% in 1938 and to fewer than 1% in 1955. Since 2000 a shift back towards home births has brought the rate up from 0.54% in 2004 to 0.72% in 2009. In the United Kingdom a similar but slower trend happened with approximately 80% of births occurring at home in the 1920s and only 1% in 1991. In Japan the change in birth location happened much later, but much faster: home birth was at 95% in 1950, but only 1.2% in 1975. Over a similar time period, maternal mortality during childbirth fell during 1900 to 1997 from 6–9 deaths per thousand to 0.077 deaths per thousand, while the infant mortality rate dropped between 1915 and 1997 from around 100 deaths per thousand births to 7.2 deaths per thousand.\n\nOne doctor described birth in a working class home in the 1920s:\n\nThis experience is contrasted with a 1920s hospital birth by Adolf Weber:\n\nMidwifery, the practice supporting a natural approach to birth, enjoyed a revival in the United States during the 1970s. Ina May Gaskin, for example, sometimes called \"the mother of authentic midwifery\" helped open The Farm Midwifery Center in Summertown, Tennessee in 1971, which is still in operation. However, although there was a steep increase in midwife-attended births between 1975 and 2002 (from less than 1.0% to 8.1%), most of these births occurred in the hospital. The US rate of out-of-hospital birth has remained steady at 1% of all births since 1989, with data from 2007 showing that 27.3% of the home births since 1989 took place in a free-standing birth center and 65.4% in a residence. Hence, the actual rate of home birth in the United States remained low (0.65%) over the twenty years prior to 2007.\n\nHome birth in the United Kingdom has also received some press since 2000. There was a movement, most notably in Wales, to increase home birth rates to 10% by 2007. Between 2005 and 2006, there was an increase of 16% of home birth rates in Wales, but by 2007 the total home birth rate was still 3% even in Wales (double the national rate). A 2001 report noted that there was a wide range of home birth rates in the UK, with some regions around 1% and others over 20%. In Australia, birth at home has fallen steadily over the years and was 0.3% as of 2008, ranging from nearly 1% in the Northern Territory to 0.1% in Queensland. In 2004, the New Zealand rate for births at home was nearly three times Australia's with a rate of 2.5% and increasing.\n\nIn the Netherlands, the trend has been somewhat different from other industrialized countries: while in 1965, two-thirds of Dutch births took place at home, that figure has dropped to about 20% in 2013, which is still more than in other industrialized countries. Less than 1% of South Korean infants are born at home.\n\nIn 2014, a comprehensive review in the \"Journal of Medical Ethics\" of 12 previously published studies encompassing 500,000 planned home births in low-risk women concluded that neonatal mortality rates for home births were triple those of hospital births. This finding echoes that of the American College of Obstetricians and Gynecologists. Due to a greater risk of perinatal death, the College advises women who are postterm (greater than 42 weeks gestation), carrying twins, or have a breech presentation not to attempt home birth. The \"Journal of Medical Ethics\" review additionally found that several studies concluded that home births had a higher risk of failing Apgar scores in newborns, as well as a delay in diagnosing hypoxia, acidosis and asphyxia. This contradicts a 2007 UK review study by the National Institute for Health and Clinical Excellence (NICE), a British governmental organization devoted to creating guidelines for coverage throughout the UK, which expressed concern for the lack of quality evidence in studies comparing the potential risks and benefits of home and hospital birthing environments in the UK. Their report noted that intrapartum-related perinatal mortality was low in all settings in the UK, but that in cases of unanticipated obstetric complications, the mortality rate was higher for home births due to the time needed to transfer the mother to an obstetric unit.\n\nA 2002 study of planned home births in the state of Washington found that home births had shorter labors than hospital births. In North America, a 2005 study found that about 12 percent of women intending to give birth at home needed to be transferred to the hospital for reasons such as a difficult labor or pain relief. A 2014 survey of American home births between 2004 and 2010 found the percent of women transferred to a hospital from a planned home birth after beginning labor to be 10.9%.\n\nBoth the \"Journal of Medical Ethics\" and NICE report noted that usage of caesarean sections were lower for women who give birth at home, and both noted a prior study that determined that women who had a planned home birth had greater satisfaction from the experience when compared with women who had a planned birth in a hospital.\n\nIn 2009 a study of 500,000 low-risk planned home and hospital births in the UK, where midwives have a strong licensing requirement, was reported in the \"British Journal of Obstetrics and Gynaecology\". The study concluded that for low-risk women there was no increase in perinatal mortality, provided that the midwives were well-trained and there was easy and quick access to hospitals. Further, the study noted there was evidence that \"low risk women with a planned home birth are less likely to experience referral to secondary care and subsequent obstetric interventions than those with a planned hospital birth.\" The study has been criticised on several grounds, including that some data might be missing and that the findings may not be representative of other populations.\n\nIn 2012, Oregon performed a study of all births in the state during the year as a part of discussing a bill regarding licensing requirements for midwives in the state. They found that the rate of intrapartum infant mortality was 0.6 deaths per thousand births for planned hospital births, and 4.8 deaths per thousand for planned home births. They further found that the death rate for planned home births attended by direct-entry midwives was 5.6 per thousand. The study noted that the statistics for Oregon were different for other areas, such as British Columbia, which had different licensing requirements. Oregon was noted by the Centers for Disease Control and Prevention as having the second-highest rate of home births in the nation in 2009, at 1.96% compared to the national average of 0.72%. A 2014 survey of nearly 17,000 voluntarily-reported home births in the United States between 2004 and 2010 found an intrapartum infant mortality rate of 1.30 per thousand; early neonatal and late neonatal mortality rates were a further 0.41 and 0.35 per thousand. The survey excluded congenital anomaly-related deaths, as well as births where the mother was transferred to a hospital prior to beginning labor.\n\nIn October 2013 the largest study of this kind was published in the American Journal of Obstetrics and Gynecology and included data on more than 13 million births in the United States, assessing deliveries by physicians and midwives in and out of the hospital from 2007 to 2010. The study indicated that babies born at home are roughly 10 times as likely to have an Apgar score of 0 after 5 minutes and almost four times as likely to have neonatal seizures or serious neurological dysfunction when compared to babies born in hospitals. The study findings showed that the risk of Apgar scores of 0 is even greater in first-born babies—14 times the risk of hospital births. The study results were confirmed by analyzing birth certificate files from the U.S. Centers for Disease Control and Prevention (CDC) and the National Center for Health Statistics. Given the study's findings, Dr. Amos Grunebaum, professor of clinical obstetrics and gynecology at Weill Cornell Medical College and lead author of the study, stated that the magnitude of risk associated with home delivery is so alarming that necessitates the need for the parents-to-be to know the risk factors. Another author, Dr. Frank Chervenak, added that the study underplayed the risks of home births, as the data used counted home births where the mother was transferred to a hospital during labor as a hospital birth.\n\nRandomized controlled trials are the \"gold standard\" of research methodology with respect to applying findings to populations; however, such a study design is not feasible or ethical for location of birth. The studies that do exist, therefore, are cohort studies conducted retrospectively by selecting hospital records and midwife records. by matched pairs (by pairing study participants based on their background characteristics), In February 2011 the American Congress of Obstetricians and Gynecologists identified several factors that make quality research on home birth difficult. These include \"lack of randomization; reliance on birth certificate data with inherent ascertainment problems; ascertainment of relying on voluntary submission of data or self-reporting; a limited ability to distinguish between planned and unplanned birth; variation in the skill, training, and certification of the birth attendant; and an inability to account for and accurately attribute adverse outcomes associated with transfers\". Quality studies, therefore, need to take steps in their design to mitigate these problems in order to produce meaningful results.\n\nThe data available on the safety of home birth in developed countries is often difficult to interpret due to issues such as differing home-birth standards between different countries, and difficult to compare with other studies because of varying definitions of perinatal mortality. Additionally, it is difficult to compare home and hospital births because only the risk profiles are different between the two groups, according to the CDC: people who choose to give birth at home are more likely to be healthy and at low risk for complications. There are also unquantifiable differences in home birth patients, such as maternal attitudes towards medical involvement in birth.\n\nWhile a woman in developed countries may choose to deliver her child at home, in a birthing center, or at hospital, health coverage and legal issues influence available options.\n\nIn April 2007, the Western Australian Government expanded coverage for birth at home across the State. Other state governments in Australia, including the Northern Territory, New South Wales and South Australia, also provide government funding for independent, private home birth.\n\nThe 2009 Federal Budget provided additional funds to Medicare to allow more midwives to work as private practitioners, allow midwives to prescribe medication under the Medicare Benefits Schedule, and assist them with medical indemnity insurance. However, this plan only covers hospital births. There are no current plans to extend Medicare and PBS funding to home birth services in Australia.\n\nAs of July 2012, all health professionals must show proof of liability insurance.\n\nIn March 2016 the Coroners Court of Victoria found against midwife Gaye Demanuel in the case of the death of Caroline Lovell.\n. \"Coroner White also called for a review of the regulation of midwives caring for women during home births, and for the government and health authorities to consider an offence banning unregistered health practitioners from taking money for attending home births.\"\n\nPublic health coverage of home birth services varies from province to province as does the availability of doctors and midwives providing home birth services. The Provinces of Ontario, British Columbia, Saskatchewan, Manitoba, Alberta, and Quebec currently cover home birth services.\n\nThere are few legal issues with a home birth in the UK. Woman can not be forced to go to a hospital. The support of the various Health Authorities of the National Health Service may vary, but in general the NHS will cover home births - the Parliamentary Under-Secretary of State for Health, Lord Hunt of King's Heath has stated \"I turn to the issue of home births. The noble Lord, Lord Mancroft, made some helpful remarks. As I understand it, although the NHS has a legal duty to provide a maternity service, there is not a similar legal duty to provide a home birth service to every woman who requests one. However, I certainly hope that when a woman wants a home birth, and it is clinically appropriate, the NHS will do all it can to support that woman in her choice of a home birth.\"\n\n27 states license or regulate in some manner direct-entry midwives, or certified professional midwife (CPM). In the other 23 states there are no licensing laws, and practicing midwives can be arrested for practicing medicine without a license. It is legal in all 50 states to hire a certified nurse midwife, or CNM, who are trained nurses, though most CNMs work in hospitals.\n\n"}
{"id": "34488928", "url": "https://en.wikipedia.org/wiki?curid=34488928", "title": "Human services", "text": "Human services\n\nHuman services is an interdisciplinary field with the objective of meeting human needs through an applied knowledge base, focusing on prevention as well as remediation of problems, and maintaining a commitment to improving the overall quality of life of service populations. The process involves the study of social technologies (practice methods, models, and theories), service technologies (programs, organizations, and systems), and scientific innovations that are designed to ameliorate problems and enhance the quality of life of individuals, families and communities to improve the delivery of service with better coordination, accessibility and accountability. The mission of human services is to promote a practice that involves simultaneously working at all levels of society (whole-person approach) in the process of promoting the autonomy of individuals or groups, making informal or formal human services systems more efficient and effective, and advocating for positive social change within society.\n\nHuman services practitioners strive to advance the autonomy of service users through civic engagement, education, health promotion and social change at all levels of society. Practitioners also engage in advocating so human systems remain accessible, integrated, efficient and effective.\n\nHuman services academic programs can be readily found in colleges and universities, which award degrees at the associate, baccalaureate, and graduate levels. Human services programs exist in countries all around the world.\n\nHuman services has its roots in charitable activities of religious and civic organizations that date back to the Colonial period. However, the academic discipline of human services did not start until the 1960s. At that time, a group of college academics started the new human services movement and began to promote the adoption of a new ideology about human service delivery and professionalism among traditional helping disciplines. The movement's major goal was to make service delivery more efficient, effective, and humane. The other goals dealt with the reeducation of traditional helping professionals (interprofessional education), to have a greater appreciation of the individual as a whole person (humanistic psychology) and to be accountable to the communities they serve (postmodernism). Furthermore, professionals would learn to take responsibility at all levels of government, use systems approaches to consider human problems, and be involved in progressive social change.\n\nTraditional academic programs such as education, nursing, social work, law and medicine were resistant to the new human services movement's ideology because it appeared to challenge their professional status. Changing the traditional concept of professionalism involved rethinking consumer control and the distribution of power. The new movement also called on human service professionals to work for social change. It was proposed that the reduction of the monopolistic control of professionals could result in democratization of knowledge and would lead to professionals advocating on behalf of clients and communities against professional establishments. The movement also hoped that human service delivery systems would become integrated, comprehensive, and more accessible, which would make them more humane for service users. Ultimately, the resistance from traditional helping professions served as the impetus for a group of educators in higher education to start the new academic discipline of human services.\n\nSome maintain that the human services discipline has a concrete identity as a profession that supplements and complements other traditional professions. Yet other professionals and scholars have not agreed upon an authoritative definition for human services.\n\nChenault and Burnford argued that human services programs must educate and train students at the graduate or postgraduate level if human services hoped to be considered a professional discipline. A progressive graduate human services program was established by Audrey Cohen (1932–1996), who was considered an innovative educator for her time. The Audrey Cohen College of Human Services, now called the Metropolitan College of New York, offered one of the first graduate programs in 1974. In the same time period, Springfield College in Massachusetts became a major force in preserving human services as an academic discipline. Currently, Springfield College is one of the oldest and largest human services program in the United States.\n\nManpower studies in the 1960s and 70s had shown that there would be a shortage of helping professionals in an array of service delivery areas. In turn, some educators proposed that the training of nonprofessionals (e.g., mental health technicians)could bridge this looming personnel shortage. One of the earliest educational initiatives to develop undergraduate curricula was undertaken by the Southern Regional Education Board (SREB), which was funded by the National Institute on Health. Professionals of the SREB Undergraduate Social Welfare Manpower Project helped colleges develop new social welfare programs, which later became known as human services. Some believed that community college human services programs were the most expedient way to train paraprofessionals for direct service jobs in areas such as mental health. Currently, a large percentage of human services programs are run at the community college level.\n\nThe development of community college human services programs was supported with government funding that was earmarked for the federal new careers initiatives. In turn, the federally funded New Careers Program was created to produce a nonprofessional career track for economically disadvantaged, underemployed, and unemployed adults as a strategy to eradicate poverty within society and to end a critical shortage of health-care personnel. Graduates from these programs successfully acquired employment as paraprofessionals, but there were limitations to their upward mobility within social service agencies because they lacked a graduate or professional degree.\n\nCurrently, there are academic programs in human services at the associate, baccalaureate, and graduate levels. There are approximately 600 human services programs throughout the United States. An online directory of human services programs lists many (but not all) of the programs state by state in conjunction with their accreditation status from the Council of Standards for Human Services Education (CSHSE).\n\nThe CSHSE offers accreditation for human services programs in higher education. The accreditation process is voluntary and labor-intensive; it is designed to assure the quality, consistency, and relevance of human service education through research-based standards and a peer-review process. According to the CSHSE's webpage there are only 43 accredited human services programs in the United States. The Standards for each level of degree accredited by CSHSE can be obtained by clicking on the following links:\n\n\nHuman services curricula are based on an interdisciplinary knowledge foundation that allows students to consider practical solutions from multiple disciplinary perspectives. Across the curriculum human services students are often taught to view human problems from a socioecological perspective (developed by Urie Bronfenbrenner) that involves viewing human strengths and problems as interconnected to a family unit, community, and society. This perspective is considered a “whole-person perspective”. Overall, undergraduate programs prepare students to be human services generalists while master’s programs prepare students to be human services administrators, and doctoral programs prepare students to be researcher-analysts and college-level educators. Research in this field focuses on an array of topics that deal with direct service issues, case management, organizational change, management of human service organizations, advocacy, community organizing, community development, social welfare policy, service integration, multiculturalism, integration of technology, poverty issues, social justice, development, and social change strategies.\n\nThe Center for Credentialing & Education (CCE) conceptualized the Human Services-Board Certified Practitioner (HS-BCP) credential with the assistance of the National Organization for Human Services (NOHS) and the Council for Standards in Human Service Education (CSHSE). The credential was created for human services practitioners seeking to advance their careers by acquiring independent verification of their practical knowledge and educational background.\n\nGraduates from human services programs can obtain a Human Services Board Certified Practitioner (HS-BCP) credential offered by the Center for Credentialing & Education (CCE). The HS-BCP certification ensures that human services practitioners offer quality services, are competent service providers, are committed to high standards, and adhere to the NOHS \"Ethical Standards of Human Service Professionals,\" as well as to help solidify the professional identity of human services practitioners. HS-BCPE Experience Requirements for the certification: HS-BCP applicants must meet post-graduation experience requirements to be eligible to take the examination. However, graduates of a CSHSE accredited degree program may sit for the HS-BCP exam without verifying their human services work experience. Otherwise experience requirements for candidates not from a CSHSE accredited program are as follows: Associate degree with post degree experience requires three years, including a minimum of 4,500 hours; Bachelor’s Degree with post degree experience requires two years, including a minimum of 3,000 hours; Master’s or Doctorate with post degree experience requires one year, including a minimum of 1,500 hours.\n\nThe HS-BCP exam is designed to verify a candidate’s human services knowledge. The exam was created as a collaborative effort of human services subject-matter experts and normed on a population of professionals in the field. The HS-BCP exam covers the following areas:\n\n\nCurrently, the three major employment roles played by human services graduates include providing direct service, performing administrative work, and working in the community. According to the Occupational Outlook Handbook, published by the US Department of Labor, the employment of human service assistants is anticipated to grow by 34% through 2016, which is faster than average for all occupations. There will also be excellent job opportunities for individuals with post-secondary degrees. But salaries remain low, which might reflect employers’ lack of understanding of the human services profession.\n\nThere are several different professional human services organizations for professionals, educators, and students to join across North America.\n\nThe National Organization for Human Services (NOHS) is a professional organization open to educators, professionals, and students interested in current issues in the field of human services. NOHS sponsors an annual conference in different parts of the United States. In addition, there are four independent human services regional organizations: (a) Mid-Atlantic Consortium for Human Services, (b) Midwest Organization for Human Services, (c) New England Organization for Human Service, and the (d) Northwest Human Services Association. All of the regional organizations are also open to educators, professionals, students and each regional organization has an annual conference in different locations throughout their region such as universities or institutions. \n\nHuman services special interest groups also exist within the American Society for Public Administration (ASPA) and the American Educational Research Association (AERA). The ASPA subsection is named the Section on Health and Human Services Administration and its purpose is to foster the development of knowledge, understanding and practice in the fields of health and human services administration and to foster professional growth and communication among academics and practitioners in these fields. Fields of health and human services administration share a common and unique focus on improving the quality of life through client-centered policies and service transactions.\n\nThe AERA special interest group is named the Education, Health and Human Service Linkages. Its purpose is to create a community of researchers and practitioners interested in developing knowledge about comprehensive school health, school linked services, and initiatives that support children and their families. This subgroup also focuses on interpersonal collaboration, integration of services, and interdisciplinary approaches. The group’s interests encompass interrelated policy, practice, and research that challenge efforts to create viable linkages among these three distinct areas.\n\nThe American Public Human Services Association (APHSA) is a nonprofit organization that pursues distinction in health and human services by working with policymakers, supporting state and local agencies, and working with partners to promote innovative, integrative and efficient solutions in health and human services policy and practice. APHSA has individual and student memberships.\n\nThe Canadian Institute for Human Services is an advocacy, education and action-research organization for the advancement of health equity, progressive education and social innovation. The institute collaborates with researchers, field practitioners, community organizations, socially conscious companies—along with various levels of government and educational institutions—to ensure that the Canadian health and human services sector remains accountable to the greater good of Canadian civil society rather than short-term professional, business or economic gains \n\n"}
{"id": "220255", "url": "https://en.wikipedia.org/wiki?curid=220255", "title": "Hypersexuality", "text": "Hypersexuality\n\nHypersexuality is a clinical diagnosis used by mental healthcare professionals for extremely frequent or suddenly increased libido. Nymphomania and satyriasis were terms previously used for the condition, in women and men respectively.\n\nHypersexuality may be a primary condition, or the symptom of another medical disease or condition, for example Klüver-Bucy syndrome or bipolar disorder. Hypersexuality may also present as a side effect of medication such as drugs used to treat Parkinson's disease, or through the administering of hormones such as testosterone and estrogen during hormone therapy.\nClinicians have yet to reach a consensus over how best to describe hypersexuality as a primary condition, or to determine the appropriateness of describing such behaviors and impulses as a separate pathology.\n\nHypersexual behaviours are viewed variously by clinicians and therapists as a type of obsessive-compulsive disorder (OCD) or \"OCD-spectrum disorder\", an addiction, or a disorder of impulsivity. A number of authors do not acknowledge such a pathology and instead assert that the condition merely reflects a cultural dislike of exceptional sexual behavior.\n\nConsistent with there not being any consensus over what causes hypersexuality, authors have used many different labels to refer to it, sometimes interchangeably, but often depending on which theory they favor or which specific behavior they were studying. Contemporary names include compulsive masturbation, compulsive sexual behavior, cybersex addiction, erotomania, \"excessive sexual drive\", hyperphilia, hypersexuality, hypersexual disorder, problematic hypersexuality, sexual addiction, sexual compulsivity, sexual dependency, sexual impulsivity, \"out of control sexual behavior\", and paraphilia-related disorder.\n\nThe \"Merriam-Webster Dictionary\" defines \"hypersexual\" as \"exhibiting unusual or excessive concern with or indulgence in sexual activity\". Sexologists have been using the term \"hypersexuality\" since the late 1800s, when Krafft-Ebing described several cases of extreme sexual behaviours in his seminal 1886 book, \"Psychopathia Sexualis.\" The author used the term \"hypersexuality\" to describe conditions that would now be termed premature ejaculation. Terms to describe males with the condition include \"donjuanist\", \"satyromaniac\",\"satyriac\" and \"satyriasist\", for women \"clitoromaniac\", \"nympho\" and \"nymphomaniac\", for teleiophilic heterosexual women \"andromaniac\", while \"hypersexualist\", \"sexaholic\", \"onanist\", \"hyperphiliac\" and \"erotomaniac\" are gender neutral terms.\n\nOther, mostly historical, names include Don Juanism, the Messalina complex, sexaholism hyperlibido and furor uterinus.\n\nThere is little consensus among experts as to the causes of hypersexuality. Some research suggests that some cases can be linked to biochemical or physiological changes that accompany dementia. Psychological needs also complicate the biological explanation, which identifies the temporal/frontal lobe of the brain as the area for regulating libido. Persons suffering from injuries to this part of the brain are at increased risk for aggressive behavior and other behavioral problems including personality changes and socially inappropriate sexual behavior such as hypersexuality. The same symptom can occur after unilateral temporal lobotomy. There are other biological factors that are associated with hypersexuality such as premenstrual changes, and the exposure to virilising hormones in childhood or in utero.\n\nIn research involving use of antiandrogens to reduce undesirable sexual behaviour such as hypersexuality, testosterone has been found to be necessary, but not sufficient, for sexual drive. Other proposed factors include a lack of physical closeness, and forgetfulness of the recent past.\n\nPathogenic overactivity of the dopaminergic mesolimbic pathway in the brain—forming either psychiatrically, during mania, or pharmacologically, as a side effect of dopamine agonists, specifically D-preferring agonists—is associated with various addictions and has been shown to result among some in overindulgent, sometimes hypersexual, behavior.\n\nThe American Association for Sex Addiction Therapy acknowledges biological factors as contributing causes of sex addiction. Other associated factors include psychological components (which affect mood and motivation as well as psychomotoric and cognitive functions), spiritual control, mood disorders, sexual trauma, and intimacy anorexia as causes or type of sex addiction.\n\nHypersexuality is known to present itself as a symptom in connection to a number of mental and neurological disorders. Some people with borderline personality disorder (sometimes referred to as BPD) can be markedly impulsive, seductive, and extremely sexual. Sexual promiscuity, sexual obsessions, and hypersexuality are very common symptoms for both men and women with BPD. On occasion for some there can be extreme forms of paraphilic drives and desires. \"Borderline\" patients, due in the opinion of some to the use of splitting, experience love and sexuality in unstable ways.\n\nPeople with bipolar disorder may often display tremendous swings in sex drive depending on their mood. As defined in the DSM-IV-TR, hypersexuality can be a symptom of hypomania or mania in bipolar disorder or schizoaffective disorder. Pick's disease causes damage to the temporal/frontal lobe of the brain; people with Pick's disease show a range of socially inappropriate behaviors.\n\nSeveral neurological conditions such as Alzheimer's disease, autism, various types of brain injury, Klüver–Bucy syndrome, Kleine–Levin syndrome, and many more neurodegenerative diseases can cause hypersexual behavior. Sexually inappropriate behavior has been shown to occur in 7-8% of Alzheimer's patients living at home, at a care facility or in a hospital setting. Hypersexuality has also been reported to result as a side-effect of some medications used to treat Parkinson's disease. Some street drugs, such as methamphetamine, may also contribute to hypersexual behavior.\n\nA positive link between the severity of dementia and occurrence of inappropriate behavior has also been found. Hypersexuality can be caused by dementia in a number of ways, including disinhibition due to organic disease, misreading of social cues, understimulation, the persistence of learned sexual behaviour after other behaviours have been lost, and the side-effects of the drugs used to treat dementia. Other possible causes of dementia-related hypersexuality include an inappropriately expressed psychological need for intimacy and forgetfulness of the recent past. As this illness progresses, increasing hypersexuality has been theorized to sometimes compensate for declining self-esteem and cognitive function.\n\nSymptoms of hypersexuality are also similar to those of sexual addiction in that they embody similar traits. These symptoms include the inability to be intimate (intimacy anorexia), depression and bipolar disorders. The resulting hypersexuality may have an impact in the person's social and occupational domains if the underlying symptoms have a large enough systemic influence.\n\n, a proposal to add \"Sexual Addiction\" to the Diagnostic and Statistical Manual of Mental Disorders (DSM) system has failed to get support of the American Psychiatric Association (APA). The DSM does include an entry called Sexual Disorder Not Otherwise Specified (Sexual Disorder NOS) to apply to, among other conditions, \"distress about a pattern of repeated sexual relationships involving a succession of lovers who are experienced by the individual only as things to be used\".\n\nThe International Statistical Classification of Diseases and Related Health Problems (ICD-10) of the World Health Organization (WHO), includes two relevant entries. One is \"Excessive Sexual Drive\" (coded F52.7), which is divided into satyriasis for males and nymphomania for females. The other is \"Excessive Masturbation\" or \"Onanism (excessive)\" (coded F98.8).\n\nSome authors have questioned whether it makes sense to discuss hypersexuality at all, arguing that labeling sexual urges \"extreme\" merely stigmatizes people who do not conform to the norms of their culture or peer group.\n\nThe ICD-11 has created a new condition classification, compulsive sexual behavior, to cover \" a persistent pattern of failure to control intense, repetitive sexual impulses or urges resulting in repetitive sexual behaviour\".\nHypersexuality may negatively impact an individual. The concept of hypersexuality as an addiction was started in the 1970s by former members of Alcoholics Anonymous who felt they experienced a similar lack of control and compulsivity with sexual behaviors as with alcohol. Multiple 12-step style self-help groups now exist for people who identify as sex addicts, including Sex Addicts Anonymous, Sexaholics Anonymous, Sex and Love Addicts Anonymous, and Sexual Compulsives Anonymous. Some hypersexuals may treat their condition with the usage of medication (such as Cyproterone acetate) or any foods considered to be anaphrodisiacs. Other hypersexuals may choose a route of consultation, such as psychotherapy, self-help groups or counselling.\n\n"}
{"id": "11827668", "url": "https://en.wikipedia.org/wiki?curid=11827668", "title": "International Health Regulations", "text": "International Health Regulations\n\nThe International Health Regulations (2005) are a legally binding instrument of international law that aim to a) assist countries to work together to save lives and livelihoods endangered by the international spread of diseases and other health risks, and b) avoid unnecessary interference with international trade and travel.\n\nThe purpose and scope of IHR 2005 are to prevent, protect against, control and provide a public health response to the international spread of disease in ways that are commensurate with and restricted to public health risks, and which avoid unnecessary interference with international traffic and trade. (Art. 2, IHR (2005))\n\nThe International Health Regulations originated with the International Sanitary Regulations adapted at the International Sanitary Conference in Paris in 1851. The cholera epidemics that hit Europe in 1830 and 1847 made apparent the need for international cooperation in public health.\n\nIn 1948, the World Health Organization Constitution came about. The Twenty-Second World Health Assembly (1969) adopted, revised and consolidated the International Sanitary Regulations, which were renamed (in the English language only) the International Health Regulations (1969). The Twenty-Sixth World Health Assembly in 1973 amended the IHR (1969) in relation to provisions on cholera. In view of the global eradication of smallpox, the Thirty-fourth World Health Assembly amended the IHR (1969) to exclude smallpox in the list of notifiable diseases.\n\nDuring the Forty-Eighth World Health Assembly in 1995, WHO and Member States agreed on the need to revise the IHR (1969). The revision of IHR (1969) came about because of its inherent limitations, most notably:\n\nThese challenges were placed against the backdrop of the increased travel and trade characteristic of the 20th century.\n\nThe IHR (2005) entered into force, generally, on 15 June 2007, and are currently binding on 196 States Parties, including all 194 Member States (countries) of WHO.\n\nThe implementation of IHR (2005) shall be:\n\nIn 2010 at The Meeting of the States Parties to the Convention on the Prohibition of the Development, Production and Stockpiling of Bacteriological (Biological) and Toxin Weapons and Their Destruction in Geneva the sanitary epidemiological reconnaissance was suggested as well-tested means for enhancing the monitoring of infections and parasitic agents, for practical implementation of the IHR (2005) with the aim was to prevent and minimize the consequences of natural outbreaks of dangerous infectious diseases as well as the threat of alleged use of biological weapons against BTWC States Parties. The significance of the sanitary epidemiological reconnaissance is pointed out in assessing the sanitary-epidemiological situation, organizing and conducting preventive activities, indicating and identifying pathogenic biological agents in the environmental sites, conducting laboratory analysis of biological materials, suppressing hotbeds of infectious diseases, providing advisory and practical assistance to local health authorities.\n\nA PHEIC is defined in the IHR (2005) as, “an extraordinary event which is determined to constitute a public health risk to other States through the international spread of disease and to potentially require a coordinated international response”. [1] This definition implies a situation that is:\nSince 2007, the WHO Director-General has declared public health emergencies of international concern in response to the following: [2]\n\n1) 2009 H1N1 influenza pandemic\n\n2) 2014 setbacks in polio global eradication efforts\n\n3) 2014 west Africa Ebola epidemic\n\n4) 2016 Zika virus outbreak\n\nCriticism of International Health Regulations\n\nRevisions to the International Health Regulations in 2005 were meant to lead to improved global health security and cooperation. However, the WHO’s perceived delayed and inadequate response to the west African Ebola Epidemic brought renewed international scrutiny to the International Health Regulations. Numerous published reports by high-level panels have assessed the International Health Regulations for inadequacies and proposed actions that can be taken to improve future responses to outbreaks. [3]\n\nOne publication reviewed seven of these major reports and identified areas of consensus on action. [4] The seven reports noted inadequate compliance with WHO’s International Health Regulations as a major contributor to the slow response to Ebola. They found three major obstacles that contributed to poor compliance: 1) countries’ core capacities, 2) unjustified trade and travel restrictions, and 3) inability to ensure that governments report outbreaks quickly.\n\n· Core Capacity\n\nThe IHR requires countries to assess their disease surveillance and response capacities and to identify if they can adequately meet their requirements. The seven Ebola reports universally agree that the country’s self-assessment capabilities are insufficient and that verification measures need to be improved upon. A significant problem is the inadequate level of core capacities in some countries, and the question of how to build upon them has been frequently raised. The reports make several recommendations to encourage governments to increase investment in outbreak identification and response programs. These include technical help from external sources conditional on mobilizing domestic resources, external financing for low income countries, pressure from the international community to increase investment, and considering outbreak preparedness as a factor in the International Monetary Fund’s country economic assessments, which influence governments’ budget priorities and access to capital markets.\n\n· Trade and Travel\n\nThe second issue frequently raised is ensuring that restrictions on trade and travel during outbreaks are justified. Because of increased attention and concern from the public and the media, many governments and private companies restricted trade and travel during the Ebola outbreak, though many of these measures were not necessary from a public health standpoint. These restrictions worsened financial repercussions and made the work of aid organizations sending support to affected regions more difficult.\n\nThere was broad consensus across the reports that bringing such restrictions to a minimum is critical to avoid further harm to countries experiencing outbreaks. Moreover, if governments assume that reporting will lead to inappropriate travel and trade restrictions, they may be hesitant to notify the international community about the outbreak. Potential solutions raised included the WHO and the UN more assertively “naming and shaming” countries and private companies that impose unjustified restrictions on WHO working with the World Trade Organization, International Civil Aviation Organization, and International Maritime Organization to develop standards and enforcement mechanisms for trade and travel restrictions.\n\n· Outbreak Reporting\n\nThe third compliance issue relates to countries’ obligation to rapidly report outbreaks. The reports recommend strengthening this obligation by WHO publicizing when countries delay reporting suspected outbreaks. In contrast, mechanisms ensuring that countries rapidly receive operational and financial support as soon as they do report were also recommended. A novel approach to encourage early notification is the World Bank’s Pandemic Emergency Financing Facility. This was created to provide rapid financing for the control of outbreaks and to protect countries from the devastating economic effects of outbreaks via an insurance program.\n\n"}
{"id": "43822871", "url": "https://en.wikipedia.org/wiki?curid=43822871", "title": "Israeli paradox", "text": "Israeli paradox\n\nThe Israeli paradox is a catchphrase, first used in 1996, to summarize the apparently paradoxical epidemiological observation that Israeli Jews have a relatively high incidence of coronary heart disease (CHD), despite having a diet relatively low in saturated fats, in apparent contradiction to the widely held belief that the high consumption of such fats is a risk factor for CHD. The paradox is that if the thesis linking saturated fats to CHD is valid, the Israelis ought to have a lower rate of CHD than comparable countries where the per capita consumption of such fats is higher.\n\nThe observation of Israel's paradoxically high rate of CHD is one of a number of paradoxical outcomes for which a literature now exists, regarding the thesis that a high consumption of saturated fats ought to lead to an increase in CHD incidence, and that a lower consumption ought to lead to the reverse outcome. The most famous of these paradoxes is known as the \"French paradox\": France enjoys a relatively low incidence of CHD despite a high per-capita consumption of saturated fat.\n\nThe Israeli paradox implies two important possibilities. The first is that the hypothesis linking saturated fats to CHD is not completely valid (or, at the extreme, is entirely invalid). The second possibility is that the link between saturated fats and CHD is valid, but that some additional factor in the typical Israeli diet, lifestyle or genes creates another CHD risk—presumably with the implication that if this factor can be identified, it can be isolated in the diet and / or lifestyle of other countries, thereby allowing both the Israelis, and others, to avoid that particular risk.\n\nIsraeli Jews eat a diet which is richer in linoleic acid (the most readily available plant-based form of omega-6 fatty acid, found in many vegetable oils) than any other population on the planet. Average per capita consumption is approximately 30 grams a day (11 kilograms annually), compared to 25 grams daily for the average American in 1985.\n\nSusan Allport summarizes the Israeli paradox in the following words: \nAllport notes that “Little butter is consumed in Israel, but large quantities of soybean, corn and safflower oil are….This translates, researchers estimate, to a linoleic acid intake of about 11 percent of calories and a ratio of linoleic to alpha linolenic acid [the most readily-available plant-based omega-3 fatty acid] in the Israeli diet of about 26:1.” She observes that mean serum cholesterol in Israel is quite low by the standards of developed countries: 210 milligrams/dl. Therefore, one distinction that can, without controversy, be attributed to the high levels of linoleic acid in the Israeli diet is the high percentage of linoleic acid in the adipose tissue of Israelis: 24% as compared to 16% in Americans and less than 10% in many northern Europeans.\n\nIn 1993, a 23-year follow-up study to the Israeli Ischemic Heart Disease Study of ten thousand public service employees (widely referred to as the “Israeli Civil Service Study”) found that there were only “weak associations of long-term coronary mortality with the dietary intake patterns of fatty acids.”\n\nThe term \"Israeli Paradox\" was first used by researchers Daniel Yam, Abraham Eliraz and Elliot Berry in a 1996 article in the \"Israel Journal of Medical Sciences\". The authors observed that Israelis consumed polyunsaturated fatty acids (primarily omega-6s rather than omega-3s) at a rate about 8% higher than in the United States and about 10-12% higher than most of Europe. They wrote, “Israeli Jews may be regarded as a population-based dietary experiment of the effect of a high omega-6 PUFA [polyunsaturated fatty acid] diet.”\n\nThe most readily-observable result of the relatively high ratio of omega-6 fats to saturated fats in the Israeli diet was the deposition of omega-6 fats in preference to saturated fats in the adipose tissue of Israelis. This had been observed as early as 1976, when an article in the \"Israel Journal of Medical Sciences\" noted that, counting polyunsaturated fats (which includes both omega-6 and omega-3 fatty acids) as a whole, the ratio of polyunsaturated fats to saturated fats in the adipose tissue of Ashkenazi Jews in Israel was 0.88:1, while for non-Ashkenazi Jews it was 1.13:1. This was, according to the authors, “the highest reported for any population on a free-choice diet.”\n\n"}
{"id": "3043713", "url": "https://en.wikipedia.org/wiki?curid=3043713", "title": "Leaf protein concentrate", "text": "Leaf protein concentrate\n\nLeaf protein concentrate (LPC) is a concentrated form of the proteins found in the leaves of plants. It has been examined as a human or animal food source, because it is potentially the cheapest, most abundant source of available protein. Although humans can derive some protein from the direct consumption of leaves as leaf vegetables, the human digestive system would not be able to deal with the enormous bulk of leaves needed to meet dietary protein requirements with leaf vegetables alone.\n\nLPC was first suggested as a human food in the 1960s, but it has not achieved much success, despite early promise. Norman Pirie (1971, 1975), the Copley Medal winner from the UK, reviewed and emphasized the importance of its benefits, which brought the subject forward. The increasing reliance on feedlot based animal rearing to satisfy human appetites for meat has increased demand for cheaper vegetable protein sources. This has recently led to renewed interest in LPC to reduce the use of human-edible vegetable protein sources in animal feed.\n\nLeaf protein is a good source of amino acids, with methionine being a limiting factor. Leaf proteins can also be rich in polyphenols. The challenges that have to be overcome before LPC from Lucerne and Cassava, two high density mono-culture crops, becomes a viable protein source for humans include the high fiber content and other antinutritional factors, such as phytate, cyanide, and tannins. Leaf for Life, a nonprofit organization dedicated to fighting malnutrition through encouraging increased consumption of vegetables and leaf crops, has extensive information on small scale LPC production using numerous plant species that both do not contain substantial concentrations of the anti-nutrients found in Cassava leaves or Lucerne and from which fiber can be removed through low tech processes.\n\nGenerally, LPC is produced by pulping leaves and pressing the juice out, heating the juice to coagulate the protein, and filtering the protein out and drying it.\n\n\n\n"}
{"id": "13139134", "url": "https://en.wikipedia.org/wiki?curid=13139134", "title": "List of counseling topics", "text": "List of counseling topics\n\nCounseling is the activity of the counselor, or a professional who counsels people, especially on personal problems and difficulties.\n\nThis is a list of counseling topics.\n\n"}
{"id": "23811783", "url": "https://en.wikipedia.org/wiki?curid=23811783", "title": "Mandated choice", "text": "Mandated choice\n\nMandated choice or mandatory choice is an approach to public policy questions in which people are required by law to state in advance whether or not they are willing to engage in a particular action. The approach contrasts with \"opt-in\" and \"opt-out\" (\"presumed consent\") models of policy formation. The approach has most frequently been applied to cadaveric organ donation, but has increasingly been considered for advance directives as well. One bioethicist, in advocating for a mandatory choice model for living wills, argues that \"while all Americans should have a right to decide how they want their lives to end, it does not follow that they should be able to avoid confronting such a choice.\"\n\nOne of the first considerations of mandated choice appeared in Great Britain's Gore Report, a 1989-1990 study funded by the British Department of Health. From 2011 all those applying for or renewing driving licences online in the UK are required to state whether they wished to donate their organs.\n\nThe American Medical Association endorsed a mandated choice model for organ donation in 1994.\n\nIt has been suggested that individuals could be compelled to choose as part of tax returns, driver's licence applications, and/or state benefits claims.\n\nA 1992 survey found that 90% of American college students favored a mandated choice model for organ donation, compared with only 60% who favored presumed consent. However, Texas implemented such a program, requiring drivers to make a choice on organ donation when obtaining licenses, and found that 80% of drivers declined to donate.\n\nChouhan and Draper propose a modified scheme of mandated choice, in which though all patients are given a choice whether to donate they are actively encouraged to do so.\n\n"}
{"id": "59030979", "url": "https://en.wikipedia.org/wiki?curid=59030979", "title": "Maria Dudycz", "text": "Maria Dudycz\n\nMaria Dudycz is an Australian health professional and advocate for people with disabilities.\n\nDudycz is most notable for her work developing the Victorian Disability Act 2006.\n\nThroughout her career, Dudycz's medical experience has seen her receive a number of Federal Government appointments. These include chairing the Advisory Panel on the marketing in Australia of Infant Formula from 2001 until 2005, and directing the National Health and Medical Research Council's Breast Cancer Centre from 2001 until 2003.\n\nDudycz also chaired the Australasian College of Legal Medicine from 1998 until 2003.\n\nIn 2018, Dudycz was added to the Victorian Honour Roll of Women.\n"}
{"id": "25678466", "url": "https://en.wikipedia.org/wiki?curid=25678466", "title": "Monthly nurse", "text": "Monthly nurse\n\nA monthly nurse is a woman who looks after a mother and her baby during the postpartum or postnatal period.\n\nHistorically, women were confined to their beds or their homes for extensive periods after giving birth; care was provided either by her female relatives (mother or mother-in-law), or, for those who could afford it, by the monthly nurse. These weeks were called confinement or lying-in, and ended with the re-introduction of the mother to the community in the Christian ceremony of the churching of women. The term \"monthly nurse\" was most common in 18th and 19th century England.\n\nThe job still exists, although it now might be described as \"postnatal doula\" or \"maternity nurse\" or \"newborn care specialist\" - all specialist sorts of nannies. A modern version of this rest period has evolved, to give maximum support to the new mother, especially if she is recovering from a difficult labour and delivery. It is especially popular in China and its diaspora, where postpartum confinement is known as \"sitting the month\".\n\nFrom long ago, the delivery of children and care of the mothers was a profession often handed down from mother to daughter, with the daughter spending many years as the pupil or apprentice. The Church supported that by a system of licensing, which required midwives to swear to certain rules relating to contraception, abortion and concealment of births and also to deliver the newborn infants for baptism or, in extreme cases, to perform the ceremony themselves.\n\nIn the mid-18th century the legal status of midwives was withdrawn and the responsibility for delivery was vested in the surgeon. The work of the nurse element had to be covered, as \"who was to look after the baby?\" Clearly, the first thought that would naturally occur to a mother was that the best person to look after her baby was a woman who had had one herself. Often, the task was allotted to motherly or grandmotherly hands and, from that requirement for postnatal care, the monthly nurse originated. \"The Nursing Record\" reported that \"there was little or no attempt at knowledge or instruction, and we know as a fact that ignorance, prejudice and neglect resulted in a goodly crop of errors, wrongs, and woes as regards the hapless infant\".\n\nThe term \"monthly nurse\" is one that is frequently used to describe the nurse who cares for lying-in cases, certainly because such a nurse frequently remains with the patient for four weeks. The term \"monthly\" is somewhat inaccurate, as there is no reason for the nurse's services to be dispensed with after ten days or retained for much longer, but it is entirely a matter of arrangement.\n\n\"The Nursing Record\" reported that \"nurses who attend the 'artisan' classes in their confinements as a rule pay a visit daily for ten days and then give up the case, as few working class mothers can afford to lie up for longer\".\n\nA monthly nurse could earn more than a midwife, as the monthly nurse was employed for periods between 10 days and often much longer and might attend several women on a part time basis. She often \"lived in\". The midwife's only duty was perceived as \"being trained to assist the parturient woman while nature does her own work and able to call upon a surgeon who could step in where nature fails and skill and science are required\". Many certified midwives transferred to the ranks of monthly nurses to benefit from an increased income.\n\nAlthough 'registration' was not available for women to act as midwives or monthly nurses a system of 'certification' was in being in the late 19th century and continued into the early 20th century. To qualify, a candidate monthly nurse would attend a course in a lying-in hospital for four or five weeks and a midwife for up to three months. The prospective midwives and monthly nurses, as a rule, paid their own charges in respect of hospital expenses and then entered practice on their own responsibility. In 1893, a Miss Gosling reported that \"although the certificated monthly nurse could be relied upon as being trustworthy and efficient, there were a number of women who attend lectures for a short time and through one cause or another fail to pass their examination and obtain a certificate nevertheless enter a 'Nurses Home' or open one for themselves\".\n\nAs might be expected rogue institutions issued certificates and diplomas “for a price”. Another that reporting on a lying in hospital and signed herself a ‘victim of the system’ said that she “witnessed the first phase of the system which turns out yearly hundreds of midwives and monthly nurses on an unsuspecting public. These would be nurses represented almost every grade of the lower classes and every degree of lack of education, and one woman, I remember could not write. Personally I found many to be dishonest, untruthful, indescribably dirty in their habits and persons, utterly unprincipled, shockingly coarse and deficient intelligence, and with not the faintest idea of discipline”’\n\nIn the late 19th century, reformers were calling not only for registration and recognition of the profession of midwife but also for the two functions of midwife and monthly nurse to be amalgamated: \"The work of midwives lies, for the most part, amongst the poor and the poor lying-in woman needs not only to be delivered, but to be visited for some ten days subsequent to her confinement\". The registration of midwives was opposed by members of the House of Lords and Parliament for many years, who argued that the delivery of infants was the responsibility of trained doctors and to allow women to do the job, even in straightforward cases, would take away doctors' income. It was not until the Midwives Act 1902, following 12 years of representation by women, that midwives were \"registered\", but it would still take several years for it to be accepted. The professional training and formal qualification of midwives, and eventually, the postnatal care offered by the National Health Service, saw the end of the monthly nurse.\n\n\n"}
{"id": "13649448", "url": "https://en.wikipedia.org/wiki?curid=13649448", "title": "Obesogen", "text": "Obesogen\n\nObesogens are foreign chemical compounds that disrupt normal development and balance of lipid metabolism, which in some cases, can lead to obesity. Obesogens may be functionally defined as chemicals that inappropriately alter lipid homeostasis and fat storage, change metabolic setpoints, disrupt energy balance or modify the regulation of appetite and satiety to promote fat accumulation and obesity.\n\nThere are many different proposed mechanisms through which obesogens can interfere with the body's adipose tissue biology. These mechanisms include alterations in the action of metabolic sensors; dysregulation of sex steroid synthesis, action or breakdown; changes in the central integration of energy balance including the regulation of appetite and satiety; and reprogramming of metabolic setpoints. Some of these proposed pathways include inappropriate modulation of nuclear receptor function which therefore allows the compounds to be classified as endocrine disrupting chemicals that act to mimic hormones in the body, altering the normal homeostasis maintained by the endocrine system.\n\nObesogens have been detected in the body both as a result of intentional administration of obesogenic chemicals in the form of pharmaceutical drugs such as diethylstilbestrol, selective serotonin reuptake inhibitor, and thiazolidinedione and as a result of unintentional exposure to environmental obesogens such as tributyltin, bisphenol A, diethylhexylphthalate, and perfluorooctanoate.\n\nThe term obesogen was coined in 2006 by Felix Grün and Bruce Blumberg of the University of California, Irvine. The topic of this proposed class of chemical compounds and how to counteract their effects is explored at length in the book \"The New American Diet\". Paula Baillie-Hamilton, a naturopath in the UK, hypothesized that obesogens make it difficult to lose weight in the \"Journal of Alternative and Complementary Medicine\" in 2002.\n\nThere are many ways in which obesogenic drugs and chemicals can disrupt the body's adipose tissue biology. The three main mechanisms of action include\n\nObesogenic drugs and chemicals have been shown to target transcription regulators found in gene networks that function to control intracellular lipid homeostasis and proliferation and differentiation on adipocytes. The major group of regulators that is targeted is a group of nuclear hormone receptors known as peroxisome proliferator activated receptors (PPARα, δ, and γ). These hormone receptors sense a variety of metabolic ligands including lipophilic hormones, dietary fatty acids and their metabolites, and, depending on the varying levels of these ligands, control transcription of genes involved in balancing the changes in lipid balance in the body. To become active and properly function as metabolic sensors and transcription regulators, the PPAR receptors must heterodimerize with another receptor known as the 9-cis retinoic acid receptor (RXR). The RXR receptor itself is the second major target of obesogens next to the PPAR receptors.\n\nThe PPARα receptor, when complexed with RXR and activated by the binding of a lipid, promotes peroxisome proliferation leading to increased fatty acid β-oxidation. Substances, such a xenobiotics that target and act as agonists of PPARα, typically act to reduce overall serum concentrations of lipids. In contrast, the PPARγ receptor, when complexed with RXR and activated by the binding of fatty acids or their derivatives, promotes lipid biosynthesis and storage of lipids is favored over fatty acid oxidation. In addition, activation promotes differentiation of preadipocytes and the conversion of mesenchymal progenitor cells to preadipocytes in adipose tissues. Substances that target and act as agonists of PPARγ/RXR complex typically act to increase overall serum concentrations of lipids.\n\nObesogens that target the PPARγ/RXR complex mimic the metabolic ligands and activate the receptor leading to upregulation of lipid accumulation which explains their obesogenic effects. However, in the case of obesogens that target the PPARα/RXR complex, which when stimulated reduces adipose mass and body weight, there are a few explanations as to how they promote obesity.\n\nThe ligand binding pockets of PPARs are very large and unspecified, allowing for different isoforms of the receptor (PPARα, δ, and γ) to be activated by the same agonist ligands or their metabolites. In addition, fatty acid oxidation stimulated by PPARα requires continuous stimulation while only a single activation event of PPARγ is required to permanently increase adipocyte differentiation and number. Therefore, it may be the case that metabolites of PPARα targeting obesogens are also activating PPARγ, providing the single activation event needed to potentially lead to a pro-adipogenic response.\n\nA second explanation points to specific PPARα targeters that have been shown to additionally cause abnormal transcriptional regulation of testicular steroidogenesis when introduced during fetal development. This abnormal regulation leads to a decreased level of androgen in the body which, itself, is obesogenic.\n\nFinally, if PPARα activation occurs during critical periods of development, the resulting decrease in lipid concentration in the developing fetus is recognized by the fetal brain as undernourishment. In this case, the developing brain makes what will become permanent changes to the body's metabolic control, leading to long-term upregulation of lipid storage and maintenance.\n\nSex steroids normally play a significant role in lipid balance in the body. Aided by other peptide hormones such as growth hormone, they act against the lipid accumulation mediated by insulin and cortisol by mobilizing lipid stores that are present. Exposure to obesogens often leads to a deficiency or change in the ratio between androgen and estrogen sex steroid levels, which modifies this method of lipid balance resulting in lowered growth hormone secretion, hypocortisolemia (low levels of circulating cortisol), and increased resistance to insulin effects.\n\nThis alteration in sex steroid levels due to obesogens can vary enormously according to both the sex of the exposed individual as well as the timing of the exposure. If the chemicals are introduced at critical windows of development, the vulnerability of an individual to their effects is much higher than if exposure occurs later in adulthood. It has been shown that obesogenic effects are apparent in female mice exposed to both phytoestrogens and DES during their neonatal periods of development, as they, though born with a lower birth weight, almost always developed obesity, high leptin levels, and altered glucose response pathways. Both phytoestrogen and DES exposed male mice did not develop obesity and, rather, showed decreased body weights with increased exposure confirming the role of gender differences in exposure response. Further studies have shown positive correlations for serum BPA levels with obese females in the human population, along with other xenoestrogen compounds suggesting the parallel roles that these effects may be having on humans.\n\nWhile hormone receptors tend to be the most obvious candidates for targets of obesogens, central mechanisms that balance and regulate the body's nutritional changes on a day-to-day basis as a whole cannot be overlooked. The HPA axis (hypothalamic-pituitary-adrenal) is involved in controlling appetite and energy homeostasis circuits which are mediated by a large number of monoaminoergic, peptidergic (use of hormones as neurotransmitters), and endocannabinoid signals that come from the digestive tract, adipose tissues, and from within the brain. It is these types of signals that provide a likely target for obesogens that have shown to have weight altering effects:\n\nNeurological disorders may enhance the susceptibility to develop the metabolic syndrome that includes obesity. Many neuropharmaceuticals used to alter behavioral pathways in patients with neurological disorders have shown to have metabolic altering side-effects leading to obesogenic phenotypes as well. These findings give evidence to conclude that an increase in lipid accumulation can result from the targeting of neurotransmitter receptors by foreign chemicals. (\"See also:\" section \"Central integration of energy balance\".)\n\nSeveral peptidergic hormone pathways controlling appetite and energy balance —such as those involving ghrelin, neuropeptide Y, and agouti-related peptide — are particularly sensitive to changes in nuclear receptor signaling pathways and can therefore be easily altered by the introduction of endocrine disruptors. Such an alteration can lead to induced feelings of hunger and decreased feelings of fullness causing an increase in food intake and inability to feel satisfied, both characteristic of obesity.\n\nSome xenoestrogens such as BPA, nonylphenol, and DEHP have all shown to act is this way, altering NPY expression and significantly shifting the feeding behaviors of exposed mice. In addition, organotins such as trimethyltin (TMT) triethyltin (TET), and tributyltin (TBT) compounds can exert their effects through similar pathways. TBT can locally disrupt aromatase regulation in the hypothalamus causing the responses of the HPA axis to hormones to become abnormal. TMT works in a similar but unique way, inducing NPY and NPY2 receptor expression initially which later is counteracted by neuronal degeneration in lesions causing decrease in signaling ability.\n\nWhile an increase in food intake is often the case after exposure, weight gain involves the body's maintenance of its metabolic setpoint as well. Given this information, it is particularly important to note that exposure during development and initial programming of these setpoints can be extremely significant throughout the remainder of life.\n\nA wide range of environmental organotins that mimic petidergic hormones in the HPA axis as mentioned before, additionally mimic lipid activators of the cannabinoid system and inhibit AMPK activity. Endocannaboid levels are high in those suffering from obesity due to hyperactivity of cannaboid signalling pathways. It is these high levels that have been found to be closely associated with increased fat stores linking the lipid activator mimics to the actual disease.\n\nRegions in the hypothalamus control the responses that establish an individuals metabolic setpoint and metabolic efficiency. These responses are adaptive in that they vary according to the individual's needs, always working to restore the metabolic setpoint through the increase or decrease of metabolic functions depending on varying energy needs. Since it is adapted, it is expected that it would be able to achieve equilibrium if the lipid balance was altered by hormones via the mechanisms mentioned above. However, since obesogenic phenotypes persist, it can be concluded that adaptive response components of the hypothalamus may be a target of obesogens as well.\n\nA person's body composition is very much predetermined before birth and changes rarely occur in adulthood. Adipocyte numbers increase during development and come to a plateau over time. After the plateau adipocytes become restricted to mostly hypertrophic growth and don't seem to change much in terms of cell number. This is demonstrated by the difficulty in altering somatotypes or more simply by the difficulty that goes along with trying to lose weight past a certain point.\n\nA particular study on PBDEs, a commonly used chemical in flame retardants, made its role in altering the functions of the thyroid hormone axis apparent. This finding leads to increased concern as neonatal thyroid status plays a large role in the integration of maternal environmental signals during development in the womb that is used for long-term body weight programming.\n\nObesogens detection in the body and resulting obesogenic effects can result as side effects from intentional administration of obesogenic chemicals in the form of pharmaceutical drugs. These pharmaceutical obesogens can show their effects through a variety of targets.\n\nThiazolidinediones (TZD), rosiglitazone, and pioglitazone are all used to treat diabetes. These drugs act as agonists of the PPAR-γ receptor leading to insulin sensitizing effects that can improve glycemic control and serum triglyceride levels. Despite the positive effects these chemicals can have in treating diabetes patients, administration also lead to unwanted PPAR-γ mediated side effects such as peripheral edema which can be followed by persistent weight gain if the drug is used over a long period of time. These side effects are particularly prominent in diabetes 2 patients, a disease that tends to result from an overabundance of adipose tissue.\n\nDES is a synthetic estrogen that was once prescribed to women to decrease the risk of miscarriage until it was found to be causing abnormalities in exposed offspring. This same chemical has been shown to cause weight gain in female mice when exposed during neonatal development. While exposure didn't lead to an abnormal birth weight, significant weight gain occurred much later in adulthood.\n\nSSRI (e.g. paroxetine), tricyclic antidepressants (e.g. amitriptyline), tetracyclic antidepressants (e.g. mirtazapine) and atypical antipsychotics (e.g. clozapine) are all neuropharmaceuticals that target neurotransmitter receptors that are involved with brain circuits that regulate behavior. Often the function of these receptors overlaps with metabolism regulation, such as that of the H1 receptor which when activated decreases AMPK activity. As a result, the administration of these drugs can have side effects including increased lipid accumulation that can result in obesity.\n\nThe mechanisms behind SSRI, tricyclic antidepressants, and atypical antipsychotics function allow them all to have potential roles in the alteration of metabolic setpoints. TZD, in particular has been linked to regulatory function in the HPT axis, however, no conclusive evidence has been determined thus far and further research is required to confirm these hypotheses.\n\nWhile obesogens can be introduced to the body intentionally via administration of obesogenic pharmaceuticals, exposure can also occur through chemical exposure to obesogens found in the environment such as organotins and xenobiotics.\n\nParticular members of the organotin class of persistent organic pollutants (POPs), namely tributyltin (TBT) and triphenyltin (TPT) are highly selective and act as very potent agonists of both the retinoid X receptors (RXR α,β, and γ) and PPARγ. This ability to target both receptors at the same time, is more effective than single receptor activation, as adopogenic signaling can be mediated through both components of the heterodimer complex. This highly effective activation mechanism can pose detrimental, long-term adipogenic effects especially if exposure occurs during development and early life.\n\nOrganotins (tin-based chemicals), used in marine anti-fouling paints, wood catalysts, plasticizers, slimicides, in industrial water systems, and fungicides on food have recently been linked to obesogenic properties when introduced in the body. Human exposure to these major environmental sources most commonly occurs through ingestion of contaminated seafood, agricultural products, and drinking water as well as from exposure to leaching from plastics.\n\nAlthough studies that have directly measured organotin levels in human tissue and blood are limited, it has been determined that vulnerability of a portion of the general population to organotin exposure at levels high enough to activate RXRs and PPARγ receptors is very probable. The high usage of organotins in both plastics and agricultural maintenance as well as the high affinity of the chemicals further confirms this conclusion.\n\nLiver samples from the late 1990s in Europe and Asia contained on average 6 and 84 ng/g wet wt respectively for total organotin levels, while later studies found levels of total organotins in US blood samples averaged around 21 ng/mL with TBT comprising around 8 ng/mL (~ 27 nM). Even more recent analyses of European blood samples found the predominant species to be TPT rather than TBT at 0.09 and 0.67 ng/mL (~0.5-2 nM). Only occasional trace amounts of TBT were found. These results indicate that organtin exposure to humans, while found to be present among many different populations, can vary in terms of type of organatin and level of exposure from region to region.\n\nOther common xenobiotics found in the environment have been shown to have PPAR activity, posing even further threats to dysregulated metabolic balance. BPA from polycarbonate plastics, phthalate plasticizers used to soften PVC plastics, and various perfluoroalkyll compounds (PFCs) that are widely used surfactants and surface repellents in consumer products are all potentially obesogenic when introduced in the body. Phthalates and PFCs in particular have been found to function as agonists for one or more of the PPARs Additionally, metabolites of DHEP such as MEHP also activate PPARγ leading to a proadipogenic response.\n\nAlthough research on endocrine disruptors or \"obesogens\" is still emerging, the public health implications so far have mainly surrounded obesity, diabetes, and cardiovascular disease.\n\nObesity has become a global epidemic, increasing for all population groups. From 1980 to 2008, the rates of obesity have doubled for adults and tripled for children. In the U.S. alone, it has been estimated that almost 100 million individuals in are obese Traditional thinking suggested that diet and exercise alone were the main contributors to obesity; however, current experimental evidence shows that obesogens might be part of the cause.\n\nThe chronic burden of obesity doesn't stop with weight gain. It can also lead to potentially debilitating chronic diseases such as diabetes, and certain environmental exposures, or obesogens, have been directly linked to Type II diabetes mellitus (T2DM). An estimated 25.8 million people, or 8.3% of the population in the US, have diabetes, and the crude prevalence of diagnosed diabetes increased by 176%, from 1980 through 2010. The disease and economic burden of diabetes, which is the seventh leading cause of death in the United States costing approximately $174 billion annually, is being addressed by organizations such as Healthy People 2020 which has diabetes listed as one of their 42 objectives. However, diabetes is a major obstacle to overcome especially when obesogens might be the uncontrolled, unsuspected cause.\n\nObesogens can be found everywhere, from water bottles to microwaveable popcorn, and from nonstick pans to shower curtains. People interact with them on a daily basis, both intentionally and unintentionally, at work, school and home. They are an unnecessary and mostly preventable potential hazard to health, which can have a large impact on how individuals gain and lose weight.\n\nBisphenol-A (BPA) is an industrial chemical and organic compound that has been used in the production of plastics and resins for over a half-century. It is used in products such as toys, medical devices, plastic food and beverage containers, shower curtains, dental sealants and compounds, and register receipts. BPA has been shown to seep into food sources from containers or into the body just by handling products made from it. Certain researchers suggest that BPA actually decreases the fat cell count in the body, but at the same time increasing the size of the ones remaining; therefore, no difference in weight is shown, and an individual is even likely to gain more.\n\nHigh-fructose corn syrup (HFCS) is found in many food products on grocery store shelves: by 2004, for example, it accounted for 40% of caloric sweeteners added to foods and beverages sold in the United States, and was the only caloric sweetener used in soft drinks. It is used as a food and drink sweetener and is an obesogen. Acting on insulin and leptin in the body, HFCS potentially increases appetite and fat production.\n\nNicotine is the chemical found in tobacco products and certain insecticides. As an obesogen, nicotine mostly acts on prenatal development after maternal smoking occurs. A strong association has been made between maternal smoking and childhood overweight/obesity, with nicotine as the single causal agent.\n\nArsenic is a metalloid (\"i.e.\", an element with some metallic properties) found in and on most naturally occurring substances on Earth. It can be found in the soil, ground water, air, and in small concentrations in food. Arsenic has many applications such as in the production of insecticides, herbicides, pesticides and electronic devices. The development of diabetes has been linked to arsenic exposure from drinking water and occupational contact.\n\nPesticides are substances used to prevent, destroy, repel or mitigate pests, and they have been used throughout all of recorded history. Some pesticides persist for short periods of time and some for long periods of time which are considered persistent organic pollutants (POPs). Several cross-sectional studies have shown pesticides as obesogens, linking them to obesity, diabetes and other morbidities.\n\nPharmaceutical drugs are also potentially obesogens. From 2005-2008, 11% of Americans aged 12 and over took antidepressant medications. Certain antidepressants, known as selectively serotonin reuptake inhibitors (SSRIs), are potentially adding to the almost 100 million obese individuals in the U.S. A key function of SSRI antidepressants is to regulate serotonin reuptake transporter (SERT) which can affect food intake and lipid accumulation leading to obesity.\n\nOrganotins such as tributyltin (TBT) and triphenyltin (TPT) are endocrine disruptors that have been shown to increase triglyceride storage in adipocytes. Although they have been widely used in the marine industry since the 1960s, other common sources of human exposure include contaminated seafood and shellfish, fungicides on crops and as antifungal agents used in wood treatments, industrial water systems and textiles. Organotins are also being used in the manufacture of PVC plastics and have been identified in drinking water and food supplies.\nPerfluorooctanoic acid (PFOA) is a surfactant used for reduction of friction, and it is also used in nonstick cookware. PFOA has been detected in the blood of more than 98% of the general US population. It is a potential endocrine disruptor. Animal studies have shown that prenatal exposure to PFOA is linked to obesity when reaching adulthood.\n\nMost of the environmental obesogens currently identified are either classified into the category of chemical mimics of metabolic hormones throughout the body or of neurotransmitters within the brain. Because they fall into these two categories, extensive opportunities for complex interactions and varied sites of action as well as multiple molecular targets are open for consideration. Changing dose ranges tend to result in varying phenotypes and timing of exposure, gender, and gender predisposition introduce even more levels of complexity in how these substances effect the human body.\n\nBecause the mechanisms behind the different effects of obesogens are so complex and not well understood, the extent to which they play in the current obesity epidemic may be greater than once thought. Epigenetic changes due to obesogen exposure must also be considered as a possibility, as they open up the potential for misregulated metabolic functions to be passed on from generation to generation. Epigenetic processes via hypermethylation of regulatory regions could lead to overexpression of different proteins, and therefore, amplification of acquired environmental effects. Research will be required in order to gain a better understanding of the mechanism of action these chemicals are involved in before the extent of the risk of exposure can be determined and methods of prevention and removal from the environment can be established.\n\n\n"}
{"id": "6427233", "url": "https://en.wikipedia.org/wiki?curid=6427233", "title": "Obstetric Flying Squad", "text": "Obstetric Flying Squad\n\nAn Obstetric Flying Squad is a form of medical retrieval team that is composed of an obstetrician, anaesthetist, midwife and other healthcare personnel who are on-call to attend to mothers with major obstetric complications occurring in the community.\n\nThe idea of having an Obstetric Flying Squad was suggested by Professor E Farquhar Murray who wrote that \"instead of rushing a shocked and collapsed patient to hospital for nursing and specialist aid, the specialist and nurse should be rushed to the patient\" (see Liang reference below). In the United Kingdom in the 1930s, the majority of births occurred in the home.\n\nThe first organised obstetric flying squad was started by H. J. Thomson in Bellshill, Lanarkshire, Scotland in 1933 to provide emergency back-up to general practitioners and midwives involved in home births.\n\nThe commonest major problems dealt with by the squad were:\n\nThe mothers were frequently severely shocked and the baby was also likely to be in a precarious position. Many lives were saved during the early years of operation of the service. Other countries with high home-birth rates subsequently copied the idea and set up similar services.\n\nIn most developed countries the majority of births now occur in hospitals and the ambulance services are well-developed, so the need for the service is extremely low. Less than 1% of births in the UK now occur in the home. The priority now is for rapid transport of distressed mothers to a hospital.\n\nThe original aim was to primarily manage obstetric complications on site, then secondarily transport the mother and child to hospital. This gradually evolved into primary on-site resuscitation than rapid transport to definitive care in a hospital. The concept of a hospital medical team going to retrieve patients continues in the concept of trauma retrieval teams.\n\nThe flying squad typically consisted of an obstetrician, anaesthetist, midwife, and a helper such as a hospital orderly. The service was activated following a phone call for assistance, and the team members would 'fly' (meaning 'travel quickly') to the scene.\nThe anaesthetic equipment initially was quite simple and consisted of a Schimmelbusch mask to administer chloroform. Later more sophisticated equipment was used. Of great importance was intravenous infusion equipment, intravenous fluids and later the provision of group O blood.\n\n"}
{"id": "21689547", "url": "https://en.wikipedia.org/wiki?curid=21689547", "title": "Occupational fatality", "text": "Occupational fatality\n\nAn occupational fatality is a death that occurs while a person is at work or performing work related tasks. Occupational fatalities are also commonly called “occupational deaths” or “work-related deaths/fatalities” and can occur in any industry or occupation.\n\nCommon causes of occupational fatalities include falls, machine-related incidents, motor vehicle accidents, electrocution, falling objects, homicides and suicides. Oftentimes, occupational fatalities can be prevented.\n\nIn the United States in 2007, 42% of occupational fatalities occurred during a transportation incident, 16% occurred after a worker came into contact with an object or equipment, 15% occurred as a result of a fall, 15% occurred as a result of assault or other violent acts in the workplace, 12% were the result of chemical or environmental exposures (9%) and 3% were the result of fires or explosions. \nMany factors contribute to a fatal incident at work. Lack of appropriate employee training and failure to provide and enforce the use of safety equipment are frequent contributors to occupational fatalities. In some cases, employees do receive safety training, but language barriers prevent the employee from fully understanding the safety procedures. Incidents can also be the result of insufficient supervision of inexperienced employees or employees who have taken on a responsibility for which they are not properly trained. Poor worksite organization, staffing and scheduling issues, unworkable policies and practices and workplace culture can all play a role in occupational fatalities. An incident leading to an occupational fatality is generally not the fault of a single person, but the result of a combination of many human and environmental factors.\nIn distinction to \"risk factors\", which may be thought to imply a causal link between such factors and fatality, statistics such as those from the U.S. Bureau of Labor Statistics on the demographics of deaths at work do not imply that age and gender are in themselves causative factors of fatality, but simply show that fatalities occur more frequently among certain groups.\nAlthough all workers are at risk for occupational fatalities, elderly workers age 65 and older are roughly three times more likely to die at work.\n\nA large majority of occupational deaths occur among men. In one U.S. study, 93% of deaths on the job involved men, with a death rate approximately 11 times higher than women. The industries with the highest death rates are mining, agriculture, forestry, fishing, and construction, all of which employ more men than women. Deaths of members in the military is currently above 90% men.\n\nOccupational fatalities are preventable. Prevention of occupational fatalities depends on the understanding that worker safety is not only the responsibility of the worker, but is the primary responsibility of the employer. Employers must train all employees in the appropriate safety procedures and maintain a safe working environment so that fatalities are less likely to occur. An occupational fatality is not just the fault of the deceased worker; instead, it is the combination of unsafe work environments, insufficient safety training, and negligible employee supervision that contribute fatal incidents. As a result, it is imperative that an employer address all the potential [risk] factors at the workplace and educate all employees in safe work practices and risk awareness.\n\nIn order to perform adequate risk assessment of injuries that occur in the workplace, health and safety professionals use resources such as the Haddon Matrix. This model assesses the risks leading up to, during, and after a death in order to prevent future incidents of a similar nature. Employers and employees can learn how to identify risk factors in their work environment in order to avoid incidents that may result in death.\n\nThe regulatory organization for occupational injury control and prevention is the Occupational Safety and Health Administration (OSHA). Formed in 1970 as an agency of the United States Department of Labor under the Occupational Safety and Health Act, OSHA exists to prevent occupational injuries and deaths by creating and enforcing standards in the workplace. OSHA standards address employee training programs, safety equipment, employer record keeping and proper maintenance of the work environment. Failure to comply with the OSHA standards can result in workplace inspections and legal action including citations and fines. In very severe cases of employer misconduct, OSHA can “red flag” an operation and send the employer to legal court.\n\nTo regulate the millions of workplaces in the United States, OSHA requires that all employers maintain a record of occupational injuries, illnesses and fatalities. Occupational fatalities must be reported to OSHA within eight hours of the incident. Failure to do so can result in legal action against the employer. Employers are responsible for staying current on OSHA standards and enforcing them in their own workplace. State OSHA organizations exist in twenty-eight states and are required to have the same or more rigorous standards than the federal OSHA standards. In these states, employers must abide by their state’s regulations. It is not the responsibility of the employee to stay current on the OSHA standards.\n\nIn addition to OSHA, the National Institute for Occupational Safety and Health (NIOSH) was formed under the Occupational Safety and Health Act as a federal research agency to formulate industry recommendations for health and safety. NIOSH is part of the Centers for Disease Control and Prevention (CDC) in the United States Department of Health and Human Services (DHHS). NIOSH analyzes workplace injury and illness data from all fifty states as well as provides support for state-based projects in occupational health and safety.\n\nUnder NIOSH, the Fatality Assessment and Control Evaluation (FACE) Program tracks and investigates occupational fatalities in order to provide recommendations for prevention. A voluntary program for individual states created in 1989, FACE is active in California, Iowa, Kentucky, Massachusetts, Michigan, New Jersey, New York, Oregon, and Washington. The primary responsibilities of the state FACE programs are to track occupational fatalities in their state, investigate select fatalities, and provide recommendations for prevention. As part of the prevention efforts, FACE programs also produce extensive prevention education materials that are disseminated to employees, employers, unions, and state organizations.\n\nNationally, the Census of Fatal Occupational Injuries (CFOI), within the U.S. Department of Labor, compiles national fatality statistics. CFOI is the key, comprehensive system in the surveillance of occupational fatalities in the United States.\n\nMany other non-governmental organizations also work to prevent occupational fatalities. Trade associations and unions play an active role in protecting workers and disseminating prevention information. The National Safety Council also works to prevent occupational fatalities as well as provide resources to employers and employees.\n\n"}
{"id": "55986440", "url": "https://en.wikipedia.org/wiki?curid=55986440", "title": "Paragraphia", "text": "Paragraphia\n\nParagraphia is a condition which results in the use of unintended letters or phonemes, words or syllables when writing. This is typically an acquired disorder derived from brain damage and it results in a diminished ability to effectively use written expression.\n\nParagraphias can be classified as function of the type of writing errors: literal paragraphias, graphemic paragraphias and morphemic paragraphias. \n"}
{"id": "33863280", "url": "https://en.wikipedia.org/wiki?curid=33863280", "title": "Pathogens and Global Health", "text": "Pathogens and Global Health\n\nPathogens and Global Health is a peer-reviewed medical journal published by Maney Publishing. It covers tropical diseases, including their microbiology, epidemiology and molecular biology, as well as medical entomology, HIV/AIDS, malaria, and tuberculosis. The editor-in-chief is Andrea Crisanti (Imperial College London).\n\nThe journal was established by Sir Ronald Ross in 1906 as \"Annals of Tropical Medicine and Parasitology\" to share the results of the Liverpool School of Tropical Medicine's research and field expeditions. In May 2011, the journal was purchased by Maney Publishing, obtaining its current title in 2012, reflecting a broader focus including the biology, immunology, genetics, treatment, and control of pathogens of medical relevance beyond a regional definition.\n\nThe journal is abstracted and indexed in:\n\nAccording to the \"Journal Citation Reports\", the journal has a 2014 impact factor of 1.656.\n\n"}
{"id": "50685288", "url": "https://en.wikipedia.org/wiki?curid=50685288", "title": "Person-centered care", "text": "Person-centered care\n\nIn health care, person-centred care is where the patients actively participate in their own medical treatment in close cooperation with the health professionals. Sometimes relatives are also included in creating the health plan The person-centred model of health care is used both for in and out patients, emergency care, palliative care as well as in rehabilitation.\n\nThe concept of person-centred care is clearly distinguished from a traditional treatment model which views the patient as a passive receiver of a medical intervention. Many health professionals are traditionally focused on the needs of the patients instead of their resources. Rather than the conventional way of making medical recommendations from health professionals to a patient, the person-centred care model allows for an inclusion of the patient and their relatives in making a joint design and mutual agreements of the medical plans and treatments. The person-centred care concept involves a partnership between the health care professionals, the patient and the relatives with a starting point in the medical history of the patient. The overall perspective of the life situation of the patient is considered to create objectives and strategies for both short and long term monitoring.\n\nThe concept of person-centred care has grown to be internationally associated with successful outcomes of the health care. Initially, the method was developed for senior patients and patients with intellectual disabilities but the ideas have later spread to other medical fields.\n\nWithin person-centred care, the patient is considered an independent and capable individual with their own abilities to make informed decisions. Autonomy and participation are always emphasised and respected. For the patient, the person-centred approach allows for involvement and extended possibilities to take responsibility for their own health and treatment.\n\nThere are four vantage points that constitute the foundation of person-centred care\n\nThe person-centred care is based on a holistic approach to health care that takes the whole person into account instead of a narrow perspective where the focus lies on the illness or the symptoms. The person-centred approach also includes the person's abilities, or resources, wishes, health and well-being as well as social and cultural factors.\n\nThere are three central themes to the peson-centred care work; the patient's narrative, the partnership and the documentation.\n\nThe health care team may consist of several different professionals with varying expertise from different health care units. The patient is a natural part of the team. Within the team, the patient and relatives have discussions with health professionals aiming to reach a mutual understanding on how to achieve a safe and accurate care for the unique patient.\n\nThe personal health or care plan is designed to capture the patient's narrative. A common understanding of strategies, goals and evaluation of the outcomes should be established. The documentation should clearly state the responsibilities of each member of the team, including the patient's own role and obligations. To fully live up to the person-centred care concept, patients should have full and easy access to all information and documentation about them. For reasons of security, accessibility and cost effectiveness, all documentation should be digital and include all medical records. The person's own notes, reports of health status and the overall health plan should also be carefully documented. The collected documentation is the foundation of the health care.\n\nResearch on person-centred care is carried out in many different universities. The University of Gothenburg Centre for Personcentred Care, GPCC, in Sweden, has been established since 2010. The centre conducts interdisciplinary research funded partly by the Swedish government's investments targeted towards care sciences.\n\nPatient-centered care is a concept which also emphasises the involvement of the patient and their families in the decision making of medical treatments. A main difference is that person-centred care describes the whole person in a wider context rather than the patient-centred approach which is based on the person's role as a patient.\n\nPeople-centred care is an umbrella term, articulated by WHO among others, which entails the right and duty for people to actively participate in decisions at all levels of the health care systems. People-centred care focuses both on the individual's right to health, access to health care and information, but also health literacy on a collective level.\n\nHealth activation is a condition where a health care consumer is equipped, educated, and motivated to be an effective manager of their own health and use of health care services. The concepts are very similar, although person-centred care places the emphasis on the healthcare provider, whereas the term health activation is used in reference to the attitude and behavior of the patient.\n\n"}
{"id": "1549377", "url": "https://en.wikipedia.org/wiki?curid=1549377", "title": "Physician–patient privilege", "text": "Physician–patient privilege\n\nPhysician–patient privilege is a legal concept, related to medical confidentiality, that protects communications between a patient and his or her doctor from being used against the patient in court. It is a part of the rules of evidence in many common law jurisdictions. Almost every jurisdiction that recognizes physician–patient privilege not to testify in court, either by statute or through case law, limits the privilege to knowledge acquired during the course of providing medical services. In some jurisdictions, conversations between a patient and physician may be privileged in both criminal and civil courts.\n\nThe privilege may cover the situation where a patient confesses to a psychiatrist that he or she committed a particular crime. It may also cover normal inquiries regarding matters such as injuries that may result in civil action. For example, any defendant that the patient may be suing at the time cannot ask the doctor if the patient ever expressed the belief that his or her condition had improved. However, the rule generally does not apply to confidences shared with physicians when they are not serving in the role of medical providers.\n\nThe rationale behind the rule is that a level of trust must exist between a physician and the patient so that the physician can properly treat the patient. If the patient were fearful of telling the truth to the physician because he or she believed the physician would report such behavior to the authorities, the treatment process could be rendered far more difficult, or the physician could make an incorrect diagnosis.\n\nFor example, a below-age of consent patient came to a doctor with a sexually transmitted disease. The doctor is usually required to obtain a list of the patient's sexual contacts to inform them that they need treatment. This is an important health concern. However, the patient may be reluctant to divulge the names of his/her older sexual partners, for fear that they will be charged with statutory rape. In some jurisdictions, the doctor cannot be forced to reveal the information revealed by his patient to anyone except to particular organizations, as specified by law, and they too are required to keep that information confidential. If, in the case, the police become aware of such information, they are not allowed to use it in court as proof of the sexual conduct, except as provided by express intent of the legislative body and formalized into law.\n\nThe law in Ontario, Canada, requires that physicians report patients who, in the opinion of the physician, may be unfit to drive for medical reasons as per Section 203 of the Highway Traffic Act (Ontario).\n\nThe law in New Hampshire places physician–patient communications on the same basis as attorney–client communications, except in cases where law enforcement officers seek blood or urine test samples and test results taken from a patient who is being investigated for driving while intoxicated.\n\nIn the United States, the Federal Rules of Evidence do not recognize doctor–patient privilege.\n\nAt the state level, the extent of the privilege varies depending on the law of the applicable jurisdiction. For example, in Texas there is only a limited physician–patient privilege in criminal proceedings, and the privilege is limited in civil cases as well.\n\nIn some Australian States a privilege exists for\" \"communication made by a person in confidence to another person ... in the course of a relationship in which the confidant was acting in a professional capacity\".\" This is often interpreted as being between a health professional and their patient.\n\nIn some jurisdictions in Australia privilege may also extend to lawyers, some Victims journalists (Shield Laws), and priests It may also be invoked in a Public interest, or Settlement Negotiations may also be privileged.\n\n"}
{"id": "9090858", "url": "https://en.wikipedia.org/wiki?curid=9090858", "title": "Portable appliance testing", "text": "Portable appliance testing\n\nPortable appliance testing (commonly known as \"PAT\", \"PAT inspection\" or (redundantly) as \"PAT testing\") is the name of a process in the United Kingdom, the Republic of Ireland, New Zealand and Australia by which electrical appliances are routinely checked for safety. The formal term for the process is \"in-service inspection & testing of electrical equipment\". Testing involves a visual inspection of the equipment and any flexible cables for good condition, and also where required, verification of earthing (grounding) continuity, and a test of the soundness of insulation between the current carrying parts, and any exposed metal that may be touched. The formal limits for pass/fail of these electrical tests vary somewhat depending on the category of equipment being tested.\n\nSimilar procedures exist in other countries, for example, testing of equipment according to \"DGUV Vorschrift 3\" in Germany.\n\nHealth and safety regulations require that electrical appliances are safe and maintained to prevent harm to workers. Many equipment manufacturers recommend testing at regular intervals to ensure continual safety; the interval between tests depending on both the type of appliance and the environment in which it is to be used. The European Low Voltage Directive governs the manufacture or importation of electrical appliances. Compliance to this has to be declared and indicated by the display of the CE mark on the product. The responsibility for this lies with the manufacturer or the importer and is policed by Trading Standards.\n\nTesting equipment has been specifically developed for PAT inspections, based on the testing equipment used by manufacturers to ensure compliance with the British Standard Code of Practice and European product standards relevant to that type of appliance. This in turn allows testing and the interpretation of results to be de-skilled to a large extent. The inspection of the appliances can largely be carried out in-house in many organisations. This can result in cost savings and more flexibility as to exactly when a PAT is carried out.\n\nPortable appliance testing is abbreviated as PAT. The correct term for the whole process is \"In-service Inspection & Testing of Electrical Equipment\" (as defined by IET/IEE and City and Guilds).\n\nIn Australia and New Zealand, the common name for PAT is simply 'Test and Tag'. The regulatory guidelines are set out in AS/NZS3760:2010.\n\nBritish law (the Electricity at Work Regulations 1989) requires that all electrical systems (including electrical appliances) are maintained (so far as is reasonably practicable) to prevent danger. Private houses are not covered by this legislation, although occupiers' liability requires householders not to deliberately expose occupants or visitors to unreasonable risks. The HSE and the local authority are responsible for the policing of this legislation.\n\nGuidance from the Institution of Engineering and Technology (IET, published under the IEE brand) and the Health and Safety Executive (HSE) recommends that a competent person must inspect the installation regularly in any public building or a place that people work. They suggest initial intervals for combined inspection and testing that range from three months (for construction equipment) to one year, and in many cases, longer periods for re-testing (certain types of appliance in schools, hotels, offices and shops).\n\nAlthough the Electricity at Work Regulations 1989 is an obligation on UK businesses, there is no obligation to undertake PAT inspection. In reality neither act nor their corresponding regulations and associated statutory instruments detail PAT inspection as an obligation, but rather impose a requirement of maintenance of safety and evidence of routine maintenance for all hand-held, portable and plug-in equipment.\n\nToday a great many private companies and other organizations do meet their legal obligations to protect their workers by an enforced PAT regime, but it is not the only route.\n\nRecent HSE publications have relaxed their tone somewhat to acknowledge this, and now point out that in many situations annual PAT is disproportionate to the risks and is often not required. In 2011, the HSE reviewed its approach to portable appliance maintenance in its own offices. Thinking about the type of equipment in use, and how it was used, the HSE looked back at the results from its annual testing of portable appliances across its estate over the last five years. Using the results of the previous tests, the HSE decided that further portable appliance tests are not needed within the foreseeable future or at all for certain types of portable equipment. Also, they decided to continue to monitor any faults reported as a result of user checks and visual inspections and review its maintenance system if evidence suggests that it needs revising. Electrical equipment will continue to be maintained by a series of user checks and visual inspections by staff that have had some training.\n\nAnnual portable appliance testing is not always necessary in low risk environments.\nYou do not need to be qualified as an electrician to carry out visual inspections.\nRegular user checks and visual inspections can be a good method of maintaining portable electric equipment.\nFor landlords maintaining legal requirements it is not compulsory for them to have all appliances tested, but they do need to show a \"duty of care\" and most letting agents recommend that a test certificate is obtained.\n\nThe earliest formal portable appliance testing and inspection of both electrical installations and high risk equipment was introduced in the UK in government estates. This was under the control of the Property Services Agency – prior to 1972 the Ministry of Public Building and Works. In some cases testing was conducted on a three-month (high-risk) and six-month (low-risk) cycle from the early 1960s onwards. Extensive record-keeping was made into log-books and generally the equipment used was an insulation resistance tester, simple hand tools and visual inspection. Evidence of testing was clearly visible to workers in the form of \"passed\", \"tested for electrical safety\" and \"do not use after...\" labels affixed to various parts of the electrical equipment used. This early testing and inspection was done under a planned maintenance scheme and pre-dated both the Health and Safety at Work Act 1974 and the Electricity at Work Act 1989.\n\nIn the UK there is no legal instrument that requires a sub-contractor to ensure that all tools and equipment are PAT inspected before bringing onto a site of work. Neither is there any legal instrument which obliges the site owner to ensure third-party equipment is PAT inspected either by themselves or the equipment owner.\n\nThe internal policies of many UK businesses and educational establishments make mistaken reference to PAT inspection being a legal requirement under the Electricity at Work Regulations, which is false. Having such a policy is legitimate for internal reasons, but it is not underwritten by law, it is only their interpretation. Therefore, it is not a legal requirement to have a PAT inspection sticker or certificate, the obligation is that equipment must be safe.\n\nThe HSE recommend policies use phrases such as \"Equipment that is brought onto site for an event must be in a safe condition\" and refrain from overzealous statements such as \"must be PAT inspected\" which can be restrictive without improving safety. Overall it is safer if a competent person makes a visual inspection than if a layman merely observes the presence of a sticker.\n\nThis can be done by hiring an external company to test all the electrical products in a business (someone who has had some PAT training, either by an official qualification or by attending a health and safety course offered by some electrical health and safety companies) or it can be done in-house by a competent person. In a low-risk environment most dangerous defects can be found simply by checking the appliances for obvious signs of damage such as frayed cables.\n\nAdvising the user of potential danger signs can result in problems being picked up before they can result in any danger. For example, if the power cable is frayed or the plug is cracked, users need to be advised not to use the appliance and report the fault to a supervisor. This information can be put across, say by the use of a poster or in a memo. User checks are always carried out before operation, and the results are generally not recorded, unless a defect is identified.\n\nThis is a process of simply inspecting the appliance, the cable and the plug for any obvious signs of damage. According to the HSE, this process can find more than 90% of faults.\n\nAt periodic intervals, the portable appliances are tested to measure that the degree of protection to ensure that it is adequate. At these intervals, a formal visual inspection is carried out and then followed by PAT testing. Note the inside of the plug should be checked unless it is moulded or there is an unbroken seal covering the screws (bad internal wiring or an unsuitable fuse would cause the item to be classed as dangerous).\n\nThe tests an appliance is required to undergo will depend on the type of appliance, its electrical Class and subject to a risk assessment by the technician. e.g. it may not be safe to perform a leakage current test which powers up the appliance, such as a grinder, if it can not be secured to a bench; an insulation resistance test may be a safe option.\n\nThis test shows the resistance offered by the earthing rods with the connection leads. Various testing instruments are available for earthing resistance tests. The earthing resistance should be less than 1Ω.\n\nThe equipment shall have a measured resistance of the protective earth circuit, or the earthing conductor of an extension cord or appliance cord set, which does not exceed 1Ω.\n\nTesting is performed using an ohmmeter or PAT tester;\n\nThe choice of which of the tests to use is at the operator's discretion as there is merit in each test for given situations. Later model testers that are battery powered are limited to doing the \"screen test\". Older mains powered units can do all tests. Even \"type testing\" may only be testing at a fraction of the operational power of the unit.The power dissipated in the earth is only 300 watts compared to operational power which may be in excess of 2000 watts.\n\nA leakage current test performed at rated voltage with values not exceeding 5mA for Class I appliances or 1mA for Class II appliances.\n\nAlternatively, measure insulation resistance values are not less than 1MΩ for Class I and Class II appliances at 500 V d.c. or alternatively, to avoid the equipment apparently failing the test because the metal oxide varistors (MOVs), or electro-magnetic interference (EMI) suppression has triggered, for equipment containing voltage limiting devices such as MOVs, or EMI suppression, at 250 V d.c.\n\nLeakage current testing is performed using a PAT by applying a nominal voltage to the live conductors (active and neutral) of an appliance, and placing 0 volt reference on the earthed parts of a Class I appliance or the external metal parts of a Class II appliance;\n\nInsulation resistance testing is performed using an ohmmeter or portable appliance tester by applying a nominal voltage to the live conductors (active and neutral) of an appliance, and placing 0 volt reference on the earthed parts of a Class I appliance or the external metal parts of a Class II appliance;\n\nA deficiency of the insulation resistance (500V/250V d.c) test is that the d.c voltage will not activate electromagnetic switches or internal relays etc. that are common in many modern power tools, computers, TVs etc. and therefore it can only test the appliance up to that point. Appliances with these components / design should be tested using the leakage current test.\n\nIn countries where the sockets are polarised, polarity testing is a simple test that can be carried out using a polarity tester to determine whether the active and neutral of the plug end are correctly connected to the corresponding terminals at the socket end. Note: The earth is tested during the earth continuity test. In the UK, as per BS7671, the phase ('Live' or 'Hot') cable should connect with right hand side terminal of the socket (if we face the socket outlet).\n\n\n\nThere are two tests methods to be used;\n\nThis test requires specific test equipment RCD tester:\n\n(the TEST button referred to here is the test button on the RCD tester NOT the TEST button on the RCD itself. If the RCD is live, pressing the RCD's own TEST button should ALWAYS trip the RCD).\n\nThe RCD tester should be connected with the socket, with earth terminal (must !!) and \n1)select the testing range 'delta' (10mA,30mA,100mA,300mA,500mA).\n2)Select half delta range, and press TEST button - the RCD should not operate, this ensures against nuisance tripping.\n3)Select delta range, and press TEST button - The RCD should operate, within m Sec for 50 Hz,230V system (as per BS 7671)\n\nThis test requires specialised test equipment, knowledge and training;\n\nIn addition to this, many technicians also test;\n\nBest Practice is to test the RCD under 1/2, 1X and 5X rated tripping current, each at both the 0 degree and 180 degree phases.\n\nThere are two specific extra checks mandated for microwave ovens in the United Kingdom.\n\nThe first is that the device immediately ceases production of the microwave radiation when the door is opened, which checks that the safety interlock systems are functional; and the second is that any leakage when operating is less than 5 mWcm which indicates that the door and casing are not distorted and any seals are intact so that there is no hazard to those in the vicinity of the equipment.\n\nA piece of calibrated equipment is required for these tests to detect and measure leakage of the 2.4 GHz microwave radiation, it is usually a hand-held device with a sensing antenna that can be scanned over the areas where the door meets the casing to find any radiation \"hot-spots\" whilst the unit is operating. As Microwave ovens are not normally designed to be operated without a load this will usually take the form of an open container containing a quantity of water which is used to absorb the energy and as it gets warmed gives an indication that a unit not previously examined by a tester is actually producing microwaves. After checking for leakage the door is required to be opened by whatever means is provided and the measurement device is not to record a level above the given limit. In some scenarios a known quantity of water is heated for a known period of time and the temperature rise over the period of operation is used to generate an indication of the effective power output of the magnetron. This can be helpful to determine whether the oven is operating at the expected power levels indicated by labelling.\n\nElectrical appliance classes are differentiated by a series of IEC protection classes. The protocols for PAT Testing vary by appliance class. \n\nThe earth lead is connected to metal parts on both Class I and Class II appliances. For \"Class I\" during the earth test to prove continuity between earth pin and metal parts on the appliance. For \"Class II\" during the insulation test to prove the insulation between active-neutral and the metal parts of the appliance. i.e. there is no leakage from mains coming into the appliance to the metal parts that are exposed.\n\nIn the UK, there is no requirement to have a formal qualification for persons carrying out PAT Testing. The Electricity at Work regulations of 1989 simply state that where required, inspecting and testing must be carried out by a competent person, however does not mention a benchmark for competency. It has become accepted practice, however, for individuals operating as PAT Testers to hold a 2377–22 City and Guilds qualification. PAT Testers in the UK do not need to be electricians or have a background in the electrical industry. An example of a nationally recognised qualification of competence in PAT is offered by City & Guilds: 'Code of Practice for In-service Inspection and Testing of Electrical Equipment – 4th Edition (No. 2377)'. although there are others provided by EAL available.\n\nIn Australia it is a legal requirement to have attended a course or gained a qualification in order to PAT. Proof of a company’s competence takes the form of a course certificate or qualification. A formal examination process for the topic is operated in collaboration with EAL or city and guilds (the awarding body) under the authority of the QCA (The Qualifications and Curriculum Authority) who validate and authorise the qualification.\n\nAt the basic level PAT test instruments carry out basic safety checks. Most are equipped with an earth continuity test, insulation resistance test and the ability to check the wiring of detachable mains cords. Many do not however include tests which involve applying mains power to the appliance under test, for example, a protective conductor current or touch current tests. The main kind for businesses are simple PASS/FAIL testers that are easy to use, and are aimed at in-house PAT testing with minimal interpretation of results. Advanced PAT testers can give much more information and testing features but are mainly aimed at more highly skilled users.\n\nThese are the simple-to-use and comparatively much cheaper portable appliance testers for most businesses who will test in-house to carry out the testing and suitable for a wide range of businesses. They simply say PASS or FAIL when a test is carried out. Mains powered testers require AC power. Battery operated PAT testers are self-contained and convenient to use. They usually come with rechargeable batteries.\n\nThese testers have a simple \"lights\" system. They have a \"Pass\" light, a \"Fail\" light, options for Class I metal / plastic or Class II. They also will show:\n\nThese testers display more information than just pass or fail, including:\n\nThese readings require interpretation by an electrician or someone with electrical understanding. Advanced PAT testers are effective as facilities management tools because they can record the location and test status of electrical equipment and appliances.\n\nRCD testing\n\nSome units can also test Residual-current devices, following the recommendation in the current IEE Code of Practice to test any RCD fitted on an extension lead or multiway adapter. \nSome advanced PAT testers can download information to a computer. Bluetooth enabled computerised PAT testers make the two way transfer of test data between the tester and PC-based record keeping systems much simpler, and can be used with other test accessories such as label printers. Records can be maintained using PAT software such as PATorganiser\n\nAs PAT testers are sophisticated instruments, it is important to make sure that they are continuing to measure correctly. If a company fails to check and maintain calibration, it could face difficulty substantiating any measurements in the event of a claim. It is usually recommended that calibration is carried out annually on a PAT testing unit.\n\nWhen a PAT tester is calibrated it is re-configuring it to match the original specification. This includes:\n\n\nDual purpose check boxes (which are essentially known resistances either side of the test limits)have also been introduced which are capable of validating the accuracy of both electrical installation testers and portable appliance testers in the field, reducing the risk of a tester being used when not operating correctly - this also allows the re-calibration interval to be increased.\n\n"}
{"id": "25541329", "url": "https://en.wikipedia.org/wiki?curid=25541329", "title": "Project Gaia", "text": "Project Gaia\n\nProject Gaia is a U.S. non-governmental, non-profit organization involved in the creation of a commercially viable household market for alcohol-based fuels in Ethiopia and other countries in the developing world. The project considers alcohol fuels to be a solution to fuel shortages, environmental damage, and public health issues caused by traditional cooking in the developing world. Targeting poor and marginalized communities that face health issues from cooking over polluting fires, Gaia currently works in Ethiopia, Nigeria, Brazil, Haiti, and Madagascar, and is in the planning stage of projects in several other countries.\n\nMore than 3 billion people cook with wood fire worldwide. Approximately 60% of African families cook with traditional biomass, a percentage that increases to 90% for Sub-Saharan Africa. Smoke and gaseous emissions pour out of burning wood, animal dung, or crop residues, leading to lung disease and respiratory illnesses in women and children. Traditional biomass fuels release emissions that contain pollutants dangerous to health, such as small particles, carbon monoxide (CO), nitrogen dioxide, butadiene, formaldehyde, and carcinogens such as benzopyrene and benzene. The World Health Organization estimates that more than 4 million people die each year from household air pollution generated by cooking with solid fuels in poorly ventilated spaces. 500,000 of these deaths are from childhood pneumonia. Indoor air pollution is especially deadly for children; it is responsible for nearly 50% of pneumonia deaths in children under the age of five.\n\nBecause of their constant exposure to cook fires, women and children are particularly at risk. Indoor air pollution causes 56% of deaths and 80% of the global burden of disease for children under the age of five. Indoor air pollution also increases the risk of acute lower respiratory infections, chronic obstructive pulmonary disease, and is associated with tuberculosis, perinatal mortality, low birth weight, asthma, otitis media, cancer of the upper airway, and cataracts. Respiratory disease in children is the leading cause of death for children, though malaria and diarrheal diseases are better known. Indoor air pollution also disproportionately effects refugee, poor urban, and HIV/AIDs populations living in crowded and poorly ventilated conditions, and already carrying the burden of disease.\n\nMeanwhile, collecting wood involves risk to personal safety. Women and older children usually collect wood, often facing gender-based harassment and disputes with landowners who accuse them of trespassing. Women in Ethiopia's UNHCR refugee camps say that they fear assault, rape, and violence while searching for wood.\n\nBeyond these public health issues, cooking with wood fires is also unsustainable and contributes to rapid deforestation in the developing world. Where wood is already limited, its collection leads to desertification. In Africa, collection of wood for cooking and charcoal production is the primary reason for the disappearance of the forests. Further, the burning of hydrocarbon fuels, coal, charcoal, and even dung contributes to the accumulation of greenhouse gases. Smoky cooking fires and stoves contribute to the soot that is estimated to cause approximately 16% of global warming. Black carbon particles in the atmosphere are considered one of the most dangerous pollutants after carbon dioxide. Furthermore, higher rates of deforestation and desertification force women to travel further and further to gather fuelwood, increasing their vulnerability to the dangers of fuelwood collection.\n\nThe available alternatives to biomass fuels do not offer much improvement. Kerosene is imported at significant expense, and is burnt with a wick stove that does not combust the fuel efficiently and tips or spills easily. The projected retail cost of ethanol is lower than that of government-subsidized kerosene in Ethiopia today. Kerosene stoves are also prone to flare ups and explosions; accidental kerosene fires cause many injuries and deaths each year. Kerosene releases carcinogenic emissions and is often poorly refined or adulterated by the time it is acquired for domestic use. Liquified petroleum gas (LPG) burns cleanly but is expensive and cannot be produced locally. Charcoal, a processed biomass fuel, burns with less smoke, but emits carbon monoxide. Meanwhile, coal produces all of the dangerous emissions of traditional biomass fuels and depending on its quality, also produces sulfur oxides and toxic elements such as arsenic, lead, fluorine, and mercury.\nAid initiatives and emergency response projects rarely provide energy for cooking, focusing instead on food rations and necessary cookware. Occasionally the UNHCR purchases and provides biomass and transports it to communities where it is desperately needed. When aid projects do involve stoves, many undertake the dissemination of improved biomass stoves. These burn biomass fuel more efficiently but do not eliminate the household need for biomass nor eliminate indoor air pollution.\n\nAlcohol fuels have a low-flammability limit (LFL) that is higher than hydrocarbon fuels, which means they do not catch fire easily, even when spilled. They are extinguishable by water and are not prone to explosion like LPG (propane and butane). Alcohol burns cleanly, producing only carbon dioxide and water vapor, and none of the soot or toxic chemicals produced by solid fuels and kerosene.\nAlcohol fuels are clean (with particle emissions far below the WHO levels). When paired with an efficient stove, alcohol fuels can drastically improve indoor air quality, and thus improve respiratory health and quality of life by reducing the global burden of disease.\" \n\nAlcohol is beginning to gain wider recognition as a source of fuel. Because of initiatives like Project Gaia, the UNDP has placed the previously absent ethanol and methanol near the top of its “energy ladder,” a progression of fuels that range from dirty and inefficient (e.g. biomass) to clean and efficient (e.g. biogas, LPG, ethanol, methanol). Project Gaia believes in gradual introduction of ethanol-burning appliances into both the private household market and the small or large institutional market, to ensure widespread acceptance. As the alcohol fuel market increases, and acceptance of clean cookstoves grows, Project Gaia sees the potential for the use of other high-performing alcohol appliances that provide lighting, refrigeration and air-tempering, and generation.\n\nEthanol can be produced locally from byproducts that would otherwise cause environmental damage. It can be produced from biomass, natural gas, coal, and even landfill gas—resources plentiful in many developing countries. The use of alcohol fuel for household fuel, therefore, is efficient resource management—transforming waste products to a highly valuable resource. This transformation can lead to a cleaner environment, the forming of new jobs in industry, agriculture, manufacturing, and the service sector, and a decreased need for imported fuels.\n\nProject Gaia's team currently works with the alcohol-based CleanCook stove, a stable, stainless steel, one or two-burner stove adapted from the Origo stove invented by Bengt Ebbeson in 1979 and manufactured by Swedish company Dometic AB, the leading manufacturer of alcohol stoves and appliances worldwide. The Origo stove, recognized as the best alcohol stove available, is popular in the leisure markets in the U.S. and Europe, commonly for boating and camping use. The stove's working life is to projected to be 5–10 years for daily use. Project Gaia works to adapt the stove to local cooking needs and habits. Some of the adaptations made to the CleanCook include the slight raising and redesign of the pot-stand to allow for larger pots and for more oxygen to reach the flame, and the addition of handles to make transport and refill of the stove easier.\n\nDometic seeks to address the developing world market because alcohol fuel technologies are particularly appropriate for it. The CleanCook contains a non-pressurized, no-spill fuel tank that can hold methanol or ethanol. The CleanCook stove was designed with several specific safety measures. The CleanCook stove's fuel canister holds an absorbent mineral fiber covered by a protective metal mesh, preventing fuel from spilling from the canister even when the stove is inverted. The design of this fuel tank eliminates risk of explosion, flare-up, or leakage, and other safety features built into the stove make the CleanCook less likely to cause burns than other cooking methods. Meanwhile, the CleanCook is a high-performing stove, equivalent in power (1.5 to 2kW) and temperature to an LPG stove.\nTo operate the stove, its fuel canister must be filled with one liter of ethanol. If pouring the fuel results in drops around the hole, these must be wiped away. The canister must be clipped onto the base of the stove, the regulator opened, and the ethanol vapor lit with a match or lighter. After cooking, the canister should be left in position to limit further evaporation of remaining ethanol.\n\nThe US-based Aprovecho Research Center has conducted tests comparing the emissions and energy performance of the CleanCook in comparison with conventional LPG and kerosene stoves. When compared with kerosene, the CleanCook used less energy, produced lower emissions, and was quicker than to bring water to a boil. Compared with LPG, the CleanCook produced the same level of particulate emissions, but LPG released lower emissions of CO and was quicker to boil water. The UNDP in Malawi compared the CleanCook with wood and charcoal stoves and found that the CleanCook cut wood stoves’ CO emission in half, particulate emission by 99%, and energy use by 71%. The findings in comparison with charcoal showed similar results, with the energy use decreased by 55% and the emission and particulate reduction even greater than for wood stoves.\n\nProject Gaia believes that sustainable business is the best way to accomplish the goal of providing clean and safe cooking technology, and is supported by the UNDP Growing Sustainable Business (GSB) in its initiatives to bring the private and public sectors together. Project Gaia's business model involves the merging of stove manufacture and sale with fuel production and sale, thereby giving consumers confidence that they will receive fuel after purchasing stoves, and also allowing part of the stoves’ costs to transfer to the fuel. This financing mechanism is the model used for mobile phones and airtime – the initial cost of the stove would be cheaper to ensure its affordability, and this discount would be recovered over the course of several years in a very small mark-up in fuel cost. Another benefit of this system is that it allows for ownership of the right to produce the stove or parts of the stove in Ethiopia. The CleanCook is currently manufactured in Slovakia, but the possibility of local production in Ethiopia is currently being explored. Local production is in its early stages in Addis Ababa, with a starting goal of 18,000 stoves to be produced yearly. Project Gaia's business goals also include certification through the CDM Gold Standard for dissemination of CleanCook stoves and promotion of alcohol fuels.\n\nEducation about indoor air pollution is a fundamental element of Project Gaia's mission, which it has pursued by spreading awareness about the impact of IAP through workshops, seminars, media coverage, and engagement with local, state, and federal government, and organizations such as the WHO in Ethiopia, and the FDRE’s Ministry of Health. The danger of IAP and its effects on populations is often neglected, with malaria, HIV/AIDS, and infectious diseases receiving the majority of attention.\n\nProject Gaia is also working to educate the public about alcohol fuels and their potential for use in the developing world, and specifically, in the context of household energy. The market for families throughout the developing world that could benefit from the use of alcohol fuels for cooking and household energy is in the range of 600 million. As stated by the UNDP World Energy Assessment (2000), supplying modern energy services to the two billion people who still cook with traditional solid fuels and lack access to electricity is one of the most pressing problems facing humanity today.\n\nThe Ethiopian counterpart of Project Gaia, Inc. is the Ethiopian non-governmental organization, the Gaia Association, which is a UNHCR implementing partner. Ethiopia possesses all of the necessary factors for successful technology transfer of alcohol fuel: the quantity of ethanol produced by its sugar industry, the fact that it depends on imported petroleum fuels and biomass, and its need for improved fuels and safer stoves. In 2009, Ethiopia was producing approximately 8 million liters of ethanol annually. Molasses and other byproducts of the industry were being dumped in Ethiopia's rivers until about a decade ago when one of Ethiopia's five mills, Finchaa Sugar Company, solved the waste problem by acquiring a distillery and producing ethanol. Gaia answered an early query that Finchaa Sugar opened, proposing a household energy market for the ethanol, as no gasoline blending or export market was being successfully developed. Ethiopia's other mills have also become interested in producing ethanol, as a way to balance the fluctuations of sugar pricing. Ethiopia's government is developing plans to build ethanol-producing distilleries at all sugar mills, creating an ethanol supply that could fuel over 200,000 stoves.\n\nGaia is now planning to build its own micro distillery in Ethiopia as a demonstration project for what can be achieved with small and micro scale distillation plants. Gaia is working with the Ethiopian Environmental Protection authority, the Ministry of Mines and Energy and the Ministry of Agriculture to build this distillery. A range of feedstocks are being considered, including more conventional feedstocks such as molasses, and sugar cane or sweet sorghum juice, or unconventional feedstocks such as fruit waste from the markets in Addis Ababa or sugary materials from plants growing wild such as cactus, the bean pods from the mesquite tree (Prosopis), a noxious invasive species in Ethiopia, or the Giant Milkweed plant (Calotropis procera). Gaia is hoping to encourage very small scale distilleries owned by farmer groups or co-ops and small to medium-sized local entrepreneurs.\n\nThe domestic fuel blending market and the export market for ethanol remain difficult in Ethiopia for several reasons. The technical precision and regulation necessary for successful blending has prevented its success in Ethiopia; major petroleum sellers like Shell, Total, Mobil, and domestic companies such as National Oil resist the blending of ethanol in petroleum fuels because the fuel supply system is poorly regulated and the fuel is sometimes adulterated. Fuel sellers cannot trust the integrity of their fuel if blended with ethanol, because the ethanol may take up water into the fuel and promote phase separation and worsened contamination. As for the export market, Ethiopia's ethanol production is too small to attract large international buyers and has only succeeded in selling ethanol at lower prices than other exporters. Ethiopia's landlocked location also makes export difficult. The local selling of Finchaa's ethanol, therefore, provides a sustainable opportunity for Ethiopia, with prices that are affordable but also competitive with the potential export market.\nEthiopia is energy-poor and 95-98% deforested, although most of its population still depends on biomass fuels. Before beginning its pilot studies, Gaia found that refugee families use approximately 3.7 tons of fuel wood a year, and spend up to eight hours every two or three days collecting this wood.\n\nCleanCook stoves have now been tested in and placed in three UNHCR refugee camps, HIV/AIDS orphanages run by Mother Teresa's Missionaries of Charity, in a rural village in Ethiopia's Somali State, and in lower and middle-income private homes in Addis Ababa. Pilot studies conducted by the Gaia Association have yielded positive results and acceptance and widespread approval among households both in the refugee camps and in Addis Ababa. Project Gaia is also working to bring stove manufacturing to Addis Ababa by linking the Ethiopian business Makobu Enterprises PLC and Dometic AB, and encouraging government support for the designation of ethanol for household fuel purposes.\n\nRefugee camps in Eastern Ethiopia that house refugees of Somali conflict have placed heavy pressure on the environment and resources of Ethiopia for the past two decades, reaching a combined population of 600,000 refugees in the early 1990s. The UNHCR and its partner NGOs supply these camps with basic needs but have not provided cooking fuel until recently. Fuel wood gathering, the responsibility of refugee women, has resulted in the almost complete deforestation of the regions around the refugee camps, and Ethiopia's government has banned all cutting of live trees around the camps. The UNHCR must provide alternatives to wood fuel at any new refugee camps.\n\nProject Gaia is currently active in Awbarre refugee camp (formerly known as Teferi Ber) and Kebribeyah refugee camp, where all of the camp's approximately 1780 families have CleanCook stoves and a daily ration of ethanol, funded cooperatively by the UNHCR and the Gaia Association. The combined population of Kebribeyah and Awbarre camps is approximately 27,000 people, and the UNHCR is planning to re-open at least two more camps to make room for past refugees who have fled to Ethiopia again after returning to Somalia and experiencing worsening conditions since July 2007.\n\nIn 2012, Gaia Association received a grant from the Nordic Climate Fund (NCF) to demonstrate the feasibility of locally produced ethanol for cooking. The grant provided funds to build a community owned and operated ethanol microdistillery. The distillery, which is currently under construction in the Kolfe-Keranio community on the outskirts of Addis Ababa, will be run by a local women's cooperative, the Former Women Fuelwood Carriers Association.\n\nAlso in 2012, in an effort to address the environmental and social problems associated with the overreliance on traditional biomass for household cooking, Gaia Association, together with the Ethiopian Environmental Protection Authority, is piloting a project to demonstrate small-scale, community owned and driven ethanol production from sugarcane (in the form of ethanol microdistilleries) in Amhara, Oromya & Gambella regional states. Partial installation of the EMDs has been completed in two of the three regional states.\n\nProject Gaia works in collaboration with many organizations that help support pilot studies, encourage government involvement, and provide local or regional support. Gaia's lead business partners are members of the United Nations Global Compact, and many members of the Gaia team have joined the Partners for Clean Indoor Air (PCIA), which developed out of the 2002 World Summit on Sustainable Development (WSSD) in Johannesburg with the goal of cleaning up indoor air. Project Gaia is also a member of the HEDON Household Energy Network which unites key actors and stakeholders in the movement for a cleaner, more efficient, and affordable household energy sector.\n\nIn Ethiopia, Gaia has created partnerships with organizations that serve poor communities and refugee camps. Gaia's Ethiopia partners include the UNHCR-RLO and the UNDP, the Good Shepherd Sisters, the Missionaries of Charity, the Former Women Fuelwood Carriers’ Association, Finchaa Sugar Company, the Ethiopian Rural Energy Development and Promotion Center, and the Ogaden Welfare and Development Association. Under its Breathe Easy Program, the Shell Foundation is a technical and a funding partner for Gaia's Ethiopia and Brazil projects. These partners have supported Gaia's pilot studies and helped to strengthen the relationships between Ethiopian businesses and European and American businesses, with the goal of facilitating the sustainable transfer of technology, goods, and services to Ethiopia.\n\nBy 2050 Nigeria is expected to outgrow the population of the USA. The market for cookstoves- and more importantly, cooking fuels- is huge. Spurring both ethanol and methanol production will create local jobs, help give farmers new markets, and be locally sourced.\n\nNigeria, like Ethiopia has high potential for large-scale ethanol production from starchy cassava and sugarcane molasses. Nigeria also possesses an excess of natural gas in the form of gas flares; it is responsible for 40% of worldwide flaring. This natural gas can be easily converted to methanol and Nigeria's flared gas alone could fuel CleanCook stoves in every household in West Africa. Nigeria, like other West African countries, is energy poor and depends on fuelwood for cooking. In Nigeria, Project Gaia depends on the local support of the Centre of Household Energy and Health, based in the Niger Delta. Gaia's main sponsor in Delta State is the U.S. EPA in partnership with the Delta State Government.\n\nGaia's technical partner for Nigeria, HydroChem, which is part of the Linde Group, is the leading provider of small-scale hydrogen, CO, and CO2 plants. These plants are easily adapted to manufacture methanol, by converting natural gas into liquid form, and thereby creating a fuel that can be used in the household energy market. In gas or electric form, it is not feasible for the developing world market because of the investment, infrastructure, and maintenance necessary and subsidies that are required to make it affordable. HydroChem's plants are ideal for local, small-scale production at low expense in countries like Nigeria, where gas flares could be converted into household fuel.\nCurrently, Project Gaia is working with local commercial partners to begin a 10,000 stove pilot study.\n\nProject Gaia studied the acceptance of the CleanCook ethanol stove in different urban and rural households in the state of Minas Gerais, Brazil between 2006-2007. Because LPG was too costly for low-income families, most households in the study had been using firewood for cooking. In some areas, woodfuel collection is banned. Participating families could get fuel for cooking easily and cheaply at the gas pump. Households found ethanol easy, safe and affordable to use. The ability to buy ethanol in small quantities, rather than being forced to purchase large canisters of LPG, was most beneficial to low-income households.\n\nBrazil's strong social policies have helped to lift families out of poverty and increase access to energy. As Brazil develops, Project Gaia is working to replicate the Brazilian model in other project sites. Project Gaia's work continues in partnership with Prolenha.\n\nProject Gaia began work in Madagascar beginning in 2008 at the request of the Government of Madagascar. Project Gaia's work is commissioned with the objective of multifaceted contribution to the goals of the Africa Action Plan (AAP) and the Millennium Development Goals (MDGs) and to the goals of several initiatives related to the development of alternative household energy sources. Gaia's work with household or indoor air pollution will address other MDGs such as the promotion of gender equality, improving maternal health, and ensuring environmental sustainability. The Madagascar Action Plan (MAP) is one initiative that is working to promote alternative sources of energy that will decrease the burden on Madagascar's forests and to reduce child mortality, a large factor of which is the household use of solid fuels. Project Gaia's involvement also contributes to goals in several specific sectors including those of the Ministère de l’Environnement, des Eaux et Forêts et du Tourisme, the Ministère de l’Energie et des Mines, the Ministère de la Santé, du Planning Familial et de la Protection, the Ministère de l’Agriculture, Elevage et la Pêche, and the Foundation Tany Meva - piloting community-based use of ethanol for household cooking.\n\nProject Gaia launched operations in Haiti, in response to the increased need for relief after the 2010 hurricane and in the hopes of contribution to the “Build Back Better” strategy being developed by Haiti's government and other engaged nations and organizations. That Haiti is more than 98% deforested makes transition away from wood fuels essential, especially since 70% of Haiti's population continues to rely on charcoal and firewood for fuel. Since the earthquake, Haitian families have been spending at least 40% of their income on charcoal and all fuel prices have surged.\n\nMeanwhile, Haiti has great potential for local, sustainable production of ethanol because of its past status as a significant sugar cane producer and the capacity for revival of this industry. One million tons of sugar cane is produced annually – a decrease from 3 million in 1987 – and sugar mills and distilleries are already in place. Ethanol, even before it is produced locally, can still be sold locally for a lower price than Haitian charcoal.\n\nFor this project, Gaia is receiving donations of ethanol fuel for CleanCook stoves from the Brazilian government and the Brazilian ethanol industry. Gaia's approach in Haiti involves three phases: 1) small-scale emergency intervention through donation of 500–5000 stoves, and work with local mills and distilleries to scale up domestic production of ethanol 2) larger-scale scale-up of 20,000–50,000 stoves and ethanol to IDP camps and large communities, partnership with other organizations and 3) sustainability work and local development, cooperation with government to encourage the passage of a biofuels plan, assessment of the state of local mills and distilleries that are closed, and of the possibility for micro distillery.\n\nGaia's collaborators for its Haiti Project include the Government of Haiti Ministry of Women's Affairs, Government of Brazil Ministry of External Relations, UNICA, COSAN, Dometic Group, Viva Rio Haiti, Marin Biological Lab (Woods Hole), CODEP, Terra Endeavors, Inc., J&J Import, BDP International, Trees Water People, Public-Private Alliance Foundation (PPAF).\n\nGaia Association conducted thorough pilot studies on the use of different household fuels in Addis Ababa, their varying prices, and the rates of their consumption. In 2004, Gaia began testing two-burner prototype stoves in 850 homes. Before and after the 850 stoves were placed, Gaia's field staff conducted household energy audits, consumer satisfaction studies and fuel price elasticity studies. In 50 homes, Gaia monitored IAP, recording levels of CO and particulate matter. Approximately 150 of the stoves that were tested in Addis Ababa were placed in very low-income homes. This part of the study helped Gaia determine the ideal price level for ethanol so that it would be affordable to the average household and also reach the poorer households.\n\nThroughout the process of pilot studies in each of its project sites, Gaia's team documented progress and compiled reports that charted the study results. Project Gaia used the study results to develop its business plan and to answer questions such as: Who is interested in the stove? Who can produce it? How should it be adapted to better suit its use? Who can distribute it? Who can supply the fuel? How will stove and fuel be linked?\n\nTo simulate reality and ensure affordability of the stove, Gaia charged for ethanol during the Pilot studies. After a month of free fuel, during which families could get used to cooking with the stove, each household paid for its fuel, allowing fuel use and rationing to be accurately documented. After the first month of purchasing fuel, its price was increased to reflect the cost of ethanol once the local market is fully developed. Price Elasticity studies documented the impact of the price increase on the family and their capacity and willingness to pay. Pilot study data showed that these households demonstrated the necessary purchasing power to afford CleanCook stoves and their fuel, if the purchasing price of the stove was subsidized.\n\nEven the poorest urban communities pay for cooking fuels because they cannot gather their fuel, as many refugees do in rural areas of Ethiopia. Wood and charcoal prices have risen because of the need to travel further and further outside of the city to gather them. Poor families are often forced to pay more for fuel because they buy small quantities of low-quality fuel when they can afford to – sometimes before each meal. Even the lowest grade of fuel is often expensive when purchased this way.\nDomestically produced ethanol would not require as much subsidization as petroleum-based fuels like kerosene because it can be sold for a lower price. Its domestic production, meanwhile, benefits the economy, and the subsidy that is contributed for its purchase does not leave the country as it does for imported fuel.\n\nIn the context of the refugee camps, ethanol was not sold because of the lack of purchasing power among refugees. When fuel gathering exhausts the little remaining biomass, energy provisions will have to be donor-supplied. Providing stoves and improved fuel, however, can save long-term costs by increasing productivity and well-being among refugees, increasing industry, and reducing health care costs.\n\nResponse to the stoves was very positive, both in households where families had experience with modern stoves and improved fuels, and in households without previous experience. The dominant response included appreciation for the cleanliness and safety of the stove and fuel and a positive evaluation of the stove's power and efficiency. In the refugee camps, CleanCook stoves allowed more women to cook inside their homes, and diminished the need for fuel wood gathering and allowed women more time to pursue income-generating activities, take care of personal health, provide childcare, and seek education. A reduced amount of fuel wood gathering also eased tensions between refugees and local landowners who do not welcome the gathering of wood from their land.\n\nMany groups supported Gaia in its pilot studies, including the UNHCR management and logistical staff for the refugee camps, by the Refugee Care Netherlands (ZOA) staff and FDRE Administration for Refugee and Returnee Affairs (ARRA) staff, field survey staff from the Ethiopian Rural Energy Development and Promotion Center, an Ethiopian government agency under the Ministry of Mines and Energy, the UNDP Growing Sustainable Businesses Program, Winrock International's Clean Energy Group, Makobu Enterprises management personnel, Dometic AB representatives from Research & Development, the Stove Division and Marketing, the Stokes Consulting Group, UNDP-GSB consultants, Finchaa Sugar, consultants from the Center for Entrepreneurship in International Health and Development at the University of California at Berkeley, and Shell Foundation advisors.\n\n\n\n\n\n\n\n\n"}
{"id": "35709604", "url": "https://en.wikipedia.org/wiki?curid=35709604", "title": "Public health nursing", "text": "Public health nursing\n\nPublic health nursing, a term coined by Lillian Wald of the Henry Street Settlement, or community health nursing, is a nursing specialty focused on public health. Public health nurses (PHNs) or community health nurses \"integrate community involvement and knowledge about the entire population with personal, clinical understandings of the health and illness experiences of individuals and families within the population.\" Public health nursing in the United States traces back to a nurse named Lillian Wald who, in 1893, established the Henry Street Settlement in New York City and coined the expression \"public health nurse\".\n\nPublic health nurses work within communities and focus on different areas to improve the overall health of the people within that community. Some areas of employment for public health nurses are school districts, county or state health departments, and departments of correction. The public health nurse looks for areas of concern within the community and assesses and plans ways through which the concerns can be resolved or minimized. Some health concerns a public health nurse may work on are infection control, health maintenance, health coaching, as well as home care visits for welfare and to provide care to certain members of the community who may need it.\n\n"}
{"id": "15336425", "url": "https://en.wikipedia.org/wiki?curid=15336425", "title": "Risk equalization", "text": "Risk equalization\n\nRisk equalization is a way of equalizing the risk profiles of insurance members to avoid loading premiums on the insured to some predetermined extent.\n\nIn health insurance, it enables private health insurance to operate in some countries to be offered at a common rate for all even though insurers are not allowed by law to reject clients or impose special conditions for their health insurance. That is achieved by transfer payments by a risk equalization pool usually run by a neutral party, such as a government agency.\n\nIn unregulated competitive markets for individual health insurance, risk-rated premiums are observed to differ across subgroups of insured people, which are defined by rating factors such as age, gender, family size, geographic area (because costs of care may be higher or lower in some coverage areas than others) occupation, length of contract period, the level of deductible, health status at time of enrollment, health habits (smoking, drinking, exercising) and, via differentiated bonuses for multiyear no-claim, to prior costs.\n\nSome nations that encourage private insurance for health care still seek to prevent insurers from engaging in risk minimizing actions to load the premiums of people with certain high-risk profiles, typically the elderly, the sick, and to some extent, women. Thus, financial transfers are needed in order to prohibit any discriminatory practices against these subgroups without increasing costs on insurers. This is done by arranging for a third party to organize a regulatory system of risk-adjusted premium subsidies.\n\nThe financial transfers are then channeled via a so-called Subsidy Fund. In European countries such as the Netherlands, Belgium, Germany, and Switzerland, the Subsidy Fund is run by a government agency, which assesses risks for individual policy holders. In all countries that apply risk-adjusted premium subsidies in their health insurance market, the sponsor organizes it in the form of risk equalization among health insurers: the risk-adjusted premium subsidies for the insured are channelled to the insurers. Then, the Subsidy Fund is called a Risk Equalization Fund (REF). An insurer receives a relatively large sum of subsidies by the REF if the risk profile of their members is relatively unhealthy and vice versa.\n\nAlthough premiums can be rated across many subgroups of insured people, a sponsor may not want to subsidize all observed premium rate variation, in practice. The total set of risk factors used by insurers to rate their premiums can be divided in two subsets: the subset of risk factors that cause premium rate variation that the sponsor decides to subsidize, the \"S(ubsidy)\"-type risk factors; and the subset that causes premium rate variations which the sponsor does not want to subsidize, the \"N(on-subsidy)\"-type risk factors.\n\nGender, health status and (to a certain extent) age will, in most countries, be considered S-type risk factors. Examples of potential N-type risk factors are a high propensity for medical consumption, living in a region with high prices and/or overcapacity resulting in supply-induced demand, or using providers with an inefficient practice-style. The sponsor determines the specific categorization of \"S\"-type and \"N\"-type risk factors. When the government takes up the role of the sponsor, this categorization is ultimately determined by value judgments in society. Because the premium subsidies are risk-based, price competition will not be distorted by these subsidies and therefore incentives for efficiency are not reduced. \n\nThat operates in countries such as Australia, Germany, the Netherlands, Belgium, Switzerland, and Ireland. The system of risk equalization plays a crucial role in order to reduce the incentives for risk selection in this new Dutch market of regulated competition. (See Health care in the Netherlands) Dutch insurers are not allowed to risk-rate their premiums. In practice, the sponsor often encounters difficulties to find adequate measures of the S-type risk factors (such as health status) to include in the risk equalization model.\n\nThe concept was put in the US healthcare law, passed in 2010, the Patient Protection and Affordable Care Act. To achieve its aims, state and federal regulators must construct an effective system of risk adjustment or risk equalization that protects health insurers that attract a disproportionate share of patients with poor health risks and punishes those that cherry pick lower risk groups.\n\n\n\n"}
{"id": "25010568", "url": "https://en.wikipedia.org/wiki?curid=25010568", "title": "Sex Roles (journal)", "text": "Sex Roles (journal)\n\nSex Roles is a peer-reviewed scientific journal published by Springer. Articles appearing in \"Sex Roles\" are written from a feminist perspective, and topics span gender role socialization, gendered perceptions and behaviors, gender stereotypes, body image, violence against women, gender issues in employment and work environments, sexual orientation and identity, and methodological issues in gender research. The Editor-in-Chief is Janice D. Yoder.\n\n\"Sex Roles\" is abstracted/indexed in:\n\nAccording to the \"Journal Citation Reports\", the journal has a 2016 impact factor of 1.954, ranking it 1st out of 41 journals in the category \"Women's Studies\" and 11th out of 62 journals in the category, \"Social Psychology\".\n\n"}
{"id": "52668957", "url": "https://en.wikipedia.org/wiki?curid=52668957", "title": "Social Responsiveness Scale", "text": "Social Responsiveness Scale\n\nThe Social Responsiveness Scale is a quantitative measure of autistic traits in 4–18 year olds. Its correlation with behaviour problems and autism spectrum disorder symptoms has been studied. It can be assessed with an 18 question survey filled out by the childs parents.\n\nThe list of questions is subject to copyright.\n\n"}
{"id": "17475776", "url": "https://en.wikipedia.org/wiki?curid=17475776", "title": "The Heart Truth", "text": "The Heart Truth\n\nThe Heart Truth is a campaign meant to raise awareness of the risk of heart disease in women. The campaign is sponsored in the United States by the National Heart, Lung, and Blood Institute, an organization of the United States Department of Health and Human Services; a similar campaign is promoted in Canada by the Heart and Stroke Foundation of Canada. It focuses mainly on educating women aged forty to sixty, as that is the time when the risk of heart disease begins to increase.\n\nThe campaign began in March 2001 on recommendation from over seventy experts on the health of women. The research stressed the need to communicate to women about the risk of heart disease, and endorsed The Heart Truth as a means of doing so.\n\nThe logo of the campaign is a red dress. It came into being as a way to attract attention to the Heart Truth, and eliminate perceptions that heart disease is an issue only for men. The dress reminds women to focus on their \"outerselves\", as well as their \"innerselves\", especially heart health.\n\nThe campaign has also conjured a National Wear Red Day, meant to take place on the first Friday of February annually.\n\nThe Heart Truth has joined with the United States Federal Government and fashion industries, in an attempt to appeal to female audiences. Red dresses have been displayed across the country, primarily at New York's Fashion Week. The first Red Dress Collection Fashion Week took place in 2003 when nineteen designers, including Vera Wang, Oscar de la Renta, and Carmen Marc Valvo contributed dresses that were displayed in the Byrant Park Tents. Many fashion shows have been put on in recent years during the Fashion Week festivities; many famous celebrities have participated in walking the aisle, including Jenna Fischer, Sheryl Crow, Natalie Morales, Kelly Ripa, Deborah Harry, Venus Williams, Angela Bassett, Rachael Ray, Valerie Bertinelli, Christie Brinkley, Thalía, Vanessa L. Williams, Raven-Symoné, Allison Janney, Sara Ramirez, Billie Jean King, Katie Couric, Sarah, Duchess of York, Lindsay Lohan, LeAnn Rimes, Christina Milian, Fergie, Jordin Sparks, Ashanti, Hilary Duff, Mary Lynn Rajskub, Rose McGowan and Eartha Kitt.\n\nLaureen Harper, the Wife of Canadian Prime Minister Stephen Harper has been a great supporter and has served as guest of honour at the event at Nathan Phillips Square in Toronto, Ontario for many consecutive years.\n\nFormer First Lady Laura Bush has been the ambassador for the Heart Truth since 2003. She has led the federal government in giving women more information relating to heart disease. Bush has coordinated many events relating to the Heart Truth, including a White House ceremony in 2004, the Kennedy Center exhibit, the Reagan Library exhibit, and has participated in all Fashion Week events dating to 2003.\n\nA signature component of Mrs. Bush's involvement is her communication with women at hospital events featuring those living with heart disease. She promotes the campaign through various media interviews as well.\n\nIn May 2005, the Heart Truth constructed a special exhibition at the John F. Kennedy Center for the Performing Arts in Washington, D.C., known as the First Ladies Red Dress Collection. The collection featured seven red dresses worn by America's first ladies Lady Bird Johnson, Betty Ford, Rosalynn Carter, Nancy Reagan, Barbara Bush, Hillary Clinton and Laura Bush. The exhibit was unveiled by Laura Bush, in the presence of many Congressional spouses and Cabinet secretaries.\n\nIn February 2007, the Heart Truth moved that exhibit to the Ronald Reagan Presidential Library and Museum in Simi Valley, California. There, the exhibit was opened by former First Lady Nancy Reagan along with television personality Larry King and Laura Bush. A conference was held at the library with leaders of the heart disease awareness movement as well as Bush and Reagan.\n\nIn February 2013, the Heart Truth presented a fashion show at Manhattan's Hammerstein Ballroom. Celebrities who walked the runway included Minka Kelly, Soledad O'Brien, Wendy Williams, Brenda Strong, Kris Jenner, Jamie Chung, Toni Braxton, Kelly Osbourne and others.\n\n"}
{"id": "6606537", "url": "https://en.wikipedia.org/wiki?curid=6606537", "title": "Tony Mills (physician)", "text": "Tony Mills (physician)\n\nTony Mills, is an American physician who specializes in treatment of HIV and AIDS. \nHe is one of the leading clinician in the fields of Men's Health and HIV disease. Mills has served as the primary care provider for over 2,000 patients, including approximately half living with HIV. Mills received both his undergraduate and medical degrees from Duke University. He completed an internship in Internal Medicine, a residency in Anesthesiology and a fellowship in Cardiovascular Anesthesiology, all at the University of California, San Francisco. Mills is a member of many professional societies including; the Infectious Disease Society of America, International AIDS Society, IAS-USA, and the American Academy of HIV Medicine. He is the executive director of SoCal Men's Medical Group, the clinical research director of Mills Clinical Research, and the president of the Men's Health Foundation.\n\nIn May 1998, he won the title of International Mister Leather, publicly coming out as HIV-positive one day later.\n\n\nMills graduated from Duke University School of Medicine and was awarded both the Stanley Sarnoff Fellowship Award in Cardiovascular Research and the Eugene Stead Research Award. He began his clinical practice in 1991 at Columbia Presbyterian Medical Center in New York City, concentrating on heart transplantation and cardiovascular research.\n\nIn 1994, Mills was named Chief of Pediatric Cardiac Anesthesiology at the University of Miami, where he was actively involved in both the recovery community and in the gay community as an advocate for people living with HIV.\n\nIn 1999, he moved to Los Angeles and opened a general medical practice specializing in HIV care. He was certified as an HIV specialist by the American Academy of HIV Medicine in 2000 and currently serves on both the California Board and the National Board of the AAHIVM. In 2002, Mills joined the Clinical Medicine Faculty at UCLA where he works actively with residents and fellows and is a frequent lecturer.\n\nHe is the current editor of \"HIV Treatment News\" and is a frequent contributor to other HIV-related publications.\n\nOn May 5, 1998, having earned the regional title of Mister Mid-Atlantic Leather, Mills entered and won the International Mister Leather contest, competing against 61 contestants from 7 countries.\n\nSince winning the contest, Mills has been featured in the documentaries \"Beyond Vanilla\" and \"Mr. Leather\". He has also been a model for the COLT Studio Group.\n"}
{"id": "5875254", "url": "https://en.wikipedia.org/wiki?curid=5875254", "title": "Utilization management", "text": "Utilization management\n\nUtilization Management (UM) is the use of techniques that allow purchasers to manage the cost of health care benefits by assessing its appropriateness before it is provided using evidence-based criteria or guidelines. Critics have argued if cost-cutting by insurers is the focus of their use of UM criteria, it could lead to overzealous denial of care as well as retrospective denial of payment, delays in care, or unexpected financial risks to patients.\n\nUtilization Management is \"a set of techniques used by or on behalf of purchasers of health care benefits to manage health care costs by influencing patient care decision-making through case-by-case assessments of the appropriateness of care prior to its provision,\" as defined by the Institute of Medicine (IOM) Committee on Utilization Management by Third Parties (1989; IOM is now the National Academy of Medicine). Utilization review is synonymous with, or a part of, utilization management (depending on how the terms are used).\n\nUM is the evaluation of the appropriateness and medical necessity of health care services, procedures, and facilities according to evidence-based criteria or guidelines, and under the provisions of an applicable health insurance plan. Typically, UM addresses new clinical activities or inpatient admissions based on the analysis of a case. But this may relate to ongoing provision of care, especially in an inpatient setting.\n\nDischarge planning, concurrent planning, pre-certification and clinical case appeals are proactive UM procedures. It also covers proactive processes, such as concurrent clinical reviews and peer reviews as well as appeals introduced by the provider, payer or patient. A UM program comprises roles, policies, processes, and criteria.\n\nRoles included in UM may include: UM Reviewers (often a Registered Nurse with UM training), a UM program manager, and a Physician Adviser. UM policies may include the frequency of reviews, priorities, and balance of internal and external responsibilities. UM processes may include escalation processes when a clinician and the UM reviewer are unable to resolve a case, dispute processes to allow patients, caregivers, or patient advocates to challenge a point of care decision, and processes for evaluating inter-rater reliability among UM reviewers.\n\nUM criteria may be developed in house, acquired from a UM vendor, or acquired and adapted to suit local conditions. Two commonly used UM criteria frameworks are the McKesson InterQual criteria and MCG (previously known as the Milliman Care Guidelines). Similar to the Donabedian healthcare quality assurance model, UM may be done prospectively, retrospectively, or concurrently.\n\nProspective review is typically used as a method of reducing medically unnecessary admissions or procedures by denying cases that do not meet criteria, or allocating them to more appropriate care settings before the act.\n\nConcurrent review is carried out during and as part of the clinical workflow, and supports point of care decisions. The focus of concurrent UM tends to be on reducing denials and placing the patient at a medically appropriate point of care. Concurrent review may include a case-management function that includes coordinating and planning for a safe discharge or transition to the next level of care.\n\nRetrospective review considers whether an appropriate level of care applied after it was administered. Retrospective review will typically look at whether the procedure, location, and timing were appropriate according to the criteria. This form of review typically relates to payment or reimbursement according to a medical plan or medical insurance provision. Denial of the claim could relate to payment to the provider or reimbursement to the plan member. Alternatively, retrospective review may reflect a decision as to ongoing point of care. This may entail justification according to the UM criteria and plan to leave a patient at the previous (current) point of care, or to shift the patient to a higher or lower point of care that would match the UM criteria. For example, an inpatient case situated in a telemetry bed (high cost) may be evaluated on a subsequent day of stay as no longer meeting the criteria for a telemetry bed. This may be due to changes in acuity, patient response, or diagnosis, or may be due to different UM criteria set for each continued day of stay. At this time the reviewer may indicate alternatives such as a test to determine alternate criteria for continued stay at that level, transfer to a lower (or higher) point of care, or discharge to outpatient care.\n\nUM has been criticized for treating cost of care as an outcome metric, and that this confuses the objectives of healthcare and potentially reduces healthcare value by mixing up process of care with results of care.\n\nSome authors have pointed out that when cost-cutting by insurers is the focus of UM criteria, it may lead to overzealous prospective denial of care as well as retrospective denial of payment. As a result, there may be delays in care or unexpected financial risks to patients.\n\n"}
{"id": "49432423", "url": "https://en.wikipedia.org/wiki?curid=49432423", "title": "WHO Pesticide Evaluation Scheme", "text": "WHO Pesticide Evaluation Scheme\n\nThe World Health Organization (WHO) Pesticide Evaluation Scheme (WHOPES) was established in 1960 for setting norms and standards for public health pesticides.\n\n"}
