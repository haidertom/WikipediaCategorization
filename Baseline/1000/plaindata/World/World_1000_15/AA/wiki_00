{"id": "689", "url": "https://en.wikipedia.org/wiki?curid=689", "title": "Asia", "text": "Asia\n\nAsia () is Earth's largest and most populous continent, located primarily in the Eastern and Northern Hemispheres. It shares the continental landmass of Eurasia with the continent of Europe and the continental landmass of Afro-Eurasia with both Europe and Africa. Asia covers an area of , about 30% of Earth's total land area and 8.7% of the Earth's total surface area. The continent, which has long been home to the majority of the human population, was the site of many of the first civilizations. Asia is notable for not only its overall large size and population, but also dense and large settlements, as well as vast barely populated regions. Its 4.5 billion people () constitute roughly 60% of the world's population.\n\nIn general terms, Asia is bounded on the east by the Pacific Ocean, on the south by the Indian Ocean, and on the north by the Arctic Ocean. The border of Asia with Europe is a historical and cultural construct, as there is no clear physical and geographical separation between them. It is somewhat arbitrary and has moved since its first conception in classical antiquity. The division of Eurasia into two continents reflects East-West cultural, linguistic, and ethnic differences, some of which vary on a spectrum rather than with a sharp dividing line. The most commonly accepted boundaries place Asia to the east of the Suez Canal separating it from Africa; and to the east of the Turkish Straits, the Ural Mountains and Ural River, and to the south of the Caucasus Mountains and the Caspian and Black Seas, separating it from Europe.\n\nChina and India alternated in being the largest economies in the world from 1 to 1800 CE. China was a major economic power and attracted many to the east, and for many the legendary wealth and prosperity of the ancient culture of India personified Asia, attracting European commerce, exploration and colonialism. The accidental discovery of a trans-Atlantic route from Europe to America by Columbus while in search for a route to India demonstrates this deep fascination. The Silk Road became the main East-West trading route in the Asian hinterlands while the Straits of Malacca stood as a major sea route. Asia has exhibited economic dynamism (particularly East Asia) as well as robust population growth during the 20th century, but overall population growth has since fallen. Asia was the birthplace of most of the world's mainstream religions including Christianity, Islam, Judaism, Hinduism, Buddhism, Confucianism, Taoism, Jainism, Sikhism, Zoroastrianism, as well as many other religions.\n\nGiven its size and diversity, the concept of Asia—a name dating back to classical antiquity—may actually have more to do with human geography than physical geography. Asia varies greatly across and within its regions with regard to ethnic groups, cultures, environments, economics, historical ties and government systems. It also has a mix of many different climates ranging from the equatorial south via the hot desert in the Middle East, temperate areas in the east and the continental centre to vast subarctic and polar areas in Siberia.\n\nThe boundary between Asia and Africa is the Red Sea, the Gulf of Suez, and the Suez Canal. This makes Egypt a transcontinental country, with the Sinai peninsula in Asia and the remainder of the country in Africa.\n\nThe border between Asia and Europe was historically defined by European academics. The Don River became unsatisfactory to northern Europeans when Peter the Great, king of the Tsardom of Russia, defeating rival claims of Sweden and the Ottoman Empire to the eastern lands, and armed resistance by the tribes of Siberia, synthesized a new Russian Empire extending to the Ural Mountains and beyond, founded in 1721. The major geographical theorist of the empire was actually a former Swedish prisoner-of-war, taken at the Battle of Poltava in 1709 and assigned to Tobolsk, where he associated with Peter's Siberian official, Vasily Tatishchev, and was allowed freedom to conduct geographical and anthropological studies in preparation for a future book.\n\nIn Sweden, five years after Peter's death, in 1730 Philip Johan von Strahlenberg published a new atlas proposing the Urals as the border of Asia. The Russians were enthusiastic about the concept, which allowed them to keep their European identity in geography. Tatishchev announced that he had proposed the idea to von Strahlenberg. The latter had suggested the Emba River as the lower boundary. Over the next century various proposals were made until the Ural River prevailed in the mid-19th century. The border had been moved perforce from the Black Sea to the Caspian Sea into which the Ural River projects. The border between the Black Sea and the Caspian is usually placed along the crest of the Caucasus Mountains, although it is sometimes placed further north.\n\nThe border between Asia and the region of Oceania is usually placed somewhere in the Malay Archipelago. The Maluku Islands in Indonesia are often considered to lie on the border of southeast Asia, with New Guinea, to the east of the islands, being wholly part of Oceania. The terms Southeast Asia and Oceania, devised in the 19th century, have had several vastly different geographic meanings since their inception. The chief factor in determining which islands of the Malay Archipelago are Asian has been the location of the colonial possessions of the various empires there (not all European). Lewis and Wigen assert, \"The narrowing of 'Southeast Asia' to its present boundaries was thus a gradual process.\"\n\nGeographical Asia is a cultural artifact of European conceptions of the world, beginning with the Ancient Greeks, being imposed onto other cultures, an imprecise concept causing endemic contention about what it means. Asia is larger and more culturally diverse than Europe. It does not exactly correspond to the cultural borders of its various types of constituents.\n\nFrom the time of Herodotus a minority of geographers have rejected the three-continent system (Europe, Africa, Asia) on the grounds that there is no substantial physical separation between them. For example, Sir Barry Cunliffe, the emeritus professor of European archeology at Oxford, argues that Europe has been geographically and culturally merely \"the western excrescence of the continent of Asia\".\n\nGeographically, Asia is the major eastern constituent of the continent of Eurasia with Europe being a northwestern peninsula of the landmass. Asia, Europe and Africa make up a single continuous landmass – Afro-Eurasia (except for the Suez Canal) – and share a common continental shelf. Almost all of Europe and the better part of Asia sit atop the Eurasian Plate, adjoined on the south by the Arabian and Indian Plate and with the easternmost part of Siberia (east of the Chersky Range) on the North American Plate.\n\nThe idea of a place called \"Asia\" was originally a concept of Greek civilization, though this might not correspond to the entire continent currently known by that name. The English word comes from Latin literature, where it has the same form, \"Asia\". Whether \"Asia\" in other languages comes from Latin of the Roman Empire is much less certain, and the ultimate source of the Latin word is uncertain, though several theories have been published. One of the first classical writers to use Asia as a name of the whole continent was Pliny. This metonymical change in meaning is common and can be observed in some other geographical names, such as Skandinavia (from Scania).\n\nBefore Greek poetry, the Aegean Sea area was in a Greek Dark Age, at the beginning of which syllabic writing was lost and alphabetic writing had not begun. Prior to then in the Bronze Age the records of the Assyrian Empire, the Hittite Empire and the various Mycenaean states of Greece mention a region undoubtedly Asia, certainly in Anatolia, including if not identical to Lydia. These records are administrative and do not include poetry.\n\nThe Mycenaean states were destroyed about 1200 BCE by unknown agents although one school of thought assigns the Dorian invasion to this time. The burning of the palaces baked clay diurnal administrative records written in a Greek syllabic script called Linear B, deciphered by a number of interested parties, most notably by a young World War II cryptographer, Michael Ventris, subsequently assisted by the scholar, John Chadwick. A major cache discovered by Carl Blegen at the site of ancient Pylos included hundreds of male and female names formed by different methods.\n\nSome of these are of women held in servitude (as study of the society implied by the content reveals). They were used in trades, such as cloth-making, and usually came with children. The epithet \"lawiaiai\", \"captives\", associated with some of them identifies their origin. Some are ethnic names. One in particular, aswiai, identifies \"women of Asia\". Perhaps they were captured in Asia, but some others, Milatiai, appear to have been of Miletus, a Greek colony, which would not have been raided for slaves by Greeks. Chadwick suggests that the names record the locations where these foreign women were purchased. The name is also in the singular, Aswia, which refers both to the name of a country and to a female of it. There is a masculine form, aswios. This Aswia appears to have been a remnant of a region known to the Hittites as Assuwa, centered on Lydia, or \"Roman Asia\". This name, \"Assuwa\", has been suggested as the origin for the name of the continent \"Asia\". The Assuwa league was a confederation of states in western Anatolia, defeated by the Hittites under Tudhaliya I around 1400 BCE.\n\nAlternatively, the etymology of the term may be from the Akkadian word \"(w)aṣû(m)\", which means 'to go outside' or 'to ascend', referring to the direction of the sun at sunrise in the Middle East and also likely connected with the Phoenician word \"asa\" meaning east. This may be contrasted to a similar etymology proposed for \"Europe\", as being from Akkadian \"erēbu(m)\" 'to enter' or 'set' (of the sun).\n\nT.R. Reid supports this alternative etymology, noting that the ancient Greek name must have derived from \"asu\", meaning 'east' in Assyrian (\"ereb\" for \"Europe\" meaning 'west'). The ideas of \"Occidental\" (form Latin \"Occidens\" 'setting') and \"Oriental\" (from Latin \"Oriens\" for 'rising') are also European invention, synonymous with \"Western\" and \"Eastern\". Reid further emphasizes that it explains the Western point of view of placing all the peoples and cultures of Asia into a single classification, almost as if there were a need for setting the distinction between Western and Eastern civilizations on the Eurasian continent. Ogura Kazuo and Tenshin Okakura are two outspoken Japanese figures on the subject.\n\nLatin Asia and Greek Ἀσία appear to be the same word. Roman authors translated Ἀσία as Asia. The Romans named a province Asia, located in western Anatolia (in modern-day Turkey). There was an Asia Minor and an Asia Major located in modern-day Iraq. As the earliest evidence of the name is Greek, it is likely circumstantially that Asia came from Ἀσία, but ancient transitions, due to the lack of literary contexts, are difficult to catch in the act. The most likely vehicles were the ancient geographers and historians, such as Herodotus, who were all Greek. Ancient Greek certainly evidences early and rich uses of the name.\n\nThe first continental use of Asia is attributed to Herodotus (about 440 BCE), not because he innovated it, but because his \"Histories\" are the earliest surviving prose to describe it in any detail. He defines it carefully, mentioning the previous geographers whom he had read, but whose works are now missing. By it he means Anatolia and the Persian Empire, in contrast to Greece and Egypt.\n\nHerodotus comments that he is puzzled as to why three women's names were \"given to a tract which is in reality one\" (Europa, Asia, and Libya, referring to Africa), stating that most Greeks assumed that Asia was named after the wife of Prometheus (i.e. Hesione), but that the Lydians say it was named after Asies, son of Cotys, who passed the name on to a tribe at Sardis. In Greek mythology, \"Asia\" (\"Ἀσία\") or \"Asie\" (\"Ἀσίη\") was the name of a \"Nymph or Titan goddess of Lydia\".\n\nIn ancient Greek religion, places were under the care of female divinities, parallel to guardian angels. The poets detailed their doings and generations in allegoric language salted with entertaining stories, which subsequently playwrights transformed into classical Greek drama and became \"Greek mythology\". For example, Hesiod mentions the daughters of Tethys and Ocean, among whom are a \"holy company\", \"who with the Lord Apollo and the Rivers have youths in their keeping\". Many of these are geographic: Doris, Rhodea, Europa, Asia. Hesiod explains:\n\nFor there are three-thousand neat-ankled daughters of Ocean who are dispersed far and wide, and in every place alike serve the earth and the deep waters.\n\nThe Iliad (attributed by the ancient Greeks to Homer) mentions two Phrygians (the tribe that replaced the Luvians in Lydia) in the Trojan War named Asios (an adjective meaning \"Asian\"); and also a marsh or lowland containing a marsh in Lydia as .\n\nThe history of Asia can be seen as the distinct histories of several peripheral coastal regions: East Asia, South Asia, Southeast Asia and the Middle East, linked by the interior mass of the Central Asian steppes.\n\nThe coastal periphery was home to some of the world's earliest known civilizations, each of them developing around fertile river valleys. The civilizations in Mesopotamia, the Indus Valley and the Yellow River shared many similarities. These civilizations may well have exchanged technologies and ideas such as mathematics and the wheel. Other innovations, such as writing, seem to have been developed individually in each area. Cities, states and empires developed in these lowlands.\n\nThe central steppe region had long been inhabited by horse-mounted nomads who could reach all areas of Asia from the steppes. The earliest postulated expansion out of the steppe is that of the Indo-Europeans, who spread their languages into the Middle East, South Asia, and the borders of China, where the Tocharians resided. The northernmost part of Asia, including much of Siberia, was largely inaccessible to the steppe nomads, owing to the dense forests, climate and tundra. These areas remained very sparsely populated.\n\nThe center and the peripheries were mostly kept separated by mountains and deserts. The Caucasus and Himalaya mountains and the Karakum and Gobi deserts formed barriers that the steppe horsemen could cross only with difficulty. While the urban city dwellers were more advanced technologically and socially, in many cases they could do little in a military aspect to defend against the mounted hordes of the steppe. However, the lowlands did not have enough open grasslands to support a large horsebound force; for this and other reasons, the nomads who conquered states in China, India, and the Middle East often found themselves adapting to the local, more affluent societies.\n\nThe Islamic Caliphate's defeats of the Byzantine and Persian empires led to West Asia and southern parts of Central Asia and western parts of South Asia under its control during its conquests of the 7th century. The Mongol Empire conquered a large part of Asia in the 13th century, an area extending from China to Europe. Before the Mongol invasion, Song dynasty reportedly had approximately 120 million citizens; the 1300 census which followed the invasion reported roughly 60 million people.\n\nThe Black Death, one of the most devastating pandemics in human history, is thought to have originated in the arid plains of central Asia, where it then travelled along the Silk Road.\n\nThe Russian Empire began to expand into Asia from the 17th century, and would eventually take control of all of Siberia and most of Central Asia by the end of the 19th century. The Ottoman Empire controlled Anatolia, most of the Middle East, North Africa and the Balkans from the mid 16th century onwards. In the 17th century, the Manchu conquered China and established the Qing dynasty. The Islamic Mughal Empire and the Hindu Maratha Empire controlled much of India in the 16th and 18th centuries respectively.\n\nAsia is the largest continent on Earth. It covers 9% of the Earth's total surface area (or 30% of its land area), and has the largest coastline, at . Asia is generally defined as comprising the eastern four-fifths of Eurasia. It is located to the east of the Suez Canal and the Ural Mountains, and south of the Caucasus Mountains (or the Kuma–Manych Depression) and the Caspian and Black Seas. It is bounded on the east by the Pacific Ocean, on the south by the Indian Ocean and on the north by the Arctic Ocean. Asia is subdivided into 48 countries, three of them (Russia, Kazakhstan and Turkey) having part of their land in Europe.\n\nAsia has extremely diverse climates and geographic features. Climates range from arctic and subarctic in Siberia to tropical in southern India and Southeast Asia. It is moist across southeast sections, and dry across much of the interior. Some of the largest daily temperature ranges on Earth occur in western sections of Asia. The monsoon circulation dominates across southern and eastern sections, due to the presence of the Himalayas forcing the formation of a thermal low which draws in moisture during the summer. Southwestern sections of the continent are hot. Siberia is one of the coldest places in the Northern Hemisphere, and can act as a source of arctic air masses for North America. The most active place on Earth for tropical cyclone activity lies northeast of the Philippines and south of Japan. The Gobi Desert is in Mongolia and the Arabian Desert stretches across much of the Middle East. The Yangtze River in China is the longest river in the continent. The Himalayas between Nepal and China is the tallest mountain range in the world. Tropical rainforests stretch across much of southern Asia and coniferous and deciduous forests lie farther north.\n\n\nA survey carried out in 2010 by global risk analysis farm Maplecroft identified 16 countries that are extremely vulnerable to climate change. Each nation's vulnerability was calculated using 42 socio, economic and environmental indicators, which identified the likely climate change impacts during the next 30 years. The Asian countries of Bangladesh, India, Vietnam, Thailand, Pakistan and Sri Lanka were among the 16 countries facing extreme risk from climate change. Some shifts are already occurring. For example, in tropical parts of India with a semi-arid climate, the temperature increased by 0.4 °C between 1901 and 2003.\nA 2013 study by the International Crops Research Institute for the Semi-Arid Tropics (ICRISAT) aimed to find science-based, pro-poor approaches and techniques that would enable Asia's agricultural systems to cope with climate change, while benefitting poor and vulnerable farmers. The study's recommendations ranged from improving the use of climate information in local planning and strengthening weather-based agro-advisory services, to stimulating diversification of rural household incomes and providing incentives to farmers to adopt natural resource conservation measures to enhance forest cover, replenish groundwater and use renewable energy.\n\nAsia has the largest continental economy by both GDP Nominal and PPP in the world, and is the fastest growing economic region. , the largest economies in Asia are China, Japan, India, Russia, South Korea, Indonesia and Turkey based on GDP in both nominal and PPP. Based on Global Office Locations 2011, Asia dominated the office locations with 4 of the top 5 being in Asia: Hong Kong, Singapore, Tokyo, Seoul and Shanghai. Around 68 percent of international firms have office in Hong Kong.\n\nIn the late 1990s and early 2000s, the economies of China and India have been growing rapidly, both with an average annual growth rate of more than 8%. Other recent very-high-growth nations in Asia include Israel, Malaysia, Indonesia, Bangladesh, Pakistan, Thailand, Vietnam, Mongolia, Uzbekistan, Cyprus and the Philippines, and mineral-rich nations such as Kazakhstan, Turkmenistan, Iran, Brunei, the United Arab Emirates, Qatar, Kuwait, Saudi Arabia, Bahrain and Oman.\n\nAccording to economic historian Angus Maddison in his book \"The World Economy: A Millennial Perspective\", India had the world's largest economy during 0 BCE and 1000 BCE. China was the largest and most advanced economy on earth for much of recorded history, until the British Empire (excluding India) overtook it in the mid-19th century. For several decades in the late twentieth century Japan was the largest economy in Asia and second-largest of any single nation in the world, after surpassing the Soviet Union (measured in net material product) in 1986 and Germany in 1968. (NB: A number of supernational economies are larger, such as the European Union (EU), the North American Free Trade Agreement (NAFTA) or APEC). This ended in 2010 when China overtook Japan to become the world's second largest economy.\n\nIn the late 1980s and early 1990s, Japan's GDP was almost as large (current exchange rate method) as that of the rest of Asia combined. In 1995, Japan's economy nearly equaled that of the US as the largest economy in the world for a day, after the Japanese currency reached a record high of 79 yen/US$. Economic growth in Asia since World War II to the 1990s had been concentrated in Japan as well as the four regions of South Korea, Taiwan, Hong Kong and Singapore located in the Pacific Rim, known as the Asian tigers, which have now all received developed country status, having the highest GDP per capita in Asia.\nIt is forecasted that India will overtake Japan in terms of nominal GDP by 2020. By 2027, according to Goldman Sachs, China will have the largest economy in the world. Several trade blocs exist, with the most developed being the Association of Southeast Asian Nations.\n\nAsia is the largest continent in the world by a considerable margin, and it is rich in natural resources, such as petroleum, forests, fish, water, rice, copper and silver. Manufacturing in Asia has traditionally been strongest in East and Southeast Asia, particularly in China, Taiwan, South Korea, Japan, India, the Philippines, and Singapore. Japan and South Korea continue to dominate in the area of multinational corporations, but increasingly the PRC and India are making significant inroads. Many companies from Europe, North America, South Korea and Japan have operations in Asia's developing countries to take advantage of its abundant supply of cheap labour and relatively developed infrastructure.\n\nAccording to Citigroup 9 of 11 Global Growth Generators countries came from Asia driven by population and income growth. They are Bangladesh, China, India, Indonesia, Iraq, Mongolia, Philippines, Sri Lanka and Vietnam. Asia has four main financial centers: Tokyo, Hong Kong, Singapore and Shanghai. Call centers and business process outsourcing (BPOs) are becoming major employers in India and the Philippines due to the availability of a large pool of highly skilled, English-speaking workers. The increased use of outsourcing has assisted the rise of India and the China as financial centers. Due to its large and extremely competitive information technology industry, India has become a major hub for outsourcing.\n\nIn 2010, Asia had 3.3 million millionaires (people with net worth over US$1 million excluding their homes), slightly below North America with 3.4 million millionaires. Last year Asia had toppled Europe.\nCitigroup in The Wealth Report 2012 stated that Asian centa-millionaire overtook North America's wealth for the first time as the world's \"economic center of gravity\" continued moving east. At the end of 2011, there were 18,000 Asian people mainly in Southeast Asia, China and Japan who have at least $100 million in disposable assets, while North America with 17,000 people and Western Europe with 14,000 people.\n\nWith growing Regional Tourism with domination of Chinese visitors, MasterCard has released Global Destination Cities Index 2013 with 10 of 20 are dominated by Asia and Pacific Region Cities and also for the first time a city of a country from Asia (Bangkok) set in the top-ranked with 15.98 international visitors.\n\nEast Asia had by far the strongest overall Human Development Index (HDI) improvement of any region in the world, nearly doubling average HDI attainment over the past 40 years, according to the report's analysis of health, education and income data. China, the second highest achiever in the world in terms of HDI improvement since\n1970, is the only country on the \"Top 10 Movers\" list due to income rather than health or education achievements. Its per capita income increased a stunning 21-fold over the last four decades, also lifting hundreds of millions out of income poverty. Yet it was not among the region's top performers in improving school enrollment and life expectancy.\n<br>Nepal, a South Asian country, emerges as one of the world's fastest movers since 1970 mainly due to health and education achievements. Its present life expectancy is 25 years longer than in the 1970s. More than four of every five children of school age in Nepal now attend primary school, compared to just one in five 40 years ago.\n<br> Japan and South Korea ranked highest among the countries grouped on the HDI (number 11 and 12 in the world, which are in the \"very high human development\" category), followed by Hong Kong (21) and Singapore (27). Afghanistan (155) ranked lowest amongst Asian countries out of the 169 countries assessed.\n\nAsia is home to several language families and many language isolates. Most Asian countries have more than one language that is natively spoken. For instance, according to Ethnologue, more than 600 languages are spoken in Indonesia, more than 800 languages spoken in India, and more than 100 are spoken in the Philippines. China has many languages and dialects in different provinces.\n\nMany of the world's major religions have their origins in Asia, including the five most practiced in the world (excluding irreligion), which are Christianity, Islam, Hinduism, Chinese folk religion (classified as Confucianism and Taoism), and Buddhism respectively. Asian mythology is complex and diverse. The story of the Great Flood for example, as presented to Jews in the Hebrew Bible in the narrative of Noah—and later to Christians in the Old Testament, and to Muslims in the Quran—is earliest found in Mesopotamian mythology, in the Enûma Eliš and \"Epic of Gilgamesh\". Hindu mythology similarly tells about an avatar of Vishnu in the form of a fish who warned Manu of a terrible flood. Ancient Chinese mythology also tells of a Great Flood spanning generations, one that required the combined efforts of emperors and divinities to control.\n\nThe Abrahamic religions including Judaism, Christianity, Islam and Bahá'í Faith originated in West Asia.\n\nJudaism, the oldest of the Abrahamic faiths, is practiced primarily in Israel, the indigenous homeland and historical birthplace of the Hebrew nation: which today consists both of those Israelites who remained in Asia/North Africa and those who returned from diaspora in Europe, North America, and other regions; though various diaspora communities persist worldwide. Jews are the predominant ethnic group in Israel (75.6%) numbering at about 6.1 million, although the levels of adherence to Jewish religion vary. Outside of Israel there are small ancient Jewish communities in Turkey (17,400), Azerbaijan (9,100), Iran (8,756), India (5,000) and Uzbekistan (4,000), among many other places. In total, there are 14.4–17.5 million (2016, est.) Jews alive in the world today, making them one of the smallest Asian minorities, at roughly 0.3 to 0.4 percent of the total population of the continent.\n\nChristianity is a widespread religion in Asia with more than 286 million adherents according to Pew Research Center in 2010, and nearly 364 million according to Britannica Book of the Year 2014. Constituting around 12.6% of the total population of Asia. In the Philippines and East Timor, Roman Catholicism is the predominant religion; it was introduced by the Spaniards and the Portuguese, respectively. In Armenia, Cyprus, Georgia and Asian Russia, Eastern Orthodoxy is the predominant religion. In the Middle East, such as in the Levant, Syriac Christianity (Church of the East) and Oriental Orthodoxy are prevalent minority denominations, which are both Eastern Christian sects mainly adhered to Assyrian people or Syriac Christians. Saint Thomas Christians in India trace their origins to the evangelistic activity of Thomas the Apostle in the 1st century.\n\nIslam, which originated in the Hejaz located in modern-day Saudi Arabia, is the second largest and most widely-spread religion in Asia with at least 1 billion Muslims constituting around 23.8% of the total population of Asia. With 12.7% of the world Muslim population, the country currently with the largest Muslim population in the world is Indonesia, followed by Pakistan (11.5%), India (10%), Bangladesh, Iran and Turkey. Mecca, Medina and Jerusalem are the three holiest cities for Islam in all the world. The Hajj and Umrah attract large numbers of Muslim devotees from all over the world to Mecca and Medina. Iran is the largest Shi'a country.\n\nThe Bahá'í Faith originated in Asia, in Iran (Persia), and spread from there to the Ottoman Empire, Central Asia, India, and Burma during the lifetime of Bahá'u'lláh. Since the middle of the 20th century, growth has particularly occurred in other Asian countries, because Bahá'í activities in many Muslim countries has been severely suppressed by authorities. Lotus Temple is a big Baha'i Temple in India.\n\nAlmost all Asian religions have philosophical character and Asian philosophical traditions cover a large spectrum of philosophical thoughts and writings. Indian philosophy includes Hindu philosophy and Buddhist philosophy. They include elements of nonmaterial pursuits, whereas another school of thought from India, Cārvāka, preached the enjoyment of the material world. The religions of Hinduism, Buddhism, Jainism and Sikhism originated in India, South Asia. In East Asia, particularly in China and Japan, Confucianism, Taoism and Zen Buddhism took shape.\n\n, Hinduism has around 1.1 billion adherents. The faith represents around 25% of Asia's population and is the largest religion in Asia. However, it is mostly concentrated in South Asia. Over 80% of the populations of both India and Nepal adhere to Hinduism, alongside significant communities in Bangladesh, Pakistan, Bhutan, Sri Lanka and Bali, Indonesia. Many overseas Indians in countries such as Burma, Singapore and Malaysia also adhere to Hinduism.\n\nBuddhism has a great following in mainland Southeast Asia and East Asia. Buddhism is the religion of the majority of the populations of Cambodia (96%), Thailand (95%), Burma (80–89%), Japan (36–96%), Bhutan (75–84%), Sri Lanka (70%), Laos (60–67%) and Mongolia (53–93%). Large Buddhist populations also exist in Singapore (33–51%), Taiwan (35–93%), South Korea (23–50%), Malaysia (19–21%), Nepal (9–11%), Vietnam (10–75%), China (20–50%), North Korea (2–14%), and small communities in India and Bangladesh. In many Chinese communities, Mahayana Buddhism is easily syncretized with Taoism, thus exact religious statistics is difficult to obtain and may be understated or overstated. The Communist-governed countries of China, Vietnam and North Korea are officially atheist, thus the number of Buddhists and other religious adherents may be under-reported.\n\nJainism is found mainly in India and in oversea Indian communities such as the United States and Malaysia.\nSikhism is found in Northern India and amongst overseas Indian communities in other parts of Asia, especially Southeast Asia.\nConfucianism is found predominantly in Mainland China, South Korea, Taiwan and in overseas Chinese populations.\nTaoism is found mainly in Mainland China, Taiwan, Malaysia and Singapore. Taoism is easily syncretized with Mahayana Buddhism for many Chinese, thus exact religious statistics is difficult to obtain and may be understated or overstated.\n\nSome of the events pivotal in the Asia territory related to the relationship with the outside world in the post-Second World War were:\n\nThe polymath Rabindranath Tagore, a Bengali poet, dramatist, and writer from Santiniketan, now in West Bengal, India, became in 1913 the first Asian Nobel laureate. He won his Nobel Prize in Literature for notable impact his prose works and poetic thought had on English, French, and other national literatures of Europe and the Americas. He is also the writer of the national anthems of Bangladesh and India.\n\nOther Asian writers who won Nobel Prize for literature include Yasunari Kawabata (Japan, 1968), Kenzaburō Ōe (Japan, 1994), Gao Xingjian (China, 2000), Orhan Pamuk (Turkey, 2006), and Mo Yan (China, 2012). Some may consider the American writer, Pearl S. Buck, an honorary Asian Nobel laureate, having spent considerable time in China as the daughter of missionaries, and based many of her novels, namely \"The Good Earth\" (1931) and \"The Mother\" (1933), as well as the biographies of her parents of their time in China, \"The Exile\" and \"Fighting Angel\", all of which earned her the Literature prize in 1938.\n\nAlso, Mother Teresa of India and Shirin Ebadi of Iran were awarded the Nobel Peace Prize for their significant and pioneering efforts for democracy and human rights, especially for the rights of women and children. Ebadi is the first Iranian and the first Muslim woman to receive the prize. Another Nobel Peace Prize winner is Aung San Suu Kyi from Burma for her peaceful and non-violent struggle under a military dictatorship in Burma. She is a nonviolent pro-democracy activist and leader of the National League for Democracy in Burma (Myanmar) and a noted prisoner of conscience. She is a Buddhist and was awarded the Nobel Peace Prize in 1991. Chinese dissident Liu Xiaobo was awarded the Nobel Peace Prize for \"his long and non-violent struggle for fundamental human rights in China\" on 8 October 2010. He is the first Chinese citizen to be awarded a Nobel Prize of any kind while residing in China. In 2014, Kailash Satyarthi from India and Malala Yousafzai from Pakistan were awarded the Nobel Peace Prize \"for their struggle against the suppression of children and young people and for the right of all children to education\".\n\nSir C.V. Raman is the first Asian to get a Nobel prize in Sciences. He won the Nobel Prize in Physics \"for his work on the scattering of light and for the discovery of the effect named after him\".\n\nJapan has won the most Nobel Prizes of any Asian nation with 24 followed by India which has won 13.\n\nAmartya Sen, (born 3 November 1933) is an Indian economist who was awarded the 1998 Nobel Memorial Prize in Economic Sciences for his contributions to welfare economics and social choice theory, and for his interest in the problems of society's poorest members.\n\nOther Asian Nobel Prize winners include Subrahmanyan Chandrasekhar, Abdus Salam, Malala Yousafzai, Robert Aumann, Menachem Begin, Aaron Ciechanover, Avram Hershko, Daniel Kahneman, Shimon Peres, Yitzhak Rabin, Ada Yonath, Yasser Arafat, José Ramos-Horta and Bishop Carlos Filipe Ximenes Belo of Timor Leste, Kim Dae-jung, and 13 Japanese scientists. Most of the said awardees are from Japan and Israel except for Chandrasekhar and Raman (India), Abdus Salam and Malala yousafzai, (Pakistan), Arafat (Palestinian Territories), Kim (South Korea), and Horta and Belo (Timor Leste).\n\nIn 2006, Dr. Muhammad Yunus of Bangladesh was awarded the Nobel Peace Prize for the establishment of Grameen Bank, a community development bank that lends money to poor people, especially women in Bangladesh. Dr. Yunus received his PhD in economics from Vanderbilt University, United States. He is internationally known for the concept of micro credit which allows poor and destitute people with little or no collateral to borrow money. The borrowers typically pay back money within the specified period and the incidence of default is very low.\n\nThe Dalai Lama has received approximately eighty-four awards over his spiritual and political career. On 22 June 2006, he became one of only four people ever to be recognized with Honorary Citizenship by the Governor General of Canada. On 28 May 2005, he received the Christmas Humphreys Award from the Buddhist Society in the United Kingdom. Most notable was the Nobel Peace Prize, presented in Oslo, Norway on 10 December 1989.\n\nWithin the above-mentioned states are several partially recognized countries with limited to no international recognition. None of them are members of the UN:\nReferences to articles:\n\nSpecial topics:\n\nLists:\n\n\n\n"}
{"id": "53730081", "url": "https://en.wikipedia.org/wiki?curid=53730081", "title": "Asia Council", "text": "Asia Council\n\nThe Asia Council is a pan-Asian organization constituted in 2016 to serve as a continent wide forum to address Asia’s key challenges and foster cooperation among countries of Asia. The council has its headquarters in Tokyo and regional directorates in Doha, Chengdu and Bangkok. \n\nThe Asia Council operates through the council headquarters in Tokyo, three regional directorates and country offices. \n\nThe Asia Council is organized into three administrative divisions. The East Asia division has its regional directorate in Tokyo, the South Asia & South East Asia division has its regional directorate in Bangkok and the West Asia & Central Asia division has its regional directorate in Doha. \n\nThe Asia Council covers 48 countries and 6 dependent territories. \n\nThe Asia Council has seven forums. Each forum is mandated to deliberate on a defined area relating Asia. The forum is attended by decision makers and experts. \n\n\n\n\n\n\n\nThe Asia Council fellowship provides financial grant to students from Asian countries to study for a graduate degree in world’s top universities.\n\nThe Asia Council Global Leaders Fellowship is an international graduate fellowship scheme which supports students with exceptional leadership qualities from 48 countries and 6 dependent territories of Asia to undertake graduate studies at some of world’s top universities in United States and United Kingdom. \n\nThe Asia Fellowship is an international graduate fellowship scheme which supports students with exceptional leadership qualities from 48 countries and 6 dependent territories of Asia to undertake graduate studies at Asia’s top universities. \n\nThe Asia Council Einstein Fellowship is an international fellowship scheme which supports students with exceptional leadership qualities from 48 countries and 6 dependent territories of Asia to undertake study for a degree at Tokyo Institute of Technology, Nanyang Technological University, KAIST, Hong Kong University of Science and Technology, and Tsinghua University. \n\nThe council’s research and publishing division produces several reports on Asia including the Asia Security Report and Asia Statistical Report. \n\nThe Asian Review is a journal published by the Asia Council. It covers political, economic and strategic review of the continent. \n\nThe Asia Roundtable is an international conference held by the Asia Council outside Asia. The meeting discusses in detail a single issue that is geopolitically significant for the Asian region. The conference is attended by regional leaders and policy experts. \n\nThe Asia Security Dialogue is a bi-annual meeting held by the Asia Council on most pressing security issues relating Asia.\n\n\n"}
{"id": "19630739", "url": "https://en.wikipedia.org/wiki?curid=19630739", "title": "Continent", "text": "Continent\n\nA continent is one of several very large landmasses of the world. Generally identified by convention rather than any strict criteria, up to seven regions are commonly regarded as continents. Ordered from largest in area to smallest, they are: Asia, Africa, North America, South America, Antarctica, Europe, and Australia.\n\nGeologically, the continents largely correspond to areas of continental crust that are found on the continental plates. However, some areas of continental crust are regions covered with water not usually included in the list of continents. Zealandia is one such area (see submerged continents below).\n\nIslands are frequently grouped with a neighbouring continent to divide all the world's land into geopolitical regions. Under this scheme, most of the island countries and territories in the Pacific Ocean are grouped together with the continent of Australia to form a geopolitical region called \"Oceania\".\n\nBy convention, \"continents are understood to be large, continuous, discrete masses of land, ideally separated by expanses of water.\" Several of the seven conventionally recognized continents are not discrete landmasses separated completely by water. The criterion \"large\" leads to arbitrary classification: Greenland, with a surface area of is considered the world's largest island, while Australia, at is deemed the smallest continent.\n\nEarth's major landmasses all have coasts on a single, continuous World Ocean, which is divided into a number of principal oceanic components by the continents and various geographic criteria.\n\nThe most restricted meaning of \"continent\" is that of a continuous area of land or mainland, with the coastline and any land boundaries forming the edge of the continent. In this sense the term \"continental Europe\" (sometimes referred to in Britain as \"the Continent\") is used to refer to mainland Europe, excluding islands such as Great Britain, Ireland, Malta and Iceland, and the term \"continent of Australia\" may refer to the mainland of Australia, excluding Tasmania and New Guinea. Similarly, the \"continental United States\" refers to the 48 contiguous states and the District of Columbia in central North America and may include Alaska in the northwest of the continent (the two being separated by Canada), while excluding Hawaii, Puerto Rico, and Guam in the oceans.\n\nFrom the perspective of geology or physical geography, \"continent\" may be extended beyond the confines of continuous dry land to include the shallow, submerged adjacent area (the continental shelf) and the islands on the shelf (continental islands), as they are structurally part of the continent.\n\nFrom this perspective, the edge of the continental shelf is the true edge of the continent, as shorelines vary with changes in sea level. In this sense the islands of Great Britain and Ireland are part of Europe, while Australia and the island of New Guinea together form a continent.\n\nAs a cultural construct, the concept of a continent may go beyond the continental shelf to include oceanic islands and continental fragments. In this way, Iceland is considered part of Europe and Madagascar part of Africa. Extrapolating the concept to its extreme, some geographers group the Australian continental plate with other islands in the Pacific into one continent called Oceania. This divides the entire land surface of Earth into continents or quasi-continents.\n\nThe ideal criterion that each continent is a discrete landmass is commonly relaxed due to historical conventions. Of the seven most globally recognized continents, only Antarctica and Australia are completely separated from other continents by the ocean. Several continents are defined not as absolutely distinct bodies but as \"\"more or less\" discrete masses of land\". Asia and Africa are joined by the Isthmus of Suez, and North and South America by the Isthmus of Panama. In both cases, there is no complete separation of these landmasses by water (disregarding the Suez Canal and Panama Canal, which are both narrow and shallow, as well as being artificial). Both these isthmuses are very narrow compared to the bulk of the landmasses they unite.\n\nNorth America and South America are treated as separate continents in the seven-continent model. However, they may also be viewed as a single continent known as America or the Americas. This viewpoint was common in the United States until World War II, and remains prevalent in some Asian six-continent models. This remains the more common vision in Latin American countries, Spain, Portugal, France, Italy and Greece, where they are taught as a single continent.\n\nThe criterion of a discrete landmass is completely disregarded if the continuous landmass of Eurasia is classified as two separate continents: Europe and Asia. Physiographically, Europe and South Asia are peninsulas of the Eurasian landmass. However, Europe is widely considered a continent with its comparatively large land area of , while South Asia, with less than half that area, is considered a subcontinent. The alternative view—in geology and geography—that Eurasia is a single continent results in a six-continent view of the world. Some view separation of Eurasia into Asia and Europe as a residue of Eurocentrism: \"In physical, cultural and historical diversity, China and India are comparable to the entire European landmass, not to a single European country. [...].\" However, for historical and cultural reasons, the view of Europe as a separate continent continues in several categorizations.\n\nIf continents are defined strictly as discrete landmasses, embracing all the contiguous land of a body, then Africa, Asia, and Europe form a single continent which may be referred to as Afro-Eurasia. This produces a four-continent model consisting of Afro-Eurasia, America, Antarctica and Australia.\n\nWhen sea levels were lower during the Pleistocene ice ages, greater areas of continental shelf were exposed as dry land, forming land bridges. At those times Australia–New Guinea was a single, continuous continent. Likewise, the Americas and Afro-Eurasia were joined by the Bering land bridge. Other islands such as Great Britain were joined to the mainlands of their continents. At that time there were just three discrete continents: Afro-Eurasia-America, Antarctica, and Australia-New Guinea.\n\nThere are several ways of distinguishing the continents:\n\n\nThe term \"Oceania\" refers to a group of island countries and territories in the Pacific Ocean, together with the continent of Australia. Pacific islands with ties to other continents (such as Japan, Hawaii or Easter Island) are usually grouped with those continents rather than Oceania. This term is used in several different continental models instead of Australia.\n\nThe following table summarizes the area and population of each continental region using the seven continent model.\n\nThe total land area of all continents is , or 29.1% of earth's surface ().\n\nAside from the conventionally known continents, the scope and meaning of the term \"continent\" varies. Supercontinents, largely in evidence earlier in the geological record, are landmasses that comprise more than one craton or continental core. These have included Laurasia, Gondwana, Vaalbara, Kenorland, Columbia, Rodinia, and Pangaea.\n\nCertain parts of continents are recognized as subcontinents, especially the large peninsulas separated from the main continental landmass by geographical features. The most notable examples are the Indian subcontinent and the Arabian Peninsula. The Southern Cone of South America and Alaskan peninsula of North America are other examples.\n\nIn many of these cases, the \"subcontinents\" concerned are on different tectonic plates from the rest of the continent, providing a geological justification for the terminology. Greenland, generally reckoned as the world's largest island on the northeastern periphery of the North American Plate, is sometimes referred to as a subcontinent. This is a significant departure from the more conventional view of a subcontinent as comprising a very large peninsula on the fringe of a continent.\n\nWhere the Americas are viewed as a single continent (America), it is divided into two subcontinents (North America and South America) or three (with Central America being the third). When Eurasia is regarded as a single continent, Europe is treated as a subcontinent.\n\nSome areas of continental crust are largely covered by the sea and may be considered submerged continents. Notable examples are Zealandia, emerging from the sea primarily in New Zealand and New Caledonia, and the almost completely submerged Kerguelen Plateau in the southern Indian Ocean.\n\nSome islands lie on sections of continental crust that have rifted and drifted apart from a main continental landmass. While not considered continents because of their relatively small size, they may be considered microcontinents. Madagascar, the largest example, is usually considered an island of Africa but has been referred to as \"the eighth continent\" from a .\n\n\"Continents\" may be defined differently for specific purposes. The Biodiversity Information Standards organization has developed the World Geographical Scheme for Recording Plant Distributions, used in many international plant databases. This scheme divides the world into nine \"botanical continents\". Some match the traditional geographical continents, but some differ significantly. Thus the Americas are divided between Northern America (Mexico northwards) and Southern America (Central America and the Caribbean southwards) rather than between North America and South America.\n\nThe term \"continent\" translates Greek , properly \"landmass, terra firma\", the proper name of Epirus and later especially used of Asia (i.e. Asia Minor),\nThe first distinction between continents was made by ancient Greek mariners who gave the names Europe and Asia to the lands on either side of the waterways of the Aegean Sea, the Dardanelles strait, the Sea of Marmara, the Bosporus strait and the Black Sea. The names were first applied just to lands near the coast and only later extended to include the hinterlands. But the division was only carried through to the end of navigable waterways and \"... beyond that point the Hellenic geographers never succeeded in laying their finger on any inland feature in the physical landscape that could offer any convincing line for partitioning an indivisible Eurasia ...\"\n\nAncient Greek thinkers subsequently debated whether Africa (then called \"Libya\") should be considered part of Asia or a third part of the world. Division into three parts eventually came to predominate. From the Greek viewpoint, the Aegean Sea was the center of the world; Asia lay to the east, Europe to the north and west, and Africa to the south. The boundaries between the continents were not fixed. Early on, Europe–Asia boundary was taken to run from the Black Sea along the Rioni River (known then as the \"Phasis\") in Georgia. Later it was viewed as running from the Black Sea through Kerch Strait, the Sea of Azov and along the Don River (known then as the \"Tanais\") in Russia. The boundary between Asia and Africa was generally taken to be the Nile River. Herodotus in the 5th century BC, however, objected to the unity of Egypt being split into Asia and Africa (\"Libya\") and took the boundary to lie along the western border of Egypt, regarding Egypt as part of Asia. He also questioned the division into three of what is really a single landmass, a debate that continues nearly two and a half millennia later.\n\nEratosthenes, in the 3rd century BC, noted that some geographers divided the continents by rivers (the Nile and the Don), thus considering them \"islands\". Others divided the continents by isthmuses, calling the continents \"peninsulas\". These latter geographers set the border between Europe and Asia at the isthmus between the Black Sea and the Caspian Sea, and the border between Asia and Africa at the isthmus between the Red Sea and the mouth of Lake Bardawil on the Mediterranean Sea.\nThrough the Roman period and the Middle Ages, a few writers took the Isthmus of Suez as the boundary between Asia and Africa, but most writers continued to consider it the Nile or the western border of Egypt (Gibbon). In the Middle Ages, the world was usually portrayed on T and O maps, with the T representing the waters dividing the three continents. By the middle of the 18th century, \"the fashion of dividing Asia and Africa at the Nile, or at the Great Catabathmus [the boundary between Egypt and Libya] farther west, had even then scarcely passed away\".\n\nChristopher Columbus sailed across the Atlantic Ocean to the West Indies in 1492, sparking a period of European exploration of the Americas. But despite four voyages to the Americas, Columbus never believed he had reached a new continent—he always thought it was part of Asia.\n\nIn 1501, Amerigo Vespucci and Gonçalo Coelho attempted to sail around what they considered the southern end of the Asian mainland into the Indian Ocean, passing through Fernando de Noronha. After reaching the coast of Brazil, they sailed a long way further south along the coast of South America, confirming that this was a land of continental proportions and that it also extended much further south than Asia was known to. On return to Europe, an account of the voyage, called \"Mundus Novus\" (\"New World\"), was published under Vespucci's name in 1502 or 1503, although it seems that it had additions or alterations by another writer. Regardless of who penned the words, \"Mundus Novus\" credited Vespucci with saying, \"I have discovered a continent in those southern regions that is inhabited by more numerous people and animals than our Europe, or Asia or Africa\", the first known explicit identification of part of the Americas as a continent like the other three.\n\nWithin a few years, the name \"New World\" began appearing as a name for South America on world maps, such as the Oliveriana (Pesaro) map of around 1504–1505. Maps of this time though, still showed North America connected to Asia and showed South America as a separate land.\n\nIn 1507 Martin Waldseemüller published a world map, \"Universalis Cosmographia\", which was the first to show North and South America as separate from Asia and surrounded by water. A small inset map above the main map explicitly showed for the first time the Americas being east of Asia and separated from Asia by an ocean, as opposed to just placing the Americas on the left end of the map and Asia on the right end. In the accompanying book \"Cosmographiae Introductio\", Waldseemüller noted that the earth is divided into four parts, Europe, Asia, Africa and the fourth part, which he named \"America\" after Amerigo Vespucci's first name. On the map, the word \"America\" was placed on part of South America.\n\nFrom the 16th century the English noun \"continent\" was derived from the term \"continent land\", meaning continuous or connected land and translated from the Latin \"terra continens\". The noun was used to mean \"a connected or continuous tract of land\" or mainland. It was not applied only to very large areas of land—in the 17th century, references were made to the \"continents\" (or mainlands) of Isle of Man, Ireland and Wales and in 1745 to Sumatra. The word \"continent\" was used in translating Greek and Latin writings about the three \"parts\" of the world, although in the original languages no word of exactly the same meaning as \"continent\" was used.\n\nWhile \"continent\" was used on the one hand for relatively small areas of continuous land, on the other hand geographers again raised Herodotus's query about why a single large landmass should be divided into separate continents. In the mid-17th century, Peter Heylin wrote in his \"Cosmographie\" that \"A Continent is a great quantity of Land, not separated by any Sea from the rest of the World, as the whole Continent of Europe, Asia, Africa.\" In 1727, Ephraim Chambers wrote in his \"Cyclopædia,\" \"The world is ordinarily divided into two grand continents: the old and the new.\" And in his 1752 atlas, Emanuel Bowen defined a continent as \"a large space of dry land comprehending many countries all joined together, without any separation by water. Thus Europe, Asia, and Africa is one great continent, as America is another.\" However, the old idea of Europe, Asia and Africa as \"parts\" of the world ultimately persisted with these being regarded as separate continents.\n\nFrom the late 18th century, some geographers started to regard North America and South America as two parts of the world, making five parts in total. Overall though, the fourfold division prevailed well into the 19th century.\n\nEuropeans discovered Australia in 1606, but for some time it was taken as part of Asia. By the late 18th century, some geographers considered it a continent in its own right, making it the sixth (or fifth for those still taking America as a single continent). In 1813, Samuel Butler wrote of Australia as \"New Holland, an immense island, which some geographers dignify with the appellation of another continent\" and the \"Oxford English Dictionary\" was just as equivocal some decades later.\n\nAntarctica was sighted in 1820 during the First Russian Antarctic Expedition and described as a continent by Charles Wilkes on the United States Exploring Expedition in 1838, the last continent identified, although a great \"Antarctic\" (antipodean) landmass had been anticipated for millennia. An 1849 atlas labelled Antarctica as a continent but few atlases did so until after World War II.\n\nFrom the mid-19th century, atlases published in the United States more commonly treated North and South America as separate continents, while atlases published in Europe usually considered them one continent. However, it was still not uncommon for American atlases to treat them as one continent up until World War II.\n\nFrom the 1950s, most U.S. geographers divided the Americas into two continents. With the addition of Antarctica, this made the seven-continent model. However, this division of the Americas never appealed to Latin Americans, who saw their region spanning an as a single landmass, and there the conception of six continents remains dominant, as it does in scattered other countries.\n\nSome geographers regard Europe and Asia together as a single continent, dubbed \"Eurasia\". In this model, the world is divided into six continents, with North America and South America considered separate continents.\n\nGeologists use the term \"continent\" in a different manner from geographers. In geology a continent is defined by continental crust: a platform of metamorphic and igneous rock, largely of granitic composition. Some geologists restrict the term 'continent' to portions of the crust built around stable Precambrian \"shield\", typically 1.5 to 3.8 billion years old, called a craton. The craton itself is an accretionary complex of ancient mobile belts (mountain belts) from earlier cycles of subduction, continental collision and break-up from plate tectonic activity. An outward-thickening veneer of younger minimally deformed sedimentary rock covers much of the craton. The margins of geologic continents are characterized by currently active or relatively recently active mobile belts and deep troughs of accumulated marine or deltaic sediments. Beyond the margin, there is either a continental shelf and drop off to the basaltic ocean basin or the margin of another continent, depending on the current plate-tectonic setting of the continent. A continental boundary does not have to be a body of water. Over geologic time, continents are periodically submerged under large epicontinental seas, and continental collisions result in a continent becoming attached to another continent. The current geologic era is relatively anomalous in that so much of the continental areas are \"high and dry\"; that is, many parts of the continents that were once below sea level are now elevated well above it due to changes in sea levels and the subsequent uplifting of those continental areas from tectonic activity.\n\nSome argue that continents are accretionary crustal \"rafts\" that, unlike the denser basaltic crust of the ocean basins, are not subjected to destruction through the plate tectonic process of subduction. This accounts for the great age of the rocks comprising the continental cratons. By this definition, Eastern Europe, India and some other regions could be regarded as continental masses distinct from the rest of Eurasia because they have separate ancient shield areas (i.e. East European craton and Indian craton). Younger mobile belts (such as the Ural Mountains and Himalayas) mark the boundaries between these regions and the rest of Eurasia.\n\nThere are many microcontinents, or continental fragments, that are built of continental crust but do not contain a craton. Some of these are fragments of Gondwana or other ancient cratonic continents: Zealandia, which includes New Zealand and New Caledonia; Madagascar; the northern Mascarene Plateau, which includes the Seychelles. Other islands, such as several in the Caribbean Sea, are composed largely of granitic rock as well, but all continents contain both granitic and basaltic crust, and there is no clear boundary as to which islands would be considered microcontinents under such a definition. The Kerguelen Plateau, for example, is largely volcanic, but is associated with the breakup of Gondwanaland and is considered a microcontinent, whereas volcanic Iceland and Hawaii are not. The British Isles, Sri Lanka, Borneo, and Newfoundland are margins of the Laurasian continent—only separated by inland seas flooding its margins.\n\nPlate tectonics offers yet another way of defining continents. Today, Europe and most of Asia constitute the unified Eurasian Plate, which is approximately coincident with the geographic Eurasian continent excluding India, Arabia, and far eastern Russia. India contains a central shield, and the geologically recent Himalaya mobile belt forms its northern margin. North America and South America are separate continents, the connecting isthmus being largely the result of volcanism from relatively recent subduction tectonics. North American continental rocks extend to Greenland (a portion of the Canadian Shield), and in terms of plate boundaries, the North American plate includes the easternmost portion of the Asian landmass. Geologists do not use these facts to suggest that eastern Asia is part of the North American continent, even though the plate boundary extends there; the word continent is usually used in its geographic sense and additional definitions (\"continental rocks,\" \"plate boundaries\") are used as appropriate.\n\nThe movement of plates has caused the formation and break-up of continents over time, including occasional formation of a supercontinent that contains most or all of the continents. The supercontinent Columbia or Nuna formed during a period of 2.0–1.8 billion years ago and broke up about 1.5–1.3 billion years ago. The supercontinent Rodinia is thought to have formed about 1 billion years ago and to have embodied most or all of Earth's continents, and broken up into eight continents around 600 million years ago. The eight continents later re-assembled into another supercontinent called Pangaea; Pangaea broke up into Laurasia (which became North America and Eurasia) and Gondwana (which became the remaining continents).\n\nThe following table lists the seven continents with their highest and lowest points on land, sorted in decreasing highest points.\n\n† The lowest exposed points are given for North America and Antarctica. The lowest non-submarine bedrock elevations in these continents are the trough beneath Jakobshavn Glacier () and Bentley Subglacial Trench (), but these are covered by kilometers of ice.\n\nSome sources list the Kuma–Manych Depression (a remnant of the Paratethys) as the geological border between Europe and Asia. This would place the Caucasus outside of Europe, thus making Mont Blanc (elevation 4810 m) in the Graian Alps the highest point in Europe – the lowest point would still be the shore of the Caspian Sea.\n\n\n"}
{"id": "49644336", "url": "https://en.wikipedia.org/wiki?curid=49644336", "title": "Cristina Possas", "text": "Cristina Possas\n\nCristina Possas de Albuquerque (born 5 June 1948) is a Brazilian public health scientist working with infectious diseases and emerging infectious diseases from an eco-social perspective. However, her approach to social ecosystem complexity is quite different from the four-fold eco-social approach of Harvard's Nancy Krieger in that she has proposed in a 2001 English-language article in the \"Brazilian Journal of Public Health Reports\" the concept of \"social ecosystem health\" where ecosystems are increasingly changed by social human activity, favoring the emergence of diseases, so the term \"social\" should precede the prefix \"eco\".\n\nShe is a Takemi Fellow at Harvard University in Boston, where for 10 years she has been a Visiting Scientist and a Fulbright Fellow, and a professor at FIOCRUZ in Rio de Janeiro, Brazil.\n\nAs a Brazilian Brazilian public health scientist working with infectious diseases from an eco-social perspective, Cristina Possas de Albuquerque (Cristina Possas) has as a policymaker long worked closely with public health and environmental scientists and with human rights and social justice civil organizations.\n\nShe also is a full professor at FIOCRUZ in Brazil, where in 1998 she had earned a PhD in public health, and where she now does research on infectious diseases and teaches a course named \"Scientific Methodology\" in the Masters and Doctoral Programs.\n\nShe has been also a Professor of Health Policy at the National School of Public Health at the Oswaldo Cruz Foundation, Brazil. She was nominated in 1987 by the Brazilian Ministry of Health/Fiocruz to be the National Technical Coordinator of seven Groups supporting the Health Reform in Brazil, whose contributions were later incorporated into the new Brazilian 1988 Constitution.\n\nFor a decade, she also has been a Takemi Fellow at Harvard University, a Visiting Scientist and Fulbright Fellow, and a member of the Harvard New Diseases Group, coordinated by the late Richard Levins and by Tamara Awerbuch-Friedlander, collaborating with them and other outstanding members of the group in the organizing committee of the Woods Hole Conference on New Diseases, with articles with the group later published in a special supplement of the \"Annals of the New York Academy of Sciences\" on this conference and in other journals. She had held for 10 years a national position as the Head of the Research and Development Unit of the Brazilian National AIDS Program.\n\nShe has worked for more than five decades in several national and international positions with the scientific community and civil society organizations. For ten years, she had been National Director of the Research and Technological Development Unit at the Brazilian AIDS Program in the Brazilian Ministry of Health, which has been recognized globally as an outstanding public health initiative with innovative approaches to the pandemics, always centering on free and universal access to prevention and treatment. She was invited by the Director of the National AIDS Program to conceive and create a new and innovative Research and Technological Development Unit in the Program’s structure.\n\nShe has given interviews on AIDS, dengue, and zika.\n\n\nFIOCRUZ\n\n1985–present - Full professor and researcher at FIOCRUZ (Master, Doctoral and Post-Doctoral Programs in Clinical Research on Infectious Diseases). Scientific Advisor for the President of the Policy and Strategy Council of Bio-Manguinhos.\n\nPositions:\n\n\nNATIONAL TECHNICAL BIOSAFETY COMMISSION - CTNBio\n\n\nNATIONAL AIDS PROGRAM\n\n\nHARVARD UNIVERSITY\n\n\nSTATE OF SÃO PAULO, BRAZIL\n\n\nPUC OF CAMPINAS, SP.\n\n\nCITY OF CAMPINAS, SP\n\n\nUNICAMP, SP\n\n\n\nDr. Possas's writings are in six broad areas:\n\n\nThese numerous writings include:\n\n\n\n\n\n"}
{"id": "41216691", "url": "https://en.wikipedia.org/wiki?curid=41216691", "title": "David Vanderpool", "text": "David Vanderpool\n\nDavid Vanderpool (born February 18, 1960) is an American medical missionary and the CEO and founder of Live Beyond, which has provided medical, spiritual and logistical support to disaster ridden countries.\n\nVanderpool's work is unusual in that he combines his medical training with an explicit effort to convert his patients to Christianity.\n\nVanderpool was born in Dallas, Texas, graduated from St. Mark's School of Texas, and received his undergraduate degree from Abilene Christian University in 1982. He then attended the School of Medicine at Texas Tech University Health Sciences Center. After medical school, Vanderpool completed two surgical residencies at Baylor University Medical Center where he trained as a vascular surgeon.\n\nVanderpool wife, Laurie, is a speaker for Women's Retreats and Bible Studies, and speaks frequently for Down Syndrome organizations.\n\nVanderpool remained in Texas after residency and practiced as a vascular surgeon before moving to Brentwood, Tennessee in 2001 and opening his private practice Lave MD in 2003.\n\nDr. Vanderpool created Lave MD to act as both a medical facility and a spa. \nAfter the establishment of his international organization in 2005, Dr. Vanderpool used much of Lave MD's proceeds to fund the organization's efforts abroad.\n\nIn 2005, after Hurricane Katrina hit the southeastern part of the United States, Dr. Vanderpool delivered healthcare across the Mississippi Coast out of a trailer. His goal was to provide as much free healthcare as possible while the medical infrastructure could recover. Months later, Dr. Vanderpool established his organization, Medical Mobile Disaster Relief, as a 501(c)(3) non-profit organization with goals to provide disaster relief through medical clinics, clean water projects, and micro-finance projects to areas hit by disasters. \nVanderpool and the Mobile Medical Disaster Relief team began working in Mozambique in 2006. His goal was to provide healthcare to the indigenous people of the country in addition to enhancing the economy by implementing micro-finance projects among widows living in the communities. By 2008, Dr. Vanderpool teamed up with the Belmont School of Nursing to construct a nursing curriculum that could teach the Mozambique women to be self-sufficient in caring for themselves and their children.\n\nVanderpool and his Mobile Medical Disaster Relief partnered with PACODEP (Partners in Community Development) in Ghana in order to provide medical care, educate the locals on water purification and distribute water purifiers. PACODEP works to free enslaved children who are trafficked through Ghana for purposes of fishing work. Dr. Vanderpool has also partnered with local hospitals in Ghana in order to provide free invasive surgeries to these rescued children.\n\nDr. Vanderpool partnered with Mission Lazarus in 2009 to build a sustainable medical clinic in Cedeño, Honduras. Given sufficient medical supplies and equipment, Vanderpool allowed Mission Lazarus to take over the clinic and provision of healthcare for the people of Cedeño.\n\nDr. Vanderpool shifted his focus in the aftermath of the earthquake that devastated the entire country of Haiti in 2010. In 2010, Vanderpool officially changed the name of his organization from Mobile Medical Disaster Relief to Live Beyond with a stated mission to be \"an organization that chooses to Live Beyond...ourselves, our culture, our borders and this life so that others can Live Beyond…disease, hunger, poverty and despair.\"\n\nThe initial aid Vanderpool and his team brought to Haiti was primarily mobile medical care to relieve the thousands devastated and injured by the earthquake. Since then however, Vanderpool has grounded his missionary work in Thomazeau, Haiti where his organization began building a base. Vanderpool continued to provide free medical care, establishing a surgical hospital and clinic. In addition, clean water projects, orphanages, and other widow and orphan advocacy projects were begun.\n\nDr. Vanderpool is a Christian, who combines religion and the spread of his faith with his medical work. In Haiti, he has made it one of his objectives to bring Haiti away from its traditional voodoo culture and provide \"spiritual guidance\" to the Haitians in the role of Christianity, with the belief that Christianity will lead to a better Haiti. Prior to moving to Haiti, each medical outreach trip made by Vanderpool and other Live Beyond participants included prayer and Christian ministry along with healthcare to the voodoo priests, island chiefs, idol worshipers and the sick and dying in Haiti. On the Live Beyond site, Vanderpool's religious impact since being in Haiti has been characterized as leading to the baptism of dozens, the saving of tribes through \"Bibles being read in their own languages\" and \"the Kingdom is being expanded.\" Dr. Vanderpool promotes religious missionary work in tandem with his medical relief and sustainable development efforts. A worship center is being built in Thomazeau, Haiti and monthly mission trips are promoted and scheduled for Americans to travel to Thomazeau and volunteer.\n\n"}
{"id": "275715", "url": "https://en.wikipedia.org/wiki?curid=275715", "title": "Digital Radio Mondiale", "text": "Digital Radio Mondiale\n\nDigital Radio Mondiale (DRM; \"mondiale\" being Italian and French for \"worldwide\") is a set of digital audio broadcasting technologies designed to work over the bands currently used for analogue radio broadcasting including AM broadcasting, particularly shortwave, and FM broadcasting. DRM is more spectrally efficient than AM and FM, allowing more stations, at higher quality, into a given amount of bandwidth, using various MPEG-4 audio coding formats.\n\nDigital Radio Mondiale is also the name of the international non-profit consortium that has designed the platform and is now promoting its introduction. Radio France Internationale, TéléDiffusion de France, BBC World Service, Deutsche Welle, Voice of America, Telefunken (now Transradio) and Thomcast (now Ampegon) took part at the formation of the DRM consortium.\n\nThe principle of DRM is that bandwidth is the limited element, and computer processing power is cheap; modern CPU-intensive audio compression techniques enable more efficient use of available bandwidth, at the expense of processing resources.\n\nDRM can deliver up to FM-comparable sound quality on frequencies below 30 MHz (long wave, medium wave and short wave), which allow for very-long-distance signal propagation. The modes for these lower frequencies are often collectively known under the term \"DRM30\". In the VHF bands, the term \"DRM+\" is used. DRM+ is able to use available broadcast spectra between 30 and 300 MHz; generally this means band I (47 to 68 MHz), band II (87.5 to 108 MHz) and band III (174 to 230 MHz). DRM has been designed to be able to re-use portions of existing analogue transmitter facilities such as antennas, feeders, and, especially for DRM30, the transmitters themselves, avoiding major new investment. DRM is robust against the fading and interference which often plague conventional broadcasting in these frequency ranges.\n\nThe encoding and decoding can be performed with digital signal processing, so that a cheap embedded computer with a conventional transmitter and receiver can perform the rather complex encoding and decoding.\n\nAs a digital medium, DRM can transmit other data besides the audio channels (datacasting) — as well as RDS-type metadata or program-associated data as Digital Audio Broadcasting (DAB) does. DRM services can be operated in many different network configurations, from a traditional AM one-service one-transmitter model to a multi-service (up to four) multi-transmitter model, either as a single-frequency network (SFN) or multi-frequency network (MFN). Hybrid operation, where the same transmitter delivers both analogue and DRM services simultaneously is also possible.\n\nDRM incorporates technology known as Emergency Warning Features that can override other programming and activates radios which are in standby in order to receive emergency broadcasts.\n\nThe technical standard is available free-of-charge from the ETSI, and the ITU has approved its use in most of the world. Approval for ITU region 2 is pending amendments to existing international agreements. The inaugural broadcast took place on June 16, 2003, in Geneva, Switzerland, at the ITU's World Radio Conference.\n\nCurrent broadcasters include All India Radio, BBC World Service, BitExpress, Radio Exterior de España, Radio New Zealand International, Vatican Radio, Radio Romania International and Radio Kuweit.\n\nUntil now DRM receivers have typically used a personal computer. A few manufacturers have introduced DRM receivers which have thus far remained niche products due to limited choice of broadcasts. It is expected that the transition of national broadcasters to digital services on DRM, notably All India Radio, will stimulate the production of a new generation of affordable, and efficient receivers.\n\nChengdu NewStar Electronics is offering the DR111 from May 2012 on which meets the minimum requirements for DRM receivers specified by the DRM consortium and is sold worldwide.\n\nThe General Overseas Service of All India Radio broadcasts daily in DRM to Western Europe on 9.95 MHz at 17:45 to 22:30 UTC. All India Radio is in the process of replacing and refurbishing many of its domestic AM transmitters with DRM. The project which began in 2012 is scheduled to complete during 2015.\n\nThe British Broadcasting Corporation BBC has trialed the technology in the United Kingdom by broadcasting BBC Radio Devon in the Plymouth area in the MF band. The trial lasted for a year (April 2007 – April 2008). \nThe BBC also trialed DRM+ in the FM band in 2010 from the Craigkelly transmitting station in Fife, Scotland, over an area which included the city of Edinburgh. In this trial, a previously used 10 kW (ERP) FM transmitter was replaced with a 1 kW DRM+ transmitter in two different modes and the coverage compared with FM Digital Radio Mondiale was included in the 2007 Ofcom consultation on the future of radio in the United Kingdom for the AM medium wave band.\n\nRTÉ has also run single and multiple programme overnight tests during a similar period on the 252 kHz LW transmitter in Trim, Co.Meath, Ireland which was upgraded to support DRM after Atlantic 252 closed.\n\nThe Fraunhofer Institute for integrated circuits IIS offers a package for software defined radios which can be licensed to radio manufacturers. \n\nOn 28 September 2006, the Australian spectrum regulator, the Australian Communications and Media Authority, announced that it had \"placed an embargo on frequency bands potentially suitable for use by broadcasting services using Digital Radio Mondiale until spectrum planning can be completed\" \"those bands being \"5,950–6,200; 7,100–7,300; 9,500–9,900; 11,650–12,050; 13,600–13,800; 15,100–15,600; 17,550–17,900; 21,450–21,850 and 25,670–26,100 kHz.\n\nThe United States Federal Communications Commission states in that: \"For digitally modulated emissions, the Digital Radio Mondiale (DRM) standard shall be employed.\" Part 73, section 758 is for HF broadcasting only.\n\nUseful bitrates for DRM30 range from 6.1 kbit/s (Mode D) to 34.8 kbit/s (Mode A) for a 10 kHz bandwidth (±5 kHz around the central frequency). It is possible to achieve bit rates up to 72 kbit/s (Mode A) by using a standard 20 kHz (±10 kHz) wide channel. (For comparison, pure digital HD Radio can broadcast 20 kbit/s using channels 10 kHz wide and up to 60 kbit/s using 20 kHz channels.) Useful bitrate depends also on other parameters, such as:\n\nWhen DRM was originally designed, it was clear that the most robust modes offered insufficient capacity for the then state-of-the-art audio coding format MPEG-4 HE-AAC (High Efficiency Advanced Audio Coding). Therefore, the standard launched with a choice of three different audio coding systems (source coding) depending on the bitrate:\n\nHowever, with the development of MPEG-4 xHE-AAC, which is an implementation of MPEG Unified Speech and Audio Coding, the DRM standard was updated and the two speech-only coding formats, CELP and HVXC, were replaced. USAC is designed to combine the properties of a speech and a general audio coding according to bandwidth constraints and so is able to handle all kinds of programme material. Given that there were few CELP and HVXC broadcasts on-air, the decision to drop the speech-only coding formats has passed without issue.\n\nMany broadcasters still use the HE-AAC coding format because it still offers an acceptable audio quality, somewhat comparable to FM broadcast at bitrates above about 15 kbit/s. However, it is anticipated that in future, most broadcasters will adopt xHE-AAC.\n\nAdditionally, as of v2.1, the popular Dream software can broadcast using the Opus coding format. Whilst not within the current DRM standard the inclusion of this codec is provided for experimentation. Aside from perceived technical advantages over the MPEG family such as low latency (delay between coding and decoding), this codec provides an open source (therefore free to use) alternative to the proprietary MPEG family whose use is permitted at the discretion of the patent holders, unfortunately it has a substantially lower audio quality than xHE-AAC on low bitrates, which are a key to conserve bandwidth. In fact, at 8Kbps Opus actually sounds worse than analog shortwave radio. A video showing the comparison between Opus and xHE-AAC is available here. Equipment manufacturers currently pay royalties for incorporating the MPEG codecs.\n\nDRM broadcasting can be done using a choice of different bandwidths:\n\n\nThe modulation used for DRM is coded orthogonal frequency division multiplexing (COFDM), where every carrier is modulated with quadrature amplitude modulation (QAM) with a selectable error coding.\n\nThe choice of transmission parameters depends on signal robustness wanted and propagation conditions. Transmission signal is affected by noise, interference, multipath wave propagation and Doppler effect.\n\nIt is possible to choose among several error coding schemes and several modulation patterns: 64-QAM, 16-QAM and 4-QAM. OFDM modulation has some parameters that must be adjusted depending on propagation conditions. This is the carrier spacing which will determine the robustness against Doppler effect (which cause frequencies offsets, spread: Doppler spread) and OFDM guard interval which determine robustness against multipath propagation (which cause delay offsets, spread: delay spread). The DRM consortium has determined four different profiles corresponding to typical propagation conditions:\n\nThe trade-off between these profiles stands between robustness, resistance in regards to propagation conditions and useful bit rates for the service. This table presents some values depending on these profiles. The larger the carrier spacing, the more the system is resistant to Doppler effect (Doppler spread). The larger the guard interval, the greater the resistance to long multipath propagation errors (delay spread).\n\nThe resulting low-bit rate digital information is modulated using COFDM. It can run in simulcast mode by switching between DRM and AM, and it is also prepared for linking to other alternatives (e.g., DAB or FM services).\n\nDRM has been tested successfully on shortwave, mediumwave (with 9 as well as 10 kHz channel spacing) and longwave.\nThere is also a lower bandwidth two-way communication version of DRM as a replacement for SSB communications on HF - note that it is \"not\" compatible with the official DRM specification. It may be possible in some future time for the 4.5 kHz bandwidth DRM version used by the Amateur Radio community to be merged with the existing DRM specification.\n\nThe Dream software will receive the commercial versions and also limited transmission mode using the FAAC AAC encoder.\n\nError coding can be chosen to be more or less robust.\n\nThis table shows an example of useful bitrates depending on protection classes\n\nThe lower the protection class the higher the level of error correction.\n\nWhile the initial DRM standard covered the broadcasting bands below 30 MHz, the DRM consortium voted in March 2005 to begin the process of extending the system to the VHF bands up to 108 MHz.\n\nOn 31 August 2009, DRM+ (Mode E) became an official broadcasting standard with the publication of the technical specification by the European Telecommunications Standards Institute; this is effectively a new release of the whole DRM spec with the additional mode permitting operation above 30 MHz up to 174 MHz.\n\nWider bandwidth channels are used, which allows radio stations to use higher bit rates, thus providing higher audio quality. A 100 kHz DRM+ channel has sufficient capacity to carry one low-definition 0.7 megabit/s wide mobile TV channel: it would be feasible to distribute mobile TV over DRM+ rather than DMB or DVB-H. However, DRM+ (DRM Mode E) as designed and standardized only provides bitrates between 37.2 and 186.3 kbit/s depending on robustness level, using 4-QAM or 16-QAM modulations and 100 kHz bandwidth.\n\nDRM+ has been successfully tested in all the VHF bands, and this gives the DRM system the widest frequency usage; it can be used in band I, II and III. DRM+ can coexist with DAB in band III. but also the present FM-band can be utilized. The ITU has published three recommendations on DRM+, known in the documents as Digital System G. This indicate the introduction of the full DRM system (DRM 30 and DRM+).\nITU-R Rec. BS.1114 is the ITU recommendation for sound broadcasting in the frequency range 30 MHz to 3 GHz. DAB, HD-Radio and ISDB-T were already recommended in this document as Digital Systems A, C and F respectively.\n\nIn 2011, the paneuropean organisation Community Media Forum Europe has recommended to the European Commission that DRM+ should rather be used for small scale broadcasting (local radio, community radio) than DAB/DAB+.\n\n\n"}
{"id": "7257968", "url": "https://en.wikipedia.org/wiki?curid=7257968", "title": "Disease burden", "text": "Disease burden\n\nDisease burden is the impact of a health problem as measured by financial cost, mortality, morbidity, or other indicators. It is often quantified in terms of quality-adjusted life years (QALYs) or disability-adjusted life years (DALYs), both of which quantify the number of years lost due to disease (YLDs). One DALY can be thought of as one year of healthy life lost, and the overall disease burden can be thought of as a measure of the gap between current health status and the ideal health status (where the individual lives to old age free from disease and disability). According to an article published in \"The Lancet\" in June 2015, low back pain and major depressive disorder were among the top ten causes of YLDs and were the cause of more health loss than diabetes, chronic obstructive pulmonary disease, and asthma combined. The study based on data from 188 countries, considered to be the largest and most detailed analysis to quantify levels, patterns, and trends in ill health and disability, concluded that \"the proportion of disability-adjusted life years due to YLDs increased globally from 21.1% in 1990 to 31.2% in 2013.\" The environmental burden of disease is defined as the number of DALYs that can be attributed to environmental factors. These measures allow for comparison of disease burdens, and have also been used to forecast the possible impacts of health interventions. By 2014 DALYs per head were \"40% higher in low-income and middle-income regions.\"\n\nThe World Health Organization (WHO) has provided a set of detailed guidelines for measuring disease burden at the local or national level. In 2004, the health issue leading to the highest YLD for both men and women was unipolar depression; in 2010, it was lower back pain. According to an article in The Lancet published in November 2014, disorders in those aged 60 years and older represent \"23% of the total global burden of disease\" and leading contributors to disease burden in this group in 2014 were \"cardiovascular diseases (30.3%), malignant neoplasms (15.1%), chronic respiratory diseases (9.5%), musculoskeletal diseases (7.5%), and neurological and mental disorders (6.6%).\"\n\nIn 2004, the World Health Organization calculated that 1.5 billion disability-adjusted life years were lost to disease and injury.\n\nThe first study on the global burden of disease, conducted in 1990, quantified the health effects of more than 100 diseases and injuries for eight regions of the world, giving estimates of morbidity and mortality by age, sex, and region. It also introduced the DALY as a new metric to quantify the burden of diseases, injuries, and risk factors. From 2000–2002, the 1990 study was updated to include a more extensive analysis using a framework known as \"comparative risk factor assessment\".\n\nIn 2006, the WHO released a report which addressed the amount of global disease that could be prevented by reducing environmental risk factors. The report found that approximately one fourth of the global disease burden and more than one third of the burden among children was due to modifiable environmental factors. The \"environmentally-mediated\" disease burden is much higher in developing countries, with the exception of certain non-communicable diseases, such as cardiovascular diseases and cancers, where the per capita disease burden is larger in developed countries. Children have the highest death toll, with more than 4 million environmentally-caused deaths yearly, mostly in developing countries. The infant death rate attributed to environmental causes is also 12 times higher in developing countries. 85 out of the 102 major diseases and injuries classified by WHO were due to environmental factors.\n\nTo measure the environmental health impact, \"environment\" was defined as \"all the physical, chemical and biological factors external to a person, and all the related behaviours\". The definition of \"modifiable environment\" included:\n\nCertain environmental factors were excluded from this definition:\n\nThe WHO developed a methodology to quantify the health of a population using summary measures, which combine information on mortality and non-fatal health outcomes. The measures quantify either health gaps or health expectancies; the most commonly used health summary measure is the DALY.\n\nThe exposure-based approach, which measures exposure via pollutant levels, is used to calculate the environmental burden of disease. This approach requires knowledge of the outcomes associated with the relevant risk factor, exposure levels and distribution in the study population, and dose-response relationships of the pollutants.\n\nA dose-response relationship is a function of the exposure parameter assessed for the study population. Exposure distribution and dose-response relationships are combined to yield the study population's \"health impact distribution\", usually expressed in terms of incidence. The health impact distribution can then be converted into health summary measures, such as DALYs. Exposure-response relationships for a given risk factor are commonly obtained from epidemiological studies. For example, the disease burden of outdoor air pollution for Santiago, Chile, was calculated by measuring the concentration of atmospheric particulate matter (PM10), estimating the susceptible population, and combining these data with relevant dose-response relationships. A reduction of particulate matter levels in the air to recommended standards would cause a reduction of about 5,200 deaths, 4,700 respiratory hospital admissions, and 13,500,000 days of restricted activity per year, for a total population of 4.7 million.\n\nIn 2002, the WHO estimated the global environmental burden of disease by using risk assessment data to develop environmentally attributable fractions (EAFs) of mortality and morbidity for 85 categories of disease. In 2007, they released the first country-by-country analysis of the impact environmental factors had on health for its then 192 member states. These country estimates were the first step to assist governments in carrying out preventive action. The country estimates were divided into three parts:\n\n\nThe public health impacts of air pollution (annual means of PM10 and ozone), noise pollution, and radiation (radon and UV), can be quantified using DALYs. For each disease, a DALY is calculated as:\n\nNecessary data include prevalence data, exposure-response relationships, and weighting factors that give an indication of the severity of a certain disorder. When information is missing or vague, experts will be consulted in order to decide which alternative data sources to use. An uncertainty analysis is carried out so as to analyze the effects of different assumptions.\n\nWhen estimating the environmental burden of disease, a number of potential sources of error may arise in the measure of exposure and exposure-risk relationship, assumptions made in applying the exposure or exposure-risk relationship to the relevant country, health statistics, and, if used, expert opinions.\n\nGenerally, it is not possible to estimate a formal confidence interval, but it is possible to estimate a range of possible values the environmental disease burden may take based on different input parameters and assumptions. When more than one definition has to be made about a certain element in the assessment, multiple analyses can be run, using different sets of definitions. Sensitivity and decision analyses can help determine which sources of uncertainty affect the final results the most.\n\nIn the Netherlands, air pollution is associated with respiratory and cardiovascular diseases, and exposure to certain forms of radiation can lead to the development of cancer. Quantification of the health impact of the environment was done by calculating DALYs for air pollution, noise, radon, UV, and indoor dampness for the period 1980 to 2020. In the Netherlands, 2–5% of the total disease burden in 2000 could be attributed to the effects of (short-term) exposure to air pollution, noise, radon, natural UV radiation, and dampness in houses. The percentage can increase to up to 13% due to uncertainty, assuming no threshold.\n\nAmong the investigated factors, long-term PM10 exposure have the greatest impact on public health. As levels of PM10 decrease, related disease burden is also expected to decrease. Noise exposure and its associated disease burden is likely to increase to a level where the disease burden is similar to that of traffic accidents. The rough estimates do not provide a complete picture of the environmental health burden, because data are uncertain, not all environmental-health relationships are known, not all environmental factors have been included, and it was not possible to assess all potential health effects. The effects of a number of these assumptions were evaluated in an uncertainty analysis.\n\nExposure to environmental hazards may cause chronic diseases, so the magnitude of their contribution to the Canada's total disease burden is not well understood. In order to give an initial estimate of the environmental burden of disease for four major categories of disease, the developed by the WHO, EAFs developed by other researchers, and data from Canadian public health institutions were used. Results showed a total of 10,000–25,000 deaths, with 78,000–194,000 hospitalizations; 600,000–1.5 million days spent in hospital; 1.1–1.8 million restricted activity days for sufferers of asthma; 8000–24,000 new cases of cancer; 500–2,500 babies with low birth weights; and C$3.6–9.1 billion in costs each year due to respiratory disease, cardiovascular illness, cancer, and congenital affliction associated with adverse environmental exposures.\n\nThere is no consensus on the best measures of the public’s health. This is not surprising because measurements are used to accomplish diverse functions (e.g., population health assessment, evaluation of the effectiveness of interventions, formulation of health policies, and projection of future resource need). The choice of measures may also depend on individual and societal values. Measures that only consider premature death will omit the burden of living with a disease or disability, and measures that combine both in a single measure (i.e. DALYs) need to make a judgment to the significance of these measures compared to each other. Other metrics such as economic costs will not capture pain and suffering or other broader aspects of burden.\n\nDALYs are a simplification of a complex reality, and therefore only give a crude indication of environmental health impact. Relying on DALYs may make donors take a narrow approach to health care programs. Foreign aid is most often directed at diseases with the highest DALYs, ignoring the fact that other diseases, despite having lower DALYs, are still major contributors to disease burden. Less-publicized diseases thus have little or no funding for health efforts. For example, maternal death (one of the top three killers in most poor countries) and pediatric respiratory and intestinal infections maintain a high disease burden, and safe pregnancy and the prevention of coughs in infants do not receive adequate funding.\n\n"}
{"id": "21316814", "url": "https://en.wikipedia.org/wiki?curid=21316814", "title": "Earth System Governance Project", "text": "Earth System Governance Project\n\nThe Earth System Governance Project is a long-term, interdisciplinary social science research programme originally developed under the auspices of the International Human Dimensions Programme on Global Environmental Change. It started in January 2009.\n\nThe Earth System Governance Project currently consists of a network of ca. 300 active and about 2,300 indirectly involved scholars from all continents. The project has evolved into the largest social science research network in the area of governance and global environmental change. The Earth System Governance Project Office is hosted at Lund University, Sweden.\n\nThe Earth System Governance Project aims to contribute to science on the large, complex challenges of governance in an era of rapid and large-scale environmental change. The project seeks to create a better understanding of the role of institutions, organizations and governance mechanisms by which humans regulate their relationship with the natural environment. The Earth System Governance Project aims to integrate governance research at all levels. The project aims to examine problems of the ‘global commons’, but also local problems from air pollution to the preservation of waters, waste treatment or desertification and soil degradation. However, due to natural interdependencies local environmental pollution can be transformed into changes of the global system that affect other localities. Therefore, the Earth System Governance Project looks at institutions and governance processes both local and globally.\n\nThe Earth System Governance Project is a scientific effort, but also aims to assist policy responses to the pressing problems of earth system transformation \n\nThe Earth System Governance Project organizes its research according to a conceptual framework guided by five analytical problems. These are the problems of the overall \"architecture\" of earth system governance, of \"agency\" beyond the state and of the state, of the \"adaptiveness\" of governance mechanisms and processes, of their \"accountability\" and legitimacy and of modes of \"allocation and access\" in earth system governance.\n\nThe concept of Earth System Governance is defined as: \n\nIn 2001, the four then active global change research programmes (DIVERSITAS, International Geosphere-Biosphere Programme, World Climate Research Programme, and International Human Dimensions Programme on Global Environmental Change) agreed to intensify co-operation through setting up an overarching Earth System Science Partnership. The research communities represented in this Partnership contend in the 2001 Amsterdam Declaration on Global Change that the earth system now operates ‘well outside the normal state exhibited over the past 500,000 years’ and that ‘human activity is generating change that extends well beyond natural variability—in some cases, alarmingly so— and at rates that continue to accelerate.’ To cope with this challenge, the four global change research programmes have called ‘urgently’ for strategies for Earth System management’.\n\nIn March 2007, in response to the 2001 Amsterdam Declaration, the Scientific Committee of the International Human Dimensions Programme on Global Environmental Change (IHDP), the overarching social science programme in the field, mandated the drafting of the Science Plan of the Earth System Governance Project by a newly appointed Scientific Planning Committee. The Earth System Governance Project builds on the results of an earlier long-term research programme, the IHDP core project Institutional Dimensions of Global Environmental Change (IDGEC). In 2008, the Earth System Governance Project was officially launched. \n\nIn 2009, the Science and Implementation Plan of the Earth System Governance Project was published. In the science and implementation plan, the conceptual problems, cross-cutting themes, flagship projects, and its policy relevance are outlined in detail. The Science Plan was written by an international, interdisciplinary Scientific Planning Committee chaired by Prof. Frank Biermann, which drew on a consultative process that started in 2004. Several working drafts of this Science Plan have been presented and discussed at a series of international events and conferences, and numerous scholars in the field, as well as practitioners, have offered suggestions, advice, and critique.\n\nSince then, the project has evolved into a broader research alliance that builds on an international network of research centers, lead faculty and research fellows. After the termination of the IHDP in 2014, the activities of the Earth System Governance research alliance are supported by an international steering group of representatives of the main Earth System Governance Research Centres and the global group of lead faculty and research fellows.\n\nFor its activities and implementation, the Earth System Governance Project relies on a global network of experts from different academic and cultural backgrounds. The research network consists of different groups of scientific experts. The Earth System Governance Project operates under the direction of a \"Scientific Steering Group\" chaired by Frank Biermann. The role of the Scientific Steering Committee is to guide the implementation of the Earth System Governance Science Plan. The \"Lead Faculty\" of the Earth System Governance Project is a group of individual scientists who take over (shared) responsibility for the development of research on particular analytical problems. \"Earth System Governance Fellows\" are scientists who link their own research projects with the broader themes and questions raised by the Earth System Governance Science and Implementation Plan.\n\nAn important element in the project organisation is the global alliance of research centres that brings together the VU University Amsterdam; the Australian National University; Chiang Mai University; Colorado State University; Lund University; University of East Anglia; University of Oldenburg; the Stockholm Resilience Centre; the University of Toronto; the Tokyo Institute of Technology and Yale University. In addition, strong networks on earth system governance research exist in China, Latin America, Central and Eastern Europe, and Russia.\n\nSince 2007, the Project has organized major scientific conferences addressing the topics of governance and global environmental change, including:\n\n\nThe network of researchers affiliated with the Earth System Governance Project has brought out many reports and books, and has published in journals such as International Environmental Agreements: Politics, Law and Economics; Ecological Economics; Global Environmental Change; Environmental Science & Policy Global Environmental Politics and Current Opinion in Environmental Sustainability Recurring research topics of the Earth System Governance Project are water governance, climate governance and fragmentation of global environmental governance.\n\nA related MIT Press Book series is designed to address the research challenge of earth system governance. Additionally, the Project publishes regular Working Papers, which are peer-reviewed online publications that broadly address questions raised by the Project’s Science and Implementation Plan.\n\nSeveral special issues of topics related to earth system governance have been published in scientific journals over the last years.\n\nEarth system governance as a research object is quickly emerging, and as a consequence, the number of education programmes on bachelor, master and doctoral level related to earth system governance steadily increases. A number of institutes and universities currently collaborate in a Global Alliance of Earth System Governance Research Centres, including:\n\n\nA substantial number of the workshops and other events of the project are capacity-building activities. The project also organizes, endorses and provides teaching to summer schools and capacity building events and programs. In addition, members of the Scientific Steering Group and staff of the International Project Office give guest lectures around the world.\n\nThe Earth System Governance Project organizes Task Forces, international networks of senior and early career scholars with a series of working groups focused on particular ideas or idea clusters. There are currently two active Task Forces:\n\nThis Task Force aims to explore key concepts with regard to Earth System Governance, such as planetary boundaries, green economy, resilience and the Anthropocene. It aims to critically examine and further refine these novel governance ideas. \n\nThis Task Force seeks aims to advance quantitative earth system governance research by promoting new international research collaborations, fostering interaction and dialogue among existing research projects, and developing architectures to promote the building and sharing of datasets.\n\nIn 2011, the Earth System Governance Project launched an initiative on International Environmental Governance. This initiative aims to provide a forum for discussion of current and ongoing research on international environmental governance and the institutional framework for sustainable development, in the period leading up to the 2012 United Nations Conference on Sustainable Development in Rio de Janeiro, also known as ‘Rio + 20’. In addition, the initiative aims to target decision-makers and to contribute not just to a better understanding but also to actual improvements in international environmental governance towards an institutional framework that enables sustainable development.\n\nThere is widespread support for the Earth System Governance Project in the scientific community, which is reflected in the size of the research network and in various publications by experts. However, criticisms of the Earth System Governance Project have also been made. \n\nIn an internal report of the International Human Dimensions Programme it is stated that the steering group of the Earth System Governance Project is too much dominated by experts from OECD countries. Since then, the Earth System Governance Project has actively sought ways to involve experts from different regions of the world. \n\nThe idea of earth system governance has also been criticized for being too top-down, for placing too much emphasis on global governance structures. According to Mike Hulme, earth system governance represents an attempt to ‘geopolitically engineer’ our way out of the climate crisis. He questions whether the climate is governable and argues that it is way too optimistic and even hubristic to attempt to control the global climate by universal governance regimes. This interpretation of the novel concept, however, has been rejected by other scholars as being too narrow and misleading \n\n\n\n"}
{"id": "41362614", "url": "https://en.wikipedia.org/wiki?curid=41362614", "title": "FIG World Rankings", "text": "FIG World Rankings\n\nThe FIG World Rankings is a system determining a list of the World's best gymnasts competing on the Artistic Gymnastics World Cup. A separate World ranking system is also used for Rhythmic Gymnastics.\n\nThroughout the year, there are Challenge and Challenger World Cups for CII (all-around) and CIII (events) formats. A win on each event is a maximum of 50 points for a World Cup competition and 30 points for a World Challenge Cup competition.\n\nNote: Only the top three scores counted per gymnast.\n\nNote: Only the top three scores counted per gymnast.\n"}
{"id": "44023814", "url": "https://en.wikipedia.org/wiki?curid=44023814", "title": "German Parliamentary Committee investigation of the NSA spying scandal", "text": "German Parliamentary Committee investigation of the NSA spying scandal\n\nThe German Parliamentary Committee investigation of the NSA spying scandal (official title: \"1. Untersuchungsausschuss „NSA“\") was started on March 20, 2014, by the German Parliament in order to investigate the extent and background of foreign secret services spying in Germany in the light of the Global surveillance disclosures (2013–present).\nThe Committee is also in search of strategies on how to protect telecommunication with technical means.\n\nThe committee is formed by eight members of the German Parliament. The parliamentarian of the Christian Democratic Union (CDU) Clemens Binninger was head of the committee but stepped down after six days. In a statement, Binninger clarified that the other committee members had insisted on inviting Edward Snowden to testify; Binninger objected to this and resigned in protest. Patrick Sensburg (CDU) succeeded him. \n\nOn May 8, 2014, the committee unanimously decided to let US whistleblower Edward Snowden testify as a witness.\n\nOn September 23, 2014, the Green Party and The Left filed a constitutional complaint against the Christian Democratic Union, the Social Democrats and the NSA Parliamentary Committee because of the Christian Democrats' and the Social Democrats' refusal to let the witness Edward Snowden testify in Berlin. The accused proposed a video conference from Moscow which Snowden had refused.\n\nOn September 28, 2014, the Green Party and The Left filed a constitutional complaint against German chancellor Merkel. According to them, she refuses to comply with her duty according to Chapter 44 of the German constitution to ensure a real investigation; especially by refusing to ensure the legal requirements to allow the witness Edward Snowden to testify.\n\nOn July 3, 2014, the former Technical Director of the NSA, William Binney, who had become a whistleblower after the terrorist attacks of September 11, 2001, testified to the committee. He said that the NSA has a totalitarian approach that has previously only been known from dictatorships and that there is no longer such a thing as privacy. Former NSA employee Thomas Andrews Drake described the close cooperation between the NSA and the German foreign intelligence service BND.\n\nThe US journalist Glenn Greenwald was asked to testify in September, 2014. On August 1, 2014, he wrote in a letter that he was willing to support the Parliament's investigation on the espionage in Germany by the NSA. By their refusal to let the crucial witness Edward Snowden testify, German politicians had however shown that it is more important to them not to annoy the US than to really investigate and he was not willing to take part in a \"ritual\" that \"shall seem like an investigation\".\n\nIn Operation Eikonal German BND agents received \"Selector Lists\" from the NSA − search terms for their dragnet surveillance. They contain IP addresses, mobile phone numbers and email accounts with the BND surveillance system containing hundreds of thousands and possibly more than a million such targets. These lists have been subject of controversy as in 2008 it was revealed that they contained some terms targeting the European Aeronautic Defence and Space Company (EADS), the Eurocopter project as well as French administration, which were first noticed by BND employees in 2005. Other selectors were found to target the administration of Austria. After the revelations made by whistleblower Edward Snowden the BND decided to investigate the issue whose October 2013 conclusion was that at least 2,000 of these selectors were aimed at Western European or even German interests which has been a violation of the Memorandum of Agreement that the US and Germany signed in 2002 in the wake of the 9/11 terror attacks. After reports emerged in 2014 that EADS and Eurocopter had been surveillance targets the Left Party and the Greens filed an official request to obtain evidence of the violations.\n\nThe investigative Parliamentary Committee was set up in spring 2014 and reviewed the selectors and discovered 40,000 suspicious search parameters, including espionage targets in Western European governments and numerous companies. The group also confirmed suspicions that the NSA had systematically violated German interests and concluded that the Americans could have perpetrated economic espionage directly under the Germans' noses. The investigative parliamentary committee was not granted access to the NSA's selectors list as an appeal led by opposition politicians failed at Germany's top court - instead the ruling coalition appointed an administrative judge, Kurt Graulich, as a \"person of trust\" who was granted access to the list and briefed the investigative commission on its contents after analyzing the 40,000 parameters. In his almost 300-paged report Graulich concluded that European government agencies were targeted massively and that Americans hence broke contractual agreements. He also found that German targets which received special protection from surveillance of domestic intelligence agencies by Germany's Basic Law (Grundgesetz) − including numerous enterprises based in Germany − were featured in the NSA's wishlist in a surprising plenitude. While the magnitude differs there have also been problematic BND-internal selectors which have been used until end of 2013 - around two thirds of 3300 targets were related to EU and NATO states. Klaus Landefeld, member of the board at the Internet industry association Eco International, has met intelligence officials and legislators to present suggestions for improvement, like streamlining the selector system.\nOn July 4, 2014, it was revealed to the public that BND agent Markus R. had been arrested on July 2, 2014, for apparently having spied. The 31-year-old German is accused of having worked for the CIA. After his arrest, the US ambassador John B. Emerson was summoned for talks at the German Foreign Office.\n\nIt was revealed that the BND-agent had saved 218 secret BND documents on USB sticks since 2012 and sold them to US agents for a sum of 25,000 Euro in Salzburg, Austria. At least three of the documents were about the NSA Parliamentary Committee.\nThe Federal Office for the Protection of the Constitution had mistaken him for a Russian spy and asked US colleagues to help uncover him.\n\nOn July 9, 2014, a second US spy was discovered, who worked for the Secretary of Defense.\n\nIn July 2014 a Parliament technician found out that the mobile phone of Roderich Kiesewetter, representative of the Christian Democratic Union in the committee, was spied on. Kiesewetter said there is evidence that all four Party representatives in the committee have been spied on.\n\nIn the months following May 2015, Peter Pilz, a member of the Austrian parliament for the Green Party, disclosed several documents and transcripts related to operation Eikonal, in which NSA and BND cooperated for tapping transit cables at a facility of Deutsche Telekom in Frankfurt. These documents were highly sensitive and handed over to the committee for investigating the Eikonal operation. Therefore, it seems likely someone from the committee leaked these documents to Pilz. Among them are lists of communication channels from many European countries, including most of Germany's neighbours. Peter Pilz also discovered NSA spying facilities in Austria, and therefore wants a parliamentary committee on the NSA spying in his own country too.\n\nOn December 1, 2016, WikiLeaks released over 2,400 documents which it claims are from the investigation.\n\n"}
{"id": "6716259", "url": "https://en.wikipedia.org/wiki?curid=6716259", "title": "Global Burden of Disease Study", "text": "Global Burden of Disease Study\n\nThe Global Burden of Disease Study (GBD) is a comprehensive regional and global research program of disease burden that assesses mortality and disability from major diseases, injuries, and risk factors. GBD is a collaboration of over 1,800 researchers from 127 countries. Under principal investigator Christopher J.L. Murray, GBD is based out of the Institute for Health Metrics and Evaluation (IHME) at the University of Washington and funded by the Bill and Melinda Gates Foundation.\n\nThe Global Burden of Disease Study began in 1990 as a single World Bank-commissioned study, now called GBD 1990. The original project quantified the health effects of more than 100 diseases and injuries for eight regions of the world, giving estimates of morbidity and mortality by age, sex, and region. It also introduced the disability-adjusted life year (DALY) as a new metric to quantify the burden of diseases, injuries, and risk factors, to aid comparisons. GBD 1990 was \"institutionalized\" at the World Health Organization (WHO) and the research was \"conducted mainly by researchers at Harvard and WHO\".\n\nIn 2000–2002, the 1990 study was updated by WHO to include a more extensive analysis using a framework known as \"comparative risk factor assessment\".\n\nThe WHO estimates were again updated for 2004 in \"The global burden of disease: 2004 update\" (published in 2008) and in \"Global health risks\" (published in 2009).\n\nOfficial DALY estimates had not been updated by WHO since 2004 until the Global Burden of Diseases, Injuries, and Risk Factors Study 2010 (GBD 2010), also known as the Global Burden of Disease Study 2010, was published in December 2012. The work quantified the burdens of 291 major causes of death and disability and 67 risk factors disaggregated by 21 geographic regions and various age–sex groups. GBD 2010 had the Institute for Health Metrics and Evaluation as its coordinating center, but was a collaboration between several institutions including WHO and the Harvard School of Public Health. The work was funded by the Gates Foundation. The GBD 2010 estimates contributed to WHO's own estimates published in 2013, although WHO did not acknowledge the GBD 2010 estimates.\n\nThe Global Burden of Disease Study 2013 (GBD 2013) was published in 2014. The first installment, \"Smoking Prevalence and Cigarette Consumption in 187 Countries, 1980–2012\", was published in the \"Journal of the American Medical Association\" in January, and further installments were published throughout the year. IHME continued to act as the coordinating center for the work.\n\nIn October 2016, Global Burden of Disease Study 2015 (GBD 2015) was published. The work was still coordinated at IHME.\n\nThe following table summarizes GBD's growth over the years.\n\nThe GBD has three specific aims:\n\n\nThe burden of disease can be viewed as the gap between current health status and an ideal situation in which everyone lives into old age free of disease and disability. Causes of the gap are premature mortality, disability and exposure to certain risk factors that contribute to illness.\n\nThe 2013 report showed that global life expectancy for both sexes increased from 65.3 years in 1990, to 71.5 years in 2013, while the number of deaths increased from 47.5 million to 54.9 million over the same interval. Progress varied widely across demographic and national groups. Reductions in age-standardised death rates for cardiovascular diseases and cancers in high-income regions, and reductions in child deaths from diarrhoea, lower respiratory infections and neonatal causes in low-income regions drove the changes. HIV/AIDS reduced life expectancy in southern sub-Saharan Africa.\n\nFor most communicable causes of death both numbers of deaths and age-standardised death rates fell, while for most non-communicable causes, demographic shifts increased numbers of deaths but decreased age-standardised death rates.\n\nGlobal deaths from injury increased by 10.7%, from 4.3 million deaths in 1990 to 4.8 million in 2013; but age-standardised rates declined over the same period by 21%. For some causes of more than 100,000 deaths per year in 2013, age-standardised death rates increased between 1990 and 2013, including HIV/AIDS, pancreatic cancer, atrial fibrillation and flutter, drug use disorders, diabetes, chronic kidney disease and sickle-cell anaemias. Diarrhoeal diseases, lower respiratory infections, neonatal causes and malaria remain in the top five causes of death in children younger than 5 years. The most important pathogens are rotavirus for diarrhoea and pneumococcus for lower respiratory infections.\n\nGBD 2015 found that for the first time, annual deaths from measles had fallen below 100,000 in 2013 and 2015. It also found that the global annual rate of new HIV infections has largely stayed the same during the past 10 years.\n\nGBD 2015 also introduced the Socio-demographic Index (SDI) as a measure of a location's socio-demographic development that takes into account average income per person, educational attainment, and total fertility rate.\n\nThe results of the Global Burden of Disease Study have been cited by \"The New York Times\", \"The Washington Post\", Vox, and \"The Atlantic\".\n\nThe World Health Organization did not acknowledge the GBD 2010 estimates.\n\nThe following is a table of GBD publications .\n\n\"GBD 2010\" proper means the paper was published as part of the original triple issue in \"The Lancet\".\n\n\n"}
{"id": "35306935", "url": "https://en.wikipedia.org/wiki?curid=35306935", "title": "Global Internet usage", "text": "Global Internet usage\n\nGlobal Internet usage refers to the number of people who use the Internet worldwide, which can be displayed using tables, charts, maps and articles which contain more detailed information on a wide range of usage measures.\n\nAs of June 2018, 55.1% of the world's population has internet access. In 2015, the International Telecommunication Union estimated about 3.2 billion people, or almost half of the world's population, would be online by the end of the year. Of them, about 2 billion would be from developing countries, including 89 million from least developed countries.\n\nThe Web Index is a composite statistic designed and produced by the World Wide Web Foundation. It provides a multi-dimensional measure of the World Wide Web’s contribution to development and human rights globally. It covers 86 countries as of 2014, the latest year for which the index has been compiled. It incorporates indicators that assess the areas of universal access, freedom and openness, relevant content, and empowerment, which indicate economic, social, and political impacts of the Web.\n\nThe Carna Botnet was a botnet of 420,000 devices created by hackers to measure the extent of the Internet in what the creators called the \"Internet Census of 2012\". \n\n\n"}
{"id": "33330773", "url": "https://en.wikipedia.org/wiki?curid=33330773", "title": "Global Map", "text": "Global Map\n\nGlobal Map is a set of digital maps that accurately cover the whole globe to express the status of global environment. It is developed through the cooperation of National Geospatial Information Authorities (NGIAs) in the world. An initiative to develop Global Map under international cooperation, the Global Mapping Project, was advocated in 1992 by Ministry of Construction, Japan (MOC) at the time (the current Ministry of Land, Infrastructure, Transport and Tourism, Japan-MLIT).\n\nGlobal Map is digital geospatial information in 1 km resolution which satisfies the following conditions:\nThe global geospatial information developed as Global Map mainly consists of the following thematic layers:\n\nSince the United Nations Conference on the Human Environment in 1972, global environmental challenges have been recognized as an issue which is common to humankind. “The United Nations Conference on Environment and Development (the Earth Summit)” in Brazil in 1992 adopted “An action plan of humankind for sustainable development: Agenda 21.” Agenda 21 describes in many parts the importance of information for decision-making to appropriately cope with global environmental challenges. Especially geospatial information is regarded to be critical.\n\nIn response to the objectives of Agenda 21 and in recognition of the need for further contribution to the development of geospatial information, the MOC at the time (the current MLIT) advocated, in the same year, the Global Mapping Project, an international cooperation initiative to develop global geospatial information to understand the present status and changes of global environment. This concept was presented at the Fifth United Nations Regional Cartographic Conference for the Americas in New York in 1993. At the same time, resolution\ncalling for the promotion of the development of global geospatial data was adopted at this conference. Following this conference, a similar resolution\nwas adopted at Thirteenth United Nations Regional Cartographic Conference for Asia and the Pacific in Beijing in 1994.\nIn 1996, International Steering Committee for Global Mapping (ISCGM), which consists of heads or its equivalents of NGIAs, was established to promote the Global Mapping Project. Thus the mechanism for the international promotion was formed. The Geospatial Information Authority of Japan (GSI) has been serving as the secretariat of the ISCGM. In the following year, in 1997, which was five years after the Earth Summit, the nineteenth special session of the United Nations General Assembly (19th UNGASS) was held. At the paragraph 112 of the resolution adopted by the 19th UNGASS, importance of a supportive environment to enhance national capacities and capabilities for information collection and processing, especially in developing countries, to facilitate public access to information on global environmental issues was mentioned along with the description mentioning the significance of international cooperation, including global mapping, as a means to develop the supportive environment.\n\nAs a result of such movement, In 1998, a recommendatory letter to participate in the Global Mapping Project was sent from the United Nations to NGIAs of respective countries in the world.\n\nFurther, at the World Summit on Sustainable Development (Johannesburg Summit) held in 2002, global mapping was included in the Plan of Implementation.\n\n"}
{"id": "38776060", "url": "https://en.wikipedia.org/wiki?curid=38776060", "title": "Global citizens movement", "text": "Global citizens movement\n\nIn most discussions, the global citizens movement is a socio-political process rather than a political organization or party structure. The term is often used synonymously with the anti-globalization movement or the global justice movement.\n\n\"Global citizens movement\" has been used by activists to refer to a number of organized and overlapping citizens groups who seek to influence public policy often with the hope of establishing global solidarity on an issue. Such efforts include advocacy on ecological sustainability, corporate responsibility, social justice, and similar progressive issues.\n\nIn theoretical discussions of social movements, global citizens movement refers to a complex and unprecedented phenomena made possible by the unique subjective and objective conditions of the planetary phase of civilization. The term is used to distinguish the latent potential for a profound shift in values among an aware and engaged citizenry from existing transnational citizens movements which tend to focus on specific issues (such as the anti-war movement or the labor movement).\n\nThe concept of global citizenship first emerged among the Greek Cynics in the 4th Century BCE, who coined the term “cosmopolitan” – meaning \"citizen of the world\". The Stoics later elaborated on the concept. The contemporary concept of cosmopolitanism, which proposes that all individuals belong to a single moral community, has gained a new salience as scholars examine the ethical requirements of the planetary phase of civilization.\n\nThe idea that today’s objective and subjective conditions have increased the latency for an emergent global civic identity has been argued by the authors of the Global Scenario Group’s final report \"Great Transition: the Promise and Lure of the Times Ahead\". Similar arguments for the existence of a latent pool of tens of millions of people ready to identify around new values of earth consciousness have been put forth by such authors as Paul Raskin, Paul H. Ray, and David Korten. Organizations, such as Oxfam International believe that a global citizens movement rooted in social and economic justice is emerging and is necessary for ending global poverty.\n\nIn the last chapter of his book \"Red Sky at Morning\", Gus Speth describes the potential for a new type of social movement composed of \"we the people, as citizens\" rooted in the principles of the Earth Charter to lead the transition in consciousness and values necessary for the emergence of a new planetary civilization.\n\nOrion Kriegman, author of \"Dawn of the Cosmopolitan: The Hope of a Global Citizens Movement\", states, “Transnational corporations, governments, and non-governmental organizations (NGOs) remain powerful global actors, but all of these would be deeply influenced by a coherent, worldwide association of millions of people who call for priority to be placed on new values of quality of life, human solidarity, and environmental sustainability.”\n\nKriegman distinguishes this \"coherent, worldwide association of millions\" from the existing fragmented social movements active in the World Social Forum. These movements tend to be issue-specific – focused on labor, environment, human rights, feminist issues, indigenous struggles, poverty, AIDS, and numerous other interrelated but \"siloed\" efforts. Coherence among these movements would require a reframing of their work under the rubric of the struggle for a socially just and ecologically sustainable global society and the establishment of an institutional structure to defend the rights of humanity, future generations, and the biosphere.\n\nThe major critique of the notion of a global citizens movement centers on the potential for the emergence of solidarity on issues at the global level. Nationalism, racism, and the dominance of the Westphalian state system are considered antithetical to the adoption of a global civic identity. However, some scholars point out that the historical emergence of nationalism must have felt just as improbable in a time of warring city-states, and yet in retrospect it appears inevitable.\n\nA more radical critique stems from the arguments put forth by Michael Hardt and Antonio Negri in their book \"Multitude\" and enshrines Michel Foucault's notion of a “plurality of resistance” as the only legitimate path forward. This argument asserts that an organized movement among the vast multitude is both undesirable and impossible. Instead of leadership and organizational structures, Hardt and Negri put faith in the emergence of spontaneous coherence due to increasing self-organized networks among various autonomous resistance movements. They critique the notion that there could be legitimate leaders, democratically chosen through a formal network of grassroots structures, acting on behalf of a big-tent pluralistic association of global citizens to directly confront the entrenched power of transnational corporations and state governments. However, it remains unclear how a network of autonomous movements would differ in practice from the vision of an authentic global citizens movement.\n\n\n\n"}
{"id": "50835353", "url": "https://en.wikipedia.org/wiki?curid=50835353", "title": "Health systems strengthening", "text": "Health systems strengthening\n\nHealth systems strengthening (also health system strengthening, abbreviated HSS) is a term used in global health that roughly means to improve the health care system of a country. Within this general definition, it can mean increasing funding for health infrastructure, improving health policy, trying to achieve universal healthcare, or any number of other health measures.\n\nThere has been some effort to use a systems thinking approach to health systems strengthening.\n\nVarious health organizations have claimed to use health systems strengthening (while not necessarily agreeing on the definition). Some of these are:\n\n\nBoth the idea of health systems strengthening and the term itself have received attention.\n\nEven advocates of health systems strengthening admit that it can often seem like a \"distant, even abstract aim\".\n\nMarchal et al., writing in 2009, called the term \"vague\" and argued that \"most current HSS strategies are selective (i.e., they target a specific disease), and their effects may undermine progress towards the long-term goal of effective, high-quality, and inclusive health systems.\"\n\nPeter Berman, who was the lead health economist at the World Bank, has pointed out that \"Almost any support to health interventions can be considered HSS\".\n\n"}
{"id": "331039", "url": "https://en.wikipedia.org/wiki?curid=331039", "title": "Hong Kong–Zhuhai–Macau Bridge", "text": "Hong Kong–Zhuhai–Macau Bridge\n\nThe Hong Kong–Zhuhai–Macau Bridge (HZMB), officially the Hong Kong–Zhuhai–Macao Bridge, is a bridge–tunnel system consisting of a series of three cable-stayed bridges, an undersea tunnel, and four artificial islands. It is both the longest sea crossing and the longest fixed link on earth. The HZMB spans the Lingding and Jiuzhou channels, connecting Hong Kong, Macau, and Zhuhai—three major cities on the Pearl River Delta. \n\nThe HZMB was designed to last for 120 years and built with a cost of 126.9 billion yuan (US$ billion). The cost of constructing the main bridge was estimated at 51.1 billion yuan (US$ 7.56 billion) funded by bank loans and shared among the governments of mainland China, Hong Kong and Macau.\n\nOriginally set to be opened to traffic in late 2016, the structure was completed on 6 February 2018 and journalists were subsequently given rides over the bridge. On 24 October 2018, the HZMB was opened to the public after its inauguration a day earlier by Xi Jinping, General Secretary of the Communist Party of China and President of the People's Republic of China.\n\nHopewell Holdings founder and then-managing director Gordon Wu proposed the concept of a bridge-tunnel linking China, Hong Kong and Macau in the 1980s. Wu stated that he got the idea in 1983 from the Chesapeake Bay Bridge–Tunnel. In 1988, Wu pitched the concept to Guangdong and Beijing officials. He envisaged a link farther north than the current design, beginning at Black Point near Tuen Mun, Hong Kong and crossing the Pearl River estuary via Neilingding Island and Qi'ao Island. His proposed bridge would end at the Chinese village of Tangjia, and a new road would continue south through Zhuhai before terminating at Macau. Discussions stalled after the Tiananmen Square massacre in mid-1989 \"unnerved\" Wu and other foreign investors, and caused Hopewell's Hong Kong share prices to plunge.\n\nThe route proposed by Wu was promoted by the government of Zhuhai under the name Lingdingyang Bridge. In the mid-1990s, Zhuhai built a bridge between the mainland and Qi'ao Island that was intended as the first phase of this route, though the full scheme had not been approved by either the Chinese nor Hong Kong governments at that time. China's central government showed support for this project on 30 December 1997. The new Hong Kong government was reticent, stating that it was still awaiting cross-border traffic study results, and Hong Kong media questioned the environmental impact of the project with regard to air pollution, traffic and marine life.\n\nIn December 2001, the Legislative Council of Hong Kong passed a motion urging the Administration to develop the logistics industry including the construction of a bridge connecting Hong Kong, Zhuhai and Macao. At a meeting of the China/Hong Kong Conference on Co-ordination of Major Infrastructure Projects held on 20 September 2002, it was agreed that a joint study should be conducted on the transport link between Hong Kong and Pearl River West.\n\nTo coordinate the project, the Advance Work Coordination Group of HZMB was set up in 2003. Officials from three sides solved the issues such as landing points and alignments of the bridge, operations of the Border Crossing Facilities and project financing.\n\nIn August 2008, China's Central Government, the governments of Guangdong, Hong Kong and Macau agreed to finance 42% of the total costs. The remaining 58% consisted of loans (approximately 22 billion yuan or US$ 3.23 billion) from Bank of China.\n\nIn March 2009, it was further reported that China's Central Government, Hong Kong and Macau agreed to finance 22% of the total costs. The remaining 78% consisted of loans (approximately 57.3 billion yuan or US$ 8.4 billion) from a consortium of banks led by Bank of China.\n\nConstruction of the HZMB project began on 15 December 2009 on the Chinese side, with then-Vice Premier of China Li Keqiang holding a commencement ceremony. Construction of the Hong Kong section of the project began in December 2011 after a delay caused by a legal challenge regarding the environmental impact of the bridge.\n\nThe last bridge tower was erected on 2 June 2016, the last straighted-element of the straight section of the undersea tunnel was installed on 12 July 2016, while the final tunnel joint was installed on 2 May 2017. Construction of the Main Bridge, consisting of a viaduct and an undersea tunnel, was completed on 6 July 2017, and the construction of the whole project was completed on 6 February 2018.\n\nThe 55-km () HZMB consists of three main sections: the Main Bridge () in the middle of the Pearl River estuary, the Hong Kong Link Road () in the east and the Zhuhai Link Road () in the west of the estuary.\n\nThe Main Bridge, the largest part of the HZMB project, is a bridge-cum-tunnel system constructed by the mainland Chinese authorities. It connects an artificial island, housing the Boundary Crossing Facilities (BCF) for both mainland China and Macau in the west, to the Hong Kong Link Road in the east.\n\nThis section includes a 22.9-km () viaduct and a 6.7-km () undersea tunnel that runs between two artificial islands. The viaduct crosses the Pearl River estuary with three cable-stayed bridges spanning between 280 and 460 metres (920 and 1,510 ft), allowing shipping traffic to pass underneath.\n\nUnder Hong Kong jurisdiction, the Hong Kong Link Road was built by Highway Department to connect the Main Bridge to an artificial island housing the Hong Kong Boundary Crossing Facilities (HKBCF). This section includes a 9.4-km () viaduct, a 1-km () Scenic Hill Tunnel and a 1.6-km () at-grade road along the east coast of the Hong Kong International Airport.\n\nThe Zhuhai Link Road starts from an artificial island housing the Boundary Crossing Facilities for both mainland China and Macau, passes through the developed area of Gongbei via a tunnel towards Zhuhai, and connects to three major expressways, namely, the Jing-Zhu Expressway, Guang-Zhu West Expressway and Jiang-Zhu Expressway.\n\nAlthough the HZMB connects two left-hand traffic (LHT) areas, namely Hong Kong and Macau, the crossing itself is right-hand traffic (RHT), the same as in Zhuhai and other regions of China. Thus, drivers from Hong Kong and Macau need to make use of crossing viaducts to switch to RHT upon entering the bridge, and back to LHT upon leaving the bridge when they are back to Hong Kong and Macau. Traffic between Zhuhai and the bridge requires no left-right conversion as they are both RHT.\n\nShuttle buses are available 24 hours a day and depart up to every 5 minutes with tickets available for purchase from vending machines or ticket counters. They take around 40 minutes to cross the HZMB.\n\nThe HZMB Hong Kong Port can be reached from Hong Kong by taxis or various buses including CityFlyer airport (A-number) routes, or the B5 shuttle bus from Sunny Bay MTR station, or the B6 bus from Tung Chung.\n\nThe HZMB Zhuhai Port can be reached from the mainland by taxis or the L1 bus which uses historic tourist vehicles, or Line-12, 23, 25 or 3 buses.\n\nThe HZMB Macau Port can be reached from Macau by taxis or various buses including the 101X bus and the 102X bus from St Paul's and Taipa, or the HZMB Integrated Resort Connection bus from Taipa Ferry Terminal or the Exterior Ferry terminal, connecting with free casino shuttle buses.\n\nCurrently only 10,000 permits are available for private vehicles to drive across the HZMB from Hong Kong to Zhuhai. In addition, the number of vehicles permitted to enter Hong Kong and Macau from other regions is subject to a daily quota.\n\nSince the Hong Kong government imposes significant fees, taxes and administrative paperwork on private car ownership and usage to deal with road congestion, driving a car on the HZMB would incur the same restrictions as current cross-border traffic. These include applying for separate driving licences for both Hong Kong and mainland China, a Hong Kong Closed Road Permit for cross-boundary vehicles, and an Approval Notice from the Guangdong Public Security Bureau. Vehicle owners also need to ensure they have the appropriate insurance coverage for the regions they are travelling to.\n\nThe HZMB links three major cities Hong Kong, Zhuhai and Macau, which are geographically close but separated by water. With the bridge in place, travelling time between Zhuhai and Hong Kong would cut down from about four hours to 30 minutes on the road.\n\nThe HZMB project is part of a Beijing-driven strategy to create an economic hub and promote the economic development of the whole area of the Pearl River Delta, which is also known as Greater Bay Area. Hoping to leverage the bridge and create an economic zone linking the three cities, Zhuhai's Hengqin area was designated as a free trade zone in 2015.\n\nThe artificial island housing the Hong Kong Boundary Crossing Facilities (HKBCF) was reported drifting due to an unconventional method, hitherto unused in Hong Kong, for land reclamation with steel circular cells in a row pushed through the mud and filled with inert material to form a seawall.\n\nThe drifting of parts of the reclaimed island had allegedly caused a delay in the HZMB project. The Highways Department denied reports in various sources of movement up to 20 metres but admitted parts of the reclaimed land had moved \"up to six or seven metres\", claiming that some movement was expected and safety had not been jeopardised.\n\nMainland contractors were also reportedly having difficulty in constructing immersed tubes in their section of the project, with the director of the Guangdong National Development and Reform Commission stating that 2020 would be a difficult target to meet.\n\nIn 2017, the Main Bridge of the HZMB project experienced a cost overrun of about 10 billion yuan, arising from an increase in labor and material costs, as well as the refinement of the design and construction schemes.\n\nThe number of deaths and injuries that have taken place on the project has come under scrutiny in Hong Kong. Apart from nine fatal cases on the mainland side, there have been ten deaths reported on the Hong Kong side of the construction project, and somewhere between 234 and 600 injuries, depending on the source. In April 2017, the Construction Site Workers General Union, the Labour Party and the Confederation of Trade Unions demonstrated at the Central Government Complex, demanding the government take action.\n\nLawmaker Fernando Cheung also expressed concerns over the unknown death toll on the Chinese side of the project, stating: \"the project is known as the 'bridge of blood and tears’ and we are only talking about the Hong Kong side. We don’t even know what is happening in China. I suppose the situation could be 10 times worse than that in Hong Kong.\" He said that the Hong Kong Government had a responsibility to consider worker safety on the Chinese side.\n\nIn 2017, Hong Kong's Independent Commission Against Corruption (ICAC) arrested 21 employees (two senior executives, 14 laboratory technicians and five laboratory assistants) of Jacobs China Limited, a contractor of the Civil Engineering and Development Department for falsifying concrete test results, thus potentially risking the safety of bridge for public use. In December 2017, a lab technician pleaded guilty and was sentenced to imprisonment for eight months, while the others await sentencing. Hong Kong's Highways Department conducted tests again after the falsified results were exposed and found all test results met safety standards.\n\nIn April 2018, the public and media raised questions over the integrity of the seawalls protecting the artificial islands at both ends of the undersea tunnel. In footage taken by drone users and mariners, the dolosse installed at the edges of the artificial islands appear to have dislodged. Some civil engineers suggested that there was an error in design. In dismissing the safety concerns, the HZMB Authority said the dolosse were designed to be submerged and the design was working as intended. Director of Highways Department Daniel Chung denied on 8 April 2018 that the breakwater components had been washed away by waves.\n\nSubsequent aerial footage posted online showed a section of the dolosse breakwater completely underwater. Civil engineer So Yiu-kwan told Hong Kong media on 12 April 2018 that the water level, at the time the photos were taken, was about 1.74 mPD (metres above Principal Datum), but the maximum water level could reach 2.7 mPD. He said the dolosse would offer no wave protection if entirely submerged, and further alleged that they had been installed backwards.\n\nConservationists at WWF Hong Kong blamed the construction of the HZMB for the falling number of white dolphins in the waters near the bridge. The dolphins found near waters of Lantau were worst hit with numbers dropping by 60 per cent between April 2015 and March 2016.\n\n\n"}
{"id": "24367869", "url": "https://en.wikipedia.org/wiki?curid=24367869", "title": "IKF World Korfball Ranking", "text": "IKF World Korfball Ranking\n\nThe IKF World Korfball Ranking is the ranking for national korfball teams, done by the International Korfball Federation.\n\n"}
{"id": "882884", "url": "https://en.wikipedia.org/wiki?curid=882884", "title": "INF World Rankings", "text": "INF World Rankings\n\nThe INF World Rankings are published by the International Netball Federation (INF) to make it possible to compare the relative strengths of internationally active national netball teams. Initially, rankings were based on the results from the World Netball Championships, and released after the conclusion of each event, every four years. A new ranking system was implemented on 11 February 2008, wherein teams are ranked based on international tests played, currently since July 2010. Teams appear on the rankings list once they have played eight international test matches, starting from July 2010. The rankings are updated by the INF every month. Australia and New Zealand have dominated the INF world rankings in previous years and they are the only two netball nations to have ever held the number one world ranking since its introduction in 2009.\n"}
{"id": "48714730", "url": "https://en.wikipedia.org/wiki?curid=48714730", "title": "Ideoscape", "text": "Ideoscape\n\nIdeoscape is a term used to describe one of Arjun Appadurai’s five dimensions of global cultural flows. The five dimensions consist of ethnoscapes, mediascapes, technoscapes, financescapes, and ideoscapes. The suffix -\"scape\" denotes that these terms are perspectival constructs inflected by the historical, linguistic, and political situatedness of different kinds of actors: \"nation-states, multinationals, diasporic communities, as well as subnational groupings and movements\". This can either be religious, political, or economic. Because cultural exchange and transactions have typically been restricted in the past due to geographical and economical aspects, Appadurai’s five dimensions give the opportunity for cultural transactions to occur. Ideoscapes is the movement of ideologies. It is often political and usually has to do with the ideologies of states and the counterideologies of movements explicitly oriented to capturing state power or a piece of it. Ideoscapes are usually composed of ideas, terms, and images including “freedom, welfare, rights, sovereignty, representation, and democracy”.\n"}
{"id": "58845410", "url": "https://en.wikipedia.org/wiki?curid=58845410", "title": "International Forum of Independent Audit Regulators", "text": "International Forum of Independent Audit Regulators\n\nThe International Forum of Independent Audit Regulators (IFIAR) was established in India in 2006 to conduct audits of Indian businesses. It has 52 independent audit regulators as members.\n"}
{"id": "31525546", "url": "https://en.wikipedia.org/wiki?curid=31525546", "title": "List of world production", "text": "List of world production\n\nThis is a list of annual world production. (Bold number is a list of countries producing commodity)\n\n\n\nTotal production is 1,868,700 tonnes in 2003.\n"}
{"id": "41502357", "url": "https://en.wikipedia.org/wiki?curid=41502357", "title": "Lustre (treaty)", "text": "Lustre (treaty)\n\nLustre is the codename of a secret treaty signed by France and the Five Eyes (FVEY) for cooperation in signals intelligence and for mutual data exchange between their respective intelligence agencies. Its existence was revealed during the 2013 global surveillance disclosure based on documents leaked by the former NSA contractor Edward Snowden.\n\nThe Directorate-General for External Security (DGSE) of France maintains a close relationship with both the NSA and the GCHQ after discussions for increased cooperation began in November 2006. By the early 2010s, the extent of cooperation in the joint interception of digital data by the DGSE and the NSA was noted to have increased dramatically.\n\nIn 2011, a formal memorandum for data exchange was signed by the DGSE and the NSA, which facilitated the transfer of millions of metadata records from the DGSE to the NSA. In 2013, the existence of the Lustre treaty was revealed in documents leaked by the former NSA contractor Edward Snowden.\n\n\nThe French telecommunications corporation Orange S.A. shares customer call data with the French intelligence agency DGSE, and the intercepted data is handed over to GCHQ.\n\nFrom December 2012 to 8 January 2013, over 70 million metadata records were handed over to the NSA by French intelligence agencies.\n\n"}
{"id": "258979", "url": "https://en.wikipedia.org/wiki?curid=258979", "title": "Malnutrition", "text": "Malnutrition\n\nMalnutrition is a condition that results from eating a diet in which one or more nutrients are either not enough or are too much such that the diet causes health problems. It may involve calories, protein, carbohydrates, vitamins or minerals. Not enough nutrients is called undernutrition or undernourishment while too much is called overnutrition. Malnutrition is often used to specifically refer to undernutrition where an individual is not getting enough calories, protein, or micronutrients. If undernutrition occurs during pregnancy, or before two years of age, it may result in permanent problems with physical and mental development. Extreme undernourishment, known as starvation, may have symptoms that include: a short height, thin body, very poor energy levels, and swollen legs and abdomen. People also often get infections and are frequently cold. The symptoms of micronutrient deficiencies depend on the micronutrient that is lacking.\nUndernourishment is most often due to not enough high-quality food being available to eat. This is often related to high food prices and poverty. A lack of breastfeeding may contribute, as may a number of infectious diseases such as: gastroenteritis, pneumonia, malaria, and measles, which increase nutrient requirements. There are two main types of undernutrition: protein-energy malnutrition and dietary deficiencies. Protein-energy malnutrition has two severe forms: marasmus (a lack of protein and calories) and kwashiorkor (a lack of just protein). Common micronutrient deficiencies include: a lack of iron, iodine, and vitamin A. During pregnancy, due to the body's increased need, deficiencies may become more common. In some developing countries, overnutrition in the form of obesity is beginning to present within the same communities as undernutrition. Other causes of malnutrition include anorexia nervosa and bariatric surgery.\nEfforts to improve nutrition are some of the most effective forms of development aid. Breastfeeding can reduce rates of malnutrition and death in children, and efforts to promote the practice increase the rates of breastfeeding. In young children, providing food (in addition to breastmilk) between six months and two years of age improves outcomes. There is also good evidence supporting the supplementation of a number of micronutrients to women during pregnancy and among young children in the developing world. To get food to people who need it most, both delivering food and providing money so people can buy food within local markets are effective. Simply feeding students at school is insufficient. Management of severe malnutrition within the person's home with ready-to-use therapeutic foods is possible much of the time. In those who have severe malnutrition complicated by other health problems, treatment in a hospital setting is recommended. This often involves managing low blood sugar and body temperature, addressing dehydration, and gradual feeding. Routine antibiotics are usually recommended due to the high risk of infection. Longer-term measures include: improving agricultural practices, reducing poverty, improving sanitation, and the empowerment of women.\nThere were 815 million undernourished people in the world in 2017 (11% of the total population). This is a reduction of 176 million people since 1990 when 23% were undernourished. In 2012 it was estimated that another billion people had a lack of vitamins and minerals. In 2015, protein-energy malnutrition was estimated to have resulted in 323,000 deaths—down from 510,000 deaths in 1990. Other nutritional deficiencies, which include iodine deficiency and iron deficiency anemia, result in another 83,000 deaths. In 2010, malnutrition was the cause of 1.4% of all disability adjusted life years. About a third of deaths in children are believed to be due to undernutrition, although the deaths are rarely labelled as such. In 2010, it was estimated to have contributed to about 1.5 million deaths in women and children, though some estimate the number may be greater than 3 million. An additional 165 million children were estimated to have stunted growth from malnutrition in 2013. Undernutrition is more common in developing countries. Certain groups have higher rates of undernutrition, including women—in particular while pregnant or breastfeeding—children under five years of age, and the elderly. In the elderly, undernutrition becomes more common due to physical, psychological, and social factors.\n\nUnless specifically mentioned otherwise, the term malnutrition refers to undernutrition for the remainder of this article. Malnutrition can be divided into two different types, SAM and MAM. SAM refers to children with severe acute malnutrition. MAM refers to moderate acute malnutrition.\n\nMalnutrition is caused by eating a diet in which nutrients are \"not enough\" or is \"too much\" such that it causes health problems. It is a category of diseases that includes undernutrition and overnutrition. Overnutrition can result in obesity and being overweight. In some developing countries, overnutrition in the form of obesity is beginning to present within the same communities as undernutrition.\n\nHowever, the term malnutrition is commonly used to refer to undernutrition only. This applies particularly to the context of development cooperation. Therefore, \"malnutrition\" in documents by the World Health Organization, UNICEF, Save the Children or other international non-governmental organizations (NGOs) usually is equated to undernutrition.\n\nUndernutrition is sometimes used as a synonym of protein–energy malnutrition (PEM). While other include both micronutrient deficiencies and protein energy malnutrition in its definition. It differs from calorie restriction in that calorie restriction may not result in negative health effects. The term hypoalimentation means underfeeding.\n\nThe term \"severe malnutrition\" or \"severe undernutrition\" is often used to refer specifically to PEM. PEM is often associated with micronutrient deficiency. Two forms of PEM are kwashiorkor and marasmus, and they commonly coexist.\n\nKwashiorkor is mainly caused by inadequate protein intake. The main symptoms are edema, wasting, liver enlargement, hypoalbuminaemia, steatosis, and possibly depigmentation of skin and hair. Kwashiorkor is further identified by swelling of the belly, which is deceiving of actual nutritional status. The term means ‘displaced child’ and is derived from a Ghana language of West Africa, means \"the sickness the older one gets when the next baby is born,\" as this is when the older child is deprived of breast feeding and weaned to a diet composed largely of carbohydrates.\n\nMarasmus (‘to waste away’) is caused by an inadequate intake of protein and energy. The main symptoms are severe wasting, leaving little or no edema, minimal subcutaneous fat, severe muscle wasting, and non-normal serum albumin levels. Marasmus can result from a sustained diet of inadequate energy and protein, and the metabolism adapts to prolong survival. It is traditionally seen in famine, significant food restriction, or more severe cases of anorexia. Conditions are characterized by extreme wasting of the muscles and a gaunt expression.\n\nUndernutrition encompasses stunted growth (stunting), wasting, and deficiencies of essential vitamins and minerals (collectively referred to as micronutrients). The term hunger, which describes a feeling of discomfort from not eating, has been used to describe undernutrition, especially in reference to food insecurity.\n\nIn 1956, Gómez and Galvan studied factors associated with death in a group of malnourished (undernourished) children in a hospital in Mexico City, Mexico and defined categories of malnutrition: first, second, and third degree. The degrees were based on weight below a specified percentage of median weight for age. The risk of death increases with increasing degree of malnutrition. An adaptation of Gomez's original classification is still used today. While it provides a way to compare malnutrition within and between populations, the classification has been criticized for being \"arbitrary\" and for not considering overweight as a form of malnutrition. Also, height alone may not be the best indicator of malnutrition; children who are born prematurely may be considered short for their age even if they have good nutrition.\n\nJohn Conrad Waterlow established a new classification for malnutrition. Instead of using just weight for age measurements, the classification established by Waterlow combines weight-for-height (indicating acute episodes of malnutrition) with height-for-age to show the stunting that results from chronic malnutrition. One advantage of the Waterlow classification over the Gomez classification is that weight for height can be examined even if ages are not known.\n\nThese classifications of malnutrition are commonly used with some modifications by WHO.\n\nMalnutrition increases the risk of infection and infectious disease, and moderate malnutrition weakens every part of the immune system. For example, it is a major risk factor in the onset of active tuberculosis. Protein and energy malnutrition and deficiencies of specific micronutrients (including iron, zinc, and vitamins) increase susceptibility to infection. Malnutrition affects HIV transmission by increasing the risk of transmission from mother to child and also increasing replication of the virus. In communities or areas that lack access to safe drinking water, these additional health risks present a critical problem. Lower energy and impaired function of the brain also represent the downward spiral of malnutrition as victims are less able to perform the tasks they need to in order to acquire food, earn an income, or gain an education.\n\nVitamin-deficiency-related diseases (such as scurvy and rickets).\n\nHypoglycemia (low blood sugar) can result from a child not eating for 4 to 6 hours. Hypoglycemia should be considered if there is lethargy, limpness, convulsion, or loss of consciousness. If blood sugar can be measured immediately and quickly, perform a finger or heel stick.\n\nIn those with malnutrition some of the signs of dehydration differ. Children; however, may still be interested in drinking, have decreased interactions with the world around them, have decreased urine output, and may be cool to touch.\n\nProtein-calorie malnutrition can cause cognitive impairments. For humans, \"critical period varies from the final third of gestation to the first 2 years of life\". Iron deficiency anemia in children under two years of age likely affects brain function acutely and probably also chronically. Folate deficiency has been linked to neural tube defects.\n\nMalnutrition in the form of iodine deficiency is \"the most common preventable cause of mental impairment worldwide.\" \"Even moderate deficiency, especially in pregnant women and infants, lowers intelligence by 10 to 15 I.Q. points, shaving incalculable potential off a nation's development. The most visible and severe effects — disabling goiters, cretinism and dwarfism — affect a tiny minority, usually in mountain villages. But 16 percent of the world's people have at least mild goiter, a swollen thyroid gland in the neck.\"\n\nMajor causes of malnutrition include poverty and food prices, dietary practices and agricultural productivity, with many individual cases being a mixture of several factors. Clinical malnutrition, such as cachexia, is a major burden also in developed countries. Various scales of analysis also have to be considered in order to determine the sociopolitical causes of malnutrition. For example, the population of a community that is within poor governments, may be at risk if the area lacks health-related services, but on a smaller scale certain households or individuals may be at an even higher risk due to differences in income levels, access to land, or levels of education.\n\nMalnutrition can be a consequence of health issues such as gastroenteritis or chronic illness, especially the HIV/AIDS pandemic. Diarrhea and other infections can cause malnutrition through decreased nutrient absorption, decreased intake of food, increased metabolic requirements, and direct nutrient loss. Parasite infections, in particular intestinal worm infections (helminthiasis), can also lead to malnutrition. A leading cause of diarrhea and intestinal worm infections in children in developing countries is lack of sanitation and hygiene.\n\nPeople may become malnourished due to abnormal nutrient loss (due to diarrhea or chronic illness affecting the small bowel). This conditions may include Crohn's disease or untreated coeliac disease. Malnutrition may also occur due to increased energy expenditure (secondary malnutrition).\n\nA lack of adequate breastfeeding leads to malnutrition in infants and children, associated with the deaths of an estimated one million children annually. Illegal advertising of breast milk substitutes contributed to malnutrition and continued three decades after its 1981 prohibition under the \"WHO International Code of Marketing Breast Milk Substitutes\".\n\nMaternal malnutrition can also factor into the poor health or death of a baby. Over 800,000 neonatal death have occurred because of deficient growth of the fetus in the mother's womb.\n\nDeriving too much of one's diet from a single source, such as eating almost exclusively corn or rice, can cause malnutrition. This may either be from a lack of education about proper nutrition, or from only having access to a single food source.\n\nIt is not just the total amount of calories that matters but specific nutritional deficiencies such as vitamin A deficiency, iron deficiency or zinc deficiency can also increase risk of death.\n\nOvernutrition caused by overeating is also a form of malnutrition. In the United States, more than half of all adults are now overweight — a condition that, like hunger, increases susceptibility to disease and disability, reduces worker productivity, and lowers life expectancy. Overeating is much more common in the United States, where for the majority of people, access to food is not an issue. Many parts of the world have access to a surplus of non-nutritious food, in addition to increased sedentary lifestyles. Yale psychologist Kelly Brownell calls this a \"toxic food environment\" where fat and sugar laden foods have taken precedence over healthy nutritious foods.\n\nThe issue in these developed countries is choosing the right kind of food. More fast food is consumed per capita in the United States than in any other country. The reason for this mass consumption of fast food is its affordability and accessibility. Often fast food, low in cost and nutrition, is high in calories and heavily promoted. When these eating habits are combined with increasingly urbanized, automated, and more sedentary lifestyles, it becomes clear why weight gain is difficult to avoid.\n\nNot only does obesity occur in developed countries, problems are also occurring in developing countries in areas where income is on the rise. Overeating is also a problem in countries where hunger and poverty persist. In China, consumption of high-fat foods has increased while consumption of rice and other goods has decreased.\n\nOvereating leads to many diseases, such as heart disease and diabetes, that may result in death.\n\nIn Bangladesh, poor socioeconomic position was associated with chronic malnutrition since it inhibits purchase of nutritious foods such as milk, meat, poultry, and fruits. As much as food shortages may be a contributing factor to malnutrition in countries with lack of technology, the FAO (Food and Agriculture Organization) has estimated that eighty percent of malnourished children living in the developing world live in countries that produce food surpluses. The economist Amartya Sen observed that, in recent decades, famine has always been a problem of food distribution and/or poverty, as there has been sufficient food to feed the whole population of the world. He states that malnutrition and famine were more related to problems of food distribution and purchasing power.\n\nIt is argued that commodity speculators are increasing the cost of food. As the real estate bubble in the United States was collapsing, it is said that trillions of dollars moved to invest in food and primary commodities, causing the 2007–2008 food price crisis.\n\nThe use of biofuels as a replacement for traditional fuels raises the price of food. The United Nations special rapporteur on the right to food, Jean Ziegler proposes that agricultural waste, such as corn cobs and banana leaves, rather than crops themselves be used as fuel.\n\nLocal food shortages can be caused by a lack of arable land, adverse weather, lower farming skills such as crop rotation, or by a lack of technology or resources needed for the higher yields found in modern agriculture, such as fertilizers, pesticides, irrigation, machinery and storage facilities. As a result of widespread poverty, farmers cannot afford or governments cannot provide the resources necessary to improve local yields. The World Bank and some wealthy donor countries also press nations that depend on aid to cut or eliminate subsidized agricultural inputs such as fertilizer, in the name of free market policies even as the United States and Europe extensively subsidized their own farmers. Many, if not most, farmers cannot afford fertilizer at market prices, leading to low agricultural production and wages and high, unaffordable food prices.\nReasons for the unavailability of fertilizer include moves to stop supplying fertilizer on environmental grounds, cited as the obstacle to feeding Africa by the Green Revolution pioneers Norman Borlaug and Keith Rosenberg.\n\nThere are a number of potential disruptions to global food supply that could cause widespread malnutrition.\n\nGlobal warming is of importance to food security, with 95 percent of all malnourished peoples living in the relatively stable climate region of the sub-tropics and tropics. According to the latest IPCC reports, temperature increases in these regions are \"very likely.\" Even small changes in temperatures can lead to increased frequency of extreme weather conditions. Many of these have great impact on agricultural production and hence nutrition. For example, the 1998–2001 central Asian drought brought about an 80 percent livestock loss and 50 percent reduction in wheat and barley crops in Iran. Similar figures were present in other nations. An increase in extreme weather such as drought in regions such as Sub-Saharan Africa would have even greater consequences in terms of malnutrition. Even without an increase of extreme weather events, a simple increase in temperature reduces the productivity of many crop species, also decreasing food security in these regions.\n\nColony collapse disorder is a phenomenon where bees die in large numbers. Since many agricultural crops worldwide are pollinated by bees, this represents a threat to the supply of food.\n\nThe effort to bring modern agricultural techniques found in the West, such as nitrogen fertilizers and pesticides, to Asia, called the Green Revolution, resulted in increased food production and corresponding decreases in prices and malnutrition similar to those seen earlier in Western nations. This was possible because of existing infrastructure and institutions that are in short supply in Africa, such as a system of roads or public seed companies that made seeds available. Investments in agriculture, such as subsidized fertilizers and seeds, increases food harvest and reduces food prices. For example, in the case of Malawi, almost five million of its 13 million people used to need emergency food aid. However, after the government changed policy and subsidies for fertilizer and seed were introduced against World Bank strictures, farmers produced record-breaking corn harvests as production leaped to 3.4 million in 2007 from 1.2 million in 2005, making Malawi a major food exporter. This lowered food prices and increased wages for farm workers. Such investments in agriculture are still needed in other African countries like the Democratic Republic of the Congo. The country has one of the highest prevalence of malnutrition even though it is blessed with great agricultural potential John Ulimwengu explains in his article for D+C. Proponents for investing in agriculture include Jeffrey Sachs, who has championed the idea that wealthy countries should invest in fertilizer and seed for Africa’s farmers.\n\nIn Nigeria, the use of imported Ready to Use Therapeutic Food (RUTF) has been used to treat malnutrition in the North. \"Soy Kunu\", a locally sourced and prepared blend consisting of peanut, millet and soya beans may also be used.\n\nNew technology in agricultural production also has great potential to combat undernutrition. By improving agricultural yields, farmers could reduce poverty by increasing income as well as open up area for diversification of crops for household use. The World Bank itself claims to be part of the solution to malnutrition, asserting that the best way for countries to succeed in breaking the cycle of poverty and malnutrition is to build export-led economies that will give them the financial means to buy foodstuffs on the world market.\n\nThere is a growing realization among aid groups that giving cash or cash vouchers instead of food is a cheaper, faster, and more efficient way to deliver help to the hungry, particularly in areas where food is available but unaffordable. The UN's World Food Program, the biggest non-governmental distributor of food, announced that it will begin distributing cash and vouchers instead of food in some areas, which Josette Sheeran, the WFP's executive director, described as a \"revolution\" in food aid. The aid agency Concern Worldwide is piloting a method through a mobile phone operator, Safaricom, which runs a money transfer program that allows cash to be sent from one part of the country to another.\n\nHowever, for people in a drought living a long way from and with limited access to markets, delivering food may be the most appropriate way to help. Fred Cuny stated that \"the chances of saving lives at the outset of a relief operation are greatly reduced when food is imported. By the time it arrives in the country and gets to people, many will have died.\" U.S. law, which requires buying food at home rather than where the hungry live, is inefficient because approximately half of what is spent goes for transport. Cuny further pointed out \"studies of every recent famine have shown that food was available in-country — though not always in the immediate food deficit area\" and \"even though by local standards the prices are too high for the poor to purchase it, it would usually be cheaper for a donor to buy the hoarded food at the inflated price than to import it from abroad.\"\nFood banks and soup kitchens address malnutrition in places where people lack money to buy food. A basic income has been proposed as a way to ensure that everyone has enough money to buy food and other basic needs; it is a form of social security in which all citizens or residents of a country regularly receive an unconditional sum of money, either from a government or some other public institution, in addition to any income received from elsewhere.\n\nEthiopia has been pioneering a program that has now become part of the World Bank's prescribed method for coping with a food crisis and had been seen by aid organizations as a model of how to best help hungry nations. Through the country's main food assistance program, the Productive Safety Net Program, Ethiopia has been giving rural residents who are chronically short of food, a chance to work for food or cash. Foreign aid organizations like the World Food Program were then able to buy food locally from surplus areas to distribute in areas with a shortage of food. Ethiopia been pioneering a program, and Brazil has established a recycling program for organic waste that benefits farmers, urban poor, and the city in general. City residents separate organic waste from their garbage, bag it, and then exchange it for fresh fruit and vegetables from local farmers. As a result, the country's waste is reduced and the urban poor get a steady supply of nutritious food.\n\nRestricting population size is a proposed solution. Thomas Malthus argued that population growth could be controlled by natural disasters and voluntary limits through \"moral restraint.\" Robert Chapman suggests that an intervention through government policies is a necessary ingredient of curtailing global population growth. The interdependence and complementarity of population growth with poverty and malnutrition (as well as the environment) is also recognised by the United Nations. More than 200 million women worldwide do not have adequate access to family planning services. According to the World Health Organisation, \"Family planning is key to slowing unsustainable population growth and the resulting negative impacts on the economy, environment, and national and regional development efforts\".\n\nHowever, there are many who believe that the world has more than enough resources to sustain its population. Instead, these theorists point to unequal distribution of resources and under- or unutilized arable land as the cause for malnutrition problems. For example, Amartya Sen advocates that, \"no matter how a famine is caused, methods of breaking it call for a large supply of food in the public distribution system. This applies not only to organizing rationing and control, but also to undertaking work programmes and other methods of increasing purchasing power for those hit by shifts in exchange entitlements in a general inflationary situation.\" \n\nOne suggested policy framework to resolve access issues is termed food sovereignty—the right of peoples to define their own food, agriculture, livestock, and fisheries systems, in contrast to having food largely subjected to international market forces. Food First is one of the primary think tanks working to build support for food sovereignty. Neoliberals advocate for an increasing role of the free market.\n\nAnother possible long term solution would be to increase access to health facilities to rural parts of the world. These facilities could monitor undernourished children, act as supplemental food distribution centers, and provide education on dietary needs. These types of facilities have already proven very successful in countries such as Peru and Ghana.\n\nAs of 2016 is estimated that about 821,000 deaths of children less than five years old could be prevented globally per year through more widespread breastfeeding. In addition to reducing infant death, breast milk feeding provides an important source of micronutrients, clinically proven to bolster the immune system of children, and provide long-term defenses against non-communicable and allergic diseases. Breastfeeding has also been shown to improve cognitive abilities in children, with a strong correlation to individual educational achievements. As previously noted, lack of proper breastfeeding is a major factor in child mortality rates, and a primary determinant of disease development for children. The medical community recommends exclusively breastfeeding infants for 6 months, with nutritional whole food supplementation and continued breastfeeding up to 2 years or older for overall optimal health outcomes. Exclusive breastfeeding is defined as only giving an infant breast milk for six months as a source of food and nutrition. This means no other liquids, including water or semi-solid foods.\n\nBreastfeeding is noted as one of the most cost effective medical interventions for providing beneficial child health. While there are considerable differences within developed and developing countries: income, employment, social norms, and access to healthcare were found to be universal determinants of whether a mother breast or formula fed their children. Community based healthcare workers have helped alleviate financial barriers faced by newly made mothers, and provided a viable alternative to traditional and expensive hospital based medical care. Recent studies based upon surveys conducted from 1995-2010 shows exclusive breastfeeding rates have gone up globally, from 33% to 39%. Despite the growth rates, medical professionals acknowledge the need for improvement given the importance of exclusive breastfeeding.\n\nThe EndingHunger campaign is an online communication campaign aimed at raising awareness of the hunger problem. It has many worked through viral videos depicting celebrities voicing their anger about the large number of hungry people in the world.\n\nFood security and global malnutrition has long been a topic of international concern, with one of the first official global documents addressing it being the 1948 Universal Declaration of Human Rights(UDHR). Within this document it stated that access to food was part of an adequate right to a standard of living. The Right to food was asserted in the International Covenant on Economic, Social and Cultural Rights, a treaty adopted by the United Nations General Assembly on December 16, 1966. The Right to food is a human right for people to feed themselves in dignity, be free from hunger, food insecurity, and malnutrition. As of 2018, the treaty has been signed by 166 countries, by signing states agreed to take steps to the maximum of their available resources to achieve the right to adequate food.\n\nHowever, after the 1966 International Covenant the global concern for the access to sufficient food only became more present, leading to the first ever World Food Conference that was held in 1974 in Rome, Italy. The Universal Declaration on the Eradication of Hunger and Malnutrition was a UN resolution adopted November 16, 1974 by all 135 countries that attended the 1974 World Food Conference. This non-legally binding document set forth certain aspirations for countries to follow to sufficiently take action on the global food problem. Ultimately this document outline and provided guidance as to how the international community as one could work towards fighting and solving the growing global issue of malnutrition and hunger.\n\nAdoption of the right to food was included in the Additional Protocol to the American Convention on Human Rights in the area of Economic, Social, and Cultural Rights, this 1978 document was adopted by many countries in the Americas, the purpose of the document is, \"to consolidate in this hemisphere, within the framework of democratic institutions, a system of personal liberty and social justice based on respect for the essential rights of man.\"\n\nThe next document in the timeline of global inititaves for malnutrition was the 1996 Rome Declaration on World Food Security, organized by the Food and Agriculture Organization. This document reaffirmed the right to have access to safe and nutritous food by everyone, also considering that everyone gets sufficient food, and set the goals for all nations to improve their commitment to food security by halfing their amount of undernourished people by 2015. In 2004 the Food and Agriculture Organization adopted the Right to Food Guidelines, which offered states a framework of how to increase the right to food on a national basis.\n\nIn April 2012, the Food Assistance Convention was signed, the world's first legally binding international agreement on food aid. The May 2012 Copenhagen Consensus recommended that efforts to combat hunger and malnutrition should be the first priority for politicians and private sector philanthropists looking to maximize the effectiveness of aid spending. They put this ahead of other priorities, like the fight against malaria and AIDS.\n\nThe main global policy to reduce hunger and poverty are the Sustainable Development Goals, approved through the UN in 2015. In particular Goal 2: Zero hunger sets globally agreed targets to end hunger, achieve food security and improved nutrition and promote sustainable agriculture. The partnership Compact2025, led by IFPRI with the involvement of UN organisations, NGOs and private foundations develops and disseminates evidence-based advice to politicians and other decision-makers aimed at ending hunger and undernutrition in the coming 10 years, by 2025.\n\nIn June 2015, the European Union and the Bill & Melinda Gates Foundation have launched a partnership to combat undernutrition especially in children. The program will initiatilly be implemented in Bangladesh, Burundi, Ethiopia, Kenya, Laos and Niger and will help these countries to improve information and analysis about nutrition so they can develop effective national nutrition policies.\n\nThe Food and Agriculture Organization of the UN has created a partnership that will act through the African Union's CAADP framework aiming to end hunger in Africa by 2025. It includes different interventions including support for improved food production, a strengthening of social protection and integration of the right to food into national legislation.\n\nThe EndingHunger campaign is an online communication campaign aimed at raising awareness of the hunger problem. It has many worked through viral videos depicting celebrities voicing their anger about the large number of hungry people in the world.\n\nIn response to child malnutrition, the Bangladeshi government recommends ten steps for treating severe malnutrition. They are to prevent or treat dehydration, low blood sugar, low body temperature, infection, correct electrolyte imbalances and micronutrient deficiencies, start feeding cautiously, achieve catch-up growth, provide psychological support, and prepare for discharge and follow-up after recovery.\n\nAmong those who are hospitalized, nutritional support improves protein, calorie intake and weight.\n\nThe evidence for benefit of supplementary feeding is poor. This is due to the small amount of research done on this treatment.\n\nSpecially formulated foods do however appear useful in those from the developing world with moderate acute malnutrition. In young children with severe acute malnutrition it is unclear if ready-to-use therapeutic food differs from a normal diet. They may have some benefits in humanitarian emergencies as they can be eaten directly from the packet, do not require refrigeration or mixing with clean water, and can be stored for years.\n\nIn those who are severely malnourished, feeding too much too quickly can result in refeeding syndrome. This can result regardless of route of feeding and can present itself a couple of days after eating with heart failure, dysrhythmias and confusion that can result in death.\n\nManufacturers are trying to fortify everyday foods with micronutrients that can be sold to consumers such as wheat flour for Beladi bread in Egypt or fish sauce in Vietnam and the iodization of salt.\n\nFor example, flour has been fortified with iron, zinc, folic acid and other B vitamins such as thiamine, riboflavin, niacin and vitamin B12.\n\nTreating malnutrition, mostly through fortifying foods with micronutrients (vitamins and minerals), improves lives at a lower cost and shorter time than other forms of aid, according to the World Bank. The Copenhagen Consensus, which look at a variety of development proposals, ranked micronutrient supplements as number one.\n\nIn those with diarrhea, once an initial four-hour rehydration period is completed, zinc supplementation is recommended. Daily zinc increases the chances of reducing the severity and duration of the diarrhea, and continuing with daily zinc for ten to fourteen days makes diarrhea less likely recur in the next two to three months.\n\nIn addition, malnourished children need both potassium and magnesium. This can be obtained by following the above recommendations for the dehydrated child to continue eating within two to three hours of starting rehydration, and including foods rich in potassium as above. Low blood potassium is worsened when base (as in Ringer's/Hartmann's) is given to treat acidosis without simultaneously providing potassium. As above, available home products such as salted and unsalted cereal water, salted and unsalted vegetable broth can be given early during the course of a child's diarrhea along with continued eating. Vitamin A, potassium, magnesium, and zinc should be added with other vitamins and minerals if available.\n\nFor a malnourished child with diarrhea from any cause, this should include foods rich in potassium such as bananas, green coconut water, and unsweetened fresh fruit juice.\n\nThe World Health Organization (WHO) recommends rehydrating a severely undernourished child who has diarrhea relatively slowly. The preferred method is with fluids by mouth using a drink called oral rehydration solution (ORS). The oral rehydration solution is both slightly sweet and slightly salty and the one recommended in those with severe undernutrition should have half the usual sodium and greater potassium. Fluids by nasogastric tube may be use in those who do not drink. Intravenous fluids are recommended only in those who have significant dehydration due to their potential complications. These complications include congestive heart failure. Over time, ORS developed into ORT, or oral rehydration therapy, which focused on increasing fluids by supplying salts, carbohydrates, and water. This switch from type of fluid to amount of fluid was crucial in order to prevent dehydration from diarrhea.\n\nBreast feeding and eating should resume as soon as possible. Drinks such as soft drinks, fruit juices, or sweetened teas are not recommended as they contain too much sugar and may worsen diarrhea. Broad spectrum antibiotics are recommended in all severely undernourished children with diarrhea requiring admission to hospital.\n\nTo prevent dehydration readily available fluids, preferably with a modest amount of sugars and salt such as vegetable broth or salted rice water, may be used. The drinking of additional clean water is also recommended. Once dehydration develops oral rehydration solutions are preferred. As much of these drinks as the person wants can be given, unless there are signs of swelling. If vomiting occurs, fluids can be paused for 5–10 minutes and then restarting more slowly. Vomiting rarely prevents rehydration as fluid are still absorbed and the vomiting rarely last long. A severely malnourished child with what appears to be dehydration but who has not had diarrhea should be treated as if they have an infection.\n\nFor babies a dropper or syringe without the needle can be used to put small amounts of fluid into the mouth; for children under 2, a teaspoon every one to two minutes; and for older children and adults, frequent sips directly from a cup. After the first two hours, rehydration should be continued at the same or slower rate, determined by how much fluid the child wants and any ongoing diarrheal loses. After the first two hours of rehydration it is recommended that to alternate between rehydration and food.\n\nIn 2003, WHO and UNICEF recommended a reduced-osmolarity ORS which still treats dehydration but also reduced stool volume and vomiting. Reduced-osmolarity ORS is the current standard ORS with reasonably wide availability. For general use, one packet of ORS (glucose sugar, salt, potassium chloride, and trisodium citrate) is added to one liter of water; however, for malnourished children it is recommended that one packet of ORS be added to two liters of water along with an extra 50 grams of sucrose sugar and some stock potassium solution.\n\nMalnourished children have an excess of body sodium. Recommendations for home remedies agree with one liter of water (34 oz.) and 6 teaspoons sugar and disagree regarding whether it is then one teaspoon of salt added or only 1/2, with perhaps most sources recommending 1/2 teaspoon of added salt to one liter water.\n\nHypoglycemia, whether known or suspected, can be treated with a mixture of sugar and water. If the child is conscious, the initial dose of sugar and water can be given by mouth. If the child is unconscious, give glucose by intravenous or nasogastric tube. If seizures occur after despite glucose, rectal diazepam is recommended. Blood sugar levels should be re-checked on two hour intervals.\n\nHypothermia can occur. To prevent or treat this, the child can be kept warm with covering including of the head or by direct skin-to-skin contact with the mother or father and then covering both parent and child. Prolonged bathing or prolonged medical exams should be avoided. Warming methods are usually most important at night.\n\nThe figures provided in this section on epidemiology all refer to \"undernutrition\" even if the term malnutrition is used which, by definition, could also apply to too much nutrition.\n\nThe Global Hunger Index (GHI) is a multidimensional statistical tool used to describe the state of countries’ hunger situation. The GHI measures progress and failures in the global fight against hunger. The GHI is updated once a year. The data from the 2015 report shows that Hunger levels have dropped 27% since 2000. Fifty two countries remain at serious or alarming levels. In addition to the latest statistics on Hunger and Food Security, the GHI also features different special topics each year. The 2015 report include an article on conflict and food security.\n\nThere were 815 million undernourished people in the world in 2017. This was 176 million fewer people than in 1990 when it was 991 million undernourished people. This is despite the world's farmers producing enough food to feed around 12 billion people – almost double the current world population.\n\nMalnutrition, as of 2010, was the cause of 1.4% of all disability adjusted life years.\n\nMortality due to malnutrition accounted for 58 percent of the total mortality in 2006: \"In the world, approximately 62 million people, all causes of death combined, die each year. One in twelve people worldwide is malnourished and according to the Save the Children 2012 report, one in four of the world’s children are chronically malnourished. In 2006, more than 36 million died of hunger or diseases due to deficiencies in micronutrients\".\n\nIn 2010 protein-energy malnutrition resulted in 600,000 deaths down from 883,000 deaths in 1990. Other nutritional deficiencies, which include iodine deficiency and iron deficiency anemia, result in another 84,000 deaths. In 2010 malnutrition caused about 1.5 million deaths in women and children.\n\nAccording to the World Health Organization, malnutrition is the biggest contributor to child mortality, present in half of all cases. Six million children die of hunger every year. Underweight births and intrauterine growth restrictions cause 2.2 million child deaths a year. Poor or non-existent breastfeeding causes another 1.4 million. Other deficiencies, such as lack of vitamin A or zinc, for example, account for 1 million. Malnutrition in the first two years is irreversible. Malnourished children grow up with worse health and lower education achievement. Their own children tend to be smaller. Malnutrition was previously seen as something that exacerbates the problems of diseases such as measles, pneumonia and diarrhea, but malnutrition actually causes diseases, and can be fatal in its own right.\n\nThroughout history, portions of the world's population have often experienced sustained periods of hunger. In many cases, this resulted from food supply disruptions caused by war, plagues, or adverse weather. For the first few decades after World War II, technological progress and enhanced political cooperation suggested it might be possible to substantially reduce the number of people suffering from hunger. While progress was uneven, by 2000 the threat of extreme hunger subsided for many of the world's people. According to the WFP some statistics are that, \"Some 795 million people in the world do not have enough food to lead a healthy active life. That's about one in nine people on earth. The vast majority of the world's hungry people live in developing countries, where 12.9 percent of the population is undernourished.\"\n\nUntil 2006, the average international price of food had been largely stable for several decades. In the closing months of 2006, however, prices began to rise rapidly. By 2008, rice had tripled in price in some regions, and this severely affected developing countries. Food prices fell in early 2009, but rose to another record high in 2011, and have since decreased slightly. The 2008 worldwide financial crisis further increased the number of people suffering from hunger, including dramatic increases even in advanced economies such as Great Britain, the Eurozone and the United States.\n\nThe Millennium Development Goals included a commitment to a further 50% reduction in the proportion of the world's population who have extreme hunger by 2015. As of 2012, this target appeared difficult to achieve, due in part to persistent inflation in food prices. However, in late 2012 the UN's Food and Agriculture Organization (FAO) stated it is still possible to hit the target with sufficient effort. In 2013, the \"FAO\" estimated that 842 million people are undernourished (12% of the global population). Malnutrition is a cause of death for more than 3.1 million children under 5 every year. UNICEF estimates 300 million children go to bed hungry each night; and that 8000 children under the age of 5 are estimated to die of malnutrition every day.\n\nThroughout history, the need to aid those suffering from hunger has been commonly, though not universally, recognized.\n\nThe philosopher Simone Weil wrote that feeding the hungry when you have resources to do so is the most obvious of all human obligations. She says that as far back as Ancient Egypt, many believed that people had to show they had helped the hungry in order to justify themselves in the afterlife. Weil writes that Social progress is commonly held to be first of all, \"...a transition to a state of human society in which people will not suffer from hunger.\" Social historian Karl Polanyi wrote that before markets became the world's dominant form of economic organization in the 19th century, most human societies would either starve all together or not at all, because communities would invariably share their food.\n\nFrom the first age of globalization, which began in the 19th century, it became more common for people to consider problems like hunger in global terms. However, as early globalization largely coincided with the high peak of influence for classical liberalism, there was relatively little call for politicians to address world hunger.\n\nIn the late nineteenth and early twentieth century, the view that politicians ought not to intervene against hunger was increasingly challenged by campaigning journalists, with some academics and politicians also calling for or organizing intervention against world hunger, such as U.S. President Woodrow Wilson.\n\nHunger as an academic and social topic came to prominence during the Great Depression. As many individuals struggled for food, the same agricultural industries were suddenly producing large surpluses as means of increased production to counter the drop in demand from the European markets. This increased output was meant to ease the growing debt levels, however domestic demand could not keep up with prices. Instead, what is often called \"the paradox of want amid plenty,\" agricultural surpluses and large demand simply did not fit together, causing the Hoover administration to buy large amounts of product, such as grain, to stabilize prices. Initially refusing to further compromise the distressed price levels, political pressure from starving families across the country forced Congress to reconsider. With large deposits of grain already wasting away in government possession, the only political move left was to begin a process of donations to the hungry from the Farm Board, a federal oversight created in 1929 to promote the sale and stabilization of agricultural products. Instead of hunger being a reason for the allocation of large grain surpluses, waste became the eventual driving force.\n\nAfter World War II, a new international politico-economic order came into being, which was later described as Embedded liberalism.\n\nFor at least the first decade after the war, the United States, by far the period's most dominant national actor, was strongly supportive of efforts to tackle world hunger and to promote international development. It heavily funded the United Nation's development programmes, and later the efforts of other multilateral organizations like the International Monetary Fund (IMF) and the World Bank (WB).\n\nThe newly established United Nations became a leading player in co-ordinating the global fight against hunger. The UN has three agencies that work to promote food security and agricultural development: the Food and Agriculture Organization (FAO), the World Food Programme (WFP) and the International Fund for Agricultural Development (IFAD). FAO is the world's agricultural knowledge agency, providing policy and technical assistance to developing countries to promote food security, nutrition and sustainable agricultural production, particularly in rural areas.\n\nWFP's key mission is to deliver food into the hands of the hungry poor. The agency steps in during emergencies and uses food to aid recovery after emergencies. Its longer term approaches to hunger helps the transition from recovery to development. IFAD, with its knowledge of rural poverty and exclusive focus on poor rural people, designs and implements programmes to help those people access the assets, services and opportunities they need to overcome poverty.\n\nFollowing successful post WWII reconstruction of Germany and Japan, the IMF and WB began to turn their attention to the developing world. A great many civil society actors were also active in trying to combat hunger, especially after the late 1970s when global media began to bring the plight of starving people in places like Ethiopia to wider attention. Most significant of all, especially in the late 1960s and 70s, the Green revolution helped improved agricultural technology propagate throughout the world.\n\nThe United States began to change its approach to the problem of world hunger from about the mid 1950s. Influential members of the administration became less enthusiastic about methods they saw as promoting an over reliance on the state, as they feared that might assist the spread of communism. Despite this view, during the 1960s postwar era hunger within the United States was overshadowed by hunger in Europe and Asia. John F. Kennedy did in fact use Executive Order to double the amount of commodities available from the surplus commodity program as well as initiated the pilot Food Stamp Program which later became permanent in 1964.\n\nBy the 1980s, the previous consensus in favour of moderate government intervention had been displaced across the western world. The IMF and World Bank in particular began to promote market-based solutions. In cases where countries became dependent on the IMF, they sometimes forced national governments to prioritize debt repayments and sharply cut public services. This sometimes had a negative effect on efforts to combat hunger.\n\nOrganizations such as Food First raised the issue of food sovereignty and claimed that every country on earth (with the possible minor exceptions of some city-states) has sufficient agricultural capacity to feed its own people, but that the \"free trade\" economic order, which from the late 1970s to about 2008 had been associated with such institutions as the IMF and World Bank, had prevented this from happening.\n\nThe World Bank itself claimed it was part of the solution to hunger, asserting that the best way for countries to break the cycle of poverty and hunger was to build export-led economies that provide the financial means to buy foodstuffs on the world market. However, in the early 21st century the World Bank and IMF became less dogmatic about promoting free market reforms. They increasingly returned to the view that government intervention does have a role to play, and that it can be advisable for governments to support food security with policies favourable to domestic agriculture, even for countries that do not have a Comparative advantage in that area. As of 2012, the World Bank remains active in helping governments to intervene against hunger.\n\nUntil at least the 1980s—and, to an extent, the 1990s—the dominant academic view concerning world hunger was that it was a problem of demand exceeding supply. Proposed solutions often focused on boosting food production, and sometimes on birth control. There were exceptions to this, even as early as the 1940s, Lord Boyd-Orr, the first head of the UN's FAO, had perceived hunger as largely a problem of distribution, and drew up comprehensive plans to correct this. Few agreed with him at the time, however, and he resigned after failing to secure support for his plans from the US and Great Britain. In 1998, Amartya Sen won a Nobel Prize in part for demonstrating that hunger in modern times is not typically the product of a lack of food. Rather, hunger usually arises from food distribution problems, or from governmental policies in the developed and developing world. It has since been broadly accepted that world hunger results from issues with the distribution as well as the production of food. Sen's 1981 essay \"Poverty and Famines: An Essay on Entitlement and Deprivation\" played a prominent part in forging the new consensus.\n\nIn 2007 and 2008, rapidly increasing food prices caused a \"global food crisis\", increasing the numbers suffering from hunger by over a hundred million. Food riots erupted in several dozen countries; in at least two cases, Haiti and Madagascar, this led to the toppling of governments. A second \"global food crisis\" unfolded due to the spike in food prices of late 2010 and early 2011. Fewer food riots occurred, due in part to greater availability of food stock piles for relief. However, several analysts argue the food crisis was one of the causes of the Arab Spring.\nAs of 2008 roughly $300 million of aid went to basic nutrition each year, less than $2 for each child below two in the 20 worst affected countries. In contrast, at that time HIV/AIDS, which caused fewer deaths than child malnutrition, received $2.2 billion—$67 per person with HIV in all countries. In 2008 UN estimated that ending world hunger could cost about 30 billion.\n\nThe International Crops Research Institute for the Semi-Arid Tropics (ICRISAT), a member of the CGIAR consortium, partners with farmers, governments, researchers and NGOs to help farmers grow nutritious crops, such as chickpea, groundnut, pigeonpea, millet and sorghum. This helps their communities have more balanced diets and become more resilient to pests and drought. The Harnessing Opportunities for Productivity Enhancement of Sorghum and Millets in Sub-Saharan Africa and the Indian-Subcontinent (HOPE) project, for example, is increasing yields of finger millet in Tanzania by encouraging farmers to grow improved varieties. Finger millet is very high in calcium, rich in iron and fiber, and has a better energy content than other cereals. These characteristics make it ideal for feeding to infants and the elderly.\n\nSome organizations have begun working with teachers, policymakers, and managed food service contractors to mandate improved nutritional content and increased nutritional resources in school cafeterias from primary to university-level institutions. Health and nutrition have been proven to have close links with overall educational success.\n\nIn the early 21st century, there was relatively little awareness of hunger from leaders of advanced nations such as those that form the G8. Prior to 2009, efforts to fight hunger were mainly undertaken by governments of the worst affected countries, by civil society actors, and by multilateral and regional organizations. In 2009, Pope Benedict published his third encyclical, Caritas in Veritate, which emphasised the importance of fighting against hunger. The encyclical was intentionally published immediately before the July 2009 G8 Summit to maximise its influence on that event. At the Summit, which took place at L'Aquila in central Italy, the \"L'Aquila Food Security Initiative\" was launched, with a total of US$22 billion committed to combat hunger.\n\nFood prices did fall sharply in 2009 and early 2010, though analysts credit this much more to farmers increasing production in response to the 2008 spike in prices, than to the fruits of enhanced government action. However, since the 2009 G8 summit, the fight against hunger has remained a high-profile issue among the leaders of the worlds major nations, and was a prominent part of the agenda for the 2012 G-20 summit. \n\nIn April 2012, the Food Assistance Convention was signed, the world's first legally binding international agreement on food aid. The May 2012 Copenhagen Consensus recommended that efforts to combat hunger and malnutrition should be the first priority for politicians and private sector philanthropists looking to maximize the effectiveness of aid spending. They put this ahead of other priorities, like the fight against malaria and AIDS. Also in May 2012, U.S. President Barack Obama launched a \"new alliance for food security and nutrition\"—a broad partnership between private sector, governmental and civil society actors—that aimed to \"...achieve sustained and inclusive agricultural growth and raise 50 million people out of poverty over the next 10 years.\" The UK's prime minister David Cameron held a hunger summit on 12 August, the last day of the 2012 Summer Olympics.\n\nThe fight against hunger has also been joined by an increased number of regular people. While folk throughout the world had long contributed to efforts to alleviate hunger in the developing world, there has recently been a rapid increase in the numbers involved in tackling domestic hunger even within the economically advanced nations of the Global North.\n\nThis had happened much earlier in North America than it did in Europe. In the US, the Reagan administration scaled back welfare the early 1980s, leading to a vast increase of charity sector efforts to help Americans unable to buy enough to eat. According to a 1992 survey of 1000 randomly selected US voters, 77% of Americans had contributed to efforts to feed the hungry, either by volunteering for various hunger relief agencies such as food banks and soup kitchens, or by donating cash or food. \nEurope, with its more generous welfare system, had little awareness of domestic hunger until the food price inflation that began in late 2006, and especially as austerity-imposed welfare cuts began to take effect in 2010. Various surveys reported that upwards of 10% of Europe's population had begun to suffer from food insecurity. Especially since 2011, there has been a substantial increase in grass roots efforts to help the hungry by means of food banks, within both the UK and continental Europe. \n\nBy July 2012, the 2012 US drought had already caused a rapid increase in the price of grain and soy, with a knock on effect on the price of meat. As well as affecting hungry people in the US, this caused prices to rise on the global markets; the US is the world's biggest exporter of food. This led to much talk of a possible third 21st century global food crisis. The \"Financial Times\" reported that the BRICS may not be as badly affected as they were in the earlier crises of 2008 and 2011. However, smaller developing countries that must import a substantial portion of their food could be hard hit. The UN and G20 has begun contingency planning so as to be ready to intervene if a third global crisis breaks out.\nBy August 2013 however, concerns had been allayed, with above average grain harvests expected from major exporters, including Brazil, Ukraine and the U.S. 2014 also saw a good worldwide harvest, leading to speculation that grain prices could soon begin to fall.\nIn an April 2013 summit held in Dublin concerning Hunger, Nutrition, Climate Justice, and the post 2015 MDG framwework for global justice, Ireland's President Higgins said that only 10% of deaths from hunger are due to armed conflict and natural disasters, with ongoing hunger being both the \"greatest ethical failure of the current global system\" and the \"greatest ethical challenge facing the global community.\"\n$4.15 billion of new commitments were made to tackle hunger at a June 2013 Hunger Summit held in London, hosted by the governments of Britain and Brazil, together with The Children's Investment Fund Foundation.\n\nUndernutrition is an important determinant of maternal and child health, accounting for more than a third of child deaths and more than 10 percent of the total global disease burden according to 2008 studies.\n\nThe World Health Organization estimates that malnutrition accounts for 54 percent of child mortality worldwide, about 1 million children. Another estimate also by WHO states that childhood underweight is the cause for about 35% of all deaths of children under the age of five years worldwide.\n\nAs underweight children are more vulnerable to almost all infectious diseases, the \"indirect\" disease burden of malnutrition is estimated to be an order of magnitude higher than the disease burden of the \"direct\" effects of malnutrition. The combination of direct and indirect deaths from malnutrition caused by unsafe water, sanitation and hygiene (WASH) practices is estimated to lead to 860,000 deaths per year in children under five years of age.\n\nOlder sources sometimes claim this phenomenon is unique to developing countries, due to greater sexual inequality. More recent findings suggested that mothers often miss meals in advanced economies too. For example, a 2012 study undertaken by Netmums in the UK found that one in five mothers sometimes misses out on food to save their children from hunger.\n\nIn several periods and regions, gender has also been an important factor determining whether or not victims of hunger would make suitable examples for generating enthusiasm for hunger relief efforts. James Vernon, in his \"Hunger: A Modern History\", wrote that in Britain before the 20th century, it was generally only women and children suffering from hunger who could arouse compassion. Men who failed to provide for themselves and their families were often regarded with contempt. This changed after World War I, where thousands of men who had proved their manliness in combat found themselves unable to secure employment. Similarly, female gender could be advantageous for those wishing to advocate for hunger relief, with Vernon writing that being a woman helped Emily Hobhouse draw the plight of hungry people to wider attention during the Second Boer War.\nResearchers from the Centre for World Food Studies in 2003 found that the gap between levels of undernutrition in men and women is generally small, but that the gap varies from region to region and from country to country. These small-scale studies showed that female undernutrition prevalence rates exceeded male undernutrition prevalence rates in South/Southeast Asia and Latin America and were lower in Sub-Saharan Africa. Datasets for Ethiopia and Zimbabwe reported undernutrition rates between 1.5 and 2 times higher in men than in women; however, in India and Pakistan, datasets rates of undernutrition were 1.5-2 times higher in women than in men. Intra-country variation also occurs, with frequent high gaps between regional undernutrition rates. Gender inequality in nutrition in some countries such as India is present in all stages of life.\n\nStudies on nutrition concerning gender bias within households look at patterns of food allocation, and one study from 2003 suggested that women often receive a lower share of food requirements than men. Gender discrimination, gender roles, and social norms affecting women can lead to early marriage and childbearing, close birth spacing, and undernutrition, all of which contribute to malnourished mothers.\n\nWithin the household, there may be differences in levels of malnutrition between men and women, and these differences have been shown to vary significantly from one region to another, with problem areas showing relative deprivation of women. Samples of 1000 women in India in 2008 demonstrated that malnutrition in women is associated with poverty, lack of development and awareness, and illiteracy. The same study showed that gender discrimination in households can prevent a woman's access to sufficient food and healthcare. How socialization affects the health of women in Bangladesh, Najma Rivzi explains in an article about a research program on this topic. In some cases, such as in parts of Kenya in 2006, rates of malnutrition in pregnant women were even higher than rates in children.\n\nWomen in some societies are traditionally given less food than men since men are perceived to have heavier workloads. Household chores and agricultural tasks can in fact be very arduous and require additional energy and nutrients; however, physical activity, which largely determines energy requirements, is difficult to estimate.\n\nWomen have unique nutritional requirements, and in some cases need more nutrients than men; for example, women need twice as much calcium as men.\n\nDuring pregnancy and breastfeeding, women must ingest enough nutrients for themselves and their child, so they need significantly more protein and calories during these periods, as well as more vitamins and minerals (especially iron, iodine, calcium, folic acid, and vitamins A, C, and K). In 2001 the FAO of the UN reported that iron deficiency afflicted 43 percent of women in developing countries and increased the risk of death during childbirth. A 2008 review of interventions estimated that universal supplementation with calcium, iron, and folic acid during pregnancy could prevent 105,000 maternal deaths (23.6 percent of all maternal deaths). Malnutrition has been found to affect three quarters of UK women aged 16-49 indicated by them having less folic acid than the WHO recommended levels.\n\nFrequent pregnancies with short intervals between them and long periods of breastfeeding add an additional nutritional burden.\n\nAccording to the FAO, women are often responsible for preparing food and have the chance to educate their children about beneficial food and health habits, giving mothers another chance to improve the nutrition of their children.\n\nMalnutrition and being underweight are more common in the elderly than in adults of other ages. If elderly people are healthy and active, the aging process alone does not usually cause malnutrition. However, changes in body composition, organ functions, adequate energy intake and ability to eat or access food are associated with aging, and may contribute to malnutrition. Sadness or depression can play a role, causing changes in appetite, digestion, energy level, weight, and well-being. A study on the relationship between malnutrition and other conditions in the elderly found that malnutrition in the elderly can result from gastrointestinal and endocrine system disorders, loss of taste and smell, decreased appetite and inadequate dietary intake. Poor dental health, ill-fitting dentures, or chewing and swallowing problems can make eating difficult. As a result of these factors, malnutrition is seen to develop more easily in the elderly.\n\nRates of malnutrition tend to increase with age with less than 10 percent of the \"young\" elderly (up to age 75) malnourished, while 30 to 65 percent of the elderly in home care, long-term care facilities, or acute hospitals are malnourished. Many elderly people require assistance in eating, which may contribute to malnutrition. However, the mortality rate due to undernourishment may be reduced. Because of this, one of the main requirements of elderly care is to provide an adequate diet and all essential nutrients. Providing the different nutrients such as protein and energy keeps even small but consistent weight gain.\n\nIn Australia malnutrition or risk of malnutrition occurs in 80 percent of elderly people presented to hospitals for admission. Malnutrition and weight loss can contribute to sarcopenia with loss of lean body mass and muscle function. Abdominal obesity or weight loss coupled with sarcopenia lead to immobility, skeletal disorders, insulin resistance, hypertension, atherosclerosis, and metabolic disorders. A paper from the \"Journal of the American Dietetic Association\" noted that routine nutrition screenings represent one way to detect and therefore decrease the prevalence of malnutrition in the elderly.\n\n\n"}
{"id": "1728209", "url": "https://en.wikipedia.org/wiki?curid=1728209", "title": "McWorld", "text": "McWorld\n\nMcWorld is a term referring to the spreading of McDonald's restaurants throughout the world as the result of globalization, and more generally to the effects of international 'McDonaldization' of services and commercialization of goods as an element of globalization as a whole. The name also refers to a 1990s advertising campaign for McDonald's, and to a children's website launched by the firm in 2008.\n\nCritics claim that fast food chain restaurants such as McDonald's are destructive towards many aspects of the indigenous cultures in countries where they have been introduced.\n\nIn March 1992, an article first published in \"The Atlantic Monthly\" by Rutgers political science professor Benjamin Barber entitled \"Jihad vs. McWorld\", described international commercialization as one of two great clashing forces of the 21st century, the other being tribalistic fundamentalism. According to his writing, there are four imperatives which constitute the McWorld: a market imperative, a resource imperative, an information technology imperative, and an ecological imperative. The four imperatives are transnational, transideological, transcultural and ecological. The contracting idea of McWorld, the Jihad, unlike those four imperatives, stress identity of each community.\n\nThe clashing forces results from what Barber explains as the two core doctrines of our age: globalism and retribalization. It was expanded and published in 1995, and became a bestselling book. \nMcWorld does not necessarily relate to democracy. It cares about the elements of democracy, but only to the degree that it promotes economic production and consumption.\nIn the book, Barber explains that liberalization of nation state oriented markets to a globalized market does not seem democratic. Democracy and liberal capitalism are terms commonly used as a correlation; how democracy leads to capitalist economy and vice versa. However Barber argues that multinational corporations pursuing profits outside their home country due to competition has less correlation with open society.\n\nA 1999 book entitled \"Mustard Seed Versus McWorld\" by evangelical minister Tom Sine implores Christians to reject the diminution of religious values that he contends results from excessive commercialization.\n\nThe name McWorld was originally the name of a TV campaign for the restaurant by Leo Burnett that ran many of its ads during Saturday morning cartoons and on other television channels and programming blocks targeted toward children in the United States in the mid 1990s. The adverts featured the exciting McDonald's-related happenings that would purportedly occur if children ran the world. These included fantasies such as having gym class every period in school and eating McDonald's at every meal. In addition to planet Earth, McWorld ads featured children ruling school, space, and other arenas typically dominated by an adult hegemony; adults were portrayed as inferior and ineffective. Memorably, each spot concluded with the phrase, \"McWORLD! Hey, it could happen!\", as a guitar chord played in the background. One such spot won a Golden Marble Award in 1998.\n\nMcWorld was also the name of an interactive \"virtual world\" website launched by McDonald's in 2008 on happymeal.com, and aimed at children. Visitors to the website could play games, go on quests, earn points and buy accessories for their treehouses and avatars (called \"mPals\"). The website shut down on February 7, 2014 and was replaced by McPlay.\n\nWhile the McWorld website bore some similarities to the ideas in the original McWorld campaign, such as children being in charge, it is an essentially distinct concept, created independently for a younger age group. McWorld was named by a vote of kids on happymeal.com.\n\n"}
{"id": "48000422", "url": "https://en.wikipedia.org/wiki?curid=48000422", "title": "Military globalization", "text": "Military globalization\n\nMilitary globalization is defined by David Held as “the process which embodies the growing extensity and intensity of military relations among the political units of the world system. Understood as such, it reflects both the expanding network of worldwide military ties and relations, as well as the impact of key military technological innovations (from steamships to satellites), which over time, have reconstituted the world into a single geostrategic space.\" Military globalization implies firmer integration of armed forces around the world into the global military system. For Robert Keohane and Joseph Nye military globalization entails “long-distance networks of interdependence in which force, and the threat or promise of force, are employed.”\n\nHeld divides the military globalization into three distinct phenomena: \n\n\nAll three processes above “are connected to technological development, which made them possible in the first place. The result is increasing global interdependence and complexity.\"\n\nThe process of military globalization starts with the Age of Discovery, when the European colonial empires began military operations on the global scale. Their \"imperial rivalry led to the First World War, which was the first global conflict in world history.\" Keohane dates military globalization at least from the time of the conquests of Alexander the Great.\n\n"}
{"id": "885795", "url": "https://en.wikipedia.org/wiki?curid=885795", "title": "Modern history", "text": "Modern history\n\nModern history, the modern period or the modern era, is the linear, global, historiographical approach to the time frame after post-classical history. Modern history can be further broken down into periods:\n\nThis article primarily covers the 1800–1950 time period with a brief summary of 1500–1800. For a more in depth article on modern times before 1800, see Early Modern period.\n\nIn the pre-modern era, many people's sense of self and purpose was often expressed via a faith in some form of deity, be it that in a single God or in many gods. Pre-modern cultures have not been thought of creating a sense of distinct individuality, though. Religious officials, who often held positions of power, were the spiritual intermediaries to the common person. It was only through these intermediaries that the general masses had access to the divine. Tradition was sacred to ancient cultures and was unchanging and the social order of ceremony and morals in a culture could be strictly enforced.\n\nThe term \"modern\" was coined in the 16th century to indicate present or recent times (ultimately derived from the Latin adverb \"modo\", meaning \"just now\"). The European Renaissance (c. 1420–1630), which marked the transition between the Late Middle Ages and Early Modern times, started in Italy and was spurred in part by the rediscovery of classical art and literature, as well as the new perspectives gained from the Age of Discovery and the invention of the telescope and microscope, expanding the borders of thought and knowledge.\n\nIn contrast to the pre-modern era, Western civilization made a gradual transition from pre-modernity to modernity when scientific methods were developed which led many to believe that the use of science would lead to all knowledge, thus throwing back the shroud of myth under which pre-modern peoples lived. New information about the world was discovered via empirical observation, versus the historic use of reason and innate knowledge.\n\nThe term \"Early Modern\" was introduced in the English language in the 1930s to distinguish the time between what has been called the Middle Ages and time of the late Enlightenment (1800) (when the meaning of the term \"Modern Ages\" was developing its contemporary form). It is important to note that these terms stem from European history. In usage in other parts of the world, such as in Asia, and in Muslim countries, the terms are applied in a very different way, but often in the context with their contact with European culture in the Age of Discovery.\n\nIn the Contemporary era, there were various socio-technological trends. Regarding the 21st century and the late modern world, the Information Age and computers were forefront in use, not completely ubiquitous but often present in everyday life. The development of Eastern powers was of note, with China and India becoming more powerful. In the Eurasian theater, the European Union and Russian Federation were two forces recently developed. A concern for Western world, if not the whole world, was the late modern form of terrorism and the warfare that has resulted from the contemporary terrorist acts.\n\nThe modern period has been a period of significant development in the fields of science, politics, warfare, and technology. It has also been an age of discovery and globalization. During this time, the European powers and later their colonies, began a political, economic, and cultural colonization of the rest of the world.\n\nBy the late 19th and 20th centuries, modernist art, politics, science and culture has come to dominate not only Western Europe and North America, but almost every civilized area on the globe, including movements thought of as opposed to the west and globalization. The modern era is closely associated with the development of individualism, capitalism, urbanization and a belief in the possibilities of technological and political progress.\n\nWars and other perceived problems of this era, many of which come from the effects of rapid change, and the connected loss of strength of traditional religious and ethical norms, have led to many reactions against modern development. Optimism and belief in constant progress has been most recently criticized by postmodernism while the dominance of Western Europe and Anglo-America over other continents has been criticized by postcolonial theory.\n\nOne common conception of modernity is the condition of Western history since the mid-15th century, or roughly the European development of movable type and the printing press. In this context the \"modern\" society is said to develop over many periods, and to be influenced by important events that represent breaks in the continuity.\n\nThe modern era includes the early period, called the early modern period, which lasted from c. 1500 to around c. 1800 (most often 1815). Particular facets of early modernity include:\n\nImportant events in the early modern period include:\nThis combination of epoch events totally changed thinking and thought in the early modern period, and so their dates serve as well as any to separate the old from the new modes.\n\nAs an Age of Revolutions dawned, beginning with those revolts in America and France, political changes were then pushed forward in other countries partly as a result of upheavals of the Napoleonic Wars and their impact on thought and thinking, from concepts from nationalism to organizing armies.\n\nThe early period ended in a time of political and economic change as a result of mechanization in society, the American Revolution, the first French Revolution; other factors included the redrawing of the map of Europe by the of the Congress of Vienna and the peace established by Second Treaty of Paris which ended the Napoleonic Wars.\n\nAs a result of the Industrial Revolution and the earlier political revolutions, the worldviews of Modernism emerged. The industrialization of many nations was initiated with the industrialization of Britain. Particular facets of the late modernity period include:\n\nOther important events in the development of the Late modern period include:\n\nOur most recent eraModern Timesbegins with the end of these revolutions in the 19th century, and includes the World Wars era (encompassing World War I and World War II) and the emergence of socialist countries that led to the Cold War. The \"contemporary era\" follows shortly afterward with the explosion of research and increase of knowledge known as the Information Age in the latter 20th and the early 21st century. Today's Postmodern era is seen in widespread digitality.\n\nHistorians consider the early modern period to be approximately between 1500 and 1800. It follows the Late Middle Ages and is marked by the first European colonies, the rise of strong centralized governments, and the beginnings of recognizable nation-states that are the direct antecedents of today's states.\n\nThe expansion of Islam took place in North and East Africa. In West Africa, various native nations existed. The Indian Empires and civilizations of Southeast Asia were a vital link in the spice trade. On the Indian subcontinent, the Mughal Empire existed. The archipelagic empires, the Sultanate of Malacca and later the Sultanate of Johor, controlled the southern areas.\n\nVarious Chinese dynasties and Japanese shogunates controlled the Asian sphere. In Japan, the Edo period from 1600 to 1868 is also referred to as the early modern period. In Korea, the early modern period is considered to have lasted from the rise of the Joseon Dynasty to the enthronement of King Gojong. In the Americas, Pre-Columbian peoples had built a large and varied civilization, including the Aztec Empire and alliance, the Inca civilization, the Mayan Empire and cities, and the Chibcha. In the west, kingdoms of Europe moved in the direction of reformation and expansion. Russia reached the Pacific coast in 1647 and consolidated its control over the Russian Far East in the 19th century.\n\nLater religious trends of the period saw the end of the aforementioned Muslim expansion. Christians and Christendom saw the end of the Crusades and of religious unity under the Roman Catholic Church. It was during this time that the Inquisitions and the Protestant Reformation took place.\n\nDuring the early modern period, an age of discovery and trade was undertaken by the Western European nations. Portugal, Spain, the Netherlands, the United Kingdom and France went on a colonial expansion and took possession of lands and set up colonies in Africa, southern Asia, and North and South America. Turkey colonized Southeastern Europe, and parts of the West Asia and North Africa. Russia took possession in Eastern Europe, Asia, and North America.\n\nIn China, urbanization increased as the population grew and as the division of labor grew more complex. Large urban centers, such as Nanjing and Beijing, also contributed to the growth of private industry. In particular, small-scale industries grew up, often specializing in paper, silk, cotton, and porcelain goods. For the most part, however, relatively small urban centers with markets proliferated around the country. Town markets mainly traded food, with some necessary manufactures such as pins or oil. Despite the xenophobia and intellectual introspection characteristic of the increasingly popular new school of neo-Confucianism, China under the later Ming Dynasty became isolated, prohibiting the construction of ocean going sea vessels. Despite isolationist policies the Ming Economy still suffered from an inflation due to an overabundance of Spanish New World silver entering its economy through new European colonies such as Macao. Ming China was further strained by victorious but costly wars to protect Korea from Japanese Invasion.\n\nThe Qing dynasty (1644–1912) was founded after the fall of the Ming, the last Han Chinese dynasty, by the Manchus. The Manchus were formerly known as the Jurchens. When Beijing was captured by Li Zicheng's peasant rebels in 1644, the Chongzhen Emperor, the last Ming emperor, committed suicide. The Manchus then allied with former Ming general Wu Sangui and seized control of Beijing, which became the new capital of the Qing dynasty. The Manchus adopted the Confucian norms of traditional Chinese government in their rule of China proper. Schoppa, the editor of \"The Columbia Guide to Modern Chinese History\" argues, \"A date around 1780 as the beginning of modern China is thus closer to what we know today as historical 'reality'. It also allows us to have a better baseline to understand the precipitous decline of the Chinese polity in the nineteenth and twentieth centuries.\"\n\nIn pre-modern Japan following the Sengoku period of \"warring states\", central government had been largely reestablished by Oda Nobunaga and Toyotomi Hideyoshi during the Azuchi–Momoyama period. After the Battle of Sekigahara in 1600, central authority fell to Tokugawa Ieyasu who completed this process and received the title of \"shōgun\" in 1603.\nSociety in the Japanese \"Tokugawa period\" (Edo society), unlike the shogunates before it, was based on the strict class hierarchy originally established by Toyotomi Hideyoshi. The \"daimyōs\" (feudal lords) were at the top, followed by the warrior-caste of samurai, with the farmers, artisans, and traders ranking below. The country was strictly closed to foreigners with few exceptions with the \"Sakoku\" policy. Literacy among the Japanese people rose in the two centuries of isolation.\n\nIn some parts of the country, particularly smaller regions, \"daimyōs\" and samurai were more or less identical, since \"daimyōs\" might be trained as samurai, and samurai might act as local lords. Otherwise, the largely inflexible nature of this social stratification system unleashed disruptive forces over time. Taxes on the peasantry were set at fixed amounts which did not account for inflation or other changes in monetary value. As a result, the tax revenues collected by the samurai landowners were worth less and less over time. This often led to numerous confrontations between noble but impoverished samurai and well-to-do peasants. None, however, proved compelling enough to seriously challenge the established order until the arrival of foreign powers.\n\nOn the Indian subcontinent, the Mughal Empire ruled most of India in the early 18th century. The \"classic period\" ended with the death and defeat of Emperor Aurangzeb in 1707 by the rising Hindu Maratha Empire, although the dynasty continued for another 150 years. During this period, the Empire was marked by a highly centralized administration connecting the different regions. All the significant monuments of the Mughals, their most visible legacy, date to this period which was characterised by the expansion of Persian cultural influence in the Indian subcontinent, with brilliant literary, artistic, and architectural results. The Maratha Empire was located in the south west of present-day India and expanded greatly under the rule of the Peshwas, the prime ministers of the Maratha empire. In 1761, the Maratha army lost the Third Battle of Panipat which halted imperial expansion and the empire was then divided into a confederacy of Maratha states.\n\nThe development of New Imperialism saw the conquest of nearly all eastern hemisphere territories by colonial powers. The commercial colonization of India commenced in 1757, after the Battle of Plassey, when the Nawab of Bengal surrendered his dominions to the British East India Company, in 1765, when the Company was granted the \"diwani\", or the right to collect revenue, in Bengal and Bihar, or in 1772, when the Company established a capital in Calcutta, appointed its first Governor-General, Warren Hastings, and became directly involved in governance.\n\nThe Maratha states, following the Anglo-Maratha wars, eventually lost to the British East India Company in 1818 with the Third Anglo-Maratha War. The rule lasted until 1858, when, after the Indian rebellion of 1857 and consequent of the Government of India Act 1858, the British government assumed the task of directly administering India in the new British Raj. In 1819 Stamford Raffles established Singapore as a key trading post for Britain in their rivalry with the Dutch. However, their rivalry cooled in 1824 when an Anglo-Dutch treaty demarcated their respective interests in Southeast Asia. From the 1850s onwards, the pace of colonization shifted to a significantly higher gear.\n\nThe Dutch East India Company (1800) and British East India Company (1858) were dissolved by their respective governments, who took over the direct administration of the colonies. Only Thailand was spared the experience of foreign rule, although, Thailand itself was also greatly affected by the power politics of the Western powers. Colonial rule had a profound effect on Southeast Asia. While the colonial powers profited much from the region's vast resources and large market, colonial rule did develop the region to a varying extent.\n\nMany major events caused Europe to change around the start of the 16th century, starting with the Fall of Constantinople in 1453, the fall of Muslim Spain and the discovery of the Americas in 1492, and Martin Luther's Protestant Reformation in 1517. In England the modern period is often dated to the start of the Tudor period with the victory of Henry VII over Richard III at the Battle of Bosworth in 1485. Early modern European history is usually seen to span from the start of the 15th century, through the Age of Enlightenment in the 17th and 18th centuries, until the beginning of the Industrial Revolution in the late 18th century.\n\nRussia experienced territorial growth through the 17th century, which was the age of Cossacks. Cossacks were warriors organized into military communities, resembling pirates and pioneers of the New World. The native land of the Cossacks is defined by a line of Russian/Ruthenian town-fortresses located on the border with the steppe and stretching from the middle Volga to Ryazan and Tula, then breaking abruptly to the south and extending to the Dnieper via Pereyaslavl. This area was settled by a population of free people practicing various trades and crafts.\n\nIn 1648, the peasants of Ukraine joined the Zaporozhian Cossacks in rebellion against Poland-Lithuania during the Khmelnytsky Uprising, because of the social and religious oppression they suffered under Polish rule. In 1654 the Ukrainian leader, Bohdan Khmelnytsky, offered to place Ukraine under the protection of the Russian Tsar, Aleksey I. Aleksey's acceptance of this offer led to another Russo-Polish War (1654–1667). Finally, Ukraine was split along the river Dnieper, leaving the western part (or Right-bank Ukraine) under Polish rule and eastern part (Left-bank Ukraine and Kiev) under Russian. Later, in 1670–71 the Don Cossacks led by Stenka Razin initiated a major uprising in the Volga region, but the Tsar's troops were successful in defeating the rebels. In the east, the rapid Russian exploration and colonisation of the huge territories of Siberia was led mostly by Cossacks hunting for valuable furs and ivory. Russian explorers pushed eastward primarily along the Siberian river routes, and by the mid-17th century there were Russian settlements in the Eastern Siberia, on the Chukchi Peninsula, along the Amur River, and on the Pacific coast. In 1648 the Bering Strait between Asia and North America was passed for the first time by Fedot Popov and Semyon Dezhnyov.\n\nTraditionally, the European intellectual transformation of and after the Renaissance bridged the Middle Ages and the Modern era. The Age of Reason in the Western world is generally regarded as being the start of modern philosophy, and a departure from the medieval approach, especially Scholasticism. Early 17th-century philosophy is often called the Age of Rationalism and is considered to succeed Renaissance philosophy and precede the Age of Enlightenment, but some consider it as the earliest part of the Enlightenment era in philosophy, extending that era to two centuries. The 18th century saw the beginning of secularization in Europe, rising to notability in the wake of the French Revolution.\n\nThe Age of Enlightenment is a time in Western philosophy and cultural life centered upon the 18th century in which reason was advocated as the primary source and legitimacy for authority. Enlightenment gained momentum more or less simultaneously in many parts of Europe and America. Developing during the Enlightenment era, Renaissance humanism as an intellectual movement spread across Europe. The basic training of the humanist was to speak well and write (typically, in the form of a letter). The term \"umanista\" comes from the latter part of the 15th century. The people were associated with the \"studia humanitatis\", a novel curriculum that was competing with the \"quadrivium\" and scholastic logic.\n\nRenaissance humanism took a close study of the Latin and Greek classical texts, and was antagonistic to the values of scholasticism with its emphasis on the accumulated commentaries; and humanists were involved in the sciences, philosophies, arts and poetry of classical antiquity. They self-consciously imitated classical Latin and deprecated the use of medieval Latin. By analogy with the perceived decline of Latin, they applied the principle of \"ad fontes\", or back to the sources, across broad areas of learning.The quarrel of the Ancients and the Moderns was a literary and artistic quarrel that heated up in the early 1690s and shook the Académie française. The opposing two sides were, the Ancients (\"Anciens\") who constrain choice of subjects to those drawn from the literature of Antiquity and the Moderns (\"Modernes\"), who supported the merits of the authors of the century of Louis XIV. Fontenelle quickly followed with his \"Digression sur les anciens et les modernes\" (1688), in which he took the Modern side, pressing the argument that modern scholarship allowed modern man to surpass the ancients in knowledge.\n\nThe Scientific Revolution was a period when European ideas in classical physics, astronomy, biology, human anatomy, chemistry, and other classical sciences were rejected and led to doctrines supplanting those that had prevailed from Ancient Greece to the Middle Ages which would lead to a transition to modern science. This period saw a fundamental transformation in scientific ideas across physics, astronomy, and biology, in institutions supporting scientific investigation, and in the more widely held picture of the universe. Individuals started to question all manners of things and it was this questioning that led to the Scientific Revolution, which in turn formed the foundations of contemporary sciences and the establishment of several modern scientific fields.\n\nToward the middle and latter stages of the Age of Revolution, the French political and social revolutions and radical change saw the French governmental structure, previously an absolute monarchy with feudal privileges for the aristocracy and Catholic clergy transform, changing to forms based on Enlightenment principles of citizenship and inalienable rights. The first revolution led to government by the National Assembly, the second by the Legislative Assembly, and the third by the Directory.\n\nThe changes were accompanied by violent turmoil which included the trial and execution of the king, vast bloodshed and repression during the Reign of Terror, and warfare involving every other major European power. Subsequent events that can be traced to the Revolution include the Napoleonic Wars, two separate restorations of the monarchy, and two additional revolutions as modern France took shape. In the following century, France would be governed at one point or another as a republic, constitutional monarchy, and two different empires.\n\nDuring the French Revolution, the National Assembly, which existed from June 17 to July 9 of 1789, was a transitional body between the Estates-General and the National Constituent Assembly.\n\nThe Legislative Assembly was the legislature of France from October 1, 1791 to September 1792. It provided the focus of political debate and revolutionary law-making between the periods of the National Constituent Assembly and of the National Convention.\n\nThe Executive Directory was a body of five Directors that held executive power in France following the Convention and preceding the Consulate. The period of this regime (2 November 1795 until 10 November 1799), commonly known as the Directory (or Directoire) era, constitutes the second to last stage of the French Revolution. Napoleon, before seizing the title of Emperor, was elected as First Consul of the Consulate of France.\n\nThe campaigns of French Emperor and General Napoleon Bonaparte characterized the Napoleonic Era. Born on Corsica as the French invaded, and dying suspiciously on the tiny British Island of St. Helena, this brilliant commander, controlled a French Empire that, at its height, ruled a large portion of Europe directly from Paris, while many of his friends and family ruled countries such as Spain, Poland, several parts of Italy and many other Kingdoms Republics and dependencies. The Napoleonic Era changed the face of Europe forever, and old Empires and Kingdoms fell apart as a result of the mighty and \"Glorious\" surge of Republicanism.\n\nItalian unification was the political and social movement that annexed different states of the Italian peninsula into the single state of Italy in the 19th century. There is a lack of consensus on the exact dates for the beginning and the end of this period, but many scholars agree that the process began with the end of Napoleonic rule and the Congress of Vienna in 1815, and approximately ended with the Franco-Prussian War in 1871, though the last \"città irredente\" did not join the Kingdom of Italy until after World War I.\n\nToward the end of the early modern period, Europe was dominated by the evolving system of mercantile capitalism in its trade and the New Economy. European states and politics had the characteristic of Absolutism. The French power and English revolutions dominated the political scene. There eventually evolved an international balance of power that held at bay a great conflagration until years later.\n\nThe end date of the early modern period is usually associated with the Industrial Revolution, which began in Britain in about 1750. Another significant date is 1789, the beginning of the French Revolution, which drastically transformed the state of European politics and ushered in the Prince Edward Era and modern Europe.\n\nThe French and Indian Wars were a series of conflicts in North America that represented the actions there that accompanied the European dynastic wars. In Quebec, the wars are generally referred to as the Intercolonial Wars. While some conflicts involved Spanish and Dutch forces, all pitted Great Britain, its colonies and American Indian allies on one side and France, its colonies and Indian allies on the other.\n\nThe expanding French and British colonies were contending for control of the western, or interior, territories. Whenever the European countries went to war, there were actions within and by these colonies although the dates of the conflict did not necessarily exactly coincide with those of the larger conflicts.\n\nBeginning the Age of Revolution, the American Revolution and the ensuing political upheaval during the last half of the 18th century saw the Thirteen Colonies of North America overthrow the governance of the Parliament of Great Britain, and then reject the British monarchy itself to become the sovereign United States of America. In this period the colonies first rejected the authority of the Parliament to govern them without representation, and formed self-governing independent states. The Second Continental Congress then joined together against the British to defend that self-governance in the armed conflict from 1775 to 1783 known as the American Revolutionary War (also called American War of Independence).\n\nThe American Revolution began with fighting at Lexington and Concord. On July 4, 1776, they issued the Declaration of Independence, which proclaimed their independence from Great Britain and their formation of a cooperative union. In June 1776, Benjamin Franklin was appointed a member of the Committee of Five that drafted the Declaration of Independence. Although he was temporarily disabled by gout and unable to attend most meetings of the Committee, Franklin made several small changes to the draft sent to him by Thomas Jefferson.\n\nThe rebellious states defeated Great Britain in the American Revolutionary War, the first successful colonial war of independence. While the states had already rejected the governance of Parliament, through the Declaration the new United States now rejected the legitimacy of the monarchy to demand allegiance. The war raged for seven years, with effective American victory, followed by formal British abandonment of any claim to the United States with the Treaty of Paris.\n\nThe Philadelphia Convention set up the current United States; the United States Constitution ratification the following year made the states part of a single republic with a limited central government. The Bill of Rights, comprising ten constitutional amendments guaranteeing many fundamental civil rights and freedoms, was ratified in 1791.\n\nThe decolonization of the Americas was the process by which the countries in the Americas gained their independence from European rule. Decolonization began with a series of revolutions in the late 18th and early-to-mid-19th centuries. The Spanish American wars of independence were the numerous wars against Spanish rule in Spanish America that took place during the early 19th century, from 1808 until 1829, directly related to the Napoleonic French invasion of Spain. The conflict started with short-lived governing juntas established in Chuquisaca and Quito opposing the composition of the Supreme Central Junta of Seville.\n\nWhen the Central Junta fell to the French, numerous new Juntas appeared all across the Americas, eventually resulting in a chain of newly independent countries stretching from Argentina and Chile in the south, to Mexico in the north. After the death of the king Ferdinand VII, in 1833, only Cuba and Puerto Rico remained under Spanish rule, until the Spanish–American War in 1898. Unlike the Spanish, the Portuguese did not divide their colonial territory in America. The captaincies they created were subdued to a centralized administration in Salvador (later relocated to Rio de Janeiro) which reported directly to the Portuguese Crown until its independence in 1822, becoming the Empire of Brazil.\n\n[[File:Maquina vapor Watt ETSIIM.jpg|thumb|Maquina vapor Watt ETSIIM]]\n\nThe development of the [[steam engine]] started the Industrial Revolution in [[United Kingdom|Great Britain]]. The steam engine was created to pump water from coal mines, enabling them to be deepened beyond [[groundwater]] levels. The date of the Industrial Revolution is not exact. [[Eric Hobsbawm]] held that it \"broke out\" in the 1780s and was not fully felt until the 1830s or 1840s, while [[T.S. Ashton]] held that it occurred roughly between 1760 and 1830 (in effect the reigns of [[George III of the United Kingdom|George III]], The [[English Regency|Regency]], and [[George IV of the United Kingdom|George IV]]). The great changes of centuries before the 19th were more connected with ideas, religion or military conquest, and technological advance had only made small changes in the material wealth of ordinary people.\n\nThe first Industrial Revolution merged into the Second Industrial Revolution around 1850, when technological and economic progress gained momentum with the development of steam-powered ships and railways, and later in the 19th century with the [[internal combustion engine]] and [[electric power]] generation. The Second Industrial Revolution was a phase of the Industrial Revolution; labeled as the separate Technical Revolution. From a technological and a social point of view there is no clean break between the two. Major innovations during the period occurred in the chemical, electrical, petroleum, and steel industries. Specific advancements included the introduction of oil fired steam turbine and internal combustion driven steel ships, the development of the airplane, the practical commercialization of the automobile, mass production of consumer goods, the perfection of canning, mechanical refrigeration and other food preservation techniques, and the invention of the telephone.\n\n[[Industrialization]] is the process of social and economic change whereby a human group is transformed from a pre-industrial society into an industrial one. It is a subdivision of a more general [[modernization]] process, where [[social change]] and [[economic development]] are closely related with [[innovation|technological innovation]], particularly with the development of large-scale energy and metallurgy production. It is the extensive organization of an economy for the purpose of manufacturing. Industrialization also introduces a form of philosophical change, where people obtain a different attitude towards their [[nature|perception of nature]].\n\n[[File:First passenger railway 1830.jpg|left|thumb|[[Liverpool and Manchester Railway]] from 1830, world's first railway.]]\nAn [[economic system|economy]] based on [[manual labour]] was replaced by one dominated by industry and the manufacture of machinery. It began with the mechanization of the [[textile]] industries and the development of [[iron]]-making techniques, and trade expansion was enabled by the introduction of [[canal]]s, improved roads, and then [[rail transport|railways]].\n\nThe introduction of [[steam engine|steam power]] (fuelled primarily by [[coal]]) and powered machinery (mainly in [[textile manufacturing]]) underpinned the dramatic increases in production capacity. The development of all-metal [[machine tool]]s in the first two decades of the 19th century facilitated the manufacture of more production machines for manufacturing in other industries.\n\nThe modern [[petroleum industry]] started in 1846 with the discovery of the process of refining [[kerosene]] from [[coal]] by [[Nova Scotia]]n [[Abraham Pineo Gesner]]. [[Ignacy Łukasiewicz]] improved Gesner's method to develop a means of refining kerosene from the more readily available \"rock oil\" (\"petr-oleum\") [[seep]]s in 1852 and the first rock oil mine was built in [[Bóbrka, Krosno County|Bóbrka]], near [[Krosno]] in [[Galicia (Central Europe)|Galicia]] in the following year. In 1854, [[Benjamin Silliman]], a science professor at [[Yale University]] in [[New Haven, Connecticut|New Haven]], was the first to fractionate petroleum by distillation. These discoveries rapidly spread around the world.\n\n[[File:Teslathinker.jpg|thumb|Nikola Tesla with his high-frequency transformer at East Houston Street, New York|alt=|193x193px]]\n\nEngineering achievements of the revolution ranged from electrification to developments in materials science. The advancements made a great contribution to the quality of life. In the first revolution, [[Lewis Paul]] was the original inventor of roller spinning, the basis of the water frame for spinning cotton in a cotton mill. [[Matthew Boulton]] and [[James Watt]]'s improvements to the steam engine were fundamental to the changes brought by the Industrial Revolution in both the Kingdom of Great Britain and the world.\n\nIn the latter part of the second revolution, [[Thomas Alva Edison]] developed many devices that greatly influenced life around the world and is often credited with the creation of the first industrial research laboratory. In 1882, Edison switched on the world's first large-scale [[electrical supply network]] that provided 110 volts direct current to fifty-nine customers in lower Manhattan. Also toward the end of the second industrial revolution, [[Nikola Tesla]] made many contributions in the field of [[electricity]] and [[magnetism]] in the late 19th and early 20th centuries.\n\nThe Industrial Revolutions were major [[technology|technological]], [[socioeconomics|socioeconomic]], and [[culture|cultural]] changes in late 18th and early 19th centuries that began in Britain and spread throughout the world. The effects spread throughout [[Western Europe]] and North America during the 19th century, eventually affecting the majority of the world. The impact of this change on [[society]] was enormous and is often compared to the [[Neolithic revolution]], when mankind developed [[agriculture]] and gave up its [[nomad]]ic lifestyle.\n\nIt has been argued that [[Gross Domestic Product|GDP]] per capita was much more stable and progressed at a much slower rate until the industrial revolution and the emergence of the modern [[capitalism|capitalist]] economy, and that it has since increased rapidly in capitalist countries.\n\n[[File:Alexanderplatz Berlin 1848.jpg|left|thumb|[[Revolutions of 1848|German Revolution]], Berlin 1848]]\nThe [[Revolutions of 1848|European Revolutions of 1848]], known in some countries as the Spring of Nations or the Year of Revolution, were a series of political upheavals throughout the European continent. Described as a revolutionary wave, the period of unrest began in France and then, further propelled by the French Revolution of 1848, soon spread to the rest of Europe. Although most of the revolutions were quickly put down, there was a significant amount of violence in many areas, with tens of thousands of people tortured and killed. While the immediate political effects of the revolutions were reversed, the long-term reverberations of the events were far-reaching.\n\nIndustrial age [[reform movement]]s began the gradual change of society rather than with episodes of rapid fundamental changes. The reformists' ideas were often grounded in liberalism, although they also possessed aspects of utopian, socialist or religious concepts. The Radical movement campaigned for electoral reform, a reform of the Poor Laws, free trade, educational reform, postal reform, prison reform, and public sanitation.\n\nFollowing the Enlightenment's ideas, the reformers looked to the [[Scientific Revolution]] and industrial progress to solve the social problems which arose with the Industrial Revolution. Newton's natural philosophy combined a mathematics of axiomatic proof with the mechanics of physical observation, yielding a coherent system of verifiable predictions and replacing a previous reliance on revelation and inspired truth. Applied to public life, this approach yielded several successful campaigns for changes in social policy.\n\n[[File:Russian Peasant Girls-retouched.jpg|thumb|Russian Peasant Girls, 1900]]\nUnder [[Peter the Great]], Russia was proclaimed an Empire in 1721 and became recognized as a world power. Ruling from 1682 to 1725, Peter defeated Sweden in the [[Great Northern War]], forcing it to cede West [[Karelia]] and [[Ingria]] (two regions lost by Russia in the [[Time of Troubles]]), as well as [[Governorate of Estonia|Estland]] and [[Livland]], securing Russia's access to the sea and sea trade. On the [[Baltic Sea]] Peter founded a new capital called [[Saint Petersburg]], later known as Russia's \"Window to Europe\". [[Peter the Great's reforms]] brought considerable Western European cultural influences to Russia. [[Catherine the Great|Catherine II]] (\"the Great\"), who ruled in 1762–96, extended Russian political control over the Polish-Lithuanian Commonwealth and incorporated most of its territories into Russia during the [[Partitions of Poland]], pushing the Russian frontier westward and southward. Russia would [[Russian conquest of Siberia|colonize]] the vast Asian lands of [[Siberia]] expanding by land to the Pacific Coast of Asia, and [[Russian America|North America]]. As the large realm embraced [[Absolute monarchy|Absolute Monarchy]] Russia remained more conservative than its western neighbors. In the 19th century, Russia was invaded by France in 1812 but emerged as a more powerful superpower afterwards. Nonetheless industrialization did not come to Russia until the 1870s. The Medieval practice of serfdom was [[Emancipation reform of 1861|abolished]] in 1861 freeing over thirty million Russian peasants. A market economy finally emerged in the Russian Empire. However class warfare rose, and the nation was vulnerable due to rivalries with the [[United Kingdom]], the [[Ottoman Empire]], and the [[Empire of Japan|Japanese Empire]].\n\nHistorians define the 19th century [[List of time periods|historical era]] as stretching from 1815 (the [[Congress of Vienna]]) to 1914 (the outbreak of the [[World War I|First World War]]). Alternatively, [[Eric Hobsbawm]] defined the [[The long 19th century|\"Long Nineteenth Century\"]] as spanning the years 1789 to 1914.\n\n[[File:World heads of state in 1889.jpg|upright=1.6|thumb|alt=Painting of a group of standing and seated heads of state in a variety of national uniforms and formal dress|\"The World's Sovereigns\", 1889]]\n\nIn the 1800s and early 1900s, once great and powerful Empires such as Spain, Ottoman Turkey, the Mughal Empire, and the Kingdom of Portugal began to break apart. Spain, which was at one time unrivaled in Europe, had been declining for a long time when it was crippled by Napoleon Bonaparte's invasion. Sensing the time was right, Spain's vast colonies in South America began a series of rebellions that ended with almost all of the Spanish territories gaining their independence.\n\nThe once mighty Ottoman Empire was wracked with a series of revolutions, resulting with the Ottoman's only holding a small region that surrounded the capital, Istanbul.\n\n[[File:Imperalism.xcf|left|thumb|Montage that depicts European wars of imperialism. By clockwise, wars include [[French conquest of Algeria|French Algerian War]], [[First Opium War|Opium War]], [[Russian conquest of Central Asia|Russian Conquest of Central Asia]] and [[Anglo-Zulu War|Zulu War]]]]\nThe Mughal empire, which was descended from the Mongol Khanate, was bested by the upcoming [[Maratha Confederacy]]. All was going well for the [[Maratha]]s until the British took an interest in the riches of India and the British ended up ruling not just the boundaries of Modern India, but also Pakistan, Burma, Nepal, Bangladesh and some Southern Regions of Afghanistan.\n\nThe King of Portugal's vast territory of Brazil reformed into the independent Empire of Brazil. With the defeat of Napoleonic France, Britain became undoubtedly the most powerful country in the world, and by the end of the First World War controlled a Quarter of the world's population and a third of its surface. However, the power of the British Empire did not end on land, since it had the greatest navy on the planet. Electricity, steel, and petroleum enabled Germany to become a great [[Power (international)|international power]] that raced to create empires of its own.\n\nThe [[Meiji Restoration]] was a chain of events that led to enormous changes in Japan's political and social structure that was taking a firm hold at the beginning of the [[Meiji Era]] which coincided the opening of Japan by the arrival of the [[Black Ships]] of [[Commodore (USN)|Commodore]] [[Matthew C. Perry|Matthew Perry]] and made [[Imperial Japan]] a [[great power]]. [[Imperial Russia|Russia]] and [[Qing Dynasty]] China failed to keep pace with the other world powers which led to massive social unrest in both empires. The Qing Dynasty's military power weakened during the 19th century, and faced with international pressure, massive [[rebellion]]s and defeats in wars, the dynasty declined after the mid-19th century.\n\nEuropean powers controlled parts of Oceania, with French [[New Caledonia]] from 1853 and [[French Polynesia]] from 1889; the Germans established colonies in [[German New Guinea|New Guinea]] in 1884, and [[German Samoa|Samoa]] in 1900. The United States expanded into the Pacific with Hawaii becoming a [[Territory of Hawaii|U.S. territory]] from 1898. Disagreements between the US, Germany and UK over Samoa led to the [[Tripartite Convention (1899)|Tripartite Convention of 1899]].\n\n[[File:British Empire 1897.jpg|thumb|The British Empire in 1897, marked in the traditional colour for imperial British dominions on maps|alt=|left]]The Victorian era of the United Kingdom was the period of [[Queen Victoria]]'s reign from June 1837 to January 1901. This was a long period of prosperity for the British people, as profits gained from the overseas British Empire, as well as from industrial improvements at home, allowed a large, educated middle class to develop. Some scholars would extend the beginning of the period—as defined by a variety of sensibilities and political games that have come to be associated with the Victorians—back five years to the passage of the [[Reform Act 1832]].\n[[File:Fleet Street. By James Valentine c.1890..jpg|alt=Fleet Street in London, 1890.|thumb|Fleet Street in London capital of the [[British Empire]], 1890]]\nIn Britain's \"imperial century\", victory over Napoleon left Britain without any serious international rival, other than Russia in central Asia. Unchallenged at sea, Britain adopted the role of global policeman, a state of affairs later known as the \"[[Pax Britannica]]\", and a foreign policy of \"[[splendid isolation]]\". Alongside the formal control it exerted over its own colonies, Britain's dominant position in world trade meant that it effectively controlled the economies of many nominally independent countries, such as China, [[Argentina]] and [[Thailand|Siam]], which has been generally characterized as \"[[informal empire]]\". Of note during this time was the [[Anglo-Zulu War]], which was fought in 1879 between the British Empire and the [[Zulu Empire]].\n\nBritish imperial strength was underpinned by the [[Steamboat|steamship]] and the [[Telegraphy|telegraph]], new technologies invented in the second half of the 19th century, allowing it to control and defend the Empire. By 1902, the British Empire was linked together by a network of telegraph cables, the so-called [[All Red Line]]. Growing until 1922, around of territory and roughly 458 million people were added to the British Empire. The British established colonies in Australia in 1788, New Zealand in 1840 and [[Colonial Fiji|Fiji]] in 1872, with much of [[Oceania]] becoming part of the British Empire.\n\nThe [[Bourbon Restoration]] followed the ousting of Napoleon I of France in 1814. The Allies restored the Bourbon Dynasty to the French throne. The ensuing period is called the Restoration, following French usage, and is characterized by a sharp conservative reaction and the re-establishment of the Roman Catholic Church as a power in French politics. The [[July Monarchy]] was a period of liberal constitutional monarchy in France under King Louis-Philippe starting with the July Revolution (or Three Glorious Days) of 1830 and ending with the Revolution of 1848. The [[Second French Empire|Second Empire]] was the Imperial Bonapartist regime of Napoleon III from 1852 to 1870, between the Second Republic and the Third Republic, in France.\n\n[[File:BismarckundNapoleonIII.jpg|left|thumb|[[Napoleon III of France|Napoleon III]] and [[Otto von Bismarck|Bismarck]] after the [[Battle of Sedan]]]]\nThe [[Franco-Prussian War]] was a conflict between France and Prussia, while Prussia was backed up by the North German Confederation, of which it was a member, and the South German states of Baden, Württemberg and Bavaria. The complete Prussian and German victory brought about the final unification of Germany under King Wilhelm I of Prussia. It also marked the downfall of Napoleon III and the end of the Second French Empire, which was replaced by the Third Republic. As part of the settlement, almost all of the territory of Alsace-Lorraine was taken by Prussia to become a part of Germany, which it would retain until the end of World War I.\n\nThe [[French Third Republic]] was the republican government of France between the end of the Second French Empire following the defeat of Louis-Napoléon in the Franco-Prussian war in 1870 and the Vichy Regime after the invasion of France by the German Third Reich in 1940. The Third Republic endured seventy years, making it the most long-lasting regime in France since the collapse of the Ancien Régime in the French Revolution of 1789.\n\n[[File:Une Dame d´une Fortune Ordinaire dans son Intérieur au Milieu de ses Habitudes Journalières, by Jean-Baptiste Debret 1823.jpg|thumb|Depiction of [[Slavery]] in [[Brazil]], before 1823]]\n\n[[Slavery]] was greatly reduced around the world in the 19th century. Following a successful [[Haitian Revolution|slave revolt in Haiti]], Britain forced the [[Barbary pirates]] to halt their practice of kidnapping and enslaving Europeans, [[Slavery Abolition Act|banned slavery throughout its domain]], and charged its navy with ending the global [[slave trade]]. Slavery was then abolished in [[Emancipation reform of 1861 in Russia|Russia]], [[Emancipation Proclamation|America]], and [[Lei Áurea|Brazil]].\n\nFollowing the abolition of the slave trade in 1807 and propelled by economic exploitation, the [[Scramble for Africa]] was initiated formally at the [[Berlin West Africa Conference]] in 1884–1885. The Berlin Conference attempted to avoid war among the European powers by allowing the European rival countries to carve up the continent of Africa into national colonies. Africans were not consulted.\n\nThe major European powers laid claim to the areas of [[Africa]] where they could exhibit a sphere of influence over the area. These claims did not have to have any substantial land holdings or treaties to be legitimate. The European power that demonstrated its control over a territory accepted the mandate to rule that region as a national [[colony]]. The European nation that held the claim developed and benefited from their colony’s commercial interests without having to fear rival European competition. With the colonial claim came the underlying assumption that the European power that exerted control would use its mandate to offer protection and provide welfare for its colonial peoples, however, this principle remained more theory than practice. There were many documented instances of material and moral conditions deteriorating for native Africans in the late nineteenth and early twentieth centuries under European colonial rule, to the point where the colonial experience for them has been described as \"hell on earth.\"\n[[File:Kongokonferenz.jpg|left|thumb|European officials staking claims to [[Africa]] in the [[Congress of Berlin]] ]]\nAt the time of the [[Berlin Conference]], Africa contained one-fifth of the world’s population living in one-quarter of the world’s land area. However, from Europe's perspective, they were dividing an unknown continent. European countries established a few coastal colonies in Africa by the mid-nineteenth century, which included [[Cape Colony]] (Great Britain), [[Angola]] (Portugal), and [[Algeria]] (France), but until the late nineteenth century Europe largely traded with free African states without feeling the need for territorial possession. Until the 1880s most of Africa remained uncharted, with western maps from the period generally showing blank spaces for the continent’s interior.\n\nFrom the 1880s to 1914, the European powers expanded their control across the African continent, competing with each other for Africa’s land and resources. [[Great Britain]] controlled various colonial holdings in East Africa that spanned the length of the African continent from Egypt in the north to South Africa. The [[France|French]] gained major ground in West Africa, and the [[Portugal|Portuguese]] held colonies in southern Africa. [[Germany]], [[Italy]], and [[Spain]] established a small number of colonies at various points throughout the continent, which included German East Africa (Tanganyika) and German Southwest Africa for Germany, Eritrea and Libya for Italy, and the Canary Islands and Rio de Oro in northwestern Africa for Spain. Finally, for [[Leopold II of Belgium|King Leopold]] (ruled from 1865 to 1909), there was the large “piece of that great African cake” known as the [[Belgian Congo|Congo]], which became his personal fiefdom. By 1914, almost the entire continent was under European control. [[Liberia]], which was settled by freed American slaves in the 1820s, and Abyssinia ([[Ethiopia]]) in eastern Africa were the last remaining independent African states.\n\n[[File:View of Kobe LACMA M.91.377.8.jpg|left|thumb|Kobe Japan and its harbor, 1865. Hand colored.]] Around the end of the 19th century and into the 20th century, the [[Meiji era]] occurred during the reign of the [[Emperor Meiji|Meiji Emperor]]. During this time, Japan started its modernization and rose to world power status. This [[Japanese era name|era name]] means \"Enlightened Rule\". In Japan, the Meiji Restoration started in the 1860s, marking the rapid modernization by the Japanese themselves along European lines. Much research has focused on the issues of discontinuity versus continuity with the previous Tokugawa Period. It was not until the beginning of the Meiji Era that the Japanese government began taking modernization seriously. Japan expanded its military production base by opening arsenals in various locations. The [[Ministry of War (pre-modern Japan)|hyobusho]] (war office) was replaced with a [[Imperial Japanese Army|War Department]] and a [[Imperial Japanese Navy|Naval Department.]] The [[samurai]] class suffered great disappointment the following years.\n\nLaws were instituted that required every able-bodied male Japanese citizen, regardless of class, to serve a mandatory term of three years with the first reserves and two additional years with the second reserves. This action, the deathblow for the samurai warriors and their \"daimyōs\", initially met resistance from both the peasant and warrior alike. The peasant class interpreted the term for military service, ketsu-eki (\"blood tax\") literally, and attempted to avoid service by any means necessary. The Japanese government began modelling their ground forces after the French military. The French government contributed greatly to the training of Japanese officers. Many were employed at the military academy in Kyoto, and many more still were feverishly translating French field manuals for use in the Japanese ranks. Japan's modernized military gave Japan the opportunity to engage in Imperialism with its victory against the [[Qing dynasty|Qing Empire]] in the [[First Sino-Japanese War]] Japan annexed [[Taiwan under Japanese rule|Taiwan]], [[Korea under Japanese rule|Korea]] and the Chinese province of [[Shandong|Shangdong]].\n\nAfter the death of the Meiji Emperor, the [[Emperor Taishō|Taishō Emperor]] took the throne, the[[Taishō period]] was a time of democratic reform granting democratic rights to all Japanese men. Foreigners would instrumental in aiding in Japan's modernization. A key foreign observer of the remarkable and rapid changes in [[culture of Japan|Japanese society]] in this period was [[Ernest Mason Satow]].\n\n[[File:Westward the Course of Empire.jpg|thumb|American westward expansion is idealized in [[Emanuel Leutze]]'s famous painting \"Westward the Course of Empire Takes its Way\" (1861).]]\nThe [[Antebellum Age]] was a period of increasing division in the country based on the growth of slavery in the [[Southern United States|American South]] and in the western territories of [[Kansas]] and [[Nebraska]] that eventually led to the [[American Civil War|Civil War]] in 1861. The Antebellum Period is often considered to have begun with the [[Kansas–Nebraska Act]] of 1854, although it may have begun as early as 1812. This period is also significant because it marked the transition of American manufacturing to the industrial revolution.\n\n\"[[Manifest destiny]]\" was the belief that the United States was destined to expand across the North American continent, from the Atlantic seaboard to the Pacific Ocean. During this time, the United States expanded to the Pacific Ocean—\"from sea to shining sea\"—largely defining the borders of the contiguous United States as they are today.\n\n[[File:Gettysburg by Britton.ogg|left|thumb|Modern recording of [[Gettysburg Address]] originally spoken by [[President of the United States|U.S President]] [[Abraham Lincoln]]]]\n[[File:Thure de Thulstrup - L. Prang and Co. - Battle of Gettysburg - Restoration by Adam Cuerden 0.5.jpg|left|thumb|[[Battle of Gettysburg]] – Restoration by Adam Cuerden 0.5]]\n\nThe American Civil War began when seven [[Slave state|Southern slave states]] declared their [[secession]] from the U.S. and formed the [[Confederate States of America]], the Confederacy (four more states joinged the Confederacy later). Led by [[Jefferson Davis]], they fought against the [[Union (American Civil War)|U.S. federal government (the Union)]] under President [[Abraham Lincoln]], which was supported by all the free states and the five [[Border states (American Civil War)|border slave states]] in the north.\n\nNorthern leaders agreed that victory would require more than the end of fighting. Secession and Confederate nationalism had to be totally repudiated and all forms of slavery or quasi-slavery had to be eliminated. Lincoln proved effective in mobilizing support for the war goals, raising large armies and supplying them, avoiding foreign interference, and making the end of slavery a war goal. The Confederacy had a larger area than it could defend, and it failed to keep its ports open and its rivers clear as was the case in the [[Siege of Vicksburg|Battle of Vickysburg]]. The [[Union (American Civil War)|North]] kept up the pressure as the South could barely feed and clothe its soldiers. Its soldiers, especially those in the East under the command of General [[Robert E. Lee]] proved highly resourceful until they finally were overwhelmed by Generals [[Ulysses S. Grant]] and [[William Tecumseh Sherman|William T. Sherman]] in 1864–65. The [[Reconstruction era|Reconstruction Era]] (1863–77) began with the [[Emancipation Proclamation|Emancipation proclamation]] in 1863, and included freedom, full citizenship and voting rights for Southern blacks. It was followed by a reaction that left the blacks in a second class status legally, politically, socially and economically until the 1960s.\n\n[[File:Flatiron Building and Street Scene October 8th 1902 New York City.ogv|thumb|1902 New York City in early [[skyscraper]]s]]\n\nDuring the Gilded Age, there was substantial growth in population in the United States and extravagant displays of wealth and excess of America's upper-class during the post-Civil War and post-Reconstruction era, in the late 19th century. The wealth polarization derived primarily from industrial and population expansion. The businessmen of the [[Second Industrial Revolution]] created industrial towns and cities in the [[Northeastern United States|Northeast]] with new factories, and contributed to the creation of an ethnically diverse industrial [[working class]] which produced the wealth owned by rising super-rich [[Robber baron (industrialist)|industrialists and financiers called the \"robber barons\"]]. An example is the company of [[John D. Rockefeller]], who was an important figure in shaping the new oil industry. Using highly effective tactics and aggressive practices, later widely criticized, [[Standard Oil]] absorbed or destroyed most of its competition.\n\nThe creation of a modern industrial economy took place. With the creation of a [[Infrastructure|transportation and communication infrastructure]], the [[corporation]] became the dominant form of business organization and a [[Management|managerial revolution]] transformed [[business]] operations. In 1890, [[Congress of the United States|Congress]] passed the [[Sherman Antitrust Act]]—the source of all American anti-monopoly laws. The law forbade every contract, scheme, deal, or conspiracy to restrain trade, though the phrase \"restraint of trade\" remained subjective. By the beginning of the 20th century, per capita income and [[industrial production]] in the United States exceeded that of any other country except Britain. Long hours and hazardous working conditions led many workers to attempt to form [[labor union]]s despite strong opposition from industrialists and the courts. But the courts did protect the marketplace, declaring the Standard Oil group to be an \"unreasonable\" [[monopoly]] under the [[Sherman Antitrust Act]] in 1911. It ordered Standard to break up into 34 independent companies with different boards of directors.\n\n[[File:Liszt- au bord d une.ogg|thumb|An example of 19th century [[Classical music|Classical Music]] Liszt- au bord d une, 1855]]\n[[File:Darwin's finches by Gould.jpg|left|thumb|Charles Darwin's finches by Gould, 1882. [[Charles Darwin]] used the example of [[Darwin's finches|finches]] in the [[Galápagos Islands|Galapagos Islands]] as evidence for the [[Natural selection|Theory of Evolution]].]]\nReplacing the [[classical physics]] in use since the end of the scientific revolution, \"[[modern physics]]\" arose in the early 20th century with the advent of [[quantum physics]], substituting [[mathematical physics|mathematical studies]] for [[experimental physics|experimental studies]] and examining [[equation]]s to build a [[theoretical structure]]. The [[old quantum theory]] was a collection of results which predate modern [[quantum mechanics]], but were never complete or self-consistent. The collection of [[heuristic]] prescriptions for quantum mechanics were the first corrections to [[classical mechanics]]. Outside the realm of quantum physics, the various [[aether theories]] in classical physics, which supposed a \"[[classical elements|fifth element]]\" such as the [[Luminiferous aether]], were nullified by the [[Michelson–Morley experiment]]—an attempt to detect the motion of earth through the aether. In biology, [[Darwinism]] gained acceptance, promoting the concept of [[adaptation]] in the theory of [[natural selection]]. The fields of [[modern geology|geology]], [[modern astronomy|astronomy]] and [[modern psychology|psychology]] also made strides and gained new insights. In [[modern medicine|medicine]], there were advances in [[medical theory]] and [[medical treatment|treatments]].\n\nStarting one-hundred years before the 20th century, the [[Enlightenment (spiritual)|enlightenment spiritual philosophy]] was challenged in various quarters around the 1900s. Developed from earlier [[secular]] traditions, modern [[Humanism|Humanist]] [[ethics|ethical philosophies]] affirmed the dignity and worth of all people, based on the ability to determine right and wrong by appealing to universal human qualities, particularly [[rationalism|rationality]], without resorting to the supernatural or alleged divine authority from religious texts. For [[liberal humanism|liberal humanists]] such as [[Jean-Jacques Rousseau|Rousseau]] and [[Immanuel Kant|Kant]], the universal law of [[reason]] guided the way toward total emancipation from any kind of tyranny. These ideas were challenged, for example by the [[young Marx|young Karl Marx]], who criticized the project of political emancipation (embodied in the form of [[human rights]]), asserting it to be symptomatic of the very dehumanization it was supposed to oppose. For [[Friedrich Nietzsche]], humanism was nothing more than a secular version of [[theism]]. In his \"[[Genealogy of Morals]]\", he argues that human rights exist as a means for the weak to collectively constrain the strong. On this view, such rights do not facilitate emancipation of life, but rather deny it. In the 20th century, the notion that human beings are rationally autonomous was challenged by the concept that humans were driven by unconscious irrational desires.\n\n[[File:Albert Einstein 1921 by F Schmutzer.jpg|thumb|upright=0.75|Albert Einstein in 1921|alt=]]\n[[Sigmund Freud]] is renowned for his redefinition of [[sexual desire]] as the primary motivational energy of human life, as well as his therapeutic techniques, including the use of [[free association (psychology)|free association]], his [[transference|theory of transference]] in the therapeutic relationship, and the [[Dream interpretation|interpretation of dreams]] as sources of insight into [[unconscious desire]]s.\n\n[[Albert Einstein]] is known for his theories of [[special relativity]] and [[general relativity]]. He also made important contributions to [[statistical mechanics]], especially his mathematical treatment of [[Brownian motion]], his resolution of the [[Einstein solid|paradox of specific heats]], and his connection of [[Fluctuation dissipation theorem|fluctuations and dissipation]]. Despite his reservations about its interpretation, Einstein also made contributions to quantum mechanics and, indirectly, [[quantum field theory]], primarily through his theoretical studies of the [[photon]].\n\nAt the end of the 19th century, [[Social Darwinism]] was promoted and included the various ideologies based on a concept that competition among all individuals, groups, nations, or ideas was a \"natural\" framework for social evolution in human societies. In this view, society's advancement is dependent on the \"[[survival of the fittest]]\", the term was in fact coined by [[Herbert Spencer]] and referred to in \"[[The Gospel of Wealth]]\" written by [[Andrew Carnegie]].\n\n[[File:Pyramid of Capitalist System.jpg|thumb|[[Pyramid of Capitalist System]], produced in the United States|alt=|left]]\n[[Karl Marx]] summarized his approach to history and politics in the opening line of the first chapter of \"[[The Communist Manifesto]]\" (1848). He wrote:\nThe \"Manifesto\" went through a number of editions from 1872 to 1890; notable new prefaces were written by Marx and Engels for the 1872 German edition, the 1882 Russian edition, the 1883 German edition, and the 1888 English edition. In general, [[Marxism]] identified five (and one transitional) successive stages of development in Western Europe.\n\n[[File:Australia 1900.jpg|thumb|Though still tied to [[United Kingdom|Great Britain]] in the [[Commonwealth of Nations|commonwealth]] [[Australia]] achieved peaceful independence in 1901.|alt=]]\nMajor political developments saw the former [[British Empire]] lose most of its remaining political power over [[British Commonwealth|commonwealth]] countries. The [[Trans-Siberian Railway]], crossing Asia by train, was complete by 1916. Other events include the [[Israeli–Palestinian conflict]], two world wars, and the [[Cold War]].\n\nIn 1901, the [[Federation of Australia]] was the process by which the six separate British [[self-governing colony|self-governing colonies]] of [[New South Wales]], [[Queensland]], [[South Australia]], [[Tasmania]], [[Victoria (Australia)|Victoria]] and [[Western Australia]] formed one nation. They kept the systems of government that they had developed as separate colonies but also would have a federal government that was responsible for matters concerning the whole nation. When the [[Constitution of Australia]] came into force, the colonies collectively became [[States and territories of Australia|states]] of the [[Australia|Commonwealth of Australia]].\n\n[[File:Xinhai Revolution in Shanghai.jpg|thumb|[[Xinhai Revolution]] in [[Shanghai]]; [[Chen Qimei]] organized Shanghainese civilians to start the uprising and was successful. The picture above is [[Nanjing Road]] after the uprising, hung with the [[Five Races Under One Union]] Flags then used by the revolutionaries.|alt=|left]]\nThe last days of the [[Qing dynasty]] were marked with civil unrest, [[Hundred Days' Reform|failed reforms]] and foreign invasions such as the [[Boxer Rebellion|Boxer rebellion]]. Responding to these civil failures and discontent, the Qing Imperial Court did attempt to reform the government in various ways, as the decision to draft a constitution in 1906, the establishment of provincial legislatures in 1909, and the preparation for a national parliament in 1910. However, many of these measures were opposed by the conservatives of the Qing Court, and many reformers were either imprisoned or executed outright. The failures of the Imperial Court to enact such reforming measures of political liberalization and modernization caused the reformists to steer toward the road of revolution.\n\nThe assertions of Chinese philosophy began to integrate concepts of Western philosophy, as steps toward modernization. By the time of the [[Xinhai Revolution]] in 1911, there were many calls, such as the [[May Fourth Movement]], to completely abolish the old imperial institutions and practices of China. There were attempts to incorporate [[democracy]], [[republicanism]], and [[industrialism]] into Chinese philosophy, notably by [[Sun Yat-sen]] at the beginning of the 20th century\n\nIn 1912, the Republic of China was established and Sun Yat-sen was inaugurated in [[Nanjing]] as the first [[President of the Republic of China|Provisional President]]. But power in [[Beijing]] already had passed to [[Yuan Shikai]], who had effective control of the [[Beiyang Army]], the most powerful military force in China at the time. To prevent [[civil war]] and possible foreign intervention from undermining the infant republic, leaders agreed to Army's demand that China be united under a Beijing government. On March 10, in Beijing, Shikai was sworn in as the second Provisional President of the Republic of China.\n\nAfter the early 20th century revolutions, shifting alliances of [[Warlord era (China)|China's regional warlords]] waged war for control of the Beijing government. Despite the fact that various warlords gained control of the government in Beijing during the warlord era, this did not constitute a new era of control or governance, because other warlords did not acknowledge the transitory governments in this period and were a law unto themselves. These military-dominated governments were collectively known as the [[Beiyang government]]. The warlord era ended around 1927.\n\n[[File:World 1898 empires colonies territory.png|center|thumb|700x700px|The World in 1898 color coded for major empires. The [[British Empire]], the [[Russian Empire]], the [[Qing dynasty|Qing Dynasty]] and the [[United States]] were the [[List of largest empires|largest countries]] at the time. |alt=]]\n[[File:Battle of Yalu River 1904.jpg|thumb|The [[Russo-Japanese War]] was the first time a European Country was defeated by an Asian Country in modern times. The Japanese Victory shocked the world.|alt=]]\nIn 1900 the World's Population had approached approximately 1.6 billion.\nFour years into the 20th century saw the [[Russo-Japanese War]] with the [[Battle of Port Arthur]] establishing the [[Empire of Japan]] as a world power. The Russians were in constant pursuit of a [[warm water port]] on the Pacific Ocean, for their navy as well as for maritime trade. The Manchurian Campaign of the [[Russian Empire]] was fought against the Japanese over [[Manchuria]] and [[Korea]]. The major theatres of operations were Southern Manchuria, specifically the area around the [[Liaodong Peninsula]] and [[Mukden]], and the seas around Korea, Japan, and the [[Yellow Sea]]. The resulting campaigns, in which the fledgling Japanese military consistently attained victory over the Russian forces arrayed against them, were unexpected by world observers. These victories, as time transpired, would dramatically transform the distribution of power in East Asia, resulting in a reassessment of Japan's recent entry onto the world stage. The embarrassing string of defeats increased Russian popular dissatisfaction with the inefficient and corrupt Tsarist government.\n\nThe [[Russian Revolution of 1905]] was a wave of mass political unrest through vast areas of the [[Russian Empire]]. Some of it was directed against the government, while some was undirected. It included [[terrorism]], worker strikes, peasant unrests, and military mutinies. It led to the establishment of the limited constitutional monarchy, the establishment of [[State Duma of the Russian Empire]], and the [[multi-party system]].\n\nIn China, the Qing Dynasty was overthrown following the [[Xinhai Revolution]]. The Xinhai Revolution began with the [[Wuchang Uprising]] on October 10, 1911 and ended with the abdication of [[Emperor Puyi]] on February 12, 1912. The primary parties to the conflict were the Imperial forces of the [[Qing dynasty]] (1644–1911), and the revolutionary forces of the [[Tongmenghui|Chinese Revolutionary Alliance]] (Tongmenghui).\n\n[[File:Steamship Titanic.jpg|thumb|The [[RMS Titanic|Titanic]] was the largest ship constructed in its time. Deemed invincible, it was sunk by [[iceberg]]s off the coast of [[Labrador|Labrador, Canada.]] |alt=|left]]\nThe [[Edwardian era]] in the United Kingdom is the period spanning the reign of [[Edward VII of the United Kingdom|King Edward VII]] up to the end of the First World War, including the years surrounding the sinking of the [[RMS Titanic|RMS \"Titanic\"]]. In the early years of the period, the [[Second Boer War]] in South Africa split the country into anti- and pro-war factions. The imperial policies of the Conservatives eventually proved unpopular and in the [[United Kingdom general election, 1906|general election of 1906]] the Liberals won a huge landslide. The Liberal government was unable to proceed with all of its radical programme without the support of the [[House of Lords]], which was largely Conservative. Conflict between the two Houses of Parliament over the [[People's Budget]] led to a reduction in the power of the peers in 1910. The [[United Kingdom general election, January 1910|general election in January that year]] returned a [[hung parliament]] with the balance of power held by [[Labour Party (UK)|Labour]] and [[Nationalist Party (Ireland)|Irish Nationalist]] members.\n\nThe [[causes of World War I]] included many factors, including the conflicts and antagonisms of the four decades leading up to the war. The [[Triple Entente]] was the name given to the loose alignment between the [[British Empire|United Kingdom]], [[French Third Republic|France]], and [[Russian Empire|Russia]] after the signing of the [[Anglo-Russian Entente]] in 1907. The alignment of the three powers, supplemented by various agreements with [[Empire of Japan|Japan]], the United States, and [[Spain under the Restoration|Spain]], constituted a powerful counterweight to the [[Triple Alliance (1882)|Triple Alliance]] of [[German Empire|Germany]], [[Austria-Hungary]], and [[Kingdom of Italy (1861–1946)|Italy]], the third having concluded an additional secret agreement with France effectively nullifying her Alliance commitments. Militarism, alliances, imperialism, and nationalism played major roles in the conflict. The immediate origins of the war lay in the decisions taken by statesmen and generals during the [[July Crisis]] of 1914, the spark (or casus belli) for which was the assassination of [[Archduke Franz Ferdinand]] of Austria.\n\nHowever, the crisis did not exist in a void; it came after a long series of diplomatic clashes between the Great Powers over European and colonial issues in the decade prior to 1914 which had left tensions high. The diplomatic clashes can be traced to changes in the balance of power in Europe since 1870. An example is the [[Baghdad Railway]] which was planned to connect the [[Ottoman Empire]] cities of [[Konya]] and [[Baghdad]] with a line through modern-day Turkey, Syria and Iraq. The railway became a source of international disputes during the years immediately preceding World War I. Although it has been argued that they were resolved in 1914 before the war began, it has also been argued that the railroad was a cause of the First World War. Fundamentally the war was sparked by tensions over territory in the [[Balkans]]. Austria-Hungary competed with Serbia and Russia for territory and influence in the region and they pulled the rest of the great powers into the conflict through their various alliances and treaties. The [[Balkan Wars]] were two wars in South-eastern Europe in 1912–1913 in the course of which the [[Balkan League]] (Bulgaria, Montenegro, Greece, and Serbia) first captured Ottoman-held remaining part of Thessaly, Macedonia, Epirus, Albania and most of Thrace and then fell out over the division of the spoils, with incorporation of Romania this time.\n\n[[File:World War 1.gif|thumb|upright=3.4|center|Various periods of World War I; 1914.07.28 (Tsar Nicholas II of Russia orders a partial mobilization against Austria-Hungary), 1914.08.01 (Germany declares war on Russia), 1914.08.03 (Germany declares war on Russia's ally France), 1914.08.04 (Britain declares war on Germany), 1914.12 (British and German [[Christmas truce]]), 1915.12 (French and German Christmas truce), 1916.12 ([[Battle of Magdhaba]]), 1917.12 (British troops take Jerusalem from the Ottoman Empire), and 1918.11.11 (World War I ends: Germany signs an armistice agreement with the Allies).Allies and Central Powers in the First World War Allied powers and areas Central powers and colonies or occupied territory Neutral countries]] [[File:General gouraud french army world war i machinegun marne 1918.JPEG|thumb|French Army, 1918]]\nThe First World War began in 1914 and lasted to the final [[Armistice with Germany (Compiègne)|Armistice]] in 1918. The [[Allies of World War I|Allied Powers]], led by the [[Britain in World War I|British Empire]], [[French Third Republic|France]], Russia until March 1918, Japan and the United States after 1917, defeated the [[Central Powers]], led by the [[German Empire]], [[Austro-Hungarian Empire]] and the [[Ottoman Empire]]. The war caused the disintegration of four empires—the Austro-Hungarian, German, Ottoman, and Russian ones—as well as radical change in the European and West Asian maps. The Allied powers before 1917 are referred to as the [[Triple Entente]], and the Central Powers are referred to as the [[Triple Alliance (1882)|Triple Alliance]].\n\n[[File:Chemical Warfare in the Twentieth Century Q54781.jpg|left|thumb|[[Chemical warfare|Chemical Warfare]] was used for the first time in WWI. Soldiers used [[Gas mask|gas-masks]] to protect themselves]]Much of the fighting in World War I took place along the [[Western Front (World War I)|Western Front]], within a system of opposing manned trenches and fortifications (separated by a \"[[No man's land]]\") running from the [[North Sea]] to the border of Switzerland. On the [[Eastern Front (World War I)|Eastern Front]], the vast eastern plains and limited rail network prevented a trench warfare stalemate from developing, although the scale of the conflict was just as large. Hostilities also occurred on and under the sea and—for the first time—from the air. More than 9 million soldiers died on the various battlefields, and nearly that many more in the participating countries' home fronts on account of food shortages and [[genocide]] committed under the cover of various civil wars and internal conflicts. Notably, more people died of the worldwide [[Spanish flu|influenza outbreak]] at the end of the war and shortly after than died in the hostilities. The unsanitary conditions engendered by the war, severe overcrowding in barracks, wartime propaganda interfering with public health warnings, and migration of so many soldiers around the world helped the outbreak become a [[pandemic]].\n\nUltimately, World War I created a decisive break with the old [[New World Order (political)|world order]] that had emerged after the [[Napoleonic Wars]], which was modified by the mid-19th century's nationalistic revolutions. The results of World War I would be important factors in the development of World War II approximately 20 years later. More immediate to the time, the [[partitioning of the Ottoman Empire]] was a political event that redrew the political boundaries of West Asia. The huge conglomeration of territories and peoples formerly ruled by the Sultan of the Ottoman Empire was divided into several new nations. The partitioning brought the creation of the modern [[Arab world]] and the [[Republic of Turkey]]. The [[League of Nations]] granted France mandates over [[French Mandate of Syria|Syria]] and [[French Mandate of Lebanon|Lebanon]] and granted the United Kingdom mandates over [[British Mandate of Mesopotamia|Mesopotamia]] and [[British Mandate for Palestine (legal instrument)|Palestine]] (which was later divided into two regions: [[Mandatory Palestine|Palestine]] and [[Emirate of Transjordan|Transjordan]]). Parts of the Ottoman Empire on the [[Arabian Peninsula]] became parts of what are today [[Saudi Arabia]] and [[Yemen]].\n\n[[File:Lenin.gif|thumb|Lenin]]\n\nThe Russian Revolution is the series of revolutions in Russia in 1917, which destroyed the [[Tsarist autocracy]] and led to the creation of the Soviet Union. Following the abdication of [[Nicholas II of Russia]], the [[Russian Provisional Government]] was established. In October 1917, a [[October Revolution|\"red\" faction revolution]] occurred in which the [[Red Guards (Russia)|Red Guard]], armed groups of workers and deserting soldiers directed by the Bolshevik Party, seized control of [[Saint Petersburg]] (then known as [[Petrograd]]) and began an immediate armed takeover of cities and villages throughout the former [[Russian Empire]].\n\nAnother action in 1917 that is of note was the armistice signed between Russia and the Central Powers at [[Brest-Litovsk]]. As a condition for peace, the treaty by the [[Central Powers]] conceded huge portions of the former Russian Empire to [[German Empire|Imperial Germany]] and the Ottoman Empire, greatly upsetting [[nationalist]]s and [[Conservatism|conservatives]]. The Bolsheviks made peace with the [[German Empire]] and the [[Central Powers]], as they had promised the Russian people prior to the Revolution. Vladimir Lenin's decision has been attributed to his sponsorship by the foreign office of [[Wilhelm II, German Emperor]], offered by the latter in hopes that with a revolution, Russia would withdraw from World War I. This suspicion was bolstered by the German Foreign Ministry's sponsorship of Lenin's return to [[Saint Petersburg|Petrograd]]. The [[Triple Entente|Western Allies]] expressed their dismay at the Bolsheviks, upset at:\n[[File:Russian civil war posters.png|left|thumb|Two contrasting visions of the [[Russian Civil War]]. To the left is propaganda from the [[White Army]], to the right is propaganda from the [[Bolsheviks]]. ]]\nIn addition, there was a concern, shared by many [[Central Powers]] as well, that the socialist revolutionary ideas would spread to the West. Hence, many of these countries expressed their support for the Whites, including the provision of troops and supplies. [[Winston Churchill]] declared that Bolshevism must be \"strangled in its cradle\".\n\nThe Russian Civil War was a multi-party war that occurred within the former [[Russian Empire]] after the [[Russian provisional government]] collapsed and the [[Soviet republic (system of government)|Soviets]] under the domination of the [[Bolshevik]] party assumed power, first in [[Saint Petersburg|Petrograd (St. Petersburg)]] and then in other places. In the wake of the [[October Revolution]], the old Russian Imperial Army had been demobilized; the volunteer-based Red Guard was the Bolsheviks' main military force, augmented by an armed military component of the [[Cheka]], the Bolshevik state security apparatus. There was an instituted mandatory conscription of the rural peasantry into the Red Army. Opposition of rural Russians to Red Army conscription units was overcome by taking hostages and shooting them when necessary in order to force compliance. Former Tsarist officers were utilized as \"military specialists\" (\"voenspetsy\"), taking their families hostage in order to ensure loyalty. At the start of the war, three-fourths of the Red Army officer corps was composed of former Tsarist officers. By its end, 83% of all Red Army divisional and corps commanders were ex-Tsarist soldiers.\n[[File:Vladivostok intervention.jpg|thumb|In the Russian Civil War, over eleven nations intervened in favor of the [[White Movement]]. Here Japanese occupy [[Vladivostok]].]]\nThe principal fighting occurred between the [[Bolshevik]] [[Red Army]] and the forces of the [[White Movement|White Army]]. Many foreign armies warred against the Red Army, notably the [[Allied intervention in the Russian Civil War|Allied Forces]], yet many volunteer foreigners fought in both sides of the Russian Civil War. Other nationalist and regional political groups also participated in the war, including the Ukrainian nationalist [[Green armies|Green Army]], the Ukrainian anarchist [[Revolutionary Insurrectionary Army of Ukraine|Black Army]] and [[Black Guards]], and warlords such as [[Ungern von Sternberg]]. The most intense fighting took place from 1918 to 1920. Major military operations ended on 25 October 1922 when the Red Army occupied [[Vladivostok]], previously held by the [[Provisional Priamur Government]]. The last enclave of the White Forces was the [[Ayano-Maysky District]] on the Pacific coast. The majority of the fighting ended in 1920 with the defeat of General [[Pyotr Wrangel]] in the [[Crimea]], but a notable resistance in certain areas continued until 1923 (e.g., [[Kronstadt Uprising]], [[Tambov Rebellion]], [[Basmachi Revolt]], and the final resistance of the [[White movement]] in the [[Russian Far East|Far East]]).\n\nWhile the early 1920s was a time of flux for revolutionary Russia and Central Asia, the [[Union of Soviet Socialist Republics]] was proclaimed in 1922 as the successor state to the fallen [[Russian Empire]]. Revolutionary leader [[Vladimir Lenin]] died of natural causes and was succeeded by [[Joseph Stalin]].\n\nIn 1917, China declared war on Germany in the hope of recovering its lost province, then under Japanese control. The [[New Culture Movement]] occupied the period from 1917 to 1923. Chinese representatives refused to sign the [[Treaty of Versailles]], due to intense pressure from the student protesters and public opinion alike.\n[[File:Student Demonstrations, June 4th and 5th, 1919 2.jpg|left|thumb|Student Demonstrations, June 4th and 5th, 1919 2]]\n\nThe [[May Fourth Movement]] helped to rekindle the then-fading cause of republican revolution. In 1917 [[Sun Yat-sen]] had become commander-in-chief of a rival military government in [[Guangzhou]] in collaboration with southern warlords. Sun's efforts to obtain aid from the Western democracies were ignored, however, and in 1920 he turned to the Soviet Union, which had recently achieved its own revolution. The Soviets sought to befriend the Chinese revolutionists by offering scathing attacks on Western imperialism. But for political expediency, the Soviet leadership initiated a dual policy of support for both Sun and the newly established [[Chinese Communist Party]] (CCP).[[File:Sino-german cooperation.png|thumb|With [[Sino-German cooperation until 1941]], Chinese industry and military was improved just prior to the war against Japan.|alt=|150x150px]]\n\nIn early 1927, the Kuomintang-CCP rivalry led to a split in the revolutionary ranks. The CCP and the left wing of the Kuomintang had decided to move the seat of the Nationalist government from Guangzhou to [[Wuhan]]. But [[Chiang Kai-shek]], whose [[Northern Expedition]] was proving successful, set his forces to destroying the Shanghai CCP apparatus and established an anti-Communist government at Nanjing in [[April 12 Incident|April 1927]].\n\nThe \"Nanjing Decade\" of 1928–37 was one of consolidation and accomplishment under the leadership of the Nationalists, with a mixed but generally positive record in the economy, social progress, development of [[democracy in China|democracy]], and cultural creativity. Some of the harsh aspects of foreign concessions and privileges in China were moderated through diplomacy.\n\n[[File:World 1920 empires colonies territory.png|center|thumb|700x700px|The world in 1920, part of the [[interwar period]]. [[United Kingdom|Great Britain]] and [[France]] expanded greatly at the expense of the former [[German Empire]]]]\nThe interwar period was the period between the end of the First World War and the beginning of the Second World War. This period was marked by turmoil in much of the world, as Europe struggled to recover from the devastation of the First World War.[[File:A flapper girl LCCN2012645724.tif|thumb|227x227px|American, Flapper Girl. In the 1920s women experienced a degree of [[First-wave feminism|liberation]]. ]] [[File:Fatty Arbuckle, Life of the Party, 1920.ogv|left|thumb|In the 1920s [[movie theater]]s became popular in American and European communities. ]]In North America, especially the first half of this period, people experienced considerable prosperity in the Roaring Twenties. The social and societal upheaval known as the Roaring Twenties began in North America and spread to Europe in the [[aftermath of World War I]]. The \"Roaring Twenties\", often called \"[[Jazz Age|The Jazz Age]]\", saw an exposition of social, artistic, and cultural dynamism. \"[[Normalcy]]\" returned to politics, [[jazz music]] blossomed, the [[flapper]] redefined modern womanhood, [[Art Deco]] peaked. The spirit of the Roaring Twenties was marked by a general feeling of discontinuity associated with modernity, a break with traditions. Everything seemed to be feasible through modern technology. New technologies, especially automobiles, movies and radio proliferated 'modernity' to a large part of the population. The 1920s saw the general favor of practicality, in architecture as well as in daily life. The 1920s was further distinguished by several inventions and discoveries, extensive industrial growth and the rise in consumer demand and aspirations, and significant changes in [[lifestyle (sociology)|lifestyle]].[[File:I Got The Ritz From The One I Love.ogg|left|thumb|\"I got the Ritz from the one I love,\" Jazz music radio broadcast 1932]]\nEurope spent these years rebuilding and coming to terms with the vast human cost of the conflict. The economy of the United States became increasingly intertwined with that of Europe. In Germany, the [[Weimar Republic]] gave way to episodes of political and economic turmoil, which culminated with the [[Hyperinflation in the Weimar Republic|German hyperinflation]] of 1923 and the failed [[Beer Hall Putsch]] of that same year. When Germany could no longer afford war payments, Wall Street invested heavily in European debts to keep the European economy afloat as a large consumer market for American mass-produced goods. By the middle of the decade, [[economic development]] soared in Europe, and the Roaring Twenties broke out in Germany, Britain and France, the second half of the decade becoming known as the \"[[Golden Twenties]]\". In France and francophone Canada, they were also called the \"années folles\" (\"Crazy Years\")., 1935. German Nazis created large public performances to earn public support. [[File:Depression, Breadlines-long line of people waiting to be fed, New York City, in the absence of substantial government... - NARA - 196506.tif|thumb|[[Great Depression]], Breadlines-long line of people waiting to be fed, [[New York City]], [[United States]] i|alt=|150x150px]]Worldwide prosperity changed dramatically with the onset of the [[Great Depression]] in 1929. The [[Wall Street Crash of 1929]] served to punctuate the end of the previous era, as \"The Great Depression\" set in. The \"Great Depression\" was a worldwide economic [[Recession|downturn]] starting in most places in 1929 and ending at different times in the 1930s or early 1940s for different countries. It was the largest and most important [[Depression (economics)|economic depression]] in the 20th century, and is used in the 21st century as an example of how far the world's economy can fall.\n\nThe depression had devastating effects in virtually every country, rich or poor. International trade plunged by half to two-thirds, as did personal income, tax revenue, prices and profits. [[Cities in the Great Depression|Cities all around the world]] were hit hard, especially those dependent on [[heavy industry]]. Construction was virtually halted in many countries. Farming and rural areas suffered as crop prices fell by roughly 60 percent. Facing plummeting demand with few alternate sources of jobs, areas dependent on [[Primary sector of economic activity|primary sector industries]] suffered the most.\nThe Great Depression ended at different times in different countries with the [[Home front during World War II|effect lasting into the next era]]. America's Great Depression ended in 1941 with America's entry into World War II. The majority of countries set up relief programs, and most underwent some sort of political upheaval, pushing them to the left or right. In some world states, the desperate citizens turned toward nationalist [[Demagogy|demagogues]]—the most infamous being [[Adolf Hitler]]—setting the stage for the next era of war. The convulsion brought on by the worldwide depression resulted in the rise of [[Nazism]]. In Asia, Japan became an ever more assertive power, especially with regards to China.\n\n[[File:Another talk with the German Chancellor, Herr Hitler.ogg|thumb|U.K Prime Minister [[Neville Chamberlain|Nevil Chamberline]] attempted to negotiate with [[Adolf Hitler]] as Nazi Germany practiced an expansionist policy .|alt=|left]]\nThe interwar period was also marked by a radical change in the international order, away from the [[balance of power in international relations|balance of power]] that had dominated pre–World War I Europe. One main institution that was meant to bring stability was the [[League of Nations]], which was created after the First World War with the intention of maintaining world security and peace and encouraging economic growth between member countries. The League was undermined by the bellicosity of [[Nazi Germany]], [[Imperial Japan]], the Soviet Union, and [[Benito Mussolini|Mussolini's]] Italy, and by the non-participation of the United States, leading many to question its effectiveness and legitimacy.[[File:Battle at Great Wall, Laiyuan, Hebei, autumn 1937.jpg|thumb|[[Kuomintang|Nationalist Chinese]] soldiers defending at the Battle at Great Wall, Laiyuan, [[Hebei]], [[China]] autumn 1937. The [[Second Sino-Japanese War|Second Sino-Japanese war]] cost at least twenty millions lives. ]]A series of international crises strained the League to its limits, the earliest being the [[Japanese invasion of Manchuria|invasion of Manchuria]] by Japan and the [[Abyssinian crisis]] of 1935/36 in which Italy invaded [[Ethiopian Empire|Abyssinia]], one of the only free African nations at that time.\n[[File:Bundesarchiv Bild 183-R69173, Münchener Abkommen, Staatschefs.jpg|left|thumb|The signatories [[Munich Agreement]] where British Prime Minister [[Neville Chamberlain|Nevil Chamberline]] [[Appeasement|appeased]] to the [[Axis powers|Axis Powers]] regarding territorial claims in [[Sudetenland]], [[Czechoslovakia]] to avoid conflict.]]\nThe League tried to enforce economic sanctions upon Italy, but to no avail. The incident highlighted French and British weakness, exemplified by their reluctance to alienate Italy and lose her as their ally. The limited actions taken by the Western powers pushed Mussolini's Italy towards alliance with Hitler's Germany anyway. The Abyssinian war showed Hitler how weak the League was and encouraged the remilitarization of the Rhineland in flagrant disregard of the Treaty of Versailles. This was the first in a series of provocative acts culminating in the [[invasion of Poland]] in September 1939 and the beginning of the Second World War.\n\nFew Chinese had any illusions about Japanese designs on China. Hungry for raw materials and pressed by a growing population, Japan initiated the seizure of [[Manchuria]] in September 1931 and established ex-Qing emperor [[Puyi]] as head of the [[puppet state]] of [[Manchukuo]] in 1932. During the [[Sino-Japanese War (1937–1945)]], the loss of Manchuria, and its vast potential for industrial development and war industries, was a blow to the Kuomintang economy. The [[League of Nations]], established at the end of World War I, was unable to act in the face of the Japanese defiance. After 1940, conflicts between the Kuomintang and Communists became more frequent in the [[Free China (Second Sino-Japanese War)|areas not under Japanese control]]. The Communists expanded their influence wherever opportunities presented themselves through mass organizations, administrative reforms, and the land- and tax-reform measures favoring the peasants—while the Kuomintang attempted to neutralize the spread of Communist influence.\n\n[[File:Shashin Shuho No 151.jpg|left|thumb|141x141px|Japanese poster advocating for Tripartite Pact, 1941.]]\n\nThe [[Second Sino-Japanese War]] had seen tensions rise between Imperial Japan and the United States; events such as the [[Panay incident]] and the [[Nanking Massacre]] turned American public opinion against Japan. With the occupation of [[French Indochina]] in the years of 1940–41, and with the continuing war in China, the United States placed embargoes on Japan of [[strategic material]]s such as scrap metal and oil, which were vitally needed for the war effort. The Japanese were faced with the option of either withdrawing from China and losing face or seizing and securing new sources of raw materials in the resource-rich, European-controlled colonies of [[South East Asia]]—specifically [[British Malaya]] and the [[Dutch East Indies]] (modern-day [[Indonesia]]). In 1940, Imperial Japan signed the [[Tripartite Pact]] with Nazi Germany and Fascist Italy.\n\n[[File:Ww2 allied axis 1942 jun.png|center|thumb|700x700px|World War II at the height of Axis expansion (black) fighting against the [[Allies of World War II|Allies]] (blue) and [[Communist International|Comintern]] (red). It is important to note that the [[Empire of Japan]] was not at war with the [[Soviet Union]] despite being part of the [[Tripartite Pact]].]]\n[[File:Bundesarchiv Bild 101I-012-0037-23A, Polen, Straßenkampf, Infanterie.jpg|left|thumb|The German [[Invasion of Poland]] in 1939 is the official start of [[World War II|WWII]]]]\n\nThe Second World War was a global military conflict that took place in 1939–1945. It was the largest and deadliest war in history, culminating in [[the Holocaust]] and ending with the dropping of the [[atom bomb]].\n\nEven though Japan had been invading in China since 1937, the conventional view is that the war began on September 1, 1939, when [[Nazi Germany]] invaded Poland, the [[Drang nach Osten]]. Within two days the United Kingdom and France declared war on Germany, even though the fighting was confined to Poland. Pursuant to a then-secret provision of its non-aggression [[Molotov–Ribbentrop Pact]], the Soviet Union joined with Germany on September 17, 1939, to conquer Poland and to divide Eastern Europe.\nThe [[Allies of World War II|Allies]] were initially made up of Poland, the United Kingdom, France, Australia, Canada, New Zealand, South Africa, as well as [[Commonwealth of Nations|British Commonwealth]] countries which were controlled directly by the UK, such as the [[British Raj|Indian Empire]]. All of these countries declared war on Germany in September 1939.\n[[File:Children in the Holocaust concentration camp liberated by Red Army.jpg|alt=Holocaust Survivors, January 1945|thumb|Survivors of the [[The Holocaust|Holocaust]] at the infamous German concentration camp of [[Auschwitz concentration camp|Auschwitz]] located in Occupied Poland. ]]\nFollowing the lull in fighting, known as the \"[[Phoney War]]\", Germany invaded western Europe in May 1940. Six weeks later, France, in the mean time attacked by Italy as well, surrendered to Germany, which then tried unsuccessfully to conquer Britain. On September 27, Germany, Italy, and Japan signed a mutual defense agreement, the [[Tripartite Pact]], and were known as the [[Axis Powers]].Nine months later, on June 22, 1941, Germany launched a massive invasion of the Soviet Union, which promptly joined the Allies. Germany was now engaged in fighting a war on two fronts. This proved to be a mistake by Germany – Germany had not successfully carried out the invasion of Britain and the war turned against the Axis.\n\nOn December 7, 1941, [[Attack on Pearl Harbor|Japan attacked the United States at Pearl Harbor]], bringing it too into the war on the Allied side. China also joined the Allies, as eventually did most of the rest of the world. China was in turmoil at the time, and attacked Japanese armies through guerilla-type warfare. By the beginning of 1942, the major combatants were aligned as follows: the British Commonwealth, the United States, and the Soviet Union were fighting Germany and Italy; and the British Commonwealth, China, and the United States were fighting Japan. The United Kingdom, the United States, the Soviet Union and China were referred as a \"trusteeship of the powerful\" during the [[World War II]] and were recognized as the Allied \"Big Four\" in [[Declaration by United Nations]] These four countries were considered as the \"[[Four Policemen]]\" or \"Four Sheriffs\" of the [[Allies of World War II|Allies power]] and primary victors of World War II. From then through August 1945, battles raged across all of Europe, in the [[North Atlantic Ocean]], across North Africa, throughout [[Southeast Asia]], throughout China, across the Pacific Ocean and in the air over Japan.\nItaly surrendered in September 1943 and was split into a northern Germany-occupied [[puppet state]] and an Allies-friendly state in the South; Germany surrendered in May 1945. Following the [[atomic bombings of Hiroshima and Nagasaki]], [[surrender of Japan|Japan surrendered]], marking the end of the war on September 2, 1945.\n[[File:Underwater Atomic Bomb Test At Bikini Atoll.webm|alt=Atomic Bomb underwater testing, 1946|left|thumb|Atomic Bomb Test, 1946. The use of Nuclear strikes on Hiroshima and Nagasaki brought WWII to an abrupt end.]]\n[[File:Roosevelt Pearl Harbor.ogg|thumb|U.S President [[Franklin D. Roosevelt|Franklin Roosevelt]] declaring war on the [[Empire of Japan|Japanese Empire]] in the aftermath of the [[Attack on Pearl Harbor|Pearl Harbor Attack]]. Captions provided]]\n[[File:Trumann hiroshima.ogg|thumb|Excerpt of [[President of the United States|U.S President]] [[Harry S. Truman|Harry Truman']]s speech regarding the [[Atomic bombings of Hiroshima and Nagasaki|nuclear attack]] on [[Hiroshima|Hiroshima, Japan]].Captions provided]]\nIt is possible that around 62 million people [[World War II casualties|died in the war]]; estimates vary greatly. About 60% of all casualties were civilians, who died as a result of disease, starvation, [[genocide]] (in particular, the [[Holocaust]]), and aerial bombing. The former Soviet Union and China suffered the most casualties. Estimates place deaths in the Soviet Union at around 23 million, while China suffered about 10 million. No country lost a greater portion of its population than Poland: approximately 5.6 million, or 16%, of its pre-war population of 34.8 million died.\nThe Holocaust (which roughly means \"burnt whole\") was the deliberate and systematic murder of millions of Jews and other \"unwanted\" during World War II by the Nazi regime in Germany. Several differing views exist regarding whether it was intended to occur from the war's beginning, or if the plans for it came about later. Regardless, persecution of Jews extended well before the war even started, such as in the \"[[Kristallnacht]]\" (Night of Broken Glass). The Nazis used propaganda to great effect to stir up anti-Semitic feelings within ordinary Germans.\n\nAfter World War II, Europe was informally split into Western and Soviet [[Sphere of influence|spheres of influence]]. [[Western Europe]] later aligned as the [[NATO|North Atlantic Treaty Organization]] (NATO) and [[Eastern Europe]] as the [[Warsaw Pact]]. There was a shift in power from Western Europe and the [[British Empire]] to the two new superpowers, the United States and the Soviet Union. These two rivals would later face off in the [[Cold War]]. In Asia, the defeat of Japan led to its [[democratization]]. [[Chinese Civil War|China's civil war]] continued through and after the war, resulting eventually in the establishment of the [[People's Republic of China]]. The former colonies of the European powers began their road to independence.\n\n[[File:The Beatles i Hötorgscity 1963.jpg|thumb|During [[Pax Americana]], the [[Popular music|popular music industry]] flourished.Pictured here is the [[The Beatles|Beatles]] Group. |alt=|200x200px]]\n\nThe mid-20th century is distinguished from most of human history in that its most significant changes were directly or indirectly economic and technological in nature. Economic development was the force behind vast changes in everyday life, to a degree which was unprecedented in human history.\n\nOver the course of the 20th century, the world's per-capita [[gross domestic product]] grew by a factor of five, much more than all earlier centuries combined (including the 19th with its Industrial Revolution). Many economists make the case that this understates the magnitude of growth, as many of the goods and services consumed at the end of the 20th century, such as improved medicine (causing world life expectancy to increase by more than two decades) and communications technologies, were not available at any price at its beginning. However, the gulf between the world's rich and poor grew wider, and the majority of the global population remained in the poor side of the divide.\n\nStill, advancing technology and medicine has had a great impact even in the [[Global South]]. Large-scale industry and more centralized [[Mass media|media]] made brutal dictatorships possible on an unprecedented scale in the middle of the century, leading to wars that were also unprecedented. However, the increased communications contributed to [[democratization]]. Technological developments included the development of airplanes and [[space exploration]], [[nuclear technology]], advancement in [[genetics]], and the dawning of the [[Information Age]].\n\nPax Americana is an appellation applied to the historical concept of relative liberal peace in the Western world, resulting from the preponderance of power enjoyed by the United States of America starting around the start of the 20th century. Although the term finds its primary utility in the latter half of the 20th century, it has been used in various places and eras. Its modern connotations concern the peace established after the end of World War II in 1945.\n\nThe Cold War began in the mid-1940s and lasted into the early 1990s. Throughout this period, the conflict was expressed through military coalitions, espionage, weapons development, invasions, propaganda, and competitive technological development. The conflict included costly defense spending, a massive [[Conventional weapon|conventional]] and [[Nuclear weapon|nuclear]] [[arms race]], and numerous [[proxy war]]s; the two [[superpower]]s never fought one another directly.\n\n[[File:NATO vs. Warsaw (1949-1990).svg|upright=3.4|thumb|center|]]\nThe Soviet Union created the [[Eastern Bloc]] of countries that it occupied, annexing some as [[Soviet Socialist Republics]] and maintaining others as satellite states that would later form the [[Warsaw Pact]]. The United States and various western European countries began a policy of \"[[containment]]\" of [[communism]] and forged myriad alliances to this end, including [[NATO]]. Several of these western countries also coordinated efforts regarding the rebuilding of western Europe, including western Germany, which the Soviets opposed. In other regions of the world, such as Latin America and [[Southeast Asia]], the Soviet Union fostered [[communist revolution]]ary movements, which the United States and many of its allies opposed and, in some cases, attempted to \"[[rollback|roll back]]\". Many countries were prompted to align themselves with the nations that would later form either NATO or the Warsaw Pact, though other movements would also emerge.\n[[File:US and USSR nuclear stockpiles.svg|left|thumb|US and USSR nuclear stockpiles]]\nIn China [[Mao Zedong]] (\"Máo zé dōng\") utilized [[Marxism-Leninism|Marxist-Leninist]] thought. When the [[Communist Party of China]] assumed power in 1959, previous schools of Chinese thought, excepting notably [[Legalism (Chinese philosophy)|Legalism]], were denounced as backward. Many parts of China's past were even purged during the [[Cultural Revolution]]. Though initially friendly with the [[Soviet Union]] the Chinese and Soviet communists diverged in the [[Sino-Soviet split]] of 1960. Through the end of the Cold-War, China would continue on its own path from other communist countries, building better relations with the United States after 1972. China's economy would recover from the Cultural Revolution due to [[Chinese economic reform|market-oriented reforms]] led by [[Deng Xiaoping]].\n\nThe Cold War saw periods of both heightened tension and relative calm. International crises arose, such as the [[Berlin Blockade]] (1948–1949), the [[Korean War]] (1950–1953), the [[Berlin Crisis of 1961]], the [[Vietnam War]] (1955–1975), the [[Cuban Missile Crisis]] (1962), the [[Soviet–Afghan War]] (1979–1989) and [[Able Archer 83|NATO exercises in November 1983]]. There were also periods of reduced tension as both sides sought [[détente]]. Direct military attacks on adversaries were deterred by the potential for [[mutual assured destruction]] using deliverable [[nuclear weapon]]s.\n\nThe Cold War drew to a close in the late 1980s and the early 1990s. The United States under President [[Ronald Reagan]] increased diplomatic, military, and economic pressure on the Soviet Union, which was already suffering from [[Brezhnev stagnation|severe economic stagnation]]. In the second half of the 1980s, newly appointed Soviet leader [[Mikhail Gorbachev]] introduced the \"[[perestroika]]\" and \"[[glasnost]]\" reforms. [[History of the Soviet Union (1985–1991)|The Soviet Union collapsed]] in 1991, leaving the United States as the dominant military power, though Russia retained much of the massive Soviet nuclear arsenal.\n\nIn Latin America in the 1970s, leftists acquired a significant political influence which prompted the right-wing, ecclesiastical authorities and a large portion of the individual country's upper class to support coups d'état to avoid what they perceived as a communist threat. This was further fueled by Cuban and United States intervention which led to a political polarization. Most South American countries were in some periods ruled by [[military dictatorship]]s that were supported by the United States of America. In the 1970s, the regimes of the [[Southern Cone]] collaborated in [[Operation Condor]] killing many [[leftist]] dissidents, including some [[urban guerrilla]]s. However, by the early 1990s all countries had restored their democracies.\n\n[[File:Neil Armstrong small step.wav|left|thumb|Neil Armstrong small step]]\n[[File:The Earth seen from Apollo 17.jpg|left|thumb|\"[[The Blue Marble]]\", a photograph of Earth as seen from Apollo 17. The second half of the 20th century saw an increase of interest in both [[space exploration]] and the [[environmental movement]].]]\nThe [[Space Age]] is a period encompassing the activities related to the [[Space Race]], [[space exploration]], space technology, and the cultural developments influenced by these events. The Space Age began with the development of several technologies that culminated with the launch of [[Sputnik 1]] by the Soviet Union in October 1957. This was the world's first artificial satellite, orbiting the Earth in 98.1 minutes and weighing in at 83 kg. The launch of Sputnik 1 ushered a new era of political, scientific and technological achievements that became known as the Space Age. The Space Age was characterized by rapid development of new technology in a close race mostly between the United States and the Soviet Union. The Space Age brought the first human spaceflight during the [[Vostok programme]] and reached its peak with the [[Apollo program]] which captured the imagination of much of the world's population. The landing of [[Apollo 11]] was an event watched by over 500 million people around the world and is widely recognized as one of the defining moments of the 20th century. Since then and with the end of the space race due to the [[dissolution of the Soviet Union]], public attention has largely moved to other areas.\n\nThe [[humanities]] are academic disciplines which study the [[human condition]], using methods that are primarily [[Analytic induction|analytic]], critical, or speculative, as distinguished from the mainly [[empirical]] approaches of the [[natural science|natural]] and [[social sciences]]. Although many of the subjects of modern history coincide with that of standard history, the subject is taught independently by various systems of education in the world.\n\nStudents can choose the subject at university. The material covered includes from the mid-18th century, to analysis of the present day. Virtually all colleges and sixth forms that do teach modern history do it alongside standard history; very few teach the subject exclusively.\n\nAt the [[University of Oxford]] 'Modern History' has a somewhat different meaning. The contrast is not with the Middle Ages but with Antiquity. The earliest period that can be studied in the [[Final Honour School]] of Modern History begins in 285.\n\n\n\"Books\"\n\n\"Websites\"\n\n\n\n\n\n[[Category:Modern history| ]]\n[[Category:Historical eras]]\n[[Category:Historiography]]\n[[Category:Postmodern theory]]\n[[Category:Articles which contain graphical timelines]]\n[[Category:World history]]"}
{"id": "6780123", "url": "https://en.wikipedia.org/wiki?curid=6780123", "title": "Non-communicable disease", "text": "Non-communicable disease\n\nA non-communicable disease (NCD) is a disease that is not transmissible directly from one person to another. NCDs include parkinson's disease autoimmune diseases, strokes, most heart diseases, most cancers, diabetes, chronic kidney disease, osteoarthritis, osteoporosis, Alzheimer's disease, cataracts, and others. NCDs may be chronic or acute. Most are non-infectious, although there are some non-communicable infectious diseases, such as parasitic diseases in which the parasite's life cycle does not include direct host-to-host transmission. \n\nNCDs are the leading cause of death globally. In 2012, they caused 68% of all deaths (38 million) up from 60% in 2000. About half were under age 70 and half were women. Risk factors such as a person's background, lifestyle and environment increase the likelihood of certain NCDs. Every year, at least 5 million people die because of tobacco use and about 2.8 million die from being overweight. High cholesterol accounts for roughly 2.6 million deaths and 7.5 million die because of high blood pressure.\n\nRisk factors such as a person's background; lifestyle and environment are known to increase the likelihood of certain non-communicable diseases. They include age, gender, genetics, exposure to air pollution, and behaviors such as smoking, unhealthy diet and physical inactivity which can lead to hypertension and obesity, in turn leading to increased risk of many NCDs. Most NCDs are considered preventable because they are caused by modifiable risk factors.\n\nThe WHO's \"World Health Report 2002\" identified five important risk factors for non-communicable disease in the top ten leading risks to health. These are raised blood pressure, raised cholesterol, tobacco use, alcohol consumption, and overweight. The other factors associated with higher risk of NCDs include a person's economic and social conditions, also known as the \"[social determinants of health].\"\n\nIt has been estimated that if the primary risk factors were eliminated, 80% of the cases of heart disease, stroke and type 2 diabetes and 40% of cancers could be prevented. Interventions targeting the main risk factors could have a significant impact on reducing the burden of disease worldwide. Efforts focused on better diet and increased physical activity have been shown to control the prevalence of NCDs .\n\nNCDs include many environmental diseases covering a broad category of avoidable and unavoidable human health conditions caused by external factors, such as sunlight, nutrition, pollution, and lifestyle choices. The diseases of affluence are non-infectious diseases with environmental causes. Examples include:\n\nGenetic disorders are caused by errors in genetic information that produce diseases in the affected people.\nThe origin of these genetic errors can be:\n\nCystic fibrosis is an example of an inherited disease that is caused by a mutation on a gene. The faulty gene impairs the normal movement of sodium chloride in and out of cells, which causes the mucus-secreting organs to produce abnormally thick mucus. The gene is recessive, meaning that a person must have two copies of the faulty gene for them to develop the disease. Cystic fibrosis affects the respiratory, digestive and reproductive systems, as well as the sweat glands. The mucus secreted is very thick and blocks passageways in the lungs and digestive tracts. This mucus causes problems with breathing and with the digestion and absorption of nutrients.\n\nReferred to as a \"lifestyle\" disease, because the majority of these diseases are preventable illnesses, the most common causes for non-communicable diseases (NCD) include tobacco use (smoking), alcohol abuse, poor diets (high consumption of sugar, salt, saturated fats, and trans fatty acids) and physical inactivity. Currently, NCD kills 36 million people a year, a number that by some estimates is expected to rise by 17–24% within the next decade.\n\nHistorically, many NCDs were associated with economic development and were so-called a \"diseases of the rich\". The burden of non-communicable diseases in developing countries has increased however, with an estimated 80% of the four main types of NCDs — cardiovascular diseases, cancers, chronic respiratory diseases and diabetes — now occurring in low- and middle-income countries. Action Plan for the Global Strategy for the Prevention and Control of non-communicable Diseases and with two-thirds of people who are affected by diabetes now residing in developing nations, NCD can no longer be considered just a problem affecting affluent estimation of the economic impact of chronic non-communicable diseases in selected countries. New WHO report: deaths from non-communicable diseases are on the rise, with developing world hit hardest. As previously stated, in 2008 alone, NCD's were the cause of 63% of deaths worldwide; a number that is expected to rise considerably in the near future if measures are not taken.\n\nIf present growth trends are maintained, by 2020, NCDs will attribute to 7 out of every 10 deaths in developing countries, killing 52 million people annually worldwide by 2030. With statistics such as these, it comes as no surprise that international entities such as the World Health Organization & World Bank Human Development Network have identified the prevention and control of NCDs as an increasingly important discussion item on the global health agenda.\n\nThus, should policy makers and communities mobilize \"and make prevention and targeted treatment of such diseases a priority,\" sustainable measures can be implemented to stagnate (and eventually even reverse) this emerging global health threat. Potential measures currently being discussed by the(World Health Organization)-Food and Agriculture Organization includes reducing the levels of salt in foods, limiting inappropriate marketing of unhealthy foods and non-alcoholic beverages to children, imposing controls on harmful alcohol use, raising taxes on tobacco, and curbing legislation to curb smoking in public places.\n\nThe World Health Organization is the specialized agency of the United Nations (UN) that acts as coordinating authority on international public health issues, including NCDs. In May 2008, the 193 Member States of the WHO approved a six-year plan to address non-communicable diseases, especially the rapidly increasing burden in low- and middle-income countries. The plan calls for raising the priority given to NCDs in international development work'.\n\nDuring the 64th session of the United Nations General Assembly in 2010, a resolution was passed to call for a high-level meeting of the General Assembly on the prevention and treatment NCDs with the participation of heads of state and government. The resolution also encouraged UN Member States to address the issue of non-communicable diseases at the 2010 Review Summit for the Millennium Development Goals.\n\nIn order to better coordinate efforts around the globe, in 2009 the WHO announced the launch of the \"Global Non-communicable Disease Network\" (NCDnet). NCDnet will consist of leading health organizations and experts from around the world in order to fight against diseases such as cancer, cardiovascular disease, and diabetes. Ala Alwan, assistant director-general for Non-communicable Diseases and Mental Health at the WHO, said: \"integrating the prevention of non-communicable diseases and injuries into the national and global development agendas is not only achievable but also a priority for developing countries.\"\n\nThe NCD Alliance is a global partnership founded in May 2009 by four international federations representing cardiovascular disease, diabetes, cancer, and chronic respiratory disease. The NCD Alliance brings together roughly 900 national member associations to fight non-communicable disease. Long term aims of the Alliance include:\n\nThe United Nations Interagency Task Force on the Prevention and Control of Non-communicable Diseases (UNIATF) was established by the United Nations Secretary-General in 2013 in order to provide scaled up action across the UN system to support governments, in particular in low- and middle-income countries, to tackle non-communicable diseases (NCDs).\n\nThe Young Professionals Chronic Disease Network, or commonly referred to as YP-CDN, is a global network of roughly 5000 young professionals across 157 countries. The organization aims to mobilize these young people \"to take action against social injustice driven by NCDs.\".\n\nPreviously, chronic NCDs were considered a problem limited mostly to high income countries, while infectious diseases seemed to affect low income countries. The burden of disease attributed to NCDs has been estimated at 85% in industrialized nations, 70% in middle income nations, and nearly 50% in countries with the lowest national incomes. In 2008, chronic NCDs accounted for more than 60% (over 35 million) of the 57 million deaths worldwide. Given the global population distribution, almost 80% of deaths due to chronic NCDs worldwide now occur in low and middle income countries, while only 20% occur in higher income countries.\n\nNational economies are reportedly suffering significant losses because of premature deaths or inability to work resulting from heart disease, stroke and diabetes. For instance, China is expected to lose roughly $558 billion in national income between 2005 and 2015 due to early deaths. In 2005, heart disease, stroke and diabetes caused an estimated loss in international dollars of national income of 9 billion in India and 3 billion in Brazil.\n\nThe burden of chronic NCDs including mental health conditions is felt in workplaces around the world, notably due to elevated levels of absenteeism, or absence from work because of illness, and presenteeism, or productivity lost from staff coming to work and performing below normal standards due to poor health. For example, the United Kingdom experienced a loss of about 175 million days in 2006 to absence from illness among a working population of 37.7 million people. The estimated cost of absences due to illness was over 20 billion pounds in the same year. The cost due to presenteeism is likely even larger, although methods of analyzing the economic impacts of presenteeism are still being developed. Methods for analyzing the distinct workplace impacts of NCDs versus other types of health conditions are also still being developed.\n\nFor the vast majority of cancers, risk factors are environmental or lifestyle-related, thus cancers are mostly preventable NCD. Greater than 30% of cancer is preventable via avoiding risk factors including: tobacco, being overweight or obesity, low fruit and vegetable intake, physical inactivity, alcohol, sexually transmitted infections, and air pollution. Infectious agents are responsible for some cancers, for instance almost all cervical cancers are caused by human papillomavirus infection.\n\nThe first studies on cardiovascular health were performed in 1949 by Jerry Morris using occupational health data and were published in 1958. The causes, prevention, and/or treatment of all forms of cardiovascular disease remain active fields of biomedical research, with hundreds of scientific studies being published on a weekly basis. A trend has emerged, particularly in the early 2000s, in which numerous studies have revealed a link between fast food and an increase in heart disease. These studies include those conducted by the Ryan Mackey Memorial Research Institute, Harvard University and the Sydney Center for Cardiovascular Health. Many major fast food chains, particularly McDonald's, have protested the methods used in these studies and have responded with healthier menu options.\n\nA fairly recent emphasis is on the link between low-grade inflammation that hallmarks atherosclerosis and its possible interventions. C-reactive protein (CRP) is a common inflammatory marker that has been found to be present in increased levels in patients at risk for cardiovascular disease. Also osteoprotegerin which involved with regulation of a key inflammatory transcription factor called NF-κB has been found to be a risk factor of cardiovascular disease and mortality.\n\nType 2 Diabetes Mellitus is a chronic condition which is largely preventable and manageable but difficult to cure. Management concentrates on keeping blood sugar levels as close to normal (\"euglycemia\") as possible without presenting undue patient danger. This can usually be with close dietary management, exercise, and use of appropriate medications (insulin only in the case of type 1 diabetes mellitus. Oral medications may be used in the case of type 2 diabetes, as well as insulin).\n\nPatient education, understanding, and participation is vital since the complications of diabetes are far less common and less severe in people who have well-managed blood sugar levels.\nWider health problems may accelerate the deleterious effects of diabetes. These include smoking, elevated cholesterol levels, obesity, high blood pressure, and lack of regular exercise.\n\nAlthough chronic kidney disease (CKD) is not currently identified as one of WHO's main targets for global NCD control, there is compelling evidence that CKD is not only common, harmful and treatable but also a major contributing factor to the incidence and outcomes of at least three of the diseases targeted by WHO (diabetes, hypertension and CVD).\nCKD strongly predisposes to hypertension and CVD; diabetes, hypertension and CVD are all major causes of CKD; and major risk factors for diabetes, hypertension and CVD (such as obesity and smoking) also cause or exacerbate CKD. In addition, among people with diabetes, hypertension, or CVD, the subset who also have CKD are at highest risk of adverse outcomes and high health care costs. Thus, CKD, diabetes and cardiovascular disease are closely associated conditions that often coexist; share common risk factors and treatments; and would benefit from a coordinated global approach to prevention and control.\n\n\"Main Article: Chronic respiratory disease\"\n\nChronic Respiratory Diseases (CRDs) are diseases of the lungs and airways. According to the World Health Organization (WHO) hundreds of millions of people suffer daily from CRDs. Common CRDs are: Asthma, Chronic obstructive pulmonary disease, Occupational lung disease, and Pulmonary hypertension. While CRDs are not curable, various treatments are available to help improve quality of life for individuals who have them. Most treatments involve dilating major airways to improve shortness of breath among other symptoms. The main risk factors for developing CRDs are: tobacco smoking, indoor and outdoor air pollution, allergens, and occupational risks.\n\nWHO helped launch the Global Alliance against Chronic Respiratory Diseases (GARD) in 2006. GARD is voluntarily composed of national and international organizations and works toward \"reducing the global burden of chronic respiratory diseases\" and focus mainly on vulnerable populations and low and middle-income countries.\n\n\n"}
{"id": "21139", "url": "https://en.wikipedia.org/wiki?curid=21139", "title": "North America", "text": "North America\n\nNorth America is a continent entirely within the Northern Hemisphere and almost all within the Western Hemisphere; it is also considered by some to be a northern subcontinent of the Americas. It is bordered to the north by the Arctic Ocean, to the east by the Atlantic Ocean, to the west and south by the Pacific Ocean, and to the southeast by South America and the Caribbean Sea.\n\nNorth America covers an area of about 24,709,000 square kilometers (9,540,000 square miles), about 16.5% of the earth's land area and about 4.8% of its total surface.\nNorth America is the third largest continent by area, following Asia and Africa, and the fourth by population after Asia, Africa, and Europe. In 2013, its population was estimated at nearly 579 million people in 23 independent states, or about 7.5% of the world's population, if nearby islands (most notably the Caribbean) are included.\n\nNorth America was reached by its first human populations during the last glacial period, via crossing the Bering land bridge approximately 40,000 to 17,000 years ago. The so-called Paleo-Indian period is taken to have lasted until about 10,000 years ago (the beginning of the Archaic or Meso-Indian period). The Classic stage spans roughly the 6th to 13th centuries. The Pre-Columbian era ended in 1492, and the transatlantic migrations—the arrival of European settlers during the Age of Discovery and the Early Modern period. Present-day cultural and ethnic patterns reflect different kinds of interactions between European colonists, indigenous peoples, African slaves and their descendants.\n\nEuropean influences are strongest in the northern parts of the continent while indigenous and African influences are relatively stronger in the south. Because of the history of colonialism, most North Americans speak English, Spanish or French and societies and states commonly reflect Western traditions.\n\nThe Americas are usually accepted as having been named after the Italian explorer Amerigo Vespucci by the German cartographers Martin Waldseemüller and Matthias Ringmann. Vespucci, who explored South America between 1497 and 1502, was the first European to suggest that the Americas were not the East Indies, but a different landmass previously unknown by Europeans. In 1507, Waldseemüller produced a world map, in which he placed the word \"America\" on the continent of South America, in the middle of what is today Brazil. He explained the rationale for the name in the accompanying book \"Cosmographiae Introductio\":\n\nFor Waldseemüller, no one should object to the naming of the land after its discoverer. He used the Latinized version of Vespucci's name (Americus Vespucius), but in its feminine form \"America\", following the examples of \"Europa\", \"Asia\" and \"Africa\".\n\nLater, other mapmakers extended the name \"America\" to the northern continent, In 1538, Gerard Mercator used \"America\" on his map of the world for all the Western Hemisphere.\n\nSome argue that the convention is to use the surname for naming discoveries except in the case of royalty and so a derivation from \"Amerigo Vespucci\" could be problematic. Ricardo Palma (1949) proposed a derivation from the \"Amerrique\" mountains of Central America—Vespucci was the first to discover South America and the \"Amerrique\" mountains of Central America, which connected his discoveries to those of Christopher Columbus.\n\nAlfred E. Hudd proposed a theory in 1908 that the continents are named after a Welsh merchant named Richard Amerike from Bristol, who is believed to have financed John Cabot's voyage of discovery from England to Newfoundland in 1497. A minutely explored belief that has been advanced is that America was named for a Spanish sailor bearing the ancient Visigothic name of 'Amairick'. Another is that the name is rooted in a Native American language.\n\nThe United Nations formally recognizes \"North America\" as comprising three areas: Northern America, Central America, and The Caribbean. This has been formally defined by the UN Statistics Division.\n\nThe term \"North America\" maintains various definitions in accordance with location and context. In Canadian English, \"North America\" generally refers to the land mass as a whole consisting of Mexico, the United States, and Canada, although it is generally ambiguous which other countries are included, and is mostly defined by context. In the United States of America, usage of the term may refer only to Canada and the USA, and sometimes includes Greenland and Mexico (as in the North American Free Trade Agreement), as well as offshore islands.\n\nIn France, Italy, Portugal, Spain, Romania, Greece, and the countries of Latin America, the cognates of \"North America\" usually designate a subcontinent of the Americas comprising Canada, the United States, and Mexico, and often Greenland, Saint Pierre et Miquelon, and Bermuda.\n\nNorth America has been historically referred to by other names. Spanish North America (New Spain) was often referred to as \"Northern America\", and this was the first official name given to Mexico.\n\nGeographically the North American continent has many regions and subregions. These include cultural, economic, and geographic regions. Economic regions included those formed by trade blocs, such as the North American Trade Agreement bloc and Central American Trade Agreement. Linguistically and culturally, the continent could be divided into Anglo-America and Latin America. Anglo-America includes most of Northern America, Belize, and Caribbean islands with English-speaking populations (though sub-national entities, such as Louisiana and Quebec, have large Francophone populations; in Quebec, French is the sole official language).\n\nThe southern North American continent is composed of two regions. These are Central America and the Caribbean. The north of the continent maintains recognized regions as well. In contrast to the common definition of \"North America\", which encompasses the whole continent, the term \"North America\" is sometimes used to refer only to Mexico, Canada, the United States, and Greenland.\n\nThe term Northern America refers to the northern-most countries and territories of North America: the United States, Bermuda, St. Pierre and Miquelon, Canada and Greenland. Although the term does not refer to a unified region, Middle America—not to be confused with the Midwestern United States—groups the regions of Mexico, Central America, and the Caribbean.\n\nThe largest countries of the continent, Canada and the United States, also contain well-defined and recognized regions. In the case of Canada these are (from east to west) Atlantic Canada, Central Canada, Canadian Prairies, the British Columbia Coast, and Northern Canada. These regions also contain many subregions. In the case of the United States – and in accordance with the US Census Bureau definitions – these regions are: New England, Mid-Atlantic, South Atlantic States, East North Central States, West North Central States, East South Central States, West South Central States, Mountain States, and Pacific States. Regions shared between both nations included the Great Lakes Region. Megalopolises have formed between both nations in the case of the Pacific Northwest and the Great Lakes Megaregion.\n\nLaurentia is an ancient craton which forms the geologic core of North America; it formed between 1.5 and 1.0 billion years ago during the Proterozoic eon. The Canadian Shield is the largest exposure of this craton. From the Late Paleozoic to Early Mesozoic eras, North America was joined with the other modern-day continents as part of the supercontinent Pangaea, with Eurasia to its east. One of the results of the formation of Pangaea was the Appalachian Mountains, which formed some 480 million years ago, making it among the oldest mountain ranges in the world. When Pangaea began to rift around 200 million years ago, North America became part of Laurasia, before it separated from Eurasia as its own continent during the mid-Cretaceous period. The Rockies and other western mountain ranges began forming around this time from a period of mountain building called the Laramide orogeny, between 80 and 55 million years ago. The formation of the Isthmus of Panama that connected the continent to South America arguably occurred approximately 12 to 15 million years ago, and the Great Lakes (as well as many other northern freshwater lakes and rivers) were carved by receding glaciers about 10,000 years ago.\n\nNorth America is the source of much of what humanity knows about geologic time periods. The geographic area that would later become the United States has been the source of more varieties of dinosaurs than any other modern country. According to paleontologist Peter Dodson, this is primarily due to stratigraphy, climate and geography, human resources, and history. Much of the Mesozoic Era is represented by exposed outcrops in the many arid regions of the continent. The most significant Late Jurassic dinosaur-bearing fossil deposit in North America is the Morrison Formation of the western United States.\n\nThe indigenous peoples of North America have many creation myths by which they assert that they have been present on the land since its creation. The specifics of Paleo-Indian migration to and throughout the Americas, including the exact dates and routes traveled, are subject to ongoing research and discussion. The traditional theory has been that these early migrants moved into the Beringia land bridge between eastern Siberia and present-day Alaska around 25,000 to 11,000 years ago. The few agreements achieved to date are the origin from Central Asia, with widespread habitation of the Americas during the end of the last glacial period, or more specifically what is known as the late glacial maximum, around 13,000 years before present. Some genetic research indicated secondary waves of migration occurred after the initial Paleo-Indian colonization, but prior to modern Inuit, Inupiat and Yupik expansions.\n\nBefore contact with Europeans, the natives of North America were divided into many different polities, from small bands of a few families to large empires. They lived in several \"culture areas\", which roughly correspond to geographic and biological zones and give a good indication of the main lifeway or occupation of the people who lived there (e.g., the bison hunters of the Great Plains, or the farmers of Mesoamerica). Native groups can also be classified by their language family (e.g., Athapascan or Uto-Aztecan). Peoples with similar languages did not always share the same material culture, nor were they always allies. Anthropologists think that the Inuit people of the high Arctic came to North America much later than other native groups, as evidenced by the disappearance of Dorset culture artifacts from the archaeological record, and their replacement by the Thule people.\n\nDuring the thousands of years of native habitation on the continent, cultures changed and shifted. One of the oldest cultures yet found is the Clovis culture of modern New Mexico. Later cultures include the Mississippian culture and related Mound building cultures, found in the Mississippi river valley and the Pueblo culture of what is now the Four Corners. The more southern cultural groups of North America were responsible for the domestication of many common crops now used around the world, such as tomatoes and squash. Perhaps most importantly they domesticated one of the world's major staples, maize (corn).\n\nThe earliest verifiable instance of pre-Columbian trans-oceanic contact by any European culture with the landmasses that geologically constitute the \"mainland\" of modern North America has been dated to the end of the 10th century CE – this site, situated at the northernmost extent of the island named Newfoundland, is known as L'Anse aux Meadows, where unmistakable evidence of Norse settlement was uncovered in the early 1960s.\n\nAs a result of the development of agriculture in the south, many important cultural advances were made there. For example, the Maya civilization developed a writing system, built huge pyramids and temples, had a complex calendar, and developed the concept of zero around 400 CE, a few hundred years after the Mesopotamians. The Mayan culture was still present in southern Mexico and Guatemala when the Spanish explorers arrived, but political dominance in the area had shifted to the Aztec Empire whose capital city Tenochtitlan was located further north in the Valley of Mexico. The Aztecs were conquered in 1521 by Hernán Cortés.\n\nDuring the Age of Discovery, Europeans explored and staked claims to various parts of North America. Upon their arrival in the \"New World\", the Native American population declined substantially, because of violent conflicts with the invaders and the introduction of European diseases to which the Native Americans lacked immunity. Native culture changed drastically and their affiliation with political and cultural groups also changed. Several linguistic groups died out, and others changed quite quickly. The names and cultures that Europeans recorded were not necessarily the same as the names they had used a few generations before, or the ones in use today.\n\nBritain, Spain, and France took over extensive territories in North America – and fought over them. In the late 18th century and beginning of the 19th, independence movements that sprung up across the continent, led to the creation of the modern countries in the area. The 13 British colonies on the North Atlantic coast declared independence in 1776, becoming the United States of America. Canada was formed from the unification of northern territories controlled by Britain and France. New Spain, a territory that stretched from modern-day southern US to Central America, declared independence in 1810, becoming the First Mexican Empire. In 1823 the former Captaincy General of Guatemala, then part of the Mexican Empire, became the first independent state in Central America, officially changing its name to the United Provinces of Central America.\n\nNorth America occupies the northern portion of the landmass generally referred to as the New World, the Western Hemisphere, the Americas, or simply America (which, less commonly, is considered by some as a single continent with North America a subcontinent). North America's only land connection to South America is at the Isthmus of Panama. The continent is delimited on the southeast by most geographers at the Darién watershed along the Colombia-Panama border, placing all of Panama within North America. Alternatively, some geologists physiographically locate its southern limit at the Isthmus of Tehuantepec, Mexico, with Central America extending southeastward to South America from this point. The Caribbean islands, or West Indies, are considered part of North America. The continental coastline is long and irregular. The Gulf of Mexico is the largest body of water indenting the continent, followed by Hudson Bay. Others include the Gulf of Saint Lawrence and the Gulf of California.\n\nBefore the Central American isthmus formed, the region had been underwater. The islands of the West Indies delineate a submerged former land bridge, which had connected North and South America via what are now Florida and Venezuela.\n\nThere are numerous islands off the continent's coasts; principally, the Arctic Archipelago, the Bahamas, Turks & Caicos, the Greater and Lesser Antilles, the Aleutian Islands (some of which are in the Eastern Hemisphere proper), the Alexander Archipelago, the many thousand islands of the British Columbia Coast, and Newfoundland. Greenland, a self-governing Danish island, and the world's largest, is on the same tectonic plate (the North American Plate) and is part of North America geographically. In a geologic sense, Bermuda is not part of the Americas, but an oceanic island which was formed on the fissure of the Mid-Atlantic Ridge over 100 million years ago. The nearest landmass to it is Cape Hatteras, North Carolina. However, Bermuda is often thought of as part of North America, especially given its historical, political and cultural ties to Virginia and other parts of the continent.\n\nThe vast majority of North America is on the North American Plate. Parts of western Mexico, including Baja California, and of California, including the cities of San Diego, Los Angeles, and Santa Cruz, lie on the eastern edge of the Pacific Plate, with the two plates meeting along the San Andreas fault. The southernmost portion of the continent and much of the West Indies lie on the Caribbean Plate, whereas the Juan de Fuca and Cocos plates border the North American Plate on its western frontier.\n\nThe continent can be divided into four great regions (each of which contains many subregions): the Great Plains stretching from the Gulf of Mexico to the Canadian Arctic; the geologically young, mountainous west, including the Rocky Mountains, the Great Basin, California and Alaska; the raised but relatively flat plateau of the Canadian Shield in the northeast; and the varied eastern region, which includes the Appalachian Mountains, the coastal plain along the Atlantic seaboard, and the Florida peninsula. Mexico, with its long plateaus and cordilleras, falls largely in the western region, although the eastern coastal plain does extend south along the Gulf.\n\nThe western mountains are split in the middle into the main range of the Rockies and the coast ranges in California, Oregon, Washington, and British Columbia, with the Great Basin—a lower area containing smaller ranges and low-lying deserts—in between. The highest peak is Denali in Alaska.\n\nThe United States Geographical Survey (USGS) states that the geographic center of North America is \"6 miles [10 km] west of Balta, Pierce County, North Dakota\" at about , about from Rugby, North Dakota. The USGS further states that \"No marked or monumented point has been established by any government agency as the geographic center of either the 50 States, the conterminous United States, or the North American continent.\" Nonetheless, there is a field stone obelisk in Rugby claiming to mark the center. The North American continental pole of inaccessibility is located from the nearest coastline, between Allen and Kyle, South Dakota at .\n\nGeologically, Canada is one of the oldest regions in the world, with more than half of the region consisting of precambrian rocks that have been above sea level since the beginning of the Palaeozoic era. Canada's mineral resources are diverse and extensive. Across the Canadian Shield and in the north there are large iron, nickel, zinc, copper, gold, lead, molybdenum, and uranium reserves. Large diamond concentrations have been recently developed in the Arctic, making Canada one of the world's largest producers. Throughout the Shield there are many mining towns extracting these minerals. The largest, and best known, is Sudbury, Ontario. Sudbury is an exception to the normal process of forming minerals in the Shield since there is significant evidence that the Sudbury Basin is an ancient meteorite impact crater. The nearby, but less known Temagami Magnetic Anomaly has striking similarities to the Sudbury Basin. Its magnetic anomalies are very similar to the Sudbury Basin, and so it could be a second metal-rich impact crater. The Shield is also covered by vast boreal forests that support an important logging industry.\n\nThe lower 48 US states can be divided into roughly five physiographic provinces:\n\nThe geology of Alaska is typical of that of the cordillera, while the major islands of Hawaii consist of Neogene volcanics erupted over a hot spot.\n\nCentral America is geologically active with volcanic eruptions and earthquakes occurring from time to time. In 1976 Guatemala was hit by a major earthquake, killing 23,000 people; Managua, the capital of Nicaragua, was devastated by earthquakes in 1931 and 1972, the last one killing about 5,000 people; three earthquakes devastated El Salvador, one in 1986 and two in 2001; one earthquake devastated northern and central Costa Rica in 2009, killing at least 34 people; in Honduras a powerful earthquake killed seven people in 2009.\n\nVolcanic eruptions are common in the region. In 1968 the Arenal Volcano, in Costa Rica, erupted and killed 87 people. Fertile soils from weathered volcanic lavas have made it possible to sustain dense populations in the agriculturally productive highland areas.\n\nCentral America has many mountain ranges; the longest are the Sierra Madre de Chiapas, the Cordillera Isabelia, and the Cordillera de Talamanca. Between the mountain ranges lie fertile valleys that are suitable for the people; in fact, most of the population of Honduras, Costa Rica, and Guatemala live in valleys. Valleys are also suitable for the production of coffee, beans, and other crops.\n\nNorth America is a very large continent which surpasses the Arctic Circle, and the Tropic of Cancer. Greenland, along with the Canadian Shield, is tundra with average temperatures ranging from , but central Greenland is composed of a very large ice sheet. This tundra radiates throughout Canada, but its border ends near the Rocky Mountains (but still contains Alaska) and at the end of the Canadian Shield, near the Great Lakes.\nClimate west of the Cascades is described as being a temperate weather with average precipitation .\nClimate in coastal California is described to be Mediterranean, with average temperatures in cities like San Francisco ranging from over the course of the year.\n\nStretching from the East Coast to eastern North Dakota, and stretching down to Kansas, is the continental-humid climate featuring intense seasons, with a large amount of annual precipitation, with places like New York City averaging .\nStarting at the southern border of the continental-humid climate and stretching to the Gulf of Mexico (whilst encompassing the eastern half of Texas) is the subtropical climate. This area has the wettest cities in the contiguous U.S. with annual precipitation reaching in Mobile, Alabama.\nStretching from the borders of the continental humid and subtropical climates, and going west to the Cascades Sierra Nevada, south to the southern tip of durango, north to the border with tundra climate, the steppe/desert climate is the driest climate in the U.S. Highland climates cut from north to south of the continent, where subtropical or temperate climates occur just below the tropics, as in central Mexico and Guatemala. Tropical climates appear in the island regions and in the subcontinent's bottleneck. Usually of the savannah type, with rains and high temperatures constants the whole year. Found in countries and states bathed by the Caribbean Sea or to south of the Gulf of Mexico and Pacific Ocean.\n\nNotable North American fauna include the bison, black bear, prairie dog, turkey, pronghorn, raccoon, coyote and monarch butterfly.\n\nNotable plants that were domesticated in North America include tobacco, maize, squash, tomato, sunflower, blueberry, avocado, cotton, chile pepper\nand vanilla.\n\nEconomically, Canada and the United States are the wealthiest and most developed nations in the continent, followed by Mexico, a newly industrialized country. The countries of Central America and the Caribbean are at various levels of economic and human development. For example, small Caribbean island-nations, such as Barbados, Trinidad and Tobago, and Antigua and Barbuda, have a higher GDP (PPP) per capita than Mexico due to their smaller populations. Panama and Costa Rica have a significantly higher Human Development Index and GDP than the rest of the Central American nations. Additionally, despite Greenland's vast resources in oil and minerals, much of them remain untapped, and the island is economically dependent on fishing, tourism, and subsidies from Denmark. Nevertheless, the island is highly developed.\n\nDemographically, North America is ethnically diverse. Its three main groups are Caucasians, Mestizos and Blacks. There is a significant minority of Indigenous Americans and Asians among other less numerous groups.\n\nThe dominant languages in North America are English, Spanish, and French. Danish is prevalent in Greenland alongside Greenlandic, and Dutch is spoken side by side local languages in the Dutch Caribbean. The term Anglo-America is used to refer to the anglophone countries of the Americas: namely Canada (where English and French are co-official) and the United States, but also sometimes Belize and parts of the tropics, especially the Commonwealth Caribbean. Latin America refers to the other areas of the Americas (generally south of the United States) where the Romance languages, derived from Latin, of Spanish and Portuguese (but French speaking countries are not usually included) predominate: the other republics of Central America (but not always Belize), part of the Caribbean (not the Dutch-, English-, or French-speaking areas), Mexico, and most of South America (except Guyana, Suriname, French Guiana (France), and the Falkland Islands (UK)).\n\nThe French language has historically played a significant role in North America and now retains a distinctive presence in some regions. Canada is officially bilingual. French is the official language of the Province of Quebec, where 95% of the people speak it as either their first or second language, and it is co-official with English in the Province of New Brunswick. Other French-speaking locales include the Province of Ontario (the official language is English, but there are an estimated 600,000 Franco-Ontarians), the Province of Manitoba (co-official as \"de jure\" with English), the French West Indies and Saint-Pierre et Miquelon, as well as the US state of Louisiana, where French is also an official language. Haiti is included with this group based on historical association but Haitians speak both Creole and French. Similarly, French and French Antillean Creole is spoken in Saint Lucia and the Commonwealth of Dominica alongside English.\n\nChristianity is the largest religion in the United States, Canada and Mexico. According to a 2012 Pew Research Center survey, 77% of the population considered themselves Christians. Christianity also is the predominant religion in the 23 dependent territories in North America. The United States has the largest Christian population in the world, with nearly 247 million Christians (70%), although other countries have higher percentages of Christians among their populations. Mexico has the world's second largest number of Catholics, surpassed only by Brazil. A 2015 study estimates about 493,000 Christian believers from a Muslim background in North America, most of them belonging to some form of Protestantism.\n\nAccording to the same study religiously unaffiliated (include agnostic and atheist) make up about 17% of the population of Canada and the United States. No religion make up about 24% of the United States population, and 24% of Canada total population.\n\nCanada, the United States and Mexico host communities of both Jews (6 million or about 1.8%), Buddhists (3.8 million or 1.1%) and Muslims (3.4 million or 1.0%). The biggest number of Jewish individuals can be found in the United States (5.4 million), Canada (375,000) and Mexico (67,476). The United States host the largest Muslim population in North America with 2.7 million or 0.9%, While Canada host about one million Muslim or 3.2% of the population. While in Mexico there were 3,700 Muslims in the country. In 2012, \"U-T San Diego\" estimated U.S. practitioners of Buddhism at 1.2 million people, of whom 40% are living in Southern California.\n\nThe predominant religion in Central America is Christianity (96%). Beginning with the Spanish colonization of Central America in the 16th century, Roman Catholicism became the most popular religion in the region until the first half of the 20th century. Since the 1960s, there has been an increase in other Christian groups, particularly Protestantism, as well as other religious organizations, and individuals identifying themselves as having no religion. Also Christianity is the predominant religion in the Caribbean (85%). Other religious groups in the region are Hinduism, Islam, Rastafari (in Jamaica), and Afro-American religions such as Santería and Vodou.\n\nThe most populous country in North America is the United States with 318.4 million persons. The second largest country is Mexico with a population of 112,322,757. Canada is the third most populous country with 32,623,490. The majority of Caribbean island-nations have national populations under a million, though Cuba, Dominican Republic, Haiti, Puerto Rico (a territory of the United States), Jamaica, and Trinidad and Tobago each have populations higher than a million. Greenland has a small population of 55,984 for its massive size (2,166,000 km² or 836,300 mi²), and therefore, it has the world's lowest population density at 0.026 pop./km² (0.067 pop./mi²).\n\nWhile the United States, Canada, and Mexico maintain the largest populations, large city populations are not restricted to those nations. There are also large cities in the Caribbean. The largest cities in North America, by far, are Mexico City and New York. These cities are the only cities on the continent to exceed eight million, and two of three in the Americas. Next in size are Los Angeles, Toronto, Chicago, Havana, Santo Domingo, and Montreal. Cities in the sunbelt regions of the United States, such as those in Southern California and Houston, Phoenix, Miami, Atlanta, and Las Vegas, are experiencing rapid growth. These causes included warm temperatures, retirement of Baby Boomers, large industry, and the influx of immigrants. Cities near the United States border, particularly in Mexico, are also experiencing large amounts of growth. Most notable is Tijuana, a city bordering San Diego that receives immigrants from all over Latin America and parts of Europe and Asia. Yet as cities grow in these warmer regions of North America, they are increasingly forced to deal with the major issue of water shortages.\n\nEight of the top ten metropolitan areas are located in the United States. These metropolitan areas all have a population of above 5.5 million and include the New York City metropolitan area, Los Angeles metropolitan area, Chicago metropolitan area, and the Dallas–Fort Worth metroplex. Whilst the majority of the largest metropolitan areas are within the United States, Mexico is host to the largest metropolitan area by population in North America: Greater Mexico City. Canada also breaks into the top ten largest metropolitan areas with the Toronto metropolitan area having six million people. The proximity of cities to each other on the Canada–United States border and Mexico–United States border has led to the rise of international metropolitan areas. These urban agglomerations are observed at their largest and most productive in Detroit–Windsor and San Diego–Tijuana and experience large commercial, economic, and cultural activity. The metropolitan areas are responsible for millions of dollars of trade dependent on international freight. In Detroit-Windsor the Border Transportation Partnership study in 2004 concluded US$13 billion was dependent on the Detroit–Windsor international border crossing while in San Diego-Tijuana freight at the Otay Mesa Port of Entry was valued at US$20 billion.\n\nNorth America has also been witness to the growth of megapolitan areas. In the United States exists eleven megaregions that transcend international borders and comprise Canadian and Mexican metropolitan regions. These are the Arizona Sun Corridor, Cascadia, Florida, Front Range, Great Lakes Megaregion, Gulf Coast Megaregion, Northeast, Northern California, Piedmont Atlantic, Southern California, and the Texas Triangle. Canada and Mexico are also the home of megaregions. These include the Quebec City – Windsor Corridor, Golden Horseshoe – both of which are considered part of the Great Lakes Megaregion – and megalopolis of Central Mexico. Traditionally the largest megaregion has been considered the Boston-Washington, DC Corridor, or the Northeast, as the region is one massive contiguous area. Yet have allowed the Great Lakes Megalopolis to maintain status as the most populated region, being home to 53,768,125 people in 2000.\n\nThe top ten largest North American metropolitan areas by population as of 2013, based on national census numbers from the United States and census estimates from Canada and Mexico.\n\nNorth America's GDP per capita was evaluated in October 2016 by the International Monetary Fund (IMF) to be $41,830, making it the richest continent in the world, followed by Oceania.\n\nCanada, Mexico, and the United States have significant and multifaceted economic systems. The United States has the largest economy of all three countries and in the world. In 2016, the U.S. had an estimated per capita gross domestic product (PPP) of $57,466 according to the World Bank, and is the most technologically developed economy of the three. The United States' services sector comprises 77% of the country's GDP (estimated in 2010), industry comprises 22% and agriculture comprises 1.2%. The U.S. economy is also the fastest growing economy in North America and the Americas as a whole, with the highest GDP per capita in the Americas as well.\n\nCanada shows significant growth in the sectors of services, mining and manufacturing. Canada's per capita GDP (PPP) was estimated at $44,656 and it had the 11th largest GDP (nominal) in 2014. Canada's services sector comprises 78% of the country's GDP (estimated in 2010), industry comprises 20% and agriculture comprises 2%. Mexico has a per capita GDP (PPP) of $16,111 and as of 2014 is the 15th largest GDP (nominal) in the world. Being a newly industrialized country, Mexico maintains both modern and outdated industrial and agricultural facilities and operations. Its main sources of income are oil, industrial exports, manufactured goods, electronics, heavy industry, automobiles, construction, food, banking and financial services.\n\nThe North American economy is well defined and structured in three main economic areas. These areas are the North American Free Trade Agreement (NAFTA), Caribbean Community and Common Market (CARICOM), and the Central American Common Market (CACM). Of these trade blocs, the United States takes part in two. In addition to the larger trade blocs there is the Canada-Costa Rica Free Trade Agreement among numerous other free trade relations, often between the larger, more developed countries and Central American and Caribbean countries.\n\nThe North America Free Trade Agreement (NAFTA) forms one of the four largest trade blocs in the world. Its implementation in 1994 was designed for economic homogenization with hopes of eliminating barriers of trade and foreign investment between Canada, the United States and Mexico. While Canada and the United States already conducted the largest bilateral trade relationship – and to present day still do – in the world and Canada–United States trade relations already allowed trade without national taxes and tariffs, NAFTA allowed Mexico to experience a similar duty-free trade. The free trade agreement allowed for the elimination of tariffs that had previously been in place on United States-Mexico trade. Trade volume has steadily increased annually and in 2010, surface trade between the three NAFTA nations reached an all-time historical increase of 24.3% or US$791 billion. The NAFTA trade bloc GDP (PPP) is the world's largest with US$17.617 trillion. This is in part attributed to the fact that the economy of the United States is the world's largest national economy; the country had a nominal GDP of approximately $14.7 trillion in 2010. The countries of NAFTA are also some of each other's largest trade partners. The United States is the largest trade partner of Canada and Mexico; while Canada and Mexico are each other's third largest trade partners.\n\nThe Caribbean trade bloc – CARICOM – came into agreement in 1973 when it was signed by 15 Caribbean nations. As of 2000, CARICOM trade volume was US$96 billion. CARICOM also allowed for the creation of a common passport for associated nations. In the past decade the trade bloc focused largely on Free Trade Agreements and under the CARICOM Office of Trade Negotiations (OTN) free trade agreements have been signed into effect.\n\nIntegration of Central American economies occurred under the signing of the Central American Common Market agreement in 1961; this was the first attempt to engage the nations of this area into stronger financial cooperation. Recent implementation of the Central American Free Trade Agreement (CAFTA) has left the future of the CACM unclear. The Central American Free Trade Agreement was signed by five Central American countries, the Dominican Republic, and the United States. The focal point of CAFTA is to create a free trade area similar to that of NAFTA. In addition to the United States, Canada also has relations in Central American trade blocs. Currently under proposal, the Canada – Central American Free Trade Agreement (CA4) would operate much the same as CAFTA with the United States does.\n\nThese nations also take part in inter-continental trade blocs. Mexico takes a part in the G3 Free Trade Agreement with Colombia and Venezuela and has a trade agreement with the EU. The United States has proposed and maintained trade agreements under the Transatlantic Free Trade Area between itself and the European Union; the US-Middle East Free Trade Area between numerous Middle Eastern nations and itself; and the Trans-Pacific Strategic Economic Partnership between Southeast Asian nations, Australia, and New Zealand.\n\nThe Pan-American Highway route in the Americas is the portion of a network of roads nearly in length which travels through the mainland nations. No definitive length of the Pan-American Highway exists because the US and Canadian governments have never officially defined any specific routes as being part of the Pan-American Highway, and Mexico officially has many branches connecting to the US border. However, the total length of the portion from Mexico to the northern extremity of the highway is roughly .\n\nThe First Transcontinental Railroad in the United States was built in the 1860s, linking the railroad network of the eastern US with California on the Pacific coast. Finished on 10 May 1869 at the famous golden spike event at Promontory Summit, Utah, it created a nationwide mechanized transportation network that revolutionized the population and economy of the American West, catalyzing the transition from the wagon trains of previous decades to a modern transportation system. Although an accomplishment, it achieved the status of first transcontinental railroad by connecting myriad eastern US railroads to the Pacific and was not the largest single railroad system in the world. The Canadian Grand Trunk Railway (GTR) had, by 1867, already accumulated more than of track by connecting Ontario with the Canadian Atlantic provinces west as far as Port Huron, Michigan, through Sarnia, Ontario.\n\nA shared telephone system known as the North American Numbering Plan (NANP) is an integrated telephone numbering plan of 24 countries and territories: the United States and its territories, Canada, Bermuda, and 17 Caribbean nations.\n\nCanada and the United States were both former British colonies. There is frequent cultural interplay between the United States and English-speaking Canada. Greenland shares some cultural ties with the indigenous people of Canada but is considered Nordic and has strong Danish ties due to centuries of colonization by Denmark. Spanish-speaking North America shares a common past as former Spanish colonies. In Mexico and the Central American countries where civilizations like the Maya developed, indigenous people preserve traditions across modern boundaries. Central American and Spanish-speaking Caribbean nations have historically had more in common due to geographical proximity.\n\nNorthern Mexico, particularly in the cities of Monterrey, Tijuana, Ciudad Juárez, and Mexicali, is strongly influenced by the culture and way of life of the United States. Of the aforementioned cities, Monterrey has been regarded as the most Americanized city in Mexico. Immigration to the United States and Canada remains a significant attribute of many nations close to the southern border of the US. The Anglophone Caribbean states have witnessed the decline of the British Empire and its influence on the region, and its replacement by the economic influence of Northern America. In the Anglophone Caribbean. This is partly due to the relatively small populations of the English-speaking Caribbean countries, and also because many of them now have more people living abroad than those remaining at home. Northern Mexico, the Western United States and Alberta, Canada share a cowboy culture.\n\nCanada, Mexico and the US submitted a joint bid to host the 2026 FIFA World Cup.\nThe following table shows the most prominent sports leagues in North America, in order of average revenue.\n\n\n"}
{"id": "14015821", "url": "https://en.wikipedia.org/wiki?curid=14015821", "title": "Oil burden", "text": "Oil burden\n\nOil burden is the volume of petroleum consumed, multiplied by the average price, and divided by nominal gross domestic product.\nThis gives the proportion of the world economy devoted to buying oil. It is a concept developed by Veronique Riches-Flores of Société Générale.\n"}
{"id": "16985133", "url": "https://en.wikipedia.org/wiki?curid=16985133", "title": "Periphery countries", "text": "Periphery countries\n\nIn world systems theory, the periphery countries (sometimes referred to as just the periphery) are those that are less developed than the semi-periphery and core countries. These countries usually receive a disproportionately small share of global wealth. They have weak state institutions and are dependent on – according to some, exploited by – more developed countries. These countries are usually behind because of obstacles such as lack of technology, unstable government, and poor education and health systems. In some instances, the exploitation of periphery countries' agriculture, cheap labor, and natural resources aid core countries in remaining dominant. This is best described by dependency theory, which is one theory on how globalization can affect the world and the countries in it. It is, however, possible for periphery countries to rise out of their status and move into semi-periphery or core status. This can be done by doing things such as industrializing, stabilizing the government and political climate, etc.\n\nPeriphery countries are those that exist on the outer edges of global trade. There could be many reasons for a country to be considered peripheral, such as a dysfunctional or inefficient government. For example, some nations customs and ports are so inefficient that even though they are geographically closer it is cheaper to ship goods from longer distances. Other reasons such as wars, non-central location, insufficient infrastructure (rail lines, roads and communications) will keep a country in the periphery of global trade. Generally the populations tend to be poor and destitute so the core countries will exploit them for cheap labor and will even purposely interfere with their politics to keep things this way. Usually a peripheral country will specialize in one particular industry, leaving it vulnerable to economic instability and limiting international investment. Sometimes countries decide to isolate themselves, such as 14th century China.\n\nThere are a variety of reasons that periphery countries remain the way they are. One important factor that keeps countries in the periphery is the lack of development of technology. Another way periphery countries come to be is either the lack of a central government or the periphery country is under the control of another country. Periphery countries are known for exporting raw goods to core countries. What tends to happen is the maximum gain a periphery nation could earn is less than needed to maintain an equilibrium between costs and revenues. One thing periphery nations could do is to stop the increase of exports. At the beginning of the 19th century, Asia and Africa were considered periphery and their lack of development enabled the United States and Germany to remain successful core nations.\n\nAlthough periphery nations are exploited by core countries, there is a purpose to the unequal exchanges of goods. For instance, the core countries have an incentive to gain a profit and this enables the world market to further grow. At times, there is a change in the balance of trade between the periphery and core countries. This occurs when the prices of exports from periphery countries decrease at a faster rate than the exports from core nations. In this case, the governments of the periphery nation are affected in several ways. For instance, there is an increase in unemployment as well as a decrease in state income. This type of interaction is unique because the core country involved is somewhat weaker than normal. An example of this occurring is the growth of the industrial capabilities of Italy and Russia towards the end of the 19th century. This has also occurred in other periphery nations such as Brazil, Mexico, and South Africa.\n\nThe world system at this time was much different from the world system of today. Several areas were beginning to develop into trading powers but none were able to gain total control. For this reason, a core and periphery developed in each region as opposed to a global scale. Cities began to become the \"core\" with the more agricultural countryside becoming a sort of \"periphery\". The most underdeveloped region that was still involved in trade at the time was Europe. It had the weakest core and periphery areas.\n\nTwo examples of periphery countries in the late 15th century and early 16th century are Poland and Latin America. At this time, Poland was mainly exporting wheat to other areas of Europe and Poland wanted cheap labor. As a result, landlords enslaved rural workers on their estate lands. Also, Latin America experienced an enslavement of their natives and imports of slaves from Africa. Forced mining labor was placed on the slaves, which enabled Latin America to export cheap goods to Europe. Both Poland and Latin America were similar during this time period because the aristocrats of these areas became more wealthy due to their interactions with the world economy. These areas of the world were also different from during medieval times in Europe. They are different because during the late 15th century and early 16th century, Poland and Latin America were producing goods and exporting them rather than simply consuming their raw goods.\n\nThe relationship that the periphery countries have with the core countries is one that is defined by the exploitation of the periphery countries by the core countries. As many countries began to industrialize they looked for cheap goods and products. These industrialized \"core\" countries would then look to the less developed \"periphery\" countries for cheap goods. In most cases it is much easier and inexpensive to get these goods from other countries. Core countries realized this and began to use these cheap resources.\n\nFor the core countries to remain at the core, it is necessary that the periphery countries keep producing these cheap goods and remain mostly agricultural. The core countries are able to get goods very cheaply from the periphery and then are able to manufacture products and sell them at a relatively high price. The periphery countries are unable to make any gains because of this relationship and it is therefore impossible for them to ever industrialize. It is argued that if these countries are never able industrialize, they will continue to remain on the periphery.\n\nThe current relationship between core countries and periphery countries was mostly defined in the era of imperialism that occurred in the late 19th through the early 20th centuries. It was at this time that the countries with the strongest economies and militaries began to exploit those countries with weaker states. A result of this exploitation was the tendency of underdeveloped states or colonies to move more towards the production of one type of export that would then come to dominate their land, territory and lifestyle economy. Some examples of the time include Brazil's coffee production and Cuba's cigar production.\n\nPeriphery countries are continuously exploited by countries due to the exportation of surpluses of raw goods to the more technologically industrialized core countries for manufacturing and distribution. Recently some of the manufacturing has been moved to periphery countries but it is still controlled and owned by the core countries. There are, however, ways in which periphery countries can rise from their poor status and become semi-periphery countries or even core countries. It is crucial for the core countries to keep exploiting the natural resources of the periphery countries and to keep the governments semi-stable or else it could cause economic unrest for the core countries as a whole.\n\nThere are several ways in which periphery countries are able to escape their poor status. Some of these ways are stabilizing their governments, becoming more industrialized and using natural resources to benefit themselves rather than core countries, and creating a better education system. Developing a banking system that can compete on a global scale is also another way in which periphery countries can help better themselves in the global market.\n\nOne main way in which a periphery country can rise to semi-periphery or core status is through the stabilization of its government. A country with a dictatorship type government is much easier to exploit and corrupt than one with a well organized, elected government and core countries use this to their advantage. Political unrest is usually a cause for military action from the core countries in order to protect their interests and keep a cooperative dictator or government in power. Once the citizens of these countries become exploited enough, they can stage a coup in order to overthrow their government and put someone who they feel will help the country into power. If this is done successfully and the new leader is stays true to his/her word, the country can take the next necessary step in rising from periphery status and that is to start to industrialize.\n\nSome Neo-Marxists believe that it would actually be best for periphery countries to cut all economic and political ties with the core countries. This would, in their opinion, allow the periphery countries to develop and industrialize at their own pace instead of being exploited and driven by core countries demands. Doing this would allow these countries to spend their money on industrializing and bettering themselves, rather than importing goods from core countries. It also would allow these countries to become more independent from the core countries, causing them to move to semi-peripheral status.\n\nMost periphery countries rely almost entirely on agriculture and other natural resources such as oil, coal, and diamonds in order to gain some sort of profit, but this also keeps them from growing economically. In order for them to grow they must industrialize in order to produce finished goods for exportation around the world, instead of allowing the core countries to profit from their natural resources. Industrializing and adapting newer technology is one of the major ways in which periphery countries can begin to raise their standard of living and help increase the wealth of their citizens. Becoming industrialized also will help to force trade to come to their cities, if they can produce goods at competitive prices, allowing them to reach out to the global market and take hold. Once a periphery country can industrialize, and use its own resources to its own benefit, it will begin to enter semi-periphery status.\n\nIn order for a periphery country to industrialize, it must first undergo proto-industrialization. In this stage, a market-based economy begins to form, normally in rural areas, using agricultural products. Proto-industrialization also helps to organize the rural market in these country and allows for them to become more capitalistic. Finally, once these countries develop this style of economy, they can begin to build factories and machines.\n\nOne of the final steps for a periphery country to rise to semi-periphery or core status is to educate its citizens. Raising the literacy rate allows ideas to spread more quickly through a country and also allows people to better communicate with themselves and the rest of the world. Also once universities are developed a country can begin to research new technology. Researching new technology can help a country to better compete in a global market by becoming more efficient or selling new technology and industrial techniques. If education and industry is allowed to become developed enough it is entirely possible for a periphery country to rise to core country status and become a leader in the global market. Another way in which periphery countries better their education system is by spending money to send university level students and staff abroad to places such as the U.S. and Europe to receive better education.\n\nOnce the people in these countries have become educated enough and they realize their place in the world economy, they can also demand help from the core countries. Although unlikely, due to the fact that the core countries rely on the exploitation of the periphery, there have been pushes for core countries to help better the periphery countries. Some of the ideas suggested are to help aid the periphery countries in developing by exploiting them less, help the periphery countries lose some of their debt and raise the prices on goods coming from these countries to allow them to be more profitable. These policies are obviously not beneficial to the core countries and is mostly why they have never been adapted successfully but this is another way in which the periphery could rise to a higher status.\n\nDuring the early 20th century the economy of the Russian Empire was a primarily agrarian country with isolated pockets of heavy industries. The Empire fell in 1917; the core of its industrial workers shrank from 3.6 million in 1917 to 1.5 million in 1920. After the end of the Russian Civil War the Soviet Union was industrialized under the rule of Joseph Stalin. Industrialization peaked in 1929-1932 in a rapid campaign described as \"a revolution from above\". Former personal private farms were collectivized in the early 1930s and gradually supplied with tractors and other machinery. Mechanization of farm labor, among other factors, contributed to freeing up workers for the newly built factories. In 1928-1932 alone at least ten million peasants migrated to the cities, causing \"an unprecedented demographic upheaval\". Industrialization allowed the country to trade in the global trade market. By the 1950s and 60s, only about 30 years after it began to industrialize, the Soviet Union was considered by most scholars a core country along with the United States.\n\nOnce a periphery country rises up to core countries status it will be more dependent on other periphery countries for natural resources. They may also start to exploit other periphery countries to continue to better themselves. One of the biggest impacts of this rise of status is the effects it has on the people of these countries. Health care is one of the first major improvements these countries will see, people will no longer die en masse from diseases such as malaria and will be better treated for non-communicable diseases.\nEducation is also another way in which the citizens will benefit. As a country becomes richer, it is able to build more schools and better fund the schools already built. This was seen in Russia after the October Revolution. A better educated public leads to a more efficient workforce, and can also lead the country to technological breakthroughs in industry and manufacturing. These countries will also experience much less severe famine now that they are able to trade successfully on a global scale.\n\nPeriphery countries as listed in the appendix of \"Trade Globalization since 1795: waves of integration in the world-system\" that appeared in the American Sociological Review (Dunn, Kawana, Brewer (2000)).\nAnd this is the periphery listing according to Babones (2005), who notes that this list is composed of countries that \"have been consistently classified into a single one of the three zones [core, semi-periphery or periphery] of the world economy over the entire 28-year study period\".\n\n"}
{"id": "28072640", "url": "https://en.wikipedia.org/wiki?curid=28072640", "title": "Political globalization", "text": "Political globalization\n\nPolitical globalization refers to the growth of the worldwide political system, both in size and complexity. That system includes national governments, their governmental and intergovernmental organizations as well as government-independent elements of global civil society such as international non-governmental organizations and social movement organizations. One of the key aspects of the political globalization is the declining importance of the nation-state and the rise of other actors on the political scene. The creation and existence of the United Nations has been called one of the classic examples of political globalization.\n\nPolitical globalization is one of the three main dimensions of globalization commonly found in academic literature, with the two other being economic globalization and cultural globalization.\n\nWilliam R. Thompson has defined it as \"the expansion of a global political system, and its institutions, in which inter-regional transactions (including, but certainly not limited to trade) are managed\". Valentine M. Moghadam defined it as \"an increasing trend toward multilateralism (in which the United Nations plays a key role), toward an emerging 'transnational state apparatus,' and toward the emergence of national and international nongovernmental organizations that act as watchdogs over governments and have increased their activities and influence\". Manfred B. Steger in turn wrote that it \"refers to the intensification and expansion of political interrelations across the globe\". The longer definition by Colin Crouch goes as follows: \"Political globalization refers to the growing power of institutions of global governance such as the World Bank, the International Monetary Fund (IMF) and the World Trade Organization (WTO). But it also refers to the spread and influence of international non-governmental organizations, social movement organizations and transnational advocacy networks operating across borders and constituting a kind of global civil society.\" Finally, Gerard Delanty and Chris Rumford define it as \"a tension between three processes which interact to produce the complex field of global politics: global geopolitics, global normative culture and polycentric networks.\"\n\nSalvatore Babones discussing sources used by scholars for studying political globalizations noted the usefulness of Europa World Year Book for data on diplomatic relationships between countries, publications of International Institute for Strategic Studies such as \"The Military Balance\" for matters of military, and US government publication \"Patterns of Global Terrorism\" for matters of terrorism.\n\nPolitical globalization is measured by aggregating and weighting data on the number of embassies and high commissioners in a country, the number of the country's membership in international organization, its participation in the UN peacekeeping missions, and the number of international treaties signed by said country. This measure has been used by Axel Dreher, Noel Gaston, Pim Martens Jeffrey Haynes and is available from the KOF institute at ETH Zurich\n\nLike globalization itself, political globalization has several dimensions and lends itself to a number of interpretations. It has been discussed in the context of new emancipatory possibilities, as well as in the context of loss of autonomy and fragmentation of the social world. Political globalization can be seen in changes such as democratization of the world, creation of the global civil society, and moving beyond the centrality of the nation-state, particularly as the sole actor in the field of politics. Some of the questions central to the discussion of the political globalization are related to the future of the nation-state, whether its importance is diminishing and what are the causes for those changes; and understanding the emergence of the concept of global governance. The creation and existence of the United Nations has been called one of the classic examples of political globalization. Political actions by non-governmental organizations and social movements, concerned about various topics such as environmental protection, is another example.\n\nDavid Held has proposed that continuing political globalization may lead to the creation of a world government-like cosmopolitan democracy, though this vision has also been criticized as too idealistic.\n\nThere is a heated debate over Political Globalization and Nation State. The question arises whether or not political globalization signifies the decline of the nation-state. Hyper globalists argue that globalization has engulfed today's world in such a way that state boundaries are beginning to lose significance. However, skeptics disregard this as naiveté, believing that the nation-state remains the supreme actor in international relations. \n\n\n\n"}
{"id": "18994022", "url": "https://en.wikipedia.org/wiki?curid=18994022", "title": "Prehistory", "text": "Prehistory\n\nHuman prehistory is the period between the use of the first stone tools 3.3 million years ago by hominins and the invention of writing systems. The earliest writing systems appeared 5,300 years ago, but it took thousands of years for writing to be widely adopted and it was not used in some human cultures until the 19th century or even until present. The end of prehistory therefore came at very different dates in different places, and the term is less often used in discussing societies where prehistory ended relatively recently.\n\nSumer in Mesopotamia, the Indus valley civilization and ancient Egypt were the first civilizations to develop their own scripts, and to keep historical records; this took place already during the early Bronze Age. Neighboring civilizations were the first to follow. Most other civilizations reached the end of prehistory during the Iron Age. The three-age system of division of prehistory into the Stone Age, followed by the Bronze Age and Iron Age, remains in use for much of Eurasia and North Africa, but is not generally used in those parts of the world where the working of hard metals arrived abruptly with contact with Eurasian cultures, such as the Americas, Oceania, Australasia and much of Sub-Saharan Africa. These areas also, with some exceptions in Pre-Columbian civilizations in the Americas, did not develop complex writing systems before the arrival of Eurasians, and their prehistory reaches into relatively recent periods; for example 1788 is usually taken as the end of the prehistory of Australia. \n\nThe period when a culture is written about by others, but has not developed its own writing is often known as the protohistory of the culture. By definition, there are no written records from human prehistory, so dating of prehistoric materials is crucial. Clear techniques for dating were not well-developed until the 19th century.\n\nThis article is concerned with human prehistory, the time since behaviorally and anatomically modern humans first appeared until the beginning of recorded history. Earlier periods are also called \"prehistoric\"; there are separate articles for the overall history of the Earth and the history of life before humans.\n\n\nThe notion of \"prehistory\" began to surface during the Enlightenment in the work of antiquarians who used the word 'primitive' to describe societies that existed before written records. The first use of the word prehistory in English, however, occurred in the \"Foreign Quarterly Review\" in 1836.\n\nThe use of the geologic time scale for pre-human time periods, and of the three-age system for human prehistory, is a system that emerged during the late nineteenth century in the work of British, German and Scandinavian archeologists, antiquarians and anthropologists.\n\nThe main source for prehistory is archaeology, but some scholars are beginning to make more use of evidence from the natural and social sciences. This view has been articulated by advocates of deep history.\n\nThe primary researchers into human prehistory are archaeologists and physical anthropologists who use excavation, geologic and geographic surveys, and other scientific analysis to reveal and interpret the nature and behavior of pre-literate and non-literate peoples. Human population geneticists and historical linguists are also providing valuable insight for these questions. Cultural anthropologists help provide context for societal interactions, by which objects of human origin pass among people, allowing an analysis of any article that arises in a human prehistoric context. Therefore, data about prehistory is provided by a wide variety of natural and social sciences, such as paleontology, biology, archaeology, palynology, geology, archaeoastronomy, comparative linguistics, anthropology, molecular genetics and many others.\n\nHuman prehistory differs from history not only in terms of its chronology but in the way it deals with the activities of archaeological cultures rather than named nations or individuals. Restricted to material processes, remains and artifacts rather than written records, prehistory is anonymous. Because of this, reference terms that prehistorians use, such as Neanderthal or Iron Age are modern labels with definitions sometimes subject to debate.\n\nThe concept of a \"Stone Age\" is found useful in the archaeology of most of the world, though in the archaeology of the Americas it is called by different names and begins with a Lithic stage, or sometimes Paleo-Indian. The sub-divisions described below are used for Eurasia, and not consistently across the whole area. \n\n\"Palaeolithic\" means \"Old Stone Age\", and begins with the first use of stone tools. The Paleolithic is the earliest period of the Stone Age.\n\nThe early part of the Palaeolithic is called the Lower Palaeolithic, which predates \"Homo sapiens\", beginning with \"Homo habilis\" (and related species) and with the earliest stone tools, dated to around 2.5 million years ago. Evidence of control of fire by early humans during the Lower Palaeolithic Era is uncertain and has at best limited scholarly support. The most widely accepted claim is that \"H. erectus\" or \"H. ergaster\" made fires between 790,000 and 690,000 BP (before the present period) in a site at Bnot Ya'akov Bridge, Israel. The use of fire enabled early humans to cook food, provide warmth, and have a light source at night.\n\nEarly \"Homo sapiens\" originated some 200,000 years ago, ushering in the Middle Palaeolithic. Anatomic changes indicating modern language capacity also arise during the Middle Palaeolithic. During the Middle Palaeolithic Era, there is the first definitive evidence of human use of fire. Sites in Zambia have charred bone and wood that have been dated to 61,000 B.P. The systematic burial of the dead, music, early art, and the use of increasingly sophisticated multi-part tools are highlights of the Middle Paleolithic.\n\nThroughout the Palaeolithic, humans generally lived as nomadic hunter-gatherers. Hunter-gatherer societies tended to be very small and egalitarian, though hunter-gatherer societies with abundant resources or advanced food-storage techniques sometimes developed sedentary lifestyles with complex social structures such as chiefdoms, and social stratification. Long-distance contacts may have been established, as in the case of Indigenous Australian \"highways\" known as songlines.\n\nThe \"Mesolithic\", or \"Middle Stone Age\" (from the Greek \"mesos\", \"middle\", and \"lithos\", \"stone\") was a period in the development of human technology between the Palaeolithic and Neolithic periods of the Stone Age.\n\nThe Mesolithic period began at the end of the Pleistocene epoch, some 10,000 BP, and ended with the introduction of agriculture, the date of which varied by geographic region. In some areas, such as the Near East, agriculture was already underway by the end of the Pleistocene, and there the Mesolithic is short and poorly defined. In areas with limited glacial impact, the term \"Epipalaeolithic\" is sometimes preferred.\n\nRegions that experienced greater environmental effects as the last ice age ended have a much more evident Mesolithic era, lasting millennia. In Northern Europe, societies were able to live well on rich food supplies from the marshlands fostered by the warmer climate. Such conditions produced distinctive human behaviours that are preserved in the material record, such as the Maglemosian and Azilian cultures. These conditions also delayed the coming of the Neolithic until as late as 4000 BCE (6,000 BP) in northern Europe.\n\nRemains from this period are few and far between, often limited to middens. In forested areas, the first signs of deforestation have been found, although this would only begin in earnest during the Neolithic, when more space was needed for agriculture.\n\nThe Mesolithic is characterized in most areas by small composite flint tools — microliths and microburins. Fishing tackle, stone adzes and wooden objects, e.g. canoes and bows, have been found at some sites. These technologies first occur in Africa, associated with the Azilian cultures, before spreading to Europe through the Ibero-Maurusian culture of Northern Africa and the Kebaran culture of the Levant. Independent discovery is not always ruled out.\n\n\"Neolithic\" means \"New Stone Age.\" Although there were several species of human beings during the Paleolithic, by the Neolithic only \"Homo sapiens sapiens\" remained. (\"Homo floresiensis\" may have survived right up to the very dawn of the Neolithic, about 12,200 years ago.) This was a period of primitive technological and social development. It began about 10,200 BCE in some parts of the Middle East, and later in other parts of the world and ended between 4,500 and 2,000 BCE. The Neolithic is a progression of behavioral and cultural characteristics and changes, including the use of wild and domestic crops and of domesticated animals.\n\nEarly Neolithic farming was limited to a narrow range of plants, both wild and domesticated, which included einkorn wheat, millet and spelt, and the keeping of dogs, sheep and goats. By about 6,900–6,400 BCE, it included domesticated cattle and pigs, the establishment of permanently or seasonally inhabited settlements, and the use of pottery. The Neolithic period saw the development of early villages, agriculture, animal domestication, tools and the onset of the earliest recorded incidents of warfare. The Neolithic era commenced with the beginning of farming, which produced the \"Neolithic Revolution\". It ended when metal tools became widespread (in the Copper Age or Bronze Age; or, in some geographical regions, in the Iron Age).The term \"Neolithic\" is commonly used in the Old World, as its application to cultures in the Americas and Oceania that did not fully develop metal-working technology raises problems.\n\nSettlements became more permanent with some having circular houses with single rooms made of mudbrick. Settlements might have a surrounding stone wall to keep domesticated animals in and protect the inhabitants from other tribes. Later settlements have rectangular mud-brick houses where the family lived together in single or multiple rooms. Burial findings suggest an ancestor cult where people preserved skulls of the dead. The Vinča culture may have created the earliest system of writing. The megalithic temple complexes of Ġgantija are notable for their gigantic structures. Although some late Eurasian Neolithic societies formed complex stratified chiefdoms or even states, states evolved in Eurasia only with the rise of metallurgy, and most Neolithic societies on the whole were relatively simple and egalitarian. Most clothing appears to have been made of animal skins, as indicated by finds of large numbers of bone and antler pins which are ideal for fastening leather. Wool cloth and linen might have become available during the later Neolithic, as suggested by finds of perforated stones that (depending on size) may have served as spindle whorls or loom weights.\n\nIn Old World archaeology, the \"Chalcolithic\", \"Eneolithic\" or \"Copper Age\" refers to a transitional period where early copper metallurgy appeared alongside the widespread use of stone tools. During this period, some weapons and tools were made of copper. This period was still largely Neolithic in character. It is a phase of the Bronze Age before it was discovered that adding tin to copper formed the harder bronze. The Copper Age was originally defined as a transition between the Neolithic and the Bronze Age. However, because it is characterized by the use of metals, the Copper Age is considered a part of the Bronze Age rather than the Stone Age.\n\nAn archaeological site in Serbia contains the oldest securely dated evidence of copper making at high temperature, from 7,500 years ago. The find in June 2010 extends the known record of copper smelting by about 800 years, and suggests that copper smelting may have been invented in separate parts of Asia and Europe at that time rather than spreading from a single source. The emergence of metallurgy may have occurred first in the Fertile Crescent, where it gave rise to the Bronze Age in the 4th millennium BCE (the traditional view), though finds from the Vinča culture in Europe have now been securely dated to slightly earlier than those of the Fertile Crescent. Timna Valley contains evidence of copper mining 9,000 to 7,000 years ago. The process of transition from Neolithic to Chalcolithic in the Middle East is characterized in archaeological stone tool assemblages by a decline in high quality raw material procurement and use. North Africa and the Nile Valley imported its iron technology from the Near East and followed the Near Eastern course of Bronze Age and Iron Age development. However the Iron Age and Bronze Age occurred simultaneously in much of Africa.\n\nThe Bronze Age is the earliest period in which some civilizations have reached the end of prehistory, by introducing written records. The Bronze Age or parts thereof are thus considered to be part of prehistory only for the regions and civilizations who adopted or developed a system of keeping written records during later periods. The invention of writing coincides in some areas with the early beginnings of the Bronze Age. Soon after the appearance of writing, people started creating texts including written accounts of events and records of administrative matters.\n\nThe term Bronze Age refers to a period in human cultural development when the most advanced metalworking (at least in systematic and widespread use) included techniques for smelting copper and tin from naturally occurring outcroppings of ores, and then combining them to cast bronze. These naturally occurring ores typically included arsenic as a common impurity. Copper/tin ores are rare, as reflected in the fact that there were no tin bronzes in Western Asia before 3000 BCE. The Bronze Age forms part of the three-age system for prehistoric societies. In this system, it follows the Neolithic in some areas of the world.\n\nWhile copper is a common ore, deposits of tin are rare in the Old World, and often had to be traded or carried considerable distances from the few mines, stimulating the creation of extensive trading routes. In many areas as far apart as China and England, the valuable new material was used for weapons but for a long time apparently not available for agricultural tools. Much of it seems to have been hoarded by social elites, and sometimes deposited in extravagant quantities, from Chinese ritual bronzes and Indian copper hoards to European hoards of unused axe-heads.\n\nBy the end of the Bronze Age large states, which are often called empires, had arisen in Egypt, China, Anatolia (the Hittites) and Mesopotamia, all of them literate.\n\nThe Iron Age is not part of prehistory for all civilizations who had introduced written records during the Bronze Age. Most remaining civilizations did so during the Iron Age, often through conquest by the empires, which continued to expand during this period. For example, in most of Europe conquest by the Roman Empire means that the term Iron Age is replaced by \"Roman\", \"Gallo-Roman\" and similar terms after the conquest.\n\nIn archaeology, the Iron Age refers to the advent of ferrous metallurgy. The adoption of iron coincided with other changes in some past cultures, often including more sophisticated agricultural practices, religious beliefs and artistic styles, which makes the archaeological Iron Age coincide with the \"Axial Age\" in the history of philosophy. Although iron ore is common, the metalworking techniques necessary to use iron are very different from those needed for the metal used earlier, and iron was slow-spreading and for long mainly used for weapons, while bronze remained typical for tools, as well as art.\n\nAll dates are approximate and conjectural, obtained through research in the fields of anthropology, archaeology, genetics, geology, or linguistics. They are all subject to revision due to new discoveries or improved calculations. BP stands for \"Before Present (1950).\" BCE stands for Before Common Era\".\n\n\n\n\n\n\n"}
{"id": "49509083", "url": "https://en.wikipedia.org/wiki?curid=49509083", "title": "President's Malaria Initiative", "text": "President's Malaria Initiative\n\nThe President's Malaria Initiative (PMI) is a U.S. Government initiative to control and eliminate malaria, one of the leading global causes of premature death and disability. The initiative was originally launched by U.S. president George W. Bush in 2005, and has been continued by each successive U.S. president.\n\nPMI was originally created with a mission to \"reduce malaria-related mortality by 50 percent across 15 high-burden countries in sub-Saharan Africa\". PMI has since expanded to 24 malaria-endemic countries in sub-Saharan Africa and 3 additional countries in the Greater Mekong Subregion of Southeast Asia, where it seeks to further reduce malaria burden and assist countries in achieving malaria elimination. \n\nPMI works closely with national malaria programs and global partners including the World Health Organization, Roll Back Malaria, and Global Fund. Global malaria efforts, led largely part by PMI, have cut malaria mortality by over 60%, saved nearly 7 million lives, and prevented more than 1 billion malaria cases between 2000 and 2015. PMI currently supports malaria prevention and control for over 500 million at-risk people in Africa.\n\nThe \"U.S. Leadership Against HIV/AIDS, Tuberculosis, and Malaria Act of 2003\" originally authorized the U.S. Government to provide 5 years of malaria funding to bilateral partners and the Global Fund. PMI was subsequently launched by President George W. Bush in 2005. In 2008, PMI was reauthorized for another 5 years of funding by the \"Lantos-Hyde Act,\" which also called for development of a comprehensive U.S. Global Malaria Strategy, the latest version of which is the U.S. Global Malaria Strategy 2015-2020. PMI served as a major component of the Global Health Initiative, a six-year, $63-billion effort proposed by President Obama in May 2009.\n\nThe US Government, including through PMI, is currently the largest international source of financing for malaria. PMI's global budget for FY2017 was $723 million.\nPMI is interagency initiative overseen by the U.S. Global Malaria Coordinator in consultation with an Interagency Advisory Group composed of representatives from USAID, CDC, the Department of State, the Department of Defense, the National Security Council, and Office of Management and Budget. The initiative is led by USAID and implemented together with CDC. In addition to US-based staff at USAID and CDC headquarters, PMI maintains resident advisors from both agencies in each focus country.\n\nPMI currently provides direct support to 24 \"focus\" countries and 3 additional country programs in the Great Mekong Subregion. At the time of its launch in 2005, PMI provided support to just three countries: Angola, Tanzania, and Uganda. Four additional countries (Malawi, Mozambique, Rwanda, and Senegal) were added the following year. By 2007, PMI had added Benin, Ethiopia, Ghana, Kenya, Liberia, Madagascar, Mali, and Zambia—bringing the total number of focus countries to the originally envisioned total of 15 high-burden nations. \n\nIn 2010, PMI added the Democratic Republic of the Congo, Nigeria, Guinea, Zimbabwe, and the Mekong Subregion. \n\nIn 2017, with additional funding from Congress, PMI expanded to 5 more countries: Burkina Faso, Cameroon, Cote d'Ivoire, Niger, and Sierra Leone.\n\nPMI is estimated to have prevented 185 million malaria cases and nearly 1 million deaths between 2005 and 2017. Globally, malaria mortality fell by more than 60% between 2000 and 2015. The presence of a PMI program in a country has also been associated with a significant reduction in all-cause under-5 child mortality.\n\n\n"}
{"id": "41809901", "url": "https://en.wikipedia.org/wiki?curid=41809901", "title": "Privacy and Civil Liberties Oversight Board report on mass surveillance", "text": "Privacy and Civil Liberties Oversight Board report on mass surveillance\n\nThe Privacy and Civil Liberties Oversight Board report on mass surveillance was issued in January 2014 in light of the global surveillance disclosures of 2013, recommending the US end bulk data collection.\n\nThe Privacy and Civil Liberties Oversight Board was first chartered under the Intelligence Reform and Terrorism Prevention Act of 2004. The role of the board is to provide advice and review of whether adequate supervision, guidelines, and oversight exist and to \"continually review\" regulations, policies, procedures, and information sharing practices to ensure privacy and civil liberties considerations are protected. To carry out these roles, the board does not have subpoena power, but is able to request subpoenas subject to the U.S. Attorney General's discretion \"to protect sensitive law enforcement or counterterrorism information or ongoing operations.\" The U.S. Director of National Intelligence also has the power to override requests \"to protect the national security interests of the United States\"\n\nA report by former members of the 9/11 commission in December 2005 noted there was \"little urgency\" in creation of the board, whose first meeting was in 2006. It was initially composed of a chair, vice chair, and three other members. As these members served at the pleasure of the President, \"Critics... maintained that the board appeared to be a presidential appendage, devoid of the capability to exercise independent judgment and assessment or to provide impartial findings and recommendations\", according to the Congressional Research Service. Subsequently member Lanny Davis resigned in protest over the board's lack of independence, citing \"extensive redlining by Administration officials of the board's first report to Congress\" that was accepted by the other members. The board was then reconstituted under the Implementing Recommendations of the 9/11 Commission Act of 2007 (H.R. 1), beginning in January 2008, as an independent agency with appointments subject to Senate confirmation. Four members of the board were nominated by President Barack Obama in 2011, and confirmed by the Senate in August 2012. Board chairman David Medine was finally confirmed in May 2013 in the wake of the Snowden disclosures in a party-line vote with 53 Democrats supporting and 45 Republicans opposing.\n\nThe Board's report follows a series of highly publicized leaks about the operations of the global surveillance program conducted by the National Security Agency in the United States working with a number of other countries (see Five Eyes). While the program's nominal focus was on foreign nationals, the disclosures also revealed the large-scale surveillance of communications by United States citizens. These leaks were largely the work of Edward Snowden, a Booz Allen Hamilton employee with access to a wide range of top secret documents. Publications of the documents by \"The Washington Post\" and \"The Guardian\" began in June 2013.\n\nOn January 23, 2014, the board released its report, recommending the US end bulk data collection. Instead federal agencies would be able to obtain phone and other records under court orders in cases containing an individualized suspicion of wrongdoing. But there would be no storehouse, private or public, of telephone data beyond what the phone companies keep in the course of their normal business activities. The report concluded that the program \"lacks a viable legal foundation\". It concluded \"we see little evidence that the unique capabilities provided by the NSA's bulk collection of telephone records actually have yielded material counterterrorism results that could not have been achieved without the NSA's Section 215 program.\" The report concluded: “Cessation of the program would eliminate the privacy and civil liberties concerns associated with bulk collection without unduly hampering the government’s efforts, while ensuring that any governmental requests for telephone calling records are tailored to the needs of specific investigations.”\n\nThe report called for a \"Special Advocate\" to be involved in some cases before the FISA court judge. The Board also contained the recommendation to release future and past FISC decisions “that involve novel interpretations of FISA or other significant questions of law, technology or compliance.”\n\nThe report also recommended against an alternate proposal, which President Barack Obama had ordered Attorney General Eric Holder to formulate within 60 days, which would force third parties including telephone companies to conduct data retention. The panel's majority spokesman said this was due to cost and legal exposure for the companies involved, and that it was not an \"easy out\".\n\nSen. Ron Wyden, a senior member of the intelligence committee and long-time critic of the programs, released a statement in reaction to the report. Said Wyden: \"The privacy board's findings closely mirror many of the criticisms made by surveillance reform advocates. The bulk collection program was built on a murky legal foundation that raises many constitutional questions and has been proven to be an ineffective tool for collecting unique intelligence information. Moreover, as the board wrote in its report, a program where the government collects the telephone records of millions of law-abiding Americans 'fundamentally shifts the balance of power between the state and its citizens.' The board goes on to say that with the government's 'powers of compulsion and criminal prosecution,' collection of data on its own citizens 'poses unique threats to privacy,' and is expected to have a 'chilling effect on the free exercise of speech and association.'\"\n\nAsked by Patrick Mächler to what extent the report will have an impact, Edward Snowden stated: I don’t see how Congress could ignore it, as it makes it clear there is no reason at all to maintain the 215 program. Let me quote from the official report:\n\n“Cessation of the program would eliminate the privacy and civil liberties concerns associated with bulk collection without unduly hampering the government’s efforts, while ensuring that any governmental requests for telephone calling records are tailored to the needs of specific investigations.”\n\n\n"}
{"id": "161657", "url": "https://en.wikipedia.org/wiki?curid=161657", "title": "Soft power", "text": "Soft power\n\nSoft power is the ability to attract and co-opt, rather than coerce (hard power). Soft power is the ability to shape the preferences of others through appeal and attraction. A defining feature of soft power is that it is non-coercive; the currency of soft power is culture, political values, and foreign policies. Recently, the term has also been used in changing and influencing social and public opinion through relatively less transparent channels and lobbying through powerful political and non-political organizations. In 2012, Joseph Nye of Harvard University explained that with soft power, \"the best propaganda is not propaganda\", further explaining that during the Information Age, \"credibility is the scarcest resource.\"\n\nNye coined the term in a 1990 book, \"Bound to Lead: The Changing Nature of American Power\". In this book, he wrote: “when one country gets other countries to want what it wants-might be called co-optive or soft power in contrast with the hard or command power of ordering others to do what it wants.” He further developed the concept in his 2004 book, \"Soft Power: The Means to Success in World Politics\". The term is now widely used in international affairs by analysts and statesmen. For example, US Secretary of Defense Robert Gates spoke of the need to enhance American soft power by \"a dramatic increase in spending on the civilian instruments of national security – diplomacy, strategic communications, foreign assistance, civic action and economic reconstruction and development.\" In 2011, Xi Jinping was preparing to take power from General Secretary Hu Jintao, the 17th Central Committee of the Chinese Communist Party devoted a whole plenary session to the issue of culture, with the final Communiqué declaring that it was a national goal to \"build our country into a socialist cultural superpower.\" And in 2014, Xi announced, \"We should increase China's soft power, give a good Chinese narrative, and better communicate China's messages to the world.\"\n\nAccording to the Soft Power 30, an annual index published by Portland Communications and the USC Center on Public Diplomacy for 2018, the United Kingdom is the leading sovereign state in soft power. Other leading countries in soft power include France, Germany, the United States, Canada, Japan, Switzerland, Australia, Sweden, and the Netherlands. According to the 2016/17 \"Monocle\" Soft Power Survey, ranks the United States as the leading country in soft power. The Elcano Global Presence Report scores the European Union highest for soft presence when considered as a whole, and ranks the United States first among sovereign states.\n\nJoseph Nye introduced the concept of \"soft power\" in the late 1980s. For Nye, power is the ability to influence the behavior of others to get the outcomes you want. There are several ways one can achieve this: you can coerce them with threats; you can induce them with payments; or you can attract and co-opt them to want what you want. This soft power – getting others to want the outcomes you want – co-opts people rather than coerces them.\n\nIt can be contrasted with 'hard power', which is the use of coercion and payment. Soft power can be wielded not just by states but also by all actors in international politics, such as NGOs or international institutions. It is also considered the \"second face of power\" that indirectly allows you to obtain the outcomes you want. A country's soft power, according to Nye, rests on three resources: \"its culture (in places where it is attractive to others), its political values (when it lives up to them at home and abroad), and its foreign policies (when others see them as legitimate and having moral authority).\"\n\nSoft power resources are the assets that produce attraction which often leads to acquiescence. Nye asserts that, \"Seduction is always more effective than coercion, and many values like democracy, human rights, and individual opportunities are deeply seductive.\" Angelo Codevilla observed that an often overlooked essential aspect of soft power is that different parts of populations are attracted or repelled by different things, ideas, images, or prospects. Soft power is hampered when policies, culture, or values repel others instead of attracting them.\n\nIn his book, Nye argues that soft power is a more difficult instrument for governments to wield than hard power for two reasons: many of its critical resources are outside the control of governments, and soft power tends to \"work indirectly by shaping the environment for policy, and sometimes takes years to produce the desired outcomes.\" The book identifies three broad categories of soft power: \"culture\", \"political values\", and \"policies.\"\n\nIn \"The Future of Power\" (2011), Nye reiterates that soft power is a descriptive, rather than a normative, concept. Therefore, soft power can be wielded for nefarious purposes. \"Hitler, Stalin, and Mao all possessed a great deal of soft power in the eyes of their acolytes, but that did not make it good. It is not necessarily better to twist minds than to twist arms.\" Nye also claims that soft power does not contradict the international relations theory of realism. \"Soft power is not a form of idealism or liberalism. It is simply a form of power, one way of getting desired outcomes.\"\n\nSoft power has been criticized as being ineffective by authors such as Niall Ferguson in the preface to \"Colossus\". Neorealist and other rationalist and neorationalist authors (with the exception of Stephen Walt) dismiss soft power out of hand as they assert that actors in international relations respond to only two types of incentives: economic incentives and force.\n\nAs a concept, it can be difficult to distinguish between soft power from hard power. For example, Janice Bially Mattern argues that George W. Bush's use of the phrase \"you are either with us or with the terrorists\" was in fact an exercise of hard power. Though military and economic force was not used to pressure other states to join its coalition, a kind of force – representational force – was used. This kind of force threatens the identity of its partners, forcing them to comply or risk being labelled as evil. This being the case, soft power is therefore not so soft. However, rationalist authors would merely see this as an 'implied threat', and that direct economic or military sanctions would likely follow from being 'against us'.\n\nThe first attempt to measure soft power through a composite index was created and published by the Institute for Government and the media company \"Monocle\" in 2010. The IfG-Monocle Soft Power Index combined a range of statistical metrics and subjective panel scores to measure the soft power resources of 26 countries. The metrics were organized according to a framework of five sub-indices including culture, diplomacy, education, business/innovation, and government. The index is said to measure the soft power resources of countries, and does not translate directly into ability influence. \"Monocle\" has published an annual \"Soft Power Survey\" since then. As of 2016/17, the list is calculated using around 50 factors that indicate the use of soft power, including the number of cultural missions (primarily language schools), Olympic medals, the quality of a country's architecture and business brands.\n\n\"The Soft Power 30\", which includes a foreword by Joseph Nye, is a ranking of countries' soft power produced and published by the media company \"Portland\" in 2015. The ranking is based on \"the quality of a country’s political institutions, the extent of their cultural appeal, the strength of their diplomatic network, the global reputation of their higher education system, the attractiveness of their economic model, and a country’s digital engagement with the world.\"\n\nThe \"Elcano Global Presence Report\" scores the European Union first for soft presence when its member states are excluded and the EU is considered as a whole.\n\nSoft power, then, represents the third behavioral way of getting the outcomes you want. Soft power is contrasted with hard power, which has historically been the predominant realist measure of national power, through quantitative metrics such as population size, concrete military assets, or a nation's gross domestic product. But having such resources does not always produce the desired outcomes, as the United States discovered in the Vietnam War. The extent of attraction can be measured by public opinion polls, by elite interviews, and case studies.\n\nNye argues that soft power is more than influence, since influence can also rest on the hard power of threats or payments. And soft power is more than just persuasion or the ability to move people by argument, though that is an important part of it. It is also the ability to attract, and attraction often leads to acquiescence.\n\nIn international relations, soft power is generated only in part by what the government does through its policies and public diplomacy. The generation of soft power is also affected in positive (and negative) ways by a host of non-state actors within and outside the country. Those actors affect both the general public and governing elites in other countries, and create an enabling or disabling environment for government policies.\n\nIn some cases, soft power enhances the probability of other elites adopting policies that allow one to achieve preferred outcomes. In other cases, where being seen as friendly to another country is seen as a local political kiss of death, the decline or absence of soft power will prevent a government from obtaining particular goals. But even in such instances, the interactions of civil societies and non-state actors may help to further general milieu goals such as democracy, liberty, and development. Soft power is not the possession of any one country or actor.\n\nThe success of soft power heavily depends on the actor's reputation within the international community, as well as the flow of information between actors. Thus, soft power is often associated with the rise of globalization and neoliberal international relations theory. Popular culture and mass media are regularly identified as a source of soft power, as is the spread of a national language or a particular set of normative structures; a nation with a large amount of soft power and the good will that engenders it inspire others to acculturate, avoiding the need for expensive hard power expenditures. More particularly, international news was found crucial in shaping the image and reputation of foreign countries. The high prominence of the US in international news, for example, has been linked to its soft power. Positive news coverage was associated with positive international views, while negative news coverage with negative views.\n\nBecause soft power has appeared as an alternative to raw power politics, it is often embraced by ethically-minded scholars and policymakers. But soft power is a descriptive rather than a normative concept. Like any form of power, it can be wielded for good or bad purposes. While soft power can be used with bad intentions and wreak horrible consequences, it differs in terms of means. It is on this dimension that one might construct a normative preference for greater use of soft power.\n\nAcademics have engaged in several debates around soft power. These have included:\n\nOne study finds that a state's soft power has a measurable effect on its exports. Countries that are admired for their positive global influence export more, holding other things constant.\n\nThe Soviet Union competed with the U.S. for influence throughout the Cold War. The Soviets were engaged in a broad campaign to convince the world of the attractiveness of its Communist system. In 1945, the Soviet Union was very effective in attracting many in Europe from its resistance to Hitler, and in colonized areas around the world because of its opposition to European imperialism. The Soviets also employed a substantially large public diplomacy program that included: promoting their high culture, broadcasting, disseminating disinformation about the West, and sponsoring nuclear protests, peace movements, and youth organizations. Despite all of this, the Soviets' closed system and lack of popular culture impeded the ability of the Soviet Union to compete with the U.S. in terms of soft power.\n\nA number of non-democratic governments have attempted to use migration as an instrument of soft power: Egypt under the rule of Gamal Abdel Nasser trained and dispatched thousands of teachers across the Arab world in an effort to spread ideas of anti-colonialism and anti-Zionism. In Cuba, the Fidel Castro regime's medical internationalism programme has dispatched thousands of medical professionals abroad for cultural diplomacy purposes. The Chinese-sponsored Confucius Institutes across the world rely on Chinese teachers in order to strengthen the country's soft power abroad. More recently, Turkey's migration diplomacy involves sponsoring the short-term emigration of imams across Europe and North America. \n\nAfter Pope John Paul II visited Poland in 1979, some political commentators said his visit influenced events against Poland's communist government, the Soviet Union, and ultimately communism, which promoted atheism.\n\nBesides the Pope's visit, American-government broadcasting and propaganda into Soviet-occupied Europe, particularly Poland, contributed to the rise of the Solidarity movement and to the collapse of the Soviet-backed governments there and in the rest of the Warsaw Pact alliance.\n\nThe United States and Europe have consistently been sources of influence and soft power. European culture's art, literature, music, design, fashion, and even food have been global magnets for some time. Europe and the U.S. have often claimed to support human rights and international law throughout the world. Unlike the U.S., Europeans' love of football enhances their soft power globally, whereas the primary sports of the U.S. such as American football and baseball are largely unpopular on the world stage. In 2012, the European Union was awarded the Nobel Peace Prize \"for over six decades [it has] contributed to the advancement of peace and reconciliation, democracy and human rights in Europe.\" The U.S. has the largest diplomatic network in the world, the largest number of foreign journalists based in the country, and is the most popular destination for international students. American films, among other influences, have contributed to the Americanization of other cultures.\n\nAsia and more recently China have been working to use the potential soft power assets that are present in the admiration of their ancient cultures, arts, fashion and cuisine. China is presenting itself as a defender of national sovereignty, which became an issue after the NATO air campaign to oust Colonel Muammar Gaddafi and NATO's support of the rebels in Libya. The Chinese are also competing with the United States to gain influence throughout the South Pacific, however some commentators have said their recent assertiveness in this region has created an appeal for nations in this region to align with the United States thus increasing U.S. soft power in this area.\n\nSoft power extends beyond the operations of government, to the activities of the private sector and to society and culture at large. Soft power has gained more influence because it addresses the underlying dispositions of the people who have increasingly become more active in their governments. This is true even in authoritarian countries where people and institutions are increasingly able to shape the debate.\n\nThe information age has also led to the rise of soft power resources for non-state actors, Primarily, through the use of global media, and to a greater extent the internet using tools such as the World Wide Web, non-state actors have been able to increase their soft power and put pressure on governments that can ultimately affect policy outcomes. Instead of front organizations, non-state actors can create cyber advocacy organizations to recruit members and project their voice on the global stage.\n\nChina's traditional culture has been a source of attraction, building on which it has created several hundred Confucius Institutes around the world to teach its language and culture. The enrollment of foreign students in China has increased from 36,000 a decade before to at least 240,000 in 2010. China's Asian Infrastructure Investment Bank has attracted many western countries to join. In 2017, China had the second largest diplomatic network in the world.\n\nA spring 2014 Global Attitudes survey from Pew Research Center states China receives mostly positive reviews in the sub-Saharan African nations polled, although South Africans are closely divided (45% favorable, 40% unfavorable). China's increasing soft power can be explained by looking at China's economic growth and regarding economic engagement with many African countries. China's expansion of trade and investment on the African continent and the spread of Chinese-led infrastructure projects gives positive impressions of China to people in Africa. China's economic engagement in African countries is considered as much more pragmatic and in consistency with the priorities of many African countries. Moreover, China's increasing role as a global superpower seems appealing and this drives a desire to tie African economies more closely to China's economy.\n\nChina has made a systematic effort to expand and give greater profile to its soft-power policies in Africa ever since the first Forum on China-Africa Cooperation in 2000. The commitments of China's soft power ranges from health, humanitarian assistance to academic, professional and cultural exchange. China's assistance to Africa, however, is not near the U.S. assistance in Africa.\n\nCultural exchange between China and Africa can be a representative example of how China has been spreading its soft power.\nIn 2005, the first Confucius Institute was established in Africa. The institute is funded by the Chinese government and it provides Chinese language and cultural programming to the public. There are 19 institutes today in Africa and China has planned to spend 20 million renminbi for education projects in South Africa, including the teaching of Mandarin in 50 local high schools.\n\nFurthermore, there is an increasing support for cultural visitors programs which gained momentum in 2004 when the African Cultural Visitors Program was established. There is a rising number of African entrepreneurs who choose to move to China and there are also diaspora communities in many Chinese cities that have been found.\n\nOutside of Africa, Chinese soft power extends to countries like Barbados. Barbadian Prime Minister David Thompson expressed admiration for the Chinese economic model and sought to emulate the way Chinese state controlled banks guided development.\n\nFrance has long exerted a great amount of soft power. The country and its culture have for centuries been admired in many parts of the world; so much so that Thomas Jefferson is famously quoted as saying \"Every man has two countries, his own and France.\" The very term \"culture\" comes from France. In 2017, France had the third largest diplomatic network in the world.\n\nFrance was a focal point of the Age of Enlightenment; its attachment to the ideals of liberty, equality, tolerance and reason was notably embodied in the writing and publishing of the \"Encyclopédie\". The French Revolution was one of the most significant events in European and world history. France has since then been instrumental in spreading Republican ideals. The Napoleonic Code, which influenced much of the rest of Europe and beyond, is regarded as one of the most important law document of the modern era.\n\nThe French language has for centuries been an important diplomatic language. For example, French has to be used – on par with English – for all documents issued by the United Nations Treaty Series, ensuring that all UN treaties are equally valid in their English and French versions.\n\nFrance has also followed for decades a very active diplomatic and cultural policy. The \"Alliance Française\", whose aim is to promote the French language and French culture, was created as early as 1883. In \"Monocle\"'s 2015 \"Soft Power 30\" report, France was ranked first in the \"engagement\" criteria, which is intended to measure \"the reach of states’ diplomatic network and their commitment to major challenges like development and the environment.\" \"Monocle\" further noted that \"In terms of influential reach, France is the best networked state in the world and is member of more multi-lateral organisations than any other country.\" Overall, France ranked fourth in that study.\n\nFrance's laïcité, secularism, policy has inspired some countries over time. For instance, France was Atatürk's main role model for Westernization as part of the major reform efforts that he spearheaded when he was President of Turkey.\n\nFrance, and in particular Paris, has long been considered one of the most romantic places to be. France was in 2014 the most visited country in the world.\n\nThe annual soft power rankings by \"Monocle\" magazine and the Institute for Government ranks 30 countries which “best attract favor from other nations through culture, sport, cuisine, design, diplomacy and beyond.”\n\"Monocle\" magazine said: “Merkel may be painted as a stern taskmaster but it seems she has a softer side, or the country she leads does.”\nIt said Germany’s rise as a soft power should not come as a surprise.\n“The country is traditionally excellent at pursuing its ideas, values and aims using diplomatic, cultural and economic tools,\" it said. “By quietly doing the simple things well it is a country that has become a global power and the rest of us can feel comfortable with that.”\nGermans had been understandably wary about depicting a dominant image abroad, the magazine added, but it said that the country’s rise should not make everyone else feel uncomfortable. In 2017, Germany had the eighth largest diplomatic network in the world.\n\nThe famous elements of Italian soft culture are its art, music, fashion, and iconic food. Italy was the birthplace of opera, and for generations the language of opera was Italian, irrespective of the nationality of the composer. Popular tastes in drama in Italy have long favored comedy; the improvisational style known as the \"Commedia dell'arte\" began in Italy in the mid-16th century and is still performed today. Before being exported to France, the famous Ballet dance genre also originated in Italy.\nThe country boasts several world-famous cities. Rome was the ancient capital of the Roman Empire and seat of the Pope of the Catholic Church. Florence was the heart of the Renaissance, a period of great achievements in the arts that ended the Dark Ages. Other important cities include Turin, which used to be the capital of Italy, and is now one of the world's great centers of automobile engineering. Milan is a fashion capital of the World. Venice, with its intricate canal system, attracts tourists from all over the world especially during the Venetian Carnival and the Biennale.\nItaly is home to the greatest number of UNESCO World Heritage Sites (51) to date, and according to one estimate the country is home to half the world's great art treasures. The nation has, overall, an estimated 100,000 monuments of any sort (churches, cathedrals, archaeological sites, houses and statues).\n\nItaly is considered the birthplace of Western civilization and a cultural superpower. In 2017, Italy had the eleventh largest diplomatic network in the world.\n\n\"Cool Japan\" is a concept coined in 2002 as an expression of Japan’s popular culture. The concept has been adopted by the Japanese government as well as trade bodies seeking to exploit the commercial capital of the country’s culture industry. It has been described as a form of soft power, \"the ability to indirectly influence behavior or interests through cultural or ideological means.\"\nIn a 2002 article in the journal \"Foreign Policy\" titled “Japan’s Gross National Cool”, Douglas McGray wrote of Japan “reinventing superpower” as its cultural influence expanded internationally despite the economic and political problems of the “lost decade.” Surveying youth culture and the role of J-pop, manga, anime, fashion, film, consumer electronics, architecture, and cuisine, McGray highlighted Japan’s considerable soft power, posing the question of what message the country might project. He also argued that Japan’s recession may even have boosted its national cool, due to the partial discrediting of erstwhile rigid social hierarchies and big-business career paths. In 2017, Japan had the fifth largest diplomatic network in the world.\n\nRussia has been developing its soft power by investing in various public diplomacy instruments throughout the 2000s but the term was first used in an official document in 2010 as President Medvedev approved an Addendum to the national Foreign Policy Concept. The term was not defined but it was described as related to cultural diplomacy. In 2013, the term appeared in a new version of the Foreign Policy Concept where the soft power was defined as \"a comprehensive toolkit for achieving foreign policy objectives building on civil society potential, information, cultural and other methods and technologies alternative to traditional diplomacy.\" In 2007, Russian President Vladimir Putin was named \"Time\" Person of the Year. In 2013, he was named most powerful person by Forbes magazine. In 2015, Russia led the creation of the Eurasian Economic Union. In 2017, Russia had the fourth largest diplomatic network in the world. In the wake of the poisoning of Sergei and Yulia Skripal in 2018, the BBC reported that \"Its extensive diplomatic network reflects both its imperial history as a great power in the 19th Century, as well as its Cold War posture. It has a multitude of posts in Eastern Europe and former communist allies including China, Vietnam, Cuba and Angola, as well as legacies of the former USSR in Africa and Asia. The size of its network reflects the extent of its undiminished global ambition.\"\n\n\"Hallyu\", also known as the \"Korean Wave\", is a neologism referring to the spread of South Korean culture since the late 1990s. According to a \"Washington Post\" reporter, the spread of South Korean entertainment has led to higher sales of other goods and services such as food, clothing, and Korean language classes. Besides increasing the amount of exports, the \"Korean Wave\" is used by the government as a soft power tool to engage with the masses of young people all over the world, and to try reduce anti-Korean sentiment.\n\nIn 2012, the BBC's country rating poll revealed that public opinion of South Korea has been improving every year since the first rating poll for the country was conducted in 2009. In several countries such as Russia, India, China and France, public opinion of South Korea turned from slightly negative to generally positive. The report cited culture and tradition as among the most important factors contributing to positive perceptions of South Korea. This comes alongside a rapid growth in the total value of cultural exports which rose to US$4.2 billion in 2011.\n\nFirst driven by the spread of Korean dramas televised across East, South and Southeast Asia during its initial stages, the \"Korean Wave\" evolved from a regional development into a global phenomenon due to the proliferation of Korean pop (K-pop) music videos on YouTube. Currently, the spread of the \"Korean Wave\" to other regions of the world is most visibly seen among teenagers and young adults in Latin America, the Middle East, North Africa, and immigrant enclaves of the Western world.\n\nSince the period of \"Pax Britannica\" the United Kingdom has held significant soft power. Today it remains one of the most influential countries in the world, coming first in the 2015 Portland Group, Comres, Facebook report, and the \"Monocle\" survey of global soft power in 2012.\n\nThe UK has strong diplomatic relations with countries around the world, particularly countries in the Commonwealth of Nations and many others in Europe, Asia, the Middle-east, Africa and the United States. Diplomatic missions between Commonwealth countries are known as High Commissions rather than Embassies to indicate the closeness of the relationship. The UK exerts influence on countries within the European Union, and has the seventh largest global network of diplomatic missions as of 2017. Many countries around the world use the British form of democracy and government known as the Westminster system.\n\nThe influence of British culture and sports are widespread, particularly notable during the British Invasion, Cool Britannia, and more recently the Diamond Jubilee and 2012 Summer Olympics. The opening and closing ceremonies celebrated British culture and achievements with the world. London was the first city to host the modern Olympics three times. British media is broadcast internationally, notably the \"BBC World Service\", \"BBC World News\" and \"The Economist\" magazine. British film and literature have international appeal, and British theatre helps make London one of the most visited cities in the world. Schools and universities in Britain are popular destinations for students of other nations.\n\nAlongside the English language, English contract law is the most important and most used contract law in international business. London is the headquarters for four of the world's six largest law firms. The UK and more specifically London is a centre of international finance where foreign participants in financial markets come to deal with one another. It is headquarters for major international corporations, many of which choose to be listed on the London Stock Exchange.\n\nFollowing the poisoning of Sergei and Yulia Skripal in 2018, the UK responded with bilateral and multilateral diplomatic efforts that led to nations around the world expelling one hundred and fifty Russian diplomats, described by CNN as a \"remarkable diplomatic coup for Britain\". British prime minister Theresa May stated in parliament that the coordinated global response was the \"largest collective expulsion of Russian intelligence officers in history.\" Subsequently, Russia attempted to attribute some of the influence to the United States, this was denied by the various states as well as the entirety of the European Union.\n\nThe United States has long had a great deal of soft power. Examples include Franklin D. Roosevelt's four freedoms in Europe at the end of World War II, young people behind the Iron Curtain listening to the government's foreign propaganda arm Radio Free Europe, Chinese students symbolizing their protests in Tiananmen Square by creating a replica of the Statue of Liberty that they called \"Goddess of Democracy\", newly liberated Afghans in 2001 asking for a copy of the Bill of Rights and young Iranians today surreptitiously watching banned American videos and satellite television broadcasts in the privacy of their homes. America's early commitment to religious toleration, for example, was a powerful element of its overall appeal to potential immigrants; and American aid in the reconstruction of Europe after World War II was a propaganda victory to show off the prosperity and the generosity of the people of the United States.\n\nStudies of American broadcasting into the Soviet bloc, and testimonials from Czech President Václav Havel, Polish President Lech Wałęsa, and Russian President Boris Yeltsin support that soft power efforts of the United States and its allies during the Cold War were ultimately successful in creating the favorable conditions that led to the collapse of the Soviet Union.\n\n\n"}
{"id": "266344", "url": "https://en.wikipedia.org/wiki?curid=266344", "title": "Space debris", "text": "Space debris\n\nInitially, the term space debris referred to the natural debris found in the solar system: asteroids, comets, and meteoroids. However, with the 1979 beginning of the NASA Orbital Debris Program, the term also refers to the debris (alt. space waste or space garbage) from the mass of defunct, artificially created objects in space, especially Earth orbit. These include old satellites and spent rocket stages, as well as the fragments from their disintegration and collisions. \n\nAs of December 2016, five satellite collisions have generated space debris. Space debris is also known as \"orbital debris\", \"space junk\", \"space waste\", \"space trash\", \"space litter\" or \"space garbage\".\n, the United States Strategic Command tracked a total of 17,852 artificial objects in orbit above the Earth, including 1,419 operational satellites. However, these are just objects large enough to be tracked. , more than 170 million bits of debris smaller than , about 670,000 pieces of debris 1–10 cm, and around 29,000 larger pieces were estimated to be in orbit around the earth. Collisions with debris have become a hazard to spacecraft; they cause damage akin to sandblasting, especially to solar panels and optics like telescopes or star trackers that cannot be covered with a ballistic Whipple shield (unless it is transparent).\n\nBelow Earth-altitude, pieces of debris are denser than meteoroids; most are dust from solid rocket motors, surface erosion debris like paint flakes, and frozen coolant from RORSAT (nuclear-powered satellites). \nFor comparison, the International Space Station orbits in the range, and the 2009 satellite collision and 2007 antisat test occurred at altitude. The ISS has Whipple shielding; however, known debris with a collision chance over 1/10,000 are avoided by maneuvering the station.\n\nThe Kessler syndrome, a runaway chain reaction of collisions exponentially increasing the amount of debris, has been hypothesized to ensue beyond a critical density. This could affect useful polar-orbiting bands, increases the cost of protection for spacecraft missions and could destroy live satellites. Whether Kessler syndrome is already underway has been debated. The measurement, mitigation, and potential removal of debris are conducted by some participants in the space industry.\n\nThere are estimated to be over 51 million pieces of debris smaller than as of July 2013. There are approximately 670,000 pieces from one to ten cm. The current count of large debris (defined as 10 cm across or larger) is 29,000. The technical measurement cutoff is c. . Over 98 percent of the 1,900 tons of debris in low Earth orbit (as of 2002) was accounted for by about 1,500 objects, each over . Total mass is mostly constant despite addition of many smaller objects, since they reenter the atmosphere sooner. Using a 2008 figure of 8,500 known items, it is estimated at .\n\nIn LEO there are few \"universal orbits\" which keep spacecraft in particular rings (in contrast to GEO, a single widely used orbit). The closest are sun-synchronous orbits that keep a constant angle between the Sun and the orbital plane; they are polar, meaning they cross over the polar regions. LEO satellites orbit in many planes, up to 15 times a day, causing frequent approaches between objects (the density of objects is much higher in LEO).\n\nOrbits are further changed by perturbations (which in LEO include unevenness of the Earth's gravitational field), and collisions can occur from any direction. For these reasons, the Kessler syndrome applies mostly to the LEO region; impacts occur at up to 16 km/s (twice the orbital speed) if head-on – the 2009 satellite collision occurred at 11.7 km/s, creating much spall in the critical size range. These can cross other orbits and lead to a cascade effect. A large-enough collision (e.g. between a space station and a defunct satellite) could make low Earth orbit impassable.\n\nManned missions are mostly at and below, where air drag helps clear zones of fragments. Atmospheric expansion as a result of space weather raises the critical altitude by increasing drag; in the 90s, it was a factor in reduced debris density. Another was fewer launches by Russia; the USSR made most of their launches in the 1970s and 1980s.\n\nAt higher altitudes, where air drag is less significant, orbital decay takes longer. Slight atmospheric drag, lunar perturbations, Earth's gravity perturbations, solar wind and solar radiation pressure can gradually bring debris down to lower altitudes (where it decays), but at very high altitudes this may take millennia. Although high-altitude orbits are less commonly used than LEO and the onset of the problem is slower, the numbers progress toward the critical threshold more quickly.\n\nMany communications satellites are in geostationary orbits (GEO), clustering over specific targets and sharing the same orbital path. Although velocities are low between GEO objects, when a satellite becomes derelict (such as Telstar 401) it assumes a geosynchronous orbit; its orbital inclination increases about .8° and its speed increases about per year. Impact velocity peaks at about . Orbital perturbations cause longitude drift of the inoperable spacecraft and precession of the orbital plane. Close approaches (within 50 meters) are estimated at one per year. The collision debris pose less short-term risk than from an LEO collision, but the satellite would likely become inoperable. Large objects, such as solar-power satellites, are especially vulnerable to collisions.\n\nAlthough the ITU now requires proof a satellite can be moved out of its orbital slot at the end of its lifespan, studies suggest this is insufficient. Since GEO orbit is too distant to accurately measure objects under , the nature of the problem is not well known. Satellites could be moved to empty spots in GEO, requiring less maneuvring and making it easier to predict future motion. Satellites or boosters in other orbits, especially stranded in geostationary transfer orbit, are an additional concern due to their typically high crossing velocity.\n\nDespite efforts to reduce risk, spacecraft collisions have occurred. The European Space Agency telecom satellite Olympus-1 was struck by a meteoroid on 11 August 1993 and eventually moved to a graveyard orbit. On 29 March 2006, the Russian Express-AM11 communications satellite was struck by an unknown object and rendered inoperable; its engineers had enough contact time with the satellite to send it into a graveyard orbit.\n\nIn 1958, the United States launched Vanguard I into a medium Earth orbit (MEO). , it, and the upper stage of its launch rocket, are the oldest surviving man-made space objects still in orbit. In a catalog of known launches until July 2009, the Union of Concerned Scientists listed 902 operational satellites from a known population of 19,000 large objects and about 30,000 objects launched.\n\nAn example of additional dead satellite debris are the remains of the 1970s/80s Soviet RORSAT naval surveillance satellite program. The satellite's BES-5 nuclear reactor were cooled with a coolant loop of sodium-potassium alloy, creating a potential problem when the satellite reached end of life. While many satellites were nominally boosted into medium-altitude graveyard orbits, not all were. Even satellites which had been properly moved to a higher orbit had an eight-percent probability of puncture and coolant release over a 50-year period. The coolant freezes into droplets of solid sodium-potassium alloy, forming additional debris.\n\nThese events continue to occur. For example, in February 2015, the USAF Defense Meteorological Satellite Program Flight 13 (DMSP-F13) exploded on orbit, creating at least 149 debris objects, which were expected to remain in orbit for decades.\n\nAccording to Edward Tufte's book \"Envisioning Information\", space debris includes a glove lost by astronaut Ed White on the first American space-walk (EVA); a camera lost by Michael Collins near Gemini 10; a thermal blanket lost during STS-88; garbage bags jettisoned by Soviet cosmonauts during Mir's 15-year life, a wrench and a toothbrush. Sunita Williams of STS-116 lost a camera during an EVA. During an STS-120 EVA to reinforce a torn solar panel, a pair of pliers was lost, and in an STS-126 EVA, Heidemarie Stefanyshyn-Piper lost a briefcase-sized tool bag.\n\nIn characterizing the problem of space debris, it was learned that much debris was due to rocket upper stages (e.g. the Inertial Upper Stage) which end up in orbit, and break up due to decomposition of unvented unburned fuel. However, a major known impact event involved an (intact) Ariane booster. Although NASA and the United States Air Force now require upper-stage passivation, other launchers do not.\nLower stages, like the Space Shuttle's solid rocket boosters or Apollo program's Saturn IB launch vehicles, do not reach orbit.\n\nOn 11 March 2000 a Chinese Long March 4 CBERS-1 upper stage exploded in orbit, creating a debris cloud.\nA Russian Briz-M booster stage exploded in orbit over South Australia on 19 February 2007. Launched on 28 February 2006 carrying an Arabsat-4A communications satellite, it malfunctioned before it could use up its propellant. Although the explosion was captured on film by astronomers, due to the orbit path the debris cloud has been difficult to measure with radar. By 21 February 2007, over 1,000 fragments were identified. A 14 February 2007 breakup was recorded by Celestrak. Eight breakups occurred in 2006, the most since 1993. Another Briz-M broke up on 16 October 2012 after a failed 6 August Proton-M launch. The amount and size of the debris was unknown. A Long March 7 rocket booster created a fireball visible from portions of Utah, Nevada, Colorado, Idaho and California on the evening of 27 July 2016; its disintegration was widely reported on social media.\n\nA past debris source was the testing of anti-satellite weapons (ASATs) by the U.S. and Soviet Union during the 1960s and 1970s. North American Aerospace Defense Command (NORAD) files only contained data for Soviet tests, and debris from U.S. tests were only identified later. By the time the debris problem was understood, widespread ASAT testing had ended; the U.S. Program 437 was shut down in 1975.\n\nThe U.S. restarted their ASAT programs in the 1980s with the Vought ASM-135 ASAT. A 1985 test destroyed a satellite orbiting at , creating thousands of debris larger than . Due to the altitude, atmospheric drag decayed the orbit of most debris within a decade. A \"de facto\" moratorium followed the test.\n\nChina's government was condemned for the military implications and the amount of debris from the 2007 anti-satellite missile test, the largest single space debris incident in history (creating over 2,300 pieces golf-ball size or larger, over 35,000 or larger, and one million pieces or larger). The target satellite orbited between and , the portion of near-Earth space most densely populated with satellites. Since atmospheric drag is low at that altitude the debris is slow to return to Earth, and in June 2007 NASA's Terra environmental spacecraft maneuvered to avoid impact from the debris.\n\nOn 20 February 2008, the U.S. launched an SM-3 missile from the USS \"Lake Erie\" to destroy a defective U.S. spy satellite thought to be carrying of toxic hydrazine propellant. The event occurred at about , and the resulting debris has a perigee of or lower. The missile was aimed to minimize the amount of debris, which (according to Pentagon Strategic Command chief Kevin Chilton) had decayed by early 2009.\n\nThe vulnerability of satellites to debris and the possibility of attacking LEO satellites to create debris clouds, has triggered speculation that it is possible for countries unable to make a precision attack. An attack on a satellite of 10 tonnes or more would heavily damage the LEO environment.\n\nSpace junk is a threat to active satellites and spaceships. The Earth's orbit may even become impassable as the risk of collision grows too high.\n\nAlthough spacecraft are protected by Whipple shields, solar panels, which are exposed to the Sun, wear from low-mass impacts. These produce a cloud of plasma which is an electrical risk to the panels.\n\nSatellites are believed to have been destroyed by micrometeorites and orbital debris (MMOD). The earliest suspected loss was of Kosmos 1275, which disappeared on 24 July 1981 (a month after launch). Kosmos contained no volatile propellant, therefore, there appeared to be nothing internal to the satellite which could have caused the destructive explosion which took place. However the case has not been proven and another hypothesis forwarded is that the battery exploded. Tracking showed it broke up, into 300 new objects.\n\nMany impacts have been confirmed since. Olympus-1 was struck by a meteoroid on 11 August 1993, and left adrift. On 24 July 1996, the French microsatellite Cerise was hit by fragments of an Ariane-1 H-10 upper-stage booster which exploded in November 1986. On 29 March 2006, the Russian Ekspress AM11 communications satellite was struck by an unknown object and rendered inoperable; its engineers had sufficient time in contact with the spacecraft to send it to a parking orbit out of GEO. On October 13, 2009, Terra suffered a single battery cell failure anomaly and a battery heater control anomaly which were likely the result of an MMOD strike. On March 12, 2010, Aura lost power from one-half of one of its 11 solar panels and this was also attributed to an MMOD strike. On May 22, 2013 GOES-13 was hit by an MMOD which caused it to lose track of the stars that it uses to maintain attitude. It took nearly a month for the spacecraft to return to operation.\n\nThe first major satellite collision occurred on 10 February 2009 at 16:56 UTC. The deactivated Kosmos 2251 and the operational Iridium 33 collided, over northern Siberia. The relative speed of impact was about , or about . Both satellites were destroyed, with accurate estimates of the number of debris unavailable. On 22 January 2013 BLITS (a Russian laser-ranging satellite) was struck by debris suspected to be from the 2007 Chinese anti-satellite missile test, changing its orbit and spin rate.\n\nSatellites frequently have to perform Collision Avoidance Maneuvers and managers have to monitor debris as part of maneuver planning. For example, in January 2017, the European Space Agency planned to alter orbit of one of its $319 million Swarm mission spacecrafts, based on data from the US Joint Space Operations Center, to end the risk of collision from Cosmos-375, an old Russian satellite. Cosmos-375, which was destroyed by Soviet operators when its mission was complete, had previously threatened to impact the International Space Station in 2011.\n\nFrom the early Space Shuttle missions, NASA used NORAD to monitor the Shuttle's orbital path for debris. In the 1980s, this used much of its capacity. The first collision-avoidance maneuver occurred during STS-48 in September 1991, a seven-second thruster burn to avoid debris from Kosmos 955. Similar maneuvers followed on missions 53, 72 and 82.\n\nOne of the first events to publicize the debris problem occurred on \"Challenger\"'s second flight, STS-7. A fleck of paint struck its front window, creating a pit over wide. On STS-59 in 1994, \"Endeavour\"'s front window was pitted about half its depth. Minor debris impacts increased from 1998.\n\nWindow chipping and minor damage to thermal protection system tiles (TPS) was already common by the 1990s. The Shuttle was later flown tail-first to take the debris load mostly on the engines and rear cargo bay (not used in orbit or during descent, and less critical for post-launch operation). When flying to the ISS, the two connected spacecraft were flipped around so the better-armored station shielded the orbiter.\nNASA's study concluded that debris accounted for half of the overall risk to the Shuttle. Executive-level decision to proceed was required if catastrophic impact was likelier than 1 in 200. On a normal (low-orbit) mission to the ISS the risk was c. 1 in 300, but STS-125 (the Hubble repair mission) at was initially calculated at a 1-in-185 risk (due to the 2009 satellite collision). A re-analysis with better debris numbers reduced the estimated risk to 1 in 221, and the mission went ahead.\n\nDebris incidents continued on later Shuttle missions. During STS-115 in 2006 a fragment of circuit board bored a small hole through the radiator panels in \"Atlantis\"' cargo bay. On STS-118 in 2007 debris blew a bullet-like hole through \"Endeavour\"s radiator panel.\n\nImpact wear was notable on Mir, the Soviet space station, since it remained in space for long periods with its original module panels.\n\nAlthough the ISS uses Whipple shielding to protect itself from minor debris, portions (notably its solar panels) cannot be protected easily. In 1989, the ISS panels were predicted to degrade c. 0.23% in four years, and they were overdesigned by 1%. A maneuver is performed if \"there is a greater than one-in-10,000 chance of a debris strike\". , there have been sixteen maneuvers in the fifteen years the ISS had been in orbit.\n\nThe crew sheltered in the Soyuz on three occasions due to late debris-proximity warnings. In addition to the sixteen firings and three Soyuz-capsule shelter orders, one attempted maneuver failed (due to not having the several days' warning necessary to upload the manoeuvre timeline to the station's computer). A March 2009 close call involved debris believed to be a piece of the Kosmos 1275 satellite. In 2013, the ISS did not maneuver to avoid debris, after a record four debris maneuvers the previous year.\n\nAlthough most manned space activity takes place at altitudes below , a Kessler syndrome cascade in that region would rain down into lower altitudes and the decay time scale is such that \"the resulting [low Earth orbit] debris environment is likely to be too hostile for future space use\".\n\nIn a Kessler syndrome, satellite lifetimes would be measured in years or months. New satellites could be launched through the debris field into higher orbits or placed in lower orbits (where decay removes the debris), but the utility of the region between is the reason for its amount of debris.\n\nAlthough most debris burns up in the atmosphere, larger objects can reach the ground intact. According to NASA, an average of one cataloged piece of debris has fallen back to Earth each day for the past 50 years. Despite their size, there has been no significant property damage from the debris.\n\nIn 1969 five sailors on a Japanese ship were injured by space debris. In 1997 an Oklahoma woman, Lottie Williams, was injured when she was hit in the shoulder by a piece of blackened, woven metallic material confirmed as part of the propellant tank of a Delta II rocket which launched a U.S. Air Force satellite the year before.\n\nThe original re-entry plan for Skylab called for the station to remain in space for eight to ten years after its final mission in February 1974. High solar activity expanded the upper atmosphere, resulting in higher-than-expected drag and bringing its orbit closer to Earth than planned. On 11 July 1979 Skylab re-entered the Earth's atmosphere and disintegrated, raining debris along a path over the southern Indian Ocean and Western Australia.\n\nOn 12 January 2001, a Star 48 Payload Assist Module (PAM-D) rocket upper stage re-entered the atmosphere after a \"catastrophic orbital decay\", crashing in the Saudi Arabian desert. It was identified as the upper-stage rocket for NAVSTAR 32, a GPS satellite launched in 1993.\n\nIn the 2003 \"Columbia\" disaster, large parts of the spacecraft reached the ground and entire equipment systems remained intact. More than 83,000 pieces, along with the remains of the six astronauts, were recovered in an area from three to 10 miles around Hemphill in Sabine County, TX. More pieces were found in a line from west Texas to east Louisiana, with the westernmost piece found in Littlefield, TX and the easternmost found southwest of Mora, LA. Although there is significant evidence that debris fell in Nevada, Utah, and New Mexico, debris was only found in Texas, Arkansas and Louisiana. In a rare case of property damage, a foot-long metal bracket smashed through the roof of a dentist office. NASA warned the public to avoid contact with the debris because of the possible presence of hazardous chemicals. 15 years after the failure, people were still sending in pieces with the last,as of February 1, 2018, found in the spring of 2017.\n\nOn 27 March 2007, airborne debris from a Russian spy satellite was seen by the pilot of a LAN Airlines Airbus A340 carrying 270 passengers whilst flying over the Pacific Ocean between Santiago and Auckland. The debris was within of the aircraft.\n\nRadar and optical detectors such as lidar are the main tools for tracking space debris. Although objects under have reduced orbital stability, debris as small as 1 cm can be tracked, however determining orbits to allow re-acquisition is difficult. Most debris remain unobserved. The NASA Orbital Debris Observatory tracked space debris with a liquid mirror transit telescope. FM Radio waves can detect debris, after reflecting off them onto a receiver. Optical tracking may be a useful early-warning system on spacecraft.\n\nThe U.S. Strategic Command keeps a catalog of known orbital objects, using ground-based radar and telescopes, and a space-based telescope (originally to distinguish from hostile missiles). The 2009 edition listed about 19,000 objects. Other data come from the ESA Space Debris Telescope, TIRA, the Goldstone, Haystack, and EISCAT radars and the Cobra Dane phased array radar, to be used in debris-environment models like the ESA Meteoroid and Space Debris Terrestrial Environment Reference (MASTER).\n\nReturned space hardware is a valuable source of information on the directional distribution and composition of the (sub-millimetre) debris flux. The LDEF satellite deployed by mission STS-41-C \"Challenger\" and retrieved by STS-32 \"Columbia\" spent 68 months in orbit to gather debris data. The EURECA satellite, deployed by STS-46 \"Atlantis\" in 1992 and retrieved by STS-57 \"Endeavour\" in 1993, was also used for debris study.\n\nThe solar arrays of Hubble were returned by missions STS-61 \"Endeavour\" and STS-109 \"Columbia\", and the impact craters studied by the ESA to validate its models. Materials returned from Mir were also studied, notably the Mir Environmental Effects Payload (which also tested materials intended for the ISS).\n\nA debris cloud resulting from a single event is studied with scatter plots known as Gabbard diagrams, where the perigee and apogee of fragments are plotted with respect to their orbital period. Gabbard diagrams of the early debris cloud prior to the effects of perturbations, if the data were available, are reconstructed. They often include data on newly observed, as yet uncatalogued fragments. Gabbard diagrams can provide important insights into the features of the fragmentation, the direction and point of impact.\n\nAn average of about one tracked object per day has been dropping out of orbit for the past 50 years, averaging almost three objects per day at solar maximum (due to the heating and expansion of the Earth's atmosphere), but one about every three days at solar minimum, usually 5½ yr later. In addition to natural atmospheric effects, corporations, academics and government agencies have proposed plans and technology to deal with space debris, but , most of these are theoretical, and there is no extant business plan for debris reduction.\n\nA number of scholars have also observed that institutional factors—political, legal, economic and cultural \"rules of the game\"—are the greatest impediment to the cleanup of near-Earth space. There is no commercial incentive, since costs aren't assigned to polluters, but a number of suggestions have been made. However, effects to date are limited. In the US, governmental bodies have been accused of backsliding on previous commitments to limit debris growth, \"let alone tackling the more complex issues of removing orbital debris.\"\n\nUpper stage passivation (e.g. of Delta boosters) by releasing residual propellants reduces debris from orbital explosions; however not all boosters implement this. Although there is no international treaty minimizing space debris, the United Nations Committee on the Peaceful Uses of Outer Space (COPUOS) published voluntary guidelines in 2007. As of 2008, the committee is discussing international \"rules of the road\" to prevent collisions between satellites.\nBy 2013, various legal regimes existed, typically instantiated in the launch licenses that are required for a launch in all spacefaring nations.\n\nThe U.S. has a set of standard practices for civilian (NASA) and military (DoD and USAF) orbital-debris mitigation, as has the European Space Agency. In 2007, the ISO began preparing an international standard for space-debris mitigation. Germany and France have posted bonds to safeguard property from debris damage.\n\nWhen originally proposed in 2015, the OneWeb constellation, initially planned to have ~700 satellites anticipated on orbit after 2018, would only state that they would re-enter the atmosphere within 25 years of retirement.\nBy October 2017, both OneWeb—and also SpaceX, with their large Starlink constellation—had filed documents with the US FCC with more aggressive space debris mitigation plans. Both companies committed to a deorbit plan for post-mission satellites which will explicitly move the satellites into orbits where they will reenter the Earth's atmosphere within approximately one year following end-of-life.\n\nWith a \"one-up, one-down\" launch-license policy for Earth orbits, launchers would rendezvous with, capture and de-orbit a derelict satellite from approximately the same orbital plane. Another possibility is the robotic refueling of satellites. Experiments have been flown by NASA, and SpaceX is developing large-scale on-orbit propellant transfer technology and tanker spacecraft.\n\nAnother approach to debris mitigation is to explicitly design the mission architecture to always leave the rocket second-stage in an elliptical geocentric orbit with a low-perigee, thus ensuring rapid orbital decay and avoiding long-term orbital debris from spent rocket bodies. Such missions require the use of a small kick stage to circularize the orbit, but the kick stage itself may be designed with the excess-propellant capability to be able to self-deorbit.\n\nAlthough the ITU requires geostationary satellites to move to a graveyard orbit at the end of their lives, the selected orbital areas do not sufficiently protect GEO lanes from debris. Rocket stages (or satellites) with enough propellant may make a direct, controlled de-orbit, or if this would require too much propellant, a satellite may be brought to an orbit where atmospheric drag would cause it to eventually de-orbit. This was done with the French Spot-1 satellite, reducing its atmospheric re-entry time from a projected 200 years to about 15 by lowering its altitude from to about .\n\nPassive methods of increasing the orbital decay rate of spacecraft debris have been proposed. Instead of rockets, an electrodynamic tether could be attached to a spacecraft at launch; at the end of its lifetime, the tether would be rolled out to slow the spacecraft. Other proposals include a booster stage with a sail-like attachment and a large, thin, inflatable balloon envelope.\n\nA consensus of speakers at a meeting in Brussels on 30 October 2012 organized by the Secure World Foundation (a U.S. think tank) and the French International Relations Institute reported that removal of the largest debris would be required to prevent the risk to spacecraft becoming unacceptable in the foreseeable future (without any addition to the inventory of dead spacecraft in LEO). Removal costs and legal questions about ownership and the authority to remove defunct satellites have stymied national or international action. Current space law retains ownership of all satellites with their original operators, even debris or spacecraft which are defunct or threaten active missions.\n\nA well-studied solution uses a remotely controlled vehicle to rendezvous with, capture and return debris to a central station.\nOne such system is Space Infrastructure Servicing, a commercially developed refueling depot and service spacecraft for communications satellites in geosynchronous orbit originally scheduled for a 2015 launch. The SIS would be able to \"push dead satellites into graveyard orbits.\" The Advanced Common Evolved Stage family of upper stages is being designed with a high leftover-propellant margin (for derelict capture and de-orbit) and in-space refueling capability for the high delta-v required to de-orbit heavy objects from geosynchronous orbit. A tug-like satellite to drag debris to a safe altitude for it to burn up in the atmosphere has been researched. When debris is identified the satellite creates a difference in potential between the debris and itself, then using its thrusters to move itself and the debris to a safer orbit.\n\nA variation of this approach is for the remotely controlled vehicle to rendezvous with debris, capture it temporarily to attach a smaller de-orbit satellite and drag the debris with a tether to the desired location. The \"mothership\" would then tow the debris-smallsat combination for atmospheric entry or move it to a graveyard orbit. One such system is the proposed Busek ORbital DEbris Remover (ORDER), which would carry over 40 SUL (satellite on umbilical line) de-orbit satellites and propellant sufficient for their removal.\nOn 7 January 2010 Star, Inc. reported that it received a contract from the Space and Naval Warfare Systems Command for a feasibility study of the ElectroDynamic Debris Eliminator (EDDE) propellantless spacecraft for space-debris removal.\nIn February 2012 the Swiss Space Center at École Polytechnique Fédérale de Lausanne announced the Clean Space One project, a nanosatellite demonstration project for matching orbit with a defunct Swiss nanosatellite, capturing it and de-orbiting together. The mission has seen several evolutions to reach a pac-man inspired capture model.\n\nThe laser broom uses a ground-based laser to ablate the front of the debris, producing a rocket-like thrust which slows the object. With continued application, the debris would fall enough to be influenced by atmospheric drag. During the late 1990s, the U.S. Air Force's Project Orion was a laser-broom design. Although a test-bed device was scheduled to launch on a Space Shuttle in 2003, international agreements banning powerful laser testing in orbit limited its use to measurements. The Space Shuttle \"Columbia\" disaster postponed the project and according to Nicholas Johnson, chief scientist and program manager for NASA's Orbital Debris Program Office, \"There are lots of little gotchas in the Orion final report. There's a reason why it's been sitting on the shelf for more than a decade.\"\n\nThe momentum of the laser-beam photons could directly impart a thrust on the debris sufficient to move small debris into new orbits out of the way of working satellites. NASA research in 2011 indicates that firing a laser beam at a piece of space junk could impart an impulse of per second, and keeping the laser on the debris for a few hours per day could alter its course by per day. One drawback is the potential for material degradation; the energy may break up the debris, adding to the problem. A similar proposal places the laser on a satellite in Sun-synchronous orbit, using a pulsed beam to push satellites into lower orbits to accelerate their reentry. A proposal to replace the laser with an Ion Beam Shepherd has been made, and other proposals use a foamy ball of aerogel or a spray of water,\ninflatable balloons,\nelectrodynamic tethers,\nboom electroadhesion,\nand dedicated anti-satellite weapons.\n\nOn 28 February 2014, Japan's Japan Aerospace Exploration Agency (JAXA) launched a test \"space net\" satellite. The launch was an operational test only. In December 2016 the country sent a space junk collector via Kounotori 6 to the ISS by which JAXA scientists experiment to pull junk out of orbit using a tether. The system failed to extend a 700-meter tether from a space station resupply vehicle that was returning to Earth. On 6 February the mission was declared a failure and leading researcher Koichi Inoue told reporters that they \"believe the tether did not get released\".\n\nSince 2012, the European Space Agency has designed a mission to remove large space debris from orbit. The mission, e.Deorbit, is scheduled for launch during 2023 with an objective to remove debris heavier than from LEO. Several capture techniques are being studied, including a net, a harpoon and a combination robot arm and clamping mechanism.\n\nHolger Krag of the European Space Agency states that as of 2017 there is no binding international regulatory framework with no progress occurring at the respective UN body in Vienna.\n\nIn 1946 during the Giacobinid meteor shower, Helmut Landsberg collected several small magnetic particles that were apparently associated with the shower. Fred Whipple was intrigued by this and wrote a paper that demonstrated that particles of this size were too small to maintain their velocity when they encountered the upper atmosphere. Instead, they quickly decelerated and then fell to Earth unmelted. In order to classify these sorts of objects, he coined the term \"micro-meteorite\".\n\nWhipple, in collaboration with Fletcher Watson of the Harvard Observatory, led an effort to build an observatory to directly measure the velocity of the meteors that could be seen. At the time the source of the micro-meteorites was not known. Direct measurements at the new observatory were used to locate the source of the meteors, demonstrating that the bulk of material was left over from comet tails, and that none of it could be shown to have an extra-solar origin. Today it is understood that meteoroids of all sorts are leftover material from the formation of the Solar System, consisting of particles from the interplanetary dust cloud or other objects made up from this material, like comets.\n\nThe early studies were based on optical measurements only. In 1957, Hans Pettersson conducted one of the first direct measurements of the fall of space dust on the Earth, estimating it to be 14,300,000 tons per year. This suggested that the meteoroid flux in space was much higher than the number based on telescope observations. Such a high flux presented a very serious risk to missions deeper in space, specifically the high-orbiting Apollo capsules. To determine whether the direct measurement was accurate, a number of additional studies followed, including the Pegasus satellite program. These showed that the rate of meteors passing into the atmosphere, or flux, was in line with the optical measurements, at around 10,000 to 20,000 tons per year.\n\nWhipple's work pre-dated the space race and it proved useful when space exploration started only a few years later. His studies had demonstrated that the chance of being hit by a meteoroid large enough to destroy a spacecraft was extremely remote. However, a spacecraft would be almost constantly struck by micrometeorites, about the size of dust grains.\n\nWhipple had already developed a solution to this problem in 1946. Originally known as a \"meteor bumper\" and now termed the Whipple shield, this consists of a thin foil film held a short distance away from the spacecraft's body. When a micrometeoroid strikes the foil, it vaporizes into a plasma that quickly spreads. By the time this plasma crosses the gap between the shield and the spacecraft, it is so diffused that it is unable to penetrate the structural material below. The shield allows a spacecraft body to be built to just the thickness needed for structural integrity, while the foil adds little additional weight. Such a spacecraft is lighter than one with panels designed to stop the meteoroids directly.\n\nFor spacecraft that spend the majority of their time in orbit, some variety of the Whipple shield has been almost universal for decades. Later research showed that ceramic fibre woven shields offer better protection to hypervelocity (~7 km/s) particles than aluminium shields of equal weight. Another modern design uses multi-layer flexible fabric, as in NASA's design for its never-flown TransHab expandable space habitation module,\nand the Bigelow Expandable Activity Module, which was launched in April 2016 and attached to the ISS for two years of orbital testing.\n\nTo avoid artificial space debris, many—but not all—research satellites are launched on elliptical orbits with perigees inside Earth's atmosphere so they will destroy themselves. Willy Ley predicted in 1960 that \"In time, a number of such accidentally too-lucky shots will accumulate in space and will have to be removed when the era of manned space flight arrives\". After the launch of Sputnik 1 in 1957, the North American Aerospace Defense Command (NORAD) began compiling a database (the Space Object Catalog) of all known rocket launches and objects reaching orbit: satellites, protective shields and upper- and lower-stage booster rockets. NASA published modified versions of the database in two-line element set, and during the early 1980s the CelesTrak bulletin board system re-published them.\nThe trackers who fed the database were aware of other objects in orbit, many of which were the result of in-orbit explosions. Some were deliberately caused during 1960s anti-satellite weapon (ASAT) testing, and others were the result of rocket stages blowing up in orbit as leftover propellant expanded and ruptured their tanks. To improve tracking, NORAD employee John Gabbard kept a separate database. Studying the explosions, Gabbard developed a technique for predicting the orbital paths of their products, and Gabbard diagrams (or plots) are now widely used. These studies were used to improve the modelling of orbital evolution and decay.\n\nWhen the NORAD database became publicly available during the 1970s, NASA scientist Donald J. Kessler applied the technique developed for the asteroid-belt study to the database of known objects. In 1978 Kessler and Burton Cour-Palais co-authored \"Collision Frequency of Artificial Satellites: The Creation of a Debris Belt\", demonstrating that the process controlling asteroid evolution would cause a similar collision process in LEO in decades rather than billions of years. They concluded that by about 2000, space debris would outpace micrometeoroids as the primary ablative risk to orbiting spacecraft.\n\nAt the time, it was widely thought that drag from the upper atmosphere would de-orbit debris faster than it was created. However, Gabbard was aware that the number and type of objects in space were under-represented in the NORAD data and was familiar with its behaviour. In an interview shortly after the publication of Kessler's paper, Gabbard coined the term \"Kessler syndrome\" to refer to the accumulation of debris; it became widely used after its appearance in a 1982 \"Popular Science\" article, which won the Aviation-Space Writers Association 1982 National Journalism Award.\n\nThe lack of hard data about space debris prompted a series of studies to better characterize the LEO environment. In October 1979, NASA provided Kessler with funding for further studies. Several approaches were used by these studies.\n\nOptical telescopes or short-wavelength radar was used to measure the number and size of space objects, and these measurements demonstrated that the published population count was at least 50% too low. Before this, it was believed that the NORAD database accounted for the majority of large objects in orbit. Some objects (typically, U.S. military spacecraft) were found to be omitted from the NORAD list, and others were not included because they were considered unimportant. The list could not easily account for objects under in size—in particular, debris from exploding rocket stages and several 1960s anti-satellite tests.\n\nReturned spacecraft were microscopically examined for small impacts, and sections of Skylab and the Apollo Command/Service Module which were recovered were found to be pitted. Each study indicated that the debris flux was higher than expected and debris was the primary source of collisions in space. LEO already demonstrated the Kessler syndrome.\n\nIn 1978 Kessler found that 42 percent of cataloged debris was the result of 19 events, primarily explosions of spent rocket stages (especially U.S. Delta rockets). He discovered this by first identifying those launches that were described having a large number of objects associated with a payload, then researching the literature to determine the rockets used in the launch. In 1979, this finding resulted in establishment of the NASA Orbital Debris Program after a briefing to NASA senior management, overturning the previously held belief that most unknown debris was from old ASAT tests, but from US upper stage rocket explosions and could be easily managed by depleting the unused fuel following the payload injection the upper stage Delta rocket. Beginning in 1986, when it was discovered that other international agencies were possibly experiencing the same type of problem, NASA expanded its program to include international agencies, the first being the European Space Agency. A number of other Delta components in orbit (Delta was a workhorse of the U.S. space program) had not yet exploded.\n\nDuring the 1980s, the U.S. Air Force conducted an experimental program to determine what would happen if debris collided with satellites or other debris. The study demonstrated that the process differed from micrometeoroid collisions, with large chunks of debris created which would become collision threats.\n\nIn 1991, Kessler published \"Collisional cascading: The limits of population growth in low Earth orbit\" with the best data then available. Citing the USAF conclusions about creation of debris, he wrote that although almost all debris objects (such as paint flecks) were lightweight, most of its mass was in debris about or heavier. This mass could destroy a spacecraft on impact, creating more debris in the critical-mass area. According to the National Academy of Sciences:\n\nA 1-kg object impacting at 10 km/s, for example, is probably capable of catastrophically breaking up a 1,000-kg spacecraft if it strikes a high-density element in the spacecraft. In such a breakup, numerous fragments larger than 1 kg would be created.\n\nKessler's analysis divided the problem into three parts. With a low-enough density, the addition of debris by impacts is slower than their decay rate and the problem is not significant. Beyond that is a critical density, where additional debris leads to additional collisions. At densities beyond this critical mass production exceeds decay, leading to a cascading chain reaction reducing the orbiting population to small objects (several cm in size) and increasing the hazard of space activity. This chain reaction is known as the Kessler syndrome.\n\nIn an early 2009 historical overview, Kessler summed up the situation:\n\nAggressive space activities without adequate safeguards could significantly shorten the time between collisions and produce an intolerable hazard to future spacecraft. Some of the most environmentally dangerous activities in space include large constellations such as those initially proposed by the Strategic Defense Initiative in the mid-1980s, large structures such as those considered in the late-1970s for building solar power stations in Earth orbit, and anti-satellite warfare using systems tested by the USSR, the U.S., and China over the past 30 years. Such aggressive activities could set up a situation where a single satellite failure could lead to cascading failures of many satellites in a period much shorter than years.\n\nDuring the 1980s, NASA and other U.S. groups attempted to limit the growth of debris. One effective solution was implemented by McDonnell Douglas on the Delta booster, by having the booster move away from its payload and vent any propellant remaining in its tanks. This eliminated the pressure buildup in the tanks which caused them to explode in the past. Other countries were slower to adopt this measure and, due especially to a number of launches by the Soviet Union, the problem grew throughout the decade.\n\nA new battery of studies followed as NASA, NORAD and others attempted to better understand the orbital environment, with each adjusting the number of pieces of debris in the critical-mass zone upward. Although in 1981 (when Schefter's article was published) the number of objects was estimated at 5,000, new detectors in the Ground-based Electro-Optical Deep Space Surveillance system found new objects. By the late 1990s, it was thought that most of the 28,000 launched objects had already decayed and about 8,500 remained in orbit. By 2005 this was adjusted upward to 13,000 objects, and a 2006 study increased the number to 19,000 as a result of an ASAT test and a satellite collision. In 2011, NASA said that 22,000 objects were being tracked.\n\nThe growth in the number of objects as a result of the late-1990s studies sparked debate in the space community on the nature of the problem and the earlier dire warnings. According to Kessler's 1991 derivation and 2001 updates, the LEO environment in the altitude range should be cascading. However, only one major incident has occurred: the 2009 satellite collision between Iridium 33 and Cosmos 2251. The lack of obvious short-term cascading has led to speculation that the original estimates overstated the problem. According to Kessler a cascade would not be obvious until it was well advanced, which might take years.\n\nA 2006 NASA model suggested that if no new launches took place the environment would retain the then-known population until about 2055, when it would increase on its own. Richard Crowther of Britain's Defence Evaluation and Research Agency said in 2002 that he believed the cascade would begin about 2015. The National Academy of Sciences, summarizing the professional view, noted widespread agreement that two bands of LEO space—900 to and —were already past critical density.\n\nIn the 2009 European Air and Space Conference, University of Southampton researcher Hugh Lewis predicted that the threat from space debris would rise 50 percent in the next decade and quadruple in the next 50 years. , more than 13,000 close calls were tracked weekly.\n\nA 2011 report by the U.S. National Research Council warned NASA that the amount of orbiting space debris was at a critical level. According to some computer models, the amount of space debris \"has reached a tipping point, with enough currently in orbit to continually collide and create even more debris, raising the risk of spacecraft failures\". The report called for international regulations limiting debris and research of disposal methods.\n\nThe plot of episode 4 (\"Conflict\") of Gerry Anderson's 1970 TV series \"UFO\" includes routine missions for the disposal of spent satellites by bombing.\n\n\"Salvage 1\" (1979 TV series) deals humorously with a scrap dealer who establish a space junk salvage company.\n\n\"Planetes\" is a manga (1999-2004) and anime series (2003-2004) that gives focus on a team which is responsible for the collection and disposal of space debris. The DVDs for the TV series include interviews with NASA's Orbital Debris Program Office.\n\nIn 2009, Rhett & Link wrote a song called \"Space Junk\" and made an accompanying music video for the TV series \"Brink\". The lyrics refer to two men tasked to clean up debris such as satellites and expended rockets.\n\n\"Gravity\" is a 2013 survival film, directed by Alfonso Cuaron, about a disaster on a space mission caused by Kessler syndrome.\n\n\n\n"}
{"id": "43150808", "url": "https://en.wikipedia.org/wiki?curid=43150808", "title": "Spy Booth", "text": "Spy Booth\n\nSpy Booth was an artwork by Banksy in Cheltenham, England. The piece has been seen as a critique of the global surveillance disclosures of 2013.\n\nIn 2014, Robin Barton and Bankrobber London helped with the preservation of the artwork, and attempted to broker the removal and sale of the piece. However the artwork was painted onto a Grade II listed building - 153-159 Fairview Road - and the council prevented it from being removed, giving it retrospective listed building consent in 2015 and affording it some protection from removal. Despite this, the artwork was either removed or destroyed in August 2016.\n\nThe GCHQ has used the picture on its website as a symbolic image for its \"what we do\" page.\n"}
{"id": "26692723", "url": "https://en.wikipedia.org/wiki?curid=26692723", "title": "Study of global communication", "text": "Study of global communication\n\nGlobal communication is the term used to describe ways to connect, share, relate and mobilize across geographic, political, economic, social and cultural divides. It redefines soft and hard power as well as information power and diplomacy in ways not considered by traditional theories of international relations. \n\nGlobal Communication implies a transfer of knowledge and ideas from centers of power to peripheries and the imposition of a new intercultural hegemony by means of the \"soft power\" of global news and entertainment.\n\nThe study of global communication is an interdisciplinary field that studies the continuous flows of information used in transferring values, opinions, knowledge, culture across boundaries. \n\nWith the end of the twentieth century and the turn of a new millennium, the global arena and the field of international communication were undergoing significant changes. Some authors started to use the term global communication because it goes beyond the bounds of individual states and emphasizes communication between and among peoples across borders and, importantly, the rise of transnational media corporations.\n\nInternational communication traditionally refers to communication between and among nation-states and connotes issues of national sovereignty, control of national information resources, and the supremacy of national governments.\n\nNevertheless, earlier International communication theories have failed to develop models or research agendas that match the reality of the contemporary role of global communication . The old theories only explain part of the global picture and the theories of modernization, dependency, and cultural imperialism have failed to satisfactorily explain global communication.\n\nThe term \"global\", implies a declining role of the state and state sovereignty. As a term, \"international\" has within it notions of bilateral or multilateral decisions. \"Global\" could be seen as an aspiration, also as a fear, of the weakening of the state. In addition, global may imply something more pervasive, more geographically inclusive than international. \n\nThe study of global communication increased dramatically after World War II due to military considerations coupled with their economic and political implications. Earlier attempts at theorizing have failed to develop models or research agendas that match the reality of the contemporary role of global communication .\n\nMore global communication research was written in the decade from 1945–1955; most of the research of the 1950s dealt with propaganda and the cold war. By 1970, global communication research had grown to include a great variety of subjects, especially comparative mass communication systems, communication and national development and propaganda and public opinion.\n\nFrom the point of view of global communication scholars, previous theories of modernization, dependency, and cultural imperialism have failed to satisfactorily explain global communication. The old theories only explain part of the global picture.\n\nThe emergence of global communication technologies may be considered the origin of the field of global communication in the nineteenth century. Numerous technical advances such as the creation of a new major global communication phenomenon, convergence, digital environments and the internet are some of the major engines driving the change from international communication to global communication.\n\nWith the collapse of the Soviet Union, the shadow of Cold War has lifted to reveal shifting political, economic, and cultural alliances and conflicts. The increasing importance of these currents, especially in the cultural sphere, demands a reconsideration of the nature of the international communication field within the rubric of international relations.\n\nThree key players are usually recognized as the founders of the international news agencies. In 1835, Charles-Louis Havas created the world's first news agency; In 1849, Bernhard Wolff started publishing stock market news and daily reports from Paris, London, Amsterdam, and Frankfurt; In 1849, Paul Julius Freiherr von Reuter established his own commercial service, the Reuter agency, and organized a worldwide exchange of news in 1870.\n\nIn 1859, Reuter, Havas and the German Wolff agency reached an agreement to exchange news from all over the world, which was known as the League of Allied Agencies, or the \" Ring Combination\". In 1848, American News Agency Associated Press was founded and was formally admitted into the \"Ring Combination\" in 1887.\n\nThere are some major factors that point to the growing importance of global communication in the world of the twenty-first century:\n\n\n\nTranscultural Political Economy is a concept that is presented in Global Communications by Paula Chakravartty and Yeuzhi Zhao. This concept looks at global communications and media studies in three major areas: global flows of information and culture, decentralizing the conceptual parameters of global information and media studies, and the normative debates in global communications in the context of neoliberalism. Transcultural Political Economy is a multidisciplinary study that focuses on the tensions between political economy and cultural studies. It \"integrate[s] institutional and cultural analyzes and address urgent questions in global communications in the context of economic integration, empire formation, and the tensions associated with adapting new privatized technologies, neoliberalized and globalized institutional structures, and hybrid cultural forms and practices\". Transcultural Political Economy addresses the issues surrounding the practice of neoliberalism and its creation of unequal power structures within the world system. \nGlobalization theory was popularized in the 1990s as a model for understanding global communication. The concept of globalization inspired a number of theories from various schools of thought in communication studies that each emphasize different aspects of globalization. Many globalization theories highlight actors in the business sector as leaders in the processes of global integration. Transnationalizing business is often celebrated as progression toward a more interconnected world. Globalization theories are often associated with theories of modernity. Some scholars view globalization as the social, political, economic, and cultural integration of societies into a capitalist system; Others see globalization as a successor to modernity, while some see it as an iteration of imperialism. Some question the usefulness and legitimacy of globalization theory, arguing that it does not adequately conceptualize current international relations or function as a lens through which to examine everyday events. Many scholars criticize globalization theories as overzealous toward and unrealistic about the extent of global integration. Some scholars criticize social theorists for offering opinions and predictions based on theory with little practical evidence. In contrast, some scholars work to dispute the pessimistic views of globalization theory.\n\nWorld-system theory is a macro-sociological perspective that seeks to explain the dynamics of the \"capitalist world economy\" as a \"total social system\". A world-system is what Wallerstein terms a \"world-economy\", integrated through the market rather than a political centre, in which two or more regions are interdependent with respect to necessities like food, fuel, and two or more polities compete for domination without the emergence of one single centre forever. World-system theory was first articulated by Immanuel Wallerstein. There are three major sources of the world-system theory which conceived by Wallerstein: the Annales school's general methodology, Marx's focus on accumulation process and competitive class struggles and so on, and dependence theory's neo-Marxist explanation of development processes.\n\nReferring to the transnational division of labor, world-system divides the world into core countries, peripheral countries, semi-peripheral countries and external areas. The core countries usually developed a strong central governments, extensive bureaucracies and large mercenary armies, which permit the local bourgeoisie to obtain control over international commerce and extract capital surpluses from the trade for benefits. The peripheral countries often lack strong central governments or been controlled by core countries, they export raw materials and rely on coercive labor practices. Semi-peripheries which served as buffers between the core and the peripheries. They retain limited but declining access to international banking and the production of high-cost high-quality manufactured goods.[3] External areas such as Russia maintain their own economic systems, they want to remain outside the modern world economy.\n\nThe theory of modernization was developed by Daniel Lerner (1958) in the \"Passing of traditional society.\" Lerner's description of \"modernised\" is an individual having the ability to be empathetic and being able to see oneself in another person's situation. This concept has come from the transition of traditional societies to modern societies, where modern societies is distinctively industrial, urban, literate, and participant. This theory looks at development in a linear fashion, concluding that nations need to develop into a modern society to make it a sustainable and flourishing nation. Developing modernized societies include technological advancement, and developing media sectors to help create a participatory culture. \n\"See also:\" Post-structuralism, imperialism, modernity,\n\nPost-colonialism is a theoretical approach to looking at literature that examines the colonizer-colonized experience. It deals with the adaptation of formerly colonized nations and their development in cultural, political, economical aspects. Some Notable theoreticians include: Frantz Fanon, Edward Said, Gayatri Spivak, R Siva Kumar, Dipesh Chakrabarty, Derek Gregory.\n\nCultural imperialism is a mighty civilization exerts culture influence over another. Less economically prominent cultures often import culture from Western countries, which have the economic means to produce a majority of the world's cultural media, mostly via the global transmission of media. The weak civilization adopts the mighty civilization's customs, philosophies, worldviews and general ways of life. The theoretical foundations of the academic study of cultural imperialism mostly come from Michel Foucault's concept of biopower, governmentality and Edward Saïd's concept of Post-colonialism, which theories see cultural imperialism as the cultural legacy of colonialism or forms of Western hegemony. Media effect study which integrated with political-economy traditional is the core argument of cultural imperialism. There are two opposite effects of media study. The negative one is that Western media imposes socio-political conflicts to the developing country and the latter one's resistance to the media effects to preserve their traditional cultural identities. The positive effects are the issues of the process of civilization such as women's right or racial equality with exposing to Western media. Now the term of cultural imperialism usually refers to America's global culture expansion to the rest of world, which include brand name products, video media, fast food and so on.\n\nCommunication for Development (C4D) is a praxis oriented aspect of global communication studies that approaches global development with a focus on action and participation for social change enacted through communication systems. C4D underlines \"voice, citizenship and collective action\" as central values that promote citizen-led development where the visiting party provides guidance rather than direction within the host community. C4D often incorporates bottom-up theories of social change with the aim to create sustainable change which is believed to be more likely to occur if the efforts are planned, implemented, and sustained by community members themselves. Some development workers and academics suggest that a shared definition of communication for development should be clarified, because disagreement within the field can detract from the characteristics that most scholars view as central to current development, including participatory action research (PAR). Many C4D projects revolve around media systems as a central site for social change, which differentiates C4D from other approaches to development. Theories behind C4D highlight that development projects should be contextually situated and that communication technology will affect different types of social change accordingly.\n\nGlobal media studies is a field of media study in a global scope. Media study deals with the content, history and effects of media. Media study often draws on theories and methods from the disciplines of cultural studies, rhetoric, philosophy, communication studies, feminist theory, political economy and sociology. Among these study approaches, political economic analysis is non-ignorable in understanding the current media and communication developments. But the political economic research has become more resilient because of stronger empirical studies, and the potential connections to policy-making and alternative praxis.\n\nEach country has its own distinct media ecosystem. The media of mainland China is state-run, so the political subjects are under the strict regulations set by the government while other areas such as sports, finance, and increasingly lucrative entertainment industry face less regulation from government. Canada has a well-developed media sector, but the mass media is threatened by the direct outcome of American economic and cultural imperialism which hinder the form of Canada's media identity. Many of the media in America are controlled by large for-profit corporations who reap revenues from advertisings, subscriptions and the sale of copyrighted materials. Currently, six corporations (Comcast, The Walt Disney Company, News Corporation, Time Warner, Viacom and CBS Corporation) have controlled roughly 90% of the America media. Such successes come from the policies of the federal government or the tendency to natural monopolies in the industry.\n\nImmanuel Wallerstein's world system theory develops a basic framework to understand global power shifts in the rise of the modern world. Wallerstein proposes four different categories: core, semi-periphery, periphery, and external, in terms of different region's relative position in the world system. The core regions are the ones that benefited the most from the capitalist world economy, such as England and France. The peripheral areas relied on and exported raw materials to the core, such as Eastern Europe and Latin America. The semi-peripheries are either core regions in decline or peripheries attempting to improve their relative position in the world system, such as Portugal and Spain. The external areas managed to remain outside the modern world economy, such as Russia.\n\nThere are two basic types of global power shifts in the 21st century. One is traditional power transition amongst states, which follows Wallerstein's world system theory. For instance, the global power shifts from the West to the East since the rise of Asia. The other is power diffusion, the way that power move from states to non-states actors. For instance, \"climate change, drug trade, financial flows, pandemics, all these things that cross borders outsider the control of governments.\"\n\nPublic sphere theory, attributed to Jurgen Habermas, is a theory that in its basic premise conceives of democratic governments as those that can stand criticism that comes from public spheres. Public spheres are places, physical or imagined, where people discuss any kind of topic, particularly topics of a societal or political nature. Global public sphere is, therefore, a public that is made of people from across the globe, who come together to discuss and act on issues that concern them. The concept of global public sphere is linked to the shift of public sphere, from restricted to nation-state, to made of individuals and groups connected across as well as within borders.\n\nSince Plato, it can be argued that philosophers have been thinking about versions of a common space for all people to debate in; however, a global public sphere that can fit the description above began to appear much later. In the second half of the 20th century, the legacy of World War II and technological advancements created a new sense of the global and started the economic and political phenomena that we now call globalization. This includes the expansion of humankind into space, which gave individuals the sense of a global unity, the growth of satellite technology, which allowed for people across the globe to view the same television channels, and the internet, which can provide access to an unprecedented amount of information and spaces to connect with other people.\n\nThe term \"culture industry\" appeared in the post-war period. At that time, culture and industry were argued to be opposites. Cultural industries\" are also referred to as the \"Creative industries.\n\nIn the present day, there remain different interpretations of culture as an industry. For some, cultural industries are simply those industries that produce cultural goods and services.\n\nIn the United Nations Educational, Scientific and Cultural Organization (UNESCO), the cultural industries are regarded as those industries that \"combine the creation, production and commercialization of contents which are intangible and cultural in nature. These contents are typically protected by copyright and they can take the form of goods or services\". According to UNESCO, an essential part of cultural industries is that they are \"central in promoting and maintaining cultural diversity and in ensuring democratic access to culture\". \"Cultural industries\" combines the cultural and economic, which gives the cultural industries a distinctive profile.\n\nIn France, the \"cultural industries\" have recently been defined as a set of economic activities that combine the functions of conception, creation and production of culture with more industrial functions in the large-scale manufacture and commercialization cultural products.\n\nIn Canada, economic activities involved in the preservation of heritage are also included in the definition of culture.\n\nSince the rise of the cultural industries has occurred simultaneously with economic globalization, cultural industries have close connections with globalization and global communication.\n\nHerbert Schiller argued that the 'entertainment, communications and information (ECI) complex were having a direct impact on culture and human consciousness. As Schiller argued, the result of transnational corporate expansion is the perpetuation of cultural imperialism, defined as \"the sum of the processes by which a society is brought into the modern world system and how its dominating stratum is attracted, pressured, forced, and sometimes bribed into shaping social institutions to correspond to, or even promote, the values and structures of the dominating centre of the system\".\n\nThe second wave of transnational corporate expansion, which began in the 1970s with the emergence of Export Processing Zones in developing countries, is focused on the development of global production networks. This process was described as a \"new international division of labour\" (NIDL) by the German political economists Frӧbel et al. (1980).\n\nErnst and Kim have argued that GPNs are changing the nature of the multinational corporation itself, from \"stand alone overseas investment projects, to \"global network flagships\" that integrate their dispersed supply, knowledge and customer bases into global and regional production networks\", entailing a shift from top-down hierarchical models of corporate control to increasingly networked and collective forms of organization.\n\nThe largest firms in media and media-related industries have a very high international profile. Global media empires such as Disney, News Corporation, Time-Warner and Viacom-CBS now derive 25-45 per cent of their revenues outside of the United States.\n\nIt is often argued that the global media are dominated by a small number of powerful media conglomerates. Herman and McChesney (1997) argued that the global media were \"dominated by three or four dozen large transnational corporations (TNCs) with fewer than ten mostly US-based media conglomerates towering over the global market.\" Similarly, Manfred Steger has observed that \" to a very large extent, the global cultural flows of our time are generated and directed by global media empires that rely on powerful communication technologies to spread their message.\" He also argued that during the last two decades, a few very large TNCs would come to dominate the global market for entertainment, news, television, and film.\n\nDiaspora is often confused with exodus. Diasporas are minority groups that have a sense of connection with a larger community outside of the borders they currently inhabit, and through diasporic media create a sense of a larger identity and community, whether imagined or real. In scholarly work about diaspora in communication studies, the view of nation and culture as interchangeable terms is no longer prevalent. Stuart Hall theorized of hybridity, which he distinguished from \"old style pluralism\", \"nomadic voyaging of the postmodern\", and \"global homogenization\". Hybridity is the retention of an original identity and strong ties to an original country and tradition, but with the understanding that there is no unchanged, ideal nation of the past that they can return to. To be hybrid is to also adapt to a new culture and tradition without simply assimilating in it, but rather negotiating a place between the \"original\" and \"new\" cultures. In Communication studies, diaspora is discussed as the identity that unifies people across time and space, sometimes existing in physical spaces and other times existing in imagined 'non-spaces'. However, it has been argued that the concept of 'diaspora' implies ethnic homogeneity and essentializes identity to only ethnicity. One of the most cited and well-known works in the field of diasporic media is Hamid Naficy's work on exiled Iranian Americans' creation of cable television in the United States.\n\nDiasporic media refer to media that address the needs of particular ethnic, religious, and/or linguistic groups that live in multicultural settings . Diasporic media can be in the diaspora's traditional language or in another language, and they can include news or media from the \"origin\" country or they can contain the diaspora's local news or media. Diasporic media can be created in radio, television, film, music, in newspapers, magazines, and other publishing, as well as online. It can be argued that the development and spread of satellite television is an instrumental element of the growth of diasporic media today. Satellite television allowed migrants to access the news and popular culture from their homeland, as well as allowing people who speak the same language to access the same channels that might be produced outside of the \"homeland\"\n\nContemporary studies of diaspora show that diasporic media are part of the change in the tendency Immanuel Wallerstein described in his world systems theory. The world systems theory postulates that much of the flow of people in the world has been from the 'periphery', or economically-developing states, towards the centre; which are often metropolitan, economically-wealthy states that grew their wealth in colonialist entrepreneurship. However, contrary to the movement of people, the flow of information (including media products), has tended to be from the centre to the periphery.\n\nThe advancement of media and technology have played the pivotal role in process of globalization and global communication. Cable television, ISDN, digitalization, direct broadcast satellites as well as the Internet have created a situation where vast amounts of information can be transferred around the globe in a matter of seconds.\n\nDuring the early 20th century, telegraph, telephony, and radio started the process of global communication. As media technologies developed intensely, they were thought to create, in Marshall McLuhan’ s famous words, a ‘‘global village.’’ The launch of Sputnik, the world’ s first artificial satellite, on October 4, 1957, marked the beginning of technologies that would further interconnect the world. The first live global television broadcast occurred when Neil Armstrong stepped onto the moon in July 1969. In November 1972, pay TV caused expansion of cable when Service Electric offered Home Box Office over its cable system. By 2000, over direct broadcast satellite, a household could receive channels from all over the world. Now with the World Wide Web, smart phones, tablet devices, smart televisions and other digital media devices, billions of people are now able to access media content that was once tied to particular communications media (print, broadcast) or platforms (newspapers, magazines, radio, television, cinema).\n\nJustice in communication studies includes, but is not limited to, the concern with democratic process and fostering democratic publics . Jurgen Habermas theorized of public sphere (in \"The Structural Transformation of the Public Sphere\") as the space that is created whenever matters of common concern are discussed between the state and civil society. Thus, public sphere includes not only the media, but also public protest in the form of marches, demonstrations, et cetera. There are, however, critiques of political economy in whose view it is impossible to work within the current system to produce democratic publics. Such a critique is that produced by Karl Marx, who saw institutions such as parliament, the state, the 'acceptable' public sphere, economic enterprises, and so on as structurally produced and perpetuated by a capitalist system, and thus they can not be mobilized to change it. In such a system, there can only be illusory justice, which is fair only within the logic of the system. This illusion of justice is produced through dominating ideology.\n\nAnother issue of justice in communication studies is the question of decolonizing research methods and theoretical discourse . The idea of decolonizing research comes from a rejection of the functionalist approach, which assumed that research can be conducted in a vacuum, free of ideology or the researcher's biases. This approach assumed cultures to be unchanging, homogenous, and isolated from each other. The purpose of decolonizing research and discourse is to 'uncloak' research as an unbiased power structure, and produce research that is more self-aware. The approach in decolonizing research methods attempts to create methodologies that treat the people in the study as participants or partners, rather than subjects - which is a term that in itself carries strong connotations of colonialism. Decolonizing research also involves moving away from Eurocentric models that are assumed to work anywhere else, and instead to create work that is more useful in local contexts. Decolonial approaches specifically seek to produce knowledge about the mechanisms and effects of colonialism. These approaches allow former subjects to 'talk back', which is a reflection of independent agency, on the colonizer's own terms of research, rather than to be 'given' a voice, which is an unequal power structure.\n\n"}
{"id": "39958236", "url": "https://en.wikipedia.org/wiki?curid=39958236", "title": "Super-spreader", "text": "Super-spreader\n\nA super-spreader is a host—an organism infected with a disease—that infects, disproportionally, more secondary contacts than other hosts who are, also, infected with the same disease. A sick human can be a super-spreader; they would be more likely to infect others than most people with the disease. Super-spreaders are thus of high concern in epidemiology (the study of the spread of diseases).\n\nSome cases of super-spreading conform to the 20/80 rule, where, approximately, 20% of infected individuals are responsible for 80% of transmissions, although super-spreading can still be said to occur when super-spreaders account for a higher or lower percentage of transmissions. In epidemics with super-spreading, the majority of individuals infect relatively few secondary contacts.\n\nSuper-spreading events are shaped by multiple factors including a decline in herd immunity, nosocomial infections, virulence, viral load, misdiagnosis, airflow dynamics, immune suppression, and co-infection with another pathogen.\n\nAlthough loose definitions of super-spreading exist, some effort has been made at defining what qualifies as a super-spreading event (SSE) more explicit. Lloyd-Smith et al. (2005) define a protocol to identify a super-spreading event as follows:\n\nThis protocol defines a 99th-percentile SSE as a case which causes more infections than would occur in 99% of infectious histories in a homogeneous population.\n\nDuring the 2003 SARS outbreak in Beijing, China, epidemiologists defined a super-spreader as an individual with transmission of SARS to at least eight contacts.\n\nSuper-spreaders may or may not show any symptoms of the disease.\n\nSuper-spreaders have been identified who excrete a higher than normal number of pathogens during the time they are infectious. This causes their contacts to be exposed to higher viral/bacterial loads than would be seen in the contacts of non-superspreaders with the same duration of exposure.\n\nThe basic reproduction number R is the average number of secondary infections caused by a typical infective person in a totally susceptible population. The basic reproductive number is found by multiplying the average number of contacts by the average probability that a susceptible individual will become infected, which is called the shedding potential. R = Number of contacts X Shedding potential \n\nThe individual reproductive number represents the number of secondary infections caused by a specific individual during the time that individual is infectious. Some individuals have significantly higher than average individual reproductive numbers and are known as super-spreaders. Through contact tracing, epidemiologists have identified super-spreaders in measles, tuberculosis, rubella, monkeypox, smallpox, Ebola hemorrhagic fever and SARS.\n\nMen with HIV who were co-infected with at least one other sexually transmitted disease, such as gonorrhea, hepatitis C, and herpes simplex 2 virus, were found to have an eight-fold higher HIV shedding rate than men without co-infection. This shedding rate was calculated in men with similar HIV viral loads. Once treatment for the co-infection had been completed, the HIV shedding rate returned to levels comparable to men without co-infection.\n\nHerd immunity, or herd effect, refers to the indirect protection that immunized community members provide to non-immunized members in preventing the spread of contagious disease. The greater the number of immunized individuals, the less likely an outbreak can occur because there are fewer susceptible contacts. In epidemiology, herd immunity is known as a \"dependent happening\" because it influences transmission over time. As a pathogen that confers immunity to the survivors moves through a susceptible population, the number of susceptible contacts declines. Even if susceptible individuals remain, their contacts are likely to be immunized, preventing any further spread of the infection. The proportion of immune individuals in a population above which a disease may no longer persist is the \"herd immunity threshold\". Its value varies with the virulence of the disease, the efficacy of the vaccine, and the contact parameter for the population. That is not to say that an outbreak can't occur, but it will be limited. \n\nThe first cases of SARS occurred in mid-November 2002 in the Guangdong Province of China. This was followed by an outbreak in Hong Kong in February, 2003. A Guangdong Province doctor, Liu Jianlun, who had treated SARS cases there, had contracted the virus and was symptomatic. Despite his symptoms, he traveled to Hong Kong to attend a family wedding. He stayed on the ninth floor of the Metropole Hotel in Kowloon, infecting 16 other hotel guests also staying on that floor (pictured above). The guests then traveled to Canada, Singapore, Taiwan, and Vietnam, spreading SARS to those locations and transmitting what became a global epidemic.\n\nIn another case during this same outbreak, a 54-year-old male was admitted to a hospital with coronary heart disease, chronic renal failure and type two diabetes. He had been in contact with a patient known to have SARS. Shortly after his admission he developed fever, cough, myalgia and sore throat. The admitting physician suspected SARS. The patient was transferred to another hospital for treatment of his coronary artery disease. While there, his SARS symptoms became more pronounced. Later, it was discovered he had transmitted SARS to 33 other patients in just two days. He was transferred back to the original hospital where he died of SARS.\n\nThe SARS pandemic was eventually contained, but not before it caused 8,273 cases and 775 deaths. Within two weeks of the original outbreak in Guangdong Province, SARS had spread to 37 countries.\n\nMeasles is a highly contagious, air-borne virus that reappears even among vaccinated populations. In one Finnish town in 1989, an explosive school-based outbreak resulted in 51 cases, several of whom had been previously vaccinated. One child alone, infected 22 others. It was noted during this outbreak that when vaccinated siblings shared a bedroom with an infected sibling, seven out of nine became infected as well.\n\nTyphoid fever is a human-specific disease caused by the bacterium \"Salmonella typhi\". It is highly contagious and becoming resistant to antibiotics. S. typhi is susceptible to creating asymptomatic carriers. The most famous carriers are Mary Mallon, known as Typhoid Mary, from New York City, and Mr. N. the Milker, from Folkstone, England. Both were active around the same time. Mallon infected 51 people from 1902 to 1909. Mr. N. infected more than 200 people over 14 years from 1901 to 1915. At the request of health officials, Mr. N. gave up working in food service. Mallon refused to give up working in food service and eventually was involuntarily quarantined at Brothers Island in New York, where she stayed until she died in November 1938, aged 69.\n\nIt has been found that \"Salmonella typhi\" persists in infected mice macrophages that have cycled from an inflammatory state to a non-inflammatory state. The bacteria remain and reproduce without causing further symptoms in the mice, and that this explains why carriers are asymptomatic.\n\n\n"}
{"id": "41909781", "url": "https://en.wikipedia.org/wiki?curid=41909781", "title": "Timeline of global surveillance disclosures (2013–present)", "text": "Timeline of global surveillance disclosures (2013–present)\n\nThis timeline of global surveillance disclosures from 2013 to t sent day is a chronological list of the global surveillance disclosures that began in 2013. The disclosures have been largely instigated by revelations from the former American National Security Agency contractor Edward Snowden.\n\nIn April 2012, Defense contractor Edward Snowden began downloading sensitive Western intelligence material while working for the American computer corporation Dell. By the end of the year, Snowden had made his first contact with journalist Glenn Greenwald of \"The Guardian\".\n\nIn January 2013, Snowden contacted documentary filmmaker Laura Poitras. In March 2013, Snowden took up a new job at Booz Allen Hamilton in Hawaii, specifically to gain access to additional top-secret documents that could be leaked. In April 2013, Poitras asked Greenwald to meet her in New York City. In May 2013, Snowden was permitted temporary leave from his position at the NSA in Hawaii, on the pretext of receiving treatment for his epilepsy. Towards the end of May, Snowden flew to Hong Kong.\n\nAfter the U.S.-based editor of \"The Guardian\" held several meetings in New York City, it was decided that Greenwald, Poitras, and \"The Guardian\"s defence and intelligence correspondent Ewen MacAskill would fly to Hong Kong to meet Snowden. On June 5, in the first media report based on the leaked material, \"The Guardian\" exposed a top secret court order showing that the NSA had collected phone records from over 120 million Verizon subscribers. Under the order, the numbers of both parties on a call, as well as the location data, unique identifiers, time of call, and duration of call were handed over to the FBI, which turned over the records to the NSA. According to the \"Wall Street Journal\", the Verizon order is part of a controversial data program, which seeks to stockpile records on all calls made in the U.S., but doesn't collect information directly from T-Mobile USA and Verizon Wireless, in part because of their foreign ownership ties.\n\nOn June 7, 2013, the second media disclosure, the revelation of the PRISM surveillance program, was published simultaneously by \"The Guardian\" and \"The Washington Post\".\n\nDocuments provided by Snowden to \"Der Spiegel\" revealed how the NSA spied on various diplomatic missions of the European Union (EU) including the EU's delegation to the United States in Washington D.C., the EU's delegation to the United Nations in New York, and the Council of the European Union in Brussels, as well as the United Nations Headquarters in New York. During specific episodes within a four-year period, the NSA hacked several Chinese mobile phone companies, the Chinese University of Hong Kong, and Tsinghua University in Beijing, and the Asian fiber-optic network operator Pacnet. Only Australia, Canada, New Zealand, and the UK are explicitly exempted from NSA attacks, whose main target in the EU is Germany. A method of bugging encrypted fax machines used at an EU embassy is codenamed Dropmire.\n\nDuring the 2009 G-20 London summit, the British intelligence agency Government Communications Headquarters (GCHQ) intercepted the communications of foreign diplomats. In addition, the GCHQ has been intercepting and storing mass quantities of fiber-optic traffic via Tempora. Two principal components of Tempora are called \"Mastering the Internet\" (MTI) and \"Global Telecoms Exploitation\". The data is preserved for three days while metadata is kept for thirty days. Data collected by the GCHQ under Tempora is shared with the National Security Agency (NSA) of the United States.\n\nFrom 2001 to 2011, the NSA collected vast amounts of metadata records detailing the email and internet usage of Americans via Stellar Wind, which was later terminated due to operational and resource constraints. It was subsequently replaced by newer surveillance programs such as ShellTrumpet, which \"processed its one trillionth metadata record\" by the end of December 2012.\n\nAccording to the Boundless Informant, over 97 billion pieces of intelligence were collected over a 30-day period ending in March 2013. Out of all 97 billion sets of information, about 3 billion data sets originated from U.S. computer networks and around 500 million metadata records were collected from German networks.\n\nSeveral weeks later, it was revealed that the Bundesnachrichtendienst (BND) of Germany transfers massive amounts of metadata records to the NSA.\n\nAccording to the Brazilian newspaper \"O Globo\", the NSA spied on millions of emails and calls of Brazilian citizens, while Australia and New Zealand have been involved in the joint operation of the NSA's global analytical system XKeyscore. Among the numerous allied facilities contributing to XKeyscore are four installations in Australia and one in New Zealand:\n\n\n\"O Globo\" released an NSA document titled \"\", which revealed the specific locations and codenames of the FORNSAT intercept stations in 2002.\n\nAccording to Edward Snowden, the NSA has established secret intelligence partnerships with many Western governments. The Foreign Affairs Directorate (FAD) of the NSA is responsible for these partnerships, which, according to Snowden, are organized such that foreign governments can \"insulate their political leaders\" from public outrage in the event that these global surveillance partnerships are leaked.\n\nIn an interview published by \"Der Spiegel\", Snowden accused the NSA of being \"in bed together with the Germans\". The NSA granted the German intelligence agencies BND (foreign intelligence) and BfV (domestic intelligence) access to its controversial XKeyscore system. In return, the BND turned over copies of two systems named Mira4 and Veras, reported to exceed the NSA's SIGINT capabilities in certain areas. Every day, massive amounts of metadata records are collected by the BND and transferred to the NSA via the Bad Aibling Station near Munich, Germany. In December 2012 alone, the BND handed over 500 million metadata records to the NSA.\n\nIn a document dated January 2013, the NSA acknowledged the efforts of the BND to undermine privacy laws:\n\nAccording to an NSA document dated April 2013, Germany has now become the NSA's \"most prolific partner\". Under a section of a separate document leaked by Snowden titled \"Success Stories\", the NSA acknowledged the efforts of the German government to expand the BND's international data sharing with partners:\n\nIn addition, the German government was well aware of the PRISM surveillance program long before Edward Snowden made details public. According to Angela Merkel's spokesman Steffen Seibert, there are two separate PRISM programs - one is used by the NSA and the other is used by NATO forces in Afghanistan. Both surveillance programs are \"not identical\".\n\n\"The Guardian\" revealed further details of the NSA's XKeyscore tool, which allows government analysts to search through vast databases containing emails, online chats and the browsing histories of millions of individuals without prior authorization. Microsoft \"developed a surveillance capability to deal\" with the interception of encrypted chats on Outlook.com, within five months after the service went into testing. NSA had access to Outlook.com emails because “Prism collects this data prior to encryption.”\n\nIn addition, Microsoft worked with the FBI to enable the NSA to gain access to its cloud storage service SkyDrive. An internal NSA document dating from 3 August 2012 described the PRISM surveillance program as a \"team sport\".\n\nEven if there is no reason to suspect U.S. citizens of wrongdoing, the CIA's National Counterterrorism Center is allowed to examine federal government files for possible criminal behavior. Previously the NTC was barred to do so, unless a person was a terror suspect or related to an investigation.\n\nSnowden also confirmed that Stuxnet was cooperatively developed by the United States and Israel. In a report unrelated to Edward Snowden, the French newspaper \"Le Monde\" revealed that France's DGSE was also undertaking mass surveillance, which it described as \"illegal and outside any serious control\".\n\nDocuments leaked by Edward Snowden that were seen by \"Süddeutsche Zeitung\" (SZ) and \"Norddeutscher Rundfunk\" revealed that several telecom operators have played a key role in helping the British intelligence agency Government Communications Headquarters (GCHQ) tap into worldwide fiber-optic communications. The telecom operators are:\n\nEach of them were assigned a particular area of the international fiber-optic network for which they were individually responsible. The following networks have been infiltrated by the GCHQ: TAT-14 (Europe-USA), Atlantic Crossing 1 (Europe-USA), Circe South (France-UK), Circe North (The Netherlands-UK), Flag Atlantic-1, Flag Europa-Asia, SEA-ME-WE 3 (Southeast Asia-Middle East-Western Europe), SEA-ME-WE 4 (Southeast Asia-Middle East-Western Europe), Solas (Ireland-UK), UK-France 3, UK-Netherlands 14, ULYSSES (Europe-UK), Yellow (UK-USA) and Pan European Crossing.\n\nTelecommunication companies who participated were \"forced\" to do so and had \"no choice in the matter\". Some of the companies were subsequently paid by GCHQ for their participation in the infiltration of the cables. According to the SZ the GCHQ has access to the majority of internet and telephone communications flowing throughout Europe, can listen to phone calls, read emails and text messages, see which websites internet users from all around the world are visiting. It can also retain and analyse nearly the entire European internet traffic.\n\nThe GCHQ is collecting all data transmitted to and from the United Kingdom and Northern Europe via the undersea fibre optic telecommunications cable SEA-ME-WE 3. The Security and Intelligence Division (SID) of Singapore co-operates with Australia in accessing and sharing communications carried by the SEA-ME-WE-3 cable. The Australian Signals Directorate (ASD) is also in a partnership with British, American and Singaporean intelligence agencies to tap undersea fibre optic telecommunications cables that link Asia, the Middle East and Europe and carry much of Australia's international phone and internet traffic.\n\nThe U.S. runs a top-secret surveillance program known as the Special Collection Service (SCS), which is based in over 80 U.S. consulates and embassies worldwide. The NSA hacked the United Nations' video conferencing system in Summer 2012 in violation of a UN agreement.\n\nThe NSA is not just intercepting the communications of Americans who are in direct contact with foreigners targeted overseas, but also searching the contents of vast amounts of e-mail and text communications into and out of the country by Americans who mention information about foreigners under surveillance. It also spied on the Al Jazeera and gained access to its internal communications systems.\n\nThe NSA has built a surveillance network that has the capacity to reach roughly 75% of all U.S. Internet traffic. U.S. Law-enforcement agencies use tools used by computer hackers to gather information on suspects. An internal NSA audit from May 2012 identified 2776 incidents i.e. violations of the rules or court orders for surveillance of Americans and foreign targets in the U.S. in the period from April 2011 through March 2012, while U.S. officials stressed that any mistakes are not intentional.\n\nThe FISA Court that is supposed to provide critical oversight of the U.S. government's vast spying programs has limited ability to do and it must trust the government to report when it improperly spies on Americans. A legal opinion declassified on August 21, 2013 revealed that the NSA intercepted for three years as many as 56,000 electronic communications a year of Americans who weren’t suspected of having links to terrorism, before FISC court that oversees surveillance found the operation unconstitutional in 2011. Under the Corporate Partner Access project, major U.S. telecommunications providers receive hundreds of millions of dollars each year from the NSA. Voluntary cooperation between the NSA and the providers of global communications took off during the 1970s under the cover name BLARNEY.\n\nA letter drafted by the Obama administration specifically to inform Congress of the government's mass collection of Americans’ telephone communications data was withheld from lawmakers by leaders of the House Intelligence Committee in the months before a key vote affecting the future of the program.\n\nThe NSA paid GCHQ over £100 Million between 2009 and 2012, in exchange for these funds GCHQ \"must pull its weight and be seen to pull its weight.\" Documents referenced in the article explain that the weaker British laws regarding spying are \"a selling point\" for the NSA. GCHQ is also developing the technology to \"exploit any mobile phone at any time.\" The NSA has under a legal authority a secret backdoor into its databases gathered from large Internet companies enabling it to search for U.S. citizens' email and phone calls without a warrant.\n\nThe Privacy and Civil Liberties Oversight Board urged the U.S. intelligence chiefs to draft stronger US surveillance guidelines on domestic spying after finding that several of those guidelines have not been updated up to 30 years. U.S. intelligence analysts have deliberately broken rules designed to prevent them from spying on Americans by choosing to ignore so-called \"minimisation procedures\" aimed at protecting privacy.\n\nAfter the U.S. Foreign Secret Intelligence Court ruled in October 2011 that some of the NSA's activities were unconstitutional, the agency paid millions of dollars to major internet companies to cover extra costs incurred in their involvement with the PRISM surveillance program.\n\n\"Mastering the Internet\" (MTI) is part of the Interception Modernisation Programme (IMP) of the British government that involves the insertion of thousands of DPI (deep packet inspection) \"black boxes\" at various internet service providers, as revealed by the British media in 2009.\n\nIn 2013, it was further revealed that the NSA had made a £17.2  million financial contribution to the project, which is capable of vacuuming signals from up to 200 fibre-optic cables at all physical points of entry into Great Britain.\n\n\"The Guardian\" and \"The New York Times\" reported on secret documents leaked by Snowden showing that the NSA has been in \"collaboration with technology companies\" as part of \"an aggressive, multipronged effort\" to weaken the encryption used in commercial software, and the GCHQ has a team dedicated to cracking \"Hotmail, Google, Yahoo and Facebook\" traffic.\nIsrael, Sweden and Italy are also cooperating with American and British intelligence agencies. Under a secret treaty codenamed \"Lustre\", French intelligence agencies transferred millions of metadata records to the NSA.\n\nThe Obama Administration secretly won permission from the Foreign Intelligence Surveillance Court in 2011 to reverse restrictions on the National Security Agency’s use of intercepted phone calls and e-mails, permitting the agency to search deliberately for Americans’ communications in its massive databases. The searches take place under a surveillance program Congress authorized in 2008 under Section 702 of the Foreign Intelligence Surveillance Act. Under that law, the target must be a foreigner “reasonably believed” to be outside the United States, and the court must approve the targeting procedures in an order good for one year. But a warrant for each target would thus no longer be required. That means that communications with Americans could be picked up without a court first determining that there is probable cause that the people they were talking to were terrorists, spies or “foreign powers.” The FISC extended the length of time that the NSA is allowed to retain intercepted U.S. communications from five years to six years with an extension possible for foreign intelligence or counterintelligence purposes. Both measures were done without public debate or any specific authority from Congress.\n\nA special branch of the NSA called \"Follow the Money\" (FTM) monitors international payments, banking and credit card transactions and later stores the collected data in the NSA's own financial databank \"Tracfin\". The NSA monitored the communications of Brazil's president Dilma Rousseff and her top aides. The agency also spied on Brazil's oil firm Petrobras as well as French diplomats, and gained access to the private network of the Ministry of Foreign Affairs of France and the SWIFT network.\n\nIn the United States, the NSA uses the analysis of phone call and e-mail logs of American citizens to create sophisticated graphs of their social connections that can identify their associates, their locations at certain times, their traveling companions and other personal information. The NSA routinely shares raw intelligence data with Israel without first sifting it to remove information about U.S. citizens.\n\nIn an effort codenamed GENIE, computer specialists can control foreign computer networks using \"covert implants,” a form of remotely transmitted malware on tens of thousands of devices annually. As worldwide sales of smartphones began exceeding those of feature phones, the NSA decided to take advantage of the smartphone boom. This is particularly advantageous because the smartphone combines a myriad of data that would interest an intelligence agency, such as social contacts, user behavior, interests, location, photos and credit card numbers and passwords.\n\nAn internal NSA report from 2010 stated that the spread of the smartphone has been occurring \"extremely rapidly\"—developments that \"certainly complicate traditional target analysis.\" According to the document, the NSA has set up task forces assigned to several smartphone manufacturers and operating systems, including Apple Inc.'s iPhone and iOS operating system, as well as Google's Android mobile operating system. Similarly, Britain's GCHQ assigned a team to study and crack the BlackBerry.\n\nUnder the heading \"iPhone capability\", the document notes that there are smaller NSA programs, known as \"scripts\", that can perform surveillance on 38 different features of the iOS 3 and iOS 4 operating systems. These include the mapping feature, voicemail and photos, as well as Google Earth, Facebook and Yahoo! Messenger.\nOn October 4, 2013, \"The Washington Post\" and \"The Guardian\" jointly reported that the NSA and the GCHQ have made repeated attempts to spy on anonymous Internet users who have been communicating in secret via the anonymity network Tor. Several of these surveillance operations involve the implantation of malicious code into the computers of Tor users who visit particular websites. The NSA and GCHQ have partly succeeded in blocking access to the anonymous network, diverting Tor users to insecure channels. The government agencies were also able to uncover the identity of some anonymous Internet users.\n\nThe Communications Security Establishment Canada (CSEC) has been using a program called Olympia to map the communications of Brazil's Mines and Energy Ministry by targeting the metadata of phone calls and emails to and from the ministry.\n\nThe Australian Federal Government knew about the PRISM surveillance program months before Edward Snowden made details public.\n\nThe NSA monitored the public email account of former Mexican president Felipe Calderón (thus gaining access to the communications of high-ranking cabinet members), the E-Mails of several high-ranking members of Mexico's security forces and text and the mobile phone communication of current Mexican president Enrique Peña Nieto. The NSA tries to gather cellular and landline phone numbers—often obtained from American diplomats—for as many foreign officials as possible. The contents of the phone calls are stored in computer databases that can regularly be searched using keywords.\n\nThe NSA has been monitoring telephone conversations of 35 world leaders. The U.S. government's first public acknowledgment that it tapped the phones of world leaders was reported on October 28, 2013 by the Wall Street Journal after an internal U.S. government review turned up NSA monitoring of some 35 world leaders. The GCHQ has tried to keep its mass surveillance program a secret because it feared a \"damaging public debate\" on the scale of its activities which could lead to legal challenges against them.\n\n\"The Guardian\" revealed that the NSA had been monitoring telephone conversations of 35 world leaders after being given the numbers by an official in another U.S. government department. A confidential memo revealed that the NSA encouraged senior officials in such Departments as the White House, State and The Pentagon, to share their \"Rolodexes\" so the agency could add the telephone numbers of leading foreign politicians to their surveillance systems. Reacting to the news, German leader Angela Merkel, arriving in Brussels for an EU summit, accused the U.S. of a breach of trust, saying: \"We need to have trust in our allies and partners, and this must now be established once again. I repeat that spying among friends is not at all acceptable against anyone, and that goes for every citizen in Germany.\" The NSA collected in 2010 data on ordinary Americans’ cellphone locations, but later discontinued it because it had no “operational value.”\n\nUnder Britain's MUSCULAR programme, the NSA and the GCHQ have secretly broken into the main communications links that connect Yahoo and Google data centers around the world and thereby gained the ability to collect metadata and content at will from hundreds of millions of user accounts.\n\nThe mobile phone of German Chancellor Angela Merkel might have been tapped by U.S. intelligence. According to the Spiegel this monitoring goes back to 2002 and ended in the summer of 2013, while the New York Times reported that Germany has evidence that the NSA's surveillance of Merkel began during George W. Bush's tenure. After learning from \"Der Spiegel\" magazine that the NSA has been listening in to her personal mobile phone, Merkel compared the snooping practices of the NSA with those of the Stasi.\n\nOn October 31, 2013, Hans-Christian Ströbele, a member of the German Bundestag, met Snowden in Moscow and revealed the former intelligence contractor's readiness to brief the German government on NSA spying.\n\nA highly sensitive signals intelligence collection program known as Stateroom involves the interception of radio, telecommunications and internet traffic. It is operated out of the diplomatic missions of the Five Eyes (Australia, Britain, Canada, New Zealand, United States) in numerous locations around the world. The program conducted at U.S. diplomatic missions is run in concert by the U.S. intelligence agencies NSA and CIA in a joint venture group called \"Special Collection Service\" (SCS), whose members work undercover in shielded areas of the American Embassies and Consulates, where they are officially accredited as diplomats and as such enjoy special privileges. Under diplomatic protection, they are able to look and listen unhindered. The SCS for example used the American Embassy near the Brandenburg Gate in Berlin to monitor communications in Germany's government district with its parliament and the seat of the government.\n\nUnder the Stateroom surveillance programme, Australia operates clandestine surveillance facilities to intercept phone calls and data across much of Asia.\n\nIn France, the NSA targeted people belonging to the worlds of business, politics or French state administration. The NSA monitored and recorded the content of telephone communications and the history of the connections of each target i.e. the metadata. The actual surveillance operation was performed by French intelligence agencies on behalf of the NSA. The cooperation between France and the NSA was confirmed by the Director of the NSA, Keith B. Alexander, who asserted that foreign intelligence services collected phone records in \"war zones\" and \"other areas outside their borders\" and provided them to the NSA.\n\nThe French newspaper \"Le Monde\" also disclosed new PRISM and Upstream slides (See Page 4, 7 and 8) coming from the \"PRISM/US-984XN Overview\" presentation.\n\nIn Spain, the NSA intercepted the telephone conversations, text messages and emails of millions of Spaniards, and spied on members of the Spanish government. Between December 10, 2012 and January 8, 2013, the NSA collected metadata on 60 million telephone calls in Spain.\n\nAccording to documents leaked by Snowden, the surveillance of Spanish citizens was jointly conducted by the NSA and the intelligence agencies of Spain.\n\nThe \"New York Times\" reported that the NSA carries out an eavesdropping effort, dubbed Operation Dreadnought, against the Iranian leader Ayatollah Ali Khamenei. During his 2009 visit to Iranian Kurdistan, the agency collaborated with the GCHQ and the U.S.'s National Geospatial-Intelligence Agency, collecting radio transmissions between aircraft and airports, examining Khamenei's convoy with satellite imagery, and enumerating military radar stations. According to the story, an objective of the operation is \"communications fingerprinting\": the ability to distinguish Khamenei's communications from those of other people in Iran.\n\nThe same story revealed an operation code-named Ironavenger, in which the NSA intercepted e-mails sent between a country allied with the United States and the government of \"an adversary\". The ally was conducting a spear-phishing attack: its e-mails contained malware. The NSA gathered documents and login credentials belonging to the enemy country, along with knowledge of the ally's capabilities for attacking computers.\n\nAccording to the British newspaper \"The Independent\", the British intelligence agency GCHQ maintains a listening post on the roof of the British Embassy in Berlin that is capable of intercepting mobile phone calls, wi-fi data and long-distance communications all over the German capital, including adjacent government buildings such as the Reichstag (seat of the German parliament) and the Chancellery (seat of Germany's head of government) clustered around the Brandenburg Gate.\n\nOperating under the code-name \"Quantum Insert\", the GCHQ set up a fake website masquerading as LinkedIn, a social website used for professional networking, as part of its efforts to install surveillance software on the computers of the telecommunications operator Belgacom. In addition, the headquarters of the oil cartel OPEC were infiltrated by the GCHQ as well as the NSA, which bugged the computers of nine OPEC employees and monitored the General Secretary of OPEC.\n\nFor more than three years the GCHQ has been using an automated monitoring system code-named \"Royal Concierge\" to infiltrate the reservation systems of at least 350 upscale hotels in many different parts of the world in order to target, search and analyze reservations to detect diplomats and government officials. First tested in 2010, the aim of the \"Royal Concierge\" is to track down the travel plans of diplomats, and it is often supplemented with surveillance methods related to human intelligence (HUMINT). Other covert operations include the wiretapping of room telephones and fax machines used in targeted hotels as well as the monitoring of computers hooked up to the hotel network.\n\nIn November 2013, the Australian Broadcasting Corporation and \"The Guardian\" revealed that the Australian Signals Directorate (DSD) had attempted to listen to the private phone calls of the president of Indonesia and his wife. The Indonesian foreign minister, Marty Natalegawa, confirmed that he and the president had contacted the ambassador in Canberra. Natalegawa said any tapping of Indonesian politicians’ personal phones “violates every single decent and legal instrument I can think of—national in Indonesia, national in Australia, international as well”.\n\nOther high-ranking Indonesian politicians targeted by the DSD include:\n\n\nCarrying the title \"3G impact and update\", a classified presentation leaked by Snowden revealed the attempts of the ASD/DSD to keep up to pace with the rollout of 3G technology in Indonesia and across Southeast Asia. The ASD/DSD motto placed at the bottom of each page reads: \"Reveal their secrets—protect our own.\"\n\nUnder a secret deal approved by British intelligence officials, the NSA has been storing and analyzing the internet and email records of UK citizens since 2007. The NSA also proposed in 2005 a procedure for spying on the citizens of the UK and other Five-Eyes nations alliance, even where the partner government has explicitly denied the U.S. permission to do so. Under the proposal, partner countries must neither be informed about this particular type of surveillance, nor the procedure of doing so.\n\nTowards the end of November, \"The New York Times\" released an internal NSA report outlining the agency's efforts to expand its surveillance abilities. The five-page document asserts that the law of the United States has not kept up with the needs of the NSA to conduct mass surveillance in the \"golden age\" of signals intelligence, but there are grounds for optimism because, in the NSA's own words:\n\nThe report, titled \"SIGNT Strategy 2012–2016\", also said that the U.S. will try to influence the \"global commercial encryption market\" through \"commercial relationships\", and emphasized the need to \"revolutionize\" the analysis of its vast data collection to \"radically increase operational impact\".\n\nOn November 23, 2013, the Dutch newspaper \"NRC Handelsblad\" reported that the Netherlands was targeted by U.S. intelligence agencies in the immediate aftermath of World War II. This period of surveillance lasted from 1946 to 1968, and also included the interception of the communications of other European countries including Belgium, France, West Germany and Norway. The Dutch Newspaper also reported that NSA infected more than 50,000 computer networks worldwide, often covertly, with malicious spy software, sometimes in cooperation with local authorities, designed to steal sensitive information.\n\nAccording to the classified documents leaked by Snowden, the Australian Signals Directorate, formerly known as the Defence Signals Directorate, had offered to share information on Australian citizens with the other intelligence agencies of the UKUSA Agreement. Data shared with foreign countries include \"bulk, unselected, unminimised metadata\" such as \"medical, legal or religious information\".\n\n\"The Washington Post\" revealed that the NSA has been tracking the locations of mobile phones from all over the world by tapping into the cables that connect mobile networks globally and that serve U.S. cellphones as well as foreign ones. In the process of doing so, the NSA collects more than five billion records of phone locations on a daily basis. This enables NSA analysts to map cellphone owners’ relationships by correlating their patterns of movement over time with thousands or millions of other phone users who cross their paths.\n\nThe Washington Post also reported that the NSA makes use of location data and advertising tracking files generated through normal internet browsing i.e. tools that enable Internet advertisers to track consumers from Google and others to get information on potential targets, to pinpoint targets for government hacking and to bolster surveillance.\n\nThe Norwegian Intelligence Service (NIS), which cooperates with the NSA, has gained access to Russian targets in the Kola Peninsula and other civilian targets. In general, the NIS provides information to the NSA about \"Politicians\", \"Energy\" and \"Armament\". A top secret memo of the NSA lists the following years as milestones of the Norway-United States of America SIGNT agreement, or NORUS Agreement:\n\nThe NSA considers the NIS to be one of its most reliable partners. Both agencies also cooperate to crack the encryption systems of mutual targets. According to the NSA, Norway has made no objections to its requests from the NIS.\n\nOn 5 December, Sveriges Television reported that the National Defence Radio Establishment (FRA) has been conducting a clandestine surveillance operation in Sweden, targeting the internal politics of Russia. The operation was conducted on behalf of the NSA, receiving data handed over to it by the FRA. The Swedish-American surveillance operation also targeted Russian energy interests as well as the Baltic states. As part of the UKUSA Agreement, a secret treaty was signed in 1954 by Sweden with the United States, the United Kingdom, Canada, Australia and New Zealand, regarding collaboration and intelligence sharing.\n\nAs a result of Snowden's disclosures, the notion of Swedish neutrality in international politics was called into question. In an internal document dating from the year 2006, the NSA acknowledged that its \"relationship\" with Sweden is \"protected at the TOP SECRET level because of that nation’s political neutrality.\" Specific details of Sweden's cooperation with members of the UKUSA Agreement include:\n\nIn order to identify targets for government hacking and surveillance, both the GCHQ and the NSA have used advertising cookies operated by Google, known as Pref, to \"pinpoint\" targets. According to documents leaked by Snowden, the Special Source Operations of the NSA has been sharing information containing \"logins, cookies, and GooglePREFID\" with the Tailored Access Operations division of the NSA, as well as Britain's GCHQ agency.\n\nDuring the 2010 G-20 Toronto summit, the U.S. embassy in Ottawa was transformed into a security command post during a six-day spying operation that was conducted by the NSA and closely co-ordinated with the Communications Security Establishment Canada (CSEC). The goal of the spying operation was, among others, to obtain information on international development, banking reform, and to counter trade protectionism to support \"U.S. policy goals.\" On behalf of the NSA, the CSEC has set up covert spying posts in 20 countries around the world.\n\nIn Italy the Special Collection Service of the NSA maintains two separate surveillance posts in Rome and Milan. According to a secret NSA memo dated September 2010, the Italian embassy in Washington, D.C. has been targeted by two spy operations of the NSA:\n\nDue to concerns that terrorist or criminal networks may be secretly communicating via computer games, the NSA, the GCHQ, the CIA, and the FBI have been conducting surveillance and scooping up data from the networks of many online games, including massively multiplayer online role-playing games (MMORPGs) such as World of Warcraft, as well as virtual worlds such as Second Life, and the Xbox gaming console.\n\nThe NSA has cracked the most commonly used cellphone encryption technology, A5/1. According to a classified document leaked by Snowden, the agency can \"process encrypted A5/1\" even when it has not acquired an encryption key. In addition, the NSA uses various types of cellphone infrastructure, such as the links between carrier networks, to determine the location of a cellphone user tracked by Visitor Location Registers.\n\nUS district court judge for the District of Columbia, Richard Leon, declared on December 16, 2013, that the mass collection of metadata of Americans’ telephone records by the National Security Agency probably violates the fourth amendment prohibition of unreasonable searches and seizures. Leon granted the request for a preliminary injunction that blocks the collection of phone data for two private plaintiffs (Larry Klayman, a conservative lawyer, and Charles Strange, father of a cryptologist killed in Afghanistan when his helicopter was shot down in 2011) and ordered the government to destroy any of their records that have been gathered. But the judge stayed action on his ruling pending a government appeal, recognizing in his 68-page opinion the “significant national security interests at stake in this case and the novelty of the constitutional issues.”\n\nHowever federal judge William H. Pauley III in New York City ruled the U.S. government’s global telephone data-gathering system is needed to thwart potential terrorist attacks, and that it can only work if everyone’s calls are swept in. U.S. District Judge Pauley also ruled that Congress legally set up the program and that it does not violate anyone’s constitutional rights. The judge also concluded that the telephone data being swept up by NSA did not belong to telephone users, but to the telephone companies. He further ruled that when NSA obtains such data from the telephone companies, and then probes into it to find links between callers and potential terrorists, this further use of the data was not even a search under the Fourth Amendment. He also concluded that the controlling precedent is Smith v. Maryland: “Smith’s bedrock holding is that an individual has no legitimate expectation of privacy in information provided to third parties,” Judge Pauley wrote. The American Civil Liberties Union declared on January 2, 2012 that it will appeal Judge Pauley's ruling that NSA bulk the phone record collection is legal. \"The government has a legitimate interest in tracking the associations of suspected terrorists, but tracking those associations does not require the government to subject every citizen to permanent surveillance,” deputy ACLU legal director Jameel Jaffer said in a statement.\n\nIn recent years, American and British intelligence agencies conducted surveillance on more than 1,100 targets, including the office of an Israeli prime minister, heads of international aid organizations, foreign energy companies and a European Union official involved in antitrust battles with American technology businesses.\n\nA catalog of high-tech gadgets and software developed by the NSA's Tailored Access Operations (TAO) was leaked by the German news magazine \"Der Spiegel\". Dating from 2008, the catalog revealed the existence of special gadgets modified to capture computer screenshots and USB flash drives secretly fitted with radio transmitters to broadcast stolen data over the airwaves, and fake base stations intended to intercept mobile phone signals, as well as many other secret devices and software implants listed here:\n\nThe Tailored Access Operations (TAO) division of the NSA intercepted the shipping deliveries of computers and laptops in order to install spyware and physical implants on electronic gadgets. This was done in close cooperation with the FBI and the CIA. NSA officials responded to the Spiegel reports with a statement, which said: \"Tailored Access Operations is a unique national asset that is on the front lines of enabling NSA to defend the nation and its allies. [TAO's] work is centred on computer network exploitation in support of foreign intelligence collection.\"\n\nIn a separate disclosure unrelated to Snowden, the French Trésor public, which runs a certificate authority, was found to have issued fake certificates impersonating Google in order to facilitate spying on French government employees via man-in-the-middle attacks.\n\nThe NSA is working to build a powerful quantum computer capable of breaking all types of encryption. The effort is part of a US$79.7 million research program known as \"Penetrating Hard Targets\". It involves extensive research carried out in large, shielded rooms known as Faraday cages, which are designed to prevent electromagnetic radiation from entering or leaving. Currently, the NSA is close to producing basic building blocks that will allow the agency to gain \"complete quantum control on two semiconductor qubits\". Once a quantum computer is successfully built, it would enable the NSA to unlock the encryption that protects data held by banks, credit card companies, retailers, brokerages, governments and health care providers.\n\nAccording to the New York Times the NSA is monitoring approximately 100.000 computers worldwide with spy software named Quantum. Quantum enables the NSA to conduct surveillance on those computers on the one hand and can also create a digital highway for launching cyberattacks on the other hand. Among the targets are the Chinese and Russian military, but also trade institutions within the European Union. The NYT also reported that the NSA can access and alter computers which are not connected with the internet by a secret technology in use by the NSA since 2008. The prerequisite is the physically insertion of the radio frequency hardware by a spy, a manufacturer or an unwitting user. The technology relies on a covert channel of radio waves that can be transmitted from tiny circuit boards and USB cards inserted surreptitiously into the computers. In some cases, they are sent to a briefcase-size relay station that intelligence agencies can set up miles away from the target. The technology can also transmit malware back to the infected computer.\n\nChannel 4 and \"The Guardian\" revealed the existence of Dishfire, a massive database of the NSA that collects hundreds of millions of text messages on a daily basis. The GCHQ has been given full access to the database, which it uses to obtain personal information of Britons by exploiting a legal loophole.\n\nEach day, the database receives and stores the following amounts of data:\n\n\nThe database is supplemented with an analytical tool known as the Prefer program, which processes SMS messages to extract other types of information including contacts from missed call alerts.\nAccording to a joint diclosure by \"the New York Times\", \"the Guardian\", and ProPublica, the NSA and the GCHQ have begun working together to collect and store data from dozens of smartphone application software by 2007 at the latest. A 2008 GCHQ report leaked by Snowden asserts that \"anyone using Google Maps on a smartphone is working in support of a GCHQ system\". The NSA and the GCHQ have traded recipes for various purposes such as grabbing location data and journey plans that are made when a target uses Google Maps, and vacuuming up address books, buddy lists, phone logs and geographic data embedded in photos posted on the mobile versions of numerous social networks such as Facebook, Flickr, LinkedIn, Twitter and other services. In a separate 20-page report dated 2012, the GCHQ cited the popular smarphone game \"Angry Birds\" as an example of how an application could be used to extract user data. Taken together, such forms of data collection would allow the agencies to collect vital information about a user's life, including his or her home country, current location (through geolocation), age, gender, ZIP code, marital status, income, ethnicity, sexual orientation, education level, number of children, etc.\n\nA GCHQ document dated August 2012 provided details of the Squeaky Dolphin surveillance program, which enables the GCHQ to conduct broad, real-time monitoring of various social media features and social media traffic such as YouTube video views, the Like button on Facebook, and Blogspot/Blogger visits without the knowledge or consent of the companies providing those social media features. The agency’s “Squeaky Dolphin” program can collect, analyze and utilize YouTube, Facebook and Blogger data in specific situations in real time for analysis purposes. The program also collects the addresses from the billion of videos watched daily as well as some user information for analysis purposes.\n\nDuring the 2009 United Nations Climate Change Conference in Copenhagen, the NSA and its Five Eyes partners monitored the communications of delegates of numerous countries. This was done to give their own policymakers a negotiating advantage.\n\nThe Communications Security Establishment Canada (CSEC) has been tracking Canadian air passengers via free Wi-Fi services at a major Canadian airport. Passengers who exited the airport terminal continued to be tracked as they showed up at other Wi-Fi locations across Canada. In a CSEC document dated May 2012, the agency described how it had gained access to two communications systems with over 300,000 users in order to pinpoint a specific imaginary target. The operation was executed on behalf of the NSA as a trial run to test a new technology capable of tracking down \"any target that makes occasional forays into other cities/regions.\" This technology was subsequently shared with Canada's Five Eyes partners - Australia, New Zealand, Britain, and the United States.\n\nAccording to research by \"Süddeutsche Zeitung\" and TV network NDR the mobile phone of former German chancellor Gerhard Schröder was monitored from 2002 onwards, reportedly because of his government's opposition to military intervention in Iraq. The source of the latest information is a document leaked by NSA whistleblower Edward Snowden. The document, containing information about the National Sigint Requirement List (NSRL), had previously been interpreted as referring only to Angela Merkel's mobile. However \"Süddeutsche Zeitung\" and NDR claim to have confirmation from NSA insiders that the surveillance authorisation pertains not to the individual, but the political post – which in 2002 was still held by Schröder. According to research by the two media outlets, Schröder was placed as number 388 on the list, which contains the names of persons and institutions to be put under surveillance by the NSA.\n\nThe GCHQ launched a cyber-attack on the activist network \"Anonymous\", using denial-of-service attack (DoS) to shut down a chatroom frequented by the network's members and to spy on them. The attack, dubbed Rolling Thunder, was conducted by a GCHQ unit known as the Joint Threat Research Intelligence Group (JTRIG). The unit successfully uncovered the true identities of several Anonymous members.\n\nThe NSA Section 215 bulk telephony metadata program which seeks to stockpile records on all calls made in the U.S. is collecting less than 30 percent of all Americans’ call records because of an inability to keep pace with the explosion in cellphone use according to the Washington Post.. The controversial program permits the NSA after a warrant granted by the secret Foreign Intelligence Surveillance Court to record numbers, length and location of every call from the participating carriers in.\n\nThe NSA has built an infrastructure which enables it to covertly hack into computers on a mass scale by using automated systems that reduce the level of human oversight in the process. The NSA relies on an automated system codenamed TURBINE which in essence enables the automated management and control of a large network of implants (a form of remotely transmitted malware on selected individual computer devices or in bulk on tens of thousands of devices). As quoted by \"The Intercept\", TURBINE is designed to \"allow the current implant network to scale to large size (millions of implants) by creating a system that does automated control implants by groups instead of individually.\" The NSA has shared many of its files on the use of implants with its counterparts in the so-called Five Eyes surveillance alliance – the United Kingdom, Canada, New Zealand, and Australia.\n\nAmong other things due to TURBINE and its control over the implants the NSA is capable of:\n\n\nThe TURBINE implants are linked to, and relies upon, a large network of clandestine surveillance \"sensors\" that the NSA has installed at locations across the world, including the agency's headquarters in Maryland and eavesdropping bases used by the agency in Misawa, Japan and Menwith Hill, England. Codenamed as TURMOIL, the sensors operate as a sort of high-tech surveillance dragnet, monitoring packets of data as they are sent across the Internet. When TURBINE implants exfiltrate data from infected computer systems, the TURMOIL sensors automatically identify the data and return it to the NSA for analysis. And when targets are communicating, the TURMOIL system can be used to send alerts or \"tips\" to TURBINE, enabling the initiation of a malware attack. To identify surveillance targets, the NSA uses a series of data \"selectors\" as they flow across Internet cables. These selectors can include email addresses, IP addresses, or the unique \"cookies\" containing a username or other identifying information that are sent to a user's computer by websites such as Google, Facebook, Hotmail, Yahoo, and Twitter, unique Google advertising cookies that track browsing habits, unique encryption key fingerprints that can be traced to a specific user, and computer IDs that are sent across the Internet when a Windows computer crashes or updates.\n\nThe CIA was accused by U.S. Senate Intelligence Committee Chairwoman Dianne Feinstein of spying on a stand-alone computer network established for the committee in its investigation of allegations of CIA abuse in a George W. Bush-era detention and interrogation program.\n\nA voice interception program codenamed MYSTIC began in 2009. Along with RETRO, short for \"retrospective retrieval\" (RETRO is voice audio recording buffer that allows retrieval of captured content up to 30 days into the past), the MYSTIC program is capable of recording \"100 percent\" of a foreign country's telephone calls, enabling the NSA to rewind and review conversations up to 30 days and the relating metadata. With the capability to store up to 30 days of recorded conversations MYSTIC enables the NSA to pull an instant history of the person's movements, associates and plans.\n\nOn March 21, \"Le Monde\" published slides from an internal presentation of the Communications Security Establishment Canada, which attributed a piece of malicious software to French intelligence. The CSEC presentation concluded that the list of malware victims matched French intelligence priorities and found French cultural reference in the malware's code, including the name Babar, a popular French children's character, and the developer name \"Titi\".\n\nThe French telecommunications corporation Orange S.A. shares its call data with the French intelligence agency DGSE, which hands over the intercepted data to GCHQ.\n\nThe NSA has spied on the Chinese technology company Huawei. Huawei is a leading manufacturer of smartphones, tablets, mobile phone infrastructure, and WLAN routers and installs fiber optic cable. According to \"Der Spiegel\" this \"kind of technology […] is decisive in the NSA's battle for data supremacy.\" The NSA, in an operation named \"Shotgiant\", was able to access Huawei's email archive and the source code for Huawei's communications products. The US government has had longstanding concerns that Huawei may not be independent of the People's Liberation Army and that the Chinese government might use equipment manufactured by Huawei to conduct cyberespionage or cyberwarfare. The goals of the NSA operation were to assess the relationship between Huawei and the PLA, to learn more the Chinese government's plans and to use information from Huawei to spy on Huawei's customers, including Iran, Afghanistan, Pakistan, Kenya, and Cuba. Former Chinese President Hu Jintao, the Chinese Trade Ministry, banks, as well as telecommunications companies were also targeted by the NSA.\n\n\"The Intercept\" published a document of an NSA employee discussing how to build a database of IP addresses, webmail, and Facebook accounts associated with system administrators so that the NSA can gain access to the networks and systems they administer.\n\nAt the end of March 2014, Der Spiegel and \"The Intercept\" published, based on a series of classified files from the archive provided to reporters by NSA whistleblower Edward Snowden, articles related to espionage efforts by GCHQ and NSA in Germany. The British GCHQ targeted three German internet firms for information about Internet traffic passing through internet exchange points, important customers of the German internet providers, their technology suppliers as well as future technical trends in their business sector and company employees. The NSA was granted by the Foreign Intelligence Surveillance Court the authority for blanket surveillance of Germany, its people and institutions, regardless whether those affected are suspected of having committed an offense or not, without an individualized court order specifying on March 7, 2013. In addition Germany's chancellor Angela Merkel was listed in a surveillance search machine and database named Nymrod along with 121 others foreign leaders. As \"The Intercept\" wrote: \"The NSA uses the Nymrod system to 'find information relating to targets that would otherwise be tough to track down,' according to internal NSA documents. Nymrod sifts through secret reports based on intercepted communications as well as full transcripts of faxes, phone calls, and communications collected from computer systems. More than 300 'cites' for Merkel are listed as available in intelligence reports and transcripts for NSA operatives to read.\"\n\nTowards the end of April, Edward Snowden said that the United States surveillance agencies spy on Americans more than anyone else in the world, contrary to anything that has been said by the government up until this point.\n\nAn article published by Ars Technica shows NSA's Tailored Access Operations (TAO) employees intercepting a Cisco router.\n\n\"The Intercept\" and WikiLeaks revealed information about which countries were having their communications collected as part of the MYSTIC surveillance program. On May 19, \"The Intercept\" reported that the NSA is recording and archiving nearly every cell phone conversation in the Bahamas with a system called SOMALGET, a subprogram of MYSTIC. The mass surveillance has been occurring without the Bahamian government's permission. Aside from the Bahamas, \"The Intercept\" reported NSA interception of cell phone metadata in Kenya, the Philippines, Mexico and a fifth country it did not name due to \"credible concerns that doing so could lead to increased violence.\" WikiLeaks released a statement on May 23 claiming that Afghanistan was the unnamed nation.\n\nIn a statement responding to the revelations, the NSA said \"the implication that NSA's foreign intelligence collection is arbitrary and unconstrained is false.\"\n\nThrough its global surveillance operations the NSA exploits the flood of images included in emails, text messages, social media, videoconferences and other communications to harvest millions of images. These images are then used by the NSA in sophisticated facial recognition programs to track suspected terrorists and other intelligence targets.\n\nVodafone revealed that there were secret wires that allowed government agencies direct access to their networks. This access does not require warrants and the direct access wire is often equipment in a locked room. In six countries where Vodafone operates, the law requires telecommunication companies to install such access or allows governments to do so. Vodafone did not name these countries in case some governments retaliated by imprisoning their staff. Shami Chakrabarti of Liberty said \"For governments to access phone calls at the flick of a switch is unprecedented and terrifying. Snowden revealed the internet was already treated as fair game. Bluster that all is well is wearing pretty thin – our analogue laws need a digital overhaul.\" Vodafone published its first Law Enforcement Disclosure Report on June 6, 2014. Vodafone group privacy officer Stephen Deadman said \"These pipes exist, the direct access model exists. We are making a call to end direct access as a means of government agencies obtaining people's communication data. Without an official warrant, there is no external visibility. If we receive a demand we can push back against the agency. The fact that a government has to issue a piece of paper is an important constraint on how powers are used.\" Gus Hosein, director of Privacy International said \"I never thought the telcos would be so complicit. It's a brave step by Vodafone and hopefully the other telcos will become more brave with disclosure, but what we need is for them to be braver about fighting back against the illegal requests and the laws themselves.\"\n\nAbove-top-secret documentation of a covert surveillance program named Overseas Processing Centre 1 (OPC-1) (codenamed \"CIRCUIT\") by GCHQ was published by \"The Register\". Based on documents leaked by Edward Snowden, GCHQ taps into undersea fiber optic cables via secret spy bases near the Strait of Hormuz and Yemen. BT and Vodafone are implicated.\n\nThe Danish newspaper Dagbladet Information and \"The Intercept\" revealed on June 19, 2014, the NSA mass surveillance program codenamed RAMPART-A. Under RAMPART-A, 'third party' countries tap into fiber optic cables carrying the majority of the world's electronic communications and are secretly allowing the NSA to install surveillance equipment on these fiber-optic cables. The foreign partners of the NSA turn massive amounts of data like the content of phone calls, faxes, e-mails, internet chats, data from virtual private networks, and calls made using Voice over IP software like Skype over to the NSA. In return these partners receive access to the NSA's sophisticated surveillance equipment so that they too can spy on the mass of data that flows in and out of their territory. Among the partners participating in the NSA mass surveillance program are Denmark and Germany.\n\nDuring the week of July 4, a 31-year-old male employee of Germany's intelligence service BND was arrested on suspicion of spying for the United States. The employee is suspected of spying on the German Parliamentary Committee investigating the NSA spying scandal.\n\nFormer NSA official and whistleblower William Binney spoke at a Centre for Investigative Journalism conference in London. According to Binney, \"at least 80% of all audio calls, not just metadata, are recorded and stored in the US. The NSA lies about what it stores.\" He also stated that the majority of fiber optic cables run through the U.S., which \"is no accident and allows the US to view all communication coming in.\"\n\n\"The Washington Post\" released a review of a cache provided by Snowden containing roughly 160,000 text messages and e-mails intercepted by the NSA between 2009 and 2012. The newspaper concluded that nine out of ten account holders whose conversations were recorded by the agency \"were not the intended surveillance targets but were caught in a net the agency had cast for somebody else.\" In its analysis, \"The Post\" also noted that many of the account holders were Americans.\n\nOn July 9, a soldier working within Germany's Federal Ministry of Defence (BMVg) fell under suspicion of spying for the United States. As a result of the July 4 case and this one, the German government expelled the CIA station chief in Germany on July 17.\n\nOn July 18, former State Department official John Tye released an editorial in \"The Washington Post\", highlighting concerns over data collection under Executive Order 12333. Tye's concerns are rooted in classified material he had access to through the State Department, though he has not publicly released any classified materials.\n\n\"The Intercept\" reported that the NSA is \"secretly providing data to nearly two dozen U.S. government agencies with a 'Google-like' search engine\" called ICREACH. The database, \"The Intercept\" reported, is accessible to domestic law enforcement agencies including the FBI and the Drug Enforcement Administration and was built to contain more than 850 billion metadata records about phone calls, emails, cellphone locations, and text messages.\n\nBased on documents obtained from Snowden, \"The Intercept\" reported that the NSA and GCHQ had broken into the internal computer network of Gemalto and stolen the encryption keys that are used in SIM cards no later than 2010. , the company is the world's largest manufacturer of SIM cards, making about two billion cards a year. With the keys, the intelligence agencies could eavesdrop on cell phones without the knowledge of mobile phone operators or foreign governments.\n\n\"The New Zealand Herald\", in partnership with \"The Intercept\", revealed that the New Zealand government used XKeyscore to spy on candidates for the position of World Trade Organization director general and also members of the Solomon Islands government.\n\nIn January 2015, the DEA revealed that it had been collecting metadata records for all telephone calls made by Americans to 116 countries linked to drug trafficking. The DEA's program was separate from the telephony metadata programs run by the NSA. In April, \"USA Today\" reported that the DEA's data collection program began in 1992 and included all telephone calls between the United States and from Canada and Mexico. Current and former DEA officials described the program as the precursor of the NSA's similar programs. The DEA said its program was suspended in September 2013, after a review of the NSA's programs and that it was \"ultimately terminated.\"\n\n\n"}
{"id": "30468705", "url": "https://en.wikipedia.org/wiki?curid=30468705", "title": "UCI World Ranking", "text": "UCI World Ranking\n\nFor the 2016 season the UCI revamped the points system used to rank men's road cycling riders. The following table summarises the new rankings, how points are scored towards them and how points are scaled. \n\nThe following is a list of riders and nations that achieved the number one position at the end of the year:\n\nSource:\n\n"}
{"id": "6629974", "url": "https://en.wikipedia.org/wiki?curid=6629974", "title": "WHO Collaborating Centres", "text": "WHO Collaborating Centres\n\nWorld Health Organization Collaborating Centres are institutions such as research institutes, parts of universities or academies from \"over 700 institutions in 80 countries\" that work with WHO in disciplines such as occupational health, food safety, and communicable disease prevention. The participating institutions partner with WHO to perform research, provide training, or offer other services in furthering the WHO health agenda. These partners are designated by the WHO director-general as a part of a collaborative network. By using networks of established organizations, WHO is able to strengthen the scientific validity of its work and lower the costs of research.\n\nThe World Health Organization has established networks related to a variety of health topics. For example, WHO has put in place centres focused on organ transplants, hearing loss prevention, hepatitis, leprosy, ethics, and maternal health. To move the work forward, WHO has numerous designated centres in each inhabited continent. The network of centres for reference and research on influenza draws upon resources from Japan, the United States, the United Kingdom, and Australia. The network of WHO collaborating centres in occupational health is chaired by Dr. John Howard, director of the U.S. National Institute for Occupational Safety and Health, and contains more than 60 designated organizations from across the globe.\n"}
{"id": "3157429", "url": "https://en.wikipedia.org/wiki?curid=3157429", "title": "WTA Rankings", "text": "WTA Rankings\n\nThe WTA Rankings are the ratings defined by the Women's Tennis Association, introduced in November 1975.\n\nThe WTA rankings are based on a rolling 52-week, cumulative system. A player's ranking is determined by her results at a maximum of 16 tournaments for singles and 11 for doubles and points are awarded based on how far a player advances in a tournament. The basis for calculating a player's ranking are those tournaments that yield the highest ranking points during the rolling 52-week period with the condition that they must include points from the 4 Grand Slams, the 4 Premier Mandatory tournaments and the WTA Finals. In addition, for Top 20 players, their best two results at Premier 5 tournaments will also count. Up until 2016, the WTA also distributed ranking points, for singles players only, who competed at the Summer Olympics. However, this has since been discontinued.\n\nThe points distribution for tournaments in 2017 is shown below. Points earned in 2013 were a little different in some cases and retained their value until they expired after 52 weeks.\n\n\"+H\" indicates that Hospitality is provided.\n\nIn ITF tournaments, the main draw is normally 32 for singles and 16 for doubles. Losers in the first round of doubles will receive points equal to that shown in the R32 column above. For subsequent rounds (quarter-finals onwards) the points are the same as for singles.\n\nThe following is a chronological list of players who have achieved the number one position in singles since the WTA began producing computerized rankings on November 3, 1975 (active players in green):\n\n\"Last update: 19 November 2018\"\n\nThe year-end number one player is the player at the head of the WTA rankings following the completion of the final tournament of the calendar year.\n\nThe following is a list of singles players who were ranked world No. 5 or higher but not No. 1 (active players in green):\n\n\n"}
{"id": "7827316", "url": "https://en.wikipedia.org/wiki?curid=7827316", "title": "World Administrative Radio Conference", "text": "World Administrative Radio Conference\n\nThe World Administrative Radio Conference (WARC) was a technical conference of the International Telecommunication Union (ITU) where delegates from member nations of the ITU met to revise or amend the entire international Radio Regulations pertaining to all telecommunication services throughout the world. The conference was held in Geneva, Switzerland, with preparatory conferences held in Panama City, Panama.\n\nIn 1992 at an \"Additional Plenipotentiary Conference\" in Geneva the ITU was restructured and as a result from 1993 the conference became known as the World Radiocommunication Conference or WRC. \n\n\n"}
{"id": "6872117", "url": "https://en.wikipedia.org/wiki?curid=6872117", "title": "World Englishes", "text": "World Englishes\n\nWorld Englishes is a term for emerging localized or indigenized varieties of English, especially varieties that have developed in territories influenced by the United Kingdom or the United States. The study of World Englishes consists of identifying varieties of English used in diverse sociolinguistic contexts globally and analyzing how sociolinguistic histories, multicultural backgrounds and contexts of function influence the use of English in different regions of the world.\n\nThe issue of World Englishes was first raised in 1978 to examine concepts of regional Englishes globally. Pragmatic factors such as appropriateness, comprehensibility and interpretability justified the use of English as an international and intra-national language. In 1988, at a Teachers of English to Speakers of Other Languages (TESOL) conference in Honolulu, Hawaii, the International Committee of the Study of World Englishes (ICWE) was formed. In 1992, the ICWE formally launched the International Association for World Englishes (IAWE) at a conference of \"World Englishes Today\", at the University of Illinois, USA. There is now an academic journal devoted to the study of this topic, titled \"World Englishes\".\n\nCurrently, there are approximately 75 territories where English is spoken either as a first language (L1) or as an unofficial or institutionalized second language (L2) in fields such as government, law and education. It is difficult to establish the total number of Englishes in the world, as new varieties of English are constantly being developed and discovered.\n\nThe notions of World English and World Englishes are far from similar, although the terms are often mistakenly used interchangeably. \"World English\" refers to the English language as a lingua franca used in business, trade, diplomacy and other spheres of global activity, while \"World Englishes\" refers to the different varieties of English and English-based creoles developed in different regions of the world. Alternatively, the term \"Global Englishes\" has been used by scholars in the field to emphasise the more recent spread of English due to globalization, which has resulted in increased usage of English as a lingua franca.\n\nEnglish is a West Germanic language that originated from the Anglo-Frisian dialects brought by Germanic invaders into Britain. Initially, Old English was a diverse group of dialects, reflecting the varied origins of the Anglo-Saxon kingdoms of England. Eventually, one of these dialects, Late West Saxon, came to dominate.\n\nThe original Old English language was then influenced by two further waves of invasion: the first by speakers of the Scandinavian branch of the Germanic language family, who conquered and colonized parts of Britain in the 8th and 9th centuries; the second by the Normans in the 11th century, who spoke Old Norman and ultimately developed a Norman variety called Anglo-Norman. For two centuries after the Norman Conquest, French became the language of everyday life among the upper classes in England. Although the language of the masses remained English, the bilingual character of England in this period was thus formed.\n\nDuring the Middle English period, France and England experienced a process of separation. This period of conflicting interests and feelings of resentment was later termed the Hundred Years' War. By the beginning of the 14th century, English had regained universal use and become the principal tongue of all England, but not without having undergone significant change.\n\nDuring the Renaissance, patriotic feelings regarding English brought about the recognition of English as the national language of England. The language was advocated as acceptable for learned and literary use. With the Great Vowel Shift, the language in this period matured to a standard and differed significantly from the Middle English period, becoming recognizably \"modern\".\n\nBy the 18th century, three main forces were driving the direction of the English language: (1) to reduce the language to rule and effect a standard of correct usage; (2) to refine the language by removing supposed defects and introducing certain improvements; and (3) to fix English permanently in the desired form. This desire for system and regularity in the language contrasted with the individualism and spirit of independence characterized by the previous age.\n\nBy the 19th century, the expansion of the British Empire, as well as global trade, had led to the spread of English around the world. The rising importance of some of England's larger colonies and former colonies, such as the rapidly developing United States, enhanced the value of the English varieties spoken in these regions, encouraging the belief, among the local populations, that their distinct varieties of English should be granted equal standing with the standard of Great Britain.\n\nThe first diaspora involved relatively large-scale migrations of mother-tongue English speakers from England, Scotland and Ireland predominantly to North America and the Caribbean, Australia, South Africa and New Zealand. Over time, their own English dialects developed into modern American, Canadian, West Indian, South African, Australian, and New Zealand Englishes. In contrast to the English of Great Britain, the varieties spoken in modern North America and Caribbean, South Africa, Australia, and New Zealand have been modified in response to the changed and changing sociolinguistic contexts of the migrants, for example being in contact with indigenous Native American, Khoisan and Bantu, Aboriginal or Maori populations in the colonies.\n\nThe second diaspora was the result of the colonization of Asia and Africa, which led to the development of 'New Englishes', the second-language varieties of English. In colonial Africa, the history of English is distinct between West and East Africa. English in West Africa began with trade. particularly the slave trade. English soon gained official status in what are today Gambia, Sierra Leone, Ghana, Nigeria and Cameroon, and some of the pidgin and creoles which developed from English contact, including Krio (Sierra Leone) and Cameroon Pidgin, have large numbers of speakers now.\n\nAs for East Africa, extensive British settlements were established in what are now Kenya, Uganda, Tanzania, Malawi, Zambia and Zimbabwe, where English became a crucial language of the government, education and the law. From the early 1960s, the six countries achieved independence in succession; but English remained the official language and had large numbers of second language speakers in Uganda, Zambia, Zimbabwe and Malawi (along with Chewa).\n\nEnglish was formally introduced to the sub-continent of South Asia (India, Bangladesh, Pakistan, Sri Lanka, Nepal and Bhutan) during the second half of the eighteenth century. In India, English was given status through the implementation of Macaulay 'Minute' of 1835, which proposed the introduction of an English educational system in India. Over time, the process of 'Indianisation' led to the development of a distinctive national character of English in the Indian sub-continent.\n\nBritish influence in South-East Asia and the South Pacific began in the late eighteenth century, involving primarily the territories now known as Singapore, Malaysia and Hong Kong. Papua New Guinea, also a British protectorate, exemplified the English-based pidgin - Tok Pisin.\n\nThe Americans came late in South-East Asia but their influence spread like wildfire as their reforms on education in the Philippines progressed in their less than half a century colonization of the islands. English has been taught since the American period and is one of the official languages of the Philippines. Ever since English became the official language, a localized variety gradually emerged - Philippine English. Lately, linguist Wilkinson Daniel Wong Gonzales argued that this variety has in itself more varieties, suggesting that we move towards Philippine Englishes paradigm to progress further in Schneider's dynamic model after gathering evidences of such happening.\n\nNowadays, English is also learnt in other countries in neighbouring areas, most notably in Taiwan, Japan and Korea, with the latter two having begun to consider the possibility of making English their official second language.\n\nThe spread of English around the world is often discussed in terms of three distinct groups of users, where English is used respectively as:\n\n\n The most influential model of the spread of English is Braj Kachru's model of World Englishes. In this model the diffusion of English is captured in terms of three Concentric Circles of the language: The Inner Circle, the Outer Circle, and the Expanding Circle.\n\nThe Inner Circle refers to English as it originally took shape and was spread across the world in the first diaspora. In this transplantation of English, speakers from England carried the language to Australia, New Zealand and North America. The Inner Circle thus represents the traditional historical and sociolinguistic bases of English in regions where it is now used as a primary language: the United Kingdom, the United States, Australia, New Zealand, Ireland, anglophone Canada and South Africa, and some of the Caribbean territories. English is the native language or mother tongue of most people in these countries. The total number of English speakers in the inner circle is as high as 380 million, of whom some 120 million are outside the United States.\n\nThe Outer Circle of English was produced by the second diaspora of English, which spread the language through imperial expansion by Great Britain in Asia and Africa. In these regions, English is not the native tongue, but serves as a useful lingua franca between ethnic and language groups. Higher education, the legislature and judiciary, national commerce and so on may all be carried out predominantly in English. This circle includes India, Nigeria, Bangladesh, Pakistan, Malaysia, Tanzania, Kenya, non-Anglophone South Africa, the Philippines (colonized by the US) and others. The total number of English speakers in the outer circle is estimated to range from 150 million to 300 million. Singapore, while in the Outer Circle, may be drifting into the Inner Circle as English becomes more often used as a home language (see Languages of Singapore), much as Ireland did earlier. Countries where most people speak an English-based creole and retain standard English for official purposes, such as Jamaica and Papua New Guinea, are also in the Outer Circle.\n\nFinally, the Expanding Circle encompasses countries where English plays no historical or governmental role, but where it is nevertheless widely used as a medium of international communication. This includes much of the rest of the world's population not categorized above, including territories such as China, Russia, Japan, non-Anglophone Europe (especially the Netherlands and Nordic countries), South Korea, Egypt and Indonesia. The total in this expanding circle is the most difficult to estimate, especially because English may be employed for specific, limited purposes, usually in a business context. The estimates of these users range from 100 million to one billion.\n\nThe inner circle (UK, US etc.) is 'norm-providing'; that means that English language norms are developed in these countries. The outer circle (mainly New Commonwealth countries) is 'norm-developing'. The expanding circle (which includes much of the rest of the world) is 'norm-dependent', because it relies on the standards set by native speakers in the inner circle.\n\nEdgar Werner Schneider tries to avoid a purely geographical and historical approach evident in the 'circles' models and incorporates sociolinguistic concepts pertaining to acts of identity.\nHe outlines five characteristic stages in the spread of English:\n\nPhase 1 - Foundation: This is the initial stage of the introduction of English to a new territory over an extended period of time. Two linguistic processes are operative at this stage: (a) language contact between English and indigenous languages; (b) contact between different dialects of English of the settlers which eventually results in a new stable dialect (see koiné). At this stage, bilingualism is marginal. A few members of the local populace may play an important role as interpreters, translators and guides. Borrowings are limited to lexical items; with local place names and terms for local fauna and flora being adopted by the English.\n\nPhase 2 - Exonormative stabilization: At this stage, the settler communities tend to stabilize politically under British rule. English increases in prominence and though the colloquial English is a colonial koiné, the speakers look to England for their formal norms. Local vocabulary continues to be adopted. Bilingualism increases amongst the indigenous population through education and increased contacts with English settlers. Knowledge of English becomes an asset, and a new indigenous elite develops.\n\nPhase 3 - Nativisation: According to Schneider, this is the stage at which a transition occurs as the English settler population starts to accept a new identity based on present and local realities, rather than sole allegiance to their 'mother country'. By this time, the indigenous strand has also stabilized an L2 system that is a synthesis of substrate effects, interlanguage processes and features adopted from the settlers' koiné English. Neologisms stabilize as English is made to adapt to local sociopolitical and cultural practices.\n\nPhase 4 - Endonormative stabilization: This stage is characterized by the gradual acceptance of local norms, supported by a new locally rooted linguistic self-confidence. By this time political events have made it clear that the settler and indigenous strands are inextricably bound in a sense of nationhood independent of Britain. Acceptance of local English(es) expresses this new identity. National dictionaries are enthusiastically supported, at least for new lexis (and not always for localized grammar). Literary creativity in local English begins to flourish.\n\nPhase 5 - Differentiation: At this stage there is a change in the dynamics of identity as the young nation sees itself as less defined by its differences from the former colonial power as a composite of subgroups defined on regional, social and ethnic lines. Coupled with the simple effects of time in effecting language change (with the aid of social differentiation) the new English koiné starts to show greater differentiation.\n\nThe oldest map of the spread of English is Strevens's world map of English. His world map, even predating that of Kachru's three circles, showed that since American English became a separate variety from British English, all subsequent Englishes have had affinities with either one or the other.\n\nMcArthur's \"wheel model\" has an idealized central variety called \"World Standard English,\" which is best represented by \"written international English.\" The next circle is made of regional standards or standards that are emerging. Finally, the outer layer consists of localized varieties which may have similarities with the regional standards or emerging standards.\n\nAlthough the model is neat, it raises several problems. Firstly, the three different types of English — ENL, ESL and EFL, are conflated in the second circle. Secondly, the multitude of Englishes in Europe are also missing in this layer. Finally, the outside layer includes pidgins, creoles and L2 Englishes. Most scholars would argue that English pidgins and creoles do not belong to one family: rather they have overlapping multiple memberships.\n\nManfred Görlach's and McArthur's models are reasonably similar. Both exclude English varieties in Europe. As Görlach does not include EFLs at all, his model is more consistent, though less comprehensive. Outside the circle are mixed varieties (pidgins, creoles and mixed languages involving English), which are better categorized as having partial membership.\n\nIn Modiano's model of English, the center consists of users of English as an International Language, with a core set of features which are comprehensible to the majority of native and competent non-native speakers of English. The second circle consists of features which may become internationally common or may fall into obscurity. Finally, the outer area consists of five groups (American English, British English, other major varieties, local varieties, foreign varieties) each with features peculiar to their own speech community and which are unlikely to be understood by most members of the other four groups.\n\nThe World Englishes paradigm is not static, and neither are rapidly changing realities of language use worldwide. The use of English in the Outer and Expanding Circle societies (refer to Kachru's Three Circles of English) continues its rapid spread, while at the same time new patterns of language contact and variety differentiation emerge. The different varieties range from English in the Inner circle societies such as the United States, Canada, South Africa, Australia and New Zealand, to the Outer circle post-colonial societies of Asia and Africa.\nThe World Englishes initiative, in recognizing and describing the New Englishes of the Caribbean, Africa and Asia, has been partly motivated by a consideration of the local linguistic factors and partly by a consideration of the wider cultural and political contexts of language acquisition and use. This, in turn, has involved the creative rewriting of discourses towards a recognition of pluralism and multiple possibilities for scholarship. The notion of varieties in this context is similarly dynamic, as new contexts, new realities, new discourses, and new varieties continue to emerge.\n\nThe terms \"language\" and \"dialect\" are not easily defined concepts. It is often suggested that languages are autonomous, while dialects are heteronomous. It is also said that dialects, in contrast with languages, are mutually intelligible, though this is not always the case. Dialects are characteristically spoken, do not have a codified form and are used only in certain domains.\nIn order to avoid the difficult dialect-language distinction, linguists tend to prefer a more neutral term, \"variety\", which covers both concepts and is not clouded by popular usage. This term is generally used when discussing World Englishes.\n\nTwo scenarios have been advanced about English's future status as the major world language: it will ultimately fragment into a large number of mutually unintelligible varieties (in effect, languages), or it will converge so that differences across groups of speakers are largely eliminated.\n\nIf English is, numerically speaking, the language of 'others', then the center of gravity of the language is almost certain to shift in the direction of the 'others'. In the words of Widdowson, there is likely to be a paradigm shift from one of language distribution to one of language spread:\nIn this new paradigm, English spreads and adapts according to the linguistic and cultural preferences of its users in the Outer and Expanding circles (refer to Kachru's Three Circles of English). However, if English is genuinely to become the language of 'others', then the 'others' have to be accorded – or perhaps more likely, accord themselves – at least the same English language rights as those claimed by mother-tongue speakers.\n\nThe other potential shift in the linguistic center of gravity is that English could lose its international role altogether, or come to share it with a number of equals. Although this would not happen mainly as a result of native-speaker resistance to the spread of non-native speaker Englishes and the consequent abandoning of English by large numbers of non-native speakers, the latter could play a part.\n\nAs evidence that English may eventually give way to another language (or languages) as the world's lingua franca, David Crystal cites Internet data:\n\nOn the other hand, there are at least 1500 languages present on the internet now and that figure is likely to increase. Nevertheless, Crystal predicts that English will retain its dominant presence.\n\n"}
{"id": "23486603", "url": "https://en.wikipedia.org/wiki?curid=23486603", "title": "World Health Imaging, Telemedicine, and Informatics Alliance", "text": "World Health Imaging, Telemedicine, and Informatics Alliance\n\nThe World Health Imaging, Telemedicine and Informatics Alliance (WHITIA) is a non-profit global health technology and social venture established in 2006 by affiliates of Northwestern University near Chicago, Illinois. WHITIA cultivates high-level strategic relationships with non-governmental organizations, imaging industry innovators and academic institutions in order to integrate and deliver meaningful, sustainable, diagnostic technology to underserved communities worldwide. WHITIA's vision is to facilitate the deployment of thousands of digital medical imaging systems worldwide, providing one billion people with access to diagnostic imaging. WHITIA was formerly known as the World Health Imaging Alliance (WHIA) until it formally expanded its scope in June 2009.\n\nWHITIA's first formal public launch was in April 2009 at the Healthcare Information and Management Systems Society (HIMSS) Annual Conference & Exhibition in Chicago, Illinois. WHITIA announced strategic partners including SEDECAL, Carestream Health and Merge Healthcare, receiving extensive coverage in Health IT magazines and publications. At the 2009 Annual Conference of the Society for Imaging Informatics in Medicine (SIIM) in Charlotte, North Carolina, WHITIA announced its partnership with SIIM, which will allow both organizations to collaborate on specific initiatives. WHITIA was recently ranked #15 of the top 25 most influential people, institutions, and organizations in the radiology industry.\n\nAt the 2009 RSNA Annual Meeting, WHITIA launched, Remi-d, a remote-operated screening X-ray system for use in the developing world. Its strengths in these areas stem from the higher burden of Human Immunodeficiency Virus (HIV) and Tuberculosis (TB) co-infection, high incidences of Black Lung disease, or outbreaks of other infectious respiratory diseases. The teleradiology and remote-controlled features of Remi-d allow resource-limited areas such as sub-Saharan Africa, South and Central America and Southeast Asia, where Radiologists and Radiographers are in short supply to have a functioning X-ray service.\n\nWHITIA currently has pilot integrated digital X-ray sites in South Africa and Guatemala at established clinics in need and is expanding to new qualified sites in partnership with NGOs such as Rotary International while cooperating with the local and national governments.\n\nThe Guatemala pilot sites in urban Guatemala City and rural Río Hondo provide essential healthcare technology to thousands of people in the communities served. They are designed to be models for the wider expansion of the WHITIA network throughout the clinics in need in urban and rural Guatemala. The system's specific design for Guatemala City is an integration of some of WHITIA's partners' strengths and generosity:\n\n\nThis project was largely funded by several US and Guatemalan Rotary clubs along with the key resource support of the Guatemalan municipal and national governments.\n\n"}
{"id": "33583", "url": "https://en.wikipedia.org/wiki?curid=33583", "title": "World Health Organization", "text": "World Health Organization\n\nThe World Health Organization (WHO) is a specialized agency of the United Nations that is concerned with international public health. It was established on 7 April 1948, and is headquartered in Geneva, Switzerland. The WHO is a member of the United Nations Development Group. Its predecessor, the Health Organization, was an agency of the League of Nations.\n\nThe constitution of the World Health Organization had been signed by 61 countries on 22 July 1946, with the first meeting of the World Health Assembly finishing on 22 July 1946. It incorporated the and the League of Nations Health Organization. Since its establishment, it has played a leading role in the eradication of smallpox. Its current priorities include communicable diseases, in particular HIV/AIDS, Ebola, malaria and tuberculosis; the mitigation of the effects of non-communicable diseases such as sexual and reproductive health, development, and aging; nutrition, food security and healthy eating; occupational health; substance abuse; and driving the development of reporting, publications, and networking.\n\nThe WHO is responsible for the World Health Report, the worldwide World Health Survey, and World Health Day. The current Director-General of the WHO is Tedros Adhanom, who started his five-year term on 1 July 2017.\n\nThe International Sanitary Conferences, originally held on 23 June 1851, were the first predecessors of the WHO. A series of 14 conferences that lasted from 1851 to 1938, the International Sanitary Conferences worked to combat many diseases, chief among them cholera, yellow fever, and the bubonic plague. The conferences were largely ineffective until the seventh, in 1892; when an International Sanitary Convention that dealt with cholera was passed. Five years later, a convention for the plague was signed. In part as a result of the successes of the Conferences, the Pan-American Sanitary Bureau, and the were soon founded in 1902 and 1907, respectively. When the League of Nations was formed in 1920, they established the Health Organization of the League of Nations. After World War II, the United Nations absorbed all the other health organizations, to form the WHO.\n\nDuring the 1945 United Nations Conference on International Organization, Szeming Sze, a delegate from China, conferred with Norwegian and Brazilian delegates on creating an international health organization under the auspices of the new United Nations. After failing to get a resolution passed on the subject, Alger Hiss, the Secretary General of the conference, recommended using a declaration to establish such an organization. Sze and other delegates lobbied and a declaration passed calling for an international conference on health. The use of the word \"world\", rather than \"international\", emphasized the truly global nature of what the organization was seeking to achieve. The constitution of the World Health Organization was signed by all 51 countries of the United Nations, and by 10 other countries, on 22 July 1946. It thus became the first specialized agency of the United Nations to which every member subscribed. Its constitution formally came into force on the first World Health Day on 7 April 1948, when it was ratified by the 26th member state. The first meeting of the World Health Assembly finished on 24 July 1948, having secured a budget of (then ) for the 1949 year. Andrija Stampar was the Assembly's first president, and G. Brock Chisholm was appointed Director-General of WHO, having served as Executive Secretary during the planning stages. Its first priorities were to control the spread of malaria, tuberculosis and sexually transmitted infections, and to improve maternal and child health, nutrition and environmental hygiene. Its first legislative act was concerning the compilation of accurate statistics on the spread and morbidity of disease. The logo of the World Health Organization features the Rod of Asclepius as a symbol for healing.\n\nIn 1947 the WHO established an epidemiological information service via telex, and by 1950 a mass tuberculosis inoculation drive using the BCG vaccine was under way. In 1955, the malaria eradication programme was launched, although it was later altered in objective. 1965 saw the first report on diabetes mellitus and the creation of the International Agency for Research on Cancer.\n\nIn 1958, Viktor Zhdanov, Deputy Minister of Health for the USSR, called on the World Health Assembly to undertake a global initiative to eradicate smallpox, resulting in Resolution WHA11.54. At this point, 2 million people were dying from smallpox every year.\n\nIn 1966, the WHO moved its headquarters from the Ariana wing at the Palace of Nations to a newly constructed HQ elsewhere in Geneva.\n\nIn 1967, the World Health Organization intensified the global smallpox eradication by contributing $2.4 million annually to the effort and adopted a new disease surveillance method. The initial problem the WHO team faced was inadequate reporting of smallpox cases. WHO established a network of consultants who assisted countries in setting up surveillance and containment activities. The WHO also helped contain the last European outbreak in Yugoslavia in 1972. After over two decades of fighting smallpox, the WHO declared in 1979 that the disease had been eradicated – the first disease in history to be eliminated by human effort. Also in 1967, the WHO launched the Special Programme for Research and Training in Tropical Diseases and the World Health Assembly voted to enact a resolution on Disability Prevention and Rehabilitation, with a focus on community-driven care.\n\nIn 1974, the Expanded Programme on Immunization and the control programme of onchocerciasis was started, an important partnership between the Food and Agriculture Organization (FAO), the United Nations Development Programme (UNDP), and the World Bank.\n\nIn 1977, the first list of essential medicines was drawn up, and a year later the ambitious goal of \"Health For All\" was declared.\n\nIn 1986, the WHO began its global programme on HIV/AIDS. Two years later preventing discrimination against sufferers was attended to and in 1996 UNAIDS was formed.\n\nIn 1988, the Global Polio Eradication Initiative was established.\n\nIn 1998, WHO's Director-General highlighted gains in child survival, reduced infant mortality, increased life expectancy and reduced rates of \"scourges\" such as smallpox and polio on the fiftieth anniversary of WHO's founding. He, did, however, accept that more had to be done to assist maternal health and that progress in this area had been slow.\n\nIn 2000, the Stop TB Partnership was created along with the UN's formulation of the Millennium Development Goals. In 2001 the measles initiative was formed, and credited with reducing global deaths from the disease by 68% by 2007. In 2002, The Global Fund to Fight AIDS, Tuberculosis and Malaria was drawn up to improve the resources available. In 2006, the organization endorsed the world's first official HIV/AIDS Toolkit for Zimbabwe, which formed the basis for a global prevention, treatment and support plan to fight the AIDS pandemic.\n\nThe WHO's Constitution states that its objective \"is the attainment by all people of the highest possible level of health\".\n\nThe WHO fulfills this objective through its functions as defined in its Constitution: (a) To act as the directing and coordinating authority on international health work; (b) To establish and maintain effective collaboration with the United Nations, specialized agencies, governmental health administrations, professional groups and such other organizations as may be deemed appropriate; (c) To assist Governments, upon request, in strengthening health services; (d) To furnish appropriate technical assistance and, in emergencies, necessary aid upon the request or acceptance of Governments; (e) To provide or assist in providing, upon the request of the United Nations, health services and facilities to special groups, such as the peoples of trust territories; (f) To establish and maintain such administrative and technical services as may be required, including epidemiological and statistical services; (g) to stimulate and advance work to eradicate epidemic, endemic and other diseases; (h) To promote, in co-operation with other specialized agencies where necessary, the prevention of accidental injuries; (i) To promote, in co-operation with other specialized agencies where necessary, the improvement of nutrition, housing, sanitation, recreation, economic or working conditions and other aspects of environmental hygiene; (j) To promote co-operation among scientific and professional groups which contribute to the advancement of health; (k) To propose conventions, agreements and regulations, and make recommendations with respect to international health matters and to perform.\n\n, the WHO has defined its role in public health as follows:\n\nThe 2012–2013 WHO budget identified 5 areas among which funding was distributed. Two of those five areas related to communicable diseases: the first, to reduce the \"health, social and economic burden\" of communicable diseases in general; the second to combat HIV/AIDS, malaria and tuberculosis in particular.\n\nAs of 2015, the WHO has worked within the UNAIDS network and strived to involve sections of society other than health to help deal with the economic and social effects of HIV/AIDS. In line with UNAIDS, WHO has set itself the interim task between 2009 and 2015 of reducing the number of those aged 15–24 years who are infected by 50%; reducing new HIV infections in children by 90%; and reducing HIV-related deaths by 25%.\n\nDuring the 1970s, WHO had dropped its commitment to a global malaria eradication campaign as too ambitious, it retained a strong commitment to malaria control. WHO's Global Malaria Programme works to keep track of malaria cases, and future problems in malaria control schemes. As of 2012, the WHO was to report as to whether RTS,S/AS01, were a viable malaria vaccine. For the time being, insecticide-treated mosquito nets and insecticide sprays are used to prevent the spread of malaria, as are antimalarial drugs – particularly to vulnerable people such as pregnant women and young children.\n\nBetween 1990 and 2010, WHO's help has contributed to a 40% decline in the number of deaths from tuberculosis, and since 2005, over 46 million people have been treated and an estimated 7 million lives saved through practices advocated by WHO. These include engaging national governments and their financing, early diagnosis, standardising treatment, monitoring of the spread and effect of tuberculosis and stabilising the drug supply. It has also recognized the vulnerability of victims of HIV/AIDS to tuberculosis.\n\nIn 1988, WHO launched the Global Polio Eradication Initiative to eradicate polio. It has also been successful in helping to reduce cases by 99% since which partnered WHO with Rotary International, the US Centers for Disease Control and Prevention (CDC), the United Nations Children's Fund (UNICEF), and smaller organizations. , it has been working to immunize young children and prevent the re-emergence of cases in countries declared \"polio-free\". In 2017, a study was conducted where why Polio Vaccines may not be enough to eradicate the Virus & conduct new technology. Polio is now on the verge of extinction, thanks to a Global Vaccination Drive. the World Health Organization (WHO) stated the eradication programme has saved millions from deadly disease. \n\nAnother of the thirteen WHO priority areas is aimed at the prevention and reduction of \"disease, disability and premature deaths from chronic noncommunicable diseases, mental disorders, violence and injuries, and visual impairment\". The Division of Noncommunicable Diseases for Promoting Health through the Life-course Sexual and Reproductive Health has published the magazine, \"Entre Nous\", across Europe since 1983.\n\nThe WHO estimates that 12.6 million people died as a result of living or working in an unhealthy environment in 2012 – this accounts for nearly 1 in 4 of total global deaths. Environmental risk factors, such as air, water and soil pollution, chemical exposures, climate change, and ultraviolet radiation, contribute to more than 100 diseases and injuries. This can result in a number of pollution-related diseases.\n\n\nWHO works to \"reduce morbidity and mortality and improve health during key stages of life, including pregnancy, childbirth, the neonatal period, childhood and adolescence, and improve sexual and reproductive health and promote active and healthy aging for all individuals\".\n\nIt also tries to prevent or reduce risk factors for \"health conditions associated with use of tobacco, alcohol, drugs and other psychoactive substances, unhealthy diets and physical inactivity and unsafe sex\".\n\nThe WHO works to improve nutrition, food safety and food security and to ensure this has a positive effect on public health and sustainable development.\n\nThe WHO promotes road safety as a means to reduce traffic-related injuries.\n\nThe WHO has also worked on global initiatives in surgery, including emergency and essential surgical care, trauma care, and safe surgery. The WHO Surgical Safety Checklist is in current use worldwide in the effort to improve patient safety.\n\nThe World Health Organization's primary objective in natural and man-made emergencies is to coordinate with member states and other stakeholders to \"reduce avoidable loss of life and the burden of disease and disability.\"\n\nOn 5 May 2014, WHO announced that the spread of polio was a world health emergency – outbreaks of the disease in Asia, Africa, and the Middle East were considered \"extraordinary\".\n\nOn 8 August 2014, WHO declared that the spread of Ebola was a public health emergency; an outbreak which was believed to have started in Guinea had spread to other nearby countries such as Liberia and Sierra Leone. The situation in West Africa was considered very serious.\n\nWHO addresses government health policy with two aims: firstly, \"to address the underlying social and economic determinants of health through policies and programmes that enhance health equity and integrate pro-poor, gender-responsive, and human rights-based approaches\" and secondly \"to promote a healthier environment, intensify primary prevention and influence public policies in all sectors so as to address the root causes of environmental threats to health\".\n\nThe organization develops and promotes the use of evidence-based tools, norms and standards to support member states to inform health policy options. It oversees the implementation of the International Health Regulations, and publishes a series of medical classifications; of these, three are over-reaching \"reference classifications\": the International Statistical Classification of Diseases (ICD), the International Classification of Functioning, Disability and Health (ICF) and the International Classification of Health Interventions (ICHI). Other international policy frameworks produced by WHO include the International Code of Marketing of Breast-milk Substitutes (adopted in 1981), Framework Convention on Tobacco Control (adopted in 2003) and the Global Code of Practice on the International Recruitment of Health Personnel (adopted in 2010).\n\nIn terms of health services, WHO looks to improve \"governance, financing, staffing and management\" and the availability and quality of evidence and research to guide policy. It also strives to \"ensure improved access, quality and use of medical products and technologies\". WHO – working with donor agencies and national governments – can improve their use of and their reporting about their use of research evidence.\n\nThe remaining two of WHO's thirteen identified policy areas relate to the role of WHO itself:\n\nThe WHO along with the World Bank constitute the core team responsible for administering the International Health Partnership (IHP+). The IHP+ is a group of partner governments, development agencies, civil society and others committed to improving the health of citizens in developing countries. Partners work together to put international principles for aid effectiveness and development co-operation into practice in the health sector.\n\nThe organization relies on contributions from renowned scientists and professionals to inform its work, such as the WHO Expert Committee on Biological Standardization, the WHO Expert Committee on Leprosy, and the WHO Study Group on Interprofessional Education & Collaborative Practice.\n\nWHO runs the Alliance for Health Policy and Systems Research, targeted at improving health policy and systems.\n\nWHO also aims to improve access to health research and literature in developing countries such as through the HINARI network.\n\nWHO collaborates with the Global Fund to fight AIDS, Tuberculosis and Malaria, UNITAID, and the United States President's Emergency Plan for AIDS Relief to spearhead and fund the development of HIV programs. \n\nWHO created the Civil Society Reference Group on HIV, which brings together other networks that are involved in policy making and the dissemination of guidelines.\n\nWHO, a sector of the United Nations, partners with UNAIDS to contribute to the development of HIV responses in different areas of the world.\n\nWHO facilitates technical partnerships through the Technical Advisory Committee on HIV, which they created to develop WHO guidelines and policies.\n\nEach year, the organization marks World Health Day and other observances focusing on a specific health promotion topic. World Health Day falls on 7 April each year, timed to match the anniversary of WHO's founding. Recent themes have been vector-borne diseases (2014), healthy ageing (2012) and drug resistance (2011).\n\nThe other official global public health campaigns marked by WHO are World Tuberculosis Day, World Immunization Week, World Malaria Day, World No Tobacco Day, World Blood Donor Day, World Hepatitis Day, and World AIDS Day.\n\nAs part of the United Nations, the World Health Organization supports work towards the Millennium Development Goals. Of the eight Millennium Development Goals, three – reducing child mortality by two-thirds, to reduce maternal deaths by three-quarters, and to halt and begin to reduce the spread of HIV/AIDS – relate directly to WHO's scope; the other five inter-relate and affect world health.\n\nThe World Health Organization works to provide the needed health and well-being evidence through a variety of data collection platforms, including the World Health Survey covering almost 400,000 respondents from 70 countries, and the \"Study on Global Aging and Adult Health\" (SAGE) covering over 50,000 persons over 50 years old in 23 countries. The Country Health Intelligence Portal (CHIP), has also been developed to provide an access point to information about the health services that are available in different countries. The information gathered in this portal is used by the countries to set priorities for future strategies or plans, implement, monitor, and evaluate it.\n\nThe WHO has published various tools for measuring and monitoring the capacity of national health systems and health workforces. The Global Health Observatory (GHO) has been the WHO's main portal which provides access to data and analyses for key health themes by monitoring health situations around the globe.\n\nThe \"WHO Assessment Instrument for Mental Health Systems\" (WHO-AIMS), the \"WHO Quality of Life Instrument\" (WHOQOL), and the \"Service Availability and Readiness Assessment\" (SARA) provide guidance for data collection. Collaborative efforts between WHO and other agencies, such as through the Health Metrics Network, also aim to provide sufficient high-quality information to assist governmental decision making. WHO promotes the development of capacities in member states to use and produce research that addresses their national needs, including through the Evidence-Informed Policy Network (EVIPNet). The Pan American Health Organization (PAHO/AMRO) became the first region to develop and pass a policy on research for health approved in September 2009.\n\nOn 10 December 2013, a new WHO database, known as MiNDbank, went online. The database was launched on Human Rights Day, and is part of WHO's QualityRights initiative, which aims to end human rights violations against people with mental health conditions. The new database presents a great deal of information about mental health, substance abuse, disability, human rights, and the different policies, strategies, laws, and service standards being implemented in different countries. It also contains important international documents and information. The database allows visitors to access the health information of WHO member states and other partners. Users can review policies, laws, and strategies and search for the best practices and success stories in the field of mental health.\n\nThe WHO regularly publishes a \"World Health Report\", its leading publication, including an expert assessment of a specific global health topic. Other publications of WHO include the \"Bulletin of the World Health Organization\", the \"Eastern Mediterranean Health Journal\" (overseen by EMRO), the \"Human Resources for Health\" (published in collaboration with BioMed Central), and the \"Pan American Journal of Public Health\" (overseen by PAHO/AMRO).\n\nIn 2016, the World Health Organization drafted a global health sector strategy on HIV. In the draft, the World Health Organization outlines its commitment to ending the AIDS epidemic by the year 2030 with interim targets for the year 2020. In order to make achievements towards these targets, the draft lists actions that countries and the WHO can take, such as a commitment to universal health coverage, medical accessibility, prevention and eradication of disease, and efforts to educate the public. Some notable points made in the draft include addressing gender inequity where females are nearly twice as likely as men to get infected with HIV and tailoring resources to mobilized regions where the health system may be compromised due to natural disasters, etc. Among the points made, it seems clear that although the prevalence of HIV transmission is declining, there is still a need for resources, health education, and global efforts to end this epidemic. \n\nThe World Health Organization is a member of the United Nations Development Group.\n\n, the WHO has 194 member states: all of them Member States of the United Nations except for the Cook Islands and Niue. (A state becomes a full member of WHO by ratifying the treaty known as the Constitution of the World Health Organization.) , it also had two associate members, Puerto Rico and Tokelau. Several other countries have been granted observer status. Palestine is an observer as a \"national liberation movement\" recognized by the League of Arab States under United Nations Resolution 3118. The Holy See also attends as an observer, as does the Order of Malta. In 2010, Taiwan was invited under the name of \"Republic of China\".\n\nWHO Member States appoint delegations to the World Health Assembly, WHO's supreme decision-making body. All UN Member States are eligible for WHO membership, and, according to the WHO website, \"other countries may be admitted as members when their application has been approved by a simple majority vote of the World Health Assembly\". Liechtenstein is currently the only UN member not in the WHO membership.\n\nIn addition, the UN observer organizations International Committee of the Red Cross and International Federation of Red Cross and Red Crescent Societies have entered into \"official relations\" with WHO and are invited as observers. In the World Health Assembly they are seated alongside the other NGOs.\n\nThe World Health Assembly (WHA) is the legislative and supreme body of WHO. Based in Geneva, it typically meets yearly in May. It appoints the Director-General every five years and votes on matters of policy and finance of WHO, including the proposed budget. It also reviews reports of the Executive Board and decides whether there are areas of work requiring further examination. The Assembly elects 34 members, technically qualified in the field of health, to the Executive Board for three-year terms. The main functions of the Board are to carry out the decisions and policies of the Assembly, to advise it and to facilitate its work. The current chairman of the executive board is Dr. Assad Hafeez.\n\nThe regional divisions of WHO were created between 1949 and 1952, and are based on article 44 of the WHO's constitution, which allowed the WHO to \"establish a [single] regional organization to meet the special needs of [each defined] area\". Many decisions are made at regional level, including important discussions over WHO's budget, and in deciding the members of the next assembly, which are designated by the regions.\n\nEach region has a Regional Committee, which generally meets once a year, normally in the autumn. Representatives attend from each member or associative member in each region, including those states that are not fully recognized. For example, Palestine attends meetings of the Eastern Mediterranean Regional office. Each region also has a regional office. Each Regional Office is headed by a Regional Director, who is elected by the Regional Committee. The Board must approve such appointments, although as of 2004, it had never over-ruled the preference of a regional committee. The exact role of the board in the process has been a subject of debate, but the practical effect has always been small. Since 1999, Regional Directors serve for a once-renewable five-year term, and typically take their position on 1 February.\n\nEach Regional Committee of the WHO consists of all the Health Department heads, in all the governments of the countries that constitute the Region. Aside from electing the Regional Director, the Regional Committee is also in charge of setting the guidelines for the implementation, within the region, of the health and other policies adopted by the World Health Assembly. The Regional Committee also serves as a progress review board for the actions of WHO within the Region.\n\nThe Regional Director is effectively the head of WHO for his or her Region. The RD manages and/or supervises a staff of health and other experts at the regional offices and in specialized centres. The RD is also the direct supervising authority—concomitantly with the WHO Director-General—of all the heads of WHO country offices, known as WHO Representatives, within the Region.\n\nThe head of the organization is the Director-General, elected by the World Health Assembly. The term lasts for 5 years, and Director-Generals are typically appointed in May, when the Assembly meets. The current Director-General is Dr. Tedros Adhanom Ghebreyesus, who was appointed on 1 July 2017.\n\nThe WHO employs 8,500 people in 147 countries. In support of the principle of a tobacco-free work environment, the WHO does not recruit cigarette smokers. The organization has previously instigated the Framework Convention on Tobacco Control in 2003.\n\nThe WHO operates \"Goodwill Ambassadors\"; members of the arts, sports, or other fields of public life aimed at drawing attention to WHO's initiatives and projects. There are currently five Goodwill Ambassadors (Jet Li, Nancy Brinker, Peng Liyuan, Yohei Sasakawa and the Vienna Philharmonic Orchestra) and a further ambassador associated with a partnership project (Craig David).\n\nThe World Health Organization operates 150 country offices in six different regions. It also operates several liaison offices, including those with the European Union, United Nations and a single office covering the World Bank and International Monetary Fund. It also operates the International Agency for Research on Cancer in Lyon, France, and the WHO Centre for Health Development in Kobe, Japan. Additional offices include those in Pristina; the West Bank and Gaza; the US-Mexico Border Field Office in El Paso; the Office of the Caribbean Program Coordination in Barbados; and the Northern Micronesia office. There will generally be one WHO country office in the capital, occasionally accompanied by satellite-offices in the provinces or sub-regions of the country in question.\n\nThe country office is headed by a WHO Representative (WR). , the only WHO Representative outside Europe to be a national of that country was for the Libyan Arab Jamahiriya (\"Libya\"); all other staff were international. WHO Representatives in the Region termed the Americas are referred to as PAHO/WHO Representatives. In Europe, WHO Representatives also serve as Head of Country Office, and are nationals with the exception of Serbia; there are also Heads of Country Office in Albania, the Russian Federation, Tajikistan, Turkey, and Uzbekistan. The WR is member of the UN system country team which is coordinated by the UN System Resident Coordinator.\n\nThe country office consists of the WR, and several health and other experts, both foreign and local, as well as the necessary support staff. The main functions of WHO country offices include being the primary adviser of that country's government in matters of health and pharmaceutical policies.\n\nThe WHO is financed by contributions from member states and outside donors. , the largest annual assessed contributions from member states came from the United States ($110 million), Japan ($58 million), Germany ($37 million), United Kingdom ($31 million) and France ($31 million). The combined 2012–2013 budget has proposed a total expenditure of $3,959 million, of which $944 million (24%) will come from assessed contributions. This represented a significant fall in outlay compared to the previous 2009–2010 budget, adjusting to take account of previous underspends. Assessed contributions were kept the same. Voluntary contributions will account for $3,015 million (76%), of which $800 million is regarded as highly or moderately flexible funding, with the remainder tied to particular programmes or objectives.\n\nIn recent years, the WHO's work has involved increasing collaboration with external bodies. , a total of 473 non-governmental organizations (NGO) had some form of partnership with WHO. There were 189 partnerships with international NGOs in formal \"official relations\" – the rest being considered informal in character. Partners include the Bill and Melinda Gates Foundation and the Rockefeller Foundation.\n\nIn 1959, the WHO signed Agreement WHA 12–40 with the International Atomic Energy Agency (IAEA). A selective reading of this document (clause 3) can result in the understanding that the IAEA is able to prevent the WHO from conducting research or work on some areas, as seen hereafter. The agreement states here that the WHO recognizes the IAEA as having responsibility for peaceful nuclear energy without prejudice to the roles of the WHO of promoting health. However, the following paragraph adds that \n\n\"whenever either organization proposes to initiate a programme or activity on a subject in which the other organization has or may have a substantial interest, the first party shall consult the other with a view to adjusting the matter by mutual agreement\". \n\nThe nature of this statement has led some pressure groups and activists (including Women in Europe for a Common Future) to claim that the WHO is restricted in its ability to investigate the effects on human health of radiation caused by the use of nuclear power and the continuing effects of nuclear disasters in Chernobyl and Fukushima. They believe WHO must regain what they see as \"independence\". However as pointed out by Foreman in clause 2 it states.\n\n“2. In particular, and in accordance with the Constitution of the World Health Organization and the Statute of the International Atomic Energy Agency and its agreement with the United Nations together with the exchange of letters related thereto, and taking into account the respective co-ordinating responsibilities of both organizations, it is recognized by the World Health Organization that the International Atomic Energy Agency has the primary responsibility for encouraging, assisting and co- ordinating research and development and practical application of atomic energy for peaceful uses throughout the world without prejudice to the right of the World Health Organization to concern itself with promoting, developing, assisting and co-ordinating international health work, including research, in all its aspects.”\n\nClearly suggesting that the WHO is free to do as it sees fit on nuclear, radiation and other matters which relate to health.\n\nIn 2003, the WHO denounced the Roman Curia's health department's opposition to the use of condoms, saying: \"These incorrect statements about condoms and HIV are dangerous when we are facing a global pandemic which has already killed more than 20 million people, and currently affects at least 42 million.\" , the Catholic Church remains opposed to increasing the use of contraception to combat HIV/AIDS. At the time, the World Health Assembly President, Guyana's Health Minister Leslie Ramsammy, has condemned Pope Benedict's opposition to contraception, saying he was trying to \"create confusion\" and \"impede\" proven strategies in the battle against the disease.\n\nThe aggressive support of the Bill & Melinda Gates Foundation for intermittent preventive therapy of malaria triggered a memo from the former WHO malaria chief Akira Kochi.\n\nSome of the research undertaken or supported by WHO to determine how people's lifestyles and environments are influencing whether they live in better or worse health can be controversial, as illustrated by a 2003 joint WHO/FAO report on nutrition and the prevention of chronic non-communicable disease, which recommended that sugar should form no more than 10% of a healthy diet. The report led to lobbying by the sugar industry against the recommendation, to which the WHO/FAO responded by including in the report this statement: \"The Consultation recognized that a population goal for free sugars of less than 10% of total energy is controversial\". It also stood by its recommendation based upon its own analysis of scientific studies. In 2014, WHO reduced recommended sugar levels by half and said that sugar should make up no more than 5% of a healthy diet.\n\nIn 2007, the WHO organized work on pandemic influenza vaccine development through clinical trials in collaboration with many experts and health officials. A pandemic involving the H1N1 influenza virus was declared by the then Director-General Margaret Chan in April 2009. Margret Chan declared in 2010 that the H1N1 has moved into the post-pandemic period.\n\nBy the post-pandemic period critics claimed the WHO had exaggerated the danger, spreading \"fear and confusion\" rather than \"immediate information\". Industry experts countered that the 2009 pandemic had led to \"unprecedented collaboration between global health authorities, scientists and manufacturers, resulting in the most comprehensive pandemic response ever undertaken, with a number of vaccines approved for use three months after the pandemic declaration. This response was only possible because of the extensive preparations undertaken during the last decade\".\n\nFollowing the 2014 Ebola outbreak in West Africa, the organization was heavily criticized for its bureaucracy, insufficient financing, regional structure, and staffing profile.\n\nAn internal WHO report on the Ebola response pointed to underfunding and the lack of \"core capacity\" in health systems in developing countries as the primary weaknesses of the existing system. At the annual World Health Assembly in 2015, Director-General Margaret Chan announced a $100 million Contingency Fund for rapid response to future emergencies, of which it had received $26.9 million by April 2016 (for 2017 disbursement). WHO has budgeted an additional $494 million for its Health Emergencies Programme in 2016–17, for which it had received $140 million by April 2016.\n\nThe program was aimed at rebuilding WHO capacity for direct action, which critics said had been lost due to budget cuts in the previous decade that had left the organization in an advisory role dependent on member states for on-the-ground activities. In comparison, billions of dollars have been spent by developed countries on the 2013–2016 Ebola epidemic and 2015–16 Zika epidemic.\n\nThe WHO has a Framework Convention on Tobacco implementation database which is one of the only mechanisms to help enforce compliance with the FCTC. However, there have been reports of numerous discrepancies between it and national implementation reports on which it was built. As researchers Hoffman and Rizvi report \"As of July 4, 2012, 361 (32·7%) of 1104 countries' responses were misreported: 33 (3·0%) were clear errors (eg, database indicated “yes” when report indicated “no”), 270 (24·5%) were missing despite countries having submitted responses, and 58 (5·3%) were, in our opinion, misinterpreted by WHO staff\".\n\nThe World Health Organization sub-department, the International Agency for Research on Cancer (IARC), has been criticized for the way it analyses the tendency of certain substances and activities to cause cancer and for having a politically motivated bias when it selects studies for its analysis. Ed Yong, a British science journalist, has criticized the agency and its \"confusing\" category system for misleading the public. Marcel Kuntz, a French director of research at the French National Centre for Scientific Research, criticized the agency for its classification of potentially carcinogenic substances. He claimed that this classification did not take into account the extent of exposure: for example, red meat is qualified as probably carcinogenic, but the quantity of consumed red meat at which it could become dangerous is not specified.\n\nControversies have erupted multiple times when the IARC has classified many things as Class 2a (probable carcinogens), including cell phone signals, glyphosate, drinking hot beverages, and working as a barber.\n\nPolitical pressure from China has led to Taiwan being barred from membership of the WHO and other UN-affiliated organizations, and in both 2017 and 2018 the WHO refused to allow Taiwanese delegates to attend the WHO annual assembly. On multiple occasions Taiwanese journalists have been denied access to report on the assembly.\n\nIn May 2018, 172 members of the United States House of Representatives wrote to the Director General of the World Health Organization to argue for Taiwan's inclusion as an observer at the WHA.\n\nAccording to The Associated Press, the WHO routinely spends about $200 million a year on travel expenses, more than it spends to tackle mental health problems, HIV/AIDS, Tuberculosis and Malaria combined. In 2016, Margaret Chan, Director-General of WHO from November 2006 to June 2017, stayed in a $1000 per night hotel room while visiting West Africa.\n\nOn 21 October 2017, the Director General Tedros Adhanom Ghebreyesus appointed former Zimbabwean president Robert Mugabe as a WHO Goodwill Ambassador to help promote the fight against non-communicable diseases. The appointment address praised Mugabe for his commitment to public health in Zimbabwe.\n\nThe appointment attracted widespread condemnation and criticism in WHO member states and international organizations due to Robert Mugabe's poor record on human rights and presiding over a decline in Zimbabwe's public health. Due to the outcry, the following day the appointment was revoked.\n\nThe seat of the organization is in Geneva, Switzerland. It was designed by Swiss architect Jean Tschumi and inaugurated in 1966. In 2017, the organization launched an international competition to redesign and extend its headquarters.\n\n"}
{"id": "7982330", "url": "https://en.wikipedia.org/wiki?curid=7982330", "title": "World population estimates", "text": "World population estimates\n\nThis article lists estimates of world population, as well as projections of future developments.\nIn summary, estimates for the progression of world population since the late medieval period are in the following ranges:\nEstimates for pre-modern times are necessarily fraught with great uncertainties, and few of the published estimates have confidence intervals; in the absence of a straightforward means to assess the error of such estimates, a rough idea of expert consensus can be gained by comparing the values given in independent publications. Population estimates cannot be considered accurate to more than two decimal digits;\nfor example, world population for the year 2012 was estimated at\n7.02, 7.06 and 7.08 billion by the United States Census Bureau, the Population Reference Bureau and the United Nations Department of Economic and Social Affairs, respectively, corresponding to \na spread of estimates of the order of 0.8%.\n\nAs a general rule, the confidence of estimates on historical world population decreases for the more distant past. \nRobust population data only exists for the last two or three centuries. Until the late 18th century, few governments had ever performed an accurate census. In many early attempts, such as in Ancient Egypt and the Persian Empire, the focus was on counting merely a subset of the population for purposes of taxation or military service. \nPublished estimates for the 1st century (\"AD 1\") suggest an uncertainty of the order of 50% (estimates range between 150 and 330 million).\nSome estimates extend their timeline into deep prehistory, to \"10,000 BC\", i.e. the early Holocene, when world population estimates range roughly between one and ten million (with an uncertainty of up to an order of magnitude).\n\nEstimates for yet deeper prehistory, into the Paleolithic, are of a different nature. At this time human populations consisted entirely of non-sedentary hunter-gatherer populations, with anatomically modern humans existing alongside archaic human varieties, some of which are still ancestral to the modern human population due to interbreeding with modern humans during the Upper Paleolithic. \nEstimates of the size of these populations are a topic of paleoanthropology. A late human population bottleneck is postulated by some scholars at approximately 70,000 years ago, during the Toba catastrophe, when \"Homo sapiens\" population may have dropped to as low as between 1,000 and 10,000 individuals.\nFor the time of speciation of \"Homo sapiens\", some 200,000 years ago, an effective population size of the order of 10,000 to 30,000 individuals has been estimated, with an actual \"census population\" of early \"Homo sapiens\" of roughly 100,000 to 300,000 individuals.\nThe question of \"how many people have ever lived?\" or \"what percentage of people who have ever lived are alive today\" can be traced to the 1970s.\nThe more dramatic phrasing of \"the living outnumber the dead\" also dates to the 1970s, a time of population explosion and growing fears of human overpopulation in the wake of decolonization and before the adoption of China's one-child policy.\nThe claim that \"the living outnumber the dead\" was never accurate (although it may be roughly accurate if only ancestral population is considered). Arthur C. Clarke in \"\" (1968) has the claim that \"Behind every man now alive stand 30 ghosts, for that is the ratio by which the dead outnumber the living\", which was roughly accurate at the time of writing.\n\nEstimates of the \"total number of people who have ever lived\" is 107.6 billion as of 2011. \nThe answer naturally depends on the definition of \"people\", i.e. is only \"Homo sapiens\" to be counted, or all of genus \"Homo\", but due to the small population sizes in the Lower Paleolithic, the order of magnitude of the estimate is not affected by the choice of cut-off date substantially more than by the uncertainty of estimates throughout the Neolithic to Iron Age.\nThe estimate is more crucially affected by the estimate of infant mortalities vs. stillborn infants, due to the very high infant mortality throughout the pre-modern period. An estimate on the \"total number of people who have ever lived\" as of 1995 was calculated by Haub (1995) at \"about 105 billion births since the dawn of the human race\" with a cut-off date at 50,000 BC (beginning of the Upper Paleolithic), and an inclusion of a high infant mortality rate throughout pre-modern history.\n\nThe following table uses astronomical year numbering for dates, negative numbers corresponding roughly to the corresponding year BC (i.e. -10000 = 10,001 BC, etc.). The table starts counting around the Late Glacial Maximum period, in which ice retreated and humans started to spread into the northern hemisphere.\n\nFrom the beginning of the early modern period until the 20th century, world population has been characterized by a faster-than-exponential growth.\nFor the period of Classical Antiquity to the Middle Ages, roughly 500 BC to AD 1500, there has also been a general tendency of growth (estimated at roughly a factor 4 to 5 over the 2,000 year period), but not strictly monotonic: A noticeable dip in world population is assumed due to the Black Death in the mid-14th century.\n\nFor times after World War II, demographic data of some accuracy becomes available for a significant number of countries, and population estimates are often given as grand totals of numbers (typically given by country) of widely diverging accuracies. Some sources give these numbers rounded to the nearest million or the nearest thousand, while others give them without any rounding.\n\nTaking these numbers at face value would \nbe false precision; in spite of being stated to four, seven or even ten digits, they should not be interpreted as accurate to more than three digits at best (estimates by the United States Census Bureau and by the United Nations differ by about 0.5–1.5%).\n\n, the population of the world is projected to reach 8 billion in 2025, and 9 billion by about 2040/42. Kapitza (1996) estimated an asymptotic limit of population growth of 14 billion, 90% of which (12.6 billion) expected to be reached by 2135.\n\nReasonable predictions of population development are possible for the next 30 years or so, representing the period of fertility of the children alive today. Projections of population reaching more than one generation into the future are highly speculative: Thus, the United Nations Department of Economic and Social Affairs report of 2004 projected the world population to peak at 9.22 billion in 2075 and then stabilise at a value close to 9 billion; \nBy contrast, a 2014 projection by the United Nations Population Division predicts a population close to 11 billion by 2100 without any declining trend in the foreseeable future. On the other hand, a conservative scenario published in 2012 assumes that a maximum of 8 billion will be reached before 2040.\n\nThe following table shows projections of world population for the 21st century.\nOther, historical projections include\n\nPopulation estimates for world regions based on Maddison (2007), in millions.\nThe row showing total world population includes the average growth rate per year over the period separating each column from the preceding one.\n\nWhen considering population estimates by world region, it is worth noting that population history of the indigenous peoples of the Americas before the 1492 voyage of Christopher Columbus has proven difficult to establish, with many historians arguing for an estimate of 50 million people throughout the Americas, and some estimating that populations may have reached 100 million people or more. It is therefore estimated by some that populations in Mexico, Central, and South America could have reached 37 million by 1492. Additionally, the population estimate of 2 million for North America for the same time period represents the low end of modern estimates, and some estimate the population to have been as high as 18 million.\n\n\n"}
