{"id": "5334607", "url": "https://en.wikipedia.org/wiki?curid=5334607", "title": "Africa", "text": "Africa\n\nAfrica is the world's second largest and second most-populous continent (behind Asia in both categories). At about 30.3 million km (11.7 million square miles) including adjacent islands, it covers 6% of Earth's total surface area and 20% of its land area. With\n/1e9 round 1 billion people as of , it accounts for about 16% of the world's human population. The continent is surrounded by the Mediterranean Sea to the north, the Isthmus of Suez and the Red Sea to the northeast, the Indian Ocean to the southeast and the Atlantic Ocean to the west. The continent includes Madagascar and various archipelagos. It contains 54 fully recognised sovereign states (countries), nine territories and two \"de facto\" independent states with limited or no recognition. The majority of the continent and its countries are in the Northern Hemisphere, with a substantial portion and number of countries in the Southern Hemisphere.\n\nAfrica's average population is the youngest amongst all the continents; the median age in 2012 was 19.7, when the worldwide median age was 30.4. Algeria is Africa's largest country by area, and Nigeria is its largest by population. Africa, particularly central Eastern Africa, is widely accepted as the place of origin of humans and the Hominidae clade (great apes), as evidenced by the discovery of the earliest hominids and their ancestors as well as later ones that have been dated to around 7 million years ago, including \"Sahelanthropus tchadensis\", \"Australopithecus africanus\", \"A. afarensis\", \"Homo erectus\", \"H. habilis\" and \"H. ergaster\"—the earliest \"Homo sapiens\" (modern human), found in Ethiopia, date to circa 200,000 years ago. Africa straddles the equator and encompasses numerous climate areas; it is the only continent to stretch from the northern temperate to southern temperate zones.\n\nAfrica hosts a large diversity of ethnicities, cultures and languages. In the late 19th century, European countries colonised almost all of Africa; most present states in Africa originated from a process of decolonisation in the 20th century. African nations cooperate through the establishment of the African Union, which is headquartered in Addis Ababa.\n\n\"Afri\" was a Latin name used to refer to the inhabitants of then-known northern Africa to the west of the Nile river, and in its widest sense referred to all lands south of the Mediterranean (Ancient Libya). This name seems to have originally referred to a native Libyan tribe, an ancestor of modern Berbers; see Terence for discussion. The name had usually been connected with the Phoenician word \"\" meaning \"dust\", but a 1981 hypothesis has asserted that it stems from the Berber word \"ifri\" (plural \"ifran\") meaning \"cave\", in reference to cave dwellers. The same word may be found in the name of the Banu Ifran from Algeria and Tripolitania, a Berber tribe originally from Yafran (also known as \"Ifrane\") in northwestern Libya.\n\nUnder Roman rule, Carthage became the capital of the province it then named \"Africa Proconsularis\", following its defeat of the Carthaginians in the Third Punic War in 146 BC, which also included the coastal part of modern Libya. The Latin suffix \"-ica\" can sometimes be used to denote a land (e.g., in \"Celtica\" from \"Celtae\", as used by Julius Caesar). The later Muslim region of Ifriqiya, following its conquest of the Byzantine (Eastern Roman) Empire's \"Exarchatus Africae\", also preserved a form of the name.\n\nAccording to the Romans, Africa lay to the west of Egypt, while \"Asia\" was used to refer to Anatolia and lands to the east. A definite line was drawn between the two continents by the geographer Ptolemy (85–165 AD), indicating Alexandria along the Prime Meridian and making the isthmus of Suez and the Red Sea the boundary between Asia and Africa. As Europeans came to understand the real extent of the continent, the idea of \"Africa\" expanded with their knowledge.\n\nOther etymological hypotheses have been postulated for the ancient name \"Africa\":\n\nAfrica is considered by most paleoanthropologists to be the oldest inhabited territory on Earth, with the human species originating from the continent. During the mid-20th century, anthropologists discovered many fossils and evidence of human occupation perhaps as early as 7 million years ago (BP=before present). Fossil remains of several species of early apelike humans thought to have evolved into modern man, such as \"Australopithecus afarensis\" (radiometrically dated to approximately 3.9–3.0 million years BP, \"Paranthropus boisei\" (c. 2.3–1.4 million years BP) and \"Homo ergaster\" (c. 1.9 million–600,000 years BP) have been discovered.\n\nAfter the evolution of \"Homo sapiens sapiens\" approximately 150,000 to 100,000 years BP in Africa, the continent was mainly populated by groups of hunter-gatherers. These first modern humans left Africa and populated the rest of the globe during the Out of Africa II migration dated to approximately 50,000 years BP, exiting the continent either across Bab-el-Mandeb over the Red Sea, the Strait of Gibraltar in Morocco, or the Isthmus of Suez in Egypt.\n\nOther migrations of modern humans within the African continent have been dated to that time, with evidence of early human settlement found in Southern Africa, Southeast Africa, North Africa, and the Sahara.\n\nThe size of the Sahara has historically been extremely variable, with its area rapidly fluctuating and at times disappearing depending on global climatic conditions. At the end of the Ice ages, estimated to have been around 10,500 BC, the Sahara had again become a green fertile valley, and its African populations returned from the interior and coastal highlands in Sub-Saharan Africa, with rock art paintings depicting a fertile Sahara and large populations discovered in Tassili n'Ajjer dating back perhaps 10 millennia. However, the warming and drying climate meant that by 5000 BC, the Sahara region was becoming increasingly dry and hostile. Around 3500 BC, due to a tilt in the earth's orbit, the Sahara experienced a period of rapid desertification. The population trekked out of the Sahara region towards the Nile Valley below the Second Cataract where they made permanent or semi-permanent settlements. A major climatic recession occurred, lessening the heavy and persistent rains in Central and Eastern Africa. Since this time, dry conditions have prevailed in Eastern Africa and, increasingly during the last 200 years, in Ethiopia.\n\nThe domestication of cattle in Africa preceded agriculture and seems to have existed alongside hunter-gatherer cultures. It is speculated that by 6000 BC, cattle were domesticated in North Africa. In the Sahara-Nile complex, people domesticated many animals, including the donkey and a small screw-horned goat which was common from Algeria to Nubia.\n\nAround 4000 BC, the Saharan climate started to become drier at an exceedingly fast pace. This climate change caused lakes and rivers to shrink significantly and caused increasing desertification. This, in turn, decreased the amount of land conducive to settlements and helped to cause migrations of farming communities to the more tropical climate of West Africa.\n\nBy the first millennium BC, ironworking had been introduced in Northern Africa and quickly spread across the Sahara into the northern parts of sub-Saharan Africa, and by 500 BC, metalworking began to become commonplace in West Africa. Ironworking was fully established by roughly 500 BC in many areas of East and West Africa, although other regions didn't begin ironworking until the early centuries AD. Copper objects from Egypt, North Africa, Nubia, and Ethiopia dating from around 500 BC have been excavated in West Africa, suggesting that Trans-Saharan trade networks had been established by this date.\n\nAt about 3300 BC, the historical record opens in Northern Africa with the rise of literacy in the Pharaonic civilization of Ancient Egypt. One of the world's earliest and longest-lasting civilizations, the Egyptian state continued, with varying levels of influence over other areas, until 343 BC. Egyptian influence reached deep into modern-day Libya and Nubia, and, according to Martin Bernal, as far north as Crete.\n\nAn independent centre of civilization with trading links to Phoenicia was established by Phoenicians from Tyre on the north-west African coast at Carthage.\n\nEuropean exploration of Africa began with Ancient Greeks and Romans. In 332 BC, Alexander the Great was welcomed as a liberator in Persian-occupied Egypt. He founded Alexandria in Egypt, which would become the prosperous capital of the Ptolemaic dynasty after his death.\n\nFollowing the conquest of North Africa's Mediterranean coastline by the Roman Empire, the area was integrated economically and culturally into the Roman system. Roman settlement occurred in modern Tunisia and elsewhere along the coast. The first Roman emperor native to North Africa was Septimius Severus, born in Leptis Magna in present-day Libya—his mother was Italian Roman and his father was Punic.\n\nChristianity spread across these areas at an early date, from Judaea via Egypt and beyond the borders of the Roman world into Nubia; by AD 340 at the latest, it had become the state religion of the Aksumite Empire. Syro-Greek missionaries, who arrived by way of the Red Sea, were responsible for this theological development.\n\nIn the early 7th century, the newly formed Arabian Islamic Caliphate expanded into Egypt, and then into North Africa. In a short while, the local Berber elite had been integrated into Muslim Arab tribes. When the Umayyad capital Damascus fell in the 8th century, the Islamic centre of the Mediterranean shifted from Syria to Qayrawan in North Africa. Islamic North Africa had become diverse, and a hub for mystics, scholars, jurists, and philosophers. During the above-mentioned period, Islam spread to sub-Saharan Africa, mainly through trade routes and migration.\n\nPre-colonial Africa possessed perhaps as many as 10,000 different states and polities characterized by many different sorts of political organization and rule. These included small family groups of hunter-gatherers such as the San people of southern Africa; larger, more structured groups such as the family clan groupings of the Bantu-speaking peoples of central, southern, and eastern Africa; heavily structured clan groups in the Horn of Africa; the large Sahelian kingdoms; and autonomous city-states and kingdoms such as those of the Akan; Edo, Yoruba, and Igbo people in West Africa; and the Swahili coastal trading towns of Southeast Africa.\n\nBy the ninth century AD, a string of dynastic states, including the earliest Hausa states, stretched across the sub-Saharan savannah from the western regions to central Sudan. The most powerful of these states were Ghana, Gao, and the Kanem-Bornu Empire. Ghana declined in the eleventh century, but was succeeded by the Mali Empire which consolidated much of western Sudan in the thirteenth century. Kanem accepted Islam in the eleventh century.\n\nIn the forested regions of the West African coast, independent kingdoms grew with little influence from the Muslim north. The Kingdom of Nri was established around the ninth century and was one of the first. It is also one of the oldest kingdoms in present-day Nigeria and was ruled by the Eze Nri. The Nri kingdom is famous for its elaborate bronzes, found at the town of Igbo-Ukwu. The bronzes have been dated from as far back as the ninth century.\nThe Kingdom of Ife, historically the first of these Yoruba city-states or kingdoms, established government under a priestly oba ('king' or 'ruler' in the Yoruba language), called the \"Ooni of Ife\". Ife was noted as a major religious and cultural centre in West Africa, and for its unique naturalistic tradition of bronze sculpture. The Ife model of government was adapted at the Oyo Empire, where its obas or kings, called the \"Alaafins of Oyo\", once controlled a large number of other Yoruba and non-Yoruba city-states and kingdoms; the Fon \"Kingdom of Dahomey\" was one of the non-Yoruba domains under Oyo control.\n\nThe Almoravids were a Berber dynasty from the Sahara that spread over a wide area of northwestern Africa and the Iberian peninsula during the eleventh century. The Banu Hilal and Banu Ma'qil were a collection of Arab Bedouin tribes from the Arabian Peninsula who migrated westwards via Egypt between the eleventh and thirteenth centuries. Their migration resulted in the fusion of the Arabs and Berbers, where the locals were Arabized, and Arab culture absorbed elements of the local culture, under the unifying framework of Islam.\nFollowing the breakup of Mali, a local leader named Sonni Ali (1464–1492) founded the Songhai Empire in the region of middle Niger and the western Sudan and took control of the trans-Saharan trade. Sonni Ali seized Timbuktu in 1468 and Jenne in 1473, building his regime on trade revenues and the cooperation of Muslim merchants. His successor Askia Mohammad I (1493–1528) made Islam the official religion, built mosques, and brought to Gao Muslim scholars, including al-Maghili (d.1504), the founder of an important tradition of Sudanic African Muslim scholarship. By the eleventh century, some Hausa states – such as Kano, jigawa, Katsina, and Gobir – had developed into walled towns engaging in trade, servicing caravans, and the manufacture of goods. Until the fifteenth century, these small states were on the periphery of the major Sudanic empires of the era, paying tribute to Songhai to the west and Kanem-Borno to the east.\n\nSlavery had long been practiced in Africa. Between the 7th and 20th centuries, the Arab slave trade (also known as \"slavery in the east\") took 18 million slaves from Africa via trans-Saharan and Indian Ocean routes. Between the 15th and the 19th centuries, the Atlantic slave trade took an estimated 7–12 million slaves to the New World. In addition, more than 1 million Europeans were captured by Barbary pirates and sold as slaves in North Africa between the 16th and 19th centuries.\n\nIn West Africa, the decline of the Atlantic slave trade in the 1820s caused dramatic economic shifts in local polities. The gradual decline of slave-trading, prompted by a lack of demand for slaves in the New World, increasing anti-slavery legislation in Europe and America, and the British Royal Navy's increasing presence off the West African coast, obliged African states to adopt new economies. Between 1808 and 1860, the British West Africa Squadron seized approximately 1,600 slave ships and freed 150,000 Africans who were aboard.\nAction was also taken against African leaders who refused to agree to British treaties to outlaw the trade, for example against \"the usurping King of Lagos\", deposed in 1851. Anti-slavery treaties were signed with over 50 African rulers. The largest powers of West Africa (the Asante Confederacy, the Kingdom of Dahomey, and the Oyo Empire) adopted different ways of adapting to the shift. Asante and Dahomey concentrated on the development of \"legitimate commerce\" in the form of palm oil, cocoa, timber and gold, forming the bedrock of West Africa's modern export trade. The Oyo Empire, unable to adapt, collapsed into civil wars.\n\nIn the late 19th century, the European imperial powers engaged in a major territorial scramble and occupied most of the continent, creating many colonial territories, and leaving only two fully independent states: Ethiopia (known to Europeans as \"Abyssinia\"), and Liberia. Egypt and Sudan were never formally incorporated into any European colonial empire; however, after the British occupation of 1882, Egypt was effectively under British administration until 1922.\n\nThe Berlin Conference held in 1884–85 was an important event in the political future of African ethnic groups. It was convened by King Leopold II of Belgium, and attended by the European powers that laid claim to African territories. The Berlin Conference sought to end the European powers' Scramble for Africa, by agreeing on political division and spheres of influence. They set up the political divisions of the continent, by spheres of interest, that exist in Africa today.\n\nImperial rule by Europeans would continue until after the conclusion of World War II, when almost all remaining colonial territories gradually obtained formal independence. Independence movements in Africa gained momentum following World War II, which left the major European powers weakened. In 1951, Libya, a former Italian colony, gained independence. In 1956, Tunisia and Morocco won their independence from France. Ghana followed suit the next year (March 1957), becoming the first of the sub-Saharan colonies to be granted independence. Most of the rest of the continent became independent over the next decade.\n\nPortugal's overseas presence in Sub-Saharan Africa (most notably in Angola, Cape Verde, Mozambique, Guinea-Bissau and São Tomé and Príncipe) lasted from the 16th century to 1975, after the Estado Novo regime was overthrown in a military coup in Lisbon. Rhodesia unilaterally declared independence from the United Kingdom in 1965, under the white minority government of Ian Smith, but was not internationally recognized as an independent state (as Zimbabwe) until 1980, when black nationalists gained power after a bitter guerrilla war. Although South Africa was one of the first African countries to gain independence, the state remained under the control of the country's white minority through a system of racial segregation known as apartheid until 1994.\n\nToday, Africa contains 54 sovereign countries, most of which have borders that were drawn during the era of European colonialism. Since colonialism, African states have frequently been hampered by instability, corruption, violence, and authoritarianism. The vast majority of African states are republics that operate under some form of the presidential system of rule. However, few of them have been able to sustain democratic governments on a permanent basis, and many have instead cycled through a series of coups, producing military dictatorships.\n\nGreat instability was mainly the result of marginalization of ethnic groups, and graft under these leaders. For political gain, many leaders fanned ethnic conflicts, some of which had been exacerbated, or even created, by colonial rule. In many countries, the military was perceived as being the only group that could effectively maintain order, and it ruled many nations in Africa during the 1970s and early 1980s. During the period from the early 1960s to the late 1980s, Africa had more than 70 coups and 13 presidential assassinations. Border and territorial disputes were also common, with the European-imposed borders of many nations being widely contested through armed conflicts.\n\nCold War conflicts between the United States and the Soviet Union, as well as the policies of the International Monetary Fund, also played a role in instability. When a country became independent for the first time, it was often expected to align with one of the two superpowers. Many countries in Northern Africa received Soviet military aid, while others in Central and Southern Africa were supported by the United States, France or both. The 1970s saw an escalation of Cold War intrigues, as newly independent Angola and Mozambique aligned themselves with the Soviet Union, and the West and South Africa sought to contain Soviet influence by supporting friendly regimes or insurgency movements. In Rhodesia, Soviet and Chinese-backed leftist guerrillas of the Zimbabwe Patriotic Front waged a brutal guerrilla war against the country's white government. There was a major famine in Ethiopia, when hundreds of thousands of people starved. Some claimed that Marxist economic policies made the situation worse. The most devastating military conflict in modern independent Africa has been the Second Congo War; this conflict and its aftermath has killed an estimated 5.5 million people. Since 2003 there has been an ongoing conflict in Darfur which has become a humanitarian disaster. Another notable tragic event is the 1994 Rwandan Genocide in which an estimated 800,000 people were murdered. AIDS in post-colonial Africa has also been a prevalent issue.\n\nIn the 21st century, however, the number of armed conflicts in Africa has steadily declined. For instance, the civil war in Angola came to an end in 2002 after nearly 30 years. This coincided with many countries abandoning communist-style command economies and opening up for market reforms. The improved stability and economic reforms have led to a great increase in foreign investment into many African nations, mainly from China, which has spurred quick economic growth in many countries, seemingly ending decades of stagnation and decline. Several African economies are among the world's fastest growing . A significant part of this growth, which is sometimes referred to as Africa Rising, can also be attributed to the facilitated diffusion of information technologies and specifically the mobile telephone. Migration from African nations has increased dramatically in the last decade.\n\nAfrica is the largest of the three great southward projections from the largest landmass of the Earth. Separated from Europe by the Mediterranean Sea, it is joined to Asia at its northeast extremity by the Isthmus of Suez (transected by the Suez Canal), wide. (Geopolitically, Egypt's Sinai Peninsula east of the Suez Canal is often considered part of Africa, as well.)\n\nThe coastline is long, and the absence of deep indentations of the shore is illustrated by the fact that Europe, which covers only – about a third of the surface of Africa – has a coastline of . From the most northerly point, Ras ben Sakka in Tunisia (37°21' N), to the most southerly point, Cape Agulhas in South Africa (34°51'15\" S), is a distance of approximately . Cape Verde, 17°33'22\" W, the westernmost point, is a distance of approximately to Ras Hafun, 51°27'52\" E, the most easterly projection that neighbours Cape Guardafui, the tip of the Horn of Africa.\n\nAfrica's largest country is Algeria, and its smallest country is Seychelles, an archipelago off the east coast. The smallest nation on the continental mainland is The Gambia.\n\nThe African Plate is a major tectonic plate straddling the equator as well as the prime meridian. It includes much of the continent of Africa, as well as oceanic crust which lies between the continent and various surrounding ocean ridges. Between and , the Somali Plate began rifting from the African Plate along the East African Rift. Since the continent of Africa consists of crust from both the African and the Somali plates, some literature refers to the African Plate as the \"Nubian Plate\" to distinguish it from the continent as a whole.\n\nGeologically, Africa includes the Arabian Peninsula; the Zagros Mountains of Iran and the Anatolian Plateau of Turkey mark where the African Plate collided with Eurasia. The Afrotropic ecozone and the Saharo-Arabian desert to its north unite the region biogeographically, and the Afro-Asiatic language family unites the north linguistically.\n\nThe climate of Africa ranges from tropical to subarctic on its highest peaks. Its northern half is primarily desert, or arid, while its central and southern areas contain both savanna plains and dense jungle (rainforest) regions. In between, there is a convergence, where vegetation patterns such as sahel and steppe dominate. Africa is the hottest continent on earth and 60% of the entire land surface consists of drylands and deserts. The record for the highest-ever recorded temperature, in Libya in 1922 (), was discredited in 2013.\n\nAfrica boasts perhaps the world's largest combination of density and \"range of freedom\" of wild animal populations and diversity, with wild populations of large carnivores (such as lions, hyenas, and cheetahs) and herbivores (such as buffalo, elephants, camels, and giraffes) ranging freely on primarily open non-private plains. It is also home to a variety of \"jungle\" animals including snakes and primates and aquatic life such as crocodiles and amphibians. In addition, Africa has the largest number of megafauna species, as it was least affected by the extinction of the Pleistocene megafauna.\n\nAfrica has over 3,000 protected areas, with 198 marine protected areas, 50 biosphere reserves, and 80 wetlands reserves. Significant habitat destruction, increases in human population and poaching are reducing Africa's biological diversity and arable land. Human encroachment, civil unrest and the introduction of non-native species threaten biodiversity in Africa. This has been exacerbated by administrative problems, inadequate personnel and funding problems.\n\nDeforestation is affecting Africa at twice the world rate, according to the United Nations Environment Programme (UNEP). According to the University of Pennsylvania African Studies Center, 31% of Africa's pasture lands and 19% of its forests and woodlands are classified as degraded, and Africa is losing over four million hectares of forest per year, which is twice the average deforestation rate for the rest of the world. Some sources claim that approximately 90% of the original, virgin forests in West Africa have been destroyed. Over 90% of Madagascar's original forests have been destroyed since the arrival of humans 2000 years ago. About 65% of Africa's agricultural land suffers from soil degradation.\n\nThere are clear signs of increased networking among African organizations and states. For example, in the civil war in the Democratic Republic of the Congo (former Zaire), rather than rich, non-African countries intervening, neighbouring African countries became involved (see also Second Congo War). Since the conflict began in 1998, the estimated death toll has reached 5 million.\n\nThe African Union (AU) is a 55-member federation consisting of all of Africa's states. The union was formed, with Addis Ababa, Ethiopia, as its headquarters, on 26 June 2001. The union was officially established on 9 July 2002 as a successor to the Organisation of African Unity (OAU). In July 2004, the African Union's Pan-African Parliament (PAP) was relocated to Midrand, in South Africa, but the African Commission on Human and Peoples' Rights remained in Addis Ababa. There is a policy in effect to decentralize the African Federation's institutions so that they are shared by all the states.\n\nThe African Union, not to be confused with the AU Commission, is formed by the Constitutive Act of the African Union, which aims to transform the African Economic Community, a federated commonwealth, into a state under established international conventions. The African Union has a parliamentary government, known as the African Union Government, consisting of legislative, judicial and executive organs. It is led by the African Union President and Head of State, who is also the President of the Pan-African Parliament. A person becomes AU President by being elected to the PAP, and subsequently gaining majority support in the PAP. The powers and authority of the President of the African Parliament derive from the Constitutive Act and the Protocol of the Pan-African Parliament, as well as the inheritance of presidential authority stipulated by African treaties and by international treaties, including those subordinating the Secretary General of the OAU Secretariat (AU Commission) to the PAP. The government of the AU consists of all-union (federal), regional, state, and municipal authorities, as well as hundreds of institutions, that together manage the day-to-day affairs of the institution.\n\nPolitical associations such as the African Union offer hope for greater co-operation and peace between the continent's many countries. Extensive human rights abuses still occur in several parts of Africa, often under the oversight of the state. Most of such violations occur for political reasons, often as a side effect of civil war. Countries where major human rights violations have been reported in recent times include the Democratic Republic of the Congo, Sierra Leone, Liberia, Sudan, Zimbabwe, and Côte d'Ivoire.\n\nAlthough it has abundant natural resources, Africa remains the world's poorest and most underdeveloped continent, the result of a variety of causes that may include corrupt governments that have often committed serious human rights violations, failed central planning, high levels of illiteracy, lack of access to foreign capital, and frequent tribal and military conflict (ranging from guerrilla warfare to genocide). According to the United Nations' Human Development Report in 2003, the bottom 24 ranked nations (151st to 175th) were all African.\n\nPoverty, illiteracy, malnutrition and inadequate water supply and sanitation, as well as poor health, affect a large proportion of the people who reside in the African continent. In August 2008, the World Bank announced revised global poverty estimates based on a new international poverty line of $1.25 per day (versus the previous measure of $1.00). 81% of the Sub-Saharan Africa population was living on less than $2.50 (PPP) per day in 2005, compared with 86% for India.\n\nSub-Saharan Africa is the least successful region of the world in reducing poverty ($1.25 per day); some 50% of the population living in poverty in 1981 (200 million people), a figure that rose to 58% in 1996 before dropping to 50% in 2005 (380 million people). The average poor person in sub-Saharan Africa is estimated to live on only 70 cents per day, and was poorer in 2003 than in 1973, indicating increasing poverty in some areas. Some of it is attributed to unsuccessful economic liberalization programmes spearheaded by foreign companies and governments, but other studies have cited bad domestic government policies more than external factors.\n\nAfrica is now at risk of being in debt once again, particularly in Sub-Saharan African countries.  The last debt crisis in 2005 was resolved with help from the heavily indebted poor countries scheme (HIPC).  The HIPC resulted in some positive and negative effects on the economy in Africa. About ten years after the 2005 debt crisis in Sub-Saharan Africa was resolved, Zambia fell back into dept.  A small reason was due to the fall in copper prices in 2011, but the bigger reason was that a large amount of the money Zambia borrowed was wasted or pocketed by the elite.\n\nFrom 1995 to 2005, Africa's rate of economic growth increased, averaging 5% in 2005. Some countries experienced still higher growth rates, notably Angola, Sudan and Equatorial Guinea, all of which had recently begun extracting their petroleum reserves or had expanded their oil extraction capacity.\n\nIn a recently published analysis based on World Values Survey data, the Austrian political scientist Arno Tausch maintained that several African countries, most notably Ghana, perform quite well on scales of mass support for democracy and the market economy.\n\nTausch's global value comparison based on the World Values Survey derived the following factor analytical scales: 1. The non-violent and law-abiding society 2. Democracy movement 3. Climate of personal non-violence 4. Trust in institutions 5. Happiness, good health 6. No redistributive religious fundamentalism 7. Accepting the market 8. Feminism 9. Involvement in politics 10. Optimism and engagement 11. No welfare mentality, acceptancy of the Calvinist work ethics. The spread in the performance of African countries with complete data, Tausch concluded “is really amazing”. While one should be especially hopeful about the development of future democracy and the market economy in Ghana, the article suggests pessimistic tendencies for Egypt and Algeria, and especially for Africa's leading economy, South Africa. High Human Inequality, as measured by the UNDP’s Human Development Report’s Index of Human Inequality, further impairs the development of Human Security. Tausch also maintains that the certain recent optimism, corresponding to economic and human rights data, emerging from Africa, is reflected in the development of a civil society.\n\nThe continent is believed to hold 90% of the world's cobalt, 90% of its platinum, 50% of its gold, 98% of its chromium, 70% of its tantalite, 64% of its manganese and one-third of its uranium. The Democratic Republic of the Congo (DRC) has 70% of the world's coltan, a mineral used in the production of tantalum capacitors for electronic devices such as cell phones. The DRC also has more than 30% of the world's diamond reserves. Guinea is the world's largest exporter of bauxite. As the growth in Africa has been driven mainly by services and not manufacturing or agriculture, it has been growth without jobs and without reduction in poverty levels. In fact, the food security crisis of 2008 which took place on the heels of the global financial crisis pushed 100 million people into food insecurity.\n\nIn recent years, the People's Republic of China has built increasingly stronger ties with African nations and is Africa's largest trading partner. In 2007, Chinese companies invested a total of US$1 billion in Africa.\n\nA Harvard University study led by professor Calestous Juma showed that Africa could feed itself by making the transition from importer to self-sufficiency. \"African agriculture is at the crossroads; we have come to the end of a century of policies that favoured Africa's export of raw materials and importation of food. Africa is starting to focus on agricultural innovation as its new engine for regional trade and prosperity.\"\n\nDuring US President Barack Obama's visit to Africa in July 2013, he announced a US$7 billion plan to further develop infrastructure and work more intensively with African heads of state. He also announced a new programme named Trade Africa, designed to boost trade within the continent as well as between Africa and the US.\n\nAfrica's population has rapidly increased over the last 40 years, and consequently, it is relatively young. In some African states, more than half the population is under 25 years of age. The total number of people in Africa increased from 229 million in 1950 to 630 million in 1990. As of , the population of Africa is estimated at / 1e9 round 1 billion . Africa's total population surpassing other continents is fairly recent; African population surpassed Europe in the 1990s, while the Americas was overtaken sometime around the year 2000; Africa's rapid population growth is expected to overtake the only two nations currently larger than its population, at roughly the same time – India and China's 1.4 billion people each will swap ranking around the year 2022. This increase in number of babies born in Africa compared to the rest of the world is expected to reach approximately 37% in the year 2050, an increase of 21% since 1990 alone.\n\nSpeakers of Bantu languages (part of the Niger–Congo family) are the majority in southern, central and southeast Africa. The Bantu-speaking peoples from the Sahel progressively expanded over most of Sub-Saharan Africa. But there are also several Nilotic groups in South Sudan and East Africa, the mixed Swahili people on the Swahili Coast, and a few remaining indigenous Khoisan (\"San\" or \"Bushmen\") and Pygmy peoples in southern and central Africa, respectively. Bantu-speaking Africans also predominate in Gabon and Equatorial Guinea, and are found in parts of southern Cameroon. In the Kalahari Desert of Southern Africa, the distinct people known as the Bushmen (also \"San\", closely related to, but distinct from \"Hottentots\") have long been present. The San are physically distinct from other Africans and are the indigenous people of southern Africa. Pygmies are the pre-Bantu indigenous peoples of central Africa.\n\nThe peoples of West Africa primarily speak Niger–Congo languages, belonging mostly to its non-Bantu branches, though some Nilo-Saharan and Afro-Asiatic speaking groups are also found. The Niger–Congo-speaking Yoruba, Igbo, Fulani, Akan and Wolof ethnic groups are the largest and most influential. In the central Sahara, Mandinka or Mande groups are most significant. Chadic-speaking groups, including the Hausa, are found in more northerly parts of the region nearest to the Sahara, and Nilo-Saharan communities, such as the Songhai, Kanuri and Zarma, are found in the eastern parts of West Africa bordering Central Africa.\n\nThe peoples of North Africa consist of three main indigenous groups: Berbers in the northwest, Egyptians in the northeast, and Nilo-Saharan-speaking peoples in the east. The Arabs who arrived in the 7th century AD introduced the Arabic language and Islam to North Africa. The Semitic Phoenicians (who founded Carthage) and Hyksos, the Indo-Iranian Alans, the Indo- European Greeks, Romans, and Vandals settled in North Africa as well. Significant Berber communities remain within Morocco and Algeria in the 21st century, while, to a lesser extent, Berber speakers are also present in some regions of Tunisia and Libya. The Berber-speaking Tuareg and other often-nomadic peoples are the principal inhabitants of the Saharan interior of North Africa. In Mauritania, there is a small but near-extinct Berber community in the north and Niger–Congo-speaking peoples in the south, though in both regions Arabic and Arab culture predominates. In Sudan, although Arabic and Arab culture predominate, it is mostly inhabited by groups that originally spoke Nilo-Saharan, such as the Nubians, Fur, Masalit and Zaghawa, who, over the centuries, have variously intermixed with migrants from the Arabian peninsula. Small communities of Afro-Asiatic-speaking Beja nomads can also be found in Egypt and Sudan.\nIn the Horn of Africa, some Ethiopian and Eritrean groups (like the Amhara and Tigrayans, collectively known as Habesha) speak languages from the Semitic branch of the Afro-Asiatic language family, while the Oromo and Somali speak languages from the Cushitic branch of Afro-Asiatic.\n\nPrior to the decolonization movements of the post-World War II era, Europeans were represented in every part of Africa. Decolonization during the 1960s and 1970s often resulted in the mass emigration of white settlers – especially from Algeria and Morocco (1.6 million \"pieds-noirs\" in North Africa), Kenya, Congo, Rhodesia, Mozambique and Angola. Between 1975 and 1977, over a million colonials returned to Portugal alone. Nevertheless, white Africans remain an important minority in many African states, particularly Zimbabwe, Namibia, Réunion, and the Republic of South Africa. The country with the largest white African population is South Africa. Dutch and British diasporas represent the largest communities of European ancestry on the continent today.\n\nEuropean colonization also brought sizable groups of Asians, particularly from the Indian subcontinent, to British colonies. Large Indian communities are found in South Africa, and smaller ones are present in Kenya, Tanzania, and some other southern and southeast African countries. The large Indian community in Uganda was expelled by the dictator Idi Amin in 1972, though many have since returned. The islands in the Indian Ocean are also populated primarily by people of Asian origin, often mixed with Africans and Europeans. The Malagasy people of Madagascar are an Austronesian people, but those along the coast are generally mixed with Bantu, Arab, Indian and European origins. Malay and Indian ancestries are also important components in the group of people known in South Africa as Cape Coloureds (people with origins in two or more races and continents). During the 20th century, small but economically important communities of Lebanese and Chinese have also developed in the larger coastal cities of West and East Africa, respectively.\n\nBy most estimates, well over a thousand languages (UNESCO has estimated around two thousand) are spoken in Africa. Most are of African origin, though some are of European or Asian origin. Africa is the most multilingual continent in the world, and it is not rare for individuals to fluently speak not only multiple African languages, but one or more European ones as well. There are four major language families indigenous to Africa:\n\nFollowing the end of colonialism, nearly all African countries adopted official languages that originated outside the continent, although several countries also granted legal recognition to indigenous languages (such as Swahili, Yoruba, Igbo and Hausa). In numerous countries, English and French (\"see African French\") are used for communication in the public sphere such as government, commerce, education and the media. Arabic, Portuguese, Afrikaans and Spanish are examples of languages that trace their origin to outside of Africa, and that are used by millions of Africans today, both in the public and private spheres. Italian is spoken by some in former Italian colonies in Africa. German is spoken in Namibia, as it was a former German protectorate.\n\nSome aspects of traditional African cultures have become less practised in recent years as a result of neglect and suppression by colonial and post-colonial regimes. For example, African customs were discouraged, and African languages were prohibited in mission schools. Leopold II of Belgium attempted to \"civilize\" Africans by discouraging polygamy and witchcraft.\n\nObidoh Freeborn posits that colonialism is one element that has created the character of modern African art. According to authors Douglas Fraser and Herbert M. Cole, \"The precipitous alterations in the power structure wrought by colonialism were quickly followed by drastic iconographic changes in the art.\" Fraser and Cole assert that, in Igboland, some art objects \"lack the vigor and careful craftsmanship of the earlier art objects that served traditional functions. Author Chika Okeke-Agulu states that \"the racist infrastructure of British imperial enterprise forced upon the political and cultural guardians of empire a denial and suppression of an emergent sovereign Africa and modernist art.\" In Soweto, the West Rand Administrative Board established a Cultural Section to collect, read, and review scripts before performances could occur. Editors F. Abiola Irele and Simon Gikandi comment that the current identity of African literature had its genesis in the \"traumatic encounter between Africa and Europe.\" On the other hand, Mhoze Chikowero believes that Africans deployed music, dance, spirituality, and other performative cultures to (re)asset themselves as active agents and indigenous intellectuals, to unmake their colonial marginalization and reshape their own destinies.\" \n\nThere is now a resurgence in the attempts to rediscover and revalue African traditional cultures, under such movements as the African Renaissance, led by Thabo Mbeki, Afrocentrism, led by a group of scholars, including Molefi Asante, as well as the increasing recognition of traditional spiritualism through decriminalization of Vodou and other forms of spirituality.\n\nAfrican art and architecture reflect the diversity of African cultures. The region's oldest known beads were made from \"Nassarius\" shells and worn as personal ornaments 72,000 years ago. The Great Pyramid of Giza in Egypt was the world's tallest structure for 4,000 years, until the completion of Lincoln Cathedral around the year 1300. The stone ruins of Great Zimbabwe are also noteworthy for their architecture, as are the monolithic churches at Lalibela, Ethiopia, such as the Church of Saint George.\n\nEgypt has long been a cultural focus of the Arab world, while remembrance of the rhythms of sub-Saharan Africa, in particular West Africa, was transmitted through the Atlantic slave trade to modern samba, blues, jazz, reggae, hip hop, and rock. The 1950s through the 1970s saw a conglomeration of these various styles with the popularization of Afrobeat and Highlife music. Modern music of the continent includes the highly complex choral singing of southern Africa and the dance rhythms of the musical genre of soukous, dominated by the music of the Democratic Republic of Congo. Indigenous musical and dance traditions of Africa are maintained by oral traditions, and they are distinct from the music and dance styles of North Africa and Southern Africa. Arab influences are visible in North African music and dance and, in Southern Africa, Western influences are apparent due to colonization.\n\nFifty-four African countries have football (soccer) teams in the Confederation of African Football. Egypt has won the African Cup seven times, and a record-making three times in a row. Cameroon, Nigeria, Senegal, Ghana, and Algeria have advanced to the knockout stage of recent FIFA World Cups. South Africa hosted the 2010 World Cup tournament, becoming the first African country to do so.\n\nCricket is popular in some African nations. South Africa and Zimbabwe have Test status, while Kenya is the leading non-test team and previously had One-Day International cricket (ODI) status (from 10 October 1997, until 30 January 2014). The three countries jointly hosted the 2003 Cricket World Cup. Namibia is the other African country to have played in a World Cup. Morocco in northern Africa has also hosted the 2002 Morocco Cup, but the national team has never qualified for a major tournament. Rugby is a popular sport in South Africa, Namibia, and Zimbabwe.\n\nAfricans profess a wide variety of religious beliefs, and statistics on religious affiliation are difficult to come by since they are often a sensitive topic for governments with mixed religious populations. According to the World Book Encyclopedia, Islam is the largest religion in Africa, followed by Christianity. According to Encyclopædia Britannica, 45% of the population are Christians, 40% are Muslims, and 10% follow traditional religions. A small number of Africans are Hindu, Buddhist, Confucianist, Baha'i, or Jewish. There is also a minority of people in Africa who are irreligious.\n\nThe countries in this table are categorized according to the scheme for geographic subregions used by the United Nations, and data included are per sources in cross-referenced articles. Where they differ, provisos are clearly indicated.\n"}
{"id": "18959138", "url": "https://en.wikipedia.org/wiki?curid=18959138", "title": "Antarctica", "text": "Antarctica\n\nAntarctica ( or , ) is Earth's southernmost continent. It contains the geographic South Pole and is situated in the Antarctic region of the Southern Hemisphere, almost entirely south of the Antarctic Circle, and is surrounded by the Southern Ocean. At , it is the fifth-largest continent. For comparison, Antarctica is nearly twice the size of Australia. About 98% of Antarctica is covered by ice that averages in thickness, which extends to all but the northernmost reaches of the Antarctic Peninsula.\n\nAntarctica, on average, is the coldest, driest, and windiest continent, and has the highest average elevation of all the continents. Antarctica is a desert, with annual precipitation of only 200 mm (8 in) along the coast and far less inland. The temperature in Antarctica has reached −89.2 °C (−128.6 °F) (or even −94.7 °C (−135.8 °F) as measured from space), though the average for the third quarter (the coldest part of the year) is −63 °C (−81 °F). Anywhere from 1,000 to 5,000 people reside throughout the year at research stations scattered across the continent. Organisms native to Antarctica include many types of algae, bacteria, fungi, plants, protista, and certain animals, such as mites, nematodes, penguins, seals and tardigrades. Vegetation, where it occurs, is tundra.\n\nAlthough myths and speculation about a \"Terra Australis\" (\"Southern Land\") date back to antiquity, Antarctica is noted as the last region on Earth in recorded history to be discovered, unseen until 1820 when the Russian expedition of Fabian Gottlieb von Bellingshausen and Mikhail Lazarev on \"Vostok\" and \"Mirny\" sighted the Fimbul ice shelf. The continent, however, remained largely neglected for the rest of the 19th century because of its hostile environment, lack of easily accessible resources, and isolation. In 1895, the first confirmed landing was conducted by a team of Norwegians.\n\nAntarctica is a \"de facto\" condominium, governed by parties to the Antarctic Treaty System that have consulting status. Twelve countries signed the Antarctic Treaty in 1959, and thirty-eight have signed it since then. The treaty prohibits military activities and mineral mining, prohibits nuclear explosions and nuclear waste disposal, supports scientific research, and protects the continent's ecozone. Ongoing experiments are conducted by more than 4,000 scientists from many nations.\n\nThe name \"Antarctica\" is the romanised version of the Greek compound word ἀνταρκτική (\"antarktiké\"), feminine of ἀνταρκτικός (\"antarktikós\"), meaning \"opposite to the Arctic\", \"opposite to the north\".\n\nAristotle wrote in his book \"Meteorology\" about an \"Antarctic region\" in c. 350 BC Marinus of Tyre reportedly used the name in his unpreserved world map from the 2nd century CE. The Roman authors Hyginus and Apuleius (1–2 centuries CE) used for the South Pole the romanised Greek name \"polus antarcticus,\" from which derived the Old French \"pole antartike\" (modern \"pôle antarctique\") attested in 1270, and from there the Middle English \"pol antartik\" in a 1391 technical treatise by Geoffrey Chaucer (modern \"Antarctic Pole\").\n\nBefore acquiring its present geographical connotations, the term was used for other locations that could be defined as \"opposite to the north\". For example, the short-lived French colony established in Brazil in the 16th century was called \"France Antarctique\".\n\nThe first formal use of the name \"Antarctica\" as a continental name in the 1890s is attributed to the Scottish cartographer John George Bartholomew.\n\nThe long-imagined (but undiscovered) south polar continent was originally called \"Terra Australis\", sometimes shortened to 'Australia' as seen in a woodcut illustration titled \"Sphere of the winds\", contained in an astrological textbook published in Frankfurt in 1545. Although the longer Latin phrase was better known, the shortened name \"Australia\" was used in Europe's scholarly circles.\n\nThen in the nineteenth century, the colonial authorities in Sydney removed the Dutch name from New Holland. Instead of inventing a new name to replace it, they took the name \"Australia\" from the south polar continent, leaving it nameless for some eighty years. During that period, geographers had to make do with clumsy phrases such as \"the Antarctic Continent\". They searched for a more poetic replacement, suggesting various names such as Ultima and Antipodea. Eventually \"Antarctica\" was adopted in the 1890s.\n\nAntarctica has no indigenous population, and there is no evidence that it was seen by humans until the 19th century. However, in February 1775, during his second voyage, Captain Cook called the existence of such a polar continent \"probable\" and in another copy of his journal he wrote:\"[I] firmly believe it and its more than probable that we have seen a part of it\".\n\nHowever, belief in the existence of a \"Terra Australis\"—a vast continent in the far south of the globe to \"balance\" the northern lands of Europe, Asia and North Africa—had prevailed since the times of Ptolemy in the 1st century AD. Even in the late 17th century, after explorers had found that South America and Australia were not part of the fabled \"Antarctica\", geographers believed that the continent was much larger than its actual size.\nIntegral to the story of the origin of Antarctica's name is that it was not named \"Terra Australis\"—this name was given to Australia instead, because of the misconception that no significant landmass could exist further south. Explorer Matthew Flinders, in particular, has been credited with popularising the transfer of the name \"Terra Australis\" to Australia. He justified the titling of his book \"A Voyage to Terra Australis\" (1814) by writing in the introduction:\n\nEuropean maps continued to show this hypothesised land until Captain James Cook's ships, and , crossed the Antarctic Circle on 17 January 1773, in December 1773 and again in January 1774. Cook came within about of the Antarctic coast before retreating in the face of field ice in January 1773.\n\nAccording to various organisations (the National Science Foundation, NASA, the University of California, San Diego, the Russian State Museum of the Arctic and Antarctic, among others), ships captained by three men sighted Antarctica or its ice shelf in 1820: Fabian Gottlieb von Bellingshausen (a captain in the Imperial Russian Navy), Edward Bransfield (a captain in the Royal Navy), and Nathaniel Palmer (a sealer from Stonington, Connecticut).\n\nThe First Russian Antarctic Expedition led by Bellingshausen and Mikhail Lazarev on the 985-ton sloop-of-war \"Vostok\" (\"East\") and the 530-ton support vessel \"Mirny\" (\"Peaceful\") reached a point within of Queen Maud's Land and recorded the sight of an ice shelf at , on 27 January 1820, which became known as the Fimbul ice shelf. This happened three days before Bransfield sighted land and ten months before Palmer did so in November 1820. The first documented landing on Antarctica was by the American sealer John Davis, apparently at Hughes Bay, near Cape Charles, in West Antarctica on 7 February 1821, although some historians dispute this claim. The first recorded and confirmed landing was at Cape Adair in 1895 (by the Norwegian-Swedish whaling ship \"Antarctic\").\n\nOn 22 January 1840, two days after the discovery of the coast west of the Balleny Islands, some members of the crew of the 1837–40 expedition of Jules Dumont d'Urville disembarked on the highest islet of a group of rocky islands about 4 km from Cape Géodésie on the coast of Adélie Land where they took some mineral, algae, and animal samples.\n\nIn December 1839, as part of the United States Exploring Expedition of 1838–42 conducted by the United States Navy (sometimes called the \"Ex. Ex.\", or \"the Wilkes Expedition\"), an expedition sailed from Sydney, Australia, into the Antarctic Ocean, as it was then known, and reported the discovery \"of an Antarctic continent west of the Balleny Islands\" on 25 January 1840. That part of Antarctica was named \"Wilkes Land\", a name it retains to this day.\n\nExplorer James Clark Ross passed through what is now known as the Ross Sea and discovered Ross Island (both of which were named after him) in 1841. He sailed along a huge wall of ice that was later named the Ross Ice Shelf. Mount Erebus and Mount Terror are named after two ships from his expedition: and . Mercator Cooper landed in East Antarctica on 26 January 1853.\n\nDuring the Nimrod Expedition led by Ernest Shackleton in 1907, parties led by Edgeworth David became the first to climb Mount Erebus and to reach the South Magnetic Pole. Douglas Mawson, who assumed the leadership of the Magnetic Pole party on their perilous return, went on to lead several expeditions until retiring in 1931. In addition, Shackleton and three other members of his expedition made several firsts in December 1908 – February 1909: they were the first humans to traverse the Ross Ice Shelf, the first to traverse the Transantarctic Mountains (via the Beardmore Glacier), and the first to set foot on the South Polar Plateau. An expedition led by Norwegian polar explorer Roald Amundsen from the ship \"Fram\" became the first to reach the geographic South Pole on 14 December 1911, using a route from the Bay of Whales and up the Axel Heiberg Glacier. One month later, the doomed Scott Expedition reached the pole.\n\nRichard E. Byrd led several voyages to the Antarctic by plane in the 1930s and 1940s. He is credited with implementing mechanised land transport on the continent and conducting extensive geological and biological research. The first women to set foot on Antarctica did so in the 1930s with Caroline Mikkelsen landing on an island of Antarctica in 1935, and Ingrid Christensen stepping onto the mainland in 1937.\n\nIt was not until 31 October 1956, that anyone set foot on the South Pole again; on that day a U.S. Navy group led by Rear Admiral George J. Dufek successfully landed an aircraft there. The first women to step onto the South Pole were Pam Young, Jean Pearson, Lois Jones, Eileen McSaveney, Kay Lindsay and Terry Tickhill in 1969.\n\nThe first person to sail single-handed to Antarctica was the New Zealander David Henry Lewis, in 1972, in the 10-metre steel sloop \"Ice Bird\".\n\nOn 28 April 1979, Air New Zealand Flight 901, a McDonnell Douglas DC-10-30, crashed into Mount Erebus, killing all 257 people on board.\n\nPositioned asymmetrically around the South Pole and largely south of the Antarctic Circle, Antarctica is the southernmost continent and is surrounded by the Southern Ocean; alternatively, it may be considered to be surrounded by the southern Pacific, Atlantic, and Indian Oceans, or by the southern waters of the World Ocean. There are a number of rivers and lakes in Antarctica, the longest river being the Onyx. The largest lake, Vostok, is one of the largest sub-glacial lakes in the world. Antarctica covers more than , making it the fifth-largest continent, about 1.3 times as large as Europe. The coastline measures and is mostly characterised by ice formations, as the following table shows:\n\nAntarctica is divided in two by the Transantarctic Mountains close to the neck between the Ross Sea and the Weddell Sea. The portion west of the Weddell Sea and east of the Ross Sea is called West Antarctica and the remainder East Antarctica, because they roughly correspond to the Western and Eastern Hemispheres relative to the Greenwich meridian.\n\nAbout 98% of Antarctica is covered by the Antarctic ice sheet, a sheet of ice averaging at least thick. The continent has about 90% of the world's ice (and thereby about 70% of the world's fresh water). If all of this ice were melted, sea levels would rise about . In most of the interior of the continent, precipitation is very low, down to per year; in a few \"blue ice\" areas precipitation is lower than mass loss by sublimation, and so the local mass balance is negative. In the dry valleys, the same effect occurs over a rock base, leading to a desiccated landscape.\n\nWest Antarctica is covered by the West Antarctic Ice Sheet. The sheet has been of recent concern because of the small possibility of its collapse. If the sheet were to break down, ocean levels would rise by several metres in a relatively geologically short period of time, perhaps a matter of centuries. Several Antarctic ice streams, which account for about 10% of the ice sheet, flow to one of the many Antarctic ice shelves: see ice-sheet dynamics.\n\nEast Antarctica lies on the Indian Ocean side of the Transantarctic Mountains and comprises Coats Land, Queen Maud Land, Enderby Land, Mac. Robertson Land, Wilkes Land, and Victoria Land. All but a small portion of this region lies within the Eastern Hemisphere. East Antarctica is largely covered by the East Antarctic Ice Sheet.\n\nVinson Massif, the highest peak in Antarctica at , is located in the Ellsworth Mountains. Antarctica contains many other mountains, on both the main continent and the surrounding islands. Mount Erebus on Ross Island is the world's southernmost active volcano. Another well-known volcano is found on Deception Island, which is famous for a giant eruption in 1970. Minor eruptions are frequent, and lava flow has been observed in recent years. Other dormant volcanoes may potentially be active. In 2004, a potentially active underwater volcano was found in the Antarctic Peninsula by American and Canadian researchers.\n\nAntarctica is home to more than 70 lakes that lie at the base of the continental ice sheet. Lake Vostok, discovered beneath Russia's Vostok Station in 1996, is the largest of these subglacial lakes. It was once believed that the lake had been sealed off for 500,000 to one million years, but a recent survey suggests that, every so often, there are large flows of water from one lake to another.\n\nThere is some evidence, in the form of ice cores drilled to about above the water line, that Lake Vostok's waters may contain microbial life. The frozen surface of the lake shares similarities with Jupiter's moon, Europa. If life is discovered in Lake Vostok, it would strengthen the argument for the possibility of life on Europa. On 7 February 2008, a NASA team embarked on a mission to Lake Untersee, searching for extremophiles in its highly alkaline waters. If found, these resilient creatures could further bolster the argument for extraterrestrial life in extremely cold, methane-rich environments.\n\nIn September 2018, researchers at the National Geospatial-Intelligence Agency released a high resolution terrain map (detail down to the size of a car, and less in some areas) of Antarctica, named the \"Reference Elevation Model of Antarctica\" (REMA).\n\nMore than 170 million years ago, Antarctica was part of the supercontinent Gondwana. Over time, Gondwana gradually broke apart, and Antarctica as we know it today was formed around 25 million years ago. Antarctica was not always cold, dry, and covered in ice sheets. At a number of points in its long history, it was farther north, experienced a tropical or temperate climate, was covered in forests, and inhabited by various ancient life forms.\n\nDuring the Cambrian period, Gondwana had a mild climate. West Antarctica was partially in the Northern Hemisphere, and during this period large amounts of sandstones, limestones and shales were deposited. East Antarctica was at the equator, where sea floor invertebrates and trilobites flourished in the tropical seas. By the start of the Devonian period (416 Ma), Gondwana was in more southern latitudes and the climate was cooler, though fossils of land plants are known from this time. Sand and silts were laid down in what is now the Ellsworth, Horlick and Pensacola Mountains. Glaciation began at the end of the Devonian period (360 Ma), as Gondwana became centred on the South Pole and the climate cooled, though flora remained. During the Permian period, the land became dominated by seed plants such as \"Glossopteris\", a pteridosperm which grew in swamps. Over time these swamps became deposits of coal in the Transantarctic Mountains. Towards the end of the Permian period, continued warming led to a dry, hot climate over much of Gondwana.\n\nAs a result of continued warming, the polar ice caps melted and much of Gondwana became a desert. In Eastern Antarctica, seed ferns or pteridosperms became abundant and large amounts of sandstone and shale were laid down at this time. Synapsids, commonly known as \"mammal-like reptiles\", were common in Antarctica during the Early Triassic and included forms such as \"Lystrosaurus\". The Antarctic Peninsula began to form during the Jurassic period (206–146 Ma), and islands gradually rose out of the ocean. Ginkgo trees, conifers, bennettites, horsetails, ferns and cycads were plentiful during this period. In West Antarctica, coniferous forests dominated through the entire Cretaceous period (146–66 Ma), though southern beech became more prominent towards the end of this period. Ammonites were common in the seas around Antarctica, and dinosaurs were also present, though only three Antarctic dinosaur genera (\"Cryolophosaurus\" and \"Glacialisaurus\", from the Hanson Formation, and \"Antarctopelta\") have been described to date. It was during this era that Gondwana began to break up.\n\nHowever, there is some evidence of antarctic marine glaciation during the Cretaceous period.\n\nThe cooling of Antarctica occurred stepwise, as the continental spread changed the oceanic currents from longitudinal equator-to-pole temperature-equalising currents to latitudinal currents that preserved and accentuated latitude temperature differences.\n\nAfrica separated from Antarctica in the Jurassic, around 160 Ma, followed by the Indian subcontinent in the early Cretaceous (about 125 Ma). By the end of the Cretaceous, about 66 Ma, Antarctica (then connected to Australia) still had a subtropical climate and flora, complete with a marsupial fauna. In the Eocene epoch, about 40 Ma Australia-New Guinea separated from Antarctica, so that latitudinal currents could isolate Antarctica from Australia, and the first ice began to appear. During the Eocene–Oligocene extinction event about 34 million years ago, CO levels have been found to be about 760 ppm and had been decreasing from earlier levels in the thousands of ppm.\n\nAround 23 Ma, the Drake Passage opened between Antarctica and South America, resulting in the Antarctic Circumpolar Current that completely isolated the continent. Models of the changes suggest that declining CO levels became more important. The ice began to spread, replacing the forests that then covered the continent.\n\nSince about 15 Ma, the continent has been mostly covered with ice.\n\nFossil \"Nothofagus\" leaves in the Meyer Desert Formation of the Sirius Group show that intermittent warm periods allowed \"Nothofagus\" shrubs to cling to the Dominion Range as late as 3–4 Ma (mid-late Pliocene). After that, the Pleistocene ice age covered the whole continent and destroyed all major plant life on it.\n\nThe geological study of Antarctica has been greatly hindered by nearly all of the continent being permanently covered with a thick layer of ice. However, new techniques such as remote sensing, ground-penetrating radar and satellite imagery have begun to reveal the structures beneath the ice.\n\nGeologically, West Antarctica closely resembles the Andes mountain range of South America. The Antarctic Peninsula was formed by uplift and metamorphism of sea bed sediments during the late Paleozoic and the early Mesozoic eras. This sediment uplift was accompanied by igneous intrusions and volcanism. The most common rocks in West Antarctica are andesite and rhyolite volcanics formed during the Jurassic period. There is also evidence of volcanic activity, even after the ice sheet had formed, in Marie Byrd Land and Alexander Island. The only anomalous area of West Antarctica is the Ellsworth Mountains region, where the stratigraphy is more similar to East Antarctica.\n\nEast Antarctica is geologically varied, dating from the Precambrian era, with some rocks formed more than 3 billion years ago. It is composed of a metamorphic and igneous platform which is the basis of the continental shield. On top of this base are coal and various modern rocks, such as sandstones, limestones and shales laid down during the Devonian and Jurassic periods to form the Transantarctic Mountains. In coastal areas such as Shackleton Range and Victoria Land some faulting has occurred.\n\nThe main mineral resource known on the continent is coal. It was first recorded near the Beardmore Glacier by Frank Wild on the Nimrod Expedition, and now low-grade coal is known across many parts of the Transantarctic Mountains. The Prince Charles Mountains contain significant deposits of iron ore. The most valuable resources of Antarctica lie offshore, namely the oil and natural gas fields found in the Ross Sea in 1973. Exploitation of all mineral resources is banned until 2048 by the Protocol on Environmental Protection to the Antarctic Treaty.\n\nAntarctica is the coldest of Earth's continents. It used to be ice-free until about 34 million years ago, when it became covered with ice. The coldest natural air temperature ever recorded on Earth was at the Soviet (now Russian) Vostok Station in Antarctica on 21 July 1983. For comparison, this is 10.7 °C (20 °F) colder than subliming dry ice at one atmosphere of partial pressure, but since CO only makes up 0.039% of air, temperatures of less than would be needed to produce dry ice snow in Antarctica. A lower air temperature of was recorded in 2010 by satellite—however, it may be influenced by ground temperatures and was not recorded at a height of above the surface as required for the official air temperature records. Antarctica is a frozen desert with little precipitation; the South Pole receives less than per year, on average. Temperatures reach a minimum of between and in the interior in winter and reach a maximum of between and near the coast in summer. Sunburn is often a health issue as the snow surface reflects almost all of the ultraviolet light falling on it. Given the latitude, long periods of constant darkness or constant sunlight create climates unfamiliar to human beings in much of the rest of the world.\nEast Antarctica is colder than its western counterpart because of its higher elevation. Weather fronts rarely penetrate far into the continent, leaving the centre cold and dry. Despite the lack of precipitation over the central portion of the continent, ice there lasts for extended periods. Heavy snowfalls are common on the coastal portion of the continent, where snowfalls of up to in 48 hours have been recorded.\n\nAt the edge of the continent, strong katabatic winds off the polar plateau often blow at storm force. In the interior, wind speeds are typically moderate. During clear days in summer, more solar radiation reaches the surface at the South Pole than at the equator because of the 24 hours of sunlight each day at the Pole.\n\nAntarctica is colder than the Arctic for three reasons. First, much of the continent is more than above sea level, and temperature decreases with elevation in the troposphere. Second, the Arctic Ocean covers the north polar zone: the ocean's relative warmth is transferred through the icepack and prevents temperatures in the Arctic regions from reaching the extremes typical of the land surface of Antarctica. Third, the Earth is at aphelion in July (i.e., the Earth is farthest from the Sun in the Antarctic winter), and the Earth is at perihelion in January (i.e., the Earth is closest to the Sun in the Antarctic summer). The orbital distance contributes to a colder Antarctic winter (and a warmer Antarctic summer) but the first two effects have more impact.\n\nThe aurora australis, commonly known as the southern lights, is a glow observed in the night sky near the South Pole created by the plasma-full solar winds that pass by the Earth. Another unique spectacle is diamond dust, a ground-level cloud composed of tiny ice crystals. It generally forms under otherwise clear or nearly clear skies, so people sometimes also refer to it as clear-sky precipitation. A sun dog, a frequent atmospheric optical phenomenon, is a bright \"spot\" beside the true sun.\n\nSeveral governments maintain permanent manned research stations on the continent. The number of people conducting and supporting scientific research and other work on the continent and its nearby islands varies from about 1,000 in winter to about 5,000 in the summer, giving it a population density between 70 and 350 inhabitants per million square kilometres (180 and 900 per million square miles) at these times. Many of the stations are staffed year-round, the winter-over personnel typically arriving from their home countries for a one-year assignment. An Orthodox church—Trinity Church, opened in 2004 at the Russian Bellingshausen Station—is manned year-round by one or two priests, who are similarly rotated every year.\nThe first semi-permanent inhabitants of regions near Antarctica (areas situated south of the Antarctic Convergence) were British and American sealers who used to spend a year or more on South Georgia, from 1786 onward. During the whaling era, which lasted until 1966, the population of that island varied from over 1,000 in the summer (over 2,000 in some years) to some 200 in the winter. Most of the whalers were Norwegian, with an increasing proportion of Britons. The settlements included Grytviken, Leith Harbour, King Edward Point, Stromness, Husvik, Prince Olav Harbour, Ocean Harbour and Godthul. Managers and other senior officers of the whaling stations often lived together with their families. Among them was the founder of Grytviken, Captain Carl Anton Larsen, a prominent Norwegian whaler and explorer who, along with his family, adopted British citizenship in 1910.\n\nThe first child born in the southern polar region was Norwegian girl Solveig Gunbjørg Jacobsen, born in Grytviken on 8 October 1913, and her birth was registered by the resident British Magistrate of South Georgia. She was a daughter of Fridthjof Jacobsen, the assistant manager of the whaling station, and Klara Olette Jacobsen. Jacobsen arrived on the island in 1904 and became the manager of Grytviken, serving from 1914 to 1921; two of his children were born on the island.\n\nEmilio Marcos Palma was the first person born south of the 60th parallel south (the continental limit according to the Antarctic Treaty), as well as the first one born on the Antarctic mainland, in 1978 at Base Esperanza, on the tip of the Antarctic Peninsula; his parents were sent there along with seven other families by the Argentine government to determine if the continent was suitable for family life. In 1984, Juan Pablo Camacho was born at the Frei Montalva Station, becoming the first Chilean born in Antarctica. Several bases are now home to families with children attending schools at the station. As of 2009, eleven children were born in Antarctica (south of the 60th parallel south): eight at the Argentine Esperanza Base and three at the Chilean Frei Montalva Station.\n\nFew terrestrial vertebrates live in Antarctica, and those that do are limited to the sub-Antarctic islands. Invertebrate life includes microscopic mites like the \"Alaskozetes antarcticus\", lice, nematodes, tardigrades, rotifers, krill and springtails. The flightless midge \"Belgica antarctica\", up to in size, is the largest purely terrestrial animal in Antarctica. The snow petrel is one of only three birds that breed exclusively in Antarctica.\n\nSome species of marine animals exist and rely, directly or indirectly, on the phytoplankton. Antarctic sea life includes penguins, blue whales, orcas, colossal squids and fur seals. The emperor penguin is the only penguin that breeds during the winter in Antarctica, while the Adélie penguin breeds farther south than any other penguin. The southern rockhopper penguin has distinctive feathers around the eyes, giving the appearance of elaborate eyelashes. King penguins, chinstrap penguins, and gentoo penguins also breed in the Antarctic.\n\nThe Antarctic fur seal was very heavily hunted in the 18th and 19th centuries for its pelt by sealers from the United States and the United Kingdom. The Weddell seal, a \"true seal\", is named after Sir James Weddell, commander of British sealing expeditions in the Weddell Sea. Antarctic krill, which congregate in large schools, is the keystone species of the ecosystem of the Southern Ocean, and is an important food organism for whales, seals, leopard seals, fur seals, squid, icefish, penguins, albatrosses and many other birds.\n\nA census of sea life carried out during the International Polar Year and which involved some 500 researchers was released in 2010. The research is part of the global Census of Marine Life and has disclosed some remarkable findings. More than 235 marine organisms live in both polar regions, having bridged the gap of . Large animals such as some cetaceans and birds make the round trip annually. More surprising are small forms of life such as sea cucumbers and free-swimming snails found in both polar oceans. Various factors may aid in their distribution – fairly uniform temperatures of the deep ocean at the poles and the equator which differ by no more than 5 °C, and the major current systems or marine conveyor belt which transport eggs and larval stages.\n\nAbout 1,150 species of fungi have been recorded from Antarctica, of which about 750 are non-lichen-forming and 400 are lichen-forming. Some of these species are cryptoendoliths as a result of evolution under extreme conditions, and have significantly contributed to shaping the impressive rock formations of the McMurdo Dry Valleys and surrounding mountain ridges. The apparently simple morphology, scarcely differentiated structures, metabolic systems and enzymes still active at very low temperatures, and reduced life cycles shown by such fungi make them particularly suited to harsh environments such as the McMurdo Dry Valleys. In particular, their thick-walled and strongly melanised cells make them resistant to UV light. Those features can also be observed in algae and cyanobacteria, suggesting that these are adaptations to the conditions prevailing in Antarctica. This has led to speculation that, if life ever occurred on Mars, it might have looked similar to Antarctic fungi such as \"Cryomyces antarcticus\", and \"Cryomyces minteri\". Some of these fungi are also apparently endemic to Antarctica. Endemic Antarctic fungi also include certain dung-inhabiting species which have had to evolve in response to the double challenge of extreme cold while growing on dung, and the need to survive passage through the gut of warm-blooded animals.\n\nAbout 298 million years ago Permian forests started to cover the continent, and tundra vegetation survived as late as 15 million years ago, but the climate of present-day Antarctica does not allow extensive vegetation to form. A combination of freezing temperatures, poor soil quality, lack of moisture, and lack of sunlight inhibit plant growth. As a result, the diversity of plant life is very low and limited in distribution. The flora of the continent largely consists of bryophytes. There are about 100 species of mosses and 25 species of liverworts, but only three species of flowering plants, all of which are found in the Antarctic Peninsula: \"Deschampsia antarctica\" (Antarctic hair grass), \"Colobanthus quitensis\" (Antarctic pearlwort) and the non-native \"Poa annua\" (annual bluegrass). Growth is restricted to a few weeks in the summer.\n\nSeven hundred species of algae exist, most of which are phytoplankton. Multicoloured snow algae and diatoms are especially abundant in the coastal regions during the summer. Bacteria have been found living in the cold and dark as deep as under the ice.\n\nThe Protocol on Environmental Protection to the Antarctic Treaty (also known as the Environmental Protocol or Madrid Protocol) came into force in 1998, and is the main instrument concerned with conservation and management of biodiversity in Antarctica. The Antarctic Treaty Consultative Meeting is advised on environmental and conservation issues in Antarctica by the Committee for Environmental Protection. A major concern within this committee is the risk to Antarctica from unintentional introduction of non-native species from outside the region.\n\nThe passing of the Antarctic Conservation Act (1978) in the U.S. brought several restrictions to U.S. activity on Antarctica. The introduction of alien plants or animals can bring a criminal penalty, as can the extraction of any indigenous species. The overfishing of krill, which plays a large role in the Antarctic ecosystem, led officials to enact regulations on fishing. The Convention for the Conservation of Antarctic Marine Living Resources (CCAMLR), a treaty that came into force in 1980, requires that regulations managing all Southern Ocean fisheries consider potential effects on the entire Antarctic ecosystem. Despite these new acts, unregulated and illegal fishing, particularly of Patagonian toothfish (marketed as Chilean Sea Bass in the U.S.), remains a serious problem. The illegal fishing of toothfish has been increasing, with estimates of 32,000 tonnes (35,300 short tons) in 2000.\n\nSeveral countries claim sovereignty in certain regions. While a few of these countries have mutually recognised each other's claims, the validity of these claims is not recognised universally.\n\nNew claims on Antarctica have been suspended since 1959, although in 2015 Norway formally defined Queen Maud Land as including the unclaimed area between it and the South Pole. Antarctica's status is regulated by the 1959 Antarctic Treaty and other related agreements, collectively called the Antarctic Treaty System. Antarctica is defined as all land and ice shelves south of 60° S for the purposes of the Treaty System. The treaty was signed by twelve countries including the Soviet Union (and later Russia), the United Kingdom, Argentina, Chile, Australia, and the United States. It set aside Antarctica as a scientific preserve, established freedom of scientific investigation and environmental protection, and banned military activity on Antarctica. This was the first arms control agreement established during the Cold War.\n\nIn 1983 the Antarctic Treaty Parties began negotiations on a convention to regulate mining in Antarctica. A coalition of international organisations launched a public pressure campaign to prevent any minerals development in the region, led largely by Greenpeace International, which operated its own scientific station—World Park Base—in the Ross Sea region from 1987 until 1991 and conducted annual expeditions to document environmental effects of humans on Antarctica. In 1988, the Convention on the Regulation of Antarctic Mineral Resources (CRAMRA) was adopted. The following year, however, Australia and France announced that they would not ratify the convention, rendering it dead for all intents and purposes. They proposed instead that a comprehensive regime to protect the Antarctic environment be negotiated in its place. The Protocol on Environmental Protection to the Antarctic Treaty (the \"Madrid Protocol\") was negotiated as other countries followed suit and on 14 January 1998 it entered into force. The Madrid Protocol bans all mining in Antarctica, designating Antarctica a \"natural reserve devoted to peace and science\".\nThe Antarctic Treaty prohibits any military activity in Antarctica, including the establishment of military bases and fortifications, military manoeuvres, and weapons testing. Military personnel or equipment are permitted only for scientific research or other peaceful purposes. The only documented military land manoeuvre has been the small Operation NINETY by the Argentine military in 1965.\n\nThe Argentine, British and Chilean claims all overlap, and have caused friction. On 18 December 2012, the British Foreign and Commonwealth Office named a previously unnamed area Queen Elizabeth Land in tribute to Queen Elizabeth II's Diamond Jubilee. On 22 December 2012, the UK ambassador to Argentina, John Freeman, was summoned to the Argentine government as protest against the claim. Argentine–UK relations had previously been damaged throughout 2012 due to disputes over the sovereignty of the nearby Falkland Islands, and the 30th anniversary of the Falklands War.\n\nThe areas shown as Australia's and New Zealand's claims were British territory until they were handed over following the countries' independence. Australia currently claims the largest area. The claims of Britain, Australia, New Zealand, France and Norway are all recognised by each other.\n\nOther countries participating as members of the Antarctic Treaty have a territorial interest in Antarctica, but the provisions of the Treaty do not allow them to make their claims while it is in force.\n\nThere is no economic activity in Antarctica at present, except for fishing off the coast and small-scale tourism, both based outside Antarctica.\n\nAlthough coal, hydrocarbons, iron ore, platinum, copper, chromium, nickel, gold and other minerals have been found, they have not been in large enough quantities to exploit. The 1991 Protocol on Environmental Protection to the Antarctic Treaty also restricts a struggle for resources. In 1998, a compromise agreement was reached to place an indefinite ban on mining, to be reviewed in 2048, further limiting economic development and exploitation. The primary economic activity is the capture and offshore trading of fish. Antarctic fisheries in 2000–01 reported landing 112,934 tonnes.\nSmall-scale \"expedition tourism\" has existed since 1957 and is currently subject to Antarctic Treaty and Environmental Protocol provisions, but in effect self-regulated by the International Association of Antarctica Tour Operators (IAATO). Not all vessels associated with Antarctic tourism are members of IAATO, but IAATO members account for 95% of the tourist activity. Travel is largely by small or medium ship, focusing on specific scenic locations with accessible concentrations of iconic wildlife. A total of 37,506 tourists visited during the 2006–07 Austral summer with nearly all of them coming from commercial ships; 38,478 were recorded in 2015–16.\n\nThere has been some concern over the potential adverse environmental and ecosystem effects caused by the influx of visitors. Some environmentalists and scientists have made a call for stricter regulations for ships and a tourism quota. The primary response by Antarctic Treaty Parties has been to develop, through their Committee for Environmental Protection and in partnership with IAATO, \"site use guidelines\" setting landing limits and closed or restricted zones on the more frequently visited sites. Antarctic sightseeing flights (which did not land) operated out of Australia and New Zealand until the fatal crash of Air New Zealand Flight 901 in 1979 on Mount Erebus, which killed all 257 aboard. Qantas resumed commercial overflights to Antarctica from Australia in the mid-1990s.\n\nAntarctic fisheries in 1998–99 (1 July – 30 June) reported landing 119,898 tonnes legally.\n\nAbout thirty countries maintain about seventy research stations (40 year-round or permanent, and 30 summer-only) in Antarctica, with an approximate population of 4000 in summer and 1000 in winter.\n\nThe ISO 3166-1 alpha-2 \"AQ\" is assigned to the entire continent regardless of jurisdiction. Different country calling codes and currencies are used for different settlements, depending on the administrating country. The Antarctican dollar, a souvenir item sold in the United States and Canada, is not legal tender.\n\nEach year, scientists from 28 different nations conduct experiments not reproducible in any other place in the world. In the summer more than 4,000 scientists operate research stations; this number decreases to just over 1,000 in the winter. McMurdo Station, which is the largest research station in Antarctica, is capable of housing more than 1,000 scientists, visitors, and tourists.\n\nResearchers include biologists, geologists, oceanographers, physicists, astronomers, glaciologists, and meteorologists. Geologists tend to study plate tectonics, meteorites from outer space, and resources from the breakup of the supercontinent Gondwana. Glaciologists in Antarctica are concerned with the study of the history and dynamics of floating ice, seasonal snow, glaciers, and ice sheets. Biologists, in addition to examining the wildlife, are interested in how harsh temperatures and the presence of people affect adaptation and survival strategies in a wide variety of organisms. Medical physicians have made discoveries concerning the spreading of viruses and the body's response to extreme seasonal temperatures. Astrophysicists at Amundsen–Scott South Pole Station study the celestial dome and cosmic microwave background radiation. Many astronomical observations are better made from the interior of Antarctica than from most surface locations because of the high elevation, which results in a thin atmosphere; low temperature, which minimises the amount of water vapour in the atmosphere; and absence of light pollution, thus allowing for a view of space clearer than anywhere else on Earth. Antarctic ice serves as both the shield and the detection medium for the largest neutrino telescope in the world, built below Amundsen–Scott station.\n\nSince the 1970s an important focus of study has been the ozone layer in the atmosphere above Antarctica. In 1985, three British scientists working on data they had gathered at Halley Station on the Brunt Ice Shelf discovered the existence of a hole in this layer. It was eventually determined that the destruction of the ozone was caused by chlorofluorocarbons (CFCs) emitted by human products. With the ban of CFCs in the Montreal Protocol of 1989, climate projections indicate that the ozone layer will return to 1980 levels between 2050 and 2070.\n\nIn September 2006 NASA satellite data revealed that the Antarctic ozone hole was larger than at any other time on record, at . The impacts of the depleted ozone layer on climate changes occurring in Antarctica are not well understood.\n\nIn 2007 The Polar Geospatial Center was founded. The Polar Geospatial Center uses geospatial and remote sensing technology to provide mapping services to American federally funded research teams. Currently, the Polar Geospatial Center can image all of Antarctica at 50 cm resolution every 45 days.\n\nOn 6 September 2007 Belgian-based International Polar Foundation unveiled the Princess Elisabeth station, the world's first zero-emissions polar science station in Antarctica to research climate change. Costing $16.3 million, the prefabricated station, which is part of the International Polar Year, was shipped to the South Pole from Belgium by the end of 2008 to monitor the health of the polar regions. Belgian polar explorer Alain Hubert stated: \"This base will be the first of its kind to produce zero emissions, making it a unique model of how energy should be used in the Antarctic.\" Johan Berte is the leader of the station design team and manager of the project which conducts research in climatology, glaciology and microbiology.\n\nIn January 2008 British Antarctic Survey (BAS) scientists, led by Hugh Corr and David Vaughan, reported (in the journal \"Nature Geoscience\") that 2,200 years ago, a volcano erupted under Antarctica's ice sheet (based on airborne survey with radar images). The biggest eruption in Antarctica in the last 10,000 years, the volcanic ash was found deposited on the ice surface under the Hudson Mountains, close to Pine Island Glacier.\n\nA study from 2014 estimated that during the Pleistocene, the East Antarctic Ice Sheet (EAIS) thinned by at least , and that thinning since the Last Glacial Maximum for the EAIS area is less than and probably started after c. 14 ka.\n\nMeteorites from Antarctica are an important area of study of material formed early in the solar system; most are thought to come from asteroids, but some may have originated on larger planets. The first meteorite was found in 1912, and named the Adelie Land meteorite. In 1969, a Japanese expedition discovered nine meteorites. Most of these meteorites have fallen onto the ice sheet in the last million years. Motion of the ice sheet tends to concentrate the meteorites at blocking locations such as mountain ranges, with wind erosion bringing them to the surface after centuries beneath accumulated snowfall. Compared with meteorites collected in more temperate regions on Earth, the Antarctic meteorites are well-preserved.\n\nThis large collection of meteorites allows a better understanding of the abundance of meteorite types in the solar system and how meteorites relate to asteroids and comets. New types of meteorites and rare meteorites have been found. Among these are pieces blasted off the Moon, and probably Mars, by impacts. These specimens, particularly ALH84001 discovered by ANSMET, are at the centre of the controversy about possible evidence of microbial life on Mars. Because meteorites in space absorb and record cosmic radiation, the time elapsed since the meteorite hit the Earth can be determined from laboratory studies. The elapsed time since fall, or terrestrial residence age, of a meteorite represents more information that might be useful in environmental studies of Antarctic ice sheets.\n\nIn 2006 a team of researchers from Ohio State University used gravity measurements by NASA's GRACE satellites to discover the Wilkes Land crater, which probably formed about 250 million years ago.\n\nIn January 2013 an meteorite was discovered frozen in ice on the Nansen ice field by a Search for Antarctic Meteorites, Belgian Approach (SAMBA) mission.\n\nIn January 2015 reports emerged of a circular structure, supposedly a meteorite crater, on the surface snow of King Baudouin Ice Shelf. Satellite images from 25 years ago seemingly show it.\n\nDue to its location at the South Pole, Antarctica receives relatively little solar radiation except along the southern summer. This means that it is a very cold continent where water is mostly in the form of ice. Precipitation is low (most of Antarctica is a desert) and almost always in the form of snow, which accumulates and forms a giant ice sheet which covers the land. Parts of this ice sheet form moving glaciers known as ice streams, which flow towards the edges of the continent. Next to the continental shore are many ice shelves. These are floating extensions of outflowing glaciers from the continental ice mass. Offshore, temperatures are also low enough that ice is formed from seawater through most of the year. It is important to understand the various types of Antarctic ice to understand possible effects on sea levels and the implications of global cooling.\n\nSea ice extent expands annually in the Antarctic winter and most of this ice melts in the summer. This ice is formed from the ocean water and floats in the same water and thus does not contribute to rise in sea level. The extent of sea ice around Antarctica has remained roughly constant in recent decades, although the thickness changes are unclear.\n\nMelting of floating ice shelves (ice that originated on the land) does not in itself contribute much to sea-level rise (since the ice displaces only its own mass of water). However, it is the outflow of the ice from the land to form the ice shelf which causes a rise in global sea level. This effect is offset by snow falling back onto the continent. Recent decades have witnessed several dramatic collapses of large ice shelves around the coast of Antarctica, especially along the Antarctic Peninsula. Concerns have been raised that disruption of ice shelves may result in increased glacial outflow from the continental ice mass.\n\nOn the continent itself, the large volume of ice present stores around 70% of the world's fresh water. This ice sheet is constantly gaining ice from snowfall and losing ice through outflow to the sea.\n\nSheperd et al. 2012, found that different satellite methods for measuring ice mass and change were in good agreement and combining methods leads to more certainty with East Antarctica, West Antarctica, and the Antarctic Peninsula changing in mass by +14 ± 43, –65 ± 26, and –20 ± 14 gigatonnes (Gt) per year. The same group's 2018 systematic review study estimated that ice loss across the entire continent was 43 gigatonnes per year on average during the period from 1992 to 2002 but has accelerated to an average of 220 gigatonnes per year during the five years from 2012 to 2017. NASA's Climate Change website indicates a compatible overall trend of greater than 100 gigatonnes of ice loss per year since 2002.\n\nA single 2015 study by H. Jay Zwally et al. found instead that the net change in ice mass is slightly positive at approximately 82 gigatonnes per year (with significant regional variation) which would result in Antarctic activity reducing global sea-level rise by 0.23 mm per year. However, one critic, Eric Rignot of NASA's Jet Propulsion Laboratory, states that this outlying study's findings \"are at odds with all other independent methods: re-analysis, gravity measurements, mass budget method, and other groups using the same data\" and appears to arrive at more precise values than current technology and mathematical approaches would permit.\n\nEast Antarctica is a cold region with a ground base above sea level and occupies most of the continent. This area is dominated by small accumulations of snowfall which becomes ice and thus eventually seaward glacial flows. The mass balance of the East Antarctic Ice Sheet as a whole is thought to be slightly positive (lowering sea level) or near to balance. However, increased ice outflow has been suggested in some regions.\n\nSome of Antarctica has been warming up; particularly strong warming has been noted on the Antarctic Peninsula. A study by Eric Steig published in 2009 noted for the first time that the continent-wide average surface temperature trend of Antarctica is slightly positive at >0.05 °C (0.09 °F) per decade from 1957 to 2006. This study also noted that West Antarctica has warmed by more than 0.1 °C (0.2 °F) per decade in the last 50 years, and this warming is strongest in winter and spring. This is partly offset by autumn cooling in East Antarctica. There is evidence from one study that Antarctica is warming as a result of human carbon dioxide emissions, but this remains ambiguous. The amount of surface warming in West Antarctica, while large, has not led to appreciable melting at the surface, and is not directly affecting the West Antarctic Ice Sheet's contribution to sea level. Instead the recent increases in glacier outflow are believed to be due to an inflow of warm water from the deep ocean, just off the continental shelf. The net contribution to sea level from the Antarctic Peninsula is more likely to be a direct result of the much greater atmospheric warming there.\n\nIn 2002 the Antarctic Peninsula's Larsen-B ice shelf collapsed. Between 28 February and 8 March 2008, about of ice from the Wilkins Ice Shelf on the southwest part of the peninsula collapsed, putting the remaining of the ice shelf at risk. The ice was being held back by a \"thread\" of ice about wide, prior to its collapse on 5 April 2009. According to NASA, the most widespread Antarctic surface melting of the past 30 years occurred in 2005, when an area of ice comparable in size to California briefly melted and refroze; this may have resulted from temperatures rising to as high as .\n\nA study published in \"Nature Geoscience\" in 2013 (online in December 2012) identified central West Antarctica as one of the fastest-warming regions on Earth. The researchers present a complete temperature record from Antarctica's Byrd Station and assert that it \"reveals a linear increase in annual temperature between 1958 and 2010 by 2.4±1.2 °C\".\n\nThere is a large area of low ozone concentration or \"ozone hole\" over Antarctica. This hole covers almost the whole continent and was at its largest in September 2008, when the longest lasting hole on record remained until the end of December. The hole was detected by scientists in 1985 and has tended to increase over the years of observation. The ozone hole is attributed to the emission of chlorofluorocarbons or CFCs into the atmosphere, which decompose the ozone into other gases.\n\nSome scientific studies suggest that ozone depletion may have a dominant role in governing climatic change in Antarctica (and a wider area of the Southern Hemisphere). Ozone absorbs large amounts of ultraviolet radiation in the stratosphere. Ozone depletion over Antarctica can cause a cooling of around 6 °C in the local stratosphere. This cooling has the effect of intensifying the westerly winds which flow around the continent (the polar vortex) and thus prevents outflow of the cold air near the South Pole. As a result, the continental mass of the East Antarctic ice sheet is held at lower temperatures, and the peripheral areas of Antarctica, especially the Antarctic Peninsula, are subject to higher temperatures, which promote accelerated melting. Models also suggest that the ozone depletion/enhanced polar vortex effect also accounts for the recent increase in sea ice just offshore of the continent.\n\n\n"}
{"id": "44329400", "url": "https://en.wikipedia.org/wiki?curid=44329400", "title": "Anthony Costello", "text": "Anthony Costello\n\nAnthony Costello (born 20 February 1953) is a British paediatrician. Until 2015. Costello was Professor of International Child Health and Director of the Institute for Global Health at the University College London. Costello was most notable for his work on improving survival among mothers and their newborn infants in poor populations of developing countries.\n\nCostello was born in Beckenham, and graduated from school at St Joseph's Academy. Costello attended St Catharine's College, Cambridge where he took a degree in Experimental Psychology and qualified as a doctor in Medical Sciences after clinical training at the Middlesex Hospital in London. He then trained in Paediatrics and Neonatology at University College London.\n\nHe and his wife, Helen, have two sons, Harry and Ned, and one daughter, Freya.<ref name=\"https://www.thelancet.com/journals/lancet/article/PIIS0140-6736(09)60929-6/fulltext\"></ref>\n\nAfter living in Baglung district in western Nepal from 1984–1986, two days walk from a road, he became fascinated by challenges to mother and child health in poor, remote populations. His areas of scientific expertise include the evaluation of cost-effective interventions to reduce maternal and newborn deaths, women’s groups, strategies to tackle malnutrition, international aid and the health effects of climate change. In 1999 he published a pioneering book on how to improve newborn infant health in developing countries.\n\nWith a Nepali organisation (MIRA), that he helped to establish, a large community trial of participatory learning and action using women’s groups in the remote mountains of Makwanpur district, Nepal was published in The Lancet in 2004. He went on to establish partnerships and further studies with local organisations in eastern India, Mumbai, Bangladesh and Malawi. Seven cluster randomised controlled trials of women’s groups in Nepal, India, Bangladesh and Malawi, led to a meta-analysis published in the Lancet in May 2013.\n\nResults showed that in populations where more than 30% of pregnant women joined the women's group programme, maternal death and newborn deaths were cut by one third. The intervention has now been recommended by the World Health Organisation for scale-up in poor, rural populations.\nCostello chaired the 2009 Lancet Commission on Managing the Health Effects of Climate Change, and was co-chair of a new Lancet Commission which links the UK, China, Norway and Sweden on emergency actions to tackle the climate health crisis, published in June 2015.\n\nAt WHO he has helped to lead the Global Strategy for Women’s, Children’s and Adolescents’ Health (2016‒2030) with its three objectives of survive, thrive and transform – to end preventable mortality, to promote health and well-being, and to expand enabling environments. Its guiding principles include equity, universality, human rights, development effectiveness and sustainability.\n\nWith the WHO team he has also launched the global accelerated action for the health of adolescents (AA-HA!) and established an expert review group called Maternal and Newborn Information for tracking Outcomes and Results (MONITOR) to harmonize maternal and newborn health indicators.\n\nIn February 2017, together with UNICEF and UNFPA he helped to launched the Network for Improving Quality of Care for Maternal, Newborn and Child Health to introduce evidence-based interventions to improve quality of care for maternal and newborn health supported by a learning system. The Network involves Ethiopia, Nigeria, India, Bangladesh, Malawi, Côte d'Ivoire, Uganda, Tanzania and Ghana. He also leads work on community empowerment for family health - what it means, how to measure it, and how to plan interventions at district level.\n\nWith the Lancet he is a co-chair of their new Countdown Commission on Climate Change which reports progress annually on climate change adaptation, mitigation, economics, energy policy and public engagement. With UNICEF he is helping WHO to coordinate a new Lancet Commission on redesigning child health for the Sustainable Development Goals era.\n\nCostello holds fellowships of the Academy of Medical Sciences and of the Royal College of Physicians. In April 2011, Costello received the James Spence Medal, the highest honour of the Royal College of Paediatrics and Child Health where he is a fellow. He serves on the Board of the global Partnership for Maternal, Newborn & Child Health, chaired by Dr Graca Machel. In May 2016 he received the BMJ Lifetime Achievement Award.\n\n"}
{"id": "1724862", "url": "https://en.wikipedia.org/wiki?curid=1724862", "title": "Capsella bursa-pastoris", "text": "Capsella bursa-pastoris\n\nCapsella bursa-pastoris, known by its common name shepherd's purse because of its triangular flat fruits, which are purse-like, is a small annual and ruderal flowering plant in the mustard family Brassicaceae that grows up to tall. It is native to eastern Europe and Asia minor, but is naturalized and considered a common weed in many parts of the world, especially in colder climates, including British Isles, where it is regarded as an archaeophyte, North America and China, but also in the Mediterranean and North Africa. \"C. bursa-pastoris\" is the second most common weed in the world.\n\n\"C. bursa-pastoris\" plants grow from a rosette of lobed leaves at the base. From the base emerges a stem about tall, which bears a few pointed leaves which partly grasp the stem. The flowers, which appear in any month of the year in the British Isles, are white and small, in diameter, with four petals and six stamens. They are borne in loose racemes, and produce flattened, two-chambered seed pods known as siliques, which are triangular to heart-shaped, each containing several seeds.\n\nLike a number of other plants in several plant families, its seeds contain a substance known as mucilage, a condition known as myxospermy. The adaptive value of myxospermy is unknown, although the fact that mucilage becomes sticky when wet has led some to propose that \"C. bursa-pastoris\" traps insects which then provide nutrients to the seedling, which would make it protocarnivorous.\n\n\"Capsella bursa-pastoris\" is closely related to the model organism such as \"Arabidopsis thaliana\" and is also used as a model organism, due to the variety of genes expressed throughout its life cycle that can be compared to genes that are well studied in \"A. thaliana\". Unlike most flowering plants, it flowers almost all year round. Like other annual ruderals exploiting disturbed ground, \"C. bursa-pastoris\" reproduces entirely from seed, has a long soil seed bank, and short generation time, and is capable of producing several generations each year.\n\nIt was formally described by the Swedish botanist Carl Linnaeus in his seminal publication 'Species Plantarum' in 1753, and then published by Friedrich Kasimir Medikus in \"Pflanzen-Gattungen\" (Pfl.-Gatt.) on page 85 in 1792.\n\n\"Capsella bursa-pastoris\" subsp. \"thracicus\" (Velen.) Stoj. & Stef. is the only known subspecies.\n\nWilliam Coles (botanist) wrote in his book, 'Adam in Eden' (published in 1657), \"It is called Shepherd's purse or Scrip (wallet) from the likeness of the seed hath with that kind of leathearne bag, wherein Shepherds carry their Victualls [food and drink] into the field.\"\"\nIn England and Scotland, it was once commonly called 'mother's heart', which is derived from a child's game/trick of picking the seed pod, which then would burst and the child would be accused of 'breaking his mother's heart'.\n\nCommon on cultivated ground, waysides and meadows, disturbed land and roadworks.\n\n\"C. bursa-pastoris\" is gathered from the wild, or grown. It has many uses, including for food, to supplement animal feed, for cosmetics, and in traditional medicine. It is cultivated as a commercial food crop in Asia.\n\nIn Chinese, this plant is known as \"jìcài\" (; ). It is commonly used in food in Shanghai and the surrounding Jiangnan region, where it is stir-fried with rice cakes and other ingredients or as part of the filling in wontons.\n\nIt is one of the ingredients of the symbolic dish consumed in the Japanese spring-time festival, \"Nanakusa-no-sekku\".\n\nIn Korea, it is known as \"naengi\" () and used as a root vegetable in the characteristic Korean dish, namul (fresh greens and wild vegetables).\n\nFumaric acid is one chemical substance that has been isolated from \"C. bursa-pastoris\".\n\n\n\n"}
{"id": "31950461", "url": "https://en.wikipedia.org/wiki?curid=31950461", "title": "Centre for Health and International Relations", "text": "Centre for Health and International Relations\n\nThe Centre for Health and International Relations (CHAIR) was founded (2003) in the belief that there are compelling reasons for linking international relations, foreign policy, security and health. CHAIR is based in the Department of International Politics, Aberystwyth University, Aberystwyth, Wales. The founder and director is Professor Colin McInnes.\n\nIn addition to research into the global politics of health broadly defined, CHAIR is also involved in research in the following areas:\n\n\nAssociated staff working on the ongoing 'The Transformation of Global Health Governance: Competing World Views and Crises' project from the London School of Hygiene and Tropical Medicine:\n\n"}
{"id": "41413797", "url": "https://en.wikipedia.org/wiki?curid=41413797", "title": "Countess of Dufferin Fund", "text": "Countess of Dufferin Fund\n\nThe Countess of Dufferin Fund was established by Hariot Hamilton-Temple-Blackwood, Marchioness of Dufferin and Ava, more commonly known as Lady Dufferin, in 1885 and was dedicated to improving women’s healthcare in India. The Fund was founded after Queen Victoria gave Lady Dufferin the task of improving healthcare for women in India. The Fund provided scholarships for women to be educated in the medical field as doctors, hospital assistants, nurses, and midwives. It also financed the construction of female hospitals, dispensaries, and female only wards in preexisting hospitals. The Fund marks the beginning of Western medicine for women in India and global health as a diplomatic concern.\n\nDuring the 19th century there was a major push in India to improve healthcare for women, especially maternal health. Lying-in hospitals were built as well as training and teaching hospitals. Many hospitals were also constructing wards for women and learning to treat female-specific diseases.\n\nIn 1885, Lady Dufferin set up the Fund after being contacted by Queen Victoria who gave her the task of helping the suffering women of India. Queen Victoria had been recently contacted by Elizabeth Bielby, a missionary in India who focused on women’s health. During Beilby’s mission, she had treated the Maharani of Puna who gave her a message to relay to the Queen of England. The message said that “the women of India suffer when they are sick.” In response, Queen Victoria wrote back to the Maharani saying: Lady Dufferin then started the Fund after being summoned to Windsor Castle by the Queen who gave her the task of improving healthcare and education for the women of India. A visit from Dr. Mary Scharlieb, the first female British doctor to practice in India, also mobilized the Queen to act on women’s poor health and suffering in India. She met with Queen Victoria and expressed a similar message as the Maharni’s: the dire situation of Indian women.\n\nLady Dufferin established the Fund in 1885 and immediately began creating projects and channeling money towards women’s health and teaching in India. The Countess of Dufferin Fund is also known as “The National Association for Supplying Medical Aid to the Women of India” and the “Lady Dufferin Fund.” This Fund marked one of the first diplomatic pushes to improve global health in the world, and the introduction of western medicine for women in India.\n\nThe Fund had three primary goals: providing medical tuition, medical relief, and female nurses and midwives to assist in hospitals and private homes. The Fund supplied scholarships for the medical education (medical tuition) of women in India. The education of traditional Indian midwives, called dias, was a major goal of the Fund because many western doctors observed the dais’s practices and found their traditions to be harmful. For example, the dais’s would massage the abdomen of the mother to speed up labor however that tradition caused uterine prolapse, a widespread issue amongst Indian women at the time. Because the dias’ methods were viewed as violent and extremely harmful, the Fund put forth money to educate them on successful ways to help women before, during, and after childbirth. The Fund also provided medical relief by establishing dispensaries and cottage hospitals for women and children under female superintendence. In addition, it opened female wards in existing hospitals also under female management as well as all women hospitals called zenana hospitals. The Fund also supplied trained female nurses and midwives in hospitals and in private homes.\n\nThe Fund financed treatment and teaching hospitals in Bihar, Calcutta, Madras, Karachi, Delhi, Bombay and in many of the United Provinces (roughly present day Uttar Pradesh and Uttarakhand). Most notably, the Fund sponsored the Lady Aitchison Hospital in Lahore, India. The Lady Aitchison Hospital, also known as the Aitchison Memorial Hospital, was a major center for training nurses and tradition midwives like the dias. Many of the hospitals the Fund financed are still functioning today. For example, the Lady Dufferin Hospital in Karachi is the largest solely female dedicated hospital today in Pakistan.\n\nThe Fund’s major financial basis was donations. Many saw this as a source of instability because donations were based on the popularity of Lady Dufferin and her husband as well as the favors expected by the donors in return. The administration of the Fund consisted of a central committee of members of the Viceroy’s Council and Home Department. It also included many influential Englishmen and Indians such as the Maharja Sir Jotendro Mohun Tagore, Sir Syed Ahmed Khan, and Sir Dinshaw Maneckji Petit.\n\nThere were three major criticisms of the Fund: its teaching, effectiveness, and integrity. Many believed the Fund was inefficient. Some argued that they inadequately taught doctors and employed subpar medical practitioners. By October 1908, only 43 completely qualified women medical professionals were working under the Fund however only 11 held university degrees. The British Medical Journal wrote in 1908 that, “The Government appears to have a perfect delight in swamping the country with unqualified medical practitioners.\" The Fund was also criticized as ineffective for placing male doctors in Zenana hospitals. Zenana women’s traditions forbade them from seeing, so they could be treated by the doctors the Fund provided. In addition, the Fund was also criticized for not giving the women they were educating enough reason to stay with learning medicine. The Fund paid for their education however their salaries were not high enough so it made more financial sense for the women to marry before they could give back to the hospital that educated them. The Fund set up this cycle of educating women who would then work for them, however, it was ineffective in some cases because the women would not stick with the medical profession for financial reasons. Because of the many criticisms of ineffectiveness, some of the Fund’s highest critics like The British Medical Journal called for a reorganization of the Fund. Some also believed vanity, not philanthropy, was the source of motivation for establishing the Fund. They criticized both Lady Dufferin for starting the Fund for her personal image and Queen Victoria for supporting the Fund for Britain’s international image.\n\nThe Fund continued even after Lady Dufferin’s term ended. Lady Lansdowne, who succeeded Lady Dufferin, continued to put work into the Fund which was passed down from vicereine to vicereine until 1947. In 1947, India gained independence from Great Britain and the Fund was taken over by the Central Indian Government by the Countess of Dufferin’s Fund Act, 1957. Once the Fund was taken over by the Central Government, it became obsolete. In 2005 a bill was passed repealing the Countess of Dufferin’s Fund Act, 1957, for unclear reasons.\n\n"}
{"id": "30250507", "url": "https://en.wikipedia.org/wiki?curid=30250507", "title": "Discrete logarithm records", "text": "Discrete logarithm records\n\nDiscrete logarithm records are the best results achieved to date in solving the discrete logarithm problem, which is the problem of finding solutions \"x\" to the equation \"g\" = \"h\" given elements \"g\" and \"h\" of a finite cyclic group \"G\". The difficulty of this problem is the basis for the security of several cryptographic systems, including Diffie–Hellman key agreement, ElGamal encryption, the ElGamal signature scheme, the Digital Signature Algorithm, and the elliptic curve cryptography analogs of these. Common choices for \"G\" used in these algorithms include the multiplicative group of integers modulo \"p\", the multiplicative group of a finite field, and the group of points on an elliptic curve over a finite field.\n\nOn 16 June 2016, Thorsten Kleinjung, Claus Diem, Arjen K. Lenstra, Christine Priplata, and Colin Stahlke announced the computation of a discrete logarithm modulo a 232-digit (768-bit) safe prime, using the number field sieve. The computation was started in February 2015 and took approximately 6600 core years scaled to an Intel Xeon E5-2660 at 2.2 GHz.\n\nPrevious records for integers modulo \"p\" include:\n\n\nThe current record () in a finite field of characteristic 2 was announced by Robert Granger, Thorsten Kleinjung, and Jens Zumbrägel on 31 January 2014. This team was able to compute discrete logarithms in GF(2) using about 400,000 core hours. New features of this computation include a modified method for obtaining the logarithms of degree two elements and a systematically optimized descent strategy.\n\nPrevious records in a finite field of characteristic 2 were announced by:\n\n\nThe current record () in a finite field of characteristic 2 of prime degree was announced by Thorsten Kleinjung on 17 October 2014. The calculation was done in a field of 2 elements and followed essentially the path sketched for formula_1 in with two main exceptions in the linear algebra computation and the descent phase. The total running time was less than four core years. The previous record in a finite field of characteristic 2 of prime degree was announced by the CARAMEL group on April 6, 2013. They used the function field sieve to compute a discrete logarithm in a field of 2 elements.\n\nThe current record () for a field of characteristic 3 was announced by \nGora Adj, Isaac Canales-Martinez, Nareli Cruz-Cortés, Alfred Menezes, Thomaz Oliveira, Francisco Rodriguez-Henriquez, and Luis Rivera-Zamarripa on . The calculation was done in the 4841-bit finite field with 3 elements and was performed on several computers at CINVESTAV and\nthe University of Waterloo. In total, about 200 core years of computing time was expended on the computation.\nPrevious records in a finite field of characteristic 3 were announced:\n\n\nOver fields of \"moderate\"-sized characteristic, notable computations as of 2005 included those a field of 65537 elements (401 bits) announced on 24 Oct 2005, and in a field of 370801 elements (556 bits) announced on 9 Nov 2005. The current record (as of 2013) for a finite field of \"moderate\" characteristic was announced on 6 January 2013. The team used a new variation of the function field sieve for the medium prime case to compute a discrete logarithm in a field of 33341353 elements (a 1425-bit finite field). The same technique had been used a few weeks earlier to compute a discrete logarithm in a field of 33553771 elements (an 1175-bit finite field).\n\nOn 25 June 2014, Razvan Barbulescu, Pierrick Gaudry, Aurore Guillevic, and François Morain announced a new computation of a discrete logarithm in a finite field whose order has 160 digits and is a degree 2 extension of a prime field. The algorithm used was the number field sieve (NFS), with various modifications. The total computing time was equivalent to 68 days on one core of CPU (sieving) and 30 hours on a GPU (linear algebra).\n\nCerticom Corp. has issued a series of Elliptic Curve Cryptography challenges. Level I involves fields of 109-bit and 131-bit sizes. Level II includes 163, 191, 239, 359-bit sizes. All Level II challenges are currently believed to be computationally infeasible.\n\nThe Level I challenges which have been met are:\n\n\nNone of the 131-bit (or larger) challenges have been met .\n\nIn July 2009, Joppe W. Bos, Marcelo E. Kaihara, Thorsten Kleinjung, Arjen K. Lenstra and Peter L. Montgomery announced that they had carried out a discrete logarithm computation on an elliptic curve modulo a 112-bit prime. The computation was done on a cluster of over 200 PlayStation 3 game consoles over about 6 months. They used the common parallelized version of Pollard rho method.\n\nIn April 2014, Erich Wenger and Paul Wolfger from Graz University of Technology solved the discrete logarithm of a 113-bit Koblitz curve in extrapolated 24 days using an 18-core Virtex-6 FPGA cluster. In January 2015,the same researchers solved the discrete logarithm of an elliptic curve defined over a 113-bit binary field. The average runtime is around 82 days using a 10-core Kintex-7 FPGA cluster.\n\nOn 2 December 2016, Daniel J. Bernstein, Susanne Engels, Tanja Lange, Ruben Niederhagen, Christof Paar, Peter Schwabe, and Ralf Zimmermann announced the solution of a generic 117.35-bit elliptic curve discrete logarithm problem on a binary curve, using an optimized FPGA implementation of a parallel version of Pollard's rho algorithm. The attack ran for about six months on 64 to 576 FPGAs in parallel.\n\nOn 23 August 2017, Takuya Kusaka, Sho Joichi, Ken Ikuta, Md. Al-Amin Khandaker, Yasuyuki Nogami, Satoshi Uehara, Nariyoshi Yamai, and Sylvain Duquesne announced that they had solved a discrete logarithm problem on a 114-bit \"pairing-friendly\" Barreto-Naehrig (BN) curve, using the special sextic twist property of the BN curve to efficiently carry out the random walk of Pollard’s rho method. The implementation used 2000 CPU cores and took about 6 months to solve the problem.\n\n"}
{"id": "3104716", "url": "https://en.wikipedia.org/wiki?curid=3104716", "title": "Dispatches (radio program)", "text": "Dispatches (radio program)\n\nDispatches was a Canadian radio program, which aired on CBC Radio One from 2001 to 2012. The program, which was hosted by Rick MacInnes-Rae, aired documentary reports on international news and feature topics. Its opening music was an excerpt from Mark Knopfler's song \"What It Is.\" \n\nOriginally a half-hour program, the program expanded to a one-hour format in April 2007, incorporating some features of CBC Radio's cancelled world music series \"Global Village\".\n\nOn April 10, 2012, the CBC announced it would cancel the program as part of its budget cuts as a result of the 2012 Canadian federal budget.\n\nThe last program aired on June 21, 2012 and was repeated June 24.\n"}
{"id": "16173391", "url": "https://en.wikipedia.org/wiki?curid=16173391", "title": "GMA Life TV", "text": "GMA Life TV\n\nGMA Life TV, is a Filipino Channel incorporated in February 2008 by GMA Network, is an international Filipino television channel, and the company's secondary international channel to GMA Pinoy TV.\n\nThe programming of GMA Pinoy TV consists mostly of shows from the Philippines from GMA Network and GMA News TV as well as previously aired shows, documentaries, films, and sports events from the Philippines. The delay of GMA Pinoy TV's and GMA Life TV's dramas all air on one-episode delay basis. Most weekend shows are up to date, with the exception of some shows that air on a one-episode delay basis.\n\nThis secondary channel carries programming from GMA News TV in the Philippines and some GMA Network proper's programming.\n\nIt was launched 16 February 2008 in Japan and is available in the Middle East, North Africa, and Southern Europe through Orbit Showtime.\n\nIn June 2008 it was launched in Australia and New Zealand on UBI World TV. It is now also available in the whole United States via Dish Network and DirecTV.\n\nOn 2 November 2009, it was launched in Hawaii via Oceanic Time Warner Cable on 'a la carte' basis or packaged with another Tagalog station (GMA Pinoy TV or other channels) for Digital Cable subscribers and GMA Life TV also available via Time Warner Cable in the United States.\n\nIn September 2010, GMA Life TV has 122,000 subscribers while its sister channel, GMA Pinoy TV has a total of 256,000 subscribers. This is based on a report that the Corporate Communications Department of GMA Network has posted.\n\nRecently, as of 3rd Quarter of 2010, GMA Life TV has around 126,000 subscribers. The number of subscribers began to substantially increase when the channel was launched during the early part of this year in other areas such us Florida (via Brighhouse Tampa), Texas (via En-Touch), Hong Kong (via PCCW), and Central California (via Comcast).\n\nBy 2011, GMA Life TV will air re-runs of previous GMA shows as QTV will be re-formatted into GMA News TV. GMA News TV will also be available in the worldwide market.\n\nThis did occur from 20 February 2011 onward, the decision was to run movies temporarily in place of the previously aired programs from the now-ceased QTV.\n\nOn 28 February 2011 for the first time in GMA Worldwide, Inc. history, two programs that started airing on GMA Pinoy TV that is still running with new episodes was switched over to GMA Life TV, where it will then air its new episodes, to occupy free time slots. This then led to replays of other Kapuso shows. The shows included Pinoy MD and Love ni Mister, Love ni Misis. All this will be straightened out once GMA News TV starts its international operations sometime 2nd to 3rd Quarter of 2011.\n\nIn a report published showing GMA Network's revenues for 2010 with a total of PhP 14.299 Billion, GMA Life TV has 125,000 subscribers (an estimated 800,000 viewers count) while GMA Pinoy TV has an accounted 273,000 subscribers (around 1.8 Million viewers).\n\nOn 10 March 2011 GMA Pinoy TV and GMA Life TV expanded even more in the United States in Florida. The Florida counties that are part of the extension include Brevard, Flagler, Lake, Marion, Orange, Osceola, Seminole, Sumter and Volusia.\n\nIn a recent report, dated 16 May 2011, regarding the consolidated gross revenues of GMA Network, Inc. in the first quarter of 2011, it is noted that GMA Pinoy TV's subscriptions grew by 10 percent (approximately 300,000 subscribers) and GMA Life TV's subscriptions grew by 2 percent (approximately 128,000), respectively. This consisted of PhP 230 million worth of advertising and subscription revenues.\n\nAnd in May/June 2011, GMA Pinoy TV/GMA Life TV sent many of its artists to various places worldwide, it's the first time that Kapuso artists were sent almost consecutively, to celebrate Philippine Independence Day/Filipino Culture Fiestas and to promote GMA Pinoy TV/GMA Life TV and its shows. They consisted of Iza Calzado & Dennis Trillo who went to Hawaii on 7 May 2011, then on 28 May 2011, Jolina Magdangal and JayR visited National City in San Diego, CA. On the 113th Philippine Independence Day proper, Dennis Trillo & Iza Calzado were sent this time to Hong Kong to represent GMA Pinoy TV and perform there and in Singapore, Dingdong Dantes and Rachel Ann Go were present. And on 26 June 2011, Aljur Abrenica, Jennylyn Mercado, Iza Calzado and Mark Bautista will be sent to be part of Philippine-American Friendship Day in Jersey City, New Jersey.\n\nIn another recent report with regards to GMA Network's consolidated revenues in the 2nd quarter of 2011, GMA Pinoy TV's subscribers grew to 279,000 from 274,000 (actual count) and an estimated amount of 1.85 million viewers and GMA Life TV's subscribers grew as well, an estimated 1 million viewers. Also, GMA Network recently launched GMA News TV in the past month and will be made available to all areas where GMA Pinoy TV and GMA Life TV reaches by 3rd Quarter of 2011. GMA Pinoy TV and GMA Life TV were further made to subscribers in key Filipino-American areas in the US, namely New Jersey, Philadelphia, Pennsylvania, Virginia, Maryland and Washington DC. They were also launched in many parts of Detroit, Hawaii and upstate New York and New England. The channels were also launched by additional carriers in Australia and Papua New Guinea.\n\nOn 22 September 2011, GMA Life TV will air two previously-aired GMA programs dubbed in English, they are Una Kang Naging Akin to be renamed in English as \"When You Were Mine\" and Impostora to be renamed \"Impostor\". This is the first time that GMA's dramas will be dubbed and re-aired in English.\n\nIn a press statement released by GMA Network, Inc., it is stated that GMA Life TV, as of 1st Quarter of 2012, has approximately 117,000 subscribers, which amounts to nearly 1.2 million viewers.\n\nFor the first time in GMA history, Idol sa Kusina, a program originally aired on QTV then GMA News TV and is simulcasted on GMA Life TV moves to GMA Pinoy TV on 14 July 2012.\n\nSingTel mio TV will launch this channel with effect from 14 June 2013 at 00:00 hrs SST.\n\nOn 22 December 2013 GMA Life TV started airing its first Tagalog-dubbed Koreanovela, Full House, followed by A Rosy Life on 20 January 2014.\n\nOn 13 March 2014 GMA Life TV began airing its third Tagalog-dubbed Koreanovela Sassy Girl, followed by Cruel Love on 7 April 2014 and Dal-ja's Spring on 14 April 2014.\n\nBasta Every Day, Happy! was supposed to premiere on 12 May 2014 (in some territories like the US on the evening of 11 May 2014) and air throughout its entire duration on GMA Pinoy TV but the programming department of GMA International, GMA's international subsidiary, decided to move the show over to GMA Life TV as it adheres more to GMA Life TV's overall theme as a lifestyle channel as well as the show is a replacement for , so the programming department had seen that the move was done rightfully so. On 30 June 2014, A Rosy Life re-airs for the second time on GMA Life TV, followed by the second re-run of Sassy Girl on 22 September 2014 and Cruel Love on 17 November 2014.\n\n\n"}
{"id": "44023814", "url": "https://en.wikipedia.org/wiki?curid=44023814", "title": "German Parliamentary Committee investigation of the NSA spying scandal", "text": "German Parliamentary Committee investigation of the NSA spying scandal\n\nThe German Parliamentary Committee investigation of the NSA spying scandal (official title: \"1. Untersuchungsausschuss „NSA“\") was started on March 20, 2014, by the German Parliament in order to investigate the extent and background of foreign secret services spying in Germany in the light of the Global surveillance disclosures (2013–present).\nThe Committee is also in search of strategies on how to protect telecommunication with technical means.\n\nThe committee is formed by eight members of the German Parliament. The parliamentarian of the Christian Democratic Union (CDU) Clemens Binninger was head of the committee but stepped down after six days. In a statement, Binninger clarified that the other committee members had insisted on inviting Edward Snowden to testify; Binninger objected to this and resigned in protest. Patrick Sensburg (CDU) succeeded him. \n\nOn May 8, 2014, the committee unanimously decided to let US whistleblower Edward Snowden testify as a witness.\n\nOn September 23, 2014, the Green Party and The Left filed a constitutional complaint against the Christian Democratic Union, the Social Democrats and the NSA Parliamentary Committee because of the Christian Democrats' and the Social Democrats' refusal to let the witness Edward Snowden testify in Berlin. The accused proposed a video conference from Moscow which Snowden had refused.\n\nOn September 28, 2014, the Green Party and The Left filed a constitutional complaint against German chancellor Merkel. According to them, she refuses to comply with her duty according to Chapter 44 of the German constitution to ensure a real investigation; especially by refusing to ensure the legal requirements to allow the witness Edward Snowden to testify.\n\nOn July 3, 2014, the former Technical Director of the NSA, William Binney, who had become a whistleblower after the terrorist attacks of September 11, 2001, testified to the committee. He said that the NSA has a totalitarian approach that has previously only been known from dictatorships and that there is no longer such a thing as privacy. Former NSA employee Thomas Andrews Drake described the close cooperation between the NSA and the German foreign intelligence service BND.\n\nThe US journalist Glenn Greenwald was asked to testify in September, 2014. On August 1, 2014, he wrote in a letter that he was willing to support the Parliament's investigation on the espionage in Germany by the NSA. By their refusal to let the crucial witness Edward Snowden testify, German politicians had however shown that it is more important to them not to annoy the US than to really investigate and he was not willing to take part in a \"ritual\" that \"shall seem like an investigation\".\n\nIn Operation Eikonal German BND agents received \"Selector Lists\" from the NSA − search terms for their dragnet surveillance. They contain IP addresses, mobile phone numbers and email accounts with the BND surveillance system containing hundreds of thousands and possibly more than a million such targets. These lists have been subject of controversy as in 2008 it was revealed that they contained some terms targeting the European Aeronautic Defence and Space Company (EADS), the Eurocopter project as well as French administration, which were first noticed by BND employees in 2005. Other selectors were found to target the administration of Austria. After the revelations made by whistleblower Edward Snowden the BND decided to investigate the issue whose October 2013 conclusion was that at least 2,000 of these selectors were aimed at Western European or even German interests which has been a violation of the Memorandum of Agreement that the US and Germany signed in 2002 in the wake of the 9/11 terror attacks. After reports emerged in 2014 that EADS and Eurocopter had been surveillance targets the Left Party and the Greens filed an official request to obtain evidence of the violations.\n\nThe investigative Parliamentary Committee was set up in spring 2014 and reviewed the selectors and discovered 40,000 suspicious search parameters, including espionage targets in Western European governments and numerous companies. The group also confirmed suspicions that the NSA had systematically violated German interests and concluded that the Americans could have perpetrated economic espionage directly under the Germans' noses. The investigative parliamentary committee was not granted access to the NSA's selectors list as an appeal led by opposition politicians failed at Germany's top court - instead the ruling coalition appointed an administrative judge, Kurt Graulich, as a \"person of trust\" who was granted access to the list and briefed the investigative commission on its contents after analyzing the 40,000 parameters. In his almost 300-paged report Graulich concluded that European government agencies were targeted massively and that Americans hence broke contractual agreements. He also found that German targets which received special protection from surveillance of domestic intelligence agencies by Germany's Basic Law (Grundgesetz) − including numerous enterprises based in Germany − were featured in the NSA's wishlist in a surprising plenitude. While the magnitude differs there have also been problematic BND-internal selectors which have been used until end of 2013 - around two thirds of 3300 targets were related to EU and NATO states. Klaus Landefeld, member of the board at the Internet industry association Eco International, has met intelligence officials and legislators to present suggestions for improvement, like streamlining the selector system.\nOn July 4, 2014, it was revealed to the public that BND agent Markus R. had been arrested on July 2, 2014, for apparently having spied. The 31-year-old German is accused of having worked for the CIA. After his arrest, the US ambassador John B. Emerson was summoned for talks at the German Foreign Office.\n\nIt was revealed that the BND-agent had saved 218 secret BND documents on USB sticks since 2012 and sold them to US agents for a sum of 25,000 Euro in Salzburg, Austria. At least three of the documents were about the NSA Parliamentary Committee.\nThe Federal Office for the Protection of the Constitution had mistaken him for a Russian spy and asked US colleagues to help uncover him.\n\nOn July 9, 2014, a second US spy was discovered, who worked for the Secretary of Defense.\n\nIn July 2014 a Parliament technician found out that the mobile phone of Roderich Kiesewetter, representative of the Christian Democratic Union in the committee, was spied on. Kiesewetter said there is evidence that all four Party representatives in the committee have been spied on.\n\nIn the months following May 2015, Peter Pilz, a member of the Austrian parliament for the Green Party, disclosed several documents and transcripts related to operation Eikonal, in which NSA and BND cooperated for tapping transit cables at a facility of Deutsche Telekom in Frankfurt. These documents were highly sensitive and handed over to the committee for investigating the Eikonal operation. Therefore, it seems likely someone from the committee leaked these documents to Pilz. Among them are lists of communication channels from many European countries, including most of Germany's neighbours. Peter Pilz also discovered NSA spying facilities in Austria, and therefore wants a parliamentary committee on the NSA spying in his own country too.\n\nOn December 1, 2016, WikiLeaks released over 2,400 documents which it claims are from the investigation.\n\n"}
{"id": "28919960", "url": "https://en.wikipedia.org/wiki?curid=28919960", "title": "Global Strategy for Women's and Children's Health", "text": "Global Strategy for Women's and Children's Health\n\nThe Global Strategy for Women's and Children's Health was a program of the United Nations (UN) directed at improving women's and children's health in the developing world.\n\nThe program was announced by UN Secretary-General Ban Ki-moon in September 2010. At the time of the announcement, the program was valued at $US40 billion over a five-year period, funded by state and private donors, with the UN hoping for more pledges to follow. The objective of the program was to save the lives of 16 million people during the period of the program. As the Millennium Development Goals 4, 5 and 6 were showing the slowest rate of progression, this program was also instituted to gain momentum in achieving them. The implementation of the program was led by the World Health Organization, reporting to the UN.\n\nThe aid-based program was accompanied by pledges from some developing nations (including Tanzania and Rwanda) to increase their own domestic spending on health care. According to the UN, around $8.6 million of the program's funding came from what it described as \"low-income countries\".\n\nInternational aid group Oxfam expressed doubts about the program, including the extent to which its funding was genuinely new.\n\nThe Global Strategy managed to improve the coordination of global efforts towards the improvement of women’s and children’s health as well as enhancing strategies to tackle this. It gave rise to the “Every Woman Every Child” movement, which assisted in the mobilisation of stakeholders. Despite not reaching targets, reductions in both child and maternal mortality were noted; of 49% and 45% respectively from 1990 to 2013. 2.4 million maternal and child deaths were prevented.  Prevention of mother-to-child transmission of HIV, oral rehydration therapy and exclusive breastfeeding coverage showed the most significant improvement, however, progress in vaccination and pneumonia was still lagging.\n\nThe United Nations also reported increased collaboration with the private sector, with an increase in donor funding of US$19.8 billion noted from September 2010 to May 2014.\n\nThe Global Health Strategy for Women, Children and Adolescents 2016-2030 was launched in September 2015, building on experience gained from 2010-2015, with the inclusion of adolescents as an additional target group. It is aligned with the Sustainable Development Goals (SDGs) and has 3 main objectives; namely for women, adolescents and children to \"survive\", \"thrive\" and \"transform\". It is an evidence-based, multi-sectoral approach and emphasises the need to address equity, with interventions applied across the life-course. \n\nAlthough high returns on investment are projected, a monitoring report done in May 2018 shows that expected targets may not be achieved in time.\n\n"}
{"id": "43616057", "url": "https://en.wikipedia.org/wiki?curid=43616057", "title": "Global citizenship education", "text": "Global citizenship education\n\nGlobal citizenship education (GCE) is a form of civic learning that involves students' active participation in projects that address global issues of a social, political, economic, or environmental nature. The two main elements of GCE are 'global consciousness'; the moral or ethical aspect of global issues, and 'global competencies', or skills meant to enable learners to participate in changing and developing the world. The promotion of GCE was a response by governments and NGOs to the emergence of supranational institution, regional economic blocs, and the development of information and communications technologies. These have allresulted in the emergence of a more globally oriented and collaborative approach to education. GCE addresses themes such as peace and human rights, intercultural understanding, citizenship education, respect for diversity and tolerance, and inclusiveness.Schools in action, global citizens for sustainable development: a guide for students\n\nGlobal citizenship consists of voluntary practices oriented to human rights, social justice, and environmentalism at the local, regional, and global level. Unlike national citizenship, global citizenship does not denote any legal status or allegiance to an actual form of government. The emergence of regional economic blocs, supra-national political institutions such as the European Union, and the advancement of ICTs, has caused governments to try to prepare national populations to be competitive in the global jobs market. This has led to the introduction of global citizenship education programs at primary, secondary, and tertiary level, but also at independent NGOs, grass roots organizations, and other large scale educational organizations, such as the International Baccalaureate Organization and UNESCO.\n\nThe most important features of global citizenship education are voluntary action that can extend from local to international collectives; the practice of cultural empathy; and a focus on active participation in social and political life at the local and global level. In the late 1990s, OXFAM UK designed a curriculum for global citizenship education\nwhich stressed \"the 'active' role of global citizens\".In this approach, individuals and groups both inside and outside the educational sector might take action that addresses human rights, trade, poverty, health, and environmental issues, for example. This is sometimes called the 'global consciousness' aspect of GCE. However, organizations such as UNESCO have also begun to emphasize 'global competencies', including science and technology into their GCE curricula, to \"strengthen linkages between education and economic development\".\n\nIn the present era of globalization, the recognition of global interdependence on the part of the general public has led to a higher degree of interest in global citizenship in education. No Though modern schooling may have been oriented to education suitable for the nation-state throughout the 19th and 20th centuries, in the 21st century, citizenship is understood in global terms, so that schooling might improve individual nations' global competitiveness. Many universities worldwide have responded to the need for a globally oriented education by sending their students to study abroad in increasing numbers, and some have announced that this will soon become a mandatory degree requirement.\n\nMany governments also now promote GCE for the cohesion of society. The large numbers of people migrating across national borders means that the diversity of ethnic, religious, and linguistic groups, \"has raised [...] complex and difficult questions about citizenship, human rights, democracy, and education\". In addition, global issues related to sustainability, such as the world's future energy arrangements, have also been incorporated into the domain of global citizenship education.\n\nWhile GCE can take different forms, it has some common elements, which include fostering in learners the following competences:\n\nMost educators agree that \"global citizenship is a learned and nurtured behavior\", and the most widely used classroom strategy for developing global skills is project-based learning. This pedagogical technique can be utilized in the case of almost any school subject, \"[and] is the primary pedagogical strategy in the discourse of global competencies. Educators see it as an important method for developing the tools- technical and emotional- for success in the global society\". With the aim of nurturing students' potential to be both learners and citizens, the project-based approach has been used successfully in community-based learning, for example.\n\nAnother important pedagogical feature of GCE is learning through communicative practices outside the classroom that \"harness […] the educational force of the wider culture\". If students are encouraged \"to see themselves as political agents\", educators assume they are more likely to acquire the knowledge, skills and abilities that enable them to become agents of change. Another important element of the student-centered participatory nature of GCED, is that students, through their engagement with others via Social Network Services, create their own forms of global citizenship through dialogue, learning, and action. This is an important element, for example, in the activities of grassroots organizations like 'GIN' (Global Issues Network), which involves students and teachers in projects that address global issues such as human rights, trade rules, and deforestation. Such student-driven, student-led projects combine both the 'global consciousness' and 'global competence' aspects of GCED.\n\n\nOrganizations implementing GCE programs, such as UNESCO, now emphasize the importance of expanding both students' 'global consciousness' and 'global competence'. 'Global consciousness' represents the ethical or moral dimension of global citizenship, whereas 'global competence' \"features a blend of the technical-rational and the dispositional or attitudinal\".\n\nHowever, some view global consciousness and global competence as being closely related. The OECD, for instance, focuses on global competencies called 'psychosocial resources', of which there are three main types: \"using tools interactively (technology and language skills), interacting in heterogeneous groups (cooperation, empathy), and acting autonomously (realizing one's identity, conducting life plans, defending and asserting rights\".\n\nGCE identifies three learner attributes, which refer to the traits and qualities that global citizenship education aims to develop in learners and correspond to the key learning outcomes mentioned earlier. These are: informed and critically literate; socially connected and respectful of diversity; ethically responsible and engaged. The three learner attributes draw on a review of the literature and of citizenship education conceptual frameworks, a review of approaches and curricula, as well as technical consultations and recent work by UNESCO on global citizenship education.\n\nLearners develop their understanding of the world, global themes, governance structures and systems, including politics, history and economics; understand the rights and responsibilities of individuals and groups (for example, women’s and children’s rights, indigenous rights, corporate social responsibility); and, recognise the interconnectedness of local, national and global issues, structures and processes. Learners develop the skills of critical inquiry (for example, where to find information and how to analyse and use evidence), media literacy and an understanding of how information is mediated and communicated. They develop their ability to inquire into global themes and issues (for example, globalisation, interdependence, migration, peace and conflict, sustainable development) by planning investigations, analysing data and communicating their findings. A key issue is the way in which language is used and, more specifically, how critical literacy is affected by the dominance of the English language and how this influences non-English speakers’ access to information.\n\nLearners learn about their identities and how they are situated within multiple relationships (for example, family, friends, school, local community, country), as a basis for understanding the global dimension of citizenship. They develop an understanding of difference and diversity (for example, culture, language, gender, sexuality, religion), of how beliefs and values influence people’s views about those who are different, and of the reasons for, and impact of, inequality and discrimination. Learners also consider common factors that transcend difference, and develop the knowledge, skills, values and attitudes required for respecting difference and living with others.\n\nLearners explore their own beliefs and values and those of others. They understand how beliefs and values inform social and political decision-making at local, national, regional and global levels, and the challenges for governance of contrasting and conflicting beliefs and values. Learners also develop their understanding of social justice issues in local, national, regional and global contexts and how these are interconnected. Ethical issues (for example, relating to climate change, consumerism, economic globalisation, fair trade, migration, poverty and wealth, sustainable development, terrorism, war) are also addressed. Learners are expected to reflect on ethical conflicts related to social and political responsibilities and the wider impact of their choices and decisions. Learners also develop the knowledge, skills, values and attitudes to care for others and the environment and to engage in civic action. These include compassion, empathy, collaboration, dialogue, social entrepreneurship and active participation. They learn about opportunities for engagement as citizens at local, national and global levels, and examples of individual and collective action taken by others to address global issues and social injustice.\n\nHigh Resolves is a secondary school educational initiative (implemented by the FYA, the only national, independent non-profit organisation for young people in Australia) consisting of a Global Citizenship Programme for Year 8 students and a Global Leadership Programme for Year 9 and 10 students. It aims to enable students to consider their personal role in developing their society as a global community through workshops, simulations, leadership skills training and hands-on action projects.\n\nIn England, the Department for Education and Skills produced \"Developing the global dimension in the school curriculum\", a publication for head teachers, teachers, senior managers and those with responsibility for curriculum development. It aims to show how the global dimension can be integrated in the curriculum and across the school. It provides examples of how to integrate the global dimension from age 3 to age 16, outlining eight key concepts – global citizenship, conflict resolution, diversity, human rights, interdependence, sustainable development, values and perceptions, and social justice. For example, it gives guidance for the promotion of personal, social and emotional development of the youngest learners through discussion of photographs of children from around the world, activities, stories, and discussion of different places children have visited.\n\nActivate is a network of young leaders in South Africa which aims to bring about change though creative solutions to problems in society. Youth from all backgrounds and provinces in the country participate in a two-year programme. In the first year, there are three residential training programmes, working on a particular task. In the second year, participants form action groups on specific tasks, taking their work into the public domain. In one example, an Activator describes how he works in his local community to discourage young people away from joining gangs and engaging in substance abuse. He draws on his own negative experiences with gangs and drugs, having served seven years in jail. On being interviewed, he states: “My vision for South Africa is to see young people standing up and becoming role models... Be yourself, be real and pursue your dreams”.\n\nCouncil for Global Citizenship Education — part of the Global Citizenship Foundation, a non-profit organisation based in India — is an initiative for schools to adopt a participatory whole-school approach to global citizenship education. The initiative fosters continuous professional development (CPD) of educators; teacher led contextualization, design, and development of GCED curriculum; engagement of children through the '100 Acts of Global Citizenship' School Challenge; and community through a Global Citizenship Festival at '100 Acts of Global Citizenship' participating schools. It has also been implemented in 10 States of India.\n\nPeace First, a non-profit organisation based in the United States, has a programme in which youth volunteers work with children to design and implement community projects in a participatory way. The rationale is that children are natural creative thinkers and problem solvers. The programme focuses on developing social and emotional skills of self- awareness, empathy, inclusivity and relationships. It has also been implemented in rural areas of Colombia through a partnership between local governments and Colombian NGOs. Peace First has additionally developed a curriculum that can be used in schools. It addresses themes such as friendship, fairness, cooperation, con ict resolution and consequences of actions through experiential activities and cooperative games. For example, 1st graders learn about communicating their feelings, 3rd graders develop skills and awareness around communication and cooperation, 4th graders practice courage and taking a stand and 5th graders explore how to resolve and de-escalate conflicts.\n\nTokyo Global Engineering Corporation is an education-services organization that provides capstone education programs free of charge to engineering students and other stakeholders. These programs are intended to complement—but not to replace—coursework required by academic degree programs of study. The programs are educational opportunities, and students are not paid money for their participation. All correspondence among members is completed via e-mail, and all meetings are held via Skype, with English as the language of instruction and publication. Students and other stakeholders are never asked to travel or leave their geographic locations, and are encouraged to publish organizational documents in their personal, primary languages, when English is a secondary language.\n\nGCE and ESD pursue the same vision: It is all about empowering learners of all ages to become proactive contributors to a more just, peaceful, tolerant, inclusive and sustainable world. Both GCED and ESD:\nBoth GCE and ESD help learners understand the interconnected world in which they live and the complexities of the global challenges faced. GCE and ESD help learners to develop their knowledge, skills, attitudes and values so that they can address these challenges responsibly and effectively now and in the future.\n\nSome fundamentalist critics believe GCE might undermine religious education and promote secular values. Others are concerned that the pedagogical approach of most global citizenship education curricula are too often produced in particular Northern, Western contexts. Some critics claim that GCE curricula promote values that are too individualistic. Dill, for example, claims that \"the majority of the world experiences social and communal life not in terms of isolated individuals, but as collective identities and traditions. For many of these groups, the dominant forms of global citizenship education and its moral order will be experienced as coercive and unjust', so 'global' citizenship curriculum should be seen as a local practice, \"which diverse cultures will conceptualize and construct differently\".\n\n\n"}
{"id": "46260828", "url": "https://en.wikipedia.org/wiki?curid=46260828", "title": "Global silver trade from the 16th to 18th centuries", "text": "Global silver trade from the 16th to 18th centuries\n\nThe global silver trade between the Americas and Europe from the sixteenth to nineteenth centuries was a spillover of the Columbian Exchange which had a profound effect on the world economy. In fact, many scholars consider the silver trade to mark the beginning of a genuinely global economy, with one historian noting that silver \"went round the world and made the world go round.\" Although global, much of that silver ended up in the hands of the Chinese, as they accepted it as a form of currency. In addition to the global economic changes the silver trade engendered, it also put into motion a wide array of political transformations in the early modern era. \"New World mines,\" concluded several prominent historians \"supported the Spanish empire,\" acting as a linchpin of the Spanish economy.\n\nSpaniards at the time of the Age of Exploration discovered vast amounts of silver, much of which was from the Potosí silver mines, to fuel their trade economy. Potosí's deposits were rich and Spanish American silver mines were the world's cheapest sources of it. The Spanish acquired the silver, minting it into the peso de ocho (a currency) to then use it as a means of purchase; that currency was so widespread that even the United States accepted it as valid until the Coinage Act of 1857. As the Spanish need for silver increased, new innovations for more efficient extraction of silver were developed, such as the amalgamation method of using mercury to extract silver from ore.\n\nIn the two centuries that followed the discovery of Potosí, the Spanish silver mines in the Americas produced 40,000 tons of silver. Altogether, more than 150,000 tons of silver were shipped from Potosí by the end of the 18th century. From 1500 to 1800, Mexico and Peru produced about 80% of the world's silver with 30% of it eventually ending up in China (largely because of British merchants who used it to purchase exotic Chinese commodities). In the late 16th and early 17th century, Japan was also exporting heavily into China and the foreign trade at large.\n\nAs has been demonstrated, China dominated silver imports. The market value of silver in the Ming territory was double its value elsewhere, which provided great arbitrage profit for the Europeans and Japanese. The abundance of silver in China made it easy for the country to mint it into coinage. That process was so widespread that local Chinese government officials would demand taxes to be paid in silver to the point that silver eventually backed all of China's economy.\n\nThe world's first paper money (\"flying money\") was invented by the Chinese and they needed some commodity to back it. Traditional coins were useful, but the amount of coins needed for large purchases could be bulky and dangerous to transport. That problem was solved when the Chinese created small pieces of paper with pictures of the coin printed on them. By the nature of their geography, China had no real amount of precious metals of their own to back the paper money they invented. Because the Spaniards didn't find gold but did find copious amounts of silver, the Spaniards and the rest of Europe used this silver to purchase the commodities of choice from China, solving both of their problems.\n\nA result of the Spanish colonization of the Americas was the discovery, production, and trade of precious metals. The Spanish, along with other European nations, had a great desire for Chinese goods such as silk and porcelain. The Europeans did not have any goods or commodities which China desired, so they traded silver to make up for their trade deficit. The two most important mining colonies of the Spanish Empire were Peru and Mexico, who were estimated to have provided one-hundred thousand tons of silver from the mid 16th Century to the end of the colonial period. The richest, and most productive mine in the Americas was that of Potosí in what is modern day Bolivia. The richest camp in Mexico was in the city of Zacatecas, however the production of this mine was far less than that of Potosi.\n\nMore simplistic native mining techniques dominated American mining for the early part of the 16th Century. Mining in the Americas became formally industrialized when the process of mercury amalgamation became popularized. Mercury amalgamation was invented by a Spaniard in central Mexico in the 1550s. Historians dispute what individual was the first to invent the process, however most agree that it was a Spaniard. Mercury was the one of the highest costs of production for the Americas, since much of it had to be shipped. The ratio of Mercury to silver produced was about two to one. Furthermore, German miners introduced the stamp mill and lead smelting in the 1530s. Gunpowder was often used to blast of large holes to create the mine shafts, although there were not many deep shafts. Potosí had the most amount of ore, however it was lower quality than that of Mexico.\n\nMining production in the Americas largely depended on native labor in both Mexico and Peru. In Mexico, many of the natives worked as wage laborers by the middle of the 17th Century. However, the labor system known as the repartimiento still existed in some places. Silver production in Mexico was relatively cheap when compared to that of Peru, and the general trend of Mexican labor systems was that towards waged labor. In Peru mines, the mit'a system was a dominant form of native labor subjection, although waged laborers worked on the mines as well. Natives under the mit'a system were paid much less, and this was necessary for the production of silver to continue in Peru where costs were relatively high.\n\nThe ultimate destination for the mass amounts of silver produced in the Americas and Japan was China. Silver from the Americas flowed mostly across the Atlantic and made its way to the far east. A popular route was around the Cape of Good Hope into the east, and sometimes it came over land. Major outposts for the silver trade were located in Southeast Asian countries, such as the Philippines. The city of Manilla served as a primary outpost of the exchange of goods between the Americas, Japan, and China. However, there is a large amount of silver that crossed across the Pacific Ocean directly from the Americas as well. There are not many records of the amount of silver which crossed the Pacific due to it being discouraged by the Spanish monarchy, so estimates highly vary.\n\nSilver also found its way across other parts of the world as well. India and Europe both received a fair amount of silver. This silver was often locally traded for other commodities, such as gold or crops. In India, silver flowed from the south to the north, and gold flowed the opposite way. Often silver and gold were manufactured into jewelry or hoarded as treasure.\n\nChina was the ultimate destination in which silver would flow towards. In exchange, the Chinese traded their popular goods such as silk and porcelain. China had a high demand for silver due to its shift from paper money to coins in the early period of the Ming Dynasty. The Ming paper currency eventually failed due to self-imposed inflation along with an inability to stop the production of counterfeit bills. The Ming attempted to produce copper coins as a new form of currency, but production was inconsistent. Hence silver became of high value because it was a valid currency that could be processed abroad. The bimetallic ratio of silver to gold was about two to one, which meant that European and Japanese merchants made a large amount of profit. In the 1640s, the bimetallic ratios in China converged with the rest of the world, before experiencing another population boom. The new population boom was a product of the introduction of New World crops into China, mainly sweet potatoes, which could be more easily grown. By this time, the silver mines in Japan were largely depleted and the New World became China's primary source for silver.\n\nInitially, Japan served as China's primary source for silver in the 16th Century. In exchange for silver, China would provide Japan with silk and gold. Japan and China did not directly trade with each other, due to political tensions. This meant that European entities and countries, such as the Dutch and Portuguese served as a middle man between the two countries.\n\nIn the famed \"The Wealth of Nations,\" Adam Smith noted the sheer force and great reach of the global silver trade. He was impressed by its market value but more intrigued with the way this single item of commerce brought together new and old worlds i.e. the Americas and China. Although China acted as the cog running the wheel of global trade, Japan's huge contribution of silver exports to China were critical to the world economy and China's liquidity and success with the commodity. Historians posit Europeans would have been left out of world trade, and China may have fallen prey to conquest by settlers of the Americas if not for Japanese silver mining. Silver was paramount to East Asia's introduction into the global trade market. Under the Ming and Qing empires, China hoarded silver to boost its economy and increase its trading power.\n\nMany historians argue that silver was responsible for the birth of global economics and trade. According to this view, global trade commenced in 1571 when Manila was founded and became the first trading post linking America and Asia due to the expansive and profitable silver trade. In fact, research shows the amount of silver traveling from Manila to China was approximately three million pesos or 94,000 kilograms in the early 1600s.\n\nThe rarity of silver production was seen as an opportunity for China to control the currency's value and support its own national currency. Silver was one of the only accepted trade items from Europeans and its value in China was astronomical compared to rest of the world. In fact, its value was twice that of Spain in the 16th and 17th centuries. Between 1600 and 1800 China received 100 tons of silver on average per year. A large populace near the Lower Yangzte averaged a hundreds of taels of silver per household in the late 16th century.\n\nSilver even played a large role when defending Toyotomi Hideyoshi's attemptive take over of Ming ruled Joseon Korea. The Ming Ministiry of War sent approximately 140,000 liang of silver to its soldiers and required provinces to provide silver as tax for the war effort as well. In the sixteenth century, the daimyos of Southwest Japan hoped for unhinged global trade but were stopped due to Ming China trade policies. Still, Japan became a player in the global economy via frequent Ming merchant ships arriving to extract Japan's abundance of silver and exchange goods. Japan increased its wealth through successful trilateral trade with Portugal and China as Japan now had Chinese goods to offer the Portuguese who had silver mines of their own. Founder of the Ming China dynasty, Hongwu, actually sought to eliminate silver from the market due to his fear of inflation which he previously experienced in the Yuan dynasty. His attempt involved imposing harsh limits on silver mining to stop its flow into the market and subsequently replaced it with baochao or paper money. However, the currency never popularized and silver proved its mainstay as a global currency.\n\nDespite some restrictions, silver continued to drive trade through its popularity in Europe. This, combined with a high British demand for Chinese tea, created chronic trade deficits for European governments, which were forced to risk silver deficits to supply merchants in Asia. As supplies of silver decreased in Europe, Europeans had less ability to purchase highly coveted Chinese goods. Merchants were no longer able to sustain the China trade through profits made by selling Chinese goods in the West and were forced to take bullion out of circulation in Europe to buy goods in China.\n\nIn the 19th century, American merchants began to introduce opium to Chinese markets. The demand for opium rose rapidly and was so profitable that Chinese opium dealers began to seek out more suppliers of the drug, thus inaugurating the opium trade; one merchant declared that Opium \"is like gold. It can sell any time.\" From 1804 to 1820, a period when the Qing Dynasty needed to finance the suppression of the White Lotus Rebellion, Chinese merchants were soon exporting silver to pay for opium rather than Europeans paying for Chinese goods with the precious metal.\n\nThe Qing imperial court debated whether and how to end the opium trade, eventually settling on regulations on consumption. That measure, however, resulted in an increase in drug smuggling by Europeans and Chinese traders. In 1810, the Daoguang Emperor issued an edict concerning the matter, declaring, \"Opium has a harm. Opium is a poison, undermining our good customs and morality. Its use is prohibited by law.\" Following a debate at court in 1836 on whether to legalize the drug or crack down on its use, the emperor decided on the latter. An upright official, Commissioner Lin Zexu led the campaign against opium as a kind of \"drug czar.\" The British, offended by the seizure of their property in opium, sent a large naval expedition to China to end the restrictive conditions under which they had long traded with that country. Thus began the first Opium War, in which Britain's industrialized military might was proven in China's rout. The Treaty of Nanking, which ended the war in 1842 largely on British terms, imposed numerous restrictions on Chinese sovereignty and opened five ports to European traders.\n\n"}
{"id": "2306731", "url": "https://en.wikipedia.org/wiki?curid=2306731", "title": "Health human resources", "text": "Health human resources\n\nHealth human resources (HHR) – also known as human resources for health (HRH) or health workforce – is defined as \"all people engaged in actions whose primary intent is to enhance health\", according to the World Health Organization's \"World Health Report 2006\". Human resources for health are identified as one of the core building blocks of a health system. They include physicians, nursing professionals, midwives, dentists, allied health professions, community health workers, social health workers and other health care providers, as well as health management and support personnel – those who may not deliver services directly but are essential to effective health system functioning, including health services managers, medical records and health information technicians, health economists, health supply chain managers, medical secretaries and others. \n\nThe field of health human resources deals with issues such as planning, development, performance, management, retention, information, and research on human resources for the health care sector. In recent years, raising awareness of the critical role of HRH in strengthening health system performance and improving population health outcomes has placed the health workforce high on the global health agenda.\n\nThe World Health Organization (WHO) estimates a shortage of almost 4.3 million physicians, midwives, nurses and support workers worldwide. The shortage is most severe in 57 of the poorest countries, especially in sub-Saharan Africa. The situation was declared on World Health Day 2006 as a \"health workforce crisis\" – the result of decades of underinvestment in health worker education, training, wages, working environment and management. \n\nShortages of skilled for health workers are also reported in many specific care areas. For example, there is an estimated shortage of 1.18 million mental health professionals, including 55,000 psychiatrists, 628,000 nurses in mental health settings, and 493,000 psychosocial care providers needed to treat mental disorders in 144 low- and middle-income countries. Shortages of skilled birth attendants in many developing countries remains an important barrier to improving maternal health outcomes. Many countries, both developed and developing, report maldistribution of skilled health workers leading to shortages in rural and underserved areas.\n\nRegular statistical updates on the global health workforce situation are collated in the WHO \"Global Health Observatory\". However, the evidence base remains fragmented and incomplete, largely related to weaknesses in the underlying human resource information systems (HRIS) within countries.\n\nIn order to learn from best practices in addressing health workforce challenges and strengthening the evidence base, an increasing number of HHR practitioners from around the world are focusing on issues such as HHR advocacy, surveillance and collaborative practice. Some examples of global HRH partnerships include:\n\nHealth workforce research is the investigation of how social, economic, organizational, political and policy factors affect access to health care professionals, and how the organization and composition of the workforce itself can affect health care delivery, quality, equity, and costs. \n\nMany government health departments, academic institutions and related agencies have established research programs to identify and quantify the scope and nature of HHR problems leading to health policy in building an innovative and sustainable health services workforce in their jurisdiction. Some examples of HRH information and research dissemination programs include:\n\n\nIn some countries and jurisdictions, health workforce planning is distributed among labour market participants. In others, there is an explicit policy or strategy adopted by governments and systems to plan for adequate numbers, distribution and quality of health workers to meet health care goals. For one, the International Council of Nurses reports:\nThe objective of HHRP [health human resources planning] is to provide the right number of health care workers with the right knowledge, skills, attitudes, and qualifications, performing the right tasks in the right place at the right time to achieve the right predetermined health targets.\n\nAn essential component of planned HRH targets is supply and demand modeling, or the use of appropriate data to link population health needs and/or health care delivery targets with human resources supply, distribution and productivity. The results are intended to be used to generate evidence-based policies to guide workforce sustainability. In resource-limited countries, HRH planning approaches are often driven by the needs of targeted programmes or projects, for example, those responding to the Millennium Development Goals or, more recently, the Sustainable Development Goals.\n\nThe WHO \"Workload Indicators of Staffing Need\" (WISN) is an HRH planning and management tool that can be adapted to local circumstances. It provides health managers a systematic way to make staffing decisions in order to better manage their human resources, based on a health worker's workload, with activity (time) standards applied for each workload component at a given health facility.\n\nThe main international policy framework for addressing shortages and maldistribution of health professionals is the \"Global Code of Practice on the International Recruitment of Health Personnel\", adopted by the WHO's 63rd World Health Assembly in 2010. The Code was developed in a context of increasing debate on international health worker recruitment, especially in some higher income countries, and its impact on the ability of many developing countries to deliver primary health care services. Although non-binding on the Member States and recruitment agencies, the Code promotes principles and practices for the ethical international recruitment of health personnel. It also advocates the strengthening of health personnel information systems to support effective health workforce policies and planning in countries.\n\n\n"}
{"id": "50835353", "url": "https://en.wikipedia.org/wiki?curid=50835353", "title": "Health systems strengthening", "text": "Health systems strengthening\n\nHealth systems strengthening (also health system strengthening, abbreviated HSS) is a term used in global health that roughly means to improve the health care system of a country. Within this general definition, it can mean increasing funding for health infrastructure, improving health policy, trying to achieve universal healthcare, or any number of other health measures.\n\nThere has been some effort to use a systems thinking approach to health systems strengthening.\n\nVarious health organizations have claimed to use health systems strengthening (while not necessarily agreeing on the definition). Some of these are:\n\n\nBoth the idea of health systems strengthening and the term itself have received attention.\n\nEven advocates of health systems strengthening admit that it can often seem like a \"distant, even abstract aim\".\n\nMarchal et al., writing in 2009, called the term \"vague\" and argued that \"most current HSS strategies are selective (i.e., they target a specific disease), and their effects may undermine progress towards the long-term goal of effective, high-quality, and inclusive health systems.\"\n\nPeter Berman, who was the lead health economist at the World Bank, has pointed out that \"Almost any support to health interventions can be considered HSS\".\n\n"}
{"id": "24367869", "url": "https://en.wikipedia.org/wiki?curid=24367869", "title": "IKF World Korfball Ranking", "text": "IKF World Korfball Ranking\n\nThe IKF World Korfball Ranking is the ranking for national korfball teams, done by the International Korfball Federation.\n\n"}
{"id": "3780225", "url": "https://en.wikipedia.org/wiki?curid=3780225", "title": "Inter Press Service", "text": "Inter Press Service\n\nInter Press Service (IPS) is a global news agency. Its main focus is the production of news and analysis about events and processes affecting economic, social and political development. The agency largely covers news on the Global South, civil society, and globalization.\n\nInter Press Service was set up in 1964 as a non-profit international cooperative of journalists. Its founders were the Italian journalist Roberto Savio and the Argentine political scientist Pablo Piacentini. Initially, the primary objective of IPS was to fill the information gap between Europe and Latin America after the political turbulence following the Cuban revolution of 1959 (Giffard in Salwen and Garrison, 1991).\n\nLater, the network expanded to include all continents, beginning with a Latin American base in Costa Rica in 1982, and extended its editorial focus. In 1994, IPS changed its legal status to that of a \"public-benefit organization for development cooperation\".\n\nIn 1996 IPS had permanent offices and correspondents in 41 countries, covering 108 nations. It had as subscribers over 600 print media, around 80 news agencies and database services, and 65 broadcast media, in addition to over 500 NGOs and institutions.\n\nIPS’s stated aims are to give prominence to the voices of marginalized and vulnerable people and groups, report from the perspectives of developing countries, and to reflect the views of civil society. The mainstreaming of gender in reporting and the assessment of the impacts of globalization are a priority.\n\nIPS may be unique in its concentration on developing countries and the strong relationships with civil society. For this reason, IPS has even been termed the probably \"largest and most credible of all 'alternatives' in the world of news agencies\" (Boyd-Barrett and Rantanen, 1998: 174/5), being the \"first and only independent and professional news agency which provides on a daily basis information with a Third World focus and point of view\" (Boyd-Barrett and Thussu, 1992: 94; cf. Giffard, 1998: 191; Fenby, 1986).\n\nDespite all the laudable aims, it is, however, important to see that IPS has never possessed the resources to be a major player in the international media landscape. Because of its focus on longer background pieces focusses on development issues impacting the lives of people in the South instead of concise news, it is yet to be a news provider for mainstream media in developed countries. In fairness to IPS, mainstream media often rely on their own fly in and out journalists from where IPS reports.\n\nIPS is registered as an international not-for-profit association. It has 'general' NGO consultative status with ECOSOC at the United Nations, and the OECD status of \"ODA eligible international organization\".\n\nFive editorial desks coordinate the network of journalists around the world: Montevideo (regional bureau for Latin America), Berlin-London (Europe and the Mediterranean), Bangkok (Asia and the Pacific), New York (North America and the Caribbean) and Johannesburg (Africa). Most of IPS's journalists and editors are native to the country or region in which they are working.\n\nIPS receives funding from various sources: through its subscribers and media clients, as beneficiary of multilateral and national development cooperation programmes, and as recipient of project financing from foundations. It is not, as most other agencies, financed by a country or a group of newspapers. Hence, the agency’s budget is comparatively small. Still it manages to be \"roughly the sixth largest international news-gathering organization\" (Rauch, 2003: 89).\n\nThe actual role of IPS in the international mediascape is hard to assess. Clipping services are expensive, and do not exist in many countries where IPS is strong. Additionally, in some countries news agencies are not credited in bylines. One study by the UN's Food and Agriculture Organization for media coverage of the FAO in 1991 found that of the nearly 3000 clippings with news agency bylines, 13% credited IPS, making it the third-most cited agency. IPS reports were collected from 138 different publications in 39 countries - more countries than any other agency. IPS was particularly strong in Latin America - 72% of clippings from Latin America with news agency bylines came from IPS.\n\n\n\n\n\n"}
{"id": "579744", "url": "https://en.wikipedia.org/wiki?curid=579744", "title": "International Union of Railways", "text": "International Union of Railways\n\nThe UIC () or International Union of Railways is an international rail transport industry body.\n\nThe railways of Europe originated as many separate concerns, and there were many border changes after World War I and the Treaty of Versailles. Colonial railways were the responsibility of the mother country. Into this environment the UIC was created on 17 October 1922, with the aim of standardising industry practices.\n\nTicket revenue sharing was originally undertaken with the UIC Franc currency equivalent. UIC classification and UIC Country Codes allowed precise determination of rolling stock capabilities and ownership, with wagons assigned unique UIC wagon numbers. The 1990s GSM-R radio telecommunication system is an international interoperability specification covering voice and signalling systems for railway communications whose specification is maintained by the International Union of Railways project ERTMS.\n\nThe UIC's mission is \"to promote rail transport at world level and meet the challenges of mobility and sustainable development.\"\n\nThe UIC's main objectives are to:\n\n\nWhen founded in 1922 the UIC had 51 members from 29 countries, including Japan and China. They were soon joined by the USSR, the Middle East and North Africa. Today, the UIC has 194 members across 5 continents. Of these there are:\n\nOn 12 November 2010, the UIC opened an African regional office in Tunis with the support of SNCFT.\n\nIn order to provide a common understanding and reduce potential confusion, the UIC has established standard international railway terminology and a trilingual (English-French-German) thesaurus of terms. The thesaurus was the result of cooperation with the European Conference of Ministers of Transport (ECMT/) and was published in 1995.\n\nThe UIC has established systems for the classification of locomotives and their axle arrangements, coaches and goods wagons.\n\nUIC plays an important role in standardization of railway parts, data and terminology. Therefore, UIC codes (also known as UIC leaflet) are developed since the beginning of UIC's work. A new term for these UIC leaflets is used by UIC for better understanding: International Railway Solution (IRS).\n\nSome UIC codes are:\n\n\n\n"}
{"id": "22613352", "url": "https://en.wikipedia.org/wiki?curid=22613352", "title": "International Vaccine Institute", "text": "International Vaccine Institute\n\nThe International Vaccine Institute (IVI) is an independent, nonprofit, international organization that was founded on the belief that the health of children in developing countries can be dramatically improved by the use of new and improved vaccines. Working in collaboration with the international scientific community, public health organizations, governments, and industry, IVI is involved in all areas of the vaccine spectrum – from new vaccine design in the laboratory to vaccine development and evaluation in the field to facilitating sustainable introduction of vaccines in countries where they are most needed.\n\nCreated initially as an initiative of the UN Development Programme (UNDP), IVI began formal operations as an independent international organization in 1997 in Seoul, Republic of Korea. Currently, IVI has 35 countries and the World Health Organization (WHO) as signatories to its Establishment Agreement. The Institute has a unique mandate to work exclusively on vaccine development and introduction specifically for people in developing countries, with a focus on neglected diseases affecting these regions.\n\nIn 1992, Dr. Seung-Il Shin, then Senior Health Advisor for the United Nations Development Programme (UNDP), initiated a study to explore the feasibility of establishing an international institute devoted to vaccine research and development within the framework of the Children’s Vaccine Initiative (CVI). \n\nBased on the results of Dr. Shin’s feasibility study, in 1993 the UNDP adopted a formal proposal to establish the International Vaccine Institute (IVI). In 1994, following a call for proposals to host IVI in the Asia Pacific region, the UNDP and the Republic of Korea reached an agreement to host the center in Seoul. In 1995, UNDP opened an interim IVI office on the campus of Seoul National University and the Institute began its initial work and organizational development.\n\nIn 1995 and 1996, the UNDP and the South Korean government jointly developed the basic framework and Constitution of IVI. In order to establish IVI as an independent international organization, the UNDP and Korean government elected to establish the institute through an intergovernmental agreement of UN member states, as sanctioned in the Vienna Convention on the Laws of Treaties of 1969.\n\nOn October 28, 1996, the IVI Establishment Agreement opened for signatures at the UN headquarters in New York City. Representatives of Bangladesh, Bhutan, Indonesia, Kazakhstan, Mongolia, the Netherlands, Panama, Republic of Korea, Romania, Thailand, Vietnam, Uzbekistan, and the WHO were the first to sign the agreement, followed shortly thereafter by Senegal and the Philippines.\n\nIn 1997 more countries followed, with Brazil, China, Egypt, Israel, Jamaica, Kyrgyzstan, Myanmar, Nepal, Pakistan, Papua New Guinea, Peru, Sri Lanka, Sweden, Tajikistan, and Turkey signing the Agreement.\n\nOn May 29, 1997, The IVI Establishment Agreement entered into force following the submission of instruments of ratification by South Korea, Sweden, and Uzbekistan. The IVI Establishment Agreement entered the United Nations Depository of Treaties under Chapter IX. HEALTH, section 3.\n\nOn September 24, 1998 the IVI Headquarters Agreement was signed at a formal ceremony at UNDP headquarters by South Korea’s Minister of Foreign Affairs and Trade, Hon. Hong Soon-Young and IVI Board Chairman, Dr. Barry Bloom. The Headquarters Agreement established IVI as a legal entity with diplomatic immunity in South Korea, becoming the first international organization to be headquartered in South Korea. In accordance with IVI’s independent status, the Institute formally separated from the UNDP in 1998. \nThe IVI headquarters building, located in Seoul National University’s Research Park in Seoul, South Korea, was designed by a consortium consisting of Samwoo Architects of South Korea and Payette Associates of Boston, USA. Construction began in 1998 and was completed in 2003. The building houses laboratories, animal facilities, offices, an auditorium, and a library. A separate 1,300m² pilot plant facility, intended for the production of test lots of vaccines for training and evaluation purposes, was constructed but never completed due to a lack of funding.\n\nIVI’s first major initiative, Diseases of the Most Impoverished (DOMI) was a program of research and technical assistance to accelerate the introduction of new vaccines against typhoid fever, cholera, and shigellosis into public health programs for the poor in developing countries. From 2000 to 2006, IVI’s DOMI program carried out vaccination campaigns, disease surveillance, and research studies in Bangladesh, China, India, Indonesia, Mozambique, Pakistan, Thailand, and Vietnam.  At the conclusion of the DOMI program, IVI synthesized the diverse epidemiological, clinical, economic, and behavioral findings of these studies in order to facilitate informed decision-making by policymakers at the national level on the use of vaccines against the diseases targeted by DOMI. The Bill and Melinda Gates Foundation was the primary funder of the DOMI program with a US$40 million contribution.\n\nThe DOMI Typhoid program was initiated to address the barriers of access to typhoid vaccines in the developing world and to accelerate the introduction of modern typhoid vaccines in countries where they were needed. DOMI Typhoid utilized the Vi-Polysaccharide (Vi-PS) vaccine because it is easily and inexpensively produced by manufacturers in developing countries, it is given in a single dose, and it is relatively thermostable. DOMI Typhoid operated in five study sites: Heichi, China; Kolkata, India; North Jakarta, Indonesia; Karachi, Pakistan; and Hue, Vietnam. From these sites, IVI experts conducted disease surveillance, disease burden studies, cost-of-illness studies, socio-behavioral studies, and vaccine demonstration projects. IVI presented the accumulated evidence from DOMI Typhoid in case studies to officials in each host country. As a result, policymakers in Pakistan, Indonesia, and Vietnam agreed to introduce school-based typhoid vaccination campaigns on a pilot basis. Results from China uncovered a previously unknown growing incidence rate of paratyphoid A infections in China’s Guangxi province, which led to IVI launching its Paratyphoid in China project.\n\nThe DOMI Cholera program sought to develop and accelerate the use of an affordable cholera vaccine in cholera-endemic countries. DOMI Cholera operated from five study sites: Matlab, Bangladesh; North Jakarta, Indonesia; Kolkata, India; Beira, Mozambique; and Hue, Vietnam. From these sites, IVI conducted disease burden, economic, and socio-behavioral studies and cholera vaccination campaigns. The studies found a high demand for cholera vaccine and high incidence rates (3-9/1,000) in children 5 years old and younger. IVI’s vaccination campaign in Beira saw more than 44,000 children and adults receive the internationally-licensed Dukoral vaccine. In Kolkata, IVI vaccinated more than 67,000 children and adults using an oral cholera vaccine produced by Vietnam’s VaBiotech. At the onset of DOMI Cholera, the only internationally licensed cholera vaccine available was Dukoral, but at $90 for the two-dose series, this vaccine was too expensive for public use in many of the poorest developing countries. VaBiotech’s cholera vaccine, originally developed by Vietnam’s National Institute of Health and Epidemiology following a technology transfer from the University of Gothenburg in Sweden, was not licensed for international use but showed great promise as a low-cost vaccine for the developing world. As a result, the Gates Foundation provided IVI with additional funding to establish the Cholera Vaccine Initiative (CVI) with the goal of reformulating the VaBiotech vaccine for international use. \n\nIVI established the DOMI Shigellosis program with the ultimate goal of accelerating the development and introduction of a safe and protective shigella vaccine to control epidemic and endemic disease. Between 2000 and 2004, the DOMI Shigella program ran disease surveillance sites in 6 locations across Southeast Asia: Dhaka, Bangladesh; Hebei, China; Karachi, Pakistan and neighboring villages; North Jakarta, Indonesia; Nha Trang, Vietnam; and Saraburi Province, Thailand. The program enhanced understanding of what an effective shigella vaccine will need and established an accurate disease burden in the countries where it operated. The program first evaluated an oral shigella vaccine (SC602) in Bangladesh, but it failed to elicit an immune response. The high disease burden, serotype diversity, and high levels AMR discovered at IVI's surveillance sites underscored the need for a vaccine that protects against all common strains of the disease. As a result, IVI’s Division of Laboratory Sciences initiated a multiyear program to sequence the Shigella genome and identify common proteins in different Shigella species that could be used to develop a vaccine against all common strains of the disease.\n\nIVI initiated the Rotavirus Diarrhea Program to provide policymakers in developing countries with the disease burden evidence and economic data needed to ensure the inclusion of rotavirus vaccines in their national immunization programs. Running from 1999 to 2010, the program conducted disease surveillance and economic studies in Cambodia, China, India, Indonesia, Laos, Mongolia, South Korea, Sri Lanka, and Vietnam. In 2007, in collaboration with Vietnam’s National Institute of Hygiene and Epidemiology (NIHE), IVI conducted a Phase II trial of GSK’s RotaRix rotavirus vaccine in Khanh Hoa, Vietnam.  In 2009, in collaboration with NIHE, PATH, and Merck, IVI completed a phase III trial of Merck’s RotaTeq rotavirus vaccine in Nha Trang Vietnam, where it vaccinated 900 infants.  \n\nThe Pediatric Dengue Vaccine Initiative was a product development partnership launched by IVI to accelerate the introduction of safe and effective dengue vaccines for children in dengue-endemic countries. From 2002 to 2010, PDVI operated in Brazil, Cambodia, Colombia, India, Indonesia, Laos, Malaysia, Mexico, Myanmar, Nicaragua, Sri Lanka, Thailand and Vietnam. PDVI made many contributions to dengue vaccine development, including:\n\n\nIn 2010, PDVI transitioned into the Dengue Vaccine Initiative (DVI).\n\nThe successor to PDVI, the Dengue Vaccine Initiative was an IVI-led consortium with the World Health Organization, the Sabin Vaccine Institute, the Initiative for Vaccine Research (IVR), and the International Vaccine Access Center (IVAC) at Johns Hopkins University. DVI continued the work of PDVI and focused on laying the groundwork for dengue vaccine decision-making and vaccine introduction in endemic areas. Each consortium member was responsible for a specific component: WHO – information and guidance documents, and regulatory training activities; Johns Hopkins – dengue vaccine financing and strategic demand forecasting; and Sabin – communications and advocacy. Besides leading the consortium, IVI generated evidence for decision-making such as disease burden, country vaccine introduction cases and a global investment case. From 2010 to 2016, DVI operated in Brazil, Burkina Faso, Cambodia, Colombia, Gabon, India, Kenya, Thailand, and Vietnam. From 2013-2015, with funding from Germany’s Federal Ministry of Education & Research (BMBF), IVI continued support for preclinical development of dengue vaccines by Brazil’s Butantan Institute and Vietnam’s VaBiotech. After the 2016 licensure of Sanofi Pasteur’s Dengvaxia vaccine, the DVI project came to a close and its IVI staff transitioned to a new project, the Global Dengue & Aedes-Transmitted Disease Consortium (GDAC).\n\nAs of January 2011, IVI includes 40 countries and the World Health Organization (WHO) as signatories to its Establishment Agreement. To be more specific, there are 33 signatory countries, which are the states that express their consent to be bound by a treaty by signing the treaty without the need for ratification, acceptance of approval. These states may definitively sign a treaty only when the treaty so permits. In addition, there are 16 parties which involve Liberia, Republic of Korea, Brazil, Mongolia, China, Sri Lanka, Ecuador, Sweden, Netherlands, Oman, Pakistan, Uzbekistan, Vietnam, Peru, Philippines, and the World Health Organization (WHO). A major difference between a signatory and a party is that a party to a treaty is a state or other entity with treaty-making capacity that has expressed its consent to be bound by that treaty by an act of ratification, acceptance, approval or accession etc. where that treaty has entered into force for that particular State.\n\nIn short, in addition to the World Health Organization (WHO), 40 countries include Bangladesh, Liberia**, Republic of Korea**, Bhutan, Malta, Romania, Brazil**, Mongolia**, Senegal, China**, Myanmar, Sri Lanka**, Ecuador**, Nepal, Sweden**, Egypt, Netherlands**, Tajikistan, Indonesia, Oman**, Thailand, Israel, Pakistan**, Turkey, Jamaica, Panama, Uzbekistan**, Kazakhstan, Papua New Guinea, Vietnam**, Kyrgyzstan, Peru**, Lebanon, Philippines**, Cote d'Ivoire, India, Kuwait, Slovakia, Spain, and United Arab Emirates.\n\n\n"}
{"id": "28502092", "url": "https://en.wikipedia.org/wiki?curid=28502092", "title": "Ipas (organization)", "text": "Ipas (organization)\n\nIpas is a global non-profit organization that works to increase women's ability to exercise their sexual and reproductive rights, and to end deaths and injuries from unsafe abortion. Ipas's work is grounded in the belief that women everywhere must have the opportunity to determine their futures, care for their families and manage their fertility. Through local, national and global partnerships, they work to ensure that women receive safe, respectful and comprehensive abortion care, including counselling and contraception, to prevent future unwanted pregnancies.\n\nIpas works to improve women's access and right to safe abortion care and reproductive health services by:\n\n\n\n\nIn 2017, Ipas (Nigeria), called for a review of Nigeria's law criminalising abortion in the country. At a workshop covering Woman’s Reproductive Health and Rights in Abuja, Nigeria, Ipas's Country Director spoke out about the issue, saying \"Any law that does not recognise or reflect the true situation of the country is a dead law.\" foegal \n\nIpas was founded in 1973, following the passage of the Helms Amendment, which prohibits use of U.S. foreign aid to support abortion services overseas. Ipas began manufacturing and distributing manual vacuum aspiration (MVA) instruments in 1973. Ipas launched the postabortion family planning initiative in 1990. In 2003 Ipas created, defined and implemented the concept of \"woman-centered comprehensive abortion care\". Ipas University, a self-paced online learning site for reproductive health care providers, launched in 2009. Womancare Global, a wholly owned subsidiary of Ipas, also launched in 2009, to market and distribute the MVA instruments, as well as a number of other reproductive-health technologies. By 2010 Ipas had offices in 13 countries and serves 40 countries in Africa, Asia and the Americas.\n\n\"Not Yet Rain\" (2009) is a documentary film about abortion in Ethiopia, produced by filmmaker Lisa Russell and Ipas.\n\n"}
{"id": "14109447", "url": "https://en.wikipedia.org/wiki?curid=14109447", "title": "Journal of World History", "text": "Journal of World History\n\nThe Journal of World History is a peer-reviewed academic journal that presents historical analysis from a global point of view, focusing especially on forces that cross the boundaries of cultures and civilizations, including large-scale population movements, economic fluctuations, transfers of technology, the spread of infectious diseases, long-distance trade, and the spread of religious faiths, ideas, and values.\n\nThe journal was established in 1990 by Jerry H. Bentley at the University of Hawaii to serve as the official journal of the World History Association. It is published by the University of Hawaii Press. Initially produced twice a year, it became a quarterly in 2003.\n\nIn 2000, it was included in Project MUSE, which now contains archives going back to vol. 7 (1996). In 2009, it was included in JSTOR, with a moving wall of 3 years.\n\n"}
{"id": "52535321", "url": "https://en.wikipedia.org/wiki?curid=52535321", "title": "KOF Globalisation Index", "text": "KOF Globalisation Index\n\nThe KOF Index of Globalisation is an index of the degree of globalisation of 122 countries. It was conceived by Axel Dreher at the Konjunkturforschungsstelle of ETH Zurich, in Switzerland. It was first published in 2002, and covered the period from 1970 until that year. A new version of it was published in 2017.\n\nThe index is based on three principal criteria: economic, political and social. Unlike the Maastricht Globalisation Index, it does not take into account environmental factors.\n\n"}
{"id": "1669332", "url": "https://en.wikipedia.org/wiki?curid=1669332", "title": "Largest known prime number", "text": "Largest known prime number\n\nThe largest known prime number () is 2 − 1, a number with 23,249,425 digits. It was found by the Great Internet Mersenne Prime Search (GIMPS) in 2017.\n\nEuclid proved that there is no largest prime number, and many mathematicians and hobbyists continue to search for large prime numbers.\n\nMany of the largest known primes are Mersenne primes. , the seven largest known primes are Mersenne primes. The last sixteen record primes were Mersenne primes.\n\nThe fast Fourier transform implementation of the Lucas–Lehmer primality test for Mersenne numbers is fast compared to other known primality tests for other kinds of numbers.\n\nThe record is currently held by 2 − 1 with 23,249,425 digits, found by GIMPS in December 2017. Its value is:\n\nThe first and last 120 digits are shown above.\n\nThe Great Internet Mersenne Prime Search (GIMPS) currently offers a US$3000 research discovery award for participants who download and run their free software and whose computer discovers a new Mersenne prime having fewer than 100 million digits.\n\nThere are several prizes offered by the Electronic Frontier Foundation for record primes. GIMPS is also coordinating its long-range search efforts for primes of 100 million digits and larger and will split the Electronic Frontier Foundation's US$150,000 prize with a winning participant.\n\nThe record passed one million digits in 1999, earning a US$50,000 prize. In 2008 the record passed ten million digits, earning a US$100,000 prize and a Cooperative Computing Award from the Electronic Frontier Foundation. \"Time\" called it the 29th-top invention of 2008. Both the US$50,000 and the US$100,000 prizes were won by participation in GIMPS. Additional prizes are being offered for the first prime number found with at least one hundred million digits and the first with at least one billion digits.\n\nThe following table lists the progression of the largest known prime number in ascending order. Here M = 2 − 1 is the Mersenne number with exponent \"n\". The longest record-holder known was M = 524,287, which was the largest known prime for 144 years. No records are known before 1456.\n\nGIMPS found the fourteen latest records (all of them Mersenne primes) on ordinary computers operated by participants around the world.\n\nA list of the 5,000 largest known primes is maintained by Chris K. Caldwell, of which the twenty largest are listed below.\n\n"}
{"id": "55868631", "url": "https://en.wikipedia.org/wiki?curid=55868631", "title": "List of antibiotic-resistant bacteria", "text": "List of antibiotic-resistant bacteria\n\nA list of antibiotic resistant bacteria is provided below. These bacteria have shown antibiotic resistance (or antimicrobial resistance).\n\nNDM-1 is an enzyme that makes bacteria resistant to a broad range of beta-lactam antibiotics.\n\nNDM-1 (New Delhi Metallo-beta-lactamase-1) originated in India. In Indian hospitals hospital-acquired infections are common and with the new super-bugs on rise in India, this can make them dangerous. Mapping of sewage and water supply samples that were NDM-1-positive indicates widespread infection in New Delhi already back in 2011.\n\nNDM-1 was first detected in a \"Klebsiella pneumoniae\" isolate from a Swedish patient of Indian origin in 2008. It was later detected in bacteria in India, Pakistan, the United Kingdom, the United States, Canada and Japan.\n\n\"Clostridium difficile\" is a nosocomial pathogen that causes diarrheal disease worldwide. Diarrhea caused by \"C. difficile\" can be life-threatening. Infections are most frequent in people who have had recent medical and/or antibiotic treatment. \"C. difficile\" infections commonly occur during hospitalization.\n\nAccording to a 2015 CDC report, \"C. difficile\" caused almost 500,000 infections in the United States over a year period. Associated with these infections were an estimated 15,000 deaths. The CDC estimates that \"C. difficile\" infection costs could amount to $3.8 billion over a 5-year span.\n\n\"C. difficile\" colitis is most strongly associated with fluoroquinolones, cephalosporins, carbapenems, and clindamycin.\n\nSome research suggests the overuse of antibiotics in the raising of livestock is contributing to outbreaks of bacterial infections such as \"C. difficile\".[16]\n\nAntibiotics, especially those with a broad activity spectrum (such as clindamycin) disrupt normal intestinal flora. This can lead to an overgrowth of \"C. difficile\", which flourishes under these conditions. Pseudomembranous colitis can follow, creating generalized inflammation of the colon and the development of \"pseudomembrane\", a viscous collection of inflammatory cells, fibrin, and necrotic cells.[4] Clindamycin-resistant \"C. difficile\" was reported as the causative agent of large outbreaks of diarrheal disease in hospitals in New York, Arizona, Florida and Massachusetts between 1989 and 1992. Geographically dispersed outbreaks of \"C. difficile\" strains resistant to fluoroquinolone antibiotics, such as ciprofloxacin and levofloxacin, were also reported in North America in 2005.\n\nMultidrug-resistant \"Enterococcus faecalis\" and \"Enterococcus faecium\" are associated with nosocomial infections. These strains include: penicillin-resistant \"Enterococcus\", vancomycin-resistant \"Enterococcus\", and linezolid-resistant \"Enterococcus\".\n\nTuberculosis (TB) resistant to antibiotics is called MDR TB (multidrug-resistant TB). Globally, MDR TB causes 150,000 deaths annually. The rise of the HIV/AIDS epidemic has contributed to this.\n\nMycobacterium tuberculosis is an obligate pathogen that has evolved to ensure its persistence in human populations. This is evident in the fact that Mycobacterium tuberculosis must cause a pulmonary disease in order to be successfully transmitted from one person to another. Tuberculosis better known as TB has one of the highest mortality rates among pathogens in the world. Mortality rates have not seen a significant decrease due to its growing resistance to certain antibiotics. Although there is years worth of research and many work hours that have been devoted to the creation of a vaccine, one still does not exist. TB has a very high level of virulence which is mainly due in part to the fact it is extremely transmissible. TB was considered one of the most prevalent diseases, and did not have a cure until the discovery of streptomycin by Selman Waksman in 1943. However, the bacteria soon developed resistance. Since then, drugs such as isoniazid and rifampin have been used. \"M. tuberculosis\" develops resistance to drugs by spontaneous mutations in its genomes. These types of mutations can lead to genotype and phenotype changes that can contribute to reproductive success therefore they can be passed on and evolve into resistant bacteria. Resistance to one drug is common, and this is why treatment is usually done with more than one drug. Extensively drug-resistant TB (XDR TB) is TB that is also resistant to the second line of drugs.\n\nResistance of \"Mycobacterium tuberculosis\" to isoniazid, rifampin, and other common treatments has become an increasingly relevant clinical challenge. Evidence is lacking for whether these bacteria have plasmids. \"M. tuberculosis\" lack the opportunity to interact with other bacteria in order to share plasmids.\n\n\"Mycoplasma genitalium\" is a small pathogenic bacterium that lives on the ciliated epithelial cells of the urinary and genital tracts in humans. It is still controversial whether or not this bacterium is to be recognized as a sexually transmitted pathogen. Infection with \"Mycoplasma genitalium\" sometimes produces clinical symptoms, or a combination of symptoms, but sometimes can be asymptomatic. It causes inflammation in the urethra (urethritis) both in men and women, which is associated with mucopurulent discharge in the urinary tract, and burning while urinating.\n\nTreatment of \"Mycoplasma genitalium\" infections is becoming increasingly difficult due to rapidly developing multi-drug resistance, and diagnosis and treatment is further hampered by the fact that \"M. genitalium\" infections are not routinely detected. Azithromycin is the most common first-line treatment, but the commonly-used 1 gram single-dose azithromycin treatment can lead to the bacteria commonly developing resistance to azithromycin. An alternative five-day treatment with azithromycin showed no development of antimicrobial resistance. Efficacy of azithromycin against \"M. genitalium\" has decreased substantially, which is thought to occur through SNPs in the 23S rRNA gene. The same SNPs are thought to be responsible for resistance against josamycin which is prescribed in some countries. Moxifloxacin can be used as a second-line treatment in case azithromycin is not able to eradicate the infection. However, resistance against moxifloxacin has been observed since 2007, thought to be due to \"parC\" SNPs. Tetracyclines, including doxycycline, have a low clinical eradication rate for \"M. genitalium\" infections. A few cases have been described where doxycycline, azithromycin and moxifloxacin had all failed, but pristinamycin was still able to eradicate the infection.\n\n\"Staphylococcus aureus\" is one of the major resistant pathogens. Found on the mucous membranes and the human skin of around a third of the population, it is extremely adaptable to antibiotic pressure. It was one of the earlier bacteria in which penicillin resistance was found—in 1947, just four years after the drug started being mass-produced. Methicillin was then the antibiotic of choice, but has since been replaced by oxacillin because of significant kidney toxicity. Methicillin-resistant \"Staphylococcus aureus\" (MRSA) was first detected in Britain in 1961, and is now \"quite common\" in hospitals. MRSA was responsible for 37% of fatal cases of sepsis in the UK in 1999, up from 4% in 1991. Half of all \"S. aureus\" infections in the US are resistant to penicillin, methicillin, tetracycline and erythromycin.\n\n\"Streptococcus pyogenes\" (Group A \"Streptococcus\": GAS) infections can usually be treated with many different antibiotics. Strains of \"S. pyogenes\" resistant to macrolide antibiotics have emerged; however, all strains remain uniformly susceptible to penicillin.\n\nResistance of \"Streptococcus pneumoniae\" to penicillin and other beta-lactams is increasing worldwide. The major mechanism of resistance involves the introduction of mutations in genes encoding penicillin-binding proteins. Selective pressure is thought to play an important role, and use of beta-lactam antibiotics has been implicated as a risk factor for infection and colonization. \"S. pneumoniae\" is responsible for pneumonia, bacteremia, otitis media, meningitis, sinusitis, peritonitis and arthritis.\n\n\"Campylobacter\" causes diarrhea (often bloody), fever, and abdominal cramps. Serious complications such as temporary paralysis can also occur. Physicians rely on ciprofloxacin and azithromycin for treating patients with severe disease although \"Campylobacter\" is showing resistance to these antibiotics.\n\n\"Neisseria gonorrhoeae\" is a sexually transmitted pathogen that causes gonorrhea, a sexually transmitted disease that can result in discharge and inflammation at the urethra, cervix, pharynx, or rectum. It can cause pelvic pain, pain on urination, penile and vaginal discharge, as well as systemic symptoms. It can also cause severe reproductive complications.\n\nAs of 2013 hard-to-treat or untreatable infections of carbapenem-resistant Enterobacteriaceae (CRE), also known as carbapenemase-producing Enterobacteriaceae (CPE), were increasing among patients in medical facilities. CRE are resistant to nearly all available antibiotics. Almost half of hospital patients who get bloodstream CRE infections die from the infection.\n\nKlebsiella pneumoniae carbapenemase (KPC)-producing bacteria are a group of emerging highly drug-resistant Gram-negative bacilli causing infections associated with significant morbidity and mortality whose incidence is rapidly increasing in a variety of clinical settings around the world. \"Klebsiella pneumoniae\" includes numerous mechanisms for antibiotic resistance, many of which are located on highly mobile genetic elements. Carbapenem antibiotics (heretofore often the treatment of last resort for resistant infections) are generally not effective against KPC-producing organisms.\n\nInfection with \"Escherichia coli\" and \"Salmonella\" can result from the consumption of contaminated food and polluted water. Both of these bacteria are well known for causing nosocomial (hospital-linked) infections, and often, these strains found in hospitals are antibiotic resistant because of adaptations to wide spread antibiotic use. When both bacteria are spread, serious health conditions arise. Many people are hospitalized each year after becoming infected, with some dying as a result. Since 1993, some strains of \"E. coli\" have become resistant to multiple types of fluoroquinolone antibiotics.\n\nAlthough mutation alone plays a huge role in the development of antibiotic resistance, a 2008 study found that high survival rates after exposure to antibiotics could not be accounted for by mutation alone. This study focused on the development of resistance in \"E. coli\" to three antibiotic drugs: ampicillin, tetracycline, and nalidixic acid. The researchers found that some antibiotic resistance in \"E. coli\" developed because of epigenetic inheritance rather than by direct inheritance of a mutated gene. This was further supported by data showing that reversion to antibiotic sensitivity was relatively common as well. This could only be explained by epigenetics. Epigenetics is a type of inheritance in which gene expression is altered rather than the genetic code itself. There are many modes by which this alteration of gene expression can occur, including methylation of DNA and histone modification; however, the important point is that both inheritance of random mutations and epigenetic markers can result in the expression of antibiotic resistance genes.\n\nResistance to polymyxins first appear in 2011. An easier way for this resistance to spread, a plasmid known as MCR-1 was discovered in 2015.\n\n\"Acinetobacter\" is a gram-negative bacteria that causes pneumonia or bloodstream infections in critically ill patients. Multidrug-resistant \"Acinetobacter\" have become very resistant to antibiotics.\n\nOn November 5, 2004, the Centers for Disease Control and Prevention (CDC) reported an increasing number of \"Acinetobacter baumannii\" bloodstream infections in patients at military medical facilities in which service members injured in the Iraq/Kuwait region during Operation Iraqi Freedom and in Afghanistan during Operation Enduring Freedom were treated. Most of these showed multidrug resistance (MRAB), with a few isolates resistant to all drugs tested.\n\n\"Pseudomonas aeruginosa\" is a highly prevalent opportunistic pathogen. One of the most worrisome characteristics of \"P. aeruginosa\" is its low antibiotic susceptibility, which is attributable to a concerted action of multidrug efflux pumps with chromosomally encoded antibiotic resistance genes (e.g., \"mexAB-oprM\", \"mexXY\") and the low permeability of the bacterial cellular envelopes. \"P. aeruginosa\" has the ability to produce 4-hydroxy-2-alkylquinolines (HAQs) and it has been found that HAQs have prooxidant effects, and overexpressing modestly increased susceptibility to antibiotics. The study experimented with the \"P. aeruginosa\" biofilms and found that a disruption of relA and spoT genes produced an inactivation of the Stringent response (SR) in cells with nutrient limitation, which provides cells be more susceptible to antibiotics.\n\n\n"}
{"id": "2165161", "url": "https://en.wikipedia.org/wiki?curid=2165161", "title": "List of busiest airports by international passenger traffic", "text": "List of busiest airports by international passenger traffic\n\nThe following is a list of the world's largest airports by international passenger traffic.\n\nAirports Council International's (January–December) preliminary figures are as follows.\n\nAirports Council International's (January–December) preliminary figures are as follows.\nAirports Council International's (January–December) figures are as follows.\n\nAirports Council International's (January–December) figures are as follows.\n\nAirports Council International's (January–December) figures are as follows.\n\nAirports Council International's (January–December) figures are as follows.\n\n\n"}
{"id": "3288078", "url": "https://en.wikipedia.org/wiki?curid=3288078", "title": "Macrohistory", "text": "Macrohistory\n\nMacrohistory seeks out large, long-term trends in world history, searching for ultimate patterns through a comparison of proximate details. For example, a macro-historical study might examine Japanese feudalism and European feudalism in order to decide whether feudal structures are an inevitable outcome given certain conditions. Macro-historical studies often \"assume that macro-historical processes repeat themselves in explainable and understandable ways\".\n\nExamples of macro-historical analysis include Oswald Spengler's assertion that the lifespan of civilizations is limited and ultimately they decay, and Arnold J. Toynbee's historical synthesis in explaining the rise and fall of civilizations. The Battle of Ain Jalut is considered by many historians to be of great macro-historical importance, as it marked the high water point of Mongol conquests, and the first time they had ever been decisively defeated.\n\nAccording to economists Robert Solow, Brian Snowdon, and Jason Collins, Oded Galor's unified growth theory is another example of macro-historical analysis that has significantly contributed to the understanding of process of development over the entire course of human history and the role of deep-rooted factors in the transition from stagnation to growth and in the emergence of the vast inequality across the globe.\n\n"}
{"id": "885795", "url": "https://en.wikipedia.org/wiki?curid=885795", "title": "Modern history", "text": "Modern history\n\nModern history, the modern period or the modern era, is the linear, global, historiographical approach to the time frame after post-classical history. Modern history can be further broken down into periods:\n\nThis article primarily covers the 1800–1950 time period with a brief summary of 1500–1800. For a more in depth article on modern times before 1800, see Early Modern period.\n\nIn the pre-modern era, many people's sense of self and purpose was often expressed via a faith in some form of deity, be it that in a single God or in many gods. Pre-modern cultures have not been thought of creating a sense of distinct individuality, though. Religious officials, who often held positions of power, were the spiritual intermediaries to the common person. It was only through these intermediaries that the general masses had access to the divine. Tradition was sacred to ancient cultures and was unchanging and the social order of ceremony and morals in a culture could be strictly enforced.\n\nThe term \"modern\" was coined in the 16th century to indicate present or recent times (ultimately derived from the Latin adverb \"modo\", meaning \"just now\"). The European Renaissance (c. 1420–1630), which marked the transition between the Late Middle Ages and Early Modern times, started in Italy and was spurred in part by the rediscovery of classical art and literature, as well as the new perspectives gained from the Age of Discovery and the invention of the telescope and microscope, expanding the borders of thought and knowledge.\n\nIn contrast to the pre-modern era, Western civilization made a gradual transition from pre-modernity to modernity when scientific methods were developed which led many to believe that the use of science would lead to all knowledge, thus throwing back the shroud of myth under which pre-modern peoples lived. New information about the world was discovered via empirical observation, versus the historic use of reason and innate knowledge.\n\nThe term \"Early Modern\" was introduced in the English language in the 1930s to distinguish the time between what has been called the Middle Ages and time of the late Enlightenment (1800) (when the meaning of the term \"Modern Ages\" was developing its contemporary form). It is important to note that these terms stem from European history. In usage in other parts of the world, such as in Asia, and in Muslim countries, the terms are applied in a very different way, but often in the context with their contact with European culture in the Age of Discovery.\n\nIn the Contemporary era, there were various socio-technological trends. Regarding the 21st century and the late modern world, the Information Age and computers were forefront in use, not completely ubiquitous but often present in everyday life. The development of Eastern powers was of note, with China and India becoming more powerful. In the Eurasian theater, the European Union and Russian Federation were two forces recently developed. A concern for Western world, if not the whole world, was the late modern form of terrorism and the warfare that has resulted from the contemporary terrorist acts.\n\nThe modern period has been a period of significant development in the fields of science, politics, warfare, and technology. It has also been an age of discovery and globalization. During this time, the European powers and later their colonies, began a political, economic, and cultural colonization of the rest of the world.\n\nBy the late 19th and 20th centuries, modernist art, politics, science and culture has come to dominate not only Western Europe and North America, but almost every civilized area on the globe, including movements thought of as opposed to the west and globalization. The modern era is closely associated with the development of individualism, capitalism, urbanization and a belief in the possibilities of technological and political progress.\n\nWars and other perceived problems of this era, many of which come from the effects of rapid change, and the connected loss of strength of traditional religious and ethical norms, have led to many reactions against modern development. Optimism and belief in constant progress has been most recently criticized by postmodernism while the dominance of Western Europe and Anglo-America over other continents has been criticized by postcolonial theory.\n\nOne common conception of modernity is the condition of Western history since the mid-15th century, or roughly the European development of movable type and the printing press. In this context the \"modern\" society is said to develop over many periods, and to be influenced by important events that represent breaks in the continuity.\n\nThe modern era includes the early period, called the early modern period, which lasted from c. 1500 to around c. 1800 (most often 1815). Particular facets of early modernity include:\n\nImportant events in the early modern period include:\nThis combination of epoch events totally changed thinking and thought in the early modern period, and so their dates serve as well as any to separate the old from the new modes.\n\nAs an Age of Revolutions dawned, beginning with those revolts in America and France, political changes were then pushed forward in other countries partly as a result of upheavals of the Napoleonic Wars and their impact on thought and thinking, from concepts from nationalism to organizing armies.\n\nThe early period ended in a time of political and economic change as a result of mechanization in society, the American Revolution, the first French Revolution; other factors included the redrawing of the map of Europe by the of the Congress of Vienna and the peace established by Second Treaty of Paris which ended the Napoleonic Wars.\n\nAs a result of the Industrial Revolution and the earlier political revolutions, the worldviews of Modernism emerged. The industrialization of many nations was initiated with the industrialization of Britain. Particular facets of the late modernity period include:\n\nOther important events in the development of the Late modern period include:\n\nOur most recent eraModern Timesbegins with the end of these revolutions in the 19th century, and includes the World Wars era (encompassing World War I and World War II) and the emergence of socialist countries that led to the Cold War. The \"contemporary era\" follows shortly afterward with the explosion of research and increase of knowledge known as the Information Age in the latter 20th and the early 21st century. Today's Postmodern era is seen in widespread digitality.\n\nHistorians consider the early modern period to be approximately between 1500 and 1800. It follows the Late Middle Ages and is marked by the first European colonies, the rise of strong centralized governments, and the beginnings of recognizable nation-states that are the direct antecedents of today's states.\n\nThe expansion of Islam took place in North and East Africa. In West Africa, various native nations existed. The Indian Empires and civilizations of Southeast Asia were a vital link in the spice trade. On the Indian subcontinent, the Mughal Empire existed. The archipelagic empires, the Sultanate of Malacca and later the Sultanate of Johor, controlled the southern areas.\n\nVarious Chinese dynasties and Japanese shogunates controlled the Asian sphere. In Japan, the Edo period from 1600 to 1868 is also referred to as the early modern period. In Korea, the early modern period is considered to have lasted from the rise of the Joseon Dynasty to the enthronement of King Gojong. In the Americas, Pre-Columbian peoples had built a large and varied civilization, including the Aztec Empire and alliance, the Inca civilization, the Mayan Empire and cities, and the Chibcha. In the west, kingdoms of Europe moved in the direction of reformation and expansion. Russia reached the Pacific coast in 1647 and consolidated its control over the Russian Far East in the 19th century.\n\nLater religious trends of the period saw the end of the aforementioned Muslim expansion. Christians and Christendom saw the end of the Crusades and of religious unity under the Roman Catholic Church. It was during this time that the Inquisitions and the Protestant Reformation took place.\n\nDuring the early modern period, an age of discovery and trade was undertaken by the Western European nations. Portugal, Spain, the Netherlands, the United Kingdom and France went on a colonial expansion and took possession of lands and set up colonies in Africa, southern Asia, and North and South America. Turkey colonized Southeastern Europe, and parts of the West Asia and North Africa. Russia took possession in Eastern Europe, Asia, and North America.\n\nIn China, urbanization increased as the population grew and as the division of labor grew more complex. Large urban centers, such as Nanjing and Beijing, also contributed to the growth of private industry. In particular, small-scale industries grew up, often specializing in paper, silk, cotton, and porcelain goods. For the most part, however, relatively small urban centers with markets proliferated around the country. Town markets mainly traded food, with some necessary manufactures such as pins or oil. Despite the xenophobia and intellectual introspection characteristic of the increasingly popular new school of neo-Confucianism, China under the later Ming Dynasty became isolated, prohibiting the construction of ocean going sea vessels. Despite isolationist policies the Ming Economy still suffered from an inflation due to an overabundance of Spanish New World silver entering its economy through new European colonies such as Macao. Ming China was further strained by victorious but costly wars to protect Korea from Japanese Invasion.\n\nThe Qing dynasty (1644–1912) was founded after the fall of the Ming, the last Han Chinese dynasty, by the Manchus. The Manchus were formerly known as the Jurchens. When Beijing was captured by Li Zicheng's peasant rebels in 1644, the Chongzhen Emperor, the last Ming emperor, committed suicide. The Manchus then allied with former Ming general Wu Sangui and seized control of Beijing, which became the new capital of the Qing dynasty. The Manchus adopted the Confucian norms of traditional Chinese government in their rule of China proper. Schoppa, the editor of \"The Columbia Guide to Modern Chinese History\" argues, \"A date around 1780 as the beginning of modern China is thus closer to what we know today as historical 'reality'. It also allows us to have a better baseline to understand the precipitous decline of the Chinese polity in the nineteenth and twentieth centuries.\"\n\nIn pre-modern Japan following the Sengoku period of \"warring states\", central government had been largely reestablished by Oda Nobunaga and Toyotomi Hideyoshi during the Azuchi–Momoyama period. After the Battle of Sekigahara in 1600, central authority fell to Tokugawa Ieyasu who completed this process and received the title of \"shōgun\" in 1603.\nSociety in the Japanese \"Tokugawa period\" (Edo society), unlike the shogunates before it, was based on the strict class hierarchy originally established by Toyotomi Hideyoshi. The \"daimyōs\" (feudal lords) were at the top, followed by the warrior-caste of samurai, with the farmers, artisans, and traders ranking below. The country was strictly closed to foreigners with few exceptions with the \"Sakoku\" policy. Literacy among the Japanese people rose in the two centuries of isolation.\n\nIn some parts of the country, particularly smaller regions, \"daimyōs\" and samurai were more or less identical, since \"daimyōs\" might be trained as samurai, and samurai might act as local lords. Otherwise, the largely inflexible nature of this social stratification system unleashed disruptive forces over time. Taxes on the peasantry were set at fixed amounts which did not account for inflation or other changes in monetary value. As a result, the tax revenues collected by the samurai landowners were worth less and less over time. This often led to numerous confrontations between noble but impoverished samurai and well-to-do peasants. None, however, proved compelling enough to seriously challenge the established order until the arrival of foreign powers.\n\nOn the Indian subcontinent, the Mughal Empire ruled most of India in the early 18th century. The \"classic period\" ended with the death and defeat of Emperor Aurangzeb in 1707 by the rising Hindu Maratha Empire, although the dynasty continued for another 150 years. During this period, the Empire was marked by a highly centralized administration connecting the different regions. All the significant monuments of the Mughals, their most visible legacy, date to this period which was characterised by the expansion of Persian cultural influence in the Indian subcontinent, with brilliant literary, artistic, and architectural results. The Maratha Empire was located in the south west of present-day India and expanded greatly under the rule of the Peshwas, the prime ministers of the Maratha empire. In 1761, the Maratha army lost the Third Battle of Panipat which halted imperial expansion and the empire was then divided into a confederacy of Maratha states.\n\nThe development of New Imperialism saw the conquest of nearly all eastern hemisphere territories by colonial powers. The commercial colonization of India commenced in 1757, after the Battle of Plassey, when the Nawab of Bengal surrendered his dominions to the British East India Company, in 1765, when the Company was granted the \"diwani\", or the right to collect revenue, in Bengal and Bihar, or in 1772, when the Company established a capital in Calcutta, appointed its first Governor-General, Warren Hastings, and became directly involved in governance.\n\nThe Maratha states, following the Anglo-Maratha wars, eventually lost to the British East India Company in 1818 with the Third Anglo-Maratha War. The rule lasted until 1858, when, after the Indian rebellion of 1857 and consequent of the Government of India Act 1858, the British government assumed the task of directly administering India in the new British Raj. In 1819 Stamford Raffles established Singapore as a key trading post for Britain in their rivalry with the Dutch. However, their rivalry cooled in 1824 when an Anglo-Dutch treaty demarcated their respective interests in Southeast Asia. From the 1850s onwards, the pace of colonization shifted to a significantly higher gear.\n\nThe Dutch East India Company (1800) and British East India Company (1858) were dissolved by their respective governments, who took over the direct administration of the colonies. Only Thailand was spared the experience of foreign rule, although, Thailand itself was also greatly affected by the power politics of the Western powers. Colonial rule had a profound effect on Southeast Asia. While the colonial powers profited much from the region's vast resources and large market, colonial rule did develop the region to a varying extent.\n\nMany major events caused Europe to change around the start of the 16th century, starting with the Fall of Constantinople in 1453, the fall of Muslim Spain and the discovery of the Americas in 1492, and Martin Luther's Protestant Reformation in 1517. In England the modern period is often dated to the start of the Tudor period with the victory of Henry VII over Richard III at the Battle of Bosworth in 1485. Early modern European history is usually seen to span from the start of the 15th century, through the Age of Enlightenment in the 17th and 18th centuries, until the beginning of the Industrial Revolution in the late 18th century.\n\nRussia experienced territorial growth through the 17th century, which was the age of Cossacks. Cossacks were warriors organized into military communities, resembling pirates and pioneers of the New World. The native land of the Cossacks is defined by a line of Russian/Ruthenian town-fortresses located on the border with the steppe and stretching from the middle Volga to Ryazan and Tula, then breaking abruptly to the south and extending to the Dnieper via Pereyaslavl. This area was settled by a population of free people practicing various trades and crafts.\n\nIn 1648, the peasants of Ukraine joined the Zaporozhian Cossacks in rebellion against Poland-Lithuania during the Khmelnytsky Uprising, because of the social and religious oppression they suffered under Polish rule. In 1654 the Ukrainian leader, Bohdan Khmelnytsky, offered to place Ukraine under the protection of the Russian Tsar, Aleksey I. Aleksey's acceptance of this offer led to another Russo-Polish War (1654–1667). Finally, Ukraine was split along the river Dnieper, leaving the western part (or Right-bank Ukraine) under Polish rule and eastern part (Left-bank Ukraine and Kiev) under Russian. Later, in 1670–71 the Don Cossacks led by Stenka Razin initiated a major uprising in the Volga region, but the Tsar's troops were successful in defeating the rebels. In the east, the rapid Russian exploration and colonisation of the huge territories of Siberia was led mostly by Cossacks hunting for valuable furs and ivory. Russian explorers pushed eastward primarily along the Siberian river routes, and by the mid-17th century there were Russian settlements in the Eastern Siberia, on the Chukchi Peninsula, along the Amur River, and on the Pacific coast. In 1648 the Bering Strait between Asia and North America was passed for the first time by Fedot Popov and Semyon Dezhnyov.\n\nTraditionally, the European intellectual transformation of and after the Renaissance bridged the Middle Ages and the Modern era. The Age of Reason in the Western world is generally regarded as being the start of modern philosophy, and a departure from the medieval approach, especially Scholasticism. Early 17th-century philosophy is often called the Age of Rationalism and is considered to succeed Renaissance philosophy and precede the Age of Enlightenment, but some consider it as the earliest part of the Enlightenment era in philosophy, extending that era to two centuries. The 18th century saw the beginning of secularization in Europe, rising to notability in the wake of the French Revolution.\n\nThe Age of Enlightenment is a time in Western philosophy and cultural life centered upon the 18th century in which reason was advocated as the primary source and legitimacy for authority. Enlightenment gained momentum more or less simultaneously in many parts of Europe and America. Developing during the Enlightenment era, Renaissance humanism as an intellectual movement spread across Europe. The basic training of the humanist was to speak well and write (typically, in the form of a letter). The term \"umanista\" comes from the latter part of the 15th century. The people were associated with the \"studia humanitatis\", a novel curriculum that was competing with the \"quadrivium\" and scholastic logic.\n\nRenaissance humanism took a close study of the Latin and Greek classical texts, and was antagonistic to the values of scholasticism with its emphasis on the accumulated commentaries; and humanists were involved in the sciences, philosophies, arts and poetry of classical antiquity. They self-consciously imitated classical Latin and deprecated the use of medieval Latin. By analogy with the perceived decline of Latin, they applied the principle of \"ad fontes\", or back to the sources, across broad areas of learning.The quarrel of the Ancients and the Moderns was a literary and artistic quarrel that heated up in the early 1690s and shook the Académie française. The opposing two sides were, the Ancients (\"Anciens\") who constrain choice of subjects to those drawn from the literature of Antiquity and the Moderns (\"Modernes\"), who supported the merits of the authors of the century of Louis XIV. Fontenelle quickly followed with his \"Digression sur les anciens et les modernes\" (1688), in which he took the Modern side, pressing the argument that modern scholarship allowed modern man to surpass the ancients in knowledge.\n\nThe Scientific Revolution was a period when European ideas in classical physics, astronomy, biology, human anatomy, chemistry, and other classical sciences were rejected and led to doctrines supplanting those that had prevailed from Ancient Greece to the Middle Ages which would lead to a transition to modern science. This period saw a fundamental transformation in scientific ideas across physics, astronomy, and biology, in institutions supporting scientific investigation, and in the more widely held picture of the universe. Individuals started to question all manners of things and it was this questioning that led to the Scientific Revolution, which in turn formed the foundations of contemporary sciences and the establishment of several modern scientific fields.\n\nToward the middle and latter stages of the Age of Revolution, the French political and social revolutions and radical change saw the French governmental structure, previously an absolute monarchy with feudal privileges for the aristocracy and Catholic clergy transform, changing to forms based on Enlightenment principles of citizenship and inalienable rights. The first revolution led to government by the National Assembly, the second by the Legislative Assembly, and the third by the Directory.\n\nThe changes were accompanied by violent turmoil which included the trial and execution of the king, vast bloodshed and repression during the Reign of Terror, and warfare involving every other major European power. Subsequent events that can be traced to the Revolution include the Napoleonic Wars, two separate restorations of the monarchy, and two additional revolutions as modern France took shape. In the following century, France would be governed at one point or another as a republic, constitutional monarchy, and two different empires.\n\nDuring the French Revolution, the National Assembly, which existed from June 17 to July 9 of 1789, was a transitional body between the Estates-General and the National Constituent Assembly.\n\nThe Legislative Assembly was the legislature of France from October 1, 1791 to September 1792. It provided the focus of political debate and revolutionary law-making between the periods of the National Constituent Assembly and of the National Convention.\n\nThe Executive Directory was a body of five Directors that held executive power in France following the Convention and preceding the Consulate. The period of this regime (2 November 1795 until 10 November 1799), commonly known as the Directory (or Directoire) era, constitutes the second to last stage of the French Revolution. Napoleon, before seizing the title of Emperor, was elected as First Consul of the Consulate of France.\n\nThe campaigns of French Emperor and General Napoleon Bonaparte characterized the Napoleonic Era. Born on Corsica as the French invaded, and dying suspiciously on the tiny British Island of St. Helena, this brilliant commander, controlled a French Empire that, at its height, ruled a large portion of Europe directly from Paris, while many of his friends and family ruled countries such as Spain, Poland, several parts of Italy and many other Kingdoms Republics and dependencies. The Napoleonic Era changed the face of Europe forever, and old Empires and Kingdoms fell apart as a result of the mighty and \"Glorious\" surge of Republicanism.\n\nItalian unification was the political and social movement that annexed different states of the Italian peninsula into the single state of Italy in the 19th century. There is a lack of consensus on the exact dates for the beginning and the end of this period, but many scholars agree that the process began with the end of Napoleonic rule and the Congress of Vienna in 1815, and approximately ended with the Franco-Prussian War in 1871, though the last \"città irredente\" did not join the Kingdom of Italy until after World War I.\n\nToward the end of the early modern period, Europe was dominated by the evolving system of mercantile capitalism in its trade and the New Economy. European states and politics had the characteristic of Absolutism. The French power and English revolutions dominated the political scene. There eventually evolved an international balance of power that held at bay a great conflagration until years later.\n\nThe end date of the early modern period is usually associated with the Industrial Revolution, which began in Britain in about 1750. Another significant date is 1789, the beginning of the French Revolution, which drastically transformed the state of European politics and ushered in the Prince Edward Era and modern Europe.\n\nThe French and Indian Wars were a series of conflicts in North America that represented the actions there that accompanied the European dynastic wars. In Quebec, the wars are generally referred to as the Intercolonial Wars. While some conflicts involved Spanish and Dutch forces, all pitted Great Britain, its colonies and American Indian allies on one side and France, its colonies and Indian allies on the other.\n\nThe expanding French and British colonies were contending for control of the western, or interior, territories. Whenever the European countries went to war, there were actions within and by these colonies although the dates of the conflict did not necessarily exactly coincide with those of the larger conflicts.\n\nBeginning the Age of Revolution, the American Revolution and the ensuing political upheaval during the last half of the 18th century saw the Thirteen Colonies of North America overthrow the governance of the Parliament of Great Britain, and then reject the British monarchy itself to become the sovereign United States of America. In this period the colonies first rejected the authority of the Parliament to govern them without representation, and formed self-governing independent states. The Second Continental Congress then joined together against the British to defend that self-governance in the armed conflict from 1775 to 1783 known as the American Revolutionary War (also called American War of Independence).\n\nThe American Revolution began with fighting at Lexington and Concord. On July 4, 1776, they issued the Declaration of Independence, which proclaimed their independence from Great Britain and their formation of a cooperative union. In June 1776, Benjamin Franklin was appointed a member of the Committee of Five that drafted the Declaration of Independence. Although he was temporarily disabled by gout and unable to attend most meetings of the Committee, Franklin made several small changes to the draft sent to him by Thomas Jefferson.\n\nThe rebellious states defeated Great Britain in the American Revolutionary War, the first successful colonial war of independence. While the states had already rejected the governance of Parliament, through the Declaration the new United States now rejected the legitimacy of the monarchy to demand allegiance. The war raged for seven years, with effective American victory, followed by formal British abandonment of any claim to the United States with the Treaty of Paris.\n\nThe Philadelphia Convention set up the current United States; the United States Constitution ratification the following year made the states part of a single republic with a limited central government. The Bill of Rights, comprising ten constitutional amendments guaranteeing many fundamental civil rights and freedoms, was ratified in 1791.\n\nThe decolonization of the Americas was the process by which the countries in the Americas gained their independence from European rule. Decolonization began with a series of revolutions in the late 18th and early-to-mid-19th centuries. The Spanish American wars of independence were the numerous wars against Spanish rule in Spanish America that took place during the early 19th century, from 1808 until 1829, directly related to the Napoleonic French invasion of Spain. The conflict started with short-lived governing juntas established in Chuquisaca and Quito opposing the composition of the Supreme Central Junta of Seville.\n\nWhen the Central Junta fell to the French, numerous new Juntas appeared all across the Americas, eventually resulting in a chain of newly independent countries stretching from Argentina and Chile in the south, to Mexico in the north. After the death of the king Ferdinand VII, in 1833, only Cuba and Puerto Rico remained under Spanish rule, until the Spanish–American War in 1898. Unlike the Spanish, the Portuguese did not divide their colonial territory in America. The captaincies they created were subdued to a centralized administration in Salvador (later relocated to Rio de Janeiro) which reported directly to the Portuguese Crown until its independence in 1822, becoming the Empire of Brazil.\n\n[[File:Maquina vapor Watt ETSIIM.jpg|thumb|Maquina vapor Watt ETSIIM]]\n\nThe development of the [[steam engine]] started the Industrial Revolution in [[United Kingdom|Great Britain]]. The steam engine was created to pump water from coal mines, enabling them to be deepened beyond [[groundwater]] levels. The date of the Industrial Revolution is not exact. [[Eric Hobsbawm]] held that it \"broke out\" in the 1780s and was not fully felt until the 1830s or 1840s, while [[T.S. Ashton]] held that it occurred roughly between 1760 and 1830 (in effect the reigns of [[George III of the United Kingdom|George III]], The [[English Regency|Regency]], and [[George IV of the United Kingdom|George IV]]). The great changes of centuries before the 19th were more connected with ideas, religion or military conquest, and technological advance had only made small changes in the material wealth of ordinary people.\n\nThe first Industrial Revolution merged into the Second Industrial Revolution around 1850, when technological and economic progress gained momentum with the development of steam-powered ships and railways, and later in the 19th century with the [[internal combustion engine]] and [[electric power]] generation. The Second Industrial Revolution was a phase of the Industrial Revolution; labeled as the separate Technical Revolution. From a technological and a social point of view there is no clean break between the two. Major innovations during the period occurred in the chemical, electrical, petroleum, and steel industries. Specific advancements included the introduction of oil fired steam turbine and internal combustion driven steel ships, the development of the airplane, the practical commercialization of the automobile, mass production of consumer goods, the perfection of canning, mechanical refrigeration and other food preservation techniques, and the invention of the telephone.\n\n[[Industrialization]] is the process of social and economic change whereby a human group is transformed from a pre-industrial society into an industrial one. It is a subdivision of a more general [[modernization]] process, where [[social change]] and [[economic development]] are closely related with [[innovation|technological innovation]], particularly with the development of large-scale energy and metallurgy production. It is the extensive organization of an economy for the purpose of manufacturing. Industrialization also introduces a form of philosophical change, where people obtain a different attitude towards their [[nature|perception of nature]].\n\n[[File:First passenger railway 1830.jpg|left|thumb|[[Liverpool and Manchester Railway]] from 1830, world's first railway.]]\nAn [[economic system|economy]] based on [[manual labour]] was replaced by one dominated by industry and the manufacture of machinery. It began with the mechanization of the [[textile]] industries and the development of [[iron]]-making techniques, and trade expansion was enabled by the introduction of [[canal]]s, improved roads, and then [[rail transport|railways]].\n\nThe introduction of [[steam engine|steam power]] (fuelled primarily by [[coal]]) and powered machinery (mainly in [[textile manufacturing]]) underpinned the dramatic increases in production capacity. The development of all-metal [[machine tool]]s in the first two decades of the 19th century facilitated the manufacture of more production machines for manufacturing in other industries.\n\nThe modern [[petroleum industry]] started in 1846 with the discovery of the process of refining [[kerosene]] from [[coal]] by [[Nova Scotia]]n [[Abraham Pineo Gesner]]. [[Ignacy Łukasiewicz]] improved Gesner's method to develop a means of refining kerosene from the more readily available \"rock oil\" (\"petr-oleum\") [[seep]]s in 1852 and the first rock oil mine was built in [[Bóbrka, Krosno County|Bóbrka]], near [[Krosno]] in [[Galicia (Central Europe)|Galicia]] in the following year. In 1854, [[Benjamin Silliman]], a science professor at [[Yale University]] in [[New Haven, Connecticut|New Haven]], was the first to fractionate petroleum by distillation. These discoveries rapidly spread around the world.\n\n[[File:Teslathinker.jpg|thumb|Nikola Tesla with his high-frequency transformer at East Houston Street, New York|alt=|193x193px]]\n\nEngineering achievements of the revolution ranged from electrification to developments in materials science. The advancements made a great contribution to the quality of life. In the first revolution, [[Lewis Paul]] was the original inventor of roller spinning, the basis of the water frame for spinning cotton in a cotton mill. [[Matthew Boulton]] and [[James Watt]]'s improvements to the steam engine were fundamental to the changes brought by the Industrial Revolution in both the Kingdom of Great Britain and the world.\n\nIn the latter part of the second revolution, [[Thomas Alva Edison]] developed many devices that greatly influenced life around the world and is often credited with the creation of the first industrial research laboratory. In 1882, Edison switched on the world's first large-scale [[electrical supply network]] that provided 110 volts direct current to fifty-nine customers in lower Manhattan. Also toward the end of the second industrial revolution, [[Nikola Tesla]] made many contributions in the field of [[electricity]] and [[magnetism]] in the late 19th and early 20th centuries.\n\nThe Industrial Revolutions were major [[technology|technological]], [[socioeconomics|socioeconomic]], and [[culture|cultural]] changes in late 18th and early 19th centuries that began in Britain and spread throughout the world. The effects spread throughout [[Western Europe]] and North America during the 19th century, eventually affecting the majority of the world. The impact of this change on [[society]] was enormous and is often compared to the [[Neolithic revolution]], when mankind developed [[agriculture]] and gave up its [[nomad]]ic lifestyle.\n\nIt has been argued that [[Gross Domestic Product|GDP]] per capita was much more stable and progressed at a much slower rate until the industrial revolution and the emergence of the modern [[capitalism|capitalist]] economy, and that it has since increased rapidly in capitalist countries.\n\n[[File:Alexanderplatz Berlin 1848.jpg|left|thumb|[[Revolutions of 1848|German Revolution]], Berlin 1848]]\nThe [[Revolutions of 1848|European Revolutions of 1848]], known in some countries as the Spring of Nations or the Year of Revolution, were a series of political upheavals throughout the European continent. Described as a revolutionary wave, the period of unrest began in France and then, further propelled by the French Revolution of 1848, soon spread to the rest of Europe. Although most of the revolutions were quickly put down, there was a significant amount of violence in many areas, with tens of thousands of people tortured and killed. While the immediate political effects of the revolutions were reversed, the long-term reverberations of the events were far-reaching.\n\nIndustrial age [[reform movement]]s began the gradual change of society rather than with episodes of rapid fundamental changes. The reformists' ideas were often grounded in liberalism, although they also possessed aspects of utopian, socialist or religious concepts. The Radical movement campaigned for electoral reform, a reform of the Poor Laws, free trade, educational reform, postal reform, prison reform, and public sanitation.\n\nFollowing the Enlightenment's ideas, the reformers looked to the [[Scientific Revolution]] and industrial progress to solve the social problems which arose with the Industrial Revolution. Newton's natural philosophy combined a mathematics of axiomatic proof with the mechanics of physical observation, yielding a coherent system of verifiable predictions and replacing a previous reliance on revelation and inspired truth. Applied to public life, this approach yielded several successful campaigns for changes in social policy.\n\n[[File:Russian Peasant Girls-retouched.jpg|thumb|Russian Peasant Girls, 1900]]\nUnder [[Peter the Great]], Russia was proclaimed an Empire in 1721 and became recognized as a world power. Ruling from 1682 to 1725, Peter defeated Sweden in the [[Great Northern War]], forcing it to cede West [[Karelia]] and [[Ingria]] (two regions lost by Russia in the [[Time of Troubles]]), as well as [[Governorate of Estonia|Estland]] and [[Livland]], securing Russia's access to the sea and sea trade. On the [[Baltic Sea]] Peter founded a new capital called [[Saint Petersburg]], later known as Russia's \"Window to Europe\". [[Peter the Great's reforms]] brought considerable Western European cultural influences to Russia. [[Catherine the Great|Catherine II]] (\"the Great\"), who ruled in 1762–96, extended Russian political control over the Polish-Lithuanian Commonwealth and incorporated most of its territories into Russia during the [[Partitions of Poland]], pushing the Russian frontier westward and southward. Russia would [[Russian conquest of Siberia|colonize]] the vast Asian lands of [[Siberia]] expanding by land to the Pacific Coast of Asia, and [[Russian America|North America]]. As the large realm embraced [[Absolute monarchy|Absolute Monarchy]] Russia remained more conservative than its western neighbors. In the 19th century, Russia was invaded by France in 1812 but emerged as a more powerful superpower afterwards. Nonetheless industrialization did not come to Russia until the 1870s. The Medieval practice of serfdom was [[Emancipation reform of 1861|abolished]] in 1861 freeing over thirty million Russian peasants. A market economy finally emerged in the Russian Empire. However class warfare rose, and the nation was vulnerable due to rivalries with the [[United Kingdom]], the [[Ottoman Empire]], and the [[Empire of Japan|Japanese Empire]].\n\nHistorians define the 19th century [[List of time periods|historical era]] as stretching from 1815 (the [[Congress of Vienna]]) to 1914 (the outbreak of the [[World War I|First World War]]). Alternatively, [[Eric Hobsbawm]] defined the [[The long 19th century|\"Long Nineteenth Century\"]] as spanning the years 1789 to 1914.\n\n[[File:World heads of state in 1889.jpg|upright=1.6|thumb|alt=Painting of a group of standing and seated heads of state in a variety of national uniforms and formal dress|\"The World's Sovereigns\", 1889]]\n\nIn the 1800s and early 1900s, once great and powerful Empires such as Spain, Ottoman Turkey, the Mughal Empire, and the Kingdom of Portugal began to break apart. Spain, which was at one time unrivaled in Europe, had been declining for a long time when it was crippled by Napoleon Bonaparte's invasion. Sensing the time was right, Spain's vast colonies in South America began a series of rebellions that ended with almost all of the Spanish territories gaining their independence.\n\nThe once mighty Ottoman Empire was wracked with a series of revolutions, resulting with the Ottoman's only holding a small region that surrounded the capital, Istanbul.\n\n[[File:Imperalism.xcf|left|thumb|Montage that depicts European wars of imperialism. By clockwise, wars include [[French conquest of Algeria|French Algerian War]], [[First Opium War|Opium War]], [[Russian conquest of Central Asia|Russian Conquest of Central Asia]] and [[Anglo-Zulu War|Zulu War]]]]\nThe Mughal empire, which was descended from the Mongol Khanate, was bested by the upcoming [[Maratha Confederacy]]. All was going well for the [[Maratha]]s until the British took an interest in the riches of India and the British ended up ruling not just the boundaries of Modern India, but also Pakistan, Burma, Nepal, Bangladesh and some Southern Regions of Afghanistan.\n\nThe King of Portugal's vast territory of Brazil reformed into the independent Empire of Brazil. With the defeat of Napoleonic France, Britain became undoubtedly the most powerful country in the world, and by the end of the First World War controlled a Quarter of the world's population and a third of its surface. However, the power of the British Empire did not end on land, since it had the greatest navy on the planet. Electricity, steel, and petroleum enabled Germany to become a great [[Power (international)|international power]] that raced to create empires of its own.\n\nThe [[Meiji Restoration]] was a chain of events that led to enormous changes in Japan's political and social structure that was taking a firm hold at the beginning of the [[Meiji Era]] which coincided the opening of Japan by the arrival of the [[Black Ships]] of [[Commodore (USN)|Commodore]] [[Matthew C. Perry|Matthew Perry]] and made [[Imperial Japan]] a [[great power]]. [[Imperial Russia|Russia]] and [[Qing Dynasty]] China failed to keep pace with the other world powers which led to massive social unrest in both empires. The Qing Dynasty's military power weakened during the 19th century, and faced with international pressure, massive [[rebellion]]s and defeats in wars, the dynasty declined after the mid-19th century.\n\nEuropean powers controlled parts of Oceania, with French [[New Caledonia]] from 1853 and [[French Polynesia]] from 1889; the Germans established colonies in [[German New Guinea|New Guinea]] in 1884, and [[German Samoa|Samoa]] in 1900. The United States expanded into the Pacific with Hawaii becoming a [[Territory of Hawaii|U.S. territory]] from 1898. Disagreements between the US, Germany and UK over Samoa led to the [[Tripartite Convention (1899)|Tripartite Convention of 1899]].\n\n[[File:British Empire 1897.jpg|thumb|The British Empire in 1897, marked in the traditional colour for imperial British dominions on maps|alt=|left]]The Victorian era of the United Kingdom was the period of [[Queen Victoria]]'s reign from June 1837 to January 1901. This was a long period of prosperity for the British people, as profits gained from the overseas British Empire, as well as from industrial improvements at home, allowed a large, educated middle class to develop. Some scholars would extend the beginning of the period—as defined by a variety of sensibilities and political games that have come to be associated with the Victorians—back five years to the passage of the [[Reform Act 1832]].\n[[File:Fleet Street. By James Valentine c.1890..jpg|alt=Fleet Street in London, 1890.|thumb|Fleet Street in London capital of the [[British Empire]], 1890]]\nIn Britain's \"imperial century\", victory over Napoleon left Britain without any serious international rival, other than Russia in central Asia. Unchallenged at sea, Britain adopted the role of global policeman, a state of affairs later known as the \"[[Pax Britannica]]\", and a foreign policy of \"[[splendid isolation]]\". Alongside the formal control it exerted over its own colonies, Britain's dominant position in world trade meant that it effectively controlled the economies of many nominally independent countries, such as China, [[Argentina]] and [[Thailand|Siam]], which has been generally characterized as \"[[informal empire]]\". Of note during this time was the [[Anglo-Zulu War]], which was fought in 1879 between the British Empire and the [[Zulu Empire]].\n\nBritish imperial strength was underpinned by the [[Steamboat|steamship]] and the [[Telegraphy|telegraph]], new technologies invented in the second half of the 19th century, allowing it to control and defend the Empire. By 1902, the British Empire was linked together by a network of telegraph cables, the so-called [[All Red Line]]. Growing until 1922, around of territory and roughly 458 million people were added to the British Empire. The British established colonies in Australia in 1788, New Zealand in 1840 and [[Colonial Fiji|Fiji]] in 1872, with much of [[Oceania]] becoming part of the British Empire.\n\nThe [[Bourbon Restoration]] followed the ousting of Napoleon I of France in 1814. The Allies restored the Bourbon Dynasty to the French throne. The ensuing period is called the Restoration, following French usage, and is characterized by a sharp conservative reaction and the re-establishment of the Roman Catholic Church as a power in French politics. The [[July Monarchy]] was a period of liberal constitutional monarchy in France under King Louis-Philippe starting with the July Revolution (or Three Glorious Days) of 1830 and ending with the Revolution of 1848. The [[Second French Empire|Second Empire]] was the Imperial Bonapartist regime of Napoleon III from 1852 to 1870, between the Second Republic and the Third Republic, in France.\n\n[[File:BismarckundNapoleonIII.jpg|left|thumb|[[Napoleon III of France|Napoleon III]] and [[Otto von Bismarck|Bismarck]] after the [[Battle of Sedan]]]]\nThe [[Franco-Prussian War]] was a conflict between France and Prussia, while Prussia was backed up by the North German Confederation, of which it was a member, and the South German states of Baden, Württemberg and Bavaria. The complete Prussian and German victory brought about the final unification of Germany under King Wilhelm I of Prussia. It also marked the downfall of Napoleon III and the end of the Second French Empire, which was replaced by the Third Republic. As part of the settlement, almost all of the territory of Alsace-Lorraine was taken by Prussia to become a part of Germany, which it would retain until the end of World War I.\n\nThe [[French Third Republic]] was the republican government of France between the end of the Second French Empire following the defeat of Louis-Napoléon in the Franco-Prussian war in 1870 and the Vichy Regime after the invasion of France by the German Third Reich in 1940. The Third Republic endured seventy years, making it the most long-lasting regime in France since the collapse of the Ancien Régime in the French Revolution of 1789.\n\n[[File:Une Dame d´une Fortune Ordinaire dans son Intérieur au Milieu de ses Habitudes Journalières, by Jean-Baptiste Debret 1823.jpg|thumb|Depiction of [[Slavery]] in [[Brazil]], before 1823]]\n\n[[Slavery]] was greatly reduced around the world in the 19th century. Following a successful [[Haitian Revolution|slave revolt in Haiti]], Britain forced the [[Barbary pirates]] to halt their practice of kidnapping and enslaving Europeans, [[Slavery Abolition Act|banned slavery throughout its domain]], and charged its navy with ending the global [[slave trade]]. Slavery was then abolished in [[Emancipation reform of 1861 in Russia|Russia]], [[Emancipation Proclamation|America]], and [[Lei Áurea|Brazil]].\n\nFollowing the abolition of the slave trade in 1807 and propelled by economic exploitation, the [[Scramble for Africa]] was initiated formally at the [[Berlin West Africa Conference]] in 1884–1885. The Berlin Conference attempted to avoid war among the European powers by allowing the European rival countries to carve up the continent of Africa into national colonies. Africans were not consulted.\n\nThe major European powers laid claim to the areas of [[Africa]] where they could exhibit a sphere of influence over the area. These claims did not have to have any substantial land holdings or treaties to be legitimate. The European power that demonstrated its control over a territory accepted the mandate to rule that region as a national [[colony]]. The European nation that held the claim developed and benefited from their colony’s commercial interests without having to fear rival European competition. With the colonial claim came the underlying assumption that the European power that exerted control would use its mandate to offer protection and provide welfare for its colonial peoples, however, this principle remained more theory than practice. There were many documented instances of material and moral conditions deteriorating for native Africans in the late nineteenth and early twentieth centuries under European colonial rule, to the point where the colonial experience for them has been described as \"hell on earth.\"\n[[File:Kongokonferenz.jpg|left|thumb|European officials staking claims to [[Africa]] in the [[Congress of Berlin]] ]]\nAt the time of the [[Berlin Conference]], Africa contained one-fifth of the world’s population living in one-quarter of the world’s land area. However, from Europe's perspective, they were dividing an unknown continent. European countries established a few coastal colonies in Africa by the mid-nineteenth century, which included [[Cape Colony]] (Great Britain), [[Angola]] (Portugal), and [[Algeria]] (France), but until the late nineteenth century Europe largely traded with free African states without feeling the need for territorial possession. Until the 1880s most of Africa remained uncharted, with western maps from the period generally showing blank spaces for the continent’s interior.\n\nFrom the 1880s to 1914, the European powers expanded their control across the African continent, competing with each other for Africa’s land and resources. [[Great Britain]] controlled various colonial holdings in East Africa that spanned the length of the African continent from Egypt in the north to South Africa. The [[France|French]] gained major ground in West Africa, and the [[Portugal|Portuguese]] held colonies in southern Africa. [[Germany]], [[Italy]], and [[Spain]] established a small number of colonies at various points throughout the continent, which included German East Africa (Tanganyika) and German Southwest Africa for Germany, Eritrea and Libya for Italy, and the Canary Islands and Rio de Oro in northwestern Africa for Spain. Finally, for [[Leopold II of Belgium|King Leopold]] (ruled from 1865 to 1909), there was the large “piece of that great African cake” known as the [[Belgian Congo|Congo]], which became his personal fiefdom. By 1914, almost the entire continent was under European control. [[Liberia]], which was settled by freed American slaves in the 1820s, and Abyssinia ([[Ethiopia]]) in eastern Africa were the last remaining independent African states.\n\n[[File:View of Kobe LACMA M.91.377.8.jpg|left|thumb|Kobe Japan and its harbor, 1865. Hand colored.]] Around the end of the 19th century and into the 20th century, the [[Meiji era]] occurred during the reign of the [[Emperor Meiji|Meiji Emperor]]. During this time, Japan started its modernization and rose to world power status. This [[Japanese era name|era name]] means \"Enlightened Rule\". In Japan, the Meiji Restoration started in the 1860s, marking the rapid modernization by the Japanese themselves along European lines. Much research has focused on the issues of discontinuity versus continuity with the previous Tokugawa Period. It was not until the beginning of the Meiji Era that the Japanese government began taking modernization seriously. Japan expanded its military production base by opening arsenals in various locations. The [[Ministry of War (pre-modern Japan)|hyobusho]] (war office) was replaced with a [[Imperial Japanese Army|War Department]] and a [[Imperial Japanese Navy|Naval Department.]] The [[samurai]] class suffered great disappointment the following years.\n\nLaws were instituted that required every able-bodied male Japanese citizen, regardless of class, to serve a mandatory term of three years with the first reserves and two additional years with the second reserves. This action, the deathblow for the samurai warriors and their \"daimyōs\", initially met resistance from both the peasant and warrior alike. The peasant class interpreted the term for military service, ketsu-eki (\"blood tax\") literally, and attempted to avoid service by any means necessary. The Japanese government began modelling their ground forces after the French military. The French government contributed greatly to the training of Japanese officers. Many were employed at the military academy in Kyoto, and many more still were feverishly translating French field manuals for use in the Japanese ranks. Japan's modernized military gave Japan the opportunity to engage in Imperialism with its victory against the [[Qing dynasty|Qing Empire]] in the [[First Sino-Japanese War]] Japan annexed [[Taiwan under Japanese rule|Taiwan]], [[Korea under Japanese rule|Korea]] and the Chinese province of [[Shandong|Shangdong]].\n\nAfter the death of the Meiji Emperor, the [[Emperor Taishō|Taishō Emperor]] took the throne, the[[Taishō period]] was a time of democratic reform granting democratic rights to all Japanese men. Foreigners would instrumental in aiding in Japan's modernization. A key foreign observer of the remarkable and rapid changes in [[culture of Japan|Japanese society]] in this period was [[Ernest Mason Satow]].\n\n[[File:Westward the Course of Empire.jpg|thumb|American westward expansion is idealized in [[Emanuel Leutze]]'s famous painting \"Westward the Course of Empire Takes its Way\" (1861).]]\nThe [[Antebellum Age]] was a period of increasing division in the country based on the growth of slavery in the [[Southern United States|American South]] and in the western territories of [[Kansas]] and [[Nebraska]] that eventually led to the [[American Civil War|Civil War]] in 1861. The Antebellum Period is often considered to have begun with the [[Kansas–Nebraska Act]] of 1854, although it may have begun as early as 1812. This period is also significant because it marked the transition of American manufacturing to the industrial revolution.\n\n\"[[Manifest destiny]]\" was the belief that the United States was destined to expand across the North American continent, from the Atlantic seaboard to the Pacific Ocean. During this time, the United States expanded to the Pacific Ocean—\"from sea to shining sea\"—largely defining the borders of the contiguous United States as they are today.\n\n[[File:Gettysburg by Britton.ogg|left|thumb|Modern recording of [[Gettysburg Address]] originally spoken by [[President of the United States|U.S President]] [[Abraham Lincoln]]]]\n[[File:Thure de Thulstrup - L. Prang and Co. - Battle of Gettysburg - Restoration by Adam Cuerden 0.5.jpg|left|thumb|[[Battle of Gettysburg]] – Restoration by Adam Cuerden 0.5]]\n\nThe American Civil War began when seven [[Slave state|Southern slave states]] declared their [[secession]] from the U.S. and formed the [[Confederate States of America]], the Confederacy (four more states joinged the Confederacy later). Led by [[Jefferson Davis]], they fought against the [[Union (American Civil War)|U.S. federal government (the Union)]] under President [[Abraham Lincoln]], which was supported by all the free states and the five [[Border states (American Civil War)|border slave states]] in the north.\n\nNorthern leaders agreed that victory would require more than the end of fighting. Secession and Confederate nationalism had to be totally repudiated and all forms of slavery or quasi-slavery had to be eliminated. Lincoln proved effective in mobilizing support for the war goals, raising large armies and supplying them, avoiding foreign interference, and making the end of slavery a war goal. The Confederacy had a larger area than it could defend, and it failed to keep its ports open and its rivers clear as was the case in the [[Siege of Vicksburg|Battle of Vickysburg]]. The [[Union (American Civil War)|North]] kept up the pressure as the South could barely feed and clothe its soldiers. Its soldiers, especially those in the East under the command of General [[Robert E. Lee]] proved highly resourceful until they finally were overwhelmed by Generals [[Ulysses S. Grant]] and [[William Tecumseh Sherman|William T. Sherman]] in 1864–65. The [[Reconstruction era|Reconstruction Era]] (1863–77) began with the [[Emancipation Proclamation|Emancipation proclamation]] in 1863, and included freedom, full citizenship and voting rights for Southern blacks. It was followed by a reaction that left the blacks in a second class status legally, politically, socially and economically until the 1960s.\n\n[[File:Flatiron Building and Street Scene October 8th 1902 New York City.ogv|thumb|1902 New York City in early [[skyscraper]]s]]\n\nDuring the Gilded Age, there was substantial growth in population in the United States and extravagant displays of wealth and excess of America's upper-class during the post-Civil War and post-Reconstruction era, in the late 19th century. The wealth polarization derived primarily from industrial and population expansion. The businessmen of the [[Second Industrial Revolution]] created industrial towns and cities in the [[Northeastern United States|Northeast]] with new factories, and contributed to the creation of an ethnically diverse industrial [[working class]] which produced the wealth owned by rising super-rich [[Robber baron (industrialist)|industrialists and financiers called the \"robber barons\"]]. An example is the company of [[John D. Rockefeller]], who was an important figure in shaping the new oil industry. Using highly effective tactics and aggressive practices, later widely criticized, [[Standard Oil]] absorbed or destroyed most of its competition.\n\nThe creation of a modern industrial economy took place. With the creation of a [[Infrastructure|transportation and communication infrastructure]], the [[corporation]] became the dominant form of business organization and a [[Management|managerial revolution]] transformed [[business]] operations. In 1890, [[Congress of the United States|Congress]] passed the [[Sherman Antitrust Act]]—the source of all American anti-monopoly laws. The law forbade every contract, scheme, deal, or conspiracy to restrain trade, though the phrase \"restraint of trade\" remained subjective. By the beginning of the 20th century, per capita income and [[industrial production]] in the United States exceeded that of any other country except Britain. Long hours and hazardous working conditions led many workers to attempt to form [[labor union]]s despite strong opposition from industrialists and the courts. But the courts did protect the marketplace, declaring the Standard Oil group to be an \"unreasonable\" [[monopoly]] under the [[Sherman Antitrust Act]] in 1911. It ordered Standard to break up into 34 independent companies with different boards of directors.\n\n[[File:Liszt- au bord d une.ogg|thumb|An example of 19th century [[Classical music|Classical Music]] Liszt- au bord d une, 1855]]\n[[File:Darwin's finches by Gould.jpg|left|thumb|Charles Darwin's finches by Gould, 1882. [[Charles Darwin]] used the example of [[Darwin's finches|finches]] in the [[Galápagos Islands|Galapagos Islands]] as evidence for the [[Natural selection|Theory of Evolution]].]]\nReplacing the [[classical physics]] in use since the end of the scientific revolution, \"[[modern physics]]\" arose in the early 20th century with the advent of [[quantum physics]], substituting [[mathematical physics|mathematical studies]] for [[experimental physics|experimental studies]] and examining [[equation]]s to build a [[theoretical structure]]. The [[old quantum theory]] was a collection of results which predate modern [[quantum mechanics]], but were never complete or self-consistent. The collection of [[heuristic]] prescriptions for quantum mechanics were the first corrections to [[classical mechanics]]. Outside the realm of quantum physics, the various [[aether theories]] in classical physics, which supposed a \"[[classical elements|fifth element]]\" such as the [[Luminiferous aether]], were nullified by the [[Michelson–Morley experiment]]—an attempt to detect the motion of earth through the aether. In biology, [[Darwinism]] gained acceptance, promoting the concept of [[adaptation]] in the theory of [[natural selection]]. The fields of [[modern geology|geology]], [[modern astronomy|astronomy]] and [[modern psychology|psychology]] also made strides and gained new insights. In [[modern medicine|medicine]], there were advances in [[medical theory]] and [[medical treatment|treatments]].\n\nStarting one-hundred years before the 20th century, the [[Enlightenment (spiritual)|enlightenment spiritual philosophy]] was challenged in various quarters around the 1900s. Developed from earlier [[secular]] traditions, modern [[Humanism|Humanist]] [[ethics|ethical philosophies]] affirmed the dignity and worth of all people, based on the ability to determine right and wrong by appealing to universal human qualities, particularly [[rationalism|rationality]], without resorting to the supernatural or alleged divine authority from religious texts. For [[liberal humanism|liberal humanists]] such as [[Jean-Jacques Rousseau|Rousseau]] and [[Immanuel Kant|Kant]], the universal law of [[reason]] guided the way toward total emancipation from any kind of tyranny. These ideas were challenged, for example by the [[young Marx|young Karl Marx]], who criticized the project of political emancipation (embodied in the form of [[human rights]]), asserting it to be symptomatic of the very dehumanization it was supposed to oppose. For [[Friedrich Nietzsche]], humanism was nothing more than a secular version of [[theism]]. In his \"[[Genealogy of Morals]]\", he argues that human rights exist as a means for the weak to collectively constrain the strong. On this view, such rights do not facilitate emancipation of life, but rather deny it. In the 20th century, the notion that human beings are rationally autonomous was challenged by the concept that humans were driven by unconscious irrational desires.\n\n[[File:Albert Einstein 1921 by F Schmutzer.jpg|thumb|upright=0.75|Albert Einstein in 1921|alt=]]\n[[Sigmund Freud]] is renowned for his redefinition of [[sexual desire]] as the primary motivational energy of human life, as well as his therapeutic techniques, including the use of [[free association (psychology)|free association]], his [[transference|theory of transference]] in the therapeutic relationship, and the [[Dream interpretation|interpretation of dreams]] as sources of insight into [[unconscious desire]]s.\n\n[[Albert Einstein]] is known for his theories of [[special relativity]] and [[general relativity]]. He also made important contributions to [[statistical mechanics]], especially his mathematical treatment of [[Brownian motion]], his resolution of the [[Einstein solid|paradox of specific heats]], and his connection of [[Fluctuation dissipation theorem|fluctuations and dissipation]]. Despite his reservations about its interpretation, Einstein also made contributions to quantum mechanics and, indirectly, [[quantum field theory]], primarily through his theoretical studies of the [[photon]].\n\nAt the end of the 19th century, [[Social Darwinism]] was promoted and included the various ideologies based on a concept that competition among all individuals, groups, nations, or ideas was a \"natural\" framework for social evolution in human societies. In this view, society's advancement is dependent on the \"[[survival of the fittest]]\", the term was in fact coined by [[Herbert Spencer]] and referred to in \"[[The Gospel of Wealth]]\" written by [[Andrew Carnegie]].\n\n[[File:Pyramid of Capitalist System.jpg|thumb|[[Pyramid of Capitalist System]], produced in the United States|alt=|left]]\n[[Karl Marx]] summarized his approach to history and politics in the opening line of the first chapter of \"[[The Communist Manifesto]]\" (1848). He wrote:\nThe \"Manifesto\" went through a number of editions from 1872 to 1890; notable new prefaces were written by Marx and Engels for the 1872 German edition, the 1882 Russian edition, the 1883 German edition, and the 1888 English edition. In general, [[Marxism]] identified five (and one transitional) successive stages of development in Western Europe.\n\n[[File:Australia 1900.jpg|thumb|Though still tied to [[United Kingdom|Great Britain]] in the [[Commonwealth of Nations|commonwealth]] [[Australia]] achieved peaceful independence in 1901.|alt=]]\nMajor political developments saw the former [[British Empire]] lose most of its remaining political power over [[British Commonwealth|commonwealth]] countries. The [[Trans-Siberian Railway]], crossing Asia by train, was complete by 1916. Other events include the [[Israeli–Palestinian conflict]], two world wars, and the [[Cold War]].\n\nIn 1901, the [[Federation of Australia]] was the process by which the six separate British [[self-governing colony|self-governing colonies]] of [[New South Wales]], [[Queensland]], [[South Australia]], [[Tasmania]], [[Victoria (Australia)|Victoria]] and [[Western Australia]] formed one nation. They kept the systems of government that they had developed as separate colonies but also would have a federal government that was responsible for matters concerning the whole nation. When the [[Constitution of Australia]] came into force, the colonies collectively became [[States and territories of Australia|states]] of the [[Australia|Commonwealth of Australia]].\n\n[[File:Xinhai Revolution in Shanghai.jpg|thumb|[[Xinhai Revolution]] in [[Shanghai]]; [[Chen Qimei]] organized Shanghainese civilians to start the uprising and was successful. The picture above is [[Nanjing Road]] after the uprising, hung with the [[Five Races Under One Union]] Flags then used by the revolutionaries.|alt=|left]]\nThe last days of the [[Qing dynasty]] were marked with civil unrest, [[Hundred Days' Reform|failed reforms]] and foreign invasions such as the [[Boxer Rebellion|Boxer rebellion]]. Responding to these civil failures and discontent, the Qing Imperial Court did attempt to reform the government in various ways, as the decision to draft a constitution in 1906, the establishment of provincial legislatures in 1909, and the preparation for a national parliament in 1910. However, many of these measures were opposed by the conservatives of the Qing Court, and many reformers were either imprisoned or executed outright. The failures of the Imperial Court to enact such reforming measures of political liberalization and modernization caused the reformists to steer toward the road of revolution.\n\nThe assertions of Chinese philosophy began to integrate concepts of Western philosophy, as steps toward modernization. By the time of the [[Xinhai Revolution]] in 1911, there were many calls, such as the [[May Fourth Movement]], to completely abolish the old imperial institutions and practices of China. There were attempts to incorporate [[democracy]], [[republicanism]], and [[industrialism]] into Chinese philosophy, notably by [[Sun Yat-sen]] at the beginning of the 20th century\n\nIn 1912, the Republic of China was established and Sun Yat-sen was inaugurated in [[Nanjing]] as the first [[President of the Republic of China|Provisional President]]. But power in [[Beijing]] already had passed to [[Yuan Shikai]], who had effective control of the [[Beiyang Army]], the most powerful military force in China at the time. To prevent [[civil war]] and possible foreign intervention from undermining the infant republic, leaders agreed to Army's demand that China be united under a Beijing government. On March 10, in Beijing, Shikai was sworn in as the second Provisional President of the Republic of China.\n\nAfter the early 20th century revolutions, shifting alliances of [[Warlord era (China)|China's regional warlords]] waged war for control of the Beijing government. Despite the fact that various warlords gained control of the government in Beijing during the warlord era, this did not constitute a new era of control or governance, because other warlords did not acknowledge the transitory governments in this period and were a law unto themselves. These military-dominated governments were collectively known as the [[Beiyang government]]. The warlord era ended around 1927.\n\n[[File:World 1898 empires colonies territory.png|center|thumb|700x700px|The World in 1898 color coded for major empires. The [[British Empire]], the [[Russian Empire]], the [[Qing dynasty|Qing Dynasty]] and the [[United States]] were the [[List of largest empires|largest countries]] at the time. |alt=]]\n[[File:Battle of Yalu River 1904.jpg|thumb|The [[Russo-Japanese War]] was the first time a European Country was defeated by an Asian Country in modern times. The Japanese Victory shocked the world.|alt=]]\nIn 1900 the World's Population had approached approximately 1.6 billion.\nFour years into the 20th century saw the [[Russo-Japanese War]] with the [[Battle of Port Arthur]] establishing the [[Empire of Japan]] as a world power. The Russians were in constant pursuit of a [[warm water port]] on the Pacific Ocean, for their navy as well as for maritime trade. The Manchurian Campaign of the [[Russian Empire]] was fought against the Japanese over [[Manchuria]] and [[Korea]]. The major theatres of operations were Southern Manchuria, specifically the area around the [[Liaodong Peninsula]] and [[Mukden]], and the seas around Korea, Japan, and the [[Yellow Sea]]. The resulting campaigns, in which the fledgling Japanese military consistently attained victory over the Russian forces arrayed against them, were unexpected by world observers. These victories, as time transpired, would dramatically transform the distribution of power in East Asia, resulting in a reassessment of Japan's recent entry onto the world stage. The embarrassing string of defeats increased Russian popular dissatisfaction with the inefficient and corrupt Tsarist government.\n\nThe [[Russian Revolution of 1905]] was a wave of mass political unrest through vast areas of the [[Russian Empire]]. Some of it was directed against the government, while some was undirected. It included [[terrorism]], worker strikes, peasant unrests, and military mutinies. It led to the establishment of the limited constitutional monarchy, the establishment of [[State Duma of the Russian Empire]], and the [[multi-party system]].\n\nIn China, the Qing Dynasty was overthrown following the [[Xinhai Revolution]]. The Xinhai Revolution began with the [[Wuchang Uprising]] on October 10, 1911 and ended with the abdication of [[Emperor Puyi]] on February 12, 1912. The primary parties to the conflict were the Imperial forces of the [[Qing dynasty]] (1644–1911), and the revolutionary forces of the [[Tongmenghui|Chinese Revolutionary Alliance]] (Tongmenghui).\n\n[[File:Steamship Titanic.jpg|thumb|The [[RMS Titanic|Titanic]] was the largest ship constructed in its time. Deemed invincible, it was sunk by [[iceberg]]s off the coast of [[Labrador|Labrador, Canada.]] |alt=|left]]\nThe [[Edwardian era]] in the United Kingdom is the period spanning the reign of [[Edward VII of the United Kingdom|King Edward VII]] up to the end of the First World War, including the years surrounding the sinking of the [[RMS Titanic|RMS \"Titanic\"]]. In the early years of the period, the [[Second Boer War]] in South Africa split the country into anti- and pro-war factions. The imperial policies of the Conservatives eventually proved unpopular and in the [[United Kingdom general election, 1906|general election of 1906]] the Liberals won a huge landslide. The Liberal government was unable to proceed with all of its radical programme without the support of the [[House of Lords]], which was largely Conservative. Conflict between the two Houses of Parliament over the [[People's Budget]] led to a reduction in the power of the peers in 1910. The [[United Kingdom general election, January 1910|general election in January that year]] returned a [[hung parliament]] with the balance of power held by [[Labour Party (UK)|Labour]] and [[Nationalist Party (Ireland)|Irish Nationalist]] members.\n\nThe [[causes of World War I]] included many factors, including the conflicts and antagonisms of the four decades leading up to the war. The [[Triple Entente]] was the name given to the loose alignment between the [[British Empire|United Kingdom]], [[French Third Republic|France]], and [[Russian Empire|Russia]] after the signing of the [[Anglo-Russian Entente]] in 1907. The alignment of the three powers, supplemented by various agreements with [[Empire of Japan|Japan]], the United States, and [[Spain under the Restoration|Spain]], constituted a powerful counterweight to the [[Triple Alliance (1882)|Triple Alliance]] of [[German Empire|Germany]], [[Austria-Hungary]], and [[Kingdom of Italy (1861–1946)|Italy]], the third having concluded an additional secret agreement with France effectively nullifying her Alliance commitments. Militarism, alliances, imperialism, and nationalism played major roles in the conflict. The immediate origins of the war lay in the decisions taken by statesmen and generals during the [[July Crisis]] of 1914, the spark (or casus belli) for which was the assassination of [[Archduke Franz Ferdinand]] of Austria.\n\nHowever, the crisis did not exist in a void; it came after a long series of diplomatic clashes between the Great Powers over European and colonial issues in the decade prior to 1914 which had left tensions high. The diplomatic clashes can be traced to changes in the balance of power in Europe since 1870. An example is the [[Baghdad Railway]] which was planned to connect the [[Ottoman Empire]] cities of [[Konya]] and [[Baghdad]] with a line through modern-day Turkey, Syria and Iraq. The railway became a source of international disputes during the years immediately preceding World War I. Although it has been argued that they were resolved in 1914 before the war began, it has also been argued that the railroad was a cause of the First World War. Fundamentally the war was sparked by tensions over territory in the [[Balkans]]. Austria-Hungary competed with Serbia and Russia for territory and influence in the region and they pulled the rest of the great powers into the conflict through their various alliances and treaties. The [[Balkan Wars]] were two wars in South-eastern Europe in 1912–1913 in the course of which the [[Balkan League]] (Bulgaria, Montenegro, Greece, and Serbia) first captured Ottoman-held remaining part of Thessaly, Macedonia, Epirus, Albania and most of Thrace and then fell out over the division of the spoils, with incorporation of Romania this time.\n\n[[File:World War 1.gif|thumb|upright=3.4|center|Various periods of World War I; 1914.07.28 (Tsar Nicholas II of Russia orders a partial mobilization against Austria-Hungary), 1914.08.01 (Germany declares war on Russia), 1914.08.03 (Germany declares war on Russia's ally France), 1914.08.04 (Britain declares war on Germany), 1914.12 (British and German [[Christmas truce]]), 1915.12 (French and German Christmas truce), 1916.12 ([[Battle of Magdhaba]]), 1917.12 (British troops take Jerusalem from the Ottoman Empire), and 1918.11.11 (World War I ends: Germany signs an armistice agreement with the Allies).Allies and Central Powers in the First World War Allied powers and areas Central powers and colonies or occupied territory Neutral countries]] [[File:General gouraud french army world war i machinegun marne 1918.JPEG|thumb|French Army, 1918]]\nThe First World War began in 1914 and lasted to the final [[Armistice with Germany (Compiègne)|Armistice]] in 1918. The [[Allies of World War I|Allied Powers]], led by the [[Britain in World War I|British Empire]], [[French Third Republic|France]], Russia until March 1918, Japan and the United States after 1917, defeated the [[Central Powers]], led by the [[German Empire]], [[Austro-Hungarian Empire]] and the [[Ottoman Empire]]. The war caused the disintegration of four empires—the Austro-Hungarian, German, Ottoman, and Russian ones—as well as radical change in the European and West Asian maps. The Allied powers before 1917 are referred to as the [[Triple Entente]], and the Central Powers are referred to as the [[Triple Alliance (1882)|Triple Alliance]].\n\n[[File:Chemical Warfare in the Twentieth Century Q54781.jpg|left|thumb|[[Chemical warfare|Chemical Warfare]] was used for the first time in WWI. Soldiers used [[Gas mask|gas-masks]] to protect themselves]]Much of the fighting in World War I took place along the [[Western Front (World War I)|Western Front]], within a system of opposing manned trenches and fortifications (separated by a \"[[No man's land]]\") running from the [[North Sea]] to the border of Switzerland. On the [[Eastern Front (World War I)|Eastern Front]], the vast eastern plains and limited rail network prevented a trench warfare stalemate from developing, although the scale of the conflict was just as large. Hostilities also occurred on and under the sea and—for the first time—from the air. More than 9 million soldiers died on the various battlefields, and nearly that many more in the participating countries' home fronts on account of food shortages and [[genocide]] committed under the cover of various civil wars and internal conflicts. Notably, more people died of the worldwide [[Spanish flu|influenza outbreak]] at the end of the war and shortly after than died in the hostilities. The unsanitary conditions engendered by the war, severe overcrowding in barracks, wartime propaganda interfering with public health warnings, and migration of so many soldiers around the world helped the outbreak become a [[pandemic]].\n\nUltimately, World War I created a decisive break with the old [[New World Order (political)|world order]] that had emerged after the [[Napoleonic Wars]], which was modified by the mid-19th century's nationalistic revolutions. The results of World War I would be important factors in the development of World War II approximately 20 years later. More immediate to the time, the [[partitioning of the Ottoman Empire]] was a political event that redrew the political boundaries of West Asia. The huge conglomeration of territories and peoples formerly ruled by the Sultan of the Ottoman Empire was divided into several new nations. The partitioning brought the creation of the modern [[Arab world]] and the [[Republic of Turkey]]. The [[League of Nations]] granted France mandates over [[French Mandate of Syria|Syria]] and [[French Mandate of Lebanon|Lebanon]] and granted the United Kingdom mandates over [[British Mandate of Mesopotamia|Mesopotamia]] and [[British Mandate for Palestine (legal instrument)|Palestine]] (which was later divided into two regions: [[Mandatory Palestine|Palestine]] and [[Emirate of Transjordan|Transjordan]]). Parts of the Ottoman Empire on the [[Arabian Peninsula]] became parts of what are today [[Saudi Arabia]] and [[Yemen]].\n\n[[File:Lenin.gif|thumb|Lenin]]\n\nThe Russian Revolution is the series of revolutions in Russia in 1917, which destroyed the [[Tsarist autocracy]] and led to the creation of the Soviet Union. Following the abdication of [[Nicholas II of Russia]], the [[Russian Provisional Government]] was established. In October 1917, a [[October Revolution|\"red\" faction revolution]] occurred in which the [[Red Guards (Russia)|Red Guard]], armed groups of workers and deserting soldiers directed by the Bolshevik Party, seized control of [[Saint Petersburg]] (then known as [[Petrograd]]) and began an immediate armed takeover of cities and villages throughout the former [[Russian Empire]].\n\nAnother action in 1917 that is of note was the armistice signed between Russia and the Central Powers at [[Brest-Litovsk]]. As a condition for peace, the treaty by the [[Central Powers]] conceded huge portions of the former Russian Empire to [[German Empire|Imperial Germany]] and the Ottoman Empire, greatly upsetting [[nationalist]]s and [[Conservatism|conservatives]]. The Bolsheviks made peace with the [[German Empire]] and the [[Central Powers]], as they had promised the Russian people prior to the Revolution. Vladimir Lenin's decision has been attributed to his sponsorship by the foreign office of [[Wilhelm II, German Emperor]], offered by the latter in hopes that with a revolution, Russia would withdraw from World War I. This suspicion was bolstered by the German Foreign Ministry's sponsorship of Lenin's return to [[Saint Petersburg|Petrograd]]. The [[Triple Entente|Western Allies]] expressed their dismay at the Bolsheviks, upset at:\n[[File:Russian civil war posters.png|left|thumb|Two contrasting visions of the [[Russian Civil War]]. To the left is propaganda from the [[White Army]], to the right is propaganda from the [[Bolsheviks]]. ]]\nIn addition, there was a concern, shared by many [[Central Powers]] as well, that the socialist revolutionary ideas would spread to the West. Hence, many of these countries expressed their support for the Whites, including the provision of troops and supplies. [[Winston Churchill]] declared that Bolshevism must be \"strangled in its cradle\".\n\nThe Russian Civil War was a multi-party war that occurred within the former [[Russian Empire]] after the [[Russian provisional government]] collapsed and the [[Soviet republic (system of government)|Soviets]] under the domination of the [[Bolshevik]] party assumed power, first in [[Saint Petersburg|Petrograd (St. Petersburg)]] and then in other places. In the wake of the [[October Revolution]], the old Russian Imperial Army had been demobilized; the volunteer-based Red Guard was the Bolsheviks' main military force, augmented by an armed military component of the [[Cheka]], the Bolshevik state security apparatus. There was an instituted mandatory conscription of the rural peasantry into the Red Army. Opposition of rural Russians to Red Army conscription units was overcome by taking hostages and shooting them when necessary in order to force compliance. Former Tsarist officers were utilized as \"military specialists\" (\"voenspetsy\"), taking their families hostage in order to ensure loyalty. At the start of the war, three-fourths of the Red Army officer corps was composed of former Tsarist officers. By its end, 83% of all Red Army divisional and corps commanders were ex-Tsarist soldiers.\n[[File:Vladivostok intervention.jpg|thumb|In the Russian Civil War, over eleven nations intervened in favor of the [[White Movement]]. Here Japanese occupy [[Vladivostok]].]]\nThe principal fighting occurred between the [[Bolshevik]] [[Red Army]] and the forces of the [[White Movement|White Army]]. Many foreign armies warred against the Red Army, notably the [[Allied intervention in the Russian Civil War|Allied Forces]], yet many volunteer foreigners fought in both sides of the Russian Civil War. Other nationalist and regional political groups also participated in the war, including the Ukrainian nationalist [[Green armies|Green Army]], the Ukrainian anarchist [[Revolutionary Insurrectionary Army of Ukraine|Black Army]] and [[Black Guards]], and warlords such as [[Ungern von Sternberg]]. The most intense fighting took place from 1918 to 1920. Major military operations ended on 25 October 1922 when the Red Army occupied [[Vladivostok]], previously held by the [[Provisional Priamur Government]]. The last enclave of the White Forces was the [[Ayano-Maysky District]] on the Pacific coast. The majority of the fighting ended in 1920 with the defeat of General [[Pyotr Wrangel]] in the [[Crimea]], but a notable resistance in certain areas continued until 1923 (e.g., [[Kronstadt Uprising]], [[Tambov Rebellion]], [[Basmachi Revolt]], and the final resistance of the [[White movement]] in the [[Russian Far East|Far East]]).\n\nWhile the early 1920s was a time of flux for revolutionary Russia and Central Asia, the [[Union of Soviet Socialist Republics]] was proclaimed in 1922 as the successor state to the fallen [[Russian Empire]]. Revolutionary leader [[Vladimir Lenin]] died of natural causes and was succeeded by [[Joseph Stalin]].\n\nIn 1917, China declared war on Germany in the hope of recovering its lost province, then under Japanese control. The [[New Culture Movement]] occupied the period from 1917 to 1923. Chinese representatives refused to sign the [[Treaty of Versailles]], due to intense pressure from the student protesters and public opinion alike.\n[[File:Student Demonstrations, June 4th and 5th, 1919 2.jpg|left|thumb|Student Demonstrations, June 4th and 5th, 1919 2]]\n\nThe [[May Fourth Movement]] helped to rekindle the then-fading cause of republican revolution. In 1917 [[Sun Yat-sen]] had become commander-in-chief of a rival military government in [[Guangzhou]] in collaboration with southern warlords. Sun's efforts to obtain aid from the Western democracies were ignored, however, and in 1920 he turned to the Soviet Union, which had recently achieved its own revolution. The Soviets sought to befriend the Chinese revolutionists by offering scathing attacks on Western imperialism. But for political expediency, the Soviet leadership initiated a dual policy of support for both Sun and the newly established [[Chinese Communist Party]] (CCP).[[File:Sino-german cooperation.png|thumb|With [[Sino-German cooperation until 1941]], Chinese industry and military was improved just prior to the war against Japan.|alt=|150x150px]]\n\nIn early 1927, the Kuomintang-CCP rivalry led to a split in the revolutionary ranks. The CCP and the left wing of the Kuomintang had decided to move the seat of the Nationalist government from Guangzhou to [[Wuhan]]. But [[Chiang Kai-shek]], whose [[Northern Expedition]] was proving successful, set his forces to destroying the Shanghai CCP apparatus and established an anti-Communist government at Nanjing in [[April 12 Incident|April 1927]].\n\nThe \"Nanjing Decade\" of 1928–37 was one of consolidation and accomplishment under the leadership of the Nationalists, with a mixed but generally positive record in the economy, social progress, development of [[democracy in China|democracy]], and cultural creativity. Some of the harsh aspects of foreign concessions and privileges in China were moderated through diplomacy.\n\n[[File:World 1920 empires colonies territory.png|center|thumb|700x700px|The world in 1920, part of the [[interwar period]]. [[United Kingdom|Great Britain]] and [[France]] expanded greatly at the expense of the former [[German Empire]]]]\nThe interwar period was the period between the end of the First World War and the beginning of the Second World War. This period was marked by turmoil in much of the world, as Europe struggled to recover from the devastation of the First World War.[[File:A flapper girl LCCN2012645724.tif|thumb|227x227px|American, Flapper Girl. In the 1920s women experienced a degree of [[First-wave feminism|liberation]]. ]] [[File:Fatty Arbuckle, Life of the Party, 1920.ogv|left|thumb|In the 1920s [[movie theater]]s became popular in American and European communities. ]]In North America, especially the first half of this period, people experienced considerable prosperity in the Roaring Twenties. The social and societal upheaval known as the Roaring Twenties began in North America and spread to Europe in the [[aftermath of World War I]]. The \"Roaring Twenties\", often called \"[[Jazz Age|The Jazz Age]]\", saw an exposition of social, artistic, and cultural dynamism. \"[[Normalcy]]\" returned to politics, [[jazz music]] blossomed, the [[flapper]] redefined modern womanhood, [[Art Deco]] peaked. The spirit of the Roaring Twenties was marked by a general feeling of discontinuity associated with modernity, a break with traditions. Everything seemed to be feasible through modern technology. New technologies, especially automobiles, movies and radio proliferated 'modernity' to a large part of the population. The 1920s saw the general favor of practicality, in architecture as well as in daily life. The 1920s was further distinguished by several inventions and discoveries, extensive industrial growth and the rise in consumer demand and aspirations, and significant changes in [[lifestyle (sociology)|lifestyle]].[[File:I Got The Ritz From The One I Love.ogg|left|thumb|\"I got the Ritz from the one I love,\" Jazz music radio broadcast 1932]]\nEurope spent these years rebuilding and coming to terms with the vast human cost of the conflict. The economy of the United States became increasingly intertwined with that of Europe. In Germany, the [[Weimar Republic]] gave way to episodes of political and economic turmoil, which culminated with the [[Hyperinflation in the Weimar Republic|German hyperinflation]] of 1923 and the failed [[Beer Hall Putsch]] of that same year. When Germany could no longer afford war payments, Wall Street invested heavily in European debts to keep the European economy afloat as a large consumer market for American mass-produced goods. By the middle of the decade, [[economic development]] soared in Europe, and the Roaring Twenties broke out in Germany, Britain and France, the second half of the decade becoming known as the \"[[Golden Twenties]]\". In France and francophone Canada, they were also called the \"années folles\" (\"Crazy Years\")., 1935. German Nazis created large public performances to earn public support. [[File:Depression, Breadlines-long line of people waiting to be fed, New York City, in the absence of substantial government... - NARA - 196506.tif|thumb|[[Great Depression]], Breadlines-long line of people waiting to be fed, [[New York City]], [[United States]] i|alt=|150x150px]]Worldwide prosperity changed dramatically with the onset of the [[Great Depression]] in 1929. The [[Wall Street Crash of 1929]] served to punctuate the end of the previous era, as \"The Great Depression\" set in. The \"Great Depression\" was a worldwide economic [[Recession|downturn]] starting in most places in 1929 and ending at different times in the 1930s or early 1940s for different countries. It was the largest and most important [[Depression (economics)|economic depression]] in the 20th century, and is used in the 21st century as an example of how far the world's economy can fall.\n\nThe depression had devastating effects in virtually every country, rich or poor. International trade plunged by half to two-thirds, as did personal income, tax revenue, prices and profits. [[Cities in the Great Depression|Cities all around the world]] were hit hard, especially those dependent on [[heavy industry]]. Construction was virtually halted in many countries. Farming and rural areas suffered as crop prices fell by roughly 60 percent. Facing plummeting demand with few alternate sources of jobs, areas dependent on [[Primary sector of economic activity|primary sector industries]] suffered the most.\nThe Great Depression ended at different times in different countries with the [[Home front during World War II|effect lasting into the next era]]. America's Great Depression ended in 1941 with America's entry into World War II. The majority of countries set up relief programs, and most underwent some sort of political upheaval, pushing them to the left or right. In some world states, the desperate citizens turned toward nationalist [[Demagogy|demagogues]]—the most infamous being [[Adolf Hitler]]—setting the stage for the next era of war. The convulsion brought on by the worldwide depression resulted in the rise of [[Nazism]]. In Asia, Japan became an ever more assertive power, especially with regards to China.\n\n[[File:Another talk with the German Chancellor, Herr Hitler.ogg|thumb|U.K Prime Minister [[Neville Chamberlain|Nevil Chamberline]] attempted to negotiate with [[Adolf Hitler]] as Nazi Germany practiced an expansionist policy .|alt=|left]]\nThe interwar period was also marked by a radical change in the international order, away from the [[balance of power in international relations|balance of power]] that had dominated pre–World War I Europe. One main institution that was meant to bring stability was the [[League of Nations]], which was created after the First World War with the intention of maintaining world security and peace and encouraging economic growth between member countries. The League was undermined by the bellicosity of [[Nazi Germany]], [[Imperial Japan]], the Soviet Union, and [[Benito Mussolini|Mussolini's]] Italy, and by the non-participation of the United States, leading many to question its effectiveness and legitimacy.[[File:Battle at Great Wall, Laiyuan, Hebei, autumn 1937.jpg|thumb|[[Kuomintang|Nationalist Chinese]] soldiers defending at the Battle at Great Wall, Laiyuan, [[Hebei]], [[China]] autumn 1937. The [[Second Sino-Japanese War|Second Sino-Japanese war]] cost at least twenty millions lives. ]]A series of international crises strained the League to its limits, the earliest being the [[Japanese invasion of Manchuria|invasion of Manchuria]] by Japan and the [[Abyssinian crisis]] of 1935/36 in which Italy invaded [[Ethiopian Empire|Abyssinia]], one of the only free African nations at that time.\n[[File:Bundesarchiv Bild 183-R69173, Münchener Abkommen, Staatschefs.jpg|left|thumb|The signatories [[Munich Agreement]] where British Prime Minister [[Neville Chamberlain|Nevil Chamberline]] [[Appeasement|appeased]] to the [[Axis powers|Axis Powers]] regarding territorial claims in [[Sudetenland]], [[Czechoslovakia]] to avoid conflict.]]\nThe League tried to enforce economic sanctions upon Italy, but to no avail. The incident highlighted French and British weakness, exemplified by their reluctance to alienate Italy and lose her as their ally. The limited actions taken by the Western powers pushed Mussolini's Italy towards alliance with Hitler's Germany anyway. The Abyssinian war showed Hitler how weak the League was and encouraged the remilitarization of the Rhineland in flagrant disregard of the Treaty of Versailles. This was the first in a series of provocative acts culminating in the [[invasion of Poland]] in September 1939 and the beginning of the Second World War.\n\nFew Chinese had any illusions about Japanese designs on China. Hungry for raw materials and pressed by a growing population, Japan initiated the seizure of [[Manchuria]] in September 1931 and established ex-Qing emperor [[Puyi]] as head of the [[puppet state]] of [[Manchukuo]] in 1932. During the [[Sino-Japanese War (1937–1945)]], the loss of Manchuria, and its vast potential for industrial development and war industries, was a blow to the Kuomintang economy. The [[League of Nations]], established at the end of World War I, was unable to act in the face of the Japanese defiance. After 1940, conflicts between the Kuomintang and Communists became more frequent in the [[Free China (Second Sino-Japanese War)|areas not under Japanese control]]. The Communists expanded their influence wherever opportunities presented themselves through mass organizations, administrative reforms, and the land- and tax-reform measures favoring the peasants—while the Kuomintang attempted to neutralize the spread of Communist influence.\n\n[[File:Shashin Shuho No 151.jpg|left|thumb|141x141px|Japanese poster advocating for Tripartite Pact, 1941.]]\n\nThe [[Second Sino-Japanese War]] had seen tensions rise between Imperial Japan and the United States; events such as the [[Panay incident]] and the [[Nanking Massacre]] turned American public opinion against Japan. With the occupation of [[French Indochina]] in the years of 1940–41, and with the continuing war in China, the United States placed embargoes on Japan of [[strategic material]]s such as scrap metal and oil, which were vitally needed for the war effort. The Japanese were faced with the option of either withdrawing from China and losing face or seizing and securing new sources of raw materials in the resource-rich, European-controlled colonies of [[South East Asia]]—specifically [[British Malaya]] and the [[Dutch East Indies]] (modern-day [[Indonesia]]). In 1940, Imperial Japan signed the [[Tripartite Pact]] with Nazi Germany and Fascist Italy.\n\n[[File:Ww2 allied axis 1942 jun.png|center|thumb|700x700px|World War II at the height of Axis expansion (black) fighting against the [[Allies of World War II|Allies]] (blue) and [[Communist International|Comintern]] (red). It is important to note that the [[Empire of Japan]] was not at war with the [[Soviet Union]] despite being part of the [[Tripartite Pact]].]]\n[[File:Bundesarchiv Bild 101I-012-0037-23A, Polen, Straßenkampf, Infanterie.jpg|left|thumb|The German [[Invasion of Poland]] in 1939 is the official start of [[World War II|WWII]]]]\n\nThe Second World War was a global military conflict that took place in 1939–1945. It was the largest and deadliest war in history, culminating in [[the Holocaust]] and ending with the dropping of the [[atom bomb]].\n\nEven though Japan had been invading in China since 1937, the conventional view is that the war began on September 1, 1939, when [[Nazi Germany]] invaded Poland, the [[Drang nach Osten]]. Within two days the United Kingdom and France declared war on Germany, even though the fighting was confined to Poland. Pursuant to a then-secret provision of its non-aggression [[Molotov–Ribbentrop Pact]], the Soviet Union joined with Germany on September 17, 1939, to conquer Poland and to divide Eastern Europe.\nThe [[Allies of World War II|Allies]] were initially made up of Poland, the United Kingdom, France, Australia, Canada, New Zealand, South Africa, as well as [[Commonwealth of Nations|British Commonwealth]] countries which were controlled directly by the UK, such as the [[British Raj|Indian Empire]]. All of these countries declared war on Germany in September 1939.\n[[File:Children in the Holocaust concentration camp liberated by Red Army.jpg|alt=Holocaust Survivors, January 1945|thumb|Survivors of the [[The Holocaust|Holocaust]] at the infamous German concentration camp of [[Auschwitz concentration camp|Auschwitz]] located in Occupied Poland. ]]\nFollowing the lull in fighting, known as the \"[[Phoney War]]\", Germany invaded western Europe in May 1940. Six weeks later, France, in the mean time attacked by Italy as well, surrendered to Germany, which then tried unsuccessfully to conquer Britain. On September 27, Germany, Italy, and Japan signed a mutual defense agreement, the [[Tripartite Pact]], and were known as the [[Axis Powers]].Nine months later, on June 22, 1941, Germany launched a massive invasion of the Soviet Union, which promptly joined the Allies. Germany was now engaged in fighting a war on two fronts. This proved to be a mistake by Germany – Germany had not successfully carried out the invasion of Britain and the war turned against the Axis.\n\nOn December 7, 1941, [[Attack on Pearl Harbor|Japan attacked the United States at Pearl Harbor]], bringing it too into the war on the Allied side. China also joined the Allies, as eventually did most of the rest of the world. China was in turmoil at the time, and attacked Japanese armies through guerilla-type warfare. By the beginning of 1942, the major combatants were aligned as follows: the British Commonwealth, the United States, and the Soviet Union were fighting Germany and Italy; and the British Commonwealth, China, and the United States were fighting Japan. The United Kingdom, the United States, the Soviet Union and China were referred as a \"trusteeship of the powerful\" during the [[World War II]] and were recognized as the Allied \"Big Four\" in [[Declaration by United Nations]] These four countries were considered as the \"[[Four Policemen]]\" or \"Four Sheriffs\" of the [[Allies of World War II|Allies power]] and primary victors of World War II. From then through August 1945, battles raged across all of Europe, in the [[North Atlantic Ocean]], across North Africa, throughout [[Southeast Asia]], throughout China, across the Pacific Ocean and in the air over Japan.\nItaly surrendered in September 1943 and was split into a northern Germany-occupied [[puppet state]] and an Allies-friendly state in the South; Germany surrendered in May 1945. Following the [[atomic bombings of Hiroshima and Nagasaki]], [[surrender of Japan|Japan surrendered]], marking the end of the war on September 2, 1945.\n[[File:Underwater Atomic Bomb Test At Bikini Atoll.webm|alt=Atomic Bomb underwater testing, 1946|left|thumb|Atomic Bomb Test, 1946. The use of Nuclear strikes on Hiroshima and Nagasaki brought WWII to an abrupt end.]]\n[[File:Roosevelt Pearl Harbor.ogg|thumb|U.S President [[Franklin D. Roosevelt|Franklin Roosevelt]] declaring war on the [[Empire of Japan|Japanese Empire]] in the aftermath of the [[Attack on Pearl Harbor|Pearl Harbor Attack]]. Captions provided]]\n[[File:Trumann hiroshima.ogg|thumb|Excerpt of [[President of the United States|U.S President]] [[Harry S. Truman|Harry Truman']]s speech regarding the [[Atomic bombings of Hiroshima and Nagasaki|nuclear attack]] on [[Hiroshima|Hiroshima, Japan]].Captions provided]]\nIt is possible that around 62 million people [[World War II casualties|died in the war]]; estimates vary greatly. About 60% of all casualties were civilians, who died as a result of disease, starvation, [[genocide]] (in particular, the [[Holocaust]]), and aerial bombing. The former Soviet Union and China suffered the most casualties. Estimates place deaths in the Soviet Union at around 23 million, while China suffered about 10 million. No country lost a greater portion of its population than Poland: approximately 5.6 million, or 16%, of its pre-war population of 34.8 million died.\nThe Holocaust (which roughly means \"burnt whole\") was the deliberate and systematic murder of millions of Jews and other \"unwanted\" during World War II by the Nazi regime in Germany. Several differing views exist regarding whether it was intended to occur from the war's beginning, or if the plans for it came about later. Regardless, persecution of Jews extended well before the war even started, such as in the \"[[Kristallnacht]]\" (Night of Broken Glass). The Nazis used propaganda to great effect to stir up anti-Semitic feelings within ordinary Germans.\n\nAfter World War II, Europe was informally split into Western and Soviet [[Sphere of influence|spheres of influence]]. [[Western Europe]] later aligned as the [[NATO|North Atlantic Treaty Organization]] (NATO) and [[Eastern Europe]] as the [[Warsaw Pact]]. There was a shift in power from Western Europe and the [[British Empire]] to the two new superpowers, the United States and the Soviet Union. These two rivals would later face off in the [[Cold War]]. In Asia, the defeat of Japan led to its [[democratization]]. [[Chinese Civil War|China's civil war]] continued through and after the war, resulting eventually in the establishment of the [[People's Republic of China]]. The former colonies of the European powers began their road to independence.\n\n[[File:The Beatles i Hötorgscity 1963.jpg|thumb|During [[Pax Americana]], the [[Popular music|popular music industry]] flourished.Pictured here is the [[The Beatles|Beatles]] Group. |alt=|200x200px]]\n\nThe mid-20th century is distinguished from most of human history in that its most significant changes were directly or indirectly economic and technological in nature. Economic development was the force behind vast changes in everyday life, to a degree which was unprecedented in human history.\n\nOver the course of the 20th century, the world's per-capita [[gross domestic product]] grew by a factor of five, much more than all earlier centuries combined (including the 19th with its Industrial Revolution). Many economists make the case that this understates the magnitude of growth, as many of the goods and services consumed at the end of the 20th century, such as improved medicine (causing world life expectancy to increase by more than two decades) and communications technologies, were not available at any price at its beginning. However, the gulf between the world's rich and poor grew wider, and the majority of the global population remained in the poor side of the divide.\n\nStill, advancing technology and medicine has had a great impact even in the [[Global South]]. Large-scale industry and more centralized [[Mass media|media]] made brutal dictatorships possible on an unprecedented scale in the middle of the century, leading to wars that were also unprecedented. However, the increased communications contributed to [[democratization]]. Technological developments included the development of airplanes and [[space exploration]], [[nuclear technology]], advancement in [[genetics]], and the dawning of the [[Information Age]].\n\nPax Americana is an appellation applied to the historical concept of relative liberal peace in the Western world, resulting from the preponderance of power enjoyed by the United States of America starting around the start of the 20th century. Although the term finds its primary utility in the latter half of the 20th century, it has been used in various places and eras. Its modern connotations concern the peace established after the end of World War II in 1945.\n\nThe Cold War began in the mid-1940s and lasted into the early 1990s. Throughout this period, the conflict was expressed through military coalitions, espionage, weapons development, invasions, propaganda, and competitive technological development. The conflict included costly defense spending, a massive [[Conventional weapon|conventional]] and [[Nuclear weapon|nuclear]] [[arms race]], and numerous [[proxy war]]s; the two [[superpower]]s never fought one another directly.\n\n[[File:NATO vs. Warsaw (1949-1990).svg|upright=3.4|thumb|center|]]\nThe Soviet Union created the [[Eastern Bloc]] of countries that it occupied, annexing some as [[Soviet Socialist Republics]] and maintaining others as satellite states that would later form the [[Warsaw Pact]]. The United States and various western European countries began a policy of \"[[containment]]\" of [[communism]] and forged myriad alliances to this end, including [[NATO]]. Several of these western countries also coordinated efforts regarding the rebuilding of western Europe, including western Germany, which the Soviets opposed. In other regions of the world, such as Latin America and [[Southeast Asia]], the Soviet Union fostered [[communist revolution]]ary movements, which the United States and many of its allies opposed and, in some cases, attempted to \"[[rollback|roll back]]\". Many countries were prompted to align themselves with the nations that would later form either NATO or the Warsaw Pact, though other movements would also emerge.\n[[File:US and USSR nuclear stockpiles.svg|left|thumb|US and USSR nuclear stockpiles]]\nIn China [[Mao Zedong]] (\"Máo zé dōng\") utilized [[Marxism-Leninism|Marxist-Leninist]] thought. When the [[Communist Party of China]] assumed power in 1959, previous schools of Chinese thought, excepting notably [[Legalism (Chinese philosophy)|Legalism]], were denounced as backward. Many parts of China's past were even purged during the [[Cultural Revolution]]. Though initially friendly with the [[Soviet Union]] the Chinese and Soviet communists diverged in the [[Sino-Soviet split]] of 1960. Through the end of the Cold-War, China would continue on its own path from other communist countries, building better relations with the United States after 1972. China's economy would recover from the Cultural Revolution due to [[Chinese economic reform|market-oriented reforms]] led by [[Deng Xiaoping]].\n\nThe Cold War saw periods of both heightened tension and relative calm. International crises arose, such as the [[Berlin Blockade]] (1948–1949), the [[Korean War]] (1950–1953), the [[Berlin Crisis of 1961]], the [[Vietnam War]] (1955–1975), the [[Cuban Missile Crisis]] (1962), the [[Soviet–Afghan War]] (1979–1989) and [[Able Archer 83|NATO exercises in November 1983]]. There were also periods of reduced tension as both sides sought [[détente]]. Direct military attacks on adversaries were deterred by the potential for [[mutual assured destruction]] using deliverable [[nuclear weapon]]s.\n\nThe Cold War drew to a close in the late 1980s and the early 1990s. The United States under President [[Ronald Reagan]] increased diplomatic, military, and economic pressure on the Soviet Union, which was already suffering from [[Brezhnev stagnation|severe economic stagnation]]. In the second half of the 1980s, newly appointed Soviet leader [[Mikhail Gorbachev]] introduced the \"[[perestroika]]\" and \"[[glasnost]]\" reforms. [[History of the Soviet Union (1985–1991)|The Soviet Union collapsed]] in 1991, leaving the United States as the dominant military power, though Russia retained much of the massive Soviet nuclear arsenal.\n\nIn Latin America in the 1970s, leftists acquired a significant political influence which prompted the right-wing, ecclesiastical authorities and a large portion of the individual country's upper class to support coups d'état to avoid what they perceived as a communist threat. This was further fueled by Cuban and United States intervention which led to a political polarization. Most South American countries were in some periods ruled by [[military dictatorship]]s that were supported by the United States of America. In the 1970s, the regimes of the [[Southern Cone]] collaborated in [[Operation Condor]] killing many [[leftist]] dissidents, including some [[urban guerrilla]]s. However, by the early 1990s all countries had restored their democracies.\n\n[[File:Neil Armstrong small step.wav|left|thumb|Neil Armstrong small step]]\n[[File:The Earth seen from Apollo 17.jpg|left|thumb|\"[[The Blue Marble]]\", a photograph of Earth as seen from Apollo 17. The second half of the 20th century saw an increase of interest in both [[space exploration]] and the [[environmental movement]].]]\nThe [[Space Age]] is a period encompassing the activities related to the [[Space Race]], [[space exploration]], space technology, and the cultural developments influenced by these events. The Space Age began with the development of several technologies that culminated with the launch of [[Sputnik 1]] by the Soviet Union in October 1957. This was the world's first artificial satellite, orbiting the Earth in 98.1 minutes and weighing in at 83 kg. The launch of Sputnik 1 ushered a new era of political, scientific and technological achievements that became known as the Space Age. The Space Age was characterized by rapid development of new technology in a close race mostly between the United States and the Soviet Union. The Space Age brought the first human spaceflight during the [[Vostok programme]] and reached its peak with the [[Apollo program]] which captured the imagination of much of the world's population. The landing of [[Apollo 11]] was an event watched by over 500 million people around the world and is widely recognized as one of the defining moments of the 20th century. Since then and with the end of the space race due to the [[dissolution of the Soviet Union]], public attention has largely moved to other areas.\n\nThe [[humanities]] are academic disciplines which study the [[human condition]], using methods that are primarily [[Analytic induction|analytic]], critical, or speculative, as distinguished from the mainly [[empirical]] approaches of the [[natural science|natural]] and [[social sciences]]. Although many of the subjects of modern history coincide with that of standard history, the subject is taught independently by various systems of education in the world.\n\nStudents can choose the subject at university. The material covered includes from the mid-18th century, to analysis of the present day. Virtually all colleges and sixth forms that do teach modern history do it alongside standard history; very few teach the subject exclusively.\n\nAt the [[University of Oxford]] 'Modern History' has a somewhat different meaning. The contrast is not with the Middle Ages but with Antiquity. The earliest period that can be studied in the [[Final Honour School]] of Modern History begins in 285.\n\n\n\"Books\"\n\n\"Websites\"\n\n\n\n\n\n[[Category:Modern history| ]]\n[[Category:Historical eras]]\n[[Category:Historiography]]\n[[Category:Postmodern theory]]\n[[Category:Articles which contain graphical timelines]]\n[[Category:World history]]"}
{"id": "23527766", "url": "https://en.wikipedia.org/wiki?curid=23527766", "title": "Ohio State University Health Sciences Center for Global Health", "text": "Ohio State University Health Sciences Center for Global Health\n\nThe Health Sciences Center for Global Health (HSCGH) at The Ohio State University (OSU) is a collaborative program among the OSU Colleges of Dentistry, Medicine, Nursing, Optometry, Pharmacy, Public Health, School of Health and Rehabilitation Sciences and Veterinary Medicine. The HSCGH is led jointly by the Colleges of Medicine (COM) and Public Health (CPH).\n\nThe HSCGH was created to increase student interest in global careers, prepare students for those careers and to promote, develop and coordinate interdisciplinary global health education and research throughout the health sciences colleges and the larger community. The Board of Trustees approved the creation of the HSCGH in July 2007.\n\nThe National Institutes of Health (NIH) Fogarty Framework Grant was awarded to OSU in September 2008. The NIH John E. Fogarty International Center grant supports the creation of new, multidisciplinary educational programs as well as an administrative infrastructure to support activities.\n\n\nThe Graduate Interdisciplinary Specialization in Global Health (GISGH) is a university-wide program that offers current OSU graduate and professional students advanced educational opportunities in the field of global health. The goal of the GISGH is to help prepare graduates to be active participants in the advancement of global health through academic enrichment, service-learning, and research pertaining to issues of global health. The specialization's core course, Introduction to Global Health, focuses on the basic components of population health, while the electives allow students to pursue topics across the other health sciences colleges for a truly interdisciplinary experience.\n\nDiane L. Gorgas, MD, a professor of Emergency Medicine at The Ohio State University’s Wexner Medical Center. She currently serves as the Executive Director of the Office of Global Health. and of OSU's Health Sciences Center for Global Health. She is nationally involved as an item writer and case developer and case administrator for the American Board of Emergency Medicine (ABEM) and sits on the Emergency Medicine Review Committee for the Accreditation Council for Graduate Medical Education (ACGME). She was the Residency Training Program Director in Emergency Medicine for many years, and has a long standing interest in and study of educational methods. Her other associated research interests include global health, emotional intelligence, competency assessment and learning styles. Dr. Gorgas currently directs the OSU Greif Neonatal Survival program, which works to improve the lives of mothers and infants in low-income countries through self-sustaining education and training programs to increase the in-country capacity of healthcare workers.\n\nPamela Potter serves as the administrative director for the Health Sciences Center for Global Health at The Ohio State University and in that role provides oversight of the daily administration and operation of the center. She is also the associate director of the Office of Global Health in the College of Medicine. Prior to her role at OSU, she served as the Chief of Staff to the Executive Dean for the Georgetown University School of Medicine.\n\n"}
{"id": "2176164", "url": "https://en.wikipedia.org/wiki?curid=2176164", "title": "Our World (TV special)", "text": "Our World (TV special)\n\nOur World was the first live, international, satellite television production, which was broadcast on 25 June 1967. Creative artists, including the Beatles, opera singer Maria Callas, and painter Pablo Picasso – representing nineteen nations – were invited to perform or appear in separate segments featuring their respective countries. The two-and-a-half-hour event had the largest television audience ever up to that date: an estimated 400 to 700 million people around the globe watched the broadcast. Today, it is most famous for the segment from the United Kingdom starring the Beatles. They performed their song \"All You Need Is Love\" for the first time to close the broadcast.\n\nThe project was conceived by BBC producer Aubrey Singer. It was transferred to the European Broadcasting Union, but the master control room for the broadcast was still at the BBC in London. The satellites used were Intelsat I (known as \"Early Bird\"), Intelsat 2-2 (\"Lani Bird\"), Intelsat 2–3 (\"Canary Bird\"), and NASA's ATS-1.\n\nIt took ten months to bring everything together. One hitch was the sudden pull-out of the Eastern Bloc countries headed by the Soviet Union in the week leading up to the broadcast. Apparently it was a protest at the Western nations' response to the Six-Day War.\n\nThe ground rules included that no politicians or heads of state could participate in the broadcast. In addition, everything had to be \"live\", so no use of videotape or film was permitted. Ten thousand technicians, producers and interpreters took part in the broadcast. Each country had its own announcers, due to language issues, and interpreters voiced over the original sound when not in a country's native language. Fourteen countries participated in the production, which was transmitted to 24 countries, with an estimated audience of between 400 and 700 million people.\n\n, , , the and withdrew before the broadcast, in protest for the Six-Day War.\n\nThe opening credits were accompanied by the Our World theme sung in 22 different languages by the Vienna Boys' Choir.\n\nCanada's CBC Television had Marshall McLuhan being interviewed in a Toronto television control room. At 7:17 pm GMT, the show switched to the United States' segment about the Glassboro, New Jersey conference between American president Lyndon Johnson and Soviet premier Alexei Kosygin; since \"Our World\" insisted that no politicians be shown, only the house where the conference was being held was televised. National Educational Television's (NET) Dick McCutcheon ended up talking about the impact of the new television technology on a global scale.\n\nThe show switched back to Canada at 7:18 pm GMT. Segments that were beamed worldwide were from a Ghost Lake, Alberta ranch, showing a rancher, and his cutting horse, cutting out a herd of cattle. The last Canadian segment was from Kitsilano Beach, located in Vancouver's Point Grey district at 7:19 pm GMT.\n\nAt 7:20 pm GMT, the program shifted continents to Asia, with Tokyo, Japan being the next segment. It was 4:20 a.m. local time and NHK showed the construction of the Tokyo subway system.\n\nThe equator was crossed for the first time in the program when it switched to the Australian contributions, which was at 5:22 a.m. Australian Eastern Standard Time (AEST). This was the most technically complicated point in the broadcast, as both the Japanese and Australian satellite ground stations had to reverse their actions: Tokyo had to go from transmit mode to receive mode, while Melbourne had to switch from receive to transmit mode. The first segment dealt with trams leaving the South Melbourne tram depot with Australian Broadcasting Commission's Brian King explaining that sunrise was many hours away as it was winter there. Two scientific segments, later on in the broadcast, were also included; one, presented from Canberra by the ABC's Eric Hunter showed experiments being carried out by the Commonwealth Scientific and Industrial Research Organisation (CSIRO) to extend the frequency of cereal crop cycles, while the second dealt with the Parkes Observatory tracking a deep space object.\n\nThe broadcast took place at the height of the Vietnam War. The Beatles were asked to write a song with a positive message. At 8:54 GMT the Beatles topped the event with their debut performance of \"All You Need Is Love\". The Beatles invited many of their friends to the event to create a festive atmosphere and to join in on the song's chorus. Among the friends were members of the Rolling Stones, Eric Clapton, Marianne Faithfull, Keith Moon and Graham Nash. \n\nAlthough the program was originally recorded and transmitted in black-and-white, for its use in the 1995 TV special \"The Beatles Anthology\", the Beatles' performance on the 1967 program was colourised, using colour photographs taken at the event as a reference. The sequence opens in its original monochromatic format and rapidly morphs into full colour, conveying the brightly coloured flower power and psychedelic-style clothing worn by the Beatles and their guests that was popular during what was subsequently dubbed the \"Summer of Love\".\n\nIn the novel \"The Light of Other Days\" by Arthur C. Clarke and Stephen Baxter, the global media empire run by Hiram Patterson is called OurWorld, the name chosen after the character saw the program as a child and was inspired to change the world.\n"}
{"id": "23450639", "url": "https://en.wikipedia.org/wiki?curid=23450639", "title": "Outline of the Post-War New World Map", "text": "Outline of the Post-War New World Map\n\nThe Outline of the Post-War New World Map was a map completed before the attack on Pearl Harbor and self-published on February 25, 1942 by Maurice Gomberg of Philadelphia, United States. It shows a proposed political division of the world after World War II in the event of an Allied victory in which the United States of America, the United Kingdom, and the Soviet Union as well as the Republic of China would rule. The map includes a manifesto describing a \"New World Moral Order\", along with quotes from Roosevelt's Four Freedoms speech.\n\nGomberg created the map as a personal project, and little else is known of him. The map has been highlighted by New World Order conspiracy theorists who believe it represents some broader view of the US government, and has also been widely circulated online.\n\nThe map proposes a total of 14 independent sovereign states, 4 of them democracies and 10 of them demilitarized, and 3 \"quarantined\" states (the fate of 2 are to be eventually integrated into sovereign states).\n\nThe United States has 82 states, not including Security Outposts in the Pacific and the Atlantic, gaining all of Canada, Mexico, and Central America, among other places:\n\nStates:\nAlabama - Alberta - Alaska - Arizona - Arkansas - The Bahamas - California (historical Alta (Upper) California) - Colorado - Columbia - Connecticut - Costa Rica - Cuba - Delaware - Florida - Georgia - Greenland - Guatemala (Guatemala and Belize)\n- Haiti (including all of Hispaniola) - Honduras - Idaho - Illinois - Indiana - Iowa - Jamaica - Kansas - Keewatin (pre-1999 Northwest Territories east of the 110° meridian) - Kentucky - Labrador (mainland Newfoundland and Labrador) - Leeward Islands - Louisiana - Lower California (consisting of the Baja California peninsula) - Maine - Mackenzie (pre-1999 Northwest Territories west of the 110° meridian) - Manitoba - Maryland - Martinique - Massachusetts - Mexico (the remainder of Mexico) - Michigan - Minnesota - Mississippi - Missouri - Montana - Nevada - New Brunswick - Newfoundland (Island Newfoundland and Labrador) - New Hampshire - New Jersey - New Mexico - New York - Nicaragua - North Carolina - North Dakota - Nova Scotia - Ohio - Oklahoma - Ontario - Oregon - Panama - Pennsylvania - Prince Edward Island - Puerto Rico - Quebec - Rhode Island - El Salvador - Saskatchewan - South Carolina - South Dakota - Tennessee - Texas - Trinidad - Utah - Vermont - Virginia - Virgin Islands - Washington - West Virginia - Windward Islands - Wisconsin - Wyoming - Yukon\n\nProtectorates:\n- Celebes - Hainan - Halmahera Islands\n- Iceland - Moluccas Islands - Commonwealth of the Philippines - Taiwan\n\nPort \"Peace-security bases\":\nDakar and Freetown on the Atlantic coast of Africa\n\nThe British Commonwealth of Nations is headquartered in the United Kingdom, including England (including Wales) and Scotland, but not Northern Ireland. The Commonwealth includes the Faroe Islands and the former colonies of Madagascar (in early 1942 still a Vichy French colony), Ceylon, the Andaman Islands, Cyprus, Malta, most of Indonesia (in 1942 a Dutch colony occupied by Japan; other parts are given to the US), as well as the then British colonies that are now Singapore and Malaysian Borneo, South Georgia, the Bismarck Archipelago, the Solomon Islands, and the countries of Australia and New Zealand.\n\nPort \"Peace-security bases\":\n\nThe Soviet Union would expand to be far larger than its then-current size, expanding to 24 (later 25) Soviet Socialist Republics (while downgrading preexisting SSR's to Autonomous Soviet Socialist Republics):\n\nSoviet Socialist Republics:\nArmenia - Azerbaijan - Bulgaria - Czech Republic - Estonia - Finland - Georgia - Hungary - Iran - Latvia - Lithuania - Manchuria (Northern Manchuria) - Moldavia (all of Bessarabia) - Mongolia - Poland - Romania - Russia (which includes Kazakhstan and Kirghizia (Kyrgyzstan) - Slovakia - Tajikistan - Turkmenistan - Ukraine - Uzbekistan - White Russia (Byelorussia) - Yugoslavia\n<br>\nAt an unspecified later time, Germany (which includes Austria and former Polish territory formerly part of the Weimar Republic sans East Prussia). Germany is termed \"quarantined Germany\" until full integration\n\nEverything below the Darién Gap, and offshore islands including the Falkland Islands:\n\n- Argentina - Bolivia - Brazil - Chile - Colombia - Ecuador - Guiana - Paraguay - Peru - Uruguay - Venezuela\n\nChina (without Formosa or Hainan) - Inner Mongolia - Indo China (Cambodia, Laos, and Vietnam) - Korea - Malaya - Sinkiang - Thailand - Tibet\n\nBelgium (including Luxembourg) - France (including Monaco and all of Germany west of the Rhine river) - Netherlands - Portugal - San Marino (later included in Italy) - Spain (including Andorra) - Switzerland (including Liechtenstein) - Vatican City (later included in Italy)\n<br>\nAt an unspecified later time, Italy (which is termed \"quarantined Italy\" until full integration)\n\nDenmark (without the Faroe Islands and Greenland) - Norway (including Spitsbergen, without Jan Mayen) - Sweden\n\nBesides the port \"peace-security bases\", areas included are Algeria - Angola - Bechuanaland (Botswana) - Congo (including Burundi) - Dahomey (Benin, including Togo) - Egypt - Equatorial Africa (including Equatorial Guinea, eastern Guinea, and São Tomé and Príncipe) - Eritrea - Ethiopia (including Djibouti) - Gold Coast (Ghana) - Kenya - Liberia - Libya - Morocco - Mozambique (including southern Malawi) - Nigeria - Rhodesia (northern Malawi, Zambia, and Zimbabwe) - Senegal (including the Gambia, western Guinea, and Guinea-Bissau) - Somaliland (Somalia) - South Africa (including Lesotho and Swaziland) - South West Africa (Namibia) - Sudan - Tanganyika (including Rwanda) - Tunisia - Uganda - West Africa (including Sahrawi Arab Democratic Republic/Western Sahara and Sierra Leone, without Benin, Senegal, and Togo)\n\nAden (South Yemen) - Hejaz - Iraq (including Kuwait) - Lebanon - Oman (including Bahrain, Qatar, and United Arab Emirates) - Saudi Arabia - Syria - Yemen (North Yemen)\n\nAfghanistan - Balochistan - Bhutan - Burma - India (including Pakistan without Balochistan) - Nepal\n\nAlbania - Greece\n<br>\nPoint #24 notes the inclusion of Macedonia. that \"Macedonia\" is not clearly defined; the map seems to not include the modern country of that name, but rather labels Macedonia as Greek Macedonia.\n\nConsists of the whole of the island of Ireland\n\nIncludes all of modern Israel, Jordan and Palestine, taking in parts of modern Syria and a slice of northern Saudi Arabia.\n\nAll of Asian Turkey. European Turkey is placed under joint control of the USSR and Turkey - cf. points #27 and #28.\n\nMentioned as \"quarantined Germany\", all of Weimar Republic territory east of the Rhine river but west of the former Polish Corridor, plus Austria, eventually supposed to become a full Soviet Socialist Republic in the Soviet Union\nMentioned as \"quarantined Italy\", all of modern Italy and the Julian March (pre 1941), eventually supposed to become a full state in the United States of Europe\nMentioned as \"quarantined Japan\", all of modern Japan and Iturup, Kunashir, Shikotan, Habomai (but not including Bonin Islands). Later fate presumed to be an independent democracy\n\n\n"}
{"id": "44041240", "url": "https://en.wikipedia.org/wiki?curid=44041240", "title": "Pacification theory", "text": "Pacification theory\n\nPacification theory is a counter-hegemonic approach to the study of police and security which views the contemporary security-industrial complex as both an organizing and systematic war strategy targeting domestic and foreign enemies while simultaneously acting as a process that actively fabricates a social order conducive to capitalist accumulation. According to its academic proponents, such an approach to police and security reveals inherent class war dimensions that have been reinforced by police intellectuals since at least the eighteenth century.\n\nAt base, pacification reflects the need to fabricate productive territories and subjects conducive to exploitation. As Neocleous, Rigakos and Wall explain: \"The extraction of surplus, as Adam Smith admits, can ‘be squeezed out of [the labourer] by violence only, and not by any interest of his own’ if he can subsist otherwise such as through access to communal land. This, in short, is the foundational bourgeois logic for the compulsion to pacify.\"\n\nPacification theory may vary in its use depending on the analyst, but most scholars associated with Anti-security would likely agree that its central tenets encompass:\nAssociated with this last point and serving an essential component of pacification is its immediate connection to making subjects economically \"productive\" both historically within the plans of military and colonial overseers and by contemporary police actions, both domestic and international. Neocleous has characterized this process as making war through peace:\n\nA final element of pacification invoked by scholars in this field of study is its connection to the apparent primacy of security thinking and planning in a capitalist economy. This pronouncement is often linked to Karl Marx's assertion that \"security is the supreme concept of bourgeois society\" in the Jewish Question. A connection believed to be so embedded that Rigakos has argued that \"security \"is\" hegemony\".\n\nThe development of pacification theory is a re-appropriation of the historical usage of the term. It is offered as an alternative to security as part of a broader analytic Anti-security project. The development of pacification theory to re-cast security is believed to help radical scholars grasp the inherent objectives and operation of security politics since the Enlightenment, and is intended to give activists a ground to stand against the securitization of political discourse that increasingly surrounds the policing of dissent in the post-9/11 period.\n\nDuring the social uprisings in the 1960s in North America and Europe against the Vietnam War, pacification came to connote bombing people into submission and waging an ideological war against the opposition. However, after the Vietnam War, pacification was dropped from the official discourse as well as from the discourse of opposition. Although approach towards the term and practices of pacification both in the concept’s sixteenth-century and twentieth-century colonial meanings were somehow related to the concepts of war, security and police power, the real connection between pacification and these concepts has never been revealed in the literature on international relations, conflict studies, criminology or political science. Neocleous has argued that the connection between pacification and the ideological discourse on security is related to the terms use in broader Western social and political thought in general, and liberal theory in particular. In short, that liberalism’s key concept is less liberty and more security and that liberal doctrine is inherently less committed to peace and far more to legitimizing violence.\n\nIn Anti-security: A Declaration, Neocleous and Rigakos provocatively summed this argument in the following way: \"In the works of the founders of the liberal tradition - that is, the founders of bourgeois ideology - liberty is security and security is liberty. For the ruling class, security always has and always will triumph over liberty because ‘liberty’ has never been intended as a counter-weight to security. Liberty has always been security’s lawyer.\"\n\nFrom the late seventeenth and early eighteenth century onwards, the growth of towns in Europe generated a concern over “masterlesse men,” as Thomas Hobbes puts it, and their forms of behaviour exposed in urban life such as gambling, drinking, adultery, blasphemy and wandering. Pacification, then, functions as a thread that connects sixteenth-century European colonialism and the fabrication of liberal social order in eighteenth and nineteenth centuries to the US project in Vietnam and contemporary military exercises of Empire both throughout the globe and domestically.\n\nIn Security/Capital, Rigakos offers a General Theory of Pacification. He argues that pacification is composed of three overlapping strata: (1) Dispossession; (2) Exploitation and (3) Commodification. Commodification is itself composed of three processes: (a) valorization; (b) prudentialization; and (c) fetishization. According to Rigakos, while different in their strategic targets of intervention, each of these three strata of pacification in their aggregate nonetheless both produce and rely on:\n\nThe aggregate effect of this theory is the conclusion that the global economic system is now conditioned by pacification as it never has been before. Rigakos suggests that \"the security–\nindustrial complex is, materially and ideologically, the blast furnace of global capitalism, fuelling both the conditions for the system’s perpetuation while feeding relentlessly on the surpluses it has exacted.\"\n\n"}
{"id": "38530392", "url": "https://en.wikipedia.org/wiki?curid=38530392", "title": "Per Fugelli", "text": "Per Fugelli\n\nPer Fugelli (7 December 1943 – 13 September 2017) was a Norwegian physician and professor of General Practice at the University of Bergen from 1984 to 1992, and social medicine at the University of Oslo from 1992 until his death in 2017.\n\nFugelli was born in Stavanger, Norway, on December 7, 1943. He studied medicine at University of Oslo.\n\nFrom 1971–73 Fugelli was a general practitioner in Værøy and Røst, and from 1977 to 1980 in Porsanger. During this time he earned his PhD and graduated in 1978. In 1984, he became a Professor of General Practice at the University of Bergen, where he stayed until 1992. He became a Professor of social medicine at University of Oslo´s Institute of Health and Society. In 2013, he became Emeritus.\n\nIn 1993 Fugelli wrote: \"The patient Earth is sick. Global environmental disruptions can have serious consequences for human health. It's time for doctors to give a world diagnosis and advise on treatment,\" predating the founding of planetary health. He is the subject of the documentary \"I die\" by filmmaker Erik Poppe.\n\nHe was a frequent contributor to the public debate on health and medical questions. Among his early books are \"Tilbake til huslegen\" from 1975, \"Doktor på Værøy og Røst\" from 1977, and \"Helsetilstand og helsetjeneste på Værøy og Røst\" from 1978.\n\nHe published the essay collections \"Med sordin og kanon\" and \"Helse og rettferdighet\" in 1990, \"0-visjonen\" in 2003, and \"Nokpunktet\" in 2008. He has been editor or co-editor of several works, including \"Huslegen\" from 1985, \"Medisinsk leksikon\" from 1990, \"Medisin og helse\" from 1993, and \"Verdier og penger i helsetjenesten\" from 2009.\n\nFugelli was married, had two children, and three grandchildren by the time he died.\n\nIn 2009, he was diagnosed with colorectal cancer. It metastasized into his lungs and by 2012, the cancer was declared terminal. Nevertheless, Fugelli continued to write and work as long as he was able, with his final published article written six weeks before his death. He died at Jæren on 13 September 2017, aged 73.\n\nFugelli won the 2010 Karl Evang Prize and in 2013, the Freedom of Expression Foundation Prize. \n"}
{"id": "28072640", "url": "https://en.wikipedia.org/wiki?curid=28072640", "title": "Political globalization", "text": "Political globalization\n\nPolitical globalization refers to the growth of the worldwide political system, both in size and complexity. That system includes national governments, their governmental and intergovernmental organizations as well as government-independent elements of global civil society such as international non-governmental organizations and social movement organizations. One of the key aspects of the political globalization is the declining importance of the nation-state and the rise of other actors on the political scene. The creation and existence of the United Nations has been called one of the classic examples of political globalization.\n\nPolitical globalization is one of the three main dimensions of globalization commonly found in academic literature, with the two other being economic globalization and cultural globalization.\n\nWilliam R. Thompson has defined it as \"the expansion of a global political system, and its institutions, in which inter-regional transactions (including, but certainly not limited to trade) are managed\". Valentine M. Moghadam defined it as \"an increasing trend toward multilateralism (in which the United Nations plays a key role), toward an emerging 'transnational state apparatus,' and toward the emergence of national and international nongovernmental organizations that act as watchdogs over governments and have increased their activities and influence\". Manfred B. Steger in turn wrote that it \"refers to the intensification and expansion of political interrelations across the globe\". The longer definition by Colin Crouch goes as follows: \"Political globalization refers to the growing power of institutions of global governance such as the World Bank, the International Monetary Fund (IMF) and the World Trade Organization (WTO). But it also refers to the spread and influence of international non-governmental organizations, social movement organizations and transnational advocacy networks operating across borders and constituting a kind of global civil society.\" Finally, Gerard Delanty and Chris Rumford define it as \"a tension between three processes which interact to produce the complex field of global politics: global geopolitics, global normative culture and polycentric networks.\"\n\nSalvatore Babones discussing sources used by scholars for studying political globalizations noted the usefulness of Europa World Year Book for data on diplomatic relationships between countries, publications of International Institute for Strategic Studies such as \"The Military Balance\" for matters of military, and US government publication \"Patterns of Global Terrorism\" for matters of terrorism.\n\nPolitical globalization is measured by aggregating and weighting data on the number of embassies and high commissioners in a country, the number of the country's membership in international organization, its participation in the UN peacekeeping missions, and the number of international treaties signed by said country. This measure has been used by Axel Dreher, Noel Gaston, Pim Martens Jeffrey Haynes and is available from the KOF institute at ETH Zurich\n\nLike globalization itself, political globalization has several dimensions and lends itself to a number of interpretations. It has been discussed in the context of new emancipatory possibilities, as well as in the context of loss of autonomy and fragmentation of the social world. Political globalization can be seen in changes such as democratization of the world, creation of the global civil society, and moving beyond the centrality of the nation-state, particularly as the sole actor in the field of politics. Some of the questions central to the discussion of the political globalization are related to the future of the nation-state, whether its importance is diminishing and what are the causes for those changes; and understanding the emergence of the concept of global governance. The creation and existence of the United Nations has been called one of the classic examples of political globalization. Political actions by non-governmental organizations and social movements, concerned about various topics such as environmental protection, is another example.\n\nDavid Held has proposed that continuing political globalization may lead to the creation of a world government-like cosmopolitan democracy, though this vision has also been criticized as too idealistic.\n\nThere is a heated debate over Political Globalization and Nation State. The question arises whether or not political globalization signifies the decline of the nation-state. Hyper globalists argue that globalization has engulfed today's world in such a way that state boundaries are beginning to lose significance. However, skeptics disregard this as naiveté, believing that the nation-state remains the supreme actor in international relations. \n\n\n\n"}
{"id": "6800650", "url": "https://en.wikipedia.org/wiki?curid=6800650", "title": "SR International – Radio Sweden", "text": "SR International – Radio Sweden\n\nRadio Sweden () is Sweden's official international broadcasting station. It is a non-commercial and politically independent public service broadcasting company.\n\nSR International is part of Sveriges Radio (SR), Sweden's non-commercial public-radio broadcasting organization. The service was founded in 1938, at the approach of World War II, as a way of keeping Swedes living abroad informed of happenings in Sweden and of Swedish opinion. Programming was at first in Swedish only, but in 1939 English- and German-language broadcasts were added.\n\nAfter the war, further language services were added: in French, Portuguese, Russian, and Spanish. At the close of the Cold War, the services in French, Portuguese, and Spanish were gradually phased out and replaced by new services in Estonian and Latvian. The latter services were withdrawn once Estonia and Latvia had developed their own independent media and joined the European Union.\n\nRadio Sweden also operated a service in Belarusian between 2004 and 2009.\n\nIn the 1990s Radio Sweden was merged with SR's Immigrant Languages Department to form the SR International channel. For a while, immigrant language services, such as those in Arabic and Kurdish, were additionally carried on shortwave.\n\nThe Radio Sweden English Service seeks to provide a window on the diverse perspectives and issues in Sweden today.\n\nIts programs and website offer a ”smörgåsbord” of news and current affairs, science and technology, lifestyle, and culture.\n\nIn the beginning of 2016, the international service underwent a major reorganization. English language news about Sweden for overseas listeners were moved to a 30 minutes-a-week show on P2 and to social media. Minority languages service was further focused on those minorities living in Sweden and integrated with Swedish language services. \n\nService in languages of the oppressed nations, such as Somali, were moved to an independent organization. News for overseas listeners in foreign languages other than English, such as Russian and German, has been canceled.\nThere have been several foreign-leagugaes services in the history of Radio Sweden. The German Programm got disfunct after 76 years of service.\n\nOne of the most popular programs on Radio Sweden was \"Sweden Calling DXers\", founded in 1948 by Arne Skoog. He reasoned that shortwave listening or DXing was a very young hobby, and that by providing information in a weekly program for shortwave listeners about their hobby, Radio Sweden was teaching its own audience about how to listen better. While the first program was based solely on Arne's own listening, listeners were encouraged to write in with their own news, and soon virtually all of the program was based on listener's letters (an early example of interactvity).\n\nThe program was carried on Tuesdays in all of Radio Sweden's services except Swedish.\n\nWhen Arne Skoog retired in connection with the program's 30th anniversary in 1978, the program was taken over by George Wood, a member of the Radio Sweden English Service. After a number of years, as media changed, it began to cover less about shortwave, and more about satellite radio and television. Later coverage was extended to the Internet, the focus was shifted to concentrating on Swedish media, and the name of the English version of the program was changed to \"MediaScan\". \"MediaScan\" was the first English-language radio program in Europe (and the second overall in Europe) to post audio on the Internet (on the ftp sites ftp.funet.fi and ftp.sunet.se, as well as via Internet Multicasting).\n\nIn 2001 the program was discontinued, but remains in a somewhat sporadic form on the Radio Sweden website.\n\nAnother popular Radio Sweden program was \"The Saturday Show\", with Roger Wallis and Kim Loughran, which ran from 1967 to 1981. The program was launched to showcase Swedish rock and pop music in a world dominated by American and British rock. Using Radio Sweden's relatively high-powered medium wave transmitter on 1179 kHz, the entire program was 90 minutes in length, and featured many satirical sketches, often political and sometimes controversial. A 30-minute segment of the entire broadcast was the Radio Sweden shortwave program on Saturdays.\n\nOn October 20, 2010, Radio Sweden ceased broadcasts on shortwave and medium wave, as Swedish Radio's management decided that the Internet had matured enough to support international broadcasts. At the same time the English Service was extended to national broadcasts on FM, but programming on weekends was discontinued. Services in English also continue on satellite, both using Swedish Television's satellite and the World Radio Network.\n\n\n"}
{"id": "58097959", "url": "https://en.wikipedia.org/wiki?curid=58097959", "title": "Sahul (continent)", "text": "Sahul (continent)\n\nSahul was a prehistoric continent that consisted of Australia, New Guinea, Tasmania and Seram.\n\nSahul was partially submerged around 18,000 years ago.\n\nSahul and Sunda were points of early human migrations after leaving Africa.\n"}
{"id": "13015576", "url": "https://en.wikipedia.org/wiki?curid=13015576", "title": "Shortwave relay station", "text": "Shortwave relay station\n\nShortwave relay stations are transmitter sites used by international broadcasters to extend their coverage to areas that cannot be reached easily from their home state. E.g., the BBC operates an extensive net of relay stations.\n\nThese days the programs are fed to the relay sites by satellite, cable/optical fiber or the Internet. Frequencies, transmitter power and antennas depend on the desired coverage. Some regional relays even operate in the medium wave or FM bands.\n\nRelay stations are also important to reach listeners in countries that practice radio jamming. Depending on the effect of the shortwave dead zone the target countries can jam the programs only locally, e.g. for bigger cities. For this purpose Radio Free Europe/Radio Liberty with studios in Munich, Germany operated a relay station in Portugal, in the extreme west of Europe, to reach then-communist Eastern Europe.\n\nTwo and only one broadcasting technology couples all of the components of a traditional shortwave relay station into one unit: the ALLISS module. For persons totally unfamiliar with the concepts of how shortwave relay stations operate this design may be the most understandable.\n\nThe ALLISS module is a fully rotatable antenna system for high power (typically 500 kW only) shortwave radio broadcasting—it essentially is a self contained shortwave relay station.\n\nMost of the world's shortwave relay stations do not use this technology, due to its cost (15m EUR per ALLISS module: Transmitter + Antenna + Automation equipment).\n\nA traditional shortwave relay station—depending on how many transmitters and antennas that it will have—may take up to two years to plan. After planning is completed, it may take up to five years to construct the relay station.\n\nThe historically long design and planning cycle for shortwave relay stations ended in the 1990s. Many advanced software planning tools (not related to the relay station design proper) became available. Choosing a series of sites for a relay station is about 100 times faster using Google Earth, for example. With the modern graphical version of Ioncap, simplified propagation studies can completed in less than a week for any chosen site.\n\nIn some cases, existing relay stations can have their designs more or less duplicated, thus speeding up development time. However, there is one general exception to this: the ALLISS Module. From initial planning to deployment of ALLISS Modules may take a mere 1.5 years to 9 months depending on the number of modules deployed at one time in a particular sector of a country.\n\nThese are considered general operating parameters:\n\nGeneral requirements of shortwave relay stations:\n\n\nThe IEEE Book series \"The History of International Broadcasting\" (Volume I) describes mobile shortwave relay stations used by the German propaganda ministry during WWII, to avoid them being located by radio direction finding and bombed by the Allies. They consisted of a generator truck, transmitter truck and an antenna truck, and are thought to have had a radiated power of about 50 kW. Radio Industry Zagreb (RIZ Transmitters) currently produces mobile shortwave transmitters.\n\nThe International broadcasting center of TDF (Télédiffusion de France) is at Issoudun/Saint-Aoustrille. As of 2011, Issoudun is utilized by TDF for shortwave transmissions. The site uses 12 rotary ALLISS antennas fed by 12 transmitters of 500 kW each to transmit shortwave broadcasts by Radio France Internationale (RFI), along with other broadcast services.\n\n"}
{"id": "4515987", "url": "https://en.wikipedia.org/wiki?curid=4515987", "title": "Social effects of H5N1", "text": "Social effects of H5N1\n\nThe social impact of H5N1 is the effect or influence of H5N1 in human society; especially the financial, political, social, and personal responses to both actual and predicted deaths in birds, humans, and other animals. Billions of dollars are being raised and spent to research H5N1 and prepare for a potential avian influenza pandemic. Over ten billion dollars have been lost and over two hundred million birds have been killed to try to contain H5N1. People have reacted by buying less chicken causing poultry sales and prices to fall. Many individuals have stockpiled supplies for a possible flu pandemic.\n\nOn November 1, 2005 President George W. Bush unveiled the National Strategy To Safeguard Against The Danger of Pandemic Influenza backed by a request to Congress for $7.1 billion to begin implementing the plan.\n\nOn January 18, 2006 donor nations pledged two billion US dollars to combat bird flu at the two-day International Pledging Conference on Avian and Human Influenza held in China. Over ten billion dollars have been spent and over two hundred million birds have been killed to try to contain H5N1.\n\nAccording to \"The New York Times\", due to the H5N1 threat, as of March 2006: \"governments worldwide have spent billions planning for a potential influenza pandemic: buying medicines, running disaster drills, [and] developing strategies for tighter border controls.\"\n\nInvestment strategies are being altered to manage the effects of H5N1. This changes the valuations of trillions of dollars' worth of stocks worldwide as investors move assets in accordance with both fears and hopes.\n\nPoultry farming practices have changed due to H5N1:\n\nFor example, after nearly two years of using mainly culling to control the virus, the Vietnam government in 2005 adopted a combination of mass poultry vaccination, disinfecting, culling, information campaigns and bans on live poultry in cities.\n\nThe cost of poultry farming has increased, while the cost to consumers has gone down due to fears from H5N1 driving demand below supply, resulting in devastating losses for many poultry farmers. Poor poultry farmers can't afford mandated measures keeping their bird livestock from contact with wild birds (and other measures) thus risking losing their livelihood altogether. Multinational poultry farming is increasingly becoming a profit loser as H5N1 achieves status as endemic in wild birds worldwide.\n\nFinancial ruin for poor poultry farmers, that can be as severe as threatening starvation, has caused some to commit suicide and many others to stop cooperating with efforts to deal with H5N1; further increasing the human toll, the spread of the disease and the chances for a pandemic mutation.\n\nUS HHS Secretary Michael O. Leavitt has said \"Everything you say in advance of a pandemic is alarmist; anything you do after it starts is inadequate.\"\n\nH5N1, like everything else, is subject to political spin; wherein every interest group picks and chooses among the facts to support their favorite cause resulting in a distortion of the overall picture, the motivations of the people involved and the believability of the predictions.\n\nDonald Rumsfeld, formerly United States Secretary of Defense, is a past board member and current minor shareholder of Gilead Sciences which owns intellectual property rights to Oseltamivir (also called \"Tamiflu\"). In November 2005, George W. Bush urged Congress to pass 7.1 billion in emergency funding to prepare for the possible bird flu pandemic, of which one billion is solely dedicated to the purchase, and distribution of Tamiflu.\n\nSome believe \"The deadly H5N1 strain of bird flu is essentially a problem of industrial poultry practices.\"\n\nOthers have a more nuanced position. According to the CDC article \"H5N1 Outbreaks and Enzootic Influenza\" by Robert G. Webster et al.:\"Transmission of highly pathogenic H5N1 from domestic poultry back to migratory waterfowl in western China has increased the geographic spread. The spread of H5N1 and its likely reintroduction to domestic poultry increase the need for good agricultural vaccines. In fact, the root cause of the continuing H5N1 pandemic threat may be the way the pathogenicity of H5N1 viruses is masked by cocirculating influenza viruses or bad agricultural vaccines.\" Dr. Robert Webster explains: \"If you use a good vaccine you can prevent the transmission within poultry and to humans. But if they have been using vaccines now [in China] for several years, why is there so much bird flu? There is bad vaccine that stops the disease in the bird but the bird goes on pooping out virus and maintaining it and changing it. And I think this is what is going on in China. It has to be. Either there is not enough vaccine being used or there is substandard vaccine being used. Probably both. It's not just China. We can't blame China for substandard vaccines. I think there are substandard vaccines for influenza in poultry all over the world.\" In response to the same concerns, Reuters reports Hong Kong infectious disease expert Lo Wing-lok saying that \"The issue of vaccines has to take top priority\", and Julie Hall, in charge of the WHO's outbreak response in China, saying that China's vaccinations could be \"masking\" the virus. The BBC reported that Dr Wendy Barclay, a virologist at the University of Reading, UK said: \"The Chinese have made a vaccine based on reverse genetics made with H5N1 antigens, and they have been using it. There has been a lot of criticism of what they have done, because they have protected their chickens against death from this virus but the chickens still get infected; and then you get drift - the virus mutates in response to the antibodies - and now we have a situation where we have five or six 'flavours' of H5N1 out there.\"\n\nSome have called for tax breaks due to H5N1. A May 7, 2006 report from India E-News states that: \"Pakistani poultry farmers have sought a 10-year tax exemption to support their dwindling business after the detection of the H5N1 strain of bird flu triggered a fall in demand and prices, a poultry trader said. \"We have asked the government to give us tax exemption on income from the poultry business for at least 10 years to meet losses caused by the bird flu scare\", Abdul Basit told DPA. Basit, vice president of the Chamber of Commerce and Industry (LCCI) in the country's commercial hub of Lahore, was part of a delegation of the Pakistan Poultry Association, which met food ministry officials to present their demand. The federal poultry board of the food ministry is to meet on May 9 to consider the tax-cut demand for the poultry business in the upcoming national budget due in mid-June.\"\n\nReuters reported that WHO expert Hassan al-Bushra said:\nEven now, we remain unsure about Tamiflu's real effectiveness. As for a vaccine, work cannot start on it until the emergence of a new virus, and we predict it would take six to nine months to develop it. For the moment, we cannot by any means count on a potential vaccine to prevent the spread of a contagious influenza virus, whose various precedents in the past 90 years have been highly pathogenic. However, it is crucial that countries in the Middle East invest and start developing their own research and technical facilities, where they can produce their own drugs when the time comes rather than wait to import expensive medicines from abroad at the risk of their population.\n\nIf a pandemic occurs, local response will be more important than national or international response, as every community will have its own resources swamped dealing with its own problems. International groups, nations, local governments, corporations, schools, and groups of all kinds have made plans and run drills to prepare for an H5N1 pandemic.\n\nOnline avian flu forums have received increasing attention. Self-help groups have organized to provide news and information about resources, aid and relief efforts in preparation for avian flu.\n\nBritish reports warn that in response to an influenza pandemic local groups will not be able to rely on the armed forces, widespread infection could occur in days not months, an effective vaccine can not be counted on, and the huge death toll could swamp mortuaries so \"A key point for local planning is likely to be the identification of potential sites for the location of facilities for the temporary storage of bodies\".\n\nMany individuals have stockpiled supplies (Tamiflu, food, water, etc.) for a possible flu pandemic.\n\nIndividuals have started web sites and companies using interest and ignorance in H5N1 to sell information, cures, and advertising space. Some even use concern over H5N1 to find victims for their malware.\n\nA significant effect of H5N1 has been personal fear concerning the unknown, even by those most in-the-know. Dr. David Nabarro, chief avian flu coordinator for the United Nations, describes himself as \"quite scared\"; says avian flu has too many unanswered questions; and if the disease starts spreading to humans, borders will close, airports will shut down, and travelers everywhere will be stranded. With evaluations of the threat ranging from those who say it is a hoax to those who warn of billions of humans dying, uncertainty and fear motivate personal behaviors around the world affecting many people even before the threat becomes reality.\n\nThe 1998 chart-topping hit song \"One Week\" by Barenaked Ladies includes the lines \"Chickity China the Chinese chicken / Have a drumstick and your brain stops tickin'\", a reference to the outbreaks of H5N1 in Hong Kong around the time the song was written.\n\nThe annual flu season deaths and costs caused by viruses other than H5N1 provide a point of contrast - something to compare against. According to the United States Government, the annual flu in the United States:\nresults in approximately 36,000 deaths and more than 200,000 hospitalizations each year. In addition to this human toll, influenza is annually responsible for a total cost of over $10 billion in the United States. A pandemic, or worldwide outbreak of a new influenza virus, could dwarf this impact by overwhelming our health and medical capabilities, potentially resulting in hundreds of thousands of deaths, millions of hospitalizations, and hundreds of billions of dollars in direct and indirect costs.\n\nThe \"New England Journal of Medicine\" reported that: \"A study by the Congressional Budget Office estimates that the consequences of a severe pandemic could, in the United States, include 200 million people infected, 90 million clinically ill, and 2 million dead. The study estimates that 30 percent of all workers would become ill and 2.5 percent would die, with 30 percent of workers missing a mean of three weeks of work — resulting in a decrease in the gross domestic product of 5 percent. Furthermore, 18 million to 45 million people would require outpatient care, and economic costs would total approximately $675 billion.\" One study concludes that a pandemic that reduced the available dock workers by 28% would cut the throughput capacity for containers arriving at American ports on the West coast by 45%.\n\n"}
{"id": "27247561", "url": "https://en.wikipedia.org/wiki?curid=27247561", "title": "Tanks of the U.S. in the World Wars", "text": "Tanks of the U.S. in the World Wars\n\nAs the American army did not have tanks of its own, the French two-man Renault FT Light Tank was used by US in the later stages of World War I. It was cheap and well-suited for mass production, and in addition to its traversable turret another innovative feature of the FT was its engine located at the rear. This pattern, with the gun located in a mounted turret and rear engine, became the standard for most succeeding tanks across the world even to this day.\n\nThe M1917 was a US tank accepted by the army in October 1918 and is primarily based on the plans of the French Renault FT. The crew, a driver and gunner, were separated from the engine by a bulkhead. Steel idler wheels replaced the wooden idlers fitted to French examples. Approximately 64 of the M1917 were built before the end of World War I and 10 were sent to Europe, but too late to be used in combat. After the war Van Dorn Iron Works, the Maxwell Motor Company, and the C.L. Best Tractor Company created 950 more. 374 had cannons and 526 had machine guns and 50 were signal (radio) tanks. A later modification, the M1917A1, was a lengthened, rebuilt, updated version compared to the French one, having a 100 hp Franklin engine and an electric self-starter rather than a crank starter.\n\nU.S. troops also used the British Heavy Tanks Mk V and Mk V* (pronounced \"Mark Five\" and \"Mark Five Star\"). A battalion trained in England and saw action in France in the last six weeks of the War. On a small number of occasions, U.S. troops were supported by tank units of the French Army operating Schneider and Saint-Chamond machines.\n\nMarmon-Herrington tanks that could not be delivered because of the fall of the Dutch East Indies were taken over by the US. The CTLS-4TAC and -4TAY tanks were redesignated Light Tank T14 and T16 respectively. They were used for training, some were used in Alaska and by the US Marines. The CTLS-4TAC has the turret offset to the left, the CTLS-4TAY to the right.\n\nThe Stuart was an upgrade of the M2 Light Tank.\n\nThe initial upgrade was designated the M3 Stuart and had thicker armor, modified suspension and a 37mm mm gun. Production of the M3 and later the M5 Stuart started in March 1941 and continued until October 1943 with a total of 25,000 produced.\n\nAn upgrade of the M3, which was initially called M4 but later redesignated the M5, was developed with improved engines and produced through 1942. The M5 featured a redesigned hull and driver's hatches moved to the top. The M5 gradually replaced the M3 in production from 1942 and was in turn succeeded by the Light Tank M24 in 1944.\n\nThe British Army was the first to use the M3 in combat. In November 1941, some 170 Stuarts took part in Operation Crusader, with poor results.\n\nAlthough the high losses suffered by Stuart-equipped units during the operation had more to do with better tactics and training of the enemy than superiority of enemy tanks in the North African campaign, the operation revealed that the M3 had several technical faults. Mentioned in the British complaints were the 37 mm gun, a limited range and poor internal layout. The two-man turret crew was a significant weakness, and some British units tried to fight with three-man turret crews. Crews liked its high speed and mechanical reliability.\n\nFrom the summer of 1942, when enough US medium tanks had been received, the British usually kept Stuarts out of tank-to-tank combat. M3s, M3A3s, and M5s continued in British service until the end of the war, but British armor units had a smaller proportion of these light tanks than US units.\n\nThe other major Lend-Lease recipient of the M3, the Soviet Union, was even less happy with the tank, considering it undergunned, underarmored, likely to catch fire, and too sensitive to fuel quality. The narrow tracks were highly unsuited to operation in winter conditions, as they resulted in high ground pressures that sunk the tank. However, the Stuart was superior to early-war Soviet light tanks such as the T-60, which were often underpowered and possessed even lighter armament than the Stuart. In 1943, the Red Army tried out the M5 and decided that the upgraded design wasn't much better than the M3. Being less desperate than in 1941, the Soviets turned down an American offer to supply the M5. M3s continued in Red Army service at least until 1944.\n\nIn US Army service, the M3 first saw combat in the Philippines. Two battalions, comprising the Provisional Tank Group fought in the Bataan peninsula campaign. When the American army joined the North African Campaign in late 1942, Stuart units still formed a large part of its armor strength.\n\nAfter the disastrous Battle of the Kasserine Pass the US quickly disbanding most of their light tank battalions and subordinating the Stuarts to medium tank battalions performing the traditional cavalry missions of scouting and screening. For the rest of the war, most US tank battalions had three companies of M4 Shermans and one company of M3s or M5/M5A1s.\n\nIn the European theater, Allied light tanks had to be given cavalry and infantry fire support roles since their main cannon armament could not compete with heavier enemy AFVs. However, the Stuart was still effective in combat in the Pacific Theater, as Japanese tanks were both relatively rare and were generally much weaker than even Allied light tanks. Japanese infantrymen were poorly equipped with anti-tank weapons and tended to attack tanks using close-assault tactics. In this environment, the Stuart was only moderately more vulnerable than medium tanks. In addition, the poor terrain and roads common to the theatre were unsuitable for the much heavier M4 medium tanks, and so initially, only light armor could be deployed. Heavier M4s were eventually brought to overcome heavily entrenched positions, though the Stuart continued to serve in a combat capacity until the end of the war.\n\nThe US liquidated its Stuarts when it got sufficient numbers of M24 Chaffees but the tank remained in service until the end of the war and well after. In addition to the United States, United Kingdom and Soviet Union, who were the primary users, it was also used by France and China.\n\nIn April 1943 the government started work on the M24 Chaffee, designated Light Tank T24 as a replacement for the M3/M5 Stuart after the original replacement, the M7, was rejected in March. Every effort was made to keep the weight of the vehicle under 20 tons. The armor was kept light, and a lightweight 75 mm gun was developed. The design also featured wider tracks and torsion bar suspension. It had relatively low silhouette and a three-man turret. In mid-October the first pilot vehicle was delivered and production began in 1944 under the designation Light Tank M24. 4,730 were produced by the time production was stopped in August 1945.\n\nThe first thirty-four M24s reached Europe in November 1944 and were issued to the U.S. 2nd Cavalry Group (Mechanized) in France. These were then issued to F Company, 2nd Cavalry Reconnaissance Battalion and F Company, 42nd Cavalry Reconnaissance Battalion which each received seventeen M24s. During the Battle of the Bulge in December 1944, these units and their new tanks were rushed to the southern sector; two of the M24s were detached to serve with the 740th Tank Battalion of the U.S. First Army.\n\nThe M24 started to enter widespread issue in December 1944 but they were slow in reaching the front-line combat units. By the end of the war many armored divisions were still mainly equipped with the M5. Some armored divisions did not receive their first M24s until the war was over.\n\nReports were generally positive. Crews liked the improved off-road performance and reliability, but were most appreciative of the 75 mm main gun, as a vast improvement over the 37 mm. The M24 was still not up to the challenge of fighting tanks, however, the bigger gun at least gave it a chance to defend itself. Its light armor made it vulnerable in tank vs. tank actions.\n\nThe M24s contribution to winning the war was insignificant, as too few arrived too late to replace the M5s of the armored divisions.\n\nThe U.S. Army designed the M4 as a replacement for the M3 Medium. The designated goals were to produce a fast, dependable medium tank able to support infantry, provide breakthrough striking capacity, and defeat any tank currently in use by the Axis nations. In April 1941 the U.S. Armored Force Board chose the simplest of five designs. Known as the T6, the design was a modified M3 hull and chassis, carrying a newly designed turret mounting the Lee's main gun. This became the Sherman.\n\nThe prototype M4 was completed in September 1941. The T6 became standardized as the M4, and production after modifications began in October 1941.\n\nThe U.S. Army had seven main sub-designations for M4 variants during the production period: M4, M4A1, M4A2, M4A3, M4A4, M4A5, and M4A6. These designations were manufactured concurrently at different locations.\n\nWhile most Shermans ran on gasoline, the M4A2 and M4A6 had diesel engines: the M4A2 with a pair of GMC 6-71 straight six engines, the M4A6 a Caterpillar RD1820 radial. These, plus the M4A4 which used the Chrysler A57 multibank engine, were mostly supplied to Allied countries under Lend-Lease. \"M4\" can refer specifically to the initial sub-type with its Continental radial engine, or generically, to the entire family of seven Sherman sub-types, depending on context. Many details of production, shape, strength, and performance improved throughout production without a change to the tank's basic model number; more durable suspension units, safer \"wet\" (W) ammunition stowage, and stronger armor arrangements, such as the M4 Composite, which had a cast front hull section mated to a welded rear hull. British nomenclature differed from that employed by the U.S.\n\nEarly Shermans mounted a 75 mm medium-velocity general-purpose gun. Although Ordnance began work on the Medium Tank T20 as a Sherman replacement, ultimately the Army decided to minimize production disruption by incorporating elements of other tank designs into Sherman production. Later M4A1, M4A2, and M4A3 models received the larger T23 turret, with a high-velocity 76 mm gun M1, which reduced the number of HE and smoke rounds carried and increased the number of anti-tank rounds. Later, the M4 and M4A3 were factory-produced with a 105 mm howitzer and a new distinctive mantlet in the original turret. The first standard-production 76 mm gun Sherman was an M4A1, accepted in January 1944, and the first standard-production 105 mm howitzer Sherman was an M4 accepted in February 1944.\n\nIn June–July 1944, the Army accepted a limited run of 254 M4A3E2 \"Jumbo\" Shermans, which had very thick armor, and the 75 mm gun in a new, heavier T23-style turret, in order to assault fortifications. The M4A3 was the first to be factory-produced with the HVSS (horizontal volute spring suspension) suspension with wider tracks to distribute weight, and the smooth ride of the HVSS with its experimental E8 designation led to the nickname \"Easy Eight\" for Shermans so equipped. Both the Americans and the British developed a wide array of special attachments for the Sherman; few saw combat, and most remained experimental. Those which saw action included the bulldozer blade for Sherman dozer tanks, Duplex Drive for \"swimming\" Sherman tanks, R3 flame thrower for \"Zippo\" flame tanks, and the T34 60-tube \"Calliope\" 4.5\" rocket launcher for the Sherman turret. The British variants (DD's and mine flails) were among \"Hobart's Funnies,\" named after their commander, Percy Hobart of the 79th Armoured Division. \n\nThe M4 Sherman's basic chassis was used for all the sundry roles of a modern mechanized force: roughly 50,000 Sherman tanks, plus thousands more derivative vehicles under different model numbers. These included M32 and M74 \"tow truck\"-style recovery tanks with winches, booms, and an 81 mm mortar for smoke screens; M34 (from M32B1) and M35 (from M10A1) artillery prime movers; M7B1, M12, M40, and M43 self-propelled artillery; and the M10 and M36 tank destroyers.\n\nThe M4 Sherman served with the U.S. Army and Marine Corps during World War II. The U.S. also supplied large numbers to the various Allied countries. Shermans were used during the war by British and Commonwealth armies, the Soviet Union's Red Army, Free French forces, the Polish army in exile, China's National Revolutionary Army, and Brazil's Expeditionary Force.\n\nThe U.S. Marine Corps used the diesel M4A2 and gasoline-powered M4A3 in the Pacific. However, the Chief of the Army's Armored Force, Lt. Gen. Jacob L. Devers, ordered that no diesel-engined Sherman tanks be used by the Army outside the Zone of Interior (the continental U.S.). The U.S. Army used all types for either training or testing within the United States, but intended the M4A2 and M4A4 to be the primary Lend-Lease exports.\n\nThe M4A1 Sherman first saw combat at the Second Battle of El Alamein in October 1942 with the British 8th Army. The first U.S. Shermans in battle were M4A1s in Operation \"Torch\" the next month. At this time, Shermans successfully engaged German Panzer IIIs with long barreled 50mm L60 guns, and Panzer IVs with short barreled 75 mm L24 guns. Additional M4 and M4A1s replaced M3 Lees in U.S. tank battalions over the course of the North African campaigns. However, by June 1944, most German tanks were up-gunned and 75 mm Shermans were outgunned. The M4 and M4A1 were the main types in U.S. units until late 1944, when the Army began replacing them with the preferred M4A3 with its more powerful engine. Some M4s and M4A1s continued in U.S. service for the rest of the war.\n\nThe first Sherman to enter combat with the 76 mm gun (July 1944) was the M4A1, closely followed by the M4A3. By the end of the war, half the U.S. Army Shermans in Europe had the 76 mm gun. The first HVSS Sherman to see combat was the M4A3E8(76)W in December 1944. The M4A3E8 (76)W was arguably the best of the US Sherman tanks.\n\n\n\n\n\n\nThe Encyclopaedia of Tanks and Armoured Fighting Vehicles; Published in 2007 by Amber Books Ltd.\n"}
{"id": "50166890", "url": "https://en.wikipedia.org/wiki?curid=50166890", "title": "Timeline of global health", "text": "Timeline of global health\n\nThis page is a timeline of global health, including major conferences, interventions, cures, and crises.\n\nDuring this pre-WWII era, there are three big trends that operate separately, but sometimes affect each other in development and outcomes.\n\nFirst, a trend of urbanization (fueled by the Industrial Revolution) as well as greater global trade and migration leads to new challenges, including those in urban sanitation and infectious diseases/pandemics. Six global cholera pandemics happen in this period because of increased commerce and migration.\n\nSecond, there is a lot of development on the underlying theory of disease, advancements in vaccine and antibiotic development, and a variety of experimental large-scale eradication and control programs. One big example: the germ theory of diseases begins to become accepted and popularized starting around 1850. Another big example is the development of the smallpox vaccine by Edward Jenner in 1796. Systematic eradication and control efforts include the Rockefeller Sanitary Commission and efforts to eradicate smallpox. Antitoxins and vaccines for numerous diseases including cholera and tuberculosis are developed during this period, building on a trend of greater understanding of and control over microorganisms.\n\nA third theme during this era is the formation of various preliminary international alliances and conferences, including the International Sanitary Conferences, Pan American Health Organization, Office International d'Hygiène Publique, and the League of Nations Health Committee. This is closely intertwined with the other two trends. For instance, the cholera pandemics mentioned above, as well as the growing scientific understanding of the germ theory of disease, are both key impetuses for the International Sanitary Conferences.\n\nFollowing the end of World War II, the first batch of big organizations, both international and national (with international cooperation), including the United Nations and World Health Organization (WHO), form. Beginning with the United Nations Relief and Rehabilitation Administration for relief of victims of war in 1943, there is a big push to begin creating large scale health initiatives, non-governmental organizations, and worldwide global health programs by the United Nations to improve quality of life around the world. UNICEF, the World Health Organization, as well as the UNRRA are all part of United Nations efforts to benefit global health beginning with developing countries. These various programs aim to aid in economic endeavors by providing loans, direct disease prevention programs, health education, etc.\n\nAfter wrapping up complications caused by the end of the war, there is an international energy put in into eradication, beginning with the complete smallpox eradication in 1979. There is greater dissatisfaction with WHO for its focus on disease/infection control at the expense of trying to improve general living conditions, as well as disappointment at its low budget and staffing. This atmosphere spurs other organizations to provide their own forms of aid. The Alma Ata Declaration and selective primary healthcare are created to express urgent action by all governments and citizens to protect and promote the health of all people equally. More organizations form following these new active attitudes toward global health, including the International Agency for Research on Cancer and the Doctors Without Borders organization. Publications like the WHO Model List of Essential Medicines highlight basic medicines required by most adults and children to survive, and set priorities for healthcare fund allocation in developing countries. Generally, there is more buy-in for the idea that direct, targeted efforts to address healthcare could be worthwhile and benefit many countries.\n\nCertain specific efforts increase in efficiency and productivity, including improvement in maternal and child health and a focus on HIV/AIDS, tuberculosis, and malaria (the 'Big Three') in developing countries. During this time period, the child survival revolution (CSR), which helps reduce child mortality in the developing world, and GOBI-FFF are both advocated by James P. Grant. The World Summit for Children also takes place, becoming one of the largest ever gathering of heads of states and government to commit a set of goals to improve the well-being of children. Finally, HIV/AIDS becomes the focus of many governmental and non-governmental organizations, leading to the formation of the Global Programme on AIDS (GPA) by efforts of the World Health Organization. However, these health organizations also make significant advancements to tuberculosis treatments, including the DOTS strategy and the formation of the Stop TB Partnership.\n\nUN's Millennium Development Goals establishes health care as an important goal (not just combating infectious diseases). Later in 2015, the Sustainable Development Goals build on the MDGs to outline the objectives that will transform our world by ending poverty, helping the environment, and improving health and education. More specific disease-targeting organizations are created primarily to fund healthcare plans in developing countries, including the President's Emergency Plan for AIDS Relief and The Global Fund to Fight AIDS, Tuberculosis and Malaria. These organizations (especially the WHO) adopt new strategies and initiatives, including the 3 by 5 Initiative to widen the access to antiretroviral treatment, the WHO Framework Convention on Tobacco Control, etc. Private large donors such as the Bill & Melinda Gates Foundation begin to play an important role in shaping the funding landscape and direction of efforts in global health.\n\nThe following events are selected for inclusion in the timeline:\n\n\nWe do \"not\" include:\n\n\n\n\n\n"}
{"id": "51099085", "url": "https://en.wikipedia.org/wiki?curid=51099085", "title": "To Save Humanity", "text": "To Save Humanity\n\nTo Save Humanity is a 2015 collection of 96 essays on global health from a collection of authors who range from heads of states, movie stars, scientists at leading universities, activists, and Nobel Prize winners. Each contributor was asked the same question: \"What is the single most important thing for the future of global health over the next fifty years?\" The collection was edited by Julio Frenk and Steven J. Hoffman.\n\nThe Global Strategy Lab called the collection \"unparalleled\" and \"a primer on the major issues of our time and a blueprint for post-2015 health and development,\" and featured it in their annual conference.\n\nThe Health Impact Fund also featured the collection at their conference.\n\nThe Lancet described the book as \"testimony to the complexity of global health politics,\" and called it \"a reminder that the breadth of individual and institutional engagement with global health cannot be fully captured by one set of global goals.\"\n\nVox has republished several of the articles for free online as part of a series entitled \"One Change to Save the World.\"\n"}
{"id": "6312082", "url": "https://en.wikipedia.org/wiki?curid=6312082", "title": "Transnational citizenship", "text": "Transnational citizenship\n\nTransnational citizenship redefines traditional notions of citizenship and replaces an individual's singular national loyalties with the ability to belong to multiple nation states, as made visible in the political, cultural, social and economic realms. Unlike national citizenship, where individuals interact in such capacities with one sovereign state, transnational citizenship transcends pre-established territorial boundaries in order to create a modern meaning of \"belonging\" in an increasingly globalized society. Additionally, while preconceived notions of citizenship are often divided between national, social and individual forms of identity, all three categories serve to contribute to the meaning of transnational citizenship. State citizenship can be defined as an individual establishing their sense of belonging by espousing to the liberal-democratic values of the state in the public sphere. When applied to transnational citizenship, an individual would have the opportunity to be civically engaged in multiple societies. A Dominican politician who lives in Santo Domingo yet canvasses in a highly dense Dominican American population in Boston, Massachusetts for external votes is an example of a transnational citizens functioning politically between two states.\n\nIn terms of the categories of social and individual forms of belonging, transnational citizens are marked by multiple identities and allegiances, and often travel between two or more countries, all in which they have created sizeable networks of differing functions. Similar to global or cosmopolitan citizenship, it is composed of cross-national and multi-layered memberships to certain societies. Transnational citizenship is based on the idea that a new global framework consistent of subgroups of national identities will eventually replace membership to one sole nation-state. In a hyper-realized version of transnational citizenship, \"states become intermediaries between the local and the global.\" Institutionalizing transnational citizenship would loosen ties between territories and citizenship and would ultimately result in a reconstruction of world order that forever changes the capacity in which individuals interact with government institutions.\n\nWhile some relate transnational citizenship to any historic shift or fusion of identities within nation-states, modern conceptions of the term have only surfaced in the past twenty years. Many attribute the evolution of the term to the rising situation of globalization. Globalization is defined by a heightened international access to the world capital market system and increased abilities to more rapid forms of communication. Due to the convenience and ease of modern international exchanges, globalization has become the process by which international economies as well as individuals interact with one another. Since post-Cold War 1989, the evolved \"global political economy\" has resulted in massive \"reconfigurations of the world's arenas\". Globalization transformed a confined geo-political system into one that relies heavily on multiple levels of local, national and global interactions. For example, China's industrialization from an agricultural society to a manufacturing society chronicled by excessive imports and exports contribute to a need to interconnect societies from all corners of the globe. The wealth that private institutions experienced from globalization resulted in \"further extensions of corporations in search of faraway resources and markets\".\n\nBeyond resulting in substantial political and economical shifts, globalization has also affected social and cultural practices between people. According to citizenship scholars like Andrew Vandenberg, such acts of globalization eventually \"ended the constraints of space and time that conditioned all earlier human transactions, practices, and therefore identities. With the growth and distribution of technology, more people all over the world have come to establish personal relationships with one another. Former state-regulated formal encounters are now replaced by modern informal and all the more frequent interactions. Rapid world economic growth has consequently led to international migrations. In recent years, in conjunction with globalization, increased instances of uncontrolled and predominantly illegal international migrations contribute to opportunities for escalating transnational identities. Because obvious ties surface between immigrants, their home countries, and the receiving countries, the civic ramifications are widespread. Thus international immigration contributes to loosening individual state ties. Once in their host countries, immigrants form social networks while maintaining ties to their homeland. Some organizations function in both countries, which serves to further enhance the notion that international migrants act as transnational citizens in multiple lands.\n\nIt is important to draw a distinction between transnational citizenship and multiculturalism among national citizens. While transnational citizens bring cultural and societal elements of their home countries to their host countries and vice versa, multiculturalism results from the fusion of differing ethnic minorities or indigenous peoples on a micro scale of a particular local environment. These interactions are described as \"crosscutting and always mutually situational identifications.\" Therefore, ethnic minorities and majorities alike intermingle in a mutually shared space. All different types of individuals function within the same system, and eventually collective national identities are formed. On the other hand, transnational citizens live within the context of two or more societies that differ in size, scope, populations, laws, morals, and cultural codes. While transnational citizens interact with those already present in each respective community, they are functioning within divergent spaces. They base their interactions more on the need to reconcile two completely diverse localities into a greater context that traverses international borders, politics, and ways of life.\n\n\"The question for the future of citizenship is whether a 'global' citizenship can transcend citizenships defined by 'local' stages on the basis of blood and birth through an act of the state itself.\" — Henry Teune\n\nSome scholars consider the creation of the European Union as the pilot case for testing Teune's question about whether or not transnational citizenship can surpass national citizenship. Starting in the early 1980s, European national migration control officials met and established a consensus over the relationship between migration, asylum and crime. The control officials deemed migration as a security issue, and called for a \"multi-level governance\" in order to control migratory practices. Virginie Guiraudon generates the theory of \"venue-shopping\" in order to describe how cross-national policies prevailed. Venue-shopping is the process by which political members seek out specific governmental settings in order to establish their ideal policy outcomes. Political actors circumvented national levels of control in order to establish a \"transnational cooperation\" among nation-states. Starting in 1981, citizens with passports from European countries were able to move freely across borders into other European countries. Due to the increased ease of traversing borders, the \"Europeanization\" of individuals began to occur in which a new transnational identity could be conceived.\n\nThe creation of the European Union only accelerated growing notions of transnational citizenship across the continent. The European Union came into being on November 1, 1993, when European nations signed the Treaty of Maastricht into law. The treaty established \"community policy\" in six new areas, one of which is termed \"trans-European networks.\" The treaty also discusses the specific effects of the merger on a new formation of European citizenship. The benefits of European citizenship include the ability for citizens to freely cross borders into and subsequently reside in other European countries, the right to vote in elections and run for office in both municipal and European elections in the state in which the citizen resides, the right to access any member country's diplomatic or consular services in a third-party country in which the citizen's birth nation is not represented, and the citizen's right to petition to the European parliament. The treaty thus instituted \"European citizenship over and above national citizenship\". This ultimately facilitates a new form of \"European identity\" that allows for members of the European Union to function as transnational actors beyond their countries' borders, establishing the entire continent as one cohesive entity.\n\nFinally, the creation of the euro serves as the pinnacle of Europe's newfound economic unification. On January 1, 1999, the euro replaced the pre-existing currency in 11 European countries. The Treaty of Maastricht also created the European System of Central Banks, which consists of the Central European Bank and national central banks working together to establish monetary policy across participating countries. Some scholars consider the act of unifying the currency as \"culminating the progress toward economic and monetary union in Europe\". While the aspect of monetary union is clear, the far-reaching effects of economic union between countries could be considered a cause for debate. Regardless, the euro allows for transnational citizens of the European Union to not only move freely across borders, but also to experience easier monetary exchanges through the ability to use a currency that is present in both the citizens' home and host countries. The political, economic, and social ramifications that result from the invention of the European Union help contribute to the construction of European citizens as the international model for transnational citizenship.\n\n\n"}
{"id": "55897564", "url": "https://en.wikipedia.org/wiki?curid=55897564", "title": "Transnational gangs", "text": "Transnational gangs\n\nTransnational gangs can be described as gangs that are located in multiple countries. When these gangs commit crimes in one country, their plans for the crime can sometimes be put together in another country. These gangs or mara are able to move around efficiently from one place to another. Transnational gangs are not a normal street gang because they are much larger in size and located in more than one country; they are considered to be able to pose a significant threat for the safety of the countries they are located in.\n\nThe Mara Salvatrucha, or the MS-13 gang are located in the United States, Canada, Mexico and El Salvador. The Mara Salvatrucha gang was founded in the 1980s in Los Angeles, California. The original members of the Mara Salvatrucha were refugees from El Salvador that came to the United States . The Barrio 18 or the 18th Street Gang is the Mara Salvatrucha's main rivals.\n\nThe 18th Street Gang, also known as the Barrio 18, are located in the United States, Canada, Central America and Mexico. The 18th Street Gang is a youth gang. The current gang was founded in the 1980s in Los Angeles, California. The 18th Street Gang was originally part of the Clanton 14 Gang, but they divided in the 1980s; the gang was originally started by Mexican immigrants. The Mara Salvatucha are considered the 18th Street Gang's enemies .\n\nThe Barrio Azteca are a prison gang that are located in the United States and Mexico. The Barrio Azteca was founded in 1986 in El Paso, Texas. The prison gang was started by Mexican prisoners that were incarcerated in El Paso. The Sinaloa Cartel is one of the Barrio Azteca's major enemies.\n\nThe Hells Angels are an outlaw motorcycle gang. They are located in the United States, Europe, Canada, Oceania, Africa, Asia, South America and Central America. The gang was founded on March 7, 1948 in Fontana, California and San Bernardino, California. The Hells Angels are considered to be a gang because they fit the definition of a gang; one of the requirements to be considered a gang is to participate in some form of crime. The crimes that the Hells Angels participate in include the transportation, production and distribution of drugs, assault, homicide, motorcycle theft, extortion, money laundering.\n\nThe Black Pistons are an outlaw motorcycle gang. They are located in the United States, Wales, Scotland, Iceland, Australia, England, Norway, Ireland, Belgium, Switzerland, Poland, Germany and Canada. The gang was founded in Germany in 2002 as a support for the Outlaws Motorcycle Club. They are considered a gang because they meet the requirements of a gang, which include committing crimes. The crimes that the Black Pistons commit are theft, assault, intimidation, extortion, fraud and many others.\n"}
{"id": "1632710", "url": "https://en.wikipedia.org/wiki?curid=1632710", "title": "Universal history", "text": "Universal history\n\nA universal history is a work aiming at the presentation of the history of humankind as a whole, coherent unit.\n\nA universal chronicle or world chronicle traces history from the beginning of written information about the past up to the present. \nUniversal history embraces the events of all times and nations in so far as scientific treatment of them is possible.\n\nUniversal history in the Western tradition is commonly divided into three parts, viz. ancient, medieval, and modern time. The division on ancient and medieval periods is less sharp or absent in the Arabic and Asian historiographies. A synoptic view of universal history led some scholars, beginning with Karl Jaspers, to distinguish the Axial Age synchronous to \"classical antiquity\" of the Western tradition. Jaspers also proposed a more universal periodization—prehistory, history and planetary history. All distinguished earlier periods belong to the second period (history) which is a relatively brief transitory phase between two much longer periods.\n\nThe roots of historiography in the 19th century are bound up with the concept that history written with a strong connection to the primary sources could be integrated with \"the big picture\", i.e. to a general, universal history. For example, Leopold von Ranke, probably the pre-eminent historian of the 19th century, founder of Rankean historical positivism, the classic mode of historiography that now stands against postmodernism, attempted to write a Universal History at the close of his career. The works of world historians Oswald Spengler and Arnold J. Toynbee are examples of attempts to integrate primary source-based history and Universal History. Spengler's work is more general; Toynbee created a theory that would allow the study of \"civilizations\" to proceed with integration of source-based history writing and Universal History writing. Both writers attempted to incorporate teleological theories into general presentations of the history. Toynbee found as the \"telos\" (\"goal\") of universal history the emergence of a single World State.\n\nA project of Universal history may be seen in the Hebrew Bible, \nwhich from the point of view of its redactors in the 5th century BC presents a history of humankind from creation to the Flood, \nand from there a history of the Israelites down to the present. The Seder Olam is a 2nd-century CE rabbinic interpretation of this chronology.\n\nIn Greco-Roman antiquity, the first universal history was written by Ephorus (fl. 4th century BC). This work has been lost, but its influence can be seen in the ambitions of Polybius (203–120 BC) and Diodorus (fl. 1st century BC) to give comprehensive accounts of their worlds. Herodotus' \"History\" is the earliest surviving member of the Greco-Roman world-historical tradition, although under some definitions of universal history it does not qualify as universal because it reflects no attempt to describe an overall direction of history or a principle or set of principles governing or underlying it. Polybius was the first to attempt a universal history in this stricter sense of the term:\n\n\"Metamorphoses\" by Ovid has been considered as a universal history because of its comprehensive chronology—from the creation of mankind to the death of Julius Caesar a year before the poet's birth. In Leipzig are preserved five fragments dating to the 2nd century AD and coming from a world chronicle. Its author is unknown, but was perhaps a Christian. Later, universal history provided an influential lens on the rise of Christianity in the Roman Empire in such works as Eusebius's \"Ecclesiastical History\", Augustine's \"City of God\", and Orosius' \"History Against the Pagans\".\n\nDuring the Han Dynasty (202 BCE – 220 CE) of China, Sima Qian (145–86 BC) was the first Chinese historian to attempt a universal history—from the earliest mythological origins of his civilization to his present day—in his \"Records of the Grand Historian\". Although his generation was the first in China to discover the existence of kingdoms in Central Asia and India, his work did not attempt to cover the history of these regions.\n\nThe \"universal chronicle\" traces history from the beginning of the world up to the present and was an especially popular genre of historiography in medieval Western Europe. The universal chronicle differs from the ordinary chronicle in its much broader chronological and geographical scope, giving, in principle, a continuous account of the progress of world history from the creation of the world up to the author's own times, but in practice often narrowing down to a more limited geographical range as it approaches those times.\n\nThe \"Chronica\" of Eusebius of Caesarea (c. 275–339) is considered to be the starting point of this tradition. The second book of this work consisted of a set of concordance tables (\"Chronici canones\") that for the first time synchronized the several concurrent chronologies in use with different peoples. Eusebius' chronicle became known to the Latin West through the translation by Jerome (c. 347–420).\n\nUniversal chronicles are sometimes organized around a central ideological theme, such as the Augustinian idea of the tension between the heavenly and the earthly state, as depicted in the City of God, which plays a major role in Otto von Freising's \"Historia de duabus civitatibus\". Agustine’s thesis depicts the history of the world as universal warfare between God and the Devil. This metaphysical war is not limited by time but only by geography as it takes place on planet Earth. In this war God moves (by divine intervention/ Providence) those governments, political /ideological movements and military forces aligned (or aligned the most) with the Catholic Church (the City of God) in order to oppose by all means—including military—those governments, political/ideological movements and military forces aligned (or aligned the most) with the Devil (the City of Devil).\n\nIn other cases, any obvious theme may be lacking. Some universal chronicles bear a more or less encyclopedic character, with many digressions on non-historical subjects, as is the case with the \"Chronicon\" of Helinand of Froidmont.\n\nOther notable universal chroniclers of the Medieval West include Bede (c. 672 or 673–735), the Christherre-Chronik, Helinand of Froidmont (c. 1160—after 1229), Isidore of Seville (c. 560–636), Jans der Enikel, Matthew Paris (c. 1200-1259), Ranulf Higdon (c. 1280-1363), Rudolf von Ems, Sigebert of Gembloux (c. 1030–1112), Otto von Freising (c. 1114–1158), and Vincent of Beauvais (c. 1190-1264?).\n\nThe tradition of universal history can even be seen in the works of medieval historians whose purpose may not have been to chronicle the ancient past, but nonetheless included it in a local history of more recent times. One such example is the \"Decem Libri Historiarum\" of Gregory of Tours (d. 594), where only the first of his ten books describes creation and ancient history, while the last six books focus on events in his own lifetime and region. While this reading of Gregory is currently a widely accepted hypothesis in historical circles, the central purpose of Gregory's writing is still a topic of hot debate.\n\nIn the medieval Islamic world (13th century), universal history in this vein was taken up by Muslim historians such as Tarikh-i Jahangushay-i Juvaini (\"The History of The World Conqueror\") by Ala'iddin Ata-Malik Juvayni, Jami al-Tawarikh (\"Compendium of Chronicles\") by Rashid-al-Din Hamadani (now held at the University of Edinburgh) and the \"Muqaddimah\" by Ibn Khaldun.\n\nAn early European project was the \"Universal History\" of George Sale and others, written in the mid-18th century.\n\nChristian writers as late as Bossuet in his \"Discours sur l'histoire universelle\" (Speech of Universal History) are still reflecting on and continuing the Medieval tradition of universal history.\nSpeech of Universal History is considered by many Catholics as an actual second edition or continuation of the City of God. In this work Bossuet continues to provide an update of universal history according to Augustine’s thesis of universal war between those humans that follow God and those who follow the Devil. This concept of world history guided by Divine Providence in a universal war between God and Devil is part of the official doctrine of the Catholic Church as most recently stated in the Second Vatican Council' s Gaudium et Spes document: \"The Church . . . holds that in her most benign Lord and Master can be found the key, the focal point and the goal of man, as well as of all human history...all of human life, whether individual or collective, shows itself to be a dramatic struggle between good and evil, between light and darkness...The Lord is the goal of human history the focal point of the longings of history and of civilization, the center of the human race, the joy of every heart and the answer to all its yearnings.\"\n\nIn the 19th century, universal histories proliferated.\n\nPhilosophers such as Kant, Herder, Schiller and Hegel, and political philosophers such as Marx and Herbert Spencer, presented general theories of history that shared essential characteristics with the Biblical account: they conceived of history as a coherent whole, governed by certain basic characteristics or immutable principles. Kant who was one of the earliest thinkers to use the term \"Universal History\" described its meaning in \"Idea for a Universal History with a Cosmopolitan Purpose\":\nAncient history is the study of the past from the beginning of recorded human history to the Early Middle Ages. In India, the period includes the early period of the Middle Kingdoms, and, in China, the time up to the Qin Dynasty is included.\n\nThe Bronze Age forms part of the three-age system. In this system, it follows the Neolithic Age in some areas of the world. In the 24th century BC, the Akkadian Empire was founded. The First Intermediate Period of Egypt (c. 22nd century BC) was followed by the Middle Kingdom of Egypt between the 21st to 17th centuries BC. The Sumerian Renaissance also developed c. 21st century BC. Around the 18th century BC, the Second Intermediate Period of Egypt began. By 1600 BC, Mycenaean Greece developed, the beginning of the Shang Dynasty in China emerged and there was evidence of a fully developed Chinese writing system. Also around 1600 BC, the beginning of Hittite dominance of the Eastern Mediterranean region is seen. From the 16th to 11th centuries BC the New Kingdom of Egypt dominated the Nile Valley. Between 1550 BC and 1292 BC, the Amarna Period developed.\n\nThe Iron Age is the last principal period in the three-age system, preceded by the Bronze Age. Its date and context vary depending on the country or geographical region.\nDuring the 13th to 12th centuries BC, the Ramesside Period occurred in Egypt. Around c. 1200 BC, the Trojan War was thought to have taken place. By c. 1180 BC, the disintegration of the Hittite Empire was underway.\n\nIn 1046 BC, the Zhou force, led by King Wu of Zhou, overthrows the last king of the Shang Dynasty. The Zhou Dynasty is established in China shortly thereafter. In 1000 BC, the Mannaeans Kingdom begins in Western Asia. Around the 10th to 7th centuries BC, the Neo-Assyrian Empire forms in Mesopotamia. In 800 BC, the rise of Greek city-states begins. In 776 BC, the first recorded Olympic Games are held.\n\nThe post-classical era, also known as the Middle Ages, is a historical period following the Iron Age, fully underway by the 5th century and lasting to the 15th century, and preceding the early Modern Era. The medieval history is the middle period, or the middle age, in a three-period division of history: Classic, Medieval, and Modern. The precise dates of the beginning, culmination, and end of the medieval history are more or less arbitrarily assumed according to the point of view adopted. Any hard and fast line drawn to designate either the beginning or close of the period in question is arbitrary. The widest limits given, viz., the irruption of the Visigoths over the boundaries of the Roman Empire, for the beginning, and the Middle Ages of the 16th century, for the close, may be taken as inclusively sufficient, and embrace, beyond dispute, every movement or phase of history that can be claimed as properly belonging to the medieval history.\n\nIn Europe, the period saw the large-scale European Migration and fall of the Western Roman Empire. In South Asia, the middle kingdoms of India were the classical period of the region. The \"Medieval\" period on the Indian subcontinent lasts for some 1,500 years, and ends in the 13th century. During the late medieval history, several Islamic empires were established in the Indian subcontinent. In East Asia, the Mid-Imperial China age begins with the reunification of China and ends with China was conquered by the Mongol Empire. The Golden Horde invaded North and West Asia and parts of eastern Europe in the 13th century and established and maintained their khanate until the end of the medieval history.\n\nThe Early medieval history saw the continuation of trends set up in ancient history (and, for Europe, late Antiquity). The period is usually considered to open with those migrations of the German Tribes which led to the destruction of the Roman Empire in the West in 375, when the Huns fell upon the Gothic tribes north of the Black Sea and forced the Visigoths over the boundaries of the Roman Empire on the lower Danube. A later date, however, is sometimes assumed, viz., when Odoacer deposed Romulus Augustulus, the last of the Roman Emperors of the West, in 476. Depopulation, deurbanization, and increased barbarian invasion were seen across the Old World. North Africa and the Middle East, once part of the Eastern Roman Empire, became Islamic. Later in European history, the establishment of the feudal system allowed a return to systemic agriculture. There was sustained urbanization in northern and western Europe.\n\nDuring the High medieval history in Europe, Christian-oriented art and architecture flourished and Crusades were mounted to recapture the Holy Land from Muslim control. The influence of the emerging states in Europe was tempered by the ideal of an international Christendom. The codes of chivalry and courtly love set rules for proper European behavior, while the European Scholastic philosophers attempted to reconcile Christian faith and reason.\n\nDuring the Late medieval history in Europe, the centuries of prosperity and growth came to a halt.\nThe close of the medieval history is also variously fixed; some make it coincide with the rise of Humanism and the Renaissance in Italy, in the 14th century; with the Fall of Constantinople, in 1453; with the discovery of America by Columbus in 1492; or, again, with the great religious schism of the 16th century. A series of famines and plagues, such as the medieval Great Famine and the Black Death, reduced the population around half before the calamities in the late medieval history. Along with depopulation came social unrest and endemic warfare. Western Europe experienced serious peasant risings: the Jacquerie, the Peasants' Revolt, and the Hundred Years' War. To add to the many problems of the period, the unity of the Catholic Church was shattered by the Western Schism. Collectively the events are a crisis of the Late medieval history.\n\nModern history describes the historical period after the Middle history. Modern history can be further broken down into the \"early modern period\" and the \"late modern period\" after the French Revolution and the Industrial Revolution. \"Contemporary history\" describes the span of historic events that are immediately relevant to the present time. The Great Divergence refers to the period of time in which the process by which the Western Europe and the parts of the New World overcame pre-modern growth constraints and emerged during the 19th century as the powerful and wealthy world civilization of the time, eclipsing Qing China, Mughal India, Tokugawa Japan, and the Ottoman Empire.\n\nThe modern era began approximately in the 16th century. Many major events caused Europe to change around the start of the 16th century, starting with the Fall of Constantinople in 1453, the fall of Muslim Spain and the discovery of the Americas in 1492, and Martin Luther's Protestant Reformation in 1517. In England the modern period is often dated to the start of the Tudor period with the victory of Henry VII over Richard III at the Battle of Bosworth in 1485. Early modern European history is usually seen to span from around the start of the 15th century, through the Age of Reason and the Age of Enlightenment in the 17th and 18th centuries, until the beginning of the Industrial Revolution in the late 18th century.\n\nThe modern era includes the early period, called the early modern period, which lasted from c. 1500 to around c. 1800 (most often 1815). Particular facets of early modernity include:\n\nThe early period ended in a time of political and economic change as a result of mechanization in society, the American Revolution, the first French Revolution; other factors included the redrawing of the map of Europe by the of the Congress of Vienna and the peace established by Second Treaty of Paris which ended the Napoleonic Wars.\n\nAs a result of the Industrial Revolutions and the earlier political revolutions, the worldviews of Modernism emerged. The industrialization of many nations was initiated with the industrialization of Britain. Particular facets of the late modernity period include:\n\nOther important events in the development of the Late modern period include:\n\nThe contemporary \"Great Divergence\" is a term given to a period starting in late 1970s when inequality grew substantially in the United States and to a lesser extent in other countries such as Canada and the United Kingdom. The term originated with Nobel laureate, Princeton economist and \"New York Times\" columnist Paul Krugman, and is a reference to the \"Great Compression\", an earlier era in the 1930s and 40s when income became dramatically more equal in the United States and elsewhere.\n\n\n\n\n"}
{"id": "15380061", "url": "https://en.wikipedia.org/wiki?curid=15380061", "title": "Water scarcity", "text": "Water scarcity\n\nWater scarcity is the lack of fresh water resources to meet water demand. It affects every continent and was listed in 2015 by the World Economic Forum as the largest global risk in terms of potential impact over the next decade. It is manifested by partial or no satisfaction of expressed demand, economic competition for water quantity or quality, disputes between users, irreversible depletion of groundwater, and negative impacts on the environment. One-third of the global population (2 billion people) live under conditions of severe water scarcity at least 1 month of the year. Half a billion people in the world face severe water scarcity all year round. Half of the world’s largest cities experience water scarcity.\n\nA mere 0.014% of all water on Earth is both fresh and easily accessible. Of the remaining water, 97% is saline and a little less than 3% is hard to access. Technically, there is a sufficient amount of freshwater on a global scale. However, due to unequal distribution (exacerbated by climate change) resulting in some very wet and some very dry geographic locations, plus a sharp rise in global freshwater demand in recent decades driven by industry, humanity is facing a water crisis. Demand is expected to outstrip supply by 40% in 2030, if current trends continue.\n\nThe essence of global water scarcity is the geographic and temporal mismatch between freshwater demand and availability. The increasing world population, improving living standards, changing consumption patterns, and expansion of irrigated agriculture are the main driving forces for the rising global demand for water. Climate change, such as altered weather-patterns (including droughts or floods), deforestation, increased pollution, green house gases, and wasteful use of water can cause insufficient supply. At the global level and on an annual basis, enough freshwater is available to meet such demand, but spatial and temporal variations of water demand and availability are large, leading to (physical) water scarcity in several parts of the world during specific times of the year. All causes of water scarcity are related to human interference with the water cycle. Scarcity varies over time as a result of natural hydrological variability, but varies even more so as a function of prevailing economic policy, planning and management approaches. Scarcity can be expected to intensify with most forms of economic development, but, if correctly identified, many of its causes can be predicted, avoided or mitigated.\n\nSome countries have already proven that decoupling water use from economic growth is possible. For example, in Australia, water consumption declined by 40% between 2001 and 2009 while the economy grew by more than 30%. The International Resource Panel of the UN states that governments have tended to invest heavily in largely inefficient solutions: mega-projects like dams, canals, aqueducts, pipelines and water reservoirs, which are generally neither environmentally sustainable nor economically viable. The most cost-effective way of decoupling water use from economic growth, according to the scientific panel, is for governments to create holistic water management plans that take into account the entire water cycle: from source to distribution, economic use, treatment, recycling, reuse and return to the environment.\n\nThe total amount of easily accessible freshwater on Earth, in the form of surface water (rivers and lakes) or groundwater (in aquifers, for example), is 14.000 cubic kilometres (nearly 3359 cubic miles). Of this total amount, 'just' 5.000 cubic kilometres are being used and reused by humanity. Hence, in theory, there is more than enough freshwater available to meet the demands of the current world population of 7 billion people, and even support population growth to 9 billion or more. Due to the unequal geographical distribution and especially the unequal consumption of water, however, it is a scarce resource in some parts of the world and for some parts of the population.\n\nScarcity as a result of consumption is caused primarily by the extensive use of water in agriculture/livestock breeding and industry. People in developed countries generally use about 10 times more water daily than those in developing countries. A large part of this is \"indirect use\" in water-intensive agricultural and industrial production processes of consumer goods, such as fruit, oil seed crops and cotton. Because many of these production chains have been globalised, a lot of water in developing countries is being used and polluted in order to produce goods destined for consumption in developed countries.\n\nWater scarcity can result from two mechanisms:\n\n\nPhysical water scarcity results from inadequate natural water resources to supply a region's demand, and economic water scarcity results from poor management of the sufficient available water resources. According to the United Nations Development Programme, the latter is found more often to be the cause of countries or regions experiencing water scarcity, as most countries or regions have enough water to meet household, industrial, agricultural, and environmental needs, but lack the means to provide it in an accessible manner.\nAround one fifth of the world's population currently live in regions affected by Physical water scarcity, where there is inadequate water resources to meet a country's or regional demand, including the water needed to fulfill the demand of ecosystems to function effectively. Arid regions frequently suffer from physical water scarcity. It also occurs where water seems abundant but where resources are over-committed, such as when there is over development of hydraulic infrastructure for irrigation. Symptoms of physical water scarcity include environmental degradation and declining groundwater as well as other forms of exploitation or overuse.\n\nEconomic water scarcity is caused by a lack of investment in infrastructure or technology to draw water from rivers, aquifers or other water sources, or insufficient human capacity to satisfy the demand for water. One quarter of the world's population is affected by economic water scarcity. Economic water scarcity includes a lack of infrastructure, causing the people without reliable access to water to have to travel long distances to fetch water, that is often contaminated from rivers for domestic and agricultural uses. Large parts of Africa suffer from economic water scarcity; developing water infrastructure in those areas could therefore help to reduce poverty. Critical conditions often arise for economically poor and politically weak communities living in already dry environment. Consumption increases with GDP per capita in most developed countries the average amount is around 200–300 litres daily. In underdeveloped countries (e.g. African countries such as Mozambique), average daily water consumption per capita was below 10 L. This is against the backdrop of international organisations, which recommend a minimum of 20 L of water (not including the water needed for washing clothes), available at most 1 km from the household. Increased water consumption is correlated with increasing income, as measured by GDP per capita. In countries suffering from water shortages water is the subject of speculation.\n\nThe United Nations Committee on Economic, Social and Cultural Rights established a foundation of five core attributes for water security. They declare that the human right to water entitles everyone to sufficient, safe, acceptable, physically accessible, and affordable water for personal and domestic use.\n\nAt the 2000 Millennium Summit, the United Nations addressed the effects of economic water scarcity by making increased access to safe drinking water an international development goal. During this time, they drafted the Millennium Development Goals and all 189 UN members agreed on eight goals. MDG 7 sets a target for reducing the proportion of the population without sustainable safe drinking water access by half by 2015. This would mean that more than 600 million people would gain access to a safe source of drinking water. In 2016, the Sustainable Development Goals replaced the Millennium Development Goals\n\nWater scarcity has many negative impacts on the environment, including lakes, rivers, wetlands and other fresh water resources. The resulting water overuse that is related to water scarcity, often located in areas of irrigation agriculture, harms the environment in several ways including increased salinity, nutrient pollution, and the loss of floodplains and wetlands. Furthermore, water scarcity makes flow management in the rehabilitation of urban streams problematic.\n\nThrough the last hundred years, more than half of the Earth's wetlands have been destroyed and have disappeared. These wetlands are important not only because they are the habitats of numerous inhabitants such as mammals, birds, fish, amphibians, and invertebrates, but they support the growing of rice and other food crops as well as provide water filtration and protection from storms and flooding. Freshwater lakes such as the Aral Sea in central Asia have also suffered. Once the fourth largest freshwater lake, it has lost more than 58,000 square km of area and vastly increased in salt concentration over the span of three decades.\n\nSubsidence, or the gradual sinking of landforms, is another result of water scarcity. The U.S. Geological Survey estimates that subsidence has affected more than 17,000 square miles in 45 U.S. states, 80 percent of it due to groundwater usage. In some areas east of Houston, Texas the land has dropped by more than nine feet due to subsidence. Brownwood, a subdivision near Baytown, Texas, was abandoned due to frequent flooding caused by subsidence and has since become part of the Baytown Nature Center.\n\nAquifer drawdown or overdrafting and the pumping of fossil water increases the total amount of water within the hydrosphere subject to transpiration and evaporation processes, thereby causing accretion in water vapour and cloud cover, the primary absorbers of infrared radiation in the earth's atmosphere. Adding water to the system has a forcing effect on the whole earth system, an accurate estimate of which hydrogeological fact is yet to be quantified.\n\nApart from the conventional surface water sources of freshwater such as rivers and lakes, other resources of freshwater such as groundwater and glaciers have become more developed sources of freshwater, becoming the main source of clean water. Groundwater is water that has pooled below the surface of the Earth and can provide a usable quantity of water through springs or wells. These areas where groundwater is collected are also known as aquifers. Glaciers provide freshwater in the form meltwater, or freshwater melted from snow or ice, that supply streams or springs as temperatures rise. More and more of these sources are being drawn upon as conventional sources' usability decreases due to factors such as pollution or disappearance due to climate changes. The exponential growth rate of the human population is a main contributing factor in the increasing use of these types of water resources.\n\nUntil recent 2015, groundwater was not a highly utilized resource. In the 1960s, more and more groundwater aquifers developed. Changes in knowledge, technology and funding have allowed for focused development into abstracting water from groundwater resources away from surface water resources. These changes allowed for progress in society such as the \"agricultural groundwater revolution\", expanding the irrigation sector allowing for increased food production and development in rural areas. Groundwater supplies nearly half of all drinking water in the world. The large volumes of water stored underground in most aquifers have a considerable buffer capacity allowing for water to be withdrawn during periods of drought or little rainfall. This is crucial for people that live in regions that cannot depend on precipitation or surface water as a supply alone, instead providing reliable access to water all year round. As of 2010, the world's aggregated groundwater abstraction is estimated at approximately 1,000 kmper year, with 67% used for irrigation, 22% used for domestic purposes and 11% used for industrial purposes. The top ten major consumers of abstracted water (India, China, United States of America, Pakistan, Iran, Bangladesh, Mexico, Saudi Arabia, Indonesia, and Italy) make up 72% of all abstracted water use worldwide. Groundwater has become crucial for the livelihoods and food security of 1.2 to 1.5 billion rural households in the poorer regions of Africa and Asia.\n\nAlthough groundwater sources are quite prevalent, one major area of concern is the renewal rate or recharge rate of some groundwater sources. Abstracting from groundwater sources that are non-renewable could lead to exhaustion if not properly monitored and managed. Another concern of increased groundwater usage is the diminished water quality of the source over time. Reduction of natural outflows, decreasing stored volumes, declining water levels and water degradation are commonly observed in groundwater systems. Groundwater depletion may result in many negative effects such as increased cost of groundwater pumping, induced salinity and other water quality changes, land subsidence, degraded springs and reduced baseflows. Human pollution is also harmful to this important resource.\n\nTo set up a big plant near a water abundant area, bottled water companies need to extract groundwater from a source at a rate more than the replenishment rate leading to the persistent decline in the groundwater levels. The groundwater is taken out, bottled, and then shipped all over the country or world and this water never goes back. When the water table depletes beyond a critical limit, bottling companies just move from that area leaving a grave water scarcity. Groundwater depletion impacts everyone and everything in the area who uses water: farmers, businesses, animals, ecosystems, tourism, and the regular guy getting his water from a well. Millions of gallons of water out of the ground leaves the water table depleted uniformly and not just in that area because the water table is connected across the landmass. Bottling Plants generate water scarcity and impact ecological balance. They lead to water stressed areas which bring in droughts.\n\nGlaciers are noted as a vital water source due to their contribution to stream flow. Rising global temperatures have noticeable effects on the rate at which glaciers melt, causing glaciers in general to shrink worldwide. Although the meltwater from these glaciers are increasing the total water supply for the present, the disappearance of glaciers in the long term will diminish available water resources. Increased meltwater due to rising global temperatures can also have negative effects such as flooding of lakes and dams and catastrophic results.\n\nHydrologists today typically assess water scarcity by looking at the population-water equation. This is done by comparing the amount of total available water resources per year to the population of a country or region. A popular approach to measuring water scarcity has been to rank countries according to the amount of annual water resources available per person. For example, according to the Falkenmark Water Stress Indicator, a country or region is said to experience \"water stress\" when annual water supplies drop below 1,700 cubic metres per person per year. At levels between 1,700 and 1,000 cubic metres per person per year, periodic or limited water shortages can be expected. When water supplies drop below 1,000 cubic metres per person per year, the country faces \"water scarcity\". The United Nations' FAO states that by 2025, 1.9 billion people will live in countries or regions with absolute water scarcity, and two-thirds of the world population could be under stress conditions. The World Bank adds that climate change could profoundly alter future patterns of both water availability and use, thereby increasing levels of water stress and insecurity, both at the global scale and in sectors that depend on water.\n\nOther ways of measuring water scarcity include examining the physical existence of water in nature, comparing nations with lower or higher volumes of water available for use. This method often fails to capture the accessibility of the water resource to the population that may need it. Others have related water availability to population.\n\nAnother measurement, calculated as part of a wider assessment of water management in 2007, aimed to relate water availability to how the resource was actually used. It therefore divided water scarcity into 'physical' and 'economic'. Physical water scarcity is where there is not enough water to meet all demands, including that needed for ecosystems to function effectively. Arid regions frequently suffer from physical water scarcity. It also occurs where water seems abundant but where resources are over-committed, such as when there is overdevelopment of hydraulic infrastructure for irrigation. Symptoms of physical water scarcity include environmental degradation and declining groundwater. Water stress harms living things because every organism needs water to live.\n\nRenewable freshwater supply is a metric often used in conjunction when evaluating water scarcity. This metric is informative because it can describe the total available water resource each country contains. By knowing the total available water source, an idea can be gained about whether a country is prone to experiencing physical water scarcity. This metric has its faults in that it is an average; precipitation delivers water unevenly across the planet each year and annual renewable water resources vary from year to year. This metric also does not describe the accessibility of water to individuals, households, industries, or the government. Lastly, as this metric is a description of a whole country, it does not accurately portray whether a country is experiencing water scarcity. Canada and Brazil both have very high levels of available water supply, but still experience various water related problems.\n\nIt can be observed that tropical countries in Asia and Africa have low availability of freshwater resources.\n\nThe following table displays the average annual renewable freshwater supply by country including both surface-water and groundwater supplies. This table represents data from the UN FAO AQUASTAT, much of which are produced by modeling or estimation as opposed to actual measurements.\n\nThe United Nations (UN) estimates that, of 1.4 billion cubic kilometers (1 quadrillion acre-feet) of water on Earth, just 200,000 cubic kilometers (162.1 billion acre-feet) represent fresh water available for human consumption.\n\nMore than one in every six people in the world is water stressed, meaning that they do not have sufficient access to potable water. Those that are water stressed make up 1.1 billion people in the world and are living in developing countries. According to the Falkenmark Water Stress Indicator, a country or region is said to experience \"water stress\" when annual water supplies drop below 1,700 cubic metres per person per year. At levels between 1,700 and 1,000 cubic meters per person per year, periodic or limited water shortages can be expected. When a country is below 1,000 cubic meters per person per year, the country then faces water scarcity . In 2006, about 700 million people in 43 countries were living below the 1,700 cubic metres per person threshold. Water stress is ever intensifying in regions such as China, India, and Sub-Saharan Africa, which contains the largest number of water stressed countries of any region with almost one fourth of the population living in a water stressed country. The world's most water stressed region is the Middle East with averages of 1,200 cubic metres of water per person. In China, more than 538 million people are living in a water-stressed region. Much of the water stressed population currently live in river basins where the usage of water resources greatly exceed the renewal of the water source.\n\nAnother popular opinion is that the amount of available freshwater is decreasing because of climate change. Climate change has caused receding glaciers, reduced stream and river flow, and shrinking lakes and ponds. Many aquifers have been over-pumped and are not recharging quickly. Although the total fresh water supply is not used up, much has become polluted, salted, unsuitable or otherwise unavailable for drinking, industry and agriculture. To avoid a global water crisis, farmers will have to strive to increase productivity to meet growing demands for food, while industry and cities find ways to use water more efficiently.\n\nA New York Times article, \"Southeast Drought Study Ties Water Shortage to Population, Not Global Warming\", summarizes the findings of Columbia University researcher on the subject of the droughts in the American Southeast between 2005 and 2007. The findings published in the \"Journal of Climate\" say that the water shortages resulted from population size more than rainfall. Census figures show that Georgia’s population rose from 6.48 to 9.54 million between 1990 and 2007. After studying data from weather instruments, computer models, and tree ring measurements, they found that the droughts were not unprecedented and result from normal climate patterns and random weather events. \"Similar droughts unfolded over the last thousand years\", the researchers wrote, \"Regardless of climate change, they added, similar weather patterns can be expected regularly in the future, with similar results.\" As the temperature increases, rainfall in the Southeast will increase but because of evaporation the area may get even drier. The researchers concluded with a statement saying that any rainfall comes from complicated internal processes in the atmosphere and are very hard to predict because of the large amount of variables.\n\nWhen there is not enough potable water for a given population, the threat of a \"water crisis\" is realized.\nThe United Nations and other world organizations consider a variety of regions to have water crises of global concern. Other organizations, such as the Food and Agriculture Organization, argue that there are no water crises in such places, but steps must still be taken to avoid one.\n\nThere are several principal manifestations of the water crisis.\n\nWaterborne diseases caused by lack of sanitation and hygiene are one of the leading causes of death worldwide. For children under age five, waterborne diseases are a leading cause of death. According to the World Bank, 88 percent of all waterborne diseases are caused by unsafe drinking water, inadequate sanitation and poor hygiene.\n\nWater is the underlying tenuous balance of safe water supply, but controllable factors such as the management and distribution of the water supply itself contribute to further scarcity.\n\nA 2006 United Nations report focuses on issues of governance as the core of the water crisis, saying \"There is enough water for everyone\" and \"Water insufficiency is often due to mismanagement, corruption, lack of appropriate institutions, bureaucratic inertia and a shortage of investment in both human capacity and physical infrastructure\". Official data also shows a clear correlation between access to safe water and GDP per capita.\n\nIt has also been claimed, primarily by economists, that the water situation has occurred because of a lack of property rights, government regulations and subsidies in the water sector, causing prices to be too low and consumption too high.\n\nVegetation and wildlife are fundamentally dependent upon adequate freshwater resources. Marshes, bogs and riparian zones are more obviously dependent upon sustainable water supply, but forests and other upland ecosystems are equally at risk of significant productivity changes as water availability is diminished. In the case of wetlands, considerable area has been simply taken from wildlife use to feed and house the expanding human population. But other areas have suffered reduced productivity from gradual diminishing of freshwater inflow, as upstream sources are diverted for human use. In seven states of the U.S. over 80 percent of all historic wetlands were filled by the 1980s, when Congress acted to create a \"no net loss\" of wetlands.\n\nIn Europe extensive loss of wetlands has also occurred with resulting loss of biodiversity. For example, many bogs in Scotland have been developed or diminished through human population expansion. One example is the Portlethen Moss in Aberdeenshire.\n\nOn Madagascar's highland plateau, a massive transformation occurred that eliminated virtually all the heavily forested vegetation in the period 1970 to 2000. The slash and burn agriculture eliminated about ten percent of the total country's native biomass and converted it to a barren wasteland. These effects were from overpopulation and the necessity to feed poor indigenous peoples, but the adverse effects included widespread gully erosion that in turn produced heavily silted rivers that \"run red\" decades after the deforestation. This eliminated a large amount of usable fresh water and also destroyed much of the riverine ecosystems of several large west-flowing rivers. Several fish species have been driven to the edge of extinction and some, such as the disturbed Tokios coral reef formations in the Indian Ocean, are effectively lost.\nIn October 2008, Peter Brabeck-Letmathe, chairman and former chief executive of Nestlé, warned that the production of biofuels will further deplete the world's water supply.\n\nThere are many other countries of the world that are severely impacted with regard to human health and inadequate drinking water. The following is a partial list of some of the countries with significant populations (numerical population of affected population listed) whose only consumption is of contaminated water:\n\nSeveral world maps showing various aspects of the problem can be found in this graph article.\n\nWater deficits, which are already spurring heavy grain imports in numerous smaller countries, may soon do the same in larger countries, such as China and India. The water tables are falling in scores of countries (including Northern China, the US, and India) due to widespread overpumping using powerful diesel and electric pumps. Other countries affected include Pakistan, Iran, and Mexico. This will eventually lead to water scarcity and cutbacks in grain harvest. Even with the overpumping of its aquifers, China is developing a grain deficit. When this happens, it will almost certainly drive grain prices upward. Most of the 3 billion people projected to be added worldwide by mid-century will be born in countries already experiencing water shortages. Unless population growth can be slowed quickly, it is feared that there may not be a practical non-violent or humane solution to the emerging world water shortage.\n\nAfter China and India, there is a second tier of smaller countries with large water deficits — Algeria, Egypt, Iran, Mexico, and Pakistan.\n\nAccording to a UN climate report, the Himalayan glaciers that are the sources of Asia's biggest rivers – Ganges, Indus, Brahmaputra, Yangtze, Mekong, Salween and Yellow – could disappear by 2035 as temperatures rise. It was later revealed that the source used by the UN climate report actually stated 2350, not 2035. Approximately 2.4 billion people live in the drainage basin of the Himalayan rivers. India, China, Pakistan, Bangladesh, Nepal and Myanmar could experience floods followed by droughts in coming decades. In India alone, the Ganges provides water for drinking and farming for more than 500 million people. The west coast of North America, which gets much of its water from glaciers in mountain ranges such as the Rocky Mountains and Sierra Nevada, also would be affected.\n\nBy far the largest part of Australia is desert or semi-arid lands commonly known as the outback. In June 2008 it became known that an expert panel had warned of long term, possibly irreversible, severe ecological damage for the whole Murray-Darling basin if it does not receive sufficient water by October. Water restrictions are currently in place in many regions and cities of Australia in response to chronic shortages resulting from drought. The Australian of the year 2007, environmentalist Tim Flannery, predicted that unless it made drastic changes, Perth in Western Australia could become the world’s first ghost metropolis, an abandoned city with no more water to sustain its population. However, Western Australia's dams reached 50% capacity for the first time since 2000 as of September 2009. As a result, heavy rains brought forth positive results for the region. Nonetheless, the following year, 2010, Perth suffered its second-driest winter on record and the water corporation tightened water restrictions for spring.\n\nConstruction of wastewater treatment plants and reduction of groundwater overdrafting appear to be obvious solutions to the worldwide problem; however, a deeper look reveals more fundamental issues in play. Wastewater treatment is highly capital intensive, restricting access to this technology in some regions; furthermore the rapid increase in population of many countries makes this a race that is difficult to win. As if those factors are not daunting enough, one must consider the enormous costs and skill sets involved to maintain wastewater treatment plants even if they are successfully developed.\n\nReducing groundwater overdrafting is usually politically unpopular, and can have major economic impacts on farmers. Moreover, this strategy necessarily reduces crop output, something the world can ill-afford given the current population.\n\nAt more realistic levels, developing countries can strive to achieve primary wastewater treatment or secure septic systems, and carefully analyse wastewater outfall design to minimize impacts to drinking water and to ecosystems. Developed countries can not only share technology better, including cost-effective wastewater and water treatment systems but also in hydrological transport modeling. At the individual level, people in developed countries can look inward and reduce over consumption, which further strains worldwide water consumption. Both developed and developing countries can increase protection of ecosystems, especially wetlands and riparian zones. There measures will not only conserve biota, but also render more effective the natural water cycle flushing and transport that make water systems more healthy for humans.\n\nA range of local, low-tech solutions are being pursued by a number of companies. These efforts center around the use of solar power to distill water at temperatures slightly beneath that at which water boils. By developing the capability to purify any available water source, local business models could be built around the new technologies, accelerating their uptake. For example, Bedouins from the town of Dahab in Egypt have installed Aqua Danial's Water Stellar, which uses a solar thermal collector measuring two square meters to distill from 40 to 60 liters per day from any local water source. This is five times more efficient than conventional stills and eliminates the need for polluting plastic PET bottles or transportation of water supply.\n\nIt is alleged that the likelihood of conflict rises if the rate of change within the basin exceeds the capacity of institution to absorb that change. Although water crisis is closely related to regional tensions, history showed that acute conflicts over water are far less than the record of cooperation.\n\nThe key lies in strong institutions and cooperation. The Indus River Commission and the Indus Water Treaty survived two wars between India and Pakistan despite their hostility, proving to be a successful mechanism in resolving conflicts by providing a framework for consultation inspection and exchange of data. The Mekong Committee has also functioned since 1957 and survived the Vietnam War. In contrast, regional instability results when there is an absence of institutions to co-operate in regional collaboration, like Egypt's plan for a high dam on the Nile. However, there is currently no global institution in place for the management and management of trans-boundary water sources, and international co-operation has happened through ad hoc collaborations between agencies, like the Mekong Committee which was formed due to an alliance between UNICEF and the US Bureau of Reclamation. Formation of strong international institutions seems to be a way forward – they fuel early intervention and management, preventing the costly dispute resolution process.\n\nOne common feature of almost all resolved disputes is that the negotiations had a \"need-based\" instead of a \"right–based\" paradigm. Irrigable lands, population, technicalities of projects define \"needs\". The success of a need-based paradigm is reflected in the only water agreement ever negotiated in the Jordan River Basin, which focuses in needs not on rights of riparians. In the Indian subcontinent, irrigation requirements of Bangladesh determine water allocations of the Ganges River. A need-based, regional approach focuses on satisfying individuals with their need of water, ensuring that minimum quantitative needs are being met. It removes the conflict that arises when countries view the treaty from a national interest point of view, move away from the zero-sum approach to a positive sum, integrative approach that equitably allocated the water and its benefits.\n\nThe Blue Peace framework developed by Strategic Foresight Group in partnership with the Governments of Switzerland and Sweden offers a unique policy structure which promotes sustainable management of water resources combined with cooperation for peace. By making the most of shared water resources through cooperation rather than mere allocation between countries, the chances for peace can be increased. The Blue Peace approach has proven to be effective in cases like the Middle East and the Nile basin. NGOs like Water.org, There Is No Limit Foundation, and are leading the way in providing access to clean water.\n\n\n\n"}
{"id": "1536556", "url": "https://en.wikipedia.org/wiki?curid=1536556", "title": "World Radio TV Handbook", "text": "World Radio TV Handbook\n\nThe World Radio TV Handbook, also known as WRTH, is a directory of virtually every radio and TV station on Earth, published yearly. It was started in 1947 by Oluf Lund Johansen (1891–1975) as the \"World Radio Handbook\" (WRH). The word \"TV\" was added to the title in 1965, when Jens M. Frost (1919–1999) took over as editor. It had then already included data for television broadcasting for some years. After the 40th edition in 1986, Frost handed over editorship to Andrew G. (Andy) Sennitt.\n\nThe first edition that bears an edition number is the 4th edition, published in 1949. The three previous editions appear to have been:\n\nSummer Supplements appear to have been issued from 1959 through 1971. From 1959 through 1966 they were called the Summer Supplement. From 1967 through 1971 they were called the Summer Edition.\n\nThrough the 1969 edition, the WRTH indicated the date on which the manuscript was completed.\n\nIssues with covers in Danish are known to have been available for the years 1948 May-November (2d ed.), 1950-51 (5th ed.; cover and 1st page in Danish, rest in English, most ads in Danish), 1952 (6th ed.; cover and 1st page in Danish, rest in English, most ads in Danish), and probably others. The 1952 English ed., which is completely in English, has an extra page with world times and agents, and ads in English which are sometimes different from the ads in the Danish edition. Also, the 1953 ed. mentions the availability of a German edition.\n\nOluf Lund Johansen published, in conjunction with Libreria Hispanoamericana of Barcelona, Spain, a softbound Spanish-language version of the 1960 WRTH. The book was printed in Spain and called \"Guia Mundial de Radio y Television\", and carried the WRTH logo at the time as well as all the editorial references contained in the English-language version. \n\nHardbound editions are known to have been available for the years 1963 through 1966, 1968, 1969, and 1975-1978, and probably others.\n\n\n"}
{"id": "227630", "url": "https://en.wikipedia.org/wiki?curid=227630", "title": "World economy", "text": "World economy\n\nThe world economy or global economy is the economy of the world, considered as the international exchange of goods and services that is expressed in monetary units of account. In some contexts, the two terms are distinguished: the \"international\" or \"global economy\" being measured separately and distinguished from national economies while the \"world economy\" is simply an aggregate of the separate countries' measurements. Beyond the minimum standard concerning value in production, use and exchange the definitions, representations, models and valuations of the world economy vary widely. It is inseparable from the geography and ecology of Earth.\n\nIt is common to limit questions of the world economy exclusively to human economic activity and the world economy is typically judged in monetary terms, even in cases in which there is no efficient market to help valuate certain goods or services, or in cases in which a lack of independent research or government cooperation makes establishing figures difficult. Typical examples are illegal drugs and other black market goods, which by any standard are a part of the world economy, but for which there is by definition no legal market of any kind.\n\nHowever, even in cases in which there is a clear and efficient market to establish a monetary value, economists do not typically use the current or official exchange rate to translate the monetary units of this market into a single unit for the world economy since exchange rates typically do not closely reflect worldwide value, for example in cases where the volume or price of transactions is closely regulated by the government.\nRather, market valuations in a local currency are typically translated to a single monetary unit using the idea of purchasing power. This is the method used below, which is used for estimating worldwide economic activity in terms of real United States dollars or euros. However, the world economy can be evaluated and expressed in many more ways. It is unclear, for example, how many of the world's 7.62 billion people have most of their economic activity reflected in these valuations.\n\nAccording to Maddison, until the middle of 19th century, global output was dominated by China and India. Waves of Industrial Revolution in Western Europe and Northern America shifted the shares to the Western Hemisphere. As of 2017, the following 15 countries or regions have reached an economy of at least US$2 trillion by GDP in nominal or PPP terms: Brazil, China, India, Germany, France, Indonesia, Italy, Japan, South Korea, Mexico, Russia, Turkey, the United Kingdom, the United States and the European Union.\n\nThe following two tables list the country groups with individual countries designated by the IMF. Members of the G-20 major economies are in bold.\n\nThe following two tables list the 25 largest economies by GDP (nominal), twenty largest economies by GDP (PPP). Members of the G-20 major economies are in bold.\n\nThe following is a list of the twenty largest economies by nominal GDP at peak value as of the specific year according to International Monetary Fund.\n\nThe following is a list of twenty largest economies by GDP based on purchasing power parity at peak value as of the specific year according to the International Monetary Fund and the \"CIA World Factbook\".\n\n\n\n\n\n\n\nTelephones – main lines in use: 843,923,500 (2007)4,263,367,600 (2008)\n\nTransportation infrastructure worldwide includes:\n\n\nTo promote exports, many government agencies publish on the web economic studies by sector and country. Among these agencies include the USCS (US DoC) and FAS (USDA) in the United States, EDC and AAFC in Canada, Ubifrance in France, UKTI in the UK, HKTDC and JETRO in Asia, Austrade and NZTE in Oceania. Through Partnership Agreements, the Federation of International Trade Associations publishes studies from several of these agencies (USCS, FAS, AAFC, UKTI and HKTDC) as well as other non-governmental organizations on its website GlobalTrade.net.\n\n\nRegional economies:\n\nEvents:\n\nLists:\n\n"}
{"id": "33666830", "url": "https://en.wikipedia.org/wiki?curid=33666830", "title": "World population milestones", "text": "World population milestones\n\nWorld population milestones were unnoticed until the 20th century, since there were no reliable data on global population dynamics.\n\nIt is estimated that the population of the world reached one billion for the first time in 1804. It would be another 123 years before it reached two billion in 1927, but it took only 33 years to rise by another billion people, reaching three billion in 1960. Thereafter, the global population reached four billion in 1974, five billion in 1987, six billion in 1999 and, by some estimates, seven billion in October 2011 with other estimates being in March 2012. It is projected to reach eight billion by 2024–2030. According to current projections, the world's population is likely to reach around nine billion by 2035–2050, with alternative scenarios ranging from a low of 7.4 billion to a high of more than 10.6 billion. Projected figures vary depending on underlying statistical assumptions and which variables are manipulated in projection calculations, especially the fertility variable. Long-range predictions to 2150 range from a population decline to 3.2 billion in the\n'low scenario', to 'high scenarios' of 24.8 billion. One scenario predicted a massive increase to 256 billion by 2150, assuming fertility remains at 1995 levels.\n\nThere is no estimation for the exact day or month the world's population surpassed each of the one and two billion marks. The days of three and four billion were not officially noted, but the International Database of the United States Census Bureau places them in July 1959 and April 1974.\n\nThe Day of Five Billion, 11 July 1987, was designated by the United Nations Population Fund as the approximate day on which world population reached five billion. Matej Gašpar from Zagreb, Croatia (then SR Croatia, SFR Yugoslavia), was chosen as the symbolic 5-billionth person concurrently alive on Earth. The honor went to Zagreb because the 1987 Summer Universiade was taking place in the city at the time.\n\nThe United Nations Population Fund designated 12 October 1999 as the approximate day on which the world population reached six billion. It was officially designated The Day of Six Billion. Demographers do not universally accept this date as being exact. In fact there has been subsequent research which places the day of six billion nearer to 18 June or 19 June 1999. The International Programs division of the United States Census Bureau estimated that the world population reached six billion on 21 April 1999. United Nations Population Fund spokesman Omar Gharzeddine disputed the date of the Day of Six Billion by stating, \"The U.N. marked the '6 billionth' [person] in 1999, and then a couple of years later the Population Division itself reassessed its calculations and said, actually, no, it was in 1998.\"\n\nOn the Day of Six Billion, UN Secretary-General Kofi Annan was in Sarajevo, Bosnia and Herzegovina to monitor the Dayton Agreement. At midnight he went to Koševo Hospital, where Adnan Mević, born at 12.01 am, was named the symbolic 6 billionth concurrently alive person on Earth. He is the first son of Fatima Mević and Jasminko Mević and weighed 3.5 kg.\n\nThe \"Day of Seven Billion\" was targeted by the United States Census Bureau to be in March 2012, while the Population Division of the United Nations suggested 31 October 2011, and the latter date was officially designated by the United Nations Population Fund (UNFPA) as the approximate day on which the world's population reached seven billion people. United Nations Secretary General Ban Ki-moon spoke at the United Nations building in New York City on this milestone in the size of world population, and promoted the website 7 Billion Actions. Ban Ki-moon did not choose a symbolic seven billionth baby, but several groups proposed candidates: Nargis Kumar of Uttar Pradesh, India, Danica May Camacho of Manila, Philippines and Wattalage Muthumai of Colombo, Sri Lanka.\n\nNational or subnational governments have sometimes made similar designations based on the date estimated by a demographic agency. Some national milestones relate to citizens rather than residents. Commentators in countries with high immigration have pointed out that a population milestone may be reached by an immigrant rather than natural increase. \n\n"}
{"id": "28682776", "url": "https://en.wikipedia.org/wiki?curid=28682776", "title": "Zero world government", "text": "Zero world government\n\nZero world government refers to a hypothetical future in which national governments cease to exist or to matter, as a result of globalization and a worldwide rejection of politics, as known today. The term contrasts with that of one world government in the sense that rather than there being a world-state, there are no political states in the world.\n\n"}
