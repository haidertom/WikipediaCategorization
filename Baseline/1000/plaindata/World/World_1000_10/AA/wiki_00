{"id": "36880490", "url": "https://en.wikipedia.org/wiki?curid=36880490", "title": "BWF World Ranking", "text": "BWF World Ranking\n\nThe BWF World Ranking is the official ranking of the Badminton World Federation for badminton players who participate in tournaments sanctioned by Badminton World Federation. It is used to determine the qualification for the World Championships and Summer Olympic Games, as well as BWF World Tour tournaments. Seedings of draws at all BWF-sanctioned tournaments are conducted using the BWF World Ranking.\nPlayers under 19 years of age are eligible to rank in the BWF World Junior Ranking, which were introduced in January 2011. The following lists are the rankings:\n\nThe ranking points are awarded based on the level and progress of the tournament from each player/pair. Ranking points calculated are based on the tournaments each players/pairs participate in from the last 52 weeks. If a player or pair has participated in ten or fewer World Ranking tournaments, then the ranking is worked out by adding together the points won at tournaments in the last 52 weeks. If a player or pair has participated in 11 or more World Ranking tournaments, only the 10 highest points scored in the tournaments during the 52-week period count towards their ranking. The highest possible ranking points are 116,000.\n\nPoints were awarded according to the following table from 2007 to 2017:\n\nSince 2018, BWF has started a new system for counting points:\n\nThe following is a list of players who have achieved the number one position since 1 January 2009 (active players in purple, and current number 1 players are marked in bold):\n\n\"Last update: 29 November 2018\"\n\nThe following is a list of players who were ranked world no. 5 or higher but not no. 1 in the period since the introduction of the BWF computer rankings (active players in purple):\n\n\"Last update: 29 November 2018\"\n"}
{"id": "37968016", "url": "https://en.wikipedia.org/wiki?curid=37968016", "title": "Disease Control Priorities Project", "text": "Disease Control Priorities Project\n\nThe Disease Control Priorities Project (DCPP) is an ongoing project that aims to determine priorities for disease control across the world, particularly in low-income countries. The project is most well known for the second edition of the report \"Disease Control Priorities in Developing Countries\" (published in 2006, often abbreviated as \"DCP2\" and sometimes referred to as \"the DCP2 Report\").\n\nThe Disease Control Priorities Project is a joint enterprise of a number of groups, including the University of Washington Department of Global Health, the World Bank, the Fogarty International Center (National Institutes of Health), World Health Organization, Population Reference Bureau, Gates Foundation, and the International Decision Support Initiative. Notable editors involved in the project include Dean Jamison, Alan Lopez, Colin Mathers, Christopher J.L. Murray, George Alleyne, Prabhat Jha, and Anne Mills.\n\nThe first edition of \"Disease Control Priorities in Developing Countries\", commonly referred to as \"DCP1\", was published in 1993. \"DCP1\" is cited in the 1993 World Development Report.\n\n\"DCP1\" is organized into five parts:\n\n\nEach part has chapters within it; there are 29 chapters in all. The report spans more than 700 pages and has as contributors 79 authors in addition to the four editors.\n\nThe second edition of \"Disease Control Priorities in Developing Countries\", commonly referred to as \"DCP2\" and sometimes referred to as \"the DCP2 Report\", was published in 2006. \"DCP2\" is organized into 73 chapters, and is a 1400-page report by more than 350 specialists around the world with the goal of providing policy recommendations to reduce global disease burdens. The report is in English, but translations for some of the chapters to Arabic, Chinese, French, and Spanish are available. The report has been released under the Creative Commons attribution license (CC-BY) and a copy of \"DCP2\" can be downloaded from the World Bank's Open Knowledge Repository. The full text of the report can also be read online on the National Center for Biotechnology Information (National Institutes of Health) website.\n\nIn comparison to \"DCP1\", \"DCP2\" is more systematic in its coverage.\n\nFor third edition, the name of the report was shortened to \"Disease Control Priorities\". The third edition is commonly referred to as \"DCP3\", and is under preparation and is expected to be published over the time period 2015–2016. It has nine separate volumes. The final and summary volume will be published in 2016. Separate volumes on cancer and surgery will be published earlier. Toby Ord of Giving What We Can is on the board.\n\nAs of April 2016, four of nine volumes of \"DCP3\" have been published online.\n\nThe nine volumes are as follows:\n\n\nIn addition to \"DCP1\", \"DCP2\", and \"DCP3\", the DCPP has produced other background papers and major publications. These include the following:\n\n\n"}
{"id": "1065199", "url": "https://en.wikipedia.org/wiki?curid=1065199", "title": "Encyclopedia of Life Support Systems", "text": "Encyclopedia of Life Support Systems\n\nThe Encyclopedia of Life Support Systems (EOLSS) is an encyclopedia on the science of sustainable development and conservation of life support systems on earth. The extensive publication is published under the patronage of UNESCO.\n\nIn 1996, international scientists, engineers, and policy makers were invited by the UNESCO to Washington, Tokyo, Moscow, Mexico City, Beijing and the Bahamas to generate a detailed list of EOLSS contents and achieve a global consensus and acceptance of its structure. Life support system means a natural or human constructed system that furthers the life of the biosphere in a sustainable fashion. The knowledge is structured in 21 topic areas: Earth and atmospheric sciences; Mathematical sciences; Biological, physiological and health sciences; Biotechnology; Land use, land cover and soil sciences; Tropical biology and conservation management; Social sciences and humanities; Physical sciences, engineering and technology resources; Control systems, robotics and automation; Chemical sciences engineering and technology resources; Water sciences, engineering and technology resources; Energy sciences, engineering and technology resources; Environmental and ecological sciences, engineering and technology resources; Food and agricultural sciences, engineering and technology resources; Human resources policy, development and management; Natural resources policy and management; Development and economic sciences; Institutional and infrastructural resources; Technology, information and system management resources; Area studies (Africa, Brazil, Canada and USA, China, Europe, Japan, Russia); Desalination and water resources.\n\nThe way the encyclopedia is being compiled can be defined as crowdsourcing, where thousands of intellectuals from all over the world and across various academic institutions are contributing. The encyclopedia relies on strict standards of peer review and the joint Paris-based UNESCO and EOLSS secretariat manages the encyclopedia in progress. It has got a publishing wing in Oxford as well which specializes in publishing various materials such as books, e-books or scholarly subscriptions.\n\nThe web portal of EOLSS receives maximum hits from the United States and India. If each hit is assumed to originate from one unique user, then USA has about 28% and India has about 10% of the users of this portal. The number of hits falls dramatically after we consider USA and India, whereas countries like Australia, Canada, and the United Kingdom each contribute almost about 5% of the total hits. This shows that people familiar with English are feeling more comfortable in utilizing this project for their educational or knowledge seeking purposes and researches. The proportion of users from states of the European Union who do not speak English as native language is about 8 percent in total. The statistics are updated as of May 2018.\n\n1. Earth and Atmospheric Sciences\n2. Mathematical Sciences\n3. Biological, Physiological and Health Sciences\n4. Biotechnology\n5. Tropical Biology and Conservation Management\n6. Social Sciences and Humanities\n7. Physical Sciences, Engineering and Technology Resources\n8. Control Systems, Robotics and Automation\n9. Chemical Sciences, Engineering and Technology Resources\n10. Water Sciences, Engineering and Technology Resources\n11. Compendium of Desalination and Water Resources\n12. Energy Sciences, Engineering and Technology Resources\n13. Environmental and Ecological Sciences, Engineering and Technology Resources\n14. Food and Agricultural Sciences, Engineering and Technology Resources\n15. Human Resources Policy, Development and Management\n16. Natural Resources Policy and Management\n17. Development and Economic sciences\n18. Institutional and Infrastructural Resources\n19. Technology, Information and Sytems Management Resources\n20. Land Use, Land Cover and Soil Sciences\n21. Area Studies (Regional Sustainable Development Reviews)\n\n"}
{"id": "9239", "url": "https://en.wikipedia.org/wiki?curid=9239", "title": "Europe", "text": "Europe\n\nEurope is a continent located entirely in the Northern Hemisphere and mostly in the Eastern Hemisphere. It is bordered by the Arctic Ocean to the north, the Atlantic Ocean to the west and the Mediterranean Sea to the south. It comprises the westernmost part of Eurasia.\n\nSince around 1850, Europe is most commonly considered to be separated from Asia by the watershed divides of the Ural and Caucasus Mountains, the Ural River, the Caspian and Black Seas and the waterways of the Turkish Straits. Although the term \"continent\" implies physical geography, the land border is somewhat arbitrary and has moved since its first conception in classical antiquity. The division of Eurasia into two continents reflects East-West cultural, linguistic and ethnic differences, some of which vary on a spectrum rather than with a sharp dividing line. The border does not follow political boundaries, with Turkey, Russia and Kazakhstan being transcontinental countries.\n\nEurope covers about , or 2% of the Earth's surface (6.8% of land area). Politically, Europe is divided into about fifty sovereign states of which the Russian Federation is the largest and most populous, spanning 39% of the continent and comprising 15% of its population. Europe had a total population of about 741 million (about 11% of the world population) . The European climate is largely affected by warm Atlantic currents that temper winters and summers on much of the continent, even at latitudes along which the climate in Asia and North America is severe. Further from the sea, seasonal differences are more noticeable than close to the coast.\n\nEurope, in particular ancient Greece, was the birthplace of Western civilization. The fall of the Western Roman Empire in 476 AD and the subsequent Migration Period marked the end of ancient history and the beginning of the Middle Ages. Renaissance humanism, exploration, art and science led to the modern era. Since the Age of Discovery started by Portugal and Spain, Europe played a predominant role in global affairs. Between the 16th and 20th centuries, European powers controlled at various times the Americas, almost all of Africa and Oceania and the majority of Asia.\n\nThe Age of Enlightenment, the subsequent French Revolution and the Napoleonic Wars shaped the continent culturally, politically and economically from the end of the 17th century until the first half of the 19th century. The Industrial Revolution, which began in Great Britain at the end of the 18th century, gave rise to radical economic, cultural and social change in Western Europe and eventually the wider world. Both world wars took place for the most part in Europe, contributing to a decline in Western European dominance in world affairs by the mid-20th century as the Soviet Union and the United States took prominence. During the Cold War, Europe was divided along the Iron Curtain between NATO in the West and the Warsaw Pact in the East, until the revolutions of 1989 and fall of the Berlin Wall.\n\nIn 1949 the Council of Europe was founded, following a speech by Sir Winston Churchill, with the idea of unifying Europe to achieve common goals. It includes all European states except for Belarus, Kazakhstan and Vatican City. Further European integration by some states led to the formation of the European Union (EU), a separate political entity that lies between a confederation and a federation. The EU originated in Western Europe but has been expanding eastward since the fall of the Soviet Union in 1991. The currency of most countries of the European Union, the euro, is the most commonly used among Europeans; and the EU's Schengen Area abolishes border and immigration controls among most of its member states. The European Anthem is \"Ode to Joy\", and states celebrate peace and unity on Europe Day.\n\nIn classical Greek mythology, Europa (, \"Eurṓpē\") is the name of either a Phoenician princess or of a queen of Crete. The name contains the elements εὐρύς (\"eurús\"), \"wide, broad\" and ὤψ (\"ōps\", gen. ὠπός, \"ōpós\") \"eye, face, countenance\", hence their composite \"Eurṓpē\" would mean \"wide-gazing\" or \"broad of aspect\". \"Broad\" has been an epithet of Earth herself in the reconstructed Proto-Indo-European religion and the poetry devoted to it. For the second part compare also the divine attributes of \"grey-eyed\" Athena (γλαυκῶπις, \"glaukōpis\") or ox-eyed Hera (βοῶπις, \"boōpis\").\n\nThere have been attempts to connect \"Eurṓpē\" to a Semitic term for \"west\", this being either Akkadian \"erebu\" meaning \"to go down, set\" (said of the sun) or Phoenician \" 'ereb\" \"evening, west\", which is at the origin of Arabic Maghreb and Hebrew \"ma'arav\". Michael A. Barry, professor in Princeton University's Near Eastern Studies Department, finds the mention of the word \"Ereb\" on an Assyrian stele with the meaning of \"night, [the country of] sunset\", in opposition to \"Asu\" \"[the country of] sunrise\", i.e. Asia. The same naming motive according to \"cartographic convention\" appears in Greek Ἀνατολή (\"Anatolḗ\" \"[sun] rise\", \"east\", hence Anatolia). Martin Litchfield West stated that \"phonologically, the match between Europa's name and any form of the Semitic word is very poor.\" Next to these hypotheses there is also a Proto-Indo-European root \"*hregʷos\", meaning \"darkness\", which also produced Greek \"Erebus\".\n\nMost major world languages use words derived from \"Eurṓpē\" or \"Europa\" to refer to the continent. Chinese, for example, uses the word \"Ōuzhōu\" (歐洲/欧洲); a similar Chinese-derived term is also sometimes used in Japanese such as in the Japanese name of the European Union, , despite the katakana being more commonly used. In some Turkic languages the originally Persian name \"Frangistan\" (\"land of the Franks\") is used casually in referring to much of Europe, besides official names such as \"Avrupa\" or \"Evropa\".\n\nThe prevalent definition of Europe as a geographical term has been in use since the mid-19th century.\nEurope is taken to be bounded by large bodies of water to the north, west and south; Europe's limits to the far east are usually taken to be the Urals, the Ural River, and the Caspian Sea; to the southeast, including the Caucasus Mountains, the Black Sea and the waterways connecting the Black Sea to the Mediterranean Sea.\n\nIslands are generally grouped with the nearest continental landmass, hence Iceland is generally considered to be part of Europe, while the nearby island of Greenland is usually assigned to North America. Nevertheless, there are some exceptions based on sociopolitical and cultural differences. Cyprus is closest to Anatolia (or Asia Minor), but is usually considered part of Europe both culturally and politically and is a member state of the EU. Malta was considered an island of Northwest Africa for centuries.\n\n\"Europe\" as used specifically in British English may also refer to Continental Europe exclusively.\n\nThe first recorded usage of \"Eurṓpē\" as a geographic term is in the Homeric Hymn to Delian Apollo, in reference to the western shore of the Aegean Sea. As a name for a part of the known world, it is first used in the 6th century BC by Anaximander and Hecataeus. Anaximander placed the boundary between Asia and Europe along the Phasis River (the modern Rioni River) in the Caucasus, a convention still followed by Herodotus in the 5th century BC. Herodotus mentioned that the world had been divided by unknown persons into three parts, Europe, Asia, and Libya (Africa), with the Nile and the Phasis forming their boundaries—though he also states that some considered the River Don, rather than the Phasis, as the boundary between Europe and Asia. Europe's eastern frontier was defined in the 1st century by geographer Strabo at the River Don. The \"Book of Jubilees\" described the continents as the lands given by Noah to his three sons; Europe was defined as stretching from the Pillars of Hercules at the Strait of Gibraltar, separating it from Northwest Africa, to the Don, separating it from Asia.\n\nThe convention received by the Middle Ages and surviving into modern usage is that of the Roman era used by Roman era authors such as Posidonius, Strabo and Ptolemy,\nwho took the Tanais (the modern Don River) as the boundary.\n\nThe term \"Europe\" is first used for a cultural sphere in the Carolingian Renaissance of the 9th century. From that time, the term designated the sphere of influence of the Western Church, as opposed to both the Eastern Orthodox churches and to the Islamic world.\n\nA cultural definition of Europe as the lands of Latin Christendom coalesced in the 8th century, signifying the new cultural condominium created through the confluence of Germanic traditions and Christian-Latin culture, defined partly in contrast with Byzantium and Islam, and limited to northern Iberia, the British Isles, France, Christianised western Germany, the Alpine regions and northern and central Italy. The concept is one of the lasting legacies of the Carolingian Renaissance: \"Europa\" often figures in the letters of Charlemagne's court scholar, Alcuin.\n\nThe question of defining a precise eastern boundary of Europe arises in the Early Modern period, as the eastern extension of Muscovy began to include Northern Asia.\n\nThroughout the Middle Ages and into the 18th century, the traditional division of the landmass of Eurasia into two continents, Europe and Asia, followed Ptolemy, with the boundary following the Turkish Straits, the Black Sea, the Kerch Strait, the Sea of Azov and the Don (ancient Tanais). But maps produced during the 16th to 18th centuries tended to differ in how to continue the boundary beyond the Don bend at Kalach-na-Donu (where it is closest to the Volga, now joined with it by the Volga–Don Canal), into territory not described in any detail by the ancient geographers.\n\nPhilip Johan von Strahlenberg in 1725 was the first to depart from the classical Don boundary by drawing the line along the Volga, following the Volga north until the Samara Bend, along Obshchy Syrt (the drainage divide between Volga and Ural) and then north along Ural Mountains. This was adopted by the Russian Empire, and introduced the convention that would eventually become adopted as standard, but not without criticism by many modern analytical geographers.\n\nThe mapmakers continued to differ on the boundary between the lower Don and Samara well into the 19th century. The published by the Russian Academy of Sciences has the boundary follow the Don beyond Kalach as far as Serafimovich before cutting north towards Arkhangelsk, while other 18th- to 19th-century mapmakers such as John Cary followed Strahlenberg's prescription. To the south, the Kuma–Manych Depression was identified circa 1773 by a German naturalist, Peter Simon Pallas, as a valley that, once upon a time, connected the Black Sea and the Caspian Sea, and subsequently was proposed as a natural boundary between continents.\n\nBy the mid-19th century, there were three main conventions, one following the Don, the Volga–Don Canal and the Volga, the other following the Kuma–Manych Depression to the Caspian and then the Ural River, and the third abandoning the Don altogether, following the Greater Caucasus watershed to the Caspian. The question was still treated as a \"controversy\" in geographical literature of the 1860s, with Douglas Freshfield advocating the Caucasus crest boundary as the \"best possible\", citing support from various \"modern geographers\".\n\nIn Russia and the Soviet Union, the boundary along the Kuma–Manych Depression was the most commonly used as early as 1906. In 1958, the Soviet Geographical Society formally recommended that the boundary between the Europe and Asia be drawn in textbooks from Baydaratskaya Bay, on the Kara Sea, along the eastern foot of Ural Mountains, then following the Ural River until the Mugodzhar Hills, and then the Emba River; and Kuma–Manych Depression, thus placing the Caucasus entirely in Asia and the Urals entirely in Europe. However, most geographers in the Soviet Union favoured the boundary along the Caucasus crest and this became the standard convention in the later 20th century, although the Kuma–Manych boundary remained in use in some 20th-century maps.\n\n\"Homo erectus georgicus\", which lived roughly 1.8 million years ago in Georgia, is the earliest hominid to have been discovered in Europe. Other hominid remains, dating back roughly 1 million years, have been discovered in Atapuerca, Spain. Neanderthal man (named after the Neandertal valley in Germany) appeared in Europe 150,000 years ago (115,000 years ago it is found already in Poland) and disappeared from the fossil record about 28,000 years ago, with their final refuge being present-day Portugal. The Neanderthals were supplanted by modern humans (Cro-Magnons), who appeared in Europe around 43,000 to 40,000 years ago.\n\nThe European Neolithic period—marked by the cultivation of crops and the raising of livestock, increased numbers of settlements and the widespread use of pottery—began around 7000 BC in Greece and the Balkans, probably influenced by earlier farming practices in Anatolia and the Near East. It spread from the Balkans along the valleys of the Danube and the Rhine (Linear Pottery culture) and along the Mediterranean coast (Cardial culture). Between 4500 and 3000 BC, these central European neolithic cultures developed further to the west and the north, transmitting newly acquired skills in producing copper artefacts. In Western Europe the Neolithic period was characterised not by large agricultural settlements but by field monuments, such as causewayed enclosures, burial mounds and megalithic tombs. The Corded Ware cultural horizon flourished at the transition from the Neolithic to the Chalcolithic. During this period giant megalithic monuments, such as the Megalithic Temples of Malta and Stonehenge, were constructed throughout Western and Southern Europe.\n\nThe European Bronze Age began c. 3200 BC in Greece with the Minoan civilization on Crete, the first advanced civilization in Europe. The Minoans were followed by the Myceneans, who collapsed suddenly around 1200 BC, ushering the European Iron Age. Iron Age colonisation by the Greeks and Phoenicians gave rise to early Mediterranean cities. Early Iron Age Italy and Greece from around the 8th century BC gradually gave rise to historical Classical antiquity, whose beginning is sometimes dated to 776 BC, the year the first Olympic Games.\n\nAncient Greece was the founding culture of Western civilisation. Western democratic and rationalist culture are often attributed to Ancient Greece. The Greeks city-state, the polis, was the fundamental political unit of classical Greece. In 508 BC, Cleisthenes instituted the world's first democratic system of government in Athens. The Greek political ideals were rediscovered in the late 18th century by European philosophers and idealists. Greece also generated many cultural contributions: in philosophy, humanism and rationalism under Aristotle, Socrates and Plato; in history with Herodotus and Thucydides; in dramatic and narrative verse, starting with the epic poems of Homer; in drama with Sophocles and Euripides, in medicine with Hippocrates and Galen; and in science with Pythagoras, Euclid and Archimedes. In the course of the 5th century BC, several of the Greek city states would ultimately check the Achaemenid Persian advance in Europe through the Greco-Persian Wars, considered a pivotal moment in world history, as the 50 years of peace that followed are known as Golden Age of Athens, the seminal period of ancient Greece that laid many of the foundations of Western civilization.\n\nGreece was followed by Rome, which left its mark on law, politics, language, engineering, architecture, government and many more key aspects in western civilisation. Expanding from their base in Italy beginning in the 3rd century BC, the Romans gradually expanded to eventually rule the entire Mediterranean basin and western Europe by the turn of the millennium. The Roman Republic ended in 27 BC, when Augustus proclaimed the Roman Empire. The two centuries that followed are known as the \"pax romana\", a period of unprecedented peace, prosperity, and political stability in most of Europe.\n\nThe empire continued to expand under emperors such as Antoninus Pius and Marcus Aurelius, who spent time on the Empire's northern border fighting Germanic, Pictish and Scottish tribes. The Empire began to decline in the 3rd century, particularly in the west. Christianity was legalised by Constantine I in 313 AD after three centuries of imperial persecution. Constantine also permanently moved the capital of the empire from Rome to the city of Byzantium, which was renamed Constantinople in his honour (modern-day Istanbul) in 330 AD. Christianity became the sole official religion of the empire in 380 AD, and in 391-392 AD, the emperor Theodosius outlawed pagan religions. This is sometimes considered to mark the end of antiquity; alternatively antiquity is considered to end with the fall of the Western Roman Empire in 476 AD; the closure of the pagan Platonic Academy of Athens in 529 AD; or the rise of Islam in the early 7th century AD.\n\nDuring the decline of the Roman Empire, Europe entered a long period of change arising from what historians call the \"Age of Migrations\". There were numerous invasions and migrations amongst the Ostrogoths, Visigoths, Goths, Vandals, Huns, Franks, Angles, Saxons, Slavs, Avars, Bulgars and, later on, the Vikings, Pechenegs, Cumans and Magyars. Renaissance thinkers such as Petrarch would later refer to this as the \"Dark Ages\". Isolated monastic communities were the only places to safeguard and compile written knowledge accumulated previously; apart from this very few written records survive and much literature, philosophy, mathematics, and other thinking from the classical period disappeared from Western Europe though they were preserved in the east, in the Byzantine Empire.\n\nWhile the Roman empire in the west continued to decline, Roman traditions and the Roman state remained strong in the predominantly Greek-speaking Eastern Roman Empire, also known as the Byzantine Empire. During most of its existence, the Byzantine Empire was the most powerful economic, cultural, and military force in Europe. Emperor Justinian I presided over Constantinople's first golden age: he established a legal code that forms the basis of many modern legal systems, funded the construction of the Hagia Sophia, and brought the Christian church under state control.\n\nFrom the 7th century onwards, as the Byzantines and neighbouring Sasanid Persians were severely weakened due the protracted, centuries-lasting and frequent Byzantine–Sasanian wars, the Muslim Arabs began to make inroads into historically Roman territory, taking the Levant and North Africa and making inroads into Asia Minor. In the mid 7th century AD, following the Muslim conquest of Persia, Islam penetrated into the Caucasus region. Over the next centuries Muslim forces took Cyprus, Malta, Crete, Sicily and parts of southern Italy. Between 711 and 720, most of the Iberian Peninsula was brought under Muslim rule — save for small areas in the northwest (Asturias) and largely Basque regions in the Pyrenees. This territory, under the Arabic name Al-Andalus, became part of the expanding Umayyad Caliphate. The unsuccessful second siege of Constantinople (717) weakened the Umayyad dynasty and reduced their prestige. The Umayyads were then defeated by the Frankish leader Charles Martel at the Battle of Poitiers in 732, which ended their northward advance.\n\nDuring the Dark Ages, the Western Roman Empire fell under the control of various tribes. The Germanic and Slav tribes established their domains over Western and Eastern Europe respectively. Eventually the Frankish tribes were united under Clovis I. Charlemagne, a Frankish king of the Carolingian dynasty who had conquered most of Western Europe, was anointed \"Holy Roman Emperor\" by the Pope in 800. This led in 962 to the founding of the Holy Roman Empire, which eventually became centred in the German principalities of central Europe.\n\nEast Central Europe saw the creation of the first Slavic states and the adoption of Christianity (circa 1000 AD). The powerful West Slavic state of Great Moravia spread its territory all the way south to the Balkans, reaching its largest territorial extent under Svatopluk I and causing a series of armed conflicts with East Francia. Further south, the first South Slavic states emerged in the late 7th and 8th century and adopted Christianity: the First Bulgarian Empire, the Serbian Principality (later Kingdom and Empire), and the Duchy of Croatia (later Kingdom of Croatia). To the East, the Kievan Rus expanded from its capital in Kiev to become the largest state in Europe by the 10th century. In 988, Vladimir the Great adopted Orthodox Christianity as the religion of state. Further East, Volga Bulgaria became an Islamic state in the 10th century, but was eventually absorbed into Russia several centuries later.\n\nThe period between the year 1000 and 1300 is known as the High Middle Ages, during which the population of Europe experienced significant growth, culminating in the Renaissance of the 12th century. Economic growth, together with the lack of safety on the mainland trading routes, made possible the development of major commercial routes along the coast of the Mediterranean and Baltic Seas. The growing wealth and independence acquired by some coastal cities gave the Maritime Republics a leading role in the European scene.\n\nThe Middle Ages on the mainland were dominated by the two upper echelons of the social structure: the nobility and the clergy. Feudalism developed in France in the Early Middle Ages and soon spread throughout Europe. A struggle for influence between the nobility and the monarchy in England led to the writing of the Magna Carta and the establishment of a parliament. The primary source of culture in this period came from the Roman Catholic Church. Through monasteries and cathedral schools, the Church was responsible for education in much of Europe.\n\nThe Papacy reached the height of its power during the High Middle Ages. An East-West Schism in 1054 split the former Roman Empire religiously, with the Eastern Orthodox Church in the Byzantine Empire and the Roman Catholic Church in the former Western Roman Empire. In 1095 Pope Urban II called for a crusade against Muslims occupying Jerusalem and the Holy Land. In Europe itself, the Church organised the Inquisition against heretics. In Spain, the Reconquista concluded with the fall of Granada in 1492, ending over seven centuries of Islamic rule in the Iberian Peninsula.\n\nIn the east a resurgent Byzantine Empire recaptured Crete and Cyprus from the Muslims and reconquered the Balkans. Constantinople was the largest and wealthiest city in Europe from the 9th to the 12th centuries, with a population of approximately 400,000. The Empire was weakened following the defeat at Manzikert and was weakened considerably by the sack of Constantinople in 1204, during the Fourth Crusade. Although it would recover Constantinople in 1261, Byzantium fell in 1453 when Constantinople was taken by the Ottoman Empire.\n\nIn the 11th and 12th centuries, constant incursions by nomadic Turkic tribes, such as the Pechenegs and the Cuman-Kipchaks, caused a massive migration of Slavic populations to the safer, heavily forested regions of the north and temporarily halted the expansion of the Rus' state to the south and east. Like many other parts of Eurasia, these territories were overrun by the Mongols. The invaders, who became known as Tatars, were mostly Turkic-speaking peoples under Mongol suzerainty. They established the state of the Golden Horde with headquarters in Crimea, which later adopted Islam as a religion and ruled over modern-day southern and central Russia for more than three centuries. After the collapse of Mongol dominions, the first Romanian states (principalities) emerged in the 14th century: Moldova and Walachia. Previously, these territories were under the successive control of Pechenegs and Cumans. From the 12th to the 15th centuries, the Grand Duchy of Moscow grew from a small principality under Mongol rule to the largest state in Europe, overthrowing the Mongols in 1480 and eventually becoming the Tsardom of Russia. The state was consolidated under Ivan III the Great and Ivan the Terrible, steadily expanding to the east and south over the next centuries.\n\nThe Great Famine of 1315–1317 was the first crisis that would strike Europe in the late Middle Ages. The period between 1348 and 1420 witnessed the heaviest loss. The population of France was reduced by half. Medieval Britain was afflicted by 95 famines, and France suffered the effects of 75 or more in the same period. Europe was devastated in the mid-14th century by the Black Death, one of the most deadly pandemics in human history which killed an estimated 25 million people in Europe alone—a third of the European population at the time.\n\nThe plague had a devastating effect on Europe's social structure; it induced people to live for the moment as illustrated by Giovanni Boccaccio in \"The Decameron\" (1353). It was a serious blow to the Roman Catholic Church and led to increased persecution of Jews, beggars, and lepers. The plague is thought to have returned every generation with varying virulence and mortalities until the 18th century. During this period, more than 100 plague epidemics swept across Europe.\n\nThe Renaissance was a period of cultural change originating in Florence and later spreading to the rest of Europe. The rise of a new humanism was accompanied by the recovery of forgotten classical Greek and Arabic knowledge from monastic libraries, often translated from Arabic into Latin. The Renaissance spread across Europe between the 14th and 16th centuries: it saw the flowering of art, philosophy, music, and the sciences, under the joint patronage of royalty, the nobility, the Roman Catholic Church, and an emerging merchant class. Patrons in Italy, including the Medici family of Florentine bankers and the Popes in Rome, funded prolific quattrocento and cinquecento artists such as Raphael, Michelangelo, and Leonardo da Vinci.\n\nPolitical intrigue within the Church in the mid-14th century caused the Western Schism. During this forty-year period, two popes—one in Avignon and one in Rome—claimed rulership over the Church. Although the schism was eventually healed in 1417, the papacy's spiritual authority had suffered greatly.\n\nIn the 15th century, Europe started to extend itself beyond its geographic frontiers. Spain and Portugal, the greatest naval powers of the time, took the lead in exploring the world. A Spanish expedition led by Christopher Columbus reached the New World in 1492, the Portuguese sailor Vasco da Gama opened the ocean route to the East in 1498 and the Spanish explorer Sebastian Elcano made the first circumnavigation of the globe in 1519–1522, and soon after the Spanish and Portuguese began establishing huge global empires in the Americas, Asia, Africa and Oceania. France, the Netherlands and England soon followed in building large colonial empires with vast holdings in Africa, the Americas, and Asia.\n\nThe Church's power was further weakened by the Protestant Reformation (1517–1648), initially sparked by the works of German theologian Martin Luther, an attempt to start a reform within the Church. The Reformation also damaged the Holy Roman Emperor's influence, as German princes became divided between Protestant and Roman Catholic faiths. This eventually led to the Thirty Years War (1618–1648), which crippled the Holy Roman Empire and devastated much of Germany, killing between 25 and 40 percent of its population. In the aftermath of the Peace of Westphalia, France rose to predominance within Europe.\n\nThe 17th century in southern, central and eastern Europe was a period of general decline. Central and Eastern Europe experienced more than 150 famines in a 200-year period between 1501 and 1700. From the Union of Krewo (1385) central and eastern Europe was dominated by Kingdom of Poland and Grand Duchy of Lithuania. Between 1648 and 1655 in the central and eastern Europe ended hegemony of the Polish–Lithuanian Commonwealth. From the 15th to 18th centuries, when the disintegrating khanates of the Golden Horde were conquered by Russia, Tatars from the Crimean Khanate frequently raided Eastern Slavic lands to capture slaves. Further east, the Nogai Horde and Kazakh Khanate frequently raided the Slavic-speaking areas of Russia, Ukraine and Poland for hundreds of years, until the Russian expansion and conquest of most of northern Eurasia (i.e. Eastern Europe, Central Asia and Siberia). Meanwhile, in the south, the Ottomans had conquered the Balkans by the 15th century, laying siege to Vienna in 1529. In the Battle of Lepanto in 1571, the Holy League checked Ottoman power in the Mediterranean. The Ottomans again laid siege to Vienna in 1683, but the Battle of Vienna permanently ended their advance into Europe, and marked the political hegemony of the Habsburg dynasty in central Europe.\n\nThe Renaissance and the New Monarchs marked the start of an Age of Discovery, a period of exploration, invention, and scientific development. Among the great figures of the Western scientific revolution of the 16th and 17th centuries were Copernicus, Kepler, Galileo, and Isaac Newton. According to Peter Barrett, \"It is widely accepted that 'modern science' arose in the Europe of the 17th century (towards the end of the Renaissance), introducing a new understanding of the natural world.\"\n\nThe Age of Enlightenment was a powerful intellectual movement during the 18th century promoting scientific and reason-based thoughts. Discontent with the aristocracy and clergy's monopoly on political power in France resulted in the French Revolution and the establishment of the First Republic as a result of which the monarchy and many of the nobility perished during the initial reign of terror. Napoleon Bonaparte rose to power in the aftermath of the French Revolution and established the First French Empire that, during the Napoleonic Wars, grew to encompass large parts of Europe before collapsing in 1815 with the Battle of Waterloo. Napoleonic rule resulted in the further dissemination of the ideals of the French Revolution, including that of the nation-state, as well as the widespread adoption of the French models of administration, law, and education. The Congress of Vienna, convened after Napoleon's downfall, established a new balance of power in Europe centred on the five \"Great Powers\": the UK, France, Prussia, Austria, and Russia. This balance would remain in place until the Revolutions of 1848, during which liberal uprisings affected all of Europe except for Russia and the UK. These revolutions were eventually put down by conservative elements and few reforms resulted. The year 1859 saw the unification of Romania, as a nation-state, from smaller principalities. In 1867, the Austro-Hungarian empire was formed; and 1871 saw the unifications of both Italy and Germany as nation-states from smaller principalities.\n\nIn parallel, the Eastern Question grew more complex ever since the Ottoman defeat in the Russo-Turkish War (1768–1774). As the dissolution of the Ottoman Empire seemed imminent, the Great Powers struggled to safeguard their strategic and commercial interests in the Ottoman domains. The Russian Empire stood to benefit from the decline, whereas the Habsburg Empire and Britain perceived the preservation of the Ottoman Empire to be in their best interests. Meanwhile, the Serbian revolution (1804) and Greek War of Independence (1821) marked the beginning of the end of Ottoman rule in the Balkans, which ended with the Balkan Wars in 1912-1913. Formal recognition of the \"de facto\" independent principalities of Montenegro, Serbia and Romania ensued at the Congress of Berlin in 1878.\n\nThe Industrial Revolution started in Great Britain in the last part of the 18th century and spread throughout Europe. The invention and implementation of new technologies resulted in rapid urban growth, mass employment, and the rise of a new working class. Reforms in social and economic spheres followed, including the first laws on child labour, the legalisation of trade unions, and the abolition of slavery. In Britain, the Public Health Act of 1875 was passed, which significantly improved living conditions in many British cities. Europe's population increased from about 100 million in 1700 to 400 million by 1900. The last major famine recorded in Western Europe, the Irish Potato Famine, caused death and mass emigration of millions of Irish people. In the 19th century, 70 million people left Europe in migrations to various European colonies abroad and to the United States. Demographic growth meant that, by 1900, Europe's share of the world's population was 25%.\n\nTwo world wars and an economic depression dominated the first half of the 20th century. World War I was fought between 1914 and 1918. It started when Archduke Franz Ferdinand of Austria was assassinated by the Yugoslav nationalist Gavrilo Princip. Most European nations were drawn into the war, which was fought between the Entente Powers (France, Belgium, Serbia, Portugal, Russia, the United Kingdom, and later Italy, Greece, Romania, and the United States) and the Central Powers (Austria-Hungary, Germany, Bulgaria, and the Ottoman Empire). The war left more than 16 million civilians and military dead. Over 60 million European soldiers were mobilised from 1914 to 1918.\n\nRussia was plunged into the Russian Revolution, which threw down the Tsarist monarchy and replaced it with the communist Soviet Union. Austria-Hungary and the Ottoman Empire collapsed and broke up into separate nations, and many other nations had their borders redrawn. The Treaty of Versailles, which officially ended World War I in 1919, was harsh towards Germany, upon whom it placed full responsibility for the war and imposed heavy sanctions.\n\nExcess deaths in Russia over the course of World War I and the Russian Civil War (including the postwar famine) amounted to a combined total of 18 million. In 1932–1933, under Stalin's leadership, confiscations of grain by the Soviet authorities contributed to the second Soviet famine which caused millions of deaths; surviving kulaks were persecuted and many sent to Gulags to do forced labour. Stalin was also responsible for the Great Purge of 1937–38 in which the NKVD executed 681,692 people; millions of people were deported and exiled to remote areas of the Soviet Union.\n\nThe social revolutions sweeping through Russia also affected other European nations following The Great War: in 1919, with the Weimar Republic in Germany, and the First Austrian Republic; in 1922, with Mussolini's one party fascist government in the Kingdom of Italy, and in Ataturk's Turkish Republic, adopting the Western alphabet, and state secularism.\nEconomic instability, caused in part by debts incurred in the First World War and 'loans' to Germany played havoc in Europe in the late 1920s and 1930s. This and the Wall Street Crash of 1929 brought about the worldwide Great Depression. Helped by the economic crisis, social instability and the threat of communism, fascist movements developed throughout Europe placing Adolf Hitler in power of what became Nazi Germany.\n\nIn 1933, Hitler became the leader of Germany and began to work towards his goal of building Greater Germany. Germany re-expanded and took back the Saarland and Rhineland in 1935 and 1936. In 1938, Austria became a part of Germany following the Anschluss. Later that year, following the Munich Agreement signed by Germany, France, the United Kingdom and Italy, Germany annexed the Sudetenland, which was a part of Czechoslovakia inhabited by ethnic Germans, and in early 1939, the remainder of Czechoslovakia was split into the Protectorate of Bohemia and Moravia, controlled by Germany, and the Slovak Republic. At the time, Britain and France preferred a policy of appeasement.\n\nWith tensions mounting between Germany and Poland over the future of Danzig, the Germans turned to the Soviets, and signed the Molotov–Ribbentrop Pact, which allowed the Soviets to invade the Baltic states and parts of Poland and Romania. Germany invaded Poland on 1 September 1939, prompting France and the United Kingdom to declare war on Germany on 3 September, opening the European Theatre of World War II. The Soviet invasion of Poland started on 17 September and Poland fell soon thereafter. On 24 September, the Soviet Union attacked the Baltic countries and later, Finland. The British hoped to land at Narvik and send troops to aid Finland, but their primary objective in the landing was to encircle Germany and cut the Germans off from Scandinavian resources. Around the same time, Germany moved troops into Denmark. The Phoney War continued.\n\nIn May 1940, Germany attacked France through the Low Countries. France capitulated in June 1940. By August Germany began a bombing offensive on Britain, but failed to convince the Britons to give up. In 1941, Germany invaded the Soviet Union in Operation Barbarossa. On 7 December 1941 Japan's attack on Pearl Harbor drew the United States into the conflict as allies of the British Empire and other allied forces.\n\nAfter the staggering Battle of Stalingrad in 1943, the German offensive in the Soviet Union turned into a continual fallback. The Battle of Kursk, which involved the largest tank battle in history, was the last major German offensive on the Eastern Front. In June 1944, British and American forces invaded France in the D-Day landings, opening a new front against Germany. Berlin finally fell in 1945, ending World War II in Europe. The war was the largest and most destructive in human history, with 60 million dead across the world. More than 40 million people in Europe had died as a result of World War II, including between 11 and 17 million people who perished during the Holocaust. The Soviet Union lost around 27 million people (mostly civilians) during the war, about half of all World War II casualties. By the end of World War II, Europe had more than 40 million refugees. Several post-war expulsions in Central and Eastern Europe displaced a total of about 20 million people.\n\nWorld War I and especially World War II diminished the eminence of Western Europe in world affairs. After World War II the map of Europe was redrawn at the Yalta Conference and divided into two blocs, the Western countries and the communist Eastern bloc, separated by what was later called by Winston Churchill an \"Iron Curtain\". The United States and Western Europe\nestablished the NATO alliance and later the Soviet Union and Central Europe established the Warsaw Pact.\n\nThe two new superpowers, the United States and the Soviet Union, became locked in a fifty-year-long Cold War, centred on nuclear proliferation. At the same time decolonisation, which had already started after World War I, gradually resulted in the independence of most of the European colonies in Asia and Africa. In the 1980s the reforms of Mikhail Gorbachev and the Solidarity movement in Poland accelerated the collapse of the Eastern bloc and the end of the Cold War. Germany was reunited, after the symbolic fall of the Berlin Wall in 1989, and the maps of Central and Eastern Europe were redrawn once more.\n\nEuropean integration also grew after World War II. The Treaty of Rome in 1957 established the European Economic Community between six Western European states with the goal of a unified economic policy and common market. In 1967 the EEC, European Coal and Steel Community and Euratom formed the European Community, which in 1993 became the European Union. The EU established a parliament, court and central bank and introduced the euro as a unified currency. Between 2004 and 2013, more Central and Eastern European countries began joining, expanding the EU to its current size of 28 European countries, and once more making Europe a major economical and political centre of power. However, in June 2016 the people of the United Kingdom, in a non-binding referendum on EU membership voted to leave the European Union.\n\nEurope makes up the western fifth of the Eurasian landmass. It has a higher ratio of coast to landmass than any other continent or subcontinent. Its maritime borders consist of the Arctic Ocean to the north, the Atlantic Ocean to the west, and the Mediterranean, Black, and Caspian Seas to the south.\nLand relief in Europe shows great variation within relatively small areas. The southern regions are more mountainous, while moving north the terrain descends from the high Alps, Pyrenees, and Carpathians, through hilly uplands, into broad, low northern plains, which are vast in the east. This extended lowland is known as the Great European Plain, and at its heart lies the North German Plain. An arc of uplands also exists along the north-western seaboard, which begins in the western parts of the islands of Britain and Ireland, and then continues along the mountainous, fjord-cut spine of Norway.\n\nThis description is simplified. Sub-regions such as the Iberian Peninsula and the Italian Peninsula contain their own complex features, as does mainland Central Europe itself, where the relief contains many plateaus, river valleys and basins that complicate the general trend. Sub-regions like Iceland, Britain, and Ireland are special cases. The former is a land unto itself in the northern ocean which is counted as part of Europe, while the latter are upland areas that were once joined to the mainland until rising sea levels cut them off.\n\nEurope lies mainly in the temperate climate zones, being subjected to prevailing westerlies. The climate is milder in comparison to other areas of the same latitude around the globe due to the influence of the Gulf Stream. The Gulf Stream is nicknamed \"Europe's central heating\", because it makes Europe's climate warmer and wetter than it would otherwise be. The Gulf Stream not only carries warm water to Europe's coast but also warms up the prevailing westerly winds that blow across the continent from the Atlantic Ocean.\n\nTherefore, the average temperature throughout the year of Naples is , while it is only in New York City which is almost on the same latitude. Berlin, Germany; Calgary, Canada; and Irkutsk, in the Asian part of Russia, lie on around the same latitude; January temperatures in Berlin average around higher than those in Calgary, and they are almost higher than average temperatures in Irkutsk. Similarly, northern parts of Scotland have a temperate marine climate. The yearly average temperature in city of Inverness is . However, Churchill, Manitoba, Canada, is on roughly the same latitude and has an average temperature of , giving it a nearly subarctic climate.\n\nThe geological history of Europe traces back to the formation of the Baltic Shield (Fennoscandia) and the Sarmatian craton, both around 2.25 billion years ago, followed by the Volgo–Uralia shield, the three together leading to the East European craton (≈ Baltica) which became a part of the supercontinent Columbia. Around 1.1 billion years ago, Baltica and Arctica (as part of the Laurentia block) became joined to Rodinia, later resplitting around 550 million years ago to reform as Baltica. Around 440 million years ago Euramerica was formed from Baltica and Laurentia; a further joining with Gondwana then leading to the formation of Pangea. Around 190 million years ago, Gondwana and Laurasia split apart due to the widening of the Atlantic Ocean. Finally, and very soon afterwards, Laurasia itself split up again, into Laurentia (North America) and the Eurasian continent. The land connection between the two persisted for a considerable time, via Greenland, leading to interchange of animal species. From around 50 million years ago, rising and falling sea levels have determined the actual shape of Europe, and its connections with continents such as Asia. Europe's present shape dates to the late Tertiary period about five million years ago.\nThe geology of Europe is hugely varied and complex, and gives rise to the wide variety of landscapes found across the continent, from the Scottish Highlands to the rolling plains of Hungary. Europe's most significant feature is the dichotomy between highland and mountainous Southern Europe and a vast, partially underwater, northern plain ranging from Ireland in the west to the Ural Mountains in the east. These two halves are separated by the mountain chains of the Pyrenees and Alps/Carpathians. The northern plains are delimited in the west by the Scandinavian Mountains and the mountainous parts of the British Isles. Major shallow water bodies submerging parts of the northern plains are the Celtic Sea, the North Sea, the Baltic Sea complex and Barents Sea.\n\nThe northern plain contains the old geological continent of Baltica, and so may be regarded geologically as the \"main continent\", while peripheral highlands and mountainous regions in the south and west constitute fragments from various other geological continents. Most of the older geology of western Europe existed as part of the ancient microcontinent Avalonia.\n\nHaving lived side-by-side with agricultural peoples for millennia, Europe's animals and plants have been profoundly affected by the presence and activities of man. With the exception of Fennoscandia and northern Russia, few areas of untouched wilderness are currently found in Europe, except for various national parks.\n\nThe main natural vegetation cover in Europe is mixed forest. The conditions for growth are very favourable. In the north, the Gulf Stream and North Atlantic Drift warm the continent. Southern Europe could be described as having a warm, but mild climate. There are frequent summer droughts in this region. Mountain ridges also affect the conditions. Some of these (Alps, Pyrenees) are oriented east-west and allow the wind to carry large masses of water from the ocean in the interior. Others are oriented south-north (Scandinavian Mountains, Dinarides, Carpathians, Apennines) and because the rain falls primarily on the side of mountains that is oriented towards the sea, forests grow well on this side, while on the other side, the conditions are much less favourable. Few corners of mainland Europe have not been grazed by livestock at some point in time, and the cutting down of the pre-agricultural forest habitat caused disruption to the original plant and animal ecosystems.\n\nProbably 80 to 90 percent of Europe was once covered by forest. It stretched from the Mediterranean Sea to the Arctic Ocean. Although over half of Europe's original forests disappeared through the centuries of deforestation, Europe still has over one quarter of its land area as forest, such as the broadleaf and mixed forests, taiga of Scandinavia and Russia, mixed rainforests of the Caucasus and the Cork oak forests in the western Mediterranean. During recent times, deforestation has been slowed and many trees have been planted. However, in many cases monoculture plantations of conifers have replaced the original mixed natural forest, because these grow quicker. The plantations now cover vast areas of land, but offer poorer habitats for many European forest dwelling species which require a mixture of tree species and diverse forest structure. The amount of natural forest in Western Europe is just 2–3% or less, in European Russia 5–10%. The country with the smallest percentage of forested area is Iceland (1%), while the most forested country is Finland (77%).\n\nIn temperate Europe, mixed forest with both broadleaf and coniferous trees dominate. The most important species in central and western Europe are beech and oak. In the north, the taiga is a mixed spruce–pine–birch forest; further north within Russia and extreme northern Scandinavia, the taiga gives way to tundra as the Arctic is approached. In the Mediterranean, many olive trees have been planted, which are very well adapted to its arid climate; Mediterranean Cypress is also widely planted in southern Europe. The semi-arid Mediterranean region hosts much scrub forest. A narrow east-west tongue of Eurasian grassland (the steppe) extends eastwards from Ukraine and southern Russia and ends in Hungary and traverses into taiga to the north.\n\nGlaciation during the most recent ice age and the presence of man affected the distribution of European fauna. As for the animals, in many parts of Europe most large animals and top predator species have been hunted to extinction. The woolly mammoth was extinct before the end of the Neolithic period. Today wolves (carnivores) and bears (omnivores) are endangered. Once they were found in most parts of Europe. However, deforestation and hunting caused these animals to withdraw further and further. By the Middle Ages the bears' habitats were limited to more or less inaccessible mountains with sufficient forest cover. Today, the brown bear lives primarily in the Balkan peninsula, Scandinavia, and Russia; a small number also persist in other countries across Europe (Austria, Pyrenees etc.), but in these areas brown bear populations are fragmented and marginalised because of the destruction of their habitat. In addition, polar bears may be found on Svalbard, a Norwegian archipelago far north of Scandinavia. The wolf, the second largest predator in Europe after the brown bear, can be found primarily in Central and Eastern Europe and in the Balkans, with a handful of packs in pockets of Western Europe (Scandinavia, Spain, etc.).\n\nEuropean wild cat, foxes (especially the red fox), jackal and different species of martens, hedgehogs, different species of reptiles (like snakes such as vipers and grass snakes) and amphibians, different birds (owls, hawks and other birds of prey).\n\nImportant European herbivores are snails, larvae, fish, different birds, and mammals, like rodents, deer and roe deer, boars, and living in the mountains, marmots, steinbocks, chamois among others. A number of insects, such as the small tortoiseshell butterfly, add to the biodiversity.\n\nThe extinction of the dwarf hippos and dwarf elephants has been linked to the earliest arrival of humans on the islands of the Mediterranean.\n\nSea creatures are also an important part of European flora and fauna. The sea flora is mainly phytoplankton. Important animals that live in European seas are zooplankton, molluscs, echinoderms, different crustaceans, squids and octopuses, fish, dolphins, and whales.\n\nBiodiversity is protected in Europe through the Council of Europe's Bern Convention, which has also been signed by the European Community as well as non-European states.\n\nThe political map of Europe is substantially derived from the re-organisation of Europe following the Napoleonic Wars in 1815. The prevalent form of government in Europe is parliamentary democracy, in most cases in the form of Republic; in 1815, the prevalent form of government was still the Monarchy. Europe's remaining eleven monarchies are constitutional.\n\nEuropean integration is the process of political, legal, economic (and in some cases social and cultural) integration of European states as it has been pursued by the powers sponsoring the Council of Europe since the end of World War II\nThe European Union has been the focus of economic integration on the continent since its foundation in 1993. More recently, the Eurasian Economic Union has been established as a counterpart comprising former Soviet states.\n\n28 European states are members of the politico-economic European Union, 26 of the border-free Schengen Area and 19 of the monetary union Eurozone. Among the smaller European organizations are the Nordic Council, the Benelux, the Baltic Assembly and the Visegrád Group.\n\nThe list below includes all entities falling even partially under any of the various common definitions of Europe, geographic or political.\n\nWithin the above-mentioned states are several de facto independent countries with limited to no international recognition. None of them are members of the UN:\n\nSeveral dependencies and similar territories with broad autonomy are also found within or in close proximity to Europe. This includes Åland (a region of Finland), two constituent countries of the Kingdom of Denmark (other than Denmark itself), three Crown dependencies, and two British Overseas Territories. Svalbard is also included due to its unique status within Norway, although it is not autonomous. Not included are the three countries of the United Kingdom with devolved powers and the two Autonomous Regions of Portugal, which despite having a unique degree of autonomy, are not largely self-governing in matters other than international affairs. Areas with little more than a unique tax status, such as Heligoland and the Canary Islands, are also not included for this reason.\n\nAs a continent, the economy of Europe is currently the largest on Earth and it is the richest region as measured by assets under management with over $32.7 trillion compared to North America's $27.1 trillion in 2008. In 2009 Europe remained the wealthiest region. Its $37.1 trillion in assets under management represented one-third of the world's wealth. It was one of several regions where wealth surpassed its precrisis year-end peak. As with other continents, Europe has a large variation of wealth among its countries. The richer states tend to be in the West; some of the Central and Eastern European economies are still emerging from the collapse of the Soviet Union and the breakup of Yugoslavia.\n\nThe European Union, a political entity composed of 28 European states, comprises the largest single economic area in the world. 19 EU countries share the euro as a common currency.\nFive European countries rank in the top ten of the world's largest national economies in GDP (PPP). This includes (ranks according to the CIA): Germany (5), the UK (6), Russia (7), France (8), and Italy (10).\n\nThere is huge disparity between many European countries in terms of their income. The richest in terms of GDP per capita is Monaco with its US$172,676 per capita (2009) and the poorest is Moldova with its GDP per capita of US$1,631 (2010). Monaco is the richest country in terms of GDP per capita in the world according to the World Bank report.\n\nAs a whole, Europe's GDP per capita is US$21,767 according to a 2016 International Monetary Fund assessment.\n\nCapitalism has been dominant in the Western world since the end of feudalism. From Britain, it gradually spread throughout Europe. The Industrial Revolution started in Europe, specifically the United Kingdom in the late 18th century, and the 19th century saw Western Europe industrialise. Economies were disrupted by World War I but by the beginning of World War II they had recovered and were having to compete with the growing economic strength of the United States. World War II, again, damaged much of Europe's industries.\n\n\nAfter World War II the economy of the UK was in a state of ruin, and continued to suffer relative economic decline in the following decades. Italy was also in a poor economic condition but regained a high level of growth by the 1950s. West Germany recovered quickly and had doubled production from pre-war levels by the 1950s. France also staged a remarkable comeback enjoying rapid growth and modernisation; later on Spain, under the leadership of Franco, also recovered, and the nation recorded huge unprecedented economic growth beginning in the 1960s in what is called the Spanish miracle. The majority of Central and Eastern European states came under the control of the Soviet Union and thus were members of the Council for Mutual Economic Assistance (COMECON).\n\nThe states which retained a free-market system were given a large amount of aid by the United States under the Marshall Plan. The western states moved to link their economies together, providing the basis for the EU and increasing cross border trade. This helped them to enjoy rapidly improving economies, while those states in COMECON were struggling in a large part due to the cost of the Cold War. Until 1990, the European Community was expanded from 6 founding members to 12. The emphasis placed on resurrecting the West German economy led to it overtaking the UK as Europe's largest economy.\n\n\nWith the fall of communism in Central and Eastern Europe in 1991, the post-socialist states began free market reforms: Poland, Hungary, and Slovenia adopted them reasonably quickly, while Ukraine and Russia are still in the process of doing so.\n\nAfter East and West Germany were reunited in 1990, the economy of West Germany struggled as it had to support and largely rebuild the infrastructure of East Germany.\n\nBy the millennium change, the EU dominated the economy of Europe comprising the five largest European economies of the time namely Germany, the United Kingdom, France, Italy, and Spain. In 1999, 12 of the 15 members of the EU joined the Eurozone replacing their former national currencies by the common euro. The three who chose to remain outside the Eurozone were: the United Kingdom, Denmark, and Sweden. The European Union is now the largest economy in the world.\n\nFigures released by Eurostat in 2009 confirmed that the Eurozone had gone into recession in 2008. It impacted much of the region. In 2010, fears of a sovereign debt crisis developed concerning some countries in Europe, especially Greece, Ireland, Spain, and Portugal. As a result, measures were taken, especially for Greece, by the leading countries of the Eurozone. The EU-27 unemployment rate was 10.3% in 2012. For those aged 15–24 it was 22.4%.\n\nIn 2017, the population of Europe was estimated to be 742 million according to , which is slightly more than one-ninth of the world's population. This number includes Asian Russia (Siberia, about 36 million people) but excludes the European part of Turkey, Cyprus and the Transcaucasian countries.\nA century ago, Europe had nearly a quarter of the world's population. The population of Europe has grown in the past century, but in other areas of the world (in particular Africa and Asia) the population has grown far more quickly. Among the continents, Europe has a relatively high population density, second only to Asia. Most of Europe is in a mode of Sub-replacement fertility, which means that each new(-born) generation is being less populous than the older.\nThe most densely populated country in Europe (and in the world) is the microstate of Monaco.\n\nPan and Pfeil (2004) count 87 distinct \"peoples of Europe\", of which 33 form the majority population in at least one sovereign state, while the remaining 54 constitute ethnic minorities.\nAccording to UN population projection, Europe's population may fall to about 7% of world population by 2050, or 653 million people (medium variant, 556 to 777 million in low and high variants, respectively). Within this context, significant disparities exist between regions in relation to fertility rates. The average number of children per female of child-bearing age is 1.52. According to some sources, this rate is higher among Muslims in Europe. The UN predicts a steady population decline in Central and Eastern Europe as a result of emigration and low birth rates.\n\nEurope is home to the highest number of migrants of all global regions at 70.6 million people, the IOM's report said. In 2005, the EU had an overall net gain from immigration of 1.8 million people. This accounted for almost 85% of Europe's total population growth. The European Union plans to open the job centres for legal migrant workers from Africa. In 2008, 696,000 persons were given citizenship of an EU27 member state, a decrease from 707,000 the previous year.\n\nEmigration from Europe began with Spanish and Portuguese settlers in the 16th century, and French and English settlers in the 17th century. But numbers remained relatively small until waves of mass emigration in the 19th century, when millions of poor families left Europe.\n\nToday, large populations of European descent are found on every continent. European ancestry predominates in North America, and to a lesser degree in South America (particularly in Uruguay, Argentina, Chile and Brazil, while most of the other Latin American countries also have a considerable population of European origins). Australia and New Zealand have large European derived populations. Africa has no countries with European-derived majorities (or with the exception of Cape Verde and probably São Tomé and Príncipe, depending on context), but there are significant minorities, such as the White South Africans. In Asia, European-derived populations predominate in Northern Asia (specifically Russians) and some parts of Northern Kazakhstan.\n\nEurope has about 225 indigenous languages, mostly falling within three Indo-European language groups: the Romance languages, derived from the Latin of the Roman Empire; the Germanic languages, whose ancestor language came from southern Scandinavia; and the Slavic languages. Slavic languages are most spoken by the number of native speakers in Europe, they are spoken in Central, Eastern, and Southeastern Europe. Romance languages are spoken primarily in south-western Europe as well as in Romania and Moldova, in Eastern Europe. Germanic languages are spoken in Northern Europe, the British Isles and some parts of Central Europe. Other Indo-European languages outside the three main groups include the Baltic group (that is, Latvian and Lithuanian), the Celtic group (that is, Irish, Scottish Gaelic, Manx, Welsh, Cornish, and Breton), Greek, Armenian, and Albanian.\n\nA distinct non-Indo-European family of Uralic languages (Estonian, Finnish, Hungarian, Erzya, Komi, Mari, Moksha, and Udmurt) is spoken mainly in Estonia, Finland, Hungary, and parts of Russia. Turkic languages include Azerbaijani and Turkish, in addition to smaller languages in Eastern and Southeast Europe (Balkan Gagauz Turkish, Bashkir, Chuvash, Crimean Tatar, Karachay-Balkar, Kumyk, Nogai, and Tatar). Kartvelian languages (Georgian, Mingrelian, and Svan) are spoken primarily in Georgia. Two other language families reside in the North Caucasus (termed Northeast Caucasian, most notably including Chechen, Avar, and Lezgin; and Northwest Caucasian, most notably including Adyghe). Maltese is the only Semitic language that is official within the EU, while Basque is the only European language isolate.\n\nMultilingualism and the protection of regional and minority languages are recognised political goals in Europe today. The Council of Europe Framework Convention for the Protection of National Minorities and the Council of Europe's European Charter for Regional or Minority Languages set up a legal framework for language rights in Europe.\n\nThe four most populous cities of Europe are Istanbul, Moscow, Paris and London, each have over 10 million residents, and as such have been described as megacities. While Istanbul has the highest total population, one third lies on the Asian side of the Bosporus, making Moscow the most populous city entirely in Europe.\nThe next largest cities in order of population are Saint Petersburg, Madrid, Berlin and Rome, each having over 3 million residents.\n\nWhen considering the commuter belts or metropolitan areas, within the EU (for which comparable data is available) London covers the largest population, followed in order by Paris, Madrid, Barcelona, Berlin, the Ruhr area, Rome, Milan, Athens and Warsaw.\n\n\"Europe\" as a cultural concept is substantially derived from the shared heritage of the Roman Empire and its culture.\nThe boundaries of Europe were historically understood as those of Christendom (or more specifically Latin Christendom), as established or defended throughout the medieval and early modern history of Europe, especially against Islam, as in the Reconquista and the Ottoman wars in Europe.\n\nThis shared cultural heritage is combined by overlapping indigenous national cultures and folklores, roughly divided into Slavic, Latin (Romance) and Germanic, but with several components not part of either of these group (notably Greek and Celtic).\nCultural contact and mixtures characterise much of European regional cultures; Kaplan (2014) describes Europe as \"embracing maximum cultural diversity at minimal geographical distances\".\n\nHistorically, religion in Europe has been a major influence on European art, culture, philosophy and law.\n\nThe largest religion in Europe is Christianity, with 76.2% of Europeans considering themselves Christians, including Catholic, Eastern Orthodox and various Protestant denominations. Among Protestants, the most popular are historically state-supported European denominations such as Lutheranism, Anglicanism and the Reformed faith. Other Protestant denominations such as historically significant ones like Anabaptists were never supported by any state and thus are not so widespread, as well as these newly arriving from the United States such as Pentecostalism, Adventism, Methodism, Baptists and various Evangelical Protestants; although Methodism and Baptists both have European origins. The notion of \"Europe\" and the \"Western World\" has been intimately connected with the concept of \"Christianity and Christendom\"; many even attribute Christianity for being the link that created a unified European identity.\n\nChristianity, including the Roman Catholic Church, has played a prominent role in the shaping of Western civilization since at least the 4th century, and for at least a millennium and a half, Europe has been nearly equivalent to Christian culture, even though the religion was inherited from the Middle East. Christian culture was the predominant force in western civilization, guiding the course of philosophy, art, and science.\n\nThe second most popular religion is Islam (6%) concentrated mainly in the Balkans and eastern Europe (Bosnia and Herzegovina, Albania, Kosovo, Kazakhstan, North Cyprus, Turkey, Azerbaijan, North Caucasus, and the Volga-Ural region). Other religions, including Judaism, Hinduism, and Buddhism are minority religions (though Tibetan Buddhism is the majority religion of Russia's Republic of Kalmykia). The 20th century saw the revival of Neopaganism through movements such as Wicca and Druidry.\n\nEurope has become a relatively secular continent, with an increasing number and proportion of irreligious, atheist and agnostic people, who make up about 18.2% of Europe's population, currently the largest secular population in the Western world. There are a particularly high number of self-described non-religious people in the Czech Republic, Estonia, Sweden, former East Germany, and France.\n\n\n\n\n\nHistorical Maps\n"}
{"id": "56678291", "url": "https://en.wikipedia.org/wiki?curid=56678291", "title": "Freedom Online Coalition", "text": "Freedom Online Coalition\n\nThe Freedom Online Coalition is a group of 30 governments that work together to advance Internet freedom, mainly through coordinating diplomatic initiatives and interventions at relevant international fora. The Freedom Online Coalition was launched at a conference of the Dutch Government in The Hague, the Netherlands in December 2011. \n\nParticipating countries commit to \"promoting the freedoms of expression, association, and peaceful assembly with respect to the Internet and connection technologies\". They endorse the principle that the human rights that people have offline should enjoy the same protection online, including freedom of assembly and the right to organize, freedom of religion and the right to be protected by the \"arbitrary\" intrude into his privacy. Its work builds on the resolution on \"The Promotion, Protection and Enjoyment of Human Rights on the Internet\" adopted by United Nations Human Rights Council in July 2012. \n\nThe group has held annual meetings in The Netherlands (2011), Kenya (2012), Tunisia (2013), Estonia (2014), Mongolia (2015) and Costa Rica (2016) . The group has 30 member countries (February 2018). In 2018, the Chairmanship of the Freedom Online Coalition is held by the German Government.\n\n"}
{"id": "28165893", "url": "https://en.wikipedia.org/wiki?curid=28165893", "title": "Global Health Share Initiative", "text": "Global Health Share Initiative\n\nGlobal HealthShare® initiative (GHS) is a program launched in June 2010 by the University of California, Davis Center of Excellence for Nutritional Genomics. The program plans to share resources to improve the health in developing countries. Alliances with other research institutions, humanitarian organizations and private companies around the world have been made to meet this goal.\n\nThe organization will use a multidisciplinary approach against chronic and infectious disease, breaking down their efforts into two main research \"cores\": the Nutrition Core and Mucosal Immunity Core. The former focuses on fighting malnutrition and the latter on developing new ways to deliver vaccines more effectively to developing countries. The program differentiates itself from others like it by focusing on the delivery of the beneficial product to the end-user rather than the academic research process.\n\n\n"}
{"id": "13305402", "url": "https://en.wikipedia.org/wiki?curid=13305402", "title": "Global brain", "text": "Global brain\n\nThe global brain is a neuroscience-inspired and futurological vision of the planetary information and communications technology network that interconnects all humans and their technological artifacts. As this network stores ever more information, takes over ever more functions of coordination and communication from traditional organizations, and becomes increasingly intelligent, it increasingly plays the role of a brain for the planet Earth.\n\nProponents of the global brain hypothesis claim that the Internet increasingly ties its users together into a single information processing system that functions as part of the collective nervous system of the planet. The intelligence of this network is collective or distributed: it is not centralized or localized in any particular individual, organization or computer system. Therefore, no one can command or control it. Rather, it self-organizes or emerges from the dynamic networks of interactions between its components. This is a property typical of complex adaptive systems.\n\nThe World-wide web in particular resembles the organization of a brain with its webpages (playing a role similar to neurons) connected by hyperlinks (playing a role similar to synapses), together forming an associative network along which information propagates. This analogy becomes stronger with the rise of social media, such as Facebook, where links between personal pages represent relationships in a social network along which information propagates from person to person.\nSuch propagation is similar to the spreading activation that neural networks in the brain use to process information in a parallel, distributed manner.\n\nAlthough some of the underlying ideas were already expressed by Nikola Tesla in the late 19th century and were written about by many others before him, the term “global brain” was coined in 1982 by Peter Russell in his book \"The Global Brain\". How the Internet might be developed to achieve this was set out in 1986. The first peer-reviewed article on the subject was published by Gottfried Mayer-Kress in 1995, while the first algorithms that could turn the world-wide web into a collectively intelligent network were proposed by Francis Heylighen and Johan Bollen in 1996.\n\nReviewing the strands of intellectual history that contributed to the global brain hypothesis, Francis Heylighen distinguishes four perspectives: \"“organicism”\", \"“encyclopedism”\", \"“emergentism”\" and \"“evolutionary cybernetics”\". He asserts that these developed in relative independence but now are converging in his own scientific re-formulation.\n\nIn the 19th century, the sociologist Herbert Spencer saw society as a social organism and reflected about its need for a nervous system. Entomologist William Wheeler developed the concept of the ant colony as a spatially extended organism, and in the 1930s he coined the term superorganism to describe such an entity. This concept was later adopted by thinkers such as Gregory Stock in his book Metaman and Joel de Rosnay to describe planetary society as a superorganism.\n\nThe mental aspects of such an organic system at the planetary level were perhaps first broadly elaborated by palaeontologist and Jesuit priest Pierre Teilhard de Chardin. In 1945, he described a coming “planetisation” of humanity, which he saw as the next phase of accelerating human “socialisation”. Teilhard described both socialization and planetization as irreversible, irresistible processes of \"macrobiological development\" culminating in the emergence of a noosphere, or global mind (see Emergentism below).\n\nThe more recent living systems theory describes both organisms and social systems in terms of the \"critical subsystems\" (\"organs\") they need to contain in order to survive, such as an internal transport system, a resource reserve, and a decision-making system. This theory has inspired several thinkers, including Peter Russell and Francis Heylighen to define the global brain as the network of information processing subsystems for the planetary social system.\n\nIn the perspective of encyclopedism, the emphasis is on developing a universal knowledge network. The first systematic attempt to create such an integrated system of the world's knowledge was the 18th century \"Encyclopédie\" of Denis Diderot and Jean le Rond d'Alembert. However, by the end of the 19th century, the amount of knowledge had become too large to be published in a single synthetic volume. To tackle this problem, Paul Otlet founded the science of documentation, now called information science. In the 1930s he envisaged a World Wide Web-like system of associations between documents and telecommunication links that would make all the world's knowledge available immediately to anybody. H. G. Wells proposed a similar vision of a collaboratively developed world encyclopedia that would be constantly updated by a global university-like institution. He called this a World Brain, as it would function as a continuously updated memory for the planet, although the image of humanity acting informally as a more organic global brain is a recurring motif in other of his works.\n\nTim Berners-Lee, the inventor of the World Wide Web, too, was inspired by the free-associative possibilities of the brain for his invention. The brain can link different kinds of information without any apparent link otherwise; Berners-Lee thought that computers could become much more powerful if they could imitate this functioning, i.e. make links between any arbitrary piece of information. The most powerful implementation of encyclopedism to date is Wikipedia, which integrates the associative powers of the world-wide-web with the collective intelligence of its millions of contributors, approaching the ideal of a global memory. The Semantic web, also first proposed by Berners-Lee, is a system of protocols to make the pieces of knowledge and their links readable by machines, so that they could be used to make automatic inferences, thus providing this brain-like network with some capacity for autonomous \"thinking\" or reflection.\n\nThis approach focuses on the emergent aspects of the evolution and development of complexity, including the spiritual, psychological, and moral-ethical aspects of the global brain, and is at present the most speculative approach. The global brain is here seen as a natural and emergent process of planetary evolutionary development. Here again Pierre Teilhard de Chardin attempted a synthesis of science, social values, and religion in his The Phenomenon of Man, which argues that the \"telos\" (drive, purpose) of universal evolutionary process is the development of greater levels of both complexity and consciousness. Teilhard proposed that if life persists then planetization, as a biological process producing a global brain, would necessarily also produce a global mind, a new level of planetary consciousness and a technologically supported network of thoughts which he called the \"noosphere\". Teilhard's proposed technological layer for the noosphere can be interpreted as an early anticipation of the Internet and the Web.\n\nPhysicist and philosopher Peter Russell elaborates a similar view, and stresses the importance of personal spiritual growth, in order to build and to achieve synergy with the spiritual dimension of the emerging superorganism. This approach is most popular in New Age circles, which emphasize growth in consciousness rather than scientific modelling or the implementation of technological and social systems.\n\nSystems theorists and cyberneticists commonly describe the emergence of a higher order system in evolutionary development as a “metasystem transition” (a concept introduced by Valentin Turchin) or a “major evolutionary transition”. Such a metasystem consists of a group of subsystems that work together in a coordinated, goal-directed manner. It is as such much more powerful and intelligent than its constituent systems. Francis Heylighen has argued that the global brain is an emerging metasystem with respect to the level of individual human intelligence, and investigated the specific evolutionary mechanisms that promote this transition\n\nIn this scenario, the Internet fulfils the role of the network of “nerves” that interconnect the subsystems and thus coordinates their activity. The cybernetic approach makes it possible to develop mathematical models and simulations of the processes of self-organization through which such coordination and collective intelligence emerges.\n\nIn 1994 Kevin Kelly, in his popular book \"\", posited the emergence of a \"hive mind\" from a discussion of cybernetics and evolutionary biology.\n\nIn 1996, Francis Heylighen and Ben Goertzel founded the Global Brain group, a discussion forum grouping most of the researchers that had been working on the subject of the global brain to further investigate this phenomenon. The group organized the first international conference on the topic in 2001 at the Vrije Universiteit Brussel.\n\nAfter a period of relative neglect, the Global Brain idea has recently seen a resurgence in interest, in part due to talks given on the topic by Tim O'Reilly, the Internet forecaster who popularized the term Web 2.0, and Yuri Milner, the social media investor. In January 2012, the Global Brain Institute (GBI) was founded at the Vrije Universiteit Brussel to develop a mathematical theory of the “brainlike” propagation of information across the Internet. In the same year, Thomas W. Malone and collaborators from the MIT Center for Collective Intelligence have started to explore how the global brain could be “programmed” to work more effectively, using mechanisms of collective intelligence. The complexity scientist Dirk Helbing and his NervousNet group have recently started developing a \"Planetary Nervous System\", which includes a \"Global Participatory Platform\", as part of the large-scale FuturICT project, thus preparing some of the groundwork for a Global Brain.\n\nIn July 2017 Elon Musk founded the company Neuralink, which aims to create a Neural Lace, which is a concept invented by the novelist Iain M. Banks and basically refers to a machine interface woven into the brain, to allow the user to access all available human information. A core driver behind this business idea is Mr Musk’s argument, that human beings soon have to embrace brain implants to stay relevant in a world which, he believes, will soon be dominated by artificial intelligence. The firm raised $27m from 12 Investors in 2017 .\n\nA common criticism of the idea that humanity would become directed by a global brain is that this would reduce individual diversity and freedom, and lead to mass surveillance. This criticism is inspired by totalitarian forms of government, as exemplified by George Orwell's character of \"Big Brother\". It is also inspired by the analogy between collective intelligence or swarm intelligence and insect societies, such as beehives and ant colonies, in which individuals are essentially interchangeable. In a more extreme view, the global brain has been compared with the Borg, the race of collectively thinking cyborgs conceived by the Star Trek science fiction franchise.\n\nGlobal brain theorists reply that the emergence of distributed intelligence would lead to the exact opposite of this vision. The reason is that effective collective intelligence requires diversity of opinion, decentralization and individual independence, as demonstrated by James Surowiecki in his book The Wisdom of Crowds. Moreover, a more distributed form of decision-making would decrease the power of governments, corporations or political leaders, thus increasing democratic participation and reducing the dangers of totalitarian control.\n\n\n\n\nFor more references, check the GBI bibliography:\n\n"}
{"id": "52613646", "url": "https://en.wikipedia.org/wiki?curid=52613646", "title": "Globalization of sports", "text": "Globalization of sports\n\nGlobalization of sports refers to the process of expansion of the idea of sport across the world and phenomena is how that are associated with it.The field of sports in the 20th-21st century was naturally influenced by the process of globalisation.Globalization not only impacts the way in which sports are conducted and organised but also how they are perceived and what they mean in today's world.\n\nThe roots of modern sports can be found in the mid-nineteenth century in Great Britain and the United States where first professional sports were organised in mining and industrial towns and cities. Back then, sport competition was conducted mostly on local and national level. First signs of globalization in that matter appeared because of the global hegemonic position that Great Britain had in the nineteenth century. Due to the British influence, popularity of such sports as football, rugby or cricket grew rapidly throughout the world, very often replacing traditional local games.\n\nAnother step in the globalization of sports was taken with the establishment of the International Olympic Committee in 1894 and the revival of the Olympics Games in 1896. The first modern Olympics Games were organised in its ancient birthplace of Athens and attracted athletes from fourteen nations. Despite that most of the participants were European, the Olympics Games of 1896 initiated regularly held international sport competition that soon spread on a global scale. In 1904 in Paris, the International Federation of Association Football (FIFA) was founded by representatives of France, Belgium, Denmark, Netherlands, Spain, Sweden and Switzerland. Shortly after, other European associations joined and by 1909 FIFA was consisted only from nations of the “old continent”. This state of affairs did not last long and South Africa was the first country from overseas that joined the Federation followed by Argentina and Chile (1912) and the United States (1913). This allowed FIFA to start its global activities and in 1930 the first FIFA World Cup was organised. The tournament which was hosted in Uruguay was announced as a great success despite that only four European teams were able to participate. Meanwhile other international sport federations started appearing and after the World War II, when the United States rose as a global power, “American” sports like basketball and volleyball began getting more popularity around the world. International Volleyball Federation (FIVB) organised their first World Championship in 1949 while International Basketball Federation (FIBA) hosted theirs in 1950.\n\nLater on, globalization of sports was fueled by the expansion of technology and the introduction of commercial aspects to sports. On one hand newspaper, radio and especially television exposed sports to the wider international audience (first FIFA World Cup was televised in 1956 and first Summer Olympics in 1960), on the other commercial advertising allowed to profit from them. \n\nSince the end of the world it was said that twentieth century the globalization of sports rapidly accelerated by bringing television and corporate sponsorship. It led to the commercialization of sport and gave birth to the sport industry. Therefore, in the context of globalization, sport in the contemporary world can be characterised by the following tendencies:\n\n\n\n\n\n\nGlobalization of sports has also a negative impact that can be visible in the following issues:\n\n\n"}
{"id": "19283335", "url": "https://en.wikipedia.org/wiki?curid=19283335", "title": "Great Depression", "text": "Great Depression\n\nThe Great Depression was a severe worldwide economic depression that took place mostly during the 1930s, beginning in the United States. The timing of the Great Depression varied across nations; in most countries it started in 1929 and lasted until the late-1930s. It was the longest, deepest, and most widespread depression of the 20th century. In the 21st century, the Great Depression is commonly used as an example of how intensely the world's economy can decline.\n\nThe Great Depression started in the United States after a major fall in stock prices that began around September 4, 1929, and became worldwide news with the stock market crash of October 29, 1929 (known as Black Tuesday). Between 1929 and 1932, worldwide gross domestic product (GDP) fell by an estimated 15%. By comparison, worldwide GDP fell by less than 1% from 2008 to 2009 during the Great Recession. Some economies started to recover by the mid-1930s. However, in many countries the negative effects of the Great Depression lasted until the beginning of World War II.\n\nThe Great Depression had devastating effects in countries both rich and poor. Personal income, tax revenue, profits and prices dropped, while international trade plunged by more than 50%. Unemployment in the U.S. rose to 25% and in some countries rose as high as 33%.\n\nCities around the world were hit hard, especially those dependent on heavy industry. Construction was virtually halted in many countries. Farming communities and rural areas suffered as crop prices fell by about 60%. Facing plummeting demand with few alternative sources of jobs, areas dependent on primary sector industries such as mining and logging suffered the most.\n\nEconomic historians usually attribute the start of the Great Depression to the sudden devastating collapse of U.S. stock market prices on October 29, 1929, known as Black Tuesday. However, some dispute this conclusion and see the stock crash as a symptom, rather than a cause, of the Great Depression.\n\nEven after the Wall Street Crash of 1929 optimism persisted for some time. John D. Rockefeller said \"These are days when many are discouraged. In the 93 years of my life, depressions have come and gone. Prosperity has always returned and will again.\" The stock market turned upward in early 1930, returning to early 1929 levels by April. This was still almost 30% below the peak of September 1929.\n\nTogether, government and business spent more in the first half of 1930 than in the corresponding period of the previous year. On the other hand, consumers, many of whom had suffered severe losses in the stock market the previous year, cut back their expenditures by 10%. In addition, beginning in the mid-1930s, a severe drought ravaged the agricultural heartland of the U.S.\n\nBy mid-1930, interest rates had dropped to low levels, but expected deflation and the continuing reluctance of people to borrow meant that consumer spending and investment were depressed. By May 1930, automobile sales had declined to below the levels of 1928. Prices in general began to decline, although wages held steady in 1930. Then a deflationary spiral started in 1931. Farmers faced a worse outlook; declining crop prices and a Great Plains drought crippled their economic outlook. At its peak, the Great Depression saw nearly 10% of all Great Plains farms change hands despite federal assistance.\n\nThe decline in the U.S. economy was the factor that pulled down most other countries at first; then, internal weaknesses or strengths in each country made conditions worse or better. Frantic attempts to shore up the economies of individual nations through protectionist policies, such as the 1930 U.S. Smoot–Hawley Tariff Act and retaliatory tariffs in other countries, exacerbated the collapse in global trade. By 1933, the economic decline had pushed world trade to one-third of its level just four years earlier.\n\nChange in economic indicators 1929–32\n\nThe two classical competing theories of the Great Depression are the Keynesian (demand-driven) and the monetarist explanation. There are also various heterodox theories that downplay or reject the explanations of the Keynesians and monetarists. The consensus among demand-driven theories is that a large-scale loss of confidence led to a sudden reduction in consumption and investment spending. Once panic and deflation set in, many people believed they could avoid further losses by keeping clear of the markets. Holding money became profitable as prices dropped lower and a given amount of money bought ever more goods, exacerbating the drop in demand. Monetarists believe that the Great Depression started as an ordinary recession, but the shrinking of the money supply greatly exacerbated the economic situation, causing a recession to descend into the Great Depression.\n\nEconomists and economic historians are almost evenly split as to whether the traditional monetary explanation that monetary forces were the primary cause of the Great Depression is right, or the traditional Keynesian explanation that a fall in autonomous spending, particularly investment, is the primary explanation for the onset of the Great Depression. Today the controversy is of lesser importance since there is mainstream support for the debt deflation theory and the expectations hypothesis that building on the monetary explanation of Milton Friedman and Anna Schwartz add non-monetary explanations.\n\nThere is consensus that the Federal Reserve System should have cut short the process of monetary deflation and banking collapse. If they had done this, the economic downturn would have been far less severe and much shorter.\n\nBritish economist John Maynard Keynes argued in \"The General Theory of Employment, Interest and Money\" that lower aggregate expenditures in the economy contributed to a massive decline in income and to employment that was well below the average. In such a situation, the economy reached equilibrium at low levels of economic activity and high unemployment.\n\nKeynes' basic idea was simple: to keep people fully employed, governments have to run deficits when the economy is slowing, as the private sector would not invest enough to keep production at the normal level and bring the economy out of recession. Keynesian economists called on governments during times of economic crisis to pick up the slack by increasing government spending and/or cutting taxes.\n\nAs the Depression wore on, Franklin D. Roosevelt tried public works, farm subsidies, and other devices to restart the U.S. economy, but never completely gave up trying to balance the budget. According to the Keynesians, this improved the economy, but Roosevelt never spent enough to bring the economy out of recession until the start of World War II.\n\nMonetarists follow the explanation given by Milton Friedman and Anna J. Schwartz. They argued that the Great Depression was caused by the banking crisis that caused one-third of all banks to vanish, a reduction of bank shareholder wealth and more importantly monetary contraction of 35%, which they called \"The Great Contraction.\" This caused a price drop of 33% (deflation). By not lowering interest rates, by not increasing the monetary base and by not injecting liquidity into the banking system to prevent it from crumbling, the Federal Reserve passively watched the transformation of a normal recession into the Great Depression. Friedman and Schwartz argued that the downward turn in the economy, starting with the stock market crash, would merely have been an ordinary recession if the Federal Reserve had taken aggressive action. This view was endorsed by Fed Governor Ben Bernanke in a speech honoring Friedman and Schwartz with this statement:\n\nThe Federal Reserve allowed some large public bank failures – particularly that of the New York Bank of United States – which produced panic and widespread runs on local banks, and the Federal Reserve sat idly by while banks collapsed. Friedman and Schwartz argued that, if the Fed had provided emergency lending to these key banks, or simply bought government bonds on the open market to provide liquidity and increase the quantity of money after the key banks fell, all the rest of the banks would not have fallen after the large ones did, and the money supply would not have fallen as far and as fast as it did.\n\nWith significantly less money to go around, businesses could not get new loans and could not even get their old loans renewed, forcing many to stop investing. This interpretation blames the Federal Reserve for inaction, especially the New York branch.\n\nOne reason why the Federal Reserve did not act to limit the decline of the money supply was the gold standard. At that time, the amount of credit the Federal Reserve could issue was limited by the Federal Reserve Act, which required 40% gold backing of Federal Reserve Notes issued. By the late 1920s, the Federal Reserve had almost hit the limit of allowable credit that could be backed by the gold in its possession. This credit was in the form of Federal Reserve demand notes. A \"promise of gold\" is not as good as \"gold in the hand\", particularly when they only had enough gold to cover 40% of the Federal Reserve Notes outstanding. During the bank panics a portion of those demand notes were redeemed for Federal Reserve gold. Since the Federal Reserve had hit its limit on allowable credit, any reduction in gold in its vaults had to be accompanied by a greater reduction in credit. On April 5, 1933, President Roosevelt signed Executive Order 6102 making the private ownership of gold certificates, coins and bullion illegal, reducing the pressure on Federal Reserve gold.\n\nFrom the point of view of today's mainstream schools of economic thought, government should strive to keep the interconnected macroeconomic aggregates money supply and/or aggregate demand on a stable growth path. When threatened by the forecast of a depression central banks should pour liquidity into the banking system and the government should cut taxes and accelerate spending in order to keep the nominal money stock and total nominal demand from collapsing. At the beginning of the Great Depression most economists believed in Say's law and the self-equilibrating powers of the market and failed to explain the severity of the Depression. Outright leave-it-alone liquidationism was a position mainly held by the Austrian School. The liquidationist position was that a depression is good medicine. The idea was the benefit of a depression was to liquidate failed investments and businesses that have been made obsolete by technological development in order to release factors of production (capital and labor) from unproductive uses so that these could be redeployed in other sectors of the technologically dynamic economy. They argued that even if self-adjustment of the economy took mass bankruptcies, then so be it. An increasingly common view among economic historians is that the adherence of some Federal Reserve policymakers to the liquidationist thesis led to disastrous consequences. Regarding the policies of President Hoover, economists like Barry Eichengreen and J. Bradford DeLong point out that President Hoover tried to keep the federal budget balanced until 1932, when he lost confidence in his Secretary of the Treasury Andrew Mellon and replaced him. Despite liquidationist expectations, a large proportion of the capital stock was not redeployed but vanished during the first years of the Great Depression. According to a study by Olivier Blanchard and Lawrence Summers, the recession caused a drop of net capital accumulation to pre-1924 levels by 1933. Milton Friedman called the leave-it-alone liquidationism \"dangerous nonsense\". He wrote:\nThe monetary explanation has two weaknesses. First it is not able to explain why the demand for money was falling more rapidly than the supply during the initial downturn in 1930–31. Second it is not able to explain why in March 1933 a recovery took place although short term interest rates remained close to zero and the Money supply was still falling. These questions are addressed by modern explanations that build on the monetary explanation of Milton Friedman and Anna Schwartz but add non-monetary explanations.\n\nIrving Fisher argued that the predominant factor leading to the Great Depression was a vicious circle of deflation and growing over-indebtedness. He outlined nine factors interacting with one another under conditions of debt and deflation to create the mechanics of boom to bust. The chain of events proceeded as follows:\n\nDuring the Crash of 1929 preceding the Great Depression, margin requirements were only 10%. Brokerage firms, in other words, would lend $9 for every $1 an investor had deposited. When the market fell, brokers called in these loans, which could not be paid back. Banks began to fail as debtors defaulted on debt and depositors attempted to withdraw their deposits \"en masse\", triggering multiple bank runs. Government guarantees and Federal Reserve banking regulations to prevent such panics were ineffective or not used. Bank failures led to the loss of billions of dollars in assets.\n\nOutstanding debts became heavier, because prices and incomes fell by 20–50% but the debts remained at the same dollar amount. After the panic of 1929, and during the first 10 months of 1930, 744 U.S. banks failed. (In all, 9,000 banks failed during the 1930s). By April 1933, around $7 billion in deposits had been frozen in failed banks or those left unlicensed after the March Bank Holiday. Bank failures snowballed as desperate bankers called in loans which the borrowers did not have time or money to repay. With future profits looking poor, capital investment and construction slowed or completely ceased. In the face of bad loans and worsening future prospects, the surviving banks became even more conservative in their lending. Banks built up their capital reserves and made fewer loans, which intensified deflationary pressures. A vicious cycle developed and the downward spiral accelerated.\n\nThe liquidation of debt could not keep up with the fall of prices which it caused. The mass effect of the stampede to liquidate increased the value of each dollar owed, relative to the value of declining asset holdings. The very effort of individuals to lessen their burden of debt effectively increased it. Paradoxically, the more the debtors paid, the more they owed. This self-aggravating process turned a 1930 recession into a 1933 great depression.\n\nFisher's debt-deflation theory initially lacked mainstream influence because of the counter-argument that debt-deflation represented no more than a redistribution from one group (debtors) to another (creditors). Pure re-distributions should have no significant macroeconomic effects.\n\nBuilding on both the monetary hypothesis of Milton Friedman and Anna Schwartz as well as the debt deflation hypothesis of Irving Fisher, Ben Bernanke developed an alternative way in which the financial crisis affected output. He builds on Fisher's argument that dramatic declines in the price level and nominal incomes lead to increasing real debt burdens which in turn leads to debtor insolvency and consequently leads to lowered aggregate demand, a further decline in the price level then results in a debt deflationary spiral. According to Bernanke, a small decline in the price level simply reallocates wealth from debtors to creditors without doing damage to the economy. But when the deflation is severe falling asset prices along with debtor bankruptcies lead to a decline in the nominal value of assets on bank balance sheets. Banks will react by tightening their credit conditions, that in turn leads to a credit crunch which does serious harm to the economy. A credit crunch lowers investment and consumption and results in declining aggregate demand which additionally contributes to the deflationary spiral.\n\nSince economic mainstream turned to the new neoclassical synthesis, expectations are a central element of macroeconomic models. According to Peter Temin, Barry Wigmore, Gauti B. Eggertsson and Christina Romer, the key to recovery and to ending the Great Depression was brought about by a successful management of public expectations. The thesis is based on the observation that after years of deflation and a very severe recession important economic indicators turned positive in March 1933 when Franklin D. Roosevelt took office. Consumer prices turned from deflation to a mild inflation, industrial production bottomed out in March 1933, and investment doubled in 1933 with a turnaround in March 1933. There were no monetary forces to explain that turn around. Money supply was still falling and short term interest rates remained close to zero. Before March 1933 people expected further deflation and a recession so that even interest rates at zero did not stimulate investment. But when Roosevelt announced major regime changes people began to expect inflation and an economic expansion. With these positive expectations, interest rates at zero began to stimulate investment just as they were expected to do. Roosevelt's fiscal and monetary policy regime change helped to make his policy objectives credible. The expectation of higher future income and higher future inflation stimulated demand and investments. The analysis suggests that the elimination of the policy dogmas of the gold standard, a balanced budget in times of crises and small government led endogenously to a large shift in expectation that accounts for about 70–80 percent of the recovery of output and prices from 1933 to 1937. If the regime change had not happened and the Hoover policy had continued, the economy would have continued its free fall in 1933, and output would have been 30% lower in 1937 than in 1933.\n\nThe recession of 1937–38, which slowed down economic recovery from the Great Depression, is explained by fears of the population that the moderate tightening of the monetary and fiscal policy in 1937 would be first steps to a restoration of the pre-March 1933 policy regime.\n\nTwo prominent theorists in the Austrian School on the Great Depression include Austrian economist Friedrich Hayek and American economist Murray Rothbard, who wrote \"America's Great Depression\" (1963). In their view, much like the monetarists, the Federal Reserve (of which was created in 1913) shoulders much of the blame; however unlike the Monetarists, they argue that the key cause of the Depression was the expansion of the money supply in the 1920s, of which led to an unsustainable credit-driven boom.\n\nIn the Austrian view it was this inflation of the money supply that led to an unsustainable boom in both asset prices (stocks and bonds) and capital goods. Therefore, by the time the Federal Reserve tightened in 1928 it was far too late to prevent an economic contraction. In February 1929 Hayek published a paper predicting the Federal Reserve's actions would lead to a crisis starting in the stock and credit markets.\n\nAccording to Rothbard, the government support for failed enterprises and efforts to keep wages above their market values actually prolonged the Depression. Unlike Rothbard, after 1970 Hayek believed that the Federal Reserve had further contributed to the problems of the Depression by permitting the money supply to shrink during the earliest years of the Depression. However, during the Depression (in 1932 and in 1934) Hayek had criticized both the Federal Reserve and the Bank of England for not taking a more contractionary stance.\n\nHans Sennholz, another prominent Austrian economist, argued that most boom and busts that plagued the American economy, such as those in 1819–20, 1839–43, 1857–60, 1873–78, 1893–97, and 1920–21, were generated by government creating a boom through easy money and credit, which was soon followed by the inevitable bust. The spectacular crash of 1929 followed five years of reckless credit expansion by the Federal Reserve System under the Coolidge Administration. The passing of the Sixteenth Amendment, the passage of The Federal Reserve Act, rising government deficits, the passage of the Hawley-Smoot Tariff Act, and the Revenue Act of 1932, exacerbated and prolonged the crisis.\n\nLudwig von Mises wrote in the 1930s: \"Credit expansion cannot increase the supply of real goods. It merely brings about a rearrangement. It diverts capital investment away from the course prescribed by the state of economic wealth and market conditions. It causes production to pursue paths which it would not follow unless the economy were to acquire an increase in material goods. As a result, the upswing lacks a solid base. It is not a real prosperity. It is illusory prosperity. It did not develop from an increase in economic wealth, i.e. the accumulation of savings made available for productive investment. Rather, it arose because the credit expansion created the illusion of such an increase. Sooner or later, it must become apparent that this economic situation is built on sand.\"\n\nTwo economists of the 1920s, Waddill Catchings and William Trufant Foster, popularized a theory that influenced many policy makers, including Herbert Hoover, Henry A. Wallace, Paul Douglas, and Marriner Eccles. It held the economy produced more than it consumed, because the consumers did not have enough income. Thus the unequal distribution of wealth throughout the 1920s caused the Great Depression.\n\nAccording to this view, the root cause of the Great Depression was a global over-investment in heavy industry capacity compared to wages and earnings from independent businesses, such as farms. The proposed solution was for the government to pump money into the consumers' pockets. That is, it must redistribute purchasing power, maintaining the industrial base, and re-inflating prices and wages to force as much of the inflationary increase in purchasing power into consumer spending. The economy was overbuilt, and new factories were not needed. Foster and Catchings recommended federal and state governments to start large construction projects, a program followed by Hoover and Roosevelt.\n\nThe first three decades of the 20th century saw economic output surge with electrification, mass production and motorized farm machinery, and because of the rapid growth in productivity there was a lot of excess production capacity and the work week was being reduced.\n\nThe dramatic rise in productivity of major industries in the U.S. and the effects of productivity on output, wages and the work week are discussed by Spurgeon Bell in his book \"Productivity, Wages, and National Income\" (1940).\n\nThe gold standard was the primary transmission mechanism of the Great Depression. Even countries that did not face bank failures and a monetary contraction first hand were forced to join the deflationary policy since higher interest rates in countries that performed a deflationary policy led to a gold outflow in countries with lower interest rates. Under the gold standard's price–specie flow mechanism, countries that lost gold but nevertheless wanted to maintain the gold standard had to permit their money supply to decrease and the domestic price level to decline (deflation).\n\nThere is also consensus that protectionist policies such as the Smoot–Hawley Tariff Act helped to worsen the depression.\n\nSome economic studies have indicated that just as the downturn was spread worldwide by the rigidities of the Gold Standard, it was suspending gold convertibility (or devaluing the currency in gold terms) that did the most to make recovery possible.\n\nEvery major currency left the gold standard during the Great Depression. The UK was the first to do so. Facing speculative attacks on the pound and depleting gold reserves, in September 1931 the Bank of England ceased exchanging pound notes for gold and the pound was floated on foreign exchange markets.\n\nThe UK, Japan, and the Scandinavian countries left the gold standard in 1931. Other countries, such as Italy and the U.S., remained on the gold standard into 1932 or 1933, while a few countries in the so-called \"gold bloc\", led by France and including Poland, Belgium and Switzerland, stayed on the standard until 1935–36.\n\nAccording to later analysis, the earliness with which a country left the gold standard reliably predicted its economic recovery. For example, The UK and Scandinavia, which left the gold standard in 1931, recovered much earlier than France and Belgium, which remained on gold much longer. Countries such as China, which had a silver standard, almost avoided the depression entirely. The connection between leaving the gold standard as a strong predictor of that country's severity of its depression and the length of time of its recovery has been shown to be consistent for dozens of countries, including developing countries. This partly explains why the experience and length of the depression differed between national economies.\n\nMany economists have argued that the sharp decline in international trade after 1930 helped to worsen the depression, especially for countries significantly dependent on foreign trade. In a 1995 survey of American economic historians, two-thirds agreed that the Smoot–Hawley Tariff Act at least worsened the Great Depression. Most historians and economists partly blame the American Smoot–Hawley Tariff Act (enacted June 17, 1930) for worsening the depression by seriously reducing international trade and causing retaliatory tariffs in other countries. While foreign trade was a small part of overall economic activity in the U.S. and was concentrated in a few businesses like farming, it was a much larger factor in many other countries. The average \"ad valorem\" rate of duties on dutiable imports for 1921–25 was 25.9% but under the new tariff it jumped to 50% during 1931–35. In dollar terms, American exports declined over the next four (4) years from about $5.2 billion in 1929 to $1.7 billion in 1933; so, not only did the physical volume of exports fall, but also the prices fell by about 1/3 as written. Hardest hit were farm commodities such as wheat, cotton, tobacco, and lumber.\n\nGovernments around the world took various steps into spending less money on foreign goods such as: \"imposing tariffs, import quotas, and exchange controls\". These restrictions formed a lot of tension between trade nations, causing a major deduction during the depression. Not all countries enforced the same measures of protectionism. Some countries raised tariffs drastically and enforced severe restrictions on foreign exchange transactions, while other countries condensed \"trade and exchange restrictions only marginally\":\n\nThe consensus view among economists and economic historians is that the passage of the Smoot-Hawley Tariff exacerbated the Great Depression, although there is disagreement as to how much. In the popular view, the Smoot-Hawley Tariff was a leading cause of the depression. However, many economists hold the opinion that the tariff act did not greatly worsen the depression.\n\nThe financial crisis escalated out of control in mid-1931, starting with the collapse of the Credit Anstalt in Vienna in May. This put heavy pressure on Germany, which was already in political turmoil. With the rise in violence of Nazi and communist movements, as well as investor nervousness at harsh government financial policies. Investors withdrew their short-term money from Germany, as confidence spiraled downward. The Reichsbank lost 150 million marks in the first week of June, 540 million in the second, and 150 million in two days, June 19–20. Collapse was at hand. U.S. President Herbert Hoover called for a moratorium on Payment of war reparations. This angered Paris, which depended on a steady flow of German payments, but it slowed the crisis down and the moratorium, was agreed to in July 1931. International conference in London later in July produced no agreements but on August 19 a standstill agreement froze Germany's foreign liabilities for six months. Germany received emergency funding from private banks in New York as well as the Bank of International Settlements and the Bank of England. The funding only slowed the process; it's nothing. Industrial failures began in Germany, a major bank closed in July and a two-day holiday for all German banks was declared. Business failures more frequent in July, and spread to Romania and Hungary. The crisis continued to get worse in Germany, bringing political upheaval that finally led to the coming to power of Hitler's Nazi regime in January 1933.\n\nThe world financial crisis now began to overwhelm Britain; investors across the world started withdrawing their gold from London at the rate of £2.5 million per day. Credits of £25 millions each from the Bank of France and the Federal Reserve Bank of New York and an issue of £15 millions fiduciary note slowed, but did not reverse the British crisis. The financial crisis now caused a major political crisis in Britain in August 1931. With deficits mounting, the bankers demanded a balanced budget; the divided cabinet of Prime Minister Ramsay MacDonald's Labour government agreed; it proposed to raise taxes, cut spending and most controversially, to cut unemployment benefits 20%. The attack on welfare was totally unacceptable to the Labour movement. MacDonald wanted to resign, but King George V insisted he remain and form an all-party coalition \"National government.\" The Conservative and Liberals parties signed on, along with a small cadre of Labour, but the vast majority of Labour leaders denounced MacDonald as a traitor for leading the new government. Britain went off the gold standard, and suffered relatively less than other major countries in the Great Depression. In the 1931 British election the Labour Party was virtually destroyed, leaving MacDonald as Prime Minister for a largely Conservative coalition.\n\nIn most countries of the world, recovery from the Great Depression began in 1933. In the U.S., recovery began in early 1933, but the U.S. did not return to 1929 GNP for over a decade and still had an unemployment rate of about 15% in 1940, albeit down from the high of 25% in 1933.\n\nThere is no consensus among economists regarding the motive force for the U.S. economic expansion that continued through most of the Roosevelt years (and the 1937 recession that interrupted it). The common view among most economists is that Roosevelt's New Deal policies either caused or accelerated the recovery, although his policies were never aggressive enough to bring the economy completely out of recession. Some economists have also called attention to the positive effects from expectations of reflation and rising nominal interest rates that Roosevelt's words and actions portended. It was the rollback of those same reflationary policies that led to the interruption of a recession beginning in late 1937. One contributing policy that reversed reflation was the Banking Act of 1935, which effectively raised reserve requirements, causing a monetary contraction that helped to thwart the recovery. GDP returned to its upward trend in 1938.\n\nAccording to Christina Romer, the money supply growth caused by huge international gold inflows was a crucial source of the recovery of the United States economy, and that the economy showed little sign of self-correction. The gold inflows were partly due to devaluation of the U.S. dollar and partly due to deterioration of the political situation in Europe. In their book, \"A Monetary History of the United States\", Milton Friedman and Anna J. Schwartz also attributed the recovery to monetary factors, and contended that it was much slowed by poor management of money by the Federal Reserve System. Former Chairman of the Federal Reserve Ben Bernanke agreed that monetary factors played important roles both in the worldwide economic decline and eventual recovery. Bernanke also saw a strong role for institutional factors, particularly the rebuilding and restructuring of the financial system, and pointed out that the Depression should be examined in an international perspective.\n\nWomen's primary role were as housewives; without a steady flow of family income, their work became much harder in dealing with food and clothing and medical care. Birthrates fell everywhere, as children were postponed until families could financially support them. The average birthrate for 14 major countries fell 12% from 19.3 births per thousand population in 1930, to 17.0 in 1935. In Canada, half of Roman Catholic women defied Church teachings and used contraception to postpone births.\n\nAmong the few women in the labor force, layoffs were less common in the white-collar jobs and they were typically found in light manufacturing work. However, there was a widespread demand to limit families to one paid job, so that wives might lose employment if their husband was employed. Across Britain, there was a tendency for married women to join the labor force, competing for part-time jobs especially.\n\nIn rural and small-town areas, women expanded their operation of vegetable gardens to include as much food production as possible. In the United States, agricultural organizations sponsored programs to teach housewives how to optimize their gardens and to raise poultry for meat and eggs. In American cities, African American women quiltmakers enlarged their activities, promoted collaboration, and trained neophytes. Quilts were created for practical use from various inexpensive materials and increased social interaction for women and promoted camaraderie and personal fulfillment.\n\nOral history provides evidence for how housewives in a modern industrial city handled shortages of money and resources. Often they updated strategies their mothers used when they were growing up in poor families. Cheap foods were used, such as soups, beans and noodles. They purchased the cheapest cuts of meat—sometimes even horse meat—and recycled the Sunday roast into sandwiches and soups. They sewed and patched clothing, traded with their neighbors for outgrown items, and made do with colder homes. New furniture and appliances were postponed until better days. Many women also worked outside the home, or took boarders, did laundry for trade or cash, and did sewing for neighbors in exchange for something they could offer. Extended families used mutual aid—extra food, spare rooms, repair-work, cash loans—to help cousins and in-laws.\n\nIn Japan, official government policy was deflationary and the opposite of Keynesian spending. Consequently, the government launched a nationwide campaign to induce households to reduce their consumption, focusing attention on spending by housewives.\n\nIn Germany, the government tried to reshape private household consumption under the Four-Year Plan of 1936 to achieve German economic self-sufficiency. The Nazi women's organizations, other propaganda agencies and the authorities all attempted to shape such consumption as economic self-sufficiency was needed to prepare for and to sustain the coming war. The organizations, propaganda agencies and authorities employed slogans that called up traditional values of thrift and healthy living. However, these efforts were only partly successful in changing the behavior of housewives.\n\nThe common view among economic historians is that the Great Depression ended with the advent of World War II. Many economists believe that government spending on the war caused or at least accelerated recovery from the Great Depression, though some consider that it did not play a very large role in the recovery. It did help in reducing unemployment.\n\nThe rearmament policies leading up to World War II helped stimulate the economies of Europe in 1937–39. By 1937, unemployment in Britain had fallen to 1.5 million. The mobilization of manpower following the outbreak of war in 1939 ended unemployment.\n\nWhen the United States entered into the war in 1941, it finally eliminated the last effects from the Great Depression and brought the U.S. unemployment rate down below 10%. In the U.S., massive war spending doubled economic growth rates, either masking the effects of the Depression or essentially ending the Depression. Businessmen ignored the mounting national debt and heavy new taxes, redoubling their efforts for greater output to take advantage of generous government contracts.\n\nThe majority of countries set up relief programs and most underwent some sort of political upheaval, pushing them to the right. Many of the countries in Europe and Latin America that were democracies saw them overthrown by some form of dictatorship or authoritarian rule, most famously in Germany in 1933. The Dominion of Newfoundland gave up democracy voluntarily.\n\nAustralia's dependence on agricultural and industrial exports meant it was one of the hardest-hit developed countries. Falling export demand and commodity prices placed massive downward pressures on wages. Unemployment reached a record high of 29% in 1932, with incidents of civil unrest becoming common. After 1932, an increase in wool and meat prices led to a gradual recovery.\n\nHarshly affected by both the global economic downturn and the Dust Bowl, Canadian industrial production had fallen to only 58% of the 1929 level by 1932, the second lowest level in the world after the United States, and well behind nations such as Britain, which fell to only 83% of the 1929 level. Total national income fell to 56% of the 1929 level, again worse than any nation apart from the United States. Unemployment reached 27% at the depth of the Depression in 1933.\n\nThe League of Nations labeled Chile the country hardest hit by the Great Depression because 80% of government revenue came from exports of copper and nitrates, which were in low demand. Chile initially felt the impact of the Great Depression in 1930, when GDP dropped 14%, mining income declined 27%, and export earnings fell 28%. By 1932, GDP had shrunk to less than half of what it had been in 1929, exacting a terrible toll in unemployment and business failures.\n\nInfluenced profoundly by the Great Depression, many national leaders promoted the development of local industry in an effort to insulate the economy from future external shocks. After six years of government austerity measures, which succeeded in reestablishing Chile's creditworthiness, Chileans elected to office during the 1938–58 period a succession of center and left-of-center governments interested in promoting economic growth by means of government intervention.\n\nPrompted in part by the devastating 1939 Chillán earthquake, the Popular Front government of Pedro Aguirre Cerda created the Production Development Corporation (Corporación de Fomento de la Producción, CORFO) to encourage with subsidies and direct investments an ambitious program of import substitution industrialization. Consequently, as in other Latin American countries, protectionism became an entrenched aspect of the Chilean economy.\n\nChina was largely unaffected by the Depression, mainly by having stuck to the Silver standard. However, the U.S. silver purchase act of 1934 created an intolerable demand on China's silver coins, and so in the end the silver standard was officially abandoned in 1935 in favor of the four Chinese national banks' \"legal note\" issues. China and the British colony of Hong Kong, which followed suit in this regard in September 1935, would be the last to abandon the silver standard. In addition, the Nationalist Government also acted energetically to modernize the legal and penal systems, stabilize prices, amortize debts, reform the banking and currency systems, build railroads and highways, improve public health facilities, legislate against traffic in narcotics and augment industrial and agricultural production. On November 3, 1935, the government instituted the fiat currency (fapi) reform, immediately stabilizing prices and also raising revenues for the government.\n\nThe sharp fall in commodity prices, and the steep decline in exports, hurt the economies of the European colonies in Africa and Asia. The agricultural sector was especially hard hit. For example, sisal had recently become a major export crop in Kenya and Tanganyika. During the depression it suffered severely from low prices and marketing problems that affected all colonial commodities in Africa. Sisal producers established centralized controls for the export of their fibre. There was widespread unemployment and hardship among peasants, labourers, colonial auxiliaries, and artisans. The budgets of colonial governments were cut, which forced the reduction in ongoing infrastructure projects, such as the building and upgrading of roads, ports and communications. The budget cuts delayed the schedule for creating systems of higher education.\n\nThe depression severely hurt the export-based Belgian Congo economy because of the drop in international demand for raw materials and for agricultural products. For example, the price of peanuts fell from 125 to 25 centimes. In some areas, as in the Katanga mining region, employment declined by 70%. In the country as a whole, the wage labour force decreased by 72.000 and many men returned to their villages. In Leopoldville, the population decreased by 33%, because of this labour migration.\n\nPolitical protests were not common. However, there was a growing demand that the paternalistic claims be honored by colonial governments to respond vigorously. The theme was that economic reforms were more urgently needed than political reforms. French West Africa launched an extensive program of educational reform centered around \"rural schools\" designed to modernize agriculture and stem the flow of under-employed farm workers to cites where unemployment was high. Students were trained in traditional arts, crafts, and farming techniques and were then expected to return to their own villages and towns.\n\nThe crisis affected France a bit later than other countries, hitting hard around 1931. While the 1920s grew at the very strong rate of 4.43% per year, the 1930s rate fell to only 0.63%.\n\nThe depression was relatively mild: unemployment peaked under 5%, the fall in production was at most 20% below the 1929 output; there was no banking crisis.\n\nHowever, the depression had drastic effects on the local economy, and partly explains the February 6, 1934 riots and even more the formation of the Popular Front, led by SFIO socialist leader Léon Blum, which won the elections in 1936. Ultra-nationalist groups also saw increased popularity, although democracy prevailed into World War II.\n\nFrance's relatively high degree of self-sufficiency meant the damage was considerably less than in nations like Germany.\n\nThe Great Depression hit Germany hard. The impact of the Wall Street Crash forced American banks to end the new loans that had been funding the repayments under the Dawes Plan and the Young Plan. The financial crisis escalated out of control and mid-1931, starting with the collapse of the Credit Anstalt in Vienna in May. This put heavy pressure on Germany, which was already in political turmoil with the rise in violence of Nazi and communist movements, as well as with investor nervousness at harsh government financial policies. Investors withdrew their short-term money from Germany, as confidence spiraled downward. The Reichsbank lost 150 million marks in the first week of June, 540 million in the second, and 150 million in two days, June 19–20. Collapse was at hand. U.S. President Herbert Hoover called for a moratorium on Payment of war reparations. This angered Paris, which depended on a steady flow of German payments, but it slowed the crisis down and the moratorium, was agreed to in July 1931. An international conference in London later in July produced no agreements but on August 19 a standstill agreement froze Germany's foreign liabilities for six months. Germany received emergency funding from private banks in New York as well as the Bank of International Settlements and the Bank of England. The funding only slowed the process. Industrial failures began in Germany, a major bank closed in July and a two-day holiday for all German banks was declared. Business failures became more frequent in July, and spread to Romania and Hungary.\n\nIn 1932, 90% of German reparation payments were cancelled. (In the 1950s, Germany repaid all its missed reparations debts.) Widespread unemployment reached 25% as every sector was hurt. The government did not increase government spending to deal with Germany's growing crisis, as they were afraid that a high-spending policy could lead to a return of the hyperinflation that had affected Germany in 1923. Germany's Weimar Republic was hit hard by the depression, as American loans to help rebuild the German economy now stopped. The unemployment rate reached nearly 30% in 1932, bolstering support for the Nazi (NSDAP) and Communist (KPD) parties, causing the collapse of the politically centrist Social Democratic Party. Hitler ran for the Presidency in 1932, and while he lost to the incumbent Hindenburg in the election, it marked a point during which both Nazi Party and the Communist parties rose in the years following the crash to altogether possess a Reichstag majority following the general election in July 1932.\n\nHitler followed an autarky economic policy, creating a network of client states and economic allies in central Europe and Latin America. By cutting wages and taking control of labor unions, plus public works spending, unemployment fell significantly by 1935. Large-scale military spending played a major role in the recovery.\n\nThe reverberations of the Great Depression hit Greece in 1932. The Bank of Greece tried to adopt deflationary policies to stave off the crises that were going on in other countries, but these largely failed. For a brief period the drachma was pegged to the U.S. dollar, but this was unsustainable given the country's large trade deficit and the only long-term effects of this were Greece's foreign exchange reserves being almost totally wiped out in 1932. Remittances from abroad declined sharply and the value of the drachma began to plummet from 77 drachmas to the dollar in March 1931 to 111 drachmas to the dollar in April, 1931. This was especially harmful to Greece as the country relied on imports from the UK, France and the Middle East for many necessities. Greece went off the gold standard in April, 1932 and declared a moratorium on all interest payments. The country also adopted protectionist policies such as import quotas, which a number of European countries did during the time period.\n\nProtectionist policies coupled with a weak drachma, stifling imports, allowed Greek industry to expand during the Great Depression. In 1939, Greek Industrial output was 179% that of 1928. These industries were for the most part \"built on sand\" as one report of the Bank of Greece put it, as without massive protection they would not have been able to survive. Despite the global depression, Greece managed to suffer comparatively little, averaging an average growth rate of 3.5% from 1932 to 1939. The dictatorial regime of Ioannis Metaxas took over the Greek government in 1936, and economic growth was strong in the years leading up to the Second World War.\n\nIcelandic post-World War I prosperity came to an end with the outbreak of the Great Depression. The Depression hit Iceland hard as the value of exports plummeted. The total value of Icelandic exports fell from 74 million kronur in 1929 to 48 million in 1932, and was not to rise again to the pre-1930 level until after 1939. Government interference in the economy increased: \"Imports were regulated, trade with foreign currency was monopolized by state-owned banks, and loan capital was largely distributed by state-regulated funds\". Due to the outbreak of the Spanish Civil War, which cut Iceland's exports of saltfish by half, the Depression lasted in Iceland until the outbreak of World War II (when prices for fish exports soared).\n\nHow much India was affected has been hotly debated. Historians have argued that the Great Depression slowed long-term industrial development. Apart from two sectors—jute and coal—the economy was little affected. However, there were major negative impacts on the jute industry, as world demand fell and prices plunged. Otherwise conditions were fairly stable. Local markets in agriculture and small-scale industry showed modest gains.\n\nFrank Barry and Mary E. Daly have argued that:\n\nThe Great Depression hit Italy very hard. As industries came close to failure they were bought out by the banks in a largely illusionary bail-out—the assets used to fund the purchases were largely worthless. This led to a financial crisis peaking in 1932 and major government intervention. The Industrial Reconstruction Institute (IRI) was formed in January 1933 and took control of the bank-owned companies, suddenly giving Italy the largest state-owned industrial sector in Europe (excluding the USSR). IRI did rather well with its new responsibilities—restructuring, modernising and rationalising as much as it could. It was a significant factor in post-1945 development. But it took the Italian economy until 1935 to recover the manufacturing levels of 1930—a position that was only 60% better than that of 1913.\n\nThe Great Depression did not strongly affect Japan. The Japanese economy shrank by 8% during 1929–31. Japan's Finance Minister Takahashi Korekiyo was the first to implement what have come to be identified as Keynesian economic policies: first, by large fiscal stimulus involving deficit spending; and second, by devaluing the currency. Takahashi used the Bank of Japan to sterilize the deficit spending and minimize resulting inflationary pressures. Econometric studies have identified the fiscal stimulus as especially effective.\n\nThe devaluation of the currency had an immediate effect. Japanese textiles began to displace British textiles in export markets. The deficit spending proved to be most profound and went into the purchase of munitions for the armed forces. By 1933, Japan was already out of the depression. By 1934, Takahashi realized that the economy was in danger of overheating, and to avoid inflation, moved to reduce the deficit spending that went towards armaments and munitions.\n\nThis resulted in a strong and swift negative reaction from nationalists, especially those in the army, culminating in his assassination in the course of the February 26 Incident. This had a chilling effect on all civilian bureaucrats in the Japanese government. From 1934, the military's dominance of the government continued to grow. Instead of reducing deficit spending, the government introduced price controls and rationing schemes that reduced, but did not eliminate inflation, which remained a problem until the end of World War II.\n\nThe deficit spending had a transformative effect on Japan. Japan's industrial production doubled during the 1930s. Further, in 1929 the list of the largest firms in Japan was dominated by light industries, especially textile companies (many of Japan's automakers, such as Toyota, have their roots in the textile industry). By 1940 light industry had been displaced by heavy industry as the largest firms inside the Japanese economy.\n\nBecause of high levels of U.S. investment in Latin American economies, they were severely damaged by the Depression. Within the region, Chile, Bolivia and Peru were particularly badly affected.\n\nBefore the 1929 crisis, links between the world economy and Latin American economies had been established through American and British investment in Latin American exports to the world. As a result, Latin Americans export industries felt the depression quickly. World prices for commodities such as wheat, coffee and copper plunged. Exports from all of Latin America to the U.S. fell in value from $1.2 billion in 1929 to $335 million in 1933, rising to $660 million in 1940.\n\nBut on the other hand, the depression led the area governments to develop new local industries and expand consumption and production. Following the example of the New Deal, governments in the area approved regulations and created or improved welfare institutions that helped millions of new industrial workers to achieve a better standard of living.\n\nFrom roughly 1931 to 1937, the Netherlands suffered a deep and exceptionally long depression. This depression was partly caused by the after-effects of the Stock Market Crash of 1929 in the U.S., and partly by internal factors in the Netherlands. Government policy, especially the very late dropping of the Gold Standard, played a role in prolonging the depression. The Great Depression in the Netherlands led to some political instability and riots, and can be linked to the rise of the Dutch national-socialist party NSB. The depression in the Netherlands eased off somewhat at the end of 1936, when the government finally dropped the Gold Standard, but real economic stability did not return until after World War II.\n\nNew Zealand was especially vulnerable to worldwide depression, as it relied almost entirely on agricultural exports to the United Kingdom for its economy. The drop in exports led to a lack of disposable income from the farmers, who were the mainstay of the local economy. Jobs disappeared and wages plummeted, leaving people desperate and charities unable to cope. Work relief schemes were the only government support available to the unemployed, the rate of which by the early 1930s was officially around 15%, but unofficially nearly twice that level (official figures excluded Māori and women). In 1932, riots occurred among the unemployed in three of the country's main cities (Auckland, Dunedin, and Wellington). Many were arrested or injured through the tough official handling of these riots by police and volunteer \"special constables\".\n\nAlready under the rule of a dictatorial junta, the Ditadura Nacional, Portugal suffered no turbulent political effects of the Depression, although António de Oliveira Salazar, already appointed Minister of Finance in 1928 greatly expanded his powers and in 1932 rose to Prime Minister of Portugal to found the Estado Novo, an authoritarian corporatist dictatorship. With the budget balanced in 1929, the effects of the depression were relaxed through harsh measures towards budget balance and autarky, causing social discontent but stability and, eventually, an impressive economic growth.\n\nIn the years immediately preceding the depression, negative developments in the island and world economies perpetuated an unsustainable cycle of subsistence for many Puerto Rican workers. The 1920s brought a dramatic drop in Puerto Rico's two primary exports, raw sugar and coffee, due to a devastating hurricane in 1928 and the plummeting demand from global markets in the latter half of the decade. 1930 unemployment on the island was roughly 36% and by 1933 Puerto Rico's per capita income dropped 30% (by comparison, unemployment in the United States in 1930 was approximately 8% reaching a height of 25% in 1933). To provide relief and economic reform, the United States government and Puerto Rican politicians such as Carlos Chardon and Luis Munoz Marin created and administered first the Puerto Rico Emergency Relief Administration (PRERA) 1933 and then in 1935, the Puerto Rico Reconstruction Administration (PRRA).\n\nAs world trade slumped, demand for South African agricultural and mineral exports fell drastically. The Carnegie Commission on Poor Whites had concluded in 1931 that nearly one third of Afrikaners lived as paupers. The social discomfort caused by the depression was a contributing factor in the 1933 split between the \"gesuiwerde\" (purified) and \"smelter\" (fusionist) factions within the National Party and the National Party's subsequent fusion with the South African Party. Unemployment programs were begun that focused primarily on the white population.\n\nThe Soviet Union was the world's sole communist state with very little international trade. Its economy was not tied to the rest of the world and was only slightly affected by the Great Depression. Its forced transformation from a rural to an industrial society succeeded in building up heavy industry, at the cost of millions of lives in rural Russia and Ukraine.\n\nAt the time of the Depression, the Soviet economy was growing steadily, fuelled by intensive investment in heavy industry. The apparent economic success of the Soviet Union at a time when the capitalist world was in crisis led many Western intellectuals to view the Soviet system favorably. Jennifer Burns wrote:\nDespite all of this, The Great Depression caused mass immigration to the Soviet Union, mostly from Finland and Germany. Soviet Russia was at first happy to help these immigrants settle, because they believed they were victims of capitalism who had come to help the Soviet cause. However, when the Soviet Union entered the war in 1941, most of these Germans and Finns were arrested and sent to Siberia, while their Russian-born children were placed in orphanages. Their fate is unknown.\n\nSpain had a relatively isolated economy, with high protective tariffs and was not one of the main countries affected by the Depression. The banking system held up well, as did agriculture.\n\nBy far the most serious negative impact came after 1936 from the heavy destruction of infrastructure and manpower by the civil war, 1936–39. Many talented workers were forced into permanent exile. By staying neutral in the Second World War, and selling to both sides, the economy avoided further disasters.\n\nBy the 1930s, Sweden had what America's \"Life magazine\" called in 1938 the \"world's highest standard of living\". Sweden was also the first country worldwide to recover completely from the Great Depression. Taking place in the midst of a short-lived government and a less-than-a-decade old Swedish democracy, events such as those surrounding Ivar Kreuger (who eventually committed suicide) remain infamous in Swedish history. The Social Democrats under Per Albin Hansson formed their first long-lived government in 1932 based on strong interventionist and welfare state policies, monopolizing the office of Prime Minister until 1976 with the sole and short-lived exception of Axel Pehrsson-Bramstorp's \"summer cabinet\" in 1936. During forty years of hegemony, it was the most successful political party in the history of Western liberal democracy.\n\nIn Thailand, then known as the Kingdom of Siam, the Great Depression contributed to the end of the absolute monarchy of King Rama VII in the Siamese revolution of 1932.\n\nThe World Depression broke at a time when the United Kingdom had still not fully recovered from the effects of the First World War more than a decade earlier. The country was driven off the gold standard in 1931.\n\nThe world financial crisis began to overwhelm Britain in 1931; investors across the world started withdrawing their gold from London at the rate of £2.5 million per day. Credits of £25 millions each from the Bank of France and the Federal Reserve Bank of New York and an issue of £15 millions fiduciary note slowed, but did not reverse the British crisis. The financial crisis now caused a major political crisis in Britain in August 1931. With deficits mounting, the bankers demanded a balanced budget; the divided cabinet of Prime Minister Ramsay MacDonald's Labour government agreed; it proposed to raise taxes, cut spending and most controversially, to cut unemployment benefits by 20%. The attack on welfare was totally unacceptable to the Labour movement. MacDonald wanted to resign, but King George V insisted he remain and form an all-party coalition \"National Government\". The Conservative and Liberals parties signed on, along with a small cadre of Labour, but the vast majority of Labour leaders denounced MacDonald as a traitor for leading the new government. Britain went off the gold standard, and suffered relatively less than other major countries in the Great Depression. In the 1931 British election, the Labour Party was virtually destroyed, leaving MacDonald as Prime Minister for a largely Conservative coalition.\n\nThe effects on the northern industrial areas of Britain were immediate and devastating, as demand for traditional industrial products collapsed. By the end of 1930 unemployment had more than doubled from 1 million to 2.5 million (20% of the insured workforce), and exports had fallen in value by 50%. In 1933, 30% of Glaswegians were unemployed due to the severe decline in heavy industry. In some towns and cities in the north east, unemployment reached as high as 70% as shipbuilding fell by 90%. The National Hunger March of September–October 1932 was the largest of a series of hunger marches in Britain in the 1920s and 1930s. About 200,000 unemployed men were sent to the work camps, which continued in operation until 1939.\n\nIn the less industrial Midlands and Southern England, the effects were short-lived and the later 1930s were a prosperous time. Growth in modern manufacture of electrical goods and a boom in the motor car industry was helped by a growing southern population and an expanding middle class. Agriculture also saw a boom during this period.\n\nHoover's first measures to combat the depression were based on voluntarism by businesses not to reduce their workforce or cut wages. But businesses had little choice and wages were reduced, workers were laid off, and investments postponed.\n\nIn June 1930 Congress approved the Smoot–Hawley Tariff Act which raised tariffs on thousands of imported items. The intent of the Act was to encourage the purchase of American-made products by increasing the cost of imported goods, while raising revenue for the federal government and protecting farmers. Other nations increased tariffs on American-made goods in retaliation, reducing international trade, and worsening the Depression.\n\nIn 1931, Hoover urged bankers to set up the National Credit Corporation so that big banks could help failing banks survive. But bankers were reluctant to invest in failing banks, and the National Credit Corporation did almost nothing to address the problem.\nBy 1932, unemployment had reached 23.6%, peaking in early 1933 at 25%. Drought persisted in the agricultural heartland, businesses and families defaulted on record numbers of loans, and more than 5,000 banks had failed. Hundreds of thousands of Americans found themselves homeless, and began congregating in shanty towns – dubbed \"Hoovervilles\" – that began to appear across the country. In response, President Hoover and Congress approved the Federal Home Loan Bank Act, to spur new home construction, and reduce foreclosures. The final attempt of the Hoover Administration to stimulate the economy was the passage of the Emergency Relief and Construction Act (ERA) which included funds for public works programs such as dams and the creation of the Reconstruction Finance Corporation (RFC) in 1932. The Reconstruction Finance Corporation was a Federal agency with the authority to lend up to $2 billion to rescue banks and restore confidence in financial institutions. But $2 billion was not enough to save all the banks, and bank runs and bank failures continued. Quarter by quarter the economy went downhill, as prices, profits and employment fell, leading to the political realignment in 1932 that brought to power Franklin Delano Roosevelt. It is important to note, however, that after volunteerism failed, Hoover developed ideas that laid the framework for parts of the New Deal.\n\nShortly after President Franklin Delano Roosevelt was inaugurated in 1933, drought and erosion combined to cause the Dust Bowl, shifting hundreds of thousands of displaced persons off their farms in the Midwest. From his inauguration onward, Roosevelt argued that restructuring of the economy would be needed to prevent another depression or avoid prolonging the current one. New Deal programs sought to stimulate demand and provide work and relief for the impoverished through increased government spending and the institution of financial reforms.\n\nDuring a \"bank holiday\" that lasted five days, the Emergency Banking Act was signed into law. It provided for a system of reopening sound banks under Treasury supervision, with federal loans available if needed. The Securities Act of 1933 comprehensively regulated the securities industry. This was followed by the Securities Exchange Act of 1934 which created the Securities and Exchange Commission. Although amended, key provisions of both Acts are still in force. Federal insurance of bank deposits was provided by the FDIC, and the Glass–Steagall Act.\n\nThe Agricultural Adjustment Act provided incentives to cut farm production in order to raise farming prices. The National Recovery Administration (NRA) made a number of sweeping changes to the American economy. It forced businesses to work with government to set price codes through the NRA to fight deflationary \"cut-throat competition\" by the setting of minimum prices and wages, labor standards, and competitive conditions in all industries. It encouraged unions that would raise wages, to increase the purchasing power of the working class. The NRA was deemed unconstitutional by the Supreme Court of the United States in 1935.\nThese reforms, together with several other relief and recovery measures, are called the First New Deal. Economic stimulus was attempted through a new alphabet soup of agencies set up in 1933 and 1934 and previously extant agencies such as the Reconstruction Finance Corporation. By 1935, the \"Second New Deal\" added Social Security (which was later considerably extended through the Fair Deal), a jobs program for the unemployed (the Works Progress Administration, WPA) and, through the National Labor Relations Board, a strong stimulus to the growth of labor unions. In 1929, federal expenditures constituted only 3% of the GDP. The national debt as a proportion of GNP rose under Hoover from 20% to 40%. Roosevelt kept it at 40% until the war began, when it soared to 128%.\n\nBy 1936, the main economic indicators had regained the levels of the late 1920s, except for unemployment, which remained high at 11%, although this was considerably lower than the 25% unemployment rate seen in 1933. In the spring of 1937, American industrial production exceeded that of 1929 and remained level until June 1937. In June 1937, the Roosevelt administration cut spending and increased taxation in an attempt to balance the federal budget.\nThe American economy then took a sharp downturn, lasting for 13 months through most of 1938. Industrial production fell almost 30 per cent within a few months and production of durable goods fell even faster. Unemployment jumped from 14.3% in 1937 to 19.0% in 1938, rising from 5 million to more than 12 million in early 1938. Manufacturing output fell by 37% from the 1937 peak and was back to 1934 levels.\n\nProducers reduced their expenditures on durable goods, and inventories declined, but personal income was only 15% lower than it had been at the peak in 1937. As unemployment rose, consumers' expenditures declined, leading to further cutbacks in production. By May 1938 retail sales began to increase, employment improved, and industrial production turned up after June 1938. After the recovery from the Recession of 1937–38, conservatives were able to form a bipartisan conservative coalition to stop further expansion of the New Deal and, when unemployment dropped to 2% in the early 1940s, they abolished WPA, CCC and the PWA relief programs. Social Security remained in place.\n\nBetween 1933 and 1939, federal expenditure tripled, and Roosevelt's critics charged that he was turning America into a socialist state. The Great Depression was a main factor in the implementation of social democracy and planned economies in European countries after World War II (see Marshall Plan). Keynesianism generally remained the most influential economic school in the United States and in parts of Europe until the periods between the 1970s and the 1980s, when Milton Friedman and other neoliberal economists formulated and propagated the newly created theories of neoliberalism and incorporated them into the Chicago School of Economics as an alternative approach to the study of economics. Neoliberalism went on to challenge the dominance of the Keynesian school of Economics in the mainstream academia and policy-making in the United States, having reached its peak in popularity in the election of the presidency of Ronald Reagan in the United States, and Margaret Thatcher in the United Kingdom.\n\nThe Great Depression has been the subject of much writing, as authors have sought to evaluate an era that caused both financial and emotional trauma. Perhaps the most noteworthy and famous novel written on the subject is \"The Grapes of Wrath\", published in 1939 and written by John Steinbeck, who was awarded both the Nobel Prize for literature and the Pulitzer Prize for the work. The novel focuses on a poor family of sharecroppers who are forced from their home as drought, economic hardship, and changes in the agricultural industry occur during the Great Depression. Steinbeck's \"Of Mice and Men\" is another important novella about a journey during the Great Depression. Additionally, Harper Lee's \"To Kill a Mockingbird\" is set during the Great Depression. Margaret Atwood's Booker prize-winning \"The Blind Assassin\" is likewise set in the Great Depression, centering on a privileged socialite's love affair with a Marxist revolutionary. The era spurred the resurgence of social realism, practiced by many who started their writing careers on relief programs, especially the Federal Writers' Project in the U.S.\n\nA number of works for younger audiences are also set during the Great Depression, among them the Kit Kittredge series of \"American Girl\" books written by Valerie Tripp and illustrated by Walter Rane, released to tie in with the dolls and playsets sold by the company. The stories, which take place during the early to mid 1930s in Cincinnati, focuses on the changes brought by the Depression to the titular character's family and how the Kittredges dealt with it. A theatrical adaptation of the series entitled \"\" was later released in 2008 to positive reviews. Similarly, \"Christmas After All\", part of the \"Dear America\" series of books for older girls, take place in 1930s Indianapolis; while \"Kit Kittredge\" is told in a third-person viewpoint, \"Christmas After All\" is in the form of a fictional journal as told by the protagonist Minnie Swift as she recounts her experiences during the era, especially when her family takes in an orphan cousin from Texas.\n\nThe term \"The Great Depression\" is most frequently attributed to British economist Lionel Robbins, whose 1934 book \"The Great Depression\" is credited with formalizing the phrase, though Hoover is widely credited with popularizing the term, informally referring to the downturn as a depression, with such uses as \"Economic depression cannot be cured by legislative action or executive pronouncement\" (December 1930, Message to Congress), and \"I need not recount to you that the world is passing through a great depression\" (1931).\nThe term \"depression\" to refer to an economic downturn dates to the 19th century, when it was used by varied Americans and British politicians and economists. Indeed, the first major American economic crisis, the Panic of 1819, was described by then-president James Monroe as \"a depression\", and the most recent economic crisis, the Depression of 1920–21, had been referred to as a \"depression\" by then-president Calvin Coolidge.\n\nFinancial crises were traditionally referred to as \"panics\", most recently the major Panic of 1907, and the minor Panic of 1910–11, though the 1929 crisis was called \"The Crash\", and the term \"panic\" has since fallen out of use. At the time of the Great Depression, the term \"The Great Depression\" was already used to refer to the period 1873–96 (in the United Kingdom), or more narrowly 1873–79 (in the United States), which has retroactively been renamed the Long Depression.\n\nOther economic downturns have been called a \"great depression\", but none had been as widespread, or lasted for so long. Various nations have experienced brief or extended periods of economic downturns, which were referred to as \"depressions\", but none have had such a widespread global impact.\n\nThe collapse of the Soviet Union, and the breakdown of economic ties which followed, led to a severe economic crisis and catastrophic fall in the standards of living in the 1990s in post-Soviet states and the former Eastern Bloc, which was even worse than the Great Depression. Even before Russia's financial crisis of 1998, Russia's GDP was half of what it had been in the early 1990s, and some populations are still poorer than they were in 1989, including Moldova, Central Asia, and the Caucasus.\n\nThe worldwide economic decline after 2008 has been compared to the 1930s.\n\nThe causes of the Great Recession seem similar to the Great Depression, but significant differences exist. The previous chairman of the Federal Reserve, Ben Bernanke, had extensively studied the Great Depression as part of his doctoral work at MIT, and implemented policies to manipulate the money supply and interest rates in ways that were not done in the 1930s. Generally speaking, the recovery of the world's financial systems tended to be quicker during the Great Depression of the 1930s as opposed to the late-2000s recession.\n\n1928 and 1929 were the times in the 20th century that the wealth gap reached such skewed extremes; half the unemployed had been out of work for over six months, something that was not repeated until the late-2000s recession. 2007 and 2008 eventually saw the world reach new levels of wealth gap inequality that rivalled the years of 1928 and 1929.\n\nGeneral:\n\n\n\n"}
{"id": "2596739", "url": "https://en.wikipedia.org/wiki?curid=2596739", "title": "Health geography", "text": "Health geography\n\nHealth geography is the application of geographical information, perspectives, and methods to the study of health, disease, and health care.\n\nThe study of health geography has been influenced by repositioning medical geography within the field of social geography due to a shift towards a social model in health care, rather than a medical model. This advocates for the redefinition of health and health care away from prevention and treatment of illness only to one of promoting well-being in general. Under this model, some previous illnesses (e.g., mental ill health) are recognized as behavior disturbances only, and other types of medicine (e.g., complementary or alternative medicine and traditional medicine) are studied by the medicine researchers, sometimes with the aid of health geographers without medical education. This shift changes the definition of care, no longer limiting it to spaces such as hospitals or doctor's offices. Also, the social model gives priority to the intimate encounters performed at non-traditional spaces of medicine and healthcare as well as to the individuals as health consumers.\n\nThis alternative methodological approach means that medical geography is broadened to incorporate philosophies such as Marxian political economy, structuralism, social interactionism, humanism, feminism and queer theory.\n\nThe relationship between space and health dates back to Hippocrates, who stated that \"airs, waters, places\" all played significant roles impacting human health and history. A classic piece of research in health geography was done in 1854 as a cholera outbreak gripped a neighborhood in London. Death tolls rang around the clock and the people feared that they were being infected by vapors coming from the ground. John Snow predicted that if he could locate the source of the disease, it could be contained. He drew maps demonstrating the homes of people who had died of cholera and the locations of water pumps. He found that one pump, the public pump on Broad Street, was central to most of the victims. He concluded that infected water from the pump was the culprit. He instructed the authorities to remove the handle to the pump, making it unusable. As a result, the number of new cholera cases decreased.\n\nHealth geography is considered to be divided into two distinct elements. The first of which is focused on geographies of disease and ill health, involving descriptive research quantifying disease frequencies and distributions, and analytic research concerned with finding what characteristics make an individual or population susceptible to disease. This requires an understanding of epidemiology. The second component of health geography is the geography of health care, primarily facility location, accessibility, and utilization. This requires the use of spatial analysis and often borrows from behavioral economics.\n\nHsenting health risks, from natural disasters, to interpersonal violence, stress, and other potential dangers.\n\nAlthough healthcare is a public good, it is not equally available to all individuals. Demand for public services is continuously increasing. People need advance knowledge and the latest prediction technology, that health geography offers. The latest example of such technology is Telemedicine. Many people in the United States are not able to access proper healthcare because of inequality in health insurance and the means to afford medical care.\n\nMobility and Disease Tracking\n\nWith the advent of mobile technology and its spread, it is now possible to track individual mobility. By correlating the movement of individuals through tracking the devices using access towers or other tracking systems, it is now possible to determine and even control disease spread. While privacy laws question the legality of tracking individuals, the commercial mobile service providers are using covert techniques or obtaining government waivers to allow permission to track people .\n\nNotable health geographers include:\n\n\n\n"}
{"id": "1061357", "url": "https://en.wikipedia.org/wiki?curid=1061357", "title": "History of Eurasia", "text": "History of Eurasia\n\nThe history of Eurasia is the collective history of a continental area with several distinct peripheral coastal regions: the Middle East, South Asia, East Asia, Southeast Asia, and Europe, linked by the interior mass of the Eurasian steppe of Central Asia and Eastern Europe. Perhaps beginning with the Steppe Route trade, the early Silk Road, the Eurasian view of history seeks establishing genetic, cultural, and linguistic links between European and Asian cultures of antiquity. In fact, much interest in this area lies with the presumed origin of the speakers of the Proto-Indo-European language and Chariot warfare in Central Eurasia.\n\nFossilized remains of Homo ergaster and Homo erectus between 1.8 and 1.0 million years old have been found in Europe (Georgia (Dmanisi), Spain), Indonesia (e.g., Sangiran and Trinil), Vietnam, and China (e.g., Shaanxi). (See also:Multiregional hypothesis.) The first remains are of Olduwan culture, later of Acheulean and Clactonian culture. Finds of later fossils, such as Homo cepranensis, are local in nature, so the extent of human residence in Eurasia during 1,000,000 - 300,000 ybp remains a mystery.\n\nGeologic temperature records indicate two intense ice ages dated around 650000 ybp and 450000 ybp. These would have presented any humans outside the tropics unprecedented difficulties. Indeed, fossils from this period are very few, and little can be said of human habitats in Eurasia during this period. The few finds are of Homo antecessor and Homo heidelbergensis, and [Lantian Man]] in China. \n\nAfter this,Homo neanderthalensis, with his Mousterian technology, emerged, in areas from Europe to western Asia and continued to be the dominant group of humans in Europe and the Middle East up until 40000-28000 ybp. Peking man has also been dated to this period. During the Eemian Stage, humans probably (e.g. Wolf Cave) spread wherever their technology and skills allowed. TBhe Sahara dried up, forming a difficult area for peoples to cross.\n\nThe birth of the first modern humans (Homo sapiens idaltu) has been dated between 200000 to 130000 ybp (see:Mitochondrial Eve, Single-origin hypothesis), that is, to the coldest phase of the Riss glaciation. Remains of Aterian culture appear in the archaeological evidence.\n\nIn the beginning of the last ice age a supervolcano erupted in Indonesia. Theory states the effects of the eruption caused global climatic changes for many years, effectively obliterating most of the earlier cultures. Y-chromosomal Adam (90000 - 60000 BP, dated data) was initially dated here. Neanderthals survived this abrupt change in the environment, so it's possible for other human groups too. According to the theory humans survived in Africa, and began to resettle areas north, as the effects of the eruption slowly vanished. Upper Paleolithic revolution began after this extreme event, the earliest finds are dated c.50000 BCE.\n\nA divergence in genetical evidence occurs during the early phase of the glaciation. Descendants of female haplogroups M, N and male CT are the ones found among Eurasian peoples today.\n\nThe Southern Dispersal scenario postulates the arrival of anatomically modern humans to Eurasia beginning about 70,000 BC. Moving along the southern coast of Asia, they reached Maritime Southeast Asia by about 65,000 years ago. \nThe establishment of population centers in Western Asia, the Indian subcontinent and in East Asia is attested by about 50,000 years ago.\nThe Eurasian Upper Paleolithic proper is taken after c. 45,000 years ago, with the Cro-Magnon expansion of into Europe (Mousterian), and the expansion into the Mammoth steppe of Northern Asia.\n\nTracing back minute differences in the genomes of modern humans by methods of genetic genealogy, can and have been used to produce models of historical migration. Though these give indications of the routes taken by ancestral humans, the dating of the various genetic markers is improving. The earliest migrations (dated c. 75.000 BP) from the Red Sea shores have been most likely along southern coast of Asia. After this, tracking and timing genetical markers gets increasingly difficult. What is known, is that on areas, of what is now Iraq, Iran, Pakistan and Afghanistan, genetic markers diversify (from about 60000 BCE), and subsequent migrations emerge to all directions (even back to the Levant) and North Africs. From the foothills of the Zagros, big game hunting cultures developed which spread across the Eurasian steppe. Crossing the Caucasus and the Ural Mountains were the ancestors of Samoyeds and the ancestors of Uralic peoples, developing sleds, skis and canoes. Through Kazakhstan moved the ancestors of the Indigenous Americans (dated 50000 - 40000 BCE). Eastbound (maybe through Dzungaria and the Tarim Basin went the ancestors of the northern Chinese and Koreans. It is possible that the routes taken by the Indo-European ancestors travelled across the Bosphorus. Genetic evidence suggests a number of separate migrations (1.Anatoleans 2.Tocharians, 3 Celto-Illyrians, 4.Germanic and Slav, - possibly in this order). Archaeological evidence has not been identified for a number of different groups. On historical linguistic evidence, see for example classification of Thracian. The traditional view of associating early Celts with the Hallstatt culture, and the Nordic Bronze Age with Germanic peoples. The Roman Empire spread after the first widespread use of iron outside Central Europe from the Villanovan culture area. Most likely there was trade also in these periods, e.g. with amber and salt being major products. \n\nInfluences from northern Africa via Gibraltar and Sicilia cannot be readily discounted. Many other questions remain open, too; for example, Neanderthals were still present at this time. More genetic data is being gathered by various research programs.\n\nAs the ice age ended, major environmental changes happened, such as sea level rise (est. 120m), vegetation changes, and animals disappearing in the Holocene extinction event. At the same time Neolithic revolution began and humans started to make pottery, began to cultivate crops and domesticated some animal species. \n\nNeolithic cultures in Eurasia are many, and best discussed in separate articles. Some of the articles on this subject include: Natufian culture, Jōmon culture, List of Neolithic cultures of China and Mehrgarh. European sites are many, they are discussed in Prehistoric Europe. The finding of Ötzi the Iceman (dated 3300 BC) provides an important insight to Chalcolithic period in Europe. Proto-languages of various peoples have been forming in this period, though no literal evidence can (by definition) be found. Later migrations further complicate the study of migrations in this period.\n\nOrigins of writing are dated to fourth millennium BC. Writing may have started independently on various areas of Eurasia. It appears the skill spread relatively fast, giving people a new way of communication.\n\nThe three regions of Western, Eastern and Southern Asia developed in a similar manner with each of the three regions developing early civilizations around fertile river valleys. The civilizations in Mesopotamia, the Indus Valley, and China (along the Yellow River and the Yangtze) shared many similarities and likely exchanged technologies and ideas such as mathematics and the wheel. Ancient Egypt also shared this model. These civilizations were most likely in more or less regular contact with each other by the early versions of the silk road. \n\nEurope was different, however. It was somewhat further north and contained no river systems to support agriculture. Thus Europe remained comparatively undeveloped, with only the southern tips of the region (Greece and Italy) being able to fully borrow crops, technologies, and ideas from the Middle East and North Africa. Similarly, civilization didn't arise in Southeast Asia until contact was made with ancient India, which gave rise to Indianized kingdoms in Indochina and the Malay archipelago. The steppe region had long been inhabited by mounted nomads, and from the central steppes they could reach all areas of the Asian continent. The northern part of the silk road traversed this region. \n\nOne such central expansion out of the steppe is that of the Proto-Indo-Europeans which spread their languages into the Middle East, India, Europe, and to the borders of China (with the Tocharians). Throughout their history, up to the development of gunpowder, all the areas of Eurasia would be repeatedly menaced by the Indo-Iranian, Turkic and Mongol nomads from the steppe.\n\nA difference between Europe and most of the regions of Eurasia is that each of the latter regions has few obstructions internally even though it is ringed by mountains and deserts. This meant that it was easier to establish unified control over the entire region, and this did occur with massive empires consistently dominating the Middle East, China, and at times, much of India. Europe, however, is riddled with internal mountain ranges: The Carpathians, the Alps, the Pyrenees and many others. Throughout its history, Europe has thus usually been divided into many small states, much like the Middle East and Indian subcontinent for much of their history.\n\nThe Iron Age made large stands of timber essential to a nation's success because smelting iron required so much fuel, and the pinnacles of human civilizations gradually moved as forests were destroyed. In Europe the Mediterranean region was supplanted by the German and Frankish lands. In the Middle East the main power center became Anatolia with the once dominant Mesopotamia its vassal. In China, the economical, agricultural, and industrial center moved from the northern Yellow River to the southern Yangtze, though the political center remained in the north. In part this is linked to technological developments, such as the mouldboard plough, that made life in once undeveloped areas more bearable.\n\nThe civilizations in China, India, and Mediterranean, connected by the silk road, became the principal civilizations in Eurasia in early CE times. Later development of Eurasian history of mankind is told in other articles.\n\n\n"}
{"id": "29778453", "url": "https://en.wikipedia.org/wiki?curid=29778453", "title": "Human trafficking in Azerbaijan", "text": "Human trafficking in Azerbaijan\n\nAzerbaijan is also not so far from this disaster. Youth is the main place among people exploitation by human traffickers. There is a struggle in human trafficking for many years.\nThere are some aid center for victims in human trafficking as the Main Department on struggle against Human traffic and the Aid Center to the victims of Human trafficking in Azerbaijan.\n\nU.S. State Department's Office to Monitor and Combat Trafficking in Persons placed the country in \"Tier 2\" in 2017.\n\nMain Department on Combating Trafficking in Human Beings was established in order to effectively execute the tasks identified in the National Action Plan, ensure of security of the victims of trafficking in persons, provide them with professional aid, protection of information collected in unique center in combat with human trafficking. In fight against human trafficking trained and professional police members work in this department.\n\nThe Aid Center to the Victims of Human Trafficking operates as nonprofit institutions.The aim of the center to the victims of human trafficking are protection the rights and interests of the victims of human trafficking, medical, psychological aid, social rehabilitation of themde, their reintegration into society, to help return to normal life.\nCentre makes plan of social rehabilitation of victims of human trafficking for responding each of the individual human and civil rights, help legally for the recovery of the rights of victims, ensure them employment and training, providing with residential area for the victims of trafficking.If the victims are children, this center report immediately information about them to commission of protection their rights and the Commission on guardianship and custody.\n\nAid center of victims of human trafficking provided with different types of assistance to 93 people ( 63 of them were real victims, the others were potential victims) in 2016.\n\nPartnership is very important in this area. Usually the representatives of US and Azerbaijan criticise each other within the OSCE Human Dimension Implementation Meetings. US is the biggest partner of Azerbaijan in the fight against human trafficking.\nIn March 2016, the US embassy and the Main Department for Combating Human Trafficking in Azerbaijan, organised an international conference.\nDuring the conference, carried out the coordination of joint activities of law enforcement agencies. One of the programs carried out by the United States in Azerbaijan aimed at combating human trafficking.\nWith the support of the United States, steps were taken to create of shelters for victims of human trafficking and to increase the capacity of the labor of the people there.\nThe United States joins forces with Azerbaijan in the fight against human trafficking in harmony with the priorities identified in the Concept of \"Azerbaijan 2020: vision to future\". USAID is executing joint projects with the fitting bodies of Azerbaijan.\n\n"}
{"id": "3780225", "url": "https://en.wikipedia.org/wiki?curid=3780225", "title": "Inter Press Service", "text": "Inter Press Service\n\nInter Press Service (IPS) is a global news agency. Its main focus is the production of news and analysis about events and processes affecting economic, social and political development. The agency largely covers news on the Global South, civil society, and globalization.\n\nInter Press Service was set up in 1964 as a non-profit international cooperative of journalists. Its founders were the Italian journalist Roberto Savio and the Argentine political scientist Pablo Piacentini. Initially, the primary objective of IPS was to fill the information gap between Europe and Latin America after the political turbulence following the Cuban revolution of 1959 (Giffard in Salwen and Garrison, 1991).\n\nLater, the network expanded to include all continents, beginning with a Latin American base in Costa Rica in 1982, and extended its editorial focus. In 1994, IPS changed its legal status to that of a \"public-benefit organization for development cooperation\".\n\nIn 1996 IPS had permanent offices and correspondents in 41 countries, covering 108 nations. It had as subscribers over 600 print media, around 80 news agencies and database services, and 65 broadcast media, in addition to over 500 NGOs and institutions.\n\nIPS’s stated aims are to give prominence to the voices of marginalized and vulnerable people and groups, report from the perspectives of developing countries, and to reflect the views of civil society. The mainstreaming of gender in reporting and the assessment of the impacts of globalization are a priority.\n\nIPS may be unique in its concentration on developing countries and the strong relationships with civil society. For this reason, IPS has even been termed the probably \"largest and most credible of all 'alternatives' in the world of news agencies\" (Boyd-Barrett and Rantanen, 1998: 174/5), being the \"first and only independent and professional news agency which provides on a daily basis information with a Third World focus and point of view\" (Boyd-Barrett and Thussu, 1992: 94; cf. Giffard, 1998: 191; Fenby, 1986).\n\nDespite all the laudable aims, it is, however, important to see that IPS has never possessed the resources to be a major player in the international media landscape. Because of its focus on longer background pieces focusses on development issues impacting the lives of people in the South instead of concise news, it is yet to be a news provider for mainstream media in developed countries. In fairness to IPS, mainstream media often rely on their own fly in and out journalists from where IPS reports.\n\nIPS is registered as an international not-for-profit association. It has 'general' NGO consultative status with ECOSOC at the United Nations, and the OECD status of \"ODA eligible international organization\".\n\nFive editorial desks coordinate the network of journalists around the world: Montevideo (regional bureau for Latin America), Berlin-London (Europe and the Mediterranean), Bangkok (Asia and the Pacific), New York (North America and the Caribbean) and Johannesburg (Africa). Most of IPS's journalists and editors are native to the country or region in which they are working.\n\nIPS receives funding from various sources: through its subscribers and media clients, as beneficiary of multilateral and national development cooperation programmes, and as recipient of project financing from foundations. It is not, as most other agencies, financed by a country or a group of newspapers. Hence, the agency’s budget is comparatively small. Still it manages to be \"roughly the sixth largest international news-gathering organization\" (Rauch, 2003: 89).\n\nThe actual role of IPS in the international mediascape is hard to assess. Clipping services are expensive, and do not exist in many countries where IPS is strong. Additionally, in some countries news agencies are not credited in bylines. One study by the UN's Food and Agriculture Organization for media coverage of the FAO in 1991 found that of the nearly 3000 clippings with news agency bylines, 13% credited IPS, making it the third-most cited agency. IPS reports were collected from 138 different publications in 39 countries - more countries than any other agency. IPS was particularly strong in Latin America - 72% of clippings from Latin America with news agency bylines came from IPS.\n\n\n\n\n\n"}
{"id": "23927971", "url": "https://en.wikipedia.org/wiki?curid=23927971", "title": "International Health Partnership", "text": "International Health Partnership\n\nThe International Health Partnership (IHP+) is a group of partners committed to improving the health of citizens in developing countries. International organizations, bilateral agencies and country governments all sign the IHP+ Global Compact. They commit to putting the principles for aid effectiveness and development cooperation into practice in the health sector. IHP+ achieves results by mobilizing national governments, development agencies, civil society and others to support a single, country-led national health strategy. Partners aim to hold one another to account. This global initiative is administered by the World Health Organization and the World Bank.\n\nFaster progress to achieve results requires governments, CSOs, the private sector and especially international development partners to take action. The most critical areas for action for development partners have become known as the \"seven behaviours\". Global health agency leaders recently endorsed these seven behaviours, and have met five times in order to discuss obstacles to progress in these key seven areas. As of June 2014, Agencies have committed to action in one specific area: streamlining the measurement of results and accountability.\n\nImproving health and health services involves governments, health workers, civil society, parliamentarians and other stakeholders working together. In developing countries, money for health comes from both domestic and external resources. This means that governments must also work with a range of international development partners. These partners are increasing in number, use different funding streams and have diverse bureaucratic demands. As a result, development efforts can become fragmented and resources can be wasted.\n\nIn 2005, the Paris Declaration on Aid Effectiveness set out principles for making aid more effective. These principles include ownership, alignment, harmonization, mutual accountability and managing for results. In 2011, the Busan High Level Forum on Aid Effectiveness signaled a shift in thinking from traditional aid effectiveness to a broader, more inclusive approach to development cooperation, a greater emphasis on considering domestic and external resources together, and on results.\n\nIHP+ began in September 2007 in order to put these international principles into practice in the health sector and accelerate progress towards the Millennium Development Goals. Since then it has worked to encourage more inclusive national health planning and joint assessment (JANS) processes, more unified support to national plans through country compacts, one monitoring & evaluation platform to track strategy implementation, greater mutual accountability, improved civil society engagement and financial management harmonization and alignment. The initiative arose from pre-existing developments aimed at improving health outcomes and improving aid effectiveness, including the High-level Forum (HLF) on the Health MDGs, the post-HLF process, and the HLF on Aid Effectiveness.\n\nIHP+ has partners from around the world including developing countries, civil society organizations (CSOs) and development partners. Originally, 26 signatories including 7 countries, 18 bilateral and multilateral partners, and the Bill & Melinda Gates Foundation signed the IHP+ Global Compact for achieving the health-related Millennium Development Goals. The Global Compact is the initiative's foundational global document that is signed by all partner countries, international agencies and bilateral donors when they join the IHP+. The Compact sets out the goals and approach of IHP+ and contains collective and individual commitments by signatories to adhere to agreed aid effectiveness principles. Signatories agree to support country and government-led national health plans. Currently, there are 63 signatories to the Global Compact (as of June 2014).\n\nCivil Society Organizations (CSOs) play an important role in IHP+ at both the country and global level. At the country level, civil society encompasses patient groups, health workers, medical or health unions and associations, faith-based organizations, non-governmental organizations, community-based organizations, academic institutions, media, advocacy groups, refugees, women, youth and other neglected or vulnerable groups. At the global level, civil society is included in IHP+ governance bodies. The IHP+ Steering Committee and Reference Group each include one northern and one southern civil society representative, who draw on a Civil Society Consultative Group of up to 12 civil society members, to discuss IHP+ related issues and activities. IHP+ thematic working groups also include civil society representation. IHP+ supports a small grants programme for southern CSOs called the Health Policy Action Fund. This is designed to strengthen the capacity of southern CSOs so that they can engage more meaningfully with national health policy processes.\n\nIn addition, IHP+ collaborates with related initiatives including Harmonization for Health in Africa (HHA); the Global Health Workforce Alliance (GHWA); H8; Health Metrics Network (HMN); Providing for Health (P4H); and follow-up activities of the Commission for Information and Accountability for Women’s and Children’s Health (COIA).\n\nIHP+ encourages inclusive ways of working, and governments, CSOs, private organizations, parliaments and international development partners all have a role to play in the process. IHP+ works in six key areas to change the way we work together in countries: \n\nIHP+ also works to enable greater south-south knowledge exchange. Experience of working with multiple partners to deliver better results is growing. IHP+ is fostering more systematic approaches to learning with and across countries, through existing networks where possible.\n\n"}
{"id": "22613352", "url": "https://en.wikipedia.org/wiki?curid=22613352", "title": "International Vaccine Institute", "text": "International Vaccine Institute\n\nThe International Vaccine Institute (IVI) is an independent, nonprofit, international organization that was founded on the belief that the health of children in developing countries can be dramatically improved by the use of new and improved vaccines. Working in collaboration with the international scientific community, public health organizations, governments, and industry, IVI is involved in all areas of the vaccine spectrum – from new vaccine design in the laboratory to vaccine development and evaluation in the field to facilitating sustainable introduction of vaccines in countries where they are most needed.\n\nCreated initially as an initiative of the UN Development Programme (UNDP), IVI began formal operations as an independent international organization in 1997 in Seoul, Republic of Korea. Currently, IVI has 35 countries and the World Health Organization (WHO) as signatories to its Establishment Agreement. The Institute has a unique mandate to work exclusively on vaccine development and introduction specifically for people in developing countries, with a focus on neglected diseases affecting these regions.\n\nIn 1992, Dr. Seung-Il Shin, then Senior Health Advisor for the United Nations Development Programme (UNDP), initiated a study to explore the feasibility of establishing an international institute devoted to vaccine research and development within the framework of the Children’s Vaccine Initiative (CVI). \n\nBased on the results of Dr. Shin’s feasibility study, in 1993 the UNDP adopted a formal proposal to establish the International Vaccine Institute (IVI). In 1994, following a call for proposals to host IVI in the Asia Pacific region, the UNDP and the Republic of Korea reached an agreement to host the center in Seoul. In 1995, UNDP opened an interim IVI office on the campus of Seoul National University and the Institute began its initial work and organizational development.\n\nIn 1995 and 1996, the UNDP and the South Korean government jointly developed the basic framework and Constitution of IVI. In order to establish IVI as an independent international organization, the UNDP and Korean government elected to establish the institute through an intergovernmental agreement of UN member states, as sanctioned in the Vienna Convention on the Laws of Treaties of 1969.\n\nOn October 28, 1996, the IVI Establishment Agreement opened for signatures at the UN headquarters in New York City. Representatives of Bangladesh, Bhutan, Indonesia, Kazakhstan, Mongolia, the Netherlands, Panama, Republic of Korea, Romania, Thailand, Vietnam, Uzbekistan, and the WHO were the first to sign the agreement, followed shortly thereafter by Senegal and the Philippines.\n\nIn 1997 more countries followed, with Brazil, China, Egypt, Israel, Jamaica, Kyrgyzstan, Myanmar, Nepal, Pakistan, Papua New Guinea, Peru, Sri Lanka, Sweden, Tajikistan, and Turkey signing the Agreement.\n\nOn May 29, 1997, The IVI Establishment Agreement entered into force following the submission of instruments of ratification by South Korea, Sweden, and Uzbekistan. The IVI Establishment Agreement entered the United Nations Depository of Treaties under Chapter IX. HEALTH, section 3.\n\nOn September 24, 1998 the IVI Headquarters Agreement was signed at a formal ceremony at UNDP headquarters by South Korea’s Minister of Foreign Affairs and Trade, Hon. Hong Soon-Young and IVI Board Chairman, Dr. Barry Bloom. The Headquarters Agreement established IVI as a legal entity with diplomatic immunity in South Korea, becoming the first international organization to be headquartered in South Korea. In accordance with IVI’s independent status, the Institute formally separated from the UNDP in 1998. \nThe IVI headquarters building, located in Seoul National University’s Research Park in Seoul, South Korea, was designed by a consortium consisting of Samwoo Architects of South Korea and Payette Associates of Boston, USA. Construction began in 1998 and was completed in 2003. The building houses laboratories, animal facilities, offices, an auditorium, and a library. A separate 1,300m² pilot plant facility, intended for the production of test lots of vaccines for training and evaluation purposes, was constructed but never completed due to a lack of funding.\n\nIVI’s first major initiative, Diseases of the Most Impoverished (DOMI) was a program of research and technical assistance to accelerate the introduction of new vaccines against typhoid fever, cholera, and shigellosis into public health programs for the poor in developing countries. From 2000 to 2006, IVI’s DOMI program carried out vaccination campaigns, disease surveillance, and research studies in Bangladesh, China, India, Indonesia, Mozambique, Pakistan, Thailand, and Vietnam.  At the conclusion of the DOMI program, IVI synthesized the diverse epidemiological, clinical, economic, and behavioral findings of these studies in order to facilitate informed decision-making by policymakers at the national level on the use of vaccines against the diseases targeted by DOMI. The Bill and Melinda Gates Foundation was the primary funder of the DOMI program with a US$40 million contribution.\n\nThe DOMI Typhoid program was initiated to address the barriers of access to typhoid vaccines in the developing world and to accelerate the introduction of modern typhoid vaccines in countries where they were needed. DOMI Typhoid utilized the Vi-Polysaccharide (Vi-PS) vaccine because it is easily and inexpensively produced by manufacturers in developing countries, it is given in a single dose, and it is relatively thermostable. DOMI Typhoid operated in five study sites: Heichi, China; Kolkata, India; North Jakarta, Indonesia; Karachi, Pakistan; and Hue, Vietnam. From these sites, IVI experts conducted disease surveillance, disease burden studies, cost-of-illness studies, socio-behavioral studies, and vaccine demonstration projects. IVI presented the accumulated evidence from DOMI Typhoid in case studies to officials in each host country. As a result, policymakers in Pakistan, Indonesia, and Vietnam agreed to introduce school-based typhoid vaccination campaigns on a pilot basis. Results from China uncovered a previously unknown growing incidence rate of paratyphoid A infections in China’s Guangxi province, which led to IVI launching its Paratyphoid in China project.\n\nThe DOMI Cholera program sought to develop and accelerate the use of an affordable cholera vaccine in cholera-endemic countries. DOMI Cholera operated from five study sites: Matlab, Bangladesh; North Jakarta, Indonesia; Kolkata, India; Beira, Mozambique; and Hue, Vietnam. From these sites, IVI conducted disease burden, economic, and socio-behavioral studies and cholera vaccination campaigns. The studies found a high demand for cholera vaccine and high incidence rates (3-9/1,000) in children 5 years old and younger. IVI’s vaccination campaign in Beira saw more than 44,000 children and adults receive the internationally-licensed Dukoral vaccine. In Kolkata, IVI vaccinated more than 67,000 children and adults using an oral cholera vaccine produced by Vietnam’s VaBiotech. At the onset of DOMI Cholera, the only internationally licensed cholera vaccine available was Dukoral, but at $90 for the two-dose series, this vaccine was too expensive for public use in many of the poorest developing countries. VaBiotech’s cholera vaccine, originally developed by Vietnam’s National Institute of Health and Epidemiology following a technology transfer from the University of Gothenburg in Sweden, was not licensed for international use but showed great promise as a low-cost vaccine for the developing world. As a result, the Gates Foundation provided IVI with additional funding to establish the Cholera Vaccine Initiative (CVI) with the goal of reformulating the VaBiotech vaccine for international use. \n\nIVI established the DOMI Shigellosis program with the ultimate goal of accelerating the development and introduction of a safe and protective shigella vaccine to control epidemic and endemic disease. Between 2000 and 2004, the DOMI Shigella program ran disease surveillance sites in 6 locations across Southeast Asia: Dhaka, Bangladesh; Hebei, China; Karachi, Pakistan and neighboring villages; North Jakarta, Indonesia; Nha Trang, Vietnam; and Saraburi Province, Thailand. The program enhanced understanding of what an effective shigella vaccine will need and established an accurate disease burden in the countries where it operated. The program first evaluated an oral shigella vaccine (SC602) in Bangladesh, but it failed to elicit an immune response. The high disease burden, serotype diversity, and high levels AMR discovered at IVI's surveillance sites underscored the need for a vaccine that protects against all common strains of the disease. As a result, IVI’s Division of Laboratory Sciences initiated a multiyear program to sequence the Shigella genome and identify common proteins in different Shigella species that could be used to develop a vaccine against all common strains of the disease.\n\nIVI initiated the Rotavirus Diarrhea Program to provide policymakers in developing countries with the disease burden evidence and economic data needed to ensure the inclusion of rotavirus vaccines in their national immunization programs. Running from 1999 to 2010, the program conducted disease surveillance and economic studies in Cambodia, China, India, Indonesia, Laos, Mongolia, South Korea, Sri Lanka, and Vietnam. In 2007, in collaboration with Vietnam’s National Institute of Hygiene and Epidemiology (NIHE), IVI conducted a Phase II trial of GSK’s RotaRix rotavirus vaccine in Khanh Hoa, Vietnam.  In 2009, in collaboration with NIHE, PATH, and Merck, IVI completed a phase III trial of Merck’s RotaTeq rotavirus vaccine in Nha Trang Vietnam, where it vaccinated 900 infants.  \n\nThe Pediatric Dengue Vaccine Initiative was a product development partnership launched by IVI to accelerate the introduction of safe and effective dengue vaccines for children in dengue-endemic countries. From 2002 to 2010, PDVI operated in Brazil, Cambodia, Colombia, India, Indonesia, Laos, Malaysia, Mexico, Myanmar, Nicaragua, Sri Lanka, Thailand and Vietnam. PDVI made many contributions to dengue vaccine development, including:\n\n\nIn 2010, PDVI transitioned into the Dengue Vaccine Initiative (DVI).\n\nThe successor to PDVI, the Dengue Vaccine Initiative was an IVI-led consortium with the World Health Organization, the Sabin Vaccine Institute, the Initiative for Vaccine Research (IVR), and the International Vaccine Access Center (IVAC) at Johns Hopkins University. DVI continued the work of PDVI and focused on laying the groundwork for dengue vaccine decision-making and vaccine introduction in endemic areas. Each consortium member was responsible for a specific component: WHO – information and guidance documents, and regulatory training activities; Johns Hopkins – dengue vaccine financing and strategic demand forecasting; and Sabin – communications and advocacy. Besides leading the consortium, IVI generated evidence for decision-making such as disease burden, country vaccine introduction cases and a global investment case. From 2010 to 2016, DVI operated in Brazil, Burkina Faso, Cambodia, Colombia, Gabon, India, Kenya, Thailand, and Vietnam. From 2013-2015, with funding from Germany’s Federal Ministry of Education & Research (BMBF), IVI continued support for preclinical development of dengue vaccines by Brazil’s Butantan Institute and Vietnam’s VaBiotech. After the 2016 licensure of Sanofi Pasteur’s Dengvaxia vaccine, the DVI project came to a close and its IVI staff transitioned to a new project, the Global Dengue & Aedes-Transmitted Disease Consortium (GDAC).\n\nAs of January 2011, IVI includes 40 countries and the World Health Organization (WHO) as signatories to its Establishment Agreement. To be more specific, there are 33 signatory countries, which are the states that express their consent to be bound by a treaty by signing the treaty without the need for ratification, acceptance of approval. These states may definitively sign a treaty only when the treaty so permits. In addition, there are 16 parties which involve Liberia, Republic of Korea, Brazil, Mongolia, China, Sri Lanka, Ecuador, Sweden, Netherlands, Oman, Pakistan, Uzbekistan, Vietnam, Peru, Philippines, and the World Health Organization (WHO). A major difference between a signatory and a party is that a party to a treaty is a state or other entity with treaty-making capacity that has expressed its consent to be bound by that treaty by an act of ratification, acceptance, approval or accession etc. where that treaty has entered into force for that particular State.\n\nIn short, in addition to the World Health Organization (WHO), 40 countries include Bangladesh, Liberia**, Republic of Korea**, Bhutan, Malta, Romania, Brazil**, Mongolia**, Senegal, China**, Myanmar, Sri Lanka**, Ecuador**, Nepal, Sweden**, Egypt, Netherlands**, Tajikistan, Indonesia, Oman**, Thailand, Israel, Pakistan**, Turkey, Jamaica, Panama, Uzbekistan**, Kazakhstan, Papua New Guinea, Vietnam**, Kyrgyzstan, Peru**, Lebanon, Philippines**, Cote d'Ivoire, India, Kuwait, Slovakia, Spain, and United Arab Emirates.\n\n\n"}
{"id": "9318325", "url": "https://en.wikipedia.org/wiki?curid=9318325", "title": "List of longest wooden ships", "text": "List of longest wooden ships\n\nThis is a list of the world's longest wooden ships. The vessels are sorted by ship length including bowsprit, if known.\n\nFinding the world's longest wooden ship is not straightforward since there are several contenders, depending on which definitions are used. For example, some of these ships benefited from substantial iron or even steel components since the flexing of wood members can lead to significant leaking as the wood members become longer. Some of these ships were not very seaworthy, and a few sank either immediately after launch or soon thereafter. Some of the more recent large ships were never able or intended to leave their berths, and function as floating museums. Finally, not all of the claims to the title of the world's longest wooden ship are credible or verifiable.\n\nA further problem is that especially wooden ships have more than one \"length\". The most used measure in length for registering a ship is the \"length of the topmost deck\" – the \"length on deck\" (LOD) – 'measured from leading edge of stem post to trailing edge of stern post on deck level' or the \"length between perpendiculars\" (LPP, LBP) – 'measured from leading edge of stem post to trailing edge of stern post in the construction waterline (CWL)'. In this method of measuring bowsprit including jibboom and out-board part of spanker boom if any have both no effect on the ship's length. The longest length for comparing ships, the total \"overall\" length (LOA) based on sparred length, should be given if known.\n\nThe longest wooden ship ever built, the six-masted New England gaff schooner \"Wyoming\", had a \"total length\" of (measured from tip of jib boom (30 metres) to tip of spanker boom (27 metres) and a \"length on deck\" of . The -difference is due to her extremely long jib boom of her out-board length being .\n\n"}
{"id": "19731", "url": "https://en.wikipedia.org/wiki?curid=19731", "title": "Midgard", "text": "Midgard\n\nMidgard (an anglicised form of Old Norse ; Old English , Swedish and Danish \"Midgård\", Old Saxon , Old High German , Gothic \"Midjun-gards\"; \"middle yard\") is the name for Earth (equivalent in meaning to the Greek term , \"inhabited\") inhabited by and known to humans in early Germanic cosmology, and specifically one of the Nine Worlds in Norse mythology.\n\nThis name occurs in Old Norse literature as . In Old Saxon \"Heliand\" it appears as and in Old High German poem \"Muspilli\" it appears as . The Gothic form is attested in the Gospel of Luke as a translation of the Greek word . The word is present in Old English epic and poetry as ; later transformed to or (\"Middle-earth\") in Middle English literature.\n\nAll these forms are from a Common Germanic \"*midja-gardaz\" (\"*meddila-\", \"*medjan-\"), a compound of \"*midja-\" \"middle\" and \"*gardaz\" \"yard, enclosure\".\nIn early Germanic cosmology, the term stands alongside \"world\" (Old English \"weorold\", Old Saxon \"werold\", Old High German \"weralt\", Old Frisian \"warld\" and Old Norse \"verǫld\"), from a Common Germanic compound \"*wira-alđiz\", the \"age of men\".\n\nMidgard is a realm in Norse mythology. It is one of the Nine Worlds—the only one that is completely visible to mankind (the others may intersect with this visible realm but are mostly invisible). Pictured as placed somewhere in the middle of Yggdrasil, Midgard is between the land of Niflheim—the land of ice—to the north and Muspelheim—the land of fire—to the south. Midgard is surrounded by a world of water, or ocean, that is impassable. The ocean is inhabited by the great sea serpent Jörmungandr (Miðgarðsormr), who is so huge that he encircles the world entirely, grasping his own tail. The concept is similar to that of the Ouroboros. Midgard was also connected to Asgard, the home of the gods, by the Bifröst, the rainbow bridge, guarded by Heimdallr.\n\nIn Norse mythology, \"Miðgarðr\" became applied to the wall around the world that the gods constructed from the eyebrows of the giant Ymir as a defense against the Jotuns who lived in Jotunheim, east of \"Manheimr\", the \"home of men\", a word used to refer to the entire world. The gods slew the giant Ymir, the first created being, and put his body into the central void of the universe, creating the world out of his body: his flesh constituting the land, his blood the oceans, his bones the mountains, his teeth the cliffs, his hairs the trees, and his brains the clouds. Aurgelmir's skull was held by four dwarfs, Nordri, Sudri, Austri, and Vestri, who represent the four points on the compass and became the dome of heaven. The sun, moon, and stars were said to be scattered sparks in the skull.\nAccording to the Eddas, Midgard will be destroyed at Ragnarök, the battle at the end of the world. Jörmungandr will arise from the ocean, poisoning the land and sea with his venom and causing the sea to rear up and lash against the land. The final battle will take place on the plane of Vígríðr, following which Midgard and almost all life on it will be destroyed, with the earth sinking into the sea, only to rise again, fertile and green when the cycle repeats and the creation begins again.\n\nAlthough most surviving instances of the word Midgard refer to spiritual matters, it was also used in more mundane situations, as in the Viking Age runestone poem from the inscription Sö 56 from Fyrby:\n\nThe Danish and Swedish form or , the Norwegian or , as well as the Icelandic and Faroese form , all derive from the Old Norse term.\n\nThe name \"middangeard\" occurs six times in the Old English epic poem \"Beowulf\", and is the same word as Midgard in Old Norse. The term is equivalent in meaning to the Greek term Oikoumene, as referring to the known and inhabited world.\n\nThe concept of Midgard occurs many times in Middle English. The association with \"earth\" (OE \"eorðe\") in Middle English \"middellærd\", \"middelerde\" is by popular etymology; the continuation of \"geard\" \"enclosure\" is \"yard\". An early example of this transformation is from the Ormulum:\n\nThe usage of \"Middle-earth\" as a name for a setting was popularized by Old English scholar J. R. R. Tolkien in his \"The Lord of the Rings\" and other fantasy works; he was originally inspired by the references to \"middangeard\" and \"Éarendel\" in the Old English poem \"Crist\".\n\n\"Mittilagart\" is mentioned in the 9th-century Old High German \"Muspilli\" (v. 54) meaning \"the world\" as opposed to the sea and the heavens:\n"}
{"id": "6324816", "url": "https://en.wikipedia.org/wiki?curid=6324816", "title": "Mutual recognition agreement", "text": "Mutual recognition agreement\n\nA mutual recognition agreement (MRA) is an international agreement by which two or more countries agree to recognize one another's conformity assessments.\nA mutual recognition arrangement is an international arrangement based on such an agreement.\n\nMRAs have become increasingly common since the formation of the World Trade Organization in 1995. They have been forged within and among various trade blocs, including APEC and the European Union.\n\nMRAs are most commonly applied to goods, such as various quality control MRAs. However, the term is sometimes applied to agreements on the recognition of professional qualifications as well.\n\nAccreditation Bodies, under the International Accreditation Forum use the term \"Multilateral Recognition Agreements\" in a similar sense.\n\n\n"}
{"id": "34291649", "url": "https://en.wikipedia.org/wiki?curid=34291649", "title": "Pacific Health Summit", "text": "Pacific Health Summit\n\nThe goal of the Pacific Health Summit is to connect science, industry, and policy for a healthier world. Traditionally, the main work of the Summit has been an annual meeting, where top decision makers convene to discuss how to realize the dream of a healthier future through the effective utilization of scientific advances, combined with industrial innovation and appropriate policies. In autumn of 2012 on the heels of its eight major conference, the Summit shifted its focus from an annual meeting to more targeted work that builds on the past themes and concrete outcomes. As it has since 2005, the Summit will continue to provide a year-round forum for world leaders to grapple with problems and solutions, share best practices, and forge effective collaborations.\n\nThe first Pacific Health Summit was held in Seattle, Washington in 2005 with foundational support from the Bill & Melinda Gates Foundation and the Russell Family Foundation. It was the co-creation of 2001 Nobel Prize in physiology or medicine winner Leland H. Hartwell, businessman and philanthropist George F. Russell Jr., Co-chair of the Bill & Melinda Gates Foundation William H. Gates Sr., and Founding Director of the National Bureau of Asian Research’s Center for Health and Aging Michael Birt. NBR has been the Secretariat of the Summit since the inaugural 2005 conference. \n\nBirt, founding Executive Director of the Summit, stepped down from his role as NBR’s Center for Health and Aging Director in 2009, and from his role as Executive Director in 2012, handing the mantle to Claire Topal, the Summit's Managing Director, who ran the Summit and managed the team from 2009-2012. She now serves as Senior Advisor for International Health to NBR. Nualchan Sakchalathorn, the Summit’s Project Director, served on the Summit team from 2007-2012.\n\nBuilding on Bill Gates Sr.'s strong personal support, in 2007 Tachi Yamada, then President of Global Health at the Bill & Melinda Gates Foundation, took on a decisive leadership role and formally established the Foundation as the Summit's third co-presenting organization. \n\nIn 2008, the Wellcome Trust joined the Summit as the fourth official co-presenting organization, and Trust Director, Sir Mark Walport, joined our Executive Committee. Both Sir William Castell, Chairman of the Wellcome Trust, who has participated in the Summit since its first year, and Sir Mark provided crucial leadership as the Summit began its rotation in London for the annual meeting.\n\nPeter Neupert, then Corporate Vice President for Health Solutions Strategy for Microsoft, and Craig Mundie, Chief Research and Strategy Officer of Microsoft, consistently provided a private sector perspective to the Summit's strategic discussions. Additionally, GE Healthcare, through Bill Castell in 2005, has always provided critical advice and perspective, as well as the critical founding sponsorship for the annual meeting. \n\nOut of this initial foundation of leadership, the Summit grew into one of the world's premier global health gatherings every year. In 2012, eight years after the inaugural meeting, which was never designed to take place in perpetuity, global health is in an exciting new place. The Summit’s interactive format has proliferated, and decision-makers across all sectors and geographies are collaborating on all the critical global health issues the Summit sought to address: health technology, pandemic flu, MDR-TB, vaccines, malnutrition, maternal and newborn health, and many more. While the Summit is proud of eight years of transformational conversations, countless new friendships, and exciting partnerships, there is still much work to do – and so much momentum on which to build. NBR is looking forward and excited to build on the Summit’s legacy in the years to come.\n\nThe Summit has been a catalyst for partnerships, and the setting of several global health announcements. In 2007, during a speech at the Summit, Margaret Chan, Director-General of the World Health Organization, publicized “a new initiative to establish a world stockpile of vaccines to prepare for the threat of pandemic influenza.” At the Summit in 2009, Sanofi bolstered that stockpile with 100 million donated doses of flu vaccine. \nOther partnerships furthered by, or formed at, the Summit have helped to lead to the establishment of the MSD Wellcome Trust Hilleman Laboratories, the Access to Nutrition Index, the Critical Path to TB Drug Regimens, and a cervical cancer vaccination deal between Merck & Co. and Qiagen. \nAdditionally, the denial of a US visa for 2009 Summit participant Paul Thorn of the Tuberculosis Survival Project, due to his HIV positive status, became the impetus behind the repeal of a US travel restriction law on individuals carrying the HIV/AIDs virus.\n\nThe Summit Secretariat, the Center for Health and Aging at The National Bureau of Asian Research, publishes reports, videos, and photos of Summit sessions and workshops. The organization also produces expert interviews and thought pieces with Summit participants on past themes and current global health topics. Prior to each Summit, the Center publishes a ‘Calls for Collaboration’ report. The publication contains submissions from organizations inviting Summit participants to partner and collaborate around specific areas of need.\n\n\n"}
{"id": "38530392", "url": "https://en.wikipedia.org/wiki?curid=38530392", "title": "Per Fugelli", "text": "Per Fugelli\n\nPer Fugelli (7 December 1943 – 13 September 2017) was a Norwegian physician and professor of General Practice at the University of Bergen from 1984 to 1992, and social medicine at the University of Oslo from 1992 until his death in 2017.\n\nFugelli was born in Stavanger, Norway, on December 7, 1943. He studied medicine at University of Oslo.\n\nFrom 1971–73 Fugelli was a general practitioner in Værøy and Røst, and from 1977 to 1980 in Porsanger. During this time he earned his PhD and graduated in 1978. In 1984, he became a Professor of General Practice at the University of Bergen, where he stayed until 1992. He became a Professor of social medicine at University of Oslo´s Institute of Health and Society. In 2013, he became Emeritus.\n\nIn 1993 Fugelli wrote: \"The patient Earth is sick. Global environmental disruptions can have serious consequences for human health. It's time for doctors to give a world diagnosis and advise on treatment,\" predating the founding of planetary health. He is the subject of the documentary \"I die\" by filmmaker Erik Poppe.\n\nHe was a frequent contributor to the public debate on health and medical questions. Among his early books are \"Tilbake til huslegen\" from 1975, \"Doktor på Værøy og Røst\" from 1977, and \"Helsetilstand og helsetjeneste på Værøy og Røst\" from 1978.\n\nHe published the essay collections \"Med sordin og kanon\" and \"Helse og rettferdighet\" in 1990, \"0-visjonen\" in 2003, and \"Nokpunktet\" in 2008. He has been editor or co-editor of several works, including \"Huslegen\" from 1985, \"Medisinsk leksikon\" from 1990, \"Medisin og helse\" from 1993, and \"Verdier og penger i helsetjenesten\" from 2009.\n\nFugelli was married, had two children, and three grandchildren by the time he died.\n\nIn 2009, he was diagnosed with colorectal cancer. It metastasized into his lungs and by 2012, the cancer was declared terminal. Nevertheless, Fugelli continued to write and work as long as he was able, with his final published article written six weeks before his death. He died at Jæren on 13 September 2017, aged 73.\n\nFugelli won the 2010 Karl Evang Prize and in 2013, the Freedom of Expression Foundation Prize. \n"}
{"id": "24458151", "url": "https://en.wikipedia.org/wiki?curid=24458151", "title": "Planetary boundaries", "text": "Planetary boundaries\n\nPlanetary boundaries is a concept involving Earth system processes which contain environmental boundaries, proposed in 2009 by a group of Earth system and environmental scientists led by Johan Rockström from the Stockholm Resilience Centre and Will Steffen from the Australian National University. The group wanted to define a \"safe operating space for humanity\" for the international community, including governments at all levels, international organizations, civil society, the scientific community and the private sector, as a precondition for sustainable development. The framework is based on scientific evidence that human actions since the Industrial Revolution have become the main driver of global environmental change. \n\nAccording to the paradigm, \"transgressing one or more planetary boundaries may be deleterious or even catastrophic due to the risk of crossing thresholds that will trigger non-linear, abrupt environmental change within continental-to planetary-scale systems.\" The Earth system process boundaries mark the safe zone for the planet to the extent that they are not crossed. As of 2009, two boundaries have already been crossed, while others are in imminent danger of being crossed.\n\nIn 2009, a group of Earth system and environmental scientists led by Johan Rockström from the Stockholm Resilience Centre and Will Steffen from the Australian National University collaborated with 26 leading academics, including Nobel laureate Paul Crutzen, Goddard Institute for Space Studies climate scientist James Hansen and the German Chancellor's chief climate adviser Hans Joachim Schellnhuber and identified nine \"planetary life support systems\" essential for human survival, attempting to quantify how far seven of these systems had been pushed already. They estimated how much further humans can go before planetary habitability is threatened. \nEstimates indicated that three of these boundaries—climate change, biodiversity loss, and the biogeochemical flow boundary—appear to have been crossed. The boundaries were \"rough, first estimates only, surrounded by large uncertainties and knowledge gaps\" which interact in complex ways that are not yet well understood. Boundaries were defined to help define a \"safe space for human development\", which was an improvement on approaches aiming at minimizing human impacts on the planet. The 2009 report was presented to the General Assembly of the Club of Rome in Amsterdam. An edited summary of the report was published as the featured article in a special 2009 edition of \"Nature\". \nalongside invited critical commentary from leading academics like Nobel laureate Mario J. Molina and biologist Cristián Samper.\n\nIn 2015, a second paper was published in \"Science\" to update the Planetary Boundaries concept and findings were presented at the World Economic Forum in Davos, January 2015.\n\nA 2018 study, co-authored by Rockström, calls into question the international agreement to limit warming to 2 degrees above pre-industrial temperatures set forth in the Paris Agreement. The scientists raise the possibility that even if greenhouse gas emissions are substantially reduced to limit warming to 2 degrees, that might be the \"threshold\" at which self-reinforcing climate feedbacks add additional warming until the climate system stabilizes in a hothouse climate state. This would make parts of the world uninhabitable, raise sea levels by up to , and raise temperatures by to levels that are higher than any interglacial period in the past 1.2 million years. Rockström notes that whether this would occur \"is one of the most existential questions in science.\" Study author Katherine Richardson stresses, \"We note that the Earth has never in its history had a quasi-stable state that is around 2 °C warmer than the preindustrial and suggest that there is substantial risk that the system, itself, will ‘want’ to continue warming because of all of these other processes – even if we stop emissions. This implies not only reducing emissions but much more.”\n\nThe idea that our planet has limits, including the burden placed upon it by human activities, has been around for some time. In 1972, \"The Limits to Growth\" was published. It presented a model in which five variables: world population, industrialization, pollution, food production, and resources depletion, are examined, and considered to grow exponentially, whereas the ability of technology to increase resources availability is only linear. Subsequently, the report was widely dismissed, particularly by economists and businessmen, and it has often been claimed that history has proved the projections to be incorrect. In 2008, Graham Turner from the Commonwealth Scientific and Industrial Research Organisation (CSIRO) published \"A comparison of \"The Limits to Growth\" with thirty years of reality\". Turner found that the observed historical data from 1970 to 2000 closely matches the simulated results of the \"standard run\" limits of growth model for almost all the outputs reported. \"The comparison is well within uncertainty bounds of nearly all the data in terms of both magnitude and the trends over time.\" Turner also examined a number of reports, particularly by economists, which over the years have purported to discredit the limits-to-growth model. Turner says these reports are flawed, and reflect misunderstandings about the model. In 2010, Nørgård, Peet and Ragnarsdóttir called the book a \"pioneering report\", and said that it \"has withstood the test of time and, indeed, has only become more relevant.\"\n\"Our Common Future\" was published in 1987 by United Nations' World Commission on Environment and Development. It tried to recapture the spirit of the Stockholm Conference. Its aim was to interlock the concepts of development and environment for future political discussions. It introduced the famous definition for sustainable development:\n\nOf a different kind is the approach made by James Lovelock. In the 1970s he and microbiologist Lynn Margulis presented the Gaia theory or hypothesis, that states that all organisms and their inorganic surroundings on Earth are integrated into a single self-regulating system. The system has the ability to react to perturbations or deviations, much like a living organism adjusts its regulation mechanisms to accommodate environmental changes such as temperature (homeostasis). Nevertheless, this capacity has limits. For instance, when a living organism is subjected to a temperature that is lower or higher than its living range, it can perish because its regulating mechanism cannot make the necessary adjustments. Similarly the Earth may not be able to react to large deviations in critical parameters. In his book \"The Revenge of Gaia\", he affirms that the destruction of rainforests and biodiversity, compounded with the increase of greenhouse gases made by humans, is producing global warming.\n\nThe Holocene began about 10,000 years ago. It is the current interglacial period, and it has proven to be a relatively stable environment of the Earth. There have been natural environmental fluctuations during the Holocene, but the key atmospheric and biogeochemical parameters have been relatively stable. This stability and resilience has allowed agriculture to develop and complex societies to thrive. According to Rockström \"et al.\", we \"have now become so dependent on those investments for our way of life, and how we have organized society, technologies, and economies around them, that we must take the range within which Earth System processes varied in the Holocene as a scientific reference point for a desirable planetary state.\"\n\nSince the industrial revolution, according to Paul Crutzen, Will Steffen and others, the planet has entered a new epoch, the Anthropocene. In the Anthropocene, humans have become the main agents of change to the Earth system. There have been well publicized scientific warnings about risks in the areas of climate change and stratospheric ozone. However, other biophysical processes are also important. For example, since the advent of the Anthropocene, the rate at which species are being extinguished has increased over 100 times, and humans are now the driving force altering global river flows as well as water vapor flows from the land surface. Continuing pressure on the Earth's biophysical systems from human activities raises concerns that further pressure could be destabilizing, and precipitate sudden or irreversible changes to the environment. According to Rockström et al., \"Up to 30% of all mammal, bird, and amphibian species will be threatened with extinction this century.\" It is difficult to address the issue, because the predominant paradigms of social and economic development are largely indifferent to the looming possibilities of large scale environmental disasters triggered by humans. Legal boundaries can help keep human activities in check, but are only as effective as the political will to make and enforce them.\n\nThresholds and boundaries\n\nThe \"threshold\", or climatological tipping point, is the value at which a very small increment for the control variable (like CO) produces a large, possibly catastrophic, change in the response variable (global warming).\n\nThe threshold points are difficult to locate, because the Earth System is very complex. Instead of defining the threshold value, the study establishes a range, and the threshold is supposed to lie inside it. The lower end of that range is defined as the \"boundary\". Therefore, it defines a safe space, in the sense that as long as we are below the boundary, we are below the threshold value. If the boundary is crossed, we enter into a danger zone.\n\nThe proposed framework lays the groundwork for shifting approach to governance and management, away from the essentially sectoral analyses of limits to growth aimed at minimizing negative externalities, toward the estimation of the safe space for human development. Planetary boundaries define, as it were, the boundaries of the \"planetary playing field\" for humanity if major human-induced environmental change on a global scale is to be avoided\n\nTransgressing one or more planetary boundaries may be highly damaging or even catastrophic, due to the risk of crossing thresholds that trigger non-linear, abrupt environmental change within continental- to planetary-scale systems. The 2009 study identified nine planetary boundaries and, drawing on current scientific understanding, the researchers proposed quantifications for seven of them. These seven are climate change (CO concentration in the atmosphere < 350 ppm and/or a maximum change of +1 W/m in radiative forcing); ocean acidification (mean surface seawater saturation state with respect to aragonite ≥ 80% of pre-industrial levels); stratospheric ozone (less than 5% reduction in total atmospheric O from a pre-industrial level of 290 Dobson Units); biogeochemical nitrogen (N) cycle (limit industrial and agricultural fixation of N to 35 Tg N/yr) and phosphorus (P) cycle (annual P inflow to oceans not to exceed 10 times the natural background weathering of P); global freshwater use (< 4000 km/yr of consumptive use of runoff resources); land system change (< 15% of the ice-free land surface under cropland); and the rate at which biological diversity is lost (annual rate of < 10 extinctions per million species). The two additional planetary boundaries for which the group had not yet been able to determine a boundary level are chemical pollution and atmospheric aerosol loading.\n\nChristopher Field, director of the Carnegie Institution's Department of Global Ecology, is impressed: \"This kind of work is critically important. Overall, this is an impressive attempt to define a safety zone.\" But the conservation biologist Stuart Pimm is not impressed: \"I don’t think this is in any way a useful way of thinking about things... The notion of a single boundary is just devoid of serious content. In what way is an extinction rate 10 times the background rate acceptable?\" and the environmental policy analyst Bill Clark thinks: \"Tipping points in the earth system are dense, unpredictable... and unlikely to be avoidable through early warning indicators. It follows that... 'safe operating spaces' and 'planetary boundaries' are thus highly suspect and potentially the new 'opiates'.\"\n\nThe biogeochemist William Schlesinger queries whether thresholds are a good idea for pollutions at all. He thinks waiting until we near some suggested limit will just permit us to continue to a point where it is too late. \"Management based on thresholds, although attractive in its simplicity, allows pernicious, slow and diffuse degradation to persist nearly indefinitely.\"\n\nThe hydrologist David Molden thinks planetary boundaries are a welcome new approach in the 'limits to growth' debate. \"As a scientific organizing principle, the concept has many strengths ... the numbers are important because they provide targets for policymakers, giving a clear indication of the magnitude and direction of change. They also provide benchmarks and direction for science. As we improve our understanding of Earth processes and complex inter-relationships, these benchmarks can and will be updated ... we now have a tool we can use to help us think more deeply—and urgently—about planetary limits and the critical actions we have to take.\"\n\nThe ocean chemist Peter Brewer queries whether it is \"truly useful to create a list of environmental limits without serious plans for how they may be achieved ... they may become just another stick to beat citizens with. Disruption of the global nitrogen cycle is one clear example: it is likely that a large fraction of people on Earth would not be alive today without the artificial production of fertilizer. How can such ethical and economic issues be matched with a simple call to set limits? ... food is not optional.\"\n\nThe environment advisor Steve Bass says the \"description of planetary boundaries is a sound idea. We need to know how to live within the unusually stable conditions of our present Holocene period and not do anything that causes irreversible environmental change ... Their paper has profound implications for future governance systems, offering some of the 'wiring' needed to link governance of national and global economies with governance of the environment and natural resources. The planetary boundaries concept should enable policymakers to understand more clearly that, like human rights and representative government, environmental change knows no borders.\"\n\nThe climate change policy advisor Adele Morris thinks that price-based policies are also needed to avoid political and economic thresholds. \"Staying within a 'safe operating space' will require staying within all the relevant boundaries, including the electorate’s willingness to pay.\"\n\nIn their report (2012) entitled \"Resilient People, Resilient Planet: A future worth choosing\", The High-level Panel on Global Sustainability called for bold global efforts, \"including launching a major global scientific initiative, to strengthen the interface between science and policy. We must define, through science, what scientists refer to as \"planetary boundaries\", \"environmental thresholds\" and \"tipping points\".\"\n\nIn 2011, at their second meeting, the High-level Panel on Global Sustainability of the United Nations had incorporated the concept of planetary boundaries into their framework, stating that their goal was: \"To eradicate poverty and reduce inequality, make growth inclusive, and production and consumption more sustainable while combating climate change and respecting the range of other planetary boundaries.\"\n\nElsewhere in their proceedings, panel members have expressed reservations about the political effectiveness of using the concept of \"planetary boundaries\": \"Planetary boundaries are still an evolving concept that should be used with caution [...] The planetary boundaries question can be divisive as it can be perceived as a tool of the \"North\" to tell the \"South\" not to follow the resource intensive and environmentally destructive development pathway that rich countries took themselves... This language is unacceptable to most of the developing countries as they fear that an emphasis on boundaries would place unacceptable brakes on poor countries.\"\n\nHowever, the concept is routinely used in the proceedings of the United Nations, and in the \"UN Daily News\". For example, the UNEP Executive Director Achim Steiner states that the challenge of agriculture is to \"feed a growing global population without pushing humanity's footprint beyond planetary boundaries.\" The United Nations Environment Programme (UNEP) Yearbook 2010 also repeated Rockström's message, conceptually linking it with ecosystem management and environmental governance indicators.\n\nThe planetary boundaries concept is also used in proceedings by the European Commission, and was referred to in the European Environment Agency synthesis report \"The European environment – state and outlook 2010\".\n\nRadiative forcing is a measure of the difference between the incoming radiation energy and the outgoing radiation energy acting across the boundary of the earth. Positive radiative forcing results in warming. From the start of the industrial revolution in 1750 to 2005, the increase in atmospheric carbon dioxide has led to a positive radiative forcing, averaging about 1.66 W/m².\n\nThe climate scientist Myles Allen thinks setting \"a limit on long-term atmospheric carbon dioxide concentrations merely distracts from the much more immediate challenge of limiting warming to 2 °C.\" He says the concentration of carbon dioxide is not a control variable we can \"meaningfully claim to control\", and he questions whether keeping carbon dioxide levels below 350 ppm will avoid more than 2 °C of warming.\n\nAdele Morris, policy director, Climate and Energy Economics Project, Brookings Institution, makes a criticism from the economical-political point of view. She puts emphasis in choosing policies that minimize costs and preserve consensus. She favors a system of green-house gas emissions tax, and emissions trading, as ways to prevent global warming. She thinks that too-ambitious objectives, like the boundary limit on CO, may discourage such actions.\n\nAccording to the biologist Cristián Samper, a \" boundary that expresses the probability of families of species disappearing over time would better reflect our potential impacts on the future of life on Earth.\"\n\nThe conservation ecologist Gretchen Daily claims that \"it is time to confront the hard truth that traditional approaches to conservation, taken alone, are doomed to fail. Nature reserves are too small, too few, too isolated and too subject to change to support more than a tiny fraction of Earth’s biodiversity. The challenge is to make conservation attractive—from economic and cultural perspectives. We cannot go on treating nature like an all-you-can-eat buffet. We depend on nature for food security, clean water, climate stability, seafood, timber, and other biological and physical services. To maintain these benefits, we need not just remote reserves but places everywhere—more like 'ecosystem service stations.' A few pioneers are integrating conservation and human development. The Costa Rican government is paying landowners for ecosystem services from tropical forests, including carbon offsets, hydropower production, biodiversity conservation and scenic beauty. China is investing $100 billion in \"ecocompensation,\" including innovative policy and finance mechanisms that reward conservation and restoration. The country is also creating \"ecosystem function conservation areas\" that make up 18 percent of its land area. Colombia and South Africa have made dramatic policy changes, too. Three advances would help the rest of the world scale such models of success. One: new science and tools to value and account for natural capital, in biophysical, economic and other terms [...] Two: compelling demonstrations of such tools in resource policy. Three: cooperation among governments, development organizations, corporations and communities to help nations build more durable economies while also maintaining critical ecosystem services.\"\n\nSince the industrial revolution, the Earth's nitrogen cycle has been disturbed even more than the carbon cycle. \"Human activities now convert more nitrogen from the atmosphere into reactive forms than all of the Earth´s terrestrial processes combined. Much of this new reactive nitrogen pollutes waterways and coastal zones, is emitted back to the atmosphere in changed forms, or accumulates in the terrestrial biosphere.\" Only a small part of the fertilizers applied in agriculture is used by plants. Most of the nitrogen and phosphorus ends up in rivers, lakes and the sea, where excess amounts stress aquatic ecosystems. For example, fertilizer which discharges from rivers into the Gulf of Mexico has damaged shrimp fisheries because of hypoxia.\n\nThe biogeochemist William Schlesinger thinks waiting until we near some suggested limit for nitrogen deposition and other pollutions will just permit us to continue to a point where it is too late. He says the boundary suggested for phosphorus is not sustainable, and would exhaust the known phosphorus reserves in less than 200 years.\n\nWith regard to nitrogen, the biogeochemist and ecosystem scientist Robert Howarth says: \"Human activity has greatly altered the flow of nitrogen across the globe. The single largest contributor is fertilizer use. But the burning of fossil fuels actually dominates the problem in some regions, such as the northeastern U.S. The solution in that case is to conserve energy and use it more efficiently. Hybrid vehicles are another excellent fix; their nitrogen emissions are significantly less than traditional vehicles because their engines turn off while the vehicle is stopped. (Emissions from conventional vehicles actually rise when the engine is idling.) Nitrogen emissions from U.S. power plants could be greatly reduced, too, if plants that predate the Clean Air Act and its amendments were required to comply; these plants pollute far out of proportion to the amount of electricity they produce.\n\nIn agriculture, many farmers could use less fertilizer, and the reductions in crop yields would be small or nonexistent. Runoff from corn fields is particularly avoidable because corn’s roots penetrate only the top few inches of soil and assimilate nutrients for only two months of the year. In addition, nitrogen losses can be reduced by 30 percent or more if farmers plant winter cover crops, such as rye or wheat, which can help the soil hold nitrogen. These crops also increase carbon sequestration in soils, mitigating climate change. Better yet is to grow perennial plants such as grasses rather than corn; nitrogen losses are many times lower. Nitrogen pollution from concentrated animal feeding operations (CAFOs) is a huge problem.\n\nAs recently as the 1970s, most animals were fed local crops, and the animals’ wastes were returned to the fields as fertilizer. Today most U.S. animals are fed crops grown hundreds of miles away, making it “uneconomical” to return the manure. The solution? Require CAFO owners to treat their wastes, just as municipalities must do with human wastes. Further, if we ate less meat, less waste would be generated and less synthetic fertilizer would be needed to grow animal feed. Eating meat from animals that are range-fed on perennial grasses would be ideal. The explosive growth in the production of ethanol as a biofuel is greatly aggravating nitrogen pollution. Several studies have suggested that if mandated U.S. ethanol targets are met, the amount of nitrogen flowing down the Mississippi River and fueling the Gulf of Mexico dead zone may increase by 30 to 40 percent. The best alternative would be to forgo the production of ethanol from corn. If the country wants to rely on biofuels, it should instead grow grasses and trees and burn these to co-generate heat and electricity; nitrogen pollution and greenhouse gas emissions would be much lower.\"\n\nWith regard to phosphorus, the ocean engineer David Vaccari says that the most sustainable environmental flow of phosphorus \"would be the natural flux: seven million metric tons per year (Mt/yr). To hit that mark yet satisfy our usage of 22 Mt/yr, we would have to recycle or reuse 72 percent of our phosphorus [...] The flow could be reduced with existing technologies... [lowering] the loss to waterways from 22 to 8.25 Mt/yr, not very much above the natural flux.\"\n\nPeak phosphorus is a concept to describe the point in time at which the maximum global phosphorus production rate is reached. Phosphorus is a scarce finite resource on earth and means of production other than mining are unavailable because of its non-gaseous environmental cycle. According to some researchers, Earth's phosphorus reserves are expected to be completely depleted in 50–100 years and peak phosphorus to be reached in approximately 2030.\n\nSurface ocean acidity has increased thirty percent since the industrial revolution. About one quarter of the additional carbon dioxide generated by humans is dissolved in the oceans, where it forms carbonic acid. This acidity inhibits the ability of corals, shellfish and plankton to build shells and skeletons. Knock-on effects could have serious consequences for fish stocks. This boundary is clearly interconnected with the climate change boundaries, since the concentration of carbon dioxide in the atmosphere is also the underlying control variable for the ocean acidification boundary.\n\nThe ocean chemist Peter Brewer thinks \"ocean acidification has impacts other than simple changes in pH, and these may need boundaries too.\"\n\nThe marine chemist Scott Doney thinks \"the main tactics are raising energy efficiency, switching to renewable and nuclear power, protecting forests and exploring carbon sequestration technologies. Regionally, nutrient runoff to coastal waters not only creates dead zones but also amplifies acidification. The excess nutrients cause more phytoplankton to grow, and as they die the added CO2 from their decay acidifies the water. We have to be smarter about how we fertilize fields and lawns and treat livestock manure and sewage ... Locally, acidic water could be buffered with limestone or chemical bases produced electrochemically from seawater and rocks. More practical may be protecting specific shellfish beds and aquaculture fisheries. Larval mollusks such as clams and oysters appear to be more susceptible to acidification than adults, and recycling old clamshells into the mud may help buffer pH and provide better substrate for larval attachment. The drop in ocean pH is expected to accelerate in coming decades, so marine ecosystems will have to adapt. We can enhance their chances for success by reducing other insults such as water pollution and overfishing, making them better able to withstand some acidification while we transition away from a fossil-fuel energy economy.\"\n\nAcross the planet, forests, wetlands and other vegetation types are being converted to agricultural and other land uses, impacting freshwater, carbon and other cycles, and reducing biodiversity.\n\nThe environment advisor Steve Bass says research tells us that \"the sustainability of land use depends less on percentages and more on other factors. For example, the environmental impact of 15 per cent coverage by intensively farmed cropland in large blocks will be significantly different from that of 15 per cent of land farmed in more sustainable ways, integrated into the landscape. The boundary of 15 per cent land-use change is, in practice, a premature policy guideline that dilutes the authors' overall scientific proposition. Instead, the authors might want to consider a limit on soil degradation or soil loss. This would be a more valid and useful indicator of the state of terrestrial health.\"\n\nThe Earth systems scientist Eric Lambin thinks that \"intensive agriculture should be concentrated on land that has the best potential for high-yield crops ... We can avoid losing the best agricultural land by controlling land degradation, freshwater depletion and urban sprawl. This step will require zoning and the adoption of more efficient agricultural practices, especially in developing countries. The need for farmland can be lessened, too, by decreasing waste along the food distribution chain, encouraging slower population growth, ensuring more equitable food distribution worldwide and significantly reducing meat consumption in rich countries.\"\n\nHuman pressures on global freshwater systems are having dramatic effects. The freshwater cycle is another boundary significantly affected by climate change. Freshwater resources, such as lakes and aquifers, are usually renewable resources which naturally recharge (the term fossil water is sometimes used to describe aquifers which don't recharge). Overexploitation occurs if a water resource is mined or extracted at a rate that exceeds the recharge rate. Recharge usually comes from area streams, rivers and lakes. Forests enhance the recharge of aquifers in some locales, although generally forests are a major source of aquifer depletion. Depleted aquifers can become polluted with contaminants such as nitrates, or permanently damaged through subsidence or through saline intrusion from the ocean. This turns much of the world's underground water and lakes into finite resources with peak usage debates similar to oil. Though Hubbert's original analysis did not apply to renewable resources, their overexploitation can result in a Hubbert-like peak. A modified Hubbert curve applies to any resource that can be harvested faster than it can be replaced.\n\nThe hydrologist Peter Gleick comments: \"Few rational observers deny the need for boundaries to freshwater use. More controversial is defining where those limits are or what steps to take to constrain ourselves within them. Another way to describe these boundaries is the concept of peak water. Three different ideas are useful. 'Peak renewable' water limits are the total renewable flows in a watershed. Many of the world's major rivers are already approaching this threshold—when evaporation and consumption surpass natural replenishment from precipitation and other sources. 'Peak nonrenewable' limits apply where human use of water far exceeds natural recharge rates, such as in fossil groundwater basins of the Great Plains, Libya, India, northern China and parts of California's Central Valley. 'Peak ecological' water is the idea that for any hydrological system, increasing withdrawals eventually reach the point where any additional economic benefit of taking the water is outweighed by the additional ecological destruction that causes. Although it is difficult to quantify this point accurately, we have clearly passed the point of peak ecological water in many basins around the world where huge damage has occurred ... The good news is that the potential for savings, without hurting human health or economic productivity, is vast. Improvements in water-use efficiency are possible in every sector. More food can be grown with less water (and less water contamination) by shifting from conventional flood irrigation to drip and precision sprinklers, along with more accurately monitoring and managing soil moisture. Conventional power plants can change from water cooling to dry cooling, and more energy can be generated by sources that use extremely little water, such as photovoltaics and wind.\"\n\nThe hydrologist David Molden says \"a global limit on water consumption is necessary, but the suggested planetary boundary of 4,000 cubic kilometres per year is too generous.\"\n\nThe stratospheric ozone layer protectively filters ultraviolet radiation (UV) from the Sun, which would otherwise damage biological systems. The actions taken after the Montreal Protocol appeared to be keeping the planet within a safe boundary. However, in 2011, according to a paper published in \"Nature\", the boundary was unexpectedly pushed in the Arctic; \"... the fraction of the Arctic vortex in March with total ozone less than 275 Dobson units (DU) is typically near zero, but reached nearly 45%\".\n\nThe Nobel laureate in chemistry, Mario Molina, says \"five per cent is a reasonable limit for acceptable ozone depletion, but it doesn't represent a tipping point\".\n\nThe physicist David Fahey says that as a result of the Montreal Protocol \"stratospheric ozone depletion will largely reverse by 2100. The gain has relied, in part, on intermediate substitutes, notably hydrochlorofluorocarbons (HCFCs), and the growing use of compounds that cause no depletion, such as hydrofluorocarbons (HFCs). Ongoing success depends on several steps:\n\"The two panels will also have to evaluate climate change and ozone recovery together. Climate change affects ozone abundance by altering the chemical composition and dynamics of the stratosphere, and compounds such as HCFCs and HFCs are greenhouse gases. For example, the large projected demand for HFCs could significantly contribute to climate change.\"\n\nAerosol particles in the atmosphere impact the health of humans and influence monsoon and global atmospheric circulation systems. Some aerosols produce clouds which cool the Earth by reflecting sunlight back to space, while others, like soot, produce thin clouds in the upper stratosphere which behave like a greenhouse, warming the Earth. On balance, anthropogenic aerosols probably produce a net negative radiative forcing (cooling influence). Worldwide each year, aerosol particles result in about 800,000 premature deaths. Aerosol loading is sufficiently important to be included among the planetary boundaries, but it is not yet clear whether an appropriate safe threshold measure can be identified.\n\nSome chemicals, such as persistent organic pollutants, heavy metals and radionuclides, have potentially irreversible additive and synergic effects on biological organisms, reducing fertility and resulting in permanent genetic damage. Sublethal uptakes are drastically reducing marine bird and mammal populations. This boundary seems important, although it is hard to quantify.\n\nA Bayesian emulator for persistent organic pollutants has been developed which can potentially be used to quantify the boundaries for chemical pollution. To date, critical exposure levels of polychlorinated biphenyls (PCBs) above which mass mortality events of marine mammals are likely to occur, have been proposed as a chemical pollution planetary boundary.\n\nA planetary boundary may interact in a manner that changes the safe operating level of other boundaries. Rockström \"et al.\" 2009 did not analyze such interactions, but they suggested that many of these interactions will reduce rather than expand the proposed boundary levels.\n\nFor example, the land use boundary could shift downward if the freshwater boundary is breached, causing lands to become arid and unavailable for agriculture. At a regional level, water resources may decline in Asia if deforestation continues in the Amazon. Such considerations suggest the need for \"extreme caution in approaching or transgressing any individual planetary boundaries.\"\n\nAnother example has to do with coral reefs and marine ecosystems. In 2009, showed that, since 1990, calcification in the reefs of the Great Barrier that they examined decreased at a rate unprecedented over the last 400 years (14% in less than 20 years). Their evidence suggests that the increasing temperature stress and the declining ocean saturation state of aragonite is making it difficult for reef corals to deposit calcium carbonate. explored how multiple stressors, such as increased nutrient loads and fishing pressure, move corals into less desirable ecosystem states. showed that ocean acidification will significantly change the distribution and abundance of a whole range of marine life, particularly species \"that build skeletons, shells, and tests of biogenic calcium carbonate. \"Increasing temperatures, surface UV radiation levels and ocean acidity all stress marine biota, and the combination of these stresses may well cause perturbations in the abundance and diversity of marine biological systems that go well beyond the effects of a single stressor acting alone.\"\n\nIn 2012 Kate Raworth from Oxfam noted the Rockstrom concept does not take human population growth into account. She suggested social boundaries should be incorporated into the planetary boundary structure, such as jobs, education, food, access to water, health services and energy and to accommodate an environmentally safe space compatible with poverty eradication and \"rights for all\". Within planetary limits and an equitable social foundation lies a doughnut shaped area which is the area where there is a \"safe and just space for humanity to thrive in\".\n\nIn 2012, Steven Running suggested a tenth boundary, the annual net global primary production of all terrestrial plants, as an easily determinable measure integrating many variables that will give \"a clear signal about the health of ecosystems\".\n\nThe United Nations secretary general Ban Ki-moon endorsed the concept of planetary boundaries on 16 March 2012, when he presented the key points of the report of his High Level Panel on Global Sustainability to an informal plenary of the UN General Assembly. Ban stated: \"The Panel’s vision is to eradicate poverty and reduce inequality, to make growth inclusive and production and consumption more sustainable, while combating climate change and respecting a range of other planetary boundaries.\" The concept was incorporated into the so-called \"zero draft\" of the outcome of the United Nations Conference on Sustainable Development to be convened in Rio de Janeiro 20–22 June 2012. However, the use of the concept was subsequently withdrawn from the text of the conference, \"partly due to concerns from some poorer countries that its adoption could lead to the sidelining of poverty reduction and economic development. It is also, say observers, because the idea is simply too new to be officially adopted, and needed to be challenged, weathered and chewed over to test its robustness before standing a chance of being internationally accepted at UN negotiations.\"\n\nThe planetary boundary framework was updated in 2015. It was suggested that three of the boundaries (including climate change) might push the Earth system into a new state if crossed; these also strongly influence the remaining boundaries. In the paper, the framework is developed to make it more applicable at the regional scale.\n\nHuman activities related to agriculture and nutrition globally contribute to the transgression of four out of nine planetary boundaries. Surplus nutrient flows (N, P) into aquatic and terrestrial ecosystems are of highest importance, followed by excessive land-system change and biodiversity loss. Whereas in the case of biodiversity loss, P cycle and land-system change, the transgression is in the zone of uncertainty—indicating an increasing risk (yellow circle in the figure), the N boundary related to agriculture is more than 200% transgressed—indicating a high risk (red marked circle in the figure).\n\n\n\n\n----\n\n\n"}
{"id": "42863440", "url": "https://en.wikipedia.org/wiki?curid=42863440", "title": "Pulse vaccination strategy", "text": "Pulse vaccination strategy\n\nThe pulse vaccination strategy is a method used to eradicate an epidemic by repeatedly vaccinating a group at risk, over a defined age range, until the spread of the pathogen has been stopped. It is most commonly used during measles and polio epidemics to quickly stop the spread and contain the outbreak.\n\nWhere T= time units is a constant fraction p of susceptible subjects vaccinated in a relatively short time. This yields the differential equations for the susceptible and vaccinated subjects as\n\nFurther, by setting , one obtains that the dynamics of the susceptible subjects is given by:\n\nand that the eradication condition is:\n\n\n"}
{"id": "4843981", "url": "https://en.wikipedia.org/wiki?curid=4843981", "title": "Recorded history", "text": "Recorded history\n\nRecorded history or written history is a historical narrative based on a written record or other documented communication. It contrasts with other narratives of the past, such as mythological, oral or archeological traditions. For broader world history, recorded history begins with the accounts of the ancient world around the 4th millennium BC, and coincides with the invention of writing. For some geographic regions or cultures, written history is limited to a relatively recent period in human history because of the limited use of written records. Moreover, human cultures do not always record all of the information relevant to later historians, such as the full impact of natural disasters or the names of individuals; thus, recorded history for particular types of information is limited based on the types of records kept. Because of this, recorded history in different contexts may refer to different periods of time depending on the topic.\n\nThe interpretation of recorded history often relies on historical method, or the set of techniques and guidelines by which historians use primary sources and other evidence to research and then to write accounts of the past. The question of the nature, and even the possibility, of an effective method for interpreting recorded history is raised in the philosophy of history as a question of epistemology. The study of different historical methods is known as historiography, which focuses on examining how different interpreters of recorded history create different interpretations of historical evidence.\n\nPrehistory traditionally refers to the span of time before recorded history, ending with the invention of writing systems. Prehistory refers to the past in an area where no written records exist, or where the writing of a culture is not understood.\n\nProtohistory refers to the transition period between prehistory and history, after the advent of literacy in a society but before the writings of the first historians. Protohistory may also refer to the period during which a culture or civilization has not yet developed writing, but other cultures have noted its existence in their own writings.\n\nMore complete writing systems were preceded by proto-writing. Early examples are the Jiahu symbols (c. 6600 BCE), Vinča signs (c. 5300 BCE), early Indus script (c. 3500 BCE) and Nsibidi script (c. before 500 CE). There is disagreement concerning exactly when prehistory becomes history, and when proto-writing became \"true writing\" However, invention of the first writing systems is roughly contemporary with the beginning of the Bronze Age in the late Neolithic of the late 4th millennium BCE. The Sumerian archaic cuneiform script and the Egyptian hieroglyphs are generally considered the earliest writing systems, both emerging out of their ancestral proto-literate symbol systems from 3400–3200 BCE with earliest coherent texts from about 2600 BCE.\n\nThe earliest chronologies date back to the earliest civilizations of Early Dynastic Period Egypt, Mesopotamia & Sumerians which emerged independently of each other from roughly 3500 B.C. Earliest recorded history, which varies greatly in quality and reliability, deals with Pharaohs and their reigns, made by ancient Egyptians. Much of the earliest recorded history was re-discovered relatively recently due to archaeological dig sites findings. A number of different traditions have developed in different parts of the world as to how to interpret these ancient accounts.\n\nIn China, the earliest history was recorded in oracle bone script which was deciphered and may date back to around late 2nd millennium B.C.. The \"Zuo Zhuan\", attributed to Zuo Qiuming in the , is the earliest written of narrative history in the world and covers the period from . The \"Classic of History\" is one of the Five Classics of Chinese classic texts and one of the earliest narratives of China. The \"Spring and Autumn Annals\", the official chronicle of the State of Lu covering the period from , is among the earliest surviving historical texts to be arranged on annalistic principles in the world. It is traditionally attributed to Confucius (551–479 B.C. ). \"Zhan Guo Ce\" was a renowned ancient Chinese historical compilation of sporadic materials on the Warring States period compiled between the .\n\nSima Qian (around ) was the first in China to lay the groundwork for professional historical writing. His written work was the \"Shiji\" (\"Records of the Grand Historian\"), a monumental lifelong achievement in literature. Its scope extends as far back as the , and it includes many treatises on specific subjects and individual biographies of prominent people, and also explores the lives and deeds of commoners, both contemporary and those of previous eras. His work influenced every subsequent author of history in China, including the prestigious Ban family of the Eastern Han Dynasty era.\n\nHerodotus of Halicarnassus (484 B.C. – c. 425 B.C. ) has generally been acclaimed as the \"father of history\" composing his \"The Histories\" written from 450s to the 420s B.C. . However, his contemporary Thucydides (c. 460 B.C. – c. 400 B.C.) is credited with having first approached history with a well-developed historical method in his work the \"History of the Peloponnesian War\". Thucydides, unlike Herodotus, regarded history as being the product of the choices and actions of human beings, and looked at cause and effect, rather than as the result of divine intervention.\n\nSaint Augustine was influential in Christian and Western thought at the beginning of the medieval period. Through the Medieval and Renaissance periods, history was often studied through a sacred or religious perspective. Around 1800, German philosopher and historian Georg Wilhelm Friedrich Hegel brought philosophy and a more secular approach in historical study.According to John Tosh, \"From the High Middle Ages (c.1000–1300) onwards, the written word survives in greater abundance than any other source for Western history.\" Western historians developed methods comparable to modern historiographic research in the 17th and 18th centuries, especially in France and Germany, where they began investigating these source materials to write histories of their past. Many of these histories had strong ideological and political ties to their historical narratives. In the 20th century, academic historians began focusing less on epic nationalistic narratives, which often tended to glorify the nation or great men, to more objective and complex analyses of social and intellectual forces. A major trend of historical methodology in the 20th century was a tendency to treat history more as a social science rather than as an art, which traditionally had been the case. French historians associated with the Annales School introduced quantitative history, using raw data to track the lives of typical individuals, and were prominent in the establishment of cultural history.\n\nIn Sri Lanka, the oldest historical text is the Mahavamsa. Buddhist monks of the Anuradhapura Maha Viharaya maintained chronicles of Sri Lankan history starting from the third century BCE. These annals were combined and compiled into a single document in the 5th century by the Mahanama of Anuradhapura while Dhatusena of Anuradhapura was ruling the Anuradhapura Kingdom. It was written based on prior ancient compilations known as the \"Atthakatha\", which were commentaries written in Sinhala. An earlier document known as the \"Dipavamsa\"(4th century CE) \"Island Chronicles\" is much simpler and contains less information than the \"Mahavamsa\" and was probably compiled using the \"Atthakatha\" on the \"Mahavamsa\" as well.\n\nA companion volume, the \"Culavamsa\" \"Lesser Chronicle\", compiled by Sinhala monks, covers the period from the 4th century to the British takeover of Sri Lanka in 1815. The \"Culavamsa\" was compiled by a number of authors of different time periods.\n\nThe combined work, sometimes referred to collectively as the \"Mahavamsa\", provides a continuous historical record of over two millennia, and is considered one of the world's longest unbroken historical accounts. It is one of the few documents containing material relating to the Nāga and Yakkha peoples, indigenous inhabitants of Lanka prior to the legendary arrival of Prince Vijaya from Singha Pura of Kalinga.\n\nIn the preface to his book, the \"Muqaddimah\" (1377), the Arab historian and early sociologist, Ibn Khaldun, warned of seven mistakes that he thought that historians regularly committed. In this criticism, he approached the past as strange and in need of interpretation. Ibn Khaldun often criticized \"idle superstition and uncritical acceptance of historical data.\" As a result, he introduced a scientific method to the study of history, and he often referred to it as his \"new science\". His historical method also laid the groundwork for the observation of the role of state, communication, propaganda and systematic bias in history, and he is thus considered to be the \"father of historiography\" or the \"father of the philosophy of history\".\n\nWhile recorded history begins with the invention of writing, over time new ways of recording history have come along with the advancement of technology. History can now be recorded through photography, audio recordings, and video recordings. More recently, Internet archives have been saving copies of webpages, documenting the history of the Internet. Other methods of collecting historical information have also accompanied the change in technologies; for example, since at least the 20th century, attempts have been made to preserve oral history by recording it. Until the 1990s this was done using analogue recording methods such as cassettes and reel-to-reel tapes. With the onset of new technologies, there are now digital recordings, which may be recorded to CDs. Nevertheless, historical record and interpretation often relies heavily on written records, partially because it dominates the extant historical materials, and partially because historians are used to communicating and researching in that medium.\n\nThe historical method comprises the techniques and guidelines by which historians use primary sources and other evidence to research and then to write history. Primary sources are first-hand evidence of history (usually written, but sometimes captured in other mediums) made at the time of an event by a present person. Historians think of those sources as the closest to the origin of the information or idea under study. These types of sources can provide researchers with, as Dalton and Charnigo put it, \"direct, un-mediated information about the object of study.\"\n\nHistorians use other types of sources to understand history as well. Secondary sources are written accounts of history based upon the evidence from primary sources. These are sources which, usually, are accounts, works, or research that analyse, assimilate, evaluate, interpret, and/or synthesize primary sources. Tertiary sources are compilations based upon primary and secondary sources and often tell a more generalized account built on the more specific research found in the first two types of sources.\n\n\n"}
{"id": "2823596", "url": "https://en.wikipedia.org/wiki?curid=2823596", "title": "SINPO code", "text": "SINPO code\n\nSINPO, an acronym for Signal, Interference, Noise, Propagation, and Overall, is a Signal Reporting Code used to describe the quality of radiotelegraph transmissions. SINPFEMO, an acronym for Signal, Interference, Noise, Propagation, frequency of Fading, Modulation, dEpth, and Overall is used to describe the quality of radiotelephony transmissions. SINPFEMO code consists of the SINPO code plus the addition of three letters to describe additional features of radiotelephony transmissions. These codes are defined by \"Recommendation ITU-R Sm.1135, SINPO and SINPFEMO codes\".\n\nSINPO code is most frequently used in reception reports written by shortwave listeners. Each letter of the code stands for a specific factor of the signal, and each item is graded on a 1 to 5 scale (where 1 stands for nearly undetectable/severe/unusable and 5 for excellent/nil/extremely strong).\n\nThe code originated with the CCIR (a predecessor to the ITU-R) in 1951, and was widely used by BBC shortwave listeners to submit signal reports, with many going so far as to mail audio recordings to the BBC's offices.\n\nSINPO and SINPFEMO are the official signal reporting codes for international civil aviation and ITU-R.\n\nThe use of the SINPO code can be subjective and may vary from person to person. Not all shortwave listeners are conversant with the SINPO code and prefer using plain language instead.\n\n\nEach category is rated from 1 to 5 with 1 being 'unusable' or 'severe' and 5 being 'perfect' or 'nil'. MANY raters misunderstand the code and will rate everything either 55555 or 11111 when in reality both extremes are unusual in the extreme. '55555' essentially means 'perfect reception akin to a local station' while that is occasionally possible, when talking about long-distance short-wave reception, it is almost never the case.\n\nAnother common mistake in rating is presenting an 'O' higher than any previously rated element. By definition, a station cannot present 'perfect' reception if there is any Noise or Interference or Fading present. In other words, it is NOT 'perfect local quality' reception if any of those things are present.\n\nAn extension of SINPO code, for use in radiotelephony (voice over radio) communications, SINPFEMO is an acronym for Signal, Interference, Noise, Propagation, frequency of Fading, dEpth, Modulation, and Overall.\n\nIn responding to a shortwave reception, the SINPO indicates to the transmitting station the overall quality of the reception.\n\nThe SINPO code in normal use consists of the 5 rating numbers listed without the letters, as in the examples below:\n\n\nGenerally, a SINPO with a code number starting with a 2 or lower would not be worth reporting, unless there is no noise, interference or loss of propagation, since it would be likely the signal would be unintelligible.\n\nAlthough the original SINPO code established technical specifications for each number (i.e., a number 3 in the P column meant a fixed number of fades per minute), these are rarely adhered to by reporters. The 'S' meter displays the relative strength of the received RF signal in decibels; however, this should not be used as the sole indication of signal strength, as no two S meters are calibrated exactly alike, and many lower-priced receivers omit the S meter altogether. References to a \"SINFO\" code may also be found in some literature. In this case, the 'F' stands for Fading, instead of 'P' for Propagation, but the two codes are interchangeable. It was presumed that the average listener would be more familiar with the meaning of \"fading\" than \"propagation\". A simple way to ensure the rating applied is useful is to rate the \"O\" column first based on the intelligibility of the station. If you can understand everything easily, the station will rate a 4 or higher. If you have to work hard, but can understand everything '3' is the appropriate rating. If you cannot understand everything although you put great effort into it, a '2' is appropriate, and if you cannot understand the programming at all '1' is the appropriate rating.\n\nSome listeners may not know how to distinguish between the 'I' which indicates interference from adjacent stations, and the 'N' which describes natural atmospheric or man-made noise; also for some listeners, the rating for 'Propagation' may not be completely understood. As a result of this confusion, many stations suggest the SIO code – a simpler code which makes the limitations noted above not relevant. Despite this, some books and periodicals maintain the SINPO code is the best for DX reporters.\n\nSINPO is said to have evolved from the BBC's RAISO format (full version: RAFISBEMVO, which measured:\n\n"}
{"id": "19946174", "url": "https://en.wikipedia.org/wiki?curid=19946174", "title": "Schøyen Collection", "text": "Schøyen Collection\n\nThe Schøyen Collection is the largest private manuscript collection in the world, mostly located in Oslo and London. Formed in the 20th century by Martin Schøyen, it comprises manuscripts of global provenance, spanning 5,000 years of history. It contains more than 13,000 manuscript items; the oldest is about 5,300 years old. There are manuscripts from 134 different countries and territories, representing 120 distinct languages.\n\nThe variety of manuscripts—geographic, linguistic, textual and material—even more than its size makes the Schøyen Collection unique. The collection has a website with many items illustrated and described. The provenance of the various cuneiform materials held by the Schøyen Collection remains subject to controversy.\n\nAmong the most notable items of the collection are the following:\n\n"}
{"id": "56739018", "url": "https://en.wikipedia.org/wiki?curid=56739018", "title": "Sharp power", "text": "Sharp power\n\nSharp power is the use of manipulative diplomatic policies by one country to influence and undermine the political system of a target country. Sharp power can include attempts by one country to manipulate and manage information about itself in the news media and educational systems of another country, for the purpose of misleading or dividing public opinion in a target country, or for masking or diverting attention away from negative information about itself. \n\nSharp power is distinguished from soft power, which are attractive policies that project a positive impression of one country and promote greater understanding with another country, ultimately to influence the decisions of another country through persuasion. Soft power policies can include student exchanges and the sponsoring of cultural and sporting events. Sharp power is also distinct from hard power, which are coercive policies by one country to compel another country into taking action or changing its decisions. Hard power can include military force, economic sanctions, and diplomatic threats. \n\nThe term \"sharp power\" was coined in November of 2017 by the National Endowment for Democracy, and published in an article in Foreign Affairs Magazine, to describe aggressive and subversive policies employed by authoritarian governments as a projection of state power in democratic countries, policies that cannot be described as either hard power or soft power. The NED article specifically names the Russian state-funded RT News Network and the Chinese state-sponsored Confucius Institute educational partnerships as examples of sharp power. According NED, autocratic states \"are not necessarily seeking to 'win hearts and minds,' the common frame of reference for soft power efforts, but they are surely seeking to manipulate their target audiences by distorting the information that reaches them.\" \n\nSince 2018, the term \"sharp power\" has been used in news articles, scholarly discussions, and Congressional hearings. Even representatives of the Chinese Communist Party have used the term, dismissing Western claims that their country has engaged in sharp power practices. \n\n\n"}
{"id": "12559281", "url": "https://en.wikipedia.org/wiki?curid=12559281", "title": "Social forum", "text": "Social forum\n\nA social forum is an open meeting place for civil society organizations and individuals opposed to neoliberalism and what its participants regard as the domination of the world by capital and imperialism. The first social forum was the World Social Forum (WSF) held in January 2001 in Porto Alegre. It was designed as a counter forum to the World Economic Forum (WEF) held in Davos at the same time. While the WSF regards the WEF as a meeting of the political and economic elite of the world, the WSF gathers social forces and aims to promotes democratization and social justice.\n\nAfter the first WSF, the social forum idea was replicated across the world in various transnational, national, and local social forums. Most, though not all, social forums adhere to the WSF Charter of Principles, drawn up by the International Council of the WSF. The diversity of participants in the social forums has reflected the diversity of the Global Justice Movement (or Anti-Globalization Movement).\n\n\n\n\n\nSocial forums in Italy include:\n\nSocial forums in Sweden include:\n\n\n\n"}
{"id": "26769", "url": "https://en.wikipedia.org/wiki?curid=26769", "title": "South America", "text": "South America\n\nSouth America is a continent in the Western Hemisphere, mostly in the Southern Hemisphere, with a relatively small portion in the Northern Hemisphere. It may also be considered a subcontinent of the Americas, which is how it is viewed in the Spanish and Portuguese-speaking regions of the Americas. The reference to South America instead of other regions (like Latin America or the Southern Cone) has increased in the last decades due to changing geopolitical dynamics (in particular, the rise of Brazil).\n\nIt is bordered on the west by the Pacific Ocean and on the north and east by the Atlantic Ocean; North America and the Caribbean Sea lie to the northwest. It includes twelve sovereign states (Argentina, Bolivia, Brazil, Chile, Colombia, Ecuador, Guyana, Paraguay, Peru, Suriname, Uruguay, and Venezuela), a part of France (French Guiana), and a non-sovereign area (the Falkland Islands, a British Overseas Territory though this is disputed by Argentina). In addition to this, the ABC islands of the Kingdom of the Netherlands, Trinidad and Tobago, and Panama may also be considered part of South America.\n\nSouth America has an area of 17,840,000 square kilometers (6,890,000 sq mi). Its population has been estimated at more than floor(/1e6) million. South America ranks fourth in area (after Asia, Africa, and North America) and fifth in population (after Asia, Africa, Europe, and North America). Brazil is by far the most populous South American country, with more than half of the continent's population, followed by Colombia, Argentina, Venezuela and Peru. In recent decades Brazil has also concentrated half of the region's GDP and has become a first regional power.\n\nMost of the population lives near the continent's western or eastern coasts while the interior and the far south are sparsely populated. The geography of western South America is dominated by the Andes mountains; in contrast, the eastern part contains both highland regions and vast lowlands where rivers such as the Amazon, Orinoco, and Paraná flow. Most of the continent lies in the tropics.\n\nThe continent's cultural and ethnic outlook has its origin with the interaction of indigenous peoples with European conquerors and immigrants and, more locally, with African slaves. Given a long history of colonialism, the overwhelming majority of South Americans speak Portuguese or Spanish, and societies and states reflect Western traditions.\n\nSouth America occupies the southern portion of the Americas. The continent is generally delimited on the northwest by the Darién watershed along the Colombia–Panama border, although some may consider the border instead to be the Panama Canal. Geopolitically and geographically all of Panama – including the segment east of the Panama Canal in the isthmus – is typically included in North America alone and among the countries of Central America. Almost all of mainland South America sits on the South American Plate.\n\nSouth America is home to the world's highest uninterrupted waterfall, Angel Falls in Venezuela; the highest single drop waterfall Kaieteur Falls in Guyana; the largest river (by volume), the Amazon River; the longest mountain range, the Andes (whose highest mountain is Aconcagua at ); the driest non-polar place on earth, the Atacama Desert; the largest rainforest, the Amazon Rainforest; the highest capital city, La Paz, Bolivia; the highest commercially navigable lake in the world, Lake Titicaca; and, excluding research stations in Antarctica, the world's southernmost permanently inhabited community, Puerto Toro, Chile.\n\nSouth America's major mineral resources are gold, silver, copper, iron ore, tin, and petroleum. These resources found in South America have brought high income to its countries especially in times of war or of rapid economic growth by industrialized countries elsewhere. However, the concentration in producing one major export commodity often has hindered the development of diversified economies. The fluctuation in the price of commodities in the international markets has led historically to major highs and lows in the economies of South American states, often causing extreme political instability. This is leading to efforts to diversify production to drive away from staying as economies dedicated to one major export.\n\nSouth America is one of the most biodiverse continents on earth. South America is home to many interesting and unique species of animals including the llama, anaconda, piranha, jaguar, vicuña, and tapir. The Amazon rainforests possess high biodiversity, containing a major proportion of the Earth's species.\n\nBrazil is the largest country in South America, encompassing around half of the continent's land area and population. The remaining countries and territories are divided among three regions: The Andean States, the Guianas and the Southern Cone.\n\nTraditionally, South America also includes some of the nearby islands. Aruba, Bonaire, Curaçao, Trinidad, Tobago, and the federal dependencies of Venezuela sit on the northerly South American continental shelf and are often considered part of the continent. Geo-politically, the island states and overseas territories of the Caribbean are generally grouped as a part or subregion of North America, since they are more distant on the Caribbean Plate, even though San Andres and Providencia are politically part of Colombia and Aves Island is controlled by Venezuela.\n\nOther islands that are included with South America are the Galápagos Islands that belong to Ecuador and Easter Island (in Oceania but belonging to Chile), Robinson Crusoe Island, Chiloé (both Chilean) and Tierra del Fuego (split in between Chile and Argentina). In the Atlantic, Brazil owns Fernando de Noronha, Trindade and Martim Vaz, and the Saint Peter and Saint Paul Archipelago, while the Falkland Islands are governed by the United Kingdom, whose sovereignty over the islands is disputed by Argentina. South Georgia and the South Sandwich Islands may be associated with either South America or Antarctica.\n\nThe distribution of the average temperatures in the region presents a constant regularity from the 30° of latitude south, when the isotherms tend, more and more, to be confused with the degrees of latitude.\n\nIn temperate latitudes, winters are milder and summers warmer than in North America. Because its most extensive part of the continent is in the equatorial zone, the region has more areas of equatorial plains than any other region.\n\nThe average annual temperatures in the Amazon basin oscillate around 27 °C, with low thermal amplitudes and high rainfall indices. Between the Maracaibo Lake and the mouth of the Orinoco, predominates an equatorial climate of the type Congolese, that also includes parts of the Brazilian territory.\n\nThe east-central Brazilian plateau has a humid and warm tropical climate. The northern and eastern parts of the Argentine pampas have a humid subtropical climate with dry winters and humid summers of the Chinese type, while the western and eastern ranges have a subtropical climate of the dinaric type. At the highest points of the Andean region, climates are colder than the ones occurring at the highest point of the Norwegian fjords. In the Andean plateaus, the warm climate prevails, although it is tempered by the altitude, while in the coastal strip, there is an equatorial climate of the Guinean type. From this point until the north of the Chilean coast appear, successively, Mediterranean oceanic climate, temperate of the Breton type and, already in Tierra del Fuego, cold climate of the Siberian type.\n\nThe distribution of rainfall is related to the regime of winds and air masses. In most of the tropical region east of the Andes, winds blowing from the northeast, east and southeast carry moisture from the Atlantic, causing abundant rainfall. However, due to a consistently strong wind shear and a weak Intertropical Convergence Zone, tropical cyclones are practically unknown in the South Atlantic. In the Orinoco lhanos and in the Guianas plateau, the precipitation levels go from moderate to high. The Pacific coast of Colombia and northern Ecuador are rainy regions. The Atacama Desert, along this stretch of coast, is one of the driest regions in the world. The central and southern parts of Chile are subject to extratropical cyclones, and most of the Argentine Patagonia is desert. In the pampas of Argentina, Uruguay and South of Brazil the rainfall is moderate, with rains well distributed during the year. The moderately dry conditions of the Chaco oppose the intense rainfall of the eastern region of Paraguay. In the semiarid coast of the Brazilian Northeast the rains are linked to a monsoon regime.\n\nImportant factors in the determination of climates are sea currents, such as the current Humboldt and Falklands. The equatorial current of the South Atlantic strikes the coast of the Northeast and there is divided into two others: the current of Brazil and a coastal current that flows to the northwest towards the Antilles, where there it moves towards northeast course thus forming the most Important and famous ocean current in the world, the Gulf Stream.\n\nSouth America is believed to have been joined with Africa from the late Paleozoic Era to the early Mesozoic Era, until the supercontinent Pangaea began to rift and break apart about 225 million years ago. Therefore, South America and Africa share similar fossils and rock layers.\n\nSouth America is thought to have been first inhabited by humans when people were crossing the Bering Land Bridge (now the Bering Strait) at least 15,000 years ago from the territory that is present-day Russia. They migrated south through North America, and eventually reached South America through the Isthmus of Panama.\n\nThe first evidence for the existence of the human race in South America dates back to about 9000 BC, when squashes, chili peppers and beans began to be cultivated for food in the highlands of the Amazon Basin. Pottery evidence further suggests that manioc, which remains a staple food today, was being cultivated as early as 2000 BC.\n\nBy 2000 BC, many agrarian communities had been settled throughout the Andes and the surrounding regions. Fishing became a widespread practice along the coast, helping establish fish as a primary source of food. Irrigation systems were also developed at this time, which aided in the rise of an agrarian society.\n\nSouth American cultures began domesticating llamas, vicuñas, guanacos, and alpacas in the highlands of the Andes circa 3500 BC. Besides their use as sources of meat and wool, these animals were used for transportation of goods.\n\nThe rise of plant growing and the subsequent appearance of permanent human settlements allowed for the multiple and overlapping beginnings of civilizations in South America.\n\nOne of the earliest known South American civilizations was at Norte Chico, on the central Peruvian coast. Though a pre-ceramic culture, the monumental architecture of Norte Chico is contemporaneous with the pyramids of Ancient Egypt. Norte Chico governing class established a trade network and developed agriculture then followed by Chavín by 900 BC, according to some estimates and archaeological finds. Artifacts were found at a site called Chavín de Huantar in modern Peru at an elevation of 3,177 meters. Chavín civilization spanned 900 BC to 300 BC.\n\nIn the central coast of Peru, around the beginning of the 1st millennium AD, Moche (100 BC – 700 AD, at the northern coast of Peru), Paracas and Nazca (400 BC – 800 AD, Peru) cultures flourished with centralized states with permanent militia improving agriculture through irrigation and new styles of ceramic art. At the Altiplano, Tiahuanaco or Tiwanaku (100 BC – 1200 AD, Bolivia) managed a large commercial network based on religion.\n\nAround the 7th century, both Tiahuanaco and Wari or Huari Empire (600–1200, Central and northern Peru) expanded its influence to all the Andean region, imposing the Huari urbanism and Tiahuanaco religious iconography.\n\nThe Muisca were the main indigenous civilization in what is now Colombia. They established the Muisca Confederation of many clans, or \"cacicazgos\", that had a free trade network among themselves. They were goldsmiths and farmers.\n\nOther important Pre-Columbian cultures include: the Cañaris (in south central Ecuador), Chimú Empire (1300–1470, Peruvian northern coast), Chachapoyas, and the Aymaran kingdoms (1000–1450, Western Bolivia and southern Peru).\n\nHolding their capital at the great city of Cusco, the Inca civilization dominated the Andes region from 1438 to 1533. Known as \"Tawantin suyu\", and \"the land of the four regions,\" in Quechua, the Inca Empire was highly distinct and developed. Inca rule extended to nearly a hundred linguistic or ethnic communities, some 9 to 14 million people connected by a 25,000 kilometer road system. Cities were built with precise, unmatched stonework, constructed over many levels of mountain terrain. Terrace farming was a useful form of agriculture.\n\nThe Mapuche in Central and Southern Chile resisted the European and Chilean settlers, waging the Arauco War for more than 300 years.\n\nIn 1494, Portugal and Spain, the two great maritime European powers of that time, on the expectation of new lands being discovered in the west, signed the Treaty of Tordesillas, by which they agreed, with the support of the Pope, that all the land outside Europe should be an exclusive duopoly between the two countries.\n\nThe treaty established an imaginary line along a north-south meridian 370 leagues west of the Cape Verde Islands, roughly 46° 37' W. In terms of the treaty, all land to the west of the line (known to comprise most of the South American soil) would belong to Spain, and all land to the east, to Portugal. As accurate measurements of longitude were impossible at that time, the line was not strictly enforced, resulting in a Portuguese expansion of Brazil across the meridian.\n\nBeginning in the 1530s, the people and natural resources of South America were repeatedly exploited by foreign conquistadors, first from Spain and later from Portugal. These competing colonial nations claimed the land and resources as their own and divided it in colonies.\n\nEuropean infectious diseases (smallpox, influenza, measles, and typhus) – to which the native populations had no immune resistance – caused large-scale depopulation of the native population under Spanish control. Systems of forced labor, such as the haciendas and mining industry's mit'a also contributed to the depopulation. After this, African slaves, who had developed immunities to these diseases, were quickly brought in to replace them.\n\nThe Spaniards were committed to converting their native subjects to Christianity and were quick to purge any native cultural practices that hindered this end; however, many initial attempts at this were only partially successful, as native groups simply blended Catholicism with their established beliefs and practices. Furthermore, the Spaniards brought their language to the degree they did with their religion, although the Roman Catholic Church's evangelization in Quechua, Aymara, and Guaraní actually contributed to the continuous use of these native languages albeit only in the oral form.\n\nEventually, the natives and the Spaniards interbred, forming a mestizo class. At the beginning, many mestizos of the Andean region were offspring of Amerindian mothers and Spanish fathers. After independence, most mestizos had native fathers and European or mestizo mothers.\n\nMany native artworks were considered pagan idols and destroyed by Spanish explorers; this included many gold and silver sculptures and other artifacts found in South America, which were melted down before their transport to Spain or Portugal. Spaniards and Portuguese brought the western European architectural style to the continent, and helped to improve infrastructures like bridges, roads, and the sewer system of the cities they discovered or conquered. They also significantly increased economic and trade relations, not just between the old and new world but between the different South American regions and peoples. Finally, with the expansion of the Portuguese and Spanish languages, many cultures that were previously separated became united through that of Latin American.\n\nGuyana was first a Dutch, and then a British colony, though there was a brief period during the Napoleonic Wars when it was colonized by the French. The country was once partitioned into three parts, each being controlled by one of the colonial powers until the country was finally taken over fully by the British.\n\nIndigenous peoples of the Americas in various European colonies were forced to work in European plantations and mines; along with African slaves who were also introduced in the proceeding centuries. The colonists were heavily dependent on indigenous labor during the initial phases of European settlement to maintain the subsistence economy, and natives were often captured by expeditions. The importation of African slaves began midway through the 16th century, but the enslavement of indigenous peoples continued well into the 17th and 18th centuries. The Atlantic slave trade brought African slaves primarily to South American colonies, beginning with the Portuguese since 1502. The main destinations of this phase were the Caribbean colonies and Brazil, as European nations built up economically slave-dependent colonies in the New World. Nearly 40% of all African slaves trafficked to the Americas went to Brazil. An estimated 4.9 million slaves from Africa came to Brazil during the period from 1501 to 1866.\n\nWhile the Portuguese, English and French settlers enslaved mainly African blacks, the Spaniards became very disposed of the natives. In 1750 Portugal abolished native slavery in the colonies because they considered them unfit for labour and began to import even more African slaves. Slaves were brought to the mainland on so-called slave ships, under inhuman conditions and ill-treatment, and those who survived were sold into the slave markets.\n\nAfter independence, all South American countries maintained slavery for some time. The first South American country to abolish slavery was Chile in 1823, Uruguay in 1830, Bolivia in 1831, Colombia and Ecuador in 1851, Argentina in 1853, Peru and Venezuela in 1854, Paraguay in 1869, and in 1888 Brazil was the last South American nation and the last country in western world to abolish slavery.\n\nThe European Peninsular War (1807–1814), a theater of the Napoleonic Wars, changed the political situation of both the Spanish and Portuguese colonies. First, Napoleon invaded Portugal, but the House of Braganza avoided capture by escaping to Brazil. Napoleon also captured King Ferdinand VII of Spain, and appointed his own brother instead. This appointment provoked severe popular resistance, which created Juntas to rule in the name of the captured king.\n\nMany cities in the Spanish colonies, however, considered themselves equally authorized to appoint local Juntas like those of Spain. This began the Spanish American wars of independence between the patriots, who promoted such autonomy, and the royalists, who supported Spanish authority over the Americas. The Juntas, in both Spain and the Americas, promoted the ideas of the Enlightenment. Five years after the beginning of the war, Ferdinand VII returned to the throne and began the Absolutist Restoration as the royalists got the upper hand in the conflict.\n\nThe independence of South America was secured by Simón Bolívar (Venezuela) and José de San Martín (Argentina), the two most important \"Libertadores\". Bolívar led a great uprising in the north, then led his army southward towards Lima, the capital of the Viceroyalty of Peru. Meanwhile, San Martín led an army across the Andes Mountains, along with Chilean expatriates, and liberated Chile. He organized a fleet to reach Peru by sea, and sought the military support of various rebels from the Viceroyalty of Peru. The two armies finally met in Guayaquil, Ecuador, where they cornered the Royal Army of the Spanish Crown and forced its surrender.\n\nIn the Portuguese Kingdom of Brazil, Dom Pedro I (also Pedro IV of Portugal), son of the Portuguese King Dom João VI, proclaimed the independent Kingdom of Brazil in 1822, which later became the Empire of Brazil. Despite the Portuguese loyalties of garrisons in Bahia, Cisplatina and Pará, independence was diplomatically accepted by the crown in Portugal in 1825, on condition of a high compensation paid by Brazil mediatized by the United Kingdom.\n\nThe newly independent nations began a process of fragmentation, with several civil and international wars. However, it was not as strong as in Central America. Some countries created from provinces of larger countries stayed as such up to modern times (such as Paraguay or Uruguay), while others were reconquered and reincorporated into their former countries (such as the Republic of Entre Ríos and the Riograndense Republic).\n\nThe first separatist attempt was in 1820 by the Argentine province of Entre Ríos, led by a caudillo. In spite of the \"Republic\" in its title, General Ramírez, its caudillo, never really intended to declare an independent Entre Rios. Rather, he was making a political statement in opposition to the monarchist and centralist ideas that back then permeated Buenos Aires politics. The \"country\" was reincorporated at the United Provinces in 1821.\n\nIn 1825 the Cisplatine Province declared its independence from the Empire of Brazil, which led to the Cisplatine War between the imperials and the Argentine from the United Provinces of the Río de la Plata to control the region. Three years later, the United Kingdom intervened in the question by proclaiming a tie and creating in the former Cisplatina a new independent country: The Oriental Republic of Uruguay.\n\nLater in 1836, while Brazil was experiencing the chaos of the regency, Rio Grande do Sul proclaimed its independence motivated by a tax crisis. With the anticipation of the coronation of Pedro II to the throne of Brazil, the country could stabilize and fight the separatists, which the province of Santa Catarina had joined in 1839. The Conflict came to an end by a process of compromise by which both Riograndense Republic and Juliana Republic were reincorporated as provinces in 1845.\n\nThe Peru–Bolivian Confederation, a short-lived union of Peru and Bolivia, was blocked by Chile in the War of the Confederation (1836–1839) and again during the War of the Pacific (1879–1883). Paraguay was virtually destroyed by Argentina and Brazil in the Paraguayan War.\n\nSouth American history in early 19th century was built almost exclusively on wars. Despite the Spanish American wars of independence and the Brazilian War of Independence, the new nations quickly began to suffer with internal conflicts and wars among themselves.\n\nIn 1825 the proclamation of independence of Cisplatina led to the Cisplatine War between historical rivals the Empire of Brazil and the United Provinces of the Río de la Plata, Argentina's predecessor. The result was a stalemate, ending with the British arranging for the independence of Uruguay. Soon after, another Brazilian province proclaimed its independence leading to the Ragamuffin War which Brazil won.\n\nBetween 1836 and 1839 the War of the Confederation broke out between the short-lived Peru-Bolivian Confederation and Chile, with the support of the Argentine Confederation. The war was fought mostly in the actual territory of Peru and ended with a Confederate defeat and the dissolution of the Confederacy and annexation of many territories by Argentina.\n\nMeanwhile, the Argentine Civil Wars plagued Argentina since its independence. The conflict was mainly between those who defended the centralization of power in Buenos Aires and those who defended a confederation. During this period it can be said that \"there were two Argentines\": the Argentine Confederation and the Argentine Republic. At the same time the political instability in Uruguay led to the Uruguayan Civil War among the main political factions of the country. All this instability in the platine region interfered with the goals of other countries such as Brazil, which was soon forced to take sides. In 1851 the Brazilian Empire, supporting the centralizing unitarians, and the Uruguayan government invaded Argentina and deposed the caudillo, Juan Manuel Rosas, who ruled the confederation with an iron hand. Although the Platine War did not put an end to the political chaos and civil war in Argentina, it brought temporary peace to Uruguay where the Colorados faction won, supported by the Brazilian Empire, British Empire, French Empire and the Unitarian Party of Argentina.\n\nPeace lasted only a short time: in 1864 the Uruguayan factions faced each other again in the Uruguayan War. The Blancos supported by Paraguay started to attack Brazilian and Argentine farmers near the borders. The Empire made an initial attempt to settle the dispute between Blancos and Colorados without success. In 1864, after a Brazilian ultimatum was refused, the imperial government declared that Brazil's military would begin reprisals. Brazil declined to acknowledge a formal state of war, and, for most of its duration, the Uruguayan–Brazilian armed conflict was an undeclared war which led to the deposition of the \"Blancos\" and the rise of the pro-Brazilian \"Colorados\" to power again. This angered the Paraguayan government, which even before the end of the war invaded Brazil, beginning the biggest and deadliest war in both South American and Latin American histories: the Paraguayan War.\n\nThe Paraguayan War began when the Paraguayan dictator Francisco Solano López ordered the invasion of the Brazilian provinces of Mato Grosso and Rio Grande do Sul. His attempt to cross Argentinian territory without Argentinian approval led the pro-Brazilian Argentine government into the war. The pro-Brazilian Uruguayan government showed its support by sending troops. In 1865 the three countries signed the Treaty of the Triple Alliance against Paraguay. At the beginning of the war, the Paraguayans took the lead with several victories, until the Triple Alliance organized to repel the invaders and fight effectively. This was the second total war experience in the world after the American Civil War. It was deemed the greatest war effort in the history of all participating countries, taking almost 6 years and ending with the complete devastation of Paraguay. The country lost 40% of its territory to Brazil and Argentina and lost 60% of its population, including 90% of the men. The dictator Lopez was killed in battle and a new government was instituted in alliance with Brazil, which maintained occupation forces in the country until 1876.\n\nThe last South American war in the 19th century was the War of the Pacific with Bolivia and Peru on one side and Chile on the other. In 1879 the war began with Chilean troops occupying Bolivian ports, followed by Bolivia declaring war on Chile which activated an alliance treaty with Peru. The Bolivians were completely defeated in 1880 and Lima was occupied in 1881. The peace was signed with Peru in 1883 while a truce was signed with Bolivia in 1884. Chile annexed territories of both countries leaving Bolivia with no path to the sea.\n\nIn the new century, as wars became less violent and less frequent, Brazil entered into a small conflict with Bolivia for the possession of the Acre, which was acquired by Brazil in 1902. In 1917 Brazil declared war on the Central Powers and join the allied side in the World War I, sending a small fleet to the Mediterranean Sea and some troops to be integrated with the British and French troops. Brazil was the only South American country that fought in WWI. Later in 1932 Colombia and Peru entered a short armed conflict for territory in the Amazon. In the same year Paraguay declared war on Bolivia for possession of the Chaco, in a conflict that ended three years later with Paraguay's victory. Between 1941 and 1942 Peru and Ecuador fought decisively for territories claimed by both that were annexed by Peru, usurping Ecuador's frontier with Brazil.\n\nAlso in this period the first naval battle of World War II was fought on the continent, in the River Plate, between British forces and German submarines. The Germans still made numerous attacks on Brazilian ships on the coast, causing Brazil to declare war on the Axis powers in 1942, being the only South American country to fight in this war (and in both World Wars). Brazil sent naval and air forces to combat German and Italian submarines off the continent and throughout the South Atlantic, in addition to sending an expeditionary force to fight in the Italian Campaign.\n\nA brief war was fought between Argentina and the UK in 1982, following an Argentine invasion of the Falkland Islands, which ended with an Argentine defeat. The last international war to be fought on South American soil was the 1995 Cenepa War between Ecuador and the Peru along their mutual border.\n\nWars became less frequent in the 20th century, with Bolivia-Paraguay and Peru-Ecuador fighting the last inter-state wars. Early in the 20th century, the three wealthiest South American countries engaged in a vastly expensive naval arms race which was catalyzed by the introduction of a new warship type, the \"dreadnought\". At one point, the Argentine government was spending a fifth of its entire yearly budget for just two dreadnoughts, a price that did not include later in-service costs, which for the Brazilian dreadnoughts was sixty percent of the initial purchase.\n\nThe continent became a battlefield of the Cold War in the late 20th century. Some democratically elected governments of Argentina, Brazil, Chile, Uruguay and Paraguay were overthrown or displaced by military dictatorships in the 1960s and 1970s. To curtail opposition, their governments detained tens of thousands of political prisoners, many of whom were tortured and/or killed on inter-state collaboration. Economically, they began a transition to neoliberal economic policies. They placed their own actions within the US Cold War doctrine of \"National Security\" against internal subversion. Throughout the 1980s and 1990s, Peru suffered from an internal conflict.\n\nArgentina and Britain fought the Falklands War in 1982.\n\nColombia has had an ongoing, though diminished internal conflict, which started in 1964 with the creation of Marxist guerrillas (FARC-EP) and then involved several illegal armed groups of leftist-leaning ideology as well as the private armies of powerful drug lords. Many of these are now defunct, and only a small portion of the ELN remains, along with the stronger, though also greatly reduced, FARC. These leftist groups smuggle narcotics out of Colombia to fund their operations, while also using kidnapping, bombings, land mines and assassinations as weapons against both elected and non-elected citizens.\n\nRevolutionary movements and right-wing military dictatorships became common after World War II, but since the 1980s, a wave of democratization passed through the continent, and democratic rule is widespread now. Nonetheless, allegations of corruption are still very common, and several countries have developed crises which have forced the resignation of their governments, although, on most occasions, regular civilian succession has continued.\nInternational indebtedness turned into a severe problem in the late 1980s, and some countries, despite having strong democracies, have not yet developed political institutions capable of handling such crises without resorting to unorthodox economic policies, as most recently illustrated by Argentina's default in the early 21st century. The last twenty years have seen an increased push towards regional integration, with the creation of uniquely South American institutions such as the Andean Community, Mercosur and Unasur. Notably, starting with the election of Hugo Chávez in Venezuela in 1998, the region experienced what has been termed a pink tide – the election of several leftist and center-left administrations to most countries of the area, except for the Guianas and Colombia.\n\nHistorically, the Hispanic countries were founded as Republican dictatorships led by caudillos. Brazil was the only exception, being a constitutional monarchy for its first 67 years of independence, until a coup d'état proclaimed a republic. In the late 19th century, the most democratic countries were Brazil, Chile, Argentina and Uruguay.\n\nIn the interwar period, nationalism grew stronger on the continent, influenced by countries like Nazi Germany and Fascist Italy. A series of authoritarian rules broke out in South American countries with views bringing them closer to the Axis Powers, like Vargas's Brazil. In the late 20th century, during the Cold War, many countries became military dictatorships under American tutelage in attempts to avoid the influence of the Soviet Union. After the fall of the authoritarian regimes, these countries became democratic republics.\n\nDuring the first decade of the 21st century, South American governments have drifted to the political left, with leftist leaders being elected in Chile, Uruguay, Brazil, Argentina, Ecuador, Bolivia, Paraguay, Peru and Venezuela. Most South American countries are making increasing use of protectionist policies, helping local development.\n\nAll South American countries are presidential republics with the exceptions of Peru, which is a semi-presidential republic, and Suriname, a parliamentary republic. French Guiana is a French overseas department, while the Falkland Islands and South Georgia and the South Sandwich Islands are British overseas territories. It is currently the only inhabited continent in the world without monarchies; the Empire of Brazil existed during the 19th century and there was an unsuccessful attempt to establish a Kingdom of Araucanía and Patagonia in southern Argentina and Chile. Also in the twentieth century, Suriname was established as a constituent kingdom of the Kingdom of the Netherlands and Guyana retained the British monarch as head of state for 4 years after its independence.\n\nRecently, an intergovernmental entity has been formed which aims to merge the two existing customs unions: Mercosur and the Andean Community, thus forming the third-largest trade bloc in the world.\nThis new political organization, known as Union of South American Nations, seeks to establish free movement of people, economic development, a common defense policy and the elimination of tariffs.\n\nSouth America has over floor(/1e6) million inhabitants and a population growth rate of about 0.6% per year. There are several areas of sparse demographics such as tropical forests, the Atacama Desert and the icy portions of Patagonia. On the other hand, the continent presents regions of high population density, such as the great urban centers. The population is formed by descendants of Europeans (mainly Spaniards, Portuguese and Italians), Africans and indigenous peoples. There is a high percentage of mestizos that vary greatly in composition by place. There is also a minor population of Asians, especially in Brazil. The two main languages are by far Spanish and Portuguese, followed by French, English and Dutch in smaller numbers.\n\nSpanish and Portuguese are the most spoken languages in South America, with approximately 200 million speakers each. Spanish is the official language of most countries, along with other native languages in some countries. Portuguese is the official language of Brazil. Dutch is the official language of Suriname; English is the official language of Guyana, although there are at least twelve other languages spoken in the country, including Portuguese, Chinese, Hindustani and several native languages. English is also spoken in the Falkland Islands. French is the official language of French Guiana and the second language in Amapá, Brazil.\n\nIndigenous languages of South America include Quechua in Peru, Bolivia, Ecuador, Argentina, Chile and Colombia; Wayuunaiki in northern Colombia (La Guajira) and northwestern Venezuela (Zulia); Guaraní in Paraguay and, to a much lesser extent, in Bolivia; Aymara in Bolivia, Peru, and less often in Chile; and Mapudungun is spoken in certain pockets of southern Chile and, more rarely, Argentina. At least three South American indigenous languages (Quechua, Aymara, and Guarani) are recognized along with Spanish as national languages.\n\nOther languages found in South America include Hindustani and Javanese in Suriname; Italian in Argentina, Brazil, Uruguay, Venezuela and Chile; and German in certain pockets of Argentina, Brazil, and Chile. German is also spoken in many regions of the southern states of Brazil, Riograndenser Hunsrückisch being the most widely spoken German dialect in the country; among other Germanic dialects, a Brazilian form of East Pomeranian is also well represented and is experiencing a revival. Welsh remains spoken and written in the historic towns of Trelew and Rawson in the Argentine Patagonia. There are also small clusters of Japanese-speakers in Brazil, Colombia and Peru. Arabic speakers, often of Lebanese, Syrian, or Palestinian descent, can be found in Arab communities in Argentina, Colombia, Brazil, Venezuela and in Paraguay.\n\nAn estimated 90% of South Americans are Christians (82% Roman Catholic, 8% other Christian denominations mainly traditional Protestants and Evangelicals but also Orthodox), accounting for c. 19% of Christians worldwide.\n\nAfrican descendent religions and Indigenous religions are also common throughout all South America, some examples of are Santo Daime, Candomblé, Umbanda and Encantados.\n\nCrypto-Jews or Marranos, conversos, and Anusim were an important part of colonial life in Latin America.\n\nBoth Buenos Aires, Argentina and São Paulo, Brazil figure among the largest Jewish populations by urban area.\n\nJapanese Buddhism, Shintoism, and Shinto-derived Japanese New Religions are common in Brazil and Peru. Korean Confucianism is especially found in Brazil while Chinese Buddhism and Chinese Confucianism have spread throughout the continent.\n\nKardecist Spiritism can be found in several countries.\n\nPart of Religions in South America (2013):\n\nGenetic admixture occurs at very high levels in South America. In Argentina, the European influence accounts for 65–79% of the genetic background, Amerindian for 17–31% and sub-Saharan African for 2–4%. In Colombia, the sub-Saharan African genetic background varied from 1% to 89%, while the European genetic background varied from 20% to 79%, depending on the region.\nIn Peru, European ancestries ranged from 1% to 31%, while the African contribution was only 1% to 3%. The Genographic Project determined the average Peruvian from Lima had about 28% European ancestry, 68% Native American, 2% Asian ancestry and 2% sub-Saharan African.\n\nDescendants of indigenous peoples, such as the Quechua and Aymara, or the Urarina of Amazonia make up the majority of the population in Bolivia (56%) and, per some sources, in Peru (44%). In Ecuador, Amerindians are a large minority that comprises two-fifths of the population. The native European population is also a significant element in most other former Portuguese colonies.\n\nPeople who identify as of primarily or totally European descent, or identify their phenotype as corresponding to such group, are more of a majority in Argentina, and Uruguay and more than half of the population of Chile (64.7%) and (48.4%) in Brazil. In Venezuela, according to the national census 42% of the population is primarily native Spanish, Italian and Portuguese descendants. In Colombia, people who identify as European descendant are about 37%. In Peru, European descendants are the third group in number (15%).\n\nMestizos (mixed European and Amerindian) are the largest ethnic group in Paraguay, Venezuela, Colombia and Ecuador and the second group in Peru.\n\nSouth America is also home to one of the largest populations of Africans. This group is significantly present in Brazil, Colombia, Guyana, Suriname, French Guiana, Venezuela and Ecuador.\n\nBrazil followed by Peru have the largest Japanese, Korean and Chinese communities in South America. East Indians form the largest ethnic group in Guyana and Suriname.\n\nIn many places indigenous people still practice a traditional lifestyle based on subsistence agriculture or as hunter-gatherers. There are still some uncontacted tribes residing in the Amazon Rainforest.\nThe most populous country in South America is Brazil with /1e6 round 1 million people. The second largest country is Colombia with a population of . Argentina is the third most populous country with .\n\nWhile Brazil, Argentina, and Colombia maintain the largest populations, large city populations are not restricted to those nations. The largest cities in South America, by far, are São Paulo, Bogotá, and Lima. These cities are the only cities on the continent to exceed eight million, and three of five in the Americas. Next in size are Rio de Janeiro, Santiago, Caracas, Buenos Aires and Salvador.\n\nFive of the top ten metropolitan areas are in Brazil. These metropolitan areas all have a population of above 4 million and include the São Paulo metropolitan area, Rio de Janeiro metropolitan area, and Belo Horizonte metropolitan area. Whilst the majority of the largest metropolitan areas are within Brazil, Argentina is host to the second largest metropolitan area by population in South America: the Buenos Aires metropolitan region is above 13 million inhabitants.\n\nSouth America has also been witness to the growth of megapolitan areas. In Brazil four megaregions exist including the Expanded Metropolitan Complex of São Paulo with more than 32 million inhabitants. The others are the Greater Rio, Greater Belo Horizonte and Greater Porto Alegre. Colombia also has four megaregions which comprise 72% of its population, followed by Venezuela, Argentina and Peru which are also homes of megaregions.\n\nThe top ten largest South American metropolitan areas by population as of 2015, based on national census numbers from each country:\n\nSouth America relies less on the export of both manufactured goods and natural resources than the world average; merchandise exports from the continent were 16% of GDP on an exchange rate basis, compared to 25% for the world as a whole. Brazil (the seventh largest economy in the world and the largest in South America) leads in terms of merchandise exports at $251 billion, followed by Venezuela at $93 billion, Chile at $86 billion, and Argentina at $84 billion.\n\nSince 1930, the continent has experienced remarkable growth and diversification in most economic sectors. Most agricultural and livestock products are destined for the domestic market and local consumption. However, the export of agricultural products is essential for the balance of trade in most countries.\n\nThe main agrarian crops are export crops, such as soy and wheat. The production of staple foods such as vegetables, corn or beans is large, but focused on domestic consumption. Livestock raising for meat exports is important in Argentina, Paraguay, Uruguay and Colombia. In tropical regions the most important crops are coffee, cocoa and bananas, mainly in Brazil, Colombia and Ecuador. Traditionally, the countries producing sugar for export are Peru, Guyana and Suriname, and in Brazil, sugar cane is also used to make ethanol. On the coast of Peru, northeast and south of Brazil, cotton is grown. Fifty percent of the South American surface is covered by forests, but timber industries are small and directed to domestic markets. In recent years, however, transnational companies have been settling in the Amazon to exploit noble timber destined for export. The Pacific coastal waters of South America are the most important for commercial fishing. The anchovy catch reaches thousands of tons, and tuna is also abundant (Peru is a major exporter). The capture of crustaceans is remarkable, particularly in northeastern Brazil and Chile.\n\nOnly Brazil and Argentina are part of the G20 (industrial countries), while only Brazil is part of the G8+5 (the most powerful and influential nations in the world). In the tourism sector, a series of negotiations began in 2005 to promote tourism and increase air connections within the region. Punta del Este, Florianópolis and Mar del Plata are among the most important resorts in South America.\n\nThe most industrialized countries in South America are Brazil, Argentina, Chile, Colombia, Venezuela and Uruguay respectively. These countries alone account for more than 75 percent of the region's economy and add up to a GDP of more than US$3.0 trillion. Industries in South America began to take on the economies of the region from the 1930s when the Great Depression in the United States and other countries of the world boosted industrial production in the continent. From that period the region left the agricultural side behind and began to achieve high rates of economic growth that remained until the early 1990s when they slowed due to political instabilities, economic crises and neoliberal policies.\n\nSince the end of the economic crisis in Brazil and Argentina that occurred in the period from 1998 to 2002, which has led to economic recession, rising unemployment and falling population income, the industrial and service sectors have been recovering rapidly. Chile, Argentina and Brazil have recovered fastest, growing at an average of 5% per year. All of South America after this period has been recovering and showing good signs of economic stability, with controlled inflation and exchange rates, continuous growth, a decrease in social inequality and unemployment–factors that favor industry.\n\nThe main industries are: electronics, textiles, food, automotive, metallurgy, aviation, naval, clothing, beverage, steel, tobacco, timber, chemical, among others. Exports reach almost US$400 billion annually, with Brazil accounting for half of this.\n\nThe economic gap between the rich and poor in most South American nations is larger than on most other continents. The richest 10% receive over 40% of the nation's income in Bolivia, Brazil, Chile, Colombia, and Paraguay, while the poorest 20% receive 3% or less in Bolivia, Brazil, and Colombia. This wide gap can be seen in many large South American cities where makeshift shacks and slums lie in the vicinity of skyscrapers and upper-class luxury apartments; nearly one in nine South Americans live on less than $2 per day (on a purchasing power parity basis).\n\nTourism has increasingly become a significant source of income for many South American countries. Historical relics, architectural and natural wonders, a diverse range of foods and culture, vibrant and colorful cities, and stunning landscapes attract millions of tourists every year to South America. Some of the most visited places in the region are Iguazu Falls, Recife, Olinda, Machu Picchu, Bariloche, the Amazon rainforest, Rio de Janeiro, São Luís, Salvador, Fortaleza, Maceió, Buenos Aires, Florianópolis, San Ignacio Miní, Isla Margarita, Natal, Lima, São Paulo, Angel Falls, Brasília, Nazca Lines, Cuzco, Belo Horizonte, Lake Titicaca, Salar de Uyuni, Jesuit Missions of Chiquitos, Los Roques archipelago, Gran Sabana, Patagonia, Tayrona National Natural Park, Santa Marta, Bogotá, Medellín, Cartagena, Perito Moreno Glacier and the Galápagos Islands.\n\nIn 2016 Brazil hosted the 2016 Summer Olympics.\n\nSouth Americans are culturally influenced by their indigenous peoples, the historic connection with the Iberian Peninsula and Africa, and waves of immigrants from around the globe.\n\nSouth American nations have a rich variety of music. Some of the most famous genres include vallenato and cumbia from Colombia, pasillo from Colombia and Ecuador, samba, bossa nova and música sertaneja from Brazil, and tango from Argentina and Uruguay. Also well known is the non-commercial folk genre Nueva Canción movement which was founded in Argentina and Chile and quickly spread to the rest of the Latin America. People on the Peruvian coast created the fine guitar and cajon duos or trios in the most mestizo (mixed) of South American rhythms such as the Marinera (from Lima), the Tondero (from Piura), the 19th century popular Creole Valse or Peruvian Valse, the soulful Arequipan Yaravi, and the early 20th century Paraguayan Guarania. In the late 20th century, Spanish rock emerged by young hipsters influenced by British pop and American rock. Brazil has a Portuguese-language pop rock industry as well a great variety of other music genres.\n\nThe literature of South America has attracted considerable critical and popular acclaim, especially with the Latin American Boom of the 1960s and 1970s, and the rise of authors such as Mario Vargas Llosa, Gabriel García Márquez in novels and Jorge Luis Borges and Pablo Neruda in other genres. The Brazilians Machado de Assis and João Guimarães Rosa are widely regarded as the greatest Brazilian writers.\n\nBecause of South America's broad ethnic mix, South American cuisine has African, South American Indian, South Asian, East Asian, and European influences. Bahia, Brazil, is especially well known for its West African–influenced cuisine. Argentines, Chileans, Uruguayans, Brazilians, Bolivians, and Venezuelans regularly consume wine. People in Argentina, Paraguay, Uruguay, southern Chile, Bolivia and Brazil drink mate, an herb which is brewed. The Paraguayan version, terere, differs from other forms of mate in that it is served cold. Pisco is a liquor distilled from grapes in Peru and Chile. Peruvian cuisine mixes elements from Chinese, Japanese, Spanish, Italian, African, Arab, Andean, and Amazonic food.\n\nThe artist Oswaldo Guayasamín (1919–1999) from Ecuador, represented with his painting style the feeling of the peoples of Latin America highlighting social injustices in various parts of the world. The Colombian Fernando Botero (1932) is one of the greatest exponents of painting and sculpture that continues still active and has been able to develop a recognizable style of his own. For his part, the Venezuelan Carlos Cruz-Diez has contributed significantly to contemporary art, with the presence of works around the world.\n\nCurrently several emerging South American artists are recognized by international art critics: Guillermo Lorca – Chilean painter, Teddy Cobeña – Ecuadorian sculptor and recipient of international sculpture award in France) and Argentine artist Adrián Villar Rojas – winner of the Zurich Museum Art Award among many others.\n\nA wide range of sports are played in the continent of South America, with football being the most popular overall, while baseball is the most popular in Venezuela.\n\nOther sports include basketball, cycling, polo, volleyball, futsal, motorsports, rugby (mostly in Argentina and Uruguay), handball, tennis, golf, field hockey, boxing and cricket.\n\nSouth America hosted its first Olympic Games in Rio de Janeiro, Brazil in 2016 and will host the Youth Olympic Games in Buenos Aires, Argentina in 2018.\n\nSouth America shares with Europe supremacy over the sport of football as all winners in FIFA World Cup history and all winning teams in the FIFA Club World Cup have come from these two continents. Brazil holds the record at the FIFA World Cup with five titles in total. Argentina and Uruguay have two titles each. So far four South American nations have hosted the tournament including the first edition in Uruguay (1930). The other three were Brazil (1950, 2014), Chile (1962), and Argentina (1978).\n\nSouth America is home to the longest running international football tournament; the Copa América, which has been regularly contested since 1916. Uruguay won the Copa América a record 15 times, surpassing hosts Argentina in 2011 to reach 15 titles (they were previously equal at 14 titles each during the 2011 Copa América).\n\nAlso, in South America, a multi-sport event, the South American Games, are held every four years. The first edition was held in La Paz in 1978 and the most recent took place in Santiago in 2014.\n\nSouth American Cricket Championship is an international limited-overs cricket tournament played since 1995 featuring national teams from South America and certain other invited sides including teams from North America, currently played annually but until 2013 was usually played every two seasons.\n\nDue to the diversity of topography and pluviometric precipitation conditions, the region's water resources vary enormously in different areas. In the Andes, navigation possibilities are limited, except for the Magdalena River, Lake Titicaca and the lakes of the southern regions of Chile and Argentina. Irrigation is an important factor for agriculture from northwestern Peru to Patagonia. Less than 10% of the known electrical potential of the Andes had been used until the mid-1960s.\n\nThe Brazilian Highlands has a much higher hydroelectric potential than the Andean region and its possibilities of exploitation are greater due to the existence of several large rivers with high margins and the occurrence of great differences forming huge cataracts, such as those of Paulo Afonso, Iguaçu and others. The Amazon River system has about 13,000 km of waterways, but its possibilities for hydroelectric use are still unknown.\n\nMost of the continent's energy is generated through hydroelectric power plants, but there is also an important share of thermoelectric and wind energy. Brazil and Argentina are the only South American countries that generate nuclear power, each with two nuclear power plants. In 1991 these countries signed a peaceful nuclear cooperation agreement.\n\nSouth American transportation systems are still deficient, with low kilometric densities. The region has about 1,700,000 km of highways and 100,000 km of railways, which are concentrated in the coastal strip, and the interior is still devoid of communication.\n\nOnly two railroads are continental: the Transandina, which connects Buenos Aires, in Argentina to Valparaíso, in Chile, and the Brazil–Bolivia Railroad, which makes it the connection between the port of Santos in Brazil and the city of Santa Cruz de la Sierra, in Bolivia. In addition, there is the Pan-American Highway, which crosses the Andean countries from north to south, although some stretches are unfinished.\n\nTwo areas of greater density occur in the railway sector: the platinum network, which develops around the Platine region, largely belonging to Argentina, with more than 45,000 km in length; And the Southeast Brazil network, which mainly serves the state of São Paulo, state of Rio de Janeiro and Minas Gerais. Brazil and Argentina also stand out in the road sector. In addition to the modern roads that extend through northern Argentina and south-east and south of Brazil, a vast road complex aims to link Brasilia, the federal capital, to the South, Southeast, Northeast and Northern regions of Brazil.\n\nThe Port of Callao is the main port of Peru.\nSouth America has one of the largest bays of navigable inland waterways in the world, represented mainly by the Amazon basin, the Platine basin, the São Francisco and the Orinoco basins, Brazil having about 54,000 km navigable, while Argentina has 6,500 km and Venezuela, 1,200 km.\n\nThe two main merchant fleets also belong to Brazil and Argentina. The following are those of Chile, Venezuela, Peru and Colombia. The largest ports in commercial movement are those of Buenos Aires, Santos, Rio de Janeiro, Bahía Blanca, Rosario, Valparaiso, Recife, Salvador, Montevideo, Paranaguá, Rio Grande, Fortaleza, Belém and Maracaibo.\n\nIn South America, commercial aviation has a magnificent expansion field, which has one of the largest traffic density lines in the world, Rio de Janeiro–São Paulo, and large airports, such as Congonhas, São Paulo–Guarulhos International and Viracopos (São Paulo), Rio de Janeiro International and Santos Dumont (Rio de Janeiro), Ezeiza (Buenos Aires), Confins International Airport (Belo Horizonte), Curitiba International Airport (Curitiba), Brasilia, Caracas, Montevideo, Lima, Bogotá, Recife, Salvador, Salgado Filho International Airport (Porto Alegre), Fortaleza, Manaus and Belém.\n\nThe main public transport in major cities is the bus. Many cities also have a diverse system of metro and subway trains, the first of which was the Buenos Aires subte, opened 1913. The Santiago subway is the largest network in South America, with 103 km, while the São Paulo subway is the largest in transportation, with more than 4.6 million passengers per day and was voted the best in the Americas. In Rio de Janeiro was installed the first railroad of the continent, in 1854. Today the city has a vast and diversified system of metropolitan trains, integrated with buses and subway. Recently it was also inaugurated in the city a Light Rail System called VLT, a small electrical trams at low speed, while São Paulo inaugurated its monorail, the first of South America. In Brazil, an express bus system called Bus Rapid Transit (BRT), which operates in several cities, has also been developed.\n\n\n"}
{"id": "198796", "url": "https://en.wikipedia.org/wiki?curid=198796", "title": "Spanish flu", "text": "Spanish flu\n\nThe 1918 influenza pandemic (January 1918 – December 1920; colloquially known as Spanish flu) was an unusually deadly influenza pandemic, the first of the two pandemics involving H1N1 influenza virus. It infected 500 million people around the world, including people on remote Pacific islands and in the Arctic, and resulted in the deaths of 50 to 100 million (three to five percent of the world's population), making it one of the deadliest natural disasters in human history.\n\nInfectious disease already limited life expectancy in the early 20th century. But in the first year of the pandemic, life expectancy in the United States dropped by about 12 years. Most influenza outbreaks disproportionately kill juvenile, elderly, or already weakened patients; in contrast, the 1918 pandemic predominantly killed previously healthy young adults.\n\nScientists offer several possible explanations for the high mortality rate of the 1918 influenza pandemic. Some research suggests that the specific variant of the virus had an unusually aggressive nature. One group of researchers recovered the virus from the bodies of frozen victims, and found that transfection in animals caused a rapidly progressive respiratory failure and death through a cytokine storm (overreaction of the body's immune system). It was postulated that the strong immune reactions of young adults ravaged the body, whereas the weaker immune systems of children and middle-aged adults resulted in fewer deaths among those groups.\n\nMore recent investigations, based mainly on original medical reports from the period of the pandemic, found that the viral infection itself was not more aggressive than any previous influenza, but that the special circumstances of the epidemic (malnourishment, overcrowded medical camps and hospitals, poor hygiene) promoted bacterial superinfection that killed most of the victims, typically after a somewhat prolonged death bed.\n\nHistorical and epidemiological data are inadequate to identify the pandemic's geographic origin. It was implicated in the outbreak of encephalitis lethargica in the 1920s.\n\nTo maintain morale, wartime censors minimized early reports of illness and mortality in Germany, the United Kingdom, France, and the United States. Papers were free to report the epidemic's effects in neutral Spain (such as the grave illness of King Alfonso XIII). This created a false impression of Spain as especially hard hit, thereby giving rise to the pandemic's nickname, \"Spanish Flu\".\n\nHistorian Alfred W. Crosby recorded that the flu originated in the U.S. state of Kansas, and popular writer John Barry echoed Crosby in describing Haskell County, as the point of origin. But already in late 1917, there had been a first wave of epidemic in at least 14 US military camps.\n\nInvestigative work in 1999 by a British team, led by virologist John Oxford of St Bartholomew's Hospital and the Royal London Hospital, identified the major troop staging and hospital camp in Étaples, France, as being the center of the Spanish flu. In late 1917, military pathologists reported the onset of a new disease with high mortality that they later recognized as the flu. The overcrowded camp and hospital — which treated thousands of victims of chemical attacks and other casualties of war — was an ideal site for the spreading of a respiratory virus; 100,000 soldiers were in transit every day. It also was home to a live piggery, and poultry were regularly brought in for food supplies from surrounding villages. Oxford and his team postulated that a significant precursor virus, harbored in birds, mutated so it could migrate to pigs that were kept near the front.\n\nEarlier hypotheses of the epidemic's origin have varied. Some hypothesized the flu originated in East Asia, a common area for transmission of disease from animals to humans because of dense living conditions. Claude Hannoun, the leading expert on the 1918 flu for the Pasteur Institute, asserted the former virus was likely to have come from China, mutating in the United States near Boston and spreading to Brest, France, Europe's battlefields, Europe, and the world via Allied soldiers and sailors as the main spreaders. He considered several other hypotheses of origin, such as Spain, Kansas (United States), and Brest, as being possible, but not likely.\n\nPolitical scientist Andrew Price-Smith published data from the Austrian archives suggesting the influenza had earlier origins, beginning in Austria in early 1917.\n\nIn 2014, historian Mark Humphries of the Memorial University of Newfoundland in St. John's stated that newly unearthed records confirmed that one of the side stories of the war, the mobilization of 96,000 Chinese laborers to work behind the British and French lines on World War I's western front, might have been the source of the pandemic. In the report, Humphries found archival evidence that a respiratory illness that struck northern China in November 1917 was identified a year later by Chinese health officials as identical to the \"Spanish\" flu. A report published in 2016 in the Journal of the Chinese Medical Association found no evidence that the 1918 virus was imported to Europe via Chinese and Southeast Asian soldiers and workers. It found evidence that the virus had been circulating in the European armies for months and possibly years before the 1918 pandemic.\n\nWhen an infected person sneezes or coughs, more than half a million virus particles can be spread to those close by. The close quarters and massive troop movements of World War I hastened the pandemic, and probably both increased transmission and augmented mutation; the war may also have increased the lethality of the virus. Some speculate the soldiers' immune systems were weakened by malnourishment, as well as the stresses of combat and chemical attacks, increasing their susceptibility.\n\nA large factor in the worldwide occurrence of this flu was increased travel. Modern transportation systems made it easier for soldiers, sailors, and civilian travelers to spread the disease.\n\nIn the United States, the disease was first observed in Haskell County, Kansas, in January 1918, prompting local doctor Loring Miner to warn the U.S. Public Health Service's academic journal. On 4 March 1918, company cook Albert Gitchell reported sick at Fort Riley, an American military facility that at the time was training American troops during World War I, making him the first recorded victim of the flu. Within days, 522 men at the camp had reported sick. By 11 March 1918, the virus had reached Queens, New York. Failure to take preventive measures in March/April was later criticised.\n\nIn August 1918, a more virulent strain appeared simultaneously in Brest, France; in Freetown, Sierra Leone; and in the U.S. in Boston, Massachusetts. The Spanish flu also spread through Ireland, carried there by returning Irish soldiers. The Allies of World War I came to call it the Spanish flu, primarily because the pandemic received greater press attention after it moved from France to Spain in November 1918. Spain was not involved in the war and had not imposed wartime censorship.\n\nThe global mortality rate from the 1918/1919 pandemic is not known, but an estimated 10% to 20% of those who were infected died. With about a third of the world population infected, this case-fatality ratio means 3% to 6% of the entire global population died. Influenza may have killed as many as 25 million people in its first 25 weeks. Older estimates say it killed 40–50 million people, while current estimates say 50–100 million people worldwide were killed.\n\nThis pandemic has been described as \"the greatest medical holocaust in history\" and may have killed more people than the Black Death. It is said that this flu killed more people in 24 weeks than AIDS killed in 24 years, and more in a year than the Black Death killed in a century, although the Black Death killed a much higher percentage of the world's smaller population at the time.\n\nThe disease killed in every area of the globe. As many as 17 million people died in India, about 5% of the population. The death toll in India's British-ruled districts alone was 13.88 million.\n\nIn Japan, of the 23 million people who were affected, 390,000 died. In the Dutch East Indies (now Indonesia), 1.5 million were assumed to have died among 30 million inhabitants. In Tahiti 13% of the population died during one month. Similarly, in Samoa 22% of the population of 38,000 died within two months.\n\nIn Iran, the mortality was very high: according to an estimate, between 902,400 and 2,431,000, or 8.0% to 21.7% of the total population died.\n\nIn the U.S., about 28% of the population became infected, and 500,000 to 675,000 died. Native American tribes were particularly hard hit. In the Four Corners area alone, 3,293 deaths were registered among Native Americans. Entire Inuit and Alaskan Native village communities died in Alaska. In Canada 50,000 died.\nIn Brazil, 300,000 died, including president Rodrigues Alves. In Britain, as many as 250,000 died; in France, more than 400,000.\nIn West Africa the influenza epidemic killed at least 100,000 people in Ghana. Tafari Makonnen (the future Haile Selassie, Emperor of Ethiopia) was one of the first Ethiopians who contracted influenza but survived. Many of his subjects did not; estimates for fatalities in the capital city, Addis Ababa, range from 5,000 to 10,000, or higher. In British Somaliland, one official estimated that 7% of the native population died.\n\nThis huge death toll was caused by an extremely high infection rate of up to 50% and the extreme severity of the symptoms, suspected to be caused by cytokine storms. Symptoms in 1918 were so unusual that initially influenza was misdiagnosed as dengue, cholera, or typhoid. One observer wrote, \"One of the most striking of the complications was hemorrhage from mucous membranes, especially from the nose, stomach, and intestine. Bleeding from the ears and petechial hemorrhages in the skin also occurred\". The majority of deaths were from bacterial pneumonia, a common secondary infection associated with influenza. The virus also killed people directly, by causing massive hemorrhages and edema in the lung.\n\nThe unusually severe disease killed up to 20% of those infected, as opposed to the usual flu epidemic mortality rate of 0.1%.\n\nThe pandemic mostly killed young adults. In 1918–1919, 99% of pandemic influenza deaths in the U.S. occurred in people under 65, and nearly half in young adults 20 to 40 years old. In 1920, the mortality rate among people under 65 had decreased sixfold to half the mortality rate of people over 65, but still 92% of deaths occurred in people under 65. This is unusual, since influenza is normally most deadly to weak individuals, such as infants (under age two), the very old (over age 70), and the immunocompromised. In 1918, older adults may have had partial protection caused by exposure to the 1889–1890 flu pandemic, known as the Russian flu.\nAccording to historian John M. Barry, the most vulnerable of all – \"those most likely, of the most likely\", to die – were pregnant women. He reported that in thirteen studies of hospitalized women in the pandemic, the death rate ranged from 23% to 71%.\nOf the pregnant women who survived childbirth, over one-quarter (26%) lost the child.\n\nAnother oddity was that the outbreak was widespread in the summer and autumn (in the Northern Hemisphere); influenza is usually worse in winter.\n\nModern analysis has shown the virus to be particularly deadly because it triggers a cytokine storm, which ravages the stronger immune system of young adults.\n\nIn fast-progressing cases, mortality was primarily from pneumonia, by virus-induced pulmonary consolidation. Slower-progressing cases featured secondary bacterial pneumonias, and there may have been neural involvement that led to mental disorders in some cases. Some deaths resulted from malnourishment.\n\nA study – conducted by He et al. – used a mechanistic modelling approach to study the three waves of the 1918 influenza pandemic. They tried to study the factors that underlie variability in temporal patterns, and the patterns of mortality and morbidity. Their analysis suggests that temporal variations in transmission rate provide the best explanation, and the variation in transmission required to generate these three waves is within biologically plausible values.\n\nAnother study by He et al. used a simple epidemic model, to incorporate three factors, including school opening and closing, temperature changes over the course of the outbreak, and human behavioral changes in response to the outbreak, to infer the cause of the three waves of the 1918 influenza pandemic. Their modelling results showed that all three factors are important but human behavioral responses showed the largest effects.\n\nThe second wave of the 1918 pandemic was much deadlier than the first. The first wave had resembled typical flu epidemics; those most at risk were the sick and elderly, while younger, healthier people recovered easily. But by August, when the second wave began in France, Sierra Leone, and the United States, the virus had mutated to a much deadlier form. As the PBS \"American Experience\": Influenza 1918 episode says, October 1918 was the deadliest month of the whole pandemic.\n\nThis increased severity has been attributed to the circumstances of the First World War. In civilian life, natural selection favors a mild strain. Those who get very ill stay home, and those mildly ill continue with their lives, preferentially spreading the mild strain. In the trenches, natural selection was reversed. Soldiers with a mild strain stayed where they were, while the severely ill were sent on crowded trains to crowded field hospitals, spreading the deadlier virus. The second wave began and the flu quickly spread around the world again. Consequently, during modern pandemics, health officials pay attention when the virus reaches places with social upheaval (looking for deadlier strains of the virus).\n\nThe fact that most of those who recovered from first-wave infections had become immune showed that it must have been the same strain of flu. This was most dramatically illustrated in Copenhagen, which escaped with a combined mortality rate of just 0.29% (0.02% in the first wave and 0.27% in the second wave) because of exposure to the less-lethal first wave. For the rest of the population, the second wave was far more deadly; the most vulnerable people were those like the soldiers in the trenches – young previously healthy adults.\n\nEven in areas where mortality was low, so many adults were incapacitated that much of everyday life was hampered. Some communities closed all stores or required customers to leave orders outside. There were reports that health-care workers could not tend the sick nor the gravediggers bury the dead because they too were ill. Mass graves were dug by steam shovel and bodies buried without coffins in many places.\n\nSeveral Pacific island territories were particularly hard-hit. The pandemic reached them from New Zealand, which was too slow to implement measures to prevent ships carrying the flu from leaving its ports. From New Zealand, the flu reached Tonga (killing 8% of the population), Nauru (16%) and Fiji (5%, 9,000 people).\n\nWorst affected was Western Samoa, formerly German Samoa, which had been occupied by New Zealand in 1914. 90% of the population was infected; 30% of adult men, 22% of adult women and 10% of children died. By contrast, Governor John Martin Poyer prevented the flu from reaching American Samoa by imposing a blockade. The disease spread fastest through the higher social classes among the indigenous peoples, because of the custom of gathering oral tradition from chiefs on their deathbeds; many community elders were infected through this process.\n\nIn New Zealand, 8,573 deaths were attributed to the 1918 pandemic influenza, resulting in a total population fatality rate of 0.74%. Māori were 10 times as likely to die as pākehā (Europeans), because of their poorer and more crowded housing, and rural population. \n\nIn Ireland, the Spanish Flu accounted for 10% of the total deaths in 1918.\n\nData analysis revealed 6,520 recorded deaths in Savannah-Chatham County, Georgia (population = 83,252) for the three-year period from January 1, 1917, to December 31, 1919. Of these deaths, influenza was specifically listed as the cause of death in 316 cases, representing 4.85 percent of all causes of death for the total time period.\n\nIn Japan, 257,363 deaths were attributed to influenza by July 1919, giving an estimated 0.425% mortality rate, much lower than nearly all other Asian countries for which data are available. The Japanese government severely restricted sea travel to and from the home islands when the pandemic struck.\n\nIn the Pacific, American Samoa and the French colony of New Caledonia also succeeded in preventing even a single death from influenza through effective quarantines. In Australia, nearly 12,000 perished.\n\nBy the end of the pandemic, the isolated island of Marajó, in Brazil's Amazon River Delta had not reported an outbreak.\n\nSaint Helena also reported no deaths.\n\nIn a 2009 paper published in the journal \"Clinical Infectious Diseases\", Karen Starko proposed that aspirin poisoning contributed substantially to the fatalities. She based this on the reported symptoms in those dying from the flu, as reported in the post mortem reports still available, and also the timing of the big \"death spike\" in October 1918. This occurred shortly after the Surgeon General of the United States Army and the \"Journal of the American Medical Association\" both recommended very large doses of 8 to 31 grams of aspirin per day as part of treatment. These levels will produce hyperventilation in 33% of patients, as well as lung edema in 3% of patients. Starko also notes that many early deaths showed \"wet,\" sometimes hemorrhagic lungs, whereas late deaths showed bacterial pneumonia. She suggests that the wave of aspirin poisonings was due to a \"perfect storm\" of events: Bayer's patent on aspirin expired, so many companies rushed in to make a profit and greatly increased the supply; this coincided with the Spanish flu; and the symptoms of aspirin poisoning were not known at the time.\n\nAs an explanation for the universally high mortality rate, this hypothesis was questioned in a letter to the journal published in April 2010 by Andrew Noymer and Daisy Carreon of the University of California, Irvine, and Niall Johnson of the Australian Commission on Safety and Quality in Health Care. They questioned the universal applicability of the aspirin theory, given the high mortality rate in countries such as India, where there was little or no access to aspirin at the time compared to the rate where aspirin was plentiful. They concluded that \"the salicylate [aspirin] poisoning hypothesis [was] difficult to sustain as the primary explanation for the unusual virulence of the 1918–1919 inﬂuenza pandemic\". In response, Starko said there was anecdotal evidence of aspirin use in India and argued that even if aspirin over-prescription had not contributed to the high Indian mortality rate, it could still have been a factor for high rates in areas where other exacerbating factors present in India played less of a role.\n\nAfter the lethal second wave struck in late 1918, new cases dropped abruptly – almost to nothing after the peak in the second wave. In Philadelphia, for example, 4,597 people died in the week ending 16 October, but by 11 November, influenza had almost disappeared from the city. One explanation for the rapid decline of the lethality of the disease is that doctors got better at preventing and treating the pneumonia that developed after the victims had contracted the virus; but John Barry stated in his book that researchers have found no evidence to support this.\n\nAnother theory holds that the 1918 virus mutated extremely rapidly to a less lethal strain. This is a common occurrence with influenza viruses: there is a tendency for pathogenic viruses to become less lethal with time, as the hosts of more dangerous strains tend to die out (see also \"Deadly Second Wave\", above).\n\nA 2006 study in the \"Journal of Political Economy\" found that \"cohorts in utero during the pandemic displayed reduced educational attainment, increased rates of physical disability, lower income, lower socioeconomic status, and higher transfer payments compared with other birth cohorts.\" A 2018 study found that the pandemic reduced educational attainment in populations.\n\nAcademic Andrew Price-Smith has made the argument that the virus helped tip the balance of power in the later days of the war towards the Allied cause. He provides data that the viral waves hit the Central Powers before they hit the Allied powers, and that both morbidity and mortality in Germany and Austria were considerably higher than in Britain and France.\n\nIn the United States, Britain and other countries, despite the relatively high morbidity and mortality rates that resulted from the epidemic in 1918–1919, the Spanish flu began to fade from public awareness over the decades until the arrival of news about bird flu and other pandemics in the 1990s and 2000s. This has led some historians to label the Spanish flu a \"forgotten pandemic\".\n\nVarious theories of why the Spanish flu was \"forgotten\" include the rapid pace of the pandemic, which killed most of its victims in the United States, for example, within a period of less than nine months, resulting in limited media coverage. The general population was familiar with patterns of pandemic disease in the late 19th and early 20th centuries: typhoid, yellow fever, diphtheria, and cholera all occurred near the same time. These outbreaks probably lessened the significance of the influenza pandemic for the public. In some areas, the flu was not reported on, the only mention being that of advertisements for medicines claiming to cure it.\n\nIn addition, the outbreak coincided with the deaths and media focus on the First World War. Another explanation involves the age group affected by the disease. The majority of fatalities, from both the war and the epidemic, were among young adults. The deaths caused by the flu may have been overlooked due to the large numbers of deaths of young men in the war or as a result of injuries. When people read the obituaries, they saw the war or postwar deaths and the deaths from the influenza side by side. Particularly in Europe, where the war's toll was extremely high, the flu may not have had a great, separate, psychological impact, or may have seemed a mere extension of the war's tragedies.\n\nThe duration of the pandemic and the war could have also played a role. The disease would usually only affect a certain area for a month before leaving, while the war, which most had initially expected to end quickly, had lasted for four years by the time the pandemic struck. This left little time for the disease to have a significant impact on the economy.\n\nRegarding global economic effects, many businesses in the entertainment and service industries suffered losses in revenue, while the health care industry reported profit gains.\nHistorian Nancy Bristow has argued that the pandemic, when combined with the increasing number of women attending college, contributed to the success of women in the field of nursing. This was due in part to the failure of medical doctors, who were predominantly men, to contain and prevent the illness. Nursing staff, who were predominantly women, felt more inclined to celebrate the success of their patient care and less inclined to identify the spread of the disease with their own work.\n\nIn Spain, sources from the period explicitly linked the Spanish flu to the cultural figure of Don Juan. The nickname for the flu, the \"Naples Soldier\", was adopted from Federico Romero and Guillermo Fernández Shaw's operetta, \"The Song of Forgetting\" (\"La canción del olvido\"), the protagonist of which is a stock Don Juan type. Federico Romero, one of the librettists, quipped that the play's most popular musical number, \"Naples Soldier\", was as catchy as the flu. Davis has argued the Spanish flu–Don Juan connection served a cognitive function, allowing Spaniards to make sense of their epidemic experience by interpreting it through a familiar template, namely the Don Juan story.\n\nThe origin of the Spanish flu pandemic, and the relationship between the near-simultaneous outbreaks in humans and swine, have been controversial. One hypothesis is that the virus strain originated at Fort Riley, Kansas, in viruses in poultry and swine which the fort bred for food; the soldiers were then sent from Fort Riley around the world, where they spread the disease. Similarities between a reconstruction of the virus and avian viruses, combined with the human pandemic preceding the first reports of influenza in swine, led researchers to conclude the influenza virus jumped directly from birds to humans, and swine caught the disease from humans.\n\nOthers have disagreed, and more recent research has suggested the strain may have originated in a nonhuman, mammalian species. An estimated date for its appearance in mammalian hosts has been put at the period 1882–1913. This ancestor virus diverged about 1913–1915 into two clades (or biological groups), which gave rise to the classical swine and human H1N1 influenza lineages. The last common ancestor of human strains dates to between February 1917 and April 1918. Because pigs are more readily infected with avian influenza viruses than are humans, they were suggested as the original recipients of the virus, passing the virus to humans sometime between 1913 and 1918.\n\nAn effort to recreate the 1918 flu strain (a subtype of avian strain H1N1) was a collaboration among the Armed Forces Institute of Pathology, the USDA ARS Southeast Poultry Research Laboratory and Mount Sinai School of Medicine in New York City. The effort resulted in the announcement (on 5 October 2005) that the group had successfully determined the virus's genetic sequence, using historic tissue samples recovered by pathologist Johan Hultin from a female flu victim buried in the Alaskan permafrost and samples preserved from American soldiers.\n\nOn 18 January 2007, Kobasa et al. (2007) reported that monkeys (\"Macaca fascicularis\") infected with the recreated flu strain exhibited classic symptoms of the 1918 pandemic, and died from a cytokine storm—an overreaction of the immune system. This may explain why the 1918 flu had its surprising effect on younger, healthier people, as a person with a stronger immune system would potentially have a stronger overreaction.\n\nOn 16 September 2008, the body of British politician and diplomat Sir Mark Sykes was exhumed to study the RNA of the flu virus in efforts to understand the genetic structure of modern H5N1 bird flu. Sykes had been buried in 1919 in a lead coffin which scientists hoped had helped preserve the virus. The coffin was found to be split because of the weight of soil over it, and the cadaver was badly decomposed. Nonetheless, samples of lung and brain tissue were taken through the split, with the coffin remaining \"in situ\" in the grave during this process.\n\nIn December 2008, research by Yoshihiro Kawaoka of the University of Wisconsin linked the presence of three specific genes (termed PA, PB1, and PB2) and a nucleoprotein derived from 1918 flu samples to the ability of the flu virus to invade the lungs and cause pneumonia. The combination triggered similar symptoms in animal testing.\n\nIn June 2010, a team at the Mount Sinai School of Medicine reported the 2009 flu pandemic vaccine provided some cross-protection against the 1918 flu pandemic strain.\n\nOne of the few things known for certain about the influenza in 1918 and for some years after was that it was, out of the laboratory, exclusively a disease of human beings.\n\nIn 2013, the AIR Worldwide Research and Modeling Group \"characterized the historic 1918 pandemic and estimated the effects of a similar pandemic occurring today using the AIR Pandemic Flu Model\". In the model, \"a modern day \"Spanish flu\" event would result in additional life insurance losses of between USD 15.3–27.8 billion in the United States alone\", with 188,000–337,000 deaths in the United States.\n\n\n"}
{"id": "19936884", "url": "https://en.wikipedia.org/wiki?curid=19936884", "title": "Speed record", "text": "Speed record\n\nA speed record is a world record for speed by a person, animal, or vehicle. The function of speed record is to record the speed of moving animate objects such as humans, animals or vehicles.\n\nOverall speed record is the record for the highest average speed regardless of any criteria, categories or classes that all the more specific records belong to, provided that the route was completed. It helps to compare various performances that differ by the type of the craft, vessel or vehicle, the departure and the arrival points (provided that the distances are comparable), number, age and gender of the crew members, departure date, etc. The distance used for calculating the overall speed record is usually the distance in a straight line. In the case of man-powered races, overall speed record doesn't always reflect the best performance. It is highly dependent on technological advantages generating the speed of the craft, vessel or vehicle.\n\n\n\n\n\n"}
{"id": "28742", "url": "https://en.wikipedia.org/wiki?curid=28742", "title": "Supercontinent", "text": "Supercontinent\n\nIn geology, a supercontinent is the assembly of most or all of Earth's continental blocks or cratons to form a single large landmass. However, many earth scientists use an alternate definition, \"a clustering of nearly all continents\", which leaves room for interpretation and is easier to apply to Precambrian times. \n\nSupercontinents have assembled and dispersed multiple times in the geologic past (see table). According to the modern definitions, a supercontinent does not exist today. The supercontinent Pangaea is the collective name describing all of the continental landmasses when they were most recently near to one another. The positions of continents have been accurately determined back to the early Jurassic, shortly before the breakup of Pangaea (see animated image). The earlier continent Gondwana is not considered a supercontinent under the first definition, since the landmasses of Baltica, Laurentia and Siberia were separate at the time. \n\nThe following table displays historical supercontinents, using a general definition.\n\nThere are two contrasting models for supercontinent evolution through geological time. The first model theorizes that at least two separate supercontinents existed comprising Vaalbara (from ~3636 to ) and Kenorland (from ~2720 to ). The Neoarchean supercontinent consisted of Superia and Sclavia. These parts of Neoarchean age broke off at ~2480 and and portions of them later collided to form Nuna (Northern Europe North America) (). Nuna continued to develop during the Mesoproterozoic, primarily by lateral accretion of juvenile arcs, and in Nuna collided with other land masses, forming Rodinia. Between ~825 and Rodinia broke apart. However, before completely breaking up, some fragments of Rodinia had already come together to form Gondwana (also known as Gondwanaland) by . Pangaea formed by through the collision of Gondwana, Laurasia (Laurentia and Baltica), and Siberia.\n\nThe second model (Kenorland-Arctica) is based on both palaeomagnetic and geological evidence and proposes that the continental crust comprised a single supercontinent from until break-up during the Ediacaran Period after . The reconstruction is derived from the observation that palaeomagnetic poles converge to quasi-static positions for long intervals between ~2.72–2.115, 1.35–1.13, and with only small peripheral modifications to the reconstruction. During the intervening periods, the poles conform to a unified apparent polar wander path. Because this model shows that exceptional demands on the paleomagnetic data are satisfied by prolonged quasi-integrity, it must be regarded as superseding the first model proposing multiple diverse continents, although the first phase (Protopangea) essentially incorporates Vaalbara and Kenorland of the first model. The explanation for the prolonged duration of the Protopangea-Paleopangea supercontinent appears to be that lid tectonics (comparable to the tectonics operating on Mars and Venus) prevailed during Precambrian times. Plate tectonics as seen on the contemporary Earth became dominant only during the latter part of geological times.\n\nThe Phanerozoic supercontinent Pangaea began to break up and is still doing so today. Because Pangaea is the most recent of Earth's supercontinents, it is the most well known and understood. Contributing to Pangaea's popularity in the classroom is the fact that its reconstruction is almost as simple as fitting the present continents bordering the Atlantic-type oceans like puzzle pieces.\n\nA supercontinent cycle is the break-up of one supercontinent and the development of another, which takes place on a global scale. Supercontinent cycles are not the same as the Wilson cycle, which is the opening and closing of an individual oceanic basin. The Wilson cycle rarely synchronizes with the timing of a supercontinent cycle. However, supercontinent cycles and Wilson cycles were both involved in the creation of Pangaea and Rodinia.\n\nSecular trends such as carbonatites, granulites, eclogites, and greenstone belt deformation events are all possible indicators of Precambrian supercontinent cyclicity, although the Protopangea-Paleopangea solution implies that Phanerozoic style of supercontinent cycles did not operate during these times. Also there are instances where these secular trends have a weak, uneven or lack of imprint on the supercontinent cycle; secular methods for supercontinent reconstruction will produce results that have only one explanation and each explanation for a trend must fit in with the rest.\n\nThe causes of supercontinent assembly and dispersal are thought to be driven by convection processes in the Earth's mantle. Approximately 660 km into the mantle, a discontinuity occurs, affecting the surface crust through processes like plumes and \"superplumes\". When a slab of subducted crust is denser than the surrounding mantle, it sinks to the discontinuity. Once the slabs build up, they will sink through to the lower mantle in what is known as a \"slab avalanche\". This displacement at the discontinuity will cause the lower mantle to compensate and rise elsewhere. The rising mantle can form a plume or superplume.\n\nBesides having compositional effects on the upper mantle by replenishing the large-ion lithophile elements, volcanism affects plate movement. The plates will be moved towards a geoidal low perhaps where the slab avalanche occurred and pushed away from the geoidal high that can be caused by the plumes or superplumes. This causes the continents to push together to form supercontinents and was evidently the process that operated to cause the early continental crust to aggregate into Protopangea. Dispersal of supercontinents is caused by the accumulation of heat underneath the crust due to the rising of very large convection cells or plumes, and a massive heat release resulted in the final break-up of Paleopangea. Accretion occurs over geoidal lows that can be caused by avalanche slabs or the downgoing limbs of convection cells. Evidence of the accretion and dispersion of supercontinents is seen in the geological rock record.\n\nThe influence of known volcanic eruptions does not compare to that of flood basalts. The timing of flood basalts has corresponded with large-scale continental break-up. However, due to a lack of data on the time required to produce flood basalts, the climatic impact is difficult to quantify. The timing of a single lava flow is also undetermined. These are important factors on how flood basalts influenced paleoclimate.\n\nGlobal paleogeography and plate interactions as far back as Pangaea are relatively well understood today. However, the evidence becomes more sparse further back in geologic history. Marine magnetic anomalies, passive margin match-ups, geologic interpretation of orogenic belts, paleomagnetism, paleobiogeography of fossils, and distribution of climatically sensitive strata are all methods to obtain evidence for continent locality and indicators of environment throughout time.\n\nPhanerozoic (541 Ma to present) and Precambrian ( to ) had primarily passive margins and detrital zircons (and orogenic granites), whereas the tenure of Pangaea contained few. Matching edges of continents are where passive margins form. The edges of these continents may rift. At this point, seafloor spreading becomes the driving force. Passive margins are therefore born during the break-up of supercontinents and die during supercontinent assembly. Pangaea's supercontinent cycle is a good example for the efficiency of using the presence, or lack of, these entities to record the development, tenure, and break-up of supercontinents. There is a sharp decrease in passive margins between 500 and during the timing of Pangaea's assembly. The tenure of Pangaea is marked by a low number of passive margins during 336 to and its break-up is indicated accurately by an increase in passive margins.\n\nOrogenic belts can form during the assembly of continents and supercontinents. The orogenic belts present on continental blocks are classified into three different categories and have implications of interpreting geologic bodies. Intercratonic orogenic belts are characteristic of ocean basin closure. Clear indicators of intercratonic activity contain ophiolites and other oceanic materials that are present in the suture zone. Intracratonic orogenic belts occur as thrust belts and do not contain any oceanic material. However, the absence of ophiolites is not strong evidence for intracratonic belts, because the oceanic material can be squeezed out and eroded away in an intercratonic environment. The third kind of orogenic belt is a confined orogenic belt which is the closure of small basins. The assembly of a supercontinent would have to show intercratonic orogenic belts. However, interpretation of orogenic belts can be difficult.\n\nThe collision of Gondwana and Laurasia occurred in the late Palaeozoic. By this collision, the Variscan mountain range was created, along the equator. This 6000-km-long mountain range is usually referred to in two parts: the Hercynian mountain range of the late Carboniferous makes up the eastern part, and the western part is called the Appalachians, uplifted in the early Permian. (The existence of a flat elevated plateau like the Tibetan Plateau is under much debate.) The locality of the Variscan range made it influential to both the northern and southern hemispheres. The elevation of the Appalachians would greatly influence global atmospheric circulation.\n\nContinents affect the climate of the planet drastically, with supercontinents having a larger, more prevalent influence. Continents modify global wind patterns, control ocean current paths and have a higher albedo than the oceans. Winds are redirected by mountains, and albedo differences cause shifts in onshore winds. Higher elevation in continental interiors produce cooler, drier climate, the phenonmenon of continentality. This is seen today in Eurasia, and rock record shows evidence of continentality in the middle of Pangaea.\n\nThe term glacio-epoch refers to a long episode of glaciation on Earth over millions of years. Glaciers have major implications on the climate particularly through sea level change. Changes in the position and elevation of the continents, the paleolatitude and ocean circulation affect the glacio-epochs. There is an association between the rifting and breakup of continents and supercontinents and glacio-epochs. According to the first model for Precambrian supercontinents described above the breakup of Kenorland and Rodinia were associated with the Paleoproterozoic and Neoproterozoic glacio-epochs, respectively. In contrast, the second solution described above shows that these glaciations correlated with periods of low continental velocity and it is concluded that a fall in tectonic and corresponding volcanic activity was responsible for these intervals of global frigidity. During the accumulation of supercontinents with times of regional uplift, glacio-epochs seem to be rare with little supporting evidence. However, the lack of evidence does not allow for the conclusion that glacio-epochs are not associated with collisional assembly of supercontinents. This could just represent a preservation bias.\n\nDuring the late Ordovician (~458.4 Ma), the particular configuration of Gondwana may have allowed for glaciation and high CO levels to occur at the same time. However, some geologists disagree and think that there was a temperature increase at this time. This increase may have been strongly influenced by the movement of Gondwana across the South Pole, which may have prevented lengthy snow accumulation. Although late Ordovician temperatures at the South Pole may have reached freezing, there were no ice sheets during the early Silurian through the late Mississippian Agreement can be met with the theory that continental snow can occur when the edge of a continent is near the pole. Therefore, Gondwana, although located tangent to the South Pole, may have experienced glaciation along its coast.\n\nThough precipitation rates during monsoonal circulations are difficult to predict, there is evidence for a large orographic barrier within the interior of Pangaea during the late Paleozoic The possibility of the SW-NE trending Appalachian-Hercynian Mountains makes the region's monsoonal circulations potentially relatable to present day monsoonal circulations surrounding the Tibetan Plateau, which is known to positively influence the magnitude of monsoonal periods within Eurasia. It is therefore somewhat expected that lower topography in other regions of the supercontinent during the Jurassic would negatively influence precipitation variations. The breakup of supercontinents may have affected local precipitation. When any supercontinent breaks up, there will be an increase in precipitation runoff over the surface of the continental land masses, increasing silicate weathering and the consumption of CO.\n\nEven though during the Archaean solar radiation was reduced by 30 percent and the Cambrian-Precambrian boundary by six percent, the Earth has only experienced three ice ages throughout the Precambrian. Erroneous conclusions are more likely to be made when models are limited to one climatic configuration (which is usually present day).\n\nCold winters in continental interiors are due to rate ratios of radiative cooling (greater) and heat transport from continental rims. To raise winter temperatures within continental interiors, the rate of heat transport must increase to become greater than the rate of radiative cooling. Through climate models, alterations in atmospheric CO content and ocean heat transport are not comparatively effective.\n\nCO models suggest that values were low in the late Cenozoic and Carboniferous-Permian glaciations. Although early Paleozoic values are much larger (more than ten percent higher than that of today). This may be due to high seafloor spreading rates after the breakup of Precambrian supercontinents and the lack of land plants as a carbon sink.\n\nDuring the late Permian, it is expected that seasonal Pangaean temperatures varied drastically. Subtropic summer temperatures were warmer than that of today by as much as 6–10 degrees and mid-latitudes in the winter were less than −30 degrees Celsius. These seasonal changes within the supercontinent were influenced by the large size of Pangaea. And, just like today, coastal regions experienced much less variation.\n\nDuring the Jurassic, summer temperatures did not rise above zero degrees Celsius along the northern rim of Laurasia, which was the northernmost part of Pangaea (the southernmost portion of Pangaea was Gondwana). Ice-rafted dropstones sourced from Russia are indicators of this northern boundary. The Jurassic is thought to have been approximately 10 degrees Celsius warmer along 90 degrees East paleolongitude compared to the present temperature of today's central Eurasia.\n\nMany studies of the Milankovitch fluctuations during supercontinent time periods have focused on the Mid-Cretaceous. Present amplitudes of Milankovitch cycles over present day Eurasia may be mirrored in both the southern and northern hemispheres of the supercontinent Pangaea. Climate modeling shows that summer fluctuations varied 14–16 degrees Celsius on Pangaea, which is similar or slightly higher than summer temperatures of Eurasia during the Pleistocene. The largest-amplitude Milankovitch cycles are expected to have been at mid- to high-latitudes during the Triassic and Jurassic.\n\nGranites and detrital zircons have notably similar and episodic appearances in the rock record. Their fluctuations correlate with Precambrian supercontinent cycles. The U–Pb zircon dates from orogenic granites are among the most reliable aging determinants. Some issues exist with relying on granite sourced zircons, such as a lack of evenly globally sourced data and the loss of granite zircons by sedimentary coverage or plutonic consumption. Where granite zircons are less adequate, detrital zircons from sandstones appear and make up for the gaps. These detrital zircons are taken from the sands of major modern rivers and their drainage basins. Oceanic magnetic anomalies and paleomagnetic data are the primary resources used for reconstructing continent and supercontinent locations back to roughly 150 Ma.\n\nPlate tectonics and the chemical composition of the atmosphere (specifically greenhouse gases) are the two most prevailing factors present within the geologic time scale. Continental drift influences both cold and warm climatic episodes. Atmospheric circulation and climate are strongly influenced by the location and formation of continents and megacontinents. Therefore, continental drift influences mean global temperature.\n\nOxygen levels of the Archaean Eon were negligible and today they are roughly 21 percent. It is thought that the Earth's oxygen content has risen in stages: six or seven steps that are timed very closely to the development of Earth's supercontinents.\nThe process of Earth's increase in atmospheric oxygen content is theorized to have started with continent-continent collision of huge land masses forming supercontinents, and therefore possibly supercontinent mountain ranges (supermountains). These supermountains would have eroded, and the mass amounts of nutrients, including iron and phosphorus, would have washed into oceans, just as we see happening today. The oceans would then be rich in nutrients essential to photosynthetic organisms, which would then be able to respire mass amounts of oxygen. There is an apparent direct relationship between orogeny and the atmospheric oxygen content). There is also evidence for increased sedimentation concurrent with the timing of these mass oxygenation events, meaning that the organic carbon and pyrite at these times were more likely to be buried beneath sediment and therefore unable to react with the free oxygen. This sustained the atmospheric oxygen increases.\n\nDuring this time, there was an increase in molybdenum isotope fractionation. It was temporary, but supports the increase in atmospheric oxygen because molybdenum isotopes require free oxygen to fractionate. Between 2.45 and the second period of oxygenation occurred, it has been called the 'great oxygenation event.' There are many pieces of evidence that support the existence of this event, including red beds appearance (meaning that Fe was being produced and became an important component in soils). The third oxygenation stage approximately is indicated by the disappearance of iron formations. Neodymium isotopic studies suggest that iron formations are usually from continental sources, meaning that dissolved Fe and Fe had to be transported during continental erosion. A rise in atmospheric oxygen prevents Fe transport, so the lack of iron formations may have been due to an increase in oxygen. The fourth oxygenation event, roughly is based on modeled rates of sulfur isotopes from marine carbonate-associated sulfates. An increase (near doubled concentration) of sulfur isotopes, which is suggested by these models, would require an increase in oxygen content of the deep oceans. Between 650 and there were three increases in ocean oxygen levels, this period is the fifth oxygenation stage. One of the reasons indicating this period to be an oxygenation event is the increase in redox-sensitive molybdenum in black shales. The sixth event occurred between 360 and and was identified by models suggesting shifts in the balance of S in sulfates and C in carbonates, which were strongly influenced by an increase in atmospheric oxygen.\n\n\n\n"}
{"id": "8268647", "url": "https://en.wikipedia.org/wiki?curid=8268647", "title": "Supranational aspects of international organizations", "text": "Supranational aspects of international organizations\n\nMany international organizations also have supranational aspects, meaning that decisions can be made by the organization as a whole that are binding on member states that disagree.\n\n\"Political Unification Revisited: On Building Supranational Communities\" by Amitai Etzioni offers the definition of supranationality that is used in this entry. Etzioni writes that supranationality can be thought of as \"a composite of several elements.\" These elements can be present alone or all together. The three elements of supranationality are defined as follows:\n\n"}
{"id": "27247561", "url": "https://en.wikipedia.org/wiki?curid=27247561", "title": "Tanks of the U.S. in the World Wars", "text": "Tanks of the U.S. in the World Wars\n\nAs the American army did not have tanks of its own, the French two-man Renault FT Light Tank was used by US in the later stages of World War I. It was cheap and well-suited for mass production, and in addition to its traversable turret another innovative feature of the FT was its engine located at the rear. This pattern, with the gun located in a mounted turret and rear engine, became the standard for most succeeding tanks across the world even to this day.\n\nThe M1917 was a US tank accepted by the army in October 1918 and is primarily based on the plans of the French Renault FT. The crew, a driver and gunner, were separated from the engine by a bulkhead. Steel idler wheels replaced the wooden idlers fitted to French examples. Approximately 64 of the M1917 were built before the end of World War I and 10 were sent to Europe, but too late to be used in combat. After the war Van Dorn Iron Works, the Maxwell Motor Company, and the C.L. Best Tractor Company created 950 more. 374 had cannons and 526 had machine guns and 50 were signal (radio) tanks. A later modification, the M1917A1, was a lengthened, rebuilt, updated version compared to the French one, having a 100 hp Franklin engine and an electric self-starter rather than a crank starter.\n\nU.S. troops also used the British Heavy Tanks Mk V and Mk V* (pronounced \"Mark Five\" and \"Mark Five Star\"). A battalion trained in England and saw action in France in the last six weeks of the War. On a small number of occasions, U.S. troops were supported by tank units of the French Army operating Schneider and Saint-Chamond machines.\n\nMarmon-Herrington tanks that could not be delivered because of the fall of the Dutch East Indies were taken over by the US. The CTLS-4TAC and -4TAY tanks were redesignated Light Tank T14 and T16 respectively. They were used for training, some were used in Alaska and by the US Marines. The CTLS-4TAC has the turret offset to the left, the CTLS-4TAY to the right.\n\nThe Stuart was an upgrade of the M2 Light Tank.\n\nThe initial upgrade was designated the M3 Stuart and had thicker armor, modified suspension and a 37mm mm gun. Production of the M3 and later the M5 Stuart started in March 1941 and continued until October 1943 with a total of 25,000 produced.\n\nAn upgrade of the M3, which was initially called M4 but later redesignated the M5, was developed with improved engines and produced through 1942. The M5 featured a redesigned hull and driver's hatches moved to the top. The M5 gradually replaced the M3 in production from 1942 and was in turn succeeded by the Light Tank M24 in 1944.\n\nThe British Army was the first to use the M3 in combat. In November 1941, some 170 Stuarts took part in Operation Crusader, with poor results.\n\nAlthough the high losses suffered by Stuart-equipped units during the operation had more to do with better tactics and training of the enemy than superiority of enemy tanks in the North African campaign, the operation revealed that the M3 had several technical faults. Mentioned in the British complaints were the 37 mm gun, a limited range and poor internal layout. The two-man turret crew was a significant weakness, and some British units tried to fight with three-man turret crews. Crews liked its high speed and mechanical reliability.\n\nFrom the summer of 1942, when enough US medium tanks had been received, the British usually kept Stuarts out of tank-to-tank combat. M3s, M3A3s, and M5s continued in British service until the end of the war, but British armor units had a smaller proportion of these light tanks than US units.\n\nThe other major Lend-Lease recipient of the M3, the Soviet Union, was even less happy with the tank, considering it undergunned, underarmored, likely to catch fire, and too sensitive to fuel quality. The narrow tracks were highly unsuited to operation in winter conditions, as they resulted in high ground pressures that sunk the tank. However, the Stuart was superior to early-war Soviet light tanks such as the T-60, which were often underpowered and possessed even lighter armament than the Stuart. In 1943, the Red Army tried out the M5 and decided that the upgraded design wasn't much better than the M3. Being less desperate than in 1941, the Soviets turned down an American offer to supply the M5. M3s continued in Red Army service at least until 1944.\n\nIn US Army service, the M3 first saw combat in the Philippines. Two battalions, comprising the Provisional Tank Group fought in the Bataan peninsula campaign. When the American army joined the North African Campaign in late 1942, Stuart units still formed a large part of its armor strength.\n\nAfter the disastrous Battle of the Kasserine Pass the US quickly disbanding most of their light tank battalions and subordinating the Stuarts to medium tank battalions performing the traditional cavalry missions of scouting and screening. For the rest of the war, most US tank battalions had three companies of M4 Shermans and one company of M3s or M5/M5A1s.\n\nIn the European theater, Allied light tanks had to be given cavalry and infantry fire support roles since their main cannon armament could not compete with heavier enemy AFVs. However, the Stuart was still effective in combat in the Pacific Theater, as Japanese tanks were both relatively rare and were generally much weaker than even Allied light tanks. Japanese infantrymen were poorly equipped with anti-tank weapons and tended to attack tanks using close-assault tactics. In this environment, the Stuart was only moderately more vulnerable than medium tanks. In addition, the poor terrain and roads common to the theatre were unsuitable for the much heavier M4 medium tanks, and so initially, only light armor could be deployed. Heavier M4s were eventually brought to overcome heavily entrenched positions, though the Stuart continued to serve in a combat capacity until the end of the war.\n\nThe US liquidated its Stuarts when it got sufficient numbers of M24 Chaffees but the tank remained in service until the end of the war and well after. In addition to the United States, United Kingdom and Soviet Union, who were the primary users, it was also used by France and China.\n\nIn April 1943 the government started work on the M24 Chaffee, designated Light Tank T24 as a replacement for the M3/M5 Stuart after the original replacement, the M7, was rejected in March. Every effort was made to keep the weight of the vehicle under 20 tons. The armor was kept light, and a lightweight 75 mm gun was developed. The design also featured wider tracks and torsion bar suspension. It had relatively low silhouette and a three-man turret. In mid-October the first pilot vehicle was delivered and production began in 1944 under the designation Light Tank M24. 4,730 were produced by the time production was stopped in August 1945.\n\nThe first thirty-four M24s reached Europe in November 1944 and were issued to the U.S. 2nd Cavalry Group (Mechanized) in France. These were then issued to F Company, 2nd Cavalry Reconnaissance Battalion and F Company, 42nd Cavalry Reconnaissance Battalion which each received seventeen M24s. During the Battle of the Bulge in December 1944, these units and their new tanks were rushed to the southern sector; two of the M24s were detached to serve with the 740th Tank Battalion of the U.S. First Army.\n\nThe M24 started to enter widespread issue in December 1944 but they were slow in reaching the front-line combat units. By the end of the war many armored divisions were still mainly equipped with the M5. Some armored divisions did not receive their first M24s until the war was over.\n\nReports were generally positive. Crews liked the improved off-road performance and reliability, but were most appreciative of the 75 mm main gun, as a vast improvement over the 37 mm. The M24 was still not up to the challenge of fighting tanks, however, the bigger gun at least gave it a chance to defend itself. Its light armor made it vulnerable in tank vs. tank actions.\n\nThe M24s contribution to winning the war was insignificant, as too few arrived too late to replace the M5s of the armored divisions.\n\nThe U.S. Army designed the M4 as a replacement for the M3 Medium. The designated goals were to produce a fast, dependable medium tank able to support infantry, provide breakthrough striking capacity, and defeat any tank currently in use by the Axis nations. In April 1941 the U.S. Armored Force Board chose the simplest of five designs. Known as the T6, the design was a modified M3 hull and chassis, carrying a newly designed turret mounting the Lee's main gun. This became the Sherman.\n\nThe prototype M4 was completed in September 1941. The T6 became standardized as the M4, and production after modifications began in October 1941.\n\nThe U.S. Army had seven main sub-designations for M4 variants during the production period: M4, M4A1, M4A2, M4A3, M4A4, M4A5, and M4A6. These designations were manufactured concurrently at different locations.\n\nWhile most Shermans ran on gasoline, the M4A2 and M4A6 had diesel engines: the M4A2 with a pair of GMC 6-71 straight six engines, the M4A6 a Caterpillar RD1820 radial. These, plus the M4A4 which used the Chrysler A57 multibank engine, were mostly supplied to Allied countries under Lend-Lease. \"M4\" can refer specifically to the initial sub-type with its Continental radial engine, or generically, to the entire family of seven Sherman sub-types, depending on context. Many details of production, shape, strength, and performance improved throughout production without a change to the tank's basic model number; more durable suspension units, safer \"wet\" (W) ammunition stowage, and stronger armor arrangements, such as the M4 Composite, which had a cast front hull section mated to a welded rear hull. British nomenclature differed from that employed by the U.S.\n\nEarly Shermans mounted a 75 mm medium-velocity general-purpose gun. Although Ordnance began work on the Medium Tank T20 as a Sherman replacement, ultimately the Army decided to minimize production disruption by incorporating elements of other tank designs into Sherman production. Later M4A1, M4A2, and M4A3 models received the larger T23 turret, with a high-velocity 76 mm gun M1, which reduced the number of HE and smoke rounds carried and increased the number of anti-tank rounds. Later, the M4 and M4A3 were factory-produced with a 105 mm howitzer and a new distinctive mantlet in the original turret. The first standard-production 76 mm gun Sherman was an M4A1, accepted in January 1944, and the first standard-production 105 mm howitzer Sherman was an M4 accepted in February 1944.\n\nIn June–July 1944, the Army accepted a limited run of 254 M4A3E2 \"Jumbo\" Shermans, which had very thick armor, and the 75 mm gun in a new, heavier T23-style turret, in order to assault fortifications. The M4A3 was the first to be factory-produced with the HVSS (horizontal volute spring suspension) suspension with wider tracks to distribute weight, and the smooth ride of the HVSS with its experimental E8 designation led to the nickname \"Easy Eight\" for Shermans so equipped. Both the Americans and the British developed a wide array of special attachments for the Sherman; few saw combat, and most remained experimental. Those which saw action included the bulldozer blade for Sherman dozer tanks, Duplex Drive for \"swimming\" Sherman tanks, R3 flame thrower for \"Zippo\" flame tanks, and the T34 60-tube \"Calliope\" 4.5\" rocket launcher for the Sherman turret. The British variants (DD's and mine flails) were among \"Hobart's Funnies,\" named after their commander, Percy Hobart of the 79th Armoured Division. \n\nThe M4 Sherman's basic chassis was used for all the sundry roles of a modern mechanized force: roughly 50,000 Sherman tanks, plus thousands more derivative vehicles under different model numbers. These included M32 and M74 \"tow truck\"-style recovery tanks with winches, booms, and an 81 mm mortar for smoke screens; M34 (from M32B1) and M35 (from M10A1) artillery prime movers; M7B1, M12, M40, and M43 self-propelled artillery; and the M10 and M36 tank destroyers.\n\nThe M4 Sherman served with the U.S. Army and Marine Corps during World War II. The U.S. also supplied large numbers to the various Allied countries. Shermans were used during the war by British and Commonwealth armies, the Soviet Union's Red Army, Free French forces, the Polish army in exile, China's National Revolutionary Army, and Brazil's Expeditionary Force.\n\nThe U.S. Marine Corps used the diesel M4A2 and gasoline-powered M4A3 in the Pacific. However, the Chief of the Army's Armored Force, Lt. Gen. Jacob L. Devers, ordered that no diesel-engined Sherman tanks be used by the Army outside the Zone of Interior (the continental U.S.). The U.S. Army used all types for either training or testing within the United States, but intended the M4A2 and M4A4 to be the primary Lend-Lease exports.\n\nThe M4A1 Sherman first saw combat at the Second Battle of El Alamein in October 1942 with the British 8th Army. The first U.S. Shermans in battle were M4A1s in Operation \"Torch\" the next month. At this time, Shermans successfully engaged German Panzer IIIs with long barreled 50mm L60 guns, and Panzer IVs with short barreled 75 mm L24 guns. Additional M4 and M4A1s replaced M3 Lees in U.S. tank battalions over the course of the North African campaigns. However, by June 1944, most German tanks were up-gunned and 75 mm Shermans were outgunned. The M4 and M4A1 were the main types in U.S. units until late 1944, when the Army began replacing them with the preferred M4A3 with its more powerful engine. Some M4s and M4A1s continued in U.S. service for the rest of the war.\n\nThe first Sherman to enter combat with the 76 mm gun (July 1944) was the M4A1, closely followed by the M4A3. By the end of the war, half the U.S. Army Shermans in Europe had the 76 mm gun. The first HVSS Sherman to see combat was the M4A3E8(76)W in December 1944. The M4A3E8 (76)W was arguably the best of the US Sherman tanks.\n\n\n\n\n\n\nThe Encyclopaedia of Tanks and Armoured Fighting Vehicles; Published in 2007 by Amber Books Ltd.\n"}
{"id": "28503451", "url": "https://en.wikipedia.org/wiki?curid=28503451", "title": "Transnational organization", "text": "Transnational organization\n\nTransnational organization is a term used in scholarly literature. It refers to international organizations (usually, international nongovernmental organizations) that \"transcend\" the idea of a nation-state.\n\nThe distinction between an international and a transnational organization is unclear and has been criticized by some scholars (ex. Colás, 2002).\n\nTransnational relations have been defined as “contacts, coalitions, and interactions across state boundaries that are not controlled by the central foreign policy organs of governments.” Examples of transnational entities are “multinational business enterprises and revolutionary movements; trade unions and scientific networks; international air transport cartels and communications activities in outer space.” Transnational social movements are “the broad tendencies that often manifest themselves in particular International Non-Governmental Organizations (INGOs).” Examples of transnational social movements include human rights, women’s, peace, labor, green, or student movements manifested in Amnesty International, the Peace Brigades International, the International Confederation of Free Trade Unions, etc. A further definition: “An organization is \"transnational\" rather than \"national\" if it carries on significant centrally-directed operations in the territory of two or more nation-states. Similarly, an organization will be called \"international\" rather than \"national\" only if the control of the organization is explicitly shared among representatives of two or more nationalities. And an organization is \"multinational\" rather than \"national\" only if people from two or more nationalities participate significantly in its operation.” “Transnational organizations are designed to facilitate the pursuit of a single interest within many national units.” \n\n\n"}
{"id": "1398195", "url": "https://en.wikipedia.org/wiki?curid=1398195", "title": "Tsunami Aid", "text": "Tsunami Aid\n\nTsunami Aid: A Concert of Hope was a worldwide benefit held for the tsunami victims of the 2004 Indian Ocean earthquake. It was broadcast on NBC and its affiliated networks of USA Network, Bravo, PAX, MSNBC, CNBC, Sci-Fi, Trio, Telemundo and other NBC Universal stations and was heard on any Clear Channel radio station. The benefit was led by the actor George Clooney on January 15, 2005, and was similar to \"\" (set up after the September 11th, 2001 attacks). Digital Media innovator Jay Samit enabled viewers to purchase digital downloads of the performances as a new way to raise money for the cause; including live recordings by Elton John, Madonna, Sheryl Crow, Eric Clapton and Roger Waters. Taking a cue from Bob Geldof (the man who had organized the Live Aid concerts for African famine relief), it consisted of famous Hollywood entertainers and former American presidents George H.W. Bush and Bill Clinton. It was two hours long with stories and entertainment from a huge array of Hollywood popstars notables that include Brad Pitt, Donald Trump, and much more. It was estimated to raise at least five million dollars by the end of the broadcast.\n"}
{"id": "8890266", "url": "https://en.wikipedia.org/wiki?curid=8890266", "title": "Tube Challenge", "text": "Tube Challenge\n\nThe Tube Challenge is the competition for the fastest time to travel to all London Underground stations, tracked as a Guinness World Record since 1960. The goal is to visit all the stations on the system, not necessarily all the lines; participants may connect between stations on foot, or by using other forms of public transport.\n\n, the record for fastest completion was held by Andi James (Finland) and Steve Wilson (UK), who completed the challenge in 15 hours, 45 minutes and 38 seconds on 21 May 2015.\n\nThe first recorded challenge took place in 1959. Although many people have attempted the challenge and held the record since, they have not always been credited in the record books. In the earlier days of the challenge, participants were permitted to use private forms of transport (such as a car or bike) to move between stations. This led to times of less than 16 hours in some earlier records, and Guinness later changed the rules to ban private transport.\n\nThe following is a list of record holders that have appeared in the printed edition of the \"Guinness Book of Records\". The record did not appear in the book until its eighth edition.\nBetween the 1960s and 1990s the record regularly appeared in the \"Guinness Book of Records\", initially listed under \"Underground Railways – circuit of\", but later just under \"Railways\" and then \"Trains\". Since the change of publishing style of the book from the 2001 edition onwards, the record – although frequently broken – has only once appeared in printed form, in the 2008 edition. More recent records have tended to be published online instead. Since the record has not regularly been published in the book, there have been two broad configurations on the system – one for 275 stations, and one for 270 once the East London Line was no longer part of the network.\n\nOn 3 April 2002 Jack Welsby set a new record time for 275 stations by traversing the system in 19 hours, 18 minutes and 45 seconds. Welsby made just one attempt, starting his route at Heathrow and finishing at Amersham.\n\nThis time was beaten on 4 May 2004 by Geoff Marshall and Neil Blake who achieved a new record time of 18 hours 35 minutes and 43 seconds. Their attempt began on the first train out of Amersham on the Metropolitan Line and ended at Upminster, and it took Guinness World Records four months to ratify it. A previous attempt had been broadcast on TV as part of \"The Tube\" TV series and another attempt had been televised as part of an ITV1 programme \"Metroland: Race Around the Underground\" on 16 October 2003.\n\nAlthough this time stood for two years before being beaten by just five seconds, it was not until Håkan Wolgé and Lars Andersson (both from Sweden) set a new record time for 275 stations that it appeared in the \"Guinness World Records\" Book again, in the 2008 edition. They set a new record of 18 hours, 25 minutes and 3 seconds, on 26 September 2006.\n\nChanges to the total number of stations meant that the record was 'reset' and broken three more times over a two-year period until Wood Lane station opened in October 2008, and the network settled at 270 stations.\n\nThe first holders of the 270-station record were Andi James, Martin Hazel and Steve Wilson who, on 14 December 2009, achieved a record time of 16 hours, 44 minutes and 16 seconds. TfL used this route four years later as part of the Art on the Underground labyrinth project to mark the 150th anniversary of the London Underground, installing permanent designs at stations in the same order that the world record route had taken, and later appeared in an \"Information Capital\" article.\n\nThe record remained unbeaten for 17 months, until Marc Gawley from Denton, Greater Manchester, set a new time of 16 hours, 29 minutes and 57 seconds on 21 April 2011. As a fast marathon runner, he revealed that he did not use any buses on the day, preferring instead to make all his connections on foot. Gawley's record was beaten 37 days later, when James and Wilson completed the challenge in just 44 seconds under Gawley's time, setting a new record of 16 hours, 29 minutes and 13 seconds on 27 May 2011.\n\nThis record stood for over two years until August 2013, before being broken by previous record holder Geoff Marshall who along with Anthony Smith, completed the challenge in 16 hours, 20 minutes and 27 seconds, the record time was then published for the first time in seven years in the \"Guinness World Records\" in the 2015 edition.\n\nClive Burgess and Ronan McDonald set a new Guinness world record time of 16 hours, 14 minutes and 10 seconds on 21 February 2015. The record was broken later that year, on 21 May, by previous record holders Andi James and Steve Wilson, in a time of 15 hours 45 minutes 38 seconds.\n\nAttempts to travel the network have been linked to charities such as Children in Need and Comic Relief. A charity attempt known as \"Tube Relief\" was organised, following the 7 July 2005 London bombings, to raise money for the London Bombings Relief Charitable Fund. Fifty-one people rode the entire tube network for the day, raising over £10,000 towards the official charity fund. A Sue Ryder charity event took place in November 2011, when ten teams competed against each other to have their photo taken outside as many of the 270 stations as possible. Former record holder Geoff Marshall subsequently organised a mass-participant event in 2014, called \"Walk The Tube\", which has become an annual event, raising tens of thousands of pounds in the process.\n\n\n"}
{"id": "3157429", "url": "https://en.wikipedia.org/wiki?curid=3157429", "title": "WTA Rankings", "text": "WTA Rankings\n\nThe WTA Rankings are the ratings defined by the Women's Tennis Association, introduced in November 1975.\n\nThe WTA rankings are based on a rolling 52-week, cumulative system. A player's ranking is determined by her results at a maximum of 16 tournaments for singles and 11 for doubles and points are awarded based on how far a player advances in a tournament. The basis for calculating a player's ranking are those tournaments that yield the highest ranking points during the rolling 52-week period with the condition that they must include points from the 4 Grand Slams, the 4 Premier Mandatory tournaments and the WTA Finals. In addition, for Top 20 players, their best two results at Premier 5 tournaments will also count. Up until 2016, the WTA also distributed ranking points, for singles players only, who competed at the Summer Olympics. However, this has since been discontinued.\n\nThe points distribution for tournaments in 2017 is shown below. Points earned in 2013 were a little different in some cases and retained their value until they expired after 52 weeks.\n\n\"+H\" indicates that Hospitality is provided.\n\nIn ITF tournaments, the main draw is normally 32 for singles and 16 for doubles. Losers in the first round of doubles will receive points equal to that shown in the R32 column above. For subsequent rounds (quarter-finals onwards) the points are the same as for singles.\n\nThe following is a chronological list of players who have achieved the number one position in singles since the WTA began producing computerized rankings on November 3, 1975 (active players in green):\n\n\"Last update: 19 November 2018\"\n\nThe year-end number one player is the player at the head of the WTA rankings following the completion of the final tournament of the calendar year.\n\nThe following is a list of singles players who were ranked world No. 5 or higher but not No. 1 (active players in green):\n\n\n"}
{"id": "1179525", "url": "https://en.wikipedia.org/wiki?curid=1179525", "title": "Willis Conover", "text": "Willis Conover\n\nWillis Clark Conover, Jr. (December 18, 1920 – May 17, 1996) was a jazz producer and broadcaster on the Voice of America for over forty years. He produced jazz concerts at the White House, the Newport Jazz Festival, and for movies and television. By arranging concerts where people of all races were welcome, he is credited with desegregating Washington D.C. nightclubs. Conover is credited with keeping interest in jazz alive in the countries of Eastern Europe through his nightly broadcasts during the Cold War.\n\nAs a young man Conover was interested in science fiction, and published a science fiction fanzine, \"Science Fantasy Correspondent\". This brought him into contact with horror writer H. P. Lovecraft. The correspondence between Lovecraft, who was at the end of his life, and the young Conover, has been published as \"Lovecraft at Last\" (Carrolton-Clark, 1975; reprint 2004).\n\nConover's father had intended for him to attend The Citadel and follow his family's tradition of military service. Instead, he attended the Maryland State Teacher's College at Salisbury, Maryland, and became a radio announcer for WTBO in Cumberland, Maryland.\n\nHe later moved to Washington, D.C., and focused on jazz in his programming, especially the Duke Ellington hour on Saturday nights. His guests on this program and Saturday morning shows included many important artists, such as Boyd Raeburn.\n\nConover came to work at the Voice of America, and eventually became a legend among jazz lovers, primarily due to the hour-long program on the Voice of America called \"Voice of America Jazz Hour\". Known for his sonorous baritone voice, many would argue that he was the most important presenter on Voice of America. His slow delivery and the use of scripts written in \"special English\" made his programmes more widely accessible and he is said to have become the first teacher of English to a whole generation of East European jazz lovers. Conover was not well known in the United States, even among jazz aficionados, as the Voice of America did not broadcast domestically except on shortwave, but his visits to Eastern Europe and Soviet Union brought huge crowds and star treatment for him.\nHe was a celebrity figure in the Soviet Union, where jazz was very popular and the Voice of America was a prime source of information as well as music.\n\nIn 1956, Conover conducted a series of interviews with jazz luminaries like Duke Ellington, Billie Holiday, Stan Getz, Peggy Lee, Stan Kenton, Benny Goodman, and Art Tatum. His interview with Tatum is noted as \"the only known in-depth recorded interview with the pianist\". These interviews were selected by the Library of Congress as a 2010 addition to the National Recording Registry, which selects recordings annually that are \"culturally, historically, or aesthetically significant\".\n\nHe died of lung cancer. He had been a smoker for 57 years.\n\nIn 1990, Conover was awarded an Honorary Doctorate of Music from Berklee College of Music. \n\nIn 2015, the University of North Texas announced its Willis Conover Collection would make digitized copies of Conover's programs available online.\n\n"}
{"id": "39804273", "url": "https://en.wikipedia.org/wiki?curid=39804273", "title": "World Archery Rankings", "text": "World Archery Rankings\n\nThe World Archery Rankings is a ranking system developed by the World Archery Federation for international competitive archery. It is calculated using a points system and published following major World Archery tournaments.\n\nThe ranking system was first developed in 2001, and calculation method updated in 2010. At present, rankings are maintained for:\n\nFrom 2006, rankings were calculated for Field Archery, but since 2012 are no longer maintained.\n\nEach archer or team earns a ranking score for each competition. The ranking scores are calculated through a combination of the ranking factor of the tournament (\"Ranking Factor\") and points based on the competitor's final position in the competition (\"Ranking Score\"). The archer's four highest ranking scores are then combined to form their total score (\"Added Ranking Score\"), which forms the basis of the ranking list.\n\nOnly results achieved at official ranking events can count towards the overall rankings. These events include the major World Archery Federation tournaments (Olympics, World Archery Championships, World Cup), and other international, regional and national events that have applied and been approved for ranking status.\n\nThe tournament ranking factor is calculated on the basis of three sub-factors, Quality, Quantity and Period, which are multiplied together to produce the overall tournament factor. The factor can vary between disciplines at the same event (e.g., the recurve and compound disciplines at the World Championships may have different Ranking Factors).\n\nFor instance, the Ranking Factor of a tournament for individuals with 60 archers, of whom 10 are in the top 50, that took place 18 months ago, would be:\n\n2.5 \"(\"Quality\": 9-12 top 50 archers)\" x\n\n0.9 \"(\"Quantity\": 33-64 archers)\" x\n\n0.5 \"(\"Period\": other tournament 12-24 months ago)\" =\n\n1.125\n\nThe Quality sub-factor takes into account the level of competition at the tournament. A score is assigned based on the prestige of the event, for the major World Archery Federation tournaments, and the number of top archers or teams at other competitions (defined as individuals ranked in the top 50 or teams ranked in the top 16). The quality factor points are assigned as follows:\n\nIndividual\n\nTeam\n\nThe Quantity sub-factor assigns a score based on the number of archers competing as follows:\n\nIndividual\nTeam\n\nThe Period sub-factor is dependent on when the competition took place, allowing the Ranking Factor to emphasise the most recent tournaments as follows:\n\nRanking Points are awarded based on the team or competitor's final position at the competition. This is then multiplied by the Ranking Factor to produce the overall Ranking Score for that team or competitor at that tournament. For instance, an archer who finished 4th in the competition used in the example above would receive the following Ranking Score:\n\n1.125 \"(\"Ranking Factor\")\" x\n\n12 \"(\"Ranking Points\": 4th place)\" =\n\n13.5\n\nA team or competitor's best four Ranking Scores are then combined to produce the Added Ranking Score, which forms the basis of the rankings list. No more than two Ranking Scores whose period factor is lower than 1 may be used to calculate the Added Ranking Score.\n\nRanking Points are awarded as follows:\n\nIndividual\nTeam\n\n\nRankings at 21 NOV 2018\n\nRankings at 18 July 2016\n\nRankings at 18 July 2016\n\nRankings at 18 July 2016\n\nRankings at 18 July 2016\n\nRankings at 18 July 2016\n\nRankings at 18 July 2016\n\nRankings at 18 July 2016\n\nRankings at 18 July 2016\n\nRankings at 18 July 2016\n\nThe following archers have achieved the number one individual position since the inception of the rankings in 2001:\n\nMen\n\nWomen\n\nMen\n\nWomen\n"}
{"id": "1838622", "url": "https://en.wikipedia.org/wiki?curid=1838622", "title": "World Environment Day", "text": "World Environment Day\n\nWorld Environment Day (WED) is celebrated on the 5th of June every year, and is the United Nation's principal vehicle for encouraging awareness and action for the protection of our environment. First held in 1974, it has been a flagship campaign for raising awareness on emerging environmental issues from marine pollution, human overpopulation, and global warming, to sustainable consumption and wildlife crime. WED has grown to become a global platform for public outreach, with participation from over 143 countries annually. Each year, WED has a new theme that major corporations, NGOs, communities, governments and celebrities worldwide adopt to advocate environmental causes.\n\nWorld Environment Day [WED] was established by the UN General Assembly in 1972 on the first day of the Stockholm Conference on the Human Environment, resulting from discussions on the integration of human interactions and the environment. Two years later, in 1974 the first WED was held with the theme \"Only One Earth\". Even though WED celebration have been held annually since 1974, in 1987 the idea for rotating the center of these activities through selecting different host countries began.\n\nFor almost five decades, World Environment Day has been raising awareness, supporting action, and driving change. Here is a timeline of key accomplishments in WEDs’ history:\nThe theme for 2018 is \"Beat Plastic Pollution\". The host nation is India. By choosing this Theme, it is aimed that people may strive to change their everyday lives to reduce the heavy burden of plastic pollution. People should be free from the over-reliance on single-use or disposables, as they have severe environmental consequences. We should liberate our natural places, our wildlife - and our own health from plastics. Indian government pledged to eliminate all single use of plastic in India by 2022.\n\nThe theme for 2017 was 'Connecting People to Nature – in the city and on the land, from the poles to the equator'. The host nation was Canada.\n\nThe 2016 WED was organized under the theme \"Go wild for life\". This edition of the WED aims to reduce and prevent the illegal trade in wildlife. Angola was chosen as the host country of the 2016 WED during the COP21 in Paris.\n\nThe Slogan of the 2015 edition of the World Environment Day is \"Seven Billion Dreams. One Planet. Consume with Care\". The slogan was picked through a voting process on social media. In Saudi Arabia, 15 women recycled 2000 plastic bags to crochet a mural in support of the WED 2015. In India, Narendra Modi planted a Kadamb sapling to celebrate the World Environment Day and raise awareness for Environment. Italy is the host country of the 43rd edition of the WED. The celebrations took place as part of Milan Expo around the theme: Feeding the Planet - Energy for Life. \n\nThe Theme of the 2014 WED is : International Year of Small Islands Developing States (SIDS). By choosing this Theme the UN General Assembly aimed to highlight the development Challenges and successes of the SIDS. In 2014, the World Environment Day focused on global warming and its impact on ocean levels. The Slogan of the WED 2014 is \"Raise your voice not the sea level\", as Barbados hosted the global celebrations of the 42nd edition of the World Environment Day. The UN Environement Programme named actor Ian Somerhalder as the official Goodwill ambassador of the WED 2014.\n\nThe 2013 theme for World Environment Day was \"Think.Eat.Save.\"\n\nThe campaign addressed the huge annual wastage and losses in food, which, if conserved, would release a large quantity of food as well as reduce the overall carbon footprint. The campaign aimed to bring about awareness in countries with lifestyles resulting in food wastage. It also aimed to empower people to make informed choices about the food they eat so as to reduce the overall ecological impact due to the worldwide production of food..The host country for the year's celebrations was Mongolia.\n\nThe theme for the 2012 World Environment Day was \"Green Economy: Does it include you?\"\n\nThe theme aimed to invite people to examine their activities and lifestyle and see how the concept of a \"Green Economy\" fits into it. The host country for the year's celebrations was Brazil.\n\nThe theme for 2011 was Forests-Nature At Your Service. Thousands of activities were organized worldwide, with beach clean-ups, concerts, exhibits, film festivals, community events and much more. That year's global host, India, is a country of wide biodiversity.\n\n'Many Species. One Planet. One Future', was the theme of 2010.\n\nIt celebrated the diversity of life on Earth as part of the 2010 International Year of Biodiversity. It was hosted in Rwanda. Thousands of activities were organized worldwide, with beach clean-ups, concerts, exhibits, film festivals, community events and much more. Each continent (except Antarctica) had a \"regional host city\", the U.N. chose Pittsburgh, Pennsylvania as the host for all North.\n\nThe theme for WED 2009 was 'Your Planet Needs You – UNite to Combat Climate Change', and Michael Jackson's 'Earth Song' was declared 'World Environment Day Song'. It was hosted in Mexico.\n\nThe host for World Environment Day 2008 was New Zealand, with the main international celebrations scheduled for Wellington. The slogan for 2008 was \"CO, Kick the Habit! Towards a Low Carbon Economy.\" New Zealand was one of the first countries to pledge to achieve carbon neutrality, and will also focus on forest management as a tool for reducing greenhouse gases. \n\nThe Chicago Botanic Garden served as the North American host for World Environment Day on 5 June 2008.\n\nThe topic for World Environment Day for 2007 was \"Melting Ice – a Hot Topic?\" During International Polar Year, WED 2007 focused on the effects that climate change is having on polar ecosystems and communities, on other ice- and snow-covered areas of the world, and the resulting global impacts.\n\nThe main international celebrations of the WED 2007 were held in the city of Tromsø, Norway, a city north of the Arctic Circle.\n\nThe topic for WED 2006 was Deserts and Desertification and the slogan was \"Don't desert drylands\".\n\nThe slogan emphasised the importance of protecting drylands. The main international celebrations of the World Environment Day 2006 were held in Algeria.\n\nThe theme for the 2005 World Environment Day was \"Green Cities\" and the slogan was \"Plan for the Planet!\".\n\nWorld Environment Day celebrations have been (and will be) hosted in the following cities:\nAn Earth Anthem penned by poet Abhay K is sung to celebrate World Environment Day.\n\n<poem>\nOur cosmic oasis, cosmic blue pearl\nthe most beautiful planet in the universe\nall the continents and the oceans \nunited we stand as flora and fauna\nunited we stand as species of one earth\ndifferent cultures, beliefs and ways\nwe are humans, the earth is our home\nall the people and the nations of the world\nall for one and one for all\nunited we unfurl the blue flag.\n</poem>\n\nIt was launched in June 2013 on the occasion of the World Environment Day by Kapil Sibal and Shashi Tharoor, then Union Ministers of India, at a function organized by the Indian Council of Cultural Relations in New Delhi. It is supported by the global organization Habitat For Humanity.\n\n\n"}
{"id": "42758135", "url": "https://en.wikipedia.org/wiki?curid=42758135", "title": "World Health Academy", "text": "World Health Academy\n\nThe World Health Academy (WHA) is an international non-governmental organization specializing in global health. It serves as a representative body for physicians worldwide, developing health policy, advancing continuing education programs, and speaking as a unified voice for medicine in international advocacy. The WHA is headquartered in Florence, Italy and led by Robert A. Schwartz, of Rutgers University.\n\nThe WHA seeks to achieve optimal health for all people across borders, uniting the medical field to harness the collective expertise and power of its members. As the medical profession’s global and authoritative voice, the WHA assists governments and national medical associations in jointly tackling health problems and improving the welfare of their citizens.\n\nThe \"International Journal of Medicine\" is an open access peer-reviewed scientific journal published by the World Health Academy. It covers primary research and secondary research from any discipline within medicine. The \"IJM\" Editorial Board consist of Nobel Prize laureates, Lasker Award recipients, and other prominent persons. \"Dermatologic Therapy\", a peer-reviewed scientific journal published by Wiley-Blackwell, is also an official journal of the WHA.\n\n\n"}
{"id": "23486603", "url": "https://en.wikipedia.org/wiki?curid=23486603", "title": "World Health Imaging, Telemedicine, and Informatics Alliance", "text": "World Health Imaging, Telemedicine, and Informatics Alliance\n\nThe World Health Imaging, Telemedicine and Informatics Alliance (WHITIA) is a non-profit global health technology and social venture established in 2006 by affiliates of Northwestern University near Chicago, Illinois. WHITIA cultivates high-level strategic relationships with non-governmental organizations, imaging industry innovators and academic institutions in order to integrate and deliver meaningful, sustainable, diagnostic technology to underserved communities worldwide. WHITIA's vision is to facilitate the deployment of thousands of digital medical imaging systems worldwide, providing one billion people with access to diagnostic imaging. WHITIA was formerly known as the World Health Imaging Alliance (WHIA) until it formally expanded its scope in June 2009.\n\nWHITIA's first formal public launch was in April 2009 at the Healthcare Information and Management Systems Society (HIMSS) Annual Conference & Exhibition in Chicago, Illinois. WHITIA announced strategic partners including SEDECAL, Carestream Health and Merge Healthcare, receiving extensive coverage in Health IT magazines and publications. At the 2009 Annual Conference of the Society for Imaging Informatics in Medicine (SIIM) in Charlotte, North Carolina, WHITIA announced its partnership with SIIM, which will allow both organizations to collaborate on specific initiatives. WHITIA was recently ranked #15 of the top 25 most influential people, institutions, and organizations in the radiology industry.\n\nAt the 2009 RSNA Annual Meeting, WHITIA launched, Remi-d, a remote-operated screening X-ray system for use in the developing world. Its strengths in these areas stem from the higher burden of Human Immunodeficiency Virus (HIV) and Tuberculosis (TB) co-infection, high incidences of Black Lung disease, or outbreaks of other infectious respiratory diseases. The teleradiology and remote-controlled features of Remi-d allow resource-limited areas such as sub-Saharan Africa, South and Central America and Southeast Asia, where Radiologists and Radiographers are in short supply to have a functioning X-ray service.\n\nWHITIA currently has pilot integrated digital X-ray sites in South Africa and Guatemala at established clinics in need and is expanding to new qualified sites in partnership with NGOs such as Rotary International while cooperating with the local and national governments.\n\nThe Guatemala pilot sites in urban Guatemala City and rural Río Hondo provide essential healthcare technology to thousands of people in the communities served. They are designed to be models for the wider expansion of the WHITIA network throughout the clinics in need in urban and rural Guatemala. The system's specific design for Guatemala City is an integration of some of WHITIA's partners' strengths and generosity:\n\n\nThis project was largely funded by several US and Guatemalan Rotary clubs along with the key resource support of the Guatemalan municipal and national governments.\n\n"}
{"id": "44940", "url": "https://en.wikipedia.org/wiki?curid=44940", "title": "World Heritage site", "text": "World Heritage site\n\nA World Heritage site is a landmark or area which is selected by the United Nations Educational, Scientific and Cultural Organization (UNESCO) as having cultural, historical, scientific or other form of significance, and is legally protected by international treaties. The sites are judged important to the collective interests of humanity.\n\nTo be selected, a World Heritage site must be an already classified landmark, unique in some respect as a geographically and historically identifiable place having special cultural or physical significance (such as an ancient ruin or historical structure, building, city, complex, desert, forest, island, lake, monument, mountain, or wilderness area). It may signify a remarkable accomplishment of humanity, and serve as evidence of our intellectual history on the planet.\n\nThe sites are intended for practical conservation for posterity, which otherwise would be subject to risk from human or animal trespassing, unmonitored/uncontrolled/unrestricted access, or threat from local administrative negligence. Sites are demarcated by UNESCO as protected zones. The list is maintained by the international World Heritage Program administered by the UNESCO World Heritage Committee, composed of 21 states parties which are elected by their General Assembly.\n\nThe programme catalogues, names, and conserves sites of outstanding cultural or natural importance to the common culture and heritage of humanity. Under certain conditions, listed sites can obtain funds from the World Heritage Fund. The program began with the \"Convention Concerning the Protection of the World's Cultural and Natural Heritage\", which was adopted by the General Conference of UNESCO on 16 November 1972. Since then, 193 state parties have ratified the convention, making it one of the most widely recognized international agreements and the world's most popular cultural program.\n\nAs of July 2018, a total of 1,092 World Heritage sites (845 cultural, 209 natural, and 38 mixed properties) exist across 167 countries. Italy, with 54 sites, has the most of any country, followed by China (53), Spain (47), France (44), Germany (44), India (37), and Mexico (35).\n\nIn 1954, the government of Egypt decided to build the new Aswan High Dam, whose resulting future reservoir would eventually inundate a large stretch of the Nile valley containing cultural treasures of ancient Egypt and ancient Nubia. In 1959, the governments of Egypt and Sudan requested UNESCO to assist their countries to protect and rescue the endangered monuments and sites. In 1960, the Director-General of UNESCO launched an appeal to the member states for an International Campaign to Save the Monuments of Nubia. This appeal resulted in the excavation and recording of hundreds of sites, the recovery of thousands of objects, as well as the salvage and relocation to higher ground of a number of important temples, the most famous of which are the temple complexes of Abu Simbel and Philae. The campaign, which ended in 1980, was considered a success. As tokens of its gratitude to countries which especially contributed to the campaign's success, Egypt donated four temples: the Temple of Dendur was moved to the Metropolitan Museum of Art in New York City, the Temple of Debod was moved to the Parque del Oeste in Madrid, the Temple of Taffeh was moved to the Rijksmuseum van Oudheden in the Netherlands, and the Temple of Ellesyia to Museo Egizio in Turin.\n\nThe project cost $80 million, about $40 million of which was collected from 50 countries. The project's success led to other safeguarding campaigns: saving Venice and its lagoon in Italy, the ruins of Mohenjo-daro in Pakistan, and the Borobodur Temple Compounds in Indonesia. UNESCO then initiated, with the International Council on Monuments and Sites, a draft convention to protect the common cultural heritage of humanity. \n\nThe United States initiated the idea of cultural conservation with nature conservation. The White House conference in 1965 called for a \"World Heritage Trust\" to preserve \"the world's superb natural and scenic areas and historic sites for the present and the future of the entire world citizenry\". The International Union for Conservation of Nature developed similar proposals in 1968, and they were presented in 1972 to the United Nations Conference on the Human Environment in Stockholm. Under the World Heritage Committee, signatory countries are required to produce and submit periodic data reporting providing the World Heritage Committee with an overview of each participating nation's implementation of the World Heritage Convention and a \"snapshot\" of current conditions at World Heritage properties.\n\nA single text was agreed on by all parties, and the \"Convention Concerning the Protection of the World Cultural and Natural Heritage\" was adopted by the General Conference of UNESCO on 16 November 1972.\n\nThe Convention came into force on 17 December 1975. As of May 2017, it has been ratified by 193 states parties, including 189 UN member states plus the Cook Islands, the Holy See, Niue, and the State of Palestine. Only four UN member states have not ratified the Convention: Liechtenstein, Nauru, Somalia and Tuvalu.\n\nA country must first list its significant cultural and natural sites; the result is called the Tentative List. A country may not nominate sites that have not been first included on the Tentative List. Next, it can place sites selected from that list into a Nomination File.\n\nThe Nomination File is evaluated by the International Council on Monuments and Sites and the World Conservation Union. These bodies then make their recommendations to the World Heritage Committee. The Committee meets once per year to determine whether or not to inscribe each nominated property on the World Heritage List and sometimes defers or refers the decision to request more information from the country which nominated the site. There are ten selection criteria – a site must meet at least one of them to be included on the list.\n\nUp to 2004, there were six criteria for cultural heritage and four criteria for natural heritage. In 2005, this was modified so that there is now only one set of ten criteria. Nominated sites must be of \"outstanding universal value\" and meet at least one of the ten criteria. These criteria have been modified or/amended several times since their creation.\n\nUNESCO designation as a World Heritage site provides \"prima facie\" evidence that such culturally sensitive sites are legally protected pursuant to the Law of War, under the Geneva Convention, its articles, protocols and customs, together with other treaties including the Hague Convention for the Protection of Cultural Property in the Event of Armed Conflict and international law.\n\nThus, the Geneva Convention treaty promulgates:\n\n\"Article 53. PROTECTION OF CULTURAL OBJECTS AND OF PLACES OF WORSHIP. Without prejudice to the provisions of the Hague Convention for the Protection of Cultural Property in the Event of Armed Conflict of 14 May 1954,' and of other relevant international instruments, it is prohibited:\n\nA country may request to extend or reduce the boundaries, modify the official name, or change the selection criteria of one of its already listed sites. Any proposal for a significant boundary change or modify the site's selection criteria must be submitted as if it were a new nomination, including first placing it on the Tentative List and then onto the Nomination File.\n\nA request for a minor boundary change, one that does not have a significantly impact on the extent of the property or affect its \"outstanding universal value\", is also evaluated by the advisory bodies before being sent to the Committee. Such proposals can be rejected by either the advisory bodies or the Committee if they judge it to be a significant change instead of a minor one.\n\nProposals to change the site's official name is sent directly to the Committee.\n\nA site may be added to the List of World Heritage in Danger if there are conditions that threaten the characteristics for which the landmark or area was inscribed on the World Heritage List. Such problems may involve armed conflict and war, natural disasters, pollution, poaching, or uncontrolled urbanization or human development. This danger list is intended to increase international awareness of the threats and to encourage counteractive measures. Threats to a site can be either proven imminent threats or potential dangers that could have adverse effects on a site.\n\nThe state of conservation for each site on the danger list is reviewed on a yearly basis, after which the committee may request additional measures, delete the property from the list if the threats have ceased or consider deletion from both the List of World Heritage in Danger and the World Heritage List.\n\nOnly two sites have ever been delisted: the Arabian Oryx Sanctuary in Oman and the Dresden Elbe Valley in Germany. The Arabian Oryx Sanctuary was directly delisted in 2007, instead of first being put on the danger list, after the Omani government decided to reduce the protected area's size by 90 percent. The Dresden Elbe Valley was first placed on the danger list in 2006 when the World Heritage Committee decided that plans to construct the Waldschlösschen Bridge would significantly alter the valley's landscape. In response, the Dresden City Council attempted to stop the bridge's construction, but after several court decisions allowed the building of the bridge to proceed, the valley was removed from the World Heritage List in 2009.\n\nThe first global assessment to quantitatively measure threats to Natural World Heritage sites found that 63 percent of sites have been damaged by increasing human pressures including encroaching roads, agriculture infrastructure and settlements over the last two decades. These activities endanger Natural World Heritage sites and could compromise their unique values. Of the Natural World Heritage sites that contain forest, 91 percent of those experienced some loss since the year 2000. Many Natural World Heritage sites are more threatened than previously thought and require immediate conservation action.\n\nThere are 1092 World Heritage sites located in 167 states. Of these, 845 are cultural, 209 are natural and 38 are mixed properties. The World Heritage Committee has divided the world into five geographic zones which it calls regions: Africa, Arab states, Asia and the Pacific, Europe and North America, and Latin America and the Caribbean.\n\nRussia and the Caucasus states are classified as European, while Mexico and the Caribbean are classified as belonging to the Latin America & Caribbean zone, despite their location in North America. The UNESCO geographic zones also give greater emphasis on administrative, rather than geographic associations. Hence, Gough Island, located in the South Atlantic, is part of the Europe & North America region because the government of the United Kingdom nominated the site.\n\nThe table below includes a breakdown of the sites according to these zones and their classification:\n<nowiki>*</nowiki>The properties \"Uvs Nuur Basin\" and \"Landscapes of Dauria\" (Mongolia, Russian Federation) are trans-regional properties located in Europe and Asia and the Pacific region. They are counted here in the Asia and the Pacific region.\n\n<nowiki>*</nowiki>The property \"The Architectural Work of Le Corbusier, an Outstanding Contribution to the Modern Movement\" (Argentina, Belgium, France, Germany, India, Japan, Switzerland) is a trans-regional property with component sites located in three regions - Europe and North America, Asia and the Pacific, and Latin America and the Caribbean. It is counted here in Europe and North America.\n\nCountries with fifteen or more World Heritage sites, as of July 2018.\nDespite the successes of World Heritage listing in promoting conservation, the UNESCO administered project has attracted criticism from some for perceived under-representation of heritage sites outside Europe, disputed decisions on site selection and adverse impact of mass tourism on sites unable to manage rapid growth in visitor numbers.\n\nA sizable lobbying industry has grown around the awards because World Heritage listing has the potential to significantly increase tourism revenue from sites selected. Site listing bids are often lengthy and costly, putting poorer countries at a disadvantage. Eritrea's efforts to promote Asmara are one example.\n\nIn 2016, the Australian government was reported to have successfully lobbied for Great Barrier Reef conservation efforts to be removed from a UNESCO report titled 'World Heritage and Tourism in a Changing Climate'. The Australian government's actions were in response to their concern about the negative impact that an 'at risk' label could have on tourism revenue at a previously designated UNESCO World Heritage site.\n\nA number of listed World Heritage locations such as George Town, Penang, and Casco Viejo, Panama, have struggled to strike the balance between the economic benefits of catering to greatly increased visitor numbers and preserving the original culture and local communities that drew the recognition.\n\n\n"}
{"id": "26512994", "url": "https://en.wikipedia.org/wiki?curid=26512994", "title": "World Trade Report", "text": "World Trade Report\n\nThe World Trade Report (WTR) is the annual report published since 2003 by the World Trade Organization. Each WTR provides an in-depth analysis of an aspect of trends in international trade, trade policy issues and the multilateral trading system.\n\nThe 2016 World Trade Report examines the participation of Small and medium-sized enterprises (SMEs) in international trade, how the international trade landscape is changing for SMEs, and what the multilateral trading system does and can do to encourage more widespread and inclusive SME participation in global markets.\n\n\n"}
{"id": "771916", "url": "https://en.wikipedia.org/wiki?curid=771916", "title": "World map", "text": "World map\n\nA world map is a map of most or all of the surface of the Earth. World maps form a distinctive category of maps due to the problem of projection. Maps by necessity distort the presentation of the earth's surface. These distortions reach extremes in a world map. The many ways of projecting the earth reflect diverse technical and aesthetic goals for world maps.\n\nWorld maps are also distinct for the global knowledge required to construct them. A meaningful map of the world could not be constructed before the European Renaissance because less than half of the earth's coastlines, let alone its interior regions, were known to any culture. New knowledge of the Earth's surface has been accumulating ever since and continues to this day.\n\nMaps of the world generally focus either on political features or on physical features. Political maps emphasize territorial boundaries and human settlement. Physical maps show geographic features such as mountains, soil type or land use. Geological maps show not only the surface, but characteristics of the underlying rock, fault lines, and subsurface structures. Choropleth maps use color hue and intensity to contrast differences between regions, such as demographic or economic statistics.\n\nA map is made using a map projection, which is any method of representing a globe on a plane. All projections distort distances and directions, and each projection distributes those distortions differently. Perhaps the most well known projection is the Mercator Projection, originally designed as a nautical chart.\n\nA thematic map shows geographic information about one or a few focused subjects. These maps \"can portray physical, social, political, cultural, economic, sociological, agricultural, or any other aspects of a city, state, region, nation, or continent\".\n\nEarly world maps cover depictions of the world from the Iron Age to the Age of Discovery and the emergence of modern geography during the early modern period. Old maps provide much information about what was known in times past, as well as the philosophy and cultural basis of the map, which were often much different from modern cartography. Maps are one means by which scientists distribute their ideas and pass them on to future generations.\n\n\n\n"}
{"id": "33666830", "url": "https://en.wikipedia.org/wiki?curid=33666830", "title": "World population milestones", "text": "World population milestones\n\nWorld population milestones were unnoticed until the 20th century, since there were no reliable data on global population dynamics.\n\nIt is estimated that the population of the world reached one billion for the first time in 1804. It would be another 123 years before it reached two billion in 1927, but it took only 33 years to rise by another billion people, reaching three billion in 1960. Thereafter, the global population reached four billion in 1974, five billion in 1987, six billion in 1999 and, by some estimates, seven billion in October 2011 with other estimates being in March 2012. It is projected to reach eight billion by 2024–2030. According to current projections, the world's population is likely to reach around nine billion by 2035–2050, with alternative scenarios ranging from a low of 7.4 billion to a high of more than 10.6 billion. Projected figures vary depending on underlying statistical assumptions and which variables are manipulated in projection calculations, especially the fertility variable. Long-range predictions to 2150 range from a population decline to 3.2 billion in the\n'low scenario', to 'high scenarios' of 24.8 billion. One scenario predicted a massive increase to 256 billion by 2150, assuming fertility remains at 1995 levels.\n\nThere is no estimation for the exact day or month the world's population surpassed each of the one and two billion marks. The days of three and four billion were not officially noted, but the International Database of the United States Census Bureau places them in July 1959 and April 1974.\n\nThe Day of Five Billion, 11 July 1987, was designated by the United Nations Population Fund as the approximate day on which world population reached five billion. Matej Gašpar from Zagreb, Croatia (then SR Croatia, SFR Yugoslavia), was chosen as the symbolic 5-billionth person concurrently alive on Earth. The honor went to Zagreb because the 1987 Summer Universiade was taking place in the city at the time.\n\nThe United Nations Population Fund designated 12 October 1999 as the approximate day on which the world population reached six billion. It was officially designated The Day of Six Billion. Demographers do not universally accept this date as being exact. In fact there has been subsequent research which places the day of six billion nearer to 18 June or 19 June 1999. The International Programs division of the United States Census Bureau estimated that the world population reached six billion on 21 April 1999. United Nations Population Fund spokesman Omar Gharzeddine disputed the date of the Day of Six Billion by stating, \"The U.N. marked the '6 billionth' [person] in 1999, and then a couple of years later the Population Division itself reassessed its calculations and said, actually, no, it was in 1998.\"\n\nOn the Day of Six Billion, UN Secretary-General Kofi Annan was in Sarajevo, Bosnia and Herzegovina to monitor the Dayton Agreement. At midnight he went to Koševo Hospital, where Adnan Mević, born at 12.01 am, was named the symbolic 6 billionth concurrently alive person on Earth. He is the first son of Fatima Mević and Jasminko Mević and weighed 3.5 kg.\n\nThe \"Day of Seven Billion\" was targeted by the United States Census Bureau to be in March 2012, while the Population Division of the United Nations suggested 31 October 2011, and the latter date was officially designated by the United Nations Population Fund (UNFPA) as the approximate day on which the world's population reached seven billion people. United Nations Secretary General Ban Ki-moon spoke at the United Nations building in New York City on this milestone in the size of world population, and promoted the website 7 Billion Actions. Ban Ki-moon did not choose a symbolic seven billionth baby, but several groups proposed candidates: Nargis Kumar of Uttar Pradesh, India, Danica May Camacho of Manila, Philippines and Wattalage Muthumai of Colombo, Sri Lanka.\n\nNational or subnational governments have sometimes made similar designations based on the date estimated by a demographic agency. Some national milestones relate to citizens rather than residents. Commentators in countries with high immigration have pointed out that a population milestone may be reached by an immigrant rather than natural increase. \n\n"}
