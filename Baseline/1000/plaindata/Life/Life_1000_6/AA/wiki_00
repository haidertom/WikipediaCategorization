{"id": "47767241", "url": "https://en.wikipedia.org/wiki?curid=47767241", "title": "2 Pistols", "text": "2 Pistols\n\nJeremy Lemont Saunders (born June 11, 1983), better known by his stage name 2 Pistols (censored as 2P), is an American rapper and songwriter. He hails from Tarpon Springs, Florida. He has released two albums and a number of singles and mixtapes. He signed with Universal in 2007 and his collaboration with T-Pain on the song \"She Got It\", which peaked at #7 on the Rhythmic Top 40 chart in 2008, and hit #2 on the Billboard Hot Rap Tracks chart.\n\nFrom a broken home, Saunders grew up being looked after by his extended family. By his teens, he became involved in local crime and in 2005 was incarcerated for eight months. After this he became involved in music promotion and formed a group called Blood Money Union, which consisted of other DJs, producers and rappers.\n\nSaunders' first success in rapping had come with a self-released record called \"Dirty Foot,\" which he wrote while still in high school and distributed in the Tarpon Springs area, at the urging of his cousin. After hearing it played in a local dance club and then witnessing another rapper's performance, 2 Pistols had his first chance to perform on-stage. After taking the stage and performing his own single (\"Dirty Foot\"), 2 Pistols' confidence in his abilities grew to a point that he began to take his chances at making a career of music seriously.\n\nHis debut album, \"Death Before Dishonor\", was released on June 17, 2008 and featured production from the Grammy-winning J.U.S.T.I.C.E. League, Da Honorable C.N.O.T.E, Bolo Da Producer, and others. Other tracks from the album included \"You Know Me feat. Ray J\" & \"Thats My Word feat.Trey Songz\",\" in addition to \"She Got It.\" The album peaked at #32 on the Billboard Top 200 chart, and rose to #10 on the Billboard Top R&B/Hip Hop album chart. 2 Pistols resides in Tampa, Florida and currently releases his music under his own label, Blood Money Union. In February 2014, the album \"Comin Back Hard\" appeared through Stage One Music.\n\n"}
{"id": "41028575", "url": "https://en.wikipedia.org/wiki?curid=41028575", "title": "2wenty", "text": "2wenty\n\n2wenty is an American mixed-media artist and photographer from Los Angeles. He is known for using light as the central theme in his work. 2wenty comes from a background in lighting television and film which gives him an intimate understanding of how light works. The sources of light he utilizes for his light paintings are handmade, as well as creating the LED paintings by hand. His works can be viewed both nationally and internationally in Los Angeles, Miami, and London.\n\n2wenty first emerged in 2011, when his \"Facebook Social Cigarettes\" posters started appearing in technology blogs all over the internet and on the streets of Los Angeles and San Francisco. The posters featured a pack of cigarettes with the Facebook logo and a tag that read, \"Social Cigarettes.\" He created over one hundred of the posters, comparing Facebook consumption with that of \"cancer sticks.\" 2wenty also created three-dimensional versions of the posters, complete with a surgeon general warning, \"Facebook may cause loss of time, poor work ethic, obesity, social disorder and possible interference of destiny. Use at your own risk.\"\n\n2wenty has had multiple collaborations with fellow aritst, Gregory Siff, in addition to This Means Mar. In December 2013, 2wenty was invited by Mar to show and create one of his \"signature\" light paintings at his solo show 'I Crave Love', which was held at Lab Art Gallery in Los Angeles.\n\nHis works have been exhibited in Los Angeles, San Francisco, Washington D.C. and at Miami's Art Basel. He has been featured in Forbes, Beverly Hills Lifestyle Magazine, LA Canvas, Leveled Magazine, Melrose and Fairfax, Gawker and as background themes for the, \"My Yahoo\", main page on Yahoo.com.\n\n2wenty expands his oeuvre into light painting photography. Through long exposures, the camera captures not only the subject, but also the source of light the artist moves across his \"canvas\". Within these detailed images,\" There is a depth and other worldliness that records both place and passage of time\", says 2wenty.\n\n"}
{"id": "53461533", "url": "https://en.wikipedia.org/wiki?curid=53461533", "title": "6ix (record producer)", "text": "6ix (record producer)\n\nArjun \"6ix\" Ivatury (born February 17, 1991) is an Indian-American record producer mostly known for his work in Gaithersburg, Maryland with rapper Logic as his in-house producer. Other artists he has worked with include Dizzy Wright, Michael Christmas, QuESt, and Jessica Andrea. He is associated with the independent record label Visionary Music Group.\n\n6ix is best known for his work with Gaithersburg, Maryland rapper Logic (2010 – present) for whom he has produced over 43 songs, including eleven from Logic's latest mixtape YSIV. The first song Logic and 6ix produced together was Love Jones from Logic's first mixtape.\n\n6ix is a native of Bowie, Maryland. His father is a doctor; his mother is an engineer; his older brother is an aerospace engineer. Before he became a full-time producer, 6ix attended the University of Maryland, College Park, where he majored in neurology and physiology. In his senior year he dropped out of college in 2013 to move to Los Angeles with Logic to pursue a career in music production; he was 30 credits shy of his degree.He was nominated for Grammys for the first time for producing the song 1-800-273-8255 by Logic featuring Alessia Cara and Khalid. In an interview with SlimShady magazine, he reportedly said that Richik Dadhich is his greatest fan. \nHe is also the executive producer of Logic's latest album \"YSIV\" released on September 28, 2018 and is also collaborating with Logic for his upcoming project called \"Ultra 85\".\n"}
{"id": "16458049", "url": "https://en.wikipedia.org/wiki?curid=16458049", "title": "9ice", "text": "9ice\n\n9ice (born Abolore Ajifolajifaola Adegbola Akande, on 17 January 1980) is a Nigerian musician. He is a south-westerner from Ogbomoso in Oyo State but he grew up in the district of Bariga in Lagos.\n\n9ice won the award for Best Hip Hop Artist at the MTV Africa Music Awards in 2008. At the third edition of the Hip Hop World Awards held in Nigeria, he won the Revelation of the Year award and Best Male Vocal Performer. A month later at the first SoundCity Music Video Awards, he was nominated in the category of Best New Artist. 9ice's biggest single is titled \"Gongo Aso\". He is the founder of Alapomeji Records.\n\nIn 2014, 9ice revealed that he was going to contest for a political office in his native Ogbomosho in Oyo state. He joined the All Progressive Congress (APC) and declared his interest to contest for a seat at the Federal House of Representatives. He lost out during the primaries. 9ice was named as a Special Adviser to the Oyo state Governor, Senator Abiola Ajimobi.\n\n9ice attended Abule Okuta Primary School and CMS Grammar School, and dropped out from his law course at the Lagos State University to concentrate on music. He grew up in a polygamous home of five wives and nine children. His parents found about his singing career a year after it started around 2000. Before then, 9ice wrote his own songs, beginning at age 14. As a big fan of Pasuma Wonder, he kick-started his singing career with Fuji music. He derives his inspiration from his environment, and music from the likes of Ebenezer Obey, King Sunny Adé, Tatalo Alamu, the late Alhaji Ayinla Omowura, and the late Alhaji Haruna Ishola.\n\nAfter recording his first demo, 9ice joined the group Mysterious Boys, with whom he did a couple of tracks before going on to form his own, now defunct, group, Abinibi. Having recorded his first demo, titled \"Risi De Alagbaja\", in 1996, and his first solo song, \"Little Money\" in 2000, 9ice had to wait until 2005 before gaining recognition in the Nigerian music market.\n\nThe first major break for 9ice came from his first single, \"Little Money\". Soon after this, he did collaborations with different Nigerian artists that prompted the release of another hit single, \"Ganja Man\". ID Cabasa produced this single, which was released with other songs in March 2006. The reggae tone and playful style of the song made it an instant hit. 9ice later founded his own record company and released all other albums under that label, Alapomeji Records.\n\n9ice's first album did well, and included the songs \"Little Money\", \"Ganja Man\", \"Make Dem Talk\" and \"Music Daddy\". \"Gongo Aso\", his second album, included songs covering success, originality, partying and women, as well as themes on the institution of marriage and gratitude. He has eight studio albums to his name which includes; Certificate (2007), Gongo Aso (2008), Tradition (2009), Versus (2011) Bashorun Gaa (2011), Deluxe Version Versus & Bashorun Gaa (2011), CNN/GRA (2014) and ID Cabasa (2016).\nHe is recognized to be the first Nigerian hip-hop artist to release two albums in a year which are C.N.N (Canceling Numerous Negativity) and G.R.A (Galvanizing Right Ahead).\n\nSince late 2007 9ice has featured prominently across Nigerian campuses in the Soundcity/MTN Campus Blast tours; Lets Go There Tour With Ariya Entertainment in NY, LA, Chicago, Houston, Maryland in 2008; Star Mega Jam in 2007-2010; and London Troxy in 2008 and 2010. 9ice performed at the Nelson Mandela 90th Birthday Tribute concert (singing \"Gongo Aso\") in London on 27 June 2008. All these shows and concerts were sold-out events. He also headlined sold-out shows in the Netherlands and Malaysia in October 2010 to mark his country's 50-year anniversary of independence. In 2010, 9ice embarked on a European tour through parts of Italy, Spain and Switzerland. He also performed live in Cyprus at Lions Garden, where many Nigerian students were in attendance.\n\n9ice is known for his use of the Yoruba language in his music, and he sometimes mixes Yoruba proverbs with English, broken English, Hausa and Igbo. 9ice compares himself to Youssou N'Dour, who has won a couple of Grammy awards with music recorded in his mother tongue, and Yvonne Chaka Chaka, whose music has also embraced her language. 9ice has been quoted as saying, \"English language has been imposed on us but God graciously gave Yoruba language to us.\"\n\nIn January 2014, 9ice announced plans to run for a political office in 2015 as a House of Representatives member representing the Ogbomoso North Constituency, Oyo state in the National Assembly. He contested for the seat under the umbrella of the APC but lost at the party primaries.\n\nIn April 2015, the “Gongo Aso” singer was appointed a special adviser to the governor of Oyo state, Abiola Ajimobi.\n\n\n\n\n\n\n"}
{"id": "57125214", "url": "https://en.wikipedia.org/wiki?curid=57125214", "title": "A Dialogue on Personal Identity and Immortality", "text": "A Dialogue on Personal Identity and Immortality\n\nA Dialogue on Personal Identity and Immortality is a book by the philosopher John Perry.\nIt has been translated into Spanish, Chinese, Persian and Korean.\n\nIt deals with standard problems in the theory of personal identity in the form of a dialogue between a terminally ill university professor at a small Midwestern college, Gretchen Weirob, and her two friends, Sam Miller and Dave Cohen. The views represented include those of Bernard Williams, John Locke, and Derek Parfit. The format of associating different philosophical positions with different characters in a dialogue recalls David Hume's \"Dialogues Concerning Natural Religion\". \n\n"}
{"id": "233013", "url": "https://en.wikipedia.org/wiki?curid=233013", "title": "Absurdism", "text": "Absurdism\n\nIn philosophy, \"the Absurd\" refers to the conflict between the human tendency to seek inherent value and meaning in life and the human inability to find any in a purposeless, meaningless or chaotic and irrational universe. The universe and the human mind do not each separately cause the Absurd, but rather, the Absurd arises by the contradictory nature of the two existing simultaneously. \n\nAs a philosophy, absurdism furthermore explores the fundamental nature of the Absurd and how individuals, once becoming conscious of the Absurd, should respond to it. The absurdist philosopher Albert Camus stated that individuals should embrace the absurd condition of human existence while also defiantly continuing to explore and search for meaning.\n\nAbsurdism shares some concepts, and a common theoretical template, with existentialism and nihilism. It has its origins in the work of the 19th-century Danish philosopher Søren Kierkegaard, who chose to confront the crisis that humans face with the Absurd by developing his own existentialist philosophy. Absurdism as a belief system was born of the European existentialist movement that ensued, specifically when Camus rejected certain aspects of that philosophical line of thought and published his essay \"The Myth of Sisyphus\". The aftermath of World War II provided the social environment that stimulated absurdist views and allowed for their popular development, especially in the devastated country of France.\n\nIn absurdist philosophy, the Absurd arises out of the fundamental disharmony between the individual's search for meaning and the meaninglessness of the universe. As beings looking for meaning in a meaningless world, humans have three ways of resolving the dilemma. Kierkegaard and Camus describe the solutions in their works, \"The Sickness Unto Death\" (1849) and \"The Myth of Sisyphus\" (1942), respectively:\n\n\nAbsurdism originated from (as well as alongside) the 20th-century strains of existentialism and nihilism; it shares some prominent starting points with both, though also entails conclusions that are uniquely distinct from these other schools of thought. All three arose from the human experience of anguish and confusion stemming from the Absurd: the apparent meaninglessness in a world in which humans, nevertheless, are compelled to find or create meaning. The three schools of thought diverge from there. Existentialists have generally advocated the individual's construction of his or her own meaning in life as well as the free will of the individual. Nihilists, on the contrary, contend that \"it is futile to seek or to affirm meaning where none can be found.\" Absurdists, following Camus's formulation, hesitantly allow the possibility for some meaning or value in life, but are neither as certain as existentialists are about the value of one's own constructed meaning nor as nihilists are about the total inability to create meaning. Absurdists following Camus also devalue or outright reject free will, encouraging merely that the individual live defiantly and authentically \"in spite of\" the psychological tension of the Absurd.\n\nCamus himself passionately worked to counter nihilism, as he explained in his essay \"The Rebel,\" while he also categorically rejected the label of \"existentialist\" in his essay \"Enigma\" and in the compilation \"The Lyrical and Critical Essays of Albert Camus\", though he was, and still is, often broadly characterized by others as an existentialist. Both existentialism and absurdism entail consideration of the practical applications of becoming conscious of the truth of existential nihilism: i.e., how a driven seeker of meaning should act when suddenly confronted with the seeming concealment, or downright absence, of meaning in the universe. Camus's own understanding of the world (e.g., \"a benign indifference\", in \"The Stranger\"), and every vision he had for its progress, however, sets him apart from the general existentialist trend.\n\nSuch a chart represents some of the overlap and tensions between existentialist and absurdist approaches to meaning. While absurdism can be seen as a kind of response to existentialism, it can be debated exactly how substantively the two positions differ from each other. The existentialist, after all, doesn't deny the reality of death. But the absurdist seems to reaffirm the way in which death ultimately nullifies our meaning-making activities, a conclusion the existentialists seem to resist through various notions of posterity or, in Sartre's case, participation in a grand humanist project.\n\nA century before Camus, the 19th century Danish philosopher Søren Kierkegaard wrote extensively about the absurdity of the world. In his journals, Kierkegaard writes about the absurd:\n\nHere is another example of the Absurd from his writings: \n\nHow can this absurdity be held or believed? Kierkegaard says: \n\nKierkegaard provides an example in \"Fear and Trembling\" (1843), which was published under the pseudonym \"Johannes de Silentio\". In the story of Abraham in the Book of Genesis, Abraham is told by God to kill his son Isaac. Just as Abraham is about to kill Isaac, an angel stops Abraham from doing so. Kierkegaard believes that through virtue of the absurd, Abraham, defying all reason and ethical duties (\"you cannot act\"), got back his son and reaffirmed his faith (\"where I have to act\").\n\nAnother instance of absurdist themes in Kierkegaard's work appears in \"The Sickness Unto Death\", which Kierkegaard signed with pseudonym \"Anti-Climacus\". Exploring the forms of despair, Kierkegaard examines the type of despair known as defiance. In the opening quotation reproduced at the beginning of the article, Kierkegaard describes how such a man would endure such a defiance and identifies the three major traits of the Absurd Man, later discussed by Albert Camus: a rejection of escaping existence (suicide), a rejection of help from a higher power and acceptance of his absurd (and despairing) condition.\n\nAccording to Kierkegaard in his autobiography \"The Point of View of My Work as an Author\", most of his pseudonymous writings are not necessarily reflective of his own opinions. Nevertheless, his work anticipated many absurdist themes and provided its theoretical background.\n\nThough the notion of the 'absurd' pervades all Albert Camus's writing, \"The Myth of Sisyphus\" is his chief work on the subject. In it, Camus considers absurdity as a confrontation, an opposition, a conflict or a \"divorce\" between two ideals. Specifically, he defines the human condition as absurd, as the confrontation between man's desire for significance, meaning and clarity on the one hand – and the silent, cold universe on the other. He continues that there are specific human experiences evoking notions of absurdity. Such a realization or encounter with the absurd leaves the individual with a choice: suicide, a leap of faith, or recognition. He concludes that recognition is the only defensible option.\n\nFor Camus, suicide is a \"confession\" that life is not worth living; it is a choice that implicitly declares that life is \"too much.\" Suicide offers the most basic \"way out\" of absurdity: the immediate termination of the self and its place in the universe.\n\nThe absurd encounter can also arouse a \"leap of faith,\" a term derived from one of Kierkegaard's early pseudonyms, \"Johannes de Silentio\" (although the term was not used by Kierkegaard himself), where one believes that there is more than the rational life (aesthetic or ethical). To take a \"leap of faith,\" one must act with the \"virtue of the absurd\" (as \"Johannes de Silentio\" put it), where a suspension of the ethical may need to exist. This faith has no expectations, but is a flexible power initiated by a recognition of the absurd. (Although at some point, one recognizes or encounters the existence of the Absurd and, in response, actively ignores it.) However, Camus states that because the leap of faith escapes rationality and defers to abstraction over personal experience, the leap of faith is not absurd. Camus considers the leap of faith as \"philosophical suicide,\" rejecting both this and physical suicide.\n\nLastly, a person can choose to embrace the absurd condition. According to Camus, one's freedom – and the opportunity to give life meaning – lies in the recognition of absurdity. If the absurd experience is truly the realization that the universe is fundamentally devoid of absolutes, then we as individuals are truly free. \"To live without appeal,\" as he puts it, is a philosophical move to define absolutes and universals subjectively, rather than objectively. The freedom of humans is thus established in a human's natural ability and opportunity to create their own meaning and purpose; to decide (or think) for him- or herself. The individual becomes the most precious unit of existence, representing a set of unique ideals that can be characterized as an entire universe in its own right. In acknowledging the absurdity of seeking any inherent meaning, but continuing this search regardless, one can be happy, gradually developing meaning from the search alone.\n\nCamus states in \"The Myth of Sisyphus\": \"Thus I draw from the absurd three consequences, which are my revolt, my freedom, and my passion. By the mere activity of consciousness I transform into a rule of life what was an invitation to death, and I refuse suicide.\" \"Revolt\" here refers to the refusal of suicide and search for meaning despite the revelation of the Absurd; \"Freedom\" refers to the lack of imprisonment by religious devotion or others' moral codes; \"Passion\" refers to the most wholehearted experiencing of life, since hope has been rejected, and so he concludes that every moment must be lived fully.\n\nAccording to absurdism, humans historically attempt to find meaning in their lives. Traditionally, this search results in one of two conclusions: either that life is meaningless, or life contains within it a purpose set forth by a higher power—a belief in God, or adherence to some religion or other abstract concept. \n\nCamus perceives filling the void with some invented belief or meaning as a mere \"act of eluding\"—that is, avoiding or escaping rather than acknowledging and embracing the Absurd. To Camus, elusion is a fundamental flaw in religion, existentialism, and various other schools of thought. If the individual eludes the Absurd, then he or she can never confront it.\nCamus also concedes that elusion is the most common.\n\nEven with a spiritual power as the answer to meaning, another question arises: What is the purpose of a belief in God? Kierkegaard believed that there is no human-comprehensible purpose of God, making faith in God absurd itself. Camus on the other hand states that to believe in God is to \"deny one of the terms of the contradiction\" between humanity and the universe (and is therefore not absurd but what he calls \"philosophical suicide\"). Camus (as well as Kierkegaard), though, suggests that while absurdity does not lead to belief in God, neither does it lead to the denial of God. Camus notes, \"I did not say 'excludes God', which would still amount to asserting\".\n\nFor Camus, the beauty people encounter in life makes it worth living. People may create meaning in their own lives, which may not be the objective meaning of life (if there is one), but can still provide something to strive for. However, he insisted that one must always maintain an ironic distance between this invented meaning and the knowledge of the absurd, lest the fictitious meaning take the place of the absurd.\n\nFreedom cannot be achieved beyond what the absurdity of existence permits; however, the closest one can come to being absolutely free is through acceptance of the Absurd. Camus introduced the idea of \"acceptance without resignation\" as a way of dealing with the recognition of absurdity, asking whether or not man can \"live without appeal\", while defining a \"conscious revolt\" against the avoidance of absurdity of the world. In a world devoid of higher meaning or judicial afterlife, the human nature becomes as close to absolutely free as is humanly possible.\n\nThe rejection of hope, in absurdism, denotes the refusal to believe in anything more than what this absurd life provides. Hope, Camus emphasizes, however, has nothing to do with despair (meaning that the two terms are not opposites). One can still live fully while rejecting hope, and, in fact, can only do so \"without\" hope. Hope is perceived by the absurdist as another fraudulent method of evading the Absurd, and by not having hope, one is motivated to live every fleeting moment to the fullest. In the words of Nikos Kazantzakis's epitaph: \"I hope for nothing. I fear nothing. I am free.\"\n\nThe absurdist is not guided by morality, but rather, by their own integrity. The absurdist is, in fact, amoral (though not necessarily immoral). The Absurdist's view of morality implies an unwavering sense of definite right and wrong at all times, while integrity implies honesty with one's self and consistency in the motivations of one's actions and decisions.\n\n\n"}
{"id": "985619", "url": "https://en.wikipedia.org/wiki?curid=985619", "title": "Agent-based model", "text": "Agent-based model\n\nAn agent-based model (ABM) is a class of computational models for simulating the actions and interactions of autonomous agents (both individual or collective entities such as organizations or groups) with a view to assessing their effects on the system as a whole. It combines elements of game theory, complex systems, emergence, computational sociology, multi-agent systems, and evolutionary programming. Monte Carlo methods are used to introduce randomness. Particularly within ecology, ABMs are also called individual-based models (IBMs), and individuals within IBMs may be simpler than fully autonomous agents within ABMs. A review of recent literature on individual-based models, agent-based models, and multiagent systems shows that ABMs are used on non-computing related scientific domains including biology, ecology and social science. Agent-based modeling is related to, but distinct from, the concept of multi-agent systems or multi-agent simulation in that the goal of ABM is to search for explanatory insight into the collective behavior of agents obeying simple rules, typically in natural systems, rather than in designing agents or solving specific practical or engineering problems.\n\nAgent-based models are a kind of microscale model that simulate the simultaneous operations and interactions of multiple agents in an attempt to re-create and predict the appearance of complex phenomena. The process is one of emergence from the lower (micro) level of systems to a higher (macro) level. As such, a key notion is that simple behavioral rules generate complex behavior. This principle, known as K.I.S.S. (\"Keep it simple, stupid\"), is extensively adopted in the modeling community. Another central tenet is that the whole is greater than the sum of the parts. Individual agents are typically characterized as boundedly rational, presumed to be acting in what they perceive as their own interests, such as reproduction, economic benefit, or social status, using heuristics or simple decision-making rules. ABM agents may experience \"learning\", adaptation, and reproduction.\n\nMost agent-based models are composed of: (1) numerous agents specified at various scales (typically referred to as agent-granularity); (2) decision-making heuristics; (3) learning rules or adaptive processes; (4) an interaction topology; and (5) an environment. ABMs are typically implemented as computer simulations, either as custom software, or via ABM toolkits, and this software can be then used to test how changes in individual behaviors will affect the system's emerging overall behavior.\n\nThe idea of agent-based modeling was developed as a relatively simple concept in the late 1940s. Since it requires computation-intensive procedures, it did not become widespread until the 1990s.\n\nThe history of the agent-based model can be traced back to the Von Neumann machine, a theoretical machine capable of reproduction. The device von Neumann proposed would follow precisely detailed instructions to fashion a copy of itself. The concept was then built upon by von Neumann's friend Stanislaw Ulam, also a mathematician; Ulam suggested that the machine be built on paper, as a collection of cells on a grid. The idea intrigued von Neumann, who drew it up—creating the first of the devices later termed cellular automata.\nAnother advance was introduced by the mathematician John Conway. He constructed the well-known Game of Life. Unlike von Neumann's machine, Conway's Game of Life operated by tremendously simple rules in a virtual world in the form of a 2-dimensional checkerboard.\n\nOne of the earliest agent-based models in concept was Thomas Schelling's segregation model, which was discussed in his paper \"Dynamic Models of Segregation\" in 1971. Though Schelling originally used coins and graph paper rather than computers, his models embodied the basic concept of agent-based models as autonomous agents interacting in a shared environment with an observed aggregate, emergent outcome.\n\nIn the early 1980s, Robert Axelrod hosted a tournament of Prisoner's Dilemma strategies and had them interact in an agent-based manner to determine a winner. Axelrod would go on to develop many other agent-based models in the field of political science that examine phenomena from ethnocentrism to the dissemination of culture.\nBy the late 1980s, Craig Reynolds' work on flocking models contributed to the development of some of the first biological agent-based models that contained social characteristics. He tried to model the reality of lively biological agents, known as artificial life, a term coined by Christopher Langton.\n\nThe first use of the word \"agent\" and a definition as it is currently used today is hard to track down. One candidate appears to be John Holland and John H. Miller's 1991 paper \"Artificial Adaptive Agents in Economic Theory\", based on an earlier conference presentation of theirs.\n\nAt the same time, during the 1980s, social scientists, mathematicians, operations researchers, and a scattering of people from other disciplines developed Computational and Mathematical Organization Theory (CMOT). This field grew as a special interest group of The Institute of Management Sciences (TIMS) and its sister society, the Operations Research Society of America (ORSA).\n\nWith the appearance of StarLogo in 1990, Swarm and NetLogo in the mid-1990s and RePast and AnyLogic in 2000, or GAMA in 2007 as well as some custom-designed code, modelling software became widely available and the range of domains that ABM was applied to, grew. Bonabeau (2002) is a good survey of the potential of agent-based modeling as of the time\n\nThe 1990s were especially notable for the expansion of ABM within the social sciences, one notable effort was the large-scale ABM, Sugarscape, developed by\nJoshua M. Epstein and Robert Axtell to simulate and explore the role of social phenomena such as seasonal migrations, pollution, sexual reproduction, combat, and transmission of disease and even culture. Other notable 1990s developments included Carnegie Mellon University's Kathleen Carley ABM, to explore the co-evolution of social networks and culture.\nDuring this 1990s timeframe Nigel Gilbert published the first textbook on Social Simulation: Simulation for the social scientist (1999) and established a journal from the perspective of social sciences: the \"Journal of Artificial Societies and Social Simulation\" (JASSS). Other than JASSS, agent-based models of any discipline are within scope of SpringerOpen journal \"Complex Adaptive Systems Modeling\" (CASM).\n\nThrough the mid-1990s, the social sciences thread of ABM began to focus on such issues as designing effective teams, understanding the communication required for organizational effectiveness, and the behavior of social networks. CMOT—later renamed Computational Analysis of Social and Organizational Systems (CASOS)—incorporated more and more agent-based modeling. Samuelson (2000) is a good brief overview of the early history, and Samuelson (2005) and Samuelson and Macal (2006) trace the more recent developments.\n\nIn the late 1990s, the merger of TIMS and ORSA to form INFORMS, and the move by INFORMS from two meetings each year to one, helped to spur the CMOT group to form a separate society, the North American Association for Computational Social and Organizational Sciences (NAACSOS). Kathleen Carley was a major contributor, especially to models of social networks, obtaining National Science Foundation funding for the annual conference and serving as the first President of NAACSOS. She was succeeded by David Sallach of the University of Chicago and Argonne National Laboratory, and then by Michael Prietula of Emory University. At about the same time NAACSOS began, the European Social Simulation Association (ESSA) and the Pacific Asian Association for Agent-Based Approach in Social Systems Science (PAAA), counterparts of NAACSOS, were organized. As of 2013, these three organizations collaborate internationally. The First World Congress on Social Simulation was held under their joint sponsorship in Kyoto, Japan, in August 2006. The Second World Congress was held in the northern Virginia suburbs of Washington, D.C., in July 2008, with George Mason University taking the lead role in local arrangements.\n\nMore recently, Ron Sun developed methods for basing agent-based simulation on models of human cognition, known as cognitive social simulation. Bill McKelvey, Suzanne Lohmann, Dario Nardi, Dwight Read and others at UCLA have also made significant contributions in organizational behavior and decision-making. Since 2001, UCLA has arranged a conference at Lake Arrowhead, California, that has become another major gathering point for practitioners in this field. In 2014, Sadegh Asgari from Columbia University and his colleagues developed an agent-based model of the construction competitive bidding. While his model was used to analyze the low-bid lump-sum construction bids, it could be applied to other bidding methods with little modifications to the model.\n\nMost computational modeling research describes systems in equilibrium or as moving between equilibria. Agent-based modeling, however, using simple rules, can result in different sorts of complex and interesting behavior. The three ideas central to agent-based models are agents as objects, emergence, and complexity.\n\nAgent-based models consist of dynamically interacting rule-based agents. The systems within which they interact can create real-world-like complexity. Typically agents are\nsituated in space and time and reside in networks or in lattice-like neighborhoods. The location of the agents and their responsive behavior are encoded in algorithmic form in computer programs. In some cases, though not always, the agents may be considered as intelligent and purposeful. In ecological ABM (often referred to as \"individual-based models\" in ecology), agents may, for example, be trees in forest, and would not be considered intelligent, although they may be \"purposeful\" in the sense of optimizing access to a resource (such as water).\nThe modeling process is best described as inductive. The modeler makes those assumptions thought most relevant to the situation at hand and then watches phenomena emerge from the agents' interactions. Sometimes that result is an equilibrium. Sometimes it is an emergent pattern. Sometimes, however, it is an unintelligible mangle.\n\nIn some ways, agent-based models complement traditional analytic methods. Where analytic methods enable humans to characterize the equilibria of a system, agent-based models allow the possibility of generating those equilibria. This generative contribution may be the most mainstream of the potential benefits of agent-based modeling. Agent-based models can explain the emergence of higher-order patterns—network structures of terrorist organizations and the Internet, power-law distributions in the sizes of traffic jams, wars, and stock-market crashes, and social segregation that persists despite populations of tolerant people. Agent-based models also can be used to identify lever points, defined as moments in time in which interventions have extreme consequences, and to distinguish among types of path dependency.\n\nRather than focusing on stable states, many models consider a system's robustness—the ways that complex systems adapt to internal and external pressures so as to maintain their functionalities. The task of harnessing that complexity requires consideration of the agents themselves—their diversity, connectedness, and level of interactions.\n\nRecent work on the Modeling and simulation of Complex Adaptive Systems has demonstrated the need for combining agent-based and complex network based models. describe a framework consisting of four levels of developing models of complex adaptive systems described using several example multidisciplinary case studies:\nOther methods of describing agent-based models include code templates and text-based methods such as the ODD (Overview, Design concepts, and Design Details) protocol.\n\nThe role of the environment where agents live, both macro and micro, is also becoming an important factor in agent-based modelling and simulation work. Simple environment affords simple agents, but complex environments generates diversity of behaviour.\n\nAgent-based modeling has been used extensively in biology, including the analysis of the spread of epidemics, and the threat of biowarfare, biological applications including population dynamics, vegetation ecology, landscape diversity, the growth and decline of ancient civilizations, evolution of ethnocentric behavior, forced displacement/migration, language choice dynamics, cognitive modeling, and biomedical applications including modeling 3D breast tissue formation/morphogenesis, the effects of ionizing radiation on mammary stem cell subpopulation dynamics, inflammation,\nand the human immune system. Agent-based models have also been used for developing decision support systems such as for breast cancer. Agent-based models are increasingly being used to model pharmacological systems in early stage and pre-clinical research to aid in drug development and gain insights into biological systems that would not be possible \"a priori\". Military applications have also been evaluated. Moreover, agent-based models have been recently employed to study molecular-level biological systems.\n\nAgent-based models have been used since the mid-1990s to solve a variety of business and technology problems. Examples of applications include the modeling of organizational behaviour and cognition, team working, supply chain optimization and logistics, modeling of consumer behavior, including word of mouth, social network effects, distributed computing, workforce management, and portfolio management. They have also been used to analyze traffic congestion.\n\nRecently, agent based modelling and simulation has been applied to various domains such as studying the impact of publication venues by researchers in the computer science domain (journals versus conferences). In addition, ABMs have been used to simulate information delivery in ambient assisted environments. A November 2016 article in arXiv analyzed an agent based simulation of posts spread in the Facebook online social network. In the domain of peer-to-peer, ad-hoc and other self-organizing and complex networks, the usefulness of agent based modeling and simulation has been shown. The use of a computer science-based formal specification framework coupled with wireless sensor networks and an agent-based simulation has recently been demonstrated.\n\nAgent based evolutionary search or algorithm is a new research topic for solving complex optimization problems.\n\nPrior to, and in the wake of the financial crisis, interest has grown in ABMs as possible tools for economic analysis. ABMs do not assume the economy can achieve equilibrium and \"representative agents\" are replaced by agents with diverse, dynamic, and interdependent behavior including herding. ABMs take a \"bottom-up\" approach and can generate extremely complex and volatile simulated economies. ABMs can represent unstable systems with crashes and booms that develop out of non-linear (disproportionate) responses to proportionally small changes. A July 2010 article in \"The Economist\" looked at ABMs as alternatives to DSGE models. The journal \"Nature\" also encouraged agent-based modeling with an editorial that suggested ABMs can do a better job of representing financial markets and other economic complexities than standard models along with an essay by J. Doyne Farmer and Duncan Foley that argued ABMs could fulfill both the desires of Keynes to represent a complex economy and of Robert Lucas to construct models based on microfoundations. Farmer and Foley pointed to progress that has been made using ABMs to model parts of an economy, but argued for the creation of a very large model that incorporates low level models. By modeling a complex system of analysts based on three distinct behavioral profiles – imitating, anti-imitating, and indifferent – financial markets were simulated to high accuracy. Results showed a correlation between network morphology and the stock market index.\n\nSince the beginning of the 21st century ABMs have been deployed in architecture and urban planning to evaluate design and to simulate pedestrian flow in the urban environment. There is also a growing field of socio-economic analysis of infrastructure investment impact using ABM's ability to discern systemic impacts upon a socio-economic network.\n\nThe agent-directed simulation (ADS) metaphor distinguishes between two categories, namely \"Systems for Agents\" and \"Agents for Systems.\" Systems for Agents (sometimes referred to as agents systems) are systems implementing agents for the use in engineering, human and social dynamics, military applications, and others. Agents for Systems are divided in two subcategories. Agent-supported systems deal with the use of agents as a support facility to enable computer assistance in problem solving or enhancing cognitive capabilities. Agent-based systems focus on the use of agents for the generation of model behavior in a system evaluation (system studies and analyses).\n\nMany agent-based modeling software are designed for serial von-Neumann computer architectures. This limits the speed and scalability of these systems. A recent development is the use of data-parallel algorithms on Graphics Processing Units GPUs for ABM simulation. The extreme memory bandwidth combined with the sheer number crunching power of multi-processor GPUs has enabled simulation of millions of agents at tens of frames per second.\n\nVerification and validation (V&V) of simulation models is extremely important. Verification involves the model being debugged to ensure it works correctly, whereas validation ensures that the right model has been built. Face validation, sensitivity analysis, calibration and statistical validation have also been demonstrated. A discrete-event simulation framework approach for the validation of agent-based systems has been proposed. A comprehensive resource on empirical validation of agent-based models can be found here.\n\nAs an example of V&V technique, consider VOMAS (virtual overlay multi-agent system), a software engineering based approach, where a virtual overlay multi-agent system is developed alongside the agent-based model. The agents in the multi-agent system are able to gather data by generation of logs as well as provide run-time validation and verification support by watch agents and also agents to check any violation of invariants at run-time. These are set by the Simulation Specialist with help from the SME (subject-matter expert). Muazi et al. also provide an example of using VOMAS for verification and validation of a forest fire simulation model.\n\nVOMAS provides a formal way of validation and verification. To develop a VOMAS, one must design VOMAS agents along with the agents in the actual simulation, preferably from the start. In essence, by the time the simulation model is complete, one can essentially consider it to be one model containing two models:\n\nUnlike all previous work on verification and validation, VOMAS agents ensure that the simulations are validated in-simulation i.e. even during execution. In case of any exceptional situations, which are programmed on the directive of the Simulation Specialist (SS), the VOMAS agents can report them. In addition, the VOMAS agents can be used to log key events for the sake of debugging and subsequent analysis of simulations. In other words, VOMAS allows for a flexible use of any given technique for the sake of verification and validation of an agent-based model in any domain.\n\nDetails of validated agent-based modeling using VOMAS along with several case studies are given in. This thesis also gives details of \"exploratory agent-based modeling\", \"descriptive agent-based modeling\" and \"validated agent-based modeling\", using several worked case study examples.\n\nMathematical models of complex systems are of three types: black-box (phenomenological), white-box (mechanistic, based on the first principles) and grey-box (mixtures of phenomenological and mechanistic models).\n\n\n\n"}
{"id": "3409", "url": "https://en.wikipedia.org/wiki?curid=3409", "title": "Being", "text": "Being\n\nIn philosophy, being means the existence of a thing. Anything that exists has being. Ontology is the branch of philosophy that studies being. Being is a concept encompassing objective and subjective features of reality and existence. Anything that partakes in being is also called a \"being\", though often this usage is limited to entities that have subjectivity (as in the expression \"human being\"). The notion of \"being\" has, inevitably, been elusive and controversial in the history of philosophy, beginning in Western philosophy with attempts among the pre-Socratics to deploy it intelligibly. The first effort to recognize and define the concept came from Parmenides, who famously said of it that \"what is-is\". Common words such as \"is\", \"are\", and \"am\" refer directly or indirectly to being.\n\nAs an example of efforts in recent times, Martin Heidegger (who himself drew on ancient Greek sources) adopted after German terms like \"Dasein\" to articulate the topic. Several modern approaches build on such continental European exemplars as Heidegger, and apply metaphysical results to the understanding of human psychology and the human condition generally (notably in the Existentialist tradition). By contrast, in mainstream Analytical philosophy the topic is more confined to abstract investigation, in the work of such influential theorists as W. V. O. Quine, to name one of many. One of the most fundamental questions that continues to exercise philosophers is posed by William James: \"How comes the world to be here at all instead of the nonentity which might be imagined in its place? ... from nothing to being there is no logical bridge.\"\n\nThe deficit of such a bridge was first encountered in history by the Pre-Socratic philosophers during the process of evolving a classification of all beings (noun). Aristotle, who wrote after the Pre-Socratics, applies the term category (perhaps not originally) to ten highest-level classes. They comprise one category of substance (ousiae) existing independently (man, tree) and nine categories of accidents, which can only exist in something else (time, place). In Aristotle, substances are to be clarified by stating their definition: a note expressing a larger class (the genus) followed by further notes expressing specific differences (differentiae) within the class. The substance so defined was a species. For example, the species, man, may be defined as an animal (genus) that is rational (difference). As the difference is potential within the genus; that is, an animal may or may not be rational, the difference is not identical to, and may be distinct from, the genus.\n\nApplied to being, the system fails to arrive at a definition for the simple reason that no difference can be found. The species, the genus, and the difference are all equally being: a being is a being that is being. The genus cannot be nothing because nothing is not a class of everything. The trivial solution that being is being added to nothing is only a tautology: being is being. There is no simpler intermediary between being and non-being that explains and classifies being.\nPre-Socratic reaction to this deficit was varied. As substance theorists they accepted a priori the hypothesis that appearances are deceiving, that reality is to be reached through reasoning. Parmenides reasoned that if everything is identical to being and being is a category of the same thing then there can be neither differences between things nor any change. To be different, or to change, would amount to becoming or being non-being; that is, not existing. Therefore, being is a homogeneous and non-differentiated sphere and the appearance of beings is illusory. Heraclitus, on the other hand, foreshadowed modern thought by denying existence. Reality does not exist, it flows, and beings are an illusion upon the flow.\n\nAristotle knew of this tradition when he began his \"Metaphysics\", and had already drawn his own conclusion, which he presented under the guise of asking what being is:\"And indeed the question which was raised of old is raised now and always, and is always the subject of doubt, viz., what being is, is just the question, what is substance? For it is this that some assert to be one, others more than one, and that some assert to be limited in number, others unlimited. And so we also must consider chiefly and primarily and almost exclusively what that is which is in this sense.\"\n\nand reiterates in no uncertain terms: \"Nothing, then, which is not a species of a genus will have an essence – only species will have it ...\". Being, however, for Aristotle, is not a genus.\n\nOne might expect a solution to follow from such certain language but none does. Instead Aristotle launches into a rephrasing of the problem, the Theory of Act and Potency. In the definition of man as a two-legged animal Aristotle presumes that \"two-legged\" and \"animal\" are parts of other beings, but as far as man is concerned, are only potentially man. At the point where they are united into a single being, man, the being, becomes actual, or real. Unity is the basis of actuality: \"... 'being' is being combined and one, and 'not being' is being not combined but more than one.\" Actuality has taken the place of existence, but Aristotle is no longer seeking to know what the actual is; he accepts it without question as something generated from the potential. He has found a \"half-being\" or a \"pre-being\", the potency, which is fully being as part of some other substance. Substances, in Aristotle, unite what they actually are now with everything they might become.\n\nSome of Thomas Aquinas' propositions were reputedly condemned by Étienne Tempier, the local Bishop of Paris (not the Papal Magisterium itself) in 1270 and 1277, but his dedication to the use of philosophy to elucidate theology was so thorough that he was proclaimed a Doctor of the Church in 1568. Those who adopt it are called Thomists.\n\nIn a single sentence, parallel to Aristotle's statement asserting that being is substance, St. Thomas pushes away from the Aristotelian doctrine: \"Being is not a genus, since it is not predicated univocally but only analogically.\" His term for analogy is Latin \"analogia\". In the categorical classification of all beings, all substances are partly the same: man and chimpanzee are both animals and the animal part in man is \"the same\" as the animal part in chimpanzee. Most fundamentally all substances are matter, a theme taken up by science, which postulated one or more matters, such as earth, air, fire or water (Empedocles). In today's chemistry the carbon, hydrogen, oxygen and nitrogen in a chimpanzee are identical to the same elements in a man.\n\nThe original text reads, \"Although equivocal predications must be reduced to univocal, still in actions, the non-univocal agent must precede the univocal agent. For the non-univocal agent is the universal cause of the whole species, as for instance the sun is the cause of the generation of all men; whereas the univocal agent is not the universal efficient cause of the whole species (otherwise it would be the cause of itself, since it is contained in the species), but is a particular cause of this individual which it places under the species by way of participation. Therefore the universal cause of the whole species is not an univocal agent; and the universal cause comes before the particular cause. But this universal agent, whilst it is not univocal, nevertheless is not altogether equivocal, otherwise it could not produce its own likeness, but rather it is to be called an analogical agent, as all univocal predications are reduced to one first non-univocal analogical predication, which is being.\"\n\nIf substance is the highest category and there is no substance, being, then the unity perceived in all beings by virtue of their existing must be viewed in another way. St. Thomas chose the analogy: all beings are like, or analogous to, each other in existing. This comparison is the basis of his Analogy of Being. The analogy is said of being in many different ways, but the key to it is the real distinction between existence and essence. Existence is the principle that gives reality to an essence not the same in any way as the existence: \"If things having essences are real, and it is not of their essence to be, then the reality of these things must be found in some principle other than (really distinct from) their essence.\" Substance can be real or not. What makes an individual substance – a man, a tree, a planet – real is a distinct act, a \"to be\", which actuates its unity. An analogy of proportion is therefore possible: \"essence is related to existence as potency is related to act.\"\n\nExistences are not things; they do not themselves exist, they lend themselves to essences, which do not intrinsically have them. They have no nature; an existence receives its nature from the essence it actuates. Existence is not being; it gives being – here a customary phrase is used, existence is a principle (a source) of being, not a previous source, but one which is continually in effect. The stage is set for the concept of God as the cause of all existence, who, as the Almighty, holds everything actual without reason or explanation as an act purely of will.\n\nAristotle's classificatory scheme had included the five predicables, or characteristics that might be predicated of a substance. One of these was the property, an essential universal true of the species, but not in the definition (in modern terms, some examples would be grammatical language, a property of man, or a spectral pattern characteristic of an element, both of which are defined in other ways). Pointing out that predicables are predicated univocally of substances; that is, they refer to \"the same thing\" found in each instance, St. Thomas argued that whatever can be said about being is not univocal, because all beings are unique, each actuated by a unique existence. It is the analogous possession of an existence that allows them to be identified as being; therefore, being is an analogous predication.\n\nWhatever can be predicated of all things is universal-like but not universal, category-like but not a category. St. Thomas called them (perhaps not originally) the \"transcendentia\", \"transcendentals\", because they \"climb above\" the categories, just as being climbs above substance. Later academics also referred to them as \"the properties of being.\" The number is generally three or four.\n\nThe nature of \"being\" has also been debated and explored in Islamic philosophy, notably by Ibn Sina (Avicenna), Suhrawardi, and Mulla Sadra. A modern linguistic approach which notices that Persian language has exceptionally developed two kinds of \"is\"es, i.e. \"ast\" (\"is\", as a copula) and \"hast\" (as an existential \"is\") examines the linguistic properties of the two lexemes in the first place, then evaluates how the statements made by other languages with regard to \"being\" can stand the test of Persian frame of reference.\n\nIn this modern linguistic approach, it is noticed that the original language of the source, e.g. Greek (like German or French or English), has only one word for two concepts, \"ast\" and \"hast\", or, like Arabic, has no word at all for either word. It therefore exploits the Persian \"hast\" (existential \"is\") versus \"ast\" (predicative \"is\" or copula) to address both Western and Islamic ontological arguments on \"being\" and \"existence\".\n\nThis linguistic method shows the scope of confusion created by languages which cannot differentiate between existential be and copula. It manifests, for instance, that the main theme of Heidegger's \"Being and Time\" is \"astī\" (is-ness) rather than \"hastī\" (existence). When, in the beginning of his book, Heidegger claims that people always talk about existence in their everyday language, without knowing what it means, the example he resorts to is: \"the sky \"is\" blue\" which in Persian can be ONLY translated with the use of the copula \"ast\", and says nothing about \"being\" or \"existence\".\n\nIn the same manner, the linguistic method addresses the ontological works written in Arabic. Since Arabic, like Latin in Europe, had become the official language of philosophical and scientific works in the so-called Islamic World, the early Persian or Arab philosophers had difficulty discussing \"being\" or \"existence\", since the Arabic language, like other Semitic languages, had no verb for either predicative \"be\" (copula) or existential \"be\". So if you try to translate the aforementioned Heidegger's example into Arabic it appears as السماء زرقاء (viz. \"The Sky-- blue\") with no linking \"is\" to be a sign of existential statement. To overcome the problem, when translating the ancient Greek philosophy, certain words were coined like ایس \"aysa\" (from Arabic لیس \"laysa\" 'not') for 'is'. Eventually the Arabic verb وجد \"wajada\" (to find) prevailed, since it was thought that whatever is existent, is to be \"found\" in the world. Hence \"existence\" or Being was called وجود \"wujud\" (Cf. Swedish \"finns\" [found]> there exist; also the Medieval Latin coinage of \"exsistere\" 'standing out (there in the world)' > appear> exist). Now, with regard to the fact that Persian, as the mother tongue of both Avicenna and Sadrā, was in conflict with either Greek or Arabic in this regard, these philosophers should have been warned implicitly by their mother tongue not to confuse two kinds of linguistic beings (viz. copula vs. existential). In fact when analyzed thoroughly, copula, or Persian \"ast\" ('is') indicates an ever-moving chain of relations with no fixed \"entity\" to hold onto (every \"entity\", say A, will be dissolved into \"A is B\" and so on, as soon as one tries to define it). Therefore, the whole reality or what we see as existence (\"found\" in our world) resembles an ever-changing world of \"astī\" (is-ness) flowing in time and space. On the other hand, while Persian \"ast\" can be considered as the 3rd person singular of the verb 'to be', there is no verb but an arbitrary one supporting \"hast\" ('is' as an existential be= exists) has neither future nor past tense and nor a negative form of its own: \"hast\" is just a single untouchable lexeme. It needs no other linguistic element to be complete (\"Hast.\" is a complete sentence meaning \"s/he it exists\"). In fact, any manipulation of the arbitrary verb, e.g. its conjugation, turns \"hast\" back into a copula.\n\nEventually from such linguistic analyses, it appears that while \"astī\" (is-ness) would resemble the world of Heraclitus, \"hastī\" (existence) would rather approaches a metaphysical concept resembling the Parmenidas's interpretation of \"existence\".\n\nIn this regard, Avicenna, who was a firm follower of Aristotle, could not accept either Heraclitian \"is-ness\" (where only constant was \"change\"), nor Parmenidean \"monist immoveable existence\" (the \"hastī\" itself being constant). To solve the contradiction, it so appeared to Philosophers of Islamic world that Aristotle considered the core of existence (i.e. its \"substance\"/\"essence\") as a fixed constant, while its facade (accident) was prone to change. To translate such a philosophical image into Persian it is like having \"hastī\" (existence) as a unique constant core covered by \"astī\" (is-ness) as a cloud of ever-changing relationships. It is clear that the Persian language, deconstructs such a composite as a sheer mirage, since it is not clear how to link the interior core (existence) with the exterior shell (is-ness). Furthermore, \"hast\" cannot be linked to anything but itself (as it is self-referent).\n\nThe argument has a theological echos as well: assuming that God is the \"Existence\", beyond time and space, a question is raised by philosophers of the Islamic world as how he, as a transcendental existence, may ever create or contact a world of \"is-ness\" in space-time.\n\nHowever, Avicenna who was more philosopher than theologian, followed the same line of argumentation as that of his ancient master, Aristotle, and tried to reconcile between \"ast\" and \"hast\", by considering the latter as higher order of existence than the former. It is like a hierarchical order of existence. It was a philosophical Tower of Babel that the restriction of his own mother tongue (Persian) would not allow to be built, but he could maneuver in Arabic by giving the two concepts the same name \"wujud\", although with different attributes. So, implicitly, \"astī\" (is-ness) appears as ممکن الوجود \"momken-al-wujud\" (contingent being), and \"hastī\" (existence) as واجب الوجود \"wājeb-al-wujud\" (necessary being).\n\nOn the other hand, centuries later, Sadrā, chose a more radical route, by inclining towards the reality of \"astī\" (is-ness), as the true mode of existence, and tried to get rid of the concept of \"hastī\" (existence as fixed or immovable). Thus, in his philosophy, the universal movement penetrates deep into the Aristotelian \"substance\"/\"essence\", in unison with changing accident. He called this deep existential change حرکت جوهری \"harekat-e jowhari\" (Substantial Movement). In such a changing existence, the whole world has to go through instantaneous annihilation and recreation incessantly, while as Avicenna had predicted in his remarks on Nature, such a universal change or substantial movement would eventually entail the shortening and lengthening of time as well which has never been observed. This logical objection, which was made on Aristotle's argumentation, could not be answered in the ancient times or medieval age, but now it does not sound contradictory to the real nature of Time (as addressed in relativity theory), so by a reverse argument, a philosopher may indeed deduce that everything is changing (moving) even in the deepest core of Being.\n\nAlthough innovated in the late medieval period, Thomism was dogmatized in the Renaissance. From roughly 1277 to 1567, it dominated the philosophic landscape. The rationalist philosophers, however, with a new emphasis on Reason as a tool of the intellect, brought the classical and medieval traditions under new scrutiny, exercising a new concept of doubt, with varying outcomes. Foremost among the new doubters were the empiricists, the advocates of scientific method, with its emphasis on experimentation and reliance on evidence gathered from sensory experience. In parallel with the revolutions against rising political absolutism based on established religion and the replacement of faith by reasonable faith, new systems of metaphysics were promulgated in the lecture halls by charismatic professors, such as Immanuel Kant, and Hegel. The late 19th and 20th centuries featured an emotional return to the concept of existence under the name of existentialism. These philosophers were concerned mainly with ethics and religion. The metaphysical side became the domain of the phenomenalists. In parallel with these philosophies Thomism continued under the protection of the Catholic Church; in particular, the Jesuit order.\n\nRationalism and empiricism have had many definitions, most concerned with specific schools of philosophy or groups of philosophers in particular countries, such as Germany. In general rationalism is the predominant school of thought in the multi-national, cross-cultural Age of reason, which began in the century straddling 1600 as a conventional date, empiricism is the reliance on sensory data gathered in experimentation by scientists of any country, who, in the Age of Reason were rationalists. An early professed empiricist, Thomas Hobbes, known as an eccentric denizen of the court of Charles II of England (an \"old bear\"), published in 1651 \"Leviathan\", a political treatise written during the English civil war, containing an early manifesto in English of rationalism.\nHobbes said:\"The Latines called Accounts of mony Rationes ... and thence it seems to proceed that they extended the word Ratio, to the faculty of Reckoning in all other things...When a man reasoneth hee does nothing else but conceive a summe totall ... For Reason ... is nothing but Reckoning ... of the consequences of generall names agreed upon, for the marking and signifying of our thoughts ...\"\nIn Hobbes reasoning is the right process of drawing conclusions from definitions (the \"names agreed upon\"). He goes on to define error as self-contradiction of definition (\"an absurdity, or senselesse Speech\") or conclusions that do not follow the definitions on which they are supposed to be based. Science, on the other hand, is the outcome of \"right reasoning,\" which is based on \"natural sense and imagination\", a kind of sensitivity to nature, as \"nature it selfe cannot erre.\"\n\nHaving chosen his ground carefully Hobbes launches an epistemological attack on metaphysics. The academic philosophers had arrived at the Theory of Matter and Form from consideration of certain natural paradoxes subsumed under the general heading of the Unity Problem. For example, a body appears to be one thing and yet it is distributed into many parts. Which is it, one or many? Aristotle had arrived at the real distinction between matter and form, metaphysical components whose interpenetration produces the paradox. The whole unity comes from the substantial form and the distribution into parts from the matter. Inhering in the parts giving them really distinct unities are the accidental forms. The unity of the whole being is actuated by another really distinct principle, the existence.\n\nIf nature cannot err, then there are no paradoxes in it; to Hobbes, the paradox is a form of the absurd, which is inconsistency: \"Natural sense and imagination, are not subject to absurdity\" and \"For error is but a deception ... But when we make a generall assertion, unlesse it be a true one, the possibility of it is inconceivable. And words whereby we conceive nothing but the sound, are those we call Absurd ...\" Among Hobbes examples are \"round quadrangle\", \"immaterial substance\", \"free subject.\" Of the scholastics he says:\"Yet they will have us beleeve, that by the Almighty power of God, one body may be at one and the same time in many places [the problem of the universals]; and many bodies at one and the same time in one place [the whole and the parts]; ... And these are but a small part of the Incongruencies they are forced to, from their disputing philosophically, instead of admiring, and adoring of the Divine and Incomprehensible Nature ...\"\n\nThe real distinction between essence and existence, and that between form and matter, which served for so long as the basis of metaphysics, Hobbes identifies as \"the Error of Separated Essences.\" The words \"Is, or Bee, or Are, and the like\" add no meaning to an argument nor do derived words such as \"Entity, Essence, Essentially, Essentiality\", which \"are the names of nothing\" but are mere \"Signes\" connecting \"one name or attribute to another: as when we say, \"a man is a living body\", we mean not that the \"man\" is one thing, the \"living body\" another, and the \"is\", or \"being\" a third: but that the \"man\", and the \"living body\", is the same thing; ...\" Metaphysiques, Hobbes says, is \"far from the possibility of being understood\" and is \"repugnant to natural reason.\"\n\nBeing to Hobbes (and the other empiricists) is the physical universe:The world, (I mean ... the Universe, that is, the whole masse of all things that are) is corporeall, that is to say, Body; and hath the dimension of magnitude, namely, Length, Bredth and Depth: also every part of Body, is likewise Body ... and consequently every part of the Universe is Body, and that which is not Body, is no part of the Universe: and because the Universe is all, that which is no part of it is nothing; and consequently no where.\"\n\nHobbes' view is representative of his tradition. As Aristotle offered the categories and the act of existence, and Aquinas the analogy of being, the rationalists also had their own system, the great chain of being, an interlocking hierarchy of beings from God to dust.\n\nIn addition to the materialism of the empiricists, under the same aegis of Reason, rationalism produced systems that were diametrically opposed now called idealism, which denied the reality of matter in favor of the reality of mind. By a 20th-century classification, the idealists (Kant, Hegel and others), are considered the beginning of continental philosophy, while the empiricists are the beginning, or the immediate predecessors, of analytical philosophy. \n\nSome philosophers deny that the concept of \"being\" has any meaning at all, since we only define an object's existence by its relation to other objects, and actions it undertakes. The term \"I am\" has no meaning by itself; it must have an action or relation appended to it. This in turn has led to the thought that \"being\" and nothingness are closely related, developed in existential philosophy.\n\nExistentialist philosophers such as Sartre, as well as continental philosophers such as Hegel and Heidegger have also written extensively on the concept of being. Hegel distinguishes between the being of objects (being in itself) and the being of people (\"Geist)\". Hegel, however, did not think there was much hope for delineating a \"meaning\" of being, because being stripped of all predicates is simply nothing.\n\nHeidegger, in his quest to re-pose the original pre-Socratic question of Being, wondered at how to meaningfully ask the question of the meaning of being, since it is both the greatest, as it includes everything that is, and the least, since no particular thing can be said of it. He distinguishes between different modes of beings: a privative mode is present-at-hand, whereas beings in a fuller sense are described as ready-to-hand. The one who asks the question of Being is described as Da-sein (\"there/here-being\") or being-in-the-world. Sartre, popularly understood as misreading Heidegger (an understanding supported by Heidegger's essay \"Letter on Humanism\" which responds to Sartre's famous address, \"Existentialism is a Humanism\"), employs modes of being in an attempt to ground his concept of freedom ontologically by distinguishing between being-in-itself and being-for-itself.\n\nBeing is also understood as one's \"state of being,\" and hence its common meaning is in the context of human (personal) experience, with aspects that involve expressions and manifestations coming from an innate \"being\", or personal character. Heidegger coined the term \"dasein\" for this property of being in his influential work \"Being and Time\" (\"this entity which each of us is himself…we shall denote by the term 'dasein.'\"), in which he argued that being or \"dasein\" links one's sense of one's body to one's perception of world. Heidegger, amongst others, referred to an innate language as the foundation of being, which gives signal to all aspects of being.\n\n\nPhilosophers\n\n"}
{"id": "23712707", "url": "https://en.wikipedia.org/wiki?curid=23712707", "title": "Buddhism and euthanasia", "text": "Buddhism and euthanasia\n\nBuddhist views, although varying on a series of canons within the three branches of Buddhism (Theravada, Mahayana, and Vajrayana), observe the concept of euthanasia, or \"mercy killing\", in a denunciatory manner. Such methods of euthanasia include voluntary, involuntary, and non-voluntary. In the past, as one school of Buddhism evolved into the next, their scriptures recorded through the oral messages of Buddha himself on Buddhist principles and values followed, guiding approximately 500 million Buddhists spanning the globe on their path to nirvana. In the Monastic Rule, or Vinaya, a consensus is reached by the Buddha on euthanasia and assisted suicide that expresses a lack of fondness of its practice.\nBuddhism does not confirm that life should be conserved by implementing whatever is necessary to prolong death, but instead expresses that the intentional precipitation of death is ethically inadmissible in every condition one is presented in.\n\nThe Vinaya Tripitaka is one of three Buddhist canonical sources that makes up the Tripitaka that most relates to euthanasia. It was created to encompass a series of case laws in which Buddha provided judgement on various matters, even though the term \"euthanasia\" is not specifically mentioned. Monks and nuns are meant to follow the decorum that is relayed, which expresses what is considered to be wrongfully killing someone (prohibited by Buddhist precepts) by the actions of another who is conferred to follow holy orders and those who are not. Outside of Vinaya, there is no specific mention in early Buddhist texts on euthanasia. During the life of Buddha, instances occurred when monks who practiced medicine were put in situations where they had to make a decision to assist in another's suicide by physically taking their life, providing the instrument used to take their life, or allowing the person to suffer, as observed in cases written in Vinaya. The Buddha, therefore, included a precept in Vinaya against the termination of another human life following the discovery that monks either took their own life or requested that others kill them because they were unhappy with their body. Buddha stated that:\n\nWith this, Buddha later expanded the precept in the third parajika, adding the punishment of excommunication for life from the Sangha after recognizing a number of monks provoking a patient to believe that he should choose death over life. In this instance, the monks praised the idea that death was beautiful to a sick monk, persuading him to take an unrevealed measure to end his life. Because of the monk's expression of virtue, as he was informed by other monks, he would receive a good rebirth. As a result of these provocations, the monk stopped eating and later died.\n\nBuddhists believe that life begins at the point of birth and ends when the individual dies. Throughout the course of the individual's life, between life and death, they are to be respected with dignity, regardless of their state of mental capacity or psychometric functions. What constitutes life in a body is usma (heat), ayu (vitality), and vinanna (sentiency). Among Buddhists, there is much confusion as to when one is truly dead. Some consider death to be when the brain loses its functionality. And there are those who disagree with this idea. When Buddha passed away, according to ancient texts, his 25-year personal attendant Ananda declared him as dead. However, Ananda was later corrected by a monk senior to him, claiming that Buddha was only in a severe state of yogistic stupor. During this yogic trance, Buddha lacked any vital signs of life, making it unclear for future Buddhists to determine the point of death when such physiological states exist and are relayed in Buddhist literary texts. Although at death one loses all physical possessions, leaving their family, loved ones, and achievements all behind, death does not destroy all that belongs to a person. The purification of their character through virtuous and meditative practices carries over into their next life and into their mental sequence of continuation.Buddhists hold the belief that upon death, they are reborn and will experience life through a series of lifetimes called samsara until they can cease to desire and nirvana is obtained. In conjunction with a person's previously attained karma, their state of mind at the point of death holds great importance when the determining of what kind of rebirth is to take place. There are six realms of life in Buddhist cosmology: Hell realm (Naraka), hungry ghost (Preta) realm, animal realm (Tiracchānayoni), human realm (Manussa), demi-god realm (Asura), and god realm (Deva). Of all the realms that exist, the human realm is the most anticipated, yet the most difficult to obtain. Based on the level of karma one garners in their current lifetime, it is determined at which realm one diverges to next upon death. The first three realms (Hell realm, hungry ghost realm, and animal realm) are the most detested of the six and are meant to cause suffering, disallowing proper mental capabilities as a result of negative karma from negative acts performed during the previous life. And although more acceptable than the former, the asura and god realm remain unfavorable as they permit one ultimate happiness but prevents the opportunity for the spirit to progress. Only through good karma can one's spirit reach the human realm. Euthanasia, although it can be considered a compassionate act, is not seen in Buddhism as an act of selflessness and benevolence, but rather an act of damage masked as help, which in turn can result in negative karma. If someone is suffering as a result of sickness, it could be a result of karma, and ending their life is not likely to end their adversity, as the suffering caused by karma will only follow them after death until its energy concludes.\n\nCompassion (karuṇā) is a Buddhist value that reinforces how Buddhism views standards in medicine, which is observed in all three schools of Buddhism. Although it can be seen as morally good, committing an act through compassion is not always justified. Taking a life out of compassion to mitigate the affliction one experiences through illness or injury is an instance in which a person may feel he or she is vindicated to exploit euthanasia. However, in Buddhism and the Bodhisattva, it can create incongruities through the infringement of life. An example of an unjustified act of compassion is a case written in the Vinaya, where a convicted man is hastily executed after a monk requests the executioner to do so, as to not prolong his suffering and the sorrowful period he has to wait. A morally good scenario of compassion would be if a patient in a hospital in his or her right mind feels that their ailment is using too much of a valuable resource in limited quantities, or causing their family to spend large sums of money that is not readily available on medical bills to keep them alive. Out of morally good compassion, he or she might decide to cease the sources of life preservation without pressure from others. Had those associations been pressuring the patient to forgo further life saving treatment, this could be observed as murderous. If the patient were terminally ill and unable to eat on their own accord, it would be a requirement of his or her associations to assist them, even if intravenous feeding was necessary. Buddhism paints life in a way that is fundamentally valuable, and it should never be surrendered for any reason, whether compassion, amity, or anything of value. It would be an ill-advised way to show compassion rooted in misconception to endorse death using compassion as the reasoning.\n\nAccording to Buddhist precepts, someone who is in a vegetative state is neither dead nor alive, but instead is said to be in some form of obscure state of mind and still a living individual. They lack the necessity to rely on outside assistance to support their ability to stay alive other than being supplemented with nutrients to keep them functioning for a significant number of years. With this, Buddhism sees someone in a vegetative state as a living, breathing life form, because the value in which one's life holds is not observed through individualism. Although Buddhism views this state of being as damaged, the individual should be treated no different than before. Both animals and humans, prior to their birth as they await in the mother's womb, have value and therefore should not have their life taken from them. However, some Buddhists are under the impression that an unconscious life, or one that lacks awareness, does not have value, which is highly debatable. Buddhism shows that even with a damaged physical organ, such as the brain, someone is still deserving of compassion because they are still capable of causing varying emotions in someone who cares for them. To end a life because of a patient's state of mind and to deny them treatment would be an act of abandonment, which would be considered irrational and unfair because Buddhists have a strong belief in compassion for all living things.\n\nA vegetative state can be considered a varied form of meditation where the sentience of someone acts in a way not ordinary to what is regularly observed. Another form of consciousness where the body does not react is that of \"formless\" rebirths. Due to these meditative states, it is often difficult to determine in a physical manner a state of consciousness. The state of consciousness, at the time, may be experiencing a demonstration of going through the processes of death in ultimate preparation so that once death does come, it can receive the best form of rebirth. During this process of preparation, the eyes and other sense-organs will cease to function, but the person's sense of touch, brain function, and life force lie in the heart. In Buddhism, considering the conception of meditative states, some in a vegetative state are therefore still sentient. Being in a continuous state of vegetation is not the same as the process by which one prepares for death, even though it does resemble it. The possibility exists for one to lack the function to breathe and think, but still be alive.\n\nMoving someone who is terminally ill into hospice care to help mitigate physical pain with pain relievers that is not intended to end their life, allowing them to pass away painless and comfortable, is a highly considerable alternative to euthanasia. Hospice care is meant to assist a person to have, what is called a \"good death.\" When one nears death, they should be able to reflect on their life peacefully. Since 1971, the San Francisco Zen Center has offered assistance to terminally ill patients, with trained hospice employees since the inception of a hospice training program in 1987. The Buddhist Hospice Trust in the United Kingdom has been an alternative to this center since its founding in 1987. Not only does it provide volunteers to visit those nearing death, but it provides an avenue for those who lost someone they cared about to receive counsel and sympathy and is also used as a center into researching death and mourning. The purpose of such a center is to truly alleviate any amount of anxiety one could experience when approaching death, leaving known loved ones behind, and to ensure a clear and calm state of mind. Hospice care employees therefore prefer not to allow a person to die in an anesthetized, unconscious state of mind. A positive transition to a future life is best obtained when one dies with a clear conscience, free of anger, dissent, and anxiety, and in a relaxed state of being. In Buddhism, a \"good death\" is facilitated by family and friends, who do their best to ensure the dying person is untroubled and uplifted.\n\n"}
{"id": "690278", "url": "https://en.wikipedia.org/wiki?curid=690278", "title": "Choice", "text": "Choice\n\nChoice involves decision making. It can include judging the merits of multiple options and selecting one or more of them. One can make a choice between imagined options (\"What would I do if...?\") or between real options followed by the corresponding action. For example, a traveller might choose a route for a journey based on the preference of arriving at a given destination as soon as possible. The preferred (and therefore chosen) route can then follow from information such as the length of each of the possible routes, traffic conditions, etc. The arrival at a choice can include more complex motivators such as cognition, instinct, and feeling.\n\nSimple choices might include what to eat for dinner or what to wear on a Saturday morning – choices that have relatively low-impact on the chooser's life overall. More complex choices might involve (for example) what candidate to vote for in an election, what profession to pursue, a life partner, etc. – choices based on multiple influences and having larger ramifications.\n\nFreedom of choice is generally cherished, whereas a severely limited or artificially restricted choice can lead to discomfort with choosing, and possibly an unsatisfactory outcome. In contrast, a choice with excessively numerous options may lead to confusion, regret of the alternatives not taken, and indifference in an unstructured existence;\nand the illusion that choosing an object or a course, necessarily leads to the control of that object or course, can cause psychological problems.\n\nThere are four main types of decisions, although they can be expressed in different ways. Brian Tracy breaks them down into:\nA fifth type, however, or fourth if avoided and \"no-brainer\" decisions are combined as one type, is the collaborative decision, which should be made in consultation with, and by agreement of others. Collaborative Decision Making revolutionized air-traffic safety by not deferring to the captain when a lesser crew member becomes aware of a problem.\n\nAnother way of looking at decisions focuses on the thought mechanism used, is the decision:\n\nRecognizing that \"type\" is an imprecise term, an alternate way to classify types of choices is to look at outcomes and the impacted entity. For example, using this approach three types of choices would be:\n\nIn this approach, establishing the types of choices makes it possible to identify the related decisions that will influence and constrain a specific choice as well as be influenced and constrained by another choice.\n\nThere are many \"executive decision maker\" products available, such as the decision wheels and the Magic 8-Ball, which randomly produce yes/no or other \"decisions\" for someone who cannot make up their mind or just wants to delegate.\n\nA ouija board is also a delegated decision.\n\nAs a moral principle, decisions should be made by those most affected by the decision, but this is not normally applied to persons in jail, who might likely make a decision other than to remain in jail. Robert Gates cited this principle in allowing photographs of returning war dead.\n\nWhen choosing between options one must make judgments about the quality of each option's attributes. For example, if one is choosing between candidates for a job, the quality of relevant attributes such as previous work experience, college or high school GPA, and letters of recommendation will be judged for each option and the decision will likely be based on these attribute judgments. However, each attribute has a different level of \"evaluability\", that is, the extent to which one can use information from that attribute to make a judgment.\n\nAn example of a highly evaluable attribute is the SAT score. It is widely known in the United States that an SAT score below 800 is very bad while an SAT score above 1500 is exceptionally good. Because the distribution of scores on this attribute is relatively well known it is a highly evaluable attribute. Compare the SAT score to a poorly evaluable attribute, such as the number of hours spent doing homework. Most employers would not know what 10,000 hours spent doing homework means because they have no idea of the distribution of scores of potential workers in the population on this attribute.\n\nAs a result, evaluability can cause preference reversals between joint and separate evaluations. For example, Hsee, George Loewenstein, Blount & Bazerman (1999) looked at how people choose between options when they are directly compared because they are presented at the same time or when they cannot be compared because one is only given a single option. The canonical example is a hiring decision made about two candidates being hired for a programming job. Subjects in an experiment were asked to give a starting salary to two candidates, Candidate J and Candidate S. However, some viewed both candidates at the same time (joint evaluation), whereas others only viewed one candidate (separate evaluation). Candidate J had experience of 70 KY programs, and a GPA of 2.5, whereas Candidate S had experience of 10 KY programs and a GPA of 3.9. The results showed that in joint evaluation both candidates received roughly the same starting salary from subjects, who apparently thought a low GPA but high experience was approximately equal to a high GPA but low experience. However, in the separate evaluation, subjects paid Candidate S, the one with the high GPA, substantially more money. The explanation for this is that KY programs is an attribute that is difficult to evaluate and thus people cannot base their judgment on this attribute in separate evaluation.\n\nPersonal factors determine food choice. They are preference, associations, habits, ethnic heritage, tradition, values, social pressure, emotional comfort, availability, convenience, economy, image, medical conditions, and nutrition.\n\nA number of research studies in economic psychology have focused on how individual behavior differs when the choice set size (the number of choices to choose from) is low versus when it is high. Of particular interest is whether individuals are more likely to purchase a product from a large versus a small choice set. Currently, the effect of choice set size on the probability of a purchase is unclear. In some cases, large choice set sizes discourage individuals from making a choice and in other cases it either encourages them or has no effect. One study compared the allure of more choice to the tyranny of too much choice. Individuals went virtual shopping in different stores that had a randomly determined set of choices ranging from 4 to 16, with some being good choices and some being bad. Researchers found a stronger effect for the allure of more choice. However, they speculate that due to random assignment of number of choices and goodness of those choices, many of the shops with fewer choices included zero or only one option that was reasonably good, which may have made it easier to make an acceptable choice when more options were available.\n\nThere is some evidence that while greater choice has the potential to improve a person's welfare, sometimes there is such a thing as too much choice. For example, in one experiment involving a choice of free soda, individuals explicitly requested to choose from six as opposed to 24 sodas, where the only benefit from the smaller choice set would be to reduce the cognitive burden of the choice. A recent study supports this research, finding that human services workers indicated preferences for scenarios with limited options over extensive-options scenarios. As the number of choices within the extensive-options scenarios increased, the preference for limited options increased as well. Attempts to explain why choice can demotivate someone from a purchase have focuses on two factors. One assumes that perusing a larger number of choices imposes a cognitive burden on the individual. The other assumes that individuals can experience regret if they make a suboptimal choice, and sometimes avoid making a choice to avoid experiencing regret.\n\nFurther research has expanded on choice overload, suggesting that there is a paradox of choice. As increasing options are available, three problems emerge. First, there is the issue of gaining adequate information about the choices in order to make a decision. Second, having more choices leads to an escalation of expectation. When there are increased options, people’s standards for what is an acceptable outcome rise; in other words, choice “spoils you.” Third, with many options available, people may come to believe they are to blame for an unacceptable result because with so many choices, they should have been able to pick the best one. If there is one choice available, and it ends up being disappointing, the world can be held accountable. When there are many options and the choice that one makes is disappointing, the individual is responsible.\n\nHowever, a recent meta-analysis of the literature on choice overload calls such studies into question (Scheibehenne, Greigeneder, and Todd, 2010). In many cases, researchers have found no effect of choice set size on people's beliefs, feelings, and behavior. Indeed, overall, the effect of \"too many options\" is minimal at best.\n\nWhile it might be expected that it is preferable to keep one’s options open, research has shown that having the opportunity to revise one’s decisions leaves people less satisfied with the decision outcome. A recent study found that participants experienced higher regret after having made a reversible decision. The results suggest that reversible decisions cause people to continue to think about the still relevant choice options, which might increase dissatisfaction with the decision and regret.\n\nIndividual personality plays a significant role in how individuals deal with large choice set sizes. Psychologists have developed a personality test that determines where an individual lies on the satisficer-maximizer spectrum. A maximizer is one who always seeks the very best option from a choice set, and may anguish after the choice is made as to whether it was indeed the best. Satisficers may set high standards but are content with a good choice, and place less priority on making the best choice. Due to this different approach to decision-making, maximizers are more likely to avoid making a choice when the choice set size is large, probably to avoid the anguish associated with not knowing whether their choice was optimal. One study looked at whether the differences in choice satisfaction between the two are partially due to a difference in willingness to commit to one’s choices. It found that maximizers reported a stronger preference for retaining the ability to revise choices. Additionally, after making a choice to buy a poster, satisficers offered higher ratings of their chosen poster and lower ratings of the rejected alternatives. Maximizers, however, were less likely to change their impressions of the posters after making their choice which left them less satisfied with their decision.\n\nMaximizers are less happy in life, perhaps due to their obsession with making optimal choices in a society where people are frequently confronted with choice. One study found that maximizers reported significantly less life satisfaction, happiness, optimism, and self-esteem, and significantly more regret and depression, than did satisficers. In regards to buying products, maximizers were less satisfied with consumer decisions and were more regretful. They were also more likely to engage in social comparison, where they analyze their relative social standing among their peers, and to be more affected by social comparisons in which others appeared to be in higher standing than them. For example, maximizers who saw their peer solve puzzles faster than themselves expressed greater doubt about their own abilities and showed a larger increase in negative mood. On the other hand, people who refrain from taking better choices through drugs or other forms of escapism tend to be much happier in life.\n\nOthers say that there is never too much choice and that there is a difference between happiness and satisfaction: a person who tries to find better decisions will often be dissatisfied, but not necessarily unhappy since his attempts at finding better choices did improve his lifestyle (even if it wasn't the \"best decision\" he will continually try to \"incrementally improve\" the decisions he takes).\n\nChoice architecture is the process of encouraging people to make good choices through grouping and ordering the decisions in a way that maximizes successful choices and minimizes the number of people who become so overwhelmed by complexity that they abandon the attempt to choose. Generally, success is improved by presenting the smaller or simpler choices first, and by choosing and promoting sensible default options.\n\nCertain choices, as personal preferences, can be central to expressing one's concept of self-identity or values. In general, the more utilitarian an item, the less the choice says about a person's self-concept. Purely functional items, such as a fire extinguisher, may be chosen solely for function alone, but non-functional items, such as music, clothing fashions, or home decorations, may instead be chosen to express a person's concept of self-identity or associated values.\n\nSophia Rosenfeld analyses critical reactions to choice in her review\nof some of the work of Iyengar,\nBen-Porath,\nGreenfield,\nand Salecl.\n\n\n\n"}
{"id": "56221934", "url": "https://en.wikipedia.org/wiki?curid=56221934", "title": "Dataism", "text": "Dataism\n\nDataism is a term that has been used to describe the mindset or philosophy created by the emerging significance of Big Data. It was first used by David Brooks in the New York Times in 2013. More recently, the term has been expanded to describe what social scientist Yuval Noah Harari has called an emerging ideology or even a new form of religion, in which 'information flow' is the 'supreme value'.\n\n\"If you asked me to describe the rising philosophy of the day, I'd say it is Data-ism\", wrote David Brooks in the New York Times in February 2013. Brooks argued that in a world of increasing complexity, relying on data can reduce cognitive biases and \"illuminate patterns of behavior we haven't yet noticed\".\n\nIn 2015, Steve Lohr's book 'Data-ism' looked at how Big Data is transforming society, using the term to describe the Big Data revolution.\n\nIn his 2016 book, , Yuval Noah Harari takes the idea of Dataism further, putting it into a historical context. He argues that all competing political or social structures can be seen as data processing systems: \"Dataism declares that the universe consists of data flows, and the value of any phenomenon or entity is determined by its contribution to data processing\".\n\nHarari posits that \"we may interpret the entire human species as a single data processing system, with individual humans serving as its chips.\" He then argues that the whole of human history can be read as a process of improving the efficiency of this system by increasing the number and variety of processors/chips (humans) in the system, increasing the number of connections between the processors and increasing the freedom of movement along existing connections. A condensed form of this argument can be read in Harari's Wired article from 2016.\n\nHarari goes on to argue that Dataism, like any other religion, has practical commandments. A Dataist should want to \"maximise dataflow by connecting to more and more media\", and believes that freedom of information is \"the greatest good of all\". Harari also argues that Aaron Swartz, who took his life in 2013 after being prosecuted for releasing hundreds of thousands of scientific papers from the JSTOR archive online for free, could be called the \"first martyr\" of Dataism.\n\nWriting in the Financial Times, Harari argued that Dataism presents an existential challenge to the dominant moral ideology of Humanism, which sees human feelings as the ultimate authority in the world: \"humanism is now facing an existential challenge and the idea of “free will” is under threat... Once Big Data systems know me better than I know myself, authority will shift from humans to algorithms.\" Harari predicts that the logical conclusion of this process is that eventually humans will give algorithms the authority to make the most important decisions in their lives, such as who to marry and which career to pursue.\n\nCommenting on Harari's characterisation of Dataism, security analyst Daniel Miessler believes that Dataism does not present the challenge to the ideology of liberal humanism that Harari claims, because humans will simultaneously be able to believe in their own importance and that of data.\n\nHarari himself raises some criticisms, such as the problem of consciousness, which Dataism is unlikely to illuminate. Humans may also find out that organisms are not algorithms, he suggests.\n\nOther analysts such as Terry Ortleib have looked at the extent to which Dataism poses a dystopian threat to humanity.\n\n\n\n"}
{"id": "31537853", "url": "https://en.wikipedia.org/wiki?curid=31537853", "title": "Death anxiety (psychology)", "text": "Death anxiety (psychology)\n\nDeath anxiety is anxiety caused by thoughts of death. One source defines death anxiety as a \"feeling of dread, apprehension or solicitude (anxiety) when one thinks of the process of dying, or ceasing to 'be'\". Also referred to as thanatophobia (fear of death), death anxiety is distinguished from necrophobia, which is a specific fear of dead or dying people and/or things (i.e., fear of others who are dead or dying, not of one's own death or dying).\n\nAdditionally, there is anxiety caused by death-recent thought-content, which might be classified within a clinical setting by a psychiatrist as morbid and/or abnormal, which for classification pre-necessitates a degree of anxiety which is persistent and interferes with everyday functioning. Lower ego integrity, more physical problems and more psychological problems are predictive of higher levels of death anxiety in elderly people perceiving themselves close to death.\n\nDeath anxiety can cause extreme timidness with a person's attitude towards discussing organ donation and anything to do with death.\n\nRobert Langs distinguishes three types of death anxiety:\n\nPredatory death anxiety arises from the fear of being harmed. It is the most basic and oldest form of death anxiety, with its origins in the first unicellular organisms’ set of adaptive resources. Unicellular organisms have receptors that have evolved to react to external dangers, along with self-protective, responsive mechanisms made to increase the likelihood of survival in the face of chemical and physical forms of attack or danger. In humans, predatory death anxiety is evoked by a variety of danger situations that put one at risk or threaten one's survival. These traumas may be physical, psychological, or both. Predatory death anxiety mobilizes an individual's adaptive resources and leads to a fight-or-flight response: active efforts to combat the danger or attempts to escape the threatening situation.\n\nPredation or predator death anxiety is a form that arises when an individual harms another, physically and/or mentally. This form of death anxiety is often accompanied by unconscious guilt. This guilt, in turn, motivates and encourages a variety of self-made decisions and actions by the perpetrator of harm to others.\n\nExistential death anxiety stems from the basic knowledge that human life must end. Existential death anxiety is known to be the most powerful form. It is said that language has created the basis for existential death anxiety through communicative and behavioral changes. Other factors include an awareness of the distinction between self and others, a full sense of personal identity, and the ability to anticipate the future.\nAwareness of human mortality arose some 150,000 years ago. In that extremely short span of evolutionary time, humans have fashioned a single basic mechanism through which they deal with the existential death anxieties this awareness has evoked—denial. Denial is effected through a wide range of mental mechanisms and physical actions, many of which go unrecognized. While denial can be adaptive in limited use, excessive use is more common and is emotionally costly. Denial is the root of such diverse actions as breaking rules, violating frames and boundaries, manic celebrations, directing violence against others, attempting to gain extraordinary wealth and power—and more. These pursuits are often activated by a death-related trauma, and while they may lead to constructive actions, more often than not, they lead to actions that are damaging to self and others.\n\nSigmund Freud hypothesized that people express a fear of death, called thanatophobia. He said he saw this as a disguise for a deeper source of concern. It was not actually death that people feared, because in Freud's view nobody believes in their own death. The unconscious does not deal with the passage of time or with negations, which does not calculate amount of time left in one's life. Furthermore, that which one does fear cannot be death itself, because one has never died. People who express death-related fears, actually are trying to deal with unresolved childhood conflicts that they cannot come to terms with or express emotion towards. The name Thanatophobia is made from the Greek figure of death known as Thanatos.\n\nDevelopmental psychologist Erik Erikson formulated the psychosocial theory that explained that people progress through a series of crises as they grow older. The theory also envelops the concept that once an individual reaches the latest stages of life, they reach the level he titled as \"ego integrity\". Ego Integrity is when one comes to terms with their life and accepts it. It was also suggested that when a person reaches the stage of late adulthood they become involved in a thorough overview of their life to date. When one can find meaning or purpose in their life, they have reached the integrity stage. In opposition, when an individual views their life as a series of failed and missed opportunities, then they do not reach the ego integrity stage. Elders that have attained this stage of ego integrity are believed to exhibit less of an influence from death anxiety.\n\nErnest Becker based this theory on existential views which turned death anxiety theories towards a new dimension. It said that death anxiety is not only real, but also it is people's most profound source of concern. He explained the anxiety as so intense that it can generate fears and phobias of everyday life—Fears of being alone or in a confined space. Based on the theory, many of people's daily behavior consist of attempts to deny death and to keep their anxiety under strict regulation.\n\nAs an individual develops mortality salience, i.e. becomes more aware of the inevitability of death, they will instinctively try to suppress it out of fear. The method of suppression usually leads to mainstreaming towards cultural beliefs, leaning for external support rather than treading alone. This behavior may range from simply thinking about death to severe phobias and desperate actions.\n\nMohammad Samir Hossain postulated the Death and adjustment hypotheses. With the declaration of the hypotheses, two things were postulated. The first part of the hypotheses theorizes that death should not be considered the end of existence. The next segment states the belief that the immortal pattern of human existence can only be adopted in a morally rich life with the attitude towards morality and materialism balanced mutually.\n\nMartin Heidegger, the German philosopher, on the one hand showed death as something conclusively determined, in the sense that it is inevitable for every human being, while on the other hand, it unmasks its indeterminate nature via the truth that one never knows when or how death is going to come. Heidegger does not engage in speculation about whether being after death is possible. He argues that all human existence is embedded in time: past, present, future, and when considering the future, we encounter the notion of death. This then creates angst. Angst can create a clear understanding in one that death is a possible mode of existence, which Heidegger described as “clearing”. Thus, angst can lead to a freedom about existence, but only if we can stop denying our mortality (as expressed in Heidegger's terminology as “stop denying being-for-death”).\n\nPaul T. P. Wong's work on the meaning management theory indicates that human reactions to death are complex, multifaceted and dynamic. His “Death Attitude Profile” identifies three types of death acceptances as Neutral, Approach, and Escape acceptances. Apart from acceptances, his work also represents different aspects of the meaning of death fear that are rooted in the bases of death anxiety. The ten meanings he proposes are finality, uncertainty, annihilation, ultimate loss, life flow disruption, leaving the loved ones, pain and loneliness, prematurity and violence of death, failure of life work completion, judgment and retribution centered.\n\nOther theories on death anxiety were introduced in the late part of the twentieth century. The existential approach, with theorists such as Rollo May and Viktor Frankl, views an individual's personality as being governed by the continuous choices and decisions in relation to the realities of life and death. Another approach is the \"regret theory\" which was introduced by Adrian Tomer and Grafton Eliason. The main focus of the theory is to target the way people evaluate the quality and/or worth of their lives. The possibility of death usually makes people more anxious if they feel that they have not and cannot accomplish any positive task in the life that they are living. Research has tried to unveil the factors that might influence the amount of anxiety people experience in life.\n\nHumans develop meanings and associate them with objects and events in their environment, provoking certain emotions within an individual. People tend to develop personal meanings of death which could accordingly be negative or positive for the individual. If they are positive, then the consequences of those meanings can be comforting (for example, ideas of a rippling effect left on those still alive). If negative they can cause emotional turmoil. Depending on the certain meaning one has associated with death, the consequences will vary accordingly whether they are negative or positive meanings.\n\nThe thought of death causes a different degree of anxiety for different individuals, depending on many factors.\n\nA 2012 study involving Christian and Muslim college students from the US, Turkey, and Malaysia found that their religiosity was positively correlated with an increased fear of death.\n\nOther studies have found a strong sense of religion in a person's life can be related to a lower sense of anxiety towards death. Although there has been no association discovered between religiosity and death anxiety, it has also been shown that death anxiety tends to be lower in individuals who regularly attend religious meetings or gatherings. On a recent study, one hundred and sixty-five church participants have been asked to fill out the \"Intrinsic Religious Motivation Scale, the Revised Death Anxiety Scale\" and the results were analyzed using factor analyses, Pearson correlation, and linear and quadratic regression. All found an inverse relationship between intrinsic religious motivation and death anxiety. In short, the more religious you are, the less anxious you are about death because you may associate death with another beginning that is promised through many religions. The study also found that gender did not have an effect on religiosity and total death anxiety.\n\nThe earliest documentation of the fear of death has been found in children as young as age 5. Psychological measures and reaction times were used to measure fear of death in young children. Recent studies that assess fear of death in children use questionnaire rating scales. There are many tests to study this including The Death Anxiety Scale for Children (DASC) developed by Schell and Seefeldt. However the most common version of this test is the revised Fear Survey Schedule for Children (FSSC-R). The FSSC-R describes specific fearful stimuli and children are asked to rate the degree to which the scenario/item makes them anxious or fearful. The most recent version of the FSSC-R presents the scenarios in a pictorial form to children as young as 4. It is called the Koala Fear Questionnaire (KFQ). The fear studies show that children's fears can be grouped into five categories. One of these categories is death and danger. This response was found amongst children age 4 to 6 on the KFQ, and from age 7 to 10. Death is the most commonly feared item and remains the most commonly feared item throughout adolescence.\n\nA study of 90 children, aged 4–8, done by Virginia Slaughter and Maya Griffiths showed that a more mature understanding of the biological concept of death was correlated to a decreased fear of death. This may suggest that it is helpful to teach children about death (in a biological sense), in order to alleviate the fear.\n\nThere has been much literature that supports the existence of a correlation between one's state of coping skills, mental health, emotions and cognitive reactions to stressful events, and one's ability to regulate affect concerning one's death anxiety. A series of tests determined that significantly high levels of death anxiety tend to occur in close relationships with an intimate partner (more so amongst females than males).\n\nThe connection between death anxiety and one's sex appears to be strong. Studies show that females tend to have more death anxiety than males. Thorson and Powell (1984) did a study to investigate this connection, and they sampled men and women from 16 years of age to over 60. The Death Anxiety Scale showed higher mean scores for women than for men. Moreover, researchers believe that age and culture could be major influences in why women score higher on death anxiety scales than men.\n\nThrough the evolutionary period, a basic method was created to deal with death anxiety and also as a means of dealing with loss. Denial is used when memories or feelings are too painful to accept and are often rejected. By maintaining that the event never happened, rather than accepting it, allows an individual more time to work through the inevitable pain. When a loved one dies in a family, denial is often implemented as a means to come to grips with the reality that the person is gone. Closer families often deal with death better than when coping individually. As society and families drift apart so does the time spent bereaving those who have died, which in turn leads to negative emotion and negativity towards death. Women, who are the child bearers and are often the ones who look after children hold greater concerns about death due to their caring role within the family. It is this common role of women that leads to greater death anxiety as it emphasize the ‘importance to live’ for her offspring. Although it is common knowledge that all living creatures die, many people do not accept their own mortality, preferring not to accept that death is inevitable, and that they will one day die.\n\nIt is during the years of young adulthood (20 to 40 years of age) that death anxiety most often begins to become prevalent. However, during the next phase of life, the middle age adult years (40–64 years of age), death anxiety peaks at its highest levels when in comparison to all other age ranges throughout the lifespan. Surprisingly, levels of death anxiety then slump off in the old age years of adulthood (65 years of age and older). This is in contrast with most people's expectations, especially regarding all of the negative connotations younger adults have about the elderly and the aging process (Kurlychek & Trenner, 1982).\n\nThere are many ways to measure death anxiety and fear. Katenbaum and Aeinsberg (1972) devised three propositions for this measurement. From this start, the ideologies about death anxiety have been able to be recorded and their attributes listed. Methods such as imagery tasks to simple questionnaires and apperception tests such as the Stroop test enable psychologists to adequately determine if a person is under stress due to death anxiety or suffering from post-traumatic stress disorder.\nThe Lester attitude death scale was developed in 1966 but not published until 1991 until its validity was proven. By measuring the general attitude towards death and also the inconsistencies with death attitudes, participants are scaled to their favorable value towards death.\n\n\n\n"}
{"id": "3157035", "url": "https://en.wikipedia.org/wiki?curid=3157035", "title": "Death drive", "text": "Death drive\n\nIn classical Freudian psychoanalytic theory, the death drive () is the drive toward death and self-destruction. It was originally proposed by Sabina Spielrein in her paper \"Destruction as the Cause of Coming Into Being\" (\"Die Destruktion als Ursache des Werdens\") in 1912, which was then taken up by Sigmund Freud in 1920 in \"Beyond the Pleasure Principle.\" This concept has been translated as \"opposition between the ego or death instincts and the sexual or life instincts\". In \"Pleasure Principle\", Freud used the plural \"death drives\" (\"Todestriebe\") much more frequently than in the singular.\n\nThe death drive opposes Eros, the tendency toward survival, propagation, sex, and other creative, life-producing drives. The death drive is sometimes referred to as \"Thanatos\" in post-Freudian thought, complementing \"Eros\", although this term was not used in Freud's own work, being rather introduced by Wilhelm Stekel in 1909 and then by Paul Federn in the present context.\n\nThe Standard Edition of Freud's works in English confuses two terms that are different in German, \"Instinkt\" (\"instinct\") and \"Trieb\" (\"drive\"), often translating both as \"instinct\"; for example, \"the hypothesis of a \"death instinct\", the task of which is to lead organic life back into the inanimate state\". \"This equating of \"instinkt\" and \"Trieb\" has created serious misunderstandings\". Freud actually refers to the term \"instinkt\" in explicit use elsewhere, and so while the concept of \"instinct\" can loosely be referred to as a \"drive,\" any essentialist or naturalist connotations of the term should be put in abeyance. In a sense, the death drive is a force that is not essential to the life of an organism (unlike an \"instinct\") and tends to denature it or make it behave in ways that are sometimes counter-intuitive. In other words, the term death \"drive\" is simply a false representation of death instinct. The term is almost universally known in scholarly literature on Freud as the \"death drive\", and Lacanian psychoanalysts often shorten it to simply \"drive\" (although Freud posited the existence of other drives as well, and Lacan explicitly states in Seminar XI that all drives are partial to the death drive). The contemporary Penguin translations of Freud translate \"Trieb\" and \"Instinkt\" as \"drive\" and \"instinct\" respectively.\n\nIt was a basic premise of Freud's that \"the course taken by mental events is automatically regulated by the pleasure principle...[associated] with an avoidance of unpleasure or a production of pleasure\". Three main types of conflictual evidence, difficult to explain satisfactorily in such terms, led Freud late in his career to look for another principle in mental life \"beyond\" the pleasure principle—a search that would ultimately lead him to the concept of the death drive.\n\nThe first problem Freud encountered was the phenomenon of repetition in (war) trauma. When Freud worked with people with trauma (particularly the trauma experienced by soldiers returning from World War I), he observed that subjects often tended to repeat or re-enact these traumatic experiences: \"dreams occurring in traumatic have the characteristic of repeatedly bringing the patient back into the situation of his accident\", contrary to the expectations of the pleasure principle.\n\nA second problematic area was found by Freud in children's play (such as the \"Fort/Da\" [Forth/here] game played by Freud's grandson, who would stage and re-stage the disappearance of his mother and even himself). \"How then does his repetition of this distressing experience as a game fit in with the pleasure principle?\"\n\nThe third problem came from clinical practice. Freud found his patients, dealing with painful experiences that had been repressed, regularly \"obliged to \"repeat\" the repressed material as a contemporary experience instead of [...] \"remembering\" it as something belonging to the past\". Combined with what he called \"the compulsion of destiny [...] come across [in] people all of whose human relationships have the same outcome\", such evidence led Freud \"to justify the hypothesis of a compulsion to repeat—something that would seem more primitive, more elementary, more instinctual than the pleasure principle which it over-rides\".\n\nHe then set out to find an explanation of such a compulsion; in Freud's own words, \"What follows is speculation, often far-fetched speculation, which the reader will consider or dismiss according to his individual predilection\". Seeking a new instinctual paradigm for such problematic repetition, he found it ultimately in \"an urge in organic life to restore an earlier state of things\"—the inorganic state from which life originally emerged. From the conservative, restorative character of instinctual life, Freud derived his death drive, with its \"pressure towards death\", and the resulting \"separation of the death instincts from the life instincts\" seen in Eros. The death drive then manifested itself in the individual creature as a force \"whose function is to assure that the organism shall follow its own path to death\".\n\nSeeking further potential clinical support for the existence of such a self-destructive force, Freud found it through a reconsideration of his views of masochism—previously \"regarded as sadism that has been turned round upon the subject's own ego\"—so as to allow that \"there \"might\" be such a thing as primary masochism—a possibility which I had contested\" before. Even with such support, however, he remained very tentative to the book's close about the provisional nature of his theoretical construct: what he called \"the whole of our artificial structure of hypotheses\".\n\nAlthough Spielrein's paper was published in 1912, Freud initially resisted the concept as he considered it to be too Jungian. Nevertheless, Freud eventually adopted the concept, and in later years would build extensively upon the tentative foundations he had set out in \"Beyond the Pleasure Principle\". In \"The Ego and the Id\" (1923) he would develop his argument to state that \"the death instinct would thus seem to express itself—though probably only in part—as an \"instinct of destruction\" directed against the external world\". The following year he would spell out more clearly that the \"libido has the task of making the destroying instinct innocuous, and it fulfils the task by diverting that instinct to a great extent outwards [...]. The instinct is then called the destructive instinct, the instinct for mastery, or the will to power\", a perhaps much more recognisable set of manifestations.\n\nAt the close of the decade, in \"Civilization and Its Discontents\" (1930), Freud acknowledged that \"To begin with it was only tentatively that I put forward the views I have developed here, but in the course of time they have gained such a hold upon me that I can no longer think in any other way\".\n\nFrom a philosophical perspective, the death drive may be viewed in relation to the work of the German philosopher Arthur Schopenhauer. His philosophy, expounded in \"The World as Will and Representation\" (1818) postulates that all exists by a metaphysical \"will\" (more clearly, a will to live), and that pleasure affirms this will. Schopenhauer's pessimism led him to believe that the affirmation of the \"will\" was a negative and immoral thing, due to his belief of life producing more suffering than happiness. The death drive would seem to manifest as a natural and psychological negation of the \"will\".\n\nFreud was well aware of such possible linkages. In a letter of 1919, he wrote that regarding \"the theme of death, [that I] have stumbled onto an odd idea via the drives and must now read all sorts of things that belong to it, for instance Schopenhauer\". Indeed, Ernest Jones (who like many analysts was not convinced of the need for the death drive, over and above an instinct of aggression) considered that \"Freud seemed to have landed in the position of Schopenhauer, who taught that 'death is the goal of life'\".\n\nHowever, as Freud put it to the imagined auditors of his \"New Introductory Lectures\" (1932), \"You may perhaps shrug your shoulders and say: \"That isn't natural science, it's Schopenhauer's philosophy!\" But, ladies and gentlemen, why should not a bold thinker have guessed something that is afterwards confirmed by sober and painstaking detailed research?\" He then went on to add that \"what we are saying is not even genuine Schopenhauer...we are not overlooking the fact that there is life as well as death. We recognise two basic instincts and give each of them its own aim\".\n\nFreud applied his new theoretical construct in \"Civilization and Its Discontents\" (1930) to the difficulties inherent in Western civilization—indeed, in civilization and in social life as a whole. In particular, given that \"a portion of the [death] instinct is diverted towards the external world and comes to light as an instinct of aggressiveness', he saw 'the inclination to aggression [...] [as] the greatest impediment to civilization\". The need to overcome such aggression entailed the formation of the [cultural] superego: \"We have even been guilty of the heresy of attributing the origin of conscience to this diversion inwards of aggressiveness\". The presence thereafter in the individual of the superego and a related sense of guilt—\"Civilization, therefore, obtains mastery over the individual's dangerous desire for aggression by [...] setting up an agency within him to watch over it\"—leaves an abiding sense of uneasiness inherent in civilized life, thereby providing a structural explanation for 'the suffering of civilized man'.\n\nFreud made a further connection between group life and innate aggression, where the former comes together more closely by directing aggression to other groups, an idea later picked up by group analysts like Wilfred Bion.\n\nIn the closing decade of Freud's life, it has been suggested, his view of the death drive changed somewhat, with \"the stress much more upon the death instinct's manifestations \"outwards\"\". Given \"the ubiquity of non-erotic aggressivity and destructiveness\", he wrote in 1930, \"I adopt the standpoint, therefore, that the inclination to aggression is an original, self-subsisting instinctual disposition in man\".\n\nIn 1933 he conceded of his original formulation of the death drive 'the improbability of our speculations. A queer instinct, indeed, directed to the destruction of its own organic home!'. He wrote moreover that \"Our hypothesis is that there are two essentially different classes of instincts: the sexual instincts, understood in the widest sense—Eros, if you prefer that name—and the aggressive instincts, whose aim is destruction\". In 1937, he went so far as to suggest privately that 'We should have a neat schematic picture if we supposed that originally, at the beginning of life, all libido was directed to the inside and all aggressiveness to the outside'. In his last writings, it was the contrast of \"two basic instincts, \"Eros\" and \"the destructive instinct\" [...] our two primal instincts, \"Eros\" and \"destructiveness\", on which he laid stress. Nevertheless, his belief in \"the death instinct [...] [as] a return to an earlier state [...] into an inorganic state\" continued to the end.\n\nAs Freud wryly commented in 1930, \"The assumption of the existence of an instinct of death or destruction has met with resistance even in analytic circles\". Indeed, Ernest Jones would comment of \"Beyond the Pleasure Principle\" that the book not only \"displayed a boldness of speculation that was unique in all his writings\" but was \"further noteworthy in being the only one of Freud's which has received little acceptance on the part of his followers\".\n\nOtto Fenichel in his compendious survey of the first Freudian half-century concluded that \"the facts on which Freud based his concept of a death instinct in no way necessitate the assumption [...] of a genuine self-destructive instinct\". Heinz Hartmann set the tone for ego psychology when he \"chose to [...] do without 'Freud's other, mainly biologically oriented set of hypotheses of the \"life\" and \"death instincts\"'\". In the object relations theory, among the independent group 'the most common repudiation was the loathsome notion of the death instinct'. Indeed, \"for most analysts Freud's idea of a primitive urge towards death, of a primary masochism, was [...] bedevilled by problems\".\n\nNevertheless, the concept has been defended, extended, and carried forward by some analysts, generally those tangential to the psychoanalytic mainstream; while among the more orthodox, arguably of \"those who, in contrast to most other analysts, take Freud's doctrine of the death drive seriously, K. R. Eissler has been the most persuasive—or least unpersuasive\".\n\nMelanie Klein and her immediate followers considered that \"the infant is exposed from birth to the anxiety stirred up by the inborn polarity of instincts—the immediate conflict between the life instinct and the death instinct\"; and Kleinians indeed built much of their theory of early childhood around the outward deflection of the latter. \"This deflection of the death instinct, described by Freud, in Melanie Klein's view consists partly of a projection, partly of the conversion of the death instinct into aggression\".\n\nFrench psychoanalyst Jacques Lacan, for his part, castigated the \"refusal to accept this culminating point of Freud's doctrine [...] by those who conduct their analysis on the basis of a conception of the \"ego\" [...] that death instinct whose enigma Freud propounded for us at the height of his experience\". Characteristically, he stressed the linguistic aspects of the death drive: \"the symbol is substituted for death in order to take possession of the first swelling of life [...]. There is therefore no further need to have recourse to the outworn notion of primordial masochism in order to understand the reason for the repetitive games in [...] his \"Fort!\" and in his \"Da!\".\"\n\nEric Berne too would proudly proclaim that he, \"besides having repeated and confirmed the conventional observations of Freud, also believes right down the line with him concerning the death instinct, and the pervasiveness of the repetition compulsion\".\n\nFor the twenty-first century, \"the death drive today [...] remains a highly controversial theory for many psychoanalysts [...] [almost] as many opinions as there are psychoanalysts\".\n\nFreud's conceptual opposition of death and eros drives in the human psyche was applied by Walter A. Davis in \"Deracination: Historicity, Hiroshima, and the Tragic Imperative\" and \"Death's Dream Kingdom: The American Psyche since 9/11.\" Davis described social reactions to both Hiroshima and 9/11 from the Freudian viewpoint of the death force. Unless they consciously take responsibility for the damage of those reactions, Davis claims that Americans will repeat them.\n\n\n"}
{"id": "9567", "url": "https://en.wikipedia.org/wiki?curid=9567", "title": "Egoism", "text": "Egoism\n\nEgoism is an ethical theory that treats self-interest as the foundation of morality.\n\nLatin word \"ego\" meaning \"I\".\n\n\nThe terms \"egoism\" and \"egotism\" may also refer to:\n\n\n"}
{"id": "41185116", "url": "https://en.wikipedia.org/wiki?curid=41185116", "title": "Eternal oblivion", "text": "Eternal oblivion\n\nIn philosophy, eternal oblivion (also referred to as non-existence or nothingness) is the permanent cessation of one's consciousness upon death. This concept is often associated with religious skepticism and atheism, and may be based in part on the lack of objective evidence for an afterlife and identifying fallacies in the arguments for an afterlife such as Occam's razor.\n\nAccording to contemporary theories of consciousness, the brain is the basis of subjective experience, agency, self-awareness, and awareness of the surrounding natural world. When brain death occurs, all brain function permanently ceases. Many people who believe that death is a permanent cessation of consciousness also believe that consciousness is dependent upon the functioning of the brain. Scientific research has discovered that some areas of the brain, like the reticular activating system or the thalamus, appear to be necessary for consciousness, because damage to these structures or their lack of function causes a loss of consciousness.\n\nThrough a naturalist analysis of the mind (an approach adopted by many philosophers of mind and neuroscientists), it is regarded as being dependent on the brain, as shown from the various effects of brain damage.\n\nThe English word \"oblivion\" (late 14c.) comes from the Old French \"oblivion\" (13c.) and directly from the Latin \"oblivionem\", meaning \"forgetfulness; a being forgotten\", which also comes from the word \"oblivisci\" (\"to forget\"). Oblivion itself means \"state of being forgotten\".\n\nThomas W. Clark, founder of Center for Naturalism, wrote a paper titled \"Death, Nothingness, and Subjectivity\". He critiqued what he saw as a flawed description of eternal oblivion as a \"plunge into darkness\". When some imagine their deaths (including the non-religious), they project themselves into a future self which experiences an eternal silent darkness. This is wrong, because without consciousness, there is no awareness of space and no basis for time. For Clark, in oblivion there isn't even an absence of experience, as we can only speak of experience when a subjective self exists.\n\nAccording to neuroscientist Giulio Tononi, consciousness is \"all we are and all we have: lose consciousness and, as far as you are concerned, your own self and the entire world dissolve into nothingness.\"\n\nSteven Pinker explains the following in the seminar Is Science Killing the Soul, a discussion with Richard Dawkins presenting evidence against the existence of souls. He elaborates;\n\nIn his book \"The Big Picture\", physicist Sean Carroll asserts that through physics we can demonstrate that death is equivalent to eternal oblivion. He writes;\n\nIn the \"Apology of Socrates\" (written by Plato), after Socrates is sentenced to death, he addresses the court. He ponders the nature of death, and summarizes that there are basically two opinions about it. The first is that it is a migration of the soul or consciousness from this existence into another, and that the souls of all previously deceased people will also be there. This excites Socrates, because he will be able to conduct his dialectic inquiries with all of the great Greek heroes and thinkers of the past. The other opinion about death is that it is oblivion, the complete cessation of consciousness, not only unable to feel but a complete lack of awareness, like a person in a deep, dreamless sleep. Socrates says that even this oblivion does not frighten him very much, because while he would be unaware, he would correspondingly be free from any pain or suffering. Indeed, Socrates says, not even the great King of Persia could say that he ever rested so soundly and peacefully as he did in a dreamless sleep.\n\nCicero, writing three centuries later in his treatise \"On Old Age\", in the voice of Cato the Elder, similarly discussed the prospects of death, frequently referring to the works of earlier Greek writers. Cicero also concluded that death was either a continuation of consciousness or cessation of it, and that if consciousness continues in some form, there is no reason to fear death; while if it is in fact eternal oblivion, he will be free of all worldly miseries, in which case he should also not be deeply troubled by death.\n\nSimilar thoughts about death were expressed by the Roman poet and philosopher Lucretius in his first-century BC didactic poem \"De rerum natura\" and by the ancient Greek philosopher Epicurus in his \"Letter to Menoeceus\", in which he writes;\n\nParaphrasing philosopher Paul Edwards, Keith Augustine and Yonatan I. Fishman note that \"the greater the damage to the brain, the greater the corresponding damage to the mind. The natural extrapolation from this pattern is all too clear — obliterate brain functioning altogether, and mental functioning too will cease\".\n\n\n"}
{"id": "4709473", "url": "https://en.wikipedia.org/wiki?curid=4709473", "title": "Gametangium", "text": "Gametangium\n\nA gametangium (plural: gametangia) is an organ or cell in which gametes are produced that is found in many multicellular protists, algae, fungi, and the gametophytes of plants. In contrast to gametogenesis in animals, a gametangium is a haploid structure and formation of gametes does not involve meiosis.\n\nDepending on the type of gamete produced in a gametangium, several types can be distinguished.\n\nFemale gametangia are most commonly called archegonia. They produce egg cells and are the sites for fertilization. Archegonia are common in algae and primitive plants as well as gymnosperms. In flowering plants, they are replaced by the embryo sac inside the ovule.\n\nThe male gametangia are most commonly called antheridia. They produce sperm cells that they release for fertilization. Antheridia producing non-motile sperm (spermatia) are called spermatangia. Some antheridia do not release their sperm. For example, the oomycete antheridium is a syncytium with many sperm nuclei and fertilization occurs via fertilization tubes growing from the antheridium and making contact with the egg cells. Antheridia are common in the gametophytes in \"lower\" plants such as bryophytes, ferns, cycads and ginkgo. In \"higher\" plants such as conifers and flowering plants, they are replaced by pollen grains.\n\nIn isogamy, the gametes look alike and cannot be classified into \"male\" or \"female.\" For example, in zygomycetes, two gametangia (single multinucleate cells at the end of hyphae) form good contact with each other and fuse into a zygosporangium. Inside the zygosporangium, the nuclei from each of the original two gametangia pair up.\n\n"}
{"id": "436824", "url": "https://en.wikipedia.org/wiki?curid=436824", "title": "Gerontology", "text": "Gerontology\n\nGerontology is the study of the social, cultural, psychological, cognitive, and biological aspects of ageing. The word was coined by Ilya Ilyich Mechnikov in 1903, from the Greek γέρων, \"geron\", \"old man\" and -λογία, \"-logia\", \"study of\". The field is distinguished from geriatrics, which is the branch of medicine that specializes in the treatment of existing disease in older adults. Gerontologists include researchers and practitioners in the fields of biology, nursing, medicine, criminology, dentistry, social work, physical and occupational therapy, psychology, psychiatry, sociology, economics, political science, architecture, geography, pharmacy, public health, housing, and anthropology.\n\nThe multidisciplinary nature of gerontology means that there are a number of sub-fields which overlap with gerontology. There are policy issues, for example, involved in government planning and the operation of nursing homes, investigating the effects of an ageing population on society, and the design of residential spaces for older people that facilitate the development of a sense of place or home. Dr. Lawton, a behavioral psychologist at the Philadelphia Geriatric Center, was among the first to recognize the need for living spaces designed to accommodate the elderly, especially those with Alzheimer's disease. As an academic discipline the field is relatively new. The USC Leonard Davis School created the first PhD, master's and bachelor's degree programs in gerontology in 1975.\n\nIn the medieval Islamic world, several physicians wrote on issues related to Gerontology. Avicenna's \"The Canon of Medicine\" (1025) offered instruction for the care of the aged, including diet and remedies for problems including constipation. Arabic physician Ibn Al-Jazzar Al-Qayrawani (Algizar, c. 898–980) wrote on the aches and conditions of the elderly (Ammar 1998, p. 4). His scholarly work covers sleep disorders, forgetfulness, how to strengthen memory, and causes of mortality Ishaq ibn Hunayn (died 910) also wrote works on the treatments for forgetfulness (U.S. National Library of Medicine, 1994).\n\nWhile the number of aged humans, and the life expectancy, tended to increase in every century since the 14th, society tended to consider caring for an elderly relative as a family issue. It was not until the coming of the Industrial Revolution that ideas shifted in favor of a societal care-system. Some early pioneers, such as Michel Eugène Chevreul, who himself lived to be 102, believed that aging itself should be a science to be studied. Élie Metchnikoff coined the term \"gerontology\" 1903.\n\nModern pioneers like James Birren began organizing gerontology as its own field in the 1940s, later being involved in starting a US government agency on aging – the National Institute on Aging – programs in gerontology at the University of Southern California and University of California, Los Angeles, and as past president of the Gerontological Society of America (founded in 1945).\n\nWith the population of people over 60 years old expected to be some 22% of the world's population by 2050, assessment and treatment methods for age-related disease burden – the term \"geroscience\" emerged in the early 21st century.\n\nThe world is forecast to undergo rapid population aging in the next several decades. In 1900, there were 3.1 million people aged 65 years and older living in the United States. However, this population continued to grow throughout the 20th century and reached 31.2, 35, and 40.3 million people in 1990, 2000, and 2010, respectively. Notably, in the United States and across the world, the \"baby boomer\" generation began to turn 65 in 2011. Recently, the population aged 65 years and older has grown at a faster rate than the total population in the United States. The total population increased by 9.7%, from 281.4 million to 308.7 million, between 2000 and 2010. However, the population aged 65 years and older increased by 15.1% during the same period. It has been estimated that 25% of the population in the United States and Canada will be aged 65 years and older by 2025. Moreover, by 2050, it is predicted that, for the first time in United States history, the number of individuals aged 60 years and older will be greater than the number of children aged 0 to 14 years. Those aged 85 years and older (oldest-old) are projected to increase from 5.3 million to 21 million by 2050. Adults aged 85–89 years constituted the greatest segment of the oldest-old in 1990, 2000, and 2010. However, the largest percentage point increase among the oldest-old occurred in the 90- to 94-year-old age group, which increased from 25.0% in 1990 to 26.4% in 2010.\n\nWith the rapid growth of the aging population, social work education and training specialized in older adults and practitioners interested in working with older adults are increasingly in demand.\n\nThere has been a considerable disparity between the number of men and women in the older population in the United States. In both 2000 and 2010, women outnumbered men in the older population at every single year of age (e.g., 65 to 100 years and over). The sex ratio, which is a measure used to indicate the balance of males to females in a population, is calculated by taking the number of males divided by the number of females, and multiplying by 100. Therefore, the sex ratio is the number of males per 100 females. In 2010, there were 90.5 males per 100 females in the 65-year-old population. However, this represented an increase from 1990 when there were 82.7 males per 100 females, and from 2000 when the sex ratio was 88.1. Although the gender gap between men and women has narrowed, women continue to have a greater life expectancy and lower mortality rates at older ages relative to men. For example, the Census 2010 reported that there were approximately twice as many women as men living in the United States at 89 years of age (361,309 versus 176,689, respectively).\n\nThe number and percentage of older adults living in the United States vary across the four different regions (Northeast, Midwest, West, and South) defined by the United States census. In 2010, the South contained the greatest number of people aged 65 years and older and 85 years and older. However, proportionately, the Northeast contains the largest percentage of adults aged 65 years and older (14.1%), followed by the Midwest (13.5%), the South (13.0%), and the West (11.9%). Relative to the Census 2000, all geographic regions demonstrated positive growth in the population of adults aged 65 years and older and 85 years and older. The most rapid growth in the population of adults aged 65 years and older was evident in the West (23.5%), which showed an increase from 6.9 million in 2000 to 8.5 million in 2010. Likewise, in the population aged 85 years and older, the West (42.8%) also showed the fastest growth and increased from 806,000 in 2000 to 1.2 million in 2010. It is worth highlighting that Rhode Island was the only state that experienced a reduction in the number of people aged 65 years and older, and declined from 152,402 in 2000 to 151,881 in 2010. Conversely, all states exhibited an increase in the population of adults aged 85 years and older from 2000 to 2010.\n\nBiogerontology is the sub-field of gerontology concerned with the biological aging process, its evolutionary origins, and potential means to intervene in the process. It involves interdisciplinary research on biological aging's causes, effects, and mechanisms. Conservative biogerontologists such as Leonard Hayflick have predicted that the human life expectancy will peak at about 92 years old, while others such as James Vaupel have predicted that in industrialized countries, life expectancies will reach 100 for children born after the year 2000. and some surveyed biogerontologists have predicted life expectancies of two or more centuries. with Aubrey de Grey offering the \"tentative timeframe\" that with adequate funding of research to develop interventions in aging such as Strategies for Engineered Negligible Senescence, \"we have a 50/50 chance of developing technology within about 25 to 30 years from now that will, under reasonable assumptions about the rate of subsequent improvements in that technology, allow us to stop people from dying of aging at any age\", leading to life expectancies of 1,000 years.\n\nBiomedical gerontology, also known as experimental gerontology and life extension, is a sub-discipline of biogerontology that endeavors to slow, prevent, and even reverse aging in both humans and animals. Most \"life extensionists\" believe the human life span can be increased within the next century, if not sooner. Biogerontologists vary in the degree to which they focus on the study of the aging process as a means of mitigating the diseases of aging or extending lifespan, although most agree that extension of lifespan will necessarily flow from reductions in age-related disease and frailty, although some argue that maximum life span cannot be altered or that it is undesirable to try. Geroscience is a recently formulated interdisciplinary field that embraces biomedical gerontology as the center of preventing diseases of aging through science.\n\nIn contrast with biogerontology, which aims to prevent age-related disease by intervening in aging processes, geriatrics is a field of medicine that studies the treatment of existing disease in aging people.\n\nThere are numerous theories of aging, and no one theory has been accepted. There is a wide spectrum of the types of theories for the causes of aging with programmed theories on one extreme and error theories on the other. Regardless of the theory, a commonality is that as humans age, functions of the body decline.\n\nStochastic theories of aging is the suggestion that aging is caused by small changes in the body over time and the body's failure to restore the system and mend the damages to the body. The cells and tissues are eventually injured due to the damage gathered over time. This causes the diminishes in an organ's function related to age. The notion of accumulated damage was first introduced by Weisman as the \"wear and tear\" theory.\n\nWear and tear theories of aging suggest that as an individual ages, body parts such as cells and organs wear out from continued use. Wearing of the body can be attributable to internal or external causes that eventually lead to an accumulation of insults which surpasses the capacity for repair. Due to these internal and external insults, cells lose their ability to regenerate, which ultimately leads to mechanical and chemical exhaustion. Some insults include chemicals in the air, food, or smoke. Other insults may be things such as viruses, trauma, free radicals, cross-linking, and high body temperature.\n\nAccumulation theories of aging suggest that aging is bodily decline that results from an accumulation of elements, whether introduced to the body from the environment or resulting from cell metabolism. An example of an accumulation theory is the Free Radical Theory of Aging.\n\nFree radicals are reactive molecules produced by cellular and environmental processes, and can damage the elements of the cell such as the cell membrane and DNA and cause irreversible damage. The free-radical theory of aging proposes that this damage cumulatively degrades the biological function of cells and impacts the process of aging. The idea that free radicals are toxic agents was first proposed by Rebeca Gerschman and colleagues in 1945, but came to prominence in 1956, when Denham Harman proposed the free-radical theory of aging and even demonstrated that free radical reactions contribute to the degradation of biological systems. Oxidative damage of many types accumulate with age, such as oxidative stress that oxygen-free radicals, because the free radical theory of aging argues that aging results from the damage generated by reactive oxygen species (ROS). ROS are small, highly reactive, oxygen-containing molecules that can damage a complex of cellular components such as fat, proteins, or from DNA, they are naturally generated in small amounts during the body's metabolic reactions. These conditions become more common as humans grow older and include diseases related to aging, such as dementia, cancer and heart disease.\n\nDNA damage has been one of the many causes in diseases related to aging. The stability of the genome is defined by the cells machinery of repair, damage tolerance, and checkpoint pathways that counteracts DNA damage. One hypothesis proposed by Gioacchino Failla in 1958 is that damage accumulation to the DNA causes aging. The hypothesis was developed soon by physicist Leó Szilárd. This theory has changed over the years as new research has discovered new types of DNA damage and mutations, and several theories of aging argue that DNA damage with or without mutations causes aging.\n\nThe cross-linking theory proposes that advanced glycation end-products (stable bonds formed by the binding of glucose to proteins) and other aberrant cross-links accumulating in aging tissues is the cause of aging. The crosslinking of proteins disables their biological functions. The hardening of the connective tissue, kidney diseases, and enlargement of the heart are connected to the cross-linking of proteins. Crosslinking of DNA can induce replication errors, and this leads to deformed cells and increases the risk of cancer.\n\nGenetic theories of aging propose that aging is programmed within each individual's genes. According to this theory, genes dictate cellular longevity. Programmed cell death, or apoptosis, is determined by a \"biological clock\" via genetic information in the nucleus of the cell. Genes responsible for apoptosis provide an explanation for cell death, but are less applicable to death of an entire organism. An increase in cellular apoptosis may correlate to aging, but is not a 'cause of death'. Environmental factors and genetic mutations can influence gene expression and accelerate aging. More recently epigenetics have been explored as a contributing factor. The epigenetic clock, which objectively measures the biological age of cells and tissues, may become useful for testing different biological aging theories.\n\nGeneral imbalance theories of aging suggest that body systems, such as the endocrine, nervous, and immune systems, gradually decline and ultimately fail to function. The rate of failure varies system by system.\n\nThe immunological theory of aging suggests that the immune system weakens as an organism ages. This makes the organism unable to fight infections and less able to destroy old and neoplastic cells. This leads to aging and will eventually lead to death. This theory of aging was developed by Ray Walford, an American gerontologist. According to Walford, incorrect immunological procedures are the cause of the process of aging.\n\nSocial gerontology is a multi-disciplinary sub-field that specializes in studying or working with older adults. Social gerontologists may have degrees or training in social work, nursing, psychology, sociology, demography, public health, or other social science disciplines. Social gerontologists are responsible for educating, researching, and advancing the broader causes of older people.\n\nBecause issues of life span and life extension need numbers to quantify them, there is an overlap with demography. Those who study the demography of the human life span differ from those who study the social demographics of aging.\n\nSeveral theories of aging are developed to observe the aging process of older adults in society as well as how these processes are interpreted by men and women as they age.\n\nActivity theory was developed and elaborated by Cavan, Havighurst, and Albrecht. According to this theory, older adults' self-concept depends on social interactions. In order for older adults to maintain morale in old age, substitutions must be made for lost roles. Examples of lost roles include retirement from a job or loss of a spouse.\n\nActivity is preferable to inactivity because it facilitates well-being on multiple levels. Because of improved general health and prosperity in the older population, remaining active is more feasible now than when this theory was first proposed by Havighurst nearly six decades ago. The activity theory is applicable for a stable, post-industrial society, which offers its older members many opportunities for meaningful participation.Weakness: Some aging persons cannot maintain a middle-aged lifestyle, due to functional limitations, lack of income, or lack of a desire to do so. Many older adults lack the resources to maintain active roles in society. On the flip side, some elders may insist on continuing activities in late life that pose a danger to themselves and others, such as driving at night with low visual acuity or doing maintenance work to the house while climbing with severely arthritic knees. In doing so, they are denying their limitations and engaging in unsafe behaviors.\n\nDisengagement theory was developed by Cumming and Henry. According to this theory, older adults and society engage in a mutual separation from each other. An example of mutual separation is retirement from the workforce. A key assumption of this theory is that older adults lose \"ego-energy\" and become increasingly self-absorbed. Additionally, disengagement leads to higher morale maintenance than if older adults try to maintain social involvement. This theory is heavily criticized for having an escape clause - namely, that older adults who remain engaged in society are unsuccessful adjusters to old age.\n\nGradual withdrawal from society and relationships preserves social equilibrium and promotes self-reflection for elders who are freed from societal roles. It furnishes an orderly means for the transfer of knowledge, capital, and power from the older generation to the young. It makes it possible for society to continue functioning after valuable older members die.\n\nContinuity theory is an elusive concept. On one hand,\nto exhibit continuity can mean to remain the same, to\nbe uniform, homogeneous, unchanging, even humdrum.\nThis static view of continuity is not very applicable\nto human aging. On the other hand, a dynamic\nview of continuity starts with the idea of a basic\nstructure which persists over time, but it allows for a\nvariety of changes to occur within the context provided\nby the basic structure. The basic structure is\ncoherent: It has an orderly or logical relation of parts\nthat is recognizably unique and that allows us to\ndifferentiate that structure from others. With the\nintroduction of the concept of time, ideas such as\ndirection, sequence, character development, and\nstory line enter into the concept of continuity as it is\napplied to the evolution of a human being. In this\ntheory, a dynamic concept of continuity is developed\nand applied to the issue of adaptation to normal\naging.\n\nA central premise of continuity theory is that, in\nmaking adaptive choices, middle-aged and older\nadults attempt to preserve and maintain existing internal\nand external structures and that they prefer to\naccomplish this objective by using continuity (i.e.,\napplying familiar strategies in familiar arenas of life).\nIn middle and later life, adults are drawn by the\nweight of past experience to use continuity as a\nprimary adaptive strategy for dealing with changes\nassociated with normal aging. To the extent that\nchange builds upon, and has links to, the person's\npast, change is a part of continuity. As a result of both\ntheir own perceptions and pressures from the social\nenvironment, individuals who are adapting to normal\naging are both predisposed and motivated\ntoward inner psychological continuity as well as outward\ncontinuity of social behavior and circumstances.\n\nContinuity theory views both internal and\nexternal continuity as robust adaptive strategies that\nare supported by both individual preference and\nsocial sanctions. Continuity theory consists of general\nadaptive principles that people who are normally \naging could be expected to follow, explanations of\nhow these principles work, and a specification of\ngeneral areas of life in which these principles could\nbe expected to apply. Accordingly, continuity theory\nhas enormous potential as a general theory of\nadaptation to individual aging.\n\nAccording to this theory, older adults born during different time periods form cohorts that define \"age strata\". There are two differences among strata: chronological age and historical experience. This theory makes two arguments. 1. Age is a mechanism for regulating behavior and as a result determines access to positions of power. 2. Birth cohorts play an influential role in the process of social change.\n\nAccording to this theory, which stems from the Life Course Perspective (Bengston and Allen, 1993), aging occurs from birth to death. Aging involves social, psychological, and biological processes. Additionally, aging experiences are shaped by cohort and period effects.\n\nAlso reflecting the life course focus,\nconsider the implications for how societies might function when age-based\nnorms vanish—a consequence of the deinstitutionalization of the life course—\nand suggest that these implications pose new challenges for theorizing aging\nand the life course in postindustrial societies. Dramatic reductions in mortality,\nmorbidity, and fertility over the past several decades have so shaken up the\norganization of the life course and the nature of educational, work, family, and\nleisure experiences that it is now possible for individuals to become old in new\nways. The configurations and content of other life stages are being altered as\nwell, especially for women. In consequence, theories of age and aging will need\nto be reconceptualized.\n\nAccording to this theory, which was developed beginning in the 1960s by Derek Price and Robert Merton and elaborated on by several researchers such as Dale Dannefer, inequalities have a tendency to become more pronounced throughout the aging process. A paradigm of this theory can be expressed in the adage \"the rich get richer and the poor get poorer\". Advantages and disadvantages in early life stages have a profound effect throughout the life span. However, advantages and disadvantages in middle adulthood have a direct influence on economic and health status in later life.\n\nEnvironmental gerontology is a specialization within gerontology that seeks an understanding and interventions to optimize the relationship between aging persons and their physical and social environments.\n\nThe field emerged in the 1930s during the first studies on behavioral and social gerontology. In the 1970s and 1980s, research confirmed the importance of the physical and social environment in understanding the aging population and improved the quality of life in old age. Studies of environmental gerontology indicate that older people prefer to age in their immediate environment, whereas spatial experience and place attachment are important for understanding the process.\n\nSome research indicates that the physical-social environment is related to the longevity and quality of life of the elderly. Precisely, the natural environment (such as natural therapeutic landscapes, therapeutic garden) contributes to active and healthy aging in the place.\n\nJurisprudential gerontology (sometimes referred to as \"geriatric jurisprudence\") is a specialization within gerontology that looks into the ways laws and legal structures interact with the aging experience. The field started from legal scholars in the field of elder law, which found that looking into legal issues of older persons without a broader inter-disciplinary perspective does not provide the ideal legal outcome. Using theories such as therapeutic jurisprudence, jurisprudential scholars critically examined existing legal institutions (e.g. adult guardianship, end of life care, or nursing homes regulations) and showed how law should look more closely to the social and psychological aspects of its real-life operation. Other streams within jurisprudential gerontology also encouraged physicians and lawyers to try to improve their cooperation and better understand how laws and regulatory institutions affect health and well being of older persons.\n"}
{"id": "175996", "url": "https://en.wikipedia.org/wiki?curid=175996", "title": "Grey goo", "text": "Grey goo\n\nGrey goo (also spelled gray goo) is a hypothetical end-of-the-world scenario involving molecular nanotechnology in which out-of-control self-replicating robots consume all biomass on Earth while building more of themselves, a scenario that has been called \"ecophagy\" (\"eating the environment\", more literally \"eating the habitation\"). The original idea assumed machines were designed to have this capability, while popularizations have assumed that machines might somehow gain this capability by accident.\n\nSelf-replicating machines of the macroscopic variety were originally described by mathematician John von Neumann, and are sometimes referred to as von Neumann machines or clanking replicators.\nThe term \"gray goo\" was coined by nanotechnology pioneer Eric Drexler in his 1986 book \"Engines of Creation\". In 2004 he stated, \"I wish I had never used the term 'gray goo'.\" \"Engines of Creation\" mentions \"gray goo\" in two paragraphs and a note, while the popularized idea of gray goo was first publicized in a mass-circulation magazine, \"Omni\", in November 1986.\n\nThe term was first used by molecular nanotechnology pioneer Eric Drexler in his book \"Engines of Creation\" (1986). In Chapter 4, \"Engines Of Abundance\", Drexler illustrates both exponential growth and inherent limits (not gray goo) by describing nanomachines that can function only if given special raw materials:\n\nImagine such a replicator floating in a bottle of chemicals, making copies of itself...the first replicator assembles a copy in one thousand seconds, the two replicators then build two more in the next thousand seconds, the four build another four, and the eight build another eight. At the end of ten hours, there are not thirty-six new replicators, but over 68 billion. In less than a day, they would weigh a ton; in less than two days, they would outweigh the Earth; in another four hours, they would exceed the mass of the Sun and all the planets combined — if the bottle of chemicals hadn't run dry long before.\n\nAccording to Drexler, the term was popularized by an article in science fiction magazine \"Omni\", which also popularized the term \"nanotechnology\" in the same issue. Drexler says arms control is a far greater issue than grey goo \"nanobugs\".\n\nIn a History Channel broadcast, a contrasting idea (a kind of gray goo) is referred to in a futuristic doomsday scenario:\n\"In a common practice, billions of nanobots are released to clean up an oil spill off the coast of Louisiana. However, due to a programming error, the nanobots devour all carbon based objects, instead of just the hydrocarbons of the oil. The nanobots destroy everything, all the while, replicating themselves. Within days, the planet is turned to dust.\"\n\nDrexler describes gray goo in Chapter 11 of \"Engines of Creation\":\n\nEarly assembler-based replicators could beat the most advanced modern organisms. 'Plants' with 'leaves' no more efficient than today's solar cells could out-compete real plants, crowding the biosphere with an inedible foliage. Tough, omnivorous 'bacteria' could out-compete real bacteria: they could spread like blowing pollen, replicate swiftly, and reduce the biosphere to dust in a matter of days. Dangerous replicators could easily be too tough, small, and rapidly spreading to stop — at least if we made no preparation. We have trouble enough controlling viruses and fruit flies.\n\nDrexler notes that the geometric growth made possible by self-replication is inherently limited by the availability of suitable raw materials.\n\nDrexler used the term \"gray goo\" not to indicate color or texture, but to emphasize the difference between \"superiority\" in terms of human values and \"superiority\" in terms of competitive success:\n\nThough masses of uncontrolled replicators need not be grey or gooey, the term \"grey goo\" emphasizes that replicators able to obliterate life might be less inspiring than a single species of crabgrass. They might be \"superior\" in an evolutionary sense, but this need not make them valuable.\nBill Joy, one of the founders of Sun Microsystems, discussed some of the problems with pursuing this technology in his now-famous 2000 article in \"Wired\" magazine, titled \"Why the Future Doesn't Need Us\". In direct response to Joy's concerns, the first quantitative technical analysis of the ecophagy scenario was published in 2000 by nanomedicine pioneer Robert Freitas.\n\nDrexler more recently conceded that there is no need to build anything that even resembles a potential runaway replicator. This would avoid the problem entirely. In a paper in the journal \"Nanotechnology\", he argues that self-replicating machines are needlessly complex and inefficient. His 1992 technical book on advanced nanotechnologies \"Nanosystems: Molecular Machinery, Manufacturing, and Computation\" describes manufacturing systems that are desktop-scale factories with specialized machines in fixed locations and conveyor belts to move parts from place to place. None of these measures would prevent a party from creating a weaponized grey goo, were such a thing possible.\n\nPrince Charles called upon the British Royal Society to investigate the \"enormous environmental and social risks\" of nanotechnology in a planned report, leading to much media commentary on grey goo. The Royal Society's report on nanoscience was released on 29 July 2004, and declared the possibility of self-replicating machines to lie too far in the future to be of concern to regulators.\n\nMore recent analysis in the paper titled \"Safe Exponential Manufacturing\" from the Institute of Physics (co-written by Chris Phoenix, Director of Research of the Center for Responsible Nanotechnology, and Eric Drexler), shows that the danger of grey goo is far less likely than originally thought. However, other long-term major risks to society and the environment from nanotechnology have been identified. Drexler has made a somewhat public effort to retract his grey goo hypothesis, in an effort to focus the debate on more realistic threats associated with knowledge-enabled nanoterrorism and other misuses.\n\nIn \"Safe Exponential Manufacturing\", which was published in a 2004 issue of \"Nanotechnology\", it was suggested that creating manufacturing systems with the ability to self-replicate by the use of their own energy sources would not be needed. The Foresight Institute also recommended embedding controls in the molecular machines. These controls would be able to prevent anyone from purposely abusing nanotechnology, and therefore avoid the grey goo scenario.\n\nGrey goo is a useful construct for considering low-probability, high-impact outcomes from emerging technologies. Thus, it is a useful tool in the ethics of technology. Daniel A. Vallero applied it as a worst-case scenario thought experiment for technologists contemplating possible risks from advancing a technology. This requires that a decision tree or event tree include even extremely low probability events if such events may have an extremely negative and irreversible consequence, i.e. application of the precautionary principle. Dianne Irving admonishes that \"any error in science will have a rippling effect...\" Vallero adapted this reference to chaos theory to emerging technologies, wherein slight permutations of initial conditions can lead to unforeseen and profoundly negative downstream effects, for which the technologist and the new technology's proponents must be held accountable.\n\n\n"}
{"id": "20556534", "url": "https://en.wikipedia.org/wiki?curid=20556534", "title": "Henri Aalto", "text": "Henri Aalto\n\nHenri Aalto (born 20 April 1989) is a Finnish football player currently playing for Finnish club FC Honka.\n\n"}
{"id": "13110480", "url": "https://en.wikipedia.org/wiki?curid=13110480", "title": "Human spirit", "text": "Human spirit\n\nThe human spirit is a component of human philosophy, psychology, art, and knowledge - the spiritual or mental part of humanity. While the term can be used with the same meaning as \"human soul\", human spirit is sometimes used to refer to the impersonal, universal or higher component of human nature in contrast to soul or psyche which can refer to the ego or lower element. The human spirit includes our intellect, emotions, fears, passions, and creativity.\n\nIn the models of Daniel A. Helminiak and Bernard Lonergan, human spirit is considered to be the mental functions of awareness, insight, understanding, judgement and other reasoning powers. It is distinguished from the separate component of psyche which comprises the entities of emotion, images, memory and personality.\n\nJohn Teske views human spirit as a social construct representing the qualities of purpose and meaning which transcend the individual human.\n\nAccording to historian Oswald Spengler, a distinction between Spirit and Soul has been made by the West and earlier civilizations which influenced its development.\nThe human spirit can be seen as the heavenly component of human's non material makeup - the part that is impersonal or universal. Whereas souls are the personal element unique to each individual. As Spengler writes in \"The Decline of the West\":\n\nIn Christianity, the Bible identifies humanity's three basic elements: spirit, soul and body. Christians emphasise that the human spirit is the 'real person', the very core of a person's being, the essential seat of their existence. When a person accepts Jesus Christ as their Saviour, it is their human spirit that is transformed as they become 'new creatures' in Jesus Christ. The soul which is the seat of the will, mind and emotions does not get converted but needs to be renewed on a daily basis through the recommended Christian disciplines such as prayer and reading the Bible. In Islam, Muslims are viewed as having their own spirits, but one that in a sense is one with God's spirit. For Spengler, the perception of unity this idea led to was important for the emergence of the \"consensus\" that maintained harmony in Islamic culture, especially during the Golden Age of Islam.\n"}
{"id": "832518", "url": "https://en.wikipedia.org/wiki?curid=832518", "title": "Intellectualism", "text": "Intellectualism\n\nIntellectualism denotes the use, development, and exercise of the intellect; the practice of being an intellectual; and the Life of the Mind. In the field of philosophy, “intellectualism” occasionally is synonymous with “rationalism”, that is, knowledge mostly derived from reason and ratiocination. Socially, “intellectualism” negatively connotes: single-mindedness of purpose (“too much attention to thinking”) and emotional coldness (“the absence of affection and feeling”).\n\nIn the view of Socrates (c. 470 – 399 BC), intellectualism allows that “one will do what is right or best just as soon as one truly understands what is right or best”; that virtue is a purely intellectual matter, since virtue and knowledge are familial relatives, which a person accrues and improves with dedication to reason. So defined, Socratic intellectualism became a key philosophic doctrine of Stoicism. The apparent, problematic consequences of this view are “Socratic paradoxes”, such as the view that there is no weakness of will — that no one knowingly does, or seeks to do, evil (moral wrong); that anyone who does, or seeks to do, moral wrong does so involuntarily; and that virtue is knowledge, that there are not many virtues, but that all virtues are one.\n\nContemporary philosophers dispute that Socrates’s conceptions of knowing truth, and of ethical conduct, can be equated with modern, post–Cartesian conceptions of knowledge and of rational intellectualism. As such, Michel Foucault demonstrated, with detailed historical study, that in Classical Antiquity (800 BC – AD 1000), “knowing the truth” is akin to “spiritual knowledge”, in the contemporarily understanding of the concept. Hence, without exclusively concerning the rational intellect, spiritual knowledge is integral to the broader principle of “caring for the self”.\n\nTypically, such care of the self-involved specific ascetic exercises meant to ensure that not only was knowledge of truth memorized, but learned, and then integrated to the self, in the course of transforming oneself into a good person. Therefore, to understand truth meant “intellectual knowledge” requiring one’s integration to the (universal) truth, and authentically living it in one’s speech, heart, and conduct. Achieving that difficult task required continual care of the self, but also meant being someone who embodies truth, and so can readily practice the Classical-era rhetorical device of parrhesia: “to speak candidly, and to ask forgiveness for so speaking”; and, by extension, practice the moral obligation to speak the truth for the common good, even at personal risk. This ancient, Socratic moral philosophic perspective contradicts the contemporary understanding of truth and knowledge as rational undertakings.\n\nMedieval theological intellectualism is a doctrine of divine action, wherein the faculty of intellect precedes, and is superior to, the faculty of the will (\"voluntas intellectum sequitur\"). As such, Intellectualism is contrasted with voluntarism, which proposes the Will as superior to the intellect, and to the emotions; hence, the stance that “according to intellectualism, choices of the Will result from that which the intellect recognizes as good; the will, itself, is determined. For voluntarism, by contrast, it is the Will which identifies which objects are good, and the Will, itself, is indetermined”. From that philosophical perspective and historical context, the Spanish Muslim polymath Averroës (1126–1198) in the 12th century, the Italian Christian theologian Thomas Aquinas (1225–1274), and the German Christian theologian Meister Eckhart (1260–1327) in the 13th century, are recognised intellectualists.\n\n"}
{"id": "58180313", "url": "https://en.wikipedia.org/wiki?curid=58180313", "title": "Jedediah Aaker", "text": "Jedediah Aaker\n\nJedediah Aaker is a musician, performer, theatrical producer, and Portland-area personality. A fixture on cable series \"Portlandia\", he has appeared in more episodes than anyone besides the series' two leads. The PDX-native is also a multiple-award-winning, \"Whisker Wars\"-celebrated competitive bearder who co-founded the Portland Beardsmen.\n\nHe has worn a Christmas sweater on the cover of \"The Portland Mercury\" and appeared as a shirtless model on the front page of \"Willamette Week\"s 2018 <q>Beer</q> issue. He has worked as bartender for a failing club featured on the fourth season of \"Bar Rescue\" and as host of Barfly Bus Tours. \"The Daily Mail\" referred to him as a <q>Portland character</q> and <q>professional party host</q>, and alternative newspaper \"Willamette Week\" dubbed him a <q>beard-about-town</q>. In a 2018 feature article titled <q>Twilight of the Hipsters</q>, the French edition of Rolling Stone introduced Aaker as an <q>actor, concert booker, chauffeur for rock stars, and distributor of the Lucky Egg vending machines that he installs in bars.</q>\n\n\"Portlandia\" first entered the global consciousness after satirical music video <q>Dream of the ‘90s</q> premiered online in December 2010 and thrust Saturday Night Live star Fred Armisen between a motley assemblage of circus folk, exotic dancers, and idiosyncratically-styled local luminaries. Standing directly to the right of Armisen throughout the video while wearing only star-spangled swim briefs, black leather, and a sizable beard, Aaker stood out as the swiftly-trending clip sparked interest worldwide in the forthcoming sketch comedy series. In the process, Aaker soon became widely known as, in the words of \"Willamette Week\", <q>that guy in underwear and a leather jacket in the ‘Dream of the ‘90s’ clip.</q>\n\nIn addition to serving as a background actor on several episodes, Aaker has played a scavenger hunt umpire, a member of Spyke’s wedding party, and a guest at Nina's birthday party. Ranking the <q>Best and Worst</q> of the series’ third season, \"Portland Monthly\" critic John Chandler determined the <q>hirsute man-about-town</q> tied with a veteran theater actress for <q>Best Performance By A Local</q>. Although <q>Aaker plays himself (presumably)</q> in episodes four and five, Chandler wrote, <q>the man’s beard has star quality.</q> By season four, Aaker had appeared in almost two dozen of the 37 episodes then filmed.\n\nAdditionally, he served as Kumail Nanjiani's assistant on webisodes of IFC's travelogue series \"Kumail Tours Portland\". When the series finished its eighth and final season in the spring of 2018, its network IFC estimated that the collective number of Aaker's various performances ranked below only co-founders Armisen and Carrie Brownstein.\n\nIn 2013, during a month-long residency at Portland’s Newspace Center for Photography, acclaimed San Francisco photographer Kirk Crippens sought out 45 PDX locals for a series of portraits eventually titled \"Portraitlandia\". Over the next year, according to \"Portland Monthly\", the <q>series went viral, drawing millions of eyes to the site of its photographer.</q> Images from the Portraitlandia series would be exhibited at London's National Portrait Gallery in conjunction with the Taylor Wessing Photographic Portrait Prize and toured the United States as part of the Photolucida exhibition.\n\nAlthough Crippens told Wired that he’d <q>expected his work to stand in sharp contrast to the fictions</q> of \"Portlandia\", the artist found <q>significant overlap</q> – particularly so when the first episode he saw after beginning the project happened to showcase Aaker and another two planned subjects. Aaker, <q>a man about town who owns his own lion suit,</q> was among Crittens’ favorite subjects. For their shoot, Crittens said Aakers made available the roof of the nightclub where he tended bar and then asked the photographer <q>if I wanted him to curl up like a kitty in the ivy growing over the top of the roof. Yes, yes I did.</q>\n\nAaker has been involved with several NW rock bands. He spent years playing bass for the Fabulous Miss Wendy, a performer who has toured with Slash and was lauded as the <q>sexiest rock star ever</q> by Revolver. He’s a founding member and bassist of The Gnash, whose music he has compared to <q>a primer-gray broken down Camaro.</q> The group recently put out a split single on the Voodoo Doughnut Recordings label entitled <q>She Took My Doughnut (And Left Me the Hole)</q>.\n\nHe’s currently a member of the re-united Lucky Thirteens with former Weaklings vocalist Bradly Battin. Battins and Aaker previously played together in short-lived 00s group DARLINS. One venue owner explained the Lucky Thirteens’ sound as <q>fun, poppy-punk like you’d hear coming out of the jukebox ... back in 2000.</q>\n\nFor several years throughout the 2000s, Aaker served as frontman of glam metal group Diamond Tuck & The Privates – <q>an eight-piece band carefully culled from the lifers of the Portland rock scene.</q> \"Willamette Week\" referred to their appearance on a (local punk mainstay) Club 21 compilation as <q>epic … buttrock.</q>\n\nIn 2009, Aaker and Diamond Tuck guitarist <q>Private</q> Mike Albano led the live band during performances of writer/director/producer Jeffery <q>Wonderful</q> Wilson’s rock opera \"Chariots of Rubber\" during performances at the Interstate Firehouse Cultural Center. A review from the Portland State University Vanguard, described the original score as <q>1980s metal with a dash of Rocky Horror Picture Show.</q>\n\nThe following year, Aaker and Wilson again joined forces as, respectively, producer and director of \"Hot Gun\". The <q>rock ’n’ roll re-imagining of the iconic blockbuster Top Gun</q> was performed live on stage at PDX nightclub Dante's.\n\nIn a review from \"Portland Monthly\", their critic admired the multimedia production’s deft manipulation of the original film – inserting previously-shot scenes of the stage actors into aerial battles – but found the comedy’s <q>laughs were loudest with the in-crowd. The novelty of seeing friends dressed funny, and/or comically incorporated into video footage, was the bulk of the fun.</q> \"Willamette Week\", however, raved \"Hot Gun\"’s <q>summer stock-'n'-roll aesthetic, artfully unchoreographed and shruggably brilliant, resembles an old movie naval pageant (coconut bras on bearded men, absent camp or shame) blessed with the underutilized talents peculiar to Portland ...</q>\n\nA year later, Aaker brought John Walterich’s online-dating satire \"Brie\" to the stage of the Tonic Lounge for a brief run. \"Willamette Week\" critic Jonathan Frochtzwajg described the production as </q>a largely amateur, very tattooed cast ... scored live by rotating punk-rock acts.</q>\n\nAaker acted in the 2017 film \"Neil Stryker and the Tyrant of Time\" alongside TV icons Walter Koenig (Ensign Pavel Chekov)and David Ogden Stiers (Major Charles Emerson Winchester). That same year, he was a central figure in the 2017 documentary \"Thank You for Supporting the Arts\", which also features Gus Van Sant. The doc explores the physical and emotional trauma suffered by author and exotic dancer Liv <q>Viva Las Vegas</q> Osthus following her fight against breast cancer and subsequent mastectomy. Osthus and Aaker were in a relationship for many years. As the first to discover her disease, he plays a significant role in the film's coverage of her struggles.\n\nAs a judge for the 2011 West Coast Beard and Mustache Championships, Aaker appeared in a 2011 episode of the \"Whisker Wars\" cable reality series. <q>You're not judging on what's the biggest or the longest but ... the grandest,</q> he told the \"Oregonian\". <q>It just has the magic glow ...</q> He’s also regularly called upon to help select the winners of various events around the Portland area such as the Bacon Cup, the Iron Bartender competition, the horror short GuignolFest, and Portland’s inaugural Pudding Wrestling Massacre. In a Vice article that takes its title (<q>Every Vagina Is A Snowflake</q>) from a quote given by Aaker, the magazine noted that the <q>returning judge</q> brought his own props overseeing the beauty pageant for exotic dancers' nether regions.\n\nAaker has been cultivating his beard for more than ten years. He’s become a notable figure in competitive bearding events, winning several freestyle awards, and can be seen in the first season of IFC series \"Whisker Wars\".\n\nAt the 2014 World Beard and Moustache Championships held inside Portland’s Keller Auditorium, which hosted 300 contestants from ten countries, Aaker emerged as a fan favorite. \"The Oregonian\" reported that he \"somehow got extra time, walking onstage in short shorts and a white fur coat, his red beard shaped into a flying V guitar, gyrating his belly to the roar of the crowd. He walked away with sixth place, but he stole the audience's heart.\"\n\nIn 2012, Aaker helped found the Portland Beardsmen – a band of facial-hair enthusiasts who compete for prizes awarded on the merits of beard size, grooming, style, creativity, and other characteristics. The nonprofit group raises funds for transportation to far-flung tournaments, and they also organize charitable efforts such as beard-wielded car washes. For their annual Beards N Roses drive each Mother’s Day, hirsute members don brightly-colored tights and tutus for delivering bouquets to Portland-area moms. Aaker helped originate the program six years ago to benefit the Uprise Books Project.\n\nOn the evening of August 28, 2012, Aaker was among 13 bearded men detained at gunpoint by officers from the Portland police on the Burnside Bridge during what \"the Oregonian\" termed \"a military-themed photo shoot for breast cancer awareness.\" After an unnamed observer’s 911 call alerted authorities about suspicious characters wearing camouflaged body armor, both sides of the bridge were closed for nearly an hour as the police investigated the situation. Two men were taken into custody for disorderly conduct and carrying an (unloaded) AR-15 assault rifle through a public space.\n\nAccording to KPTV News, the group was organized by Aaker to aid a national fundraising campaign entitled <q>Beards for Breast Cancer</q> that was assembling a calender featuring pictures of bearding groups from around the country. Since the Portland contingent was designated to appear alongside July, Aaker thought the resulting photo should feel especially patriotic, which led several of the beardsmen to don militaristic garb. (KATU News reported that Aaker himself had on an NSYNC T-shirt.) \"We are the good guys,\" Aaker told the station, \"just a bunch of dudes walking across the bridge wearing camo getting rad.\"\n"}
{"id": "55914121", "url": "https://en.wikipedia.org/wiki?curid=55914121", "title": "Jorma Aalto", "text": "Jorma Aalto\n\nJorma Aalto (born 24 August 1957) is a Finnish skier. He competed in the men's 30 kilometre cross-country ski race at the 1980 Winter Olympics.\n"}
{"id": "213380", "url": "https://en.wikipedia.org/wiki?curid=213380", "title": "Left- and right-hand traffic", "text": "Left- and right-hand traffic\n\nLeft-hand traffic (LHT) and right-hand traffic (RHT) are the practice, in bidirectional traffic, of keeping to the left side or to the right side of the road, respectively. A fundamental element to traffic flow, it is sometimes referred to as the rule of the road.\n\nRHT is used in 163 countries and territories, with the remaining 78 countries and territories using LHT. Countries that use LHT account for about a sixth of the world's area with about 35% of humanity and a quarter of its roads. In 1919, 104 of the world's territories were LHT and an equal number were RHT. From 1919 to 1986, 34 of the LHT territories switched to RHT.\n\nMany of the countries with LHT were formerly part of the British Empire. In addition, Japan, Thailand, Suriname and other countries have retained the LHT tradition. Conversely, many of the countries with RHT were formerly part of the French colonial empire or, in Europe, were subject to French rule during the Napoleonic conquests.\n\nFor rail traffic, LHT predominates in Western Europe (except Germany, Austria, Spain and the Netherlands), Latin America, and in countries formerly in the British and French Empires, whereas North American and central and eastern European train services operate RHT.\n\nAccording to the International Regulations for Preventing Collisions at Sea, water traffic is effectively RHT: a vessel proceeding along a narrow channel must keep to starboard (the right-hand side), and when two power-driven vessels are meeting head-on both must alter course to starboard also. For aircraft the US Federal Aviation Regulations suggest RHT principles, both in the air and on water.\n\nIn healthy populations, traffic safety is thought to be the same regardless of handedness, although some researchers have speculated that LHT may be safer for ageing populations since humans are more commonly right-eye dominant than left-eye dominant.\n\nAncient Greek, Egyptian, and Roman troops kept to the left when marching. In 1998, archaeologists found a well-preserved double track leading to a Roman quarry near Swindon, in southern England. The grooves in the road on the left side (viewed facing down the track away from the quarry) were much deeper than those on the right side, suggesting LHT, at least at this location, since carts would exit the quarry heavily loaded, and enter it empty.\n\nThe first reference in English law to an order for LHT was in 1756, with regard to London Bridge.\n\nSome historians, such as C. Northcote Parkinson, believed that ancient travellers on horseback or on foot generally kept to the left, since most people were right-handed. If two men riding on horseback were to start a fight, each would edge toward the left. In the year 1300, Pope Boniface VIII directed pilgrims to keep left.\n\nIn the late 1700s, traffic in the United States was RHT based on teamsters' use of large freight wagons pulled by several pairs of horses. The wagons had no driver's seat, so the (typically right-handed) postilion held his whip in his right hand and thus sat on the left rear horse. Seated on the left, the driver preferred that other wagons pass him on the left so that he could be sure to keep clear of the wheels of oncoming wagons.\n\nIn France, traditionally foot traffic had kept right, while carriage traffic kept left. Following the French Revolution, all traffic kept right. Following the Napoleonic Wars, the French imposed RHT on parts of Europe. During the colonial period, RHT was introduced by the French in New France, French West Africa, the Maghreb, French Indochina, the West Indies, French Guiana and the Réunion, among others.\n\nMeanwhile, LHT was introduced by the British in parts of Canada (Atlantic Canada and British Columbia), Australia, New Zealand, the East Africa Protectorate (now Kenya, Tanzania and Uganda), British India, Rhodesia and the Cape Colony (now Zambia, Zimbabwe and South Africa), British Malaya (now Malaysia, Brunei and Singapore), British Guiana, and British Hong Kong. LHT was also introduced by the Portuguese Empire in Portuguese Macau, Colonial Brazil, Portuguese Timor, Portuguese Mozambique, and Portuguese Angola.\n\nThe first keep-right law for driving in the United States was passed in 1792 and applied to the Philadelphia and Lancaster Turnpike. New York formalized RHT in 1804, New Jersey in 1813 and Massachusetts in 1821.\n\nIn the early 1900s some countries including Canada, Spain, and Brazil had different rules in different parts of the country. During the 1900s many countries standardised within their jurisdictions, and changed from LHT to RHT, mostly to conform with regional custom. Currently nearly all countries use one side or the other throughout their entire territory. Most exceptions are due to historical considerations and/or involve islands with no road connection to the main part of a country. China is RHT except the Special Administrative Regions of Hong Kong and Macau. The Hong Kong–Zhuhai–Macau Bridge is RHT even though both Hong Kong and Macau are LHT. The United States is RHT except the United States Virgin Islands. The United Kingdom is LHT, but its overseas territories of Gibraltar and British Indian Ocean Territory are RHT.\n\nInfluential in Europe was the 1920 Paris Convention, which advised driving on the right-hand side of the road, in order to harmonise traffic across a continent with many borders. This was despite the fact that left-hand traffic was still widespread: in 1915 for example, LHT was introduced everywhere in the Austro-Hungarian Empire; however, three years later the Empire was split up into several countries, and they all changed eventually to RHT, notably including when Nazi Germany introduced RHT with almost immediate effect in Czechoslovakia in 1938–39. \n\nSweden was LHT from about 1734 to 1967, despite having land borders with RHT countries, and approximately 90 percent of cars being left-hand drive (LHD) vehicles. A referendum was held in 1955, with an overwhelming majority voting against a change to RHT. Nevertheless, some years later the government ordered a conversion, which took place at 5 am on Sunday, 3 September 1967. The accident rate dropped sharply after the change, but soon rose back to near its original level. The day was known as \"Dagen H\" (\"H-Day\"), the 'H' being for \"Högertrafik\" (\"right traffic\"). When Iceland switched the following year, it was known as \"H-dagurinn\", again meaning \"H-Day\".\n\nIn the late 1960s, the UK Department for Transport considered switching to RHT, but declared it unsafe and too costly for such a built-up nation. Road building standards, for motorways in particular, allow asymmetrically designed road junctions, where merge and diverge lanes differ in length.\n\nToday, four countries in Europe continue to use left-hand traffic, all island nations: the UK, Cyprus, Ireland, and Malta.\n\nNationalist China adopted RHT in 1946. This convention was preserved when the CCP took the mainland and the KMT retreated to Taiwan. Hong Kong and Macau continue to be LHT.\n\nBoth North Korea and South Korea switched to RHT in 1945 after liberation from Japanese colonial power.\n\nMyanmar switched to RHT in 1970.\n\nSamoa, a former German colony, had been RHT for more than a century. It switched to LHT in 2009, being the first territory in almost 30 years to switch. The move was legislated in 2008 to allow Samoans to use cheaper right-hand drive (RHD) vehicles—which are better suited for left-hand traffic—imported from Australia, New Zealand or Japan, and to harmonise with other South Pacific nations. A political party, The People's Party, was formed to try to protest against the change, a protest group which launched a legal challenge, and an estimated 18,000 people attending demonstrations against it. The motor industry was also opposed, as 14,000 of Samoa's 18,000 vehicles are designed for RHT and the government has refused to meet the cost of conversion. After months of preparation, the switch from right to left happened in an atmosphere of national celebration. There were no reported incidents. At 05:50 local time, Monday 7 September, a radio announcement halted traffic, and an announcement at 6:00 ordered traffic to switch to LHT. The change coincided with more restrictive enforcement of speeding and seat-belt laws. That day and the following day were declared public holidays, to reduce traffic. The change included a three-day ban on alcohol sales, while police mounted dozens of checkpoints, warning drivers to drive slowly.\n\nThe Philippines was mostly LHT during its Spanish and American colonial periods, as well as during the Commonwealth era. During the Japanese occupation the Philippines remained LHT, also because LHT had been required by the Japanese; but during the Battle of Manila the liberating American forces drove their tanks to the right for easier facilitation of movement. RHT was formally finalised by Executive Order No. 34 signed by President Sergio Osmeña on 10 March 1945.\n\nA number of non-contiguous former British colonies in West Africa originally drove LHT and switched to RHT in the early 1970s to match the surrounding countries. Sierra Leone switched to RHT in 1971, Nigeria in 1972 and Ghana in 1974. Before this period The Gambia, a country entirely contained within RHT Senegal, had officially switched to RHT in 1965.\n\nRwanda, a former Belgian colony in central Africa, is RHT but is considering switching to LHT, to bring the country in line with other members of the East African Community (EAC). A survey, carried out in 2009, indicated that 54% of Rwandans were in favour of the switch. Reasons cited were the perceived lower costs of RHD vehicles as opposed to LHD versions of the same model, easier maintenance and the political benefit of harmonisation of traffic regulations with other EAC countries. The same survey also indicated that RHD cars are 16 to 49 per cent cheaper than their LHD equivalents. In 2014 an internal report from consultants to the Ministry of Infrastructure recommended a switch to LHT. In 2015, the ban on RHD vehicles was lifted; RHD trucks from neighbouring countries cost $1000 less than LHD models imported from Europe.\n\nIn one study, researchers concluded that left-hand traffic may be safer for elderly drivers, since humans are more commonly right-eye dominant than left-eye dominant. Comparing accident statistics between countries operating either LHT or RHT, Leeming concluded that LHT is superior. However, Watson has criticised the small sample size and dismisses the notion.\n\nAlthough many LHT jurisdictions are on islands, there are cases where vehicles may be driven from LHT across a border into a RHT area. Such borders are mostly located in Africa and southern Asia. The Vienna Convention on Road Traffic regulates the use of foreign registered vehicles in the 74 countries that have ratified it.\n\nLHT Thailand has three RHT neighbors: Cambodia, Laos, Myanmar. Most of its borders use a simple traffic light to do the switch, but there are also interchanges which enable the switch while keeping up a continuous flow of traffic.\n\nThere are four road border crossing points between Hong Kong and Mainland China. In 2006, the daily average number of vehicle trips recorded at Lok Ma Chau was 31,100. The next largest is Man Kam To, where there is no changeover system and the border roads on the mainland side Wenjindu intersect as one-way streets with a main road.\n\nThe Takutu River Bridge (which links LHT Guyana and RHT Brazil) is the only border in the Americas where traffic changes sides.\n\nAlthough the United Kingdom is separated from Continental Europe by the English Channel, the level of cross-Channel traffic is very high; the Channel Tunnel alone carries 3.5 million vehicles per year, by rail, between the UK and France.\n\nIn RHT jurisdictions, vehicles are configured with LHD, with the driver sitting on the left side. In LHT jurisdictions, the reverse is true. The driver's side, the side closest to the centre of the road, is sometimes called the \"offside\", while the passenger side, the side closest to the side of the road, is sometimes called the \"nearside\".\n\nHistorically there was less consistency in the relationship of the position of the driver to the handedness of traffic. Most American cars produced before 1910 were RHD. In 1908 Henry Ford standardised the Model T as LHD in RHT America, arguing that with RHD and RHT, the passenger was obliged to \"get out on the street side and walk around the car\" and that with steering from the left, the driver \"is able to see even the wheels of the other car and easily avoids danger.\" By 1915 other manufacturers followed Ford's lead, due to the popularity of the Model T.\n\nIn specialised cases, the driver will sit on the nearside, or kerbside. Examples include:\n\nGenerally, the convention is to mount a motorcycle on the left, and kickstands are usually on the left which makes it more convenient to mount on the safer kerbside as is the case in LHT. Some jurisdictions prohibit fitting a sidecar to a motorcycle's offside.\n\nMost low-beam headlamps produce an asymmetrical light suitable for use on only one side of the road. Low beam headlamps in LHT jurisdictions throw most of their light forward-leftward; those for RHT throw most of their light forward-rightward, thus illuminating obstacles and road signs while minimising glare for oncoming traffic.\n\nIn Europe, headlamps approved for use on one side of the road must be adaptable to produce adequate illumination with controlled glare for temporarily driving on the other side of the road. This may be achieved by affixing masking strips or prismatic lenses to a part of the lens or by moving all or part of the headlamp optic so all or part of the beam is shifted or the asymmetrical portion is occluded. Some varieties of the projector-type headlamp can be fully adjusted to produce a proper LHT \"or\" RHT beam by shifting a lever or other movable element in or on the lamp assembly. Some vehicles adjust the headlamps automatically when the car's GPS detects that the vehicle has moved from LHT to RHT and vice versa.\n\nBecause blackout strips and adhesive prismatic lenses reduce the safety performance of the headlamps, most countries require all vehicles registered or used on a permanent or semi-permanent basis within the country to be equipped with headlamps designed for the correct traffic-handedness.\n\nWithout sidecars attached, motorcycles, motor scooters, mopeds, and bicycles are almost symmetric with their handlebars in the centre. However, motorcycles are often equipped with automotive-type asymmetrical-beam headlamps that likewise require adjustments or replacement when brought into a country with opposite traffic-handedness.\n\nIn the European Union, vehicles must be equipped with one or two red rear fog lamps. A single rear fog lamp must be located between the vehicle's longitudinal centreline and the outer extent of the driver's side of the vehicle. Rear fog lamps are also being found on vehicles produced for Australia, New Zealand and South Africa.\n\nAn Australian news source reports that some RHD cars imported to that country did not perform as well on crash tests as the LHD versions, although the cause is unknown, and may be due to differences in testing methodology.\n\nOf the 194 countries currently recognised by the United Nations (and the unrecognised Palestine and Kosovo), 140 use RHT and 54 use LHT. A country and its territories and dependencies is counted once.\n\nSeveral states in Europe have RHT for roads but LHT for trains: Belgium, Italy, Monaco, Portugal, Slovenia, Sweden and Switzerland. France has mainly LHT for trains, except in Alsace-Lorraine, an eastern French territory which belonged to Germany for much of its history. UK and Ireland have LHT for both road and rail.\n\n\n"}
{"id": "58420452", "url": "https://en.wikipedia.org/wiki?curid=58420452", "title": "List of countries by number of births", "text": "List of countries by number of births\n\nThe following list sorts countries by the total projected number of births. The list is sourced from the United Nations World Population Prospects. Figures are from the 2017 revision of the United Nations World Population Prospects report, for the period 2015-2020, using the medium assumption. All figures are rounded and given in thousands.\n\n"}
{"id": "6974014", "url": "https://en.wikipedia.org/wiki?curid=6974014", "title": "Mika Aaltonen", "text": "Mika Aaltonen\n\nMika Aaltonen (born 16 November 1965) is a Finnish former footballer. His position was an attacking central midfielder. He also played for the Finnish national team.\n\nDuring his career (1982–1994) he played in Finland, Italy, Switzerland, Germany, and Israel for TPS, Internazionale, Bellinzona, Bologna, Hertha BSC, Hapoel Be'er Sheva F.C. and Tampereen Pallo-Veikot. He finished his career at a relatively young age because of a persistent ankle injury, and because of this, he ended up concentrating more on his studies. Aaltonen studied throughout his playing career, and after he retired from football, he earned a doctorate degree in economics.\n\nAaltonen is best remembered for a superb goal scored against Italian goalkeeper Walter Zenga in UEFA Cup in 1987 during a match between Turun Palloseura and Inter Milan at the San Siro stadium in Milan. This goal more or less earned him a transfer to Inter shortly afterwards.\n\n"}
{"id": "55914525", "url": "https://en.wikipedia.org/wiki?curid=55914525", "title": "Minna Aalto", "text": "Minna Aalto\n\nMinna Aalto (born 8 November 1965) is a Finnish sailor. She competed in multiple sailing events at the 1996 and 2000 Summer Olympics.\n"}
{"id": "58442478", "url": "https://en.wikipedia.org/wiki?curid=58442478", "title": "Mohammad Aamer (cricketer, born 1995)", "text": "Mohammad Aamer (cricketer, born 1995)\n\nMohammad Aamer (born 8 August 1995) is a Pakistani cricketer. He made his first-class debut for Zarai Taraqiati Bank Limited in the 2018–19 Quaid-e-Azam Trophy on 8 September 2018.\n"}
{"id": "57969764", "url": "https://en.wikipedia.org/wiki?curid=57969764", "title": "Mortality in the early modern age", "text": "Mortality in the early modern age\n\nThe early modern age saw various economic changes as well as several significant diseases that have affected the mortality rates. Data collection during this time was not consistent or broadly recorded and there have been efforts to reconstruct plausible statistics. Mortality rates vary on geographic location, social environment, and cultural values. There were also gender differences in the mortality rates, leading to an excess mortality rate in urban areas and in the female population. A main cause of death was stillbirth, which could be attributed to, but not limited to, maternal infections, birth complications, and congenital anomalies. Another contributing factor to the mortality rate was food insecurity and shortages as well as unemployment, both of which varied per region. A final factor was violence, which occurred mainly due to structural or systemic violence; however, violence since the 12th century has been steadily falling.\n\nData from the early modern age was not accurately or consistently collected. However, there have been a number of studies and reconstructed statistics from this era, particularly on children and women. There has not been any empirical research published and the only information has been theoretical as there has been insufficient data and sources. It was also common for many statistics to go unreported; this is especially true regarding unmarried women. Models and theoretic equations need to take into account \"social, economic, cultural, geographical, and even climatological variables\" in order to accurately reflect the statistics of the time.\n\nOne study, the Eurasia Project, has shown that boys, especially those under one year, had a higher mortality rate during childhood than girls, but the mortality rate for men and women were about equal. It has also been shown that there is a higher male mortality than female mortality rate during the time of famine. Male mortality has also been linked to \"economic modernization and urbanization ... especially for cardiovascular disease\".\n\nWomen faced increased mortality during childbirth as pregnancy and childbirth compromised the mother's immune system, with the most common causes of death being puerperal fever, toxemia, and hemorrhage. These dangers suggest an association to the excess female mortality, especially considering that women had to compete more for resources as they had no property rights and had a lower ranking in the household hierarchy. The average age of childbearing differed between Asia and Europe with an average difference of five years, which would affect cross-cultural data collection. Children born to mothers 35 years or older had a higher risk of mortality than children born to younger mothers. linking a mother's health and a child's survival.\n\nFemale infants and children often had a higher mortality rate, especially in times of food insecurity, compared to male infants and children. However, a maternal presence worked as a protective factor for children. regardless of age or gender.\n\nFood insecurity and shortages were common throughout this time period and were matched with the high food prices and high unemployment rate. This is shown through the differences in mortality rates between the lower and upper class, with poor infants being up to two times more likely to die than their wealthier counterparts. In 17th century Europe, it was common that at least one in five children from the same family would die before the age of one. Because of this high rate, it was common for there to be a large amount of children in one family so that there would be a higher rate of survival. Especially among poorer families, having multiple children was common in order to ensure there would be children to contribute to the family later on.\n\nDuring the early modern era, house calls were common. If the patient was female, the doctor, commonly an upper class male, would typically be summoned by a male family member who wound remain throughout the examination. The physician and male family member would then discuss the diagnosis and treatment plan without the female's input. This practice reinforced and perpetuated social hierarchies and patriarchal values.\n\nFood shortages and insecurity were leading concerns in the 18th century, especially in Europe, and these were exacerbated by reduced harvests yields. Disease was another leading cause of death, with rats and fleas being the common carriers of disease, specifically plagues, during this era.\n\nThe Black Death was a plague that affected much of the world, originating in Asia and spreading to Europe through diseased fleas and rats. This epidemic has been reported to have been the cause of death for approximately \"60% of the European population\".\n\nDuring the end of the 19th century, there was a plague, known as the Modern Plague, that started in China and spread to different cities through ports, reportedly causing roughly ten million deaths. This plague affected Asia, the Americas, and Africa and lasted into the 20th century. There were also epidemics that occurred locally and did not spread to national levels, notably in 18th century England. These local epidemics included fevers, dysentery, smallpox, starvation, typhoid fever, under-nutrition, cholera, malaria.\n\nBy the end of the era, disease and malnutrition were no longer the main leading causes of death.\n"}
{"id": "13590511", "url": "https://en.wikipedia.org/wiki?curid=13590511", "title": "Mycoplasma laboratorium", "text": "Mycoplasma laboratorium\n\nMycoplasma laboratorium is a designed, partially synthetic species of bacterium derived from the genome of \"Mycoplasma genitalium\". This effort in synthetic biology is being undertaken at the J. Craig Venter Institute by a team of approximately 20 scientists headed by Nobel laureate Hamilton Smith and including DNA researcher Craig Venter and microbiologist Clyde A. Hutchison III. \"Mycoplasma genitalium\" was chosen as it was the species with the smallest number of genes known at that time.\n\nOn May 21, 2010, \"Science\" reported that the Venter group had successfully synthesized the genome of the bacterium \"Mycoplasma mycoides\" from a computer record and transplanted it into an existing cell of \"Mycoplasma capricolum\" that had its DNA removed. The team used \"M. mycoides\" instead of \"M. genitalium\" because it grew faster. The new bacterium was viable—that is, capable of replicating billions of times—but not, strictly speaking, a truly synthetic life form.. \n\nIt is estimated that the synthetic genome cost US$40 million and 200 man-years to produce. Despite the controversy, Venter's company Synthetic Genomics has secured over $110 million in investment capital and inked a $300 million deal with Exxon Mobil to design algae for diesel fuel.\n\n\"Mycoplasma genitalium\" was chosen as it was the species with the smallest number of genes known at that time.\n\n\"Mycoplasma\" is a genus of bacteria of the class Mollicutes in the division Tenericutes, characterised by the lack of a cell wall (making it Gram negative) due to its parasitic or commensal lifestyle.\nIn molecular biology, the genus has received much attention, both for being a notoriously difficult-to-eradicate (immune to beta-lactam and other antibiotics) contaminant in mammalian cell cultures, and for its potential uses as a model organism: the second published complete bacterial genome sequence was that of \"Mycoplasma genitalium\", which has one of the smallest genomes of any free-living (non-parasitic) organism. The \"M. pneumoniae\" genome sequence was published soon afterward and was the first genome sequence determined by primer walking of a cosmid library instead of the whole-genome shotgun method. Consequently, this species was chosen as a model for the minimal cell project, catalog the entire protein content of a cell.\n\n\"Pelagibacter ubique\" (an α-proteobacterium of the order Rickettsiales) has the smallest known genome (1,308,759 base pairs) of any free living organism and is one of the smallest self-replicating cells known. It is possibly the most numerous bacterium in the world (perhaps 10 individual cells) and, along with other members of the SAR11 clade, are estimated to make up between a quarter and a half of all bacterial or archaeal cells in the ocean. However, this species was identified only in 2002 by rRNA sequences and was fully sequenced in 2005, being an extremely hard to cultivate species which does not reach a high growth density,\nAdditionally, several newly discovered species have fewer genes than \"M. genitalium\", but many essential genes that are missing in \"Hodgkinia cicadicola\", \"Sulcia muelleri\", \"Baumannia cicadellinicola\" (symbionts of cicadas) and \"Carsonella ruddi\" (symbiote of hackberry petiole gall psyllid, \"Pachypsylla venusta\") may be encoded in the host nucleus as these endosymbionts are acquiring an organelle-like status in a similar way to mitochondria and chloroplasts.\n\nThe team started with the bacterium \"M. genitalium\", an obligate intracellular parasite whose genome consists of 482 genes comprising 582,970 base pairs, arranged on one circular chromosome (the smallest genome of any known natural organism that can be grown in free culture). They then systematically removed genes to find a minimal set of 382 genes that can sustain life. This effort was also known as the Minimal Genome Project.\n\nThe team intends to synthesize chromosome DNA sequences consisting of these 382 genes. Once a version of the minimal 382-gene chromosome has been synthesized, it is intended to be transplanted into a \"M. genitalium\" cell to create Mycoplasma laboratorium.\n\nThe resulting Mycoplasma laboratorium bacterium is expected to be able to replicate itself with its man-made DNA, making it the most synthetic organism to date, although the molecular machinery and chemical environment that would allow it to replicate would not be synthetic.\n\nIn December 2003, the team had reported a fast method of synthesizing a genome from scratch, producing the 5386-base genome of the bacteriophage Phi X 174 in about two weeks. However, the genome of Mycoplasma laboratorium is about 50 times larger. In January 2008, the team reported to have synthesized the complete 582,970 base pair chromosome of \"M. genitalium\", with small modifications so that it won't be infectious and can be distinguished from the wild type. They named this genome \"Mycoplasma genitalium JCVI-1.0\". The team had also demonstrated the process of transplanting a (non-synthetic) genome from one \"Mycoplasma\" species to another in June 2007. In May 2010 they showed that they were able to synthesize the 1,078,809 base pair genome of \"Mycoplasma mycoides\" from scratch and transplant it into a \"Mycoplasma capricolum\" cell; the new genome then took over the cell and the new organism multiplied. The new organism was nicknamed \"Synthia\".\n\nVenter hopes to eventually synthesize bacteria to manufacture hydrogen and biofuels, and also to absorb carbon dioxide and other greenhouse gases. George M. Church, another pioneer in synthetic biology, holds that \"E. coli\" is a more efficient organism than \"M. genitalium\" and that creating a fully synthetic genome is not necessary and too costly for such tasks; he points out that synthetic genes have already been incorporated into \"E.coli\" to perform some of the above tasks.\n\nOn June 28, 2007, a team at the J. Craig Venter Institute published an article in \"Science Express\", saying that they had successfully transplanted the natural DNA from a \"Mycoplasma mycoides\" bacterium into a \"Mycoplasma capricolum\" cell, creating a bacterium which behaved like a \"M. mycoides\".\n\nOn Oct 6, 2007, Craig Venter announced in an interview with UK's \"The Guardian\" newspaper that the same team had synthesized a modified version of the single chromosome of \"Mycoplasma genitalium\" chemically. The chromosome was modified to eliminate all genes which tests in live bacteria had shown to be unnecessary. The next planned step in this \"minimal genome project\" is to transplant the synthesized minimal genome into a bacterial cell with its old DNA removed; the resulting bacterium will be called Mycoplasma laboratorium. The next day the Canadian bioethics group, ETC Group issued a statement through their representative, Pat Mooney, saying Venter's \"creation\" was \"a chassis on which you could build almost anything\". The synthesized genome had not yet been transplanted into a working cell.\n\nOn May 21, 2010, \"Science\" reported that the Venter group had successfully synthesized the genome of the bacterium \"Mycoplasma mycoides\" from a computer record, and transplanted the synthesized genome into the existing cell of a \"Mycoplasma capricolum\" bacterium that had had its DNA removed. The \"synthetic\" bacterium was viable, i.e. capable of replicating billions of times. The team had originally planned to use the \"M. genitalium\" bacterium they had previously been working with, but switched to \"M. mycoides\" because the latter bacterium grows much faster, which translated into quicker experiments. They have also shown that the natural genome of \"M. mycoides\" can be transplanted but has yet to show that the same could be done for \"M. genitalium\". Venter describes it as \"the first species... to have its parents be a computer\". The transformed bacterium is dubbed \"Synthia\" by ETC. A Venter spokesperson has declined to confirm any breakthrough at the time of this writing, likely because similar genetic introduction techniques such as transfection, transformation, transduction and protofection have been a standard research practice for many years.\n\nNow that the technique has been proven to work with the \"M. mycoides\" genome, the next project is presumably to go back to the minimized \"M. genitalium\" and transplant it into a cell to create the previously mentioned Mycoplasma laboratorium.\n\nThe creation of a new synthetic bacterium was announced in \"Science\" on March 25, 2016. It has only 473 genes, the fewest genes of any freely living organism. \nThis fast growing new cell, called Syn 3.0, was created by transplanting the genome of Mycoplasma mycoides, with all the unessential DNA removed, into a Mycoplasma capricolum, which had been emptied of its own DNA.\n\nM. genitalium was not used because it reproduces too slowly.\n\nIn order to propagate a synthetic genome, the technique to transplant an intact whole bacterial genome into another had to be developed.\nOswald Avery's pioneering experiments in the 1940s showed that some bacteria could take up naked DNA, and with the advent of molecular cloning techniques DNA elements could be transformed into competent cells, typically cloning vectors, around 5-20 kbp long, and even bacterial artificial chromosomes can be maintained.\nIn 2007, Venter's team reported that they had managed to transfer the chromosome of the species \"Mycoplasma mycoides\" to \"Mycoplasma capricolum\" by means of:\n\nThe term transformation is used to refer to insertion of a vector into a bacterial cell (by electroporation or heatshock). Here, transplantation is used akin to nuclear transplantation.\n\nThe switch from \"M. genitalium\" to \"M. mycoides\" was spurred due to the faster growth of the latter.\n\nIt is possible to create DNA sequences with the use of chemicals (oligonucleotide synthesis) which is achieved by successive rounds of deprotection and coupling of protected phosphoramidite nucleotides with geometrically decreasing yields to length, making sequences longer than 1000 base-pairs unfeasible. For longer sequences, DNA ligation is required. In 2008 Venter's group published a paper showing that they had managed to create a synthetic genome (a copy of \"M. mycoides\" sequence CP001621) by means of a hierarchical strategy:\n\nIn 2010, using the methods described above, Venter and colleagues created a strain of Mycoplasma mycoides called JCVI-syn1.0 with a synthetic genome. Initially the synthetic construct did not work, so to pinpoint the error—which caused a delay of 3 months in the whole project—a series of semi-synthetic constructs were created. Given the fact that the natural genome worked, the cause of the failed growth was an frameshift mutation in DnaA, a replication initiation factor, which, once corrected, worked and was verified.\n\nThe construction of a cell with a synthetic genome was done to test the methodology—allowing more modified genomes to be created in the future. To minimize sources of failure, the genome was created using a natural genome as a template. Due to the large size of a genome, apart from the elements required for propagation in yeast and residues from restriction sites, several differences are present in \"Mycoplasma mycoides\" JCVI-syn1.0 notably an \"E.coli\" transposon IS1 (an infection from the 10kb stage) and an 85bp duplication.\n\nHowever, the project has received heavy criticism as it claims to have created a synthetic organism. This claim arises from the fact that the genome was synthesized chemically in many pieces (a synthetic method), joined together by means of molecular biological techniques (an artificial method), and transplanted into the cytoplasm of a natural cell (after a few generations, though, the original protein content is undetectable).\nThe two species used as donor and recipient are of the same genus as the more distant two species are, the less the correct protein interactions are maintained, such as binding factors and binding sites, which mutate together (epistasis). Consequently, Paul Keim (a molecular geneticist at Northern Arizona University in Flagstaff) notes that \"there are great challenges ahead before genetic engineers can mix match, and fully design an organism's genome from scratch\" due to this issue.\nDNA is the template for protein construction and requires proteins as helper molecules to do so, a chicken-and-egg conundrum solved by the RNA world hypothesis, consequently synthetic naked DNA would require several proteins to create a viable cell. In 2000 and 2002 teams synthesized replicating hepatitis C virus (about 9600 nucleotides long) and poliovirus (about 7500 nucleotides long), Viruses, however, replicate by utilising host protein expression machinery. Furthermore, whereas DNA can easily be replicated (using DNA polymerase), transcribed (using RNA polymerase), and translated (using ribosomes and many other factors)—all \"in vitro\", such reactions, so far, use cell extracts and most components have not been synthesized de novo—that is from inorganic or synthetically made organic chemicals only.\n\nA much publicised feature of the Mycoplasma laboratorium is the presence of watermark sequences as an ultimate proof of the achievement and as a publicity stunt—similar to the tradition of chip art, inscriptions on unused portions of microchips visible only by electron microscopy. The 4 watermarks (present in figure 1 in supplementary material of ) are coded messages in the form of DNA base pairs, of 1246, 1081, 1109 and 1222 base pairs respectively, in natural peptides the 4 nucleotides encode in sets of 3 the 20 natural amino acids by means of the standard genetic code. Each amino acid \"by convention\" is represented by a letter, but in nature there is nothing which ties alanine, a molecule, to the Latin letter A, a vowel, so this convention was disregarded in the latter watermarks. In the minimal genome organism the watermark were encoded as amino acids, with V as U, both in reference to Latin inscriptions and the lack of a standard amino acid for U containing the names of the researchers:\n(VENTERINSTITVTE, CRAIGVENTER, HAMSMITH, CINDIANDCLYDE, GLASSANDCLYDE). In the synthetic organism, instead the Latin alphabet—which in English has 26 letters, which is covered only in base 4 with 3 or more digits—was encoded by an undisclosed encoding. The encoding is fixed and 3 digits make an uppercase letter or symbol, possibly randomly allocated (not table, frequency or keyboard order). The content of the watermarks is as follows:\n\nThe main controversy from the project is the undue amount of publicity it received from the press due to Venter's showmanship, to the degree that Jay Keasling, a pioneering synthetic biologist and founder of Amyris says \"The only regulation we need is of my colleague's mouth\".\n\nDespite the funding for practical applications, as stressed by George M. Church, one of the main players in the field of synthetic biology, a few changes are required to obtain useful organisms now, such as biofuel production or bioremediation. However, speculation about the distant future's possible applications is rife.\nVenter himself is prone to such speculations, having asked for example, \"What if we can make algae taste like beef?\" If it were possible to create a synthetic cell without the use of preexisting recipient cells, however, many applications would become achievable which would be otherwise unattainable, such as a completely overhauled bacterium that works in a logically controlled way—removing what has been described as 'evolutionary messiness'—with lower mutation rates, categorical gene arrangement (colinearity), the possibility of adding novel nucleotides to increase encoding (a feat achieved in vitro (PCR)); or with a completely novel genetic code, such as has been achieved by experiments in which a few additional non-canonical amino acids were added.\n\nThe J. Craig Venter Institute filed patents for the Mycoplasma laboratorium genome (the \"minimal bacterial genome\") in the U.S. and internationally in 2006. This extension of the domain of biological patents is being challenged by the watchdog organization Action Group on Erosion, Technology and Concentration.\n\nIn 2016, the Venter Institute used genes from JCVI-syn1.0 to synthesize an even smaller genome they call JCVI-syn3.0, that contains 531,560 base pairs and 473 genes. Originally in 1996, after comparing \"M. genitalium\" with another small bacterium \"Haemophilus influenza\", Arcady Mushegian and Eugene Koonin had proposed that there might be a common set of 256 genes which could be a minimal set of genes needed for viability. In this new organism, the number of genes can only be pared down to 473, 149 of which whose functions are completely unknown. The organism with the smallest known set of genes is \"Nasuia deltocephalinicola\", which is an obligate symbiont that cannot survive outside its host, has only 137 genes with a genome size of 112 kb, but the minimal genome for any free-living organism would be different depending on its environment.\n\nFrom 2002 to 2010, a team at the Hungarian Academy of Science created a strain of \"Escherichia coli\" called MDS42, which is now sold by Scarab Genomics of Madison, WI under the name of \"Clean Genome. E.coli\", where 15% of the genome of the parental strain (E. coli K-12 MG1655 ) were removed to aid in molecular biology efficiency, removing IS elements, pseudogenes and phages, resulting in better maintenance of plasmid-encoded toxic genes, which are often inactivated by transposons. Biochemistry and replication machinery were not altered.\n\n"}
{"id": "51427743", "url": "https://en.wikipedia.org/wiki?curid=51427743", "title": "Necropolitics", "text": "Necropolitics\n\nNecropolitics is the use of social and political power to dictate how some people may live and how some must die.\n\nAchille Mbembe, author of \"On the Postcolony\", was the first scholar to explore the term in depth in his article of the same name. Necropolitics is often discussed with biopower, the Foucauldian term for the use of social and political power to control people's lives.\n\nMbembe was clear that necropolitics is more than a right to kill (Foucault's \"droit de glaive\"), but also the right to expose other people (including a country's own citizens) to death. His view of necropolitics also included the right to impose social or civil death, the right to enslave others, and other forms of political violence. Necropolitics is a theory of the walking dead, namely a way of analysing how \"contemporary forms of subjugation of life to the power of death\" forces some bodies to remain in different states of being located between life and death. Mbembe uses the examples of slavery, apartheid, the colonisation of Palestine and the figure of the suicide bomber to show how different forms of necropower over the body (statist, racialised, a state of exception, urgency, martyrdom) reduce people to precarious conditions of life.\n\nJasbir Puar coined the term \"queer necropolitics\" to analyze the post-9/11 queer outrage regarding gay bashing and simultaneous queer complicity with Islamophobia. Many scholars use Puar's queer necropolics in conjunction with Judith Butler's concept of a grievable life. Queer necropolitics is the subject of an anthology from Routledge.\n"}
{"id": "30770792", "url": "https://en.wikipedia.org/wiki?curid=30770792", "title": "Nico Aaltonen", "text": "Nico Aaltonen\n\nNico Aaltonen (born June 15, 1988) is a Finnish ice hockey player who currently plays professionally in Finland for Manchester Phoenix in the Premier League. He had previously played in the SM-liiga with Lukko, Ässät, HPK and Espoo Blues. Aaltonen joined Espoo Blues on a try-out contract on August 1, 2014 which was followed by him joining Arlan Kokshetau in 2015. The same year, he joined and then left Manchester Phoenix.\n\n"}
{"id": "50218653", "url": "https://en.wikipedia.org/wiki?curid=50218653", "title": "Nicotinamide mononucleotide", "text": "Nicotinamide mononucleotide\n\nNicotinamide mononucleotide (\"NMN\" and \"β-NMN\") is a nucleotide derived from ribose and nicotinamide. Like nicotinamide riboside, NMN is a derivative of niacin, and humans have enzymes that can use NMN to generate nicotinamide adenine dinucleotide (NADH).\n\nBecause NADH is a cofactor for processes inside mitochondria, for sirtuins, and for PARP, NMN has been studied in animal models as a potential neuroprotective and anti-aging agent. Dietary supplement companies have aggressively marketed NMN products claiming those benefits, though there is no clinical study on humans published yet.\n"}
{"id": "47951796", "url": "https://en.wikipedia.org/wiki?curid=47951796", "title": "Peter Aaby", "text": "Peter Aaby\n\nPeter Aaby (Danish, born 1944 in Lund, Sweden) is trained as an anthropologist but also holds a doctoral degree in medicine. In 1978, Peter Aaby established the Bandim Health Project, a Health and Demographic Surveillance System site in Guinea-Bissau in West Africa, which he has run ever since. In 2000, Peter Aaby was awarded the Novo Nordisk Prize, the most important Danish award within health research.\n\nAaby is credited for the discovery of non-specific effects of vaccines – i.e. effects of vaccines, which go beyond the specific protective effects against the targeted diseases. The theory of non-specific effects of vaccines was established in 1991 and later documented in several trials on measles vaccine, BCG, oral polio vaccine, DTP vaccine and smallpox vaccine. As a consequence of Aaby’s work on non-specific effects of vaccines it has been recommended the WHO vaccination program in low income countries should be changed. WHO recently reviewed the evidence for non-specific effects of BCG vaccine, measles vaccine and DTP vaccine, and concluded that it would \"keep a watch on the evidence of nonspecific effects of vaccination\".\n\n\n"}
{"id": "35803963", "url": "https://en.wikipedia.org/wiki?curid=35803963", "title": "Phaedon", "text": "Phaedon\n\nPhaedon (), published in 1767, is a book by the Jewish Enlightenment philosopher Moses Mendelssohn, in which Mendelssohn offers a defense of immortality.\n\n\"Phaedon\" is a defense of the simplicity and immortality of the soul. It is a series of three dialogues, revisiting the Platonic dialogue Phaedo, in which Socrates argues for the immortality of the soul, in preparation for his own death. Many philosophers, including Plotinus, Descartes, and Leibniz, argue that the soul is simple, and that because simples cannot decompose they must be immortal. In the \"Phaedon\", Mendelssohn addresses gaps in earlier versions of this argument (an argument that Kant calls \"the Achilles of Rationalist Psychology\"). The Phaedon contains an original argument for the simplicity of the soul, and also an original argument that simples cannot suddenly disappear. It contains further original arguments that the soul must retain its rational capacities as long as it exists.\n\nMaterialistic views were at the time rampant and fashionable, and faith in immortality was at a low ebb. At this favourable juncture appeared Phädon oder über die Unsterblichkeit der Seele (Phaedo or On the Immortality of Souls; 1767). Modelled on Plato's dialogue of the same name, Mendelssohn's work possessed some of the charm of its Greek exemplar and impressed the German world with its beauty and lucidity of style. The Phaedon was an immediate success, and besides being one of the most widely read books of its time in German it was speedily translated into several European languages, including English. The author was hailed as the \"German Plato,\" or the \"German Socrates\"; royal and other aristocratic friends showered attentions on him, and it was said that \"no stranger who came to Berlin failed to pay his personal respects to the German Socrates.\"\n\nImmanuel Kant criticized Mendelssohn's argument for immortality in the second edition of the \"Critique of Pure Reason\" (1787), at B413–15. Commentators disagree over whether Kant's criticism is successful. Mendelssohn's arguments have been largely overlooked by contemporary analytic philosophers, but philosophers including Bertrand Russell and E.J. Lowe have offered arguments for the simplicity of the soul.\n\n"}
{"id": "57408363", "url": "https://en.wikipedia.org/wiki?curid=57408363", "title": "Reda Aadel", "text": "Reda Aadel\n\nReda Aadel (born 28 December 1990) is a Moroccan cyclist.\n"}
{"id": "13620318", "url": "https://en.wikipedia.org/wiki?curid=13620318", "title": "Religion of Humanity", "text": "Religion of Humanity\n\nReligion of Humanity (from French \"Religion de l'Humanité\" or \"église positiviste\") is a secular religion created by Auguste Comte, the founder of positivist philosophy. Adherents of this religion have built chapels of Humanity in France and Brazil.\n\nIn the United States and Europe, Comte's ideas influenced others, and contributed to the emergence of ethical societies and \"ethical churches\", which led to the development of Ethical culture, congregational humanist, and secular humanist organisations.\n\nComte developed the \"religion of humanity\" for positivist societies in order to fulfill the cohesive function once held by traditional worship. The religion was developed after Comte's passionate platonic relationship with Clotilde de Vaux, whom he idealised after her death. He became convinced that feminine values embodied the triumph of sentiment and morality. In a future science-based Positivist society there should also be a religion that would have power by virtue of moral force alone. In 1849, he proposed a calendar reform called the \"positivist calendar\", in which months were named after history's greatest leaders, thinkers, and artists, and arranged in chronological order. Each day was dedicated to a thinker.\n\nAccording to Tony Davies, Comte's secular and positive religion was \"a complete system of belief and ritual, with liturgy and sacraments, priesthood and pontiff, all organized around the public veneration of Humanity\", referred to as the \"Nouveau Grand-Être Suprême\" (New Supreme Great Being). \"This was later to be supplemented in a positivist trinity by the \"Grand Fétish\" (the Earth) and the \"Grand Milieu\" (Cosmic Space)\". \n\nIn \"Système de politique positive\" (1851–1854) Comte stated that the pillars of the religion are: \n\nIn \"Catéchisme positiviste\" (1851), Comte defined the Church of Humanity's seven sacraments:\n\nThe Religion of Humanity was described by Thomas Huxley as \"Catholicism minus Christianity\". In addition to a holy trinity of Humanity, the Earth and Destiny, it had a priesthood. Priests were \"required\" to be married, because of the ennobling influence of womanhood. They would conduct services, including Positivist prayer, which was \"a solemn out-pouring, whether in private or in public, of men's nobler feelings, inspiring them with larger and more comprehensive thoughts.\" The purpose of the religion was to increase altruism, so that believers acted always in the best interests of humanity as a whole. The priests would be international ambassadors of altruism, teaching, arbitrating in industrial and political disputes, and directing public opinion. They should be scholars, physicians, poets and artists. Indeed all the arts, including dancing and singing should be practiced by them, like bards in ancient societies. \n\nThis required long training. They began training from the age of twenty-eight, studying in positivist schools. From thirty-five to forty-two a priest served in an apprentice position as teacher and ritualist. Only at the age of forty-two could he become a full priest. They earned no money and could not hold offices outside the priesthood. In this way their influence was purely spiritual and moral. The High Priest of Humanity was to live in Paris, which would replace Rome as the centre of religion.\n\nDavies argues that Comte's austere and \"slightly dispiriting\" philosophy of humanity - viewed as alone in an indifferent universe (which can only be explained by \"positive\" science) - \"was even more influential in Victorian England than the theories of Charles Darwin or Karl Marx\".\n\nThe \"system\" was ultimately unsuccessful but, along with Darwin's \"On the Origin of Species\", it influenced the proliferation of various Secular Humanist organizations in the 19th century, especially through the work of secularists such as George Holyoake and Richard Congreve. Although Comte's English followers, including George Eliot and Harriet Martineau, for the most part rejected the full panoply of his system, they liked the idea of a religion of humanity and his injunction to \"vivre pour altrui\" (\"live for others\", from which comes the word \"altruism\").\n\nProfound criticism came from John Stuart Mill who advocated Comte but dismissed his Religion of Humanity in a move towards a differentiation between the (good) early Comte, the author of \"The Course in Positive Philosophy\" and the (problematic) late Comte, who authored the Religion of Humanity. While sympathising with the need for a secular religion, and appreciating Comte’s respect for “the Human Race, conceived as a continuous whole, including the past, he present and the future”, Mill thought that the details of Comte’s ritualism were not only illiberal but “could have been written by no man who had ever laughed”.\nComtean Positivism was relatively popular in Brazil. In 1881 Miguel Lemos and Raimundo Teixeira Mendes organized the \"Positivist Church of Brazil.\" In 1897 the \"Temple of Humanity\" was created. The services at the Temple could go on for up to four hours and that, combined with a certain moral strictness, led to some decline during the Republican period. Nevertheless it had appeal with the military class as Benjamin Constant joined the group before breaking with it because he deemed Mendes and Lemos as too fanatical. Cândido Rondon's conversion proved more solid as he remained an orthodox Positivist and a member of the faith long after the church's importance waned. Although declined, the church still survives in Brazil. The national flag of Brazil bears the \"Ordem e Progresso\" (\"Order and Progress\"), inspired by Comte's motto of positivism: \"L’amour pour principe et l’ordre pour base; le progrès pour but\" (\"Love as a principle and order as the basis; progress as the goal\").\n\nThere are more examples of Religion of Humanity started by positivists, and there are several authors who have given the epithet to the religion they support, whatever the religion. In India Baba Faqir Chand established Manavta Mandir (Temple of Humanity) to spread his religion of humanity with scientific attitude as explained by David C. Lane in a book \"The Unknowing Sage\". Comte influenced the thought of Victorian secularists George Holyoake (coiner of the term \"secularism\") and Richard Congreve.\n\n\n"}
{"id": "27364349", "url": "https://en.wikipedia.org/wiki?curid=27364349", "title": "Rolf M. Aagaard", "text": "Rolf M. Aagaard\n\nRolf Magdal Aagaard (born 27 March 1945) is a Norwegian photographer.\n\nHe was born in Risør. He worked for one year for \"Tiden\" and seven years for \"Fædrelandsvennen\" before being hired by \"Aftenposten\" in 1970. He was awarded the Narvesen Prize in 1979. He has also held exhibitions, and he has written books.\n"}
{"id": "54799459", "url": "https://en.wikipedia.org/wiki?curid=54799459", "title": "Rolling and wheeled creatures in fiction and legend", "text": "Rolling and wheeled creatures in fiction and legend\n\nLegends and speculative fiction reveal a longstanding human fascination with rolling and wheeled creatures. Such creatures appear in mythologies from Europe, Japan, pre-Columbian Mexico, the United States, and Australia, and in numerous modern works.\n\nThe hoop snake, a creature of legend in the United States and Australia, is said to grasp its tail in its mouth and roll like a wheel towards its prey. Japanese culture includes a similar mythical creature, the \"Tsuchinoko\".\n\nBuer, a demon mentioned in the 16th-century grimoire \"Pseudomonarchia Daemonum\", was described in Collin de Plancy's 1825 edition of \"Dictionnaire Infernal\" as having \"the shape of a star or wheel\". The 1863 edition of this book featured an illustration by Louis Le Breton, depicting a creature with five legs radially arranged.\n\nNeil R. Jones' 1937 story \"On the Planet Fragment\" featured aliens dubbed the Disci, which were shaped like wheels with limbs around the circumference. One of their methods of locomotion was a \"rolling motion like that of a cartwheel.\"\n\nThe 1944 science fiction short story \"Arena\", by Fredric Brown, features a telepathic alien called an Outsider, which is roughly spherical and moves by rolling. The story was the basis for a 1967 \"\" episode of the same name, and possibly also a 1964 episode of \"The Outer Limits\" entitled \"Fun and Games\", though neither television treatment included a spherical creature.\n\nThe Dutch graphic artist M. C. Escher invented a creature that was capable of rolling itself forward, which he named \"Pedalternorotandomovens centroculatus articulosus\". He illustrated this creature in his 1951 lithograph (also known by the English title \"Curl-up\").\n\nA 1956 Scrooge McDuck comic, \"Land Beneath the Ground!\", by Carl Barks, introduced Terries and Fermies (a play on the phrase \"terra firma\"), creatures who move from place to place by rolling. The Terries and Fermies have made a sport of their rolling abilities, causing earthquakes in the process.\n\nNorthern Irish author James White's Sector General series features \"Rollers\" from the planet Drambo, doughnut-shaped aquatic organisms that do not have hearts, but which instead must roll continuously to maintain circulation by means of gravity. The Rollers are described in the short story \"Spacebird\" in the 1980 edition of \"Ambulance Ship\", and in other works in the series.\n\nIn \"The Citadel of Chaos\" (1983) by Steve Jackson, the reader may encounter Wheelies, disc-shaped creatures with four arms who move by doing cartwheels.\n\n\"Tuf Voyaging\", a 1986 science fiction novel by George R. R. Martin, features an alien called a Rolleram, described as a \"berserk living cannonball of enormous size\", which kills its prey by rolling over it and crushing it, before digesting it externally. Adults of the species weigh approximately six metric tons and can roll faster than .\n\nIn the \"Sonic the Hedgehog\" video game series, which first appeared in 1991, the eponymous Sonic and his sidekick Tails move by rolling.\n\nThe 1995 short story \"Microbe\", by Kenyon College biologist and feminist science fiction writer Joan Slonczewski, describes an exploratory expedition to an alien world whose plant and animal life consists entirely of doughnut-shaped organisms.\n\nToy animals with wheels dating from the Pre-Columbian era were uncovered by archaeologists in Veracruz, Mexico in the 1940s. The indigenous peoples of this region did not use wheels for transportation prior to the arrival of Europeans.\n\nL. Frank Baum's 1907 children's novel \"Ozma of Oz\" features humanoid creatures with wheels instead of hands and feet, called Wheelers. Their wheels are composed of keratin, which has been suggested by biologists as a means of avoiding nutrient and waste transfer problems with living wheels. Despite moving quickly on open terrain, the Wheelers are stymied by obstacles in their path that do not hinder creatures with limbs. They also make an appearance in the 1985 film \"Return to Oz\", based partly on \"Ozma of Oz\".\n\nThe 1968 novel \"The Goblin Reservation\" by Clifford D. Simak features an intelligent alien race that uses biological wheels.\n\n\"Chorlton and the Wheelies\", a British stop-motion-animated television series that aired from 1976 to 1979, was set in \"Wheelie World\", which was inhabited by three-wheeled creatures called \"wheelies\".\n\nPiers Anthony's 1977 book \"Cluster\" and its sequels feature aliens called Polarians, which locomote by gripping and balancing atop a large ball. The ball is a living, though temporarily separable, portion of the Polarian's body.\n\nDavid Brin's \"Uplift\" Universe includes a wheeled species called the g'Kek, which are described in some detail in the 1995 novel \"Brightness Reef\". In 1996's \"Infinity's Shore\", a g'Kek is described as looking like \"a squid in a wheelchair.\" The g'Kek suffer from arthritic axles in their old age, particularly when living in a high gravity environment.\n\nA 1997 novel in the \"Animorphs\" series, \"The Andalite Chronicles\", includes an alien called a Mortron, composed of two separate entities: a yellow and black bottom half with four wheels, and a red, elongated head with razor-sharp teeth and concealed wings.\n\nThe 2000 novel \"The Amber Spyglass\", by English author Philip Pullman, features an alien race known as the Mulefa, which have diamond-shaped bodies with one leg at the front and back and one on each side. The Mulefa use large, disk-shaped seed pods as wheels. They mount the pods on bone axles on their front and back legs, while propelling themselves with their side legs. The Mulefa have a symbiotic relationship with the seed pod trees, which depend on the rolling action to crack open the pods and allow the seeds to disperse.\n\nIn the 2000 novel \"Wheelers\", by English mathematician Ian Stewart and reproductive biologist Jack Cohen, a Jovian species called \"blimps\" has developed the ability to biologically produce machines called \"wheelers\", which use wheels for locomotion.\n\nThe children's television series \"Jungle Junction\", which premiered in 2009, features hybrid jungle animals with wheels rather than legs; one such animal, Ellyvan, is a hybrid of an elephant and a van. These animals traverse their habitat on elevated highways.\n"}
{"id": "15826297", "url": "https://en.wikipedia.org/wiki?curid=15826297", "title": "Seriousness", "text": "Seriousness\n\nSeriousness (noun; adjective: serious) is an attitude of gravity, solemnity, persistence, and earnestness toward something considered to be of importance. Some notable philosophers and commentators have criticised excessive seriousness, while others have praised it. Seriousness is often contrasted with comedy, as in the seriocomedy. In the theory of humor, one must have a sense of humor and a sense of seriousness to distinguish what is supposed to be taken literally or not, or of being important or not. Otherwise, it may also be contrasted with a sense of play. How children learn a sense of seriousness to form values and differentiate between the serious and that which is not is studied in developmental psychology and educational psychology. There is a distinction between the degree of seriousness of various crimes in sentencing under the law, and also in law enforcement. There is a positive correlation with the degree of seriousness of a crime and viewer ratings of news coverage. What is or is not considered serious varies widely with different cultures.\n\nSometimes fields studying degrees of seriousness overlap, such as developmental psychology studies of development of the sense of degrees of seriousness as it relates to transgressions, which has overlap with criminology and the seriousness of crimes. \nSource:Alchilagyo.com\n\nSome use \"seriousness\" as a term of praise for scholarship or in literary review. 19th century poet, cultural critic, and literary critic, Matthew Arnold said that the most important criteria used to judge the value of a poem were \"high truth\" and \"high seriousness\".\n\nMany have expressed an attitude of disdain toward taking things too seriously, as opposed to viewing things with an attitude of humor. Poet, playwright, and philosopher Joseph Addison said that being serious is dull, \"we are growing serious, and let me tell you, that's the next step to being dull.\" Political satirist P.J. O'Rourke said that \"Seriousness is stupidity sent to college.\" Epigramist, poet, and playwright Oscar Wilde said that \"life is too important to be taken seriously.\" In a play on words, novelist Samuel Butler indicated that the \"central serious conviction in life\" is that nothing should be taken with too much seriousness, \"the one serious conviction that a man should have is that nothing is to be taken too seriously.\"\n\nIn some ascetic or puritan religious sects, an attitude of seriousness is always to be taken, and solemnity, sobriety, and puritanism with its hostility to social pleasures and indulgences are the only acceptable attitudes. Perry Miller, \"the master of American intellectual history\", wrote of excessive seriousness of the Puritans, \"simple humanity cries at last for some relief from the interminable high seriousness of the Puritan code.\"\n\nExistentialist philosopher Jean-Paul Sartre called the \"spirit of seriousness’’ the belief that there is an objective and independent goodness in things for people to discover, and that this belief leads to bad faith. He argued that people forget that values are not absolute, but are contingent and subjectively determined. In Sartre’s words, \"the spirit of seriousness has two characteristics: it considers values as transcendent ‘'givens’’, independent of human subjectivity, and it transfers the quality of ‘desirable’ from the ontological structure of things to their simple material constitution.\"\n\nSeriousness is sometimes contrasted with the comical in humor. In the performing arts and literature, the seriocomedy is a genre which blends seriousness with the comical, drama with comedy.\n\nIn the theory of humor, one must have a sense of humor and a sense of seriousness to distinguish what is supposed to be taken literally or not. An even more keen sense is needed when humor is used to make a serious point. Psychologists have studied how humor is intended to be taken as having seriousness, as when court jesters used humor to convey serious information. Conversely, when humor is not intended to be taken seriously, bad taste in humor may cross a line after which it is taken seriously, though not intended.\n\nIn Developmental psychology and educational psychology, seriousness is studied as it relates to how children develop an ability to distinguish levels of seriousness as it relates to transgressions and expenditure of time; for example, a child must learn to distinguish between levels of seriousness in admonitions such as between \"don't fidget\" and \"don't forget to look both ways when crossing the street\", which have the same linguistic and normative structure, but different levels of seriousness.\n\nThe degree of seriousness of crimes is an important factor relating to crime. One standard for measurement is the degree to which a crime affects others or society. A felony is generally considered to be a crime of \"high seriousness\", while a misdemeanor is not.\n\nIn criminal law the degree of seriousness is considered when meting out punishment to fit the crime, and in considering to what extent overcrowded prison facilities will be used. Seriousness of a crime is a major factor in considerations of the allocation of scarce law enforcement funds.\n\nThe meaning and measurement of seriousness is a major concern in public policy considerations. A quantitative scoring system called the \"seriousness score\" has been developed for use in allocating law enforcement resources and sentencing.\n\nAs to England and Wales, see section 143 of the Criminal Justice Act 2003.\n\nDegrees of seriousness are used in medicine to make decisions about care. Seriousness is related to the effects of delaying or not having medical care. In an emergency hospital, the triage nurse must evaluate levels of seriousness of medical emergencies and rank them to determine order of care. Seriousness of illness is used to make decisions as to whether to perform invasive procedures such as surgery.\n\nThere is a positive correlation between the degree of seriousness of a crime and viewer ratings of news coverage.\n\nWhat is considered serious varies widely across cultures and is studied in sociology, cultural anthropology, and criminology; being of the wrong religious faith may be considered a serious crime in some cultures; smoking marijuana may be a serious crime in some cultures and not others; homosexuality a serious crime in some cultures; and prostitution is a serious crime in some cultures. Perception of seriousness is measured in assessing varying cultural perceptions on health risks.\n"}
{"id": "39107086", "url": "https://en.wikipedia.org/wiki?curid=39107086", "title": "Sexual and reproductive health and rights", "text": "Sexual and reproductive health and rights\n\nSexual and reproductive health and rights or \"SRHR\" is an of human rights applied to sexuality and reproduction. It is a combination of four fields that in some contexts are more or less distinct from each other, but less so or not at all in other contexts. These four fields are sexual health, sexual rights, reproductive health and reproductive rights. In the concept of SRHR, these four fields are treated as separate but inherently intertwined.\n\nDistinctions between these four fields are not always made. Sexual health and reproductive health are sometimes treated as synonymous to each other, as are sexual rights and reproductive rights. In some cases, sexual rights are included in the term sexual health, or vice versa. Not only do different non-governmental organisations (NGOs) and governments use different terminologies, but different terminologies are often used within the same organization.\n\nSome of the notable global NGOs that fight for sexual and reproductive health and rights include IPPF (International Planned Parenthood Federation), ILGA (International Lesbian and Gay Alliance), WAS (World Association for Sexual Health - formerly known as World Association for Sexology), and International HIV/AIDS Alliance.\n\nGovernment-run family planning programs first began in the 1950s. However, the main objectives of these programs were often centered around population control for economic growth and development. In 1994, the International Conference on Population and Development (ICPD) in Cairo, Egypt marked a significant shift in perspective in regards to reproductive health and is considered to be the birth of the modern SRHR movement. Over the course of the conference, debates surrounding family planning shifted from that of economics to that of public health and human rights. A Program of Action (PoA) was developed by the end of the ICPD and was approved and adopted by 179 countries. The PoA affirmed sexual and reproductive health as a universal human right and outlined global goals and objectives for improving reproductive heath based around central themes of free choice, women's empowerment, and viewing sexual and reproductive health in terms of physical and emotional well-being. The PoA outlined a series of goals, based on a central mission of achieving universal access to reproductive health worldwide, that were aimed to be accomplished by 2015. In 2000, the Millennium Development Goals (MDGs) were developed, and although reproductive health was not explicitly stated as one of the goals, it became an important component to Goals 3, 4, and 5. In 2010, the original PoA was revisited by the United Nations and updated to reflect their objective of achieving universal reproductive health care by 2015. When the MDGs and ICPD PoA phased out in 2015, the next objectives for SRHR were folded into the Sustainable Development Goals, the next iteration of the MDGs which outline objectives to combat poverty through 2030.\n\nThe World Health Organization defines sexual health as: \"Sexual health is a state of physical, mental and social well-being in relation to sexuality. It requires a positive and respectful approach to sexuality and sexual relationships, as well as the possibility of having pleasurable and safe sexual experiences, free of coercion, discrimination and violence.\"\n\nUnlike the other three aspects of SRHR, the struggle for sexual rights include, and focus on, sexual pleasure and emotional sexual expression. One platform for this struggle is the WAS Declaration of Sexual Rights.\n\nThe Platform for Action from the 1995 Beijing Conference on Women established that human rights include the right of women freely and without coercion, violence or discrimination, to have control over and make decisions concerning their own sexuality, including their own sexual and reproductive health. This paragraph has been interpreted by some countries as the applicable definition of women’s sexual rights. The UN Commission on Human Rights has established that if women had more power, their ability to protect themselves against violence would be strengthened.\n\nAt the 14th World Congress of Sexology (Hong Kong, 1999), the WAS adopted the Declaration of Sexual Rights, which originally included 11 sexual rights. It was heavily revised and expanded in March 2014 by the WAS Advisory Council to include 16 sexual rights:\n\nThis Declaration influenced The Yogyakarta Principles (which were launched as a set of international principles relating to sexual orientation and gender identity on 26 March 2007), especially on the idea of each person's integrity, and right to sexual and reproductive health.\n\nIn 2015 the U.S. government said it would begin using the term \"sexual rights\" in discussions of human rights and global development.\n\nWithin the framework of the World Health Organization's (WHO) definition of health as a state of complete physical, mental and social well-being, and not merely the absence of disease or infirmity, reproductive health, or sexual health/hygiene, addresses the reproductive processes, functions and system at all stages of life. Reproductive health, therefore, implies that people are able to have a responsible, satisfying and safer sex life and that they have the capability to reproduce and the freedom to decide if, when and how often to do so. One interpretation of this implies that men and women ought to be informed of and to have access to safe, effective, affordable and acceptable methods of birth control; also access to appropriate health care services of sexual, reproductive medicine and implementation of health education programs to stress the importance of women to go safely through pregnancy and childbirth could provide couples with the best chance of having a healthy infant. On the other hand, individuals do face inequalities in reproductive health services. Inequalities vary based on socioeconomic status, education level, age, ethnicity, religion, and resources available in their environment. It is possible for example, that low income individuals lack the resources for appropriate health services and the knowledge to know what is appropriate for maintaining reproductive health.\n\nReproductive rights are legal rights and freedoms relating to reproduction and reproductive health. The World Health Organization defines reproductive rights as follows:\n\nReproductive rights rest on the recognition of the basic right of all couples and individuals to decide freely and responsibly the number, spacing and timing of their children and to have the information and means to do so, and the right to attain the highest standard of sexual and reproductive health. They also include the right of all to make decisions concerning reproduction free of discrimination, coercion and violence.\nDespite frequent changes to frameworks, overall goals for SRHR remain little changed. As first stipulated at the ICPD, universal reproductive health care remains the ultimate objective, and with each new framework, targets are developed to progress towards this. In the original ICPD Program of Action, the primary call was for universal access to healthcare, including reproductive healthcare, family planning and sexual health. Over time, these have expanded to include the right to access education regarding sexual and reproductive health, an end to female genital mutilation, and increased women's empowerment in social, political, and cultural spheres.\n\nSpecial goals and targets were also created to address adolescent sexual and reproductive health needs. Adolescents are often the most vulnerable to risks associated with sexual activity, including HIV, due to personal and social issues such as feelings of isolation, child marriage, and stigmatization. Governments realized the importance of investing in the health of adolescents as a means of establishing future well-being for their societies. As a result, the Commission on Population and Development developed a series of fundamental rights for adolescents including the right to comprehensive sex education, the right to decide all matters related to their sexuality, and access to sexual and reproductive health services without discrimination (including safe abortions wherever legal).\n\n"}
{"id": "2703791", "url": "https://en.wikipedia.org/wiki?curid=2703791", "title": "Shaker Aamer", "text": "Shaker Aamer\n\nShaker Aamer (born 21 December 1966) is a Saudi citizen who was held by the United States in the Guantanamo Bay detention camp in Cuba for more than thirteen years without charge. Amer was the last British resident to be held at Guantanamo Bay; he was released to Great Britain on October 30, 2015.\n\nAamer was seized in Afghanistan by bounty hunters, who handed him over to US forces in December 2001 during the United States invasion of the country. Two months later, the US rendered Aamer to the Guantánamo camp; he was held there without trial or charge. Aamer had been a legal resident in Britain for years before his imprisonment; the UK government repeatedly demanded his release, and many people there called for him to be released.\n\nAccording to documents published in the Guantanamo Bay files leak, the US military Joint Task Force Guantanamo believed that Aamer had led a unit of fighters in Afghanistan, including the Battle of Tora Bora, while his family was paid a stipend by Osama bin Laden. The file asserts past associations with Richard Reid and Zacarias Moussaoui. Aamer denies being involved in terrorist activity and his lawyer, Clive Stafford Smith, said the leaked documents would not stand up in court. He claimed that part of the evidence came from an unreliable witness and that confessions Aamer made had been obtained through torture. Aamer's father-in-law, Saaed Ahmed Siddique, said: \"All of these claims have no basis. If any of this was true he would be in a court now.\" The Bush administration acknowledged later that it had no evidence against Aamer.\n\nAamer has never been charged with any wrongdoing, was never on trial, and his lawyer says he is \"totally innocent.\" He was approved for transfer to Saudi Arabia by the Bush administration in 2007 and the Obama administration in 2009. He has been described as a \"charismatic leader\" who spoke up and fought for the rights of fellow prisoners. Aamer alleges that he has been subject to torture while in detention. Campaigners allege that the US refused to release Aamer because it feared he would expose torture inside the Guantanamo prison.\n\nAamer has suffered decline in his mental and physical health over the years, as he participated in hunger strikes to protest his detention conditions, and was held in solitary confinement for much of the time. He claims to have lost 40 per cent of his body weight in captivity. After a visit in November 2011, his lawyer said, \"I do not think it is stretching matters to say that he is gradually dying in Guantanamo Bay.\" In 2015, despite Aamer's deteriorating health, the US denied a request for an independent medical examination. On 25 September 2015 the US government announced that Aamer would be released to the UK within thirty days. He was released to the UK on 30 October 2015.\n\nAamer was born on 21 December 1966 and grew up in Medina in Saudi Arabia. He left the country at the age of 17. He lived and traveled in the United States, Europe and the Middle East. Aamer lived and studied in Georgia and Maryland in 1989 and 1990. During the Gulf War, he worked as a translator for the U.S. Army.\n\nHe moved to the United Kingdom in 1996 where he met Zin Siddique, a British woman; they married in 1997 and he established legal residency in Britain. They have four British children, the youngest of whom Aamer had never met, due to his having been born after Aamer's imprisonment. Aamer had indefinite leave to remain in the UK, and was applying for British citizenship.\n\nAamer worked as an Arabic translator for London law firms. Some of the solicitors he worked for dealt with immigration cases. In his spare time, Aamer helped refugees find accommodation and offered them advice on their struggles with the Home Office.\n\nIn 2012, Aamer's family lived in Battersea, South London. His wife Zin Aamer suffered from depression and mental episodes since his arrest. Saeed Siddique, Aamer's father-in-law, said in 2011, \"When he was captured, Shaker offered to let my daughter divorce him, but she said, 'No, I will wait for you.' She is still waiting.\"\n\nAamer took his family to Afghanistan in 2001, where he was working for an Islamic charity when the U.S. invaded the country later that year. The Northern Alliance took him into custody in Jalalabad in December 2001, and passed him to the Americans. The US routinely paid ransom for Arabs handed over to them. They interrogated Aamer at Bagram Theater Internment Facility and transported him to Guantánamo on 14 February 2002.\n\nAccording to Joint Task Force Guantanamo assessments from 1 November 2007, the US military believed that Aamer was a \"recruiter, financier, and facilitator\" for al-Qaeda, based partly on spurious evidence given by the informant Yasim Muhammed Basardah, a fellow detainee. The leaked documents alleged that Aamer had confessed to interrogators that he was in Tora Bora with Osama bin Laden at the time of the US bombing. The documents further note that the Saudi intelligence Mabahith identified Aamer \"as a high priority for the government of Saudi Arabia, an indication of his law enforcement value to them.\"\n\nIn 2010 the Guantanamo Review Task Force released their report of the detainee assessments. In many instances, the Task Force largely agreed with prior threat assessments of the detainees and sometimes found additional information that further substantiated such assessments. In other instances, the Task Force found prior assessments to be overstated. Some assessments, for example, contained allegations that were not supported by the underlying source document upon which they relied. Other assessments contained conclusions that were stated categorically even though derived from uncorroborated statements or raw intelligence reporting of undetermined or questionable reliability. Conversely, in a few cases, the Task Force discovered reliable information indicating that a detainee posed a greater threat in some respects than prior assessments suggested.\n\nAamer denies being involved in terrorist activity and his attorney, Clive Stafford Smith of Reprieve, said the evidence against his client \"would not stand up in court.\" He pointed out that part of the evidence comes from Yasim Muhammed Basardah, whom American judges found to be \"utterly incredible\" and who was tortured and \"promised all sorts of things.\"\n\nThe Bush administration acknowledged later that it had no evidence against Aamer, and he was cleared for transfer in 2007. The clearance was for transfer to Saudi Arabia only.\n\nIn September 2009, Zachary Katznelson, a Reprieve lawyer, said that Aamer had told of suffering severe beatings at the Bagram facility. Aamer said that close to a dozen men had beaten him, including interrogators who represented themselves as officers of MI5, the United Kingdom's internal counter-terrorism agency. Following one severe beating, he recovered from being stunned to find that all the interrogators had left the room and put a pistol on the table. He did not determine if the pistol was loaded. He said it occurred to him that it had been left either so he could kill himself, or that, if he picked it up, he could be shot and killed on the excuse he was trying to shoot them.\n\nAamer says that the \"MI5\" interrogators told him he had two choices: (1) agree to spy on suspected jihadists in the United Kingdom; or (2) remain in US custody. He said that guards/agents repeatedly knocked his head against the wall while an MI5 officer was in the room.\n\nAll I know is that I felt someone grab my head and start beating my head into the back wall – so hard that my head was bouncing. And they were shouting that they would kill me or I would die.\n\nOther former detainees have alleged similar mistreatment by MI5 and MI6 agents, including torture. Seven detainees filed suit against the British government over their mistreatment and torture. In November 2010, the British government settled the suit, paying the detainees millions of pounds in compensation. Aamer is also on the compensation list and part of the deal, but details are not known as most of the deal is still secret.\n\nAamer has been described as an unofficial spokesman for the detainees at Guantanamo. He has spoken up for the welfare of prisoners, negotiating with camp commanders and organizing protests against cruel treatment. He organized and participated in a hunger strike in 2005 in which he lost half of his weight. He demanded the prisoners be treated according to the Geneva Convention, allowing the detainees to form a grievance committee. In negotiations, the camp administration promised a healthier diet for the prisoners after he agreed to end the hunger strike. His lawyer Stafford Smith said the grievance committee was formed, but that the camp authorities disbanded it after a few days. American spokesmen Major Jeffrey Weir denied that the Americans had ever agreed to any conditions resulting from the hunger strike.\n\nIn September 2006, Aamer's attorneys filed a 16-page motion arguing for his removal from isolation in Guantanamo Bay prison. They argued extended periods of isolation were detrimental to his mental and physical health.\n\nAamer continued to take part in additional hunger strikes and was held in solitary confinement for most of the time. His lawyers described his solitary confinement as \"cruel\" and said his health was affected to a point where they feared for his life. In 2011 Stafford Smith, director of the UK branch of Reprieve, said Aamer is \"falling apart at the seams.\"\n\nOn 18 September 2006, Aamer's attorneys filed a 16-page motion arguing for his removal from isolation in Guantanamo Bay prison. The motion alleges that Aamer had been held in solitary confinement for 360 days at the time of filing, and was tortured by beatings, exposure to temperature extremes, and sleep deprivation, which together caused him to suffer to the point of becoming mentally unbalanced. The next day Katznelson filed a motion to enforce the Geneva Conventions on his behalf.\n\nAfter President Barack Obama was elected, in 2009 he convened a six-agency task force to review the status of detainees at Guantanamo. It \"unanimously recommended\" transfer of Aamer. Security officials wanted to send him to Saudi Arabia, his country of citizenship, but his attorneys argued for him to be transferred to Great Britain, where he had been resident and had family.\n\nIn September 2011, Aamer's lawyer Brent Mickum, who saw him in Guantánamo, alleges that Aamer was repeatedly beaten before their meetings. He said that Aamer's mental and physical health is deteriorating. \"It felt like he has given up: that's what 10 years, mostly in solitary confinement will do to a person,\" he said.\n\nBinyam Mohamed, an Ethiopian prisoner who formerly occupied a cell one door down from Aamer, has said since his release that he knows why Aamer is still in the prison camps.\n\nI would say the Americans are trying to keep him as silent as they could. It's not that he has anything. What happened in 2005 and 2006 is something that the Americans don't want the world to know – hunger strikes, and all the events that took place, until the three brothers who died ... insider information of all the events, probably. Obviously, Shaker doesn't have it, but the Americans think he may have some of it, and they don't like this kind of information being released.\n\nClive Stafford Smith, his lawyer and director of human rights organisation Reprieve, came to a similar conclusion. He said:\n\nI have known Shaker for some time, because he is so eloquent and outspoken about the injustices of Guantanamo he is very definitely viewed as a threat by the US. Not in the sense of being an extremist but in the sense of being someone who can rather eloquently criticise the nightmare that happened there.\n\nOmar Deghayes, a former Guantanamo Detainee who knew Aamer, said of him,\n\nHe was always forward, he would translate for people, he'd fight for them, and if he had any problems in the block he'd shout at the guards... until he would get you your rights. And that's why he's still in prison... because he's very outspoken, a very intelligent person, somebody who would fight for somebody else's rights.\n\nIn an article published in 2010, Aamer said that he was beaten for hours and subjected to interrogation methods that included asphyxiation on 9 June 2006, the same day that three fellow prisoners died in Guantanamo. The United States claimed these deaths were suicides.\n\nDescribing his treatment, Aamer said that he was strapped to a chair, fully restrained at the head, arms and legs. When MPs pressed on pressure points all over his body: his temples, just under his jawline, in the hollow beneath his ears. They bent his nose repeatedly, pinched his thighs and feet. They inflicted pain to his eyes, bent his fingers until he screamed and then they cut off his airway and put a mask over him, so he could not cry out.\n\nThe law professor Scott Horton published an award-winning article on the 2006 deaths in \"Harper's Magazine\" in 2010, suggesting that these were cases of homicide caused by extended torture, rather than suicide. He said that Aamer had been brought to \"Camp No,\" a secret interrogation black site outside the camp, with the three men who died on the day of the event. Horton described Aamer's account of having his airways cut off as \"alarming\" and wrote, \"This is the same technique that appears to have been used on the three deceased prisoners.\" Colonel Michael Bumgarner, the commander of the camps during the incident and identified in Horton's article as having been present during the interrogations, denied Horton's claims.\n\nHorton wrote that Aamer's repatriation was being delayed so that he could not testify about his alleged torture in Bagram or the events on 9 June 2006. He wrote: \"American authorities may be concerned that Aamer, if released, could provide evidence against them in criminal investigations.\"\n\nIn 2013, Aamer told his attorneys that he was among the growing group of active hunger strikers. He said he had been refusing meals since February 15 and had lost 32 pounds. In previous hunger strikes guards force-fed him with tubes down his nose. His lawyer said Aamer spent 22 hours a day alone in his cell. Aamer was not permitted visitors except his attorneys. Aamer was among a group of detainees who filed a court challenge to the authorities' practice of force feeding those on hunger strikes. A United States appellate court ruled in 2014 \"that the judiciary could oversee conditions of confinement at the prison.\"\n\nIn 2014, his lawyers filed a motion on Aamer's behalf seeking his release on the grounds that his health is \"gravely diminished,\". They argued that his various health problems could not be treated in Guantanamo and \"\"even if he receives the intensive medical and therapeutic treatment his condition requires, Mr Aamer will take many years, if not a lifetime, to achieve any significant recovery\". His lawyers argued that both the Geneva Convention and Army Regulation 190-8, require the repatriation of chronically ill prisoners. In 2015 despite Aamer's deteriorating health, the US denied attorneys' request for an independent medical examination.\n\nThe United Kingdom government initially refused to intervene on the behalf of Guantánamo detainees who were legal British residents but were not British citizens. In August 2007, Foreign Secretary David Miliband requested the release of Aamer and four other men, based on their having been granted refugee status, or similar leave, to remain in Britain as residents prior to their capture by US forces. With the repatriation of Binyam Mohammed in February 2009, all British citizens and residents other than Aamer had been released.\n\nThe UK government officials repeatedly raised Aamer's case with the Americans. On a visit to the United States on 13 March 2009, when asked about Guantánamo captives, Home Secretary Jacqui Smith said that the US administration has said they do not want to return Aamer to the UK. William Hague, the Foreign Secretary, raised Aamer's case again with Hillary Clinton, US Secretary of State, in November 2010, followed by meetings with other US officials. At the time, the US government had reached settlement with former detainees as a resolution for damages due to the use of torture in interrogation.\n\nIn September 2011, Foreign Office Minister Alistair Burt said that negotiations were ongoing and confidential. Supporters of Aamer criticized the UK government for not doing enough on his behalf; they urged the government to step up their efforts. In January 2012, \"The Independent\" revealed that the British government has spent £274,345 fighting in court to prevent Aamer's lawyers from gaining access to evidence which may prove his innocence. The newspaper reported that Aamer had several serious medical complaints from years of \"inhumane\" detention conditions, and that the UK gave false hope to his family.\n\n\nOn 30 October 2015, Aamer was flown from Cuba, stepping on British soil at 13.00 GMT. In a later interview he discussed his detention and family life. He also called upon jihadis to \"get the hell out\" of Britain, stating that civilian killings were \"not allowed\" in Islam, and went on to say that \"you cannot just go in the street and get a knife and start stabbing people\" in apparent reference to the murder of Lee Rigby.\n\n\n\n\n"}
{"id": "58687892", "url": "https://en.wikipedia.org/wiki?curid=58687892", "title": "Snoh Aalegra", "text": "Snoh Aalegra\n\nSnoh Nowrozi (born 13 September 1987), known professionally as Snoh Aalegra (), is a Swedish singer and songwriter.\n\nSnoh Nowrozi was born on 13 September 1987 in Uppsala, Sweden to Persian parents. She grew up in Enköping, Sweden.\n\nIn 2001, Aalegra signed to Sony Music Sweden at the age of 14, but no music was released though the deal. Aalegra said \"things didn't really work out with [Sony Sweden], but I'm glad I had the opportunity to be exposed to the industry at a young age.\"\n\nIn 2009, Aalegra began working on her debut album for Mamia Music, though Universal Music Sweden. Aalegra began her music career as Sheri, with the release of her debut single, \"Hit And Run\" on 16 February 2009, with production by Andreas Carlsson. The single peaked at number 12 on the Swedish Single Charts. Aalegra released her second single \"U Got Me Good\" on 4 December 2009, which peaked at number 2 on the Swedish Single Charts.\n\nOn 1 January 2010, Aalegra release her first studio album \"First Sign\", which features a cover of the 1984 song, \"Smooth Operator\" by Sade. The album also includes her first two single, \"Hit And Run\" and \"U Got Me Good\".\n\nIn July 2014, Aalegra debut her new name, Snoh Aalegra, and appeared on Common's tenth studio album, \"Nobody's Smiling\". She released her debut EP, \"There Will Be Sunshine\" on 17 November 2014 through Epic Records. The EP includes \"Bad Things\" featuring Common and \"Stockholm, Pt. II (Outro)\" featuring Cocaine 80s.\n\nOn 11 February 2015, Aalegra released her first single as Snoh Aalegra, \"Emotional\", with production by RZA. She also appeared on Vince Staples' \"Summertime '06\" in June 2015, providing vocals for the song \"Jump Off the Road\".\n\nAalegra signed with No I.D.'s ARTium Recordings in 2016 and released an EP, \"Don't Explain\" on 8 April 2016, with features and production from James Fauntleroy, No I.D., Boi-1da, Christian Rich and DJ Dahi. The EP features a cover of the 1944 song, \"Don't Explain\" by Billie Holiday and Arthur Herzog Jr..\n\nIn 2017, Aalegra released her debut album, \"Feels\", with features from Vince Staples, Vic Mensa, and Timbuktu. Her song from the album, \"Time\" was sampled on Drake’s \"More Life\" song \"Do Not Disturb\" and on September 12, 2018, her song \"Nothing Burns Like the Cold\" featuring Vince Staples, was used by Apple for their iPhone XS announcement video and commercials.\n\nAalegra briefly moved from Stockholm to London before moving to Los Angeles, California, where she has resided since 2012. Aalegra speaks Farsi, Swedish and English.\n\nAalegra cites James Brown, Michael Jackson, Whitney Houston and Prince as musical influences.\n\n"}
{"id": "168927", "url": "https://en.wikipedia.org/wiki?curid=168927", "title": "Somatic cell nuclear transfer", "text": "Somatic cell nuclear transfer\n\nIn genetics and developmental biology, somatic cell nuclear transfer (SCNT) is a laboratory strategy for creating a viable embryo from a body cell and an egg cell. The technique consists of taking an enucleated oocyte (egg cell) and implanting a donor nucleus from a somatic (body) cell. It is used in both therapeutic and reproductive cloning. Dolly the Sheep became famous for being the first successful case of the reproductive cloning of a mammal. In January 2018, a team of scientists in Shanghai announced the successful cloning of two female crab-eating macaques (named Zhong Zhong and Hua Hua) from fetal nuclei. \"Therapeutic cloning\" refers to the potential use of SCNT in regenerative medicine; this approach has been championed as an answer to the many issues concerning embryonic stem cells (ESC) and the destruction of viable embryos for medical use, though questions remain on how homologous the two cell types truly are.\n\nSomatic cell nuclear transfer is a technique for cloning in which the nucleus of a somatic cell is transferred to the cytoplasm of an enucleated egg. When this is done, the cytoplasmic factors affect the nucleus to become a zygote. The blastocyst stage is developed by the egg which helps to create embryonic stem cells from the inner cell mass of the blastocyst. The first animal that was developed by this technique was Dolly, the sheep, in 1996.\n\nThe process of somatic cell nuclear transplant involves two different cells. The first being a female gamete, known as the ovum (egg/oocyte). In human SCNT (Somatic Cell Nuclear Transfer) experiments, these eggs are obtained through consenting donors, utilizing ovarian stimulation. The second being a somatic cell, referring to the cells of the human body. Skin cells, fat cells, and liver cells are only a few examples. The nucleus of the donor egg cell is removed and discarded, leaving it 'deprogrammed.' What is left is a somatic cell and an denucleated egg cell. These are then fused by inserting the somatic cell into the 'empty' ovum. After being inserted into the egg, the somatic cell nucleus is reprogrammed by its host egg cell. The ovum, now containing the somatic cell's nucleus, is stimulated with a shock and will begin to divide. The egg is now viable and capable of producing an adult organism containing all the necessary genetic information from just one parent. Development will ensue normally and after many mitotic divisions, this single cell forms a blastocyst (an early stage embryo with about 100 cells) with an identical genome to the original organism (i.e. a clone). Stem cells can then be obtained by the destruction of this clone embryo for use in therapeutic cloning or in the case of reproductive cloning the clone embryo is implanted into a host mother for further development and brought to term.\n\nSomatic cell nuclear transplantation has become a focus of study in stem cell research. The aim of carrying out this procedure is to obtain pluripotent cells from a cloned embryo. These cells genetically matched the donor organism from which they came. This gives them the ability to create patient specific pluripotent cells, which could then be used in therapies or disease research.\n\nEmbryonic stem cells are undifferentiated cells of an embryo. These cells are deemed to have a pluripotent potential because they have the ability to give rise to all of the tissues found in an adult organism. This ability allows stem cells to create any cell type, which could then be transplanted to replace damaged or destroyed cells. Controversy surrounds human ESC work due to the destruction of viable human embryos. Leading scientists to seek an alternative method of obtaining stem cells, SCNT is one such method.\n\nA potential use of stem cells genetically matched to a patient would be to create cell lines that have genes linked to a patient's particular disease. By doing so, an \"in vitro\" model could be created, would be useful for studying that particular disease, potentially discovering its pathophysiology, and discovering therapies. For example, if a person with Parkinson's disease donated his or her somatic cells, the stem cells resulting from SCNT would have genes that contribute to Parkinson's disease. The disease specific stem cell lines could then be studied in order to better understand the condition.\n\nAnother application of SCNT stem cell research is using the patient specific stem cell lines to generate tissues or even organs for transplant into the specific patient. The resulting cells would be genetically identical to the somatic cell donor, thus avoiding any complications from immune system rejection.\n\nOnly a handful of the labs in the world are currently using SCNT techniques in human stem cell research. In the United States, scientists at the Harvard Stem Cell Institute, the University of California San Francisco, the Oregon Health & Science University, Stemagen (La Jolla, CA) and possibly Advanced Cell Technology are currently researching a technique to use somatic cell nuclear transfer to produce embryonic stem cells. In the United Kingdom, the Human Fertilisation and Embryology Authority has granted permission to research groups at the Roslin Institute and the Newcastle Centre for Life. SCNT may also be occurring in China.\n\nIn 2005, a South Korean research team led by Professor Hwang Woo-suk, published claims to have derived stem cell lines via SCNT, but supported those claims with fabricated data. Recent evidence has proved that he in fact created a stem cell line from a parthenote.\n\nThough there has been numerous successes with cloning animals, questions remain concerning the mechanisms of reprogramming in the ovum. Despite many attempts, success in creating human nuclear transfer embryonic stem cells has been limited. There lies a problem in the human cell's ability to form a blastocyst; the cells fail to progress past the eight cell stage of development. This is thought to be a result from the somatic cell nucleus being unable to turn on embryonic genes crucial for proper development. These earlier experiments used procedures developed in non-primate animals with little success.\n\nA research group from the Oregon Health & Science University demonstrated SCNT procedures developed for primates successfully using skin cells. The key to their success was utilizing oocytes in metaphase II (MII) of the cell cycle. Egg cells in MII contain special factors in the cytoplasm that have a special ability in reprogramming implanted somatic cell nuclei into cells with pluripotent states. When the ovum's nucleus is removed, the cell loses its genetic information. This has been blamed for why enucleated eggs are hampered in their reprogramming ability. It is theorized the critical embryonic genes are physically linked to oocyte chromosomes, enucleation negatively affects these factors. Another possibility is removing the egg nucleus or inserting the somatic nucleus causes damage to the cytoplast, affecting reprogramming ability.\n\nTaking this into account the research group applied their new technique in an attempt to produce human SCNT stem cells. In May 2013, the Oregon group reported the successful derivation of human embryonic stem cell lines derived through SCNT, using fetal and infant donor cells. Using MII oocytes from volunteers and their improved SCNT procedure, human clone embryos were successfully produced. These embryos were of poor quality, lacking a substantial inner cell mass and poorly constructed trophectoderm. The imperfect embryos prevented the acquisition of human ESC. The addition of caffeine during the removal of the ovum's nucleus and injection of the somatic nucleus improved blastocyst formation and ESC isolation. The ESC obtain were found to be capable of producing teratomas, expressed pluripotent transcription factors, and expressed a normal 46XX karyotype, indicating these SCNT were in fact ESC-like. This was the first instance of successfully using SCNT to reprogram human somatic cells. This study used fetal and infantile somatic cells to produce their ESC.\n\nIn April 2014, an international research team expanded on this break through. There remained the question of whether the same success could be accomplished using adult somatic cells. Epigenetic and age related changes were thought to possibly hinder an adult somatic cells ability to be reprogrammed. Implementing the procedure pioneered by the Oregon research group they indeed were able to grow stem cells generated by SCNT using adult cells from two donors, aged 35 and 75.Indicating age does not impede a cells ability to be reprogrammed\n\nLate April 2014, the New York Stem Cell Foundation was successful in creating SCNT stem cells derived from adult somatic cells. One of these lines of stem cells was derived from the donor cells of a type 1 diabetic. The group was then able to successfully culture these stem cells and induce differentiation. When injected into mice, cells of all three of the germ layers successfully formed. The most significant of these cells, were those who expressed insulin and were capable of secreting the hormone. These insulin producing cells could be used for replacement therapy in diabetics, demonstrating real SCNT stem cell therapeutic potential.\n\nThe impetus for SCNT-based stem cell research has been decreased by the development and improvement of alternative methods of generating stem cells. Methods to reprogram normal body cells into pluripotent stem cells were developed in humans in 2007. The following year, this method achieved a key goal of SCNT-based stem cell research: the derivation of pluripotent stem cell lines that have all genes linked to various diseases. Some scientists working on SCNT-based stem cell research have recently moved to the new methods of induced pluripotent stem cells. Though recent studies have put in question how similar iPS cells are to embryonic stem cells. Epigenetic memory in iPS affects the cell lineage it can differentiate into. For instance, an iPS cell derived from a blood cell will be more efficient at differentiating into blood cells, while it will be less efficient at creating a neuron. This raises the question of how well iPS cells can mimic the gold standard ESC in experiments, as stem cells are defined as having the ability to differentiate into any cell type. SCNT stem cells do not pose such a problem and continue to remain relevant in stem cell studies.\n\nThis technique is currently the basis for cloning animals (such as the famous Dolly the sheep), and has been theoretically proposed as a possible way to clone humans. Using SCNT in reproductive cloning has proven difficult with limited success. High fetal and neonatal death make the process very inefficient. Resulting cloned offspring are also plagued with development and imprinting disorders in non-human species. For these reasons, along with moral and ethical objections, reproductive cloning in humans is proscribed in more than 30 countries. Most researchers believe that in the foreseeable future it will not be possible to use the current cloning technique to produce a human clone that will develop to term. It remains a possibility, though critical adjustments will be required to overcome current limitations during early embryonic development in human SCNT.\n\nThere is also the potential for treating diseases associated with mutations in mitochondrial DNA. Recent studies show SCNT of the nucleus of a body cell afflicted with one of these diseases into a healthy oocyte prevents the inheritance of the mitochondrial disease. This treatment does not involve cloning but would produce a child with three genetic parents. A father providing a sperm cell, one mother providing the egg nucleus and another mother providing the enucleated egg cell.\n\nIn 2018, the first successful cloning of primates using somatic cell nuclear transfer, the same method as Dolly the sheep, with the birth of two live female clones (crab-eating macaques named \"Zhong Zhong\" and \"Hua Hua\") was reported.\n\nInterspecies nuclear transfer (iSCNT) is a means of somatic cell nuclear transfer used to facilitate the rescue of endangered species, or even to restore species after their extinction. The technique is similar to SCNT cloning which typically is between domestic animals and rodents, or where there is a ready supply of oocytes and surrogate animals. However, the cloning of highly endangered or extinct species requires the use of an alternative method of cloning. Interspecies nuclear transfer utilizes a host and a donor of two different organisms that are closely related species and within the same genus. In 2000, Robert Lanza was able to produce a cloned fetus of a gaur, \"Bos gaurus\", combining it successfully with a domestic cow, \"Bos taurus\".\n\nInterspecies nuclear transfer provides evidence of the universality of the triggering mechanism of the cell nucleus reprogramming. For example, Gupta et al., explored the possibility of producing transgenic cloned embryos by interspecies somatic cell nuclear transfer (iSCNT) of cattle, mice, and chicken donor cells into enucleated pig oocytes. Moreover, NCSU23 medium, which was designed for in vitro culture of pig embryos, was able to support the in vitro development of cattle, mice, and chicken iSCNT embryos up to the blastocyst stage. Furthermore, ovine oocyte cytoplast may be used for remodeling and reprogramming of human somatic cells back to the embryonic stage.\n\nSCNT can be inefficient. Stresses placed on both the egg cell and the introduced nucleus in early research were enormous, resulting in a low percentage of successfully reprogrammed cells. For example, in 1996 Dolly the sheep was born after 277 eggs were used for SCNT, which created 29 viable embryos. Only three of these embryos survived until birth, and only one survived to adulthood. As the procedure was not automated, but had to be performed manually under a microscope, SCNT was very resource intensive. The biochemistry involved in reprogramming the differentiated somatic cell nucleus and activating the recipient egg was also far from understood. However, by 2014, researchers were reporting success rates of 70-80% with cloning pigs and in 2016 a Korean company, Sooam Biotech, was reported to be producing 500 cloned embryos a day.\n\nIn SCNT, not all of the donor cell's genetic information is transferred, as the donor cell's mitochondria that contain their own mitochondrial DNA are left behind. The resulting hybrid cells retain those mitochondrial structures which originally belonged to the egg. As a consequence, clones such as Dolly that are born from SCNT are not perfect copies of the donor of the nucleus. This fact may also hamper the potential benefits of SCNT derived tissues/organs for therapy, as there may be an immunoresponse to the non-self mtDNA after transplant.\n\nProposals to use nucleus transfer techniques in human stem cell research raise a set of concerns beyond the moral status of any created embryo. These have led to some individuals and organizations who are \"not\" opposed to human embryonic stem cell research to be concerned about, or opposed to, SCNT research.\n\nOne concern is that blastula creation in SCNT-based human stem cell research will lead to the reproductive cloning of humans. Both processes use the same first step: the creation of a nuclear transferred embryo, most likely via SCNT. Those who hold this concern often advocate for strong regulation of SCNT to preclude implantation of any derived products for the intention of human reproduction, or its prohibition.\n\nA second important concern is the appropriate source of the eggs that are needed. SCNT requires human eggs, which can only be obtained from women. The most common source of these eggs today are eggs that are produced and in excess of the clinical need during IVF treatment. This is a minimally invasive procedure, but it does carry some health risks, such as ovarian hyperstimulation syndrome.\n\nOne vision for successful stem cell therapies is to create custom stem cell lines for patients. Each custom stem cell line would consist of a collection of identical stem cells each carrying the patient's own DNA, thus reducing or eliminating any problems with rejection when the stem cells were transplanted for treatment. For example, to treat a man with Parkinson's disease, a cell nucleus from one of his cells would be transplanted by SCNT into an egg cell from an egg donor, creating a unique lineage of stem cells almost identical to the patient's own cells. (There would be differences. For example, the mitochondrial DNA would be the same as that of the egg donor. In comparison, his own cells would carry the mitochondrial DNA of his mother.)\n\nPotentially millions of patients could benefit from stem cell therapy, and each patient would require a large number of donated eggs in order to successfully create a single custom therapeutic stem cell line. Such large numbers of donated eggs would exceed the number of eggs currently left over and available from couples trying to have children through assisted reproductive technology. Therefore, healthy young women would need to be induced to sell eggs to be used in the creation of custom stem cell lines that could then be purchased by the medical industry and sold to patients. It is so far unclear where all these eggs would come from.\n\nStem cell experts consider it unlikely that such large numbers of human egg donations would occur in a developed country because of the unknown long-term public health effects of treating large numbers of healthy young women with heavy doses of hormones in order to induce hyperovulation (ovulating several eggs at once). Although such treatments have been performed for several decades now, the long-term effects have not been studied or declared safe to use on a large scale on otherwise healthy women. Longer-term treatments with much lower doses of hormones are known to increase the rate of cancer decades later. Whether hormone treatments to induce hyperovulation could have similar effects is unknown. There are also ethical questions surrounding paying for eggs. In general, marketing body parts is considered unethical and is banned in most countries. Human eggs have been a notable exception to this rule for some time.\n\nTo address the problem of creating a human egg market, some stem cell researchers are investigating the possibility of creating artificial eggs. If successful, human egg donations would not be needed to create custom stem cell lines. However, this technology may be a long way off.\n\nSCNT involving human cells is currently legal for research purposes in the United Kingdom, having been incorporated into the Human Fertilisation and Embryology Act 1990. Permission must be obtained from the Human Fertilisation and Embryology Authority in order to perform or attempt SCNT.\n\nIn the United States, the practice remains legal, as it has not been addressed by federal law. However, in 2002, a moratorium on United States federal funding for SCNT prohibits funding the practice for the purposes of research. Thus, though legal, SCNT cannot be federally funded. American scholars have recently argued that because the product of SCNT is a clone embryo, rather than a human embryo, these policies are morally wrong and should be revised.\n\nIn 2003, the United Nations adopted a proposal submitted by Costa Rica, calling on member states to \"prohibit all forms of human cloning in as much as they are incompatible with human dignity and the protection of human life.\" This phrase may include SCNT, depending on interpretation.\n\nThe Council of Europe's \"Convention on Human Rights and Biomedicine\" and its \"Additional Protocol to the Convention for the Protection of Human Rights and Dignity of the Human Being with regard to the Application of Biology and Medicine, on the Prohibition of Cloning Human Being\" appear to ban SCNT of human beings. Of the Council's 45 member states, the \"Convention\" has been signed by 31 and ratified by 18. The \"Additional Protocol\" has been signed by 29 member nations and ratified by 14.\n\n\n\n"}
{"id": "2289941", "url": "https://en.wikipedia.org/wiki?curid=2289941", "title": "Terror management theory", "text": "Terror management theory\n\nTerror management theory (TMT) is an evolutionary psychology theory originally proposed by Jeff Greenberg, Sheldon Solomon, and Tom Pyszczynski and codified in their book \"The Worm at the Core: On the Role of Death in Life\" (2015). It proposes that a basic psychological conflict results from having a self-preservation instinct whilst realizing that death is inevitable and to some extent unpredictable. This conflict produces terror, and the terror is then managed by embracing cultural values, or symbolic systems that act to provide life with enduring meaning and value. \n\nExamples of cultural values that impact an individual's terror of death are those that purport to offer literal immortality (e.g. belief in afterlife, religion). However, TMT also argues that other cultural values – including those that are seemingly unrelated to death – offer symbolic immortality. For example, values of national identity, posterity, cultural perspectives on sex, and human superiority over animals have been linked to death concerns. In many cases these values are thought to offer symbolic immortality either a) by providing the sense that one is part of something greater that will ultimately outlive the individual (e.g. country, lineage, species), or b) by making one's symbolic identity superior to biological nature (i.e. you are a personality, which makes you more than a glob of cells).\n\nBecause cultural values determine that which is meaningful, they are also the foundation for self-esteem. TMT describes self-esteem as being the personal, subjective measure of how well an individual is living up to their cultural values.\n\nTMT is derived from anthropologist Ernest Becker's 1973 Pulitzer Prize-winning work of nonfiction \"The Denial of Death\", in which Becker argues most human action is taken to ignore or avoid the inevitability of death. The terror of absolute annihilation creates such a profound – albeit subconscious – anxiety in people that they spend their lives attempting to make sense of it. On large scales, societies build symbols: laws, religious meaning systems, cultures, and belief systems to explain the significance of life, define what makes certain characteristics, skills, and talents extraordinary, reward others whom they find exemplify certain attributes, and punish or kill others who do not adhere to their cultural worldview. On an individual level, self-esteem provides a buffer against death-related anxiety.\n\nCultural anthropologist Ernest Becker asserted in his 1973 book \"The Denial of Death\" that humans, as intelligent animals, are able to grasp the inevitability of death. They therefore spend their lives building and believing in cultural elements that illustrate how to make themselves stand out as individuals and give their lives significance and meaning. Death creates an anxiety in humans; it strikes at unexpected and random moments, and its nature is essentially unknowable, causing people to spend most of their time and energy to explain, forestall, and avoid it.\n\nBecker expounded upon the previous writings of Sigmund Freud, Søren Kierkegaard, Norman O. Brown, and Otto Rank. According to clinical psychiatrist Morton Levitt, Becker replaces the Freudian preoccupation with sexuality with the fear of death as the primary motivation in human behavior.\n\nPeople desire to think of themselves as beings of value and worth with a feeling of permanence, a concept in psychology known as self-esteem. This feeling counters the cognitive dissonance created by an individual's realization that they may be no more important than any other living thing. Becker refers to high self-esteem as heroism: \n\nThe rationale behind decisions regarding one's own health can be explored through a terror management model. A 2008 research article in Psychological Review proposes a three-part model for understanding how awareness of death can ironically subvert health-promoting behaviors by redirecting one's focus towards behaviors that build self-esteem instead: \"Proposition 1 suggests that conscious thoughts about death can instigate health-oriented responses aimed at removing death-related thoughts from current focal attention. Proposition 2 suggests that the unconscious resonance of death-related cognition promotes self-oriented defenses directed toward maintaining, not one's health, but a sense of meaning and self-esteem. The last proposition suggests that confrontations with the physical body may undermine symbolic defenses and thus present a previously unrecognized barrier to health promotion activities.\"\n\nTerror management theorists consider TMT to be compatible with the theory of evolution: Valid fears of dangerous things have an adaptive function that helped facilitate the survival of our ancestors' genes. However, generalized existential anxiety resulting from the clash between a desire for life and awareness of the inevitability of death is neither adaptive nor selected for. TMT views existential anxiety as an unfortunate byproduct of these two highly adaptive human proclivities rather than as an adaptation that the evolutionary process selected for its advantages. Just as human bipedalism confers advantages as well as disadvantages, death anxiety is an inevitable part of our intelligence and awareness of dangers.\n\nAnxiety in response to the inevitability of death threatened to undermine adaptive functioning and therefore needed amelioration. TMT posits that humankind used the same intellectual capacities that gave rise to this problem to fashion cultural beliefs and values that provided protection against this potential anxiety. TMT considers these cultural beliefs (even unpleasant and frightening ones, such as ritual human sacrifice) when they manage potential death anxiety in a way that promotes beliefs and behaviors which facilitated the functioning and survival of the collective.\n\nHunter-gatherers used their emerging cognitive abilities to facilitate solving practical problems, such as basic needs for nutrition, mating, and tool-making. As these abilities evolved, an explicit awareness of death also emerged. But once this awareness materialized, the potential for terror that it created put pressure on emerging conceptions of reality. Any conceptual formation that was to be widely accepted by the group needed to provide a means of managing this terror.\n\nOriginally, the emergence of morality evolved to facilitate co-existence within groups. Together with language, morality served pragmatic functions that extended survival. The struggle to deny the finality of death co-opted and changed the function of these cultural inventions. For example, Neanderthals might have begun burying their dead as a means of avoiding unpleasant odors, disease-infested parasites, or dangerous scavengers. But during the Upper Paleolithic era, these pragmatic burial practices appear to have become imbued with layers of ritual performance and supernatural beliefs, suggested by the elaborate decoration of bodies with thousands of beads or other markers. Food and other necessities were also included within the burial chamber, indicating the potential for a belief system that included life after death. In many human cultures today, funerals are viewed primarily as cultural events, viewed through the lens of morality and language, with little thought given to the utilitarian origins of burying the dead.\n\nEvolutionary history also indicates that \"the costs of ignoring threats have outweighed the costs of ignoring opportunities for self-development.\" This reinforces the concept that abstract needs for individual and group self-esteem may continue to be selected for by evolution, even when they sometimes confer risks to physical health and well-being.\n\nSelf-esteem lies at the heart of TMT and is a fundamental aspect of its core paradigms. TMT fundamentally seeks to elucidate the causes and consequences of a need for self-esteem. Theoretically, it draws heavily from Ernest Becker's conceptions of culture and self-esteem (Becker, 1971; Becker, 1973). TMT not only attempts to explain the concept of self-esteem, it also tries to explain \"why\" we need self-esteem. One explanation is that self-esteem is used as a coping mechanism for anxiety. It helps people control their sense of terror and nullify the realization that humans are just animals trying to manage the world around them. According to TMT, self-esteem is a sense of personal value that is created by beliefs in the validity of one's cultural worldview, and the belief that one is living up to the cultural standards created by that worldview.\n\nCritically, Hewstone \"et al.\" (2002) have questioned the causal direction between self-esteem and death-anxiety, evaluating whether one's self-esteem comes from their desire to reduce their death anxiety, or if death anxiety arises from a lack of self-esteem. In other words, an individual's suppression of death anxiety may arise from their overall need to increase their self-esteem in a positive manner.\n\nResearch has demonstrated that self-esteem can play an important role in physical health. In some cases, people may be so concerned with their physical appearance and boosting their self-esteem that they ignore problems or concerns with their own physical health. Arndt \"et al.\" (2009) conducted three studies to examine how peer perceptions and social acceptance of smokers contributes to their quitting, as well as if, and why these people continue smoking for outside reasons, even when faced with thoughts of death and anti-smoking prompts. Tanning and exercising were also looked at in the researchers' studies. The studies found that people are influenced by the situations around them. Specifically, Arndt \"et al.\" (2009) found in terms of their self-esteem and health, that participants who saw someone exercising were more likely to increase their intentions to exercise. In addition, the researchers found in study two that how participants reacted to an anti-smoking commercial was affected by their motivation for smoking and the situation which they were in. For instance, people who smoked for extrinsic reasons and were previously prompted with death reminders were more likely to be compelled by the anti-smoking message.\n\nAn individual's level of self-consciousness can affect their views on life and death. To a point, increasing self-consciousness is adaptive in that it helps prevent awareness of danger. However, research has demonstrated that there may be diminishing returns from this phenomenon. Individuals with higher levels of self-consciousness sometimes have increased death cognition, and a more negative outlook on life, than those with reduced self-consciousness.\n\nConversely, self-esteem can work in the opposite manner. Research has confirmed that individuals with higher self-esteem, particularly in regard to their behavior, have a more positive attitude towards their life. Specifically, death cognition in the form of anti-smoking warnings weren't effective for smokers and in fact, increased their already positive attitudes towards the behavior. The reasons behind individuals' optimistic attitudes towards smoking after mortality was made salient, indicate that people use positivity as a buffer against anxiety. Continuing to hold certain beliefs even after they are shown to be flawed creates cognitive dissonance regarding current information and past behavior, and the way to alleviate this is to simply reject new information. Therefore, anxiety buffers such as self-esteem allow individuals to cope with their fears more easily. Death cognition may in fact cause negative reinforcement that leads people to further engage in dangerous behaviors (smoking in this instance) because accepting the new information would lead to a loss of self-esteem, increasing vulnerability and awareness of mortality.\n\nThe mortality salience hypothesis (MS) states that if indeed one's cultural worldview, or one's self-esteem, serves a death-denying function, then threatening these constructs should produce defenses aimed at restoring psychological equanimity (i.e., returning the individual to a state of feeling invulnerable). In the MS paradigm, these \"threats\" are simply experiential reminders of one's own death. This can, and has, taken many different forms in a variety of study paradigms (e.g., asking participants to write about their own death; conducting the experiment near funeral homes or cemeteries; having participants watch graphic depictions of death, etc.). Like the other TMT hypotheses, the literature supporting the MS hypothesis is vast and diverse. For a meta analysis of MS research, see Burke \"et al.\" (2010).\n\nExperimentally, the MS hypothesis has been tested in close to 200 empirical articles. After participants in an experiment are asked to write about their own death (vs. a neutral, non-death control topic, such as dental pain), and then following a brief delay (distal, worldview/self-esteem defenses work the best after a delay; see Greenberg \"et al.\" (1994) for a discussion), the participants' defenses are measured. In one early TMT study assessing the MS hypothesis, Greenberg \"et al.\" (1990) had Christian participants evaluate other Christian and Jewish students that were similar demographically, but differed in their religious affiliation. After being reminded of their death (experimental MS induction), Christian participants evaluated fellow Christians more positively, and Jewish participants more negatively, relative to the control condition. Conversely, bolstering self-esteem in these scenarios leads to less worldview defense and derogation of dissimilar others.\n\nMortality salience has an influence on individuals and their decisions regarding their health. Cox \"et al.\" (2009) discuss mortality salience in terms of suntanning. Specifically, the researchers found that participants who were prompted with the idea that pale was more socially attractive along with mortality reminders, tended to lean towards decisions that resulted in more protective measures from the sun. The participants were placed in two different conditions; one group of participants were given an article relating to the fear of death, while the control group received an article unrelated to death dealing with the fear of public speaking. Additionally, they gave one group an article pertaining to the message that \"bronze is beautiful,\" one relating to the idea that \"pale is pretty,\" and one neutral article that did not speak of tan or pale skin tones. Finally, after introducing a delay activity, the researchers gave the participants a five-item questionnaire asking them about their future sun-tanning behaviors. The study illustrated that when tan skin was associated with attractiveness, mortality salience positively affected people's intentions to suntan; however, when pale skin was associated with attractiveness people's intentions to tan decreased.\n\nStudies have shown that mortality and self-esteem are important factors of the terror management theory. Jessop \"et al.\" (2008) study this relationship within four studies that all examine how people react when they are given information on risks, specifically, in terms of the mortality related to the risks of driving. More specifically, the researchers were exploring how participants acted in terms of self-esteem, and its impact on how mortality-related health-risk information would be received. Overall, Jessop \"et al.\" (2008) found that even when mortality is prominent, people who engage in certain behaviors to improve their self-esteem have a greater chance of continuing with these activities. Mortality and self-esteem are both factors that influence people's behaviors and decision-making regarding their health. Furthermore, individuals who are involved in behaviors and possess motivation to enhance their self-worth are less likely to be affected by the importance placed on health risks, in terms of mortality.\n\nSelf-esteem is important when mortality is made salient. It can allow people a coping mechanism, one that can cushion individuals' fears; and thus, impacting one's attitudes towards a given behavior. Individuals who have higher levels of self-esteem regarding their behavior(s) are less likely to have their attitudes, and thus their behaviors changed regardless of mortality salient or death messages. People will use their self-esteem to hide behind their fears of dying. In terms of smoking behaviors, people with higher smoking-based self-esteem are less susceptible to anti-smoking messages that relate to death; therefore, mortality salience and death warnings afford them with an even more positive outlook on their behavior, or in this instance their smoking.\n\nIn the Hansen \"et al.\" (2010) experiment the researchers manipulated mortality salience. In the experiment, Hansen \"et al.\" (2010) examined smokers' attitudes towards the behavior of smoking. Actual warning labels were utilized to create mortality salience in this specific experiment. The researchers first gave participants a questionnaire to measure their smoking-based self-esteem. Following the questionnaire, participants were randomly assigned to two different conditions; the first were given anti-smoking warning labels about death and the second, control group were exposed to anti-smoking warning labels not dealing with death. Before the participants' attitudes towards smoking were taken the researchers introduced an unrelated question to provide a delay. Further research has demonstrated that delays allow mortality salience to emerge because thoughts of death become non-conscious. Finally, participants were asked questions regarding their intended future smoking behavior. However, one weakness in their conduction was that the final questionnaire addressed opinions and behavioral questions, as opposed to the participants level of persuasion regarding the different anti-smoking warning labels.\n\nMany people are more motivated by social pressures, rather than health risks. Specifically for younger people, mortality salience is stronger in eliciting changes of one's behavior when it brings awareness to the immediate loss of social status or position, rather than a loss, such as death that one can not imagine and feels far off. However, there are many different factors to take into consideration, such as how strongly an individual feels toward a decision, his or her level of self-esteem, and the situation around the individual. Particularly with people's smoking behaviors, self-esteem and mortality salience have different effects on individuals' decisions. In terms of the longevity of their smoking decisions, it has been seen that individuals' smoking habits are affected, in the short-term sense, when they are exposed to mortality salience that interrelates with their own self-esteem. Moreover, people who viewed social exclusion prompts were more likely to quit smoking in the long run than those who were simply shown health-effects of smoking. More specifically, it was demonstrated that when individuals had high levels of self-esteem they were more likely to quit smoking following the social pressure messages, rather than the health risk messages. In this specific instance, terror management, and specifically mortality salience is showing how people are more motivated by the social pressures and consequences in their environment, rather than consequences relating to their health. This is mostly seen in young adult smokers with higher smoking-based self-esteems who are not thinking of their future health and the less-immediate effects of smoking on their health.\n\nAnother paradigm that TMT researchers use to get at unconscious concerns about death is what is known as the death thought accessibility (DTA) hypothesis. Essentially, the DTA hypothesis states that if individuals are motivated to avoid cognitions about death, and they avoid these cognitions by espousing a worldview or by buffering their self-esteem, then when threatened, an individual should possess more death-related cognitions (e.g., thoughts about death, and death-related stimuli) than they would when not threatened.\n\nThe DTA hypothesis has its origins in work by Greenberg \"et al.\" (1994) as an extension of their earlier terror management hypotheses (i.e., the anxiety buffer hypothesis and the mortality salience hypothesis). The researchers reasoned that if, as indicated by Wegner's research on thought suppression (1994; 1997), thoughts that are purposely suppressed from conscious awareness are often brought back with ease, then following a delay death-thought cognitions should be more available to consciousness than (a) those who keep the death-thoughts in their consciousness the whole time, and (b) those who suppress the death-thoughts but are not provided a delay. That is precisely what they found. However, other psychologists have failed to replicate these findings.\n\nIn these initial studies (i.e., Greenberg \"et al.\" (2004); Arndt \"et al.\" (1997)), and in numerous subsequent DTA studies, the main measure of DTA is a word fragment task, whereby participants can complete word fragments in distinctly death-related ways (e.g., coff_ _ as coffin, not coffee) or in non death-related ways (e.g., sk_ _l as skill, not skull). If death-thoughts are indeed more available to consciousness, then it stands to reason that the word fragments should be completed in a way that is semantically related to death.\n\nThe introduction of this hypothesis has refined TMT, and led to new avenues of research that formerly could not be assessed due to the lack of an empirically validated way of measuring death-related cognitions. Also, the differentiation between proximal (conscious, near, and threat-focused) and distal (unconscious, distant, symbolic) defenses that have been derived from DTA studies have been extremely important in understanding how people deal with their terror.\n\nIt is important to note how the DTA paradigm subtly alters, and expands, TMT as a motivational theory. Instead of solely manipulating mortality and witnessing its effects (e.g., nationalism, increased prejudice, risky sexual behavior, etc.), the DTA paradigm allows a measure of the death-related cognitions that result from various affronts to the self. Examples include threats to self-esteem and to one's worldview; the DTA paradigm can therefore assess the role of death-thoughts in self-esteem and worldview defenses. Furthermore, the DTA hypothesis lends support to TMT in that it corroborates its central hypothesis that death is uniquely problematic for human beings, and that it is fundamentally different in its effects than meaning threats, (i.e., Heine \"et al.\", 2006) and that is death itself, and not uncertainty and lack of control associated with death; Fritsche \"et al.\" (2008) explore this idea.\n\nSince its inception, the DTA hypothesis had been rapidly gaining ground in TMT investigations, and as of 2009, has been employed in over 60 published papers, with a total of more than 90 empirical studies.\n\nHow people respond to their fears and anxiety of death is investigated in TMT. Moreover, Taubman-Ben-Ari and Noy (2010) examine the idea that a person's level of self-awareness and self-consciousness should be considered in relation to their responses to their anxiety and death cognitions. The more an individual is presented with their death or death cognitions in general, the more fear and anxiety one may have; therefore, to combat said anxiety one may implement anxiety buffers.\n\nDue to a change in people's lifestyles, in the direction of more unhealthy behaviors, the leading causes of death now, being cancer and heart disease, most definitely are related to individuals' unhealthy behaviors (though the statement is over-generalising and certainly cannot be applied to every case) Age and death anxiety both are factors that should be considered in the terror management theory, in relation to health-promoting behaviors. Age undoubtedly plays some kind of role in people's health-promoting behaviors; however, an actual age related effect on death anxiety and health-promoting behaviors has yet to be seen. Although, research has demonstrated that for young adults only, when they were prompted with death related scenarios, they yielded more health-promoting behaviors, compared to those participants in their sixties. In addition, death anxiety has been found to have an effect for young adults, on their behaviors of health promotion.\n\nThe terror management health model (TMHM) explores the role that death plays on one's health and behavior. Goldenberg and Arndt (2008) state that the TMHM proposes the idea that death, despite its threatening nature, is in fact instrumental and purposeful in the conditioning of one's behavior towards the direction of a longer life.\n\nAccording to Goldenberg and Arndt (2008), certain health behaviors such as breast self-exams (BSEs) can consciously activate and facilitate people to think of death, especially their own death. While death can be instrumental for individuals, in some cases, when breast self-exams activate people's death thoughts an obstacle can present itself, in terms of health promotion, because of the experience of fear and threat. Abel and Kruger (2009) have suggested that the stress caused by increased awareness of mortality when celebrating one's birthday might explain the birthday effect, where mortality rates seem to spike around these days.\n\nOn the other hand, death and thoughts of death can serve as a way of empowering the self, not as threats. Researchers, Cooper \"et al.\" (2011) explored TMHM in terms of empowerment, specifically using BSEs under two conditions; when death thoughts were prompted, and when thoughts of death were non-conscious. According to TMHM, people's health decisions, when death thoughts are not conscious, should be based on their motivations to act appropriately, in terms of the self and identity. Cooper \"et al.\" (2011) found that when mortality and death thoughts were primed, women reported more empowerment feelings than those who were not prompted before performing a BSE.\n\nAdditionally, TMHM suggests that mortality awareness and self-esteem are important factors in individuals' decision making and behaviors relating to their health. TMHM explores how people will engage in behaviors, whether positive or negative, even with the heightened awareness of mortality, in the attempt to conform to society's expectations and improve their self-esteem. The TMHM is useful in understanding what motivates individuals regarding their health decisions and behaviors.\n\nIn terms of smoking behaviors and attitudes, the impact of warnings with death messages depends on:\n\nPeople with low self-esteem, but not high self-esteem, have more negative emotions when reminded of death. This is believed to be because these individuals lack the very defenses that TMT argues protect people from mortality concerns (e.g., solid worldviews). In contrast, positive mood states are not impacted by death thoughts for people of low or high self-esteem.\n\nIt has been suggested that culture provides meaning, organization, and a coherent world view that diminishes the psychological terror caused by the knowledge of eventual death. The terror management theory can help to explain why a leader's popularity can grow substantially during times of crisis. When a follower's mortality is made prominent they will tend to show a strong preference for iconic leaders. An example of this occurred when George W. Bush's approval rating jumped almost 50 percent following the September 11 attacks in the United States. As Forsyth (2009) posits, this tragedy made U.S. citizens aware of their mortality, and Bush provided an antidote to these existential concerns by promising to bring justice to the terrorist group responsible for the attacks.\n\nResearchers Cohen \"et al.\" (2004), in their particular study on TMT, tested the preferences for different types of leaders, while reminding people of their mortality. Three different candidates were presented to participants. The three leaders were of three different types: task-oriented (emphasized setting goals, strategic planning, and structure), relationship-oriented (emphasized compassion, trust, and confidence in others), and charismatic. The participants were then placed in one of two conditions: mortality salient or control group. In the former condition the participants were asked to describe the emotions surrounding their own death, as well as the physical act of the death itself, whereas the control group were asked similar questions about an upcoming exam. The results of the study were that the charismatic leader was favored more, and the relationship-oriented leader was favored less, in the mortality-salient condition. Further research has shown that mortality salient individuals also prefer leaders who are members of the same group, as well as men rather than women (Hoyt \"et al.\" 2010). This has links to social role theory.\n\nTMT posits that religion was created as a means for humans to cope with their own mortality. Supporting this, arguments in favor of life after death, and simply being religious, reduce the effects of mortality salience on worldview defense. Thoughts of death have also been found to increase religious beliefs. At an implicit, subconscious level, this is the case even for people who claim to be nonreligious.\n\nSeveral psychologists, especially evolutionary psychologists, have argued against terror management theory. One scholar commented that the field of psychology would be advanced by a study of paralyzed states caused by anxiety that would only be alleviated with the reworking of a person's mental state. These authors instead explain human behavior is selected to urge people to avoid situations likely to lead to death. This suggests that mortality salience effects reflect adaptive responses to solve specific life-threats rather than an unconscious attempt to avoid this realization.\n\nSince findings on mortality salience and worldview defense were first published, other researchers have claimed that the effects may have been obtained due to reasons other than death itself, such as anxiety, fear, or other aversive stimuli such as pain. Other studies have found effects similar to those that MS results in – for example, thinking about difficult personal choices to be made, being made to respond to open-ended questions regarding uncertainty, thinking about being robbed, thinking about being socially isolated, and being told that one's life lacks meaning. While these cases exist, thoughts of death have since been compared to various aversive experimental controls, such as (but not limited to) thinking about: failure, writing a critical exam, public speaking with a considerable audience, being excluded, paralysis, dental pain, intense physical pain, etc.\n\nWith regards to the studies that found similar effects, TMT theorists have argued that in the previously mentioned studies where death was not the subject thought about, the subjects would quite easily be related to death in an individual's mind due to \"linguistic or experiential connection with mortality\" (p. 332) For example, being robbed invokes thoughts of violence and being unsafe in one's own home – many people have died trying to protect their property and family. A second possible explanation for these results involves the death-thought accessibility hypothesis: these threats somehow sabotage crucial anxiety-buffering aspects of an individual's worldview or self-esteem, which increases their DTA. For example, one study found increased DTA in response to thoughts of antagonistic relations with attachment figures.\n\nThe meaning maintenance model (MMM) was initially introduced as a comprehensive motivational theory that claimed to subsume TMT, with alternative explanations for TMT findings. Essentially, it posits that people automatically give meaning to things, and when those meanings are somehow disrupted, it causes anxiety. In response, people concentrate on \"meaning maintenance to reestablish their sense of symbolic unity\" and that such \"meaning maintenance often involves the compensatory reaffirmation of alternative meaning structures\". These meanings, among other things, should \"provide a basis for prediction and control of our...environments, help [one] to cope with tragedy and trauma...and the symbolic cheating of death via adherence to the enduring values that these cultures provide\".\n\nTMT theorists argue that MMM cannot describe why different sets of meaning are preferred for a symbol by different people, and that while they may exist, \"different [(i.e., more concrete)] types of meaning have different psychological functions\". For example, MMM theorists argue that all types of meaning are basically equal, and yet one could not compare the likelihood of defensive responses resulting from exposure to a deck of cards containing black hearts with something like the September 11 attacks. TMT theorists argue, essentially, that unless something is an important element of a person's anxiety-buffering worldview or self-esteem, it will not require broad meaning maintenance.\n\nIn sum, TMT theorists believe that MMM cannot accurately claim to be an alternative to TMT because it does not seem to be able to explain the current breadth of TMT evidence. As an example, TMT theorists assert that mortality salience would not be a threat to meaning, since our eventual demise is a necessary condition of life. Therefore, it should not cause an individual to engage in general meaning maintenance. MMM also makes no attempt to explain why threatening meaning increases DTA.\n\nSome theorists have argued that it is not the idea of death and nonexistence that is unsettling to people, but the fact that uncertainty is involved. For example, these researchers posited that people defend themselves by altering their fear responses from uncertainty to an enthusiasm approach. Other researchers argue for distinguishing fear of death from fear of dying and, therein, posit that ultimately the fear of death has more to do with some other fear (e.g., fear of pain) or reflects fear of the unknown.\n\nTMT theorists agree that uncertainty can be disconcerting in some cases and it may even result in defense responses, but note that they believe the inescapability of death and the possibility of its finality regarding one's existence is most unsettling. They ask, \"'Would death be any less frightening if you knew for certain that it would come next Tuesday at 5:15 p.m., and that your hopes for an afterlife were illusory?'...Would you rather be certain that death is the end, or live with the uncertainty that it might not be?\" They also note that people actually seek out some types of uncertainty, and that being uncertain is not always very unpleasant. In contrast, there is substantial evidence that, all things being equal, uncertainty and the unknown represent fundamental fears and are only experienced as pleasant when there is sufficient contextual certainty. For example, a surprise involves uncertainty, but is only perceived as pleasant if there is sufficient certainty that the surprise will be pleasant. Consider a box received on a birthday from a trusted family member as compared to the box received at the end of the film \"Seven\" (which contains a severed head).\n\nThough TMT theorists acknowledge that many responses to mortality salience involve greater approaches (zealousness) towards important worldviews, they also note examples of mortality salience which resulted in the opposite, which offensive defensiveness cannot account for: when negative features of a group to which participants belong were made salient, people actively distanced themselves from that group under mortality salience.\n\nSeveral critiques have been proposed against TMT from evolutionary psychologists – for reasons including that fear is an adaptive response in individuals' that has come about as a result of natural selection; without these adaptations human beings would have never been able to avoid maladaptive situations. Thus, it is unlikely that people would have psychological ways of slowing-down anxiety. In response, TMT theorists argue that this critique is mixing up fear related to immediate danger with anxiety related to thoughts of threats that will or may occur eventually. TMT is talking about the protection that self-esteem and cultural worldviews offer against the threat of unavoidable death in the \"future\". While anxiety may be adaptive in avoiding entering a dangerous place (e.g. because a predator may be waiting), this doesn't mean that anxiety must be adaptive in all cases. For a more comprehensive review of TMT and evolutionary psychology, see Landau \"et al.\", 2007. Similar evolutionary critiques have been raised by researchers exploring uncertainty and unknowns (see for reviews,).\n\nCoalitional psychology (CP) is presented as another alternative to TMT, which proposes that there is an evolutionary tendency to seek safety in groups (coalitions) as a reaction to adaptive threats. People already a part of coalitional groups seek to protect their membership by exhibiting their value to the group. TMT theorists answer by arguing that CP 1) cannot account for the fact that virtually all cultures have a supernatural dimension; 2) it does not explain why cultural worldview defense is symbolic, involving allegiance to both specific and general systems of abstract meaning unrelated to specific threats, rather than focused on the specific adaptive threats it supposedly evolved to deal with; 3) it dismisses TMT's dual process account of the underlying processes that generate MS effects without providing an alternative of any kind or attempting to account for the data relevant to this aspect of the TMT analysis and 4) the experiments testing hypotheses derived from CP do not provide compelling or unique support for CP, 5) it cannot account for a host of empirical findings supporting hypotheses derived from TMT that could never be deduced from CP.\n\n\n\n\nDiscusses TMT at length\n\nTMT and self-esteem\n"}
{"id": "2044626", "url": "https://en.wikipedia.org/wiki?curid=2044626", "title": "The Future Is Wild", "text": "The Future Is Wild\n\nThe Future Is Wild is a British 2002 thirteen-part pseudo-documentary television miniseries. Based on research and interviews with several scientists, the miniseries shows how life could evolve in the future if humans were to disappear from the Earth altogether through extinction. The version broadcast on the Discovery Channel modified this premise, supposing instead that the human species had completely abandoned the Earth and had sent back probes to examine the progress of life on the planet as time progressed. The show styled itself after the format of a nature documentary. It is narrated by John de Lancie in the Discovery Channel version.\n\nThe miniseries was released with a companion book written by geologist Dougal Dixon, the author of several speculative evolution books, or \"anthropologies and zoologies of the future\" (such as \"\"), in conjunction with natural history television producer John Adams. For a time in 2005, a theme park based on this program was opened in Japan. In 2008 a special on the Discovery Channel about the development of the video game \"Spore\" was combined with airings of \"The Future Is Wild\".\n\nA documentary film version of the series was originally set to be picked up by Warner Bros., however, the series may be rebooted by production company Vanguard Animation and broadcasting at HBO.\n\nThe 2-part 2005 series \"Extraterrestrial\" (also known as \"Alien Worlds\") takes a similar scientific approach to the creation of speculative ecologies and the depiction of their inhabitants. As the title suggests, however, these are set on extrasolar planets with no connection to terrestrial evolution.\n\nTwelve ecosystems were presented, four in each of the three future periods.\n\nThe early episodes describe a world after an ice age, when giant sea-birds roam the beaches and carnivorous bats rule the skies. Ice sheets extend as far south as Paris in the northern hemisphere and as far north as Buenos Aires in the southern hemisphere. The Amazon rainforest has dried up and become grassland. The North American plains have become cold desert, and Africa has collided with Europe, enclosing the Mediterranean Sea. Without water to replace it in the dry climate, the Mediterranean has dried out into a salt flat dotted with brine lakes, as it has been in the past. Most of Europe is frozen tundra. The part of Africa east of the African Rift Valley has broken away from the rest of the continent. Asia has dried up and is now mountainous. The once warm, tropical area of Central America has been transformed into a dry area. Australia has moved north and collided with eastern Indonesia.\n\n\n\n\nIn the scenario for 100 million years in the future, the world is much hotter than at present. Octopuses and enormous tortoises have come on to the land, much of which is flooded by shallow seas surrounded by brackish swamps. Antarctica has drifted towards the tropics and is covered with dense rainforests, as it was before. Australia has collided with North America and Asia, forcing up an enormous, 12-kilometre-high mountain plateau much taller than the modern Himalayas. Greenland has been reduced to a small, temperate island. There are cold, deep ocean trenches. The Sahara has once again become the rich grassland it was millions of years ago\n\n\n\n\n\nThe hypothetical world of 200 million years from now is recovering from a mass extinction caused by a flood basalt eruption even larger than the one that created the Siberian Traps, wiping out 99% of the species on the planet. Fish have taken to the skies, squid to the forests, and the world's largest-ever desert is filled with strange worms and insects. All the continents have collided with one another and fused into a single supercontinent, a second Pangaea. (A few present-day geographical features can still be discerned, including Hudson Bay, Novaya Zemlya and the Scandinavian Peninsula, as well as the general outline of Africa.) One large global ocean with a single-current system gives rise to deadly hurricanes called hypercanes, which batter the coastlines of the continent all year long. The northwestern side of Pangaea II, drenched with an endless supply of rain, has become a temperate forest. Mountains resting at the end of the coast prevent most of the rain's moisture from reaching a long line of scrubby rainshadow deserts. The very center of the continent receives no rain at all and has become a barren, plantless desert. Only fish, arthropods, worms and mollusks were left to repopulate the Earth.\n\n\n\nEach episode generally focuses on just one food chain within a particular ecosystem.\n\n\"The Future is Wild\" is a £5-million co-production of the British Broadcasting Corporation (BBC), the Franco-German channel Arte, the German ZDF, the Austrian ORF, the Italian Mediaset, and Animal Planet and Discovery Channels Inc of the United States.\n\nThe BBC intended that the miniseries would repeat the success it had with its prehistoric documentary series \"Walking With Dinosaurs\", which attracted 17 million viewers in 1999. The program used computer-generated imagery to show the possible future of life on Earth. The 13-part series was produced in four years by independent producer John Adams, who conceived it in 1997.\n\nScientists involved in the project include the following:\n\n\"The Future is Wild\" doubled the previous ratings record for the Animal Planet channel when it was aired in the United States. The series was shown on BBC2 in late 2004.\n\nZDF Enterprises sold the television rights of the series to 18 markets: Belgium, Canada, Croatia, the Czech Republic, Ecuador, France, Germany, Hong Kong, Hungary, Japan, Korea, Mexico, the Middle East, Poland, Romania, Russia, Slovenia and Venezuela.\n\nThe series was released on three DVDs: episodes 1–5, episodes 6–9 and episodes 10–13. The three DVDs have also been released together as a set. Both the single DVDs and the three-DVD set are available for DVD regions one and two. Although the singles are available for region four, the three-DVD set is not. In addition to the complete edition, there is also an abridged region 2 3-disc version which condenses each of the three time periods into one 52-minute episode.\n\nAn educational CD-ROM entitled \"The Future Is Wild\" was produced by Sherston Software in 2006. It is designed to fit in with international school curricula for science, mathematics, geography and history.\n\nA book version was released in 2003, published by Firefly Books.\n\nIn 2008-2012 Futuroscope theme park in Poitiers, France contained an exhibit dedicated to the movie, its animals and habitats.\n\nSince 2016 Dinosaurier Park Teufelsschlucht in Ernzen has displayed multiple animals from the series. \n\n\n\n"}
{"id": "8597086", "url": "https://en.wikipedia.org/wiki?curid=8597086", "title": "Von Neumann universal constructor", "text": "Von Neumann universal constructor\n\nJohn von Neumann's universal constructor is a self-replicating machine in a cellular automata (CA) environment. It was designed in the 1940s, without the use of a computer. The fundamental details of the machine were published in von Neumann's book \"Theory of Self-Reproducing Automata\", completed in 1966 by Arthur W. Burks after von Neumann's death.\n\nVon Neumann's goal was to specify an abstract machine which, when run, would replicate itself. In his design, the machine consists of three parts: a 'blueprint' for itself, a mechanism that can read any blueprint and construct the machine (sans blueprint) specified by that blueprint, and a 'copy machine' that can make copies of any blueprint. After the mechanism has been used to construct the machine specified by the blueprint, the copy machine is used to create a copy of that blueprint, and this copy is placed into the new machine, resulting in a working replication of the original machine. Some machines will do this backwards, copying the blueprint and then building a machine.\n\nTo define his machine in more detail, von Neumann invented the concept of a cellular automaton. The one he used consists of a two-dimensional grid of cells, each of which can be in one of 29 states at any point in time. At each timestep, each cell updates its state depending on the states of the surrounding cells at the prior timestep. The rules governing these updates are identical for all cells. \n\nThe universal constructor is a certain pattern of cell states in this cellular automaton. It contains one line of cells that serve as a 'tape', encoding a sequence of instructions that serve as a 'blueprint' for the machine. The machine reads these instructions one by one and performs the corresponding actions. The instructions direct the machine to use its 'construction arm' to build a copy of the machine, without tape, at some other location in the cell grid. The tape can't contain instructions to build an equally long tape, just as a container can't contain a container of the same size. Therefore, the machine contains a separate 'copy machine' which reads the tape and places a copy into the newly constructed machine. The resulting new machine and tape is identical to the old one, and it proceeds to replicate again.\n\nVon Neumann's design has traditionally been understood to be a demonstration of the logical requirements for machine self-replication. However, it is clear that far simpler machines can achieve self-replication. Examples include trivial crystal-like growth, template replication, and Langton's loops. But von Neumann was interested in something more profound: construction, universality, and evolution.\n\nThis universal constructor can be seen as an abstract simulation of a physical universal assembler.\n\nNote that the simpler self-replicating CA structures (especially, Byl's loop and the Chou–Reggia loop) cannot exist in a wide variety of forms and thus have very limited evolvability. Other CA structures such as the Evoloop are somewhat evolvable but still don't support open-ended evolution. Commonly, simple replicators do not fully contain the machinery of construction, there being a degree to which the replicator is information copied by its surrounding environment. Although the Von Neumann design is a logical construction, it is in principle a design that could be instantiated as a physical machine. The issue of the environmental contribution to replication is somewhat open, since there are different conceptions of raw material and its availability.\n\nThe concept of a \"universal constructor\" is non-trivial because of the existence of Garden of Eden patterns. But a simple definition is that a universal constructor is able to construct any finite pattern of non-excited (quiescent) cells.\n\nVon Neumann's crucial insight is that part of the replicator has a double use; being both an active component of the construction mechanism, and being the target of a passive copying process. This part is played by the tape of instructions in Von Neumann's combination of universal constructor plus instruction tape.\n\nThe combination of a universal constructor and a tape of instructions would i) allow self-replication, and also ii) guarantee that the open-ended complexity growth observed in biological organisms was possible. The image below illustrates this possibility.\n\nThis insight is all the more remarkable because it preceded the discovery of the structure of the DNA molecule by Watson and Crick, though it followed the Avery–MacLeod–McCarty experiment which identified DNA as the molecular carrier of genetic information in living organisms. The DNA molecule is processed by separate mechanisms that carry out its instructions and copy the DNA for insertion for the newly constructed cell. The ability to achieve open-ended evolution lies in the fact that, just as in nature, errors (mutations) in the copying of the genetic tape can lead to viable variants of the automaton, which can then evolve via natural selection.\n\nArthur Burks and others extended the work of von Neumann, giving a much clearer and complete set of details regarding the design and operation of von Neumann's self-replicator. The work of J. W. Thatcher is particularly noteworthy, for he greatly simplified the design. Still, their work did not yield a complete design, cell by cell, of a configuration capable of demonstrating self-replication.\n\nRenato Nobili and Umberto Pesavento published the first fully implemented self-reproducing cellular automaton in 1995, nearly fifty years after von Neumann's work. They used a 32-state cellular automaton instead of von Neumann's original 29-state specification, extending it to allow for easier signal-crossing, explicit memory function and a more compact design. They also published an implementation of a general constructor within the original 29-state CA but not one capable of complete replication - the configuration cannot duplicate its tape, nor can it trigger its offspring; the configuration can only construct.\n\nIn 2004, D. Mange et al. reported an implementation of a self-replicator that is consistent with the designs of von Neumann.\n\nIn 2007, Nobili published a 32-state implementation that uses run-length encoding to greatly reduce the size of the tape.\n\nIn 2008, William R. Buckley published two configurations which are self-replicators within the original 29-state CA of von Neumann. Buckley claims that the crossing of signal within von Neumann 29-state cellular automata is not necessary to the construction of self-replicators. Buckley also points out that for the purposes of evolution, each replicator should return to its original configuration after replicating, in order to be capable (in theory) of making more than one copy. As published, the 1995 design of Nobili-Pesavento does not fulfill this requirement but the 2007 design of Nobili does; the same is true of Buckley's configurations.\n\nIn 2009, Buckley published with Golly a third configuration for von Neumann 29-state cellular automata, which can perform either holistic self-replication, or self-replication by partial construction. This configuration also demonstrates that signal crossing is not necessary to the construction of self-replicators within von Neumann 29-state cellular automata.\n\nC. L. Nehaniv in 2002, and also Y. Takada et al. in 2004, proposed a universal constructor directly implemented upon an asynchronous cellular automaton, rather than upon a synchronous cellular automaton.\n\nAs defined by von Neumann, universal construction entails the construction of passive configurations, only. As such, the concept of universal construction constituted nothing more than a literary (or, in this case, mathematical) device. It facilitated other proof, such as that a machine well constructed may engage in self-replication, while universal construction itself was simply assumed over a most minimal case. Universal construction under this standard is trivial. Hence, while all the configurations given here can construct any passive configuration, none can construct the real-time crossing organ devised by Gorman.\n\nAll the implementations of von Neumann's self-reproducing machine require considerable resources to run on computer. For example, in the Nobili-Pesavento 32-state implementation shown above, while the body of the machine is just 6,329 non-empty cells (within a rectangle of size 97x170), it requires a tape that is 145,315 cells long, and takes 63 billion timesteps to replicate. A simulator running at 1,000 timesteps per second would take over 2 years to make the first copy. In 1995, when the first implementation was published, the authors had not seen their own machine replicate. However, in 2008, the hashlife algorithm was extended to support the 29-state and 32-state rulesets in Golly. On a modern desktop PC, replication now takes only a few minutes, although a significant amount of memory is required.\n\nIt has been argued that the problem Von Neumann was trying to address was not self-reproduction \"per se\", but the evolutionary growth of complexity. His “proof-of-principle” designs showed how it is logically possible, by using a general purpose programmable (“universal”) constructor, to exhibit an indefinitely large class of self-reproducing machines, spanning a wide range of “complexity” (in von Neumann's sense of “the ability to do complicated things”), interconnected by a network of potential mutational pathways, including pathways from the most simple to the most complex. This is an important result, as prior to that it might have been conjectured that there is a fundamental logical barrier to the existence of such pathways; in which case, biological organisms, which do support such pathways, could not be “machines”, as conventionally understood. But the result does not show what \"other\" conditions are necessary, in practice, for evolution along such pathways from simple to complex to be spontaneously realised or followed. In his unfinished work he briefly considers conflict and interactions between his self-reproducing machines; but in practice, his particular model cannot yield evolutionary dynamics because the machines are too fragile - the vast majority of perturbations cause them effectively to disintegrate.\n\n\n"}
{"id": "18560708", "url": "https://en.wikipedia.org/wiki?curid=18560708", "title": "Voodoo death", "text": "Voodoo death\n\nVoodoo death, a term coined by Walter Cannon in 1942 also known as psychogenic death or psychosomatic death, is the phenomenon of sudden death as brought about by a strong emotional shock, such as fear. The anomaly is recognized as \"psychosomatic\" in that death is caused by an emotional response—often fear—to some suggested outside force. Voodoo death is particularly noted in native societies, and concentration- or prisoner of war camps, but the condition is not specific to any particular culture.\n\nIn 1942, Walter Bradford Cannon, MD, now looked to as a forerunner in modern physiological psychology, published a work wherein he postulated the idea that fear could affect a person to the point that their physical condition would deteriorate in response to psychological distress. Citing examples of extraordinary deaths (and their extraneous circumstances) in aboriginal societies, Cannon posited the idea that fear of supernatural consequences to broken societal taboos caused the deaths witnessed in the natives.\n\nWhat Cannon describes has since been termed \"bone-pointing syndrome,\" wherein an individual receives some sort of shock—often the breaking of some social/religious taboo—that he interprets as an ill omen for himself; his physical condition then deteriorates at a rapid rate, and he dies within a period as short as 24 hours after the initial shock.\n\nCannon discusses the example of a Maori woman who learned that the fruit she had eaten came from a tapu (tabooed) place; less than 24 hours later she was dead. Conversely, Cannon also shares the example of a young man who had fallen ill when the local witch doctor had pointed a bone at him, a societal taboo that meant a curse of death; however, when the perpetrator explained to the young man that the whole thing had been a mistake, and that no bone had been pointed at him at all, the young man's health returned instantly.\n\nCannon notes the similarities in each case: the individuals were both members of a society where beliefs in the supernatural are fiercely upheld, and both had suffered what they both believed to be some form of a curse as dictated within their personal beliefs; also, the individuals shared similar physical symptoms. And yet, in the case of the young man, once the cause for the psychological distress was removed, his mysterious illness disappeared. Cannon attributes these rather drastic physical repercussions as the workings of the emotion fear upon the mind which then leads to destruction of the physical condition.\n\nAccording to Cannon, the emotion of fear working on the mind, which he terms the \"sympathetic\" or \"sympathico-adrenal\" division of the nervous system, causes a fall in blood pressure as brought on by \"a reduction of the volume of circulating blood\". Cannon explains the loss of blood volume by the constant injection of adrenaline into the small arterioles which constrict, preventing a proper flow of blood within the body and causing a drop in blood pressure. From there, the weak blood pressure prevents the sufficient circulation of the blood by damaging the heart and nerves responsible for the maintenance of the vessels which transport blood, thus making it harder for circulation to continue since the very organs necessary to maintain proper blood circulation are deteriorating. An accelerated heart rate then ensues, followed by rapid breathing. Added to these symptoms are the effects of no food or drink in the person experiencing psychological distress: Cannon suggests that true shock, in the medical sense, could be the cause of death as a result of little food or drink. States Cannon: \"The combination of lack of food and water, anxiety, very rapid pulse and respiration, associated with a shocking experience having persistent effects, would fit well with fatal conditions reported from primitive tribes.\"\n\nSince 1942, scientists have discovered many more of the processes involved in the effect of stress upon the body, such as the region of the brain called the amygdala. The series of events by which a sensory stimulus is introduced to the mind, and the amygdala processes the emotion of fear which follows is called the \"vision-to-fear pathway\", or the \"auditory-to-fear pathway\", depending on the stimulus.\n\nThe generally recognized sequence of events, as enumerated by Esther M. Sternberg, MD, in 2002, stands as follows: various chemicals and electrical impulses are released that are transmitted by nerve fibers. Simultaneously, hormones are excreted from the brain, adrenal and pituitary glands in response to stress on the system. Cardiac arrhythmias are often the result of an overabundance of these hormones on the system.\n\nIn 1981, Wylie Vale, PhD, discovered corticotrophin, the brain's hypothalamic stress hormone, or CRH: this hormone secreted by the hypothalamus coordinates with \"the brain stem adrenaline centers involved in initiation of the sympathetic response ... to cause a massive release of both adrenaline-like nerve chemicals and stress hormones. Together these might well cause illness, including loss of appetite, weakness, cardiac arrhythmias, and even vascular collapse that could result in death.\"\n\nMartin A. Samuels, MD, elaborates further on still another process of death, stating that with the release of adrenaline and an increased heart rate, sometimes catecholamines, stress hormones, will build up, leading to calcium channels opening and remaining open, resulting in an overflow of calcium into the system, killing off cells.\n\nCannon believed that extreme emotional stress could be explained in terms of degree of sympathetic-adrenal excitation. However, an experiment performed by Curt Richter (1957) responded to Cannon's challenge with an animal model. Richter placed pre-stressed rats in a closed turbulent water. the latency to drowning was recorded. Most domestic lab rats lasted for hours while unexpectedly all of the wild rats died within 15 minutes. Richter monitored heart rate and determined whether the heart was in systole or diastole after death. He found out that heart rate slowed down prior to death and the heart was engaged with blood reflecting a state of diastole. This contradicted Cannons proposal that sympathetic adrenal over-activation is the result of death since a sympathetic over-arousal would increase both heart rate and blood pressure to severe degrees.\nRichter interpreted this that the rats died as a result of over-stimulation of the parasympathetic nervous system, specifically the vagus nerve which regulates heartbeat. The lethal vagal effect was the psychological state of hopelessness.\n\nSudden prolonged immobility or faked death is an adaptive response exhibited by many mammalian species. Hofer(1970) demonstrated that several rodent species when threatened exhibited an immobility that was accompanied by a very low heart rate. For some of the rodents that heart rate reached below 50% of the baseline. Hofer distinguished between prolonged immobility and faked death phenomenon. Unlike the behavior of \"hopelessness\" described by Richter, the death-faking occurred with a sudden motor collapse during active struggling. Hofer interpreted the fear-inducing slowing of heart rate as a vagal phenomenon.\n\nThese data suggest that vagus contributes to severe emotional states and may be related to emotional states of immobilization, such as extreme terror. Unfortunately, this immobilization technique is potentially life-threatening for mammals (but not for reptiles). Mammals would undergo states of bradycardia or hypoxia as an over-activation of parasympathetic vagus system. The organs of the oxygen-hungry mammal are deprived of oxygen due to lack of blood flow, and the animal dies.\n\nDespite Cannon's general ignorance on the particulars of physiological breakdown, scientists in the intervening years since the publication of Cannon's work, have generally agreed with his fundamental hypotheses concerning voodoo death. Criticisms that generally come against Cannon's work are directed at the hearsay nature of Cannon's case studies, but recent studies have discovered numerous examples of voodoo death in various societies. To those who allege difficulty in the experimental process of validating Cannon's theory, Barbara W. Lex, in her 1974 article titled, \"Voodoo Death: New Thoughts on an Old Explanation\", states that \"Voodoo death\" can easily be observed without complicated experiments:\n\"Pupillary constriction, easily observable and indicative of parasympathetic activation ... the amount of saliva, of perspiration, degree of muscle tonicity and skin pallor in an individual are also discernible without complicated instruments.\"\n\nHowever, there are those who contest the theories involving psychologically-induced body failure. David Lester, PhD, in 1972, contends that Cannon's evidence, particularly the evidence concerning animals, is anecdotal and irrelevant, and instead sets forth the concept of \"death by suggestion\", and supports \"giving up-given up\" complex set forth by George L. Engel, thus attributing the cause of death entirely to the psychological state of the individual in question rather than a psychological–physiological connection adduced by Cannon. Going even further, Harry D. Eastwell, MD in his 1982 article, \"Voodoo Death and the Mechanism for Dispatch of the Dying in East Arnhem, Australia\", rejects entirely the concept of \"Voodoo death\", stating that the deaths in cases reported by Cannon et al. were more likely due to dehydration rather than to any psychological response.\n\nDeeply related to these cases of sudden death is what Cannon termed the \"fight-or-flight response\", what has been classified as a \"neurophysiological-behavioral\" response pattern. \"Fight or flight\" is a phrase used to describe the instinctual and physiological responses to strong emotion within animals as well as humans. Cannon associates the two emotions of rage and fear because of the similar effects the emotions will have upon the mind and body—rage will encourage the response to \"fight\", while fear will encourage \"flight\". The mind, when faced with one or both of these emotions in response to a perceived threat, will emit adrenaline, and heart rate will increase; however, sometimes the system is overwhelmed by the responses, and collapse ensues as brought about by the workings of stress hormones.\n\nIn the case of voodoo death, the \"flight\" response overpowers the system, but there is little to no possibility for action in the mind of the individual suffering from the perceived threat—considering the state of aboriginal tribes, the victims believe themselves to be suffering from a curse in which they are condemned to die, and so they believe themselves to be unable to act to save themselves.\n\nCannon's theory concerning voodoo death opened research into various fields of psychological studies; since the publication of Cannon's work, scientists have discovered many disorders and the like related to psychosomatic responses to situations. Because of Cannon's postulation that the mind could bring about death, scientists have become open to the idea of the mind working on the body in a greater number of ways, leading to the development of psychosomatic medicine.\n\nThe advent of theories concerning voodoo death within the scientific field has also led to the development of a branch of psychology termed psychophysiology.\n\nThough cases within aboriginal societies are the most commonly cited when researchers such as Cannon set forth examples, similar cases of psychosomatic death have also been reported in other cultures.\n\nIn his 1964 article, James L. Mathis, MD, describes a case of a previously healthy man who died from asthmatic attacks when his mother \"cursed\" him for going against her wishes. Mathis proposes that \"fatal psychosomatic conditions\" were the cause of this man's death, and thus a form of voodoo death.\n\nAnother scientist—Clifton K. Meador, MD—in 1992 discussed the case of a man diagnosed with cancer who, along with his physicians and family, believed he was dying of cancer. In the autopsy after his death, however, the doctors discovered that his cancer was not at all the cause of his death. Meador deduces that the man's belief in his imminent death was the cause of his death itself.\n\n"}
