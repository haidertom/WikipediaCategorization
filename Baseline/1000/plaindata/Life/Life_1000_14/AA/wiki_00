{"id": "21160798", "url": "https://en.wikipedia.org/wiki?curid=21160798", "title": "1.8.7", "text": "1.8.7\n\n1.8.7 is a pseudonym for Jordana LeSesne, a musician and producer from Pittsburgh, Pennsylvania. She became widely known in the mid-1990s as an American Drum and Bass producer. The Village Voice described her as being \"[w]idely regarded as the top U.S. drum'n'bass producer.\" Vibe magazine called her \"one of the most respected Drum ‘n' Bass producers in the US.\" In 2015 Jordana was named as one of \"20 Women Who Shaped the History of Dance Music\" by the authoritative dance music magazine Mixmag. Previously, in 2014, For Harriet Magazine named Jordana as one of \"12 Women in Black Music History You Should Know\". In 2014, Complex Magazine UK named one of her songs to a list of \"36 Great American Drum & Bass Tracks\". She is transgender and has been living as a woman since 1998.\n\nShe has released of over 50 tracks which includes four full-length albums, several EPs, and many remixes under the alias \"1.8.7\". The album \"When Worlds Collide\", released in 1997, became known for its \"dark pummeling assaults\". She has also licensed tracks for numerous compilations as well as the Sci Fi Channel. Three of her albums charted in the Top 25 of both the CMJ (College Music Journal) and Mixmag U.S. (later Mixer Magazine, now defunct) for 1997 as well as 1998 and 1999. Her third album \"The Cities Collection\" debuted in the CMJ Top 5 climbed to the #2 position on CMJ Music Monthly's dance chart for June 2000.\n\nHer works have been reviewed by the likes of \"Billboard\", \"Spin\", \"Rolling Stone\", \"Urb\", \"Mixer\", \"Mixmag\", \"Raygun\", \"Vibe\", and Trip (Español), as well as \"Knowledge\" – the U.K. Drum and Bass magazine. In 1999 she was listed in Raygun's \"Who's Who of International DJs\". She was one of the headlining DJs on Knowledge Magazine's 28 city \"Kung Fu Knowledge\" tour in 1999. She also made \"Out Magazine's\" OUT100 for the year 2000.\n\nJordana's work has influenced other artists such as well known dubstep producer Bassnectar who heavily sampled \"5 A.M. Rinse (feat. MC Sphinx)\", the last song on her first album \"When Worlds Collide\" for his song \"Here We Go\" off of his 2010 EP and single \"Timestretch\". Additionally, electronic rock act Celldweller sampled \"Wake Up\" off of her first album as well as \"San Francisco\" off of her third album \"The Cities Collection\" in their 2013 song \"Uncrowned\".\n\nIn 1999 Drum & Bass/Hip Hop producer and label owner Hive approached Jordana to remix her song \"Defcon-1\" also off of \"When Worlds Collide\". Hive's remix appears on his 2001 album \"The Raw Uncut\". Jordana collaborated with Lady Sovereign on a song early in Sov's career after the two met through an internet chat room for StrikeFM.co.uk, an online radio station which Jordana had a show, and the now defunct UKGargageWorldwide.com forums. The two would later team up when Jordana under her Lady J alias, had Lady Sovereign MC for her radio show on Flex 103.6FM London.\n\nNew Zealand based Dubstep and Drum & Bass producer Alexis K/Unsub has also cited Jordana as an influence and the two are collaborating on music and a possible tour.\n\nJordana first came to the attention of the music industry when she was asked to remix Blondie's \"Atomic.\" Her \"Beautiful Drum & Bass Remix\" appeared along with Armand Van Helden and Diddy's remixes on the single. A little over a year following that release Mac McFarlane, the promoter of the well established and legendary New York City Drum'n'Bass club night, Konkrete Jungle, contacted Jordana to create a Konkrete Jungle themed song for a CD compilation/mix-CD. Jordana created the song \"Konkrete Jungle\" for that purpose. Described by CMJ as containing \"menacing hardstep attacks\" it was released on the Ultra Records compilation, \"Konkrete Jungle - Maximum Drum & Bass,\" mixed by BBC Radio 1 Drum'n'Bass show regular host Jumping Jack Frost. Following extensive touring throughout North America and abroad as a live Drum & Bass artist, Liquid Sky Music, an indie label distributed by Caroline Distribution signed her to a three-album contract in late 1996.\n\nOn the night of February 22, 2000 in Kent, Ohio Jordana was attacked and brutally beaten in a transphobic hate crime by a group of men including Matthew Gostlin of Akron, Ohio. Gostlin and other assailants jumped her in the parking lot outside of \"The Robin Hood\" nightclub where she had just performed on the Cities Collection tour. The attack took place while she was escorted from the event with the event promoter to the promoters car. The group of men attacked suddenly and Jordana lost consciousness almost immediately after being struck in the face. She suffered nerve damage to the lower part of her face from her lower lip down as a result. \nShe was quoted as saying in the May issue of CMJ New Music Monthly that in the seconds just prior to the attack: \"I saw his face. I remember the look on his face. It was this look of utter hate, like 'I'm going to kill you.'\" George Meesig of Cleveland, Ohio shoved one of the men away and helped Jordana. In an interview with the Village Voice following the attack, Meesig stated that Gostlin had \"misgendered\" her. Other reports from people on the message board for Breakbeat Science (a NYC based Drum'n'Bass record store) mentioned transphobic slurs being shouted during the attack. Jordana subsequently cancelled the tour to recover. Gostlin, while charged, was never arrested nor spent any time in court. Jordana's family was told by the Portage County (Ohio) prosecutor's office that attempts had been made to serve the warrant but Gostlin's whereabouts were unknown. As a result of no movement on the case by the authorities, Jordana felt that justice would not be served. They left the U.S. for England because of their concerns about personal safety and well-being.\n\nIn addition to production Jordana has also been a DJ, musician and singer. In 2001–2002 Jordana worked at Flex FM in London, England as Lady J with Lady Sovereign MCing for her during Jordana's radio show. In 2002 she held a club residency spinning UK Garage, 2-Step Garage and House Music at legendary club night Trinity in London's Vauxhall neighborhood.\n\nAfter returning to the US, Jordana has returned to her rock roots and fronts a melodic goth metal band in Seattle. Just prior to that she was asked to play bass in another band briefly where she met and became close friends with singer/songwriter Sheltia Burke.\n\nCurrently, Jordana is scoring the documentary, \"Free CeCe,\" produced and directed by Jacqueline Gares and actress/director Laverne Cox of the series Orange Is The New Black. The documentary details the struggles of CeCe McDonald, an African-American transwoman who was wrongfully incarcerated for murder for defending herself against a hate driven attack on her life outside of a bar in Minneapolis.\n\nJordana is working on a new Drum and Bass E.P. for Bristol, UK based Complex Records. Her guitar and vocal work have featured on clips of songs off of her new E.P. posted to Soundcloud and on cover songs she has posted on AfroPunk.com.\n\nJordana has been featured in several books. Her success rising from the depressed 80s economy of a \"rust belt\" city to MTV featured electronica artist garnered a mention in 2002 New York Times bestseller \"The Rise of the Creative Class\" by economist Richard Florida. She appears in two books examining the history and rise of the American rave/EDM scene: Michaelangelo Matos's \"The Underground is Massive\" details an early online exchange between her and Moby dealing with the role of live performance in a rave context. She is also mentioned in \"Rave Culture: An Insider's Overview\" by Jimi Fritz and Virginia Smallfry.\n\nAn extensive interview with Jordana was also featured in \"The New Transsexuals\" a book by rock journalist and illustrator George Petros(Thrasher, Seconds, Propaganda, EXIT magazines) published in 2012.\n\nShe currently resides in Seattle.\n\n\n\n\nIn 2002 Jordana licensed \"One vocal, background use, forty-five seconds (0:45) in length\" from her song \"Break In\" to Paramount Pictures for the Zoolander DVD release.\n\n"}
{"id": "41263577", "url": "https://en.wikipedia.org/wiki?curid=41263577", "title": "3LAU", "text": "3LAU\n\nJustin David Blau (born January 9, 1991), better known by his stage name 3LAU (pronounced \"Blau\"), is an American DJ and electronic dance music producer. Based in Las Vegas, he has released three mixtapes since 2011 and several singles, and has performed at festivals such as Electric Zoo in New York and EDC Vegas. In 2014 he released the track \"Vikings\" with Botnek on Dim Mak Records.\n\nJustin David Blau was born in Syosset, New York, United States on . Blau grew up around an artistic family, and was soon playing piano, playing guitar and singing. At thirteen, he moved with his family to Las Vegas, Nevada, where he spent the rest of his youth. He attended The Meadows School for high school and Washington University in St. Louis for college.\n\nMuch of 3LAU's music incorporates electro house, dubstep, deep house, and progressive house. He is known for his melodic take on dance music and live sets that incorporate sampling. He is also known for using Ableton.\n\nIn 2011, at the age of 20, he vacationed in Sweden, where he discovered electronic dance music. After returning to college he started producing mashups, \"mainly because [he] didn't think anyone was doing a good job.\" He soon began mixing under the professional name 3LAU, and by June of that year was uploading mashups to YouTube.\n\n3LAU gained recognition in the electronic music world in 2011 with his two bootlegs, \"Girls Who Save the World\" and \"All Night Long\". He also won a remix competition for his remix of Tiesto's \"Work Hard, Play Hard\".\n\nBefore 2012, 3LAU spent his days studying at Washington University in St. Louis and DJing at night. He has had remixes charts in the top 10 on both Beatport and Hype Machine.\n\nIn 2012, he focused on DJing and launched his 3LAU Your Mind tour. That same year, he released his second bootleg album, \"Dance Floor Filth\". In 2012, the \"Las Vegas Review-Journal\" described him as \"one of America's fastest-rising DJ-producers.\" In 2012 he released \"Dance Floor Filth 2\", an album featuring his production.\n\nIn late 2013, he went on a short tour with Carnage called the Night Riot tour. Wrote Jonah Ollman of \"Sound of Boston\" about 3LAU's live performances in late 2013, \"A nice balance of mashups, electro-house, poppy vocal samples, and a 90’s throwback here and there keep the young 20-something crowd going and constantly entertained.\"\n\nIn the summer of 2015, 3LAU did his first European tour visiting Barcelona, Germany and Ibiza. Later that fall, he went on his first Asian tour which included Bangkok, Jakarta, Tokyo and more.\n\nOn 16 February 2018, 3LAU released his debut album \"Ultraviolet\" on label \"BLUME\" featuring singles \"Touch,\" \"On My Own\", \"Walk Away\" and \"Star Crossed\". The album features collaborations with Carly Paige, Nevve, Emma Hewitt, Max Schneider, Said the Sky, and Neonheart. The album hit #1 on iTunes' Electronic Chart, as did lead single \"Touch.\" On 21 June 2018, 3LAU announced the launch of OMF (Our Music Festival), the first blockchain-powered music festival. The festival will be held on 20 October 2018 in San Francisco and will be headlined by Zedd.\n\nIn 2016, 3LAU launched the record label Blume, the first not-for-profit dance music label. Profits from all songs released on Blume are dedicated to charity. 3LAU has raised over $200,000 for non-profit Pencils of Promise. In 2013, 3LAU announced he had built his first school with the non-profit and in 2018 another school was completed in Guatemala.\n\n\n"}
{"id": "54667600", "url": "https://en.wikipedia.org/wiki?curid=54667600", "title": "4th Pyramid", "text": "4th Pyramid\n\nAdam Farag, professionally known as 4th Pyramid, is a hip hop recording artist, producer, songwriter and DJ from Toronto, Ontario of Egyptian descent. He is currently the official DJ for Toronto rapper and Juno-award winner, Jazz Cartier.\n\nPyramid entered the hip hop scene with the release of his self-produced debut album, \"The Light is But a Shade of the Darkness',\" an instrumental full-length. Chuck D of Public Enemy handpicked 4th Pyramid's 'Wallabee Strut' as one of the best 100 MP3s of 2000.\" \"He was signed to rap label Definitive Jux, owned by El-P (Run the Jewels), and appeared on the album Definitive Jux Presents III. During this time, he also collaborated with label mates including Cannibal Ox and Camu Tao.\n\n4th Pyramid released his debut solo rap album \"The Pyramid Scheme\" in 2012 after signing a deal with Universal Music Canada. The album included the single \"It's So Hot\" featuring Greg Nice of Nice and Smooth in 2010. The track was featured in the film 21 and Over and also appeared on the film's soundtrack. The album also included collaborations with Saukrates, Marco Polo & more. In 2013, he released the EP \"The Sky Belongs To The Stars\" which included the single \"Friday Nights\" produced by Grandtheft. He has toured with Wu-Tang Clan, ASAP Rocky, De La Soul, Elzhi, The Pharcyde and others.\n\n4th has produced and engineered tracks for the Black Eyed Peas, members of Wu-Tang Clan, Pete Rock and others.\n\n4th Pyramid formed the DJ group, \"Sheen Bros\" with Philadelphia DJ, Cosmo Baker and they released several mixtapes together and an EP through the Scion/AV label. 4th Pyramid continues to DJ internationally and is currently the official DJ for Capitol Records recording artist, Jazz Cartier. The two have toured Europe, North America and Canada including a tour with Post Malone.\n"}
{"id": "2188910", "url": "https://en.wikipedia.org/wiki?curid=2188910", "title": "9th Prince", "text": "9th Prince\n\n9th Prince (born Terrance Hamlin) is an American rapper, musician and one of the founding members of rap group Killarmy. He was initially responsible for bringing the various members together, under the guide of his brother. He released his solo debut album \"Granddaddy Flow\" in 2003. He is the younger brother of RZA. He went by the alias Madman & Iron Fingers, especially in his early recordings. 9th Prince gets his name from the kung-fu movie \"Shaolin Prince\". He has pushed hard for a reunion album with his group, and helped to release a 'greatest hits' album.\n\n\n\n\n"}
{"id": "32501079", "url": "https://en.wikipedia.org/wiki?curid=32501079", "title": "A-Lin", "text": "A-Lin\n\nA-Lin (), also known by her birth name Huang Li-ling and her Amis name Lisang Pacidal Koyouan, is an aboriginal Taiwanese pop singer, lyricist, and occasional composer of Amis descent.\n\nIn 1999, when A-Lin was 16 years old and taught children to sing the hymn “Amazing Grace” after the 921 earthquake in Nantou County, she was discovered by her former agent. However, he was not available when she wanted to give him contact information. Fortunately, four years later, A-Lin met him again in a bar where she sang regularly. She was officially offered a singing contract.\n\nA-Lin made her debut in the world of music at the end of 2006, achieving rapid commercial success. She has been called the next A-Mei, who is an aboriginal singer of Puyuma descent. She also got the nickname 天生歌姬, meaning \"A Born Diva\". A-Lin was often quoted as saying that A-Mei is highly influential to her music and often covers A-Mei's songs in music shows and concerts. A-Lin has won numerous music awards and is extremely popular within Mandarin-speaking world.\n\nA-Lin has performed internationally and at a sold-out concert with Xiao Yu and Shin in 2012 at the Sydney Town Hall. It was her first visit to Sydney and people flew in from different cities just to see her.\n\nIn 2013, with Avex Taiwan exiting the market due to overall poor sales from the records, all singers under Avex like Huang Li-ling were allowed to terminate their contracts early.\n\nAfter terminating her contract, A-Lin signed with her new label Sony Music. In December 2014, she released her first album after leaving Avex. This album was nominated for Best Mandarin Female Singer, making her fourth nomination without any win.\n\nIn 2015, she participated in the third season of I Am a Singer and came in sixth on the finale held in March 2015. As a result, her singing ability was recognized by more people and provided a boost to her career.\n\nWhile A-Lin considers herself as an Contralto, she has been noted to have an extremely wide vocal range closely identical to that of Amanda Seyfried, as noted in many of her songs to have over two octaves or more, and in the concert in Auckland, New Zealand, she was suffering from illnesses that made her unable to hit her falsetto range. She has also notably performed \"Remember\", by A-Mei, in infantile speech.\n\nA-Lin is the youngest in her family, her parents are Roman Catholics and love music.\nA-Lin's older sister is Taiwanese model Lea Huang, who herself entered the Super Star competition in Taiwan in July 2013 as an aspiring singer in her own right. In 2015, after A-Lin entered I Am A Singer, Lea became a contestant for ‘The most beautiful harmonies’ (最美和聲). They also have an older brother, and A-Lin's cousin-in-law is her fitness coach and also a guitarist. A-Lin was extremely popular as a student and the confidence she gained from this led to her decision to compete in a 'Singing Strength Elite Rivalry Competition' [歌唱實力精英角逐比賽]. She chose the hit song 'At least you are still there' [至少還有你], by Sandy Lam, and the competition allowed A-Lin to believe she had a future as a professional singer. In reference to talent shows being readily accessible to young people these days, A-Lin commented, \"I am impressed that there is now a \"Million Star\"-type of singing platform, which allows both everyone who enjoys singing and those who are talented singers a tough but realistic chance - an accomplished dream.\" They also have a brother.\n\nIn 1 July 2007, A-Lin married Huang Kan-lin, a well-known Taiwanese baseball player. They have a daughter, Huang Qiaoyu. (A-Lin said: \"At the first time Huang Kan-lin requested a song \"How Do I Live\" by LeAnn Rimes in her Tainan Pub Session. However, at that time because of his poor English, he wrote “Good Do A-Lin \" on a piece of paper and then spelled the song in homophonic Mandarin. The implication was that he wanted to chase after A-Lin because of her rich lips. Eventually, Huang Kan-lin proposed to A-Lin successfully by a new recorded song \" 愛情問怎麼走 (How Love Will Go)\".)\n\n"}
{"id": "13820259", "url": "https://en.wikipedia.org/wiki?curid=13820259", "title": "A-Q", "text": "A-Q\n\nGilbert Bani (born August 1, 1986) popularly known by his stage name A-Q, is a Nigerian rapper, recording artist, dancer, songwriter, and performer.\n\nA-Q was nominated for The Headies for best rap song artist 2016, for \"Agu Ji Ndi Men\".\n\nBorn into a family of six as the last child, A-Q real name Gilbert Bani was raised in Surulere where he has lived for most of his life.\n\nA-Q attended Kings College Lagos, and had his tertiary education at the University of Lagos. At a young age he started collecting hip-hop tracks, learning the lyrics and miming them. It didn’t take long before he started writing his own lyrics.\n\nA-Q signed his first record deal in 2001 with Big Leaf Records where he was for two years before he started working on his own in 2003. His first single \"(W)rap Nigeria\", produced by kraftmatics, debuted in 2004 followed by a music video. He subsequently released a full compilation album \"Listen and Understand\" in 2005 followed by another single, \"Things That We Do\", which got him an international online distribution deal. In 2006 A-Q put out a mixtape titled \"Maga Must Pay vol 1.\"\n\nA-Q took a break from music to finish his education in University of Lagos but still managed to release a single, \"Make Money\", and accompanying video in 2008 produced by Laylow and featuring Morachi and Xtrim off an online mixtape titled \"Love and Money.\"\n\nA-Q graduated in 2010 and returned to the entertainment industry, this time co-floating a record label, Hustle Inc, on whose platform he started Black Friday Twitter Freestyles, a weekly free download release. This got him online acknowledgement including from producer Don Jazzy who styled him “one of the sickest rappers alive”. He recorded and released \"The Past Present and Future\" album in December 2010 which had the singles \"Names\" and \"Champagne and Rum\". He released more music including \"Distractions\" featuring Vector in 2012, \"Machine Gun Flow\", \"555 (5beats 5verses and 5blessings)\", and \"Why\" featuring Pamela.\n\nHe released an EP in December 2012 titled \"Make Your Best Rapper Look Stupid.\" He is currently working on his sophomore album titled \"G.I.L.B.E.R.T (Grace and Glory).\"\n\n\n\n"}
{"id": "3657357", "url": "https://en.wikipedia.org/wiki?curid=3657357", "title": "A-do", "text": "A-do\n\nDu Chengyi (), known professionally as A-do (阿杜), is a Singaporean singer . The ex-construction foreman was spotted by Singaporean producer Billy Koh in his company's talent search and was signed to Ocean Butterflies Music. He released his debut album in 2002.\n"}
{"id": "50527564", "url": "https://en.wikipedia.org/wiki?curid=50527564", "title": "A Mít", "text": "A Mít\n\nA Mít (born 1997) is a Vietnamese footballer of ethnic Degar descent who plays as a midfielder for V-League (Vietnam) club SHB Đà Nẵng F.C..\n\nA Mít was born one of eight children to a poor Degar family of melon farmers in Kon Tum. At the age of just 12 A Mít traveled to Pleiku to join the HAGL – Arsenal JMG Academy. However A Mít's small size and frequent injuries and sicknesses caused him to quit and go back to his families farm. A couple years latter a scout from SHB Đà Nẵng took notice and invited A Mít to the club's academy. A Mít showed impressive improvement and in 2016 made his league debut.\n"}
{"id": "58299996", "url": "https://en.wikipedia.org/wiki?curid=58299996", "title": "A Zin Latt", "text": "A Zin Latt\n\nA Zin Latt (, also known as Kay Kay (or) Kay Zin Latt; born 18 January 1981) is a Burmese politician and physician currently serving as a Pyithu Hluttaw MP for Shwebo constituency. She is a member of the National League for Democracy.\n\nLatt was born on 18 January 1981 in Kanbalu Township, Myanmar. She graduated with M.B.B.S from University of Medicine, Mandalay. Her previous job is doctor.\n\nShe is a member of the National League for Democracy. In the Myanmar general election, 2015, she was elected as an Pyithu Hluttaw MP and elected representative from Shwebo constituency.\n"}
{"id": "985619", "url": "https://en.wikipedia.org/wiki?curid=985619", "title": "Agent-based model", "text": "Agent-based model\n\nAn agent-based model (ABM) is a class of computational models for simulating the actions and interactions of autonomous agents (both individual or collective entities such as organizations or groups) with a view to assessing their effects on the system as a whole. It combines elements of game theory, complex systems, emergence, computational sociology, multi-agent systems, and evolutionary programming. Monte Carlo methods are used to introduce randomness. Particularly within ecology, ABMs are also called individual-based models (IBMs), and individuals within IBMs may be simpler than fully autonomous agents within ABMs. A review of recent literature on individual-based models, agent-based models, and multiagent systems shows that ABMs are used on non-computing related scientific domains including biology, ecology and social science. Agent-based modeling is related to, but distinct from, the concept of multi-agent systems or multi-agent simulation in that the goal of ABM is to search for explanatory insight into the collective behavior of agents obeying simple rules, typically in natural systems, rather than in designing agents or solving specific practical or engineering problems.\n\nAgent-based models are a kind of microscale model that simulate the simultaneous operations and interactions of multiple agents in an attempt to re-create and predict the appearance of complex phenomena. The process is one of emergence from the lower (micro) level of systems to a higher (macro) level. As such, a key notion is that simple behavioral rules generate complex behavior. This principle, known as K.I.S.S. (\"Keep it simple, stupid\"), is extensively adopted in the modeling community. Another central tenet is that the whole is greater than the sum of the parts. Individual agents are typically characterized as boundedly rational, presumed to be acting in what they perceive as their own interests, such as reproduction, economic benefit, or social status, using heuristics or simple decision-making rules. ABM agents may experience \"learning\", adaptation, and reproduction.\n\nMost agent-based models are composed of: (1) numerous agents specified at various scales (typically referred to as agent-granularity); (2) decision-making heuristics; (3) learning rules or adaptive processes; (4) an interaction topology; and (5) an environment. ABMs are typically implemented as computer simulations, either as custom software, or via ABM toolkits, and this software can be then used to test how changes in individual behaviors will affect the system's emerging overall behavior.\n\nThe idea of agent-based modeling was developed as a relatively simple concept in the late 1940s. Since it requires computation-intensive procedures, it did not become widespread until the 1990s.\n\nThe history of the agent-based model can be traced back to the Von Neumann machine, a theoretical machine capable of reproduction. The device von Neumann proposed would follow precisely detailed instructions to fashion a copy of itself. The concept was then built upon by von Neumann's friend Stanislaw Ulam, also a mathematician; Ulam suggested that the machine be built on paper, as a collection of cells on a grid. The idea intrigued von Neumann, who drew it up—creating the first of the devices later termed cellular automata.\nAnother advance was introduced by the mathematician John Conway. He constructed the well-known Game of Life. Unlike von Neumann's machine, Conway's Game of Life operated by tremendously simple rules in a virtual world in the form of a 2-dimensional checkerboard.\n\nOne of the earliest agent-based models in concept was Thomas Schelling's segregation model, which was discussed in his paper \"Dynamic Models of Segregation\" in 1971. Though Schelling originally used coins and graph paper rather than computers, his models embodied the basic concept of agent-based models as autonomous agents interacting in a shared environment with an observed aggregate, emergent outcome.\n\nIn the early 1980s, Robert Axelrod hosted a tournament of Prisoner's Dilemma strategies and had them interact in an agent-based manner to determine a winner. Axelrod would go on to develop many other agent-based models in the field of political science that examine phenomena from ethnocentrism to the dissemination of culture.\nBy the late 1980s, Craig Reynolds' work on flocking models contributed to the development of some of the first biological agent-based models that contained social characteristics. He tried to model the reality of lively biological agents, known as artificial life, a term coined by Christopher Langton.\n\nThe first use of the word \"agent\" and a definition as it is currently used today is hard to track down. One candidate appears to be John Holland and John H. Miller's 1991 paper \"Artificial Adaptive Agents in Economic Theory\", based on an earlier conference presentation of theirs.\n\nAt the same time, during the 1980s, social scientists, mathematicians, operations researchers, and a scattering of people from other disciplines developed Computational and Mathematical Organization Theory (CMOT). This field grew as a special interest group of The Institute of Management Sciences (TIMS) and its sister society, the Operations Research Society of America (ORSA).\n\nWith the appearance of StarLogo in 1990, Swarm and NetLogo in the mid-1990s and RePast and AnyLogic in 2000, or GAMA in 2007 as well as some custom-designed code, modelling software became widely available and the range of domains that ABM was applied to, grew. Bonabeau (2002) is a good survey of the potential of agent-based modeling as of the time\n\nThe 1990s were especially notable for the expansion of ABM within the social sciences, one notable effort was the large-scale ABM, Sugarscape, developed by\nJoshua M. Epstein and Robert Axtell to simulate and explore the role of social phenomena such as seasonal migrations, pollution, sexual reproduction, combat, and transmission of disease and even culture. Other notable 1990s developments included Carnegie Mellon University's Kathleen Carley ABM, to explore the co-evolution of social networks and culture.\nDuring this 1990s timeframe Nigel Gilbert published the first textbook on Social Simulation: Simulation for the social scientist (1999) and established a journal from the perspective of social sciences: the \"Journal of Artificial Societies and Social Simulation\" (JASSS). Other than JASSS, agent-based models of any discipline are within scope of SpringerOpen journal \"Complex Adaptive Systems Modeling\" (CASM).\n\nThrough the mid-1990s, the social sciences thread of ABM began to focus on such issues as designing effective teams, understanding the communication required for organizational effectiveness, and the behavior of social networks. CMOT—later renamed Computational Analysis of Social and Organizational Systems (CASOS)—incorporated more and more agent-based modeling. Samuelson (2000) is a good brief overview of the early history, and Samuelson (2005) and Samuelson and Macal (2006) trace the more recent developments.\n\nIn the late 1990s, the merger of TIMS and ORSA to form INFORMS, and the move by INFORMS from two meetings each year to one, helped to spur the CMOT group to form a separate society, the North American Association for Computational Social and Organizational Sciences (NAACSOS). Kathleen Carley was a major contributor, especially to models of social networks, obtaining National Science Foundation funding for the annual conference and serving as the first President of NAACSOS. She was succeeded by David Sallach of the University of Chicago and Argonne National Laboratory, and then by Michael Prietula of Emory University. At about the same time NAACSOS began, the European Social Simulation Association (ESSA) and the Pacific Asian Association for Agent-Based Approach in Social Systems Science (PAAA), counterparts of NAACSOS, were organized. As of 2013, these three organizations collaborate internationally. The First World Congress on Social Simulation was held under their joint sponsorship in Kyoto, Japan, in August 2006. The Second World Congress was held in the northern Virginia suburbs of Washington, D.C., in July 2008, with George Mason University taking the lead role in local arrangements.\n\nMore recently, Ron Sun developed methods for basing agent-based simulation on models of human cognition, known as cognitive social simulation. Bill McKelvey, Suzanne Lohmann, Dario Nardi, Dwight Read and others at UCLA have also made significant contributions in organizational behavior and decision-making. Since 2001, UCLA has arranged a conference at Lake Arrowhead, California, that has become another major gathering point for practitioners in this field. In 2014, Sadegh Asgari from Columbia University and his colleagues developed an agent-based model of the construction competitive bidding. While his model was used to analyze the low-bid lump-sum construction bids, it could be applied to other bidding methods with little modifications to the model.\n\nMost computational modeling research describes systems in equilibrium or as moving between equilibria. Agent-based modeling, however, using simple rules, can result in different sorts of complex and interesting behavior. The three ideas central to agent-based models are agents as objects, emergence, and complexity.\n\nAgent-based models consist of dynamically interacting rule-based agents. The systems within which they interact can create real-world-like complexity. Typically agents are\nsituated in space and time and reside in networks or in lattice-like neighborhoods. The location of the agents and their responsive behavior are encoded in algorithmic form in computer programs. In some cases, though not always, the agents may be considered as intelligent and purposeful. In ecological ABM (often referred to as \"individual-based models\" in ecology), agents may, for example, be trees in forest, and would not be considered intelligent, although they may be \"purposeful\" in the sense of optimizing access to a resource (such as water).\nThe modeling process is best described as inductive. The modeler makes those assumptions thought most relevant to the situation at hand and then watches phenomena emerge from the agents' interactions. Sometimes that result is an equilibrium. Sometimes it is an emergent pattern. Sometimes, however, it is an unintelligible mangle.\n\nIn some ways, agent-based models complement traditional analytic methods. Where analytic methods enable humans to characterize the equilibria of a system, agent-based models allow the possibility of generating those equilibria. This generative contribution may be the most mainstream of the potential benefits of agent-based modeling. Agent-based models can explain the emergence of higher-order patterns—network structures of terrorist organizations and the Internet, power-law distributions in the sizes of traffic jams, wars, and stock-market crashes, and social segregation that persists despite populations of tolerant people. Agent-based models also can be used to identify lever points, defined as moments in time in which interventions have extreme consequences, and to distinguish among types of path dependency.\n\nRather than focusing on stable states, many models consider a system's robustness—the ways that complex systems adapt to internal and external pressures so as to maintain their functionalities. The task of harnessing that complexity requires consideration of the agents themselves—their diversity, connectedness, and level of interactions.\n\nRecent work on the Modeling and simulation of Complex Adaptive Systems has demonstrated the need for combining agent-based and complex network based models. describe a framework consisting of four levels of developing models of complex adaptive systems described using several example multidisciplinary case studies:\nOther methods of describing agent-based models include code templates and text-based methods such as the ODD (Overview, Design concepts, and Design Details) protocol.\n\nThe role of the environment where agents live, both macro and micro, is also becoming an important factor in agent-based modelling and simulation work. Simple environment affords simple agents, but complex environments generates diversity of behaviour.\n\nAgent-based modeling has been used extensively in biology, including the analysis of the spread of epidemics, and the threat of biowarfare, biological applications including population dynamics, vegetation ecology, landscape diversity, the growth and decline of ancient civilizations, evolution of ethnocentric behavior, forced displacement/migration, language choice dynamics, cognitive modeling, and biomedical applications including modeling 3D breast tissue formation/morphogenesis, the effects of ionizing radiation on mammary stem cell subpopulation dynamics, inflammation,\nand the human immune system. Agent-based models have also been used for developing decision support systems such as for breast cancer. Agent-based models are increasingly being used to model pharmacological systems in early stage and pre-clinical research to aid in drug development and gain insights into biological systems that would not be possible \"a priori\". Military applications have also been evaluated. Moreover, agent-based models have been recently employed to study molecular-level biological systems.\n\nAgent-based models have been used since the mid-1990s to solve a variety of business and technology problems. Examples of applications include the modeling of organizational behaviour and cognition, team working, supply chain optimization and logistics, modeling of consumer behavior, including word of mouth, social network effects, distributed computing, workforce management, and portfolio management. They have also been used to analyze traffic congestion.\n\nRecently, agent based modelling and simulation has been applied to various domains such as studying the impact of publication venues by researchers in the computer science domain (journals versus conferences). In addition, ABMs have been used to simulate information delivery in ambient assisted environments. A November 2016 article in arXiv analyzed an agent based simulation of posts spread in the Facebook online social network. In the domain of peer-to-peer, ad-hoc and other self-organizing and complex networks, the usefulness of agent based modeling and simulation has been shown. The use of a computer science-based formal specification framework coupled with wireless sensor networks and an agent-based simulation has recently been demonstrated.\n\nAgent based evolutionary search or algorithm is a new research topic for solving complex optimization problems.\n\nPrior to, and in the wake of the financial crisis, interest has grown in ABMs as possible tools for economic analysis. ABMs do not assume the economy can achieve equilibrium and \"representative agents\" are replaced by agents with diverse, dynamic, and interdependent behavior including herding. ABMs take a \"bottom-up\" approach and can generate extremely complex and volatile simulated economies. ABMs can represent unstable systems with crashes and booms that develop out of non-linear (disproportionate) responses to proportionally small changes. A July 2010 article in \"The Economist\" looked at ABMs as alternatives to DSGE models. The journal \"Nature\" also encouraged agent-based modeling with an editorial that suggested ABMs can do a better job of representing financial markets and other economic complexities than standard models along with an essay by J. Doyne Farmer and Duncan Foley that argued ABMs could fulfill both the desires of Keynes to represent a complex economy and of Robert Lucas to construct models based on microfoundations. Farmer and Foley pointed to progress that has been made using ABMs to model parts of an economy, but argued for the creation of a very large model that incorporates low level models. By modeling a complex system of analysts based on three distinct behavioral profiles – imitating, anti-imitating, and indifferent – financial markets were simulated to high accuracy. Results showed a correlation between network morphology and the stock market index.\n\nSince the beginning of the 21st century ABMs have been deployed in architecture and urban planning to evaluate design and to simulate pedestrian flow in the urban environment. There is also a growing field of socio-economic analysis of infrastructure investment impact using ABM's ability to discern systemic impacts upon a socio-economic network.\n\nThe agent-directed simulation (ADS) metaphor distinguishes between two categories, namely \"Systems for Agents\" and \"Agents for Systems.\" Systems for Agents (sometimes referred to as agents systems) are systems implementing agents for the use in engineering, human and social dynamics, military applications, and others. Agents for Systems are divided in two subcategories. Agent-supported systems deal with the use of agents as a support facility to enable computer assistance in problem solving or enhancing cognitive capabilities. Agent-based systems focus on the use of agents for the generation of model behavior in a system evaluation (system studies and analyses).\n\nMany agent-based modeling software are designed for serial von-Neumann computer architectures. This limits the speed and scalability of these systems. A recent development is the use of data-parallel algorithms on Graphics Processing Units GPUs for ABM simulation. The extreme memory bandwidth combined with the sheer number crunching power of multi-processor GPUs has enabled simulation of millions of agents at tens of frames per second.\n\nVerification and validation (V&V) of simulation models is extremely important. Verification involves the model being debugged to ensure it works correctly, whereas validation ensures that the right model has been built. Face validation, sensitivity analysis, calibration and statistical validation have also been demonstrated. A discrete-event simulation framework approach for the validation of agent-based systems has been proposed. A comprehensive resource on empirical validation of agent-based models can be found here.\n\nAs an example of V&V technique, consider VOMAS (virtual overlay multi-agent system), a software engineering based approach, where a virtual overlay multi-agent system is developed alongside the agent-based model. The agents in the multi-agent system are able to gather data by generation of logs as well as provide run-time validation and verification support by watch agents and also agents to check any violation of invariants at run-time. These are set by the Simulation Specialist with help from the SME (subject-matter expert). Muazi et al. also provide an example of using VOMAS for verification and validation of a forest fire simulation model.\n\nVOMAS provides a formal way of validation and verification. To develop a VOMAS, one must design VOMAS agents along with the agents in the actual simulation, preferably from the start. In essence, by the time the simulation model is complete, one can essentially consider it to be one model containing two models:\n\nUnlike all previous work on verification and validation, VOMAS agents ensure that the simulations are validated in-simulation i.e. even during execution. In case of any exceptional situations, which are programmed on the directive of the Simulation Specialist (SS), the VOMAS agents can report them. In addition, the VOMAS agents can be used to log key events for the sake of debugging and subsequent analysis of simulations. In other words, VOMAS allows for a flexible use of any given technique for the sake of verification and validation of an agent-based model in any domain.\n\nDetails of validated agent-based modeling using VOMAS along with several case studies are given in. This thesis also gives details of \"exploratory agent-based modeling\", \"descriptive agent-based modeling\" and \"validated agent-based modeling\", using several worked case study examples.\n\nMathematical models of complex systems are of three types: black-box (phenomenological), white-box (mechanistic, based on the first principles) and grey-box (mixtures of phenomenological and mechanistic models).\n\n\n\n"}
{"id": "1544170", "url": "https://en.wikipedia.org/wiki?curid=1544170", "title": "Anisogamy", "text": "Anisogamy\n\nAnisogamy (also called heterogamy) is the form of sexual reproduction that involves the union or fusion of two gametes, which differ in size and/or form. (The related adjectives are \"anisogamous\" and \"anisogamic\"). The smaller gamete is considered to be male (sperm cell), whereas the larger gamete is regarded as female (egg cell).\n\nThere are several types of anisogamy. Both gametes may be flagellated and therefore motile. Alternatively, both of the gametes may be non-flagellated. The latter situation occurs in some algae and plants. In the red alga \"Polysiphonia\", non-motile eggs are fertilized by non-motile sperm. In flowering plants, the gametes are non-motile cells within gametophytes.\n\nThe form of anisogamy that occurs in animals, including humans, is oogamy, where a large, non-motile egg (ovum) is fertilized by a small, motile sperm (spermatozoon). The egg is optimized for longevity, whereas the small sperm is optimized for motility and speed. The size and resources of the egg cell allow for the production of pheromones, which attract the swimming sperm cells.\n\nAnisogamy is a fundamental concept of sexual dimorphism that helps explain phenotypic differences between sexes. In most species a male and female sex exist, both of which are optimized for reproductive potential. Due to their differently sized and shaped gametes, both males and females have developed physiological and behavioral differences that optimize the individual’s fecundity. Since most egg laying females typically must bear the offspring and have a more limited reproductive cycle, this typically makes females a limiting factor in the reproductive success rate of males in a species. This process is also true for females selecting males, and assuming that males and females are selecting for different traits in partners, would result in phenotypic differences between the sexes over many generations. This hypothesis, known as the Bateman’s Principle, is used to understand the evolutionary pressures put on males and females due to anisogamy. Although this assumption has criticism, it is a generally accepted model for sexual selection within anisogamous species. The selection for different traits depending on sex within the same species is known as sex-specific selection, and accounts for the differing phenotypes found between the sexes of the same species. This sex-specific selection between sexes over time also lead to the development of secondary sex characteristics, which assist males and females in reproductive success.\n\nIn most species, both sexes choose mates based on the available phenotypes of potential mates. These phenotypes are species specific, resulting in varying strategies for successful sexual reproduction. For example, large males are sexually selected for in elephant seals for their large size helps the male fight off other males, but small males are sexually selected for in spiders for they can mate with the female more quickly while avoiding sexual cannibalism. However, despite the large range of sexually selected phenotypes, most anisogamous species follow a set of predictable desirable traits and selective behaviors based on general reproductive success models.\n\nFor internal fertilizers, female investment is high in reproduction since they typically expend more energy throughout a single reproductive event. This can be seen as early as oogenesis, for the female sacrifices gamete number for gamete size to better increase the survival chances of the potential zygote; a process more energetically demanding than spermatogenesis in males. Oogenesis occurs in the ovary, a female specific organ that also produces hormones to prepare other female-specific organs for the changes necessary in the reproductive organs to facilitate egg delivery in external fertilizers, and zygote development in internal fertilizers. The egg cell produced is not only large, but sometimes even immobile, requiring contact with the more mobile sperm to instigate fertilization.\n\nSince this process is very energy-demanding and time consuming for the female, mate choice is often integrated into the female’s behavior. Females will often be very selective of the males they choose to reproduce with, for the phenotype of the male can be indicative of the male’s physical health and heritable traits. Females employ mate choice to pressure males into displaying their desirable traits to females through courtship, and if successful, the male gets to reproduce. This encourages males and females of specific species to invest in courtship behaviors as well as traits that can display physical health to a potential mate. This process, known as sexual selection, results in the development of traits to ease reproductive success rather than individual survival, such as the inflated size of a termite queen. It is also important for females to select against potential mates that may have a sexually transmitted infection, for the disease could not only hurt the female’s reproductive ability, but also damage the resulting offspring.\n\nAlthough not uncommon in males, females are more associated with parental care. Since females are on a more limited reproductive schedule than males, a female often invests more in protecting the offspring to sexual maturity than the male. Like mate choice, the level of parental care varies greatly between species, and is often dependent on the number of offspring produced per sexual encounter.\n\nIn most species such as \"Drosophila melanogaster,\" females can utilize sperm storage, a process by which the female can store excess sperm from a mate, and fertilize her eggs long after the reproductive event if mating opportunities drop or quality of mates decreases. By being able to save sperm from more desirable mates, the female gains more control over its own reproductive success, thus allowing for the female to be more selective of males as well as making the timing of fertilization potentially more frequent if males are scarce.\n\nFor males of all species, the sperm cells they produce are optimized for ensuring fertilization of the female egg. These sperm cells are created through spermatogenesis, a form of gametogenesis that focuses on developing the most possible gametes per sexual encounter. Spermatogenesis occurs in the testes, a male specific organ that is also produces hormones that trigger the development of secondary sex characteristics. Since the male’s gametes are energetically cheap and abundant in every ejaculation, a male can greatly increase his sexual success by mating far more frequently than the female. Sperm, unlike egg cells, are also mobile, allowing for the sperm to swim towards the egg through the female’s sexual organs. Sperm competition is also a major factor in the development of sperm cells. Only one sperm can fertilize an egg, and since females can potentially reproduce with more than one male before fertilization occurs, producing sperm cells that are faster, more abundant, and more viable than that produced by other males can give a male reproductive advantage.\n\nSince females are often the limiting factor in a species reproductive success, males are often expected by the females to search and compete for the female, known as intraspecific competition. This can be seen in organisms such as bean beetles, as the male that searches for females more frequently is often more successful at finding mates and reproducing. In species undergoing this form of selection, a fit male would be one that is fast, has more refined sensory organs, and spatial awareness.\n\nSome secondary sex characteristics are not only meant for attracting mates, but also for competing with other males for copulation opportunities. Some structures, such as antlers in deer, can provide benefits to the male’s reproductive success by providing a weapon to prevent rival males from achieving reproductive success. However, other structures such as the large colorful tail feathers found in male peacocks, are a result of Fisherian Runaway as well as several more species specific factors. Due to females selecting for specific traits in males, over time, these traits are exaggerated to the point where they could hinder the male’s survivability. However, since these traits greatly benefit sexual selection, their usefulness in providing more mating opportunities overrides the possibility that the trait could lead to a shortening of its lifespan through predation or starvation. These desirable traits extend beyond physical body parts, and often extend into courtship behavior and nuptial gifts as well.\n\nAlthough some behaviors in males are meant to work within the parameters of cryptic female choice, some male traits work against it. Strong enough males, in some cases, can force themselves upon a female, forcing fertilization and overriding female choice. Since this can often be dangerous for the female, an Evolutionary Arms Race between the sexes is often an outcome.\n\nAnisogamy is the phenomenon of fertilization of large gametes (egg cells, ova) by (or with) small gametes (sperm cells: spermatozoa or spermatia). Gamete size difference is the fundamental difference between females and males. Anisogamy first evolved in multicellular haploid species after the differentiation of different mating types had already been established. However, in Ascomycetes, anisogamy evolved from isogamy before mating types.\n\nThe three main theories for the evolution of anisogamy are gamete competition, gamete limitation, and intracellular conflicts, but the last of these three is not well supported by current evidence. Both gamete competition and gamete limitation assume that anisogamy originated through disruptive selection acting on an ancestral isogamous population with external fertilization, due to a trade-off between larger gamete number and gamete size (which in turn affects zygote survival), because the total resource one individual can invest in reproduction is assumed to be fixed.\n\nThe first formal, mathematical theory proposed to explain the evolution of anisogamy was based on gamete limitation: this model assumed that natural selection would lead to gamete sizes that result in the largest population-wide number of successful fertilizations. If it is assumed that a certain amount of resources provided by the gametes are needed for the survival of the resulting zygote, and that there is a trade-off between the size and number of gametes, then this optimum was shown to be one where both small (male) and large (female) gametes are produced. However, these early models assume that natural selection acts mainly at the population level, something that is today known to be a very problematic assumption.\n\nThe first mathematical model to explain the evolution of anisogamy via individual level selection, and one that became widely accepted was the theory of gamete or sperm competition. Here, selection happens at the individual level: those individuals that produce more (but smaller) gametes also gain a larger proportion of fertilizations simply because they produce a larger number of gametes that 'seek out' those of the larger type. However, because zygotes formed from larger gametes have better survival prospects, this process can again lead to the divergence of gametes sizes into large and small (female and male) gametes. The end result is one where it seems that the numerous, small gametes compete for the large gametes that are tasked with providing maximal resources for the offspring.\n\nSome recent theoretical work has challenged the gamete competition theory, by showing that gamete limitation by itself can lead to the divergence of gamete sizes even under selection at the individual level. While this is possible, it has also been shown that gamete competition and gamete limitation are the ends of a continuum of selective pressures, and they can act separately or together depending on the conditions. These selection pressures also act in the same direction (to increase gamete numbers at the expense of size) and at the same level (individual selection). Theory also suggests that gamete limitation could only have been the dominant force of selection for the evolutionary origin of the sexes under quite limited circumstances, and the presence on average of just one competitor can makes the 'selfish' evolutionary force of gamete competition stronger than the 'cooperative' force of gamete limitation even if gamete limitation is very acute (approaching 100% of eggs remaining unfertilized).\n\nThere is then a relatively sound theory base for understanding this fundamental transition from isogamy to anisogamy in the evolution of reproduction, which is predicted to be associated with the transition to multicellularity. In fact, Hanschen et al (2018) demonstrate that anisogamy evolved from isogamous multicellular ancestors and that anisogamy would subsequently drive secondary sexual dimorphism. Some comparative empirical evidence for the gamete competition theories exists, although it is difficult to use this evidence to fully tease apart the competition and limitation theories because their testable predictions are similar. It has also been claimed that some of the organisms used in such comparative studies do not fit the theoretical assumptions well.\n\nA valuable model system to the study of the evolution of anisogamy is the volvocine algae, which group of chlorophytes is quite unique for its extant species exhibit a diversity of mating systems (isogamy and anisogamy) in addition to its extremes in both unicellularity and multicellularity with a diversity of forms in species of intermediate ranges of sizes. Marine algae have been closely studied to understand the trajectories of such diversified reproductive systems, evolution of sex and mating types, as well as the adaptiveness and stability of anisogamy.\n\n"}
{"id": "42860045", "url": "https://en.wikipedia.org/wiki?curid=42860045", "title": "Anxiety buffer disruption theory", "text": "Anxiety buffer disruption theory\n\nAnxiety buffer disruption theory (ABDT) is an application of terror management theory to explain an individual's reaction to a traumatic event, which leads to post traumatic stress disorder. Terror management theory posits that humans, unlike any other organism, are uniquely aware that death is the inevitable outcome of life. When thoughts of death are made salient, such as when a terrorist attack carries those thoughts into the level of consciousness, humans are subject to debilitating anxiety unless it can be \"buffered.\" Humans respond to the anxiety and dread mortality salience produces by clinging to their cultural worldview, through self-esteem and also close personal relationships. Cultural worldviews, with their cultural norms, religious beliefs and moral values infuse life with meaning. They give life a feeling of normalcy and also a feeling of control. There is no way to definitely prove one's cultural world view, there they are fragile human constructs and must be maintained. Clinging to a cultural worldview and self-esteem buffer the anxiety connected to thoughts of mortality. When thoughts of death are salient, humans are drawn to their cultural world view which \"stipulates appropriate social requirements, and standards for valued conduct, while instilling one's life with meaning, order and permanence.\" \n\nWhen a traumatic experience cannot be assimilated into a currently held cultural worldview, the anxiety-buffering mechanisms are disrupted. ABDT argues that individuals face overwhelming anxiety which leads to the symptoms of PTSD including re-experiencing, hyper-arousal, avoidance and disassociation. The dissociation causes atypical responses to mortality salience compared with individuals who do not suffer from an anxiety buffer disruption When the anxiety buffer disruption is mild, exaggerated coping responses, such as rejecting or taking offense at other cultures, is expected. When the anxiety buffer disruption is severe, there can be a total breakdown of coping mechanisms. The theory was proposed by Tom Pyszczynski and Pelin Kesebir.\n\nIn 1992, Janoff-Bulman delineated a theory of trauma response (Shattered Assumptions Theory). Janoff-Bulman posits that humans have basic assumptions about the world in which they live, based on the belief that the world is a benevolent and meaningful place and that the individual has self-worth. These assumptions give the individual the illusion that they have a measure of control on their own lives as well as a feeling of invulnerability. When an individual faces a traumatic event, their deeply held beliefs that the world is a benevolent and meaningful place and that they have a worthy role in that world are shattered. The world is no longer benevolent or predictable.\nTerror management theory and anxiety buffer disruption theory have taken the concept one step further.\n\nAnxiety buffer disruption theory not only focuses on the thoughts and emotions of an individual, but it also studies the behavior that results when terror management theory and shattered assumptions theory are examined together. Excessive anxiety experienced by post-traumatic stress disorder sufferers occurs because the events causing the post-traumatic stress disorder have demonstrated to these individuals that anxiety-buffering mechanisms are not capable of protecting them from death. Individuals who have high levels of peritraumatic dissociation and low levels of self-efficacy coping, two indicators of post-traumatic stress disorder, have abnormal responses to reminders of death. These individuals in turn do not utilize the coping mechanisms that are typically used to remove the fear of death: culture, self-esteem, and interpersonal relationships. In fact, in individuals with post traumatic stress disorder, mortality salience coping mechanisms are viewed as worthless and perhaps are even seen to be detestable.\n\n\nClose relationships appear to have inoculating power against basic existential threats, which allows people to respond to these threats by fulfilling relational potentials. Second, it seems the sense of relationship commitment is shaped by not only perceived relational investment, gains, and potential alternatives, as well as the existential need of denial of death awareness. Third, it seems processes of terror management not only include worldview defenses to protect the self, but also promote commitment to significant others and the expansion of the self, provided by these relationships.\n\nAn important finding is that close relationships may serve as a fundamental anxiety buffer. It appears close relationships not only protect individuals from concrete and actual threats or danger, but also offers a symbolic shield against the awareness of one’s finitude. Since the threat of death is inescapable, the support from those close to us make may make the thought of death more tolerable by giving meaning to our lives by being important to others.\n\nAs other anxiety buffers, interpersonal relationships are damaged as well in individuals with post traumatic stress disorder. People with post traumatic stress disorder have higher rates of divorce, more difficulty with their children, are more prone to domestic violence, and are emotionally distant from loved ones. All of these are damaging and as a result, terror management cannot be accomplished through close interpersonal relationships.\n\nA study looked at dissociation responses and PTSD in students who survived the 2005 Zarand earthquake in Iran. The earthquake measured 6.4 on the Richter scale, killed more than 1,500 people and displaced more than 6,700 for two months or more. It looked at dissociation one month later then two years later to see if level of dissociation predicted PTSD.\n\nFour weeks after the earthquake, researchers solicited for volunteers at local universities. All of the participants met the DSM-IV criterion for a Class A1 trauma. Many were wearing black mourning clothes or had injuries from the earthquake.\n\nThree priming conditions were employed: mortality salience, earthquake or dental pain. The researchers then evaluated how the subjects felt toward foreign aid in the wake of the disaster. The results indicated that subjects with high dissociative tendencies showed no effect of mortality salience on attitudes toward the foreign aid. Subjects with low dissociative tendencies reacted as terror management theory predicts when confronted with mortality salience and thought of the earthquake.\n\nTwo years after the quake, the researchers returned and 172 of the original respondents participated. They predicted that subjects with high PTSD symptoms would have a disrupted worldview on both foreign aid and the Islamic dress code. They found a strong relationship between dissociation and subsequent PTSD symptom severity. Even after the passage of two years, subjects with high dissociative tendencies were still not defending against existential threats in a way typical for the population who has not experienced trauma.\n\n105 students of the University of Abidjan took the Post-Traumatic Stress Checklist-Civilian Version and then an opinion survey on the civil war in Cote d'Ivoire. Last, they took part in a word-completion task designed to measure their accessibility of death-related thoughts. This study, unlike previous studies related to terror management theory, looked at the subjects' thoughts of death as related to the war, a specific traumatic event.\n\nIn the control condition, where subjects were asked to talk about their anxiety related to their worst exam, death thought accessibility was lower for those with higher levels of PTSD. This suggests that people suffering from strong PTSD repress thoughts of death. But when mortality was made salient, it provoked a marked increase in death thought accessibility for those with high PTSD. The results indicate that the anxiety buffer of death thought suppression under normal circumstances failed when subjects were reminded of the traumatic event.\n\nA second experiment was conducted with 197 students of the University of Ajidjan where they evaluated exposure (proximity) to a traumatic event. In this case, it was geographical location to the civil war. The researchers hypothesized that subjects who lived where the fighting was more constant and intense would be more likely to increase their PTSD reports when mortality was made salient. Those who lived in an area of less conflict should not increase their reports of PTSD. As a form of defensive denial, the researchers predicted they might report lower levels of symptoms. As expected, subjects who had more exposure to war reported greater PTSD symptoms in the mortality salience condition.\n"}
{"id": "3409", "url": "https://en.wikipedia.org/wiki?curid=3409", "title": "Being", "text": "Being\n\nIn philosophy, being means the existence of a thing. Anything that exists has being. Ontology is the branch of philosophy that studies being. Being is a concept encompassing objective and subjective features of reality and existence. Anything that partakes in being is also called a \"being\", though often this usage is limited to entities that have subjectivity (as in the expression \"human being\"). The notion of \"being\" has, inevitably, been elusive and controversial in the history of philosophy, beginning in Western philosophy with attempts among the pre-Socratics to deploy it intelligibly. The first effort to recognize and define the concept came from Parmenides, who famously said of it that \"what is-is\". Common words such as \"is\", \"are\", and \"am\" refer directly or indirectly to being.\n\nAs an example of efforts in recent times, Martin Heidegger (who himself drew on ancient Greek sources) adopted after German terms like \"Dasein\" to articulate the topic. Several modern approaches build on such continental European exemplars as Heidegger, and apply metaphysical results to the understanding of human psychology and the human condition generally (notably in the Existentialist tradition). By contrast, in mainstream Analytical philosophy the topic is more confined to abstract investigation, in the work of such influential theorists as W. V. O. Quine, to name one of many. One of the most fundamental questions that continues to exercise philosophers is posed by William James: \"How comes the world to be here at all instead of the nonentity which might be imagined in its place? ... from nothing to being there is no logical bridge.\"\n\nThe deficit of such a bridge was first encountered in history by the Pre-Socratic philosophers during the process of evolving a classification of all beings (noun). Aristotle, who wrote after the Pre-Socratics, applies the term category (perhaps not originally) to ten highest-level classes. They comprise one category of substance (ousiae) existing independently (man, tree) and nine categories of accidents, which can only exist in something else (time, place). In Aristotle, substances are to be clarified by stating their definition: a note expressing a larger class (the genus) followed by further notes expressing specific differences (differentiae) within the class. The substance so defined was a species. For example, the species, man, may be defined as an animal (genus) that is rational (difference). As the difference is potential within the genus; that is, an animal may or may not be rational, the difference is not identical to, and may be distinct from, the genus.\n\nApplied to being, the system fails to arrive at a definition for the simple reason that no difference can be found. The species, the genus, and the difference are all equally being: a being is a being that is being. The genus cannot be nothing because nothing is not a class of everything. The trivial solution that being is being added to nothing is only a tautology: being is being. There is no simpler intermediary between being and non-being that explains and classifies being.\nPre-Socratic reaction to this deficit was varied. As substance theorists they accepted a priori the hypothesis that appearances are deceiving, that reality is to be reached through reasoning. Parmenides reasoned that if everything is identical to being and being is a category of the same thing then there can be neither differences between things nor any change. To be different, or to change, would amount to becoming or being non-being; that is, not existing. Therefore, being is a homogeneous and non-differentiated sphere and the appearance of beings is illusory. Heraclitus, on the other hand, foreshadowed modern thought by denying existence. Reality does not exist, it flows, and beings are an illusion upon the flow.\n\nAristotle knew of this tradition when he began his \"Metaphysics\", and had already drawn his own conclusion, which he presented under the guise of asking what being is:\"And indeed the question which was raised of old is raised now and always, and is always the subject of doubt, viz., what being is, is just the question, what is substance? For it is this that some assert to be one, others more than one, and that some assert to be limited in number, others unlimited. And so we also must consider chiefly and primarily and almost exclusively what that is which is in this sense.\"\n\nand reiterates in no uncertain terms: \"Nothing, then, which is not a species of a genus will have an essence – only species will have it ...\". Being, however, for Aristotle, is not a genus.\n\nOne might expect a solution to follow from such certain language but none does. Instead Aristotle launches into a rephrasing of the problem, the Theory of Act and Potency. In the definition of man as a two-legged animal Aristotle presumes that \"two-legged\" and \"animal\" are parts of other beings, but as far as man is concerned, are only potentially man. At the point where they are united into a single being, man, the being, becomes actual, or real. Unity is the basis of actuality: \"... 'being' is being combined and one, and 'not being' is being not combined but more than one.\" Actuality has taken the place of existence, but Aristotle is no longer seeking to know what the actual is; he accepts it without question as something generated from the potential. He has found a \"half-being\" or a \"pre-being\", the potency, which is fully being as part of some other substance. Substances, in Aristotle, unite what they actually are now with everything they might become.\n\nSome of Thomas Aquinas' propositions were reputedly condemned by Étienne Tempier, the local Bishop of Paris (not the Papal Magisterium itself) in 1270 and 1277, but his dedication to the use of philosophy to elucidate theology was so thorough that he was proclaimed a Doctor of the Church in 1568. Those who adopt it are called Thomists.\n\nIn a single sentence, parallel to Aristotle's statement asserting that being is substance, St. Thomas pushes away from the Aristotelian doctrine: \"Being is not a genus, since it is not predicated univocally but only analogically.\" His term for analogy is Latin \"analogia\". In the categorical classification of all beings, all substances are partly the same: man and chimpanzee are both animals and the animal part in man is \"the same\" as the animal part in chimpanzee. Most fundamentally all substances are matter, a theme taken up by science, which postulated one or more matters, such as earth, air, fire or water (Empedocles). In today's chemistry the carbon, hydrogen, oxygen and nitrogen in a chimpanzee are identical to the same elements in a man.\n\nThe original text reads, \"Although equivocal predications must be reduced to univocal, still in actions, the non-univocal agent must precede the univocal agent. For the non-univocal agent is the universal cause of the whole species, as for instance the sun is the cause of the generation of all men; whereas the univocal agent is not the universal efficient cause of the whole species (otherwise it would be the cause of itself, since it is contained in the species), but is a particular cause of this individual which it places under the species by way of participation. Therefore the universal cause of the whole species is not an univocal agent; and the universal cause comes before the particular cause. But this universal agent, whilst it is not univocal, nevertheless is not altogether equivocal, otherwise it could not produce its own likeness, but rather it is to be called an analogical agent, as all univocal predications are reduced to one first non-univocal analogical predication, which is being.\"\n\nIf substance is the highest category and there is no substance, being, then the unity perceived in all beings by virtue of their existing must be viewed in another way. St. Thomas chose the analogy: all beings are like, or analogous to, each other in existing. This comparison is the basis of his Analogy of Being. The analogy is said of being in many different ways, but the key to it is the real distinction between existence and essence. Existence is the principle that gives reality to an essence not the same in any way as the existence: \"If things having essences are real, and it is not of their essence to be, then the reality of these things must be found in some principle other than (really distinct from) their essence.\" Substance can be real or not. What makes an individual substance – a man, a tree, a planet – real is a distinct act, a \"to be\", which actuates its unity. An analogy of proportion is therefore possible: \"essence is related to existence as potency is related to act.\"\n\nExistences are not things; they do not themselves exist, they lend themselves to essences, which do not intrinsically have them. They have no nature; an existence receives its nature from the essence it actuates. Existence is not being; it gives being – here a customary phrase is used, existence is a principle (a source) of being, not a previous source, but one which is continually in effect. The stage is set for the concept of God as the cause of all existence, who, as the Almighty, holds everything actual without reason or explanation as an act purely of will.\n\nAristotle's classificatory scheme had included the five predicables, or characteristics that might be predicated of a substance. One of these was the property, an essential universal true of the species, but not in the definition (in modern terms, some examples would be grammatical language, a property of man, or a spectral pattern characteristic of an element, both of which are defined in other ways). Pointing out that predicables are predicated univocally of substances; that is, they refer to \"the same thing\" found in each instance, St. Thomas argued that whatever can be said about being is not univocal, because all beings are unique, each actuated by a unique existence. It is the analogous possession of an existence that allows them to be identified as being; therefore, being is an analogous predication.\n\nWhatever can be predicated of all things is universal-like but not universal, category-like but not a category. St. Thomas called them (perhaps not originally) the \"transcendentia\", \"transcendentals\", because they \"climb above\" the categories, just as being climbs above substance. Later academics also referred to them as \"the properties of being.\" The number is generally three or four.\n\nThe nature of \"being\" has also been debated and explored in Islamic philosophy, notably by Ibn Sina (Avicenna), Suhrawardi, and Mulla Sadra. A modern linguistic approach which notices that Persian language has exceptionally developed two kinds of \"is\"es, i.e. \"ast\" (\"is\", as a copula) and \"hast\" (as an existential \"is\") examines the linguistic properties of the two lexemes in the first place, then evaluates how the statements made by other languages with regard to \"being\" can stand the test of Persian frame of reference.\n\nIn this modern linguistic approach, it is noticed that the original language of the source, e.g. Greek (like German or French or English), has only one word for two concepts, \"ast\" and \"hast\", or, like Arabic, has no word at all for either word. It therefore exploits the Persian \"hast\" (existential \"is\") versus \"ast\" (predicative \"is\" or copula) to address both Western and Islamic ontological arguments on \"being\" and \"existence\".\n\nThis linguistic method shows the scope of confusion created by languages which cannot differentiate between existential be and copula. It manifests, for instance, that the main theme of Heidegger's \"Being and Time\" is \"astī\" (is-ness) rather than \"hastī\" (existence). When, in the beginning of his book, Heidegger claims that people always talk about existence in their everyday language, without knowing what it means, the example he resorts to is: \"the sky \"is\" blue\" which in Persian can be ONLY translated with the use of the copula \"ast\", and says nothing about \"being\" or \"existence\".\n\nIn the same manner, the linguistic method addresses the ontological works written in Arabic. Since Arabic, like Latin in Europe, had become the official language of philosophical and scientific works in the so-called Islamic World, the early Persian or Arab philosophers had difficulty discussing \"being\" or \"existence\", since the Arabic language, like other Semitic languages, had no verb for either predicative \"be\" (copula) or existential \"be\". So if you try to translate the aforementioned Heidegger's example into Arabic it appears as السماء زرقاء (viz. \"The Sky-- blue\") with no linking \"is\" to be a sign of existential statement. To overcome the problem, when translating the ancient Greek philosophy, certain words were coined like ایس \"aysa\" (from Arabic لیس \"laysa\" 'not') for 'is'. Eventually the Arabic verb وجد \"wajada\" (to find) prevailed, since it was thought that whatever is existent, is to be \"found\" in the world. Hence \"existence\" or Being was called وجود \"wujud\" (Cf. Swedish \"finns\" [found]> there exist; also the Medieval Latin coinage of \"exsistere\" 'standing out (there in the world)' > appear> exist). Now, with regard to the fact that Persian, as the mother tongue of both Avicenna and Sadrā, was in conflict with either Greek or Arabic in this regard, these philosophers should have been warned implicitly by their mother tongue not to confuse two kinds of linguistic beings (viz. copula vs. existential). In fact when analyzed thoroughly, copula, or Persian \"ast\" ('is') indicates an ever-moving chain of relations with no fixed \"entity\" to hold onto (every \"entity\", say A, will be dissolved into \"A is B\" and so on, as soon as one tries to define it). Therefore, the whole reality or what we see as existence (\"found\" in our world) resembles an ever-changing world of \"astī\" (is-ness) flowing in time and space. On the other hand, while Persian \"ast\" can be considered as the 3rd person singular of the verb 'to be', there is no verb but an arbitrary one supporting \"hast\" ('is' as an existential be= exists) has neither future nor past tense and nor a negative form of its own: \"hast\" is just a single untouchable lexeme. It needs no other linguistic element to be complete (\"Hast.\" is a complete sentence meaning \"s/he it exists\"). In fact, any manipulation of the arbitrary verb, e.g. its conjugation, turns \"hast\" back into a copula.\n\nEventually from such linguistic analyses, it appears that while \"astī\" (is-ness) would resemble the world of Heraclitus, \"hastī\" (existence) would rather approaches a metaphysical concept resembling the Parmenidas's interpretation of \"existence\".\n\nIn this regard, Avicenna, who was a firm follower of Aristotle, could not accept either Heraclitian \"is-ness\" (where only constant was \"change\"), nor Parmenidean \"monist immoveable existence\" (the \"hastī\" itself being constant). To solve the contradiction, it so appeared to Philosophers of Islamic world that Aristotle considered the core of existence (i.e. its \"substance\"/\"essence\") as a fixed constant, while its facade (accident) was prone to change. To translate such a philosophical image into Persian it is like having \"hastī\" (existence) as a unique constant core covered by \"astī\" (is-ness) as a cloud of ever-changing relationships. It is clear that the Persian language, deconstructs such a composite as a sheer mirage, since it is not clear how to link the interior core (existence) with the exterior shell (is-ness). Furthermore, \"hast\" cannot be linked to anything but itself (as it is self-referent).\n\nThe argument has a theological echos as well: assuming that God is the \"Existence\", beyond time and space, a question is raised by philosophers of the Islamic world as how he, as a transcendental existence, may ever create or contact a world of \"is-ness\" in space-time.\n\nHowever, Avicenna who was more philosopher than theologian, followed the same line of argumentation as that of his ancient master, Aristotle, and tried to reconcile between \"ast\" and \"hast\", by considering the latter as higher order of existence than the former. It is like a hierarchical order of existence. It was a philosophical Tower of Babel that the restriction of his own mother tongue (Persian) would not allow to be built, but he could maneuver in Arabic by giving the two concepts the same name \"wujud\", although with different attributes. So, implicitly, \"astī\" (is-ness) appears as ممکن الوجود \"momken-al-wujud\" (contingent being), and \"hastī\" (existence) as واجب الوجود \"wājeb-al-wujud\" (necessary being).\n\nOn the other hand, centuries later, Sadrā, chose a more radical route, by inclining towards the reality of \"astī\" (is-ness), as the true mode of existence, and tried to get rid of the concept of \"hastī\" (existence as fixed or immovable). Thus, in his philosophy, the universal movement penetrates deep into the Aristotelian \"substance\"/\"essence\", in unison with changing accident. He called this deep existential change حرکت جوهری \"harekat-e jowhari\" (Substantial Movement). In such a changing existence, the whole world has to go through instantaneous annihilation and recreation incessantly, while as Avicenna had predicted in his remarks on Nature, such a universal change or substantial movement would eventually entail the shortening and lengthening of time as well which has never been observed. This logical objection, which was made on Aristotle's argumentation, could not be answered in the ancient times or medieval age, but now it does not sound contradictory to the real nature of Time (as addressed in relativity theory), so by a reverse argument, a philosopher may indeed deduce that everything is changing (moving) even in the deepest core of Being.\n\nAlthough innovated in the late medieval period, Thomism was dogmatized in the Renaissance. From roughly 1277 to 1567, it dominated the philosophic landscape. The rationalist philosophers, however, with a new emphasis on Reason as a tool of the intellect, brought the classical and medieval traditions under new scrutiny, exercising a new concept of doubt, with varying outcomes. Foremost among the new doubters were the empiricists, the advocates of scientific method, with its emphasis on experimentation and reliance on evidence gathered from sensory experience. In parallel with the revolutions against rising political absolutism based on established religion and the replacement of faith by reasonable faith, new systems of metaphysics were promulgated in the lecture halls by charismatic professors, such as Immanuel Kant, and Hegel. The late 19th and 20th centuries featured an emotional return to the concept of existence under the name of existentialism. These philosophers were concerned mainly with ethics and religion. The metaphysical side became the domain of the phenomenalists. In parallel with these philosophies Thomism continued under the protection of the Catholic Church; in particular, the Jesuit order.\n\nRationalism and empiricism have had many definitions, most concerned with specific schools of philosophy or groups of philosophers in particular countries, such as Germany. In general rationalism is the predominant school of thought in the multi-national, cross-cultural Age of reason, which began in the century straddling 1600 as a conventional date, empiricism is the reliance on sensory data gathered in experimentation by scientists of any country, who, in the Age of Reason were rationalists. An early professed empiricist, Thomas Hobbes, known as an eccentric denizen of the court of Charles II of England (an \"old bear\"), published in 1651 \"Leviathan\", a political treatise written during the English civil war, containing an early manifesto in English of rationalism.\nHobbes said:\"The Latines called Accounts of mony Rationes ... and thence it seems to proceed that they extended the word Ratio, to the faculty of Reckoning in all other things...When a man reasoneth hee does nothing else but conceive a summe totall ... For Reason ... is nothing but Reckoning ... of the consequences of generall names agreed upon, for the marking and signifying of our thoughts ...\"\nIn Hobbes reasoning is the right process of drawing conclusions from definitions (the \"names agreed upon\"). He goes on to define error as self-contradiction of definition (\"an absurdity, or senselesse Speech\") or conclusions that do not follow the definitions on which they are supposed to be based. Science, on the other hand, is the outcome of \"right reasoning,\" which is based on \"natural sense and imagination\", a kind of sensitivity to nature, as \"nature it selfe cannot erre.\"\n\nHaving chosen his ground carefully Hobbes launches an epistemological attack on metaphysics. The academic philosophers had arrived at the Theory of Matter and Form from consideration of certain natural paradoxes subsumed under the general heading of the Unity Problem. For example, a body appears to be one thing and yet it is distributed into many parts. Which is it, one or many? Aristotle had arrived at the real distinction between matter and form, metaphysical components whose interpenetration produces the paradox. The whole unity comes from the substantial form and the distribution into parts from the matter. Inhering in the parts giving them really distinct unities are the accidental forms. The unity of the whole being is actuated by another really distinct principle, the existence.\n\nIf nature cannot err, then there are no paradoxes in it; to Hobbes, the paradox is a form of the absurd, which is inconsistency: \"Natural sense and imagination, are not subject to absurdity\" and \"For error is but a deception ... But when we make a generall assertion, unlesse it be a true one, the possibility of it is inconceivable. And words whereby we conceive nothing but the sound, are those we call Absurd ...\" Among Hobbes examples are \"round quadrangle\", \"immaterial substance\", \"free subject.\" Of the scholastics he says:\"Yet they will have us beleeve, that by the Almighty power of God, one body may be at one and the same time in many places [the problem of the universals]; and many bodies at one and the same time in one place [the whole and the parts]; ... And these are but a small part of the Incongruencies they are forced to, from their disputing philosophically, instead of admiring, and adoring of the Divine and Incomprehensible Nature ...\"\n\nThe real distinction between essence and existence, and that between form and matter, which served for so long as the basis of metaphysics, Hobbes identifies as \"the Error of Separated Essences.\" The words \"Is, or Bee, or Are, and the like\" add no meaning to an argument nor do derived words such as \"Entity, Essence, Essentially, Essentiality\", which \"are the names of nothing\" but are mere \"Signes\" connecting \"one name or attribute to another: as when we say, \"a man is a living body\", we mean not that the \"man\" is one thing, the \"living body\" another, and the \"is\", or \"being\" a third: but that the \"man\", and the \"living body\", is the same thing; ...\" Metaphysiques, Hobbes says, is \"far from the possibility of being understood\" and is \"repugnant to natural reason.\"\n\nBeing to Hobbes (and the other empiricists) is the physical universe:The world, (I mean ... the Universe, that is, the whole masse of all things that are) is corporeall, that is to say, Body; and hath the dimension of magnitude, namely, Length, Bredth and Depth: also every part of Body, is likewise Body ... and consequently every part of the Universe is Body, and that which is not Body, is no part of the Universe: and because the Universe is all, that which is no part of it is nothing; and consequently no where.\"\n\nHobbes' view is representative of his tradition. As Aristotle offered the categories and the act of existence, and Aquinas the analogy of being, the rationalists also had their own system, the great chain of being, an interlocking hierarchy of beings from God to dust.\n\nIn addition to the materialism of the empiricists, under the same aegis of Reason, rationalism produced systems that were diametrically opposed now called idealism, which denied the reality of matter in favor of the reality of mind. By a 20th-century classification, the idealists (Kant, Hegel and others), are considered the beginning of continental philosophy, while the empiricists are the beginning, or the immediate predecessors, of analytical philosophy. \n\nSome philosophers deny that the concept of \"being\" has any meaning at all, since we only define an object's existence by its relation to other objects, and actions it undertakes. The term \"I am\" has no meaning by itself; it must have an action or relation appended to it. This in turn has led to the thought that \"being\" and nothingness are closely related, developed in existential philosophy.\n\nExistentialist philosophers such as Sartre, as well as continental philosophers such as Hegel and Heidegger have also written extensively on the concept of being. Hegel distinguishes between the being of objects (being in itself) and the being of people (\"Geist)\". Hegel, however, did not think there was much hope for delineating a \"meaning\" of being, because being stripped of all predicates is simply nothing.\n\nHeidegger, in his quest to re-pose the original pre-Socratic question of Being, wondered at how to meaningfully ask the question of the meaning of being, since it is both the greatest, as it includes everything that is, and the least, since no particular thing can be said of it. He distinguishes between different modes of beings: a privative mode is present-at-hand, whereas beings in a fuller sense are described as ready-to-hand. The one who asks the question of Being is described as Da-sein (\"there/here-being\") or being-in-the-world. Sartre, popularly understood as misreading Heidegger (an understanding supported by Heidegger's essay \"Letter on Humanism\" which responds to Sartre's famous address, \"Existentialism is a Humanism\"), employs modes of being in an attempt to ground his concept of freedom ontologically by distinguishing between being-in-itself and being-for-itself.\n\nBeing is also understood as one's \"state of being,\" and hence its common meaning is in the context of human (personal) experience, with aspects that involve expressions and manifestations coming from an innate \"being\", or personal character. Heidegger coined the term \"dasein\" for this property of being in his influential work \"Being and Time\" (\"this entity which each of us is himself…we shall denote by the term 'dasein.'\"), in which he argued that being or \"dasein\" links one's sense of one's body to one's perception of world. Heidegger, amongst others, referred to an innate language as the foundation of being, which gives signal to all aspects of being.\n\n\nPhilosophers\n\n"}
{"id": "13428111", "url": "https://en.wikipedia.org/wiki?curid=13428111", "title": "Ben Goertzel", "text": "Ben Goertzel\n\nBen Goertzel (born December 8, 1966) is the founder and CEO of SingularityNET, a blockchain-based AI marketplace. Goertzel is also the chief scientist of financial prediction firm Aidyia Holdings and robotics firm Hanson Robotics; chairman of AI software company Novamente LLC, a privately held software company; chairman of the Artificial General Intelligence Society and the OpenCog Foundation; vice chairman of futurist nonprofit Humanity+; scientific advisor of biopharma firm Genescient Corp.; advisor to Singularity University; research professor in the Fujian Key Lab for Brain-Like Intelligent Systems at Xiamen University of Technology, China; chair of the Artificial General Intelligence (AGI) conference series, and an American author and researcher in the field of artificial intelligence. He was the Director of Research of the Machine Intelligence Research Institute (formerly the Singularity Institute).\n\nGoertzel is the son of Ted Goertzel, a former professor of sociology at Rutgers University. He left high school after the tenth grade to attend Bard College at Simon's Rock, where he graduated with a bachelor's degree in Quantitative Studies. Goertzel went on to obtain a Ph.D. in mathematics from Temple University in 1989. He is the Chief Scientist of Hanson Robotics, the creator of the robot Sophia.\n\nIn May 2007, Goertzel spoke at a Google Tech talk about his approach to creating Artificial General Intelligence. He defines intelligence as the ability to detect patterns in the world and in the agent itself, measurable in terms of emergent behavior of \"achieving complex goals in complex environments.\" A \"baby-like\" artificial intelligence is initialized, then trained as an agent in a simulated or virtual world such as Second Life to produce a more powerful intelligence. Knowledge is represented in a network whose nodes and links carry probabilistic truth values as well as \"attention values,\" with the attention values resembling the weights in a neural network. Several algorithms operate on this network, the central one being a combination of a probabilistic inference engine and a custom version of evolutionary programming. Goertzel claimed that this combination is able to avoid the combinatorial explosions that both these algorithms suffer from when exposed to large problems.\n\n2009: Goertzel and Hugo de Garis starred in a 45-minute documentary called \"Singularity or Bust\". \n\n2012: The documentary \"The Singularity\" by independent filmmaker Doug Wolens showcased Goertzel's vision and understanding of making general AI general thinking\"\n\n2014: Goertzel appeared on the American science documentary television series, \"Through the Wormhole\" (episode 1, season 5).\n\n2016: Goertzel starred in the British-Israeli documentary film, \"Machine of Human Dreams\"\n\n\n"}
{"id": "53691736", "url": "https://en.wikipedia.org/wiki?curid=53691736", "title": "Beth Aala", "text": "Beth Aala\n\nBeth Aala is a three-time award winning American documentary filmmaker and film producer.\n\nAala's parents emigrated from the Philippines to the United States, where she was born. She attended Elk Grove High School in Sacramento. Aala received a bachelor's degree from UC San Diego in both Visual Arts and Communication.\n\nAala is best known for her 2013 film, \",\" which she co-directed and produced with comedian Mike Myers.\n\nShe was the codirector, with Susan Froemke and John Hoffman, of the film \"Rancher, Farmer, Fisherman,\" which premiered at the Sundance film festival in 2017.\n\nIn 2005, Aala won an Emmy Award for co-producing \"I Have Tourette's but Tourette's Doesn't Have Me.\" She also received Emmys for producing in the \"Outstanding Children's Program\" category in 2008 and 2011.\n\nIn 2006, she won a Peabody Award for co-producing the film \"The Music in Me.\"\n"}
{"id": "53296445", "url": "https://en.wikipedia.org/wiki?curid=53296445", "title": "Biomedical research in the United States", "text": "Biomedical research in the United States\n\nThe US carries out 46% of global research and development (R&D) in life sciences, making it the world leader.\n\nLife sciences accounted for 51% of federal research expenditure in 2011. \n\nThe National Institutes of Health (NIH) are considered the government's flagship biomedical research funding organization. Between 2004 and 2014, NIH funding remained relatively flat and was not increased to keep pace with inflation. The NIH budget peaked at circa $35 billion per year from 2003 to 2005 and was around $30 billion in 2015.\n\nGovernment efforts to increase allocations to research between 2013 and 2016 were often thwarted by the congressional austerity drive, with Congress withholding approval of the federal government's budget several times. Over this period, the executive’s priorities were taken forward largely thanks to collaboration between the government, industry and the non-profit sector. This was particularly true for the health sector which, like climate change, was a priority for the Obama administration.\n\nA key policy objective of the Obama administration was to develop more targeted therapies while reducing the time and cost of drug development. Developing a new drug takes well over a decade and has a failure rate of more than 95%. The most expensive failures happen in late phase clinical trials. It is thus vital to pinpoint the right biological targets (genes, proteins and other molecules) early in the process, so as to design more rational drugs and better tailored therapies.\n\nThe 21st Century Cures Act was signed into law on 13 December 2016, a year after the release of the UNESCO Science Report. The report had predicted that, ‘were the bill to pass into law, it would alter the way in which clinical trials are conducted by allowing new and adaptive trial designs that factor in personalized parameters, such as biomarkers and genetics. This provision has proven controversial, with doctors cautioning that overreliance on biomarkers as a measure of efficacy can be misleading, as they may not always reflect improved patient outcomes'.\n\nAnother government scheme hopes to increase the number of new diagnostics and therapies for patients, while reducing the time and cost of developing these. At the launch of the Accelerating Medicines Partnership in February 2014, NIH director Francis S. Collins stated that 'Currently, we are investing too much money and time in avenues that don't pan out, while patients and their families wait'. Over the five years to 2019, this public−private partnership is developing up to five pilot projects for three common but difficult-to-treat diseases: Alzheimer’s disease, type 2 (adult onset) diabetes and the autoimmune disorders of rheumatoid arthritis and lupus.\n\nThe partnership involves the National Institutes of Health (NIH) and the Food and Drug Administration (FDA), as well as 10 major biopharmaceutical companies and several non-profit organizations like the Alzheimer's Association. The industrial partners are Abbvie (US), Biogen (US), Bristol-Myers Squibb (US), GlaxoSmithKline (UK), Johnson & Johnson (US), Lilly (US), Merck (US), Pfizer (US), Sanofi (France) and Takeda (Japan).\n\nLaboratories share samples, such as blood or brain tissue from deceased patients, to identify biomarkers. They also participate in NIH clinical trials. One critical component is that industry partners have agreed to make the data and analyses arising from the partnership accessible to the broad biomedical community. They will not use any discoveries to develop their own drug until these findings have been made public.\n\nIn April 2013, the government announced another public−private partnership, this time to implement its Brain Research through Advancing Innovative Neurotechnologies (BRAIN) Initiative. The goal of this project is to leverage genetic, optical and imaging technologies to map individual neurons and complex circuits in the brain, eventually leading to a more complete understanding of this organ’s structure and function. By 2015, the BRAIN Initiative had ‘obtained commitments of over US$ 300 million in resources from federal agencies (National Institutes of Health, Food and Drug Administration, National Science Foundation, etc.), industry (National Photonics Initiative, General Electric, Google, GlaxoSmithKline, etc.) and philanthropy (foundations and universities)’.\n\nThe Precision Medicine Initiative has been another government priority. Defined as delivering the right treatment to the right patient at the right time, precision medicine tailors treatments to patients based on their unique physiology, biochemistry and genetics. In his 2016 budget request, the president asked for US$215 million to be shared by the NIH, National Cancer Institute and FDA to fund the Precision Medicine Initiative.\n\nIn 2013, US pharmaceutical companies spent $40 billion on R&D inside the US and nearly another $11 billion on R&D abroad.\n\nBetween 2005 and 2010, pharmaceutical and biopharmaceutical companies increased their investment in precision medicine by roughly 75% and a further increase of 53% is projected by 2015. Between 12% and 50% of the products in their drug development pipelines are related to personalized medicine. \n\nThe federal government and most of the 50 states that make up the United States offer R&D tax credits to particular industries and companies. Congress usually renews a tax credit every few years. According to a survey by the \"Wall Street Journal\" in 2012, companies do not factor in these credits when making décisions about investing in R&D, since they cannot rely on these credits being renewed.\n\nIn 2014, six US biopharmaceutical companies figured in the global Top 50 for the volume of expenditure on R&D. The following have figured in the Top 20 for at least ten years: Intel, Microsoft, Johnson & Johnson, Pfizer and IBM. Google was included for the first time in 2013 and Amazon in 2014, which is why the online store does not figure in the Top 50 for 2014.\n\nGlobal top 50 companies by R&D volume and intensity, 2014\n\n<nowiki>*</nowiki> R&D intensity is defined as R&D expenditure divided by net sales.\n\n<nowiki>**</nowiki> Although incorporated in the Netherlands, Airbus’s principal manufacturing facilities are located in France, Germany, Spain and the UK.\n\nSource: \"UNESCO Science Report: towards 2030\" (2015), Table 9.3, based on Hernández \"et. al\" (2014) \"EU R&D Scoreboard: the 2014 EU Industrial R&D Investment Scoreboard\". European Commission: Brussels, Table 2.2.\n\nThe National Venture Capital Association has reported that, in 2014, venture capital investment in the life sciences was at its highest level since 2008: in biotechnology, $6.0 billion was invested in 470 deals and, in life sciences overall, $8.6 billion in 789 deals (including biotechnology and medical devices). Two-thirds (68%) of the investment in biotechnology went to first-time/early-stage development deals and the remainder to the expansion stage of development (14%), seed-stage companies (11%) and late-stage companies (7%).\n\nHowever, it was the software industry which invested in the greatest number of deals overall: 1 799, for an investment of $19.8 billion. Second came internet-specific companies, garnering US$11.9 billion in investment through 1 005 deals. Many of these companies are based in the State of California, which alone concentrates 28% of US research.\n\nTotal investment in venture capital amounted to US$48.3 billion in 2014, for 4 356 deals. This represented ‘an increase of 61% in dollars and a 4% increase in deals over the prior year,’ reported the National Venture Capital Association.\n\nThe Organisation for Economic Cooperation and Development estimates that venture capital investment in the United States had fully recovered by 2014.\n\nIn recent years, a number of pharmaceutical companies have made strategic mergers to relocate their headquarters overseas to gain a tax advantage, including Medtronic and Endo International. Pfizer's own attempt to take over the British pharmaceutical company AstraZeneca aborted in 2014 after Pfizer admitted plans to cut research spending in the combined company.\n\nOne policy concern for the Obama administration has been the steep rise in the price of prescription drugs, in a country where these prices are largely unregulated. From January 2008 to December 2014, the price of commonly used branded drugs increased by a little over 127%, even as the price of commonly prescribed generic drugs decreased by almost 63%.\n\nIn 2014, spending on prescription drugs hit $374 billion. This increase in spending was fuelled by the costly new drugs on the market for treating hepatitis C ($11 billion), rather than by the millions of newly insured Americans under the Patient Protection and Affordable Care Act of 2010 ($1 billion). About 31% of this spending went on specialty drug therapies to treat inflammatory conditions, multiple sclerosis, oncology, hepatitis C and HIV, etc., and 6.4% on traditional therapies to treat diabetes, high cholesterol, pain, high blood pressure and heart disease, asthma, depression and so on’.\n\nFuelling the 'astronomic' rise in consumer prices for prescription drugs has been a new trend in the US, the acquisition of pharmaceuticals through licensing, purchase, a merger or acquisition. In the first half of 2014, the value of mergers and acquisitions by pharmaceutical companies totalled US$317.4 billion and, in the first quarter of 2015, the drug industry accounted for a little more than 45% of all US mergers and acquisitions. Several pharmaceutical companies have made strategic mergers in recent years to relocate their headquarters overseas to order to gain a tax advantage. Pfizer’s own attempt to take over the British pharmaceutical company Astrazeneca aborted in 2014, after Pfizer admitted plans to cut research spending in the combined company.\n\nThe Biologics Price Competition and Innovation Act was signed into law in March 2010 to encourage the development of generic drug competition as a cost containment measure for high-priced pharmaceuticals. Part of the government’s signatory Patient Protection and Affordable Care Act, it has created a pathway for fast-track licensure for biological products that are shown to be ‘biosimilar’ to, or ‘interchangeable’ with, an approved biological product. One inspiration for the act was that the patents for many biologic drugs will expire in the next decade.\n\nAlthough the act was passed in 2010, the first biosimilar was only approved in the US by the FDA in 2015: Zarxio, made by Sandoz. Zarxio is a biosimilar of the cancer drug Neupogen, which boosts the patient’s white blood cells to ward off infection. In September 2015, a US court ruled that the Neupogen brand manufacturer Amgen could not block Zarxio from being sold in the US. Neupogen costs about US$3 000 per chemotherapy cycle; Zarxio hit the US market on 3 September 2015 at a 15% discount.\n\nIn Europe, the same drug had been approved as early as 2008 and has been safely marketed there ever since. The lag in development of an approval pathway in the US has been criticized for impeding access to biological therapies.\n\nThe true cost savings from the use of biosimilars is difficult to assess. A 2014 study by the Rand Institute estimates a range of US$13–66 billion in savings over 2014–2024, depending upon the level of competition and FDA regulatory approval patterns.\n\nUnlike generics, biosimilars cannot be approved on the basis of minimal and inexpensive tests to prove bioequivalence. Since biological drugs are complex, heterogeneous products derived from living cells, they can only be shown to be highly similar to the appropriate reference product and therefore require demonstration that there are no clinically meaningful differences in safety and efficacy. The extent to which clinical trials are required will largely determine the cost of development.\n\nOrphan diseases affect fewer than 200 000 Americans each year. Since the Orphan Drug Act of 1983, over 400 drugs and biologic products for rare diseases have been designated by the Food and Drug Administration (as of 2015), 260 alone in 2013. In 2014, sales of the top 10 orphan drugs in the US amounted to US$18.32 billion; by 2020, orphan drugs sales worldwide are projected to account for 19% (US$ 28.16 billion) of the total US$ 176 billion in prescription drug spending.\n\nHowever, orphan drugs cost about 19.1 times more than non-orphan drugs (on an annual basis) in 2014, at an average annual cost per patient of US$ 137 782. Some are concerned that the incentives given to pharmaceutical companies to develop orphan drugs by the FDA’s orphan drug products programme is taking the companies’ attention away from developing drugs that will benefit more of the population.\n\nThere are more than 6500 medical device companies in the US, more than 80% of which have fewer than 50 employees. According to the US Department of Commerce, the market size of the medical device industry in the US is expected to reach US$ 133 billion by 2016. \n\nObservers foresee the further development and emergence of wearable health monitoring devices, telediagnosis and telemonitoring, robotics, biosensors, three-dimensional (3D) printing, new \"in vitro\" diagnostic tests and mobile apps that enable users to monitor their health and related behaviour better.\n\nUntil 2010, the United States of America was a net exporter of pharmaceuticals. Since 2011, it has become a net importer of these goods. The United States has lost its world leadership for high-tech goods. Even computing and communications equipment is now assembled in China and other emerging economies, with high-tech value-added components being produced elsewhere.\n\nThe United States is a post-industrial country. Imports of high-tech products far exceed exports. However, the United States' technologically skilled workforce produces a large volume of patents and can still profit from the license or sale of these patents. Within the United States' scientific industries active in research, 9.1% of products and services are concerned with the licensing of intellectual property rights.\n\nWhen it comes to trade in intellectual property, the United States remains unrivalled. Income from royalties and licensing amounted to $129.2 billion in 2013, the highest in the world. Japan comes a distant second, with receipts of $31.6 billion in 2013. The United States' payments for use of intellectual property amounted to $39.0 billion in 2013, exceeded only by Ireland ($46.4 billion). \n\nHealth care in the United States\n\nScience policy in the United States\n\nScience and technology in the United States\n"}
{"id": "47744979", "url": "https://en.wikipedia.org/wiki?curid=47744979", "title": "Birgit Aagard-Svendsen", "text": "Birgit Aagard-Svendsen\n\nBirgit Aagard-Svendsen (born 1956) is a Danish business executive who as of September 2015, became the executive vice-president and chief financial officer of the J. Lauritzen shipping company in Copenhagen, Denmark.\n\nAagard-Svendsen graduated in engineering from Danmarks Ingeniørakademi (1980) and in business administration at Copenhagen's Handelshøjskolen (1985). Before joining J. Lauritzen in 1998, she held management positions at Tele Danmark (1996–98) and Nordisk Film (1996–98). She has previously been chairman of the Infrastructure Commission and also headed the Committee on Corporate Governance.\n\nAagard-Svendsen is married to , a former mayor of Lyngby-Taarbæk, and has two daughters. In 2008, she was named businesswoman of the year by the Danish newspaper \"Berlingske\".\n"}
{"id": "6225471", "url": "https://en.wikipedia.org/wiki?curid=6225471", "title": "Brynjar Aa", "text": "Brynjar Aa\n\nBrynjar Aa often Brynjar Å (born July 21, 1960) is a Norwegian dramatist. He was a student at the Telemark University College and studied under Norwegian writer Eldrid Lunden. Before finishing at Bø, Aa published the book \"Babyboy\".\n\nBrynjar Aa has published over a dozen different works, all by established publishing houses, including Aschehoug, and The Norwegian Broadcasting Corporation. His work has also been reviewed in Dag og Tid and Dagbladet, and some of them have been controversial.\n\n"}
{"id": "6760", "url": "https://en.wikipedia.org/wiki?curid=6760", "title": "Cryonics", "text": "Cryonics\n\nCryonics (from Greek κρύος \"kryos\" meaning 'cold') is the low-temperature preservation (usually at −196 °C) of a human corpse, with the hope that resuscitation and restoration to life and full health may be possible in the future. Cryopreservation of humans is not reversible with present technology; cryonicists hope that medical advances will someday allow cryopreserved bodies to be revived.\n\nCryonics is regarded with skepticism within the mainstream scientific community and is not part of normal medical practice. It is not known if it will ever be possible to revive a cryopreserved human cadaver. Cryonics depends on beliefs that the frozen body has not experienced information-theoretic death. Such views are at the speculative edge of science.\n\nCryonics procedures can only begin after clinical death, and cryonics \"patients\" are legally dead. Cryonics procedures ideally begin within minutes of death, and use cryoprotectants to prevent ice formation during cryopreservation. The first corpse to be cryopreserved was that of Dr. James Bedford in 1967. As of 2014, about 250 bodies were cryopreserved in the United States, and 1,500 people had made arrangements for cryopreservation after their legal death.\n\nCryonic proponents go further than the mainstream consensus in asserting that the brain does not have to be continuously active to survive or retain memory. Cryonics controversially asserts that a human survives even within an inactive brain that has been badly damaged, provided that original encoding of memory and personality can, in theory, be adequately inferred and reconstituted from structure that remains. Cryonicists argue that as long as brain structure remains intact, there is no fundamental barrier, given our current understanding of physical law, to recovering its information content. Cryonicists argue that true \"death\" should be defined as irreversible loss of brain information critical to personal identity, rather than inability to resuscitate using current technology. The cryonics argument that death does not occur as long as brain structure remains intact and theoretically repairable has received some mainstream medical discussion in the context of the ethical concept of brain death and organ donation.\n\nCryonics uses temperatures below −130 °C, called cryopreservation, in an attempt to preserve enough brain information to permit future revival of the cryopreserved person. Cryopreservation may be accomplished by freezing, freezing with cryoprotectant to reduce ice damage, or by vitrification to avoid ice damage. Even using the best methods, cryopreservation of whole bodies or brains is very damaging and irreversible with current technology.\n\nCryonics requires future technology to repair or regenerate tissue that is diseased, damaged, or missing. Brain repairs in particular will require analysis at the molecular level. This far-future technology is usually assumed to be nanomedicine based on molecular nanotechnology. Biological repair methods or mind uploading have also been proposed.\n\nCosts can include payment for medical personnel to be on call for death, vitrification, transportation in dry ice to a preservation facility, and payment into a trust fund intended to cover indefinite storage in liquid nitrogen and future revival costs. As of 2011, U.S. cryopreservation costs can range from $28,000 to $200,000, and are often financed via life insurance. KrioRus, which stores bodies communally in large dewars, charges $12,000 to $36,000 for the procedure. Some patients opt to have only their brain cryopreserved, rather than their whole body.\n\nAs of 2014, about 250 people have been cryogenically preserved in the U.S., and around 1,500 more have signed up to be preserved. As of 2016, four facilities exist in the world to retain cryopreserved bodies: three in the U.S. and one in Russia.\n\nLong-term preservation of biological tissue can be achieved by cooling to temperatures below −130 °C. Immersion in liquid nitrogen at a temperature of −196 °C (77 kelvins and −320.8 °F) is often used for convenience. Low temperature preservation of tissue is called cryopreservation. Contrary to popular belief, water that freezes during cryopreservation is usually water outside cells, not water inside cells. Cells don't burst during freezing, but instead become dehydrated and compressed between ice crystals that surround them. Intracellular ice formation only occurs if the rate of freezing is faster than the rate of osmotic loss of water to the extracellular space.\n\nWithout cryoprotectants, cell shrinkage and high salt concentrations during freezing usually prevent frozen cells from functioning again after thawing. In tissues and organs, ice crystals can also disrupt connections between cells that are necessary for organs to function. The difficulties of recovering large animals and their individual organs from a frozen state have been long known. Attempts to recover frozen mammals by simply rewarming them were abandoned by 1957. At present, only cells, tissues, and some small organs can be reversibly cryopreserved.\n\nWhen used at high concentrations, cryoprotectants can stop ice formation completely. Cooling and solidification without crystal formation is called vitrification. The first cryoprotectant solutions able to vitrify at very slow cooling rates while still being compatible with whole organ survival were developed in the late 1990s by cryobiologists Gregory Fahy and Brian Wowk for the purpose of banking transplantable organs. This has allowed animal brains to be vitrified, warmed back up, and examined for ice damage using light and electron microscopy. No ice crystal damage was found; remaining cellular damage was due to dehydration and toxicity of the cryoprotectant solutions. Large vitrified organs tend to develop fractures during cooling, a problem worsened by the large tissue masses and very low temperatures of cryonics.\n\nThe use of vitrification rather than freezing for cryonics was anticipated in 1986, when K. Eric Drexler proposed a technique called \"fixation and vitrification\", anticipating reversal by molecular nanotechnology. In 2016, Robert L. McIntyre and Gregory Fahy at the cryobiology research company 21st Century Medicine, Inc. won the Small Animal Brain Preservation Prize of the Brain Preservation Foundation by demonstrating to the satisfaction of neuroscientist judges that a particular implementation of fixation and vitrification called \"aldehyde-stabilized cryopreservation\" could preserve a rabbit brain in \"near perfect\" condition at −135 °C, with the cell membranes, synapses, and intracellular structures intact in electron micrographs. Brain Preservation Foundation President, Ken Hayworth, said, \"This result directly answers a main skeptical and scientific criticism against cryonics—that it does not provably preserve the delicate synaptic circuitry of the brain.” However the price paid for perfect preservation as seen by microscopy was tying up all protein molecules with chemical crosslinks, completely eliminating biological viability. Actual cryonics organizations use vitrification without a chemical fixation step, sacrificing some structural preservation quality for less damage at the molecular level. Some scientists, like Joao Pedro Magalhaes, have questioned whether using a deadly chemical for fixation eliminates the possibility of biological revival, making chemical fixation unsuitable for cryonics.\n\nWhile preservation of both structure and function has been possible for brain slices using vitrification, this goal remains elusive for whole brains. In absence of a revived brain, or brain simulation from somehow scanning a preserved brain, the adequacy of present vitrification technology (with or without fixation) for preserving the anatomical and molecular basis of long-term memory as required by cryonics is still unproven.\n\nOutside the cryonics community, many scientists have a blanket skepticism toward existing preservation methods. Cryobiologist Dayong Gao states that \"we simply don't know if (subjects have) been damaged to the point where they've 'died' during vitrification because the subjects are now inside liquid nitrogen canisters.\" Biochemist Ken Storey argues (based on experience with organ transplants), that \"even if you only wanted to preserve the brain, it has dozens of different areas, which would need to be cryopreserved using different protocols.\"\n\nThose who believe that revival may someday be possible generally look toward advanced bioengineering, molecular nanotechnology, or nanomedicine as key technologies. Revival would require repairing damage from lack of oxygen, cryoprotectant toxicity, thermal stress (fracturing), freezing in tissues that do not successfully vitrify, and reversing the cause of death. In many cases extensive tissue regeneration would be necessary.\n\nAccording to Cryonics Institute president Ben Best, cryonics revival may be similar to a last in, first out process. People cryopreserved in the future, with better technology, may require less advanced technology to be revived because they will have been cryopreserved with better technology that caused less damage to tissue. In this view, preservation methods would get progressively better until eventually they are demonstrably reversible, after which medicine would begin to reach back and revive people cryopreserved by more primitive methods.\n\nHistorically, a person had little control regarding how their body was treated after death as religion had jurisdiction over the disposal of the body. However, secular courts began to exercise jurisdiction over the body and use discretion in carrying out of the wishes of the deceased person. Most countries legally treat preserved individuals as deceased persons because of laws that forbid vitrifying someone who is medically alive. In France, cryonics is not considered a legal mode of body disposal; only burial, cremation, and formal donation to science are allowed. However, bodies may legally be shipped to other countries for cryonic freezing. As of 2015, the Canadian province of British Columbia prohibits the sale of arrangements for body preservation based on cryonics. In Russia, cryonics falls outside both the medical industry and the funeral services industry, making it easier in Russia than in the U.S. to get hospitals and morgues to release cryonics candidates.\n\nIn London in 2016, the English High Court ruled in favor of a mother's right to seek cryopreservation of her terminally ill 14-year-old daughter, as the girl wanted, contrary to the father's wishes. The decision was made on the basis that the case represented a conventional dispute over the disposal of the girl's body, although the judge urged ministers to seek \"proper regulation\" for the future of cryonic preservation following concerns raised by the hospital about the competence and professionalism of the team that conducted the preservation procedures. In Alcor Life Extension Foundation v. Richardson, the Iowa Court of Appeals ordered for the disinterment of Richardson, who was buried against his wishes for cryopreservation.\n\nWriting in \"Bioethics\", David Shaw examines the ethical status of cryonics. The arguments against it include changing the concept of death, the expense of preservation and revival, lack of scientific advancement to permit revival, temptation to use premature euthanasia, and failure due to catastrophe. Arguments in favor of cryonics include the potential benefit to society, the prospect of immortality, and the benefits associated with avoiding death. Shaw explores the expense and the potential payoff, and applies an adapted version of Pascal's Wager to the question.\n\nCryopreservation was applied to human cells beginning in 1954 with frozen sperm, which was thawed and used to inseminate three women. Eight years later, the freezing of humans was first scientifically proposed by Michigan professor Robert Ettinger. In April 1966, the first human body was frozen with some thought of future reanimation, though it had been embalmed for two months. The body was placed in liquid nitrogen, and stored at just above freezing. The middle-aged woman from Los Angeles, whose name is unknown, was soon thawed out and buried by relatives.\n\nThe first body to be truly cryopreserved was James Bedford's a few hours after his cancer-caused death in 1967. He is the only cryonics patient frozen before 1974 still preserved today. Cryonics gained a poor reputation in the U.S. in the late 1970s after the Cryonics Society of California ran out of money to maintain cryopreservation of existing patients. Robert Nelson, a former TV repairman with no scientific background (who had processed Bedford's freezing before turning the body over to relatives), was sued for allowing nine bodies to decompose.\n\nIn 2018, a Y-Combinator startup called Nectome was recognized for developing a method of preserving brains with chemicals rather than by freezing. The method is fatal, performed as euthanasia under general anethesia, but the hope is that future technology would allow the brain to be physically scanned into a computer simulation, neuron by neuron.\n\nAccording to \"The New York Times\", cryonicists are predominantly nonreligious white males, outnumbering women by about three to one. According to \"The Guardian\", as of 2008, while most cryonicists used to be young, male and \"geeky\" recent demographics have shifted slightly towards whole families.\n\nIn 2015 Du Hong, a 61-year-old female writer of children's literature, became the first known Chinese national to be cryopreserved.\n\nSome scientists have expressed skepticism about cryonics in media sources, however the number of peer-reviewed papers on cryonics is limited because its speculative aspects place it outside of the focus of most academic fields. While most neuroscientists agree that all the subtleties of a human mind are contained in its anatomical structure, few neuroscientists will comment directly upon the topic of cryonics due to its speculative nature. Individuals who intend to be frozen are often \"looked at as a bunch of kooks\", despite many of them being scientists and doctors.\n\nAccording to cryonicist Aschwin de Wolf and others, cryonics can often produce intense hostility from spouses who are not cryonicists. James Hughes, the executive director of the pro-life-extension Institute for Ethics and Emerging Technologies, chooses not to personally sign up for cryonics, calling it a worthy experiment but stating laconically that \"I value my relationship with my wife.\"\n\nCryobiologist Dayong Gao states that \"People can always have hope that things will change in the future, but there is no scientific foundation supporting cryonics at this time.\" Alcor disagrees, stating that \"There are no known credible technical arguments that lead one to conclude that cryonics, carried out under good conditions today, would not work.\" As well, while it is universally agreed that \"personal identity\" is uninterrupted when brain activity temporarily ceases during incidents of accidental drowning (where people have been restored to normal functioning after being completely submerged in cold water for up to 66 minutes), some people express concern that a centuries-long cryopreservation might interrupt their conception of personal identity, such that the revived person would \"not be you\".\n\nMany people assert there would be no point in being revived in the far future if their friends and families are dead.\n\nSuspended animation is a popular subject in science fiction and fantasy settings, appearing in comic books, films, literature, and television. A survey in Germany found that about half of the respondents were familiar with cryonics, and about half of those familiar with cryonics had learned of the subject from films or television. Some commonly known examples of cryonics being used in popular culture include \"Demolition Man (film)\", \"Vanilla Sky\", \"Fallout 4\", \"Futurama\", \"Passengers\" and \"Nip/Tuck\".\nAmong the cryopreserved are L. Stephen Coles (in 2014), Hal Finney (in 2014), and Ted Williams.\n\nThe urban legend suggesting Walt Disney was cryopreserved is false; he was cremated and interred at Forest Lawn Memorial Park Cemetery. Robert A. Heinlein, who wrote enthusiastically of the concept in \"The Door into Summer\" (serialized in 1956), was cremated and had his ashes distributed over the Pacific Ocean. Timothy Leary was a long-time cryonics advocate and signed up with a major cryonics provider, but he changed his mind shortly before his death, and was not cryopreserved.\n"}
{"id": "8221", "url": "https://en.wikipedia.org/wiki?curid=8221", "title": "Death", "text": "Death\n\nDeath is the cessation of all biological functions that sustain a living organism. Phenomena which commonly bring about death include aging, predation, malnutrition, disease, suicide, homicide, starvation, dehydration, and accidents or major trauma resulting in terminal injury. In most cases, bodies of living organisms begin to decompose shortly after death.\n\nDeath – particularly the death of humans – has commonly been considered a sad or unpleasant occasion, due to the affection for the being that has died and the termination of social and familial bonds with the deceased. Other concerns include fear of death, necrophobia, anxiety, sorrow, grief, emotional pain, depression, sympathy, compassion, solitude, or saudade. Many cultures and religions have the idea of an afterlife, and also hold the idea of reward or judgement and punishment for past sin.\n\nThe word death comes from Old English \"dēaþ\", which in turn comes from Proto-Germanic *\"dauþuz\" (reconstructed by etymological analysis). This comes from the Proto-Indo-European stem *\"dheu-\" meaning the \"process, act, condition of dying\".\n\nThe concept and symptoms of death, and varying degrees of delicacy used in discussion in public forums, have generated numerous scientific, legal, and socially acceptable terms or euphemisms for death. When a person has died, it is also said they have \"passed away\", \"passed on\", \"expired\", or are \"gone\", among numerous other socially accepted, religiously specific, slang, and irreverent terms. Bereft of life, the dead person is then a \"corpse\", \"cadaver\", a \"body\", a \"set of remains\", and when all flesh has rotted away, a skeleton. The terms \"carrion\" and \"carcass\" can also be used, though these more often connote the remains of non-human animals. As a polite reference to a dead person, it has become common practice to use the participle form of \"decease\", as in \"the deceased\"; another noun form is \"decedent\". The ashes left after a cremation are sometimes referred to by the neologism \"cremains\", a portmanteau of \"cremation\" and \"remains\".\n\nSenescence refers to a scenario when a living being is able to survive all calamities, but eventually dies due to causes relating to old age. Animal and plant cells normally reproduce and function during the whole period of natural existence, but the aging process derives from deterioration of cellular activity and ruination of regular functioning. Aptitude of cells for gradual deterioration and mortality means that cells are naturally sentenced to stable and long-term loss of living capacities, even despite continuing metabolic reactions and viability. In the United Kingdom, for example, nine out of ten of all the deaths that occur on a daily basis relates to senescence, while around the world it accounts for two-thirds of 150,000 deaths that take place daily (Hayflick & Moody, 2003).\n\nAlmost all animals who survive external hazards to their biological functioning eventually die from biological aging, known in life sciences as \"senescence\". Some organisms experience negligible senescence, even exhibiting biological immortality. These include the jellyfish \"Turritopsis dohrnii\", the hydra, and the planarian. Unnatural causes of death include suicide and homicide. From all causes, roughly 150,000 people die around the world each day.<ref name=\"doi10.2202/1941-6008.1011\"></ref> Of these, two thirds die directly or indirectly due to senescence, but in industrialized countries – such as the United States, the United Kingdom, and Germany – the rate approaches 90%, i.e., nearly nine out of ten of all deaths are related to senescence.\n\nPhysiological death is now seen as a process, more than an event: conditions once considered indicative of death are now reversible. Where in the process a dividing line is drawn between life and death depends on factors beyond the presence or absence of vital signs. In general, clinical death is neither necessary nor sufficient for a determination of legal death. A patient with working heart and lungs determined to be brain dead can be pronounced legally dead without clinical death occurring. As scientific knowledge and medicine advance, formulating a precise medical definition of death becomes more difficult.\n\nSigns of death or strong indications that a warm-blooded animal is no longer alive are:\n\nThe concept of death is a key to human understanding of the phenomenon. There are many scientific approaches to the concept. For example, brain death, as practiced in medical science, defines death as a point in time at which brain activity ceases.\n\nOne of the challenges in defining death is in distinguishing it from life. As a point in time, death would seem to refer to the moment at which life ends. Determining when death has occurred is difficult, as cessation of life functions is often not simultaneous across organ systems. Such determination therefore requires drawing precise conceptual boundaries between life and death. This is difficult, due to there being little consensus on how to define life. This general problem applies to the particular challenge of defining death in the context of medicine.\n\nIt is possible to define life in terms of consciousness. When consciousness ceases, a living organism can be said to have died. One of the flaws in this approach is that there are many organisms which are alive but probably not conscious (for example, single-celled organisms). Another problem is in defining consciousness, which has many different definitions given by modern scientists, psychologists and philosophers. Additionally, many religious traditions, including Abrahamic and Dharmic traditions, hold that death does not (or may not) entail the end of consciousness. In certain cultures, death is more of a process than a single event. It implies a slow shift from one spiritual state to another.\n\nOther definitions for death focus on the character of cessation of something. In this context \"death\" describes merely the state where something has ceased, for example, life. Thus, the definition of \"life\" simultaneously defines death.\n\nHistorically, attempts to define the exact moment of a human's death have been subjective, or imprecise. Death was once defined as the cessation of heartbeat (cardiac arrest) and of breathing, but the development of CPR and prompt defibrillation have rendered that definition inadequate because breathing and heartbeat can sometimes be restarted. Events which were causally linked to death in the past no longer kill in all circumstances; without a functioning heart or lungs, life can sometimes be sustained with a combination of life support devices, organ transplants and artificial pacemakers.\n\nToday, where a definition of the moment of death is required, doctors and coroners usually turn to \"brain death\" or \"biological death\" to define a person as being dead; people are considered dead when the electrical activity in their brain ceases. It is presumed that an end of electrical activity indicates the end of consciousness. Suspension of consciousness must be permanent, and not transient, as occurs during certain sleep stages, and especially a coma. In the case of sleep, EEGs can easily tell the difference.\n\nThe category of \"brain death\" is seen as problematic by some scholars. For instance, Dr. Franklin Miller, senior faculty member at the Department of Bioethics, National Institutes of Health, notes: \"By the late 1990s... the equation of brain death with death of the human being was increasingly challenged by scholars, based on evidence regarding the array of biological functioning displayed by patients correctly diagnosed as having this condition who were maintained on mechanical ventilation for substantial periods of time. These patients maintained the ability to sustain circulation and respiration, control temperature, excrete wastes, heal wounds, fight infections and, most dramatically, to gestate fetuses (in the case of pregnant \"brain-dead\" women).\"\n\nThose people maintaining that only the neo-cortex of the brain is necessary for consciousness sometimes argue that only electrical activity should be considered when defining death. Eventually it is possible that the criterion for death will be the permanent and irreversible loss of cognitive function, as evidenced by the death of the cerebral cortex. All hope of recovering human thought and personality is then gone given current and foreseeable medical technology. At present, in most places the more conservative definition of death – irreversible cessation of electrical activity in the whole brain, as opposed to just in the neo-cortex – has been adopted (for example the Uniform Determination Of Death Act in the United States). In 2005, the Terri Schiavo case brought the question of brain death and artificial sustenance to the front of American politics.\n\nEven by whole-brain criteria, the determination of brain death can be complicated. EEGs can detect spurious electrical impulses, while certain drugs, hypoglycemia, hypoxia, or hypothermia can suppress or even stop brain activity on a temporary basis. Because of this, hospitals have protocols for determining brain death involving EEGs at widely separated intervals under defined conditions.\n\nThe death of a person has legal consequences that may vary between different jurisdictions.\nA death certificate is issued in most jurisdictions, either by a doctor, or by an administrative office upon presentation of a doctor's declaration of death.\n\nThere are many anecdotal references to people being declared dead by physicians and then \"coming back to life\", sometimes days later in their own coffin, or when embalming procedures are about to begin. From the mid-18th century onwards, there was an upsurge in the public's fear of being mistakenly buried alive, and much debate about the uncertainty of the signs of death. Various suggestions were made to test for signs of life before burial, ranging from pouring vinegar and pepper into the corpse's mouth to applying red hot pokers to the feet or into the rectum. Writing in 1895, the physician J.C. Ouseley claimed that as many as 2,700 people were buried prematurely each year in England and Wales, although others estimated the figure to be closer to 800.\n\nIn cases of electric shock, cardiopulmonary resuscitation (CPR) for an hour or longer can allow stunned nerves to recover, allowing an apparently dead person to survive. People found unconscious under icy water may survive if their faces are kept continuously cold until they arrive at an emergency room. In science fiction scenarios where such technology is readily available, real death is distinguished from reversible death.\n\nThe leading cause of human death in developing countries is infectious disease. The leading causes in developed countries are atherosclerosis (heart disease and stroke), cancer, and other diseases related to obesity and aging. By an extremely wide margin, the largest unifying cause of death in the developed world is biological aging, leading to various complications known as aging-associated diseases. These conditions cause loss of homeostasis, leading to cardiac arrest, causing loss of oxygen and nutrient supply, causing irreversible deterioration of the brain and other tissues. Of the roughly 150,000 people who die each day across the globe, about two thirds die of age-related causes. In industrialized nations, the proportion is much higher, approaching 90%. With improved medical capability, dying has become a condition to be managed. Home deaths, once commonplace, are now rare in the developed world.\nIn developing nations, inferior sanitary conditions and lack of access to modern medical technology makes death from infectious diseases more common than in developed countries. One such disease is tuberculosis, a bacterial disease which killed 1.8M people in 2015. Malaria causes about 400–900M cases of fever and 1–3M deaths annually. AIDS death toll in Africa may reach 90–100M by 2025.\n\nAccording to Jean Ziegler (United Nations Special Reporter on the Right to Food, 2000 – Mar 2008), mortality due to malnutrition accounted for 58% of the total mortality rate in 2006. Ziegler says worldwide approximately 62M people died from all causes and of those deaths more than 36M died of hunger or diseases due to deficiencies in micronutrients.\n\nTobacco smoking killed 100 million people worldwide in the 20th century and could kill 1 billion people around the world in the 21st century, a World Health Organization report warned.\n\nMany leading developed world causes of death can be postponed by diet and physical activity, but the accelerating incidence of disease with age still imposes limits on human longevity. The evolutionary cause of aging is, at best, only just beginning to be understood. It has been suggested that direct intervention in the aging process may now be the most effective intervention against major causes of death.\n\nSelye proposed a unified non-specific approach to many causes of death. He demonstrated that stress decreases adaptability of an organism and proposed to describe the adaptability as a special resource, \"adaptation energy\". The animal dies when this resource is exhausted. Selye assumed that the adaptability is a finite supply, presented at birth. Later on, Goldstone proposed the concept of a production or income of adaptation energy which may be stored (up to a limit), as a capital reserve of adaptation. In recent works, adaptation energy is considered as an internal coordinate on the \"dominant path\" in the model of adaptation. It is demonstrated that oscillations of well-being appear when the reserve of adaptability is almost exhausted.\n\nIn 2012, suicide overtook car crashes for leading causes of human injury deaths in the U.S., followed by poisoning, falls and murder. Causes of death are different in different parts of the world. In high-income and middle income countries nearly half up to more than two thirds of all people live beyond the age of 70 and predominantly die of chronic diseases. In low-income countries, where less than one in five of all people reach the age of 70, and more than a third of all deaths are among children under 15, people predominantly die of infectious diseases.\n\nAn autopsy, also known as a \"postmortem examination\" or an \"obduction\", is a medical procedure that consists of a thorough examination of a human corpse to determine the cause and manner of a person's death and to evaluate any disease or injury that may be present. It is usually performed by a specialized medical doctor called a pathologist.\n\nAutopsies are either performed for legal or medical purposes. A forensic autopsy is carried out when the cause of death may be a criminal matter, while a clinical or academic autopsy is performed to find the medical cause of death and is used in cases of unknown or uncertain death, or for research purposes. Autopsies can be further classified into cases where external examination suffices, and those where the body is dissected and an internal examination is conducted. Permission from next of kin may be required for internal autopsy in some cases. Once an internal autopsy is complete the body is generally reconstituted by sewing it back together. Autopsy is important in a medical environment and may shed light on mistakes and help improve practices.\n\nA \"necropsy\" is an older term for a postmortem examination, unregulated, and not always a medical procedure. In modern times the term is more often used in the postmortem examination of the corpses of animals.\n\nCryonics (from Greek κρύος 'kryos-' meaning 'icy cold') is the low-temperature preservation of animals and humans who cannot be sustained by contemporary medicine, with the hope that healing and resuscitation may be possible in the future.\n\nCryopreservation of people or large animals is not reversible with current technology. The stated rationale for cryonics is that people who are considered dead by current legal or medical definitions may not necessarily be dead according to the more stringent information-theoretic definition of death. It is proposed that cryopreserved people might someday be recovered by using highly advanced technology.\n\nSome scientific literature supports the feasibility of cryonics. Many other scientists regard cryonics with skepticism. By 2015, more than 300 people have undergone cryopreservation procedures since cryonics was first proposed in 1962.\n\nLife extension refers to an increase in maximum or average lifespan, especially in humans, by slowing down or reversing the processes of aging. Average lifespan is determined by vulnerability to accidents and age or lifestyle-related afflictions such as cancer, or cardiovascular disease. Extension of average lifespan can be achieved by good diet, exercise and avoidance of hazards such as smoking. Maximum lifespan is also determined by the rate of aging for a species inherent in its genes. Currently, the only widely recognized method of extending maximum lifespan is calorie restriction. Theoretically, extension of maximum lifespan can be achieved by reducing the rate of aging damage, by periodic replacement of damaged tissues, or by molecular repair or rejuvenation of deteriorated cells and tissues.\n\nA United States poll found that religious people and irreligious people, as well as men and women and people of different economic classes have similar rates of support for life extension, while Africans and Hispanics have higher rates of support than white people. 38 percent of the polled said they would desire to have their aging process cured.\n\nResearchers of life extension are a subclass of biogerontologists known as \"biomedical gerontologists\". They try to understand the nature of aging and they develop treatments to reverse aging processes or to at least slow them down, for the improvement of health and the maintenance of youthful vigor at every stage of life. Those who take advantage of life extension findings and seek to apply them upon themselves are called \"life extensionists\" or \"longevists\". The primary life extension strategy currently is to apply available anti-aging methods in the hope of living long enough to benefit from a complete cure to aging once it is developed.\n\n\"One of medicine's new frontiers: treating the dead\", recognizes that cells that have been without oxygen for more than five minutes die, not from lack of oxygen, but rather when their oxygen supply is resumed. Therefore, practitioners of this approach, e.g., at the Resuscitation Science institute at the University of Pennsylvania, \"aim to reduce oxygen uptake, slow metabolism and adjust the blood chemistry for gradual and safe reperfusion.\"\n\nBefore about 1930, most people in Western countries died in their own homes, surrounded by family, and comforted by clergy, neighbors, and doctors making house calls. By the mid-20th century, half of all Americans died in a hospital. By the start of the 21st century, only about 20 to 25% of people in developed countries died outside a medical institution. The shift away from dying at home, towards dying in a professionalized medical environment, has been termed the \"Invisible Death\". The \"Invisible Death\" process was extremely slow and infinitesimal. It took many years to shift to this new location where dying was commonly taking place outside the home.\n\nIn society, the nature of death and humanity's awareness of its own mortality has for millennia been a concern of the world's religious traditions and of philosophical inquiry. This includes belief in resurrection or an afterlife (associated with Abrahamic religions), reincarnation or rebirth (associated with Dharmic religions), or that consciousness permanently ceases to exist, known as eternal oblivion (associated with atheism).\n\nCommemoration ceremonies after death may include various mourning, funeral practices and ceremonies of honouring the deceased. The physical remains of a person, commonly known as a \"corpse\" or \"body\", are usually interred whole or cremated, though among the world's cultures there are a variety of other methods of mortuary disposal. In the English language, blessings directed towards a dead person include \"rest in peace\", or its initialism RIP.\n\nDeath is the center of many traditions and organizations; customs relating to death are a feature of every culture around the world. Much of this revolves around the care of the dead, as well as the afterlife and the disposal of bodies upon the onset of death. The disposal of human corpses does, in general, begin with the last offices before significant time has passed, and ritualistic ceremonies often occur, most commonly interment or cremation. This is not a unified practice; in Tibet, for instance, the body is given a sky burial and left on a mountain top. Proper preparation for death and techniques and ceremonies for producing the ability to transfer one's spiritual attainments into another body (reincarnation) are subjects of detailed study in Tibet. Mummification or embalming is also prevalent in some cultures, to retard the rate of decay.\n\nLegal aspects of death are also part of many cultures, particularly the settlement of the deceased estate and the issues of inheritance and in some countries, inheritance taxation.\nCapital punishment is also a culturally divisive aspect of death. In most jurisdictions where capital punishment is carried out today, the death penalty is reserved for premeditated murder, espionage, treason, or as part of military justice. In some countries, sexual crimes, such as adultery and sodomy, carry the death penalty, as do religious crimes such as apostasy, the formal renunciation of one's religion. In many retentionist countries, drug trafficking is also a capital offense. In China, human trafficking and serious cases of corruption are also punished by the death penalty. In militaries around the world courts-martial have imposed death sentences for offenses such as cowardice, desertion, insubordination, and mutiny.\n\nDeath in warfare and in suicide attack also have cultural links, and the ideas of dulce et decorum est pro patria mori, mutiny punishable by death, grieving relatives of dead soldiers and death notification are embedded in many cultures. Recently in the western world, with the increase in terrorism following the September 11 attacks, but also further back in time with suicide bombings, kamikaze missions in World War II and suicide missions in a host of other conflicts in history, death for a cause by way of suicide attack, and martyrdom have had significant cultural impacts.\n\nSuicide in general, and particularly euthanasia, are also points of cultural debate. Both acts are understood very differently in different cultures. In Japan, for example, ending a life with honor by seppuku was considered a desirable death, whereas according to traditional Christian and Islamic cultures, suicide is viewed as a sin. Death is personified in many cultures, with such symbolic representations as the Grim Reaper, Azrael, the Hindu God Yama and Father Time.\n\nIn Brazil, a human death is counted officially when it is registered by existing family members at a cartório, a government-authorized registry. Before being able to file for an official death, the deceased must have been registered for an official birth at the cartório. Though a Public Registry Law guarantees all Brazilian citizens the right to register deaths, regardless of their financial means, of their family members (often children), the Brazilian government has not taken away the burden, the hidden costs and fees, of filing for a death. For many impoverished families, the indirect costs and burden of filing for a death lead to a more appealing, unofficial, local, cultural burial, which in turn raises the debate about inaccurate mortality rates.\n\nTalking about death and witnessing it is a difficult issue with most cultures. Western societies may like to treat the dead with the utmost material respect, with an official embalmer and associated rites. Eastern societies (like India) may be more open to accepting it as a \"fait accompli\", with a funeral procession of the dead body ending in an open air burning-to-ashes of the same.\n\nMuch interest and debate surround the question of what happens to one's consciousness as one's body dies. The belief in the permanent loss of consciousness after death is often called \"eternal oblivion\". Belief that the stream of consciousness is preserved after physical death is described by the term \"afterlife\".\n\nAfter death the remains of an organism become part of the biogeochemical cycle. Animals may be consumed by a predator or a scavenger. Organic material may then be further decomposed by detritivores, organisms which recycle detritus, returning it to the environment for reuse in the food chain, where these chemicals may eventually end up being consumed and assimilated into the cells of a living organism. Examples of detritivores include earthworms, woodlice and dung beetles.\n\nMicroorganisms also play a vital role, raising the temperature of the decomposing matter as they break it down into yet simpler molecules. Not all materials need to be decomposed fully. Coal, a fossil fuel formed over vast tracts of time in swamp ecosystems, is one example.\n\nContemporary evolutionary theory sees death as an important part of the process of natural selection. It is considered that organisms less adapted to their environment are more likely to die having produced fewer offspring, thereby reducing their contribution to the gene pool. Their genes are thus eventually bred out of a population, leading at worst to extinction and, more positively, making the process possible, referred to as speciation. Frequency of reproduction plays an equally important role in determining species survival: an organism that dies young but leaves numerous offspring displays, according to Darwinian criteria, much greater fitness than a long-lived organism leaving only one.\n\nExtinction is the cessation of existence of a species or group of taxa, reducing biodiversity. The moment of extinction is generally considered to be the death of the last individual of that species (although the capacity to breed and recover may have been lost before this point). Because a species' potential range may be very large, determining this moment is difficult, and is usually done retrospectively. This difficulty leads to phenomena such as Lazarus taxa, where species presumed extinct abruptly \"reappear\" (typically in the fossil record) after a period of apparent absence. New species arise through the process of speciation, an aspect of evolution. New varieties of organisms arise and thrive when they are able to find and exploit an ecological niche – and species become extinct when they are no longer able to survive in changing conditions or against superior competition.\n\nInquiry into the evolution of aging aims to explain why so many living things and the vast majority of animals weaken and die with age (exceptions include \"Hydra\" and the already cited jellyfish \"Turritopsis dohrnii\", which research shows to be biologically immortal). The evolutionary origin of senescence remains one of the fundamental puzzles of biology. Gerontology specializes in the science of human aging processes.\n\nOrganisms showing only asexual reproduction (e.g. bacteria, some protists, like the euglenoids and many amoebozoans) and unicellular organisms with sexual reproduction (colonial or not, like the volvocine algae \"Pandorina\" and \"Chlamydomonas\") are \"immortal\" at some extent, dying only due to external hazards, like being eaten or meeting with a fatal accident. In multicellular organisms (and also in multinucleate ciliates), with a Weismannist development, that is, with a division of labor between mortal somatic (body) cells and \"immortal\" germ (reproductive) cells, death becomes an essential part of life, at least for the somatic line.\n\nThe \"Volvox\" algae are among the simplest organisms to exhibit that division of labor between two completely different cell types, and as a consequence include death of somatic line as a regular, genetically regulated part of its life history.\n\nDeath is an important subject of religious doctrine.\n\nIn Buddhist doctrine and practice, death plays an important role. Awareness of death was what motivated Prince Siddhartha to strive to find the \"deathless\" and finally to attain enlightenment. In Buddhist doctrine, death functions as a reminder of the value of having been born as a human being. Being reborn as a human being is considered the only state in which one can attain enlightenment, therefore death helps remind oneself that one should not that for granted. The belief in rebirth among Buddhists does not necessarily remove death anxiety, since all existence in the cycle of rebirth is considered filled with suffering, and being reborn many times does not necessarily mean that one progresses.\n\nDeath is part of several key Buddhist tenets, such as the Four Noble Truths and dependent origination.\n\nDeath is seen in Judaism as tragic and intimidating. Persons who come into contact with corpses are ritually impure. There are a variety of beliefs about the afterlife within Judaism, but none of them contradict the preference of life over death. This is partially because death puts a cessation to the possibility of fulfilling any commandments.\n\nBibliography\n\n"}
{"id": "14286773", "url": "https://en.wikipedia.org/wiki?curid=14286773", "title": "Delta 6 desaturase", "text": "Delta 6 desaturase\n\nDelta 6 desaturase (D6D or Δ-6-desaturase) is a desaturase enzyme that converts between types of fatty acids (termed \"6\" after omega-6 fatty acids), which are essential nutrients in the human body. The enzyme is molecularly identical across all living things (preserved across Kingdom (biology)) it is present in animals, plants, and cyanobacteria. \n\nD6D is one of the 3 fatty acid desaturases present in humans along with Δ-5 and Δ-9, named so because it was thought to convert only omega-6 fatty acids, but actually converts some others also, and is obligatory to build the longer chain omega-3 fatty acids from other simpler fatty acids in humans . In humans, it is encoded by the FADS2 gene .\n\nD6D is an desaturase enzyme, i.e. introduces a double bond in a specific position of long-chain fatty acids. Among them, it converts between various forms of Omega-3 and Omega-6 fatty acids:\n\nD6D is obligatory along with various elongases to convert to longer chain omega-3's, such as between ALA to EPA as well as EPA to DHA.\n\nGLA deficiencies in animals including humans have shown wide effects down the line -- Dihomogamma-linolenic acid (DGLA) and Prostaglandin E1 deficiency. PGE1 activates T lymphocytes, inhibits smooth muscle proliferation and thrombosis, is important in gonadal function and raises cyclic AMP levels in many tissues. It is a good candidate for a key factor lost in aging. It also affects viability of sperm. and dermatitis.\n\nLevels of D6D rapidly fall rapidly in the testes and more slowly in the liver in aging rats. D6D is a long chain PUFA rate limiter, has greater affinity for ALA than for linoleic acid, nevertheless many diets have far more linoleic acid present, resulting in reduced levels of alpha-Linolenic acid to EPA conversion. Women tend to have higher levels of D6D due to the effects of estrogen .\n\n\n\n"}
{"id": "1285228", "url": "https://en.wikipedia.org/wiki?curid=1285228", "title": "Egotism", "text": "Egotism\n\nEgotism is the drive to maintain and enhance favorable views of oneself, and generally features an inflated opinion of one's personal features and importance. It often includes intellectual, physical, social and other overestimations. \n\nThe egotist has an overwhelming sense of the centrality of the 'Me', that is to say of their personal qualities. Egotism means placing oneself at the core of one's world with no concern for others, including those \"loved\" or considered as \"close\", in any other terms except those subjectively set by the egotist.\n\nEgotism is closely related to an egocentric love for one's imagined self or narcissism – indeed some would say \"by egotism we may envisage a kind of socialized narcissism\". Egotists have a strong tendency to talk about themselves in a self-promoting fashion, and they may well be arrogant and boastful with a grandiose sense of their own importance. Their inability to recognise the accomplishments of others leaves them profoundly self-promoting; while sensitivity to criticism may lead on the egotist's part to narcissistic rage at a sense of insult.\n\nEgotism differs from both altruism – or acting to gain \"fewer\" values than are being given – and from egoism, the constant pursuit of one's self-interest. Various forms of \"empirical egoism\" have been considered consistent with egotism, but do not – which is also the case with egotism in general – necessitate having an inflated sense of self.\n\nIn developmental terms, two rather different trajectories can be distinguished with respect to egotism – the one individual, the other cultural.\n\nWith respect to the developing individual, a movement takes place from egocentricity to sociality during the process of growing up. It is normal for an infant to have an inflated – almost a majestic – sense of egotism. The over-evaluation of one's own ego regularly appears in childish forms of love – in large part because the baby is to himself everything, omnipotent to the best of their own knowledge.\n\nOptimal development allows a gradual reconciliation to a more realistic view of one's own place in the world – a lessening of the egotistical swollen head. Less adequate adjustment may later lead to what has been called defensive egotism, serving to overcompensate for the fragility of the underlying concept of self. Robin Skynner however considered that in the main growing up leads to a state where \"your ego is still there, but it's taking its proper limited place among all the other egos\".\n\nHowever, alongside such a positive trajectory of diminishing \"individual\" egotism, a rather different arc of development can be noted in cultural terms, linked to what has been seen as the increasing infantilism of (post)modern society. Whereas in the nineteenth century egotism was still widely regarded as a traditional vice – for Nathaniel Hawthorne egotism was a sort of diseased self-contemplation – Romanticism had already set in motion a countervailing current, what Richard Eldridge described as a kind of \"cultural egotism, substituting the individual imagination for vanishing social tradition\". The romantic idea of the self-creating individual – of a self-authorizing, artistic egotism – then took on broader social dimensions in the following century. Keats might still attack Wordsworth for the regressive nature of his retreat into the egotistical sublime; but by the close of the twentieth century egotism had been naturalized much more widely by the Me generation into the Culture of Narcissism.\n\nIn the 21st century, romantic egotism has been seen as feeding into techno-capitalism in two complementary ways: on the one hand, through the self-centred consumer, focused on their own self-fashioning through brand 'identity'; on the other through the equally egotistical voices of 'authentic' protest, as they rage against the machine, only to produce new commodity forms that serve to fuel the system for further consumption.\n\nThere is a question mark over the relationship between sex and egotism. Sigmund Freud popularly made the claim that love can transform the egotist, giving him or her a new sense of humility in relation to others.\n\nAt the same time, it is very apparent that egotism can readily show itself in sexual ways and indeed arguably one's whole sexuality may function in the service of egotistical needs.\n\nThe term egotism is derived from the Greek (\"εγώ\") and subsequently its Latinised ego (\"ego\"), meaning \"self\" or \"I,\" and \"-ism\", used to denote a system of belief. As such, the term shares early etymology with egoism.\n\n\n\n"}
{"id": "36444583", "url": "https://en.wikipedia.org/wiki?curid=36444583", "title": "Elisa Aaltola", "text": "Elisa Aaltola\n\nElisa Aaltola (born 1976) is a Finnish philosopher, specialised in animal philosophy, moral psychology and environmental philosophy.\n\nShe was a visiting PhD student at the Institute for Ethics, Environment, and Public Policy at Lancaster University and submitted her doctoral thesis to the University of Turku on \"Animal Individuality: Moral and Cultural Categorisations\". Her book \"Eläinten moraalinen arvo\" (Vastapaino 2004) is considered the first commercially published Finnish monograph dedicated solely to animal ethics. She is also the author of \"Animal Suffering: Philosophy and Culture\" (Palgrave MacMillan, 2012) and \"Varieties of Empathy: Moral Psychology and Animal Ethics\" (Rowman & Littlefield Int. 2018) as well as around 35 peer-reviewed papers. Her edited volumes include \"Animal Ethics and Philosophy: Questioning the Orthodoxy\" (co-edited with John Hadley, Rowman & Littlefield Int. 2014). Aaltola is an adjunct professor at the University of Turku and a senior research fellow at the University of Eastern Finland.\n\n"}
{"id": "9258", "url": "https://en.wikipedia.org/wiki?curid=9258", "title": "Ethics", "text": "Ethics\n\nEthics or moral philosophy is a branch of philosophy that involves systematizing, defending, and recommending concepts of right and wrong conduct. The field of ethics, along with aesthetics, concern matters of value, and thus comprise the branch of philosophy called axiology.\n\nEthics seeks to resolve questions of human morality by defining concepts such as good and evil, right and wrong, virtue and vice, justice and crime. As a field of intellectual inquiry, moral philosophy also is related to the fields of moral psychology, descriptive ethics, and value theory.\n\nThree major areas of study within ethics recognized today are:\n\nThe English word \"ethics\" is derived from the Ancient Greek word \"ēthikós\" (), meaning \"relating to one's character\", which itself comes from the root word \"êthos\" () meaning \"character, moral nature\". This was borrowed into Latin as \"ethica\" and then into French as \"éthique\", from which it was borrowed into English.\n\nRushworth Kidder states that \"standard definitions of \"ethics\" have typically included such phrases as 'the science of the ideal human character' or 'the science of moral duty'. Richard William Paul and Linda Elder define ethics as \"a set of concepts and principles that guide us in determining what behavior helps or harms sentient creatures\". The \"Cambridge Dictionary of Philosophy\" states that the word \"ethics\" is \"commonly used interchangeably with 'morality' ... and sometimes it is used more narrowly to mean the moral principles of a particular tradition, group or individual.\" Paul and Elder state that most people confuse ethics with behaving in accordance with social conventions, religious beliefs and the law and don't treat ethics as a stand-alone concept.\n\nThe word \"ethics\" in English refers to several things. It can refer to philosophical ethics or moral philosophy—a project that attempts to use reason to answer various kinds of ethical questions. As the English philosopher Bernard Williams writes, attempting to explain moral philosophy: \"What makes an inquiry a philosophical one is reflective generality and a style of argument that claims to be rationally persuasive.\" Williams describes the content of this area of inquiry as addressing the very broad question, \"how one should live\". Ethics can also refer to a common human ability to think about ethical problems that is not particular to philosophy. As bioethicist Larry Churchill has written: \"Ethics, understood as the capacity to think critically about moral values and direct our actions in terms of such values, is a generic human capacity.\" Ethics can also be used to describe a particular person's own idiosyncratic principles or habits. For example: \"Joe has strange ethics.\"\n\nMeta-ethics is the branch of philosophical ethics that asks how we understand, know about, and what we mean when we talk about what is right and what is wrong. An ethical question pertaining to a particular practical situation—such as, \"Should I eat this particular piece of chocolate cake?\"—cannot be a meta-ethical question (rather, this is an applied ethical question). A meta-ethical question is abstract and relates to a wide range of more specific practical questions. For example, \"Is it ever possible to have secure knowledge of what is right and wrong?\" is a meta-ethical question.\n\nMeta-ethics has always accompanied philosophical ethics. For example, Aristotle implies that less precise knowledge is possible in ethics than in other spheres of inquiry, and he regards ethical knowledge as depending upon habit and acculturation in a way that makes it distinctive from other kinds of knowledge. Meta-ethics is also important in G.E. Moore's \"Principia Ethica\" from 1903. In it he first wrote about what he called \"the naturalistic fallacy\". Moore was seen to reject naturalism in ethics, in his Open Question Argument. This made thinkers look again at second order questions about ethics. Earlier, the Scottish philosopher David Hume had put forward a similar view on the difference between facts and values.\n\nStudies of how we know in ethics divide into cognitivism and non-cognitivism; this is quite akin to the thing called descriptive and non-descriptive. Non-cognitivism is the view that when we judge something as morally right or wrong, this is neither true nor false. We may, for example, be only expressing our emotional feelings about these things. Cognitivism can then be seen as the claim that when we talk about right and wrong, we are talking about matters of fact.\n\nThe ontology of ethics is about value-bearing things or properties, i.e. the kind of things or stuff referred to by ethical propositions. Non-descriptivists and non-cognitivists believe that ethics does not need a specific ontology since ethical propositions do not refer. This is known as an anti-realist position. Realists, on the other hand, must explain what kind of entities, properties or states are relevant for ethics, how they have value, and why they guide and motivate our actions.\n\nNormative ethics is the study of ethical action. It is the branch of ethics that investigates the set of questions that arise when considering how one ought to act, morally speaking. Normative ethics is distinct from meta-ethics because normative ethics examines standards for the rightness and wrongness of actions, while meta-ethics studies the meaning of moral language and the metaphysics of moral facts. Normative ethics is also distinct from descriptive ethics, as the latter is an empirical investigation of people's moral beliefs. To put it another way, descriptive ethics would be concerned with determining what proportion of people believe that killing is always wrong, while normative ethics is concerned with whether it is correct to hold such a belief. Hence, normative ethics is sometimes called prescriptive, rather than descriptive. However, on certain versions of the meta-ethical view called moral realism, moral facts are both descriptive and prescriptive at the same time.\n\nTraditionally, normative ethics (also known as moral theory) was the study of what makes actions right and wrong. These theories offered an overarching moral principle one could appeal to in resolving difficult moral decisions.\n\nAt the turn of the 20th century, moral theories became more complex and were no longer concerned solely with rightness and wrongness, but were interested in many different kinds of moral status. During the middle of the century, the study of normative ethics declined as meta-ethics grew in prominence. This focus on meta-ethics was in part caused by an intense linguistic focus in analytic philosophy and by the popularity of logical positivism.\n\nVirtue ethics describes the character of a moral agent as a driving force for ethical behavior, and it is used to describe the ethics of Socrates, Aristotle, and other early Greek philosophers. Socrates (469–399 BC) was one of the first Greek philosophers to encourage both scholars and the common citizen to turn their attention from the outside world to the condition of humankind. In this view, knowledge bearing on human life was placed highest, while all other knowledge was secondary. Self-knowledge was considered necessary for success and inherently an essential good. A self-aware person will act completely within his capabilities to his pinnacle, while an ignorant person will flounder and encounter difficulty. To Socrates, a person must become aware of every fact (and its context) relevant to his existence, if he wishes to attain self-knowledge. He posited that people will naturally do what is good if they know what is right. Evil or bad actions are the results of ignorance. If a criminal was truly aware of the intellectual and spiritual consequences of his or her actions, he or she would neither commit nor even consider committing those actions. Any person who knows what is truly right will automatically do it, according to Socrates. While he correlated knowledge with virtue, he similarly equated virtue with joy. The truly wise man will know what is right, do what is good, and therefore be happy.\n\nAristotle (384–323 BC) posited an ethical system that may be termed \"virtuous\". In Aristotle's view, when a person acts in accordance with virtue this person will do good and be content. Unhappiness and frustration are caused by doing wrong, leading to failed goals and a poor life. Therefore, it is imperative for people to act in accordance with virtue, which is only attainable by the practice of the virtues in order to be content and complete. Happiness was held to be the ultimate goal. All other things, such as civic life or wealth, were only made worthwhile and of benefit when employed in the practice of the virtues. The practice of the virtues is the surest path to happiness.\n\nAristotle asserted that the soul of man had three natures: body (physical/metabolism), animal (emotional/appetite), and rational (mental/conceptual). Physical nature can be assuaged through exercise and care; emotional nature through indulgence of instinct and urges; and mental nature through human reason and developed potential. Rational development was considered the most important, as essential to philosophical self-awareness and as uniquely human. Moderation was encouraged, with the extremes seen as degraded and immoral. For example, courage is the moderate virtue between the extremes of cowardice and recklessness. Man should not simply live, but live well with conduct governed by virtue. This is regarded as difficult, as virtue denotes doing the right thing, in the right way, at the right time, for the right reason.\n\nThe Stoic philosopher Epictetus posited that the greatest good was contentment and serenity. Peace of mind, or \"apatheia\", was of the highest value; self-mastery over one's desires and emotions leads to spiritual peace. The \"unconquerable will\" is central to this philosophy. The individual's will should be independent and inviolate. Allowing a person to disturb the mental equilibrium is, in essence, offering yourself in slavery. If a person is free to anger you at will, you have no control over your internal world, and therefore no freedom. Freedom from material attachments is also necessary. If a thing breaks, the person should not be upset, but realize it was a thing that could break. Similarly, if someone should die, those close to them should hold to their serenity because the loved one was made of flesh and blood destined to death. Stoic philosophy says to accept things that cannot be changed, resigning oneself to the existence and enduring in a rational fashion. Death is not feared. People do not \"lose\" their life, but instead \"return\", for they are returning to God (who initially gave what the person is as a person). Epictetus said difficult problems in life should not be avoided, but rather embraced. They are spiritual exercises needed for the health of the spirit, just as physical exercise is required for the health of the body. He also stated that sex and sexual desire are to be avoided as the greatest threat to the integrity and equilibrium of a man's mind. Abstinence is highly desirable. Epictetus said remaining abstinent in the face of temptation was a victory for which a man could be proud.\n\nModern virtue ethics was popularized during the late 20th century in large part as a response to G. E. M. Anscombe's \"Modern Moral Philosophy\". Anscombe argues that consequentialist and deontological ethics are only feasible as universal theories if the two schools ground themselves in divine law. As a deeply devoted Christian herself, Anscombe proposed that either those who do not give ethical credence to notions of divine law take up virtue ethics, which does not necessitate universal laws as agents themselves are investigated for virtue or vice and held up to \"universal standards\", or that those who wish to be utilitarian or consequentialist ground their theories in religious conviction. Alasdair MacIntyre, who wrote the book \"After Virtue\", was a key contributor and proponent of modern virtue ethics, although some claim that MacIntyre supports a relativistic account of virtue based on cultural norms, not objective standards. Martha Nussbaum, a contemporary virtue ethicist, objects to MacIntyre's relativism, among that of others, and responds to relativist objections to form an objective account in her work \"Non-Relative Virtues: An Aristotelian Approach\". However, Nussbaum's accusation of relativism appears to be a misreading. In \"Whose Justice, Whose Rationality?\", MacIntyre's ambition of taking a rational path beyond relativism was quite clear when he stated \"rival claims made by different traditions […] are to be evaluated […] without relativism\" (p. 354) because indeed \"rational debate between and rational choice among rival traditions is possible” (p. 352). \"Complete Conduct Principles for the 21st Century\" blended the Eastern virtue ethics and the Western virtue ethics, with some modifications to suit the 21st Century, and formed a part of contemporary virtue ethics.\n\nHedonism posits that the principal ethic is maximizing pleasure and minimizing pain. There are several schools of Hedonist thought ranging from those advocating the indulgence of even momentary desires to those teaching a pursuit of spiritual bliss. In their consideration of consequences, they range from those advocating self-gratification regardless of the pain and expense to others, to those stating that the most ethical pursuit maximizes pleasure and happiness for the most people.\n\nFounded by Aristippus of Cyrene, Cyrenaics supported immediate gratification or pleasure. \"Eat, drink and be merry, for tomorrow we die.\" Even fleeting desires should be indulged, for fear the opportunity should be forever lost. There was little to no concern with the future, the present dominating in the pursuit of immediate pleasure. Cyrenaic hedonism encouraged the pursuit of enjoyment and indulgence without hesitation, believing pleasure to be the only good.\n\nEpicurean ethics is a hedonist form of virtue ethics. Epicurus \"...presented a sustained argument that pleasure, correctly understood, will coincide with virtue.\" He rejected the extremism of the Cyrenaics, believing some pleasures and indulgences to be detrimental to human beings. Epicureans observed that indiscriminate indulgence sometimes resulted in negative consequences. Some experiences were therefore rejected out of hand, and some unpleasant experiences endured in the present to ensure a better life in the future. To Epicurus, the \"summum bonum\", or greatest good, was prudence, exercised through moderation and caution. Excessive indulgence can be destructive to pleasure and can even lead to pain. For example, eating one food too often makes a person lose a taste for it. Eating too much food at once leads to discomfort and ill-health. Pain and fear were to be avoided. Living was essentially good, barring pain and illness. Death was not to be feared. Fear was considered the source of most unhappiness. Conquering the fear of death would naturally lead to a happier life. Epicurus reasoned if there were an afterlife and immortality, the fear of death was irrational. If there was no life after death, then the person would not be alive to suffer, fear or worry; he would be non-existent in death. It is irrational to fret over circumstances that do not exist, such as one's state of death in the absence of an afterlife.\n\nState consequentialism, also known as Mohist consequentialism, is an ethical theory that evaluates the moral worth of an action based on how much it contributes to the basic goods of a state. The \"Stanford Encyclopedia of Philosophy\" describes Mohist consequentialism, dating back to the 5th century BC, as \"a remarkably sophisticated version based on a plurality of intrinsic goods taken as constitutive of human welfare\". Unlike utilitarianism, which views pleasure as a moral good, \"the basic goods in Mohist consequentialist thinking are ... order, material wealth, and increase in population\". During Mozi's era, war and famines were common, and population growth was seen as a moral necessity for a harmonious society. The \"material wealth\" of Mohist consequentialism refers to basic needs like shelter and clothing, and the \"order\" of Mohist consequentialism refers to Mozi's stance against warfare and violence, which he viewed as pointless and a threat to social stability.\n\nStanford sinologist David Shepherd Nivison, in \"The Cambridge History of Ancient China\", writes that the moral goods of Mohism \"are interrelated: more basic wealth, then more reproduction; more people, then more production and wealth ... if people have plenty, they would be good, filial, kind, and so on unproblematically.\" The Mohists believed that morality is based on \"promoting the benefit of all under heaven and eliminating harm to all under heaven\". In contrast to Bentham's views, state consequentialism is not utilitarian because it is not hedonistic or individualistic. The importance of outcomes that are good for the community outweigh the importance of individual pleasure and pain.\n\nConsequentialism refers to moral theories that hold the consequences of a particular action form the basis for any valid moral judgment about that action (or create a structure for judgment, see rule consequentialism). Thus, from a consequentialist standpoint, a morally right action is one that produces a good outcome, or consequence. This view is often expressed as the aphorism \"The ends justify the means\".\n\nThe term \"consequentialism\" was coined by G. E. M. Anscombe in her essay \"Modern Moral Philosophy\" in 1958, to describe what she saw as the central error of certain moral theories, such as those propounded by Mill and Sidgwick. Since then, the term has become common in English-language ethical theory.\n\nThe defining feature of consequentialist moral theories is the weight given to the consequences in evaluating the rightness and wrongness of actions. In consequentialist theories, the consequences of an action or rule generally outweigh other considerations. Apart from this basic outline, there is little else that can be unequivocally said about consequentialism as such. However, there are some questions that many consequentialist theories address:\n\nOne way to divide various consequentialisms is by the many types of consequences that are taken to matter most, that is, which consequences count as good states of affairs. According to utilitarianism, a good action is one that results in an increase and positive effect, and the best action is one that results in that effect for the greatest number. Closely related is eudaimonic consequentialism, according to which a full, flourishing life, which may or may not be the same as enjoying a great deal of pleasure, is the ultimate aim. Similarly, one might adopt an aesthetic consequentialism, in which the ultimate aim is to produce beauty. However, one might fix on non-psychological goods as the relevant effect. Thus, one might pursue an increase in material equality or political liberty instead of something like the more ephemeral \"pleasure\". Other theories adopt a package of several goods, all to be promoted equally. Whether a particular consequentialist theory focuses on a single good or many, conflicts and tensions between different good states of affairs are to be expected and must be adjudicated.\n\nUtilitarianism is an ethical theory that argues the proper course of action is one that maximizes a positive effect, such as \"happiness\", \"welfare\", or the ability to live according to personal preferences. Jeremy Bentham and John Stuart Mill are influential proponents of this school of thought. In \"A Fragment on Government\" Bentham says 'it is the greatest happiness of the greatest number that is the measure of right and wrong' and describes this as a fundamental axiom. In \"An Introduction to the Principles of Morals and Legislation\" he talks of 'the principle of utility' but later prefers \"the greatest happiness principle\".\n\nUtilitarianism is the paradigmatic example of a consequentialist moral theory. This form of utilitarianism holds that the morally correct action is the one that produces the best outcome for all people affected by the action. John Stuart Mill, in his exposition of utilitarianism, proposed a hierarchy of pleasures, meaning that the pursuit of certain kinds of pleasure is more highly valued than the pursuit of other pleasures. Other noteworthy proponents of utilitarianism are neuroscientist Sam Harris, author of \"The Moral Landscape\", and moral philosopher Peter Singer, author of, amongst other works, \"Practical Ethics\".\n\nThe major division within utilitarianism is between \"act utilitarianism\" and \"rule utilitarianism\". In act utilitarianism, the principle of utility applies directly to each alternative act in a situation of choice. The right act is the one that brings about the best results (or the least amount of bad results). In rule utilitarianism, the principle of utility determines the validity of rules of conduct (moral principles). A rule like promise-keeping is established by looking at the consequences of a world in which people break promises at will and a world in which promises are binding. Right and wrong are the following or breaking of rules that are sanctioned by their utilitarian value. A proposed \"middle ground\" between these two types is Two-level utilitarianism, where rules are applied in ordinary circumstances, but with an allowance to choose actions outside of such rules when unusual situations call for it.\n\nDeontological ethics or deontology (from Greek , \"deon\", \"obligation, duty\"; and , \"-logia\") is an approach to ethics that determines goodness or rightness from examining acts, or the rules and duties that the person doing the act strove to fulfill. This is in contrast to consequentialism, in which rightness is based on the consequences of an act, and not the act by itself. Under deontology, an act may be considered right even if the act produces a bad consequence, if it follows the \"rule\" or moral law. According to the deontological view, people have a \"duty\" to act in a way that does those things that are inherently good as acts (\"truth-telling\" for example), or follow an objectively obligatory rule (as in rule utilitarianism).\n\nImmanuel Kant's theory of ethics is considered deontological for several different reasons. First, Kant argues that to act in the morally right way, people must act from duty (\"deon\"). Second, Kant argued that it was not the consequences of actions that make them right or wrong but the motives (expressed as maxims) of the person who carries out the action. Kant's argument that to act in the morally right way, one must act from duty, begins with an argument that the highest good must be both good in itself, and good without qualification. Something is 'good in itself' when it is intrinsically good, and 'good without qualification' when the addition of that thing never makes a situation ethically worse. Kant then argues that those things that are usually thought to be good, such as intelligence, perseverance and pleasure, fail to be either intrinsically good or good without qualification. Pleasure, for example, appears to not be good without qualification, because when people take pleasure in watching someone suffer, they make the situation ethically worse. He concludes that there is only one thing that is truly good:\n\nNothing in the world—indeed nothing even beyond the world—can possibly be conceived which could be called good without qualification except a \"good will\".\n\nAssociated with the pragmatists, Charles Sanders Peirce, William James, and especially John Dewey, pragmatic ethics holds that moral correctness evolves similarly to scientific knowledge: socially over the course of many lifetimes. Thus, we should prioritize social reform over attempts to account for consequences, individual virtue or duty (although these may be worthwhile attempts, if social reform is provided for).\n\nCare ethics contrasts with more well-known ethical models, such as consequentialist theories (e.g. utilitarianism) and deontological theories (e.g., Kantian ethics) in that it seeks to incorporate traditionally feminized virtues and values that—proponents of care ethics contend—are absent in such traditional models of ethics. These values include the importance of empathetic relationships and compassion.\n\nCare-focused feminism is a branch of feminist thought, informed primarily by ethics of care as developed by Carol Gilligan and Nel Noddings. This body of theory is critical of how caring is socially assigned to women, and consequently devalued. They write, “Care-focused feminists regard women’s capacity for care as a human strength,” that should be taught to and expected of men as well as women. Noddings proposes that ethical caring has the potential to be a more concrete evaluative model of moral dilemma than an ethic of justice. Noddings’ care-focused feminism requires practical application of relational ethics, predicated on an ethic of care.\n\nRole ethics is an ethical theory based on family roles. Unlike virtue ethics, role ethics is not individualistic. Morality is derived from a person's relationship with their community. Confucian ethics is an example of role ethics though this is not straightforwardly uncontested. Confucian roles center around the concept of filial piety or \"xiao\", a respect for family members. According to Roger T. Ames and Henry Rosemont, \"Confucian normativity is defined by living one's family roles to maximum effect.\" Morality is determined through a person's fulfillment of a role, such as that of a parent or a child. Confucian roles are not rational, and originate through the \"xin\", or human emotions.\n\nAnarchist ethics is an ethical theory based on the studies of anarchist thinkers. The biggest contributor to the anarchist ethics is the Russian zoologist, geographer, economist, and political activist Peter Kropotkin.\n\nStarting from the premise that the goal of ethical philosophy should be to help humans adapt and thrive in evolutionary terms, Kropotkin's ethical framework uses biology and anthropology as a basis – in order to scientifically establish what will best enable a given social order to thrive biologically and socially – and advocates certain behavioural practices to enhance humanity's capacity for freedom and well-being, namely practices which emphasise solidarity, equality, and justice.\n\nKropotkin argues that ethics itself is evolutionary, and is inherited as a sort of a social instinct through cultural history, and by so, he rejects any religious and transcendental explanation of morality. The origin of ethical feeling in both animals and humans can be found, he claims, in the natural fact of \"sociality\" (mutualistic symbiosis), which humans can then combine with the instinct for justice (i.e. equality) and then with the practice of reason to construct a non-supernatural and anarchistic system of ethics. Kropotkin suggests that the principle of equality at the core of anarchism is the same as the Golden rule: This principle of treating others as one wishes to be treated oneself, what is it but the very same principle as equality, the fundamental principle of anarchism? And how can any one manage to believe himself an anarchist unless he practices it? We do not wish to be ruled. And by this very fact, do we not declare that we ourselves wish to rule nobody? We do not wish to be deceived, we wish always to be told nothing but the truth. And by this very fact, do we not declare that we ourselves do not wish to deceive anybody, that we promise to always tell the truth, nothing but the truth, the whole truth? We do not wish to have the fruits of our labor stolen from us. And by that very fact, do we not declare that we respect the fruits of others' labor? By what right indeed can we demand that we should be treated in one fashion, reserving it to ourselves to treat others in a fashion entirely different? Our sense of equality revolts at such an idea.\n\nThe 20th century saw a remarkable expansion and evolution of critical theory, following on earlier Marxist Theory efforts to locate individuals within larger structural frameworks of ideology and action.\n\nAntihumanists such as Louis Althusser, Michel Foucault and structuralists such as Roland Barthes challenged the possibilities of individual agency and the coherence of the notion of the 'individual' itself. This was on the basis that personal identity was, at least in part, a social construction. As critical theory developed in the later 20th century, post-structuralism sought to problematize human relationships to knowledge and 'objective' reality. Jacques Derrida argued that access to meaning and the 'real' was always deferred, and sought to demonstrate via recourse to the linguistic realm that \"there is no outside-text/non-text\" (\"il n'y a pas de hors-texte\" is often mistranslated as \"there is nothing outside the text\"); at the same time, Jean Baudrillard theorised that signs and symbols or simulacra mask reality (and eventually the absence of reality itself), particularly in the consumer world.\n\nPost-structuralism and postmodernism argue that ethics must study the complex and relational conditions of actions. A simple alignment of ideas of right and particular acts is not possible. There will always be an ethical remainder that cannot be taken into account or often even recognized. Such theorists find narrative (or, following Nietzsche and Foucault, genealogy) to be a helpful tool for understanding ethics because narrative is always about particular lived experiences in all their complexity rather than the assignment of an idea or norm to separate and individual actions.\n\nZygmunt Bauman says postmodernity is best described as modernity without illusion, the illusion being the belief that humanity can be repaired by some ethic principle. Postmodernity can be seen in this light as accepting the messy nature of humanity as unchangeable.\n\nDavid Couzens Hoy states that Emmanuel Levinas's writings on the face of the Other and Derrida's meditations on the relevance of death to ethics are signs of the \"ethical turn\" in Continental philosophy that occurred in the 1980s and 1990s. Hoy describes post-critique ethics as the \"obligations that present themselves as necessarily to be fulfilled but are neither forced on one or are enforceable\" (2004, p. 103).\n\nHoy's post-critique model uses the term \"ethical resistance\". Examples of this would be an individual's resistance to consumerism in a retreat to a simpler but perhaps harder lifestyle, or an individual's resistance to a terminal illness. Hoy describes Levinas's account as \"not the attempt to use power against itself, or to mobilize sectors of the population to exert their political power; the ethical resistance is instead the resistance of the powerless\"(2004, p. 8).\n\nHoy concludes that\n\nApplied ethics is a discipline of philosophy that attempts to apply ethical theory to real-life situations. The discipline has many specialized fields, such as engineering ethics, bioethics, geoethics, public service ethics and business ethics.\n\nApplied ethics is used in some aspects of determining public policy, as well as by individuals facing difficult decisions. The sort of questions addressed by applied ethics include: \"Is getting an abortion immoral?\" \"Is euthanasia immoral?\" \"Is affirmative action right or wrong?\" \"What are human rights, and how do we determine them?\" \"Do animals have rights as well?\" and \"Do individuals have the right of self-determination?\"\n\nA more specific question could be: \"If someone else can make better out of his/her life than I can, is it then moral to sacrifice myself for them if needed?\" Without these questions, there is no clear fulcrum on which to balance law, politics, and the practice of arbitration—in fact, no common assumptions of all participants—so the ability to formulate the questions are prior to rights balancing. But not all questions studied in applied ethics concern public policy. For example, making ethical judgments regarding questions such as, \"Is lying always wrong?\" and, \"If not, when is it permissible?\" is prior to any etiquette.\n\nPeople, in general, are more comfortable with dichotomies (two opposites). However, in ethics, the issues are most often multifaceted and the best-proposed actions address many different areas concurrently. In ethical decisions, the answer is almost never a \"yes or no\", \"right or wrong\" statement. Many buttons are pushed so that the overall condition is improved and not to the benefit of any particular faction.\n\nBioethics is the study of controversial ethics brought about by advances in biology and medicine. Bioethicists are concerned with the ethical questions that arise in the relationships among life sciences, biotechnology, medicine, politics, law, and philosophy. It also includes the study of the more commonplace questions of values (\"the ethics of the ordinary\") that arise in primary care and other branches of medicine.\n\nBioethics also needs to address emerging biotechnologies that affect basic biology and future humans. These developments include cloning, gene therapy, human genetic engineering, astroethics and life in space, and manipulation of basic biology through altered DNA, RNA and proteins, e.g. \"three parent baby, where baby is born from genetically modified embryos, would have DNA from a mother, a father and from a female donor. Correspondingly, new bioethics also need to address life at its core. For example, biotic ethics value organic gene/protein life itself and seek to propagate it. With such life-centered principles, ethics may secure a cosmological future for life.\n\nBusiness ethics (also corporate ethics) is a form of applied ethics or professional ethics that examines ethical principles and moral or ethical problems that arise in a business environment, including fields like medical ethics. Business ethics represents the practices that any individual or group exhibits within an organization that can negatively or positively affect the businesses core values. It applies to all aspects of business conduct and is relevant to the conduct of individuals and entire organizations.\n\nBusiness ethics has both normative and descriptive dimensions. As a corporate practice and a career specialization, the field is primarily normative. Academics attempting to understand business behavior employ descriptive methods. The range and quantity of business ethical issues reflect the interaction of profit-maximizing behavior with non-economic concerns. Interest in business ethics accelerated dramatically during the 1980s and 1990s, both within major corporations and within academia. For example, today most major corporations promote their commitment to non-economic values under headings such as ethics codes and social responsibility charters. Adam Smith said, \"People of the same trade seldom meet together, even for merriment and diversion, but the conversation ends in a conspiracy against the public, or in some contrivance to raise prices.\" Governments use laws and regulations to point business behavior in what they perceive to be beneficial directions. Ethics implicitly regulates areas and details of behavior that lie beyond governmental control. The emergence of large corporations with limited relationships and sensitivity to the communities in which they operate accelerated the development of formal ethics regimes.\n\nIn \"Moral Machines: Teaching Robots Right from Wrong\", Wendell Wallach and Colin Allen conclude that issues in machine ethics will likely drive advancement in understanding of human ethics by forcing us to address gaps in modern normative theory and by providing a platform for experimental investigation. The effort to actually program a machine or artificial agent to behave as though instilled with a sense of ethics requires new specificity in our normative theories, especially regarding aspects customarily considered common-sense. For example, machines, unlike humans, can support a wide selection of learning algorithms, and controversy has arisen over the relative ethical merits of these options. This may reopen classic debates of normative ethics framed in new (highly technical) terms.\n\nMilitary ethics are concerned with questions regarding the application of force and the ethos of the soldier and are often understood as applied professional ethics. Just war theory is generally seen to set the background terms of military ethics. However individual countries and traditions have different fields of attention.\n\nMilitary ethics involves multiple subareas, including the following among others:\n\nPolitical ethics (also known as political morality or public ethics) is the practice of making moral judgements about political action and political agents.\n\nPublic sector ethics is a set of principles that guide public officials in their service to their constituents, including their decision-making on behalf of their constituents. Fundamental to the concept of public sector ethics is the notion that decisions and actions are based on what best serves the public's interests, as opposed to the official's personal interests (including financial interests) or self-serving political interests.\n\nPublication ethics is the set of principles that guide the writing and publishing process for all professional publications. To follow these principles, authors must verify that the publication does not contain plagiarism or publication bias. As a way to avoid misconduct in research these principles can also apply to experiments that are referenced or analyzed in publications by ensuring the data is recorded honestly and accurately.\n\nPlagiarism is the failure to give credit to another author’s work or ideas, when it is used in the publication. It is the obligation of the editor of the journal to ensure the article does not contain any plagiarism before it is published. If a publication that has already been published is proven to contain plagiarism, the editor of the journal can retract the article.\n\nPublication bias occurs when the publication is one-sided or \"prejudiced against results\". In best practice, an author should try to include information from all parties involved, or affected by the topic. If an author is prejudiced against certain results, than it can \"lead to erroneous conclusions being drawn\".\n\nMisconduct in research can occur when an experimenter falsifies results. Falsely recorded information occurs when the researcher \"fakes\" information or data, which was not used when conducting the actual experiment. By faking the data, the researcher can alter the results from the experiment to better fit the hypothesis they originally predicted. When conducting medical research, it is important to honor the healthcare rights of a patient by protecting their anonymity in the publication.\n\"Respect for autonomy\" is the principle that decision-making should allow individuals to be autonomous; they should be able to make decisions that apply to their own lives. This means that individuals should have control of their lives.\n\"Justice\" is the principle that decision-makers must focus on actions that are fair to those affected. Ethical decisions need to be consistent with the ethical theory. There are cases where the management has made decisions that seem to be unfair to the employees, shareholders, and other stakeholders (Solomon, 1992, pp49). Such decisions are unethical.\n\nRelational ethics are related to an ethics of care. They are used in qualitative research, especially ethnography and autoethnography. Researchers who employ relational ethics value and respect the connection between themselves and the people they study, and \"...between researchers and the communities in which they live and work.\" (Ellis, 2007, p. 4). Relational ethics also help researchers understand difficult issues such as conducting research on intimate others that have died and developing friendships with their participants. Relational ethics in close personal relationships form a central concept of contextual therapy.\n\nAnimal ethics is a term used in academia to describe human-animal relationships and how animals ought to be treated. The subject matter includes animal rights, animal welfare, animal law, speciesism, animal cognition, wildlife conservation, the moral status of nonhuman animals, the concept of nonhuman personhood, human exceptionalism, the history of animal use, and theories of justice.\n\nMoral psychology is a field of study that began as an issue in philosophy and that is now properly considered part of the discipline of psychology. Some use the term \"moral psychology\" relatively narrowly to refer to the study of moral development. However, others tend to use the term more broadly to include any topics at the intersection of ethics and psychology (and philosophy of mind). Such topics are ones that involve the mind and are relevant to moral issues. Some of the main topics of the field are moral responsibility, moral development, moral character (especially as related to virtue ethics), altruism, psychological egoism, moral luck, and moral disagreement.\n\nEvolutionary ethics concerns approaches to ethics (morality) based on the role of evolution in shaping human psychology and behavior. Such approaches may be based in scientific fields such as evolutionary psychology or sociobiology, with a focus on understanding and explaining observed ethical preferences and choices.\n\nDescriptive ethics is on the less philosophical end of the spectrum since it seeks to gather particular information about how people live and draw general conclusions based on observed patterns. Abstract and theoretical questions that are more clearly philosophical—such as, \"Is ethical knowledge possible?\"—are not central to descriptive ethics. Descriptive ethics offers a value-free approach to ethics, which defines it as a social science rather than a humanity. Its examination of ethics doesn't start with a preconceived theory but rather investigates observations of actual choices made by moral agents in practice. Some philosophers rely on descriptive ethics and choices made and unchallenged by a society or culture to derive categories, which typically vary by context. This can lead to situational ethics and situated ethics. These philosophers often view aesthetics, etiquette, and arbitration as more fundamental, percolating \"bottom up\" to imply the existence of, rather than explicitly prescribe, theories of value or of conduct. The study of descriptive ethics may include examinations of the following:\n\n\n\n"}
{"id": "1522987", "url": "https://en.wikipedia.org/wiki?curid=1522987", "title": "Gonochorism", "text": "Gonochorism\n\nIn biology, gonochorism (\"Greek\" offspring + disperse) or unisexualism or gonochory describes the state of having just one of at least two distinct sexes in any one individual organism. The term is most often used with animals, in which the individual organisms are often gonochorous. Gonochory is less common in plants. For example, in flowering plants, individual flowers may be hermaphroditic (i.e. with both stamens and ovaries) or gonochorous (unisexual), having either no stamens (i.e. no male parts) or no ovaries (i.e. no female parts). Among flowering plant species that have unisexual flowers, some also produce hermaphrodite flowers, and the three types occur in different arrangements on separate plants; species can be monoecious, dioecious, trioecious, polygamomonoecious, polygamodioecious, andromonoecious, or gynomonoecious.\n\nSex is most often genetically determined, but may be determined by other mechanisms. For example, alligators use temperature-dependent sex determination during egg incubation. Examples of gonochoric or dioecious pollination include hollies and kiwifruit. In these plants the male plant that supplies the pollen is referred to as the pollenizer.\n\nGonochorism stands in contrast to other reproductive strategies such as asexual reproduction and hermaphroditism. The sex of an individual may also change during its lifetime – this sequential hermaphroditism can for example be found in parrotfish and cockles.\n\n"}
{"id": "5278085", "url": "https://en.wikipedia.org/wiki?curid=5278085", "title": "Harald Aabrekk", "text": "Harald Aabrekk\n\nHarald Olav Aabrekk (born 22 February 1956) is a Norwegian football coach and a former player. He played for Sogndal and Brann in the Tippeligaen, Norway's top professional football league. He has been head coach of Sogndal, Tromsø, Brann, Haugesund, Vålerenga and Aalesund, and has also served as an assistant coach for the Norwegian national football team.\n\nAabrekk made his debut for Sogndal in 1974 in the Norwegian Second Division (the third tier of Norwegian football). Except for a short spell at SK Brann, where he played a match in the Tippeligaen (the top division) in 1978, he played for Sogndal throughout his career and was a major contributor to their promotion to the Norwegian First Division in 1981. He scored Sogndal's first goal in that division, against Molde in 1982. He played a total of 133 league games from 1973 to 1982, including 22 matches in the First Division in 1982.\n\nAabrekk started his coaching career in his home county, Sogn og Fjordane, at lower-league clubs Eid, Sandane, and Stryn, before becoming head coach of Sogndal. In 1990, his first season with them, Sogndal was promoted to the Tippeligaen in spite of a 5–0 loss against Bryne in their first home match. He then served as head coach of Tromsø in the Tippeligaen from 1993 to 1995.\n\nIn 1995, during a match against Bodø/Glimt, Aabrekk tried to stall for time by acting as if he were injured after he was run down by Bodø/Glimt's physiotherapist Truls Karlsen. Although uninjured, he was taken by ambulance to hospital. In an interview with \"Verdens Gang\", he excused his behavior by saying, \"I felt I was lying on the ground a bit too long, and I did not dare to get up so I stayed on the ground. I lay there and was thinking, 'What's going on?' It was not planned, and it was definitely not very smart. I would rather call it a blackout.\" This episode, according to the Norwegian newspaper \"Dagbladet\", was the third worst case of acting in the history of football.\n\nAabrekk later worked for the Football Association of Norway as a coordinator for youth development in Western Norway. He also served as Nils Johan Semb's assistant coach for the Norwegian under-21 team from 1992 to 1998, and when Semb succeeded Egil Olsen as head coach of the Norwegian national team in 1998, Aabrekk became assistant coach of the national senior team. At the same time, he worked as Kjell Tennfjord's assistant in Brann, and in the summer of 1998 he became their head coach. After leading Brann to the finals of the 1999 Norwegian Cup, where they lost 2–0 against Rosenborg, he received a one-year leave of absence to work full-time as assistant coach of the national team during their preparations for the UEFA Euro 2000 football championship. He resigned from his job at Brann without returning, because Brann had already hired Teitur Thordarson as their head coach and offered Aabrekk a position in their youth department, a job Brann claimed it was impossible to combine with the position of assistant coach of the national team.\n\nIn 2002 Aabrekk joined Sogndal as Torbjørn Glomnes's assistant coach and saved the team from relegation. On 17 September 2002 Haugesund announced that Aabrekk would join them as head coach on 1 December. In a match against Start on 19 October 2003, Aabrekk told referee Bjørn Hansen that he was \"bought and paid for\" twice after Aabrekk was dismissed from the bench. He was at Haugesund until the end of the 2005 season, when he joined Vålerenga's coaching staff along with Egil Olsen and Petter Myhre. On 27 July 2007 Myhre resigned as head coach of Vålerenga and Aabrekk was appointed head coach as caretaker. Following two wins and a draw in the next three matches, Vålerenga gave him the job until the end of the season.\n\nIn 2008 Aabrekk was hired by Brann as a scout. Halfway through the season, the club wanted Aabrekk and the assistant coach, Espen Steffensen, to switch jobs. Steffensen did not accept this and was fired, and Aabrekk became the new assistant coach of Brann, against the wishes of head coach Mons Ivar Mjelde. Following Brann's bad performances and local press criticism of the lack of cooperation between Mjelde and Aabrekk, Mjelde announced on 7 October 2008 that he would resign as head coach at the end of the season. At the same time, the club announced that Aabrekk would return immediately to his job as scout, a position he kept until the contract expired.\n\nAabrekk was then hired as head coach of his old club, Sogndal from 1 January 2010, succeeding Karl Oskar Emberland. In his first season there, the team won promotion to the Tippeligaen, and in the next they avoided relegation. Following the 2011 season, Aabrekk retired. In his last match, he was named \"Best Sogndal coach ever\".\n\nAabrekk was hired as manager for Aalesunds FK before the 2015 season, but sacked four matches into the season, when his club was last in the league.\n"}
{"id": "18896", "url": "https://en.wikipedia.org/wiki?curid=18896", "title": "Human spaceflight", "text": "Human spaceflight\n\nHuman spaceflight (also referred to as crewed spaceflight or manned spaceflight) is space travel with a crew or passengers aboard the spacecraft. Spacecraft carrying people may be operated directly, by human crew, or it may be either remotely operated from ground stations on Earth or be autonomous, able to carry out a specific mission with no human involvement.\n\nThe first human spaceflight was launched by the Soviet Union on 12 April 1961 as a part of the Vostok program, with cosmonaut Yuri Gagarin aboard. Humans have been continuously present in space for on the International Space Station. All early human spaceflight was crewed, where at least some of the passengers acted to carry out tasks of piloting or operating the spacecraft. After 2015, several human-capable spacecraft are being explicitly designed with the ability to operate autonomously.\n\nSince the retirement of the US Space Shuttle in 2011, only Russia and China have maintained human spaceflight capability with the Soyuz program and Shenzhou program. Currently, all expeditions to the International Space Station use Soyuz vehicles, which remain attached to the station to allow quick return if needed. The United States is developing commercial crew transportation to facilitate domestic access to ISS and low Earth orbit, as well as the Orion vehicle for beyond-low Earth orbit applications.\n\nWhile spaceflight has typically been a government-directed activity, commercial spaceflight has gradually been taking on a greater role. The first private human spaceflight took place on 21 June 2004, when SpaceShipOne conducted a suborbital flight, and a number of non-governmental companies have been working to develop a space tourism industry. NASA has also played a role to stimulate private spaceflight through programs such as Commercial Orbital Transportation Services (COTS) and Commercial Crew Development (CCDev). With its 2011 budget proposals released in 2010, the Obama administration moved towards a model where commercial companies would supply NASA with transportation services of both people and cargo transport to low Earth orbit. The vehicles used for these services could then serve both NASA and potential commercial customers. Commercial resupply of ISS began two years after the retirement of the Shuttle, and commercial crew launches could begin by 2018.\n\nHuman spaceflight capability was first developed during the Cold War between the United States and the Soviet Union (USSR), which developed the first intercontinental ballistic missile rockets to deliver nuclear weapons. These rockets were large enough to be adapted to carry the first artificial satellites into low Earth orbit. After the first satellites were launched in 1957 and 1958, the US worked on Project Mercury to launch men singly into orbit, while the USSR secretly pursued the Vostok program to accomplish the same thing. The USSR launched the first human in space, Yuri Gagarin, into a single orbit in Vostok 1 on a Vostok 3KA rocket, on 12 April 1961. The US launched its first astronaut, Alan Shepard, on a suborbital flight aboard \"Freedom 7\" on a Mercury-Redstone rocket, on 5 May 1961. Unlike Gagarin, Shepard manually controlled his spacecraft's attitude, and landed inside it. The first American in orbit was John Glenn aboard \"Friendship 7\", launched 20 February 1962 on a Mercury-Atlas rocket. The USSR launched five more cosmonauts in Vostok capsules, including the first woman in space, Valentina Tereshkova aboard Vostok 6 on 16 June 1963. The US launched a total of two astronauts in suborbital flight and four into orbit through 1963.\n\nUS President John F. Kennedy raised the stakes of the Space Race by setting the goal of landing a man on the Moon and returning him safely by the end of the 1960s. The US started the three-man Apollo program in 1961 to accomplish this, launched by the Saturn family of launch vehicles, and the interim two-man Project Gemini in 1962, which flew 10 missions launched by Titan II rockets in 1965 and 1966. Gemini's objective was to support Apollo by developing American orbital spaceflight experience and techniques to be used in the Moon mission.\n\nMeanwhile, the USSR remained silent about their intentions to send humans to the Moon, and proceeded to stretch the limits of their single-pilot Vostok capsule into a two- or three-person Voskhod capsule to compete with Gemini. They were able to launch two orbital flights in 1964 and 1965 and achieved the first spacewalk, made by Alexei Leonov on Voskhod 2 on 8 March 1965. But Voskhod did not have Gemini's capability to maneuver in orbit, and the program was terminated. The US Gemini flights did not accomplish the first spacewalk, but overcame the early Soviet lead by performing several spacewalks and solving the problem of astronaut fatigue caused by overcoming the lack of gravity, demonstrating up to two weeks endurance in a human spaceflight, and the first space rendezvous and dockings of spacecraft.\n\nThe US succeeded in developing the Saturn V rocket necessary to send the Apollo spacecraft to the Moon, and sent Frank Borman, James Lovell, and William Anders into 10 orbits around the Moon in Apollo 8 in December 1968. In July 1969, Apollo 11 accomplished Kennedy's goal by landing Neil Armstrong and Buzz Aldrin on the Moon 21 July and returning them safely on 24 July along with Command Module pilot Michael Collins. A total of six Apollo missions landed 12 men to walk on the Moon through 1972, half of which drove electric powered vehicles on the surface. The crew of Apollo 13, Lovell, Jack Swigert, and Fred Haise, survived a catastrophic in-flight spacecraft failure and returned to Earth safely without landing on the Moon.\nMeanwhile, the USSR secretly pursued human lunar orbiting and landing programs. They successfully developed the three-person Soyuz spacecraft for use in the lunar programs, but failed to develop the N1 rocket necessary for a human landing, and discontinued the lunar programs in 1974. On losing the Moon race, they concentrated on the development of space stations, using the Soyuz as a ferry to take cosmonauts to and from the stations. They started with a series of Salyut sortie stations from 1971 to 1986.\n\nAfter the Apollo program, the US launched the Skylab sortie space station in 1973, manning it for 171 days with three crews aboard Apollo spacecraft. President Richard Nixon and Soviet Premier Leonid Brezhnev negotiated an easing of relations known as détente, an easing of Cold War tensions. As part of this, they negotiated the Apollo-Soyuz Test Project, in which an Apollo spacecraft carrying a special docking adapter module rendezvoused and docked with Soyuz 19 in 1975. The American and Russian crews shook hands in space, but the purpose of the flight was purely diplomatic and symbolic.\nNixon appointed his Vice President Spiro Agnew to head a Space Task Group in 1969 to recommend follow-on human spaceflight programs after Apollo. The group proposed an ambitious Space Transportation System based on a reusable Space Shuttle which consisted of a winged, internally fueled orbiter stage burning liquid hydrogen, launched by a similar, but larger kerosene-fueled booster stage, each equipped with airbreathing jet engines for powered return to a runway at the Kennedy Space Center launch site. Other components of the system included a permanent modular space station, reusable space tug and nuclear interplanetary ferry, leading to a human expedition to Mars as early as 1986, or as late as 2000, depending on the level of funding allocated. However, Nixon knew the American political climate would not support Congressional funding for such an ambition, and killed proposals for all but the Shuttle, possibly to be followed by the space station. Plans for the Shuttle were scaled back to reduce development risk, cost, and time, replacing the piloted flyback booster with two reusable solid rocket boosters, and the smaller orbiter would use an expendable external propellant tank to feed its hydrogen-fueled main engines. The orbiter would have to make unpowered landings.\nThe two nations continued to compete rather than cooperate in space, as the US turned to developing the Space Shuttle and planning the space station, dubbed \"Freedom\". \nThe USSR launched three Almaz military sortie stations from 1973 to 1977, disguised as Salyuts. They followed Salyut with the development of \"Mir\", the first modular, semi-permanent space station, the construction of which took place from 1986 to 1996. \"Mir\" orbited at an altitude of , at a 51.6° inclination. It was occupied for 4,592 days, and made a controlled reentry in 2001.\nThe Space Shuttle started flying in 1981, but the US Congress failed to approve sufficient funds to make \"Freedom\" a reality. A fleet of four shuttles was built: \"Columbia\", \"Challenger\", \"Discovery\", and \"Atlantis\". A fifth shuttle, \"Endeavour\", was built to replace \"Challenger\", which was destroyed in an accident during launch that killed 7 astronauts on 28 January 1986. Twenty-two Shuttle flights carried a European Space Agency sortie space station called Spacelab in the payload bay from 1983 to 1998.\n\nThe USSR copied the reusable Space Shuttle orbiter, which it called \"Buran\". It was designed to be launched into orbit by the expendable Energia rocket, and capable of robotic orbital flight and landing. Unlike the US Shuttle, \"Buran\" had no main rocket engines, but like the Shuttle used its orbital maneuvering engines to perform its final orbital insertion. A single unmanned orbital test flight was successfully made in November 1988. A second test flight was planned by 1993, but the program was cancelled due to lack of funding and the dissolution of the Soviet Union in 1991. Two more orbiters were never completed, and the first one was destroyed in a hangar roof collapse in May 2002.\n\nThe dissolution of the Soviet Union in 1991 brought an end to the Cold War and opened the door to true cooperation between the US and Russia. The Soviet Soyuz and Mir programs were taken over by the Russian Federal Space Agency, now known as the Roscosmos State Corporation. The Shuttle-Mir Program included American Space Shuttles visiting the \"Mir\" space station, Russian cosmonauts flying on the Shuttle, and an American astronaut flying aboard a Soyuz spacecraft for long-duration expeditions aboard \"Mir\".\n\nIn 1993, President Bill Clinton secured Russia's cooperation in converting the planned Space Station \"Freedom\" into the International Space Station (ISS). Construction of the station began in 1998. The station orbits at an altitude of and an inclination of 51.65°.\n\nThe Space Shuttle was retired in 2011 after 135 orbital flights, several of which helped assemble, supply, and crew the ISS. \"Columbia\" was destroyed in another accident during reentry, which killed 7 astronauts on 1 February 2003.\n\nAfter Russia's launch of Sputnik 1 in 1957, Chairman Mao Zedong intended to place a Chinese satellite in orbit by 1959 to celebrate the 10th anniversary of the founding of the People's Republic of China (PRC), However, China did not successfully launch its first satellite until 24 April 1970. Mao and Premier Zhou Enlai decided on 14 July 1967, that the PRC should not be left behind, and started China's own human spaceflight program. The first attempt, the Shuguang spacecraft copied from the US Gemini, was cancelled on 13 May 1972.\nChina later designed the Shenzhou spacecraft resembling the Russian Soyuz, and became the third nation to achieve independent human spaceflight capability by launching Yang Liwei on a 21-hour flight aboard Shenzhou 5 on 15 October 2003. China launched the Tiangong-1 space station on 29 September 2011, and two sortie missions to it: Shenzhou 9 16–29 June 2012, with China's first female astronaut Liu Yang; and Shenzhou 10, 13–26 June 2013. The station was retired on 21 March 2016 and remains in a , 42.77° inclination orbit.\n\nThe European Space Agency began development in 1987 of the Hermes spaceplane, to be launched on the Ariane 5 expendable launch vehicle. The project was cancelled in 1992, when it became clear that neither cost nor performance goals could be achieved. No Hermes shuttles were ever built.\n\nJapan began development in the 1980s of the HOPE-X experimental spaceplane, to be launched on its H-IIA expendable launch vehicle. A string of failures in 1998 led to funding reduction, and the project's cancellation in 2003.\n\nUnder the Bush administration, the Constellation Program included plans for retiring the Shuttle program and replacing it with the capability for spaceflight beyond low Earth orbit. In the 2011 United States federal budget, the Obama administration cancelled Constellation for being over budget and behind schedule while not innovating and investing in critical new technologies. For beyond low Earth orbit human spaceflight NASA is developing the Orion spacecraft to be launched by the Space Launch System. Under the Commercial Crew Development plan, NASA will rely on transportation services provided by the private sector to reach low Earth orbit, such as SpaceX's Falcon 9/Dragon V2, Sierra Nevada Corporation's Dream Chaser, or Boeing's CST-100. The period between the retirement of the shuttle in 2011 and the initial operational capability of new systems in 2017, similar to the gap between the end of Apollo in 1975 and the first space shuttle flight in 1981, is referred to by a presidential Blue Ribbon Committee as the U.S. human spaceflight gap.\n\nSince the early 2000s, a variety of private spaceflight ventures have been undertaken. Several of the companies, including Blue Origin, SpaceX, Virgin Galactic, and Sierra Nevada have explicit plans to advance human spaceflight. , all four of those companies have development programs underway to fly commercial passengers.\n\nA commercial suborbital spacecraft aimed at the space tourism market is being developed by Virgin Galactic called SpaceshipTwo, and could reach space around 2018.\nBlue Origin has begun a multi-year test program of their New Shepard vehicle and carried out six successful uncrewed test flights in 2015–2016. Blue Origin plan to fly \"test passengers\" in Q2 2017, and initiate commercial flights in 2018.\n\nSpaceX and Boeing are both developing passenger-capable orbital space capsules as of 2015, planning to fly NASA astronauts to the International Space Station by 2018. SpaceX will be carrying passengers on Dragon 2 launched on a Falcon 9 launch vehicle. Boeing will be doing it with their CST-100 launched on a United Launch Alliance Atlas V launch vehicle.\nDevelopment funding for these orbital-capable technologies has been provided by a mix of government and private funds, with SpaceX providing a greater portion of total development funding for this human-carrying capability from private investment.\nThere have been no public announcements of commercial offerings for orbital flights from either company, although both companies are planning some flights with their own private, not NASA, astronauts on board.\n\nYuri Gagarin became the first human to orbit the Earth on April 12, 1961.\n\nAlan Shepard became the first American to reach space on Mercury-Redstone 3 on May 5, 1961.\n\nJohn Glenn became the first American to orbit the Earth on February 20, 1962.\n\nValentina Tereshkova became the first woman to orbit the Earth on June 16, 1963.\n\nJoseph A. Walker became the first human to pilot a spaceplane, the X-15 Flight 90, into space on July 19, 1963.\n\nAlexey Leonov became the first human to leave a spacecraft in orbit on March 18, 1965.\n\nFrank Borman, Jim Lovell, and William Anders became the first humans to travel beyond low Earth orbit (LEO) Dec 21–27, 1968, when the Apollo 8 mission took them to 10 orbits around the Moon and back.\n\nNeil Armstrong and Buzz Aldrin became the first humans to land on the Moon on July 20, 1969.\n\nSvetlana Savitskaya became the first woman to walk in space on July 25, 1984.\n\nSally Ride became the first American woman in space in 1983. Eileen Collins was the first female shuttle pilot, and with shuttle mission STS-93 in 1999 she became the first woman to command a U.S. spacecraft.\n\nThe longest single human spaceflight is that of Valeri Polyakov, who left Earth on 8 January 1994, and did not return until 22 March 1995 (a total of 437 days 17 h 58 min 16 s). Sergei Krikalyov has spent the most time of anyone in space, 803 days, 9 hours, and 39 minutes altogether. The longest period of continuous human presence in space is on the International Space Station, exceeding the previous record of almost 10 years (or 3,634 days) held by Mir, spanning the launch of Soyuz TM-8 on 5 September 1989 to the landing of Soyuz TM-29 on 28 August 1999.\n\nYang Liwei became the first human to orbit the Earth as part of the Chinese manned space program on October 15, 2003.\n\nFor many years, only the USSR (later Russia) and the United States had their own astronauts. Citizens of other nations flew in space, beginning with the flight of Vladimir Remek, a Czech, on a Soviet spacecraft on 2 March 1978, in the Interkosmos programme. , citizens from 38 nations (including space tourists) have flown in space aboard Soviet, American, Russian, and Chinese spacecraft.\n\nHuman spaceflight programs have been conducted by the former Soviet Union and current Russian Federation, the United States, the People's Republic of China and by private spaceflight company Scaled Composites.\nSpace vehicles are spacecraft used for transportation between the Earth's surface and outer space, or between locations in outer space. The following space vehicles and spaceports are currently used for launching human spaceflights:\n\nThe following space stations are currently maintained in Earth orbit for human occupation:\n\nNumerous private companies attempted human spaceflight programs in an effort to win the $10 million Ansari X Prize. The first private human spaceflight took place on 21 June 2004, when SpaceShipOne conducted a suborbital flight. SpaceShipOne captured the prize on 4 October 2004, when it accomplished two consecutive flights within one week. SpaceShipTwo, launching from the carrier aircraft White Knight Two, is planned to conduct regular suborbital space tourism.\n\nMost of the time, the only humans in space are those aboard the ISS, whose crew of six spends up to six months at a time in low Earth orbit.\n\nNASA and ESA use the term \"human spaceflight\" to refer to their programs of launching people into space. These endeavors have also been referred to as \"manned space missions,\" though because of gender specificity this is no longer official parlance according to NASA style guides.\n\nIndia has declared it will send humans to space on its orbital vehicle \"Gaganyaan\" by 2022. The Indian Space Research Organisation (ISRO) began work on this project in 2006. The objective is to carry a crew of two to low Earth orbit (LEO) and return them safely for a water-landing at a predefined landing zone. The program is proposed to be implemented in defined phases. Currently, the activities are progressing with a focus on the development of critical technologies for subsystems such as the Crew Module (CM), Environmental Control and Life Support System (ECLSS), Crew Escape System, etc. The department has initiated activities to study technical and managerial issues related to crewed missions. The program envisages the development of a fully autonomous orbital vehicle carrying 2 or 3 crew members to about 300 km low Earth orbit and their safe return.\n\nNASA is developing a plan to land humans on Mars by the 2030s. The first step in this mission begins sometime during 2020, when NASA plans to send an uncrewed craft into deep space to retrieve an asteroid. The asteroid will be pushed into the moon’s orbit, and studied by astronauts aboard Orion, NASA’s first human spacecraft in a generation. Orion’s crew will return to Earth with samples of the asteroid and their collected data. In addition to broadening America’s space capabilities, this mission will test newly developed technology, such as solar electric propulsion, which uses solar arrays for energy and requires ten times less propellant than the conventional chemical counterpart used for powering space shuttles to orbit.\n\nSeveral other countries and space agencies have announced and begun human spaceflight programs by their own technology, Japan (JAXA), Iran (ISA) and Malaysia (MNSA).\n\nA number of spacecraft have been proposed over the decades that might facilitate spaceliner passenger travel. Somewhat analogous to travel by airliner after the middle of the 20th century, these vehicles are proposed to transport a large number of passengers to destinations in space, or to destinations on Earth which travel through space. To date, none of these concepts have been built, although a few vehicles that carry fewer than 10 persons are currently in the flight testing phase of their development process.\n\nOne large spaceliner concept currently in early development is the SpaceX BFR which, in addition to replacing the Falcon 9 and Falcon Heavy launch vehicles in the legacy Earth-orbit market after 2020, has been proposed by SpaceX for long-distance commercial travel on Earth. This is to transport people on point-to-point suborbital flights between two points on Earth in under one hour, also known as \"Earth-to-Earth,\" and carrying 100+ passengers.\n\nSmall spaceplane or small capsule suborbital spacecraft have been under development for the past decade or so and, , at least one of each type are under development. Both Virgin Galactic and Blue Origin are in active development, with the SpaceShipTwo spaceplane and the New Shepard capsule, respectively. Both would carry approximately a half-dozen passengers up to space for a brief time of zero gravity before returning to the same location from where the trip began. XCOR Aerospace had been developing the Lynx single-passenger spaceplane since the 2000s but development was halted in 2017.\n\nThere are two main sources of hazard in space flight: those due to the \"environment\" of space which make it hostile to the human body, and the potential for \"mechanical\" malfunctions of the equipment required to accomplish space flight.\n\nPlanners of human spaceflight missions face a number of safety concerns.\n\nThe immediate needs for breathable air and drinkable water are addressed by the life support system of the spacecraft.\n\nMedical consequences such as possible blindness and bone loss have been associated with human space flight.\n\nOn 31 December 2012, a NASA-supported study reported that spaceflight may harm the brain of astronauts and accelerate the onset of Alzheimer's disease.\n\nIn October 2015, the NASA Office of Inspector General issued a health hazards report related to space exploration, including a human mission to Mars.\n\nOn 2 November 2017, scientists reported that significant changes in the position and structure of the brain have been found in astronauts who have taken trips in space, based on MRI studies. Astronauts who took longer space trips were associated with greater brain changes.\n\nResearchers in 2018 reported, after detecting the presence on the International Space Station (ISS) of five \"Enterobacter bugandensis\" bacterial strains, none pathogenic to humans, that microorganisms on ISS should be carefully monitored to continue assuring a medically healthy environment for astronauts.\n\nMedical data from astronauts in low Earth orbits for long periods, dating back to the 1970s, show several adverse effects of a microgravity environment: loss of bone density, decreased muscle strength and endurance, postural instability, and reductions in aerobic capacity. Over time these deconditioning effects can impair astronauts’ performance or increase their risk of injury.\n\nIn a weightless environment, astronauts put almost no weight on the back muscles or leg muscles used for standing up, which causes them to weaken and get smaller. Astronauts can lose up to twenty per cent of their muscle mass on spaceflights lasting five to eleven days. The consequent loss of strength could be a serious problem in case of a landing emergency. Upon return to Earth from long-duration flights, astronauts are considerably weakened, and are not allowed to drive a car for twenty-one days.\n\nAstronauts experiencing weightlessness will often lose their orientation, get motion sickness, and lose their sense of direction as their bodies try to get used to a weightless environment. When they get back to Earth, or any other mass with gravity, they have to readjust to the gravity and may have problems standing up, focusing their gaze, walking and turning. Importantly, those body motor disturbances after changing from different gravities only get worse the longer the exposure to little gravity. These changes will affect operational activities including approach and landing, docking, remote manipulation, and emergencies that may happen while landing. This can be a major roadblock to mission success.\n\nIn addition, after long space flight missions, male astronauts may experience severe eyesight problems. Such eyesight problems may be a major concern for future deep space flight missions, including a crewed mission to the planet Mars.\n\nWithout proper shielding, the crews of missions beyond low Earth orbit (LEO) might be at risk from high-energy protons emitted by solar flares. Lawrence Townsend of the University of Tennessee and others have studied the most powerful solar flare ever recorded. That flare was seen by the British astronomer Richard Carrington in September 1859. Radiation doses astronauts would receive from a Carrington-type flare could cause acute radiation sickness and possibly even death.\n\nAnother type of radiation, galactic cosmic rays, presents further challenges to human spaceflight beyond low Earth orbit.\n\nThere is also some scientific concern that extended spaceflight might slow down the body’s ability to protect itself against diseases. Some of the problems are a weakened immune system and the activation of dormant viruses in the body. Radiation can cause both short and long term consequences to the bone marrow stem cells which create the blood and immune systems. Because the interior of a spacecraft is so small, a weakened immune system and more active viruses in the body can lead to a fast spread of infection.\n\nDuring long missions, astronauts are isolated and confined into small spaces. Depression, cabin fever and other psychological problems may impact the crew's safety and mission success.\n\nAstronauts may not be able to quickly return to Earth or receive medical supplies, equipment or personnel if a medical emergency occurs. The astronauts may have to rely for long periods on their limited existing resources and medical advice from the ground.\n\nSpace flight requires much higher velocities than ground or air transportation, which in turn requires the use of high energy density propellants for launch, and the dissipation of large amounts of energy, usually as heat, for safe reentry through the Earth's atmosphere.\n\nSince rockets carry the potential for fire or explosive destruction, space capsules generally employ some sort of launch escape system, consisting either of a tower-mounted solid fuel rocket to quickly carry the capsule away from the launch vehicle (employed on Mercury, Apollo, and Soyuz), or else ejection seats (employed on Vostok and Gemini) to carry astronauts out of the capsule and away for individual parachute landing. The escape tower is discarded at some point before the launch is complete, at a point where an abort can be performed using the spacecraft's engines.\n\nSuch a system is not always practical for multiple crew member vehicles (particularly spaceplanes), depending on location of egress hatch(es). When the single-hatch Vostok capsule was modified to become the 2 or 3-person Voskhod, the single-cosmonaut ejection seat could not be used, and no escape tower system was added. The two Voskhod flights in 1964 and 1965 avoided launch mishaps. The Space Shuttle carried ejection seats and escape hatches for its pilot and copilot in early flights, but these could not be used for passengers who sat below the flight deck on later flights, and so were discontinued.\n\nThere have only been two in-flight launch aborts of a crewed flight. The first occurred on Soyuz 18a on 5 April 1975. The abort occurred after the launch escape system had been jettisoned, when the launch vehicle's spent second stage failed to separate before the third stage ignited. The vehicle strayed off course, and the crew separated the spacecraft and fired its engines to pull it away from the errant rocket. Both cosmonauts landed safely. The second occurred on 11 October 2018 with the launch of Soyuz MS-10. Again, both crew members survived. \n\nIn the only use of a launch escape system on a crewed flight, the planned Soyuz T-10a launch on 26 September 1983 was aborted by a launch vehicle fire 90 seconds before liftoff. Both cosmonauts aboard landed safely.\n\nThe only crew fatality during launch occurred on 28 January 1986, when the Space Shuttle \"Challenger\" broke apart 73 seconds after liftoff, due to failure of a solid rocket booster seal which caused separation of the booster and failure of the external fuel tank, resulting in explosion of the fuel. All seven crew members were killed.\n\nThe single pilot of Soyuz 1, Vladimir Komarov was killed when his capsule's parachutes failed during an emergency landing on 24 April 1967, causing the capsule to crash.\n\nThe crew of seven aboard the Space Shuttle \"Columbia\" were killed on reentry after completing a successful mission in space on 1 February 2003. A wing leading edge reinforced carbon-carbon heat shield had been damaged by a piece of frozen external tank foam insulation which broke off and struck the wing during launch. Hot reentry gasses entered and destroyed the wing structure, leading to breakup of the orbiter vehicle.\n\nThere are two basic choices for an artificial atmosphere: either an Earth-like mixture of oxygen in an inert gas such as nitrogen or helium, or pure oxygen, which can be used at lower than standard atmospheric pressure. A nitrogen-oxygen mixture is used in the International Space Station and Soyuz spacecraft, while low-pressure pure oxygen is commonly used in space suits for extravehicular activity.\n\nUse of a gas mixture carries risk of decompression sickness (commonly known as \"the bends\") when transitioning to or from the pure oxygen space suit environment. There have also been instances of injury and fatalities caused by suffocation in the presence of too much nitrogen and not enough oxygen.\n\nA pure oxygen atmosphere carries risk of fire. The original design of the Apollo spacecraft used pure oxygen at greater than atmospheric pressure prior to launch. An electrical fire started in the cabin of Apollo 1 during a ground test at Cape Kennedy Air Force Station Launch Complex 34 on 27 January 1967, and spread rapidly. The high pressure (increased even higher by the fire) prevented removal of the plug door hatch cover in time to rescue the crew. All three, Gus Grissom, Ed White, and Roger Chaffee, were killed. This led NASA to use a nitrogen/oxygen atmosphere before launch, and low pressure pure oxygen only in space.\n\nThe March 1966 Gemini 8 mission was aborted in orbit when an attitude control system thruster stuck in the on position, sending the craft into a dangerous spin which threatened the lives of Neil Armstrong and David Scott. Armstrong had to shut the control system off and use the reentry control system to stop the spin. The craft made an emergency reentry and the astronauts landed safely. The most probable cause was determined to be an electrical short due to a static electricity discharge, which caused the thruster to remain powered even when switched off. The control system was modified to put each thruster on its own isolated circuit.\n\nThe third lunar landing expedition Apollo 13 in April 1970, was aborted and the lives of the crew, James Lovell, Jack Swigert and Fred Haise, were threatened by failure of a cryogenic liquid oxygen tank en route to the Moon. The tank burst when electrical power was applied to internal stirring fans in the tank, causing the immediate loss of all of its contents, and also damaging the second tank, causing the loss of its remaining oxygen in a span of 130 minutes. This in turn caused loss of electrical power provided by fuel cells to the command spacecraft. The crew managed to return to Earth safely by using the lunar landing craft as a \"life boat\". The tank failure was determined to be caused by two mistakes. The tank's drain fitting had been damaged when it was dropped during factory testing. This necessitated use of its internal heaters to boil out the oxygen after a pre-launch test, which in turn damaged the fan wiring's electrical insulation, because the thermostats on the heaters did not meet the required voltage rating due to a vendor miscommunication.\n\nThe crew of Soyuz 11 were killed on June 30, 1971 by a combination of mechanical malfunctions: they were asphyxiated due to cabin decompression following separation of their descent capsule from the service module. A cabin ventilation valve had been jolted open at an altitude of by the stronger than expected shock of explosive separation bolts which were designed to fire sequentially, but in fact had fired simultaneously. The loss of pressure became fatal within about 30 seconds.\n\n, 22 crew members have died in accidents aboard spacecraft. Over 100 others have died in accidents during activity directly related to spaceflight or testing.\n\n\n\n"}
{"id": "832518", "url": "https://en.wikipedia.org/wiki?curid=832518", "title": "Intellectualism", "text": "Intellectualism\n\nIntellectualism denotes the use, development, and exercise of the intellect; the practice of being an intellectual; and the Life of the Mind. In the field of philosophy, “intellectualism” occasionally is synonymous with “rationalism”, that is, knowledge mostly derived from reason and ratiocination. Socially, “intellectualism” negatively connotes: single-mindedness of purpose (“too much attention to thinking”) and emotional coldness (“the absence of affection and feeling”).\n\nIn the view of Socrates (c. 470 – 399 BC), intellectualism allows that “one will do what is right or best just as soon as one truly understands what is right or best”; that virtue is a purely intellectual matter, since virtue and knowledge are familial relatives, which a person accrues and improves with dedication to reason. So defined, Socratic intellectualism became a key philosophic doctrine of Stoicism. The apparent, problematic consequences of this view are “Socratic paradoxes”, such as the view that there is no weakness of will — that no one knowingly does, or seeks to do, evil (moral wrong); that anyone who does, or seeks to do, moral wrong does so involuntarily; and that virtue is knowledge, that there are not many virtues, but that all virtues are one.\n\nContemporary philosophers dispute that Socrates’s conceptions of knowing truth, and of ethical conduct, can be equated with modern, post–Cartesian conceptions of knowledge and of rational intellectualism. As such, Michel Foucault demonstrated, with detailed historical study, that in Classical Antiquity (800 BC – AD 1000), “knowing the truth” is akin to “spiritual knowledge”, in the contemporarily understanding of the concept. Hence, without exclusively concerning the rational intellect, spiritual knowledge is integral to the broader principle of “caring for the self”.\n\nTypically, such care of the self-involved specific ascetic exercises meant to ensure that not only was knowledge of truth memorized, but learned, and then integrated to the self, in the course of transforming oneself into a good person. Therefore, to understand truth meant “intellectual knowledge” requiring one’s integration to the (universal) truth, and authentically living it in one’s speech, heart, and conduct. Achieving that difficult task required continual care of the self, but also meant being someone who embodies truth, and so can readily practice the Classical-era rhetorical device of parrhesia: “to speak candidly, and to ask forgiveness for so speaking”; and, by extension, practice the moral obligation to speak the truth for the common good, even at personal risk. This ancient, Socratic moral philosophic perspective contradicts the contemporary understanding of truth and knowledge as rational undertakings.\n\nMedieval theological intellectualism is a doctrine of divine action, wherein the faculty of intellect precedes, and is superior to, the faculty of the will (\"voluntas intellectum sequitur\"). As such, Intellectualism is contrasted with voluntarism, which proposes the Will as superior to the intellect, and to the emotions; hence, the stance that “according to intellectualism, choices of the Will result from that which the intellect recognizes as good; the will, itself, is determined. For voluntarism, by contrast, it is the Will which identifies which objects are good, and the Will, itself, is indetermined”. From that philosophical perspective and historical context, the Spanish Muslim polymath Averroës (1126–1198) in the 12th century, the Italian Christian theologian Thomas Aquinas (1225–1274), and the German Christian theologian Meister Eckhart (1260–1327) in the 13th century, are recognised intellectualists.\n\n"}
{"id": "59010226", "url": "https://en.wikipedia.org/wiki?curid=59010226", "title": "Kay 9ice", "text": "Kay 9ice\n\nPhilip Kwaku Kissi popularly known as Kay 9ice, was born on May 27, 1990 in Accra, Ghana. a Ghanaian Hiplife and Afrobeats artiste. Kay 9ice currently signed to GHOZT MUSIK a record label based in Ghana & +816 studios.\n\nPhilip hails from Akoefe in the Volta Region of Ghana. Growing up, afrobeats music became his favorite at the junior high school level Ashaiman Government School.\nHe won himself the pleasure to be the first Ghanaian to be introduced by BBC Music and to feature on BBC6 Music Mixtape.\n\nKay 9ice became part of a musical group called Rekx Bois (Real Entertainment Knocked Down Extremely). Due to his hard work musically he had his first big appearance during Flowking Stone's oseikrom shutdown show in Kumasi, Ghana. With his hard work, he got nominated for the median edition for Ashaiman Music Awards 2018 and Ghana Music Awards South Africa for which he got 7 nominations and promising Artist Of The Year 2018 respectively.\n\nHe won the Afro-Song Of The Year category in The median edition Of the Ashaiman Music Awards 2018 and was also nominated for ghana Music awards south africa. He is also the first ghanaian to be feature on BBC 6 Music Introducing Mixtape.\n"}
{"id": "51276804", "url": "https://en.wikipedia.org/wiki?curid=51276804", "title": "Khalid El-Aabidi", "text": "Khalid El-Aabidi\n\nKhalid El-Aabidi (born 14 September 1995) is a Moroccan Olympic weightlifter. He represented his country at the 2016 Summer Olympics.\n"}
{"id": "58420452", "url": "https://en.wikipedia.org/wiki?curid=58420452", "title": "List of countries by number of births", "text": "List of countries by number of births\n\nThe following list sorts countries by the total projected number of births. The list is sourced from the United Nations World Population Prospects. Figures are from the 2017 revision of the United Nations World Population Prospects report, for the period 2015-2020, using the medium assumption. All figures are rounded and given in thousands.\n\n"}
{"id": "30526821", "url": "https://en.wikipedia.org/wiki?curid=30526821", "title": "MASON (Java)", "text": "MASON (Java)\n\nMASON is a multi-agent simulation environment developed in Java.\n\nMASON is developed at George Mason University's Evolutionary Computation Laboratory in conjunction with the GMU Center for Social Complexity. First released in 2003, the environment continues to be maintained and kept up to date. The name, as well as referring to the parent institution, derives from the acronym Multi-Agent Simulator Of Neighborhoods (or Networks).\n\nMASON development started within the Java.net environment, then moved to Google Code and is now at GitHub.\n\nWhilst MASON is less extensive than other similar libraries it is designed with simplicity and execution speed as a priority.\n\nApplets developed using MASON include Craig Reynolds' Boids algorithm, \"Balls and Bands\", a simulation of Hooke's Law, an L-system generator, Conway's Game of Life, Sugarscape and autonomous multi-robot systems.\n\nMASON may be used with the Eclipse Integrated development environment.\n\n"}
{"id": "33591982", "url": "https://en.wikipedia.org/wiki?curid=33591982", "title": "Miro Aaltonen", "text": "Miro Aaltonen\n\nMiro Aaltonen (born 7 June 1993) is a Finnish professional ice hockey forward. He is currently playing with HC Vityaz of the Kontinental Hockey League (KHL). Aaltonen was selected by Atlant Moscow Oblast in the 2nd round (45th overall) of the 2011 KHL Junior Draft, and he was also selected by the Anaheim Ducks in the 6th round (177th overall) of the 2013 NHL Entry Draft.\n\nFollowing the 2016–17 season, after recording a career best 44 points in 59 games after his first season with HC Vityaz in the Kontinental Hockey League, he signed an entry-level contract with the Toronto Maple Leafs on March 17, 2017.\n\nDespite a strong push for a roster spot as the Leafs' fourth-line center, he was assigned to the Maple Leafs American Hockey League affiliate, the Toronto Marlies for the 2017–18 season. In adapting to his first North American season, Aaltonen established himself among the offensive leaders with the Marlies, contributing with 20 goals and 43 points in 64 regular season games. Unable to earn a call up to the NHL, Aaltonen continued in the post-season with the Marlies, helping claim the club's first Calder Cup in posting 13 points in 20 games.\n\nAs an impending restricted free agent from the Maple Leafs but unable to make his NHL debut, Aaltonen opted to return to the KHL on a contract with former Russian club, Vityaz on July 1, 2018.\n\n"}
{"id": "144147", "url": "https://en.wikipedia.org/wiki?curid=144147", "title": "Miscarriage", "text": "Miscarriage\n\nMiscarriage, also known as spontaneous abortion and pregnancy loss, is the natural death of an embryo or fetus before it is able to survive independently. Some use the cutoff of 20 weeks of gestation, after which fetal death is known as a stillbirth. The most common symptom of a miscarriage is vaginal bleeding with or without pain. Sadness, anxiety and guilt often occur afterwards. Tissue and clot-like material may leave the uterus and pass through and out of the vagina. When a woman keeps having miscarriages, infertility is present.\nRisk factors for miscarriage include an older parent, previous miscarriage, exposure to tobacco smoke, obesity, diabetes, thyroid problems, and drug or alcohol use. About 80% of miscarriages occur in the first 12 weeks of pregnancy (the first trimester). The underlying cause in about half of cases involves chromosomal abnormalities. Diagnosis of a miscarriage may involve checking to see if the cervix is open or closed, testing blood levels of human chorionic gonadotropin (hCG), and an ultrasound. Other conditions that can produce similar symptoms include an ectopic pregnancy and implantation bleeding.\nPrevention is occasionally possible with good prenatal care. Avoiding drugs, alcohol, infectious diseases, and radiation may decrease the risk of miscarriage. No specific treatment is usually needed during the first 7 to 14 days. Most miscarriages will complete without additional interventions. Occasionally the medication misoprostol or a procedure such as vacuum aspiration is used to remove the remaining tissue. Women who have a blood type of rhesus negative (Rh negative) may require Rho(D) immune globulin. Pain medication may be beneficial. Emotional support may help with negative emotions.\nMiscarriage is the most common complication of early pregnancy. Among women who know they are pregnant, the miscarriage rate is roughly 10% to 20%, while rates among all fertilisation is around 30% to 50%. In those under the age of 35 the risk is about 10% while it is about 45% in those over the age of 40. Risk begins to increase around the age of 30. About 5% of women have two miscarriages in a row. Some recommend not using the term \"abortion\" in discussions with those experiencing a miscarriage in an effort to decrease distress.\nSigns of a miscarriage include vaginal spotting, abdominal pain, cramping, and fluid, blood clots, and tissue passing from the vagina. Bleeding can be a symptom of miscarriage, but many women also have bleeding in early pregnancy and don't miscarry. Bleeding during pregnancy may be referred to as a threatened miscarriage. Of those who seek clinical treatment for bleeding during pregnancy, about half will miscarry. Miscarriage may be detected during an ultrasound exam, or through serial human chorionic gonadotropin (HCG) testing.\n\nMiscarriage may occur for many reasons, not all of which can be identified. Risk factors are those things that increase the likelihood of having a miscarriage but don't necessarily cause a miscarriage. Up to 70 conditions, infections, medical procedures, lifestyle factors, occupational exposures, chemical exposure, and shift work are associated with increased risk for miscarriage. Some of these risks include endocrine, genetic, uterine, or hormonal abnormalities, reproductive tract infections, and tissue rejection caused by an autoimmune disorder.\n\nMost clinically apparent miscarriages (two-thirds to three-quarters in various studies) occur during the first trimester. About 30% to 40% of all fertilized eggs miscarry, often before the pregnancy is known. The embryo typically dies before the pregnancy is expelled; bleeding into the decidua basalis and tissue necrosis causes uterine contractions to expel the pregnancy. Early miscarriages can be due to a developmental abnormality of the placenta or other embryonic tissues. In some instances an embryo does not form but other tissues do. This has been called a \"blighted ovum\".\n\nSuccessful implantation of the zygote into the uterus is most likely 8 to 10 days after conception. If the zygote has not implanted by day 10, implantation becomes increasingly unlikely in subsequent days.\n\nA chemical pregnancy is a pregnancy that was detected by testing but ends in miscarriage before or around the time of the next expected period.\n\nChromosomal abnormalities are found in more than half of embryos miscarried in the first 13 weeks. Half of embryonic miscarriages (25% of all miscarriages) have an aneuploidy (abnormal number of chromosomes). Common chromosome abnormalities found in miscarriages include autosomal trisomy (22-32%), monosomy X (5-20%), triploidy (6-8%), tetraploidy (2-4%), or other structural chromosomal abnormalities (2%). Genetic problems are more likely to occur with older parents; this may account for the higher rates observed in older women.\n\nThere is no evidence that progesterone given in the first trimester reduces the risk of miscarriage, and luteal phase progesterone deficiency may or may not be a contributing factor to miscarriage.\n\nSecond trimester losses may be due to maternal factors such as uterine malformation, growths in the uterus (fibroids), or cervical problems. These conditions also may contribute to premature birth. Unlike first-trimester miscarriages, second-trimester miscarriages are less likely to be caused by a genetic abnormality; chromosomal aberrations are found in a third of cases. Infection during the third trimester can cause a miscarriage.\n\nThe age of the pregnant woman is a significant risk factor. Miscarriage rates increase steadily with age, with more substantial increases after age 35. In those under the age of 35 the risk is about 10% while it is about 45% in those over the age of 40. Risk begins to increase around the age of 30. Paternal age is associated with increased risk.\n\nNot only is obesity associated with miscarriage, it can result in sub-fertility and other adverse pregnancy outcomes. Recurrent miscarriage is also related to obesity. Women with bulimia nervosa and anorexia nervosa may have a greater risk for miscarriage. Nutrient deficiencies have not been found to impact miscarriage rates but hyperemesis gravidarum sometimes precedes a miscarriage.\nCaffeine consumption also has been correlated to miscarriage rates, at least at higher levels of intake. However, such higher rates have been found to be statistically significant only in certain circumstances.\n\nVitamin supplementation has generally not shown to be effective in preventing miscarriage. Chinese traditional medicine has not been found to prevent miscarriage.\n\nDisorders of the thyroid may affect pregnancy outcomes. Related to this, iodine deficiency is strongly associated with an increased risk of miscarriage. The risk of miscarriage is increased in those with poorly controlled insulin-dependent diabetes mellitus. Women with well-controlled diabetes have the same risk of miscarriage as those without diabetes.\n\nIngesting food that has been contaminated with listeriosis, toxoplasmosis, and salmonella is associated with an increased risk of miscarriage.\n\nAmniocentesis and chorionic villus sampling are procedures conducted to assess the fetus. A sample of amniotic fluid is obtained by the insertion of a needle through the abdomen and into the uterus. Chorionic villus sampling is a similar procedure with a sample of tissue removed rather than fluid. These procedures are not associated with pregnancy loss during the second trimester but they are associated with miscarriages and birth defects in the first trimester. Miscarriage caused by invasive prenatal diagnosis (chorionic villus sampling (CVS) and amniocentesis) is rare (about 1%).\n\nThe effects of surgery on pregnancy are not well-known including the effects of bariatric surgery. Abdominal and pelvic surgery are not risk factors in miscarriage. Ovarian tumors and cysts that are removed have not been found to increase the risk of miscarriage. The exception to this is the removal of the corpus luteum from the ovary. This can cause fluctuations in the hormones necessary to maintain the pregnancy.\n\nImmunizations have not been found to cause miscarriage. There is no significant association between antidepressant medication exposure and spontaneous abortion. The risk of miscarriage is not likely decreased by discontinuing SSRI prior to pregnancy. Some available data suggest that there is a small increased risk of miscarriage for women taking any antidepressant, though this risk becomes less statistically significant when excluding studies of poor quality.\n\nMedicines that increase the risk of miscarriage include:\n\n\nIonizing radiation levels given to a woman during cancer treatment cause miscarriage. Exposure can also impact fertility. The use of chemotherapeutic drugs used to treat childhood cancer increases the risk of miscarriage.\n\nSeveral intercurrent diseases in pregnancy can potentially increase the risk of miscarriage, including diabetes, polycystic ovary syndrome (PCOS), hypothyroidism, certain infectious diseases, and autoimmune diseases. PCOS may increase the risk of miscarriage. Two studies suggested treatment with the drug metformin significantly lowers the rate of miscarriage in women with PCOS, but the quality of these studies has been questioned. Metformin treatment in pregnancy has not been shown to be safe. In 2007 the Royal College of Obstetricians and Gynaecologists also recommended against use of the drug to prevent miscarriage. Thrombophilias or defects in coagulation and bleeding were once thought to be a risk in miscarriage but have been subsequently questioned.\nSevere cases of hypothyroidism increase the risk of miscarriage. The effect of milder cases of hypothyroidism on miscarriage rates has not been established. A condition called luteal phase defect (LPD) is a failure of the uterine lining to be fully prepared for pregnancy. This can keep a fertilized egg from implanting or result in miscarriage.\n\n\"Mycoplasma genitalium\" infection is associated with increased risk of preterm birth and miscarriage.\n\nInfections can increase the risk of a miscarriage: rubella (German measles), cytomegalovirus, bacterial vaginosis, HIV, chlamydia, gonorrhoea, syphilis, and malaria.\n\nAutoimmunity is possible cause of recurrent or late-term miscarriages. In the case of an autoimmune-induced miscarriages the woman's body attacks the growing fetus or prevents normal pregnancy progression. Autoimmune disease may cause genetic abnormalities in embryos which in turn may lead to miscarriage. As an example, Celiac disease increases the risk of miscarriage by an odds ratio of approximately 1.4. A disruption in normal immune function can lead to the formation of antiphospholipid antibody syndrome. This will effect the ability to continue the pregnancy and if a woman has repeated miscarriages, she can be tested for it. Approximately 15% of recurrent miscarriages are related to immunologic factors. The presence of anti-thyroid autoantibodies is associated with an increased risk with an odds ratio of 3.73 and 95% confidence interval 1.8–7.6. Having Lupus also increases the risk for miscarriage.\n\nFifteen percent of women who have experienced three or more recurring miscarriages have some anatomical defect that prevents the pregnancy from being carried for the entire term. The structure of the uterus has an effect on the ability to carry a child to term. Anatomical differences are common and can be congenital. \nIn some women, cervical incompetence or cervical insufficiency occurs with the inability of the cervix to stay closed during the entire pregnancy. It does not cause first trimester miscarriages. In the second trimester it is associated with an increased risk of miscarriage. It is identified after a premature birth has occurred at about 16–18 weeks into the pregnancy. During the second trimester, major trauma can result in a miscarriage.\n\nTobacco (cigarette) smokers have an increased risk of miscarriage. There is an increased risk regardless of which parent smokes, though the risk is higher when the gestational mother smokes.\n\nNausea and vomiting of pregnancy (NVP, or morning sickness) are associated with a decreased risk. Several possible causes have been suggested for morning sickness but there is still no agreement. NVP may represent a defense mechanism which discourages the mother's ingestion of foods that are harmful to the fetus; according to this model, a lower frequency of miscarriage would be an expected consequence of the different food choices made by women experiencing NVP.\n\nChemical and occupational exposures may have some effect in pregnancy outcomes. A cause and effect relationship almost can never be established. Those chemicals that are implicated in increasing the risk for miscarriage are DDT, lead, formaldehyde, arsenic, benzene and ethylene oxide. Video display terminals and ultrasound have not been found to have an effect on the rates of miscarriage. In dental offices where nitrous oxide is used with the absence of anesthetic gas scavenging equipment, there is a greater risk of miscarriage. For women who work with cytotoxic antineoplastic chemotherapeutic agents there is a small increased risk of miscarriage. No increased risk for cosmetologists has been found.\n\nAlcohol increases the risk of miscarriage. Progesterone has not been found to be effective in preventing miscarriage. Cocaine use increases the rate of miscarriage. Some infections have been associated with miscarriage. These include \"Ureaplasma urealyticum\", \"Mycoplasma hominis\", group B streptococci, HIV-1, and syphilis. Infections of \"Chlamydia trachomatis,\" \"Camphylobacter fetus\", and \"Toxoplasma gondii\" have not been found to be linked to miscarriage.\n\nIn the case of blood loss, pain, or both, transvaginal ultrasound is performed. If a viable intrauterine pregnancy is not found with ultrasound, blood tests (serial βHCG tests) can be performed to rule out ectopic pregnancy, which is a life-threatening situation.\n\nIf hypotension, tachycardia, and anemia are discovered, exclusion of an ectopic pregnancy is important.\n\nA miscarriage may be confirmed by an obstetric ultrasound and by the examination of the passed tissue. When looking for microscopic pathologic symptoms, one looks for the products of conception. Microscopically, these include villi, trophoblast, fetal parts, and background gestational changes in the endometrium. When chromosomal abnormalities are found in more than one miscarriage, genetic testing of both parents may be done.\n\nA review article in The New England Journal of Medicine based on a consensus meeting of the Society of Radiologists in Ultrasound in America (SRU) has suggested that miscarriage should be diagnosed only if any of the following criteria are met upon ultrasonography visualization:\nA threatened miscarriage describes any bleeding during pregnancy, prior to viability, that has yet to be assessed.. At investigation it may be found that the fetus remains viable and the pregnancy continues without further problems.\n\nAn anembryonic pregnancy (also called an \"empty sac\" or \"blighted ovum\") is a condition where the gestational sac develops normally, while the embryonic part of the pregnancy is either absent or stops growing very early. This accounts for approximately half of miscarriages. All other miscarriages are classified as embryonic miscarriages, meaning that there is an embryo present in the gestational sac. Half of embryonic miscarriages have aneuploidy (an abnormal number of chromosomes).\n\nAn inevitable miscarriage occurs when the cervix has already dilated, but the fetus has yet to be expelled. This usually will progress to a complete miscarriage. The fetus may or may not have cardiac activity.\n\nA missed miscarriage is when the embryo or fetus has died, but a miscarriage has not yet occurred. It is also referred to as delayed miscarriage, silent miscarriage, or missed abortion.\n\nA septic miscarriage occurs when the tissue from a missed or incomplete miscarriage becomes infected, which carries the risk of spreading infection (septicaemia) and can be fatal.\n\nRecurrent miscarriage (\"recurrent pregnancy loss\" (RPL) or \"habitual abortion\") is the occurrence of multiple consecutive miscarriages; the exact number used to diagnose recurrent miscarriage varies. If the proportion of pregnancies ending in miscarriage is 15% and assuming that miscarriages are independent events, then the probability of two consecutive miscarriages is 2.25% and the probability of three consecutive miscarriages is 0.34%. The occurrence of recurrent pregnancy loss is 1%. A large majority (85%) of those who have had two miscarriages will conceive and carry normally afterward.\n\nThe physical symptoms of a miscarriage vary according to the length of pregnancy, though most miscarriages cause pain or cramping. The size of blood clots and pregnancy tissue that are passed become larger with longer gestations. After 13 weeks' gestation, there is a higher risk of placenta retention.\n\nPrevention of a miscarriage can sometimes be accomplished by decreasing risk factors. This may include good prenatal care, avoiding drugs and alcohol, preventing infectious diseases, and avoiding x-rays. Identifying the cause of the miscarriage may help prevent future pregnancy loss, especially in cases of recurrent miscarriage. Often there is little a person can do to prevent a miscarriage. Vitamin supplementation before or during pregnancy has not been found to affect the risk of miscarriage.\n\nPreventing a miscarriage in subsequent pregnancies may be enhanced with assessments of:\n\nMaintaining a healthy weight and good pre-natal care can reduce the risk of miscarriage. Some risk factors can be minimized by avoiding the following:\n\nWomen who miscarry early in their pregnancy usually do not require any subsequent medical treatment but they can benefit from support and counseling. Most early miscarriages will complete on their own; in other cases, medication treatment or aspiration of the products of conception can be used to remove remaining tissue. While bed rest has been advocated to prevent miscarriage, this has not been found to be of benefit. Those who are or who have experienced an abortion benefit from the use of careful medical language. Significant distress can often be managed by the ability of the clinician to clearly explain terms without suggesting that the woman or couple are somehow to blame.\n\nEvidence to support Rho(D) immune globulin after a spontaneous miscarriage is unclear. In the UK, Rho(D) immune globulin is recommended in Rh-negative women after 12 weeks gestational age and before 12 weeks gestational age in those who need surgery or medication to complete the miscarriage.\n\nNo treatment is necessary for a diagnosis of complete miscarriage (so long as ectopic pregnancy is ruled out). In cases of an incomplete miscarriage, empty sac, or missed abortion there are three treatment options: watchful waiting, medical management, and surgical treatment. With no treatment (watchful waiting), most miscarriages (65–80%) will pass naturally within two to six weeks. This treatment avoids the possible side effects and complications of medications and surgery, but increases the risk of mild bleeding, need for unplanned surgical treatment, and incomplete miscarriage. Medical treatment usually consists of using misoprostol (a prostaglandin) to contract the uterus, expelling remaining tissue out of the cervix. This works within a few days in 95% of cases. Vacuum aspiration or sharp curettage can be used, though vacuum aspiration is lower-risk and more common.\n\nIn delayed or incomplete miscarriage, treatment depends on the amount of tissue remaining in the uterus. Treatment can include surgical removal of the tissue with vacuum aspiration or misoprostol. Studies looking at the methods of anaesthesia for surgical management of incomplete miscarriage have not shown that any adaptation from normal practice is beneficial. Some organizations recommend delaying sexual relations immediately after a miscarriage to prevent infection. However, there is not sufficient evidence for the routine use of antibiotic to try to avoid infection in incomplete abortion.\n\nAn induced abortion may be performed by a physician for women who do not want to continue the pregnancy. Self-induced abortion performed by a woman or non-medical personnel is extremely dangerous and is still a cause of maternal mortality in some countries. In some locales it is illegal or carries heavy social stigma.\n\nOrganizations exist that provide information and counseling to help those who have had a miscarriage. Family and friends often conduct a memorial or burial service. Hospitals also can provide support and help memorialize the event. Depending on locale others desire to have a private ceremony. Providing appropriate support with frequent discussions and sympathetic counseling are part of evaluation and treatment. Those who experience unexplained miscarriage can be treated with emotional support.\n\nEvery woman's personal experience of miscarriage is different, and women who have more than one miscarriage may react differently to each event.\n\nIn Western cultures since the 1980s, medical providers assume that experiencing a miscarriage \"is a major loss for all pregnant women\". A miscarriage can result in anxiety, depression or stress for those involved. It can have an effect on the whole family. Many of those experiencing a miscarriage go through a grieving process. \"Prenatal attachment\" often exists that can be seen as parental sensitivity, love and preoccupation directed toward the unborn child. Serious emotional impact is usually experienced immediately after the miscarriage. Some may go through the same loss when an ectopic pregnancy is terminated. In some, the realization of the loss can take weeks. Providing family support to those experiencing the loss can be challenging because some find comfort in talking about the miscarriage while others may find the event painful to discuss. The father can have the same sense of loss. Expressing feelings of grief and loss can sometimes be harder for men. Some women are able to begin planning their next pregnancy after a few weeks of having the miscarriage. For others, planning another pregnancy can be difficult. Some facilities acknowledge the loss. Parents can name and hold their infant. They may be given mementos such as photos and footprints. Some conduct a funeral or memorial service. They may express the loss by planting a tree.\n\nSome health organizations recommend that sexual activity be delayed after the miscarriage. The menstrual cycle should resume after about three to four months. Women report that they were dissatisfied with the care they received from physicians and nurses.\n\nSome parents want to try to have a baby very soon after the miscarriage. The decision of trying to become pregnant again can be difficult. Reasons exist that may prompt parents to consider another pregnancy. For older mothers, there may be some sense of urgency. Other parents are optimistic that future pregnancies are likely to be successful. Many are hesitant and want to know about the risk of having another or more miscarriages. Some clinicians recommend that the women have one menstrual cycle before attempting another pregnancy. This is because the date of conception may be hard to determine. Also, the first menstrual cycle after a miscarriage can be much longer or shorter than expected. Parents may be advised to wait even longer if they have experienced late miscarriage or molar pregnancy, or are undergoing tests. Some parents wait for six months based upon recommendations from their health care provider.\n\nThe risks of having another miscarriage vary according to the cause. The risk of having another miscarriage after a molar pregnancy is very low. The risk of another miscarriage is highest after the third miscarriage. Pre-conception care is available in some locales.\n\nThere is a significant association between miscarriage and later development of coronary artery disease, but not of cerebrovascular disease.\n\nAmong women who know they are pregnant, the miscarriage rate is roughly 10% to 20%, while rates among all fertilized zygotes are around 30% to 50%. A 2012 review found the risk of miscarriage between 5 and 20 weeks from 11% to 22%. Up to the 13th week of pregnancy, the risk of miscarriage each week was around 2%, dropping to 1% in week 14 and reducing slowly between 14 and 20 weeks.\n\nThe precise rate is not known because a large number of miscarriages occur before pregnancies become established and before the woman is aware they are pregnant. Additionally, those with bleeding in early pregnancy may seek medical care more often than those not experiencing bleeding. Although some studies attempt to account for this by recruiting women who are planning pregnancies and testing for very early pregnancy, they still are not representative of the wider population.\n\nThe prevalence of miscarriage increases with the age of both parents. In a Danish register-based study where the prevalence of miscarriage was 11%, the prevalence rose from 9% at 22 years of age to 84% by 48 years of age. Another, later study in 2013 found that when either parent was over the age of 40, the rate of known miscarriages doubled.\n\nIn 2010, 50,000 inpatient admissions for miscarriage occurred in the UK.\n\nMost affected women and family members refer to miscarriage as the loss of a baby, rather than an embryo or fetus, and healthcare providers are expected to respect and use the language that the person chooses. Clinical terms can suggest blame, increase distress, and even cause anger. Terms that are known to cause distress in those experiencing miscarriage include:\n\n\n\"Pregnancy loss\" is a broad term that describes miscarriage, ectopic and molar pregnancies. The term \"fetal death\" applies variably in different countries and contexts, sometimes incorporating weight, and gestational age from 16 weeks in Norway, 20 weeks in the US and Australia, 24 weeks in the UK to 26 weeks in Italy and Spain. A fetus that died before birth after this gestational age may be referred to as a stillbirth. Under UK law, all stillbirths should be registered, although this does not apply to miscarriages.\n\nThe medical terminology applied to experiences during early pregnancy has changed over time. Before the 1980s, health professionals used the phrase \"spontaneous abortion\" for a miscarriage and \"induced abortion\" for a termination of the pregnancy. In the late 1980s and 1990s, doctors became more conscious of their language in relation to early pregnancy loss. Some medical authors advocated change to use of \"miscarriage\" instead of \"spontaneous abortion\" because they argued this would be more respectful and help ease a distressing experience. The change was being recommended by some in the profession in Britain in the late 1990s. In 2005 the European Society for Human Reproduction and Embryology (ESHRE) published a paper aiming to facilitate a revision of nomenclature used to describe early pregnancy events.\n\nSociety's reactions to miscarriage changed over time.  In the early 20th century, the focus was on the mother's physical health and the difficulties and disabilities that miscarriage could produce. Other reactions, such as the expense of medical treatments and relief at ending an unwanted pregnancy, were also heard. In the 1940s and 1950s, people were more likely to express relief, not because the miscarriage ended an unwanted or mistimed pregnancy, but because people believed that miscarriages were primarily caused by birth defects, and miscarrying meant that the family would not raise a child with disabilities. The dominant attitude in the mid-century was that a miscarriage, although temporarily distressing, was a blessing in disguise for the family, and that another pregnancy and a healthier baby would soon follow, especially if women trusted physicians and reduced their anxieties. Media articles were illustrated with pictures of babies, and magazine articles about miscarriage ended by introducing the healthy baby—usually a boy—that had shortly followed it.\n\nBeginning in the 1980s, miscarriage in the US was primarily framed in terms of the individual woman's personal emotional reaction, and especially her grief over a tragic outcome. The subject was portrayed in the media with images of an empty crib or an isolated, grieving woman, and stories about miscarriage were published in general-interest media outlets, not just women's magazines or health magazines. Family members were encouraged to grieve, to memorialize their losses through funerals and other rituals, and to think of themselves as being parents.  This shift to recognizing these emotional responses was partly due to medical and political successes, which created an expectation that pregnancies are typically planned and safe, and to women's demands that their emotional reactions no longer be dismissed by the medical establishments.  It also reinforces the pro-life movement's belief that human life begins at conception or early in pregnancy, and that motherhood is a desirable life goal. The modern one-size-fits-all model of grief does not fit every woman's experience, and an expectation to perform grief creates unnecessary burdens for some women. The reframing of miscarriage as a private emotional experience brought less awareness of miscarriage and a sense of silence around the subject, especially compared to the public discussion of miscarriage during campaigns for access to birth control during the early 20th century, or the public campaigns to prevent miscarriages, stillbirths, and infant deaths by reducing industrial pollution during the 1970s.\n\nIn places where induced abortion is illegal or carries social stigma, suspicion may surround miscarriage, complicating an already sensitive issue.\n\nIn the 1960s, the use of the word \"miscarriage\" in Britain (instead of \"spontaneous abortion\") occurred after changes in legislation.\n\nDevelopments in ultrasound technology (in the early 1980s) allowed them to identify earlier miscarriages.\n\nAccording to French statutes, an infant born before the age of viability, determined to be 28 weeks, is not registered as a 'child'. If birth occurs after this, the infant is granted a certificate that allows women who have given birth to a stillborn child, to have a symbolic record of that child. This certificate can include a registered and given name with the purpose of allowing a funeral and acknowledgement of the event.\n\nMiscarriage occurs in all animals that experience pregnancy, though in such contexts it is more commonly referred to as a \"spontaneous abortion\" (the two terms are synonymous). There are a variety of known risk factors in non-human animals. For example, in sheep, miscarriage may be caused by crowding through doors, or being chased by dogs. In cows, spontaneous abortion may be caused by contagious disease, such as brucellosis or \"Campylobacter\", but often can be controlled by vaccination. In many species of sharks and rays, stress induced miscarriage occurs frequently on capture.\n\nOther diseases are also known to make animals susceptible to miscarriage. Spontaneous abortion occurs in pregnant prairie voles when their mate is removed and they are exposed to a new male, an example of the Bruce effect, although this effect is seen less in wild populations than in the laboratory. Female mice who had spontaneous abortions showed a sharp rise in the amount of time spent with unfamiliar males preceding the abortion than those who did not.\n\n"}
{"id": "57969764", "url": "https://en.wikipedia.org/wiki?curid=57969764", "title": "Mortality in the early modern age", "text": "Mortality in the early modern age\n\nThe early modern age saw various economic changes as well as several significant diseases that have affected the mortality rates. Data collection during this time was not consistent or broadly recorded and there have been efforts to reconstruct plausible statistics. Mortality rates vary on geographic location, social environment, and cultural values. There were also gender differences in the mortality rates, leading to an excess mortality rate in urban areas and in the female population. A main cause of death was stillbirth, which could be attributed to, but not limited to, maternal infections, birth complications, and congenital anomalies. Another contributing factor to the mortality rate was food insecurity and shortages as well as unemployment, both of which varied per region. A final factor was violence, which occurred mainly due to structural or systemic violence; however, violence since the 12th century has been steadily falling.\n\nData from the early modern age was not accurately or consistently collected. However, there have been a number of studies and reconstructed statistics from this era, particularly on children and women. There has not been any empirical research published and the only information has been theoretical as there has been insufficient data and sources. It was also common for many statistics to go unreported; this is especially true regarding unmarried women. Models and theoretic equations need to take into account \"social, economic, cultural, geographical, and even climatological variables\" in order to accurately reflect the statistics of the time.\n\nOne study, the Eurasia Project, has shown that boys, especially those under one year, had a higher mortality rate during childhood than girls, but the mortality rate for men and women were about equal. It has also been shown that there is a higher male mortality than female mortality rate during the time of famine. Male mortality has also been linked to \"economic modernization and urbanization ... especially for cardiovascular disease\".\n\nWomen faced increased mortality during childbirth as pregnancy and childbirth compromised the mother's immune system, with the most common causes of death being puerperal fever, toxemia, and hemorrhage. These dangers suggest an association to the excess female mortality, especially considering that women had to compete more for resources as they had no property rights and had a lower ranking in the household hierarchy. The average age of childbearing differed between Asia and Europe with an average difference of five years, which would affect cross-cultural data collection. Children born to mothers 35 years or older had a higher risk of mortality than children born to younger mothers. linking a mother's health and a child's survival.\n\nFemale infants and children often had a higher mortality rate, especially in times of food insecurity, compared to male infants and children. However, a maternal presence worked as a protective factor for children. regardless of age or gender.\n\nFood insecurity and shortages were common throughout this time period and were matched with the high food prices and high unemployment rate. This is shown through the differences in mortality rates between the lower and upper class, with poor infants being up to two times more likely to die than their wealthier counterparts. In 17th century Europe, it was common that at least one in five children from the same family would die before the age of one. Because of this high rate, it was common for there to be a large amount of children in one family so that there would be a higher rate of survival. Especially among poorer families, having multiple children was common in order to ensure there would be children to contribute to the family later on.\n\nDuring the early modern era, house calls were common. If the patient was female, the doctor, commonly an upper class male, would typically be summoned by a male family member who wound remain throughout the examination. The physician and male family member would then discuss the diagnosis and treatment plan without the female's input. This practice reinforced and perpetuated social hierarchies and patriarchal values.\n\nFood shortages and insecurity were leading concerns in the 18th century, especially in Europe, and these were exacerbated by reduced harvests yields. Disease was another leading cause of death, with rats and fleas being the common carriers of disease, specifically plagues, during this era.\n\nThe Black Death was a plague that affected much of the world, originating in Asia and spreading to Europe through diseased fleas and rats. This epidemic has been reported to have been the cause of death for approximately \"60% of the European population\".\n\nDuring the end of the 19th century, there was a plague, known as the Modern Plague, that started in China and spread to different cities through ports, reportedly causing roughly ten million deaths. This plague affected Asia, the Americas, and Africa and lasted into the 20th century. There were also epidemics that occurred locally and did not spread to national levels, notably in 18th century England. These local epidemics included fevers, dysentery, smallpox, starvation, typhoid fever, under-nutrition, cholera, malaria.\n\nBy the end of the era, disease and malnutrition were no longer the main leading causes of death.\n"}
{"id": "50218653", "url": "https://en.wikipedia.org/wiki?curid=50218653", "title": "Nicotinamide mononucleotide", "text": "Nicotinamide mononucleotide\n\nNicotinamide mononucleotide (\"NMN\" and \"β-NMN\") is a nucleotide derived from ribose and nicotinamide. Like nicotinamide riboside, NMN is a derivative of niacin, and humans have enzymes that can use NMN to generate nicotinamide adenine dinucleotide (NADH).\n\nBecause NADH is a cofactor for processes inside mitochondria, for sirtuins, and for PARP, NMN has been studied in animal models as a potential neuroprotective and anti-aging agent. Dietary supplement companies have aggressively marketed NMN products claiming those benefits, though there is no clinical study on humans published yet.\n"}
{"id": "47959728", "url": "https://en.wikipedia.org/wiki?curid=47959728", "title": "Orphan Wisdom", "text": "Orphan Wisdom\n\nOrphan Wisdom is a philosophical system invented and promoted by Stephen Jenkinson that believes what modern people \"suffer from most is culture failure, amnesia of ancestry and deep family story, phantom or sham rites of passage, no instruction on how to live with each other or with the world around us or with our dead or with our history.\" Before his 2010 founding of the Orphan Wisdom School, Jenkinson directed palliative care at Mount Sinai Hospital of Toronto. Orphan Wisdom's teachings push against \"'death phobia' and 'grief illiteracy'\" to promote acceptance of death well before death in order to \"participate emotionally in their deaths as they participate in other big life events\".\n\nThe documentary film about Jenkinson and Orphan Wisdom, \"Griefwalker\", was produced by the National Film Board of Canada and filmed over twelve years by Tim Wilson.\n\nThe 2015 book \"Die Wise: A Manifesto for Sanity and Soul\" is Jenkinson's history, explication and exploration of his approach to coming to terms with death. Its dense and sometimes poetic prose is both a critique of dominant Western cultural practices and denials - in part gleaned from his years the \"death trade,\" as Jenkinson calls it - as well as what the author has learned elsewhere, particularly from indigenous peoples. His ideas also have an affinity with Buddhist teachings, which have their origin in the Buddha's confronting the reality of suffering and death.\n\n"}
{"id": "31564169", "url": "https://en.wikipedia.org/wiki?curid=31564169", "title": "Philosophy of happiness", "text": "Philosophy of happiness\n\nThe philosophy of happiness is the philosophical concern with the existence, nature, and attainment of happiness. Philosophers believe, happiness can be understood as the moral goal of life or as an aspect of chance; indeed, in most European languages the term happiness is synonymous with luck. Thus, philosophers usually explicate on happiness as either a state of mind, or a life that goes well for the person leading it.\n\nPlato (c. 428 c. 347 BCE), using Socrates (c. 470 399 BCE) as the main character in his philosophical dialogues, outlined the requirements for happiness in \"The Republic\".\n\nIn \"The Republic\", Plato asserts that those who are moral are the only ones who may be truly happy. Thus, one must understand the cardinal virtues, particularly justice. Through the thought experiment of the Ring of Gyges, Plato comes to the conclusion that one who abuses power enslaves himself to his appetites, while the man who chooses not to remains rationally in control of himself, and therefore is happy.\n\nHe also sees a type of happiness stemming from social justice through fulfilling one's social function; since this duty forms happiness, other typically seen sources of happiness such as leisure, wealth, and pleasure are deemed lesser, if not completely false, forms of happiness.\n\nAristotle (384 – 322 BCE) held that \"eudaimonia\" (Greek: ) is the goal of human thought and action. Eudaimonia is usually translated as happiness, but \"human flourishing\" may be a more accurate translation. Eudaimonia involves activity, exhibiting virtue (\"arete\", Greek: ἀρετή) in accordance with virtue.\n\nWithin the \"Nicomachean Ethics\", Aristotle points to the fact that many aims are really only intermediate aims, and are desired only because they make the achievement of higher aims possible. Therefore, things such as wealth, intelligence, and courage are valued only in relation to other things, while eudaimonia is the only thing valuable in isolation.\n\nAristotle regarded virtue as necessary for a person to be happy and held that without virtue the most that may be attained is contentment. Aristotle has been criticized for failing to show that virtue is necessary in the way he claims it to be, and he does not address this moral skepticism.\n\nAntisthenes (c. 445 – c. 365 BCE), often regarded as the founder of Cynicism, advocated an ascetic life lived in accordance with virtue. Xenophon testifies that Antisthenes had praised the joy that sprang \"from out of one's soul,\" and Diogenes Laertius relates that Antisthenes was fond of saying: \"I would rather go mad than feel pleasure.\" He maintained that virtue was sufficient in itself to ensure happiness, only needing the strength of a Socrates.\n\nHe, along with all following Cynics, rejected any conventional notions of happiness involving money, power, and fame, to lead entirely virtuous, and thus happy, lives. Thus, happiness can be gained through rigorous training (askesis, Greek: ) and by living in a way which was natural for humans, rejecting all conventional desires, preferring a simple life free from all possessions.\n\nDiogenes of Sinope (c. 412 – c. 323 BCE) is most frequently seen as the perfect embodiment of the philosophy. The Stoics themselves saw him as one of the few, if not only, who have had achieved the state of sage.\n\nStoicism was a school of philosophy established by Zeno of Citium (c. 334 – c. 262 BCE). While Zeno was syncretic in thought, his primary influence were the Cynics, with Crates of Thebes (c. 365 – c. 285 BCE) as his mentor.\n\nStoics believe that \"virtue is sufficient for happiness\". One who has attained this sense of virtue would become a sage. In the words of Epictetus, this sage would be \"sick and yet happy, in peril and yet happy, dying and yet happy, in exile and happy, in disgrace and happy,\"\n\nThe Stoics therefore spent their time trying to attain virtue. This would only be achieved if one was to dedicate their life studying Stoic logic, Stoic physics, and Stoic ethics.\n\nThe Cyrenaics were a school of philosophy established by Aristippus of Cyrene (c. 435 – c. 356 BCE). The school asserted that the only good is positive pleasure, and pain is the only evil. They posit that all feeling is momentary so all past and future pleasure have no real existence for an individual, and that among present pleasures there is no distinction of kind. Claudius Aelianus, in his \"Historical Miscellany\", writes about Aristippus:\n\nSome immediate pleasures can create more than their equivalent of pain. The wise person should be in control of pleasures rather than be enslaved to them, otherwise pain will result, and this requires judgement to evaluate the different pleasures of life.\n\nEpicureanism was founded by Epicurus (c. 341 – c. 270 BCE). The goal of his philosophy was to attain a state of tranquility (\"ataraxia\", Greek: ) and freedom from fear, as well as absence of bodily pain (\"aponia\", Greek: ). Toward these ends, Epicurus recommended an ascetic lifestyle, noble friendship, and the avoidance of politics.\n\nOne aid to achieving happiness is the \"tetrapharmakos\" or the four-fold cure:\n\n\"Do not fear god,<br>\nDo not worry about death;<br>\nWhat is good is easy to get, and<br>\nWhat is terrible is easy to endure.\"<br>\n(\"Philodemus, Herculaneum Papyrus, 1005, 4.9–14\").\n\nThe School of the Sextii was founded by Quintus Sextius the Elder (fl. 50 BCE). It characterized itself mainly as a philosophical-medical school, blending Pythagorean, Platonic, Cynic, and Stoic elements together. They argued that to achieve happiness, one ought to be vegetarian, have nightly examinations of conscience, and avoid both consumerism and politics, and believe that an elusive incorporeal power pervades the body.\n\nSt. Augustine of Hippo (354 – 430 AD) was an early Christian theologian and philosopher whose writings influenced the development of Western Christianity and Western philosophy.\n\nFor St. Augustine, all human actions revolve around love, and the primary problem humans face is the misplacing of love. Only in God can one find happiness, as He is source of happiness. Since humanity was brought forth from God, but has since fallen, one's soul dimly remembers the happiness from when one was with God. Thus, if one orients themselves toward the love of God, all other loves will become properly ordered. In this manner, St. Augustine follows the Neoplatonic tradition in asserting that happiness lays in the contemplation of the purely intelligible realm.\n\nSt. Augustine deals with the concept of happiness directly in his treatises \"De beata vita\" and \"Contra Academicos\".\n\nBoethius (c. 480–524 AD) was a philosopher, most famous for writing \"The Consolation of Philosophy\". The work has been described as having had the single most important influence on the Christianity of the Middle Ages and early Renaissance and as the last great work of the Classical Period. The book describes many themes, but among them he discusses how happiness can be attainable despite changing fortune, while considering the nature of happiness and God.\n\nHe posits that happiness is acquired by attaining the perfect good, and that perfect good is God. He then concludes that as God ruled the universe through Love, prayer to God and the application of Love would lead to true happiness.\n\nAvicenna (c. 980–1037), also known as 'Ibn-Sina', was polymath and jurist; he is regarded as one of the most significant thinkers in the Islamic Golden Age. According to him, happiness is the aim of humans, and that real happiness is pure and free from worldly interest. Ultimately, happiness is reached through the conjunction of the human intellect with the separate active intellect.\n\nAl-Ghazali (c. 1058–1111) was a Muslim theologian, jurist, philosopher, and mystic of Persian descent. Produced near the end of his life, al-Ghazali wrote \"The Alchemy of Happiness\" (\"Kimiya-yi Sa'ādat\", (). In the work, he emphasizes the importance of observing the ritual requirements of Islam, the actions that would lead to salvation, and the avoidance of sin. Only by exercising the human faculty of reason - a God-given ability - can one transform the soul from worldliness to complete devotion to God, the ultimate happiness.\n\nAccording to Al-Ghazali, there are four main constituents of happiness: self-knowledge, knowledge of God, knowledge of this world as it really is, and the knowledge of the next world as it really is.\n\nMaimonides (c. 1135-1204) was a Jewish philosopher and astronomer, who became one of the most prolific and influential Torah scholars and physicians. He writes that happiness is ultimately and essentially intellectual.\n\nSt. Thomas Aquinas (1225 – 1274 AD) was a philosopher and theologian, who became a Doctor of the Church in 1323. His system syncretized Aristotelianism and Catholic theology within his \"Summa Theologica\". The first part of the second part is divided into 114 articles, the first five deal explicitly with the happiness of humans. He states that happiness is achieved by cultivating several intellectual and moral virtues, which enable us to understand the nature of happiness and motivate us to seek it in a reliable and consistent way. Yet, one will be unable to find the greatest happiness in this life, because final happiness consists in a supernatural union with God. As such, man’s happiness does not consist of wealth, status, pleasure, or in any created good at all. Most goods do not have a necessary connection to happiness, since the ultimate object of man’s will, can only be found in God, who is the source of all good.\n\nMichel de Montaigne (1533-1592) was a French philosopher. Influenced by Aristotelianism and Christianity, alongside the conviction of the separation of public and private spheres of life, Montaigne writes that happiness is a subjective state of mind and that satisfaction differs from person to person. He continues by acknowledging that one must be allowed a private sphere of life to realize those particular attempts of happiness without the interference of society.\n\nJeremy Bentham (1748-1832) was a British philosopher, jurist, and social reformer. He is regarded as the founder of modern utilitarianism.\n\nHis particular brand of utilitarianism indicated that the most moral action is that which causes the highest amount of utility, where defined utility as the aggregate pleasure after deducting suffering of all involved in any action. Happiness, therefore, is the experience of pleasure and the lack of pain. Actions which do not promote the greatest happiness is morally wrong - such as ascetic sacrifice. This manner of thinking permits the possibility of a calculator to measure happiness and moral value.\n\nArthur Schopenhauer (1788-1860) was a German philosopher. His philosophy express that egotistical acts are those that are guided by self-interest, desire for pleasure or happiness, whereas only compassion can be a moral act.\n\nSchopenhauer explains happiness in terms of a wish that is satisfied, which in turn gives rise to a new wish. And the absence of satisfaction is suffering, which results in an empty longing. He also links happiness with the movement of time, as we feel happy when time moves faster and feel sad when time slows down.\n\nWładysław Tatarkiewicz (1886-1980) was a Polish philosopher, historian of philosophy, historian of art, esthetician, and ethicist.\n\nFor Tatarkiewicz, happiness is a fundamental ethical category.\n\nHerbert Marcuse (1898–1979) was a German-American philosopher, sociologist, and political theorist, associated with the Frankfurt School of critical theory.\n\nIn his 1937 essay 'The Affirmative Character of Culture,' he suggests culture develops tension within the structure of society, and in that tension can challenge the current social order. If it separates itself from the everyday world, the demand for happiness will cease to be external, and begin to become an object of spiritual contemplation.\n\nIn the \"One-Dimensional Man\", his criticism of consumerism suggests that the current system is one that claims to be democratic, but is authoritarian in character, as only a few individuals dictate the perceptions of freedom by only allowing certain choices of happiness to be available for purchase. He further suggests that the conception that 'happiness can be bought' is one that is psychologically damaging.\n\nViktor Frankl (1905-1997) was an Austrian neurologist, psychiatrist, Holocaust survivor and founder of logotherapy. His philosophy revolved around the emphasis on meaning, the value of suffering, and responsibility to something greater than the self; only if one encounters those questions can one be happy.\n\nRobert Nozick (1938-2002) was an American philosopher.\n\nIn his 1974 book, \"Anarchy, State, Utopia\", he proposed a thought experiment where one is given the option to enter a machine that would give the maximum amount of unending hedonistic pleasure for the entirety of one's life.\n\nScientism is the approach that the empirical sciences are the most valuable branches of learning and culture.\n\nHappiness economics is the quantitative and theoretical study of happiness, positive and negative affect, well-being, quality of life, life satisfaction and related concepts, typically combining economics with other fields such as psychology and sociology. The tracking of Gross National Happiness or the satisfaction of life grow increasingly popular as the economics of happiness challenges traditional economic aims.\n\nRichard Layard has been very influential in this area. He has shown that mental illness is the main cause of unhappiness.\n\nSonja Lyubomirsky asserted in her 2007 book, \"The How of Happiness\", that happiness is 50 percent genetically determined (based on twin studies), 10 percent circumstancial, and 40 percent subject to self-control. Lyubomirsky suggests a twelve-point program to maximize the final 40 percent.\n\nNot all cultures seek to maximise happiness, and some cultures are averse to happiness.\n\n\n\n"}
{"id": "3064846", "url": "https://en.wikipedia.org/wiki?curid=3064846", "title": "Philosophy of suicide", "text": "Philosophy of suicide\n\nIn ethics and other branches of philosophy, suicide poses difficult questions, answered differently by various philosophers. The French essayist, novelist, and playwright Albert Camus (1913–1960) began his philosophical essay \"The Myth of Sisyphus\" with the famous line \"There is but one truly serious philosophical problem and that is suicide\" ().\n\nThere are arguments in favor of allowing an individual to choose between life and suicide. Those in favor of suicide as a personal choice reject the thought that suicide is always or usually irrational, but is instead a solution to real problems; a line of last resort that can legitimately be taken when the alternative is considered worse. They believe that no being should be made to suffer unnecessarily, and suicide provides an escape from suffering.\n\nHerodotus wrote: \"When life is so burdensome, death has become for man a sought-after refuge\". Schopenhauer affirmed: \"They tell us that suicide is the greatest act of cowardice... that suicide is wrong; when it is quite obvious that there is nothing in the world to which every man has a more unassailable title than to his own life and person.\"\n\nSchopenhauer's main work, \"The World as Will and Representation\", occasionally uses the act in its examples. He denied that suicide was immoral and saw it as one's right to take one's life. In an allegory, he compared ending one's life, when subject to great suffering, to waking up from sleep when experiencing a terrible nightmare. However, most suicides were seen as an act of the will, as it takes place when one denies life's pains, and is thus different from ascetic renunciation of the will, which denies life's pleasures.\n\nAccording to Schopenhauer, moral freedom—the highest ethical aim—is to be obtained only by a denial of the will to live. Far from being a denial, suicide is an emphatic assertion of this will. For it is in fleeing from the pleasures, not from the sufferings of life, that this denial consists. When a man destroys his existence as an individual, he is not by any means destroying his will to live. On the contrary, he would like to live if he could do so with satisfaction to himself; if he could assert his will against the power of circumstance; but circumstance is too strong for him.\n\nLiberalism asserts that a person's life belongs only to them, and no other person has the right to force their own ideals that life must be lived. Rather, only the individual involved can make such a decision, and whatever decision they make should be respected.\n\nPhilosopher and psychiatrist Thomas Szasz goes further, arguing that suicide is the most basic right of all. If freedom is self-ownership—ownership over one's own life and body—then the right to end that life is the most basic of all. If others can force you to live, you do not own yourself and belong to them.\n\nJean Améry, in his book \"On Suicide: a Discourse on Voluntary Death\" (originally published in German in 1976), provides a moving insight into the suicidal mind. He argues forcefully and almost romantically that suicide represents the ultimate freedom of humanity, justifying the act with phrases such as \"we only arrive at ourselves in a freely chosen death\" and lamenting \"ridiculously everyday life and its alienation\". Améry killed himself in 1978.\n\nPhilosophical thinking in the 19th and 20th century has led, in some cases, beyond thinking in terms of pro-choice, to the point that suicide is no longer a last resort, or even something that one must justify, but something that one must justify doing. Many forms of existentialist thinking essentially begin with the premise that life is objectively meaningless, and proceed to the question of why one should not just kill oneself; they then answer this question by suggesting that the individual has the power to give personal meaning to life.\n\nAlthough George Lyman Kittredge states that \"the Stoics held that suicide is cowardly and wrong,\" the most famous stoics—Seneca the Younger, Epictetus, and Marcus Aurelius—maintain that death by one's own hand is always an option and frequently more honorable than a life of protracted misery.\n\nThe Stoics accepted that suicide was permissible for the wise person in circumstances that might prevent them from living a virtuous life. Plutarch held that accepting life under tyranny would have compromised Cato's self-consistency () as a Stoic and impaired his freedom to make the honorable moral choices. Suicide could be justified if one fell victim to severe pain or disease, but otherwise suicide would usually be seen as a rejection of one's social duty.\n\nConfucianism holds that failure to follow certain values is worse than death; hence, suicide can be morally permissible, and even praiseworthy, if it is done for the sake of those values. The Confucian emphasis on loyalty, self-sacrifice, and honour has tended to encourage altruistic suicide. Confucius wrote, \"For gentlemen of purpose and men of ren while it is inconceivable that they should seek to stay alive at the expense of ren, it may happen that they have to accept death in order to have ren accomplished.\" Mencius wrote:\nCommon philosophical opinion of suicide since modernization reflected a spread in cultural beliefs of western societies that suicide is immoral and unethical. One popular argument is that many of the reasons for committing suicide—such as depression, emotional pain, or economic hardship—are transitory and can be ameliorated by therapy and through making changes to some aspects of one's life. A common adage in the discourse surrounding suicide prevention sums up this view: \"Suicide is a permanent solution to a temporary problem.\" However, the argument against this is that while emotional pain may seem transitory to most people, and in many cases it is, in other cases it may be extremely difficult or even impossible to resolve, even through counseling or lifestyle change, depending upon the severity of the affliction and the person's ability to cope with their pain. Examples of this are incurable disease or lifelong mental illness.\n\nThe French Algerian absurdist philosopher Albert Camus saw the goal of absurdism in establishing whether suicide is a necessary response to a world which appears to be mute both on the question of God's existence (and thus what such an existence might answer) and for our search for meaning and purpose in the world. For Camus, suicide was the rejection of freedom. He thinks that fleeing from the absurdity of reality into illusions, religion, or death is not the way out. Instead of fleeing the absurd meaninglessness of life, we should embrace life passionately.\n\nExistentialist Sartre describes the position of Meursault, the protagonist of Camus' \"The Stranger\" who is condemned to death, in the following way:\n\nG. K. Chesterton calls suicide \"the ultimate and absolute evil, the refusal to take an interest in existence\". He argues that a person who kills himself, as far as he is concerned, destroys the entire world (apparently exactly repeating Maimonides' view).\n\nJohn Stuart Mill argued, in his influential essay \"On Liberty\", that since the of liberty is the power of the individual to make choices, any choice that one might make that would deprive one of the ability to make further choices should be prevented. Thus, for Mill, selling oneself into slavery should be prevented in order to avoid precluding the ability to make further choices. Concerning these matters, Mill writes in \"On Liberty\"\n\nIt could be argued that suicide prevents further choices in the same way slavery does. However, it can also be argued that there are significant differences in not having any further involvement in decisions about your life and not having any further life to make decisions about. Suicide essentially removes the condition of being alive, not the condition of making choices about your life. \n\nMill believes the individual to be the best guardian of their own interests. He uses the example of a man about to cross a broken bridge: we can forcibly stop that person and warn him of the danger, but ultimately should not prevent him from crossing the bridge—for only \"he\" knows the worth of his life balanced against the danger of crossing the bridge.\n\nToo much should not be read into \"disposing of his own lot in life\" in the passage as this is not necessarily talking about anything other than slavery. Indeed, it would be odd if Mill had intended it to be about suicide but not explored the issue fully.\n\nFrom a deontological perspective, Immanuel Kant argues against suicide in \"Fundamental Principles of The Metaphysic of Morals\". In accordance with the second formulation of his categorical imperative, Kant argues that, \"He who contemplates suicide should ask himself whether his action can be consistent with the idea of humanity as an end in itself.\" Kant's theory looks at the act only, and not at its outcomes and consequences, and claims that one is ethically required to consider whether one would be willing to \"universalise\" the act: to claim \"everyone\" should behave that way. Kant argues that choosing to commit suicide entails considering oneself as a means to an end, which he rejects: a person, he says, must \"not\" be used \"merely as means, but must in all actions always be considered as an end in himself.\" Therefore, it is unethical to commit suicide to satisfy oneself.\n\nThe social contract, according to Jean-Jacques Rousseau, is such that every man has \"a right to risk his own life in order to preserve it.\"\n\nHobbes and Locke reject the right of individuals to take their own life. Hobbes claims in his \"Leviathan\" that natural law forbids every man \"to do, that which is destructive of his life, or take away the means of preserving the same.\" Breaking this natural law is irrational and immoral. Hobbes also states that it is intuitively rational for men to want felicity and to fear death most.\n\nJapan has a form of suicide called seppuku, which is considered an honorable way to redeem oneself for transgressions or personal defeats. It was widely accepted in the days of the Samurai and even before that. It was generally seen as a privilege granted only to the samurai class; civilian criminals would thus not have this 'honor' and be executed. This reflects a view of suicide as brave and correct rather than cowardly and wrong.\n\nUtilitarianism can be used as a justification for, or an argument against, suicide. Though the death of a depressed person ends their suffering, the person's family and friends may grieve.\n\nDavid Hume left an essay on suicide to be published after his death. Most of it is concerned with the claim that suicide is an affront to God. Hume argues that suicide is no more a rebellion against God than is saving the life of someone who would otherwise die, or changing the position of anything in one's surroundings. He spends much less time dismissing arguments that it is an affront to one's duty to others or to oneself. Hume claims that suicide can be compared to retiring from society and becoming a total recluse, which is not normally considered to be immoral, although the comparison would not seem to justify a suicide that leaves in its wake children or dependents who are thereby rendered vulnerable. As for duty to self, Hume takes it to be obvious that there can be times when suicide is desirable, though he also thinks it ridiculous that anyone would consider suicide unless they first considered every other option.\n\nThose who support the right to die argue that suicide is acceptable under certain circumstances, such as incurable disease and old age. The idea is that although life is, in general, good, people who face irreversible suffering should not be forced to continue suffering.\n\nLeonard Peikoff states in his book \"\":\n\nBioethicist Jacob Appel has criticized \"arbitrary\" ethical systems that allow patients to refuse care when they are physically ill, while denying the mentally ill the right to suicide.\n\n\n\n"}
{"id": "2760602", "url": "https://en.wikipedia.org/wiki?curid=2760602", "title": "Polyworld", "text": "Polyworld\n\nPolyworld is a cross-platform (Linux, Mac OS X) program written by Larry Yaeger to evolve Artificial Intelligence through natural selection and evolutionary algorithms. \n\nIt uses the Qt graphics toolkit and OpenGL to display a graphical environment in which a population of trapezoid agents search for food, mate, have offspring, and prey on each other. The population is typically only in the hundreds, as each individual is rather complex and the environment consumes considerable computer resources. The graphical environment is necessary since the individuals actually move around the 2-D plane and must be able to \"see.\" Since some basic abilities, like eating carcasses or randomly generated food, seeing other individuals, mating or fighting with them, etc., are possible, a number of interesting behaviours have been observed to spontaneously arise after prolonged evolution, such as cannibalism, predators and prey, and mimicry.\n\nEach individual makes decisions based on a neural net using Hebbian learning; the neural net is derived from each individual's genome. The genome does not merely specify the wiring of the neural nets, but also determines their size, speed, color, mutation rate and a number of other factors. The genome is randomly mutated at a set probability, which are also changed in descendant organisms.\n\n"}
{"id": "57408363", "url": "https://en.wikipedia.org/wiki?curid=57408363", "title": "Reda Aadel", "text": "Reda Aadel\n\nReda Aadel (born 28 December 1990) is a Moroccan cyclist.\n"}
{"id": "27364349", "url": "https://en.wikipedia.org/wiki?curid=27364349", "title": "Rolf M. Aagaard", "text": "Rolf M. Aagaard\n\nRolf Magdal Aagaard (born 27 March 1945) is a Norwegian photographer.\n\nHe was born in Risør. He worked for one year for \"Tiden\" and seven years for \"Fædrelandsvennen\" before being hired by \"Aftenposten\" in 1970. He was awarded the Narvesen Prize in 1979. He has also held exhibitions, and he has written books.\n"}
{"id": "40446205", "url": "https://en.wikipedia.org/wiki?curid=40446205", "title": "Saara Aalto", "text": "Saara Aalto\n\nSaara Sofia Aalto (born 2 May 1987) is a Finnish singer, songwriter, and voice actress. She came second in the first season of \"The Voice of Finland\" in 2012. Aalto competed in the Finnish national selection to represent Finland in the Eurovision Song Contest two times, first in 2011 with \"Blessed with Love\" and then in 2016 with \"No Fear\", both times placing second. On 7 November 2017, it was announced that she had been internally selected by broadcaster Yle to represent Finland in the Eurovision Song Contest 2018 in Lisbon. The Finnish public chose the song \"Monsters\". She qualified for the grand final and finished in 25th place.\n\nShe has taken part in musicals on stage and has done a great number of dubbings for feature films and foreign television series. Saara Aalto is the voice of Princess Anna in the Finnish version of Disney's animated movie \"Frozen\". She has dueted with Adam Lambert, and had concerts together with Andrea Bocelli, José Carreras and pianist Robert Wells. She has released 5 albums on her own record label Yume Records.\n\nIn 2016, Aalto came runner-up in the thirteenth series of \"The X Factor UK\", which gained her international recognition. Every fourth Finn watched \"The X Factor\" final. Aalto was the most-Googled person in Finland in 2016. Tabloid newspaper \"Iltalehti\" readers voted Saara Aalto as Finnish person of the year 2016. Following the show, she was signed to both Sony Music UK and Finland. In June 2017, Aalto announced via social media that she will be a judge on season two of \"X Factor Suomi\". Four months later, Aalto announced she had left Sony Music for Warner Music Group, citing that her previous deal did not feel like the \"right team\".\n\nBorn in Oulunsalo, Finland, Aalto grew up in a musical family and received piano lessons from pianist and pedagogue Olga Maslak, born in Odessa, Ukraine. Saara wrote her first song at the age of five. Her close relatives include painter and documentary director Eeli Aalto and magician Simo Aalto. In 1998 at age 11, she won the Kotka Maritime Festival song contest for children with one of her own compositions. Aalto also won the Charlotte Church international singing competition, organised in US, in 2003 with her own composition. Aalto represented Finland in the Golden Stag International Song Contest in Romania in 2004. She went to the Madetoja secondary school for music, where she graduated in 2005. After graduation she moved to Helsinki to study music at the Sibelius Academy, and at the same time also studied singing in the Helsinki Pop & Jazz Conservatory.\n\nIn the past few years Saara has been a sought after performer in concerts, galas, TV-shows and musicals. She has worked with several top orchestras and military bands and toured across Finland with her own band. However, becoming an international singer, and singing in English, has always been Aalto's dream, and that is why she auditioned for \"The X Factor UK\".\n\nSinging in many languages has been Aalto's passion since childhood and she has performed to many international groups visiting Finland. One popular example is \"Let it go\" in 15 languages that she made for one group of 20 different nationalities. Aalto can sing in many languages, but she has studied English, Swedish, French, Japanese and Chinese.\n\nIn 2007, she participated in \"Talent Suomi\" (\"Talent Finland\"), finishing in the top three.\n\nIn addition to her singing career, Saara is also an accomplished actress, having starred as Dorothy and Phannee in the Finnish production of \"Wicked - The Musical\" and as Mary Magdalene in \"Jesus Christ Superstar\". From 2008 to 2011 she appeared in the Helsinki City Theatre musicals, first as Kelsi in \"High School Musical on Stage!\" and its , and then in \"Wicked\".\n\nSaara Aalto took part on two separate occasions in the selection process in a bid to represent Finland in the Eurovision Song Contest. She participated in the qualifying rounds in Finland for Eurovision 2011 with her self-penned song \"\", placing second to Paradise Oskar who went on to represent Finland with his song \"Da Da Dam\". She received 40.7% of the public televotes, compared to 46.7% for Oskar. Aalto's recording was released as a single by Yume Records, a self-release record label she founded together with Teemu Roivainen in 2011.\n\nHer second bid to represent Finland was for Eurovision Song Contest 2016 through the competition \"Uuden Musiikin Kilpailu\" where she sang another of her own compositions titled \"No Fear\", again placing second. Although winning the public vote, she was disadvantaged with the judging vote going to the eventual winner Sandhja who went on to represent Finland with the song \"Sing It Away\".\n\nIn an interview Aalto described her music.\n\nI call my music Epic Love Pop! I love dramatic and emotional music with a show feeling. I also want to bring love, joy and encouragement for everybody; be yourself and inspire others! I’ve studied piano since I was a little kid and through music schools, singing competitions, my career and concerts (in Finland and all over the world) I’ve gotten lots of experience and confidence to be who I am as an artist. (...) I’ve performed in musicals and big shows in Finland. (...) I’m happy to be versatile performer.\n\nSaara Aalto represented Finland in the Eurovision Song Contest 2018. The decision was made by the Finnish national broadcaster Yle. Her song was selected among three choices in a national final; the selected song was \"Monsters\". She qualified for the final on 8th May but finished second last (25th) on 12th May.\n\nFollowing her performance of \"Blessed with Love\" for qualification in Eurovision 2011, that December she was involved in the Santa Claus Season's Greetings Song Contest, where she sang \"Ai De Zhu Fu,\" a Chinese (Mandarin) version of her Eurovision song \"Blessed with Love\". \n\nIn November 2012 Aalto was guest vocalist at Robert Wells's highly successful show \"Rhapsody in Rock\" at Shanghai Daning theatre. And following a successful performance in Shanghai in May 2013, opening for Spanish tenor José Carreras and singing several songs in Chinese, Saara Aalto returned to the city to duet with tenor Han Peng at the Closing Ceremony of the Shanghai International Film Festival in June 2013 broadcast to over 800 million viewers in 10 countries.\n\nFinding great reception from Chinese audiences, in July 2013 Aalto went on to release a full album destined for Mainland China and other Chinese markets also titled \"Ai De Zhu Fu\". On the album she performed nine of the album's 13 tracks in Mandarin language.\n\nAalto participated in season one of \"The Voice of Finland\". In the blind auditions, broadcast on 6 February 2012 on the Finnish commercial television channel Nelonen, she performed \"Taking Chances\" from Celine Dion with all four coaches, Elastinen, Lauri Tähkä, Paula Koivuniemi and Michael Monroe turning their chairs. She opted to be part of Team Monroe.\n\nIn the Battle Round on 17 February, she was confronted with team competitor Anna Inginmaa, both singing \"True Colors\". Coach Monroe opted for Aalto to go to the next round. In the live show on 23 March, she sang \"Barracuda\" and was saved by Monroe after failing to get enough votes from the public. She followed it up with the French language \"Je suis malade\" from Serge Lama and was safe after the public vote. In the semifinals with her rendition of \"Over the Rainbow\", she qualified for the finals held on 20 April where she sang \"I'm Gonna Be Strong\" from Frankie Laine and an original song titled \"My Love\". She placed second to winner Mikko Sipola.\n\nAalto auditioned for the thirteenth UK series of \"The X Factor\" in 2016, singing a cover of Sia's \"Chandelier\", which earned her three yeses from the attending judges. At the bootcamp, she performed \"On the Radio\" by Donna Summer and was sent through as part of the over-25s category with Sharon Osbourne as her mentor. At the six-chair challenge, she performed \"I See Fire\" by Ed Sheeran and was sent home by Osbourne who stated a lack of connection. However, after the crowd chanted to bring her back, Aalto was given a chance to perform Serge Lama's \"Je suis malade\", which caused the crowd to turn on her, while Nicole Scherzinger criticized her song choice, stating \"we ain't in France\". Osbourne then asked Aalto to leave. \n\nAalto was brought back as Scherzinger's wildcard pick for the over-25s category and therefore went to judges' houses. She was chosen to proceed to the live shows after performing ABBA's \"The Winner Takes It All\". In week 1 of the live shows, Aalto finished in the bottom three after performing \"Let It Go\" and faced a sing-off against the duo Bratavio, where she performed \"Alive\" by Sia. Aalto was saved with only Louis Walsh voting to eliminate her, though praising her performance. The following week, she sang \"River Deep – Mountain High\" and finished again in the bottom three, performing \"Run\" by Snow Patrol in a sing-off against Freddy Parker. The result went to deadlock and Aalto was saved as she had the greater public vote. In week 3 she performed \"It's Oh So Quiet\" coming 2nd in the public vote, and in week 4 Aalto sang \"Bad Romance\", placing 3rd in the public vote. In week 5, Aalto finished in the bottom three for the third time after her rendition of \"Sound of the Underground\", but was saved by majority vote after performing \"Who You Are\" in a sing-off against the group Four of Diamonds. Her following performances were \"No More Tears (Enough Is Enough)\" in week 6 placing 2nd and \"My Heart Will Go On\" in week 7 again placing 2nd. In week 8, she sang \"The Winner Takes It All\" and a medley of \"Diamonds Are Forever\" and \"Diamonds Are a Girl's Best Friend\", topping the public vote for the first time. Following the eliminations of Relley C in week 3 and Honey G in week 8, Aalto became Osbourne's last remaining act in the competition.\n\nIn the semifinals on 3 December, she sang a medley of \"White Christmas\" and \"All I Want for Christmas Is You\", and \"Chandelier\" as 2nd song, topping the vote again reaching the Final 3, where on 10 December she performed \"Everybody Wants to Rule the World\" and \"Bohemian Rhapsody\", the latter as a duet with Adam Lambert, again topping the public vote. With the elimination of the group 5 After Midnight she was in the Final 2 facing Matt Terry. In the Final on 11 December, she performed \"It's Oh So Quiet\" and \"I Didn't Know My Own Strength\". In the ensuing public vote, she placed as the runner-up to eventual winner Matt Terry with 40.4% of the votes.\n\nIn her press-conference on 20 December 2016, Aalto revealed that she works with the joint effort of ROAR Global and Global career management companies, and that she has signed a five-album recording deal with Sony Music UK and Sony Music Finland. In October 2017, Aalto announced she had left Sony Music for Warner Music Group. In a statement, she said: \"I was honoured I was signed to Sony. But later I felt like they weren't the right team. When I met the people at Warner they were very excited about my style and making me into this big theatrical artist.\" On 7 November 2017, it was announced that Aalto would represent Finland in the Eurovision Song Contest 2018 in Lisbon, Portugal. The three bidding entries will be released by Yle and Saara Aalto, along with an accompanying music video, on 9, 16 and 23 February 2018, respectively. Aalto was a judge on the second series of the Finnish version of \"The X Factor\", mentoring the girls. Aalto could not attend the finale night of \"The X Factor Finland\" due to her commitments with The Eurovision, so Matt Terry represented her instead, but her category's last girl, Tika Liljegren, was crowned the winner, making Aalto the winning mentor. Aalto announced in March 2018 via social media that her first album under Warner Music Group, \"Wild Wild Wonderland\", would be released on 27 April 2018.\n\nAalto had a nine-year relationship with singer , but the couple broke up in 2013. Aalto and Roivainen continued collaborating in music. Aalto later began a relationship with a female fan, personal trainer and life coach, Meri Sopanen. They became engaged on their two-year anniversary in August 2016. Aalto now identifies as a lesbian.\n\nAalto and Sopanen moved to London in January 2017. The idea to audition for \"The X Factor\" came from Sopanen.\n\n"}
{"id": "42812764", "url": "https://en.wikipedia.org/wiki?curid=42812764", "title": "Selection shadow", "text": "Selection shadow\n\nThe selection shadow is a concept involved with the evolutionary theories of ageing that states that selection pressures on an individual decrease as an individual ages and passes sexual maturity, resulting in a \"shadow\" of time where selective fitness is not considered. Over generations, this results in maladaptive mutations that accumulate later in life due to aging being non-adaptive toward reproductive fitness. The concept was first worked out by J. B. S. Haldane and Peter Medawar in the 1940s, with Medawar creating the first graphical model.\n\nThe model developed by Medawar states that due to the dangerous conditions and pressures from the environment, including predators and diseases, most individuals in the wild die not long after sexual maturity. Therefore, there is a low probability for individuals to survive to an advanced age and suffer the effects related to aging. In conjunction with this, the effects of natural selection decrease as age increases, so that later individual performance is ignored by selection forces. This results in beneficial mutations not being selected for if they only have a positive result later in life, along with later in life deleterious mutations not being selected against. Due to the fitness of an individual not being affected once it is past its reproductive prime, later mutations and effects are considered to be in the \"shadow\" of selection.\n\nThis concept would later be adapted into Medawar's 1952 mutation accumulation hypothesis, which was itself expanded upon by George C. Williams in his 1957 antagonistic pleiotropy hypothesis.\n\nA classical requirement and constraint of the model is that the number of individuals within a population that live to reach senescence must be small in number. If this is not true for a population, then the effects of old age will not be under a selection shadow and instead affect adaptation and evolution of the population as a whole. At the same time, however, this requirement has been challenged by increasing evidence of senescence being more common in wild populations than previously expected, especially among birds and mammals, while the effects of the selection shadow remain present.\n\nSome scientists, however, have criticized the idea of aging being non-adaptive, instead adopting the theory of \"Death by Design\". This theory follows the work of August Weismann, which states that aging specifically evolved as an adaptation, and disagrees with Medawar's model as a perceived oversimplification of the impact older organisms have on evolution. It is also claimed that older organisms have a higher reproductive capacity due to being better fit in order to reach their age, rather than their capacity being equal as in Medawar's calculations.\n"}
{"id": "7513975", "url": "https://en.wikipedia.org/wiki?curid=7513975", "title": "Simo Aalto", "text": "Simo Aalto\n\nSimo Aalto (born March 19, 1960) is a Finnish magician.\n\n\n"}
{"id": "25816276", "url": "https://en.wikipedia.org/wiki?curid=25816276", "title": "Sugarscape", "text": "Sugarscape\n\nSugarscape is a model for artificially intelligent agent-based social simulation following some or all rules presented by Joshua M. Epstein & Robert Axtell in their book \"Growing Artificial Societies\".\n\nFundaments of Sugarscape models can be traced back to the University of Maryland where economist Thomas Schelling presented his paper titled \"Models of Segregation\". Written in 1969, Schelling and the rest of the social environment modelling fraternity had their options limited by a lack of adequate computing power and an applicable programming mechanism to fully develop the potential of their model.\n\nJohn Conway's agent-based simulation \"Game of Life\" was enhanced and applied to Schelling's original idea by Joshua M. Epstein and Robert Axtell in their book \"Growing Artificial Societies\". To demonstrate their findings on the field of agent-based simulation, a model was created and distributed with their book on CD-ROM. The concept of this model has come to be known as \"the Sugarscape model\". Since then, the name \"Sugarscape\" has been used for agent-based models using rules similar to those defined by Epstein & Axtell.\n\nAll Sugarscape models include the agents (inhabitants), the environment (a two-dimensional grid) and the rules governing the interaction of the agents with each other and the environment.\n\nThe original model presented by J. Epstein & R. Axtell (considered as the first large scale agent model) is based on a 51x51 cell grid, where every cell can contain different amounts of sugar (or spice). In every step agents look around, find the closest cell filled with sugar, move and metabolize. They can leave pollution, die, reproduce, inherit sources, transfer information, trade or borrow sugar, generate immunity or transmit diseases - depending on the specific scenario and variables defined at the set-up of the model.\n\nSugar in simulation could be seen as a metaphor for resources in an artificial world through which the examiner can study the effects of social dynamics such as evolution, marital status and inheritance on populations.\n\nExact simulation of the original rules provided by J. Epstein & R. Axtell in their book can be problematic and it is not always possible to recreate the same results as those presented in \"Growing Artificial Societies\".\n\nThe Sugarscape model has had several implementations, some of which are available as open source software.\n\nAn original implementation was developed in Ascape, Java software suitable for agent-based social simulation. The Sugarscape model remains part of the built-in library of models distributed with Ascape.\n\nNetLogo has been used to build Sugarscape models. Three Sugarscape scenarios are included in the NetLogo Models Library: \"Immediate Growback\", \"Constant Growback\" and \"Wealth Distribution\". Besides these three scenarios lies Iain Weaver's Sugarscape NetLogo model, which is part of the User Community Models Library. \"It builds on Owen Densmore's NetLogo community model to encompass all rules discussed in \"Growing Artificial Societies\" with the exception of the combat rule (although trivial to include, it adds little value to the model).\" The model is equipped with rich documentation including instructions for successful replication of the original Sugarscape rules.\n\nDue to the emergent nature of agent-based models (ABMs), it is critical that the population sizes in the simulations match the population sizes of the dynamic systems being modelled. However, the performance of contemporary agent simulation frameworks has been inadequate to handle such large population sizes and parallel computing frameworks designed to run on computing clusters has been limited by available bandwidth. As computing power increases with Moore's law, the size and complexity of simulation frameworks can be expected to increase. The team of R. M. D’Souza, M. Lysenko and K Rahmani from Michigan Technological University used a Sugarscape model to demonstrate the power of Graphics processing units (GPU) in ABM simulations with over 50 updates per second with agent populations exceeding 2 millions.\n\nAnother implementation can be found written in Mathematica.\n\nGMU's MASON project, available under the Academic Free License, also includes an implementation of Sugarscape.\n\n"}
{"id": "53776295", "url": "https://en.wikipedia.org/wiki?curid=53776295", "title": "Touko Aalto", "text": "Touko Aalto\n\nTouko Juhani Aalto (born 1 April 1984) is a Finnish politician. He was chairman of the Green League from 2017 to 2018. He has represented the electoral district of Central Finland in the Parliament of Finland since 2015.\n\nAalto was born in Savonlinna and later lived in Joensuu. He currently lives in Jyväskylä, Finland. Aalto graduated as Bachelor of Social Sciences from the University of Jyväskylä in 2008. He worked as a cleaner and salesperson before becoming parliamentary assistant of MP Jani Toivola in 2011.\n\nAalto was elected to the City Council of Jyväskylä in 2008 and to the Parliament in 2015. He is the first Green Member of the Parliament from Central Finland. He is currently member of the Finance Committee of the Parliament. In the 2017 municipal elections, Aalto received the most personal votes in Jyväskylä at the same time when the Green League became the largest party of Jyväskylä. On 17 June 2017, Aalto won the Green League leadership election, and became the chairman of the party.\n\nIn September 2018, Aalto took a sick leave due to exhaustion and his duties were temporarily handed to the first deputy chair Maria Ohisalo. In October 2018, he announced that he is resigning from his post, citing depression and fatigue.\n\nAalto was married with Johanna Pietiläinen from 2015 to 2017, when the couple got divorced due to Aalto's affair with a staff member of the Green League. The couple did not have any children.\n"}
{"id": "8383637", "url": "https://en.wikipedia.org/wiki?curid=8383637", "title": "Tree of life (biology)", "text": "Tree of life (biology)\n\nThe tree of life or universal tree of life is a metaphor, model and research tool used to explore the evolution of life and describe the relationships between organisms, both living and extinct, as described in a famous passage in Charles Darwin's \"On the Origin of Species\" (1859).\nTree diagrams originated in the medieval era to represent genealogical relationships.\n\nPhylogenetic tree diagrams in the evolutionary sense date back to at least the early 19th century.\n\nThe term phylogeny for the evolutionary relationships of species through time was coined by Ernst Haeckel, who went further than Darwin in proposing phylogenic histories of life. In contemporary usage, \"tree of life\" refers to the compilation of comprehensive phylogenetic databases rooted at the last universal common ancestor of life on Earth. The Open Tree of Life, first published 2015, is a project to compile such a database for free public access.\n\nAlthough the mutability of species may have appeared in paintings and trees have been used as a metaphor for other purposes (Porphyrian tree) earlier than 1800, the combination of the concept of branching evolution and the tree image did not appear before 1800. The earliest tree of life was published by the French botanist Augustin Augier in 1801. It shows the relationships between members of the plant kingdom.\n\nJean-Baptiste Lamarck (1744–1829) produced the first branching tree of animals in his \"Philosophie Zoologique\" (1809). It was an upside-down tree starting with worms and ending with mammals. However, Lamarck did not believe in common descent of all life. Instead, he believed that life consists of separate parallel lines advancing from simple to complex.\n\nThe American geologist Edward Hitchcock (1793–1864) published in 1840 the first tree of life based on paleontology in his \"Elementary Geology\". On the vertical axis are paleontological periods. Hitchcock made a separate tree for plants (left) and animals (right). The plant and the animal tree are not connected at the bottom of the chart. Furthermore, each tree starts with multiple origins. Hitchcock's tree was more realistic than Darwin's 1859 theoretical tree (see below) because Hitchcock used real names in his trees. It is also true that Hitchcock's trees were branching trees. However, they were not \"evolutionary\" trees, because Hitchcock believed that a deity was the agent of change. That was an important difference with Darwin.\n\nThe first edition of Robert Chambers' \"Vestiges of the Natural History of Creation\", which was published anonymously in 1844 in England, contained (p. 212) in the chapter \"Hypothesis of the development of the vegetable and animal kingdoms\". It shows a model of embryological development where fish (F), reptiles (R), and birds (B) represent branches from a path leading to mammals (M). In the text this branching tree idea is tentatively applied to the history of life on earth: \"there may be branching\" (p. 191), but the branching diagram is not displayed again specifically for this purpose. However, the image of a branching tree could easily have inspired others to use it explicitly as a representation of the history of life on earth.\n\nIn 1858, a year before Darwin's \"Origin\", the paleontologist Heinrich Georg Bronn (1800–1862) published a hypothetical tree labeled with letters. Although not a creationist, Bronn did not propose a mechanism of change.\n\nCharles Darwin (1809–1882) used the concept of a tree of life in the context of his theory of evolution. In \"On the Origin of Species\" (1859) Chapter IV he presented an abstract diagram of a theoretical tree of life for species of an unnamed large genus (see figure). On the horizontal base line hypothetical species within this genus are labelled A – L and are spaced irregularly to indicate how distinct they are from each other, and are above broken lines at various angles suggesting that they have diverged from one or more common ancestors. On the vertical axis divisions labelled I – XIV each represent a thousand generations. From A, diverging lines show branching descent producing new varieties, some of which become extinct, so that after ten thousand generations descendants of A have become distinct new varieties or even sub-species a, f, and m. Similarly, the descendants of I have diversified to become the new varieties w and z. The process is extrapolated for a further four thousand generations so that the descendants of A and I become fourteen new species labelled a to z. While F has continued for fourteen thousand generations relatively unchanged, species B,C,D,E,G,H,K and L have gone extinct. In Darwin's own words: \"Thus the small differences distinguishing varieties of the same species, will steadily tend to increase till they come to equal the greater differences between species of the same genus, or even of distinct genera.\". This is a branching pattern with no names given to species, unlike the more linear tree Ernst Haeckel made years later (figure below) which includes the names of species and shows a more linear development from \"lower\" to \"higher\" species. In his summary to the section, Darwin put his concept in terms of the metaphor of the tree of life:\n\nThe meaning and importance of Darwin's use of the tree of life metaphor have been extensively discussed. Stephen Jay Gould, for example, says that Darwin placed the famous passage quoted above \"at a crucial spot in his text\", where it marked the conclusion of his argument for natural selection, illustrating both the interconnectedness by descent of organisms as well as their success and failure in the history of life. David Penny has written that Darwin did not use the tree of life to describe the relationship between groups of organisms, but to suggest that, as with branches in a living tree, lineages of species competed with and supplanted one another. Petter Hellström has demonstrated that Darwin consciously chose to name his tree after the biblical tree of life, as described in Genesis, thus relating his theory to Christian mythology, and suggesting that he did so to mobilise the imagination of his readers.\n\nErnst Haeckel (1834–1919) constructed several trees of life. His first sketch (in the 1860s) of his famous tree of life shows \"Pithecanthropus alalus\" as the ancestor of \"Homo sapiens\". His 1866 tree of life from \"Generelle Morphologie der Organismen\" shows three kingdoms: Plantae, Protista and Animalia. His 1879 'Pedigree of Man' was published in \"The Evolution of Man\".\n\nThe model of a tree is still considered valid for eukaryotic life forms. , research into the earliest branches of the eukaryote tree has suggested a tree with either four or two supergroups. There does not yet appear to be a consensus; in a review article, Roger and Simpson conclude that \"with the current pace of change in our understanding of the eukaryote tree of life, we should proceed with caution.\"\n\nIn 2015, the first draft of the Open Tree of Life was published, in which information from nearly 500 previously published trees was combined into a single online database, free to browse and download.\n\nIn 2016, a new tree of life, summarizing the evolution of all known life forms, was published, illustrating the latest genetic findings that the branches were mainly composed of bacteria. The new study incorporated over a thousand newly discovered bacteria and archaea.\n\nThe prokaryotes (the two domains of bacteria and archaea) and certain animals such as bdelloid rotifers and the tardigrade have the ability to transfer genetic information between unrelated organisms through horizontal gene transfer. Recombination, gene loss, duplication, and gene creation are a few of the processes by which genes can be transferred within and between bacterial and archaeal species, causing variation that is not due to vertical transfer. There is emerging evidence of horizontal gene transfer within the prokaryotes at the single and multicell level, so the tree of life does not explain the full complexity of the situation in the prokaryotes.\n\n\n"}
{"id": "34790424", "url": "https://en.wikipedia.org/wiki?curid=34790424", "title": "Viveza criolla", "text": "Viveza criolla\n\nViveza criolla is a Spanish language phrase literally meaning \"creole' cleverness\" and may be translated as \"creoles' cunning\", describing a way of life in Chile, Argentina, Uruguay, Colombia and Venezuela, among other Latin American countries. It is a philosophy of progress along the line of least resistance and ignoring rules, a lack of sense of responsibility and consideration for others, and it extends to all social groups and throughout the whole country, although it predominates in Buenos Aires. Viveza criolla has been called \"the principal cause of a moral, cultural, economic, social and political crisis\". It is a similar concept to jeitinho brasileiro in Brazil.\n\nViveza criolla includes:\n\n\n"}
