{"id": "3360349", "url": "https://en.wikipedia.org/wiki?curid=3360349", "title": "Albert Lucas (juggler)", "text": "Albert Lucas (juggler)\n\nAlbert Lucas is an American juggler born in 1960 to Albert and Yvonne Moreira. He is notable for juggling while ice skating, setting numbers juggling records, and promoting sport juggling.\nWith the encouragement of his father, Albert began juggling at age 3 and performing at age 4 in comedy clubs, small circuses and night clubs. From age 8 to 11 he toured with Liberace and then performed in Las Vegas. From age 12 to 22, he traveled the world performing his juggling act on ice with the Ice Capades. Albert spent several years performing in the \"Around the World on Ice\" show at Busch Gardens Theme Park. Albert has performed at both the NBA Finals and the NHL Stanley Cup Finals along with numerous other sporting events including the Rhode Island vs College of Charleston basketball game in 2018.\n\nAlbert co-founded the International Sport Juggling Federation in order to develop sport juggling, including joggling, with the goal of re-introducing it to the Olympics. He has joggled in 12 marathons, including a marathon with no drops in 1987, which established a Guinness world record.\n\nAt the age of 10, Albert won the Numbers Competition with seven rings at the International Jugglers' Association Summer Festival in 1970. He was the U.S. Nationals Champion at the IJA Summer Festival in 1984. He is the first person to have qualified 10 objects in competition, juggling 10 rings for 20 catches at the IJA Summer Festivals in 1996 and 2002. He currently holds the world record for being the only person to flash 13 rings.\n\nKit Summers named a juggling trick after Albert—an \"albert throw\"—which is a reverse club throw under the leg, made from front to back without either foot leaving the floor. This trick was performed by many earlier jugglers, originating with Morris Cronin, who specialized in club juggling during the early 1900s.\n\n\n"}
{"id": "752", "url": "https://en.wikipedia.org/wiki?curid=752", "title": "Art", "text": "Art\n\nArt is a diverse range of human activities in creating visual, auditory or performing artifacts (artworks), expressing the author's imaginative, conceptual idea, or technical skill, intended to be appreciated for their beauty or emotional power. In their most general form these activities include the production of works of art, the criticism of art, the study of the history of art, and the aesthetic dissemination of art. \n\nThe three classical branches of art are painting, sculpture and architecture. Music, theatre, film, dance, and other performing arts, as well as literature and other media such as interactive media, are included in a broader definition of the arts. Until the 17th century, \"art\" referred to any skill or mastery and was not differentiated from crafts or sciences. In modern usage after the 17th century, where aesthetic considerations are paramount, the fine arts are separated and distinguished from acquired skills in general, such as the decorative or applied arts.\n\nThough the definition of what constitutes art is disputed and has changed over time, general descriptions mention an idea of imaginative or technical skill stemming from human agency and creation. The nature of art and related concepts, such as creativity and interpretation, are explored in a branch of philosophy known as aesthetics.\n\nIn the perspective of the history of art, artistic works have existed for almost as long as humankind: from early pre-historic art to contemporary art; however, some theories restrict the concept of \"artistic works\" to modern Western societies. One early sense of the definition of \"art\" is closely related to the older Latin meaning, which roughly translates to \"skill\" or \"craft,\" as associated with words such as \"artisan.\" English words derived from this meaning include \"artifact\", \"artificial\", \"artifice\", \"medical arts\", and \"military arts\". However, there are many other colloquial uses of the word, all with some relation to its etymology.\nOver time, philosophers like Plato, Aristotle, Socrates and Kant, among others, questioned the meaning of art. Several dialogues in Plato tackle questions about art: Socrates says that poetry is inspired by the muses, and is not rational. He speaks approvingly of this, and other forms of divine madness (drunkenness, eroticism, and dreaming) in the \"Phaedrus \"(265a–c), and yet in the \"\"Republic\"\" wants to outlaw Homer's great poetic art, and laughter as well. In \"Ion\", Socrates gives no hint of the disapproval of Homer that he expresses in the \"Republic\". The dialogue \"Ion\" suggests that Homer's \"Iliad\" functioned in the ancient Greek world as the Bible does today in the modern Christian world: as divinely inspired literary art that can provide moral guidance, if only it can be properly interpreted.\n\nWith regards to the literary art and the musical arts, Aristotle considered epic poetry, tragedy, comedy, dithyrambic poetry and music to be mimetic or imitative art, each varying in imitation by medium, object, and manner. For example, music imitates with the media of rhythm and harmony, whereas dance imitates with rhythm alone, and poetry with language. The forms also differ in their object of imitation. Comedy, for instance, is a dramatic imitation of men worse than average; whereas tragedy imitates men slightly better than average. Lastly, the forms differ in their manner of imitation—through narrative or character, through change or no change, and through drama or no drama. Aristotle believed that imitation is natural to mankind and constitutes one of mankind's advantages over animals.\n\nThe second, and more recent, sense of the word \"art\" as an abbreviation for \"creative art\" or \"fine art\" emerged in the early 17th century. Fine art refers to a skill used to express the artist's creativity, or to engage the audience's aesthetic sensibilities, or to draw the audience towards consideration of more refined or \"finer\" work of art.\n\nWithin this latter sense, the word \"art\" may refer to several things: (i) a study of a creative skill, (ii) a process of using the creative skill, (iii) a product of the creative skill, or (iv) the audience's experience with the creative skill. The creative arts (\"art\" as discipline) are a collection of disciplines which produce \"artworks\" (\"art\" as objects) that are compelled by a personal drive (art as activity) and convey a message, mood, or symbolism for the perceiver to interpret (art as experience). Art is something that stimulates an individual's thoughts, emotions, beliefs, or ideas through the senses. Works of art can be explicitly made for this purpose or interpreted on the basis of images or objects. For some scholars, such as Kant, the sciences and the arts could be distinguished by taking science as representing the domain of knowledge and the arts as representing the domain of the freedom of artistic expression.\n\nOften, if the skill is being used in a common or practical way, people will consider it a craft instead of art. Likewise, if the skill is being used in a commercial or industrial way, it may be considered commercial art instead of fine art. On the other hand, crafts and design are sometimes considered applied art. Some art followers have argued that the difference between fine art and applied art has more to do with value judgments made about the art than any clear definitional difference. However, even fine art often has goals beyond pure creativity and self-expression. The purpose of works of art may be to communicate ideas, such as in politically, spiritually, or philosophically motivated art; to create a sense of beauty (see aesthetics); to explore the nature of perception; for pleasure; or to generate strong emotions. The purpose may also be seemingly nonexistent.\n\nThe nature of art has been described by philosopher Richard Wollheim as \"one of the most elusive of the traditional problems of human culture\". Art has been defined as a vehicle for the expression or communication of emotions and ideas, a means for exploring and appreciating formal elements for their own sake, and as \"mimesis\" or representation. Art as mimesis has deep roots in the philosophy of Aristotle. Leo Tolstoy identified art as a use of indirect means to communicate from one person to another. Benedetto Croce and R.G. Collingwood advanced the idealist view that art expresses emotions, and that the work of art therefore essentially exists in the mind of the creator. The theory of art as form has its roots in the philosophy of Kant, and was developed in the early twentieth century by Roger Fry and Clive Bell. More recently, thinkers influenced by Martin Heidegger have interpreted art as the means by which a community develops for itself a medium for self-expression and interpretation. George Dickie has offered an institutional theory of art that defines a work of art as any artifact upon which a qualified person or persons acting on behalf of the social institution commonly referred to as \"the art world\" has conferred \"the status of candidate for appreciation\". Larry Shiner has described fine art as \"not an essence or a fate but something we have made. Art as we have generally understood it is a European invention barely two hundred years old.\"\n\nArt may be characterized in terms of mimesis (its representation of reality), narrative (storytelling), expression, communication of emotion, or other qualities. During the Romantic period, art came to be seen as \"a special faculty of the human mind to be classified with religion and science\".\n\nThe oldest documented forms of art are visual arts, which include creation of images or objects in fields including today painting, sculpture, printmaking, photography, and other visual media. Sculptures, cave paintings, rock paintings and petroglyphs from the Upper Paleolithic dating to roughly 40,000 years ago have been found, but the precise meaning of such art is often disputed because so little is known about the cultures that produced them. The oldest art objects in the world—a series of tiny, drilled snail shells about 75,000 years old—were discovered in a South African cave. Containers that may have been used to hold paints have been found dating as far back as 100,000 years. Etched shells by \"Homo erectus\" from 430,000 and 540,000 years ago were discovered in 2014.\n\nMany great traditions in art have a foundation in the art of one of the great ancient civilizations: Ancient Egypt, Mesopotamia, Persia, India, China, Ancient Greece, Rome, as well as Inca, Maya, and Olmec. Each of these centers of early civilization developed a unique and characteristic style in its art. Because of the size and duration of these civilizations, more of their art works have survived and more of their influence has been transmitted to other cultures and later times. Some also have provided the first records of how artists worked. For example, this period of Greek art saw a veneration of the human physical form and the development of equivalent skills to show musculature, poise, beauty, and anatomically correct proportions.\n\nIn Byzantine and Medieval art of the Western Middle Ages, much art focused on the expression of subjects about Biblical and religious culture, and used styles that showed the higher glory of a heavenly world, such as the use of gold in the background of paintings, or glass in mosaics or windows, which also presented figures in idealized, patterned (flat) forms. Nevertheless, a classical realist tradition persisted in small Byzantine works, and realism steadily grew in the art of Catholic Europe.\n\nRenaissance art had a greatly increased emphasis on the realistic depiction of the material world, and the place of humans in it, reflected in the corporeality of the human body, and development of a systematic method of graphical perspective to depict recession in a three-dimensional picture space.\n\nIn the east, Islamic art's rejection of iconography led to emphasis on geometric patterns, calligraphy, and architecture. Further east, religion dominated artistic styles and forms too. India and Tibet saw emphasis on painted sculptures and dance, while religious painting borrowed many conventions from sculpture and tended to bright contrasting colors with emphasis on outlines. China saw the flourishing of many art forms: jade carving, bronzework, pottery (including the stunning terracotta army of Emperor Qin), poetry, calligraphy, music, painting, drama, fiction, etc. Chinese styles vary greatly from era to era and each one is traditionally named after the ruling dynasty. So, for example, Tang dynasty paintings are monochromatic and sparse, emphasizing idealized landscapes, but Ming dynasty paintings are busy and colorful, and focus on telling stories via setting and composition. Japan names its styles after imperial dynasties too, and also saw much interplay between the styles of calligraphy and painting. Woodblock printing became important in Japan after the 17th century.\n\nThe western Age of Enlightenment in the 18th century saw artistic depictions of physical and rational certainties of the clockwork universe, as well as politically revolutionary visions of a post-monarchist world, such as Blake's portrayal of Newton as a divine geometer, or David's propagandistic paintings. This led to Romantic rejections of this in favor of pictures of the emotional side and individuality of humans, exemplified in the novels of Goethe. The late 19th century then saw a host of artistic movements, such as academic art, Symbolism, impressionism and fauvism among others.\n\nThe history of twentieth-century art is a narrative of endless possibilities and the search for new standards, each being torn down in succession by the next. Thus the parameters of impressionism, Expressionism, Fauvism, Cubism, Dadaism, Surrealism, etc. cannot be maintained very much beyond the time of their invention. Increasing global interaction during this time saw an equivalent influence of other cultures into Western art. Thus, Japanese woodblock prints (themselves influenced by Western Renaissance draftsmanship) had an immense influence on impressionism and subsequent development. Later, African sculptures were taken up by Picasso and to some extent by Matisse. Similarly, in the 19th and 20th centuries the West has had huge impacts on Eastern art with originally western ideas like Communism and Post-Modernism exerting a powerful influence.\n\nModernism, the idealistic search for truth, gave way in the latter half of the 20th century to a realization of its unattainability. Theodor W. Adorno said in 1970, \"It is now taken for granted that nothing which concerns art can be taken for granted any more: neither art itself, nor art in relationship to the whole, nor even the right of art to exist.\" Relativism was accepted as an unavoidable truth, which led to the period of contemporary art and postmodern criticism, where cultures of the world and of history are seen as changing forms, which can be appreciated and drawn from only with skepticism and irony. Furthermore, the separation of cultures is increasingly blurred and some argue it is now more appropriate to think in terms of a global culture, rather than of regional ones.\n\nIn \"The Origin of the Work of Art\", Martin Heidegger, a German philosopher and a seminal thinker, describes the essence of art in terms of the concepts of being and truth. He argues that art is not only a way of expressing the element of truth in a culture, but the means of creating it and providing a springboard from which \"that which is\" can be revealed. Works of art are not merely representations of the way things are, but actually produce a community's shared understanding. Each time a new artwork is added to any culture, the meaning of what it is to exist is inherently changed.\n\nThe creative arts are often divided into more specific categories, typically along perceptually distinguishable categories such as media, genre, styles, and form. Art form refers to the elements of art that are independent of its interpretation or significance. It covers the methods adopted by the artist and the physical composition of the artwork, primarily non-semantic aspects of the work (i.e., figurae), such as color, contour, dimension, medium, melody, space, texture, and value. Form may also include visual design principles, such as arrangement, balance, contrast, emphasis, harmony, proportion, proximity, and rhythm.\n\nIn general there are three schools of philosophy regarding art, focusing respectively on form, content, and context. Extreme Formalism is the view that all aesthetic properties of art are formal (that is, part of the art form). Philosophers almost universally reject this view and hold that the properties and aesthetics of art extend beyond materials, techniques, and form. Unfortunately, there is little consensus on terminology for these informal properties. Some authors refer to subject matter and content – i.e., denotations and connotations – while others prefer terms like meaning and significance.\n\nExtreme Intentionalism holds that authorial intent plays a decisive role in the meaning of a work of art, conveying the content or essential main idea, while all other interpretations can be discarded. It defines the subject as the persons or idea represented, and the content as the artist's experience of that subject. For example, the composition of Napoleon I on his Imperial Throne is partly borrowed from the Statue of Zeus at Olympia. As evidenced by the title, the subject is Napoleon, and the content is Ingres's representation of Napoleon as \"Emperor-God beyond time and space\". Similarly to extreme formalism, philosophers typically reject extreme intentionalism, because art may have multiple ambiguous meanings and authorial intent may be unknowable and thus irrelevant. Its restrictive interpretation is \"socially unhealthy, philosophically unreal, and politically unwise\".\n\nFinally, the developing theory of post-structuralism studies art's significance in a cultural context, such as the ideas, emotions, and reactions prompted by a work. The cultural context often reduces to the artist's techniques and intentions, in which case analysis proceeds along lines similar to formalism and intentionalism. However, in other cases historical and material conditions may predominate, such as religious and philosophical convictions, sociopolitical and economic structures, or even climate and geography. Art criticism continues to grow and develop alongside art.\n\nArt can connote a sense of trained ability or mastery of a medium. Art can also simply refer to the developed and efficient use of a language to convey meaning with immediacy and or depth. Art can be defined as an act of expressing feelings, thoughts, and observations.\n\nThere is an understanding that is reached with the material as a result of handling it, which facilitates one's thought processes.\nA common view is that the \"art\", particular in its elevated sense, requires a certain level of creative expertise by the artist, whether this be a demonstration of technical ability, an originality in stylistic approach, or a combination of these two. Traditionally skill of execution was viewed as a quality inseparable from art and thus necessary for its success; for Leonardo da Vinci, art, neither more nor less than his other endeavors, was a manifestation of skill. Rembrandt's work, now praised for its ephemeral virtues, was most admired by his contemporaries for its virtuosity. At the turn of the 20th century, the adroit performances of John Singer Sargent were alternately admired and viewed with skepticism for their manual fluency, yet at nearly the same time the artist who would become the era's most recognized and peripatetic iconoclast, Pablo Picasso, was completing a traditional academic training at which he excelled.\nA common contemporary criticism of some modern art occurs along the lines of objecting to the apparent lack of skill or ability required in the production of the artistic object. In conceptual art, Marcel Duchamp's \"Fountain\" is among the first examples of pieces wherein the artist used found objects (\"ready-made\") and exercised no traditionally recognised set of skills. Tracey Emin's \"My Bed\", or Damien Hirst's \"The Physical Impossibility of Death in the Mind of Someone Living\" follow this example and also manipulate the mass media. Emin slept (and engaged in other activities) in her bed before placing the result in a gallery as work of art. Hirst came up with the conceptual design for the artwork but has left most of the eventual creation of many works to employed artisans. Hirst's celebrity is founded entirely on his ability to produce shocking concepts. The actual production in many conceptual and contemporary works of art is a matter of assembly of found objects. However, there are many modernist and contemporary artists who continue to excel in the skills of drawing and painting and in creating \"hands-on\" works of art.\n\nArt has had a great number of different functions throughout its history, making its purpose difficult to abstract or quantify to any single concept. This does not imply that the purpose of Art is \"vague\", but that it has had many unique, different reasons for being created. Some of these functions of Art are provided in the following outline. The different purposes of art may be grouped according to those that are non-motivated, and those that are motivated (Lévi-Strauss).\n\nThe non-motivated purposes of art are those that are integral to being human, transcend the individual, or do not fulfill a specific external purpose. In this sense, Art, as creativity, is something humans must do by their very nature (i.e., no other species creates art), and is therefore beyond utility.\nMotivated purposes of art refer to intentional, conscious actions on the part of the artists or creator. These may be to bring about political change, to comment on an aspect of society, to convey a specific emotion or mood, to address personal psychology, to illustrate another discipline, to (with commercial arts) sell a product, or simply as a form of communication.\n\nThe functions of art described above are not mutually exclusive, as many of them may overlap. For example, art for the purpose of entertainment may also seek to sell a product, i.e. the movie or video game.\n\nSince ancient times, much of the finest art has represented a deliberate display of wealth or power, often achieved by using massive scale and expensive materials. Much art has been commissioned by political rulers or religious establishments, with more modest versions only available to the most wealthy in society.\n\nNevertheless, there have been many periods where art of very high quality was available, in terms of ownership, across large parts of society, above all in cheap media such as pottery, which persists in the ground, and perishable media such as textiles and wood. In many different cultures, the ceramics of indigenous peoples of the Americas are found in such a wide range of graves that they were clearly not restricted to a social elite, though other forms of art may have been. Reproductive methods such as moulds made mass-production easier, and were used to bring high-quality Ancient Roman pottery and Greek Tanagra figurines to a very wide market. Cylinder seals were both artistic and practical, and very widely used by what can be loosely called the middle class in the Ancient Near East. Once coins were widely used, these also became an art form that reached the widest range of society.\n\nAnother important innovation came in the 15th century in Europe, when printmaking began with small woodcuts, mostly religious, that were often very small and hand-colored, and affordable even by peasants who glued them to the walls of their homes. Printed books were initially very expensive, but fell steadily in price until by the 19th century even the poorest could afford some with printed illustrations. Popular prints of many different sorts have decorated homes and other places for centuries.\n\nPublic buildings and monuments, secular and religious, by their nature normally address the whole of society, and visitors as viewers, and display to the general public has long been an important factor in their design. Egyptian temples are typical in that the most largest and most lavish decoration was placed on the parts that could be seen by the general public, rather than the areas seen only by the priests. Many areas of royal palaces, castles and the houses of the social elite were often generally accessible, and large parts of the art collections of such people could often be seen, either by anybody, or by those able to pay a small price, or those wearing the correct clothes, regardless of who they were, as at the Palace of Versailles, where the appropriate extra accessories (silver shoe buckles and a sword) could be hired from shops outside.\n\nSpecial arrangements were made to allow the public to see many royal or private collections placed in galleries, as with the Orleans Collection mostly housed in a wing of the Palais Royal in Paris, which could be visited for most of the 18th century. In Italy the art tourism of the Grand Tour became a major industry from the Renaissance onwards, and governments and cities made efforts to make their key works accessible. The British Royal Collection remains distinct, but large donations such as the Old Royal Library were made from it to the British Museum, established in 1753. The Uffizi in Florence opened entirely as a gallery in 1765, though this function had been gradually taking the building over from the original civil servants' offices for a long time before. The building now occupied by the Prado in Madrid was built before the French Revolution for the public display of parts of the royal art collection, and similar royal galleries open to the public existed in Vienna, Munich and other capitals. The opening of the Musée du Louvre during the French Revolution (in 1793) as a public museum for much of the former French royal collection certainly marked an important stage in the development of public access to art, transferring ownership to a republican state, but was a continuation of trends already well established.\n\nMost modern public museums and art education programs for children in schools can be traced back to this impulse to have art available to everyone. Museums in the United States tend to be gifts from the very rich to the masses. (The Metropolitan Museum of Art in New York City, for example, was created by John Taylor Johnston, a railroad executive whose personal art collection seeded the museum.) But despite all this, at least one of the important functions of art in the 21st century remains as a marker of wealth and social status.\n\nThere have been attempts by artists to create art that can not be bought by the wealthy as a status object. One of the prime original motivators of much of the art of the late 1960s and 1970s was to create art that could not be bought and sold. It is \"necessary to present something more than mere objects\" said the major post war German artist Joseph Beuys. This time period saw the rise of such things as performance art, video art, and conceptual art. The idea was that if the artwork was a performance that would leave nothing behind, or was simply an idea, it could not be bought and sold. \"Democratic precepts revolving around the idea that a work of art is a commodity impelled the aesthetic innovation which germinated in the mid-1960s and was reaped throughout the 1970s. Artists broadly identified under the heading of Conceptual art ... substituting performance and publishing activities for engagement with both the material and materialistic concerns of painted or sculptural form ... [have] endeavored to undermine the art object qua object.\"\n\nIn the decades since, these ideas have been somewhat lost as the art market has learned to sell limited edition DVDs of video works, invitations to exclusive performance art pieces, and the objects left over from conceptual pieces. Many of these performances create works that are only understood by the elite who have been educated as to why an idea or video or piece of apparent garbage may be considered art. The marker of status becomes understanding the work instead of necessarily owning it, and the artwork remains an upper-class activity. \"With the widespread use of DVD recording technology in the early 2000s, artists, and the gallery system that derives its profits from the sale of artworks, gained an important means of controlling the sale of video and computer artworks in limited editions to collectors.\"\n\nArt has long been controversial, that is to say disliked by some viewers, for a wide variety of reasons, though most pre-modern controversies are dimly recorded, or completely lost to a modern view. Iconoclasm is the destruction of art that is disliked for a variety of reasons, including religious ones. Aniconism is a general dislike of either all figurative images, or often just religious ones, and has been a thread in many major religions. It has been a crucial factor in the history of Islamic art, where depictions of Muhammad remain especially controversial. Much art has been disliked purely because it depicted or otherwise stood for unpopular rulers, parties or other groups. Artistic conventions have often been conservative and taken very seriously by art critics, though often much less so by a wider public. The iconographic content of art could cause controversy, as with late medieval depictions of the new motif of the Swoon of the Virgin in scenes of the Crucifixion of Jesus. The \"Last Judgment\" by Michelangelo was controversial for various reasons, including breaches of decorum through nudity and the Apollo-like pose of Christ.\n\nThe content of much formal art through history was dictated by the patron or commissioner rather than just the artist, but with the advent of Romanticism, and economic changes in the production of art, the artists' vision became the usual determinant of the content of his art, increasing the incidence of controversies, though often reducing their significance. Strong incentives for perceived originality and publicity also encouraged artists to court controversy. Théodore Géricault's \"Raft of the Medusa\" (c. 1820), was in part a political commentary on a recent event. Édouard Manet's \"Le Déjeuner sur l'Herbe\" (1863), was considered scandalous not because of the nude woman, but because she is seated next to men fully dressed in the clothing of the time, rather than in robes of the antique world. John Singer Sargent's \"Madame Pierre Gautreau (Madam X)\" (1884), caused a controversy over the reddish pink used to color the woman's ear lobe, considered far too suggestive and supposedly ruining the high-society model's reputation.\nThe gradual abandonment of naturalism and the depiction of realistic representations of the visual appearance of subjects in the 19th and 20th centuries led to a rolling controversy lasting for over a century. In the twentieth century, Pablo Picasso's \"Guernica\" (1937) used arresting cubist techniques and stark monochromatic oils, to depict the harrowing consequences of a contemporary bombing of a small, ancient Basque town. Leon Golub's \"Interrogation III\" (1981), depicts a female nude, hooded detainee strapped to a chair, her legs open to reveal her sexual organs, surrounded by two tormentors dressed in everyday clothing. Andres Serrano's \"Piss Christ\" (1989) is a photograph of a crucifix, sacred to the Christian religion and representing Christ's sacrifice and final suffering, submerged in a glass of the artist's own urine. The resulting uproar led to comments in the United States Senate about public funding of the arts.\n\nBefore Modernism, aesthetics in Western art was greatly concerned with achieving the appropriate balance between different aspects of realism or truth to nature and the ideal; ideas as to what the appropriate balance is have shifted to and fro over the centuries. This concern is largely absent in other traditions of art. The aesthetic theorist John Ruskin, who championed what he saw as the naturalism of J. M. W. Turner, saw art's role as the communication by artifice of an essential truth that could only be found in nature.\n\nThe definition and evaluation of art has become especially problematic since the 20th century. Richard Wollheim distinguishes three approaches to assessing the aesthetic value of art: the Realist, whereby aesthetic quality is an absolute value independent of any human view; the Objectivist, whereby it is also an absolute value, but is dependent on general human experience; and the Relativist position, whereby it is not an absolute value, but depends on, and varies with, the human experience of different humans.\n\nThe arrival of Modernism in the late nineteenth century lead to a radical break in the conception of the function of art, and then again in the late twentieth century with the advent of postmodernism. Clement Greenberg's 1960 article \"Modernist Painting\" defines modern art as \"the use of characteristic methods of a discipline to criticize the discipline itself\". Greenberg originally applied this idea to the Abstract Expressionist movement and used it as a way to understand and justify flat (non-illusionistic) abstract painting:\n\nPop artists like Andy Warhol became both noteworthy and influential through work including and possibly critiquing popular culture, as well as the art world. Artists of the 1980s, 1990s, and 2000s expanded this technique of self-criticism beyond \"high art\" to all cultural image-making, including fashion images, comics, billboards and pornography.\n\nDuchamp once proposed that art is any activity of any kind- everything. However, the way that only certain activities are classified today as art is a social construction. There is evidence that there may be an element of truth to this. \"\" is an art history book which examines the construction of the modern system of the arts i.e. Fine Art. Shiner finds evidence that the older system of the arts before our modern system (fine art) held art to be any skilled human activity i.e. Ancient Greek society did not possess the term art but techne. Techne can be understood neither as art or craft, the reason being that the distinctions of art and craft are historical products that came later on in human history. Techne included painting, sculpting and music but also; cooking, medicine, horsemanship, geometry, carpentry, prophecy, and farming etc.\n\nFollowing Duchamp during the first half of the twentieth century, a significant shift to general aesthetic theory took place which attempted to apply aesthetic theory between various forms of art, including the literary arts and the visual arts, to each other. This resulted in the rise of the New Criticism school and debate concerning \"the intentional fallacy\". At issue was the question of whether the aesthetic intentions of the artist in creating the work of art, whatever its specific form, should be associated with the criticism and evaluation of the final product of the work of art, or, if the work of art should be evaluated on its own merits independent of the intentions of the artist.\n\nIn 1946, William K. Wimsatt and Monroe Beardsley published a classic and controversial New Critical essay entitled \"The Intentional Fallacy\", in which they argued strongly against the relevance of an author's intention, or \"intended meaning\" in the analysis of a literary work. For Wimsatt and Beardsley, the words on the page were all that mattered; importation of meanings from outside the text was considered irrelevant, and potentially distracting.\n\nIn another essay, \"The Affective Fallacy,\" which served as a kind of sister essay to \"The Intentional Fallacy\" Wimsatt and Beardsley also discounted the reader's personal/emotional reaction to a literary work as a valid means of analyzing a text. This fallacy would later be repudiated by theorists from the reader-response school of literary theory. Ironically, one of the leading theorists from this school, Stanley Fish, was himself trained by New Critics. Fish criticizes Wimsatt and Beardsley in his essay \"Literature in the Reader\" (1970).\n\nAs summarized by Gaut and Livingston in their essay \"The Creation of Art\": \"Structuralist and post-structuralists theorists and critics were sharply critical of many aspects of New Criticism, beginning with the emphasis on aesthetic appreciation and the so-called autonomy of art, but they reiterated the attack on biographical criticisms's assumption that the artist's activities and experience were a privileged critical topic.\" These authors contend that: \"Anti-intentionalists, such as formalists, hold that the intentions involved in the making of art are irrelevant or peripheral to correctly interpreting art. So details of the act of creating a work, though possibly of interest in themselves, have no bearing on the correct interpretation of the work.\"\n\nGaut and Livingston define the intentionalists as distinct from formalists stating that: \"Intentionalists, unlike formalists, hold that reference to intentions is essential in fixing the correct interpretation of works.\" They quote Richard Wollheim as stating that, \"The task of criticism is the reconstruction of the creative process, where the creative process must in turn be thought of as something not stopping short of, but terminating on, the work of art itself.\"\n\nThe end of the 20th century fostered an extensive debate known as the linguistic turn controversy, or the \"innocent eye debate\", and generally referred to as the structuralism-poststructuralism debate in the philosophy of art. This debate discussed the encounter of the work of art as being determined by the relative extent to which the conceptual encounter with the work of art dominates over the perceptual encounter with the work of art.\n\nDecisive for the linguistic turn debate in art history and the humanities were the works of yet another tradition, namely the structuralism of Ferdinand de Saussure and the ensuing movement of poststructuralism. In 1981, the artist Mark Tansey created a work of art titled \"The Innocent Eye\" as a criticism of the prevailing climate of disagreement in the philosophy of art during the closing decades of the 20th century. Influential theorists include Judith Butler, Luce Irigaray, Julia Kristeva, Michel Foucault and Jacques Derrida. The power of language, more specifically of certain rhetorical tropes, in art history and historical discourse was explored by Hayden White. The fact that language is \"not\" a transparent medium of thought had been stressed by a very different form of philosophy of language which originated in the works of Johann Georg Hamann and Wilhelm von Humboldt. Ernst Gombrich and Nelson Goodman in his book \"Languages of Art: An Approach to a Theory of Symbols\" came to hold that the conceptual encounter with the work of art predominated exclusively over the perceptual and visual encounter with the work of art during the 1960s and 1970s. He was challenged on the basis of research done by the Nobel prize winning psychologist Roger Sperry who maintained that the human visual encounter was not limited to concepts represented in language alone (the linguistic turn) and that other forms of psychological representations of the work of art were equally defensible and demonstrable. Sperry's view eventually prevailed by the end of the 20th century with aesthetic philosophers such as Nick Zangwill strongly defending a return to moderate aesthetic formalism among other alternatives.\n\n Disputes as to whether or not to classify something as a work of art are referred to as classificatory disputes about art. Classificatory disputes in the 20th century have included cubist and impressionist paintings, Duchamp's \"Fountain\", the movies, superlative imitations of banknotes, conceptual art, and video games. Philosopher David Novitz has argued that disagreement about the definition of art are rarely the heart of the problem. Rather, \"the passionate concerns and interests that humans vest in their social life\" are \"so much a part of all classificatory disputes about art\" (Novitz, 1996). According to Novitz, classificatory disputes are more often disputes about societal values and where society is trying to go than they are about theory proper. For example, when the \"Daily Mail\" criticized Hirst's and Emin's work by arguing \"For 1,000 years art has been one of our great civilising forces. Today, pickled sheep and soiled beds threaten to make barbarians of us all\" they are not advancing a definition or theory about art, but questioning the value of Hirst's and Emin's work. In 1998, Arthur Danto, suggested a thought experiment showing that \"the status of an artifact as work of art results from the ideas a culture applies to it, rather than its inherent physical or perceptible qualities. Cultural interpretation (an art theory of some kind) is therefore constitutive of an object's arthood.\"\n\nAnti-art is a label for art that intentionally challenges the established parameters and values of art; it is term associated with Dadaism and attributed to Marcel Duchamp just before World War I, when he was making art from found objects. One of these, \"Fountain\" (1917), an ordinary urinal, has achieved considerable prominence and influence on art. Anti-art is a feature of work by Situationist International, the lo-fi Mail art movement, and the Young British Artists, though it is a form still rejected by the Stuckists, who describe themselves as anti-anti-art.\n\nArchitecture is often included as one of the visual arts; however, like the decorative arts, or advertising, it involves the creation of objects where the practical considerations of use are essential in a way that they usually are not in a painting, for example.\n\nSomewhat in relation to the above, the word \"art\" is also used to apply judgments of value, as in such expressions as \"that meal was a work of art\" (the cook is an artist), or \"the art of deception\", (the highly attained level of skill of the deceiver is praised). It is this use of the word as a measure of high quality and high value that gives the term its flavor of subjectivity. Making judgments of value requires a basis for criticism. At the simplest level, a way to determine whether the impact of the object on the senses meets the criteria to be considered \"art\" is whether it is perceived to be attractive or repulsive. Though perception is always colored by experience, and is necessarily subjective, it is commonly understood that what is not somehow aesthetically satisfying cannot be art. However, \"good\" art is not always or even regularly aesthetically appealing to a majority of viewers. In other words, an artist's prime motivation need not be the pursuit of the aesthetic. Also, art often depicts terrible images made for social, moral, or thought-provoking reasons. For example, Francisco Goya's painting depicting the Spanish shootings of 3rd of May 1808 is a graphic depiction of a firing squad executing several pleading civilians. Yet at the same time, the horrific imagery demonstrates Goya's keen artistic ability in composition and execution and produces fitting social and political outrage. Thus, the debate continues as to what mode of aesthetic satisfaction, if any, is required to define 'art'.\n\nThe assumption of new values or the rebellion against accepted notions of what is aesthetically superior need not occur concurrently with a complete abandonment of the pursuit of what is aesthetically appealing. Indeed, the reverse is often true, that the revision of what is popularly conceived of as being aesthetically appealing allows for a re-invigoration of aesthetic sensibility, and a new appreciation for the standards of art itself. Countless schools have proposed their own ways to define quality, yet they all seem to agree in at least one point: once their aesthetic choices are accepted, the value of the work of art is determined by its capacity to transcend the limits of its chosen medium to strike some universal chord by the rarity of the skill of the artist or in its accurate reflection in what is termed the \"zeitgeist\". Art is often intended to appeal to and connect with human emotion. It can arouse aesthetic or moral feelings, and can be understood as a way of communicating these feelings. Artists express something so that their audience is aroused to some extent, but they do not have to do so consciously. Art may be considered an exploration of the human condition; that is, what it is to be human.\n\n\n\n\n"}
{"id": "16249324", "url": "https://en.wikipedia.org/wiki?curid=16249324", "title": "Balanced sentence", "text": "Balanced sentence\n\nA balanced sentence is a sentence that employs parallel structures of approximately the same length and importance.\n\n\n"}
{"id": "55166205", "url": "https://en.wikipedia.org/wiki?curid=55166205", "title": "Bashu culture", "text": "Bashu culture\n\nBashu culture (), sometimes also named Sichuanese culture, refers to the culture of Sichuan and Chongqing, and the surrounding areas, including parts of the neighboring provinces of Yunnan and Guizhou, since the Han Chinese groups in these two provinces also primarily speak Southwestern Mandarin nowadays. It has a long history of over 3000 years, widely regarded as one of the cradles of modern Chinese civilization.\n\n\n"}
{"id": "37103142", "url": "https://en.wikipedia.org/wiki?curid=37103142", "title": "Budapest School (Lukács)", "text": "Budapest School (Lukács)\n\nThe Budapest School (; ) was a school of thought, originally of Marxist humanism, but later of Post-Marxism and dissident Liberalism that emerged in Hungary in the early 1960s, belonging to so called Hungarian New Left. Its members were students or colleagues of Georg Lukács. The school was originally oriented towards developing Lukacs' later works on social ontology and aesthetics, but quickly began to challenge the paradigm of Lukacsian-Marxism, thus reconstructing contemporary critical theory. Most of the members later came to abandon Marxism. The School also critiqued the \"dictatorship over needs\" of the Soviet states. Most of the members were forced into exile by the pro-Soviet Hungarian government.\n\nIn a letter to the \"Times Literary Supplement\" of February 15, 1971, Georg Lukács drew attention to “The Budapest School of Marxism,” and helped attract attention to the school from Western Marxism.\n\nMembers of the school include Ágnes Heller, Ferenc Fehér , György Márkus, István Mészáros, Mihály Vajda and Maria Márkus, among others. The Budapest School's writings have been read and researched widely since the 1960s.\n\nThe 1956 Hungarian Revolution was one of the most important political events of Agnes Heller's life, for at this time she saw the effect of the academic freedoms of Marxist critical theory as dangerous to the entire political and social structure of Hungary. The uprising confirmed Heller’s ideas that what Marx really intended is for the people to have political autonomy and collective determination of social life.\n\nLukács, Heller and other Marxist critical theorists emerged from the Revolution with the belief that Marxism and socialism needed to be applied to different nations in individual ways, effectively questioning the role of the Soviet Union in Hungary’s future. These ideas set Heller on an ideological collision course with the new Moscow-supported government of János Kádár: Heller was again expelled from the Communist Party and she was dismissed from the University in 1958 for refusing to indict Lukács as a collaborator in the Revolution. She was not able to resume her research until 1963, when she was invited to join the Sociological Institute at the Hungarian Academy as a researcher (Tormey 4–18) (Grumley 5–15). \nFrom 1963 can be seen the emergence of what would later be called the “Budapest School”, a philosophical forum that was formed by Lukács to promote the renewal of Marxist criticism in the face of actually existing socialism and its theories. Other participants in the Budapest School included together with Heller her second husband Ferenc Fehér, György Márkus, Mihály Vajda and some other scholars with looser connections to the school (such as András Hegedüs, István Eörsi, János Kis and György Bence). The School emphasized the idea of the renaissance of Marxism, described by radical philosophy scholar Simon Tormey as \"a flowering of the critical, oppositional potential they believed lay within Marxism and in particular within the ‘early Marx’ ... the Marxism of the individual ‘rich in needs,' of solidarity and self-governance ... they hoped to precipitate a crisis in those systems that had the temerity to call themselves 'socialist'.\"\n\nHeller’s work from this period, concentrates on themes such as what Marx means to the character of modern societies; liberation theory as applied to the individual; the work of changing society and government from “the bottom up,” and affecting change through the level of the values, beliefs and customs of \"everyday life\". Since 1990, Heller has been more interested in the issues of aesthetics in \"The Concept of The Beautiful\" (1998), \"Time Is Out of Joint\" (2002), and \"Immortal Comedy\" (2005).\n\nThe Budapest School carried out research on the Political Economy of both the Soviet Union and Western Capitalism. The School accepted many of the critiques of Soviet planning and inefficiency from Neoclassical Economics, as well as the connection between markets and freedom. The Soviet system was condemned as a dictatorship over needs. The school also analyzed the mixed economies of modern capitalism. Most traditional Marxist economics was jettisoned. Sweden and the Nordic Model was held as a model of the mixed economy and managed capitalism. The School advocated Radical Democracy as a solution to the authoritarian and undemocratic features of the mixed economy.\n\nHeller was born in 1929. It is well known that she is the most prominent philosopher and aesthetician from Budapest School, and one of the most representative critical theorists from Eastern European Neo-Marxists. She has published more than forty books on social theory, the theory of history, the philosophy of ethics and moral and political philosophy. Since 1990s, she has been paying more attention to issues of aesthetics.\n\n\nFehér wrote an important book called Paradoxical poet in the early 1970s under the influence of Georg Lukács's The Theory of the Novel, and later published some writings about aesthetics and political philosophy with Agnes Heller in Australia and the U.S. He died in 1994.\n\nMárkus left Hungary for Australia in the late 1970s, just like Agnes Heller and Ferenc Fehér. His books include:\n\nMihály András Vajda is a Széchenyi Prize-winning Hungarian philosopher, professor, Hungarian Academy of Sciences, full member. His work is primarily in the phenomenology of the 20th-century German philosophy and theory in totalitarian societies. Between 1996 and 2000 the Kossuth Lajos University Institute of Philosophy, 2005 to 2009, the Institute for Philosophical Research Director.\n\nHe graduated from high school in 1953, and began his tertiary studies at the Lenin Institute. He studied there until 1956, then from 1957 to 1960 he attended and graduated from the Eötvös Loránd University of Sciences, Faculty of Philosophy, German Philosophy faculty. György Lukács accepted among students, the School of Budapest belonged.\n\nUpon completion of undergraduate studies he worked as a primary school teacher, and in 1961, became a scientific assistant at the Hungarian Academy of Sciences Institute of Philosophy. In 1967 he defended his candidate's dissertation in the philosophy of science. Gradually departed from the Marxist eszmétől, and in 1973 the Budapest school's role in respect of political incompetence and szabadúszóvá dismissed from the institute became a translator and worked as a language teacher. In 1977, Bremen has been a visiting professor, he lectured until 1980. In the 1980s, more than once was in New York visiting professor. 1991-1992 in Siegen, Kassel in 1994 as a teacher.\n\nIn 1989, rehabilitated, and in 1990 the Kossuth Lajos University (now University of Debrecen), senior lecturer in the Department of Philosophy University professor was appointed. In 1992 he defended his doctoral thesis. In 1994, the MTA Representative Assembly, and in 2000 became chairman of the Committee of the Institute of Philosophy, and in 2001 he was elected to the Hungarian Academy of Sciences, e-mail, and in 2007 a full member. Between 1999 and 2002 Széchenyi fellowship researched. 2005 emeritálták . Also in 2005, the Institute for Philosophical Research was appointed. The institute is headed by 2009.\n\nResearch areas at the beginning of his career, the 20th-century phenomenology, Edmund Husserl and Max Scheler's work, he later turned to social theory.\nIn this period of fascism until 1995, his book is not jelentethette Hungarian. In the 1970s, towards a critical Marxist written works, most of which are also not jelentethetett it. The regime, the possibility of post-modern philosophy, and the impossibility of questions employed. Furthermore, dealt with philosophy, history and political philosophy.\nHe published a work in which he compared Husserl's theory of the sedimentation of the European sciences with Lukacs' concept of reification.\n\nMore than eighty publications, they are mainly in Hungarian, English and German disclose to the public.\n\nHe is married with a son and a daughter.\n\nIstván Mészáros is a Hungarian Marxist philosopher, and Professor Emeritus at the University of Sussex. He held the Chair of Philosophy at Sussex for fifteen years and was earlier Professor of Philosophy and Social Science for four years at York University.\n\nHe can be linked to the so-called \"Budapest school\", a group of Hungarian philosophers who were taught or influenced by Georg Lukács, including Ágnes Heller and György Márkus.\n\nHe left his native Hungary in 1956 after the Soviet invasion and worked for a time in Turin, Italy before settling in the UK.\n\n"}
{"id": "4124045", "url": "https://en.wikipedia.org/wiki?curid=4124045", "title": "Ceremonial use of lights", "text": "Ceremonial use of lights\n\nThe ceremonial use of lights is found in the practice of many religions. Candles are extremely common and other forms of light, whether fire or other, are also used.\n\nThe ceremonial use of lights in the Christian Church probably has a double origin: in a very non-natural symbolism, and in the adaptation of certain pagan and Jewish rites and customs of which the symbolic meaning was Christianized.\n\nLight is everywhere the symbol of joy and of life-giving power, as darkness is of death and destruction. Fire is an impressive element in worship has been used in many religions. Fire-worship still has its place in at least two of the great religions of the world. The Parsis adore fire as the visible expression of Ahura Mazda, the eternal principle of light and righteousness; the Brahmans worship it as divine and omniscient. The Hindu festival of Diwali (Diyawali, from diya, light), when temples and houses are illuminated with countless lamps, is held every November to celebrate Lakhshmi, the goddess of prosperity.\n\nIn the ritual of the Jewish temple fire and light played a conspicuous part. In the Holy of Holies was a cloud of light (\"shekinali\"), symbolical of the presence of Yahweh, and before it stood the candlestick with six branches, on each of which and on the central stem was a lamp eternally burning; while in the forecourt was an altar on which the sacred fire was never allowed to go out. Similarly the Jewish synagogues have each their eternal lamp.\n\nThe Greeks and Romans, too, had their sacred fire and their ceremonial lights. In Greece the Lampadedromia or Lampadephoria (torch-race) had its origin in Greek ceremonies, connected with the relighting of the sacred fire. Pausanias mentions the golden lamp made by Callimachus which burned night and day in the sanctuary of Athena Polias on the Acropolis, and tells of a statue of Hermes Agoraios, in the market-place of Pharae in Achaea, before which lamps were lighted. Among the Romans lighted candles and lamps formed part of the cult of the domestic tutelary deities; on all festivals doors were garlanded and lamps lighted. In the Cult of Isis lamps were lighted by day. In the ordinary temples were candelabra, e.g. that in the temple of Apollo Palatinus at Rome, originally taken by Alexander from Thebes, which was in the form of a tree from the branches of which lights hung like fruit. The lamps in the pagan temples were not symbolical, but votive offerings to the gods. Torches and lamps were also carried in religious processions.\n\nThe pagan custom of burying lamps with the dead was to provide the dead with the means of obtaining light in the next world; the lamps were for the most part unlighted. It was of Asiatic origin, traces of it having been observed in Phoenicia and in the Punic colonies, but not in Egypt or Greece. In Europe it was confined to the countries under the domination of Rome.\n\nIn Christianity, from the very first, fire and light are conceived as symbols, if not as visible manifestations, of the divine nature and the divine presence. Christ is the true Light, \"and at his transfiguration the fashion Christian of his countenance was altered, and his raiment was white and glistering\"; \"when the Holy Ghost descended upon the apostles, there appeared unto them cloven tongues of fire, and it sat upon each of them\"; \"at the conversion of St Paul there shined round him a great light from heaven\"; \"while the glorified Christ is represented as standing in the midst of seven candlesticks ... his head and hairs white like wool, as white as snow; and his eyes as a flame of fire\". Christians are children of Light at perpetual war with the powers of darkness.\n\nThere is no evidence of any ceremonial use of lights in Christian worship during its first two centuries. It is recorded, indeed, that on the occasion of St. Paul's preaching at Alexandria in Troas there were many lights in the upper chamber; but this was at night. And the most that can be hazarded is that a specially large number were lighted as a festive illumination, as in modern Church festivals. As to a purely ceremonial use, such early evidence as exists is all the other way. A single sentence of Tertullian sufficiently illuminates Christian practice during the 2nd century. \"On days of rejoicing\", he says, \"we do not shade our door-posts with laurels nor encroach upon the day-light with lamp laurels\" (die lacto non laurels pastes obumbramus nec lucernis diem infringimus). Lactantius, writing early in the 4th century, is even more sarcastic in his references to the heathen practice. \"They kindle lights\", he says, \"as though to one who is in darkness. Can he be thought sane who offers the light of lamps and candles to the Author and Giver of all light? \". This is primarily an attack on votive lights, and does not necessarily exclude their ceremonial use in other ways. There is, indeed, evidence that they were so used before Lactantius wrote. The 34th canon of the Synod of Elvira (305), which was contemporary with him, forbade candles to be lighted in cemeteries during the daytime, which points to an established custom as well as to an objection to it; and in the Roman catacombs lamps have been found of the 2nd and 3rd centuries which seem to have.\n\nLucerna been ceremonial or symbolical. Again, according to the Acta of St Cyprian (died 258), his body was borne to the grave \"praelucentibus cereis\", and Prudentius, in his hymn on the 2nd and martyrdom of St Lawrence, says that in the time of St Laurentius, i.e. the middle of the 3rd century, candles stood in the churches of Rome on golden candelabra. The gift, mentioned by Anastasius, made by Constantine to the Vatican basilica, of a pharum of gold, garnished with 500 dolphins each holding a lamp, to burn before St Peters tomb, points also to a custom well established before Christianity became the state religion.\n\nWhatever previous custom may have been and for the earliest ages it is difficult to determine absolutely because the Christians held their services at night. By the close of the 4th century the ceremonial use of lights had become firmly and universally established in the Church. This is clear, to pass by much other evidence, from the controversy of St Jerome with Vigilantius.\n\nVigilantius, a presbyter of Barcelona, still occupied the position of Tertullian and Lactantius in this matter. \"We see\", he wrote, \"a rite peculiar to the pagans introduced into the churches on pretext of religion, and, while the sun is still shining, a mass of wax tapers lighted. ... A great honor to the blessed martyrs, whom they think to illustrate with contemptible little candles\" (de pilissimis cereolis). Jerome, the most influential theologian of the day, took up the cudgels against Vigilantius, who, in spite of his fatherly admonition, had dared again to open his foul mouth and send forth a filthy stink against the relics of the holy martyrs. \"If candles are lit before their tombs, are these the ensigns of idolatry?\" In his treatise contra Vigilantium he answers the question with much common sense. \"There can be no harm if ignorant and simple people or religious women, light candles in honor of the martyrs. We are not born, but reborn, Christians, and that which when done for idols was detestable is acceptable when done for the martyrs. As in the case of the woman with the precious box of ointment, it is not the gift that merits reward, but the faith that inspires it.\" As for lights in the churches, he adds that in all the churches of the East, whenever the gospel is to be read, lights are lit, though the sun be rising (jam sole rutilante), not in order to disperse the darkness, but as a visible sign of gladness (ad signum ketitiae demonstrandum). Taken in connection with a statement which almost immediately precedes this \"Cereos autem non clara luce accendimus, sicut frustra calumniaris: sed ut noctis tenebras hoc solatio temperemus \", this seems to point to the fact that the ritual use of lights in the church services, so far as already established, arose from the same conservative habit as determined the development of liturgical vestments, i.e. the lights which had been necessary at the nocturnal meetings were retained, after the hours of service had been altered, and invested with a symbolical meaning.\n\nAlready they were used at most of the conspicuous functions of the Church. Paulinus, bishop of Nola (died 431), describes the altar at the eucharist as \"crowned with crowded lights\", and even mentions the \"eternal lamp\". For their use at baptisms we have, among much other evidence, that of Zeno of Verona for the West, and that of Gregory of Nazianzus for the East. Their use at funerals is illustrated by Eusebius's description of the burial of Constantine, and Jerome's account of that of Saint Paula. At ordinations they were used, as is shown by the 6th canon of the Council of Carthage (398), which decrees that the acolyte is to hand to the newly ordained deacon \"ceroferarium cum cereo\". This symbolism was not pagan, i.e. the lamps were not placed in the graves as part of the furniture of the dead; in the Catacombs they are found only in the niches of the galleries and the arcosolia, nor can they have been votive in the sense popularized later. \"Clara coronantur densis altaria lychnis\". \"Continuum scyphus est argenteus aptus ad usum. Sal, ignis et oleum. \"Cum alii Pontifices lampads cereosque proferrent, alii choras psallentium ducerent.\".\n\nAs to the blessing of candles, according to the \"Liber pontificalis\" Pope Zosimus in 417 ordered these to be blessed, and the Gallican and Mozarabic rituals also provided for this ceremony. The Feast of the Purification of the Virgin, known as Candlemas, because on this day the candles for the whole year are blessed, was established according to some authorities by Pope Gelasius I about 492. As to the question of altar lights, however, it must be borne in mind that these were not placed upon the altar, or on a retable behind it, until the 12th century. These were originally the candles carried by the deacons, according to the \"Ordo Romanus\" (i. 8; ii. 5; iii. 7) seven in number, which were set down, either on the steps of the altar, or, later, behind it. In certain of the Eastern Churches to this day, there are no lights on the high altar; the lighted candles stand on a small altar beside it, and at various parts of the service are carried by the lectors or acolytes before the officiating priest or deacon. The crowd of lights described by Paulinus as crowning the altar were either grouped round it or suspended in front of it; they are represented by the sanctuary lamps of the Latin Church and by the crown of lights suspended in front of the altar in. the Greek.\n\nTo trace the gradual elaboration of the symbolism and use of ceremonial lights in the Church, until its full development and systematization in the Middle Ages, would be impossible here. It must suffice to note a few stages in development of the process. The burning of lights before the tombs of martyrs led naturally to their being burned also before relics and lastly before images and pictures. This latter practice, hotly denounced as idolatry during the iconoclastic controversy, was finally established as orthodox by the Second General Council of Nicaea (787), which restored the use of images. A later development, however, by which certain lights themselves came to be regarded as objects of worship and to have other lights burned before them, was condemned as idolatrous by the Synod of Noyon in 1344. The passion for symbolism extracted ever new meanings out of the candles and their use. Early in the 6th century Magnus Felix Ennodius, bishop of Pavia, pointed out the threefold elements of a wax candle (Opusc. ix. and x.), each of which would make it an offering acceptable to God; the rush-wick is the product of pure water, the wax is the offspring of virgin, bees in the flame is sent from heaven.12 Clearly, wax was a symbol of the Blessed Virgin and the holy humanity of Christ. The later Middle Ages developed the idea. Durandus, in his Rationale, interprets the wax as the body of Christ, the wick as his soul, the flame as his divine nature; and the consuming candle as symbolizing his passion and death.\n\nThis may be the Paschal Candle only. In some codices the text runs: \"Per parochias concessit licentiam benedicendi Cereum Paschalem\". In the three variants of the notice of Zosimus given in Duchesnes edition of the \"Liber pontificalis\" (I~86I892) the word cera is, however, alone used. Nor does the text imply that he gave to the suburbican churches a privilege hitherto exercised by the metropolitan church. The passage runs: Hic constituit ut diaconi leva tecta haberent de palleis linostimis per parrochias et ut cera benedicatur, &c. Per parrochias here obviously refers to the head-gear of the deacons, not to the candles.\n\nSee also the \"Peregrinoiio Sylviae\" (386), 86, &c., for the use of lights at Jerusalem, and Isidore of Seville for the usage in the West. That even in the 7th century the blessing of candles was by no means universal is proved by the 9th canon of the Council of Toledo (671):\"De benedicendo cereo et lucerna in privilegiis Paschae\". This canon states that candles and lamps are not blessed in some churches, and that inquiries have been made why we do it. In reply, the council decides that it should be done to celebrate the mystery of Christ's resurrection. See Isidore of Seville, Conc., in Migne, Pat, tat. lxxxiv. 369.\n\nIn the Eastern Orthodox Church and those Eastern Catholic Churches which follow the Byzantine Rite, there is a large amount of ceremonial use of light.\n\nThe most important usage is the reception of the Holy Fire at the Church of the Holy Sepulchre in Jerusalem on the afternoon of Holy Saturday. This flame is often taken by the faithful to locations all over the world.\n\nWhen a new temple (church building) is consecrated the bishop kindles a flame in the sanctuary which traditionally should burn perpetually from that time forward. This sanctuary lamp is usually an oil lamp located either on or above the Holy Table (altar). In addition, in the Eastern Orthodox Church there must be candles on the Holy Table during the celebration of the Divine Liturgy. In some places this takes the form of a pair of white candles, in others, it may be a pair of five-branch candlesticks. There is also traditionally a seven-branch candlestick on or behind the Holy Table, recalling the one mandated in the Old Testament Tabernacle and the Temple in Jerusalem.\n\nAround the temple, there are a number of oil lamps burning in front of the icons, especially on the iconostasis. Additionally, the faithful will offer beeswax candles in candle stands in front of important icons. The faithful offer candles as they pray for both the living and the departed. It is customary during funerals and memorial services for everyone to stand holding lit candles. Often everyone will either extinguish their candles or put them in a candle stand at a certain point near the end of the memorial service to indicate that at some point, everyone will have to surrender their soul to God.\n\nThe reading from the Gospel Book must always be accompanied by lighted candles, as a sign that Christ is the Light which enlightens all (). When the priest and deacon cense the temple, the deacon will walk with a lighted candle. During processions, and in some places during the liturgical entrances, either candles or lanterns are carried by altar servers. On certain feast days, the clergy, and sometimes all of the faithful, will stand holding candles for certain solemn moments during the service. This is especially so during Holy Week during the reading of the 12 Passion Gospels on Great Friday, and the Lamentations around the epitaphios on Great Saturday.\n\nCertain moments during the All Night Vigil will be accentuated by the lighting or extinguishing of lamps or candles. The Polyeleos is an important moment in the service when all of the lamps and candles in the church should be illuminated.\n\nWhenever the bishop celebrates the divine services, he will bless with a pair of candlesticks known as dikirion and trikirion, holding two and three candles, respectively.\n\nThe faithful will often keep a lamp burning perpetually in their icon corner. In the Russian Orthodox Church, it is customary to try to preserve the flame from the service of the 12 Passion Gospels and bring it home to bless their house: there is a custom of using the flame from this candle to mark a cross on the lintel of one's doorway before entering after the service, and of then using the flame to re-kindle the lamp in the icon corner.\n\nDuring the Paschal Vigil, after the Midnight Office, all of the candles and lamps in the temple are extinguished, with the exception of the sanctuary lamp behind the iconostasis, and all wait in silence and darkness. (In Orthodox churches, when possible, the Holy Fire arrives from the Holy Sepulchre during Holy Saturday afternoon and it is used to light anew the flame in the sanctuary lamp.) At the stroke of midnight, the priest censes around the Holy Table, and lights his candle from the sanctuary lamp. Then the Holy Doors are opened and all the people light their candles from the priest's candle. Then, all the clergy and the people exit the church and go in procession three times around it holding lighted candles and singing a hymn of the resurrection.\n\nDuring the Paschal Vigil, and throughout Bright Week, the priest will hold a special paschal candle—in the Greek tradition a single candle, in the Slavic tradition a triple candlestick—at the beginning of the service, whenever he senses, and at other special moments during the service. In the Slavic tradition, the deacon also carries a special paschal candle which he holds at the beginning, whenever he senses, and whenever he chants an ektenia (litany).\n\nIn the Ethiopian Orthodox Church, it is customary to light bonfires on the Feast of Timkat (Epiphany).\n\nIn the Latin Church or Roman Catholic Church, the use of ceremonial lights falls under three heads. (1) They may be symbolical of the light of Gods presence, of Christ as Light Roman of Light, or of the children of Light in conflict with Catholic the powers of darkness; they may even be no more than expressions of joy on the occasion of great festivals. (2) They may be votive, i.e. offered as an act of worship (latria) to God. (3) They are, in virtue of their benediction by the Church, sacramental id, i.e. efficacious for the good of men's souls and bodies, and for the confusion of the powers of darkness. With one or more of these implications, they are employed in all the public functions of the Church. At the consecration of a church twelve lights are placed around the walls at the twelve spots. Dedication where these are anointed by the bishop with holy oil, of a and on every anniversary these are relighted; at the church, dedication of an altar tapers are lighted and censed at each place where the table is anointed (Pontificale Rom. p. ii. De ecci. dedicat. seu consecrat.).\n\nAt every liturgical service, and especially at Mass and at choir services, there must be at least two lighted tapers on the altar, as symbols of the presence at Mass of God and tributes of adoration. For the Mass the rule is that there are six lights at High Mass, four at missa cantata, and two at private masses. At a Pontifical High Mass (i.e. when the bishop celebrates) the lights are seven, because seven golden candlesticks surround the risen Saviour, the chief bishop of the Church (see Rev. i. 12). At most pontifical functions, moreover, the bishop as the representative of Christ is preceded by an acolyte with a burning candle (bugia) on a candlestick. The Ceremoniale Episcoporum (i. 12) further orders that a burning lamp is to hang at all times before each altar, three in front of the high altar, and five before the reserved Sacrament, as symbols of the eternal Presence. In practice, however, it is usual to have only one Altar lamp lighted before the tabernacle in which the Host is reserved. The special symbol of the real presence of Christ is the Sanctus candle, which is lighted at the moment of consecration and kept burning until the communion. The same symbolism is intended by the lighted tapers which must accompany the Host whenever it is carried in procession, or to the sick and dying.\n\nAs symbols of light and joy, a candle is held on each side of the deacon when reading the Gospel at Mass; and the same symbolism underlies the multiplication of lights on festivals, their number varying with the importance of the occasion. As to the number of these latter no rule is laid down. They differ from liturgical lights in that, whereas these must be tapers of pure beeswax or lamps fed with pure olive oil (except by special dispensation under Certain circumstances), those used merely to add splendour to the celebration may be of any material; the only exception being, that in the decoration of the altar, gas-lights are forbidden.\n\nIn general, the ceremonial use of lights in the Roman Catholic Church is conceived as a dramatic representation in fire of the life of Christ and of the whole scheme of salvation. On Easter Eve the new fire, symbol of the light of the newly risen Christ, is produced, and from this are kindled all the lights used throughout the Christian year until, in the gathering darkness (tenebrae) of the Passion, they are gradually extinguished. This quenching of the light of the world is symbolized at the service of Tenebrae in Holy Week by the placing on a stand before the altar of thirteen lighted tapers arranged pyramidally, the rest of the church being in darkness. The penitential psalms are sung, and at the end of each a candle is extinguished. When only the central one is left it is taken down and carried behind the altar, thus symbolizing the nocturnal darkness, so our hearts are illumined by invisible fire, &c. (Missale Rom.). In the form for the blessing of candles extra diem Purificationis B. Mariae Virg. the virtue of the consecrated candles in discomfiting demons is specially brought out: that in whatever places they may be lighted, or placed, the princes of darkness may depart, and tremble, and may fly terror-stricken with all their ministers from those habitations, nor presume further to disquiet and molest those who serve thee, Almighty God (Rituale Rom.)\n\nAltar candlesticks consist of five parts: the foot, stem, knob in the centre, bowl to catch the drippings, and pricket (a sharp point on which the candle is fixed). It is permissible to use a long tube, pointed to imitate a candle, in which a small taper is forced to the top by a spring (Cong. Rit., tIth May I&78).\n\nOn Easter Eve new fire is made with a flint and steel, and blessed; from this three candles are lighted, the \"lumen Christi\", and from these again the Paschal Candle. This is the symbol of the risen and victorious Christ, and burns at every solemn service until Ascension Day, when it is extinguished and removed after the reading of the Gospel at High Mass. This, of course, symbolizes the Ascension; but meanwhile the other lamps in the church have received their light from the Paschal Candle, and so symbolize throughout the year the continued presence of the light of Christ.\n\nAt the consecration of the baptismal water the burning Paschal Candle is dipped into the font so that the power of the Holy Ghost may descend into it and make it an effective instrument of regeneration. This is the symbol of baptism as rebirth as children of Light. Lighted tapers are also placed in the hands of the newly baptized, or of their god-parents, with the admonition to preserve their baptism inviolate, so that they may go to meet the Lord when he comes to the wedding. Thus, too, as children of Light, candidates for ordination and novices about to take the vows carry lights. when they come before the bishop; and the same idea 17, CEo. underlies the custom of carrying lights at weddings, at the first communion, and by priests going to their first mass, though none of these are liturgically prescribed. Finally, lights are placed around the bodies of the dead and carried beside them to the grave, partly as symbols that they still live in the light of Christ, partly to frighten away the powers of darkness.\n\nDuring the funeral service, the Paschal Candle is placed, burning, near the coffin, as a reminder of the deceased's baptismal vows and hope of eternal life and salvation brought about by the death and resurrection of Jesus, and of faith in the resurrection of the dead.\n\nConversely, the extinction of lights is part of the ceremony of excommunication (Pontificale Rom. pars iii.). Regino, abbot of Prum, describes the ceremony as it was carried out in his day, when its terrors were yet unabated (De eccles. disciplina, Excom ii. 409). Twelve priests should stand about the bishop, holding in their hands lighted torches, which at the conclusion of the anathema or excommunication they should cast down and trample under foot. When the excommunication is removed, the symbol of reconciliation is the handing to the penitent of a burning taper.\n\nIn the Church of England the practice has been less consistent. The first Book of Common Prayer directed two lights to be placed on the altar. This direction was omitted in the second Prayer-book; but the Ornaments Rubric of Queen Elizabeth's Prayer-book again made them obligatory. The question of how far this did so is a much-disputed one and is connected with the whole problem of the meaning and scope of the rubric. Uncertainty reigns with regard to the actual usage of the Church of England from the Reformation onwards. Lighted candles certainly continued to burn in Queen Elizabeth's chapel, to the scandal of Protestant zealots. They also seem to have been retained in certain cathedral and collegiate churches. There is, however, no mention of ceremonial candles in the detailed account of the services of the Church of England given by William Harrison (Description of England, 1570). They seem never to have been illegal under the Acts of Uniformity. The use of wax lights and tapers formed one of the indictments brought by Peter Smart, a Puritan prebendary of Durham, against Dr. Burgoyne, John Cosin and others for setting up superstitious ceremonies in the cathedral contrary to the Act of Uniformity. The indictments were dismissed in 1628 by Sir James Whitelocke, chief justice of Chester and a judge of the Kings Bench, and in 1629 by Sir Henry Yelverton, a judge of Common Pleas and himself a strong Puritan.\n\nThe use of ceremonial lights was among the indictments in the impeachment of Laud and other bishops by the House of Commons, but these were not based on the Act of Uniformity. From the Restoration onwards the use of ceremonial lights, though far from universal, was usual again in cathedrals and collegiate churches.\n\nIt was not, however, until the Oxford Movement of the 19th century that their use was widely extended in parish churches. The growing custom met with some opposition; the law was appealed to, and in 1872 the Privy Council declared altar lights to be illegal (Martin v. Mackonochie). This judgment, founded as was afterwards admitted on insufficient knowledge, produced no effect. In the absence of any authoritative negative pronouncement, churches returned to practically the whole ceremonial use of lights as practised in the Roman Catholic Church.\n\nThe matter was again raised in the case of Read and others v. the Bishop of Lincoln, one of the counts of the indictment being that the bishop had, during the celebration of Holy Communion, allowed two candles to be alight on a shelf or retable behind the communion table when they were not necessary for giving light. The Archbishop of Canterbury, in whose court the case was heard (1889), decided that the mere presence of two candles on the table, burning during the service but lit before it began, was lawful under the first Prayer-Book of Edward VI. and had never been made unlawful. On the case being appealed to the Privy Council, this particular indictment was dismissed on the ground that the vicar, not the bishop, was responsible for the presence of the lights.\n\nThe custom of placing lighted candles around the bodies of the dead, especially when lying in state, has never wholly died out in the Anglican communion. In the 18th century, moreover, it was still customary in England to accompany a funeral with lighted tapers. A contemporary illustration shows a funeral cortege preceded and accompanied by boys, each carrying four lighted candles in a branched candlestick. The usage in this respect in Anglo-Catholic churches is a revival of pre-Reformation ceremonial as is found in the Roman Catholic Church.\n\nIn the Church of Ireland, a branch of Anglicanism that is both Catholic and Apostolic in origins and in the closest historical and doctrinal communion with the younger Church of England, but with a more generally robust middle- to low-church tradition in ritual, the use of candles and lanterns of all kinds is canonically forbidden except for the specific purpose of \"giving light\" during services. This conforms to that Church's similar abjuring of, for example, incense, all but rather simple clerical vestments, mitres, eucharistic wafers, the Reserved Sacrament and the elevation of the sacred elements in the Eucharist.\n\nAs a result of the Reformation, the use of ceremonial lights was either greatly modified, or totally abolished in the Protestant Churches. In the Reformed (Calvinistic) Churches altar lights were, with the rest, done away with entirely as popish and superstitious. In the Lutheran Churches they were retained, and in Evangelical Germany have even survived most of the other medieval rites and ceremonies (e.g. the use of vestments) which were not abolished at the Reformation itself. The custom of placing lighted candles around the bodies of the dead is still practised by some Protestants.\n\nIn almost all Hindu homes, lamps are lit daily, sometimes before an altar. In some houses, oil lamps or candles are lit at dawn, in some houses they are lit at both dawn and dusk, and in a few, lamps are maintained continuously.\n\nA diya, or clay lamp, is frequently used in Hindu celebrations and forms an integral part of many social rites. It is a strong symbol of enlightenment, hope, and prosperity. Diwali is the festival of lights celebrated by followers of dharmic religions.\n\nIn its traditional and simplest form, the diya is made from baked clay or terracotta and holds oil or ghee that is lit via a cotton wick.\n\nTraditional diyas have now evolved into a form wherein waxes are used as replacements for oils.\n\nLamps are lit in Sikhism on Diwali, the festival of light, as well as being lit everyday by followers of Dharmic religions.\n\nCandles are used in the religious ceremonies of many faiths.\n\n\nCandles are a traditional part of Buddhist ritual observances. Along with incense and flowers, candles (or some other type of light source, such as butter lamps) are placed before Buddhist shrines or images of the Buddha as a show of respect. They may also be accompanied by offerings of food and drink. The light of the candles is described as representing the light of the Buddha's teachings, echoing the metaphor of light used in various Buddhist scriptures. See \"Ubon Ratchathani Candle Festival\" for an example of a Buddhist festival that makes extensive use of candles.\n\n\nIn Christianity the candle is commonly used in worship both for decoration and ambiance, and as a symbol that represents the light of God or, specifically, the light of Christ. The altar candle is often placed on the altar, usually in pairs. Candles are also carried in processions, especially to either side of the processional cross. A votive candle or taper may be lit as an accompaniment to prayer.\n\nCandles are lit by worshippers in front of icons in Eastern Orthodox, Oriental Orthodox, Eastern Catholic and other churches. This is referred to as \"offering a candle\", because the candle is a symbol of the worshiper offering himself or herself to God (and proceeds from the sale of the candle are offerings by the faithful which go to help the church). Among the Eastern Orthodox, there are times when the entire congregation stands holding lit tapers, such as during the reading of the Matins Gospels on Good Friday, the Lamentations on Holy Saturday, funerals, Memorial services, etc. There are also special candles that are used by Orthodox clergy. A bishop will bless using dikirion and trikirion (candlesticks holding two and three candles, respectively). At Pascha (Easter) the priest holds a special Paschal trikirion, and the deacon holds a Paschal candle. The priest will also bless the faithful with a single candle during the Liturgy of the Presanctified Gifts (celebrated only during Great Lent).\n\nIn the Roman Catholic Church a liturgical candle must be made of at least 51% beeswax, the remainder may be paraffin or some other substance. In the Orthodox Church, the tapers offered should be 100% beeswax, unless poverty makes this impossible. The stumps from burned candles can be saved and melted down to make new candles.\n\nIn some Western churches, a special candle known as the \"Paschal candle\", specifically represents the Resurrected Christ and is lit only at Easter, funerals, and baptisms. In the Eastern Orthodox Church, during Bright Week (Easter Week) the priest holds a special Paschal trikirion (triple candlestick) and the deacon holds a large candle during all of the services at which they serve.\n\nIn Sweden (and other Scandinavian countries), St. Lucia Day is celebrated on December 13 with the crowning of a young girl with a wreath of candles.\n\nIn many Western churches, a group of candles arranged in a ring, known as an Advent wreath, are used in church services in the Sundays leading up to Christmas. In households in some Western European countries, a single candle marked with the days of December is gradually burned down, day by day, to mark the passing of the days of Advent; this is called an Advent candle.\n\n\nIn Judaism, a pair of Shabbat candles are lit on Friday evening prior to the start of the weekly Sabbath celebration. On Saturday night, a special candle with several wicks and usually braided is lit for the \"Havdalah\" ritual marking the end of the Sabbath and the beginning of the new week.\n\nThe eight-day holiday of Hanukkah, also known as the Festival of Lights, is celebrated by lighting a special Hanukkiyah each night to commemorate the rededication of the Temple in Jerusalem.\n\nA memorial candle is lit on the Yahrtzeit, or anniversary of the death of a loved one according to the Hebrew calendar. The candle burns for 24 hours. A memorial candle is also lit on Yom HaShoah, a day of remembrance for all those who perished in The Holocaust.\n\nA seven-day memorial candle is lit following the funeral of a spouse, parent, sibling or child.\n\nCandles are also lit prior to the onset of the Three Festivals (Sukkot, Passover and Shavuot) and the eve of Yom Kippur, and Rosh Hashana.\n\nA candle is also used on the night before Passover in a symbolic search for chametz, or leavened bread, which is not eaten on Passover.\n\nThe Candle is also used in celebrations of Kwanzaa, which is an African American holiday which runs from December 26 to January 1. A Kinara is used to hold candles in these celebrations. It holds seven candles; three red candles to represent African American struggles, one black candle to represent the African American people and three green candles to represent African American hopes.\n\nFor some Humanists the candle is used as a symbol of the light of reason or rationality. The Humanist festival of HumanLight often features a candle-lighting ceremony.\n\nDuring satanic rituals black candles are the only light source, except for one white candle on the altar. The dim lighting is used to create an air of mystique and the color of the candles has symbolic meaning.\n\nA common element of worship in many Unitarian Universalism churches and fellowships is the lighting of candles of joy and concern. Here members of the congregation may come up to the altar or chancel, light a votive or other candle, and share a personal concern or joy with the community. Unitarian Universalism also incorporates candle-lighting ceremonies from other spiritual traditions, from which they draw inspiration. A flaming chalice is the most widely used symbol of Unitarianism and Unitarian Universalism, and is, in reality, usually a candle, not an actual chalice of burning oil.\n\nIn Wicca and related forms of Paganism, the candle is frequently used on the altar to represent the presence of the God and Goddess, and in the four corners of a ritual circle to represent the presence of the four classical elements: Fire, Earth, Air, and Water. When used in this manner, lighting and extinguishing the candle marks the opening and closing of the ritual. The candle is also frequently used for magical meditative purposes. Altar candles are traditionally thick tall candles or long tapers which are available in many colors. In Wicca, the candles that are used come in a variety of colors, depending on the nature of the ritual or custom at hand. Some Wiccans may use red, green, blue, yellow and white or purple candles to represent the elements.\n\nUsing candles in magic based on Wiccan beliefs is known as \"sympathetic magick\" in that it is believed the candle represents the outcome the person is wanting. It is a \"like attracts like\" form of magical practice. For example, if a person is looking for a job or needs extra income a green candle (the color of American dollars) would be used. For romance, a red candle would be used (red is a universal color of love and hearts). There is an additional belief that the smoke from the candles will take the prayer requests, desires, or wishes up to the gods.\n\n\nIn raqs sharqi, candles are used as a complementary element in some dance styles. The candles can be held either on the dancer's hand or above her head, depending on what the choreography demands.\n"}
{"id": "6258", "url": "https://en.wikipedia.org/wiki?curid=6258", "title": "Civilization", "text": "Civilization\n\nA civilization or civilisation (see English spelling differences) is any complex society characterized by urban development, social stratification imposed by a cultural elite, symbolic systems of communication (for example, writing systems), and a perceived separation from and domination over the natural environment.\n\nCivilizations are intimately associated with and often further defined by other socio-politico-economic characteristics, including centralization, the domestication of both humans and other organisms, specialization of labour, culturally ingrained ideologies of progress and supremacism, monumental architecture, taxation, societal dependence upon farming and expansionism. Historically, civilization has often been understood as a larger and \"more advanced\" culture, in contrast to smaller, supposedly primitive cultures. Similarly, some scholars have described civilization as being necessarily multicultural. In this broad sense, a civilization contrasts with non-centralized tribal societies, including the cultures of nomadic pastoralists, Neolithic societies or hunter-gatherers, but it also contrasts with the cultures found within civilizations themselves. As an uncountable noun, \"civilization\" also refers to the process of a society developing into a centralized, urbanized, stratified structure. Civilizations are organized in densely populated settlements divided into hierarchical social classes with a ruling elite and subordinate urban and rural populations, which engage in intensive agriculture, mining, small-scale manufacture and trade. Civilization concentrates power, extending human control over the rest of nature, including over other human beings.\n\nCivilization, as its etymology (below) suggests, is a concept originally linked to towns and cities. The earliest emergence of civilizations is generally associated with the final stages of the Neolithic Revolution, culminating in the relatively rapid process of urban revolution and state formation, a political development associated with the appearance of a governing elite.\n\nThe English word \"civilization\" comes from the 16th-century French \"civilisé\" (\"civilized\"), from Latin \"civilis\" (\"civil\"), related to \"civis\" (\"citizen\") and \"civitas\" (\"city\"). The fundamental treatise is Norbert Elias's \"The Civilizing Process\" (1939), which traces social mores from medieval courtly society to the Early Modern period. In \"The Philosophy of Civilization\" (1923), Albert Schweitzer outlines two opinions: one purely material and the other material and ethical. He said that the world crisis was from humanity losing the ethical idea of civilization, \"the sum total of all progress made by man in every sphere of action and from every point of view in so far as the progress helps towards the spiritual perfecting of individuals as the progress of all progress\".\n\nAdjectives like \"civility\" developed in the mid-16th century. The abstract noun \"civilization\", meaning \"civilized condition\", came in the 1760s, again from French. The first known use in French is in 1757, by Victor Riqueti, marquis de Mirabeau, and the first use in English is attributed to Adam Ferguson, who in his 1767 \"Essay on the History of Civil Society\" wrote, \"Not only the individual advances from infancy to manhood, but the species itself from rudeness to civilisation\". The word was therefore opposed to barbarism or rudeness, in the active pursuit of progress characteristic of the Age of Enlightenment.\n\nIn the late 1700s and early 1800s, during the French Revolution, \"civilization\" was used in the singular, never in the plural, and meant the progress of humanity as a whole. This is still the case in French. The use of \"civilizations\" as a countable noun was in occasional use in the 19th century, but has become much more common in the later 20th century, sometimes just meaning culture (itself in origin an uncountable noun, made countable in the context of ethnography). Only in this generalized sense does it become possible to speak of a \"medieval civilization\", which in Elias's sense would have been an oxymoron.\n\nAlready in the 18th century, civilization was not always seen as an improvement. One historically important distinction between culture and civilization is from the writings of Rousseau, particularly his work about education, \"\". Here, civilization, being more rational and socially driven, is not fully in accord with human nature, and \"human wholeness is achievable only through the recovery of or approximation to an original prediscursive or prerational natural unity\" (see noble savage). From this, a new approach was developed, especially in Germany, first by Johann Gottfried Herder, and later by philosophers such as Kierkegaard and Nietzsche. This sees cultures as natural organisms, not defined by \"conscious, rational, deliberative acts\", but a kind of pre-rational \"folk spirit\". Civilization, in contrast, though more rational and more successful in material progress, is unnatural and leads to \"vices of social life\" such as guile, hypocrisy, envy and avarice. In World War II, Leo Strauss, having fled Germany, argued in New York that this opinion of civilization was behind Nazism and German militarism and nihilism.\n\nSocial scientists such as V. Gordon Childe have named a number of traits that distinguish a civilization from other kinds of society. Civilizations have been distinguished by their means of subsistence, types of livelihood, settlement patterns, forms of government, social stratification, economic systems, literacy and other cultural traits. Andrew Nikiforuk argues that \"civilizations relied on shackled human muscle. It took the energy of slaves to plant crops, clothe emperors, and build cities\" and considers slavery to be a common feature of pre-modern civilizations.\n\nAll civilizations have depended on agriculture for subsistence, with the possible exception of some early civilizations in Peru which may have depended upon maritime resources. Grain farms can result in accumulated storage and a surplus of food, particularly when people use intensive agricultural techniques such as artificial fertilization, irrigation and crop rotation. It is possible but more difficult to accumulate horticultural production, and so civilizations based on horticultural gardening have been very rare. Grain surpluses have been especially important because grain can be stored for a long time. A surplus of food permits some people to do things besides produce food for a living: early civilizations included soldiers, artisans, priests and priestesses, and other people with specialized careers. A surplus of food results in a division of labour and a more diverse range of human activity, a defining trait of civilizations. However, in some places hunter-gatherers have had access to food surpluses, such as among some of the indigenous peoples of the Pacific Northwest and perhaps during the Mesolithic Natufian culture. It is possible that food surpluses and relatively large scale social organization and division of labour predates plant and animal domestication.\n\nCivilizations have distinctly different settlement patterns from other societies. The word \"civilization\" is sometimes simply defined as \"\"'living in cities\"'\". Non-farmers tend to gather in cities to work and to trade.\n\nCompared with other societies, civilizations have a more complex political structure, namely the state. State societies are more stratified than other societies; there is a greater difference among the social classes. The ruling class, normally concentrated in the cities, has control over much of the surplus and exercises its will through the actions of a government or bureaucracy. Morton Fried, a conflict theorist and Elman Service, an integration theorist, have classified human cultures based on political systems and social inequality. This system of classification contains four categories\n\nEconomically, civilizations display more complex patterns of ownership and exchange than less organized societies. Living in one place allows people to accumulate more personal possessions than nomadic people. Some people also acquire landed property, or private ownership of the land. Because a percentage of people in civilizations do not grow their own food, they must trade their goods and services for food in a market system, or receive food through the levy of tribute, redistributive taxation, tariffs or tithes from the food producing segment of the population. Early human cultures functioned through a gift economy supplemented by limited barter systems. By the early Iron Age, contemporary civilizations developed money as a medium of exchange for increasingly complex transactions. In a village, the potter makes a pot for the brewer and the brewer compensates the potter by giving him a certain amount of beer. In a city, the potter may need a new roof, the roofer may need new shoes, the cobbler may need new horseshoes, the blacksmith may need a new coat and the tanner may need a new pot. These people may not be personally acquainted with one another and their needs may not occur all at the same time. A monetary system is a way of organizing these obligations to ensure that they are fulfilled. From the days of the earliest monetarized civilizations, monopolistic controls of monetary systems have benefited the social and political elites.\n\nWriting, developed first by people in Sumer, is considered a hallmark of civilization and \"appears to accompany the rise of complex administrative bureaucracies or the conquest state\". Traders and bureaucrats relied on writing to keep accurate records. Like money, writing was necessitated by the size of the population of a city and the complexity of its commerce among people who are not all personally acquainted with each other. However, writing is not always necessary for civilization, as shown the Inca civilization of the Andes, which did not use writing at all except from a complex recording system consisting of cords and nodes instead: the \"Quipus\", whose still functioned as a civilized society.\nAided by their division of labour and central government planning, civilizations have developed many other diverse cultural traits. These include organized religion, development in the arts, and countless new advances in science and technology.\n\nThrough history, successful civilizations have spread, taking over more and more territory, and assimilating more and more previously-uncivilized people. Nevertheless, some tribes or people remain uncivilized even to this day. These cultures are called by some \"primitive\", a term that is regarded by others as pejorative. \"Primitive\" implies in some way that a culture is \"first\" (Latin = \"primus\"), that it has not changed since the dawn of humanity, though this has been demonstrated not to be true. Specifically, as all of today's cultures are contemporaries, today's so-called primitive cultures are in no way antecedent to those we consider civilized. Anthropologists today use the term \"non-literate\" to describe these peoples.\n\nCivilization has been spread by colonization, invasion, religious conversion, the extension of bureaucratic control and trade, and by introducing agriculture and writing to non-literate peoples. Some non-civilized people may willingly adapt to civilized behaviour. But civilization is also spread by the technical, material and social dominance that civilization engenders.\n\nAssessments of what level of civilization a polity has reached are based on comparisons of the relative importance of agricultural as opposed to trade or manufacturing capacities, the territorial extensions of its power, the complexity of its division of labour, and the carrying capacity of its urban centres. Secondary elements include a developed transportation system, writing, standardized measurement, currency, contractual and tort-based legal systems, art, architecture, mathematics, scientific understanding, metallurgy, political structures and organized religion.\n\nTraditionally, polities that managed to achieve notable military, ideological and economic power defined themselves as \"civilized\" as opposed to other societies or human groupings outside their sphere of influence – calling the latter barbarians, savages, and primitives. In a modern-day context, \"civilized people\" have been contrasted with indigenous people or tribal societies.\n\n\"Civilization\" can also refer to the culture of a complex society, not just the society itself. Every society, civilization or not, has a specific set of ideas and customs, and a certain set of manufactures and arts that make it unique. Civilizations tend to develop intricate cultures, including a state-based decision making apparatus, a literature, professional art, architecture, organized religion and complex customs of education, coercion and control associated with maintaining the elite.\n\nThe intricate culture associated with civilization has a tendency to spread to and influence other cultures, sometimes assimilating them into the civilization (a classic example being Chinese civilization and its influence on nearby civilizations such as Korea, Japan and Vietnam). Many civilizations are actually large cultural spheres containing many nations and regions. The civilization in which someone lives is that person's broadest cultural identity.\n\nMany historians have focused on these broad cultural spheres and have treated civilizations as discrete units. Early twentieth-century philosopher Oswald Spengler, uses the German word \"Kultur\", \"culture\", for what many call a \"civilization\". Spengler believed a civilization's coherence is based on a single primary cultural symbol. Cultures experience cycles of birth, life, decline and death, often supplanted by a potent new culture, formed around a compelling new cultural symbol. Spengler states civilization is the beginning of the decline of a culture as \"the most external and artificial states of which a species of developed humanity is capable\".\n\nThis \"unified culture\" concept of civilization also influenced the theories of historian Arnold J. Toynbee in the mid-twentieth century. Toynbee explored civilization processes in his multi-volume \"A Study of History,\" which traced the rise and, in most cases, the decline of 21 civilizations and five \"arrested civilizations\". Civilizations generally declined and fell, according to Toynbee, because of the failure of a \"creative minority\", through moral or religious decline, to meet some important challenge, rather than mere economic or environmental causes.\n\nSamuel P. Huntington defines civilization as \"the highest cultural grouping of people and the broadest level of cultural identity people have short of that which distinguishes humans from other species\". Huntington's theories about civilizations are discussed below.\n\nAnother group of theorists, making use of systems theory, looks at a civilization as a complex system, i.e., a framework by which a group of objects can be analysed that work in concert to produce some result. Civilizations can be seen as networks of cities that emerge from pre-urban cultures and are defined by the economic, political, military, diplomatic, social and cultural interactions among them. Any organization is a complex social system and a civilization is a large organization. Systems theory helps guard against superficial but misleading analogies in the study and description of civilizations.\n\nSystems theorists look at many types of relations between cities, including economic relations, cultural exchanges and political/diplomatic/military relations. These spheres often occur on different scales. For example, trade networks were, until the nineteenth century, much larger than either cultural spheres or political spheres. Extensive trade routes, including the Silk Road through Central Asia and Indian Ocean sea routes linking the Roman Empire, Persian Empire, India and China, were well established 2000 years ago, when these civilizations scarcely shared any political, diplomatic, military, or cultural relations. The first evidence of such long distance trade is in the ancient world. During the Uruk period, Guillermo Algaze has argued that trade relations connected Egypt, Mesopotamia, Iran and Afghanistan. Resin found later in the Royal Cemetery at Ur is suggested was traded northwards from Mozambique.\n\nMany theorists argue that the entire world has already become integrated into a single \"world system\", a process known as globalization. Different civilizations and societies all over the globe are economically, politically, and even culturally interdependent in many ways. There is debate over when this integration began, and what sort of integration – cultural, technological, economic, political, or military-diplomatic – is the key indicator in determining the extent of a civilization. David Wilkinson has proposed that economic and military-diplomatic integration of the Mesopotamian and Egyptian civilizations resulted in the creation of what he calls the \"Central Civilization\" around 1500 BCE. Central Civilization later expanded to include the entire Middle East and Europe, and then expanded to a global scale with European colonization, integrating the Americas, Australia, China and Japan by the nineteenth century. According to Wilkinson, civilizations can be culturally heterogeneous, like the Central Civilization, or homogeneous, like the Japanese civilization. What Huntington calls the \"clash of civilizations\" might be characterized by Wilkinson as a clash of cultural spheres within a single global civilization. Others point to the Crusades as the first step in globalization. The more conventional viewpoint is that networks of societies have expanded and shrunk since ancient times, and that the current globalized economy and culture is a product of recent European colonialism. \n\nThe notion of world history as a succession of \"civilizations\" is an entirely modern one.\nIn the European Age of Discovery, emerging Modernity was put into stark contrast with the\nNeolithic and Mesolithic stage of the cultures of the New World, suggesting\nthat the complex states had emerged at some time in prehistory.\nThe term \"civilization\" as it is now most commonly understood, a complex state with centralisation, social stratification and specialization of labour, corresponds to early empires that arise in the Fertile Crescent in the Early Bronze Age, around roughly 3000 BC.\nGordon Childe defined the emergence of civilization as the result of two successive revolutions: the Neolithic Revolution, triggering the development of settled communities, and the Urban Revolution.\n\nAt first, the Neolithic was associated with shifting subsistence cultivation, where continuous farming led to the depletion of soil fertility resulting in the requirement to cultivate fields further and further removed from the settlement, eventually compelling the settlement itself to move. In major semi-arid river valleys, annual flooding renewed soil fertility every year, with the result that population densities could rise significantly.\nThis encouraged a secondary products revolution in which people used domesticated animals not just for meat, but also for milk, wool, manure and pulling ploughs and carts – a development that spread through the Eurasian Oecumene.\n\nThe earlier neolithic technology and lifestyle was established first in Western Asia (for example at Göbekli Tepe, from about 9,130 BCE), and later in the Yellow River and Yangtze basins in China (for example the Pengtoushan culture from 7,500 BCE), and later spread.\nMesopotamia is the site of the earliest developments of the Neolithic Revolution from around 10,000 BCE, with civilizations developing from 6,500 years ago. This area has been identified as having \"inspired some of the most important developments in human history including the invention of the wheel, the development of cuneiform script, mathematics, astronomy and agriculture.\"\nSimilar pre-civilized \"neolithic revolutions\" also began independently from 7,000 BCE in northwestern South America (the Norte Chico civilization) and Mesoamerica.\n\nThe 8.2 Kiloyear Arid Event and the 5.9 Kiloyear Interpluvial saw the drying out of semiarid regions and a major spread of deserts. This climate change shifted the cost-benefit ratio of endemic violence between communities, which saw the abandonment of unwalled village communities and the appearance of walled cities, associated with the first civilizations.\nThis \"urban revolution\" marked the beginning of the accumulation of transferrable surpluses, which helped economies and cities develop. It was associated with the state monopoly of violence, the appearance of a soldier class and endemic warfare, the rapid development of hierarchies, and the appearance of human sacrifice.\n\nThe civilized urban revolution in turn was dependent upon the development of sedentism, the domestication of grains and animals and development of lifestyles that facilitated economies of scale and accumulation of surplus production by certain social sectors. The transition from \"complex cultures\" to \"civilizations\", while still disputed, seems to be associated with the development of state structures, in which power was further monopolized by an elite ruling class who practised human sacrifice.\n\nTowards the end of the Neolithic period, various elitist Chalcolithic civilizations began to rise in various \"cradles\" from around 3300 BCE, expanding into large-scale empires in the course of the Bronze Age (Old Kingdom of Egypt, Akkadian Empire, Assyrian Empire, Old Assyrian Empire, Hittite Empire).\n\nA parallel development took place independently in the Pre-Columbian Americas, where the Mayans began to be urbanised around 500 BCE, and the fully fledged Aztec and Inca emerged by the 15th century, briefly before European contact.\n\nThe Bronze Age collapse was followed by the Iron Age around 1200 BCE, during which a number of new civilizations emerged, culminating in a period from the 8th to the 3rd century BCE which Karl Jaspers termed the Axial Age, presented as a critical transitional phase leading to classical civilization.\nWilliam Hardy McNeill proposed that this period of history was one in which culture contact between previously separate civilizations saw the \"closure of the oecumene\" and led to accelerated social change from China to the Mediterranean, associated with the spread of coinage, larger empires and new religions. This view has recently been championed by Christopher Chase-Dunn and other world systems theorists.\n\nA major technological and cultural transition to modernity began approximately 1500 CE in Western Europe, and from this beginning new approaches to science and law spread rapidly around the world, incorporating earlier cultures into the industrial and technological civilization of the present.\n\nCivilizations have generally ended in one of two ways; either through being incorporated into another expanding civilization (e.g. As Ancient Egypt was incorporated into Hellenistic Greek, and subsequently Roman civilizations), or by collapse and reversion to a simpler form, as happens in what are called Dark Ages.\n\nThere have been many explanations put forward for the collapse of civilization. Some focus on historical examples, and others on general theory.\n\nPolitical scientist Samuel Huntington, has argued that the defining characteristic of the 21st century will be a clash of civilizations. According to Huntington, conflicts between civilizations will supplant the conflicts between nation-states and ideologies that characterized the 19th and 20th centuries. These views have been strongly challenged by others like Edward Said, Muhammed Asadi and Amartya Sen. Ronald Inglehart and Pippa Norris have argued that the \"true clash of civilizations\" between the Muslim world and the West is caused by the Muslim rejection of the West's more liberal sexual values, rather than a difference in political ideology, although they note that this lack of tolerance is likely to lead to an eventual rejection of (true) democracy. In \"Identity and Violence\" Sen questions if people should be divided along the lines of a supposed \"civilization\", defined by religion and culture only. He argues that this ignores the many others identities that make up people and leads to a focus on differences.\n\nCultural Historian Morris Berman suggests in \"Dark Ages America: the End of Empire\" that in the corporate consumerist United States, the very factors that once propelled it to greatness―extreme individualism, territorial and economic expansion, and the pursuit of material wealth―have pushed the United States across a critical threshold where collapse is inevitable. Politically associated with over-reach, and as a result of the environmental exhaustion and polarization of wealth between rich and poor, he concludes the current system is fast arriving at a situation where continuation of the existing system saddled with huge deficits and a hollowed-out economy is physically, socially, economically and politically impossible. Although developed in much more depth, Berman's thesis is similar in some ways to that of Urban Planner, Jane Jacobs who argues that the five pillars of United States culture are in serious decay: community and family; higher education; the effective practice of science; taxation and government; and the self-regulation of the learned professions. The corrosion of these pillars, Jacobs argues, is linked to societal ills such as environmental crisis, racism and the growing gulf between rich and poor.\n\nSome environmental scientists also see the world entering a Planetary Phase of Civilization, characterized by a shift away from independent, disconnected nation-states to a world of increased global connectivity with worldwide institutions, environmental challenges, economic systems, and consciousness. In an attempt to better understand what a Planetary Phase of Civilization might look like in the current context of declining natural resources and increasing consumption, the Global scenario group used scenario analysis to arrive at three archetypal futures: Barbarization, in which increasing conflicts result in either a fortress world or complete societal breakdown; Conventional Worlds, in which market forces or policy reform slowly precipitate more sustainable practices; and a Great Transition, in which either the sum of fragmented Eco-Communalism movements add up to a sustainable world or globally coordinated efforts and initiatives result in a new sustainability paradigm.\n\nCultural critic and author Derrick Jensen argues that modern civilization is directed towards the domination of the environment and humanity itself in an intrinsically harmful, unsustainable, and self-destructive fashion. Defending his definition both linguistically and historically, he defines civilization as \"a culture... that both leads to and emerges from the growth of cities\", with \"cities\" defined as \"people living more or less permanently in one place in densities high enough to require the routine importation of food and other necessities of life\". This need for civilizations to import ever more resources, he argues, stems from their over-exploitation and diminution of their own local resources. Therefore, civilizations inherently adopt imperialist and expansionist policies and, to maintain these, highly militarized, hierarchically structured, and coercion-based cultures and lifestyles.\n\nThe Kardashev scale classifies civilizations based on their level of technological advancement, specifically measured by the amount of energy a civilization is able to harness. The Kardashev scale makes provisions for civilizations far more technologically advanced than any currently known to exist (see also: Civilizations and the Future and Space civilization).\n\n\n"}
{"id": "590768", "url": "https://en.wikipedia.org/wiki?curid=590768", "title": "Classic", "text": "Classic\n\nA classic is an outstanding example of a particular style; something of lasting worth or with a timeless quality; of the first or highest quality, class, or rank – something that exemplifies its class. The word can be an adjective (a \"classic\" car) or a noun (a \"classic\" of English literature). It denotes a particular quality in art, architecture, literature, design, technology, or other cultural artifacts. In commerce, products are named 'classic' to denote a long-standing popular version or model, to distinguish it from a newer variety. \"Classic\" is used to describe many major, long-standing sporting events. Colloquially, an everyday occurrence (e.g. a joke or mishap) may be described in some dialects of English as 'an absolute classic'.\n\n\"Classic\" should not be confused with \"classical\", which refers specifically to certain cultural styles, especially in music and architecture: styles generally taking inspiration from the Classical tradition, hence classicism.\n\nThe classics are the literature of ancient Greece and Rome, known as classical antiquity, and once the principal subject studied in the humanities. Classics (without the definite article) can refer to the study of philosophy, literature, history and the arts of the ancient world, as in \"reading classics at Cambridge\". From that usage came the more general concept of 'classic'.\n\nThe Chinese classics occupy a similar position in Chinese culture, and various other cultures have their own classics.\n\nBooks, films and music particularly may become \"a classic\" but a painting would more likely be called a masterpiece. A classic is often something old that is still popular.\n\nThe first known use of \"classic\" in this sense — a work so excellent that it is on the level of the \"classics\" (Greek and Latin authors) — is by the 18th-century scholar Rev. John Bowle. He applied the term to \"Don Quixote\", of which Bowle prepared an innovative edition, such as he judged that a classic work needed.\n\nSome other examples would be the book \"The Adventures of Tom Sawyer\" by Mark Twain, the 1941 film \"Citizen Kane\", and the song Heartbreak Hotel by Elvis Presley. Lists of classics are long and wide-ranging, and would vary depending on personal opinion. Classic rock is a popular radio format, playing a repertoire of old but familiar recordings.\n\nA contemporary work may be hailed as an \"instant classic\", but the criteria for classic status tends to include the test of time. The term \"classic\" is in fact often generalized to refer to any work of a certain age, regardless of whether it is any good. A cult classic may be well known but is only favored by a minority.\n\nA well known and reliable procedure, such as a demonstration of well-established scientific principle, may be described as classic: e.g. the cartesian diver experiment.\n\nManufacturers frequently describe their products as classic, to distinguish the original from a new variety, or to imply qualities in the product – although the Ford Consul Classic, a car manufactured 1961–1963, has the \"classic\" tag for no apparent reason. The iPod classic was simply called the iPod until the sixth generation, when \"classic\" was added to the name because other designs were also available – an example of a retronym. \"Coca-Cola Classic\" is the name used for the relaunch of Coca-Cola after the failure of the New Coke recipe change. Similarly, the Classic (transit bus), a transit bus manufactured from 1982–97, succeeded an unpopular futuristic design.\n\nA classic can be something old that remains prized or valuable (but not an antique). Classic cars, for example, are recognised by various collectors' organisations such as the Classic Car Club of America, who regulate the qualifying attributes that constitute classic status.\n\nMany sporting events take the name \"classic\":\n\n\nIn Spanish-speaking countries, the term \"Clásico\" refers to a match between two football teams known as traditional rivals, e.g. El Clásico in Spain.\n\n"}
{"id": "2784571", "url": "https://en.wikipedia.org/wiki?curid=2784571", "title": "Clay-body", "text": "Clay-body\n\nThe Clay-body, clay corpse, or \"Corp criadhach\" (Scottish Gaelic) might be said to be an indigenous Scottish variant of the more famous voodoo doll.\n\nSupposedly, when a witch wanted to destroy anyone to whom she had an ill will, she often made a “corpse” of clay resembling the unfortunate one, and placed it in some out-of-the-way stream under a precipice or waterfall, in such a way that the water trickled slowly on it. As the clay-body wasted, so the live body of the person it resembled was also supposed to waste away. Were the clay-body found, it was carefully preserved, and so the spell of the witch was broken. Sometimes pins were stuck in the clay body to make the death of the doomed one more painful. Several such bodies have been found, even around the turn of the 20th century.\n\n\n"}
{"id": "35012838", "url": "https://en.wikipedia.org/wiki?curid=35012838", "title": "Cultural mulatto", "text": "Cultural mulatto\n\nThe cultural mulatto is a concept introduced by Trey Ellis in his 1989 essay, \"The New Black Aesthetic\". While the term \"mulatto\" typically refers to a person of mixed black and white ancestry, a cultural mulatto is defined by Ellis as a black person who is highly educated and usually a part of the middle or upper-middle class, and therefore assimilates easily into traditionally white environments. \n\nEllis writes, \"Just as a genetic mulatto is a black person of mixed parents who can often get along fine with his white grandparents, a cultural mulatto, educated by a multi-racial mix of cultures, can also navigate easily in the white world\" (235). Cultural mulattoes are skillful code-switchers and they may be equally comfortable around blacks as around whites. Members of the New Black Aesthetic are typically cultural mulattoes. Their ability for easy interaction with both blacks and whites is what ultimately allows cultural mulattoes opportunities for class and status upward mobility.\n\nExamples of cultural mulattoes offered by Ellis include:\n\n\n"}
{"id": "394976", "url": "https://en.wikipedia.org/wiki?curid=394976", "title": "Culture shock", "text": "Culture shock\n\nCulture shock is an experience a person may have when one moves to a cultural environment which is different from one's own; it is also the personal disorientation a person may feel when experiencing an unfamiliar way of life due to immigration or a visit to a new country, a move between social environments, or simply transition to another type of life. One of the most common causes of culture shock involves individuals in a foreign environment. Culture shock can be described as consisting of at least one of four distinct phases: honeymoon, negotiation, adjustment, and adaptation. \n\nCommon problems include: information overload, language barrier, generation gap, technology gap, skill interdependence, formulation dependency, homesickness (cultural), boredom (job dependency), response ability (cultural skill set). There is no true way to entirely prevent culture shock, as individuals in any society are personally affected by cultural contrasts differently.\n\nDuring this period, the differences between the old and new culture are seen in a romantic light. For example, in moving to a new country, an individual might love the new food, the pace of life, and the locals' habits. During the first few weeks, most people are fascinated by the new culture. They associate with nationals who speak their language, and who are polite to the foreigners. Like most honeymoon periods, this stage eventually ends.\n\nAfter some time (usually around three months, depending on the individual), differences between the old and new culture become apparent and may create anxiety. Excitement may eventually give way to unpleasant feelings of frustration and anger as one continues to experience unfavorable events that may be perceived as strange and offensive to one's cultural attitude. Language barriers, stark differences in public hygiene, traffic safety, food accessibility and quality may heighten the sense of disconnection from the surroundings.\n\nWhile being transferred into a different environment puts special pressure on communication skills, there are practical difficulties to overcome, such as circadian rhythm disruption that often leads to insomnia and daylight drowsiness; adaptation of gut flora to different bacteria levels and concentrations in food and water; difficulty in seeking treatment for illness, as medicines may have different names from the native country's and the same active ingredients might be hard to recognize.\n\nStill, the most important change in the period is communication: People adjusting to a new culture often feel lonely and homesick because they are not yet used to the new environment and meet people with whom they are not familiar every day. The language barrier may become a major obstacle in creating new relationships: special attention must be paid to one's and others' culture-specific body language signs, linguistic faux pas, conversation tone, linguistic nuances and customs, and false friends. \n\nIn the case of students studying abroad, some develop additional symptoms of loneliness that ultimately affect their lifestyles as a whole. Due to the strain of living in a different country without parental support, international students often feel anxious and feel more pressure while adjusting to new cultures—even more so when the cultural distances are wide, as patterns of logic and speech are different and a special emphasis is put on rhetoric.\n\nAgain, after some time (usually 6 to 12 months), one grows accustomed to the new culture and develops routines. One knows what to expect in most situations and the host country no longer feels all that new. One becomes concerned with basic living again, and things become more \"normal\". One starts to develop problem-solving skills for dealing with the culture and begins to accept the culture's ways with a positive attitude. The culture begins to make sense, and negative reactions and responses to the culture are reduced.\n\nIn the mastery stage individuals are able to participate fully and comfortably in the host culture. Mastery does not mean total conversion; people often keep many traits from their earlier culture, such as accents and languages. It is often referred to as the bicultural stage.\n\nReverse culture shock (also known as \"re-entry shock\" or \"own culture shock\") may take place—returning to one's home culture after growing accustomed to a new one can produce the same effects as described above. These are results from the psychosomatic and psychological consequences of the readjustment process to the primary culture. The affected person often finds this more surprising and difficult to deal with than the original culture shock. This phenomenon, the reactions that members of the re-entered culture exhibit toward the re-entrant, and the inevitability of the two are encapsulated in the following saying, which is also the title of a book by Thomas Wolfe, \"You Can't Go Home Again\".\n\nReverse culture shock is generally made up of two parts: idealization and expectations. When an extended period of time is spent abroad we focus on the good from our past, cut out the bad, and create an idealized version of the past. Secondly, once removed from our familiar setting and placed in a foreign one we incorrectly assume that our previous world has not changed. We expect things to remain exactly the same as when we left them. The realization that life back home is now different, that the world has continued without us, and the process of readjusting to these new conditions as well as actualizing our new perceptions about the world with our old way of living causes discomfort and psychological anguish. \n\nThere are three basic outcomes of the Adjustment Phase:\n\nCulture shock has many different effects, time spans, and degrees of severity. Many people are handicapped by its presence and do not recognize what is bothering them.\n\nCulture shock is a subcategory of a more universal construct called transition shock. Transition shock is a state of loss and disorientation predicated by a change in one's familiar environment that requires adjustment. There are many symptoms of transition shock, including:\n\n"}
{"id": "43727113", "url": "https://en.wikipedia.org/wiki?curid=43727113", "title": "Dematerialization (art)", "text": "Dematerialization (art)\n\nDematerialization of the art object is an idea in conceptual art. In \"Six Years: The Dematerialization of the Art Object\" Lucy Lippard characterizes the period of 1966 to 1972 as one in which the art object was dematerialised through the new artistic practices of conceptual art.\n"}
{"id": "221159", "url": "https://en.wikipedia.org/wiki?curid=221159", "title": "Eight Immortals of the Wine Cup", "text": "Eight Immortals of the Wine Cup\n\nThe Eight Immortals of the Wine Cup or Eight Immortals Indulged in Wine () were a group of Tang Dynasty scholars who are known for their love of alcoholic beverages. They are not deified and \"xiān\" (\"immortal; transcendent; fairy\") is metaphorical. The term is used in a poem by Du Fu, as well as in the biography of Li Bai in the New Book of Tang.\n\nThey appeared in Du's poem in the following order:\n\n\n\n"}
{"id": "52055992", "url": "https://en.wikipedia.org/wiki?curid=52055992", "title": "Ellis Cashmore", "text": "Ellis Cashmore\n\nEllis Cashmore (10 February 1949 in Staffordshire, Great Britain) is a British sociologist and cultural critic. He is currently a visiting professor of sociology at Aston. Before teaching at Aston, he used to teach culture, media and sport at Staffordshire University, starting in 1993. Before 1993, he taught sociology at the University of Tampa, Florida; and, before this, he was a lecturer in sociology at the University of Hong Kong.\n"}
{"id": "45184563", "url": "https://en.wikipedia.org/wiki?curid=45184563", "title": "Falata-Umbroro", "text": "Falata-Umbroro\n\nFalata Umbroro (the transhumance pastoralists) originally came from western Africa and settled in the Blue Nile in the beginning of the 1950s. They were permitted by the indigenous tribes to use the natural resources.\n\nThe Falata-Umbroro actually consist several ethnic groups which have their own local language, religion and native homeland. Originally from Western African countries, including Mali, Mauritania, Cameroon and Nigeria. Falata herders breed a species of cow that is known for its big horns, brown skin and tough hide.\n"}
{"id": "274375", "url": "https://en.wikipedia.org/wiki?curid=274375", "title": "Generations of Chinese leadership", "text": "Generations of Chinese leadership\n\nSince both the Communist Party of China and the People's Liberation Army promote according to seniority, it is possible to discern distinct generations of Chinese leadership. In official discourse, each group of leadership is identified with a distinct extension of the ideology of the party. Historians have studied various periods in the development of the government of the People's Republic of China by reference to these \"generations\".\n\nWhile in English the chronological leadership groups are commonly referred to as \"generations of Chinese leadership\", there is no exact equivalent expression in Chinese. The usual term in official discourse for such a group is a \"leadership collective\", which are counted in generations. Thus, for example, the \"first generation\" of leaders identified below are labelled as \"the first generation leadership collective\". In official discourse, they are also not viewed as leaders of the \"state\" (the People's Republic of China), but rather leaders of the \"party\" (the Communist Party of China).\n\nIn the Communist Party's official discourse, the \"generational\" division and identification of the \"core leader\" for each of the first, second and third generations was set down during the leadership of Jiang Zemin and first publicised in 1999. This division and identification was not uncontroversial at the time, since the party had hitherto regarded his immediate predecessors as the party's General Secretary, Hu Yaobang and Zhao Ziyang, as its leaders, and regarded Deng Xiaoping as the \"power behind the throne\" rather than a formal leader. The revision of party history into \"generations\" helped to secure Jiang's position as \"core\" and anointed heir. Through the invention of the concept of the \"core\", it also helped to de-legitimise Jiang's deposed predecessors (such as Hu and Zhao), by relegating them from party leader to mere \"membership\" of a \"leadership collective\", which also conveniently helped to legitimise their deposition.\n\nJiang's successors have maintained this generational division, but have retreated from identifying a \"core leader\" in the fourth generation, and the succeeding general secretary Hu Jintao has never been identified in official announcements as the \"core\" of the fourth generation, preferring to be simply called by his title \"General Secretary\". Xi Jinping did continue this practice until October 2016 when the 6th Plenary of the 18th Central Committee named him as the \"core leader\" in a document.\n\nThe \"leadership collective\" at any one time usually, but not always, correlates with the members of the Politburo Standing Committee of the Communist Party of China, with the leader of the party (the Chairman or, after 1982, the General Secretary) often, but not always, the leader of this leadership collective.\n\nAlthough the first generation of leaders ruled collectively for only part of the period, and Mao Zedong was to a large extent a paramount, autocratic leader for most of the period, the successive leaders from the founding of the People's Republic in 1949 until Mao's death and the dismantling of the power of his closest deputies in 1976 are now referred to retrospectively as the \"first generation\" of leaders in official discourse.\n\nThus, the first generation, from 1949 to 1976, consisted of Mao Zedong as core, along with Zhou Enlai, Zhu De (as informal triumvirate), Liu Shaoqi, Chen Yun, Peng Dehuai, and later Lin Biao and the Gang of Four (neither Lin nor the Gang are today considered by official discourse to be part of this generation because of political antagonism resulting from the Cultural Revolution). These were the leaders that founded the People's Republic of China after the Communist victory in the Chinese Civil War. They were born between 1886 and 1907, although the Gang of Four were a distinct subgroup born 1914 to 1935. Most were born before the demise and fall of the Qing Dynasty (not including the Gang) and thus lived to see both the birth and, on the mainland, the end of the Republic of China. One heavy characteristic of these leaders were that they tended to be both political and military leaders. Most had some education outside China, and their formative experiences included the Long March, Chinese Civil War and the Second Sino-Japanese War. The guiding political ideology from the first generations were general principles of Marxism and \"Mao Zedong Thought\".\n\nOf this group, Mao, Zhou, Zhu and Liu were the four original members of the collective leadership from 1949 until the political turmoil of the late 1950s and early 1960s, which resulted in Mao gaining paramount autocratic power. Liu, then the President (or \"Chairman of the State\"), was removed from his party position in 1966, placed under house arrest in 1967, and died from torture and maltreatment in 1969. Mao, Zhou and Zhu were the only three original members of the Politburo Standing Committee of the Communist Party of China who remained in the Politburo from 1945 until their deaths in 1976 (though Zhu temporarily lost his membership between 1969-1973) and died while holding the highest party and state offices Chairman of the Communist Party (Mao), Premier of the State Council (Zhou) and Chairman of the Standing Committee of the National People's Congress, the nominal head of state (Zhu).\n\nWith the demise of Liu, Mao promoted Lin Biao as his deputy, and the \"Gang of Four\" to fill the role of his trusted henchmen. Lin fell out of favour, however, and died in 1971 while attempting to escape to the Soviet Union. The Gang of Four, which consisted of Jiang Qing (Madame Mao), and three other members meteorically promoted in the late 1960s, were the only members of the first generation of leadership to remain after Mao's death in 1976. Their demise came shortly afterwards in a political coup managed by what become the second generation of leaders.\n\nOf the other members identified above, Chen Yun was sidelined from the early 1960s, lost his party position in 1969, but survived to play an influential role in the second generation of leadership. Peng Dehuai was denounced in 1959, made a brief return to government in 1965, but was detained by Red Guards from 1966 and died in prison from torture and maltreatment in 1974. While not named above, Deng Xiaoping, the core of the second generation of leadership, also played a key role in the first generation at various times, mainly as an ally of Zhou and Peng, but was purged from government in 1976 and remained sidelined at the time of Mao's death.\n\nThe death of Mao, Zhou and Zhu in 1976, and soon afterwards the coup that resulted in the arrest of the Gang of Four, ushered in the era now identified as the \"second generation\" of leaders. The era began with Hua Guofeng as the successor to Mao, but his position was soon eclipsed by the ascendancy of Deng Xiaoping as the paramount leader, in which position he remained at least until 1992 when he resigned from his leadership positions. During this period the most power and influence had a group of old party veterans, known as the Eight Elders, whose main members were Li Xiannian and Chen Yun, together with Deng. All of them had more than 40 years of political experience.\n\nThus, in official discourse, the second generation of leadership lasted from 1976 to 1992. The official discourse of the Communist Party today identifies Deng Xiaoping as the \"core\" of this second generation, but Deng was never formally the leader of the party during this period. Instead, the formal party leaders during this time were, successively, Hua Guofeng, Hu Yaobang, Zhao Ziyang and Jiang Zemin. Other prominent leaders of this generation were Chen Yun, Li Xiannian, Ye Jianying, Peng Zhen and Wang Zhen. These leaders were also involved in the Chinese revolution, but with the exception of Deng Xiaoping, served in more junior roles, as they were all born from 1897 to 1921 (that is, some were born after the demise of the Qing Empire in the Xinhai Revolution). Like the first generation, many were educated overseas, particularly in France. Their young formative experiences were similar to the first generation. Most had some position of authority during the Cultural Revolution, although as a rule those that held power after the 1980s were purged during that decade. This generation turned the focus from class struggle and political movements to economic development and pioneering Chinese economic reform.\n\nThe dominant political ideology of the era was Deng Xiaoping Theory, which was accepted by the party in 1978, and he served in various leadership positions, although his paramount power was not overtly reflected in his formal titles. Instead, the formal leaders of the party were a series of younger leaders promoted (and then demoted) by Deng and other influential elders. The most prominent of these were Hu Yaobang (Party Chairman from 1981, General Secretary from 1982, demoted 1987, died 1989), and Zhao Ziyang (Premier from 1980, General Secretary from 1987, demoted and place under house arrest in 1989). They were replaced by Li Peng and Jiang Zemin, who would go on to become prominent members of the third generation of leaders.\n\nConsistently influential behind the scenes during Deng's time as paramount leader were fellow elder statesmen Chen Yun and Li Xiannian. Other prominent elders were Ye Jianying (until his death in 1986), Deng Yingchao (Zhou Enlai's widow, until her death in 1992) and Peng Zhen (who retired in 1988). Yang Shangkun played a brief but important role in the period between 1989 and 1992, when as President he subverted the existing constitutional convention and turned the office of President from a symbolic role into an executive one.\n\nThe transition towards the third generation of leadership began with the Tiananmen Square protests of 1989. The purges that followed led to the promotion of what became the third generation of leadership. Soon afterwards, Deng resigned his last major party post, as chairman of the central military committee, although he remained influential behind the scenes until his death in 1997.\n\nBetween 1989 and 1992, Jiang was believed to be simply a transitional figure to protect the party from a power vacuum (or even an Eastern Bloc style collapse) until a more stable successor government to Deng could be put in place. Because of this, the era of the \"third generation\" is not regarded to have begun until 1992, with the election of the new Politburo standing committee and Jiang consolidating his power without Deng.\n\nThus, the \"third generation\" lasted from 1992 to 2002, with Jiang Zemin as core, and other leaders including Li Peng, Zhu Rongji, Qiao Shi, Li Ruihuan. These leaders were born before the revolution from 1924 to 1934 but were educated afterwards before the Sino-Soviet split. Most of them received education in the Soviet Union as engineers and entered the party initially as factory managers. As a result, many of them did not wield any significant political power prior to the 1980s, spending their time during the Cultural Revolution and its aftermath working for the civil infrastructure of the state and were protected from the purges, as opposed to their predecessors. Unlike their predecessors, there is a split between the political and military leadership. Their formative experiences included the Second Sino-Japanese War and the Korean War. This generation continued economic development while China saw the emergence of various serious social issues. The political ideological innovation officially associated with this period was Jiang's \"Three Represents\".\n\nThe initial members of the third generation were mostly survivors from before 1989, including Jiang Zemin, Li Peng (who continued as premier), Qiao Shi and Li Ruihuan. Notable changes to the leadership were the elevation of Zhu Rongji in place of Li Peng as premier in 1998 and the elevation of Hu Jintao as vice president. For the first time since 1982, the three centres of power of the presidency, the party general secretaryship and the chairmanship of the central military commission were concentrated in a single person, Jiang Zemin. This enabled him to declare himself the \"core\" of the third generation of leadership.\n\nDuring this period, while Deng Xiaoping had retired from all leadership positions, he remained influential. In 1992, Deng's informal intervention ensured that market-orientated reforms were not halted by resurgent conservative elements. Deng also played an important role in nominating Hu Jintao as Jiang's successor as party secretary.\n\n2002 saw the first orderly transition of power in the Communist Party of China in accordance with rules on term limits. The new leaders were elected to the Communist Party's Politburo in 2002, and took up their governmental positions in 2003, while the most prominent of their \"third generation\" predecessors stepped down at the same time.\n\nThus, the era of the \"fourth generation\" is officially regarded to have begun in 2002, and lasted until 2012, when the next election for the party leadership occurred. The prominent leaders included Hu Jintao (as General Secretary), Wu Bangguo, Wen Jiabao, Jia Qinglin, Zeng Qinghong and Li Changchun. It is also known as the \"republican generation\" or the Hu-Wen Administration. These were promoted to top leadership at the 16th Party Congress and remained in power until the 18th Party Congress in 2012. This generation of leaders, born mainly in the World War II years from 1939 to 1944, represented a new technocratic style governance and a less centralized political structure. The majority of this generation of leaders were engineers whose academic lives were disrupted by the Cultural Revolution and, unlike both their predecessors and likely successors, have spent very little time overseas. The dominant political ideology of this era was Hu's Scientific Development Concept and a goal for a Socialist Harmonious Society.\n\nThe fifth generation came to power at the 18th Party Congress in 2012, when Hu Jintao stepped down as Party General Secretary. In the fifth generation, one sees fewer engineers and more management and finance majors, including successful entrepreneurs. Most of the fifth generation of civilian leadership, born in the postwar years 1945 to 1955, were educated at top Chinese universities. Hu Jintao's Communist Youth League faction, and the Crown Prince Party (or \"Princelings\") are seen to be the two dominant factions within the leadership.\n\nFollowing his elevation to General Secretary of the Communist Party of China and Chairman of the Central Military Commission, which oversees the People's Liberation Army, the Princeling and current General Secretary Xi Jinping succeeded Hu Jintao as the paramount leader of this generation. Premier Li Keqiang took the place of former Premier Wen Jiabao. Other prominent figures that are top figures in the 5th generation as it develops include Congress Chairmen Zhang Dejiang and Li Zhanshu, Conference Chairmen Yu Zhengsheng and Wang Yang, Secretariat Secretaries Liu Yunshan and Wang Huning, Vice President and former Discipline Secretary Wang Qishan, first Vice Premiers Zhang Gaoli and Han Zheng, former Vice President Li Yuanchao, and its leading females, former Vice Premier Liu Yandong and current Vice Premier Sun Chunlan.\n\nThe sixth generation of leaders has been expected to come to power at the 20th Party Congress in 2022. Under current unspoken Chinese political conventions the leaders of this generation would be mostly born in the 1960s, and speculated future leaders were born from 1960 to 1967. Current speculation places former Communist Youth League head and current 3rd-ranked Vice Premier Hu Chunhua as a possible core figure. Hu and Sun Zhengcai were the only Politburo members named at the 18th Party Congress in 2012 who were born after 1960, making their further advancement an apparent certainty, but Sun was purged before the 19th Party Congress. U.S.-based newspaper \"Duo Wei Times\" also listed Fujian Governor Su Shulin (since then fallen from grace), President of the Supreme People's Court Zhou Qiang, Heilongjiang Party Secretary Zhang Qingwei, and Minister of Natural Resources Lu Hao as other potential figures in this generation of leadership. Others in this rough age group ascending in the ranks include Zhang Guoqing (Mayor of Tianjin) and Chen Min'er (Politburo member).\n\nFollowing Xi Jinping's consolidation of power at the 19th Party Congress, the future of the sixth generation was cast into doubt as clear successor figures failed to be named to senior leadership posts, particularly the Politburo Standing Committee.\n\n"}
{"id": "54395985", "url": "https://en.wikipedia.org/wiki?curid=54395985", "title": "Großwilsdorf", "text": "Großwilsdorf\n\nThe villages of Großwilsdorf and the Rödel Plateau are situated in Saxony-Anhalt in the middle of Germany. It has been proposed by Germany for inscription in the List of World Heritage. The World Heritage nomination Naumburg Cathedral and the High Medieval Cultural Landscape of the Rivers Saale and Unstrut is representative for the processes that shaped the continent during the High Middle Ages between 1000 and 1300: Christianization, the so-called Landesausbau and the dynamics of cultural exchange and transfer characteristic for this very period.\n\nThe farming community of Gross-Wilsdorf consists of two villages. It is an example of a circular village structure in the Slav-German settlement.\nThe entrance of the high medieval village was traditionally marked by a pond. This pond was kept for use as service water in everyday life and for fire fighting in cases of emergency. In the village of Gross-Wilsdorf, you can also find a Romanesque choir tower church, which had been erected between the two high medieval settlement nucleuses of Gross-Wilsdorf and Kleinwilsdorf and which has partly been preserved. The tombstones of the church were ornamented with engraved crosses and a stick-like image deemed to be a walking stick. Since medieval tombstones in places other than large cathedrals and minsters have rarely been preserved, the tombstones in Gross-Wilsdorf can be considered special.\n\nThe villages of Großwilsdorf demonstrate the cultural exchange of neighboring settlers in the region, who all relied at first on their traditional settlement forms. Sorbs, Thuringians, and other groups created three individual round villages in the 12th century, that all shared the same village church built right in the middle between these settlements. During the High Middle Ages these settlers from different regions and ethnicities mixed and the villages grew together, forming nowaday Großwilsdorf. The layouts of houses and streets are still well preserved and dated to the High Middle Ages.\n\nAdjoining the village lies the Rödel Plateau with its porous limestone layers. The important limestone quarry “Rödel” served for producing large-sized cut stones of highest quality and supplied the Naumburg Cathedral with building material. Neuenburg Castle was almost entirely built using the local shell limestone. Structural elements of higher quality were made using shell limestone from the surrounding quarries. This quarry also is visible and unchanged until today, as are the transport paths for the limestone to Naumburg.\n\nIn the year 1032, King Conrad II gave the royal court of Balgstädt to the Cathedral of Naumburg, including all accessories such as rocks and cliffs (molis). The monastery was granted the permission to dig out suitable rock material there freely, to break it up, roll it out and carry it away. The great number of pits identified by way of laser scans clearly reflect that there was likely a multitude of entitled parties and producers because of the highly fragmented nature of medieval stone quarrying. Parcels could be identified in the forests on the Rödel Plateau.\n\n\n"}
{"id": "31720061", "url": "https://en.wikipedia.org/wiki?curid=31720061", "title": "Hayandose", "text": "Hayandose\n\nHayandose is a cultural category used to express membership and belonging among Zapotec migrants, described by cultural anthropologist Lourdes Gutiérrez-Nájera. Hayandose entails a process of creating ethnically-marked spaces among migrants in an effort to combat feelings of marginalization and displacement in a host country. This concept may be compared to the notion of Native Hubs developed by anthropologist Renya Ramirez to describe how urban Native Americans negotiate a transnational existence. \n“Hayandose”, in \"Beyond el Barrio: Everyday Life in Latina/o America\", examines the place of indigenous people within the broader scope of Latino Studies and also within the national political landscape. As argued in the text, indigenous subjects do not easily fit the category of \"Latino\" used to describe national identities; for example, Guatemalan, Mexican, Ecuadorian. At the same time, indigenous migrants often are targets of racism and prejudice directed towards them. The essay is in conversation with other essays in the volume that interrogate the ways that Latinos carve out niches for themselves and thrive in urban spaces within the United States. As the essay Hayandose argues, such established spaces allow migrants, struggling with separation from their home country and racist stigmatization in their host country, to engage in a “meaningful practice of belonging” in which they are able to express their cultural membership. Hayandose marks the point at which people finally feel as though they belong through the discovery of themselves in a foreign place.\n\nGutiérrez-Nájera uses Zapotecs as an example of migrants who originate from Oaxaca, México and form their own spaces of belonging in the United States, specifically in Los Angeles. The Zapotecs that Gutiérrez-Najera writes about are from Yalálag, a small rural town in the heart of Oaxaca. The community of Yalaltecos in Los Angeles comes together collectively and participates in festivals, ceremonies, \"tandas\" and other small gatherings where they can gossip in Zapotec, share food, dance, financially and emotionally support one another, and engage in other customs and traditions from their place of origin. The existence and practice of customs and traditions that were once thought to be exclusive to Yalálag, but that have now permeated American society, demonstrates the transnational character of Yalaltecos indigeneity that makes possible the process of Hayandose through the seizure and declaration of these ethnically-marked spaces. Yalaltecos living in Los Angeles have invoked their cultural identity hundreds of miles from home and have used it as a tool to resist the push for assimilation and marginalization within the United States. Therefore, the transmission and continuity of culture across national borders are essential for Yalaltecos to mark their own space and ultimately find themselves in a hostile environment far from their home country. With this new-found sense of belonging, the opportunity for “rally[ing] for indigenous rights and the development of hometown communities in Oaxaca, as well as to organize in the United States around immigrant legislation” no longer remains out of reach. This claim is reaffirmed by another scholar, Annice Jacoby, who argues that the affirmation of transnational identities allows “borders of ownership, space, and social agency” to be challenged. Once a migrant \"se hayan\", or finds themselves, they have gained a sense of belonging by affirming their cultural membership and confronting their marginalization and displacement within a space collectively or individually marked as their own in the host country. This phenomenon then acts as a bridge for migrants to gain social ascendancy and acknowledgment that has the potential to improve lives in the home and host country alike.\n\nHayandose is also reflective of a contemporary cultural process known as de/territorialization. When emigrants moved to the United States they and their culture became deterritorialized. Once they moved, however, they are simultaneously reterritorialized as they begin to form a space for themselves and practice their customs and traditions as they did in their home country. The theory of de/territorialization can be seen in the Yalalteco community in Los Angeles through the perseverance of their cultural solidarity from Oaxaca to California, and the maintenance of community ties that traverse national boundaries. The theories of de/territorialization and Hayandose entail the movement of culture from one place to another, and thus reveal the “multiple centers” culture can obtain, as opposed to just a single \"center\" being the place of origin. Because migration causes displacement among emigrants who are forced to adapt to a new environment, culture, and way of living, bringing the culture and customs that they practiced in their places of origin to their new location helps to combat these negative feelings and also adds an additional “center” to their particular culture. In regards to Gutiérrez-Nájera’s example, Yalaltecos adopted Los Angeles as an additional “center” where their customs and traditions could be expressed as they would have been in Yalálag. This sharing of cultures creates a cultural flow between the United States and Oaxacan communities that have allowed emigrants not to lose their culture and sense of who they are, but to maintain it and ultimately use it as a powerful tool to engage in practices of belonging that resist sentiments of marginalization and displacement in new locations.\n"}
{"id": "4170165", "url": "https://en.wikipedia.org/wiki?curid=4170165", "title": "History of mentalities", "text": "History of mentalities\n\nThe history of mentalities is a calque of the French term \"histoire des mentalités\", which might also be translated as \"history of attitudes\", \"mindsets\" or \"world-views\". The term describes a particular manner of researching history. This approach is associated with the \"critical turn\" (\"tournant critique\") taken by the historians of the \"Annales\" School, particularly around the 1960s.\n\nIn keeping with the \"Annales\"' interest in the \"longue durée\" (long-term), the history of mentalities focuses on the mindsets of past cultural and social groups, and their gradual changes over time, as opposed to the history of particular events, or economic trends. The term can also be seen as an equivalent to, or a form of, cultural history or historical anthropology\n\nThe history of mentalities, or \"histoire des mentalités\", is a term used to describe works of history aimed at describing and analyzing the ways in which people of a given time period thought about, interacted with, and classified the world around them. The history of mentalities has been used as a historical tool by several historians and scholars from various schools of history. Notably, the historians of the \"Annales\" School helped to develop the history of mentalities and construct a methodology from which to operate. In establishing this methodology, they sought to limit their analysis to a particular place and a particular time. This approach lends itself to the intensive study that characterizes microhistory, another field which adopted the history of mentalities as a tool of historical analysis.\n\nThe origin of the term history of mentalities lies in the writings of the \"Annales\" historians such as Georges Duby and Roger Chartier. In seeking to create works of total history, \"Annales\" historians tended not to simply rely on the political or event oriented history of past generations. Michael Harsgor points out in that the challenge of the \"Annales\" historians was not to create this deterministic history that appeared to rely heavily on teleological conclusions, such as the Marxist forms of history being written at the time. Rather, Harsgor writes that the \"Annales\" historians tasked themselves with the creation of social structures, \"which means covering the skeleton of the basic economic analysis with the flesh of demographic, cultural, mental, and event psychoanalytical data.\" It has also been said that \"Annales\" historians, in their attempts at the creation of total history, considered the history of mentalities a single aspect in the creation of that history. Simply put, they were attempting to reconstruct the world of whatever time period they were examining. In his works, such as \"The Three Orders: Feudal Society Imagined\" and his work on William Marshal, Duby focused on the development of ideologies within the structures that permeated the various aspects of an individual's life.\n\nThis development in methodology would prove crucial for other historians who would use the history of mentalities to attempt to reconstruct the worldviews of individuals and extrapolate their findings to the population at large in the form of microhistories. These historians would largely concern themselves with social and cultural history in order to form their history of mentalities, narrowing their realm historical inquiry by not concerning themselves with the broad economic serialization that had become so important for the \"Annales\" historians. Carlo Ginzburg's book, \"The Cheese and the Worms\", is archetypical of the microhistories that emerged with the history of mentalities in mind. Ginzburg attempted to reconstruct peasant mentalities in sixteenth century Italy by examining the trial records of a single miller, Domenico Scandella, called Menocchio, and trying to find currents or similarities in otherwise fragmentary and obscure evidence.\n\nSimilar techniques can be seen in Robert Darnton's \"The Great Cat Massacre\", which uses microhistory to establish the mentalities of groups at different social levels of French society. Darnton concerns himself greatly with the ways in which people viewed the world around them. He interprets the symbolic significance of journeymen printers massacring neighborhood cats as a display of frustration with the growing bourgeoisie class. Similarly, and in keeping with the tradition of the history of mentalities, Darnton devotes a chapter to an analysis of a bourgeoisie's description of his city, in an effort to determine how an individual in a given social situation would interpret and make sense of the world around them. Darnton uses this description to demonstrate that the ways in which events might be portrayed might be completely unsupported by the ways in which individuals of the time might have interpreted those events.\n\nCriticisms have emerged regarding the history of mentalities at all stages of its development. In particular, Marxist historians were quick to criticize the \"Annales\" historians for \"attempts to include the study of mentalities in a general synthesis, which can only lead to the publication of articles reflecting a basic reliance upon faith accompanied by a consequent disparagement of reason.\" Carlo Ginzburg himself has criticized the methods of the history of mentalities for its \"decidedly classless character.\"\n\n\n"}
{"id": "2838569", "url": "https://en.wikipedia.org/wiki?curid=2838569", "title": "Hype cycle", "text": "Hype cycle\n\nThe hype cycle is a branded graphical presentation developed and used by the American research, advisory and information technology firm Gartner, for representing the maturity, adoption and social application of specific technologies. The hype cycle provides a graphical and conceptual presentation of the maturity of emerging technologies through five phases.\n\nAn example of a hype cycle is found in \"Amara's law\" coined by Roy Amara, which states that We tend to overestimate the effect of a technology in the short run and underestimate the effect in the long run.\n\nEach hype cycle drills down into the five key phases of a technology's life cycle.\nThe term \"hype cycle\" and each of the associated phases are now used more broadly in the marketing of new technologies.\n\nHype (in the more general media sense of the term \"hype\") plays a large part in the adoption of new media forms by society. Terry Flew states that hype (generally the enthusiastic and strong feeling around new forms of media and technology in which people expect everything will be modified for the better) surrounding new media technologies and their popularization, along with the development of the Internet, is a common characteristic. But following shortly after the period of 'inflated expectations', as per the diagram above, the new media technologies quickly fall into a period of disenchantment, which is the end of the primary, and strongest, phase of hype.\n\nMany analyses of the Internet in the 1990s featured large amounts of hype, which created \"debunking\" responses. However, such hype and the negative and positive responses toward it have given way to research that looks empirically at new media and its impact.\n\nA longer-term historical perspective on such cycles can be found in the research of the economist Carlota Perez. D R Laurence in clinical pharmacology described a similar process in drug development in the seventies.\n\nThere have been numerous criticisms of the hype cycle, prominent among which are that it is not a cycle, that the outcome does not depend on the nature of the technology itself, that it is not scientific in nature, and that it does not reflect changes over time in the speed at which technology develops. Another is that it is limited in its application, as it prioritizes economic considerations in decision-making processes. It seems to assume that a business' performance is tied to the hype cycle, whereas this may actually have more to do with the way a company devises its branding strategy. A related criticism is that the \"cycle\" has no real benefits to the development or marketing of new technologies and merely comments on pre-existing trends. Specific disadvantages when compared to, for example, technology readiness level are:\nAn analysis of Gartner Hype Cycles since 2000 shows that few technologies actually travel through an identifiable hype cycle, and that in practice most of the important technologies adopted since 2000 were not identified early in their adoption cycles.\n\n\n"}
{"id": "350381", "url": "https://en.wikipedia.org/wiki?curid=350381", "title": "Imagery", "text": "Imagery\n\nImagery, in a literary text, is an author's use of vivid and descriptive language to add depth to their work. It appeals to human senses to deepen the reader's understanding of the work. Powerful forms of imagery engage all of the senses.\n\nThere are seven major types of imagery, each corresponding to a sense, feeling, action, or reaction:\n\n\n\n"}
{"id": "15474", "url": "https://en.wikipedia.org/wiki?curid=15474", "title": "Infanticide", "text": "Infanticide\n\nInfanticide (or infant homicide) is the intentional killing of infants.\n\nParental infanticide researchers have found that mothers are far more likely than fathers to be the perpetrators of neonaticide and slightly more likely to commit infanticide in general.\n\nAnthropologist Laila Williamson notes that \"Infanticide has been practiced on every continent and by people on every level of cultural complexity, from hunter gatherers to high civilizations, including our own ancestors. Rather than being an exception, then, it has been the rule.\"\n\nIn many past societies, certain forms of infanticide were considered permissible. \n\nThe practice of infanticide has taken many forms over time. Child sacrifice to supernatural figures or forces, such as that believed to have been practiced in ancient Carthage, may be only the most notorious example in the ancient world. \n\nA frequent method of infanticide in ancient Europe and Asia was simply to abandon the infant, leaving it to die by exposure (i.e. hypothermia, hunger, thirst, or animal attack).\n\nIn at least one island in Oceania, infanticide was carried out until the 20th century by suffocating the infant, while in pre-Columbian Mesoamerica and in the Inca Empire it was carried out by sacrifice (see below).\n\nMany Neolithic groups routinely resorted to infanticide in order to control their numbers so that their lands could support them. Joseph Birdsell believed that infanticide rates in prehistoric times were between 15% and 50% of the total number of births, while Laila Williamson estimated a lower rate ranging from 15% to 20%. Both anthropologists believed that these high rates of infanticide persisted until the development of agriculture during the Neolithic Revolution. Comparative anthropologists have calculated that 50% of female newborn babies were killed by their parents during the Paleolithic era. From the infants hominid skulls (e.g. Taung child skull) that had been traumatized, has been proposed cannibalism by Raymond A. Dart. The children were not necessarily actively killed, but neglect and intentional malnourishment may also have occurred, as proposed by Vicente Lull as an explanation for an apparent surplus of men and the below average height of women in prehistoric Menorca.\n\nArchaeologists have uncovered physical evidence of child sacrifice at several locations. Some of the best attested examples are the diverse rites which were part of the religious practices in Mesoamerica and the Inca Empire.\n\nThree thousand bones of young children, with evidence of sacrificial rituals, have been found in Sardinia. Pelasgians offered a sacrifice of every tenth child during difficult times. Syrians sacrificed children to Jupiter and Juno. Many remains of children have been found in Gezer excavations with signs of sacrifice. Child skeletons with the marks of sacrifice have been found also in Egypt dating 950–720 BCE. In Carthage \"[child] sacrifice in the ancient world reached its infamous zenith\". Besides the Carthaginians, other Phoenicians, and the Canaanites, Moabites and Sepharvites offered their first-born as a sacrifice to their gods.\n\nIn Egyptian households, at all social levels, children of both sexes were valued and there is no evidence of infanticide. The religion of the Ancient Egyptians forbade infanticide and during the Greco-Roman period they rescued abandoned babies from manure heaps, a common method of infanticide by Greeks or Romans, and were allowed to either adopt them as foundlings or raise them as slaves, often giving them names such as \"copro -\" to memorialise their rescue. Strabo considered it a peculiarity of the Egyptians that every child must be reared. Diodorus indicates infanticide was a punishable offence. Egypt was heavily dependent on the annual flooding of the Nile to irrigate the land and in years of low inundation severe famine could occur with breakdowns in social order resulting, notably between 930–1070 AD and 1180–1350 AD. Instances of cannibalism are recorded during these periods but it is unknown if this happened during the pharaonic era of Ancient Egypt. Beatrix Midant-Reynes describes human sacrifice as having occurred at Abydos in the early dynastic period (c. 3150–2850 BCE), while Jan Assmann asserts there is no clear evidence of human sacrifice ever happening in Ancient Egypt.\n\nAccording to Shelby Brown, Carthaginians, descendants of the Phoenicians, sacrificed infants to their gods. Charred bones of hundreds of infants have been found in Carthaginian archaeological sites. One such area harbored as many as 20,000 burial urns. Skeptics suggest that the bodies of children found in Carthaginian and Phoenician cemeteries were merely the cremated remains of children that died naturally.\n\nPlutarch (c. 46–120 AD) mentions the practice, as do Tertullian, Orosius, Diodorus Siculus and Philo. The Hebrew Bible also mentions what appears to be child sacrifice practiced at a place called the Tophet (from the Hebrew \"taph\" or \"toph\", to burn) by the Canaanites. Writing in the 3rd century BCE, Kleitarchos, one of the historians of Alexander the Great, described that the infants rolled into the flaming pit. Diodorus Siculus wrote that babies were roasted to death inside the burning pit of the god Baal Hamon, a bronze statue.\n\nThe historical Greeks considered the practice of adult and child sacrifice barbarous, however, the exposure of newborns was widely practiced in ancient Greece, it was even advocated by Aristotle in the case of congenital deformity — \"As to the exposure of children, let there be a law that no deformed child shall live.” In Greece the decision to expose a child was typically the father's, although in Sparta the decision was made by a group of elders. Exposure was the preferred method of disposal, as that act in itself was not considered to be murder; moreover, the exposed child technically had a chance of being rescued by the gods or any passersby. This very situation was a recurring motif in Greek mythology.\nTo notify the neighbors of a birth of a child, a woolen strip was hung over the front door to indicate a female baby and an olive branch to indicate a boy had been born. Families did not always keep their new child. After a woman had a baby, she would show it to her husband. If the husband accepted it, it would live, but if he refused it, it would die. Babies would often be rejected if they were illegitimate, unhealthy or deformed, the wrong sex, or too great a burden on the family. These babies would not be directly killed, but put in a clay pot or jar and deserted outside the front door or on the roadway. In ancient Greek religion, this practice took the responsibility away from the parents because the child would die of natural causes, for example hunger, asphyxiation or exposure to the elements.\n\nThe practice was prevalent in ancient Rome, as well. Philo was the first philosopher to speak out against it. A letter from a Roman citizen to his sister, or a pregnant wife from her husband, dating from 1 BC, demonstrates the casual nature with which infanticide was often viewed:\n\nIn some periods of Roman history it was traditional for a newborn to be brought to the \"pater familias\", the family patriarch, who would then decide whether the child was to be kept and raised, or left to die by exposure. The Twelve Tables of Roman law obliged him to put to death a child that was visibly deformed. The concurrent practices of slavery and infanticide contributed to the \"background noise\" of the crises during the Republic.\n\nInfanticide became a capital offense in Roman law in 374 AD, but offenders were rarely if ever prosecuted.\n\nAccording to mythology, Romulus and Remus, twin infant sons of the war god Mars, survived near-infanticide after being tossed into the Tiber River. According to the myth, they were raised by wolves, and later founded the city of Rome.\n\nJudaism prohibits infanticide, and has for some time, dating back to at least early Common Era. Roman historians wrote about the ideas and customs of other peoples, which often diverged from their own. Tacitus recorded that the Jews \"regard it as a crime to kill any late-born children\". Josephus, whose works give an important insight into 1st-century Judaism, wrote that God \"forbids women to cause abortion of what is begotten, or to destroy it afterward\".\n\nIn his book \"Germania\", Tacitus wrote that the ancient Germanic tribes enforced a similar prohibition. He found such mores remarkable and commented: \"[The Germani] hold it shameful to kill any unwanted child.\" Modern scholarship differs. John Boswell believed that in ancient Germanic tribes unwanted children were exposed, usually in the forest. \"It was the custom of the [Teutonic] pagans, that if they wanted to kill a son or daughter, they would be killed before they had been given any food.\" Usually children born out of wedlock were disposed that way.\n\nIn his highly influential \"Pre-historic Times\", John Lubbock described burnt bones indicating the practice of child sacrifice in pagan Britain.\n\nThe last canto, \"Marjatan poika\" (Son of Marjatta), of Finnish national epic Kalevala describes an assumed infanticide. Väinämöinen orders the infant bastard son of Marjatta to be drowned in marsh.\n\nThe Íslendingabók, a main source for the early history of Iceland, recounts that on the Conversion of Iceland to Christianity in 1000 it was provided – in order to make the transition more palatable to Pagans – that \"the old laws allowing exposure of newborn children will remain in force\".\nHowever, this provision – like other concessions made at the time to the Pagans – was abolished some years later.\n\nChristianity rejects infanticide. The \"Teachings of the Apostles\" or \"Didache\" said \"thou shalt not kill a child by abortion, neither shalt thou slay it when born\". The \"Epistle of Barnabas\" stated an identical command, both thus conflating abortion and infanticide. Apologists Tertullian, Athenagoras, Minucius Felix, Justin Martyr and Lactantius also maintained that exposing a baby to death was a wicked act. In 318 AD, Constantine I considered infanticide a crime, and in 374 AD, Valentinian I mandated the rearing of all children (exposing babies, especially girls, was still common). The Council of Constantinople declared that infanticide was homicide, and in 589 AD, the Third Council of Toledo took measures against the custom of killing their own children.\n\nWhereas theologians and clerics preached sparing their lives, newborn abandonment continued as registered in both the literature record and in legal documents. According to William L. Langer, exposure in the Middle Ages \"was practiced on gigantic scale with absolute impunity, noticed by writers with most frigid indifference\". At the end of the 12th century, notes Richard Trexler, Roman women threw their newborns into the Tiber river in daylight.\n\nHowever, it also conjectured that the notion of \"rampant\" infanticide is a myth pushed by modern historians inferring from lack of particular records, and \"there is absolutely no evidence to support such carnage.\"\n\nUnlike other European regions, in the Middle Ages the German mother had the right to expose the newborn. In Gotland, Sweden, children were also sacrificed.\n\nIn the High Middle Ages, abandoning unwanted children finally eclipsed infanticide. Unwanted children were left at the door of church or abbey, and the clergy was assumed to take care of their upbringing. This practice also gave rise to the first orphanages.\n\nHowever, very high sex ratios were common in even late medieval Europe, which may indicate sex-selective infanticide.\n\nSome Muslim sources allege that pre-Islamic Arabian society practiced infanticide as a form of \"post-partum birth control\". The word \"waʾd\" was used to describe the practice. These sources state that infanticide was practiced either out of destitution (thus practiced on males and females alike), or as \"disappointment and fear of social disgrace felt by a father upon the birth of a daughter\".\n\nSome authors believe that there is little evidence that infanticide was prevalent in pre-Islamic Arabia or early Muslim history, except for the case of the Tamim tribe, who practiced it during severe famine according to Islamic sources. Others state that \"female infanticide was common all over Arabia during this period of time\" (pre-Islamic Arabia), especially by burying alive a female newborn. A tablet discovered in Yemen, forbidding the people of a certain town from engaging in the practice, is the only written reference to infanticide within the peninsula in pre-Islamic times.\n\nInfanticide is explicitly prohibited by the Qur'an. \"And do not kill your children for fear of poverty; We give them sustenance and yourselves too; surely to kill them is a great wrong.\"\nTogether with polytheism and homicide, infanticide is regarded as a grave sin (see and ). Infanticide is also implicitly denounced in the story of Pharaoh's slaughter of the male children of Israelites (see ; ; ; ; ;).\n\nInfanticide may have been practiced as human sacrifice, as part of the pagan cult of Perun. Ibn Fadlan describes sacrificial practices at the time of his trip to Kiev Rus (present day Ukraine) in 921–922, and describes an incident of a woman voluntarily sacrificing her life as part of a funeral rite for a prominent leader, but makes no mention of infanticide. The Primary Chronicle, one of the most important literary sources before the 12th century, indicates that human sacrifice to idols may have been introduced by Vladimir the Great in 980. The same Vladimir the Great formally converted Kiev Rus into Christianity just 8 years later, but pagan cults continued to be practiced clandestinely in remote areas as late as the 13th century.\n\nIn Kamchatka, babies were killed and thrown to the dogs. American explorer George Kennan noted that among the Koryaks, a Mongoloid people of north-eastern Siberia, infanticide was still common in the nineteenth century. One of a pair of twins was always sacrificed.\n\nThe Svans killed newborn females by filling their mouths with hot ashes.\n\nInfanticide (as a crime) gained both popular and bureaucratic significance in Victorian Britain. By the mid 19th century, in the context of criminal lunacy and the insanity defence, killing one's own child(ren) attracted ferocious debate, as the role of women in society was defined by motherhood, and it was thought that any woman who murdered her own child was by definition insane and could not be held responsible for her actions. Several cases were subsequently highlighted during the Royal Commission on Capital Punishment (1864-66), as a particular felony where an effective avoidance of the death penalty had informally begun.\n\nThe New Poor Law Act of 1834 ended parish relief for unmarried mothers and allowed fathers of illegitimate children to avoid paying for \"child support\". Unmarried mothers then received little assistance and the poor were left with the option either entering the workhouse, prostitution, infanticide or abortion. By the middle of the century infanticide was common for social reasons, such as illegitimacy, and the introduction of child life insurance additionally encouraged some women to kill their children for gain. Examples are Mary Ann Cotton, who murdered many of her 15 children as well as 3 husbands, Margaret Waters, the 'Brixton Baby Farmer', a professional baby-farmer who was found guilty of infanticide in 1870, Jessie King hanged in 1889, Amelia Dyer, the 'Angel Maker', who murdered over 400 babies in her care, and Ada Chard-Williams, a baby farmer who was later hanged at Newgate prison.\n\nThe Times reported that 67 infants were murdered in London in 1861 and 150 more recorded as \"found dead\", many of which were found on the streets. Another 250 were suffocated, half of them not recorded as accidental deaths. The report noted that \"infancy in London has to creep into life in the midst of foes.\"\n\nRecording a birth as a still-birth was also another way of concealing infanticide because still-births did not need to be registered until 1926 and they did not need to be buried in public cemeteries. In 1895 the Sun (London) published an article \"Massacre of the Innocents\" highlighting the dangers of baby-farming, in the recording of stillbirths and quoting Braxton-Hicks, the London Coroner, on lying-in houses: \"I have not the slightest doubt that a large amount of crime is covered by the expression `still-birth’. There are a large number of cases of what are called newly-born children, which are found all over England, more especially in London and large towns, abandoned in streets, rivers, on commons, and so on.\" He continued \"a great deal of that crime is due to what are called lying-in houses, which are not registered, or under the supervision of that sort, where the people who act as midwives constantly, as soon as the child is born, either drop it into a pail of water or smother it with a damp cloth. It is a very common thing, also, to find that they bash their heads on the floor and break their skulls.\"\n\nThe last British woman to be executed for infanticide of her own child was Rebecca Smith, who was hanged in Wiltshire in 1849.\n\nThe Infant Life Protection Act of 1897 required local authorities to be notified within 48 hours of changes in custody or the death of children under seven years. Under the Children’s Act of 1908 \"no infant could be kept in a home that was so unfit and so overcrowded as to endanger its health, and no infant could be kept by an unfit nurse who threatened, by neglect or abuse, its proper care and maintenance.\"\n\nShort of execution, the harshest penalties were imposed on practitioners of infanticide by the legal codes of the Qin dynasty and Han dynasty of ancient China.\n\nMarco Polo, the explorer, saw newborns exposed in Manzi. China's society practiced sex selective infanticide. Philosopher Han Fei Tzu, a member of the ruling aristocracy of the 3rd century BC, who developed a school of law, wrote: \"As to children, a father and mother when they produce a boy congratulate one another, but when they produce a girl they put it to death.\" Among the Hakka people, and in Yunnan, Anhui, Sichuan, Jiangxi and Fujian a method of killing the baby was to put her into a bucket of cold water, which was called \"baby water\".\n\nInfanticide was known in China as early as the 3rd century BC, and, by the time of the Song dynasty (960–1279 AD), it was widespread in some provinces. Buddhist belief in transmigration allowed poor residents of the country to kill their newborn children if they felt unable to care for them, hoping that they would be reborn in better circumstances. Furthermore, some Chinese did not consider newborn children fully \"human\", and saw \"life\" beginning at some point after the sixth month after birth.\n\nContemporary writers from the Song dynasty note that, in Hubei and Fujian provinces, residents would only keep three sons and two daughters (among poor farmers, two sons and one daughter), and kill all babies beyond that number at birth. Initially the sex of the child was only one factor to consider. By the time of the Ming Dynasty, however (1368–1644), male infanticide was becoming increasingly uncommon. The prevalence of female infanticide remained high much longer. The magnitude of this practice is subject to some dispute; however, one commonly quoted estimate is that, by late Qing, between one fifth and one quarter of all newborn girls, across the entire social spectrum, were victims of infanticide. If one includes excess mortality among female children under 10 (ascribed to gender-differential neglect), the share of victims rises to one third.\n\nScottish Physician John Dudgeon, who worked in Beijing, China, during the Qing Dynasty said that in China, \"Infanticide does not prevail to the extent so generally believed among us, and in the north it does not exist at all.\"\nGender-selected abortion, abandonment, and infanticide are illegal in present-day China. Nevertheless, the US State Department, and the human rights organization Amnesty International have all declared that China's family planning programs, called the one child policy, contribute to infanticide. The sex gap between males and females aged 0–19 years old was estimated to be 25 million in 2010 by the United Nations Population Fund.\n\nSince feudal Japan the common slang for infanticide was \"mabiki\" (間引き) which means to pull plants from an overcrowded garden. A typical method in Japan was smothering through wet paper on the baby's mouth and nose. It became common as a method of population control. Farmers would often kill their second or third sons. Daughters were usually spared, as they could be married off, sold off as servants or prostitutes, or sent off to become geishas. Mabiki persisted in the 19th century and early 20th century. To bear twins was perceived as barbarous and unlucky and efforts were made to hide or kill one or both twins.\n\nFemale infanticide of newborn girls was systematic in feudatory Rajputs in South Asia for illegitimate female children during the Middle Ages. According to Firishta, as soon as the illegitimate female child was born she was held \"in one hand, and a knife in the other, that any person who wanted a wife might take her now, otherwise she was immediately put to death\". The practice of female infanticide was also common among the Kutch, Kehtri, Nagar, Bengal, Miazed, Kalowries in India inhabitants, and also among the Sindh in British India.\n\nIt was not uncommon that parents threw a child to the sharks in the Ganges River as a sacrificial offering. The British colonists were unable to outlaw the custom until the beginnings of the 19th century.\n\nAccording to social activists, female infanticide has remained a problem in India into the 21st century, with both NGOs and the government conducting awareness campaigns to combat it.\nIn India female infanticide is more common than the killing of male offspring, due to sex-selective infanticide. In China for example, the sex gap between males and females aged 0–19 years old was estimated to be 25 million in 2010 by the United Nations Population Fund.\n\nIn some African societies some neonates were killed because of beliefs in evil omens or because they were considered unlucky. Twins were usually put to death in Arebo; as well as by the Nama people of South West Africa; in the Lake Victoria Nyanza region; by the Tswana in Portuguese East Africa; in some parts of Igboland, Nigeria twins were sometimes abandoned in a forest at birth (as depicted in \"Things Fall Apart\"), oftentimes one twin was killed or hidden by midwives of wealthier mothers; and by the !Kung people of the Kalahari Desert. The Kikuyu, Kenya's most populous ethnic group, practiced ritual killing of twins.\n\nInfanticide is rooted in the old traditions and beliefs prevailing all over the country. A survey conducted by Disability Rights International found that 45% women interviewed by them in Kenya were pressured to kill their children born with disabilities. The pressure being much higher in the rural areas, with every second mother being forced out of three.\n\nLiterature suggests infanticide may have occurred reasonably commonly among Indigenous Australians, in all areas of Australia prior to European settlement. Infanticide may have continued to occur quite often up until the 1960s. An 1866 issue of \"The Australian News for Home Readers\" informed readers that \"the crime of infanticide is so prevalent amongst the natives that it is rare to see an infant\".\n\nAuthor Susanna de Vries in 2007 told a newspaper that her accounts of Aboriginal violence, including infanticide, were censored by publishers in the 1980s and 1990s. She told reporters that the censorship \"stemmed from guilt over the stolen children question\". Keith Windschuttle weighed in on the conversation, saying this type of censorship started in the 1970s. In the same article Louis Nowra suggested that infanticide in customary Aboriginal law may have been because it was difficult to keep an abundant number of Aboriginal children alive; there were life-and-death decisions modern-day Australians no longer have to face.\n\nAccording to William D. Rubinstein, \"Nineteenth-century European observers of Aboriginal life in South Australia and Victoria reported that about 30% of Aboriginal infants were killed at birth.\"\n\nJames Dawson wrote a passage about infanticide among Indigenous people in the western district of Victoria, which stated that \"Twins are as common among them as among Europeans; but as food is occasionally very scarce, and a large family troublesome to move about, it is lawful and customary to destroy the weakest twin child, irrespective of sex.\nIt is usual also to destroy those which are malformed.\"\n\nHe also wrote \"When a woman has children too rapidly for the convenience and necessities of the parents, she makes up her mind to let one be killed, and consults with her husband which it is to be. As the strength of a tribe depends more on males than females, the girls are generally sacrificed.\nThe child is put to death and buried, or burned without ceremony; not, however, by its father or mother, but by relatives. No one wears mourning for it.\nSickly children are never killed on account of their bad health, and are allowed to die naturally.\"\n\nIn 1937, a reverend in the Kimberley offered a \"baby bonus\" to Aboriginal families as a deterrent against infanticide and to increase the birthrate of the local Indigenous population.\n\nA Canberran journalist in 1927 wrote of the \"cheapness of life\" to the Aboriginal people local to the Canberra area 100 years before. \"If drought or bush fires had devastated the country and curtailed food supplies, babies got short shift. Ailing babies, too would not be kept\" he wrote.\n\nA bishop wrote in 1928 that it was common for Aboriginal Australians to restrict the size of their tribal groups, including by infanticide, so that the food resources of the tribal area may be sufficient for them.\n\nAnnette Hamilton, a professor of anthropology at Macquarie University who carried out research in the Aboriginal community of Maningrida in Arnhem Land during the 1960s wrote that prior to that time part-European babies born to Aboriginal mothers had not been allowed to live, and that 'mixed-unions are frowned on by men and women alike as a matter of principle'.\n\nThere is no agreement about the actual estimates of the frequency of newborn female infanticide in the Inuit population. Carmel Schrire mentions diverse studies ranging from 15–50% to 80%.\n\nPolar Inuit (Inughuit) killed the child by throwing him or her into the sea. There is even a legend in Inuit mythology, \"The Unwanted Child\", where a mother throws her child into the fjord.\n\nThe Yukon and the Mahlemuit tribes of Alaska exposed the female newborns by first stuffing their mouths with grass before leaving them to die. In Arctic Canada the Inuit exposed their babies on the ice and left them to die.\n\nFemale Inuit infanticide disappeared in the 1930s and 1940s after contact with the Western cultures from the South.\n\nThe \"Handbook of North American Indians\" reports infanticide among the Dene Natives and those of the Mackenzie Mountains.\n\nIn the Eastern Shoshone there was a scarcity of Indian women as a result of female infanticide. For the Maidu Native Americans twins were so dangerous that they not only killed them, but the mother as well. In the region known today as southern Texas, the Mariame Indians practiced infanticide of females on a large scale. Wives had to be obtained from neighboring groups.\n\nBernal Díaz recounted that, after landing on the Veracruz coast, they came across a temple dedicated to Tezcatlipoca. \"That day they had sacrificed two boys, cutting open their chests and offering their blood and hearts to that accursed idol\". In \"The Conquest of New Spain\" Díaz describes more child sacrifices in the towns before the Spaniards reached the large Aztec city Tenochtitlan.\n\nAlthough academic data of infanticides among the indigenous people in South America is not as abundant as that of North America, the estimates seem to be similar.\n\nThe Tapirapé indigenous people of Brazil allowed no more than three children per woman, and no more than two of the same sex. If the rule was broken infanticide was practiced. The Bororo killed all the newborns that did not appear healthy enough. Infanticide is also documented in the case of the Korubo people in the Amazon.\n\nThe Yanomami men killed children while raiding enemy villages. Helena Valero, a Brazilian woman kidnapped by Yanomami warriors in the 1930s, witnessed a Karawetari raid on her tribe:\n\nWhile \"qhapaq hucha\" was practiced in the Peruvian large cities, child sacrifice in the pre-Columbian tribes of the region is less documented. However, even today studies on the Aymara Indians reveal high incidences of mortality among the newborn, especially female deaths, suggesting infanticide. The Abipones, a small tribe of Guaycuruan stock, of about 5,000 by the end of the 18th century in Paraguay, practiced systematic infanticide; with never more than two children being reared in one family. The Machigenga killed their disabled children. Infanticide among the Chaco in Paraguay was estimated as high as 50% of all newborns in that tribe, who were usually buried. The infanticidal custom had such roots among the Ayoreo in Bolivia and Paraguay that it persisted until the late 20th century.\n\nInfanticide has become less common in the Western world. The frequency has been estimated to be 1 in approximately 3000 to 5000 children of all ages and 2.1 per 100,000 newborns per year. It is thought that infanticide today continues at a much higher rate in areas of extremely high poverty and overpopulation, such as parts of China and India. Female infants, then and even now, are particularly vulnerable, a factor in sex-selective infanticide. Recent estimates suggest that over 100 million girls and women are 'missing' in Asia.\n\nIn spite of the fact that it is illegal, in Benin, West Africa, parents secretly continue with infanticidal customs.\n\nAccording to \"The Hidden Gulag\" published by the Committee for Human Rights in North Korea, the People's Republic of China returns all illegal immigrants from North Korea which usually imprisons them in a short term facility. Women who are suspected of being impregnated by Chinese fathers are subjected to forced abortions; babies born alive are killed, sometimes by exposure or being buried alive.\n\nThere have been some accusations that infanticide occurs in the People's Republic of China due to the one-child policy. In the 1990s, a certain stretch of the Yangtze River was known to be a common site of infanticide by drowning, until government projects made access to it more difficult. Recent studies suggest that over 40 million girls and women are 'missing' in China (Klasen and Wink 2003).\n\nThe practice has continued in some rural areas of India. Infanticide is illegal in India.\n\nAccording to a recent report by the United Nations Children's Fund (UNICEF) up to 50 million girls and women are missing in India's population as a result of systematic sex discrimination and sex selective abortions.\n\nKillings of newborn babies have been on the rise in Pakistan, corresponding to an increase in poverty across the country. More than 1,000 infants, mostly girls, were killed or abandoned to die in Pakistan in 2009 according to a Pakistani charity organization.\n\nThe Edhi Foundation found 1,210 dead babies in 2010. Many more are abandoned and left at the doorsteps of mosques. As a result, Edhi centers feature signs \"Do not murder, lay them here.\" Though female infanticide is punishable by life in prison, such crimes are rarely prosecuted.\n\nIn November 2008 it was reported that in Agibu and Amosa villages of Gimi region of Eastern Highlands province of Papua New Guinea where tribal fighting in the region of Gimi has been going on since 1986 (many of the clashes arising over claims of sorcery) women had agreed that if they stopped producing males, allowing only female babies to survive, their tribe's stock of boys would go down and there would be no men in the future to fight. They agreed to have all newborn male babies killed. It is not known how many male babies were killed by being smothered, but it had reportedly happened to all males over a 10-year period and probably was still happening.\n\nIn England and Wales there were typically 30 to 50 homicides per million children less than 1 year old between 1982 and 1996. The younger the infant, the higher the risk. The rate for children 1 to 5 years was around 10 per million children. The homicide rate of infants less than 1 year is significantly higher than for the general population. \n\nIn English law infanticide is established as a distinct offence by the Infanticide Acts. Defined as the killing of a child under 12 months of age by their mother, the effect of the Acts are to establish a partial defence to charges of murder.\n\nIn 1983, the United States ranked eleventh for infants under 1 year killed, and fourth for those killed from 1 through 14 years (the latter case not necessarily involving filicide). In the U.S. over six hundred children were killed by their parents in 1983.\n\nIn the United States the infanticide rate during the first hour of life outside the womb dropped from 1.41 per 100,000 during 1963 to 1972 to 0.44 per 100,000 for 1974 to 1983; the rates during the first month after birth also declined, whereas those for older infants rose during this time. The legalization of abortion, which was completed in 1973, was the most important factor in the decline in neonatal mortality during the period from 1964 to 1977, according to a study by economists associated with the National Bureau of Economic Research.\n\nIn Canada 114 cases of infanticide by a parent were reported during 1964–1968. There is ongoing debate in the Canadian legal and political fields about whether section 237 of the Criminal Code, which creates the specific offence and partial defence of infanticide in Canadian law, should be amended or abolished altogether.\n\nFrom 2013 to March 2018, 28 infanticides cases done by 22 mothers and three stepmothers were reported in Spain. The most famous case was the murder of Bernardo González Parra in 1910 perpetrated by Francisco Leona Romero, Julio Hernández Rodríguez, Francisco Ortega el Moruno and Agustina Rodríguez.\n\nIn a 2012 article in the \"Journal of Medical Ethics\", a philosopher and a bioethicist jointly proposed that infanticide be legalized, calling it \"after-birth abortion\", and claiming that both \"the fetus and the newborn are potential persons\". Many replies were published to this article.\n\nEuthanasia applied to children that are gravely ill or that suffer from significant birth defects is legal in the Netherlands under rigidly controlled conditions, but controversial. Some critics have compared child euthanasia to infanticide.\n\nThere are various reasons for infanticide. Neonaticide typically has different patterns and causes than for killing of older infants. Traditional neonaticide is often related to economic necessity - inability to provide for the infant.\n\nIn the United Kingdom and the United States, older infants are typically killed for reasons related to child abuse, domestic violence or mental illness. For infants older than one day, younger infants are more at risk, and boys are more at risk than girls. Risk factors for the parent include: Family history of violence, violence in current relationship, history of abuse or neglect of children, and personality disorder and/or depression.\n\nIn the late seventeenth and early eighteenth centuries, \"loopholes\" were invented by those who wanted to avoid the damnation that was promised by most Christian doctrine as a penalty of suicide. One famous example of someone who wished to end their life but avoid the eternity in hell was Christina Johansdotter (died 1740). She was a Swedish murderer who killed a child in Stockholm with the sole purpose of being executed. She is an example of those who seek suicide through execution by committing a murder. It was a common act, frequently targeting young children or infants as they were believed to be free from sin, thus going straight to heaven.\n\nIn 1888, Lieut. F. Elton reported that Ugi beach people in the Solomon Islands killed their infants at birth by burying them, and women were also said to practice abortion. They reported that it was too much trouble to raise a child, and instead preferred to buy one from the bush people.\n\nMany historians believe the reason to be primarily economic, with more children born than the family is prepared to support. In societies that are patrilineal and patrilocal, the family may choose to allow more sons to live and kill some daughters, as the former will support their birth family until they die, whereas the latter will leave economically and geographically to join their husband's family, possibly only after the payment of a burdensome dowry price. Thus the decision to bring up a boy is more economically rewarding to the parents. However, this does not explain why infanticide would occur equally among rich and poor, nor why it would be as frequent during decadent periods of the Roman Empire as during earlier, less affluent, periods.\n\nBefore the appearance of effective contraception, infanticide was a common occurrence in ancient brothels. Unlike usual infanticide - where historically girls have been more likely to be killed - prostitutes in certain areas preferred to kill their male offspring.\n\nInstances of infanticide in Britain in 18th and 19th centuries is often attributed to the economic position of the women, with juries committing “pious perjury” in many subsequent murder cases. The knowledge of the difficulties faced in the 18th century by those women who attempted to keep their children can be seen as reason for juries to show compassion. If the woman chose to keep the child, society was not set up to ease the pressure placed upon the woman, legally, socially or economically.\n\nIn mid-18th century Britain there was assistance available for women who were not able to raise their children. The Foundling Hospital opened in 1756 and was able to take in some of the illegitimate children. However, the conditions within the hospital caused Parliament to withdraw funding and the governors to live off of their own incomes. This resulted in a stringent entrance policy, with the committee requiring that the hospital:\n\nOnce a mother had admitted her child to the hospital, the hospital did all it could to ensure that the parent and child were not re-united.\n\nMacFarlane argues in \"Illegitimacy and Illegitimates in Britain\" (1980) that English society greatly concerned itself with the burden that a bastard child places upon its communities and had gone to some lengths to ensure that the father of the child is identified in order to maintain its well-being. Assistance could be gained through maintenance payments from the father, however, this was capped \"at a miserable 2 s and 6 d a week\". If the father fell behind with the payments he could only be asked \"to pay a maximum of 13 weeks arrears\".\n\nDespite the accusations of some that women were getting a free hand-out there is evidence that many women were far from receiving adequate assistance from their parish. \"Within Leeds in 1822 ... relief was limited to 1 s per week\". Sheffield required women to enter the workhouse, whereas Halifax gave no relief to the women who required it. The prospect of entering the workhouse was certainly something to be avoided. Lionel Rose quotes Dr Joseph Rogers in \"Massacre of the Innocents ...\" (1986). Rogers, who was employed by a London workhouse in 1856 stated that conditions in the nursery were ‘wretchedly damp and miserable ... [and] ... overcrowded with young mothers and their infants’.\n\nThe loss of social standing for a servant girl was a particular problem in respect of producing a bastard child as they relied upon a good character reference in order to maintain their job and more importantly, to get a new or better job. In a large number of trials for the crime of infanticide, it is the servant girl that stood accused. The disadvantage of being a servant girl is that they had to live to the social standards of their superiors or risk dismissal and no references. Whereas within other professions, such as in the factory, the relationship between employer and employee was much more anonymous and the mother would be better able to make other provisions, such as employing a minder. The result of the lack of basic social care in Britain in the 18th and 19th century is the numerous accounts in court records of women, particularly servant girls, standing trial for the murder of their child.\n\nThere may have been no specific offence of infanticide in England before about 1623 because infanticide was a matter for the by ecclesiastical courts, possibly because infant mortality from natural causes was high (about 15% or one in six).\n\nThereafter the accusation of the suppression of bastard children by lewd mothers was a crime incurring the presumption of guilt.\n\nThe Infanticide Acts are several laws. That of 1922 made the killing of an infant child by its mother during the early months of life as a lesser crime than murder. The acts of 1938 and 1939 abolished the earlier act, but introduced the idea that postpartum depression was legally to be regarded as a form of diminished responsibility.\n\nMarvin Harris estimated that among Paleolithic hunters 23–50% of newborn children were killed. He argued that the goal was to preserve the 0.001% population growth of that time. He also wrote that female infanticide may be a form of population control. Population control is achieved not only by limiting the number of potential mothers; increased fighting among men for access to relatively scarce wives would also lead to a decline in population. For example, on the Melanesian island of Tikopia infanticide was used to keep a stable population in line with its resource base. Research by Marvin Harris and William Divale supports this argument, it has been cited as an example of environmental determinism.\n\nEvolutionary psychology has proposed several theories for different forms of infanticide. Infanticide by stepfathers, as well as child abuse in general by stepfathers, has been explained by spending resources on not genetically related children reducing reproductive success (See the Cinderella effect and Infanticide (zoology)). Infanticide is one of the few forms of violence more often done by women than men. Cross-cultural research has found that this is more likely to occur when the child has deformities or illnesses as well as when there are lacking resources due to factors such as poverty, other children requiring resources, and no male support. Such a child may have a low chance of reproductive success in which case it would decrease the mother's inclusive fitness, in particular since women generally have a greater parental investment than men, to spend resources on the child.\n\nA minority of academics subscribe to an alternate school of thought, considering the practice as \"early infanticidal childrearing\". They attribute parental infanticidal wishes to massive projection or displacement of the parents' unconscious onto the child, because of intergenerational, ancestral abuse by their own parents. Clearly, an infanticidal parent may have multiple motivations, conflicts, emotions, and thoughts about their baby and their relationship with their baby, which are often colored both by their individual psychology, current relational context and attachment history, and, perhaps most saliently, their psychopathology (See also Psychiatric section below) Almeida, Merminod, and Schechter suggest that parents with fantasies, projections, and delusions involving infanticide need to be taken seriously and assessed carefully, whenever possible, by an interdisciplinary team that includes infant mental health specialists or mental health practitioners who have experience in working with parents, children, and families.\n\nIn addition to debates over the morality of infanticide itself, there is some debate over the effects of infanticide on surviving children, and the effects of childrearing in societies that also sanction infanticide. Some argue that the practice of infanticide in any widespread form causes enormous psychological damage in children. Conversely, studying societies that practice infanticide Géza Róheim reported that even infanticidal mothers in New Guinea, who ate a child, did not affect the personality development of the surviving children; that \"these are good mothers who eat their own children\". Harris and Divale's work on the relationship between female infanticide and warfare suggests that there are, however, extensive negative effects.\n\nPostpartum psychosis is also a causative factor of infanticide. Stuart S. Asch, MD, a Professor of Psychiatry at Cornell University established the connections between some cases of infanticide and post-partum depression. The books, \"From Cradle to Grave\", and \"The Death of Innocents\", describe selected cases of maternal infanticide and the investigative research of Professor Asch working in concert with the New York City Medical Examiner's Office.\nStanley Hopwood wrote that childbirth and lactation entail severe stress on the female sex, and that under certain circumstances attempts at infanticide and suicide are common. A study published in the \"American Journal of Psychiatry\" revealed that 44% of filicidal fathers had a diagnosis of psychosis. In addition to postpartum psychosis, dissociative psychopathology and sociopathy have also been found to be associated with neonaticide in some cases\n\nIn addition, severe postpartum depression can lead to infanticide.\n\nSex selection may be one of the contributing factors of infanticide. In the absence of sex-selective abortion, sex-selective infanticide can be deduced from very skewed birth statistics. The biologically normal sex ratio for humans at birth is approximately 105 males per 100 females; normal ratios hardly ranging beyond 102–108. When a society has an infant male to female ratio which is significantly higher or lower than the biological norm, and biased data can be ruled out, sex selection can usually be inferred.\n\nIn New South Wales, infanticide is defined in Section 22A(1) of the Crimes Act 1900 (NSW) as follows:\n\nBecause Infanticide is punishable as manslaughter, as per s24, the maximum penalty for this offence is therefore 25 years imprisonment.\n\nInfanticide is illegal in India, but rarely enforced in the rural parts of India. \n\nIn Canada, a mother commits infanticide, a lesser offence than homicide, if she killed her child while \"not fully recovered from the effects of giving birth to the child and by reason thereof or of the effect of lactation consequent on the birth of the child her mind is then disturbed\".\n\nIn England and Wales, the Infanticide Act 1938 describes the offence of infanticide as one which would otherwise amount to murder (by his/her mother) if the victim was older than 12 months and the mother was not suffering from an imbalance of mind due to the effects of childbirth or lactation. Where a mother who has killed such an infant has been charged with murder rather than infanticide s.1(3) of the Act confirms that a jury has the power to find alternative verdicts of Manslaughter in English law or guilty but insane.\n\nArticle 200 of the Penal Code of Romania stipulates that the killing of a newborn during the first 24 hours, by the mother who is in a state of mental distress, shall be punished with imprisonment of one to five years. The previous Romanian Penal Code also defined infanticide (\"pruncucidere\") as a distinct criminal offence, providing for a punishment of two to seven years imprisonment, recognizing the fact that a mother's judgment may be impaired immediately after birth, but did not define the term \"infant\", and this had led to debates regarding the precise moment when infanticide becomes homicide. This issue was resolved by the new Penal Code, which came into force in 2014.\n\nIn 2009, Texas state representative Jessica Farrar proposed legislation that would define infanticide as a distinct and lesser crime than homicide. Under the terms of the proposed legislation, if jurors concluded that a mother's \"judgment was impaired as a result of the effects of giving birth or the effects of lactation following the birth\", they would be allowed to convict her of the crime of infanticide, rather than murder. The maximum penalty for infanticide would be two years in prison. Farrar's introduction of this bill prompted liberal bioethics scholar Jacob M. Appel to call her \"the bravest politician in America\".\n\nSince infanticide, especially neonaticide, is often a response to an unwanted birth, preventing unwanted pregnancies through improved sex education and increased contraceptive access are advocated as ways of preventing infanticide. Increased use of contraceptives and access to safe legal abortions have greatly reduced neonaticide in many developed nations. Some say that where abortion is illegal, as in Pakistan, infanticide would decline if safer legal abortions were available.\n\nScreening for psychiatric disorders or risk factors, and providing treatment or assistance to those at risk may help prevent infanticide. However, in developed world significant proportions of neonaticides that are detected occur in young women who deny their pregnancy, and avoid outside contacts, so they may have limited contact with health care services.\n\nIn some areas baby hatches or \"safe surrender sites\", safe places for a mother to anonymously leave an infant, are offered, in part to reduce the rate of infanticide. In other places, like the United States, safe-haven laws allow mothers to anonymously give infants to designated officials; they are frequently located at hospitals and police and fire stations. Typically such babies are put up for adoption, or cared for in orphanages.\n\nGranting women employment raises their status and autonomy. Having a gainful employment can raise the perceived worth of females. This can lead to an increase in the number of women getting an education and a decrease in the number of female infanticide. As a result, the infant mortality rate will decrease and economic development will increase.\n\nAlthough human infanticide has been widely studied, the practice has been observed in many other species of the animal kingdom since it was first seriously studied by . These include from microscopic rotifers and insects, to fish, amphibians, birds and mammals, including primates such as chacma baboons. Infanticide can be practiced by both males and females.\n\nAccording to studies carried out by Kyoto University in non-human primates, including certain types of gorillas and chimpanzees, several conditions favor the tendency to infanticide in some species (to be performed only by males), among them are: Nocturnal live, the absence of nest construction, the marked sexual dimorphism in which the male is much larger than the female, the mating in a specific season and the high period of lactation without resumption of the estrus state in the female.\n\n"}
{"id": "9863960", "url": "https://en.wikipedia.org/wiki?curid=9863960", "title": "Information cycle", "text": "Information cycle\n\nThe term information cycle refers to the way information is processed and distributed and how it changes over time. It is usually used to describe the progression of media coverage relating to a particular newsworthy event or topic during which information goes through various stages of reporting and publication. \n\nIn the cycle model, information begins circulation with a news story presented via the Internet, television, radio, or newspaper, followed by its release in magazines. The information is then researched by scholars and published in academic journals or books and presented at academic conferences. Finally, if the information is considered important enough, it is included in reference works such as handbooks and encyclopedias.\n\nAs information passes through these stages, its content and presentation changes. Initial news coverage may take place as events unfold, and offer only the basics in terms of \"who, what, when, and where.\" News magazines will provide more background information, adding the fifth W, \"why,\" especially in less frequently appearing or specialized periodicals. As more time passes, scholars will research the information and write detailed studies that take historical context and long-term meaning into account. Finally, after a few years, books regarding the information may appear. \n\nUnderstanding the information cycle helps aid researchers and academics in determining the validity of source material. For instance, the cycle model is commonly taught in library education. Information flow in this model can be thought of as a cycle because, conceptually, the published information might spark new ideas which will pass through similar stages.\n"}
{"id": "12854042", "url": "https://en.wikipedia.org/wiki?curid=12854042", "title": "Kishōtenketsu", "text": "Kishōtenketsu\n\nThe following is an example of how this might be applied to a fairytale.\n\nA specific example by the poet Sanyō Rai (頼山陽):\nThe first verse introduces the female characters of the story. The second verse gives more details about both. Verse three goes astray to an unrelated territory. Verse four explains: The main characters of the story seduce men with their eyes – killing them just as the, until now, unrelated generals who kill with bows and arrow – thus showing the relation of the daughters of Itoya and the killing generals.\n\nThe same pattern is used to arrange arguments:\n\nIn the structure of narrative and yonkoma manga, and even for document and dissertation, the style in \"kishōtenketsu\" applies to sentence or sentences, and even clause to chapter as well as the phrase for understandable introduction to conclusion.\n\nThe concept has also been used in game design, particularly in Nintendo's video games, most notably \"Super Mario\" games such as \"Super Mario Galaxy\" (2007) and \"Super Mario 3D World\" (2013); their designers Shigeru Miyamoto and Koichi Hayashida are known to utilize this concept for their game designs.\n\n"}
{"id": "2506007", "url": "https://en.wikipedia.org/wiki?curid=2506007", "title": "Kulig", "text": "Kulig\n\nKulig (sleigh rides) is an old Polish winter tradition dating back to the days of the szlachta (nobility).\n\nThe kulig was a sleigh ride party organized among the Polish aristocracy. A cavalcade of horse-pulled sleighs and sleds went from one manor house to another, entertained everywhere with hearty meals followed by dancing.\n\nNowadays Kulig rides are a popular tourist attraction in the Polish Tatra mountains, advertised and practiced in places such as Zakopane. However, this practice is often borderline illegal and dangerous for passengers and horses alike. Since they provide significant profits Kulig rides are organized no matter what the weather conditions and even out of season. Horses are forced to pull overloaded sleighs - it is not uncommon that the number of passengers exceeds the capacity of the sleigh itself. What is worse - sometimes sleighs are being pulled not on snow, but on mud, stone and even asphalt. There have been numerous reports of Kulig rides causing horses to die out of exhaustion or accidents due to bad terrain. It is not uncommon for the animals to be neglected and forced to work even when wounded or sick. People who organize Kulig rides for tourists would often take payment in cash and issue no bill or invoice evading taxation. \n"}
{"id": "17006149", "url": "https://en.wikipedia.org/wiki?curid=17006149", "title": "Last offices", "text": "Last offices\n\nThe last offices, or laying out, is the procedures performed, usually by a nurse, to the body of a dead person shortly after death has been confirmed. They can vary between hospitals and between cultures.\n\nThe word \"offices\" is related to the original Latin, in which \"officium\" means \"service, duty, business\". Hence these are the \"last duties\" carried out on the body.\n\nOften the body of the deceased is left for up to an hour as a mark of respect. The procedure then typically includes the following steps, though they can vary according to an institution's preferred practices:\n\nWashing the body of a dead person, sometimes as part of a religious ritual, is a customary funerary practice in several cultures. It was delegated to professionals in ancient Egypt, ancient Rome, by well-off Victorians, and continues so in modern America, but was traditionally performed by \"family, friends, and neighbors.\"\n\nIt is part of traditional Jewish burial rites.\n\nDuring the Inquisition in Spain, bodies undergoing preparation for burial were sometimes scrutinized for signs that they had been washed, since this was seen as a marker of secret Jewish practice (crypto-Judaism).\n\nBathing of the dead, known as \"yukan\", is also found in Buddhism. It is also found in Hinduism.\n\nIt is a religious practice in Islam, where the body is washed by members of the dead person's family. When possible, three washings are performed: first with water infused with plum leaves, then with water infused with camphor, and lastly with purified water.\n\nThe washing is usually performed by others of the same gender, although Islamic Hausa people permit spouses to wash each other's bodies.\n\nFunerary bathing is performed in traditional funerals in some countries in West Africa. The ritual washing of the dead is believed to be one of the factors which resulted in the rapid spread of Ebola virus in Guinea, Liberia and Sierra Leone in 2014.\n\nAntigone speaks of washing the dead in accordance with the Greek custom, although she was limited to pouring water on the body of her brother Polyneikes.\n\nThe custom of bathing the dead has been depicted in a number of films. In the 1995 film \"Braveheart\", a young William Wallace watches as women bathe the bodies of his father and brother, who were killed in battle against English troops during the 13th century. The 2009 film \"The White Ribbon\" depicts the washing of a deceased housewife in a Northern German village just before World War I. In the film \"A Midnight Clear\" (1992), set in the Battle of the Ardennes in World War II, a small group of soldiers are able to take a brief respite from the war when they procure a bath tub and heat up some water. After all have bathed, they wash the body of a comrade who was recently killed while trying to help a unit of German soldiers.\n\nAn episode of the HBO series \"Six Feet Under\" shows Nate Fisher's body being \"slowly and methodically\" washed by his mother and brother.\n\n"}
{"id": "24326326", "url": "https://en.wikipedia.org/wiki?curid=24326326", "title": "List of vampire traits in folklore and fiction", "text": "List of vampire traits in folklore and fiction\n\nThe following tables compare traits given to vampires in folklore and fiction. Over time, some attributes now regarded as integral became incorporated into the vampire's profile: fangs and vulnerability to sunlight appeared over the course of the 19th century, with Varney the Vampire and Count Dracula both bearing protruding teeth, and Murnau's \"Nosferatu\" (1922) the first vampire to be killed by daylight.\n\nAlthough Bram Stoker's novel is the best known vampire fiction of the 19th century, it is the aristocratic figure of Lord Ruthven who is thought to have inspired the elegant and suave creature of stage and film.\n\nThe cloak appeared in stage productions of the 1920s, with a high collar introduced by playwright Hamilton Deane to help Dracula 'vanish' on stage. Lord Ruthven and Varney were able to be healed by moonlight, although no account of this is known in traditional folklore.\n\nA \"Yes\" indicates a weakness to something, with fatal weaknesses being marked as such. Entries which are marked as \"No\" are not seen as weaknesses. \"?\" indicates a lack of information on whether this is a weakness or not; other indicates weaknesses that do not fit in one of the other categories. \n\n\n\n"}
{"id": "51202733", "url": "https://en.wikipedia.org/wiki?curid=51202733", "title": "Lurish clothing", "text": "Lurish clothing\n\nThe clothing culture of Lurs is a very interesting aspect of this ethnic group which has been developed along with their long history of coexistence with surrounding natural elements across the Iranian plateau, and geographical, cultural and religious effects. This clothing system is a symbol of Lurish people although it varies due to geographical and seasonal conditions and between Lurish subgroups. The colors of Lurish costumes differ by the Lurs location. In a general classification Lurish clothes are in three main subgroups: Feyli (common between Feyli Lurs including Laki, Minjai, Kalhori and Maleki tribes), Bakhtiari (Common between Bakhtiari Charlang and Haftlang clans) and Southern Lurish (common between Southern Lurs). \n\nShaal (شال): A long white cloth (width: 60–90 cm, length: 6–9 meters) made by chalvar nature which is twisted all round the waist.\n\nSetra (سِتره): A special cassock that its size is below the knees and usually is used for more official ceremonies. This clothes is one of the oldest types of clothing in Iran. \n\nFelt hats (کُلأ نِمِدی): A round felt made that has no edges and sometimes is surrounded by Golvani.\n\nGiva(گیوه): A local handmade shoes with sturdy leather or plastic soles and the vamp is woven by spun yarn.\n\nChugha (چوغا): A masculine wrapper that is used prominently by Bakhtiari Lurs and Feyli Lurs. Chugha is made by sheep wool and usually is woven by Bakhtiari nomads.\n\nJuma (جومه) or Kraas(کراس): A free, tall and without a collar dress which is a common clothing of Lurish women all across Luristan. \n\nKolonja (کُلُنجه): is a lacing overcoat which in many cases, the surface is embroidered and decorated by coins.\n\nSaava (ساوه): A special silk fabric that Lur women tie around the head.\n\nGolvani (گُلوَنی): A colorful and patterned type of Saava that mostly is used for celebration and joy. In recent years, Golvani is mentioned as an ethnic symbol of Lurs and every year is celebrated on a certain day (May 16) during spring season.\n\n"}
{"id": "57483945", "url": "https://en.wikipedia.org/wiki?curid=57483945", "title": "Mainzed", "text": "Mainzed\n\nmainzed ([maɪ̯nt͡sed]; acronym for Mainz Centre for Digitality in the Humanities and Cultural Studies) is a joint initiative of six scientific institutions to promote digital methodology in the humanities and cultural sciences in Mainz, Germany.\nIt was founded in the context of the academic annual celebration of the Academy of Sciences and Literature Mainz on 6 November 2015.\nPartners of mainzed are the Academy of Sciences and Literature Mainz (ADW), the Mainz University of Applied Sciences (HS Mainz), the Institute for Historical Regional Studies at the University of Mainz (IGL), the Johannes Gutenberg University Mainz (JGU), the Leibniz Institute of European History Mainz (IEG) and the Romano-Germanic Central Museum Mainz – Archaeological research institute (RGZM).\n\nmainzed is based on different long-term cooperations between various institutions in Mainz. The research and development institution for digital humanities of the Academy of Sciences and Literature Mainz, \"Digitale Akademie\", was founded in 2009 and is connected to the Institute of Historical Regional Studies at the University of Mainz, the Leibniz Institute of European History and the universities of Mainz. Since 1997, the Romano-Germanic Central Museum Mainz and the i3mainz – Institute for Spatial Information and Surveying Technology have operated the \"Competence Centre for Spatial Information Technology in the Humanities\" at the Mainz University of Applied Sciences. In autumn 2013, the informal \"Network DHMainz\" was created with the help of the Mainz Research Alliance. The network was responsible for the preparation of the Digital Humanities Day 2014 in Mainz where first drafts were made for the continuation of the initiative.\n\nmainzed was founded in order to accompany and practically implement the transformation of the humanities and cultural studies in the course of digitisation in Mainz.\nmainzed works in research, the support of research, qualification and transfer. Furthermore, it constitutes a social research infrastructure by offering a network of scientific exchange with regard to the development of projects and research foci for scientists of all qualification levels.\n\nRange of competences represented in the network:\n\n\nmainzed developed the inter-university master’s degree program \"Digital Methods in the Humanities and Cultural Studies\" in terms of organization and concept. Since 2016, each winter term 24 students have been able to begin the course of studies comprising four semesters provided that they have a bachelor’s degree in the humanities, cultural studies or with a focus in computer science.\nThe head of this degree program and director of mainzed Kai-Christian Bruhn received the academy price of the federal state Rhineland-Palatinate on 5 December 2017 in recognition of his interdisciplinary work in teaching and research.\n\nmainzed is initiator of many events promoting the dialogue with the public. An example of this is the fishbowl discussion about the topic \"digitalität und diversität – die Geisteswissenschaften im Jahr 2026\" that took place in 2016. Mainzed has organised similar annual events with national and international guest lecturers like Mercedes Bunz and Joscha Bach.\n\nmainzed is organised into an executive board composed of the founding director Kai-Christian Bruhn as well as his deputy Klaus Pietschmann, a scientific advisory board with representatives of the partner institutions and an executive office managed by Anne Klammt.\n\n"}
{"id": "39052181", "url": "https://en.wikipedia.org/wiki?curid=39052181", "title": "Make me a sandwich", "text": "Make me a sandwich\n\nMake me a sandwich is a catchphrase used in popular culture, often on the internet, to mock or discredit women in a satirical manner. It is a stereotype based on women belonging in the kitchen. \n\nFor example, during Hillary Clinton's 2008 campaign for the Democratic nomination for United States President, it was noted in the news that a Facebook group had been created titled \"Hillary Clinton: Stop Running for President and Make Me a Sandwich.\"\n\nThe phrase, as used in an intended humorous context, dates to at least 1995. \n"}
{"id": "95601", "url": "https://en.wikipedia.org/wiki?curid=95601", "title": "Mana", "text": "Mana\n\nMana, in Austronesian languages, means \"power\", \"effectiveness\", and \"prestige\". In most cases, this power and its source are understood to be supernatural and inexplicable. Its semantics are language-dependent. The concept is significant in Polynesian culture and is part of contemporary Pacific Islander culture; it came to the attention of Western anthropologists through reports from island missionaries. Its study was included in cultural anthropology—specifically, the anthropology of religion. Links were seen between \"mana\" and earlier phases of Western religion: animism at first, followed by pre-animism.\n\nAccording to the POLLEX Project, a protoform (an ancestral form of a word) for \"mana\"—noted in historical-linguistic convention as *mana-\"—existed in Proto-Oceanic, the precursor of many Pacific languages. Although the path through the tree from Proto-Oceanic to a specific language is not always clear, the word and concept are thousands of years old. According to linguist Robert Blust, \"mana\" means \"thunder, storm, or wind\" in some languages. Blust hypothesized that the term originally meant \"powerful forces of nature such as thunder and storm winds that were conceived as the expression of an unseen supernatural agency. As Oceanic-speaking peoples spread eastward, the notion of an unseen supernatural agency became detached from the physical forces of nature that had inspired it and assumed a life of its own.\"\n\nMana is a foundation of the Polynesian worldview, a spiritual quality with a supernatural origin and a sacred, impersonal force. To have \"mana\" implies influence, authority, and efficacy—the ability to perform in a given situation. The quality of \"mana\" is not limited to individuals; peoples, governments, places and inanimate objects may also possess \"mana\", and its possessors are accorded respect.\n\nIn Hawaiian and Tahitian culture, mana is a spiritual energy and healing power which can exist in places, objects and persons. Hawaiians believe that mana may be gained or lost by actions, and Hawaiians and Tahitians believe that mana is both external and internal. Sites on the Hawaiian Islands and in French Polynesia are believed to possess mana—for example, the top rim of the Haleakalā volcano on the island of Maui and the Taputapuatea marae on the island of Raiatea in the Society Islands.\n\nAncient Hawaiian believed that the island of Molokaʻi possesses mana, compared with its neighboring islands. Before the unification of Hawaii by King Kamehameha I, battles were fought for possession of the island and its south-shore fish ponds which existed until the late 19th century.\n\nA person may gain mana by \"pono\" (right actions). In ancient Hawaii, there were two paths to mana: sexual means or violence. Nature is dualistic, and everything has a counterpart. A balance between the gods Kū and Lono formed, through whom are the two paths to mana (\"ʻimihaku\", or the search for mana). Kū, the god of war and politics, offers mana through violence; this was how Kamehameha gained his mana. Lono, the god of peace and fertility, offers mana through sexuality.\n\nIn Māori, a tribe with \"mana whenua\" must have demonstrated their authority over a territory. In Māori culture, there are two essential aspects of a person's mana: \"mana tangata\", authority derived from whakapapa (genealogy) and \"mana huaanga\", defined as \"authority derived from having a wealth of resources to gift to others to bind them into reciprocal obligations\". Hemopereki Simon, from Ngati Tuwharetoa, asserts that there are many forms of mana in Maori culture. The indigenous word reflects a non-Western view of reality, complicating translation. This is confirmed by the definition of mana provided by Maori Mardsden who states that mana is:Spiritual power and authority as opposed to the purely psychic and natural force — ihi.According to Prof. Margaret Mutu mana in its traditional sense means:Power, authority, ownership, status, influence, dignity, respect derived from the god[/atua].In terms of leadership Ngāti Kahungunu legal scholar Carwyn Jones comments that, \"mana is the central concept that underlies Māori leadership and accountability.\" He also considers mana as a fundamental aspect of the constitutional traditions of Māori society.\n\nAccording to the New Zealand Ministry of Justice:\n\nIn contemporary New Zealand English, the word \"mana\", taken from the Māori, refers to a person or organisation of people of great personal prestige and character. The increased use of the term mana in New Zealand society is as a result of the politicisation of Maori issues stemming from the Māori Renaissance. \n\nLarry Niven adopted the term in his 1969 short story, \"Not Long Before the End\". In this and subsequent stories, \"mana\" was the magical fuel used to cast spells. It was a non-renewable environmental resource; heavy use of magic could deplete the mana in an area.\n\nFollowing on from the idea of mana as something that could be \"used up\", fantasy role-playing games and video games adopted the term to quantify \"magical power.\" Magic points (MPs) were used by \"\" and games influenced by it, and for example the 1987 video game \"Dungeon Master\" replaced \"magic points\" with \"mana points\" as the definition of MPs.\n\nMana was used in a number of early role-playing board games. In 1993, the collectible card game \"\" used mana terminology. Blizzard Entertainment's \"\" called its MPs \"mana\", influenced by \"Magic: The Gathering\". \"Diablo\" (1996) also used mana terminology; the two games' sequels and spin-offs, including \"World of Warcraft\", popularized the term \"mana\".\n\nMissionary Robert Henry Codrington traveled widely in Melanesia, publishing several studies of its language and culture. His 1891 book \"The Melanesians: Studies in their Anthropology and Folk-Lore\" contains the first detailed description of mana. Codrington defines it as \"a force altogether distinct from physical power, which acts in all kinds of ways for good and evil, and which it is of the greatest advantage to possess or control\".\n\nHis era had already defined animism, the concept that the energy (or life) in an object derives from a spiritual component. Georg Ernst Stahl's 18th-century animism was adopted by Edward Burnett Tylor, the founder of cultural anthropology, who presented his initial ideas about the history of religion in his 1865 \"Researches into the Early History of Mankind\" and developed them in volumes one (1871) and two (1874) of \"Primitive Culture\".\n\nIn Tylor's cultural anthropology, other primates (from which evolutionists hypothesized man had evolved) did not appear to possess culture.\n\nTylor did not try to find evidence of a non-cultural human state because he considered it unreachable, \"a condition not far removed from that of the lower animals\" and \"savage life as in some sort representing an early known state.\" He described such a hypothetical state as \"the human savage naked in both mind and body, and destitute of laws, or arts, or ideas, and almost of language\". According to Tylor, speculation about an acultural state is impossible. Using the method of comparative culture, similar to comparative anatomy and the comparative method of historical linguistics and following John Lubbock, he drew up a dual classification of cultural traits (memes and memeplexes): savage and civilised. Tylor wrote, \"From an ideal point of view, civilization may be looked upon as the general improvement of mankind by higher organization of the individual and of society ... \" and identified his model with the \"progression-theory of civilization\".\n\nTylor cited a \"minimum definition\" of religion as \"the belief in Spiritual Beings\". Noting that no savage societies lack religion and that the initial state of a religious man is beyond reach, he enumerated two stages in the evolution of religion: a simple belief in individual animae (or Doctrine of Souls) and the elaboration of dogmas. The dogmas are systems of higher spirits commanding phases of nature. In volume two of \"Primitive Culture\", Tylor called this stage the Doctrine of Spirits. He used the word \"animism\" in two different senses. The first is religion itself: a belief in the spiritual as an effective energy, shared by every specific religion. In his progression theory, an undogmatic version preceded rational theological systems. Animism is the simple Theory of the Soul, which comparative religion attempts to reconstruct.\n\nTylor's work predated Codrington's, and he was unfamiliar with the latter. The concept of mana occasioned a revision of Tylor's view of the evolution of religion. The first anthropologist to formulate a revision (which he called \"pre-animistic religion\") was Robert Ranulph Marett, in a series of papers collected and published as \"Threshold of Religion\". In its preface he takes credit for the adjective \"pre-animistic\" but not the noun \"pre-animism\", although he does not attribute it.\n\nAccording to Marett, \"Animism will not suffice as a minimum definition of religion.\" Tylor had used the term \"natural religion\", consistent with Georg Ernst Stahl's concept of a natural spiritual energy. The soul of an animal, for example, is its vital principle. Marett wrote, \"One must dig deeper\" to find the \"roots of religion\".\n\nDescribing pre-animism, Marett cited the Melanesian mana (primarily with Codrington's work): \"When the science of Comparative Religion employs a native expression such as mana ... it is obliged to disregard to some extent its original or local meaning ... Science, then, may adopt mana as a general category ... \". In Melanesia the \"animae\" are the souls of living men, the ghosts of deceased men, and spirits \"of ghost-like appearance\" or imitating living people. Spirits can inhabit other objects, such as animals or stones.\n\nThe most significant property of mana is that it is distinct from, and exists independently of, its source. \"Animae\" act only through mana. It is impersonal, undistinguished, and (like energy) transmissible between objects, which can have more or less of it. Mana is perceptible, appearing as a \"Power of awfulness\" (in the sense of awe or wonder). Objects possessing it impress an observer with \"respect, veneration, propitiation, service\" emanating from the mana's power. Marett lists a number of objects habitually possessing mana: \"startling manifestations of nature\", \"curious stones\", animals, \"human remains\", blood, thunderstorms, eclipses, eruptions, glaciers, and the sound of a bullroarer.\n\nIf mana is a distinct power, it may be treated distinctly. Marett distinguishes spells, which treat mana quasi-objectively, and prayers (which address the \"anima\"). An \"anima\" may have departed, leaving mana in the form of a spell which can be addressed by magic. Although Marett postulates an earlier pre-animistic phase, a \"rudimentary religion\" or \"magico-religious\" phase in which the mana figures without \"animae\", \"no island of pure 'pre-animism' is to be found.\" Like Tylor, he theorizes a thread of commonality between animism and pre-animism identified with the supernatural—the \"mysterious\", as opposed to the reasonable.\n\nIn 1936, Ian Hogbin criticised the universality of Marett's pre-animism: \"Mana is by no means universal and, consequently, to adopt it as a basis on which to build up a general theory of primitive religion is not only erroneous but indeed fallacious\". However, Marett intended the concept as an abstraction. Spells, for example, may be found \"from Central Australia to Scotland.\"\n\nEarly 20th-century scholars also saw mana as a universal concept, found in all human cultures and expressing fundamental human awareness of a sacred life energy. In his 1904 essay, \"Outline of a General Theory of Magic\", Marcel Mauss drew on the writings of Codrington and others to paint a picture of mana as \"power \"par excellence\", the genuine effectiveness of things which corroborates their practical actions without annihilating them\". Mauss pointed out the similarity of mana to the Iroquois orenda and the Algonquian manitou, convinced of the \"universality of the institution\"; \"a concept, encompassing the idea of magical power, was once found everywhere\".\n\nMauss and his collaborator, Henri Hubert, were criticised for this position when their 1904 \"Outline of a General Theory of Magic\" was published. \"No one questioned the existence of the notion of mana\", wrote Mauss's biographer Marcel Fournier, \"but Hubert and Mauss were criticized for giving it a universal dimension\". Criticism of mana as an archetype of life energy increased. According to Mircea Eliade, the idea of mana is not universal; in places where it is believed, not everyone has it, and \"even among the varying formulae (\"mana\", \"wakan\", \"orenda\", etc.) there are, if not glaring differences, certainly nuances not sufficiently observed in the early studies\". \"With regard to these theories founded upon the primordial and universal character of mana, we must say without delay that they have been invalidated by later research\".\n\n\n\n"}
{"id": "4197576", "url": "https://en.wikipedia.org/wiki?curid=4197576", "title": "Mos Teutonicus", "text": "Mos Teutonicus\n\nMos Teutonicus (Latin: \"the Germanic custom\") was a postmortem funerary custom used in Europe in the Middle Ages as a means of transporting, and solemnly disposing of, the bodies of high status individuals. The process involved the removal of the flesh from the body, so that the bones of the deceased could be transported hygienically from distant lands back home.\n\nDuring the Second Crusade for the Holy Land it was not thought fit for aristocrats who fell in battle, or died of natural causes, to be buried away from their homeland in Muslim territory. The transportation of the whole body back from foreign parts over long distances was impractical and unhygienic due to decomposition, which was often accelerated by the climate.\n\nGerman aristocrats were particularly concerned that burial should not take place in the Holy Land, but rather on home soil. The Florentine chronicler Boncompagno was the first to connect the procedure specifically with German aristocrats, and coins the phrase \"mos Teutonicus\", meaning ‘the Germanic custom.’\n\nEnglish and French aristocrats generally preferred embalming to \"mos Teutonicus\", involving the burial of the entrails and heart in a separate location from the corpse. One of the advantages of \"mos Teutonicus\" was that it was relatively economical in comparison with embalming, and was more hygienic.\n\nCorpse preservation was very popular in mediaeval society. The decaying body was seen as a representative of something sinful and evil. Embalming and \"mos Teutonicus\", along with tomb effigies, were a way of giving the corpse an illusion of stasis and removed the uneasy image of putrification and decay.\n\nIn 1270, the body of King Louis IX, who died in Tunis, which was Muslim territory, was subject to the process of \"mos Teutonicus\" for its transportation back to France.\n\nThe process of \"mos Teutonicus\" began with the cadaver being dismembered to facilitate the next stage in the process, in which the body parts were boiled in water or wine for several hours. The boiling had the effect of separating the flesh from the bone. Any residual was scraped from the bones, leaving a completely clean skeleton. Both the flesh and internal organs could be buried immediately, or preserved with salt in the same manner as animal meat. The bones, and any preserved flesh, would then be transported back to the deceased's home for ceremonial interment.\n\nMediaeval society generally regarded entrails as ignoble and there was no great solemnity attached to their disposal, especially among German aristocrats.\n\nAlthough the Church had a high regard for the practice, Pope Boniface VIII was known to have an especial repugnance of \"mos Teutonicus\" because of his ideal of bodily integrity. In his bull of 1300, \"De Sepulturis\", Boniface forbade the practice. The papal bull issued which banned this practice was often misinterpreted as prohibition against human dissection. This probably hindered the research of some anatomists as they feared repercussions and punishment as a result of medical autopsies but \"De Sepulturis\" only prohibited the act of \"mos Teutonicus,\" not dissection in general.\n\n\n"}
{"id": "64073", "url": "https://en.wikipedia.org/wiki?curid=64073", "title": "New World Syndrome", "text": "New World Syndrome\n\nNew World Syndrome is a set of non-communicable diseases brought on by consumption of junk food and a sedentary lifestyle, especially common to the indigenous peoples of the \"New World\" (i.e. of the Americas). Indigenous peoples of Oceania and Circumpolar peoples, and perhaps other populations of Asiatic origin are similarly affected and perhaps genetically predisposed. It is characterized by obesity, heart disease, diabetes, hypertension, and shortened life span.\n\nNew World Syndrome is linked to a change from a traditional diet and exercise to a Western diet and a sedentary lifestyle. Along with the lack of money. Traditional occupations of indigenous people—such as fishing, farming, and hunting—tended to involve constant activity, whereas modern office jobs do not. The introduction of modern transportation such as automobiles also decreased physical exertion. Meanwhile, Western foods which are rich in fat, salt, sugar, and refined starches are also imported into countries. The amount of carbohydrates in diets increases.\n\n\nRelated:\n\n"}
{"id": "22810417", "url": "https://en.wikipedia.org/wiki?curid=22810417", "title": "Official culture", "text": "Official culture\n\nOfficial culture is the culture that receives social legitimation or institutional support in a given society. Official culture is usually identified with bourgeoisie culture. For revolutionary Guy Debord, official culture is a \"rigged game\", where conservative powers forbid subversive ideas to have direct access to the public discourse, and where such ideas are integrated only after being trivialized and sterilized.\n\nA widespread observation is that a great talent has a free spirit. For instance Pushkin, which some scholar regard as Russia's first great writer, attracted the mad irritation of the Russian officialdom and particularly of the Tsar, since he \n\n\n"}
{"id": "461415", "url": "https://en.wikipedia.org/wiki?curid=461415", "title": "Peripeteia", "text": "Peripeteia\n\nPeripeteia () is a reversal of circumstances, or turning point. The term is primarily used with reference to works of literature. The Anglicized form of \"peripeteia\" is peripety.\n\nAristotle, in his Poetics, defines peripeteia as \"a change by which the action veers round to its opposite, subject always to our rule of probability or necessity.\" According to Aristotle, peripeteia, along with discovery, is the most effective when it comes to drama, particularly in a tragedy. He wrote that \"The finest form of Discovery is one attended by Peripeteia, like that which goes with the Discovery in Oedipus...\".\n\nAristotle says that peripeteia is the most powerful part of a plot in a tragedy along with discovery. A peripety is the change of the kind described from one state of things within the play to its opposite, and that too in the way we are saying, in the probable or necessary sequence of events. There is often no element like Peripeteia; it can bring forth or result in terror, mercy, or in comedies it can bring a smile or it can bring forth tears (Rizo). This is the best way to spark and maintain attention throughout the various form and genres of drama \"Tragedy imitates good actions and, thereby, measures and depicts the well-being of its protagonist. But in his formal definition, as well as throughout the Poetics, Aristotle emphasizes that\" ... Tragedy is an imitation not only of a complete action, but also of events inspiring fear or pity\" (1452a 1); in fact, at one point Aristotle isolates the imitation of \"actions that excite pity and fear\" as \"the distinctive mark of tragic imitation\" (1452b 30). Pity and fear are effected through reversal and recognition; and these \"most powerful elements of emotional interest in Tragedy-Peripeteia or Reversal of the Situation, and recognition scenes-are parts of the plot (1450a 32). has the shift of the tragic protagonist's fortune from good to bad, which is essential to the plot of a tragedy. It is often an ironic twist. Good uses of Peripeteia are those that especially are parts of a complex plot, so that they are defined by their changes of fortune being accompanied by reversal, recognition, or both\" (Smithson).\n\nPeripeteia includes changes of character, but also more external changes. A character who becomes rich and famous from poverty and obscurity has undergone peripeteia, even if his character remains the same.\n\nWhen a character learns something he had been previously ignorant of, this is normally distinguished from peripeteia as anagnorisis or discovery, a distinction derived from Aristotle's work.\n\nAristotle considered anagnorisis, leading to peripeteia, the mark of a superior tragedy. Two such plays are \"Oedipus Rex\", where the oracle's information that Oedipus had killed his father and married his mother brought about his mother's death and his own blindness and exile, and \"Iphigenia in Tauris\", where Iphigenia realizes that the strangers she is to sacrifice are her brother and his friend, resulting in all three of them escaping Tauris. These plots he considered complex and superior to simple plots without anagnorisis or peripeteia, such as when Medea resolves to kill her children, knowing they are her children, and does so. Aristotle identified \"Oedipus Rex\" as the principal work demonstrating peripety. (See Aristotle's \"Poetics\".)\n\nIn Sophocles' \"Oedipus Rex\", the peripeteia occurs towards the end of the play when the Messenger brings Oedipus news of his parentage. In the play, Oedipus is fated to murder his father and marry his mother. His parents, Laius and Jocasta, try to forestall the oracle by sending their son away to be killed, but he is actually raised by Polybus and his wife, Merope, the rulers of another kingdom. The irony of the Messenger’s information is that it was supposed to comfort Oedipus and assure him that he was the son of Polybus. Unfortunately for Oedipus, the Messenger says, \"Polybus was nothing to you, [Oedipus] that’s why, not in blood\" (Sophocles 1113). The Messenger received Oedipus from one of Laius’ servants and then gave him to Polybus. The plot comes together when Oedipus realizes that he is the son and murderer of Laius as well as the son and husband of Jocasta. Martin M. Winkler says that here, peripeteia and anagnôrisis occur at the same time \"for the greatest possible impact\" because Oedipus has been \"struck a blow from above, as if by fate or the gods. He is changing from the mighty and somewhat arrogant king of Thebes to a figure of woe\" (Winkler 57).\n\nThe instantaneous conversion of Paul on the road to Damascus is a classic example of \"peripeteia\", which Eusebius presented in his \"Life of Constantine\" as a pattern for the equally revelatory conversion of Constantine. Modern biographers of Constantine see his conversion less as a momentary phenomenon than as a step in a lifelong process.\n\nIn \"The Three Apples\", a medieval \"Arabian Nights\", after the murderer reveals himself near the middle of the story, he explains his reasons behind the murder in a flashback, which begins with him going on a journey to find three rare apples for his wife, but after returning finds out she cannot eat them due to her lingering illness. Later at work, he sees a slave passing by with one of those apples claiming that he received it from his girlfriend, a married woman with three such apples her husband gave her. He returns home and demands his wife to show him all three apples, but she only shows him two. This convinces him of her infidelity and he murders her as a result. After he disposes of her body, he returns home where his son confesses that he had stolen one of the apples and that a slave, to whom he had told about his father's journey, had fled with it. The murderer thus realizes his guilt and regrets what he has just done.\n\nThe second use of peripety occurs near the end. After finding out about the culprit behind the murder, the protagonist Ja'far ibn Yahya is ordered by Harun al-Rashid to find the tricky slave within three days, or else he will have Ja'far executed instead. After the deadline has passed, Ja'far prepares to be executed for his failure and bids his family farewell. As he hugs his youngest daughter, he feels a round object in her pocket, which is revealed to be the same apple that the culprit was holding. In the story's twist ending, the daughter reveals that she obtained it from their slave, Rayhan. Ja'far thus realizes that his own slave was the culprit all along. He then finds Rayhan and solves the case, preventing his own execution. That was a plot twist.\n\n\n\n"}
{"id": "7999309", "url": "https://en.wikipedia.org/wiki?curid=7999309", "title": "Plot drift", "text": "Plot drift\n\nPlot drift is a phenomenon in storytelling in which the plot of the story deviates from its apparent initial direction. The phenomenon can affect written works, although it is often more noticeable in performed media such as television shows or movies. Plot drift is generally (though not always) seen as contrary to good storytelling technique.\n\nA sign of plot drift can be the increased introduction of new characters and settings near the end of a story.\n\nA contrary literary technique might include the apparent introduction of plot drift, only to later reveal a connection to the rest of the story.\n"}
{"id": "4692651", "url": "https://en.wikipedia.org/wiki?curid=4692651", "title": "Portrait (literature)", "text": "Portrait (literature)\n\nThe portrait is a literary genre derived from pictorial portraiture.\n\nThe imitation of painting is apparent in the name of the genre itself, which is a painting term. Historians of antiquity recognised the task of the portrait as representation; we find the beginnings of the narrative portrait in Livy and Tacitus. However, the portrait began to emerge from the need to describe oneself (self-portrait) or one's contemporaries, as in the \"Essays\" of Montaigne. This latter work develops a line of questioning around the movement of the representation of the individual (or of a society) from the pictorial mode to the discursive mode.\n\nThe portrait can be realised in prose or in verse. Its objectives vary according to context: sociocultural, sociopolitical, historical, or again according to the subjectivity of the portraitist (the writer). Thus one can speak of a fictional portrait (corresponding to the characters who populate the fictional universe of each author) as much as a realist one (representing real-life people).\n\nThe portrait oscillates between reality and fiction, between eulogy and satire, between one portrait which imitates its original and another which moves away from it (such as the caricatures found in newspapers or in Molière). Nevertheless, the objective portrait which describes the flaws and qualities of the individual represented (or equally the object or the idea) is quite widespread. The literary portrait evolved through the centuries and its development has been shaped by writers as well as literary critics and theorists.\n\nIt is from the 1650s that the portrait began to be defined as a literary genre. It is through the social innovations of the \"précieuses\" - such as La Grande Mademoiselle who, influenced by the portrait-laden works of Madeleine de Scudéry, gathered around her (as a \"salonnière\" or ‘salon hostess’) men of letters - that the portrait was transformed into a ‘diversion of society’.\n\nThe literary portrait held to the essential aesthetic rules of the pictorial mode - that is, it had to describe the individual (model) faithfully in order to disguinguish it as a type apart. Nevertheless, it was not to be inferred from the recognition of the individual represented, but rather from the portraitist's style. This narrative representation had the function of highlighting fixed and timeless physical and mental features, as one sees in the works of Molière or in the \"Caractères\" by Jean de La Bruyère. It had to be achieved through layers of successive description - as in painting - which were only distinct phrases describing the real model's features.\n\nThe Age of Enlightenment heralded a new phase in the development of the literary portrait. It invaded literature and even contaminated music. Mozart and Beethoven excelled in this genre. Nevertheless, the portrait carried more of the psychology of the model represented than his or her physical appearance.\n\nIn Denis Diderot, it is precisely the pictorial portrait that is the occasion of a narrative self-portrait effected in the form of an artistic critique of the paintings and statues which were made of him. Thus he did not like the painting by Louis Michel van Loo portraying him:\nThe philosopher blamed the painter's wife for having prevented him from being himself: “It is this madwoman, madame Van Loo, who had come to gossip with him while he was being painted, who gave him that air, and who spoiled everything.” Diderot took to imagining what his portrait would have been like: \n\nAfter criticizing the portrait that portrays him, he writes:\n\nHe informs them, “My children, I tell you that it is not me,” and undertakes to trace in writing the real portrait of himself:\n\nAccording to Diderot, only one painter managed to make a pictorial portrait of him in which he recognizes himself and that is Jean-Baptiste Garand: by an apparent irony of fate, this success was the result of chance: \n\nMoreover, the semi-private sphere of correspondence also allowed the sketching of portraits in principle intended solely for the use of the recipient of the letter. Thus, Marie Du Deffand, taking the thermal waters at Forges-les-Eaux, was able to draw a sharp and cheerful portrait of Madame de Pecquigny, the companion that fate had assigned her during her treatment: \n\nHowever, it is not so much her spirit - or the way she uses it - that irritates Marie Du Deffand as the woman's quirks:\n\nMarie Du Deffand completes this portrait of an eccentric by linking her to her interlocutor: “I am sorry that you have in common with her the impossibility of staying a minute at rest.” After which she concludes, in accordance with the philosophy of resignation and disinterest she defends, the temporary nature of that which she will have to endure on this “holiday meeting”: \nThe evolution of the narrative portrait did not stop at the nineteenth century but, on the contrary, it became more refined and took on nuances through the intervention of Sainte-Beuve in the works or the critiques of such literary portraits.\n\nIn fact, the portrait found a true place in the novel where it represented not only real-life individuals but also fictional individuals (who could also be symbolic). It is in this way that the portrait became a predominant and recurring theme in the work of Balzac.\n\nThe portrait continued its journey throughout the twentieth century with the modern novel. In Nathalie Sarraute (in the Portrait d'un inconnu) the features are not fixed, the temporality plays its role in the genre of the moving portrait, progressive, fragmented, as in the life of a human.\n"}
{"id": "16657200", "url": "https://en.wikipedia.org/wiki?curid=16657200", "title": "Raptio", "text": "Raptio\n\nRaptio (in archaic or literary English rendered as rape) is a Latin term for the large-scale abduction of women, i.e. kidnapping for marriage or enslavement (particularly sexual slavery). The equivalent term Frauenraub, originally from German, is used in English in the field of art history.\n\nBride kidnapping is distinguished from \"raptio\" in that the former is the abduction of one woman by one man (and his friends and relatives), whereas the latter is the abduction of women by groups of men, possibly in a time of war.\n\nThe English word \"rape\" retains the Latin meaning in literary language, but the meaning is obscured by the more current meaning of \"sexual violation\". The word is akin to \"rapine\", \"rapture\", \"raptor\", \"rapacious\" and \"ravish\", and referred to the more general violations, such as looting, destruction, and capture of citizens, that are inflicted upon a town or country during war, e.g. the Rape of Nanking. The \"Oxford English Dictionary\" gives the definition \"the act of carrying away a person, especially a woman, by force\" besides the more general \"the act of taking anything by force\" (marked as \"obsolete\") and the more specific \"violation or ravishing of a woman\".\n\nEnglish \"rape\" was in use since the 14th century in the general sense of \"seize prey, take by force\", from \"raper\", an Old French legal term for \"to seize\", in turn from Latin \"rapere\" \"seize, carry off by force, abduct\". The Latin term was also used for sexual violation, but not always. It is contested that the legendary event known as \"The Rape of the Sabine Women\", while ultimately motivated sexually, did not entail sexual violation of the Sabine women on the spot, who were instead abducted, and then implored by the Romans to marry them (as opposed to striking a deal with their fathers or brothers first, as would have been required by law).\n\nThough the sexual connotation is today dominant, the word \"rape\" can be used in a non-sexual context in literary English. In Alexander Pope's \"The Rape of the Lock\", the title means \"the theft of a lock [of hair]\", exaggerating a trivial violation against a person. In the twentieth century, the classically trained J. R. R. Tolkien used the word with its old meaning of \"seizing and taking away\" in his \"The Silmarillion\". The musical comedy \"The Fantasticks\" has a controversial song (\"It Depends on What You Pay\") about \"an old-fashioned rape\". Compare also the adjective \"rapacious\" which retains the generic meaning of greedy and grasping.\n\nIn Roman Catholic canon law, \"raptio\" refers to the legal prohibition of matrimony if the bride was abducted forcibly (Canon 1089 CIC).\n\nThe practice is surmised to have been common since anthropological antiquity. In Neolithic Europe, excavation of the Linear Pottery culture site at Asparn-Schletz, Austria, the remains of numerous slain victims were found. Among them, young adult females and children were clearly under-represented, suggesting that attackers had killed the men but abducted the nubile females.\n\nAbduction of women is a common practice in warfare among tribal societies, along with cattle raiding. In historical human migrations, the tendency of mobile groups of invading males to abduct indigenous females is reflected in the greater stability of Human mitochondrial DNA haplogroups compared to Human Y-chromosome DNA haplogroups. \n\nThe Rape of the Sabine Women is an important part of the foundation legends of Rome (8th century BC). Romulus had established the settlement on the Palatine Hill with mostly male followers. Seeking wives, the Romans negotiated with the neighboring tribe of the Sabines, without success. Faced with the extinction of their community, the Romans planned to abduct Sabine women. Romulus invited Sabine families to a festival of Neptune Equester. At the meeting he gave a signal, at which the Romans grabbed the Sabine women and fought off the Sabine men. The indignant abductees were implored by Romulus to accept Roman husbands. Livy claims that no sexual assault took place. He asserted that Romulus offered them free choice and promised civil and property rights to women. According to Livy he spoke to them each in person, \"and pointed out to them that it was all owing to the pride of their parents in denying right of intermarriage to their neighbours. They would live in honourable wedlock, and share all their property and civil rights, and—dearest of all to human nature—would be the mothers of free men.\" The women married Roman men, but the Sabines went to war with the Romans. The conflict was eventually resolved when the women, who now had children by their Roman husbands, intervened in a battle to reconcile the warring parties. The tale is parodied by English short-story writer Saki in \"The Schartz-Metterklume Method\". It also serves as the main plot of the movie \"Seven Brides for Seven Brothers\".\n\nIn Sanskrit literature, the practice is known as \"Rakshasa Vivaha\" (\"devil marriage\"), mentioned e.g. by Kautilya. It is one of the eight forms of Hindu marriage, the violent seizure or rape of a girl after the defeat or destruction of her relatives (Manu Smrti 3.33).\n\nIn the 3rd century, Gothic Christianity appears to have been initiated under the influence of Christian women captured by the Goths in Moesia and Thrace: in 251 AD, the Gothic army raided the Roman provinces of Moesia and Thrace, defeated and killed the Roman emperor Decius, and took a number of (predominantly female) captives, many of whom were Christian. This is assumed to represent the first lasting contact of the Goths with Christianity.\n\nIn the Qur'an, marriage to female prisoners of war who embraced Islam is recommended for those who cannot afford to marry other Muslim women according to Islamic law (Sura 4:25).\n\nMutual abduction of women between Christian and Muslim communities was common in the Balkans under Ottoman rule, and is a frequent theme in the Hajduk songs of the period.\n\n"}
{"id": "3564116", "url": "https://en.wikipedia.org/wiki?curid=3564116", "title": "Setting (narrative)", "text": "Setting (narrative)\n\nThe setting is both the time and geographic location within a narrative, either nonfiction or fiction. A literary element, the setting helps initiate the main backdrop and mood for a story. Setting has been referred to as story world or \"milieu\" to include a context (especially society) beyond the immediate surroundings of the story. Elements of setting may include culture, historical period, geography, and hour. Along with the plot, character, theme, and style, setting is considered one of the fundamental components of fiction.\n\nSetting is an important element in a narrative and in some works the setting becomes a character itself. The term setting is often used to refer to the social milieu in which the events of a novel occur and novelist Donna Levin has described how this social milieu shapes the characters’ values. The elements of the story setting include the passage of time, which may be static in some stories or dynamic in others with, for example, changing seasons.\n\nSetting may take various forms:\n\n\n"}
{"id": "3193582", "url": "https://en.wikipedia.org/wiki?curid=3193582", "title": "Sociology of culture", "text": "Sociology of culture\n\nThe sociology of culture, and the related cultural sociology, concerns the systematic analysis of culture, usually understood as the ensemble of symbolic codes used by a members of a society, as it is manifested in the society. For Georg Simmel, culture referred to \"the cultivation of individuals through the agency of external forms which have been objectified in the course of history\". Culture in the sociological field is analyzed as the ways of thinking and describing, the ways of acting, and the material objects that together shape a people's way of life.\n\nContemporary sociologists' approach to culture is often divided between a \"sociology of culture\" and \"cultural sociology\"—the terms are similar, though not interchangeable. The \"sociology of culture\" is an older concept, and considers some topics and objects as more-or-less \"cultural\" than others. By way of contrast, Jeffrey C. Alexander introduced the term \"cultural sociology\", an approach that sees all, or most, social phenomena as inherently cultural at some level. For instance, a leading proponent of the \"strong program\" in cultural sociology, Alexander argues: \"To believe in the possibility of cultural sociology is to subscribe to the idea that every action, no matter how instrumental, reflexive, or coerced [compared to] its external environment, is embedded to some extent in a horizon of affect and meaning.\" In terms of analysis, sociology of culture often attempts to explain some discretely cultural phenomena as a product of social processes, while cultural sociology sees culture as a component of explanations of social phenomena. As opposed to the field of cultural studies, cultural sociology does not reduce all human matters to a problem of cultural encoding and decoding. For instance, Pierre Bourdieu's cultural sociology has a \"clear recognition of the social and the economic as categories which are interlinked with, but not reducible to, the cultural.\"\n\nCultural sociology first emerged in Weimar, Germany, where sociologists such as Alfred Weber used the term \"Kultursoziologie\" (cultural sociology). Cultural sociology was then \"reinvented\" in the English-speaking world as a product of the \"cultural turn\" of the 1960s, which ushered in structuralist and postmodern approaches to social science. This type of cultural sociology may loosely be regarded as an approach incorporating cultural analysis and critical theory. In the beginning of the cultural turn, sociologists tended to use qualitative methods and hermeneutic approaches to research, focusing on meanings, words, artifacts and symbols. \"Culture\" has since become an important concept across many branches of sociology, including historically quantitative and model-based subfields, such as social stratification and social network analysis.\n\nThe sociology of culture grew from the intersection between sociology, as shaped by early theorists like Marx, Durkheim, and Weber, and with the growing discipline of anthropology where researchers pioneered ethnographic strategies for describing and analyzing a variety of cultures around the world. Part of the legacy of the early development of the field is still felt in the methods (much of cultural sociological research is qualitative) in the theories (a variety of critical approaches to sociology are central to current research communities) and substantive focus of the field. For instance, relationships between popular culture, political control, and social class were early and lasting concerns in the field.\n\nAs a major contributor to conflict theory, Marx argued that culture served to justify inequality. The ruling class, or the bourgeoisie, produce a culture that promotes their interests, while repressing the interests of the proletariat. His most famous line to this effect is that \"Religion is the opium of the people\". Marx believed that the \"engine of history\" was the struggle between groups of people with diverging economic interests and thus the economy determined the cultural superstructure of values and ideologies. For this reason, Marx is a considered a \"materialist\" as he believes that the economic (material) produces the cultural (ideal), which \"stands Hegel on his head,\" who argued the ideal produced the material.\n\nDurkheim held the belief that culture has many relationships to society which include:\n\nWeber innovated the idea of a status group as a certain type of subculture. Status groups are based on things such as: race, ethnicity, religion, region, occupation, gender, sexual preference, etc. These groups live a certain lifestyle based on different values and norms. They are a culture within a culture, hence the label subculture. Weber also purported the idea that people were motivated by their material and ideal interests, which include things such as preventing one from going to hell. Weber also explains that people use symbols to express their spirituality, that symbols are used to express the spiritual side of real events, and that ideal interests are derived from symbols.\n\nFor Simmel, culture refers to \"the cultivation of individuals through the agency of external forms which have been objectified in the course of history.\" Simmel presented his analyses within a context of \"form\" and \"content\". Sociological concept and analysis can be viewed.\n\n1. Symbols: Any thing that carries particular meaning recognized by people who share the same culture.\n\n2. Language: A system of symbols that allows people to communicate with one another.\n\n3. Values: Culturally defined standards of desirability, goodness, beauty and many other things that serves as broad guidelines for social living.\n\n4. Beliefs: Specific statements that people hold to be true.\n\n5. Norms: Rules and expectations by which a society guides the behaviour of its members. The two types of norms are mores and folkways. Mores are norms that are widely observed and have a great moral significance. Folkways are norms for routine, casual interaction.\n\n6. Behavioral patterns: The typical manner in which people perform production (e.g., manual, manufactured, automated in various degrees), communicate (e.g., language content, technology choices), mark significant events (e.g., rituals of endorsing values and of punctuating steps in personal life), satisfy basic needs (e.g., for dwelling, feeding, security, sex, reproduction, entertainment), and the like.\n\n7. Artifacts: Distinct material objects, such as architecture, technologies, and artistic creations.\n\n8. Social institutions: Patterns of organization and relationships regarding governance, production, socializing, education, knowledge creation, arts, and relating to other cultures.\n\nIn an anthropological sense, culture is society based on the values and ideas without influence of the material world.\n\nCulture is like the shell of a lobster. Human nature is the organism living inside of that shell. The shell, culture, identifies the organism, or human nature. Culture is what sets human nature apart, and helps direct the life of human nature.\n\nAnthropologists lay claim to the establishment of modern uses of the culture concept as defined by Edward Burnett Tylor in the mid-19th century.\n\nMalinowski collected data from the Trobriand Islands. Descent groups across the island claim parts of the land, and to back up those claims, they tell myths of how an ancestress started a clan and how the clan descends from that ancestress. Malinowski's observations followed the research of that found by Durkheim.\n\nRadcliffe-Brown put himself in the culture of the Andaman Islanders. His research showed that group solidification among the islanders is based on music and kinship, and the rituals that involve the use of those activities. In the words of Radcliffe-Brown, \"Ritual fortifies Society\".\n\nMarcel Mauss made many comparative studies on religion, magic, law and morality of occidental and non-occidental societies, and developed the concept of total social fact, and argued that the reciprocity is the universal logic of the cultural interaction.\n\nLévi-Strauss, based, at the same time, on the sociological and anthropological positivism of Durkheim, Mauss, Malinowski and Radcliffe-Brown, on the economic and sociological marxism, on freudian and Gestalt psychology and on structural linguistics of Saussure and Jakobson, realized great studies on areas myth, kinship, religion, ritual, symbolism, magic, ideology (souvage pensée), knowledge, art and aesthetics, applying the methodological structuralism on his investigations. He searched the universal principals of human thought as a form of explaining social behaviors and structures.\n\nFrench sociologist Pierre Bourdieu's influential model of society and social relations has its roots in Marxist theories of class and conflict. Bourdieu characterizes social relations in the context of what he calls the \"field\", defined as a competitive system of social relations functioning according to its own specific logic or rules. The \"field\" is the site of struggle for power between the dominant and subordinate classes. It is within the \"field\" that legitimacy—a key aspect defining the dominant class—is conferred or withdrawn.\n\nBourdieu's theory of practice is practical rather than discursive, embodied as well as cognitive and durable though adaptive. A valid concern that sets the agenda in Bourdieu's theory of practice is how action follows regular statistical patterns without the product of accordance to rules, norms and/or conscious intention. To explain this concern, Bourdieu explains habitus and field. Habitus explains the mutually penetrating realities of individual subjectivity and societal objectivity after the function of social construction. It is employed to transcend the subjective and objective dichotomy.\n\nThe belief that culture is symbolically coded and can thus be taught from one person to another means that cultures, although bounded, can change. Cultures are both predisposed to change and resistant to it. Resistance can come from habit, religion, and the integration and interdependence of cultural traits.\n\nCultural change can have many causes, including: the environment, inventions, and contact with other cultures.\n\nSeveral understandings of how cultures change come from anthropology. For instance, in diffusion theory, the form of something moves from one culture to another, but not its meaning. For example, the ankh symbol originated in Egyptian culture but has diffused to numerous cultures. Its original meaning may have been lost, but it is now used by many practitioners of New Age religion as an arcane symbol of power or life forces. A variant of the diffusion theory, stimulus diffusion, refers to an element of one culture leading to an invention in another.\n\nContact between cultures can also result in acculturation. Acculturation has different meanings, but in this context refers to replacement of the traits of one culture with those of another, such as what happened with many Native American Indians. Related processes on an individual level are assimilation and transculturation, both of which refer to adoption of a different culture by an individual.\n\nGriswold outlined another sociological approach to cultural change. Griswold points out that it may seem as though culture comes from individuals – which, for certain elements of cultural change, is true – but there is also the larger, collective, and long-lasting culture that cannot have been the creation of single individuals as it predates and post-dates individual humans and contributors to culture. The author presents a sociological perspective to address this conflict.\n\nSociology suggests an alternative to both the view that it has always been an unsatisfying way at one extreme and the sociological individual genius view at the other. This alternative posits that culture and cultural works are collective, not individual, creations. We can best understand specific cultural objects... by seeing them not as unique to their creators but as the fruits of collective production, fundamentally social in their genesis. (p. 53)\nIn short, Griswold argues that culture changes through the contextually dependent and socially situated actions of individuals; macro-level culture influences the individual who, in turn, can influence that same culture. The logic is a bit circular, but illustrates how culture can change over time yet remain somewhat constant.\n\nIt is, of course, important to recognize here that Griswold is talking about cultural change and not the actual origins of culture (as in, \"there was no culture and then, suddenly, there was\"). Because Griswold does not explicitly distinguish between the origins of cultural change and the origins of culture, it may appear as though Griswold is arguing here for the origins of culture and situating these origins in society. This is neither accurate nor a clear representation of sociological thought on this issue. Culture, just like society, has existed since the beginning of humanity (humans being social and cultural). Society and culture co-exist because humans have social relations and meanings tied to those relations (e.g. brother, lover, friend). Culture as a super-phenomenon has no real beginning except in the sense that humans (homo sapiens) have a beginning. This, then, makes the question of the origins of culture moot – it has existed as long as we have, and will likely exist as long as we do. Cultural change, on the other hand, is a matter that can be questioned and researched, as Griswold does.\n\nCulture theory, developed in the 1980s and 1990s, sees audiences as playing an active rather than passive role in relation to mass media. One strand of research focuses on the audiences and how they interact with media; the other strand of research focuses on those who produce the media, particularly the news.\n\nComputer-mediated communication (CMC) is the process of sending messages—primarily, but not limited to text messages—through the direct use by participants of computers and communication networks. By restricting the definition to the direct use of computers in the communication process, you have to get rid of the communication technologies that rely upon computers for switching technology (such as telephony or compressed video), but do not require the users to interact directly with the computer system via a keyboard or similar computer interface. To be mediated by computers in the sense of this project, the communication must be done by participants fully aware of their interaction with the computer technology in the process of creating and delivering messages. Given the current state of computer communications and networks, this limits CMC to primarily text-based messaging, while leaving the possibility of incorporating sound, graphics, and video images as the technology becomes more sophisticated.\n\nCultural activities are institutionalised; the focus on institutional settings leads to the investigation \"of activities in the cultural sector, conceived as historically evolved societal forms of organising the conception, production, distribution, propagation, interpretation, reception, conservation and maintenance of specific cultural goods\". Cultural Institutions Studies is therefore a specific approach within the sociology of culture.\n\nKey figures in today's cultural sociology include: Julia Adams, Jeffrey Alexander, John Carroll, Diane Crane, Paul DiMaggio, Henning Eichberg, Ron Eyerman, Sarah Gatson, Andreas Glaeser, Wendy Griswold, Eva Illouz, Karin Knorr-Cetina, Michele Lamont, Annette Lareau, Stjepan Mestrovic, Philip Smith, Margaret Somers, Yasemin Soysal, Dan Sperber, Lynette Spillman, Ann Swidler, Diane Vaughan, and Viviana Zelizer.\n\n\n"}
{"id": "2917649", "url": "https://en.wikipedia.org/wiki?curid=2917649", "title": "Speech", "text": "Speech\n\nSpeech is human vocal communication using language. Each language uses phonetic combinations of a limited set of perfectly articulated and individualized vowel and consonant sounds that form the sound of its words (that is, all English words sound different from all French words, even if they are the same word, e.g., \"role\" or \"hotel\"), and using those words in their semantic character as words in the lexicon of a language according to the syntactic constraints that govern lexical words' function in a sentence. In speaking, speakers perform many different intentional speech acts, e.g., informing, declaring, asking, persuading, directing, and can use enunciation, intonation, degrees of loudness, tempo, and other non-representational or paralinguistic aspects of vocalization to convey meaning. In their speech speakers also unintentionally communicate many aspects of their social position such as sex, age, place of origin (through accent), physical states (alertness and sleepiness, vigor or weakness, health or illness), psychic states (emotions or moods), physico-psychic states (sobriety or drunkenness, normal consciousness and trance states), education or experience, and the like. \n\nAlthough people ordinarily use speech in dealing with other persons (or animals), when people swear they do not always mean to communicate anything to anyone, and sometimes in expressing urgent emotions or desires they use speech as a quasi-magical cause, as when they encourage a player in a game to do or warn them not to do something. There are also many situations in which people engage in solitary speech. People talk to themselves sometimes in acts that are a development of what some psychologists (e.g., Lev Vygotsky) have maintained is the use in thinking of silent speech in an interior monologue to vivify and organize cognition, sometimes in the momentary adoption of a dual persona as self addressing self as though addressing another person. Solo speech can be used to memorize or to test one's memorization of things, and in prayer or in meditation (e.g., the use of a mantra).\n\nResearchers study many different aspects of speech: speech production and speech perception of the sounds used in a language, speech repetition, speech errors, the ability to map heard spoken words onto the vocalizations needed to recreate them, which plays a key role in children's enlargement of their vocabulary, and what different areas of the human brain, such as Broca's area and Wernicke's area, underlie speech. Speech is the subject of study for linguistics, cognitive science, communication studies, psychology, computer science, speech pathology, otolaryngology, and acoustics. \nSpeech compares with written language\n, which may differ in its vocabulary, syntax, and phonetics from the spoken language, a situation called diglossia. \n\nThe evolutionary origins of speech are unknown and subject to much debate and speculation. While animals also communicate using vocalizations, and trained apes such as Washoe and Kanzi can use simple sign language, no animals' vocalizations are articulated phonemically and syntactically, and do not constitute speech.\n\nSpeech production is a multi-step process by which thoughts are generated into spoken utterances. Production involves the selection of appropriate words and the appropriate form of those words from the lexicon and morphology, and the organization of those words through the syntax. Then, the phonetic properties of the words are retrieved and the sentence is uttered through the articulations associated with those phonetic properties.\n\nIn linguistics (articulatory phonetics), articulation refers to how the tongue, lips, jaw, vocal cords, and other speech organs used to produce sounds are used to make sounds. Speech sounds are categorized by manner of articulation and place of articulation. Place of articulation refers to where the airstream in the mouth is constricted. Manner of articulation refers to the manner in which the speech organs interact, such as how closely the air is restricted, what form of airstream is used (e.g. pulmonic, implosive, ejectives, and clicks), whether or not the vocal cords are vibrating, and whether the nasal cavity is opened to the airstream. The concept is primarily used for the production of consonants, but can be used for vowels in qualities such as voicing and nasalization. For any place of articulation, there may be several manners of articulation, and therefore several homorganic consonants.\n\nNormal human speech is pulmonic, produced with pressure from the lungs, which creates phonation in the glottis in the larynx, which is then modified by the vocal tract and mouth into different vowels and consonants. However humans can pronounce words without the use of the lungs and glottis in alaryngeal speech, of which there are three types: esophageal speech, pharyngeal speech and buccal speech (better known as Donald Duck talk).\n\nSpeech production is a complex activity, and as a consequence errors are common, especially in children. Speech errors come in many forms and are often used to provide evidence to support hypotheses about the nature of speech. As a result, speech errors are often used in the construction of models for language production and child language acquisition. For example, the fact that children often make the error of over-regularizing the -ed past tense suffix in English (e.g. saying 'singed' instead of 'sang') shows that the regular forms are acquired earlier. Speech errors associated with certain kinds of aphasia have been used to map certain components of speech onto the brain and see the relation between different aspects of production: for example, the difficulty of expressive aphasia patients in producing regular past-tense verbs, but not irregulars like 'sing-sang' has been used to demonstrate that regular inflected forms of a word are not individually stored in the lexicon, but produced from affixation of the base form.\n\nSpeech perception refers to the processes by which humans can interpret and understand the sounds used in language. The study of speech perception is closely linked to the fields of phonetics and phonology in linguistics and cognitive psychology and perception in psychology. Research in speech perception seeks to understand how listeners recognize speech sounds and use this information to understand spoken language. Research into speech perception also has applications in building computer systems that can recognize speech, as well as improving speech recognition for hearing- and language-impaired listeners.\n\nSpeech perception is categorical, in that people put the sounds they hear into categories rather than perceiving them as a spectrum. People are more likely to be able to hear differences in sounds across categorical boundaries than within them. A good example of this is voice onset time (VOT). For example, Hebrew speakers, who distinguish voiced /b/ from voiceless /p/, will more easily detect a change in VOT from -10 ( perceived as /b/ ) to 0 ( perceived as /p/ ) than a change in VOT from +10 to +20, or -10 to -20, despite this being an equally large change on the VOT spectrum.\n\nIn speech repetition, speech being heard is quickly turned from sensory input into motor instructions needed for its immediate or delayed vocal imitation (in phonological memory). This type of mapping plays a key role in enabling children to expand their spoken vocabulary. Masur (1995) found that how often children repeat novel words versus those they already have in their lexicon is related to the size of their lexicon later on, with young children who repeat more novel words having a larger lexicon later in development. Speech repetition could help facilitate the acquisition of this larger lexicon.\n\nThere are several organic and psychological factors that can affect speech. Among these are:\n\n\nThe classical or Wernicke-Geschwind model of the language system in the brain focuses on Broca's area in the inferior prefrontal cortex, and Wernicke's area in the posterior superior temporal gyrus on the dominant hemisphere of the brain (typically the left hemisphere for language). In this model, a linguistic auditory signal is first sent from the auditory cortex to Wernicke's area. The lexicon is accessed in Wernicke's area, and these words are sent via the arcuate fasciculus to Broca's area, where morphology, syntax, and instructions for articulation are generated. This is then sent from Broca's area to the motor cortex for articulation.\n\nPaul Broca identified an approximate region of the brain in 1861 which, when damaged in two of his patients, caused severe deficits in speech production, where his patients were unable to speak beyond a few monosyllabic words. This deficit, known as Broca's or expressive aphasia, is characterized by difficulty in speech production where speech is slow and labored, function words are absent, and syntax is severely impaired, as in telegraphic speech. In expressive aphasia, speech comprehension is generally less affected except in the comprehension of grammatically complex sentences. Wernicke's area is named after Carl Wernicke, who in 1874 proposed a connection between damage to the posterior area of the left superior temporal gyrus and aphasia, as he noted that not all aphasic patients had suffered damage to the prefrontal cortex. Damage to Wernicke's area produces Wernicke's or receptive aphasia, which is characterized by relatively normal syntax and prosody but severe impairment in lexical access, resulting in poor comprehension and nonsensical or jargon speech.\n\nModern models of the neurological systems behind linguistic comprehension and production recognize the importance of Broca's and Wernicke's areas, but are not limited to them nor solely to the left hemisphere. Instead, multiple streams are involved in speech production and comprehension. Damage to the left lateral sulcus has been connected with difficulty in processing and producing morphology and syntax, while lexical access and comprehension of irregular forms (e.g. eat-ate) remain unaffected.\nMoreover, the circuits involved in human speech comprehension dynamically adapt with learning, for example, by becoming more efficient in terms of processing time when listening to familiar messages such as learned verses.\n\n\n\n"}
{"id": "22834191", "url": "https://en.wikipedia.org/wiki?curid=22834191", "title": "TV Tropes", "text": "TV Tropes\n\nTV Tropes is a wiki that collects and expands descriptions and examples of various plot conventions and plot devices, more commonly known as tropes, that are found within many creative works. Since its establishment in 2004, the site has shifted focus from covering only television and film tropes to covering those in other types of media such as literature, comics, manga, video games, music, advertisements, and toys. The nature of the site as a provider of commentary on pop culture and fiction has attracted attention and criticism from several web personalities and blogs.\nThe content of the site was published as free content from April 2008. TV Tropes changed its license in July 2012 to allow only noncommercial distribution of its content while continuing to host the prior submissions under the new license.\n\nTV Tropes was founded in 2004 by a programmer under the pseudonym \"Fast Eddie\", who described himself as having become interested in the conventions of genre fiction while studying at MIT in the 1970s and after browsing Internet forums in the 1990s. As of 2018 the site is privately owned and the site publicly lists two co-owners besides Eddie.\n\nInitially focused on the television show \"Buffy the Vampire Slayer\", TV Tropes has since increased its scope to include television series, films, novels, plays, professional wrestling, video games, anime, manga, comic strips/books, fan fiction, and many other subjects, including Internet works such as Wikipedia (referred to in-wiki as \"The Other Wiki\"). Additionally, articles on the site often relate to real life, or point out real situations where certain tropes can be applied. It has also used its informal style to describe topics such as science, philosophy, politics, and history under its Useful Notes section. TV Tropes does not have notability standards for the works it covers.\n\nThe site includes entries on various series and tropes. An article on a work includes a brief summary of the work in question along with a list of associated tropes. Trope pages are the reverse of articles on works: they give a description of the trope itself, then provide a list of the trope's appearances in various works of media. In this way TV Tropes is fully interconnected through the various connections made between the works and their tropes.\n\nFor example, the trope \"I Am Spartacus\" is a specific type of scene that appears in multiple works. It refers to scenes where a character is shielded from identification by other characters who are also claiming to be that particular character. The trope name references a famous scene in the film \"Spartacus\". This example is included, along with examples from \"South Park\", \"Power Rangers in Space\", the Talmud and even recent stories from real life. Not all examples of a trope may be cases where it is \"played straight\". They may also include cases where the trope is parodied, played with, inverted or even averted (i.e. avoided altogether in a context where it would be expected).\n\nIn addition to the tropes, most articles about a work also have a \"Your Mileage May Vary\" (YMMV) page with items that are deemed to be subjective. These items are not usually storytelling tropes, but are audience reactions which have been defined and titled. For example, the page of the well known trope \"jumping the shark\", the moment at which a series experiences a sharp decline in quality as in the notorious story point in \"Happy Days\", only contains a list of works that reference the phrase. TV Tropes does not apply the term to a show, that being a subjective opinion about the show, but cites uses of the phrase by the show (\"in-universe\"). Most articles also have various pages within them. For example, the article may have an \"Awesome\" page to describe crowning moments of awesome (i.e., a moment in a show or other fictional work that the majority of the readers or viewers regard as one of the high points); a \"Fridge\" page which describes examples of the tropes \"Fridge Logic\" (issues of a given work's internal consistency that do not typically occur to one until later), as well as the related \"Fridge Horror\" and \"Fridge Brilliance\"; a \"Laconic\" page which describes an article/trope in a few short words; and more pages that focus on a particular aspect of an article/item.\n\nTrope description pages are generally created through a standardized launching system, known as the \"Trope Launch Pad\" (TLP, formerly \"You Know That Thing Where\" [YKTTW]); site members, (referred to as \"Tropers\"), can draft a trope description and have the option of providing examples or suggesting refinements to other drafts before launch. While going through TLP is not necessary to launch a trope, it is strongly recommended in order to strengthen the trope as much as possible.\n\nThe site has created its own self-referencing meta-trope, known as \"TV Tropes Will Ruin Your Life\". The trope warns that some readers may become jaded and cynical as an unanticipated side effect of reading TV Tropes, \"[replacing] surprise almost entirely with recognition,\" referring to the inability to read books, watch films, etc. without identifying each trope as it occurs. Also mentioned is that many frequently-contributing community members self-describe themselves as addicted to the site. The community has dubbed the pattern of many tropers as taking a \"Wiki Walk,\" starting an edit on an intended article, and subsequently following links from one page to the next for hours on end without intending to, pausing occasionally to add examples the troper notices to the listings or rework articles. In the process, this leads to the discovery of entirely new tropes to analyze, edit, and add examples to. This self-perpetuating cycle of behavior has become the subject of much lampooning for the community, with tongue-in-cheek references being made in the articles for tropes such as \"Brainwashing,\" \"Hive Mind,\" and \"Tome of Eldritch Lore\" (a book of cursed knowledge which infects the reader with obsessive madness).\n\nConsiderable redesign of some aspects of content organization occurred in 2008, such as the introduction of namespaces, while 2009 saw the arrival of other languages, German being the most developed. In 2011, TV Tropes branched out into video production, and launched \"Echo Chamber\", a web series about a TV Tropes vlogger explaining and demonstrating tropes.\n\nIn an interview with TV Tropes co-founder Fast Eddie, Gawker Media's blog \"io9\" described the tone of contributions to the site as \"often light and funny\". Cyberpunk author Bruce Sterling once described its style as a \"wry fanfic analysis\". Essayist Linda Börzsei described TV Tropes as a technological continuum of classical archetypal literary criticisms, capable of deconstructing recurring elements from creative works in an ironic fashion. Economist Robin Hanson, inspired by a scholarly analysis of Victorian literature, suggests TV Tropes offers a veritable treasure trove of information about fiction - a prime opportunity for research into its nature. In Lifehacker, Nick Douglas compared TV Tropes to Wikipedia, recommending to \"use [TV Tropes] when Wikipedia feels impenetrable, when you want opinions more than facts, or when you've finished a Wikipedia page and now you want the juicy parts, the hard-to-confirm bits that Wikipedia doesn't share.\"\n\nIn October 2010, in what the site refers to as \"The Google Incident\", Google temporarily withdrew its AdSense service from the site after determining that pages regarding adult and mature tropes were inconsistent with its terms of service. \n\nIn a separate incident in 2012, in response to other complaints by Google, TV Tropes changed its guidelines to restrict coverage of sexist tropes and rape tropes. Feminist blog \"The Mary Sue\" criticized this decision, as it censored documentation of sexist tropes in video games and young adult fiction. \"ThinkProgress\" additionally condemned Google AdSense itself for \"providing a financial disincentive to discuss\" such topics. The site now separates NSFG articles (Not Safe for Google) from SFG articles (Safe for Google) in order to allow discussion of these kinds of tropes.\n\nTV Tropes content was licensed since April 2008 with the Creative Commons Attribution-Share Alike (CC-BY-SA) license for free content. In July 2012, the site changed its license notice and its existing content to the incompatible Attribution-Noncommercial-ShareAlike version (CC-BY-NC-SA). In November 2013, TV Tropes added a clause to their Terms of Use requiring all contributors to grant the site irrevocable, exclusive ownership of their contributions. In March 2015 this clause was removed, replaced with an assertion that TV Tropes does not claim ownership of user generated content. The site license also states that it is not required to attribute user content to its authors, although the Creative Commons Attribution-Noncommercial-ShareAlike license requires attribution of the original author.\n\nRegarding these and other concerns of re-licensing and advertising, a wiki called All The Tropes forked all the content from TV Tropes with the original CC-BY-SA license in late 2013. Authors of the fork attributed several actions of taking commercial rights over what is published on its website, censorship, and failing to comply with the original license to TV Tropes managers. Some editors raised concerns that keeping the content submitted with the previous copyleft license at TV Tropes is illegal, as the re-licensing had occurred without the permission of the editors and the original CC-BY-SA license did not allow its distribution under the new terms.\n\n"}
{"id": "4245811", "url": "https://en.wikipedia.org/wiki?curid=4245811", "title": "The Encyclopaedia Sinica", "text": "The Encyclopaedia Sinica\n\nThe Encyclopaedia Sinica is a 1917 English-language encyclopedia on China and China-related subjects edited by English missionary Samuel Couling. It covers a range of topics and provides insight on early 20th century perspectives towards China. Commentators report that the work is still useful at the turn of the 21st century particularly to aid the understanding of the relationship between China and the United Kingdom.\n"}
{"id": "39289519", "url": "https://en.wikipedia.org/wiki?curid=39289519", "title": "Tour-realism", "text": "Tour-realism\n\nTour-realism (T.R.) is a new trend in alternative tourism. It differs from both mass tourism and \"independent tourism\", a type of tourism involving absolutely no mediators in the tour organization. \nT.R. operators usually represent the country they live and work in. T.R. operators are known for having advanced knowledge of the culture and history of their country, as well as for having tight and constant connections with local population. Because they are residents of the country, T.R. operators are capable of providing unique services for foreign guests.\n\nTour-realism, as a tourism subgenre, tends to have certain defining characteristics. Tour-realism is generally done in small groups of 3-5 persons, usually family members or groups of friends. The approach is individualized (not reliant on standard itineraries) and focuses on high-quality offerings, with flexible choice of apartments and excursions that depend on the demands and wishes of the client. Accommodations are most often private villas, cottages etc., with an emphasis on creating an \"authentic\" experience as befits the culture and locale, again dependent on the wishes of the client. \"Extras\" in terms of excursions and services are built-in to create a personalized, unique experience for the client, and outings will include both the well-known and the obscure.\n\n"}
{"id": "178693", "url": "https://en.wikipedia.org/wiki?curid=178693", "title": "Transculturation", "text": "Transculturation\n\nTransculturation is a term coined by Cuban anthropologist Fernando Ortiz in 1947 to describe the phenomenon of merging and converging cultures.\n\nTransculturation encompasses more than transition from one culture to another; it does not consist merely of acquiring another culture (acculturation) or of losing or uprooting a previous culture (deculturation). Rather, it merges these concepts and additionally carries the idea of the consequent creation of new cultural phenomena (neoculturation). Ortiz also referred to the devastating impact of Spanish colonialism on Cuba's indigenous peoples as a \"failed transculturation\". Transculturation can often be the result of colonial conquest and subjugation, especially in a postcolonial era as native peoples struggle to regain their own sense of identity.\n\nIn simple terms, transculturation reflects the natural tendency of people (in general) to resolve conflicts over time, rather than exacerbating them. (In the modern context, both conflicts and resolutions are amplified by communication and transportation technology—the ancient tendency of cultures drifting or remaining apart has been replaced by stronger forces for bringing societies together.) Where transculturation impacts ethnicity and ethnic issues the term \"ethnoconvergence\" is sometimes used.\n\nIn one general sense, transculturation covers war, ethnic conflict, racism, multiculturalism, cross-culturalism, interracial marriage, and any other of a number of contexts that deal with more than one culture. In the other general sense, transculturation is one aspect of global phenomena and human events.\n\nThe general processes of transculturation are extremely complex—steered by powerful forces at the macrosocial level, yet ultimately resolved at the interpersonal level. The driving force for conflict is simple proximity—boundaries, once separating people (providing for a measure of isolation) become the issue of a conflict when societies encroach upon one another territorially. If a means to co-exist cannot be immediately found, then conflicts can be hostile, leading to a process by which contact between individuals leads to some resolution. Often, history shows us, the processes of co-existence begins with hostilities, and with the natural passing of polarist individuals, comes the passing of their polarist sentiments, and soon some resolution is achieved. Degrees of hostile conflict vary from outright genocidal conquest, to lukewarm infighting between differing political views within the same ethnic community.\n\nThese changes often represent differences between homeland pons, and their diasporic communities abroad. Nevertheless, obstacles to ethnoconvergence are not great. The primary issue; language, (\"hence, communication and education\") can be overcome within a single generation—as is evident in the easy acclimation of children of foreign parents. English, for example, is spoken by more non-Anglo-American people than Anglo-Americans, making it the current lingua-franca, the worldwide de facto standard international language.\n\nProcesses of transculturation become more complex within the context of globalization, given the multiple layers of abstraction that permeate everyday experiences. Elizabeth Kath argues that in the global era we can no longer consider transculturation only in relation to the face-to-face, but that we need to take into account the many layers of abstracted interactions that are interwoven through face-to-face encounters, a phenomenon that she describes as \"layers of transculturation\". Kath draws upon the concept of constitutive abstraction as seen in the work of Australian social theorists Geoff Sharp and Paul James.\n\nIt has been observed that even in monolingual, industrial societies like urban North America, some individuals do cling to a \"modernized\" primordial identity, apart from others. Some intellectuals, such as Michael Ignatieff, argue that convergence of a general culture does not directly entail a similar convergence in ethnic identities. This can become evident in social situations, where people divide into separate groups, despite being of an identical \"super-ethnicity\", such as nationality.\n\nWithin each smaller ethnicity, individuals may tend to see it perfectly justified to assimilate with other cultures, and some others view assimilation as wrong and incorrect for their culture. This common theme, representing dualist opinions of ethnoconvergence itself, within a single ethnic group is often manifested in issues of sexual partners and marriage, employment preferences, etc. These varied opinions of ethnoconvergence represent themselves in a spectrum; assimilation, homogenization, acculturation, and cultural compromise are commonly used terms for ethnoconvegence which flavor the issues to a bias.\n\nOften it's in a secular, multi-ethnic environment that cultural concerns are both minimised and exacerbated; Ethnic prides are boasted, hierarchy is created (\"center\" culture versus \"periphery\") but on the other hand, they will still share a common \"culture\", and common language and behaviours. Often the elderly, more conservative-in-association of a clan, tend to reject cross-cultural associations, and participate in ethnically similar community-oriented activities. Xenophobes tend to think of cross-cultural contact as a component of assimilation, and see this as harmful.\n\nThe obstacle to ethnoconvergence is ethnocentrism, which is the view that one's culture is of greater importance than another's. Ethnocentrism often takes different forms, as it is a highly personal bias, and manifests itself in countless aspects of culture. Religion, or belief, is the prime ethnocentric divider. Second is custom, which may overlap religion. With the adherence to each distinct component, comes the repulsion of the other. In most regions, ethnic divides are binary, meaning only two distinct cultures are present, each seeing the other as foreign. Many, however make the point that the binary example is the exception, and the norm is far more dynamic.\n\nEthnicity can be divided into two distinct areas, as they relate to ethnoconvergence: Utilitarian traits, and traditional customs.\n\nReligion, on the other hand, is a highly personal and attached part of culture. However, religion does not neatly correspond with ethnic identity. In many cosmopolitan societies, religion is everything—social, utilitarian, intellectual, political; from the point of view of people of immersed cultures; The very concept of ethnicity and its distinctions is incongruous to their immersed concepts.\n\nIn many societies, such as in those in Europe, languages are considered a significant component of ethnic values. This does not mean that most Europeans reject learning other languages. Quite the contrary, Europeans are often polyglots, and may label other individuals by their ethnicities; practical means of distinguishing cultures may resemble tendencies similar to ethnocentrism.\n\nHowever, the political and cultural significance of regional or national languages are retained because these polyglots conform to the linguistic norms of the place they visit—doing \"as the Romans do\". Thus, conforming to the \"ethnic integrity\" of the region.\n\nIt has even become a cliché that \"to learn a new language is to adopt a new soul\". There are many other examples of the essential significance of language. In pre-Russian Siberia, Tatar-Mongol colonists in the Taiga often recognized indigenous speakers of Turkic languages as their \"own people\" and non-Turkic groups as \"foreigners\", despite these indigenous groups having a similar level of material culture, and sharing much of a primitive culture with tribes foreign to the Muslim-Buddhist Tatar-Mongols.\n\nIn October 2011, U.S. communications agency Bromley launched a new model/strategy utilizing transcultural sociological theory as a means to segment and 'make sense' of the changing American cultural landscape. Returning to classic social science as a solution, Bromley has embraced the anthropological approach put forward by thinkers like Fernando Ortiz as a way to account for ethnicity and language without being limited by them as a way for viewing the world.\n\n\n\n"}
{"id": "53198297", "url": "https://en.wikipedia.org/wiki?curid=53198297", "title": "Trash Doves", "text": "Trash Doves\n\nSyd Weiler, a Sarasota, Florida-based artist and Adobe Creative resident, first made her dove sticker set for the iOS App Store in September 2016. Weiler described her visit to Minneapolis as inspiration for the illustrations, stating, \"I was sitting by a pond ... and there were just pigeons everywhere, I had never thought about pigeons before. They're funny little birds. They have really shiny, colorful, almost rainbow-y feathers, but then they bob around and waddle and beg for food. They're like doves but they eat trash.\"\n\nWeiler streamed the entire process of creating the stickers on her Twitch channel. The sticker set's illustrations feature a purple pigeon in various situations. Shortly after her Twitch stream, the stickers were released onto the iOS10 iMessage Sticker store.\n\nIn December 2016, Facebook approached Weiler, asking to license the dove artwork for their Messenger app. It officially made its Facebook debut on January 31, 2017, which included the animated sticker of a head banging pigeon. On Facebook, the stickers are used in the same way emojis are used, either through the Messenger app or on a comment section. The stickers are also available on Telegram.\n\nOn February 7, 2017, a Thai Facebook page posted a video that included a fusion of the headbanging dove from the sticker set, and a dancing cat. The video garnered over 3.5 million views in just a few days. Weiler responded to the video stating, \"I thought it was really funny. At that point, there's not much I can do about it but laugh.\" In Thailand, the usage of the meme was described by Sarasota's local media as \"a sort of cultural joke\". The Thai newspaper \"Khao Sod\" also noted that the Thai word for \"bird\", \"nok\", is also used to describe someone who is \"hopelessly single or suffering from unrequited love\", adding to its humor. Over the following weekend, the headbanging bird began to spread to English-language users, who spammed Facebook comment sections with the sticker.\n\nThe meme found its way outside of Facebook, being used on other social media websites such as YouTube, in addition to the online comment sections of news outlets such as the \"New York Times\". Some Internet users have created Trash Dove fan art, as well as reworked the image of the head banging dove into other memes, such as Salt Bae. \n\nSimilarly to Pepe the Frog, some users of the imageboard 4chan began to propose that the purple dove become a symbol of the alt-right, producing images of the dove combined with Nazi iconography, and interpreting it as a reincarnation of the Egyptian god Thoth dubbed \"Pek\" (a pun of the Egyptian god Kek, who was associated with Pepe). \"The Daily Dot\" noted that \"Like a lot of things on 4chan, it's hard to tell if this whole thing is satirical. The most likely case is that, like the white supremacist tendencies on the site, it started out as a joke but eventually turned real. Posters on liberal Facebook groups are alarmed by the bird spam—most of which is totally innocent, carried out by mainstream Facebook 'normies'—and have decided that any use of the bird is subtle fascist propaganda.\"\n\nWhen the sticker set was propelled to its meme status in February 2017, it was generally met with positive reception. Weiler commented, \"The cultural crossover is totally unanticipated, but it's bringing a lot of joy,\" adding \"I'm all about the positive. I created these to make people smile, so whenever I see people sharing Trash Dove I get really excited. I'm glad people can appropriate that.\" However, Weiler has also expressed, \"I'm a quiet homebody – I like to sit at my desk and draw, and play video games. Overnight, I was flooded with attention, and that has only sped up for five days now [...], I'm amazed at how mean people can be to someone they've never met, because of something silly online. I didn't ask for or sign up for any of this, but many people are blaming me, and I've even received some threats.\" \n\nKaitlyn Tiffany of \"The Verge\" opined that the birds in the sticker set are \"cuties\". Shortly after the meme rose to popularity, Madison Malone Kircher of \"New York\" Magazine detailed, \"since it's been about a week since Trash Dove rose to internet fame, the backlash cycle has already begun. People are starting to get irritated by the endless stream of reply comments consisting solely of Trash Doves, and some are even proposing a ban [from Facebook].\"\n"}
{"id": "35037958", "url": "https://en.wikipedia.org/wiki?curid=35037958", "title": "Will Brooker", "text": "Will Brooker\n\nWill Brooker is a writer and academic, professor of film and cultural studies at Kingston University and an author of several books of cultural studies dealing with elements of modern pop culture and fandom, specifically Batman, Star Wars and Alice in Wonderland.\n\nBrooker completed his BA (Hons) in film and English studies at the University of East Anglia in Norwich in 1991, and completed his PhD in cultural studies at Cardiff University in 1999. The title of his thesis was \"One Life, Many Faces: The Uses and Meanings of the Batman, 1939–1999\". Brooker also completed a postgraduate diploma in communication at Goldsmiths, where he is now recognised as one of a select group of notable alumni.\n\nBrooker has worked at Kingston University since 2005. From 2005 until 2007 he was senior lecturer and field leader for film studies, and from 2007 until 2009 he served as principal lecturer and director of studies for film and television. He was promoted to reader in 2009 and professor in 2013. He is currently the director of research of Kingston's Film and Television Department. On 1 January 2013, Brooker became the first British editor of \"Cinema Journal\" since the publication was established in 1967. His five-year term ran until 31 December 2017.\n\nBrooker's first major monograph, \"Batman Unmasked: Analyzing a Cultural Icon\", was published in 2001. The book is a series of essays on the comic book superhero Batman, an expansion of Brooker's PhD thesis. The essays are each centred on a different period of Batman's history, drawing out the key elements that differ from period to period, as well as those that remain constant. Entertainment Weekly describes \"Batman Unmasked\" as \"cutting through the mumbo jumbo...to deliver incisive analysis and very sharp reporting.\" Brooker engages with the major dialogues surrounding the topic, including the question of Batman's role as a patriotic figure, questions of his sexuality and the role of \"Batman\" as a viable queer text, to the point where Popmatters refers to it as \"less an analysis of the Batman than a repudiation of a number of other texts that support DC's \"official\" reading of the character\".\n\nBrooker went on to publish \"Using the Force: Creativity, Community and Star Wars Fans\" in 2002, \"Alice's Adventures: Lewis Carroll in Popular Culture\" in 2005 and \"The Blade Runner Experience: The Legacy of a Science Fiction Classic\" in 2006. More recently, he has written the BFI Film Classics edition \"Star Wars\", which received positive reviews in \"Empire\" magazine and \"The Guardian\".\n\nBetween August 2011 and March 2012, Brooker was a regular film, television and culture commentator for the \"Times Higher Education magazine.\" His reviews and articles during this period included \"The Girl with the Dragon Tattoo\" (2011), \"We Need To Talk About Kevin\" (2011), \"One Day\" (2011), \"J.Edgar\" (2012), the \"Twilight\" series (2008–2012), \"Dr. Who\" (2005–present) and the \"X Factor\".\n\nWith the release of Christopher Nolan's third Batman film, The Dark Knight Rises, and the publication of his book \"Hunting the Dark Knight: Twenty-First Century Batman\", Brooker published several high-profile articles on the character, including contributions to SFX, \"The Huffington Post\", \"Newsweek\", \"The Guardian\", Total Film and \"The Independent\". His work was also quoted in \"The New York Post\". \"Hunting the Dark Knight\" was well received by SFX magazine: 'Taking multi-faceted tilts at a multi-faceted character, Brooker elicits oodles of thinkers, critics and fans as back-up for his ideas. He also packs a loaded Bat-belt of theories and themes, roving from 'queer' slants to 'paratexts' to pizza tie-ins and beyond... Brooker's enthused rigour is infectious.'\n\nIn early 2013 Brooker teamed up with Suze Shore and Dr. Sarah Zaidan to create an online comic book series entitled \"My So Called Secret Identity\". The comic has been critically acclaimed and celebrated by \"The Guardian\", \"Times Higher Education\", and \"Ms Magazine\" due to its reimagining of women in comics through the main character, Cat, who is average in every way except for being \"really, really goddamn smart.\" Despite the high praise for \"My So Called Secret Identity\", Shore, Zaidan, and Brooker have chosen to measure the success of the comic by responses \"from individuals who have connected with Cat and her story, and told us how much it means to them, to the extent that it's even inspired them, changed their approach to life and given them more confidence.\" In addition to paving the way for more realistic and empowering representations of women in comics, the team behind \"My So Called Secret Identity\" have also taken steps to support women in a very practical way: \"My So Called Secret Identity\" is a crowd funded project which gives a portion of the funds raised to women's refuge charity A Way Out\n\n\"My So Called Secret Identity's\" success has led to both Brooker and Zaidan being invited to contribute to the British Library's \"Comics Unmasked\" exhibition. Brooker and Zaidan will join comics' greats Warren Ellis and Grant Morrison as panellists in the \"(Super)Hero with 1000 Faces\" panel on 16 June 2014 to discuss the importance of superheroes to the comics medium. \"My So Called Secret Identity\" has also been noticed by the London fashion magazine \"Stylist,\" who ran a feature--\"Comic Books: Not Just for the Boys\"—that highlighted \"My So Called Secret Identity\" \nand included brand new MSCSI content.\n\nIn November 2013, following \"Magic Words: An Evening With Alan Moore\", Brooker criticised Moore’s short film \"An Act of Faith\" on Twitter for its representation of sexualised violence. He also raised concerns about Kevin O’Neill’s suggestion that the \"Golliwog\" from League of Extraordinary Gentlemen: The Black Dossier, is “an incredibly powerful black character”, and Moore’s description of former Prime Minister Gordon Brown as a “bipolar Cyclops”. Moore responded to Brooker and other critics in a [ lengthy interview in which Moore referred to Brooker as \"Batman scholar.\" Brooker responded online on Sequart Organization. Dave Sim reflected upon the controversy, suggesting that although he feels that it is difficult to find fault with Alan Moore's \"bottom line\", he \"can't see why Dr. Brooker shouldn't vent about\" the content of Moore's work.\n\nWhile first completing his PhD and then expanding it into \"Batman Unmasked\", Will Brooker received a degree of media coverage unusual for an academic, much of it negative, and was interviewed about his work on Touch Radio. In 1999, Brooker celebrated the Caped Crusader's 60th birthday with Phil Jupitus on the latter's radio 4 programme 'Happy Birthday Batman'. In 2005, he appeared as himself in the TV Documentary \"Generation Jedi\", and in 2011 he was featured in \"Acafandom and Beyond\", an article on the blog of Henry Jenkins, Provost's Professor of Communication, Journalism, and Cinematic Arts at the University of Southern California. He has also been an invited expert on various television programmes, including ITV's \"Movie Mansions,\" Channel 5's \"Gloria's Full House,\" on which he spoke alongside Adam West and Paul Daniels, and he has debated \"Star Wars\" with Mark Kermode on BBC Two's \"The Culture Show\". He has also been interviewed twice on the online Jeff Rubin Show. In his first appearance, he explained his current work and his opinion of Joel Schumacher's Batman films and, in the second, he discussed Christopher Nolan's \"The Dark Knight Rises\".\nIn 2015 Brooker wrote for the \"New Statesman\" magazine about his support for feminism and has received media attention over his research for an academic analysis of David Bowie's life and work, \"Forever Stardust\". In particular he has received attention from the \"Guardian\", \"Rolling Stone\", and the \"Daily Mail\" for his endeavour to approximate Bowie's experiences in his own life.\n\n"}
{"id": "9810476", "url": "https://en.wikipedia.org/wiki?curid=9810476", "title": "Zombie", "text": "Zombie\n\nA zombie (Haitian French: \"\", ) is a fictional undead being created through the reanimation of a human corpse. Zombies are most commonly found in horror and fantasy genre works. The term comes from Haitian folklore, where a \"zombie\" is a dead body reanimated through various methods, most commonly magic. Modern depictions of the reanimation of the dead do not necessarily involve magic but often invoke science fictional methods such as carriers, radiation, mental diseases, vectors, pathogens, scientific accidents, etc.\n\nThe English word \"zombie\" is first recorded in 1819, in a history of Brazil by the poet Robert Southey, in the form of \"zombi\". The Oxford English Dictionary gives the origin of the word as West African, and compares it to the Kongo words ' (god) and ' (fetish). A Kimbundu-to -Portuguese dictionary from 1903 defines the related word nzumbi as soul, while a later KimbunduPortuguese dictionary defines it as being a \"spirit that is supposed to wander the earth to torment the living.\" \n\nOne of the first books to expose Western culture to the concept of the voodoo zombie was \"The Magic Island\" by W. B. Seabrook in 1929. This is the sensationalized account of a narrator who encounters voodoo cults in Haiti and their resurrected thralls. \"Time\" claimed that the book \"introduced 'zombi' into U.S. speech\".\n\nZombies have a complex literary heritage, with antecedents ranging from Richard Matheson and H. P. Lovecraft to Mary Shelley's \"Frankenstein\" drawing on European folklore of the undead. In 1932, Victor Halperin directed \"White Zombie\", a horror film starring Bela Lugosi. Here zombies are depicted as mindless, unthinking henchmen under the spell of an evil magician. Zombies, often still using this voodoo-inspired rationale, were initially uncommon in cinema, but their appearances continued sporadically through the 1930s to the 1960s, with notable films including \"I Walked with a Zombie\" (1943) and \"Plan 9 from Outer Space\" (1959).\n\nA new version of the zombie, distinct from that described in Haitian folklore, has also emerged in popular culture during the latter half of the twentieth century. This \"zombie\" is taken largely from George A. Romero's seminal film \"Night of the Living Dead\", which was in turn partly inspired by Richard Matheson's 1954 novel \"I Am Legend\". The word \"zombie\" is not used in \"Night of the Living Dead\" but was applied later by fans. The monsters in the film and its sequels, such as \"Dawn of the Dead\" and \"Day of the Dead\", as well as its many inspired works, such as \"Return of the Living Dead\" and \"Zombi 2\", are usually hungry for human flesh, although \"Return of the Living Dead\" introduced the popular concept of zombies eating brains. The \"zombie apocalypse\" concept, in which the civilized world is brought low by a global zombie infestation, became a staple of modern popular art.\n\nThe English word \"zombie\" is first recorded in 1819, in a history of Brazil by the poet Robert Southey, in the form of \"zombi\", actually referring to the Afro-Brazilian rebel leader named Zumbi and the etymology of his name in \"nzambi\". The Oxford English Dictionary gives the origin of the word as West African and compares it to the Kongo words \"nzambi\" (god) and \"zumbi\" (fetish).\n\nIn Haitian folklore, a \"zombie\" (Haitian French: \"zombi\", Haitian Creole: \"zonbi\") is an animated corpse raised by magical means, such as witchcraft.\n\nThe concept has been popularly associated with the religion of voodoo, but it plays no part in that faith's formal practices.\n\nHow the creatures in contemporary zombie films came to be called \"zombies\" is not fully clear. The film \"Night of the Living Dead\" made no spoken reference to its undead antagonists as \"zombies\", describing them instead as \"ghouls\" (though ghouls, which derive from Arabic folklore, are demons, not undead). Although George Romero used the term \"ghoul\" in his original scripts, in later interviews he used the term \"zombie\". The word \"zombie\" is used exclusively by Romero in his 1978 script for his sequel \"Dawn of the Dead\", including once in dialog. According to George Romero, film critics were influential in associating the term \"zombie\" to his creatures, and especially the French magazine \"Cahiers du Cinéma\". He eventually accepted this linkage, even though he remained convinced at the time that \"zombies\" corresponded to the undead slaves of Haitian voodoo as depicted in Bela Lugosi's \"White Zombie\".\n\nZombies are featured widely in Haitian rural folklore as dead persons physically revived by the act of necromancy of a \"bokor\", a sorcerer or witch. The \"bokor\" is opposed by the \"houngan\" or priest and the \"mambo\" or priestess of the formal voodoo religion. A zombie remains under the control of the \"bokor\" as a personal slave, having no will of its own.\n\nThe Haitian tradition also includes an incorporeal type of zombie, the \"zombie astral\", which is a part of the human soul. A \"bokor\" can capture a zombie astral to enhance his spiritual power. A zombie astral can also be sealed inside a specially decorated bottle by a \"bokor\" and sold to a client to bring luck, healing, or business success. It is believed that God eventually will reclaim the zombie's soul, so the zombie is a temporary spiritual entity.\n\nIt has been suggested that the two types of zombie reflect soul dualism, a belief of Haitian voodoo. Each type of legendary zombie is therefore missing one half of its soul (the flesh or the spirit).\n\nThe zombie belief has its roots in traditions brought to Haiti by enslaved Africans, and their subsequent experiences in the New World. It was thought that the voodoo deity Baron Samedi would gather them from their grave to bring them to a heavenly afterlife in Africa (\"Guinea\"), unless they had offended him in some way, in which case they would be forever a slave after death, as a zombie. A zombie could also be saved by feeding them salt. English professor Amy Wilentz has written that the modern concept of Zombies was strongly influenced by Haitian slavery. Slave drivers on the plantations, who were usually slaves themselves and sometimes Voodoo priests, used the fear of zombification to discourage slaves from committing suicide.\n\nWhile most scholars have associated the Haitian zombie with African cultures, a connection has also been suggested to the island's indigenous Taíno people, partly based on an early account of native shamanist practices written by the Hieronymite monk Ramón Pané, a companion of Christopher Columbus.\n\nThe Haitian zombie phenomenon first attracted widespread international attention during the United States occupation of Haiti (1915–1934), when a number of case histories of purported \"zombies\" began to emerge. The first popular book covering the topics was William Seabrook's \"The Magic Island\" (1929). Seabrooke cited Article 246 of the Haitian criminal code which was passed in 1864, asserting that it was an official recognition of zombies. This passage was later used in promotional materials for the 1932 film \"White Zombie\".\n\nIn 1937, while researching folklore in Haiti, Zora Neale Hurston encountered the case of a woman who appeared in a village. A family claimed she was Felicia Felix-Mentor, a relative who had died and been buried in 1907 at the age of 29. The woman was examined by a doctor; X-rays indicated that she did not have a leg fracture that Felix-Mentor was known to have had. Hurston pursued rumors that affected persons were given a powerful psychoactive drug, but she was unable to locate individuals willing to offer much information. She wrote, \"What is more, if science ever gets to the bottom of Vodou in Haiti and Africa, it will be found that some important medical secrets, still unknown to medical science, give it its power, rather than gestures of ceremony.\"\n\nA Central or West African origin for the Haitian zombie has been postulated based on two etymologies in the Kongo language, \"nzambi\" (\"god\") and \"zumbi\" (\"fetish\"). This root helps form the names of several deities, including the Kongo creator deity Nzambi a Mpungu and the Louisiana serpent deity Li Grand Zombi (a local version of the Haitian Damballa), but it is in fact a generic word for a divine spirit. The common African conception of beings under these names is more similar to the incorporeal \"zombie astral\", as in the Kongo Nkisi spirits.\n\nA related, but also often incorporeal, undead being is the jumbee of the English-speaking Caribbean, considered to be of the same etymology; in the French West Indies also, local \"zombies\" are recognized, but these are of a more general spirit nature.\n\nThe idea of physical zombie-like creatures is present in some South African cultures, where they are called \"xidachane\" in Sotho/Tsonga and \"maduxwane\" in Venda. In some communities, it is believed that a dead person can be zombified by a small child. It is said that the spell can be broken by a powerful enough sangoma. It is also believed in some areas of South Africa that witches can zombify a person by killing and possessing the victim's body in order to force it into slave labor. After rail lines were built to transport migrant workers, stories emerged about \"witch trains\". These trains appeared ordinary, but were staffed by zombified workers controlled by a witch. The trains would abduct a person boarding at night, and the person would then either be turned into a zombified worker, or beaten and thrown from the train a distance away from the original location.\n\nSeveral decades after Hurston's work, Wade Davis, a Harvard ethnobotanist, presented a pharmacological case for zombies in a 1983 paper in the \"Journal of Ethnopharmacology\", and later in two popular books, \"The Serpent and the Rainbow\" (1985) and \"Passage of Darkness: The Ethnobiology of the Haitian Zombie \"(1988).\n\nDavis traveled to Haiti in 1982 and, as a result of his investigations, claimed that a living person can be turned into a zombie by two special powders being introduced into the blood stream (usually via a wound). The first, \"coup de poudre\" (French: \"powder strike\"), includes tetrodotoxin (TTX), a powerful and frequently fatal neurotoxin found in the flesh of the pufferfish (order Tetraodontidae). The second powder consists of deliriant drugs such as datura. Together, these powders were said to induce a deathlike state in which the will of the victim would be entirely subjected to that of the bokor. Davis also popularized the story of Clairvius Narcisse, who was claimed to have succumbed to this practice. The most ethically questioned and least scientifically explored ingredient of the powders, is part of a recently buried child's brain.\n\nThe process described by Davis was an initial state of deathlike suspended animation, followed by re-awakening — typically \"after\" being buried — into a psychotic state. The psychosis induced by the drug and psychological trauma was hypothesised by Davis to reinforce culturally learned beliefs and to cause the individual to reconstruct their identity as that of a zombie, since they \"knew\" they were dead, and had no other role to play in the Haitian society. Societal reinforcement of the belief was hypothesized by Davis to confirm for the zombie individual the zombie state, and such individuals were known to hang around in graveyards, exhibiting attitudes of low affect.\n\nDavis's claim has been criticized, particularly the suggestion that Haitian witch doctors can keep \"zombies\" in a state of pharmacologically induced trance for many years. Symptoms of TTX poisoning range from numbness and nausea to paralysis — particularly of the muscles of the diaphragm — unconsciousness, and death, but do not include a stiffened gait or a deathlike trance. According to psychologist Terence Hines, the scientific community dismisses tetrodotoxin as the cause of this state, and Davis' assessment of the nature of the reports of Haitian zombies is viewed as overly credulous.\n\nScottish psychiatrist R. D. Laing highlighted the link between social and cultural expectations and compulsion, in the context of schizophrenia and other mental illness, suggesting that schizogenesis may account for some of the psychological aspects of zombification. Particularly, this suggests cases where schizophrenia manifests a state of catatonia.\n\nRoland Littlewood, professor of anthropology and psychiatry, published a study supporting a social explanation of the zombie phenomenon in the medical journal \"The Lancet\" in 1997.\nThe social explanation sees observed cases of people identified as zombies as a culture-bound syndrome, with a particular cultural form of adoption practiced in Haiti that unites the homeless and mentally ill with grieving families who see them as their \"returned\" lost loved ones, as Littlewood summarizes his findings in an article in \"Times Higher Education\":\n\nPulliam and Fonseca (2014) and Walz (2006) trace the zombie lineage back to ancient Mesopotamia. In the \"Descent of Ishtar\", the goddess Ishtar threatens:\n\nShe repeats this same threat in a slightly modified form in the \"Epic of Gilgamesh\".\n\"Frankenstein\" by Mary Shelley, while not a zombie novel in particular, prefigures many 20th-century ideas about zombies in that the resurrection of the dead is portrayed as a scientific process rather than a mystical one, and that the resurrected dead are degraded and more violent than their living selves. \"Frankenstein\", published in 1818, has its roots in European folklore, whose tales of vengeful dead also informed the evolution of the modern conception of the vampire. Later notable 19th-century stories about the avenging undead included Ambrose Bierce's \"The Death of Halpin Frayser\", and various Gothic Romanticism tales by Edgar Allan Poe. Though their works could not be properly considered zombie fiction, the supernatural tales of Bierce and Poe would prove influential on later writers such as H. P. Lovecraft, by Lovecraft's own admission.\n\nIn the 1920s and early 1930s, the American horror author H. P. Lovecraft wrote several novellae that explored the undead theme. \"Cool Air\", \"In the Vault\", and \"The Outsider\" all deal with the undead, but Lovecraft's \"Herbert West–Reanimator\" (1921) \"helped define zombies in popular culture\". This series of short stories featured Herbert West, a mad scientist who attempts to revive human corpses with mixed results. Notably, the resurrected dead are uncontrollable, mostly mute, primitive and extremely violent; though they are not referred to as zombies, their portrayal was prescient, anticipating the modern conception of zombies by several decades. Edgar Rice Burroughs similarly depicted animated corpses in the second book of his Venus series, again without ever using the terms \"zombie\" or \"undead\".\n\nAvenging zombies would feature prominently in the early 1950s EC Comics, which George A. Romero would later claim as an influence. The comics, including \"Tales from the Crypt\", \"Vault of Horror\" and \"Weird Science\", featured avenging undead in the Gothic tradition quite regularly, including adaptations of Lovecraft's stories, which included \"In the Vault\", \"Cool Air\" and \"Herbert West–Reanimator\".\n\nRichard Matheson's 1954 novel \"I Am Legend\", although classified as a vampire story would nonetheless have definitive impact on the zombie genre by way of George A. Romero. The novel and its 1964 film adaptation, \"The Last Man on Earth\", which concern a lone human survivor waging war against a world of vampires, would by Romero's own admission greatly influence his 1968 low-budget film \"Night of the Living Dead\"; a work that would prove to be more influential on the concept of zombies than any literary or cinematic work before it.\n\nFilms featuring zombies have been a part of cinema since the 1930s, with \"White Zombie\" (directed by Victor Halperin in 1932) being one of the earliest examples. With George A. Romero's \"Night of the Living Dead\" (1968), the zombie trope began to be increasingly linked to consumerism and consumer culture. Today, zombie films are released with such regularity (at least 55 titles were released in 2014 alone) that they can be viewed as a separate subgenre of Horror film.\n\nVoodoo-related zombie themes have also appeared in espionage or adventure themed works outside the horror genre. For example, the original \"Jonny Quest\" series (1964) and the James Bond novel and movie \"Live and Let Die\" both feature Caribbean villains who falsely claim the voodoo power of zombification in order to keep others in fear of them.\n\nThe modern conception of the zombie owes itself almost entirely to George A. Romero's 1968 film \"Night of the Living Dead\". In his films, Romero \"bred the zombie with the vampire, and what he got was the hybrid vigour of a ghoulish plague monster\". This entailed an apocalyptic vision of monsters that have come to be known as Romero zombies.\n\nRoger Ebert of the \"Chicago Sun-Times\" chided theater owners and parents who allowed children access to the film. \"I don't think the younger kids really knew what hit them,\" complained Ebert. \"They were used to going to movies, sure, and they'd seen some horror movies before, sure, but this was something else.\" According to Ebert, the film affected the audience immediately:\"The kids in the audience were stunned. There was almost complete silence. The movie had stopped being delightfully scary about halfway through, and had become unexpectedly terrifying. There was a little girl across the aisle from me, maybe nine years old, who was sitting very still in her seat and crying.\" Romero's reinvention of zombies is notable in terms of its thematics; he used zombies not just for their own sake, but as a vehicle \"to criticize real-world social ills—such as government ineptitude, bioengineering, slavery, greed and exploitation—while indulging our post-apocalyptic fantasies\". \"Night\" was the first of six films in Romero's \"Living Dead\" series. Its first sequel, \"Dawn of the Dead\", was released in 1978.\n\nLucio Fulci's \"Zombi 2\" was released just months after \"Dawn of the Dead\" as an ersatz sequel (\"Dawn of the Dead\" was released in several other countries as \"Zombi\" or \"Zombie\").\n\nThe 1981 film \"Hell of the Living Dead\" referenced a mutagenic gas as a source of zombie contagion: an idea also used in Dan O'Bannon's 1985 film \"Return of the Living Dead\". \"Return of the Living Dead\" featured zombies that hungered specifically for brains.\n\nThe mid-1980s produced few zombie films of note. Perhaps the most notable entry, \"The Evil Dead\" series, while highly influential are not technically zombie films but films about demonic possession, despite the presence of the undead. 1985's \"Re-Animator\", loosely based on the Lovecraft story, stood out in the genre, achieving nearly unanimous critical acclaim, and becoming a modest success, nearly outstripping Romero's \"Day of the Dead\" for box office returns.\n\nAfter the mid-1980s, the subgenre was mostly relegated to the underground. Notable entries include director Peter Jackson's ultra-gory film \"Braindead\" (1992) (released as \"Dead Alive\" in the U.S.), Bob Balaban's comic 1993 film \"My Boyfriend's Back\" where a self-aware high school boy returns to profess his love for a girl and his love for human flesh, and Michele Soavi's \"Dellamorte Dellamore\" (1994) (released as \"Cemetery Man\" in the U.S.). Several years later, zombies experienced a renaissance in low-budget Asian cinema, with a sudden spate of dissimilar entries including \"Bio Zombie\" (1998), \"Wild Zero\" (1999), \"Junk\" (1999), \"Versus\" (2000) and \"Stacy\" (2001).\n\nThere are not many Japanese films related to what may be considered in the West as a zombie film. Early films such as \"The Discarnates\" features little gore and no cannibalism but is about the dead returning to life looking for love rather than a story of apocalyptic destruction. The zombie themed video game \"Resident Evil\" (1996) was released to sales of 24 million copies worldwide. Most Japanese zombie films emerged in the wake of \"Resident Evil\", such as \"Versus\", \"Wild Zero\", \"Junk\" all from 2000. The zombies film released after \"Resident Evil\" behave similarly to the Zombie films of the 1970s.\n\nThe turn of the millennium coincided with a decade of box-office successes in which the zombie subgenre experienced a resurgence: the \"Resident Evil\" movies (2002, 2004, 2007, 2010, 2012, 2016), the British films \"28 Days Later\" and \"28 Weeks Later\" (2002, 2007), the \"Dawn of the Dead\" remake (2004) and the comedies \"Shaun of the Dead\" (2004) and \"Dance of the Dead\" (2008). The new interest allowed Romero to create the fourth entry in his zombie series: \"Land of the Dead\", released in the summer of 2005. Romero returned to the series with the films \"Diary of the Dead\" (2008) and \"Survival of the Dead\" (2010). \nGenerally, the zombies in these shows are the slow, lumbering and unintelligent kind first made popular in \"Night of the Living Dead\". Motion pictures created within the 2000s, however, like the \"Dawn of the Dead\" remake, and \"House of the Dead\", have featured zombies that are more agile, vicious, intelligent, and stronger than the traditional zombie. An alternate take on the zombie was 2013's film (and book) \"Warm Bodies\", where the zombie has consciousness and some intelligence.\n\nIn 2013, the AMC series \"The Walking Dead\" had the highest audience ratings in the United States for any show on broadcast or cable with an average of 5.6 million viewers in the 18- to 49-year-old demographic.\n\nIntimately tied to the concept of the modern zombie is the \"zombie apocalypse\"; the breakdown of society as a result of an initial zombie outbreak that spreads. This archetype has emerged as a prolific subgenre of apocalyptic fiction and has been portrayed in many zombie-related media after \"Night of the Living Dead\". In a zombie apocalypse, a widespread (usually global) rise of zombies hostile to human life engages in a general assault on civilization. Victims of zombies may become zombies themselves. This causes the outbreak to become an exponentially growing crisis: the spreading phenomenon swamps normal military and law enforcement organizations, leading to the panicked collapse of civilized society until only isolated pockets of survivors remain, scavenging for food and supplies in a world reduced to a pre-industrial hostile wilderness. Possible causes for zombie behavior in a modern population can be attributed to viruses, bacteria or other phenomena that reduce the mental capacity of humans causing them to behave in a very primitive and destructive fashion. \n\nThe usual subtext of the zombie apocalypse is that civilization is inherently vulnerable to the unexpected, and that most individuals if desperate enough cannot be relied on to comply with the author's ethos. The narrative of a zombie apocalypse carries strong connections to the turbulent social landscape of the United States in the 1960s, when \"Night of the Living Dead\" provided an indirect commentary on the dangers of conformity, a theme also explored in the novel \"The Body Snatchers\" (1954) and associated film \"Invasion of the Body Snatchers\" (1956). Many also feel that zombies allow people to deal with their own anxieties about the end of the world. One scholar concluded that \"more than any other monster, zombies are fully and literally apocalyptic ... they signal the end of the world as we have known it.\" While zombie apocalypse scenarios are secular, they follow a religious pattern based on Christian ideas of an end-times war and messiah.\n\nDue to a large number of thematic films and video games, the idea of a zombie apocalypse has entered the mainstream, and many fans have begun making efforts to prepare for the hypothetical future zombie apocalypse. Such efforts include creating weapons and selling educational material informing people how to survive a zombie outbreak; while most of these are tongue-in-cheek and do not represent an authentic belief that a zombie apocalypse in the near future is likely, the Centers for Disease Control and Prevention (CDC) have used the fictional scenario to demonstrate survival and emergency-preparedness techniques that may be useful in a \"real-world\" setting.\n\n\nThe stories usually follow a single group of survivors, caught up in the sudden rush of the crisis. The narrative generally progresses from the onset of the zombie plague, then initial attempts to seek the aid of authorities, the failure of those authorities, through to the sudden catastrophic collapse of all large-scale organization and the characters' subsequent attempts to survive on their own. Such stories are often squarely focused on the way their characters react to such an extreme catastrophe, and how their personalities are changed by the stress, often acting on more primal motivations (fear, self-preservation) than they would display in normal life.\n\nThere has been a growth in the number of zombie manga in the last decade, and in a list of \"10 Great Zombie Manga\", Anime News Network's Jason Thompson placed \"I Am a Hero\" at number 1, considering it \"probably the greatest zombie manga ever\". In second place was \"Living Corpse\" and in third, \"Biomega\", which he called \"the greatest science-fiction virus zombie manga ever\".\n\n\"\" was adapted into a live action film in 2014.\n\nArtist Jillian McDonald has made several works of video art involving zombies, and exhibited them in her 2006 show, \"Horror Make-Up,\" which debuted on 8 September 2006 at Art Moving Projects, a gallery in Williamsburg, Brooklyn.\n\nArtist Karim Charredib has dedicated his work to the zombie figure. In 2007, he made a video installation at villa Savoye called \"Them !!!\" where zombies walked in the villa like tourists.\n\nZombies are a popular theme for video games, particularly of, but not limited to, the stealth, survival horror, first-person shooter and role-playing game genres. Important horror fiction media franchises in this area include \"Resident Evil\", \"Dead Rising\", \"The House of the Dead\", \"Dead Island\", \"Left 4 Dead\", \"Dying Light\", \"State of Decay\", \"The Last of Us\" and the Zombies game modes from \"Call of Duty\" title series.\n\nPopCap Games' \"Plants vs. Zombies\", a humorous tower defense game, was an indie hit in 2009, featuring in several best-of lists at the end of that year. The massively multiplayer online role-playing game \"Urban Dead\", a free grid-based browser game where zombies and survivors fight for control of a ruined city, is one of the most popular games of its type.\n\nDayZ, a zombie-based survival horror mod for ArmA 2, was responsible for over 300,000 unit sales of its parent game within two months of its release. Over a year later, the developers of the mod created a standalone version of the same game, which currently is in early-access on Steam, and so far it has sold 3 million copies since its release in December 2013\n\nRomero would later opine that he believes that much of the 21st century obsessions with Zombies can be traced more towards video games than films, Noting that it was not until the 2009 film Zombieland that a Zombie film was able to grose more the 100 million.\n\nOutside of video games, zombies frequently appear in trading card games, such as \"\" or \"Yu-Gi-Oh! Trading Card Game\" (which even has a Zombie-Type for its \"monsters\"), as well as in role-playing games, such as \"Dungeons & Dragons\", tabletop games such as \"Zombies!!!\" and , and tabletop wargames, such as \"Warhammer Fantasy\" and \"40K\". The game \"Humans vs. Zombies\" is a zombie-themed live-action game played on college campuses.\n\nWriting for \"Scientific American\", Kyle Hill praised the 2013 game \"The Last of Us\" for the game's plausibility, which based its zombie enemies on a fictional strain of the \"Cordyceps\" fungus, which has real-world parasitic properties. Despite plausibility, to date there have been no documented cases of humans infected by \"Cordyceps\".\n\nOn 18 May 2011, the United States' Centers for Disease Control and Prevention (CDC) published a graphic novel, \"Preparedness 101: Zombie Apocalypse\" providing tips to survive a zombie invasion as a \"fun new way of teaching the importance of emergency preparedness\". The CDC goes on to summarize cultural references to a zombie apocalypse. It uses these to underscore the value of laying in water, food, medical supplies, and other necessities in preparation for any and all potential disasters, be they hurricanes, earthquakes, tornadoes, floods, or hordes of zombies.\n\nOn 17 October 2011, The Weather Channel in the United States published an article, \"How To Weather the Zombie Apocalypse\", that included a fictional interview with a Director of Research at the CDD, the \"Center for Disease Development\". Questions answered include \"How does the temperature affect zombies' abilities? Do they run faster in warmer temperatures? Do they freeze if it gets too cold?\"\n\nIn 2011, the US government drafted CONPLAN 8888-11,\na real plan detailing a strategy to defend against a zombie attack.\n\nMichael Jackson's music video \"Thriller\" (1983), in which he dances with a troop of zombies, has been preserved as a cultural treasure by the Library of Congress' National Film Registry. Many pop culture media have paid tribute to this video, such as a gathering of 14,000 university students dressed as zombies in Mexico City, and 1500 prisoners in orange jumpsuits recreating the zombie dance in a viral video .\n\nThe Brooklyn hip hop trio Flatbush Zombies incorporate many tropes from zombie fiction and play on the theme of a zombie apocalypse in their music. They portray themselves as \"living dead\", describing their use of psychedelics such as LSD and psilocybin mushrooms as having caused them to experience ego death and rebirth.\n\nIn the 1990s, zombie fiction emerged as a distinct literary subgenre, with the publication of \"Book of the Dead\" (1990) and its follow-up \"Still Dead: Book of the Dead 2\" (1992), both edited by horror authors John Skipp and Craig Spector. Featuring Romero-inspired stories from the likes of Stephen King, the \"Book of the Dead\" compilations are regarded as influential in the horror genre and perhaps the first true \"zombie literature\". Horror novelist Stephen King has written about zombies including his short story \"Home Delivery\" (1990) and his novel \"Cell\" (2006) concerning a struggling young artist on a trek from Boston to Maine in hopes of saving his family from a possible worldwide outbreak of zombie-like maniacs.\n\nMax Brooks's novel \"World War Z\" (2006) became a New York Times bestseller. Brooks had previously authored \"The Zombie Survival Guide\" (2003), a zombie-themed parody of pop-fiction survival guides published in 2003. Brooks has said that zombies are so popular because \"Other monsters may threaten individual humans, but the living dead threaten the entire human race... Zombies are slate wipers.\" Seth Grahame-Smith's mashup novel \"Pride and Prejudice and Zombies\" (2009) combines the full text of Jane Austen's \"Pride and Prejudice\" (1813) with a story about a zombie epidemic within the novel's British Regency period setting. In 2009, Katy Hershbereger of St. Martin's Press stated \"In the world of traditional horror, nothing is more popular right now than zombies... The living dead are here to stay.\" \nThe zombie also appears as a metaphor in protest songs, symbolizing mindless adherence to authority, particularly in law enforcement and the armed forces. Well-known examples include Fela Kuti's 1976 album \"Zombie\", and The Cranberries' 1994 single \"Zombie\".\n\nOrganized zombie walks have been staged, either as performance art or as part of protests that parody political extremism or apathy.\n\nA variation of the zombie walk is the zombie run. Here participants do a 5k run wearing a belt with several flag \"lives\". If the chasing zombies capture all of the flags the runner becomes \"infected\". If he or she reaches the finish line—which may involve wide detours—ahead of the zombies the participant is a \"survivor\". In either case an appropriate participation medal is awarded.\n\nResearchers have used theoretical zombie infections to test epidemiology modeling. One study found that all humans end up turned or dead. This is because the main epidemiological risk of zombies, besides the difficulties of neutralizing them, is that their population just keeps increasing; generations of humans merely \"surviving\" still have a tendency to feed zombie populations, resulting in gross outnumbering. The researchers explain that their methods of modelling may be applicable to the spread of political views or diseases with dormant infection.\n\nAdam Chodorow of the Sandra Day O'Connor College of Law at Arizona State University investigated the estate and income tax implications of a zombie apocalypse under United States federal and state tax codes. Neuroscientists Bradley Voytek and Timothy Verstynen have built a side career in extrapolating how ideas in neuroscience would theoretically apply to zombie brains. Their work has been featured in \"Forbes\", \"New York Magazine\", and other publications.\n\n\n"}
