{"id": "56753217", "url": "https://en.wikipedia.org/wiki?curid=56753217", "title": "APLL", "text": "APLL\n\nAPLL is an acronym and may refer to:\n"}
{"id": "46806415", "url": "https://en.wikipedia.org/wiki?curid=46806415", "title": "Abbreviation (music)", "text": "Abbreviation (music)\n\nAbbreviations in music are of two kinds, namely, abbreviations of terms related to musical expression, and the true musical abbreviations by the help of which certain passages, chords, etc., may be notated in a shortened form, to the greater convenience of both composer and performer. Abbreviations of the first kind are like most abbreviations in language; they consist for the most part of the initial letter or first syllable of the word employed—as for instance, or for the dynamic markings piano and forte, \"cresc.\" for crescendo, \"Ob.\" for oboe, \"Fag.\" for bassoon (). This article is about abbreviations used in music notation. For abbreviations of terms related to musical expression and music in general, see Glossary of musical terminology.\n\nThe continued repetition of a note or chord is expressed by a stroke or strokes across the stem, or above or below the note if it be a whole note or double whole note. The number of strokes denotes the subdivision of the written note into eighth notes, sixteenth notes, etc., unless the word tremolo or tremolando is added, in which case the repetition is as rapid as possible, without regard to the exact number of notes played. (When strokes are added to notes shorter than a quarter note, each beam counts as a stroke.) In the first bar of the example below, the half note with the single stroke across the stem in the \"written\" staff becomes 4 eighth notes in the \"played\" staff. Through the use of 2 strokes across the stem in the second bar, the next full note is expressed as a phrase of 16 sixteenth notes.\n\n\\relative c\" « { \\override Score.TimeSignature #'stencil = ##f } \\new staff { c2:8^\\markup { Written: } <e c g>: | c1:16 | \\time 2/4 f,2:32 | \\time 4/4 f4:8 a:8 c:16 f:16 | b,8:16 d:16 b:16 g:16 a:32 b:32 c:32 d:32 \\bar \"|.\" } \\new staff { c8^\\markup { Played: } c c c <e c g> <e c g> <e c g> <e c g> | c16 c c c c c c c c c c c c c c c | f,32 f f f f f f f f f f f f f f f | f8 f a a c16 c c c f f f f | b, b d d b b g g a32 a a a b b b b c c c c d d d d } »\n</score>\n\nOn bowed instruments the rapid reiteration of a single note is easy, but in piano music an octave or chord becomes necessary to produce a tremolo, the manner of writing and performing of which is seen below.\n\n\\relative c\" « { \\override Score.TimeSignature #'stencil = ##f } \\time 2/4 \\new staff { <g g'>4:16 ^\\markup { \\italic tremolo } <e' c g>:16 | \\repeat tremolo 4 { <c e>16^\\markup { \\italic tremolo } g } | s4 } \\new staff { g'32*8/12 g, g' g, g' g, g' g, g' g, g' g, <c e>32*8/12 g <c e> g <c e> g <c e> g <c e> g <c e> g | <c e>32 g <c e> g <c e> g <c e> g <c e> g <c e> g <c e> g <c e> g | s4 } »\n</score>\n\nIn the abbreviation expressed by strokes, as above, the passage to be abbreviated can contain no note of greater length than an eighth note, but it is possible also to divide a long note into quarter notes, by means of dots (sometimes known as \"divisi\" dots) placed over it, as below.\n\n\\relative c\" « { \\override Score.TimeSignature #'stencil = ##f } \\time 4/4 \\new staff { c2^\\markup { .. } c2^\\markup { .. } | c1^\\markup { ... } | s2 } \\new staff { c4 c c c | c c c c | s2 } »\n</score>\n\nThis is however seldom done, as only a small amount of space is saved. When a long note has to be repeated in the form of triplets or sextuplets, the figure 3 or 6 is usually placed over it in addition to the stroke across the stem, and the note is sometimes, though not necessarily, written dotted.\n\n\\relative c\" « { \\override Score.TimeSignature #'stencil = ##f } \\time 4/4 \\new staff { c4:8^\\markup { \\smaller { \\italic 3 } } c4:8^\\markup { \\smaller { \\italic 3 } } c2:8^\\markup { \\smaller { \\italic 6 } } \\bar \"||\" \\time 4/4 \\omit TupletNumber \\tuplet 3/2 { c4.:8^\\markup { \\smaller { \\italic 3 } } } \\tuplet 3/2 { c4.:8^\\markup { \\smaller { \\italic 3 } } } \\tuplet 6/4 { c4.:16^\\markup { \\smaller { \\italic 6 } } } \\tuplet 6/4 { c4.:16^\\markup { \\smaller { \\italic 6 } } } \\bar \"||\" } \\new staff { \\tuplet 3/2 4 {c8 c c c c c c c c c c c} | \\tuplet 3/2 4 {c8 c c c c c} \\tuplet 6/4 4 {c16 c c c c c c c c c c c} } »\n</score>\n\nThe repetition of a group of two notes is abbreviated by two notes (most often half notes or whole notes) connected by the number of strokes ordinarily used to express eighth notes, sixteenth notes, etc., according to the rate of movement intended, as below. It will be observed that a passage lasting for the value of one half note requires two half notes to express it, on account of the group consisting of two notes.\n\n\\relative c' « { \\override Score.TimeSignature #'stencil = ##f } \\time 4/4 \\new staff { \\repeat tremolo 2 { f8 a } \\repeat tremolo 4 { f16 a} | \\repeat tremolo 4 { f8 a } | s4 } \\new staff { f8 a f a f16 a f a f a f a | f8 a f a f a f a | s4 } »\n</score>\n\nAs seen above, half notes are often written with the strokes beaming the notes together (which is unambiguous as white notes with beams are not otherwise used in music), but with quarter notes and shorter the strokes must be separated from the stems to prevent them being misread as a shorter note value.\n\n\\relative c\" « { \\override Score.TimeSignature #'stencil = ##f } \\time 2/4 \\new staff { \\repeat tremolo 4 { c32 a } \\repeat tremolo 4 { g64 c } \\repeat tremolo 4 { g64 b } | s4 } \\new staff { c32 a c a c a c a g64 [c g c g c g c] g [b g b g b g b] | s4 } »\n</score>\n\nA group of three, four, or more notes is abbreviated by the repetition of the cross strokes without the notes as many times as the group has to be repeated.\n\n\\relative c' « { \\override Score.TimeSignature #'stencil = ##f } \\time 4/4 \\new staff { \\repeat percent 2 { e8[ g c g] } | \\repeat percent 4 { e16 g c g } \\bar \"||\" } \\new staff { e8 g c g e g c g | e16 g c g e g c g e g c g e g c g } »\n</score>\n\nThis can also be written with the notes forming the group are written as a chord, with the necessary number of strokes across the stem. In this case the word \"simili\" or \"segue\" is added, to show that the order of notes in the first group (which must be written out in full) is to be repeated, and to prevent the possibility of mistaking the effect intended for the repetition of the chord as a whole.\n\n\\relative c' « { \\override Score.TimeSignature #'stencil = ##f } \\time 4/4 \\new staff { d16 f a f <d f a>2.:16^\\markup { \\italic simili } | s4 } \\new staff { d16 f a f d f a f d f a f d f a f | s4 } »\n</score>\n\nAnother sign of abbreviation of a group consists of an oblique line with two dots, one on each side; this serves to indicate the repetition of a group of any number of notes of any length. This can even apply to a passage composed of several groups, provided such passage is not more than two bars in length.\n\n\\relative c' « { \\override Score.TimeSignature #'stencil = ##f } \\time 4/4 \\new staff { \\repeat percent 2 { e16 g f e f g a b c g a b c d e f g a g f e d c b a g c b a g a f } \\bar \"||\" } \\new staff { e16 g f e f g a b c g a b c d e f g a g f e d c b a g c b a g a f | e g f e f g a b c g a b c d e f g a g f e d c b a g c b a g a f } »\n</score>\n\nA more usual method of abbreviating the repetition of a passage of the length of the above is to write over it the word \"bis\" (twice), or in some cases \"ter\" (three times), or to enclose it between the dots of an ordinary repeat sign.\n\nPassages intended to be played in octaves are often written as single notes with the words \"coll' ottava\" or \"coll' 8va\" placed above or below them, according as the upper or lower octave is to be added.\n\n« { \\override Score.TimeSignature #'stencil = ##f } \\time 2/4 \\new Staff { c\"8^\\markup { \\smaller { \\italic \"coll' 8\" \\super \\italic \"va\" } } d\" e\" f\" \\bar \"||\" \\clef bass g_\\markup { \\smaller { \\italic \"coll' 8\" \\super \\italic \"va\" } } e c4 \\bar \"||\" s4 } \\new Staff { <c\" c>8 <d\" d> <e\" e> <f\" f> | \\clef bass <g g,> <e e,> <c c,>4 | s4 } »\n</score>\n\nThe word \"8\" (or sometimes \"8 alta\" or \"8 bassa\") written above or below a passage does not add octaves, but merely transposes the passage an octave higher or lower. In clarinet music the word \"chalumeau\" is used to signify that the passage is to be played an octave lower than written.\n\n« { \\override Score.TimeSignature #'stencil = ##f } \\time 4/4 \\new Staff { \\ottava #1 f8 e d c \\ottava #0 b\"^\\markup { \\smaller \\italic loco } a\" g\"4 \\bar \"||\" \\clef bass \\ottava #-1 a,8 b, c, d, \\ottava #0 e_\\markup { \\smaller \\italic loco } f g4 \\bar \"||\" \\clef treble \\ottava #-1 \\set Staff.ottavation = #\"chalumeau\" e8 g c' g f a c' a | g \\ottava #0 e'_\\markup { \\smaller \\italic clar. } g' c\" e\"2 \\bar \"||\" } \\new Staff { f8 e d c b\" a\" g\"4 | \\clef bass a,8 b, c, d, e, f, g,4 | \\clef treble e8 g c' g f a c' a | g e' g' c\" e\"2 } »\n</score>\n\nAll these alterations (which can scarcely be considered abbreviations except that they spare the use of ledger lines) are counteracted, and the passage restored to its usual position, by the ending of the enclosing bracket, the word \"loco\", or in clarinet music \"clarinette\".\n\nIn orchestral music it often happens that certain of the instruments play in unison; when this is the case the parts are sometimes not all written in the score, but the lines belonging to one or more of the instruments are left blank, and the words \"coi violini\" or \"col basso\", etc., are added, to indicate that the instruments in question have to play in unison with the violins or basses, as the case may be, or when two instruments of the same kind, such as first and second violins, have to play in unison, the word \"unisono\" or \"col primo\" is placed instead of the notes in the line belonging to the second. Where two parts are written on one staff in a score the sign \"a 2\" denotes that both play the same notes; and \"a 1\" that the second of the two is resting. The indication \"a 3\" or \"a 4\" at the head of fugues indicates the number of parts or voices in which the fugue is written.\n\nAn abbreviation which is often very troublesome to the conductor occurs in manuscript scores, when a considerable part of the composition is repeated without alteration, and the corresponding number of bars are left vacant, with the remark \"come sopra\" (as above). This is not met with in printed scores.\n\nThere are also abbreviations relating to music analysis, some of which are of great value. In figured bass, for instance, the various chords are expressed by figures, and the several authors in the nineteenth century invented or availed themselves of various methods of shortly expressing the different chords and intervals, particularly using Roman numeral analysis.\n\nGottfried Weber represents an interval by a number with one or two dots before it to express minor or diminished, and one or two after it for major or augmented.\n\nJohann Anton André makes use of a right triangle to express a triad, and a square, for a seventh chord, the inversions being indicated by one, two, or three small vertical lines across their base, and the classification into major, minor, diminished, or augmented by the numbers 1, 2, 3, or 4, placed in the centre.\n"}
{"id": "1052571", "url": "https://en.wikipedia.org/wiki?curid=1052571", "title": "Acronym", "text": "Acronym\n\nAn acronym is a word or name formed as an abbreviation from the initial components of a phrase or a word, usually individual letters (as in \"NATO\" or \"laser\") and sometimes syllables (as in \"Benelux\").\n\nThere are no universal standards for the multiple names for such abbreviations or for their orthographic styling. In English and most other languages, such abbreviations historically had limited use, but they became much more common in the 20th century. Acronyms are a type of word formation process, and they are viewed as a subtype of blending.\n\nWhereas an abbreviation may be any type of shortened form, such as words with the middle omitted (for example, \"Rd\" for \"road\" or \"Dr\" for \"Doctor\"), an acronym is a word formed from the first letter or first few letters of each word in a phrase (such as \"sonar\", created from \"und vigation and anging\"). Attestations for \"\" in German are known from 1921, and for \"acronym\" in English from 1940.\n\nAlthough the word \"acronym\" is often used to refer to any abbreviation formed from initial letters, some dictionaries and usage commentators define \"acronym\" to mean an abbreviation that is pronounced as a word, in contrast to an initialism (or alphabetism)an abbreviation formed from a string of initials (and possibly pronounced as individual letters).\nSome dictionaries include additional senses equating \"acronym\" with \"initialism\". The distinction, when made, hinges on whether the abbreviation is pronounced as a word or as a string of individual letters. Examples in reference works that make the distinction include \"NATO\" , \"scuba\" , and \"radar\" for acronyms; and \"FBI\" , \"CRT\" , and \"HTML\" for initialisms. The rest of this article uses \"acronym\" for both types of abbreviation.\n\nThe distinction is not well-maintained. According to \"Merriam-Webster's Dictionary of English Usage\": \"A number of commentators ... believe that acronyms can be differentiated from other abbreviations in being pronounceable as words. Dictionaries, however, do not make this distinction because writers in general do not. ... \"Initialism\", an older word than \"acronym\", seems to be too little known to the general public to serve as the customary term standing in contrast with \"acronym\" in a narrow sense.\" About the use of \"acronym\" to only mean those pronounced as words, \"Fowler's Modern English Usage\" (3rd ed.) states: \"The limitations of the term being not widely known to the general public, \"acronym\" is also often applied to abbreviations that are familiar but are not pronounceable as words. ... Such terms are also called \"initialisms\".\"\n\nA clearer distinction has also been drawn by Pyles & Algeo (1970), who divided \"acronyms\" as a general category into \"word acronyms\" pronounced as words, and \"initialisms\" sounded out as letters.\n\nThere is no special term for abbreviations whose pronunciation involves the combination of letter names and words or word-like pronunciations of strings of letters, such as \"JPEG\" and \"MS-DOS\" . There is also some disagreement as to what to call abbreviations that some speakers pronounce as letters and others pronounce as a word. For example, the terms \"URL\" and \"IRA\" can be pronounced as individual letters: and , respectively; or as a single word: and , respectively.\nThe spelled-out form of an acronym or initialism (that is, what it stands for) is called its \"expansion\".\n\n\nAcronymy, like retronymy, is a linguistic process that has existed throughout history but for which there was little to no naming, conscious attention, or systematic analysis until relatively recent times. Like retronymy, it became much more common in the 20th century than it had formerly been.\n\nAncient examples of acronymy (regardless of whether there was metalanguage at the time to describe it) include the following:\n\nDuring the mid- to late-19th century, an acronym-disseminating trend spread through the American and European business communities: abbreviating corporation names in places where space was limited for writing—such as on the sides of railroad cars (e.g., Richmond, Fredericksburg and Potomac Railroad → RF&P); on the sides of barrels and crates; and on ticker tape and in the small-print newspaper stock listings that got their data from it (e.g., American Telephone and Telegraph Company → AT&T). Some well-known commercial examples dating from the 1890s through 1920s include Nabisco (National Biscuit Company), Esso (from S.O., from Standard Oil), and Sunoco (Sun Oil Company).\n\nAnother driver for the adoption of acronyms was modern warfare with its many highly technical terms. While there is no recorded use of military acronyms in documents dating from the American Civil War (acronyms such as ANV for \"Army of Northern Virginia\" post-date the war itself), they had become somewhat common in World War I and were very much a part even of the vernacular language of the soldiers during World War II, who themselves were referred to as G.I.s.\n\nThe widespread, frequent use of acronyms across the whole range of registers is a relatively new linguistic phenomenon in most languages, becoming increasingly evident since the mid-20th century. As literacy rates rose, and as advances in science and technology brought with them a constant stream of new (and sometimes more complex) terms and concepts, the practice of abbreviating terms became increasingly convenient. The \"Oxford English Dictionary\" (\"OED\") records the first printed use of the word \"initialism\" as occurring in 1899, but it did not come into general use until 1965, well after \"acronym\" had become common.\n\nBy 1943, the term \"acronym\" had been used in English to recognize abbreviations (and contractions of phrases) that were pronounced as words. (It was formed from the Greek words , ', \"topmost, extreme\" and , ', \"name.\") For example, the army offense of being \"absent without official leave\" was abbreviated to \"A.W.O.L.\" in reports, but when pronounced as a word (\"awol\"), it became an acronym. While initial letters are commonly used to form an acronym, the original definition was \"a word made from the initial letters or syllables of other words\", for example UNIVAC from UNIVersal Automatic Computer.\n\nIn English, acronyms \"pronounced as words\" may be a 20th-century phenomenon. Linguist David Wilton in \"Word Myths: Debunking Linguistic Urban Legends\" claims that \"forming words from acronyms is a distinctly twentieth- (and now twenty-first-) century phenomenon. There is only one known pre-twentieth-century [English] word with an acronymic origin and it was in vogue for only a short time in 1886. The word is \"colinderies\" or \"colinda\", an acronym for the Colonial and Indian Exposition held in London in that year.\" However, although acronymic words seem not to have been \"employed in general vocabulary\" before the 20th century (as Wilton points out), the \"concept of their formation\" is treated as effortlessly understood (and evidently not novel) in a Poe story of the 1830s, \"How to Write a Blackwood Article\", which includes the contrived acronym P.R.E.T.T.Y.B.L.U.E.B.A.T.C.H.\n\nThe use of Latin and Neo-Latin terms in vernaculars has been pan-European and predates modern English. Some examples of acronyms in this class are:\n\n\nThe earliest example of a word derived from an acronym listed by the \"OED\" is \"abjud\" (now \"abjad\"), formed from the original first four letters of the Arabic alphabet in the late 18th century. Some acrostics predate this, however, such as the Restoration witticism arranging the names of some members of Charles II's Committee for Foreign Affairs to produce the \"CABAL\" ministry.\n\nAcronyms are used most often to abbreviate names of organizations and long or frequently referenced terms. The armed forces and government agencies frequently employ acronyms; some well-known examples from the United States are among the \"alphabet agencies\" (also jokingly referred to as \"alphabet soup\") created by Franklin D. Roosevelt (also of course known as FDR) under the New Deal. Business and industry also are prolific coiners of acronyms. The rapid advance of science and technology in recent centuries seems to be an underlying force driving the usage, as new inventions and concepts with multiword names create a demand for shorter, more manageable names. One representative example, from the U.S. Navy, is COMCRUDESPAC, which stands for \"commander, cruisers destroyers Pacific\"; it's also seen as \"ComCruDesPac\". \"YABA-compatible\" (where YABA stands for \"yet another bloody acronym\") is used to mean that a term's acronym can be pronounced but is not an offensive word, e.g., \"When choosing a new name, be sure it is 'YABA-compatible'.\"\n\nAcronym use has been further popularized by text messaging on mobile phones with Short Message Systems (SMS). To fit messages into the 160-character SMS limit, acronyms such as \"GF\" (girlfriend), \"LOL\" (laughing out loud), and \"DL\" (download or down low) have become popular. Some prescriptivists disdain texting acronyms and abbreviations as decreasing clarity, or as failure to use \"pure\" or \"proper\" English. Others point out that language change has happened for thousands of years, and argue that it should be embraced as inevitable, or as innovation that adapts the language to changing circumstances. In this view, the modern practice is just as legitimate as those in \"proper\" English of the current generation of speakers, such as the abbreviation of corporation names in places with limited writing space (e.g., ticker tape, newspaper column inches).\n\nIn formal writing for a broad audience, the expansion is typically given at the first occurrence of the acronym within a given text, for the benefit of those readers who do not know what it stands for. The capitalization of the original term is independent of it being acronymized, being lowercase for a common noun such as frequently asked questions (FAQ) but uppercase for a proper noun such as the United Nations (UN) (as explained at Case > Casing of expansions).\n\nIn addition to expansion at first use, some publications also have a key listing all acronyms used therein and what their expansions are. This is a convenience to readers for two reasons. The first is that if they are not reading the entire publication sequentially (which is a common mode of reading), then they may encounter an acronym without having seen its expansion. Having a key at the start or end of the publication obviates skimming over the text searching for an earlier use to find the expansion. (This is especially important in the print medium, where no search utility is available.) The second reason for the key feature is its pedagogical value in educational works such as textbooks. It gives students a way to review the meanings of the acronyms introduced in a chapter after they have done the line-by-line reading, and also a way to quiz themselves on the meanings (by covering up the expansion column and recalling the expansions from memory, then checking their answers by uncovering). In addition, this feature enables readers possessing knowledge of the abbreviations not to have to encounter expansions (redundant to such readers).\n\nExpansion at first use and the abbreviation-key feature are aids to the reader that originated in the print era, and they are equally useful in print and online. In addition, the online medium offers yet more aids, such as tooltips, hyperlinks, and rapid search via search engine technology.\n\nAcronyms often occur in jargon. An acronym may have different meanings in different areas of industry, writing, and scholarship. The general reason for this is convenience and succinctness for specialists, although it has led some to obfuscate the meaning either intentionally, to deter those without such domain-specific knowledge, or unintentionally, by creating an acronym that already existed.\n\nThe medical literature has been struggling to control the proliferation of acronyms as their use has evolved from aiding communication to hindering it. This has become such a problem that it is even evaluated at the level of medical academies such as the American Academy of Dermatology.\nAcronyms are often taught as mnemonic devices, for example in physics the colors of the visible spectrum are ROY G. BIV (red-orange-yellow-green-blue-indigo-violet). They are also used as mental checklists, for example in aviation: GUMPS, which is Gas-Undercarriage-Mixture-Propeller-Seatbelts. Other examples of mnemonic acronyms include CAN SLIM, and PAVPANIC as well as PEMDAS.\n\nIt is not uncommon for acronyms to be cited in a kind of false etymology, called a \"folk etymology\", for a word. Such etymologies persist in popular culture but have no factual basis in historical linguistics, and are examples of language-related urban legends. For example, \"cop\" is commonly cited as being derived, it is presumed, from \"constable on patrol,\" and \"posh\" from \"port out, starboard home\". With some of these specious expansions, the \"belief\" that the etymology is acronymic has clearly been tongue-in-cheek among many citers, as with \"gentlemen only, ladies forbidden\" for \"golf\", although many other (more credulous) people have uncritically taken it for fact. Taboo words in particular commonly have such false etymologies: \"shit\" from \"ship/store high in transit\" or \"special high-intensity training\" and \"fuck\" from \"for unlawful carnal knowledge\", or \"fornication under consent/command of the king\".\n\nIn English, abbreviations have traditionally been written with a full stop/period/point in place of the deleted part to show the ellipsis of letters—although the colon and apostrophe have also had this role—and with a space after full stops (e.g. \"A. D.\"). In the case of most acronyms, each letter is an abbreviation of a separate word and, in theory, should get its own termination mark. Such punctuation is diminishing with the belief that the presence of all-capital letters is sufficient to indicate that the word is an abbreviation.\n\nSome influential style guides, such as that of the BBC, no longer require punctuation to show ellipsis; some even proscribe it. Larry Trask, American author of \"The Penguin Guide to Punctuation\", states categorically that, in British English, \"this tiresome and unnecessary practice is now obsolete\".\n\nNevertheless, some influential style guides, many of them American, still require periods in certain instances. For example, The New York Times' guide recommends following each segment with a period when the letters are pronounced individually, as in \"K.G.B.\", but not when pronounced as a word, as in \"NATO\". The logic of this style is that the pronunciation is reflected graphically by the punctuation scheme.\n\nWhen a multiple-letter abbreviation is formed from a single word, periods are in general not used, although they may be common in informal usage. \"TV\", for example, may stand for a \"single\" word (\"television\" or \"transvestite\", for instance), and is in general spelled without punctuation (except in the plural). Although \"PS\" stands for the single word \"postscript\" (or the Latin \"postscriptum\"), it is often spelled with periods (\"P.S.\").\n\nThe slash ('/', or \"solidus\") is sometimes used to separate the letters in a two-letter acronym, as in \"N/A\" (\"not applicable, not available\"), \"c/o\" (\"care of\") and \"w/o\" (\"without\").\n\nInconveniently long words used frequently in related contexts can be represented according to their letter count. For example, \"i18n\" abbreviates \"internationalization\", a computer-science term for adapting software for worldwide use. The \"18\" represents the 18 letters that come between the first and the last in \"internationalization\". \"Localization\" can be abbreviated \"l10n\", \"multilingualization\" \"m17n\", and \"accessibility\" \"a11y\". In addition to the use of a specific number replacing that many letters, the more general \"x\" can be used to replace an unspecified number of letters. Examples include \"Crxn\" for \"crystallization\" and the series familiar to physicians for history, diagnosis, and treatment (\"hx\", \"dx\", \"tx\").\n\nThere is a question about how to pluralize acronyms. Often a writer will add an 's' following an apostrophe, as in \"PC's\". However, Kate Turabian, writing about style in academic writings, allows for an apostrophe to form plural acronyms \"only when an abbreviation contains internal periods or both capital and lowercase letters\". Turabian would therefore prefer \"DVDs\" and \"URLs\" and \"Ph.D.'s\". The Modern Language Association and American Psychological Association prohibit apostrophes from being used to pluralize acronyms regardless of periods (so \"compact discs\" would be \"CDs\" or \"C.D.s\"), whereas \"The New York Times\" style guide requires an apostrophe when pluralizing all abbreviations regardless of periods (preferring \"PC's, TV's and VCR's\").\n\nPossessive plurals that also include apostrophes for mere pluralization and periods appear especially complex: for example, \"the C.D.'s' labels\" (the labels of the compact discs). In some instances, however, an apostrophe may increase clarity: for example, if the final letter of an abbreviation is \"S\", as in \"SOS's\" (although abbreviations ending with S can also take \"-es\", e.g. \"SOSes\"), or when pluralizing an abbreviation that has periods.\n\nA particularly rich source of options arises when the plural of an acronym would normally be indicated in a word other than the final word if spelled out in full. A classic example is \"Member of Parliament\", which in plural is \"Members of Parliament\". It is possible then to abbreviate this as \"M's P.\" (or similar), as used by former Australian Prime Minister Ben Chifley. This usage is less common than forms with \"s\" at the end, such as \"MPs\", and may appear dated or pedantic. In common usage, therefore, \"weapons of mass destruction\" becomes \"WMDs\", \"prisoners of war\" becomes \"POWs\", and \"runs batted in\" becomes \"RBIs\".\n\nThe argument that acronyms should have no different plural form (for example, \"If \"D\" can stand for \"disc\", it can also stand for \"discs\") is in general disregarded because of the practicality in distinguishing singulars and plurals. This is not the case, however, when the abbreviation is understood to describe a plural noun already: For example, \"U.S.\" is short for \"United States, but not \"United State\". In this case, the options for making a possessive form of an abbreviation that is already in its plural form without a final \"s\" may seem awkward: for example, \"U.S.\", \"U.S.'s\", etc. In such instances, possessive abbreviations are often forgone in favor of simple attributive usage (for example, \"the U.S. economy\") or expanding the abbreviation to its full form and \"then\" making the possessive (for example, \"the United States' economy\"). On the other hand, in speech, the pronunciation \"United States's\" sometimes is used.\n\nAbbreviations that come from single, rather than multiple, words—such as \"TV\" (\"television\")—are usually pluralized without apostrophes (\"two TVs\"); most writers feel that the apostrophe should be reserved for the possessive (\"the TV's antenna\").\n\nThe most common capitalization scheme seen with acronyms is all-uppercase (all-caps), except for those few that have linguistically taken on an identity as regular words, with the acronymous etymology of the words fading into the background of common knowledge, such as has occurred with the words \"scuba\", \"laser\", and \"radar\"—these are known as \"anacronyms\". Anacronyms (note well \"-acro-\") should not be homophonously confused with anachronyms (note well \"-chron-\"), which are a type of misnomer.\n\nSmall caps are sometimes used to make the run of capital letters seem less jarring to the reader. For example, the style of some American publications, including the \"Atlantic Monthly\" and \"USA Today\", is to use small caps for acronyms longer than three letters; thus \"U.S.\" and \"FDR\" in normal caps, but \"\" in small caps. The acronyms \"AD\" and \"BC\" are often smallcapped as well, as in: \"From \".\n\nWords derived from an acronym by affixing are typically expressed in mixed case, so the root acronym is clear. For example, \"pre-WWII politics\", \"post-NATO world\", \"DNAase\". In some cases a derived acronym may also be expressed in mixed case. For example, messenger RNA and transfer RNA become mRNA and tRNA.\n\nSome publications choose to capitalize only the first letter of acronyms, reserving all-caps styling for initialisms, writing the pronounced acronyms \"Nato\" and \"Aids\" in mixed case, but the initialisms \"USA\" and \"FBI\" in all caps. For example, this is the style used in \"The Guardian\", and BBC News typically edits to this style (though its official style guide, dating from 2003, still recommends all-caps). The logic of this style is that the pronunciation is reflected graphically by the capitalization scheme.\n\nSome style manuals also base the letters' case on their number. \"The New York Times\", for example, keeps \"NATO\" in all capitals (while several guides in the British press may render it \"Nato\"), but uses lower case in \"UNICEF\" (from \"United Nations International Children's Emergency Fund\") because it is more than four letters, and to style it in caps might look ungainly (flirting with the appearance of \"shouting capitals\").\n\nWhile abbreviations typically exclude the initials of short function words (such as \"and\", \"or\", \"of\", or \"to\"), this is not always the case. (A similar set of words is sometimes left as lowercase in headers and publication titles.) Sometimes function words are included to make a pronounceable acronym, such as CORE (Congress of Racial Equality). Sometimes the letters representing these words are written in lower case, such as in the cases of TfL (Transport for London) and \"LotR\" (\"Lord of the Rings\"); this usually occurs when the acronym represents a multi-word proper noun.\n\nNumbers (both cardinal and ordinal) in names are often represented by digits rather than initial letters: as in \"4GL\" (Fourth generation language) or \"G77\" (Group of 77). Large numbers may use metric prefixes, as with \"Y2K\" for \"Year 2000\" (sometimes written \"Y2k\", because the SI symbol for 1000 is \"k\"—not \"K\", which stands for \"kelvin\"). Exceptions using initials for numbers include \"TLA\" (three-letter acronym/abbreviation) and \"GoF\" (Gang of Four). Abbreviations using numbers for other purposes include repetitions, such as \"W3C\" (\"World Wide Web Consortium\") and \"T3\" (\"Trends, Tips & Tools for Everyday Living\"); pronunciation, such as \"B2B\" (\"business to business\"); and numeronyms, such as \"i18n\" (\"internationalization\"; \"18\" represents the 18 letters between the initial \"i\" and the final \"n\").\n\nAlthough many authors of expository writing show a predisposition to capitalizing the initials of the expansion for pedagogical emphasis (trying to thrust the reader's attention toward where the letters are coming from), this sometimes conflicts with the convention of English orthography, which reserves capitals in the middle of sentences for proper nouns. Enforcing the general convention, most professional editors case-fold such expansions to their standard orthography when editing manuscripts for publication. The justification is that (1) readers are smart enough to figure out where the letters came from, even without their being capitalized for emphasis, and that (2) common nouns do not take capital initials in standard English orthography. Such house styles also usually disfavor bold or italic font for the initial letters. For example,\n\"the onset of Congestive Heart Failure (CHF)\" or \"the onset of congestive heart failure (CHF)\" if found in an unpublished manuscript would be rewritten as \"the onset of congestive heart failure (CHF)\" in the final published article when following the AMA Manual of Style.\n\nSome apparent acronyms or other abbreviations do not stand for anything and cannot be expanded to some meaning. Such pseudo-acronyms may be pronunciation-based, such as BBQ (bee-bee-cue), for \"barbecue\", or K9 (kay-nine) for \"canine\". Pseudo-acronyms also frequently develop as \"orphan initialisms\"; an existing acronym is redefined as a non-acronymous name, severing its link to its previous meaning. For example, the letters of the SAT, a US college entrance test originally dubbed \"Scholastic Aptitude Test\", no longer officially stand for anything.\n\nThis is common with companies that want to retain brand recognition while moving away from an outdated image: American Telephone and Telegraph became AT&T, Kentucky Fried Chicken became KFC to de-emphasize the role of frying in the preparation of its signature dishes, and British Petroleum became BP. Russia Today has rebranded itself as RT. Genzyme Transgenics Corporation became GTC Biotherapeutics, Inc. in order to reduce perceived corporate risk of sabotage/vandalism by Luddite activists.\n\nPseudo-acronyms may have advantages in international markets: for example, some national affiliates of International Business Machines are legally incorporated as \"IBM\" (for example, \"IBM Canada\") to avoid translating the full name into local languages. Likewise, \"UBS\" is the name of the merged Union Bank of Switzerland and Swiss Bank Corporation, and \"HSBC\" has replaced \"The Hongkong and Shanghai Banking Corporation.\" Sometimes, companies whose original name gives a clear indication of their place of origin will use acronyms when expanding to foreign markets—for example, Toronto-Dominion Bank continues to operate under the full name in Canada, but its U.S. subsidiary is known as TD Bank, just as Royal Bank of Canada used its full name in Canada (a constitutional monarchy), but its now-defunct U.S. subsidiary was called RBC Bank.\n\nRebranding can lead to redundant acronym syndrome, as when Trustee Savings Bank became TSB Bank, or when Railway Express Agency became REA Express. A few high-tech companies have taken the redundant acronym to the extreme: for example, ISM Information Systems Management Corp. and SHL Systemhouse Ltd. Examples in entertainment include the television shows \"\" and \"Navy: NCIS\" (\"Navy\" was dropped in the second season), where the redundancy was likely designed to educate new viewers as to what the initials stood for. The same reasoning was in evidence when the Royal Bank of Canada's Canadian operations rebranded to RBC Royal Bank, or when Bank of Montreal rebranded their retail banking subsidiary BMO Bank of Montreal.\n\nAnother common example is \"RAM memory\", which is redundant because \"RAM\" (\"random-access memory\") includes the initial of the word \"memory\". \"PIN\" stands for \"personal identification number\", obviating the second word in \"PIN number\"; in this case its retention may be motivated to avoid ambiguity with the homophonous word \"pin\". Other examples include \"ATM machine,\" \"EAB bank,\" \"CableACE Award,\" \"DC Comics,\" \"HIV virus,\" Microsoft's NT Technology, and the formerly redundant \"SAT test,\" now simply \"SAT Reasoning Test\"). TNN (The Nashville/National Network) also renamed itself \"The New TNN\" for a brief interlude.\n\nSometimes, the initials continue to stand for an expanded meaning, but the original meaning is simply replaced. Some examples:\n\nA \"backronym\" (or \"bacronym\") is a phrase that is constructed \"after the fact\" from a previously existing word. For example, the novelist and critic Anthony Burgess once proposed that the word \"book\" ought to stand for \"Box Of Organized Knowledge.\" A classic real-world example of this is the name of the predecessor to the Apple Macintosh, The Apple Lisa, which was said to refer to \"Local Integrated Software Architecture\", but was actually named after Steve Jobs's daughter, born in 1978.\n\nBackronyms are often times used to comedic effect. An example of creating a backronym for comedic effect would be in naming a group or organization, the name A.C.R.O.N.Y.M stands for (among other things) \"A Clever Regiment Of Nerdy Young Men\".\n\nAcronyms are sometimes contrived, that is, deliberately designed to be especially apt for the thing being named (by having a dual meaning or by borrowing the positive connotations of an existing word). Some examples of contrived acronyms are \"USA PATRIOT\", \"CAN SPAM\", \"CAPTCHA\" and \"ACT UP\". The clothing company French Connection began referring to itself as \"fcuk\", standing for \"French Connection United Kingdom.\" The company then created T-shirts and several advertising campaigns that exploit the acronym's similarity to the taboo word \"fuck.\"\n\nThe US Department of Defense's Defense Advanced Research Projects Agency (DARPA) is known for developing contrived acronyms to name projects, including \"RESURRECT\", \"NIRVANA\", and \"DUDE\". In July 2010, Wired Magazine reported that DARPA announced programs to \"..transform biology from a descriptive to a predictive field of science\" named \"BATMAN\" and \"ROBIN\" for \"Biochronicity and Temporal Mechanisms Arising in Nature\" and \"Robustness of Biologically-Inspired Networks\", a reference to the Batman and Robin comic-book superheroes.\n\nThe short-form names of clinical trials and other scientific studies constitute a large class of acronyms that includes many contrived examples, as well as many with a partial rather than complete correspondence of letters to expansion components. These trials tend to have full names that are accurately descriptive of what the trial is about but are thus also too long to serve practically as names within the syntax of a sentence, so a short name is also developed, which can serve as a syntactically useful handle and also provide at least a degree of mnemonic reminder as to the full name. Examples widely known in medicine include the ALLHAT trial (Antihypertensive and Lipid-Lowering Treatment to Prevent Heart Attack Trial) and the CHARM trial (Candesartan in Heart Failure: Assessment of Reduction in Mortality and Morbidity). The fact that RAS syndrome is often involved, as well as that the letters often don't entirely match, have sometimes been pointed out by annoyed researchers preoccupied by the idea that because the archetypal form of acronyms originated with one-to-one letter matching, there must be some moral impropriety in their ever deviating from that form. However, the raison d'être of clinical trial acronyms, as with gene and protein symbols, is simply to have a syntactically usable and recallable short name to complement the long name that is often syntactically unusable and not memorized. It is useful for the short name to give a reminder of the long name, which supports the reasonable censure of \"cutesy\" examples that provide little to no hint of it. But beyond that reasonably close correspondence, the short name's chief utility is in functioning cognitively as a name, rather than being a cryptic and forgettable string, albeit faithful to the matching of letters. However, other reasonable critiques have been (1) that it is irresponsible to mention trial acronyms without explaining them at least once by providing the long names somewhere in the document, and (2) that the proliferation of trial acronyms has resulted in ambiguity, such as 3 different trials all called ASPECT, which is another reason why failing to explain them somewhere in the document is irresponsible in scientific communication. At least one study has evaluated the citation impact and other traits of acronym-named trials compared with others, finding both good aspects (mnemonic help, name recall) and potential flaws (connotatively driven bias).\n\nSome acronyms are chosen deliberately to avoid a name considered undesirable: For example, \"Verliebt in Berlin\" (ViB), a German telenovela, was first intended to be \"Alles nur aus Liebe (All for Love)\", but was changed to avoid the resultant acronym \"ANAL\". Likewise, the Computer Literacy and Internet Technology qualification is known as \"CLaIT\", rather than \"CLIT\". In Canada, the Canadian Conservative Reform Alliance (Party) was quickly renamed to the Canadian Reform Conservative Alliance when its opponents pointed out that its initials spelled CCRAP (pronounced \"see crap\"). (The satirical magazine \"Frank\" had proposed alternatives to CCRAP, namely SSHIT and NSDAP.) Two Irish Institutes of Technology (Galway and Tralee) chose different acronyms from other institutes when they were upgraded from Regional Technical colleges. Tralee RTC became the Institute of Technology Tralee (ITT), as opposed to Tralee Institute of Technology (TIT). Galway RTC became Galway-Mayo Institute of Technology (GMIT), as opposed to Galway Institute of Technology (GIT). The charity sports organization Team in Training is known as \"TNT\" and not \"TIT\". Technological Institute of Textile & Sciences is still known as TITS. George Mason University was planning to name their law school the Antonin Scalia School of Law (ASSOL) in honor of the late Antonin Scalia, only to change it to the Antonin Scalia Law School later.\n\nA macronym, or nested acronym, is an acronym in which one or more letters stand for acronyms themselves. The word \"macronym\" is a portmanteau of \"macro-\" and \"acronym\".\n\nSome examples of macronyms are: \n\nSome macronyms can be multiply nested: the second-order acronym points to another one further down a hierarchy. In an informal competition run by the magazine \"New Scientist\", a fully documented specimen was discovered that may be the most deeply nested of all: RARS is the \"Regional ATOVS Retransmission Service\"; ATOVS is \"Advanced TOVS\"; TOVS is \"TIROS operational vertical sounder\"; and TIROS is \"Television infrared observational satellite\". Fully expanded, \"RARS\" might thus become \"Regional Advanced Television Infrared Observational Satellite Operational Vertical Sounder Retransmission Service\". However, to say that \"RARS\" stands directly for that string of words, or can be interchanged with it in syntax (in the same way that \"CHF\" can be usefully interchanged with \"congestive heart failure\"), is a prescriptive misapprehension rather than a linguistically accurate description; the true nature of such a term is closer to anacronymic than to being interchangeable like simpler acronyms are. The latter are fully reducible in an attempt to \"spell everything out and avoid all abbreviations,\" but the former are irreducible in that respect; they can be annotated with parenthetical explanations, but they cannot be eliminated from speech or writing in any useful or practical way. Just as the words \"laser\" and \"radar\" function as words in syntax and cognition without a need to focus on their acronymic origins, terms such as \"RARS\" and \"CHA2DS2–VASc score\" are irreducible in natural language; if they are purged, the form of language that is left may conform to some imposed rule, but it cannot be described as remaining natural. Similarly, protein and gene nomenclature, which uses symbols extensively, includes such terms as the name of the NACHT protein domain, which reflects the symbols of some proteins that contain the domain—NAIP (NLR family apoptosis inhibitor protein), C2TA (major histocompatibility complex class II transcription activator), HET-E (incompatibility locus protein from \"Podospora anserine\"), and TP1 (telomerase-associated protein)—but is not syntactically reducible to them. The name is thus itself more symbol than acronym, and its expansion cannot replace it while preserving its function in natural syntax as a name within a clause clearly parsable by human readers or listeners.\n\nA special type of macronym, the recursive acronym, has letters whose expansion refers back to the macronym itself. One of the earliest examples appears in \"The Hacker's Dictionary\" as MUNG, which stands for \"MUNG Until No Good\".\n\nSome examples of recursive acronyms are:\n\nIn English language discussions of languages with syllabic or logographic writing systems (such as Chinese, Japanese, and Korean), \"acronyms\" describe the short forms that take selected characters from a multi-character word.\n\nFor example, in Chinese, \"university\" (/, literally \"great learning\") is usually abbreviated simply as (\"great\") when used with the name of the institute. So \"Peking University\" () is commonly shortened to ( \"north-great\") by also only taking the first character of \"Peking\", the \"northern capital\" (). In some cases, however, other characters than the first can be selected. For example, the local short form of \"Hong Kong University\" () uses \"Kong\" () rather than \"Hong\".\n\nThere are also cases where some longer phrases are abbreviated drastically, especially in Chinese politics, where proper nouns were initially translated from Soviet Leninist terms. For instance, the full name of China's highest ruling council, the Politburo Standing Committee (PSC), is \"Standing Committee of the Central Political Bureau of the Communist Party of China\" (中国共产党中央政治局常务委员会). The term then reduced the \"Communist Party of China\" part of its name through acronyms, then the \"Standing Committee\" part, again through acronyms, to create \"中共中央政治局常委\". Alternatively, it omitted the \"Communist Party\" part altogether, creating \"Politburo Standing Committee\" (政治局常委会), and eventually just \"Standing Committee\" (常委会). The PSC's members full designations are \"Member of the Standing Committee of the Central Political Bureau of the Communist Party of China\" (中国共产党中央政治局常务委员会委员); this was eventually drastically reduced to simply \"Changwei\" (常委), with the term \"Ruchang\" (入常) used increasingly for officials destined for a future seat on the PSC. In another example, the word \"全国人民代表大会\" (National People's Congress) can be broken into four parts: \"全国\" = \"the whole nation\", \"人民\" = \"people\", \"代表\" = \"representatives\", \"大会\" = \"conference\". Yet, in its short form \"人大\" (literally \"man/people big\"), only the first characters from the second and the fourth parts are selected; the first part (\"全国\") and the third part (\"代表\") are simply ignored. In describing such abbreviations, the term \"initialism\" is inapplicable.\n\nMany proper nouns become shorter and shorter over time. For example, the CCTV New Year's Gala, whose full name is literally read as \"China Central Television Spring Festival Joint Celebration Evening Gala\" (中国中央电视台春节联欢晚会) was first shortened to \"Spring Festival Joint Celebration Evening Gala\" (春节联欢晚会), but eventually referred to as simply \"Chunwan\" (春晚). Along the same vein, \"Zhongguo Zhongyang Dianshi Tai\" (中国中央电视台) was reduced to \"Yangshi\" (央视) in the mid-2000s.\n\nMany aspects of academics in Korea follow similar acronym patterns as Chinese, owing to the languages' commonalities, like using the word for \"big\" or \"great\" i.e. \"dae\" (), to refer to universities (; \"daehak,\" literally \"great learning\" although \"big school\" is an acceptable alternate). They can be interpreted similar to American university appellations, such as \"UPenn\" or \"Texas Tech.\"\n\nSome acronyms are shortened forms of the school's name, like how Hongik University (, \"Hongik Daehakgyo\") is shortened to \"Hongdae\" (, \"Hong, the big [school]\" or \"Hong-U\") Other acronyms can refer to the university's main subject, e.g. Korea National University of Education (, \"Hanguk Gyowon Daehakgyo\") is shortened to \"Gyowondae\" (교원대, \"Big Ed.\" or \"Ed.-U\"). Other schools use a Koreanized version of their English acronym. The Korea Advanced Institute of Science and Technology (, \"Hanguk Gwahak Gisulwon\") is referred to as KAIST (, \"Kaiseuteu\") in both English and Korean. The 3 most prestigious schools in Korea are known as SKY (스카이, \"seukai\"), combining the first letter of their English names (Seoul National, Korea, and Yonsei Universities). In addition, the College Scholastic Ability Test (, \"Daehak Suhang Neungryeok Siheom\") is shortened to \"Suneung\" (, \"S.A.\").\n\nThe Japanese language makes extensive use of abbreviations, but only some of these are acronyms.\n\nChinese-based words (Sino-Japanese vocabulary) uses similar acronym formation to Chinese, like for . In some cases alternative pronunciations are used, as in Saikyō for 埼京, from , rather than Saitō.\n\nNon-Chinese foreign borrowings (gairaigo) are instead frequently abbreviated as clipped compounds, rather than acronyms, using several initial sounds. This is visible in katakana transcriptions of foreign words, but is also found with native words (written in hiragana). For example, the \"Pokémon\" media franchise's name originally stood for \"pocket monsters\" ( → ), which is still the long-form of the name in Japanese, and \"wāpuro\" stands for \"word processor\" ( → ).\n\nTo a greater degree than English does, German tends toward acronyms that use initial syllables rather than initial single letters, although it uses many of the latter type as well. Some examples of the syllabic type are \"Gestapo\" rather than \"GSP\" (for ', 'secret state police'); ' rather than \"FAK\" (for ', anti-aircraft gun); ' rather than \"KP\" (for ', detective division police). The extension of such contraction to a pervasive or whimsical degree has been mockingly labeled ' (for ', strange habit of abbreviating). Examples of include ' (for ', short in the front, long in the back, i.e., a mullet) and the mocking of Adolf Hitler's title as ' (\"\", Greatest General of all Times).\n\nIt is common to take more than just one initial letter from each of the words composing the acronym; regardless of this, the abbreviation sign gershayim is always written between the second-last and last letters of the non-inflected form of the acronym, even if by this it separates letters of the same original word. Examples (keep in mind Hebrew reads right-to-left): (for , the United States); (for , the Soviet Union); (for , Rishon LeZion); (for , the school). An example that takes only the initial letters from its component words is (\"Tzahal\", for , Israel Defense Forces). In inflected forms the abbreviation sign gershayim remains between the second-last and last letters of the non-inflected form of the acronym (e.g. \"report\", singular: , plural: ; \"squad commander\", masculine: , feminine: ).\n\nThere is also a widespread use of acronyms in Indonesia in every aspect of social life. For example, the \"Golkar\" political party stands for Partai \"Gol\"ongan \"Kar\"ya, \"Monas\" stands for \"\"Mo\"numen \"Nas\"ional\" (National Monument), the \"Angkot\" public transport stands for \"\"Ang\"kutan \"Kot\"a\" (city public transportation), \"warnet\" stands for \"\"war\"ung inter\"net\"\" (internet cafe), and many others. Some acronyms are considered formal (or officially adopted), while many more are considered informal, slang or colloquial.\n\nThe capital's metropolitan area (Jakarta and its surrounding satellite regions), \"Jabodetabek\", is another infamous acronym. This stands for \"Ja\"karta-\"Bo\"gor-\"De\"pok-\"Ta\"ngerang-\"Bek\"asi. Many highways are also named by the acronym method; e.g. \"Jalan Tol\" (Toll Road) \"Jagorawi\" (\"Ja\"karta-Bo\"gor\"-Ci\"awi\") and \"Purbaleunyi\" (\"Pur\"wakarta-\"Ba\"ndung-Ci\"leunyi\"), Joglo Semar (\"Jog\"ja-so\"lo\"-\"semar\"ang).\n\nIn some languages, especially those that use certain alphabets, many acronyms come from the governmental use, particularly in the military and law enforcement services. The Indonesian military (TNI—\"Tentara Nasional Indonesia\") and Indonesian police (POLRI—\"Kepolisian Republik Indonesia\") are infamous for heavy acronyms use. Examples include the \"Kopassus\" (\"Ko\"mando \"Pas\"ukan Khu\"sus\"; Special Forces Command), \"Kopaska\" (\"Ko\"mando \"Pas\"ukan \"Ka\"tak; Frogmen Command), \"Kodim\" (\"Ko\"mando \"Di\"strik \"M\"iliter; Military District Command—one of the Indonesian army's administrative divisions), \"Serka\" (\"Ser\"san \"K\"ep\"a\"la; Head Sergeant), \"Akmil\" (\"Ak\"ademi \"Mil\"iter; Military Academy—in Magelang) and many other terms regarding ranks, units, divisions, procedures, etc.\n\nHeavy acronym use by Indonesians, makes it difficult for foreigners and learners of Bahasa Indonesia to seek information and news in Indonesian media.\n\nAcronyms that use parts of words (not necessarily syllables) are commonplace in Russian as well, e.g. (Gazprom), for (', gas industry). There are also initialisms, such as СМИ (\"SMI\", for ', means of mass informing, i.e. mass media). Another Russian acronym, (GULag) combines two initials and three letters of the final word: it stands for (\", Chief Administration of Camps).\n\nHistorically, OTMA was an acronym sometimes used by the daughters of Emperor Nicholas II of Russia and his consort, Alexandra Feodorovna, as a group nickname for themselves, built from the first letter of each girl's name in the order of their births : Olga, Tatiana, Maria and Anastasia.\n\nIn Swahili, acronyms are common for naming organizations such as TUKI, which stands for \" (the Institute for Swahili Research). Multiple initial letters (often the initial syllable of words) are often drawn together, as seen more in some languages than others.\n\nIn Vietnamese, which has an abundance of compound words, initialisms are very commonly used for both proper and common nouns. Examples include \"TP.HCM\" (', Ho Chi Minh City), \"THPT\" (', high school), \"CLB\" (', club), \"CSDL\" (', database), \"NXB\" (', publisher), \"ÔBACE\" (', a general form of address), and \"CTTĐVN\" (', Vietnamese Martyrs). Longer examples include \"CHXHCNVN\" (', Socialist Republic of Vietnam) and \"MTDTGPMNVN\" (\"\", Viet Cong). Long initialisms have become widespread in legal contexts in Vietnam. It is also common for a writer to coin an ad-hoc initialism for repeated use in an article.\n\nEach letter in an initialism corresponds to one morpheme—that is, one syllable. When the first letter of a syllable has a tone mark or other diacritic, the diacritic may be omitted from the initialism, for example \"ĐNA\" or \"ĐNÁ\" for \"\" (Southeast Asia) and \"LMCA\" or \"LMCÂ\" for \"Liên minh châu Âu\" (European Union). The letter \"Ư\" is often replaced by \"W\" in initialisms to avoid confusion with \"U\", for example \"UBTWMTTQVN\" or \"UBTƯMTTQVN\" for \"Ủy ban Trung ương Mặt trận Tổ quốc Việt Nam\" (Central Committee of the Vietnamese Fatherland Front).\n\nInitialisms are purely a written convenience, being pronounced the same way as their expansions. As the names of many Vietnamese letters are disyllabic, it would be less convenient to pronounce an initialism by its individual letters. Acronyms pronounced as words are rare in Vietnamese, occurring when an acronym itself is borrowed from another language. Examples include ' (), a respelling of the French acronym \"SIDA\" (AIDS); ' (), a literal reading of the English initialism for Voice of America; and \"NASA\" (), borrowed directly from the English acronym.\n\nAs in Chinese, many compound words can be shortened to the first syllable when forming a longer word. For example, the term Việt Cộng is derived from the first syllables of \"Việt Nam\" (Vietnam) and \"Cộng sản\" (communist). This mechanism is limited to Sino-Vietnamese vocabulary. Unlike with Chinese, such shortened words are considered portmanteau words or blend words rather than acronyms or initialisms, because the Vietnamese alphabet still requires each component word to be written as more than one character.\n\nIn languages where nouns are declined, various methods are used. An example is Finnish, where a colon is used to separate inflection from the letters:\nThe process above is similar to how, in English, hyphens are used for clarity when prefixes are added to acronyms, thus \"pre-NATO policy\" (rather than \"preNATO\").\n\nIn languages such as Scottish Gaelic and Irish, where lenition (initial consonant mutation) is commonplace, acronyms must also be modified in situations where case and context dictate it. In the case of Scottish Gaelic, a lower case \"h\" is added after the initial consonant; for example, \"BBC Scotland\" in the genitive case would be written as \"BhBC Alba\", with the acronym pronounced \"VBC\". Likewise, the Gaelic acronym for \"television\" (\"gd: telebhisean\") is \"TBh\", pronounced \"TV\", as in English.\n\n\n\n"}
{"id": "1230569", "url": "https://en.wikipedia.org/wiki?curid=1230569", "title": "Acronym Finder", "text": "Acronym Finder\n\nAcronym Finder (AF) is a free online searchable dictionary and database of abbreviations (acronyms, initialisms, and others) and their meanings.\n\nThe entries are classified into categories such as \"Information Technology, Military/Government, Science, Slang/Pop Culture\" etc. It also contains a database of US and Canadian postal codes. For abbreviations with multiple meanings they are listed by popularity with the most common one being listed first. it claims to have over a million \"human-edited\" and verified definitions.\n\nAcronym Finder was registered and the database put online by Michael K. Molloy of Colorado in 1997 but he began compiling it in 1985 working as a computer systems officer for the USAF. Molloy first saw the need of an acronym list while integrating computers at the Randolph Air Force Base in Texas His first acronym list running up-to 30 pages. When he had retired and put AF online in 1997, his list already had 43,000 acronyms. It began mainly as a list of Military/Government abbreviations before expanding to other areas.\n\nMolloy and his wife served as the editors of the website verifying user submissions for abbreviations and adding others they found to the database. Molloy has also provided opinions on abbreviations such as \"MSG\" which Madison Square Garden wanted as a domain name (\"msg.com\") claiming trademark to the abbreviated letters. He stated that MSG also stood for more common things such as monosodium glutamate and message among others. The Garden in the end settled out of court and came to own msg.com.\n\nThe website was maintained under Mountain Data Systems, LLC by Molloy before being sold off and eventually coming under the ownership of Farlex, Inc. publishers of Thefreedictionary.com.\n\nThe website contains a database of meanings and expansions for abbreviations, acronyms, initialisms mainly in English but includes some entries in other languages such as French, German, Spanish etc. as well. It is freely accessible. The entries are further classified into categories such as \"Information Technology, Military/Government, Science, Slang/Pop Culture\" etc. It also contains a database of US and Canadian postal codes which are shown on a Map along with location information. Abbreviations with multiple expansions are listed by popularity with the most common one being presented first, these can be sorted alphabetically as well.\n\nAnyone can contribute to the database by submitting abbreviations and their meanings, these are reviewed by an by editor and categorized before being added to the database. While the database has been described as fairly accurate errors have been found in the meanings and expansions of abbreviations. The website does not list sources for the abbreviations and their meanings but it does identify people who have contributed more than 50 abbreviations to the database.\n\nThe database only contains abbreviations and their expansions and does not list other data such as grammatical category, context, source, field of the abbreviation etc.\n\nFarlex, Inc. the current owner of the website also publishes mobile apps for the Android and iOS operating systems.\n\nAcronym Finder also includes a \"Systematic Buzz Phrase Projector\", a light-hearted tool that randomly generates jargon-like phrases and abbreviations — usually initialisms that would be unpronounceable as acronyms — and meanings from 30 cleverly chosen buzz words.\n\nThe website is supported through advertisements.\n\nThe website is listed as a quick reference tool in directories like Stanford Library, Library of Congress, USC Library. It has been cited as the largest database of acronyms and has been used in computational studies for its database.\n\nListings of abbreviations on the website have also been used as a defense that an abbreviation is in public use and cannot be trademarked. While in some trademark cases citations for AF have been accepted it has been described as an unreliable reference in others.\n\nIt has garnered criticism for the fact that anyone can submit abbreviations to the site and the content is user generated. Mike Molloy the site's original owner had defended that each submission is verified before being added to the database.\n\n"}
{"id": "20831654", "url": "https://en.wikipedia.org/wiki?curid=20831654", "title": "Alan Palmer", "text": "Alan Palmer\n\nAlan Warwick Palmer (born 1926) is a British author of historical and biographical books.\n\nPalmer was educated at Bancroft's School, Woodford Green, London, and Oriel College, Oxford. He spent 19 years as senior history teacher at Highgate School before becoming a full-time writer and researcher. His late wife, Veronica Palmer collaborated on several of his books.\n\nHe was elected a Fellow of the Royal Society of Literature in 1980.\n\n\n\n"}
{"id": "1837830", "url": "https://en.wikipedia.org/wiki?curid=1837830", "title": "Apocope", "text": "Apocope\n\nIn phonology, apocope () is the loss (elision) of one or more sounds from the end of a word, especially the loss of an unstressed vowel.\n\n\"Apocope\" comes from the Greek () from () \"cutting off\", from () \"away from\" and () \"to cut\".\n\nIn historical linguistics, \"apocope\" is often the loss of an unstressed vowel.\n\n\n\nIn Estonian and the Sami languages, apocopes explain the forms of grammatical cases. For example, a nominative is described as having apocope of the final vowel, but the genitive does not. Throughout its history, however, the genitive case marker has also undergone apocope: Estonian (\"a city\") and (\"of a city\") are derived from and respectively, as can still be seen in the corresponding Finnish word. In the genitive form, the final , while it was being deleted, blocked the loss of . In colloquial Finnish, the final vowel is sometimes omitted from case markers.\n\nSome languages have apocopations that are internalized as mandatory forms. In Spanish and Italian, for example, some adjectives that come before the noun lose the final vowel or syllable if they precede a noun (mainly) in the masculine singular form. In Spanish, some adverbs and cardinal and ordinal numbers have apocopations as well.\n\n\nVarious numerous sorts of informal abbreviations might be classed as apocope:\n\nFor a list of similar apocopations in the English language, see List of English apocopations.\n\nDiminutives in Australian English lists many apocopations.\n\nThe process is also linguistically subsumed under one called \"clipping\", or \"truncation\".\n\n\n\n"}
{"id": "3556316", "url": "https://en.wikipedia.org/wiki?curid=3556316", "title": "Apples and oranges", "text": "Apples and oranges\n\nA comparison of apples and oranges occurs when two items or groups of items are compared that cannot be practically compared.\n\nThe idiom, \"comparing apples and oranges\", refers to the apparent differences between items which are popularly thought to be incomparable or incommensurable, such as apples and oranges. The idiom may also be used to indicate that a false analogy has been made between two items, such as where an \"apple\" is faulted for not being a good \"orange\".\n\nThe idiom is not unique to English. In Quebec French, it may take the form \"comparer des pommes avec des oranges\" (to compare apples and oranges), while in European French the idiom says \"comparer des pommes et des poires\" (to compare apples and pears). In Latin American Spanish, it is usually \"comparar papas y boniatos\" (comparing potatoes and sweet potatoes) or commonly for all varieties of Spanish \"comparar peras con manzanas\" (comparing pears and apples). In some other languages the term for 'orange' derives from 'apple', suggesting not only that a direct comparison between the two is possible, but that it is implicitly present in their names. Fruit other than apples and oranges can also be compared; for example, apples and pears are compared in Danish, Dutch, German, Spanish, Swedish, Croatian, Czech, Romanian, Hungarian, Italian, Slovene, Luxembourgish, Serbian, and Turkish. In fact, in the Spanish-speaking world, a common idiom is \"sumar peras con manzanas\", that is, \"to add pears and apples\"; the same thing applies in Italian (\"sommare le mele con le pere\") and Romanian (\"a aduna merele cu perele\"). In Portuguese, the expression is \"comparar laranjas com bananas\" (compare orange to banana). In Czech, the idiom \"míchat jablka s hruškami\" literally means 'to mix apples and pears'.\n\nSome languages use completely different items, such as the Serbian \"Поредити бабе и жабе\" (comparing grandmothers and toads), or the Romanian \"baba şi mitraliera\" (the grandmother and the machine gun); \"vaca şi izmenele\" (the cow and the longjohns); or \"țiganul şi carioca\" (the gypsy and the marker), or the Welsh \"mor wahanol â mêl a menyn\" (as different as honey and butter), while some languages compare dissimilar properties of dissimilar items. For example, an equivalent Danish idiom, \"Hvad er højest, Rundetårn eller et tordenskrald?\" means \"What is highest, the Round Tower or a thunderclap?\", referring to the size of the former and the sound of the latter. In Russian, the phrase \"сравнивать тёплое с мягким\" (to compare warm and soft) is used. In Argentina, a common question is \"¿En qué se parecen el amor y el ojo del hacha?\" (What do love and the eye of an axe have in common?) and emphasizes dissimilarity between two subjects; in Colombia, a similar (though more rude) version is common: \"confundir la mierda con la pomada\" (to confuse shit with ointment). In Polish, the expression \"co ma piernik do wiatraka?\" is used, meaning \"What has (is) gingerbread to a windmill?\". In Chinese, a phrase that has the similar meaning is 风马牛不相及 (fēng mǎ niú bù xiāng jí), literally meaning \"horses and cattles won't mate with each other\", and later used to describe things that are totally unrelated and incomparable.\n\nA number of more exaggerated comparisons are sometimes made, in cases in which the speaker believes the two objects being compared are radically different. For example, \"oranges with orangutans\", \"apples with dishwashers\", and so on. In English, different fruits, such as pears, plums, or lemons are sometimes substituted for oranges in this context.\n\nSometimes the two words sound similar, for example, Romanian \"merele cu perele\" (apples and pears) and the Hungarian \"szezont a fazonnal\" (the season with the fashion).\n\nAt least two tongue-in-cheek scientific studies have been conducted on the subject, each of which concluded that apples can be compared with oranges fairly easily and on a low budget and the two fruits are quite similar.\n\nThe first study, conducted by Scott A. Sandford of the NASA Ames Research Center, used infrared spectroscopy to analyze both apples and oranges. The study, which was published in the satirical science magazine \"Annals of Improbable Research\", concluded: \"[...] the comparing apples and oranges defense should no longer be considered valid. This is a somewhat startling revelation. It can be anticipated to have a dramatic effect on the strategies used in arguments and discussions in the future.\"\n\nA second study, written by Stamford Hospital's surgeon-in-chief James Barone and published in the \"British Medical Journal,\" noted that the phrase \"apples and oranges\" was appearing with increasing frequency in the medical literature, with some notable articles comparing \"Desflurane and propofol\" and \"Salmeterol and ipratropium\" with \"apples and oranges\". The study also found that both apples and oranges were sweet, similar in size, weight, and shape, that both are grown in orchards, and both may be eaten, juiced, and so on. The only significant differences found were in terms of seeds (the study used seedless oranges), the involvement of Johnny Appleseed, and color.\n\nThe \"Annals of Improbable Research\" subsequently noted that the \"earlier investigation was done with more depth, more rigour, and, most importantly, more expensive equipment\" than the \"British Medical Journal\" study.\n\nOn April Fools' Day 2014, \"The Economist\" compared worldwide production of apples and oranges from 1983 to 2013, however noted them to be \"unrelated variables\".\n\nWhile references to comparing apples and oranges are often a rhetorical device, references to adding apples and oranges are made in the case of teaching students the proper uses of units. Here, the admonition not to \"add apples and oranges\" refers to the requirement that two quantities with different units may not be combined by addition, although they may always be combined in ratio form by multiplication, so that multiplying ratios of apples and oranges is allowed. Similarly, the concept of this distinction is often used metaphorically in elementary algebra.\n\nThe admonition is really more of a mnemonic, since in general counts of objects have no intrinsic unit and, for example, a number count of apples may be dimensionless or have dimension \"fruit\"; in either of these two cases, apples and oranges may indeed be added.\n\n"}
{"id": "925519", "url": "https://en.wikipedia.org/wiki?curid=925519", "title": "Autogram", "text": "Autogram\n\nAn autogram (Greek: αὐτός = self, γράμμα = letter) is a sentence that describes itself in the sense of providing an inventory of its own characters. They were invented by Lee Sallows, who also coined the word \"autogram\". An essential feature is the use of full cardinal number names such as “one”, “two”, etc., in recording character counts. Autograms are also called ‘self-enumerating’ or ‘self-documenting’ sentences. Often, letter counts only are recorded while punctuation signs are ignored, as in this example:\n\nThe first autogram to be published was composed by Sallows in 1982 and appeared in Douglas Hofstadter's \"Metamagical Themas\" column in \"Scientific American\".\n\nThe task of producing an autogram is perplexing because the object to be described cannot be known until its description is first complete.\n\nA type of autogram that has attracted special interest is the autogramic pangram, a self-enumerating sentence in which every letter of the alphabet occurs at least once. Certain letters do not appear in either of the two autograms above, which are therefore not pangrams. The first ever self-enumerating pangram appeared in a Dutch newspaper and was composed by Rudy Kousbroek. Sallows, who lives in the Netherlands, was challenged by Kousbroek to produce a self-enumerating ‘translation’ of this pangram into English—an impossible-seeming task. This prompted Sallows to construct an electronic Pangram Machine. Eventually the machine succeeded, producing the example below which was published in Scientific American in October 1984:\n\nSallows wondered if one could produce a pangram that counts its letters as percentages of the whole sentence–a particularly difficult task since such percentages usually won't be exact integers. He mentioned the problem to Chris Patuzzo and in late 2015 Patuzzo produced the following solution:\n\nAutograms exist that exhibit extra self-descriptive features. Besides counting each letter, here the total number of letters appearing is also named:\n\nJust as an autogram is a sentence that describes itself, so there exist closed chains of sentences each of which describes its predecessor in the chain. Viewed thus, an autogram is such a chain of length 1. Here follows a chain of length 2:\nA special kind of autogram is the ‘reflexicon’ (short for “reflexive lexicon”), which is a self-descriptive word list that describes its own letter frequencies. The constraints on reflexicons are much tighter than on autograms because the freedom to choose alternative words such as “contains”, “comprises”, “employs”, and so on, is lost. However, a degree of freedom still exists through appending entries to the list that are strictly superfluous.\n\nFor example, “Sixteen e's, six f's, one g, three h's, nine i's, nine n's, five o's, five r's, sixteen s's, five t's, three u's, four v's, one w, four x's” is a reflexicon, but it includes what Sallows calls “dummy text” in the shape of “one g” and “one w”. The latter might equally be replaced with “one #”, where “#” can be any typographical sign not already listed. Sallows has made an extensive computer search and conjectures that there exist but three and only three pure (i.e., no dummy text) English reflexicons.\n\n"}
{"id": "26334944", "url": "https://en.wikipedia.org/wiki?curid=26334944", "title": "Auxiliary sciences of history", "text": "Auxiliary sciences of history\n\nAuxiliary (or ancillary) sciences of history are scholarly disciplines which help evaluate and use historical sources and are seen as auxiliary for historical research. Many of these areas of study, classification and analysis were originally developed between the 16th and 19th centuries by antiquaries, and would then have been regarded as falling under the broad heading of antiquarianism. \"History\" was at that time regarded as a largely literary skill. However, with the spread of the principles of empirical source-based history championed by the Göttingen School of History in the late 18th century and later by Leopold von Ranke from the mid-19th century onwards, they have been increasingly regarded as falling within the skill-set of the trained historian.\n\nAuxiliary sciences of history include, but are not limited to:\n\n"}
{"id": "744504", "url": "https://en.wikipedia.org/wiki?curid=744504", "title": "Circular reference", "text": "Circular reference\n\nA circular reference is a series of references where the last object references the first, resulting in a closed loop.\nA circular reference is not to be confused with the logical fallacy of a circular argument. Although a circular reference will often be unhelpful and reveal no information, such as two entries in a book index referring to each other, it is not necessarily so that a circular reference is of no use. Dictionaries, for instance, must always ultimately be a circular reference since all words in a dictionary are defined in terms of other words, but a dictionary nevertheless remains a useful reference. Sentences containing circular references can still be meaningful;\n\nis circular but not without meaning. Indeed, it can be argued that self-reference is a necessary consequence of Aristotle's Law of non-contradiction, a fundamental philosophical axiom. In this view, without self-reference, logic and mathematics become impossible, or at least, lack usefulness.\n\nCircular references can appear in computer programming when one piece of code requires the result from another, but that code needs the result from the first. For example:\n\nFunction A will show the time the sun last set based on the current date, which it can obtain by calling Function B. Function B will calculate the date based on the number of times the moon has orbited the earth since the last time Function B was called. So, Function B asks Function C just how many times that is. Function C doesn't know, but can figure it out by calling Function A to get the time the sun last set.\n\nThe entire set of functions is now worthless because none of them can return any useful information whatsoever. This leads to what is technically known as a livelock. It also appears in spreadsheets when two cells require each other's result. For example, if the value in Cell A1 is to be obtained by adding 5 to the value in Cell B1, and the value in Cell B1 is to be obtained by adding 3 to the value in Cell A1, no values can be computed. (Even if the specifications are A1:=B1+5 and B1:=A1-5, there is still a circular reference. It doesn't help that, for instance, A1=3 and B1=-2 would satisfy both formulae, as there are infinitely many other possible values of A1 and B1 that can satisfy both instances.)\n\nA circular reference represents a big problem in computing.\n\nIn ISO Standard SQL circular integrity constraints are implicitly supported within a single table. Between multiple tables circular constraints (e.g. foreign keys) are permitted by defining the constraints as deferrable (See CREATE TABLE for PostgreSQL and DEFERRABLE Constraint Examples for Oracle). In that case the constraint is checked at the end of the transaction not at the time the DML statement is executed. To update a circular reference two statements can be issued in a single transaction that will satisfy both references once the transaction is committed.\n\nA distinction should be made with processes containing a circular reference between those that are incomputable and those that are an iterative calculation with a final output. The latter may fail in spreadsheets not equipped to handle them but are nevertheless still logically valid.\n\nCircular reference in worksheets can be a very useful technique for solving implicit equations such as the Colebrook equation and many others, which might otherwise require tedious Newton-Raphson algorithms in VBA or use of macros.\n\n"}
{"id": "16264661", "url": "https://en.wikipedia.org/wiki?curid=16264661", "title": "Comparative case", "text": "Comparative case\n\nThe comparative case (abbreviated ) is a grammatical case used in languages such as Mari and Chechen to mark a likeness to something. \n\nIt is not to be confused with the comparative degree, a much more widely used paradigm used to signify heightening of adjectives and adverbs.\n\nIn Mari, the comparative case is marked with the suffix -ла ('-la') For example, if something were to taste like fish (кол - 'kol'), the form used would be колла - 'kolla'). It is also used in regard to languages, when denoting the language a person is speaking, writing, or hearing. Then, however, the accentuation varies slightly from the standard case. Usually, the suffix is not stressed. When it is used with languages, however, it is stressed.\n\nIn Chechen, it is marked with the suffix \"-l\". For example, \"sha\" is 'ice', \"shiila\" is 'cold', and \"shal shiila\" is 'cold as ice'.\n\n"}
{"id": "983601", "url": "https://en.wikipedia.org/wiki?curid=983601", "title": "Comparative genomic hybridization", "text": "Comparative genomic hybridization\n\nComparative genomic hybridization is a molecular cytogenetic method for analysing copy number variations (CNVs) relative to ploidy level in the DNA of a test sample compared to a reference sample, without the need for culturing cells. The aim of this technique is to quickly and efficiently compare two genomic DNA samples arising from two sources, which are most often closely related, because it is suspected that they contain differences in terms of either gains or losses of either whole chromosomes or subchromosomal regions (a portion of a whole chromosome). This technique was originally developed for the evaluation of the differences between the chromosomal complements of solid tumor and normal tissue, and has an improved resolution of 5–10 megabases compared to the more traditional cytogenetic analysis techniques of giemsa banding and fluorescence in situ hybridization (FISH) which are limited by the resolution of the microscope utilized.\n\nThis is achieved through the use of competitive fluorescence in situ hybridization. In short, this involves the isolation of DNA from the two sources to be compared, most commonly a test and reference source, independent labelling of each DNA sample with fluorophores (fluorescent molecules) of different colours (usually red and green), denaturation of the DNA so that it is single stranded, and the hybridization of the two resultant samples in a 1:1 ratio to a normal metaphase spread of chromosomes, to which the labelled DNA samples will bind at their locus of origin. Using a fluorescence microscope and computer software, the differentially coloured fluorescent signals are then compared along the length of each chromosome for identification of chromosomal differences between the two sources. A higher intensity of the test sample colour in a specific region of a chromosome indicates the gain of material of that region in the corresponding source sample, while a higher intensity of the reference sample colour indicates the loss of material in the test sample in that specific region. A neutral colour (yellow when the fluorophore labels are red and green) indicates no difference between the two samples in that location.\n\nCGH is only able to detect unbalanced chromosomal abnormalities. This is because balanced chromosomal abnormalities such as reciprocal translocations, inversions or ring chromosomes do not affect copy number, which is what is detected by CGH technologies. CGH does, however, allow for the exploration of all 46 human chromosomes in single test and the discovery of deletions and duplications, even on the microscopic scale which may lead to the identification of candidate genes to be further explored by other cytological techniques.\n\nThrough the use of DNA microarrays in conjunction with CGH techniques, the more specific form of array CGH (aCGH) has been developed, allowing for a locus-by-locus measure of CNV with increased resolution as low as 100 kilobases. This improved technique allows for the aetiology of known and unknown conditions to be discovered.\n\nThe motivation underlying the development of CGH stemmed from the fact that the available forms of cytogenetic analysis at the time (giemsa banding and FISH) were limited in their potential resolution by the microscopes necessary for interpretation of the results they provided. Furthermore, giemsa banding interpretation has the potential to be ambiguous and therefore has lowered reliability, and both techniques require high labour inputs which limits the loci which may be examined.\n\nThe first report of CGH analysis was by Kallioniemi and colleagues in 1992 at the University of California, San Francisco, who utilised CGH in the analysis of solid tumors. They achieved this by the direct application of the technique to both breast cancer cell lines and primary bladder tumors in order to establish complete copy number karyotypes for the cells. They were able to identify 16 different regions of amplification, many of which were novel discoveries.\n\nSoon after in 1993, du Manoir et al. reported virtually the same methodology. The authors painted a series of individual human chromosomes from a DNA library with two different fluorophores in different proportions to test the technique, and also applied CGH to genomic DNA from patients affected with either Downs syndrome or T-cell prolymphocytic leukemia as well as cells of a renal papillary carcinoma cell line. It was concluded that the fluorescence ratios obtained were accurate and that differences between genomic DNA from different cell types were detectable, and therefore that CGH was a highly useful cytogenetic analysis tool.\n\nInitially, the widespread use of CGH technology was difficult, as protocols were not uniform and therefore inconsistencies arose, especially due to uncertainties in the interpretation of data. However, in 1994 a review was published which described an easily understood protocol in detail and the image analysis software was made available commercially, which allowed CGH to be utilised all around the world.\nAs new techniques such as microdissection and degenerate oligonucleotide primed polymerase chain reaction (DOP-PCR) became available for the generation of DNA products, it was possible to apply the concept of CGH to smaller chromosomal abnormalities, and thus the resolution of CGH was improved.\n\nThe implementation of array CGH, whereby DNA microarrays are used instead of the traditional metaphase chromosome preparation, was pioneered by Solinas-Tolodo et al. in 1997 using tumor cells and Pinkel et al. in 1998 by use of breast cancer cells. This was made possible by the Human Genome Project which generated a library of cloned DNA fragments with known locations throughout the human genome, with these fragments being used as probes on the DNA microarray. Now probes of various origins such as cDNA, genomic PCR products and bacterial artificial chromosomes (BACs) can be used on DNA microarrays which may contain up to 2 million probes. Array CGH is automated, allows greater resolution (down to 100 kb) than traditional CGH as the probes are far smaller than metaphase preparations, requires smaller amounts of DNA, can be targeted to specific chromosomal regions if required and is ordered and therefore faster to analyse, making it far more adaptable to diagnostic uses.\n\nThe DNA on the slide is a reference sample, and is thus obtained from a karyotypically normal man or woman, though it is preferential to use female DNA as they possess two X chromosomes which contain far more genetic information than the male Y chromosome. Phytohaemagglutinin stimulated peripheral blood lymphocytes are used. 1mL of heparinised blood is added to 10ml of culture medium and incubated for 72 hours at 37 °C in an atmosphere of 5% CO. Colchicine is added to arrest the cells in mitosis, the cells are then harvested and treated with hypotonic potassium chloride and fixed in 3:1 methanol/acetic acid.\n\nOne drop of the cell suspension should then be dropped onto an ethanol cleaned slide from a distance of about 30 cm, optimally this should be carried out at room temperature at humidity levels of 60–70%. Slides should be evaluated by visualisation using a phase contrast microscope, minimal cytoplasm should be observed and chromosomes should not be overlapping and be 400–550 bands long with no separated chromatids and finally should appear dark rather than shiny. Slides then need to be air dried overnight at room temperature, and any further storage should be in groups of four at −20 °C with either silica beads or nitrogen present to maintain dryness. Different donors should be tested as hybridization may be variable. Commercially available slides may be used, but should always be tested first.\n\nStandard phenol extraction is used to obtain DNA from test or reference (karyotypically normal individual) tissue, which involves the combination of Tris-Ethylenediaminetetraacetic acid and phenol with aqueous DNA in equal amounts. This is followed by separation by agitation and centrifugation, after which the aqueous layer is removed and further treated using ether and finally ethanol precipitation is used to concentrate the DNA.\n\nMay be completed using DNA isolation kits available commercially which are based on affinity columns.\n\nPreferentially, DNA should be extracted from fresh or frozen tissue as this will be of the highest quality, though it is now possible to use archival material which is formalin fixed or paraffin wax embedded, provided the appropriate procedures are followed. 0.5-1 µg of DNA is sufficient for the CGH experiment, though if the desired amount is not obtained DOP-PCR may be applied to amplify the DNA, however it in this case it is important to apply DOP-PCR to both the test and reference DNA samples to improve reliability.\n\nNick translation is used to label the DNA and involves cutting DNA and substituting nucleotides labelled with fluorophores (direct labelling) or biotin or oxigenin to have fluophore conjugated antibodies added later (indirect labelling). It is then important to check fragment lengths of both test and reference DNA by gel electrophoresis, as they should be within the range of 500kb-1500kb for optimum hybridization.\n\nUnlabelled Life Technologies Corporation's Cot-1 DNA® (placental DNA enriched with repetitive sequences of length 50bp-100bp)is added to block normal repetitive DNA sequences, particularly at centromeres and telomeres, as these sequences, if detected, may reduce the fluorescence ratio and cause gains or losses to escape detection.\n\n8–12µl of each of labelled test and labelled reference DNA are mixed and 40 µg Cot-1 DNA® is added, then precipitated and subsequently dissolved in 6µl of hybridization mix, which contains 50% formamide to decrease DNA melting temperature and 10% dextran sulphate to increase the effective probe concentration in a saline sodium citrate (SSC) solution at a pH of 7.0.\n\nDenaturation of the slide and probes are carried out separately. The slide is submerged in 70% formamide/2xSSC for 5–10 minutes at 72 °C, while the probes are denatured by immersion in a water bath of 80 °C for 10 minutes and are immediately added to the metaphase slide preparation. This reaction is then covered with a coverslip and left for two to four days in a humid chamber at 40 °C.\n\nThe coverslip is then removed and 5 minute washes are applied, three using 2xSSC at room temperature, one at 45 °C with 0.1xSSC and one using TNT at room temperature. The reaction is then preincubated for 10 minutes then followed by a 60-minute, 37 °C incubation, three more 5 minute washes with TNT then one with 2xSSC at room temperature. The slide is then dried using an ethanol series of 70%/96%/100% before counterstaining with DAPI (0.35 μg/ml), for chromosome identification, and sealing with a coverslip.\n\nA fluorescence microscope with the appropriate filters for the DAPI stain as well as the two fluorophores utilised is required for visualisation, and these filters should also minimise the crosstalk between the fluorophores, such as narrow band pass filters. The microscope must provide uniform illumination without chromatic variation, be appropriately aligned and have a “plan” type of objective which is apochromatic and give a magnification of x63 or x100.\n\nThe image should be recorded using a camera with spatial resolution at least 0.1 µm at the specimen level and give an image of at least 600x600 pixels. The camera must also be able to integrate the image for at least 5 to 10 seconds, with a minimum photometric resolution of 8 bit.\n\nDedicated CGH software is commercially available for the image processing step, and is required to subtract background noise, remove and segment materials not of chromosomal origin, normalize the fluorescence ratio, carry out interactive karyotyping and chromosome scaling to standard length. A “relative copy number karyotype” which presents chromosomal areas of deletions or amplifications is generated by averaging the ratios of a number of high quality metaphases and plotting them along an ideogram, a diagram identifying chromosomes based on banding patterns. Interpretation of the ratio profiles is conducted either using fixed or statistical thresholds (confidence intervals). When using confidence intervals, gains or losses are identified when 95% of the fluorescence ratio does not contain 1.0.\n\nExtreme care must be taken to avoid contamination of any step involving DNA, especially with the test DNA as contamination of the sample with normal DNA will skew results closer to 1.0, thus abnormalities may go undetected. FISH, PCR and flow cytometry experiments may be employed to confirm results.\n\nArray comparative genomic hybridization (also microarray-based comparative genomic hybridization, matrix CGH, array CGH, aCGH) is a molecular cytogenetic technique for the detection of chromosomal copy number changes on a genome wide and high-resolution scale. Array CGH compares the patient's genome against a reference genome and identifies differences between the two genomes, and hence locates regions of genomic imbalances in the patient, utilizing the same principles of competitive fluorescence in situ hybridization as traditional CGH.\n\nWith the introduction of array CGH, the main limitation of conventional CGH, a low resolution, is overcome. In array CGH, the metaphase chromosomes are replaced by cloned DNA fragments (+100–200 kb) of which the exact chromosomal location is known. This allows the detection of aberrations in more detail and, moreover, makes it possible to map the changes directly onto the genomic sequence.\n\nArray CGH has proven to be a specific, sensitive, fast and highthroughput technique, with considerable advantages compared to other methods used for the analysis of DNA copy number changes making it more amenable to diagnostic applications. Using this method, copy number changes at a level of 5–10 kilobases of DNA sequences can be detected. , even high-resolution CGH (HR-CGH) arrays are accurate to detect structural variations (SV) at resolution of 200 bp. This method allows one to identify new recurrent chromosome changes such as microdeletions and duplications in human conditions such as cancer and birth defects due to chromosome aberrations.\n\nArray CGH is based on the same principle as conventional CGH. In both techniques, DNA from a reference (or control) sample and DNA from a test (or patient) sample are differentially labelled with two different fluorophores and used as probes that are cohybridized competitively onto nucleic acid targets. In conventional CGH, the target is a reference metaphase spread. In array CGH, these targets can be genomic fragments cloned in a variety of vectors (such as BACs or plasmids), cDNAs, or oligonucleotides.\n\nFigure 2. is a schematic overview of the array CGH technique. DNA from the sample to be tested is labeled with a red fluorophore (Cyanine 5) and a reference DNA sample is labeled with green fluorophore (Cyanine 3). Equal quantities of the two DNA samples are mixed and cohybridized to a DNA microarray of several thousand evenly spaced cloned DNA fragments or oligonucleotides, which have been spotted in triplicate on the array. After hybridization, digital imaging systems are used to capture and quantify the relative fluorescence intensities of each of the hybridized fluorophores. The resulting ratio of the fluorescence intensities is proportional to the ratio of the copy numbers of DNA sequences in the test and reference genomes. If the intensities of the flurochromes are equal on one probe, this region of the patient's genome is interpreted as having equal quantity of DNA in the test and reference samples; if there is an altered Cy3:Cy5 ratio this indicates a loss or a gain of the patient DNA at that specific genomic region.\n\nArray CGH has been implemented using a wide variety of techniques. Therefore, some of the advantages and limitations of array CGH are dependent on the technique chosen.\nThe initial approaches used arrays produced from large insert genomic DNA clones, such as BACs. The use of BACs provides sufficient intense signals to detect single-copy changes and to locate aberration boundaries accurately. However, initial DNA yields of isolated BAC clones are low and DNA amplification techniques are necessary. These techniques include ligation-mediated polymerase chain reaction (PCR), degenerate primer PCR using one or several sets of primers, and rolling circle amplification. Arrays can also be constructed using cDNA. These arrays currently yield a high spatial resolution, but the number of cDNAs is limited by the genes that are encoded on the chromosomes, and their sensitivity is low due to cross-hybridization. This results in the inability to detect single copy changes on a genome wide scale. The latest approach is spotting the arrays with short oligonucleotides. The amount of oligos is almost infinite, and the processing is rapid, cost-effective, and easy. Although oligonucleotides do not have the sensitivity to detect single copy changes, averaging of ratios from oligos that map next to each other on the chromosome can compensate for the reduced sensitivity. It is also possible to use arrays which have overlapping probes so that specific breakpoints may be uncovered.\n\nThere are two approaches to the design of microarrays for CGH applications: whole genome and targeted.\n\nWhole genome arrays are designed to cover the entire human genome. They often include clones that provide an extensive coverage across the genome; and arrays that have contiguous coverage, within the limits of the genome. Whole-genome arrays have been constructed mostly for research applications and have proven their outstanding worth in gene discovery. They are also very valuable in screening the genome for DNA gains and losses at an unprecedented resolution.\n\nTargeted arrays are designed for a specific region(s) of the genome for the purpose of evaluating that targeted segment. It may be designed to study a specific chromosome or chromosomal segment or to identify and evaluate specific DNA dosage abnormalities in individuals with suspected microdeletion syndromes or subtelomeric rearrangements. The crucial goal of a targeted microarray in medical practice is to provide clinically useful results for diagnosis, genetic counseling, prognosis, and clinical management of unbalanced cytogenetic abnormalities.\n\nConventional CGH has been used mainly for the identification of chromosomal regions that are recurrently lost or gained in tumors, as well as for the diagnosis and prognosis of cancer. This approach can also be used to study chromosomal aberrations in fetal and neonatal genomes. Furthermore, conventional CGH can be used in detecting chromosomal abnormalities and have been shown to be efficient in diagnosing complex abnormalities associated with human genetic disorders.\n\nCGH data from several studies of the same tumor type show consistent patterns of non-random genetic aberrations. Some of these changes appear to be common to various kinds of malignant tumors, while others are more tumor specific. For example, gains of chromosomal regions lq, 3q and 8q, as well as losses of 8p, 13q, 16q and 17p, are common to a number of tumor types, such as breast, ovarian, prostate, renal and bladder cancer (Figure. 3). Other alterations, such as 12p and Xp gains in testicular cancer, 13q gain 9q loss in bladder cancer, 14q loss in renal cancer and Xp loss in ovarian cancer are more specific, and might reflect the unique selection forces operating during cancer development in different organs. Array CGH is also frequently used in research and diagnostics of B cell malignancies, such as chronic lymphocytic leukemia.\n\nCri du Chat (CdC) is a syndrome caused by a partial deletion of the short arm of chromosome 5. Several studies have shown that conventional CGH is suitable to detect the deletion, as well as more complex chromosomal alterations. For example, Levy et al. (2002) reported an infant with a cat-like cry, the hallmark of CdC, but having an indistinct karyotype. CGH analysis revealed a loss of chromosomal material from 5p15.3 confirming the diagnosis clinically. These results demonstrate that conventional CGH is a reliable technique in detecting structural aberrations and, in specific cases, may be more efficient in diagnosing complex abnormalities.\n\nArray CGH applications are mainly directed at detecting genomic abnormalities in cancer. However, array CGH is also suitable for the analysis of DNA copy number aberrations that cause human genetic disorders. That is, array CGH is employed to uncover deletions, amplifications, breakpoints and ploidy abnormalities. Earlier diagnosis is of benefit to the patient as they may undergo appropriate treatments and counseling to improve their prognosis.\n\nGenetic alterations and rearrangements occur frequently in cancer and contribute to its pathogenesis. Detecting these aberrations by array CGH provides information on the locations of important cancer genes and can have clinical use in diagnosis, cancer classification and prognostification. However, not all of the losses of genetic material are pathogenetic, since some DNA material is physiologically lost during the rearrangement of immunoglobulin subgenes. In a recent study, array CGH has been implemented to identify regions of chromosomal aberration (copy-number variation) in several mouse models of breast cancer, leading to identification of cooperating genes during myc-induced oncogenesis.\n\nArray CGH may also be applied not only to the discovery of chromosomal abnormalities in cancer, but also to the monitoring of the progression of tumors. Differentiation between metastatic and mild lesions is also possible using FISH once the abnormalities have been identified by array CGH.\n\nPrader–Willi syndrome (PWS) is a paternal structural abnormality involving 15q11-13, while a maternal aberration in the same region causes Angelman syndrome (AS). In both syndromes, the majority of cases (75%) are the result of a 3–5 Mb deletion of the PWS/AS critical region. These small aberrations cannot be detected using cytogenetics or conventional CGH, but can be readily detected using array CGH. As a proof of principle Vissers et al. (2003) constructed a genome wide array with a 1 Mb resolution to screen three patients with known, FISH-confirmed microdeletion syndromes, including one with PWS. In all three cases, the abnormalities, ranging from 1.5 to 2.9Mb, were readily identified. Thus, array CGH was demonstrated to be a specific and sensitive approach in detecting submicroscopic aberrations.\n\nWhen using overlapping microarrays, it is also possible to uncover breakpoints involved in chromosomal aberrations.\n\nThough not yet a widely employed technique, the use of array CGH as a tool for preimplantation genetic screening is becoming an increasingly popular concept. It has the potential to detect CNVs and aneuploidy in eggs, sperm or embryos which may contribute to failure of the embryo to successfully implant, miscarriage or conditions such as Down syndrome (trisomy 21). This makes array CGH a promising tool to reduce the incidence of life altering conditions and improve success rates of IVF attempts. The technique involves whole genome amplification from a single cell which is then used in the array CGH method. It may also be used in couples carrying chromosomal translocations such as balanced reciprocal translocations or Robertsonian translocations, which have the potential to cause chromosomal imbalances in their offspring.\n\nA main disadvantage of conventional CGH is its inability to detect structural chromosomal aberrations without copy number changes, such as mosaicism, balanced chromosomal translocations, and inversions. CGH can also only detect gains and losses relative to the ploidy level. In addition, chromosomal regions with short repetitive DNA sequences are highly variable between individuals and can interfere with CGH analysis. Therefore, repetitive DNA regions like centromeres and telomeres need to be blocked with unlabeled repetitive DNA (e.g. Cot1 DNA) and/or can be omitted from screening. Furthermore, the resolution of conventional CGH is a major practical problem that limits its clinical applications. Although CGH has proven to be a useful and reliable technique in the research and diagnostics of both cancer and human genetic disorders, the applications involve only gross abnormalities. Because of the limited resolution of metaphase chromosomes, aberrations smaller than 5–10 Mb cannot be detected using conventional CGH.\nFor the detection of such abnormalities, a high-resolution technique is required.\nArray CGH overcomes many of these limitations. Array CGH is characterized by a high resolution, its major advantage with respect to conventional CGH. The standard resolution varies between 1 and 5 Mb, but can be increased up to approximately 40 kb by supplementing the array with extra clones. However, as in conventional CGH, the main disadvantage of array CGH is its inability to detect aberrations that do not result in copy number changes and is limited in its ability to detect mosaicism. The level of mosaicism that can be detected is dependent on the sensitivity and spatial resolution of the clones. At present, rearrangements present in approximately 50% of the cells is the detection limit. For the detection of such abnormalities, other techniques, such as SKY (Spectral karyotyping) or FISH have to still be used.\n\n\n"}
{"id": "917868", "url": "https://en.wikipedia.org/wiki?curid=917868", "title": "Comparative genomics", "text": "Comparative genomics\n\nComparative genomics is a field of biological research in which the genomic features of different organisms are compared. The genomic features may include the DNA sequence, genes, gene order, regulatory sequences, and other genomic structural landmarks. In this branch of genomics, whole or large parts of genomes resulting from genome projects are compared to study basic biological similarities and differences as well as evolutionary relationships between organisms. The major principle of comparative genomics is that common features of two organisms will often be encoded within the DNA that is evolutionarily conserved between them. Therefore, comparative genomic approaches start with making some form of alignment of genome sequences and looking for orthologous sequences (sequences that share a common ancestry) in the aligned genomes and checking to what extent those sequences are conserved. Based on these, genome and molecular evolution are inferred and this may in turn be put in the context of, for example, phenotypic evolution or population genetics.\n\nVirtually started as soon as the whole genomes of two organisms became available (that is, the genomes of the bacteria \"Haemophilus influenzae\" and \"Mycoplasma genitalium\") in 1995, comparative genomics is now a standard component of the analysis of every new genome sequence. With the explosion in the number of genome projects due to the advancements in DNA sequencing technologies, particularly the next-generation sequencing methods in late 2000s, this field has become more sophisticated, making it possible to deal with many genomes in a single study. Comparative genomics has revealed high levels of similarity between closely related organisms, such as humans and chimpanzees, and, more surprisingly, similarity between seemingly distantly related organisms, such as humans and the yeast \"Saccharomyces cerevisiae\". It has also showed the extreme diversity of the gene\ncomposition in different evolutionary lineages.\n\n\"See also\": History of genomics\n\nComparative genomics has a root in the comparison of virus genomes in the early 1980s. For example, small RNA viruses infecting animals (picornaviruses) and those infecting plants (cowpea mosaic virus) were compared and turned out to share significant sequence similarity and, in part, the order of their genes. In 1986, the first comparative genomic study at a larger scale was published, comparing the genomes of varicella-zoster virus and Epstein-Barr virus that contained more than 100 genes each.\n\nThe first complete genome sequence of a cellular organism, that of \"Haemophilus influenzae\" Rd, was published in 1995. The second genome sequencing paper was of the small parasitic bacterium \"Mycoplasma genitalium\" published in the same year. Starting from this paper, reports on new genomes inevitably became comparative-genomic studies.\n\nThe first high-resolution whole genome comparison system was developed in 1998 by Art Delcher, Simon Kasif and Steven Salzberg and applied to the comparison of entire highly related microbial organisms with their collaborators at the Institute for Genomic Research (TIGR). The system is called MUMMER and was described in a publication in Nucleic Acids Research in 1999. The system helps researchers to identify large rearrangements, single base mutations, reversals, tandem repeat expansions and other polymorphisms. In bacteria, MUMMER enables the identification of polymorphisms that are responsible for virulence, pathogenicity, and anti-biotic resistance. The system was also applied to the Minimal Organism Project at TIGR and subsequently to many other comparative genomics projects.\n\n\"Saccharomyces cerevisiae\", the baker's yeast, was the first eukaryote to have its complete genome sequence published in 1996. After the publication of the roundworm \"Caenorhabditis elegans\" genome in 1998 and together with the fruit fly \"Drosophila melanogaster\" genome in 2000, Gerald M. Rubin and his team published a paper titled \"Comparative Genomics of the Eukaryotes\", in which they compared the genomes of the eukaryotes \"D. melanogaster\", \"C. elegans\", and \"S. cerevisiae\", as well as the prokaryote \"H. influenzae\". At the same time, Bonnie Berger, Eric Lander, and their team published a paper on whole-genome comparison of human and mouse.\n\nWith the publication of the large genomes of vertebrates in the 2000s, including human, the Japanese pufferfish \"Takifugu rubripes\", and mouse, precomputed results of large genome comparisons have been released for downloading or for visualization in a genome browser. Instead of undertaking their own analyses, most biologists can access these large cross-species comparisons and avoid the impracticality caused by the size of the genomes.\n\nNext-generation sequencing methods, which were first introduced in 2007, have produced an enormous amount of genomic data and have allowed researchers to generate multiple (prokaryotic) draft genome sequences at once. These methods can also quickly uncover single-nucleotide polymorphisms, insertions and deletions by mapping unassembled reads against a well annotated reference genome, and thus provide a list of possible gene differences that may be the basis for any functional variation among strains.\n\nOne character of biology is evolution, evolutionary theory is also the theoretical foundation of comparative genomics, and at the same time the results of comparative genomics unprecedentedly enriched and developed the theory of evolution. When two or more of the genome sequence are compared, one can deduce the evolutionary relationships of the sequences in a phylogenetic tree. Based on a variety of biological genome data and the study of vertical and horizontal evolution processes, one can understand vital parts of the gene structure and its regulatory function.\n\nSimilarity of related genomes is the basis of comparative genomics. If two creatures have a recent common ancestor, the differences between the two species genomes are evolved from the ancestors’ genome. The closer the relationship between two organisms, the higher the similarities between their genomes. If there is close relationship between them, then their genome will display a linear behaviour (synteny), namely some or all of the genetic sequences are conserved. Thus, the genome sequences can be used to identify gene function, by analyzing their homology (sequence similarity) to genes of known function.\n\nOrthologous sequences are related sequences in different species: a gene exists in the original species, the species divided into two species, so genes in new species are orthologous to the sequence in the original species. Paralogous sequences are separated by gene cloning (gene duplication): if a particular gene in the genome is copied, then the copy of the two sequences is paralogous to the original gene. A pair of orthologous sequences is called orthologous pairs (orthologs), a pair of paralogous sequence is called collateral pairs (paralogs). Orthologous pairs usually have the same or similar function, which is not necessarily the case for collateral pairs. In collateral pairs, the sequences tend to evolve into having different functions.\nComparative genomics exploits both similarities and differences in the proteins, RNA, and regulatory regions of different organisms to infer how selection has acted upon these elements. Those elements that are responsible for similarities between different species should be conserved through time (stabilizing selection), while those elements responsible for differences among species should be divergent (positive selection). Finally, those elements that are unimportant to the evolutionary success of the organism will be unconserved (selection is neutral).\n\nOne of the important goals of the field is the identification of the mechanisms of eukaryotic genome evolution. It is however often complicated by the multiplicity of events that have taken place throughout the history of individual lineages, leaving only distorted and superimposed traces in the genome of each living organism. For this reason comparative genomics studies of small model organisms (for example the model Caenorhabditis elegans and closely related Caenorhabditis briggsae) are of great importance to advance our understanding of general mechanisms of evolution.\n\nComputational approaches to genome comparison have recently become a common research topic in computer science. A public collection of case studies and demonstrations is growing, ranging from whole genome comparisons to gene expression analysis. This has increased the introduction of different ideas, including concepts from systems and control, information theory, strings analysis and data mining. It is anticipated that computational approaches will become and remain a standard topic for research and teaching, while multiple courses will begin training students to be fluent in both topics.\n\nComputational tools for analyzing sequences and complete genomes are developing quickly due to the availability of large amount of genomic data. At the same time, comparative analysis tools are progressed and improved. In the challenges about these analyses, it is very important to visualize the comparative results.\n\nVisualization of sequence conservation is a tough task of comparative sequence analysis. As we know, it is highly inefficient to examine the alignment of long genomic regions manually. Internet-based genome browsers provide many useful tools for investigating genomic sequences due to integrating all sequence-based biological information on genomic regions. When we extract large amount of relevant biological data, they can be very easy to use and less time-consuming.\n\n\nAn advantage of using online tools is that these websites are being developed and updated constantly. There are many new settings and content can be used online to improve efficiency.\n\nAgriculture is a field that reaps the benefits of comparative genomics. Identifying the loci of advantageous genes is a key step in breeding crops that are optimized for greater yield, cost-efficiency, quality, and disease resistance. For example, one genome wide association study conducted on 517 rice landraces revealed 80 loci associated with several categories of agronomic performance, such as grain weight, amylose content, and drought tolerance. Many of the loci were previously uncharacterized. Not only is this methodology powerful, it is also quick. Previous methods of identifying loci associated with agronomic performance required several generations of carefully monitored breeding of parent strains, a time consuming effort that is unnecessary for comparative genomic studies.\n\nThe medical field also benefits from the study of comparative genomics. Vaccinology in particular has experienced useful advances in technology due to genomic approaches to problems. In an approach known as reverse vaccinology, researchers can discover candidate antigens for vaccine development by analyzing the genome of a pathogen or a family of pathogens. Applying a comparative genomics approach by analyzing the genomes of several related pathogens can lead to the development of vaccines that are multiprotective. A team of researchers employed such an approach to create a universal vaccine for Group B Streptococcus, a group of bacteria responsible for severe neonatal infection. Comparative genomics can also be used to generate specificity for vaccines against pathogens that are closely related to commensal microorganisms. For example, researchers used comparative genomic analysis of commensal and pathogenic strains of E. coli to identify pathogen specific genes as a basis for finding antigens that result in immune response against pathogenic strains but not commensal ones.\n\nComparative genomics also opens up new avenues in other areas of research. As DNA sequencing technology has become more accessible, the number of sequenced genomes has grown. With the increasing reservoir of available genomic data, the potency of comparative genomic inference has grown as well. A notable case of this increased potency is found in recent primate research. Comparative genomic methods have allowed researchers to gather information about genetic variation, differential gene expression, and evolutionary dynamics in primates that were indiscernible using previous data and methods. The Great Ape Genome Project used comparative genomic methods to investigate genetic variation with reference to the six great ape species, finding healthy levels of variation in their gene pool despite shrinking population size. Another study showed that patterns of DNA methylation, which are a known regulation mechanism for gene expression, differ in the prefrontal cortex of humans versus chimps, and implicated this difference in the evolutionary divergence of the two species.\n\n\n\n"}
{"id": "380406", "url": "https://en.wikipedia.org/wiki?curid=380406", "title": "Comparative psychology", "text": "Comparative psychology\n\nComparative psychology refers to the scientific study of the behavior and mental processes of non-human animals, especially as these relate to the phylogenetic history, adaptive significance, and development of behavior. Research in this area addresses many different issues, uses many different methods and explores the behavior of many different species from insects to primates.\n\nComparative psychology is sometimes assumed to emphasize cross-species comparisons, including those between humans and animals. However, some researchers feel that direct comparisons should not be the sole focus of comparative psychology and that intense focus on a single organism to understand its behavior is just as desirable; if not more so. Donald Dewsbury reviewed the works of several psychologists and their definitions and concluded that the object of comparative psychology is to establish principles of generality focusing on both proximate and ultimate causation. \n\nUsing a comparative approach to behavior allows one to evaluate the target behavior from four different, complementary perspectives, developed by Niko Tinbergen. First, one may ask how pervasive the behavior is across species (i.e. how common is the behavior between animal species?). Second, one may ask how the behavior contributes to the lifetime reproductive success of the individuals demonstrating the behavior (i.e. does the behavior result in animals producing more offspring than animals not displaying the behavior)? Theories addressing the ultimate causes of behavior are based on the answers to these two questions.\n\nThird, what mechanisms are involved in the behavior (i.e. what physiological, behavioral, and environmental components are necessary and sufficient for the generation of the behavior)? Fourth, a researcher may ask about the development of the behavior within an individual (i.e. what maturational, learning, social experiences must an individual undergo in order to demonstrate a behavior)? Theories addressing the proximate causes of behavior are based on answers to these two questions. For more details see Tinbergen's four questions.\n\nThe 9th century scholar al-Jahiz wrote works on the social organization and communication methods of animals like ants. The 11th century Arabic writer Ibn al-Haytham (Alhazen) wrote the \"Treatise on the Influence of Melodies on the Souls of Animals\", an early treatise dealing with the effects of music on an imals. In the treatise, he demonstrates how a camel's pace could be hastened or retarded with the use of music, and shows other examples of how music can affect animal behavior, experimenting with horses, birds and reptiles. Through to the 19th century, a majority of scholars in the Western world continued to believe that music was a distinctly human phenomenon, but experiments since then have vindicated Ibn al-Haytham's view that music does indeed have an effect on animals.\n\nCharles Darwin was central in the development of comparative psychology; it is thought that psychology should be spoken in terms of \"pre-\" and \"post-Darwin\" because his contributions were so influential. theory led to several hypotheses, one being that the factors that set humans apart, such as higher mental, moral and spiritual faculties, could be accounted for by evolutionary principles. In response to the vehement opposition to Darwinism was the \"anecdotal movement\" led by George Romanes who set out to demonstrate that animals possessed a \"rudimentary human mind\". Romanes is most famous for two major flaws in his work: his focus on anecdotal observations and entrenched anthropomorphism.\n\nNear the end of the 19th century, several scientists existed whose work was also very influential. Douglas Alexander Spalding was called the \"first experimental biologist\", and worked mostly with birds; studying instinct, imprinting, and visual and auditory development. Jacques Loeb emphasized the importance of objectively studying behavior, Sir John Lubbock is credited with first using mazes and puzzle devices to study learning and Conwy Lloyd Morgan is thought to be \"the first ethologist in the sense in which we presently use the word\".\n\nThroughout the long history of comparative psychology, repeated attempts have been made to enforce a more disciplined approach, in which similar studies are carried out on animals of different species, and the results interpreted in terms of their different phylogenetic or ecological backgrounds. Behavioral ecology in the 1970s gave a more solid base of knowledge against which a true comparative psychology could develop. However, the broader use of the term \"comparative psychology\" is enshrined in the names of learned societies and academic journals, not to mention in the minds of psychologists of other specialisms, so the label of the field is never likely to disappear completely.\n\nA persistent question with which comparative psychologists have been faced is the relative intelligence of different species of animal. Indeed, some early attempts at a genuinely comparative psychology involved evaluating how well animals of different species could learn different tasks. These attempts floundered; in retrospect it can be seen that they were not sufficiently sophisticated, either in their analysis of the demands of different tasks, or in their choice of species to compare. However, the definition of \"intelligence\" in comparative psychology is deeply affected by anthropomorphism, and focuses on simple tasks, complex problems, reversal learning, learning sets, and delayed alternation are plagued with practical and theoretical problems. In the literature, \"intelligence\" is defined as whatever is closest to human performance and neglects behaviors that humans are usually incapable of (e.g. echolocation). Specifically, comparative researchers encounter problems associated with individual differences, differences in motivation, differences in reinforcement, differences in sensory function, differences in motor capacities, and species-typical preparedness (i.e. some species have evolved to acquire some behaviors quicker than other behaviors).\n\nA wide variety of species have been studied by comparative psychologists. However, a small number have dominated the scene. Ivan Pavlov's early work used dogs; although they have been the subject of occasional studies, since then they have not figured prominently. Increasing interest in the study of abnormal animal behavior has led to a return to the study of most kinds of domestic animal. Thorndike began his studies with cats, but American comparative psychologists quickly shifted to the more economical rat, which remained the almost invariable subject for the first half of the 20th century and continues to be used.\n\nSkinner introduced the use of pigeons, and they continue to be important in some fields. There has always been interest in studying various species of primate; important contributions to social and developmental psychology were made by Harry F. Harlow's studies of maternal deprivation in rhesus monkeys. Cross-fostering studies have shown similarities between human infants and infant chimpanzees. Kellogg and Kellogg (1933) aimed to look at heredity and environmental effects of young primates. They found that a cross-fostered chimpanzee named Gua was better at recognizing human smells and clothing and that the Kelloggs' infant (Donald) recognised humans better by their faces. The study ended 9 months after it had begun, after the infant began to imitate the noises of Gua.\n\nNonhuman primates have also been used to show the development of language in comparison with human development. For example, Gardner (1967) successfully taught the female chimpanzee Washoe 350 words in American Sign Language. Washoe subsequently passed on some of this teaching to her adopted offspring, Loulis. A criticism of Washoe's acquisition of sign language focused on the extent to which she actually understood what she was signing. Her signs may have just based on an association to get a reward, such as food or a toy. Other studies concluded that apes do not understand linguistic input, but may form an intended meaning of what is being communicated. All great apes have been reported to have the capacity of allospecific symbolic production.\n\nInterest in primate studies has increased with the rise in studies of animal cognition. Other animals thought to be intelligent have also been increasingly studied. Examples include various species of corvid, parrots — especially the grey parrot — and dolphins. Alex (Avian Learning EXperiment) is a well known case study (1976–2007) which was developed by Pepperberg, who found that the African gray parrot Alex did not only mimic vocalisations but understood the concepts of same and different between objects. The study of non-human mammals has also included the study of dogs. Due to their domestic nature and personalities, dogs have lived closely with humans, and parallels in communication and cognitive behaviours have therefore been recognised and further researched. Joly-Mascheroni and colleagues (2008) demonstrated that dogs may be able to catch human yawns and suggested a level of empathy in dogs, a point that is strongly debated. Pilley and Reid found that a Border Collie named Chaser was able to successfully identify and retrieve 1022 distinct objects/toys.\n\nResearchers who study animal cognition are interested in understanding the mental processes that control complex behavior, and much of their work parallels that of cognitive psychologists working with humans. For example, there is extensive research with animals on attention, categorization, concept formation, memory, spatial cognition, and time estimation. Much research in these and other areas is related directly or indirectly to behaviors important to survival in natural settings, such as navigation, tool use, and numerical competence. Thus, comparative psychology and animal cognition are heavily overlapping research categories.\n\nVeterinary surgeons recognize that the psychological state of a captive or domesticated animal must be taken into account if its behavior and health are to be understood and optimized.\n\nCommon causes of disordered behavior in captive or pet animals are lack of stimulation, inappropriate stimulation, or overstimulation. These conditions can lead to disorders, unpredictable and unwanted behavior, and sometimes even physical symptoms and diseases. For example, rats who are exposed to loud music for a long period will ultimately develop unwanted behaviors that have been compared with human psychosis, like biting their owners.\n\nThe way dogs behave when understimulated is widely believed to depend on the breed as well as on the individual animal's character. For example, huskies have been known to ruin gardens and houses if they are not allowed enough activity. Dogs are also prone to psychological damage if they are subjected to violence. If they are treated very badly, they may become dangerous.\n\nThe systematic study of disordered animal behavior draws on research in comparative psychology, including the early work on conditioning and instrumental learning, but also on ethological studies of natural behavior. However, at least in the case of familiar domestic animals, it also draws on the accumulated experience of those who have worked closely with the animals.\n\nThe relationship between humans and animals has long been of interest to anthropologists as one pathway to an understanding the evolution of human behavior. Similarities between the behavior of humans and animals have sometimes been used in an attempt to understand the evolutionary significance of particular behaviors. Differences in the treatment of animals have been said to reflect a society's understanding of human nature and the place of humans and animals in the scheme of things. Domestication has been of particular interest. For example, it has been argued that, as animals became domesticated, humans treated them as property and began to see them as inferior or fundamentally different from humans.\nIngold remarks that in all societies children have to learn to differentiate and separate themselves from others. In this process, strangers may be seen as \"not people,\" and like animals. Ingold quoted Sigmund Freud: \"Children show no trace of arrogance which urges adult civilized men to draw a hard-and-fast line between their own nature and that of all other animals. Children have no scruples over allowing animals to rank as their full equals.\" With maturity however, humans find it hard to accept that they themselves are animals, so they categorize, separating humans from animals, and animals into wild animals and tame animals, and tame animals into house pets and livestock. Such divisions can be seen as similar to categories of humans: who is part of a human community and someone who isn't, that is, the outsider.\n\n\"The New York Times\" ran an article that showed the psychological benefits of animals, more specifically of children with their pets. It's been proven that having a pet does in fact improve kids' social skills. In the article, Dr. Sue Doescher, a psychologist involved in the study, stated, \"It made the children more cooperative and sharing.\" It was also shown that these kids were more confident with themselves and able to be more empathic with other children.\n\nFurthermore, in an edition of \"Social Science and Medicine\" it was stated, \"A random survey of 339 residents from Perth, Western Australia were selected from three suburbs and interviewed by telephone. Pet ownership was found to be positively associated with some forms of social contact and interaction, and with perceptions of neighborhood friendliness. After adjustment for demographic variables, pet owners scored higher on social capital and civic engagement scales.\" Results like these let us know that owning a pet provides opportunities for neighborly interaction, among many other chances for socialization among people.\n\nNoted comparative psychologists, in this broad sense, include:\n\nMany of these were active in fields other than animal psychology; this is characteristic of comparative psychologists.\n\nFields of psychology and other disciplines that draw upon, or overlap with, comparative psychology include:\n\n\n"}
{"id": "10485277", "url": "https://en.wikipedia.org/wiki?curid=10485277", "title": "Comparison microscope", "text": "Comparison microscope\n\nA comparison microscope is a device used to analyze side-by-side specimens. It consists of two microscopes connected by an optical bridge, which results in a split view window enabling two separate objects to be viewed simultaneously. This avoids the observer having to rely on memory when comparing two objects under a conventional microscope.\n\nIn the 1920s forensic ballistics was waiting at its inception. In 1929, using a comparison microscope adapted for the purpose by Calvin Goddard and his partner Phillip Gravelle used similar techniques to absolve the Chicago Police Department of participation in the St. Valentine's Day Massacre.\n\n Philip O. Gravelle, a chemist, developed a comparison microscope for use in the identification of fired bullets and cartridge cases with the support and guidance of forensic ballistics pioneer Calvin Goddard. It was a significant advance in the science of firearms identification in forensic science. The firearm from which a bullet or cartridge case has been fired is identified by the comparison of the unique striae left on the bullet or cartridge case from the worn, machined metal of the barrel, breach block, extractor, or firing pin in the gun. It was Gravelle who mistrusted his memory. \"As long as he could inspect only one bullet at a time with his microscope, and had to keep the picture of it in his memory until he placed the comparison bullet under the microscope, scientific precision could not be attained. He therefore developed the comparison microscope and Goddard made it work.\" Calvin Goddard perfected the comparison microscope and subsequently popularized its use.Sir Sydney Smith also appreciated the idea, emphasizing its importance in forensic science and firearms identification. He took the comparison microscope to Scotland and introduced it to the European scientists for firearms identification and other forensic science needs.\n\nThe modern instrument has many optical, mechanical and electronic refinements, including fiber optic illumination, video capabilities, digital imaging, automatic exposure for conventional photography, etc. Despite this evolution, however, the basic tools and techniques have remained unchanged which are to determine whether or not ammunition components were fired by a single firearm based on unique and reproducible microscopic and class characteristics, or to reach a \"no conclusion\" result if insufficient marks are present.\n\nSince, ballistic identification has benefited from a long series of structural, scientific and technological advances, law enforcement agencies have established forensic laboratories and researchers have learned much more about how to match bullets and cartridge cases to the guns used to fire them, and comparison microscopes have become more sophisticated. By the end of the 1980s, ballistic identification was an established sub-specialty of forensic science.\n\nVisualization tools have also been developed to allows the firearms examiner to verify the degree of similarity between any two tool-marks in question. These are designed to simulate the operation of the comparison microscope but is capable of rendering a 2D view of the 3D surfaces in a manner similar to that of the conventional comparison microscope.\n\nThe prevalence of hand-gun related crime in the United States compared to most other developed countries provided the impetus for the development of the comparison microscope. As with most firearms, the fired ammunition components may acquire sufficient unique and reproducible microscopic marks to be identifiable as having been fired by a single firearm. Making these comparisons is correctly referred to as firearms identification, or sometimes called as \"ballistics\".\n\nHistorically, and currently, this forensic discipline ultimately requires a microscopic side-by-side comparison of fired bullets or cartridge cases, one pair at a time, by a forensic examiner to confirm or eliminate the two items as having been fired by a single firearm. For this purpose, the traditional tool of the firearms examiner has been what is often called the ballistics comparison microscope.\n\nThe interior of a gun's barrel is machined to have grooves (called rifling) that force the bullet to rotate as it travels along it. These grooves and their counterpart, called \"lands\" imprint groove and land impressions on the surface of the bullet. Together with these land and groove impressions, imperfections on the barrel surface are incidentally transferred to the bullet's surface. Because these imperfections are randomly generated, during manufacture or due to use, they are unique to each barrel. These patterns or imperfections, therefore, amount to a \"signature\" that each barrel imprints on each of the bullets fired through it. It is this \"signature\" on the bullets imparted due to the unique imperfections on the barrel that enable the validation and identification of bullets as having originated from a particular gun. Comparison microscope is used to analyze the matching of the microscopic impressions found on the surface of bullets and casings.\n\nWhen a firearm or a bullet or cartridge case are recovered from a crime scene, forensic examiners compare the ballistic fingerprint of the recovered bullet or cartridge case with the ballistic fingerprint of a second bullet or cartridge case test-fired from the recovered firearm. If the ballistic fingerprint on the test-fired bullet or cartridge case matches the ballistic fingerprint on the recovered bullet or cartridge case, investigators know that the recovered bullet or cartridge case was also fired from the recovered gun. A confirmed link between a specific firearm and a bullet or cartridge case recovered from a crime scene constitutes a valuable lead, because investigators may be able to connect the firearm to a person, who may then become either a suspect or a source of information helpful to the investigation.\n\nForensic innovator Calvin Goddard offered ballistic identification evidence in 1921 to help secure convictions of accused murderers and anarchists Nicola Sacco and Bartolomeo Vanzetti. On April 8, 1927, Sacco and Vanzetti were finally sentenced to death in the electric chair. A worldwide outcry arose and Governor Alvin T. Fuller finally agreed to postpone the executions and set up a committee to reconsider the case. By this time, firearms examination had improved considerably, and it was now known that a semi-automatic pistol could be traced by several different methods if both bullet and casing were recovered from the scene. Automatic pistols could now be traced by unique markings of the rifling on the bullet, by firing pin indentations on the fired primer, or by unique ejector and extractor marks on the casing. The committee appointed to review the case used the services of Calvin Goddard in 1927.\nGoddard used Philip Gravelle's newly invented comparison microscope and helixometer, a hollow, lighted magnifier probe used to inspect gun barrels, to make an examination of Sacco's .32 Colt, the bullet that killed Berardelli, and the spent casings recovered from the scene of the crime. In the presence of one of the defense experts, he fired a bullet from Sacco's gun into a wad of cotton and then put the ejected casing on the comparison microscope next to casings found at the scene. Then he looked at them carefully. The first two casings from the robbery did not match Sacco's gun, but the third one did. Even the defense expert agreed that the two cartridges had been fired from the same gun. The second original defense expert also concurred. The committee upheld the convictions.\nIn October 1961, ballistics tests were run with improved technology using Sacco's Colt automatic. The results confirmed that the bullet that killed the victim, Berardelli in 1920 came from the same .32 Colt Auto taken from the pistol in Sacco's possession. Subsequent investigations in 1983 also supported Goddard's findings.\n\nColonel Goddard was the key forensic expert in solving the 1929 St. Valentine's Day Massacre in which seven gangsters were killed by rival Al Capone mobsters dressed as Chicago police officers. It also led to the establishment of the United States' first independent criminological laboratory, which was located at Northwestern University and headed by Goddard. At this new lab, ballistics, fingerprinting, blood analysis and trace evidence were all brought under one roof.\nIn 1929, using a comparison microscope adapted for the ballistics comparison by his partner, Phillip Gravelle, Goddard used similar techniques to absolve the Chicago Police Department of participation in the St. Valentine's Day Massacre. The case of Sacco and Vanzetti, which took place in Bridgewater, Massachusetts, is responsible for popularizing the use of the comparison microscope for bullet comparison. Forensic expert Calvin Goddard's conclusions were upheld when the evidence was re-examined in 1961.\n\n\n"}
{"id": "13243925", "url": "https://en.wikipedia.org/wiki?curid=13243925", "title": "Comparison of Star Trek and Star Wars", "text": "Comparison of Star Trek and Star Wars\n\n\"Star Trek\" and \"Star Wars\" are American media franchises which present alternative scenarios of space adventure. The two franchises are dominant in this setting of storytelling and have offered various forms of media productions for decades that manage billions of dollars of intellectual property, providing employment and entertainment for billions of people around the world.\n\n\"Star Trek\" was introduced as in 1966 that lasted three years. \"\" commenced in 1973 (based directly on the original series) but lasted only two seasons with a combined total of 22 episodes. With the subsequent publication of novels, comics, animated series, toys and feature films, \"Star Trek\" grew into a popular media franchise.\n\n\"Star Wars\" was introduced as a feature film, \"A New Hope\" (1977). A novelization titled \"\", based on the original script of the film, was published about a year earlier. Upon the release of the first film, \"Star Wars\" quickly grew into a popular media franchise.\n\n\"Star Trek\" debuted in television. The franchise was conceived in the style of the television Western \"Wagon Train\" and the adventure stories of Horatio Hornblower, but evolved into an idealistic, utopian prospect of future human society. Inspired by \"Gulliver's Travels\", \"Star Trek\"s main focus is of space exploration and a galactic society consisting of multiple planets and species, where conflict occasionally occurs. \"Star Trek\" occurs in the relatively distant future, specifically the 22nd through 24th centuries, with occasional time travel and interdimensional travel. The Earth of the \"Star Trek\" universe shares most of its history with the real world.\n\n\"Star Wars\" debuted in film, despite the novel based on the film's original script having been published a year before the film itself. \"Star Wars\" mainly belongs to the space opera subgenre of science fiction that follows The Hero's Journey and was inspired by works such as Beowulf, King Arthur and other mythologies, world religions, as well as ancient and medieval history. It depicts a galactic society in constant conflict. Though there are periods of peace, these are only documented in novels, comics, video games, non-feature films and other spin-off media. \"Star Wars\" is set \"a long time ago, in a galaxy far, far away\", although many characters are human, occasionally use Earth metaphors and exhibit human character traits.\n\nAlthough both \"Star Trek\" and \"Star Wars\" populate various forms of media, not all types have been produced that are mutual to both franchises. \"Star Wars\" has not produced any live-action television series while \"Star Trek\" has produced seven live-action television series.\n\n\"Star Trek\" likewise has not produced any television films; whereas \"Star Wars\" has produced at least three live-action television films outside the \"Star Wars\" film saga. The \"Star Wars Holiday Special\", \"\" and \"\" are all live-action television spin-off films set in the \"Star Wars\" universe, but not considered part of the official \"Star Wars canon\".\n\n\nAside from both having the word \"star\" in their titles, the two franchises share many similarities and commonalities. Both franchises have their origins in the space western subgenre.\n\nBoth stories depict societies consisting of multiple planets and species. The main galaxy in \"Star Trek\" consists of various planets, each inhabited by different species, united into a single state, the United Federation of Planets. \"Star Wars\" depicts a galaxy that is mostly part of a single state known as the Old Republic, inhabited by humans and countless other species, which later became the Galactic Empire and was again later reformed into a new society called the New Republic after a series of wars.\n\nBoth franchises promote philosophical and political messages.\n\nThe primary philosophies of \"Star Trek\" convey the morals of exploration and interference and how to properly confront and ethically resolve a new situation. Creator Gene Roddenberry was inspired by morality tales such as \"Gulliver's Travels\".\n\nThe primary philosophical messages of \"Star Wars\" are the ethics of good against evil and how to distinguish them from one another. \"Star Wars\" preaches against totalitarian systems and favors societies that offer equality. In an interview on the \"Star Wars\" 20th Anniversary UK Programme aired in 1997 referring to the mythology of the original \"Star Wars\" trilogy, Patrick Stewart stated \"A belief in one's own powers; especially one's own powers to do good because the underlying morality of \"Star Wars\" is a very very positive one.\"\n\nThere have been actors from both franchises who have appeared on common television series such as \"The Outer Limits\" and \"Seaquest\".\n\nBoth franchises also derive significantly from history and ancient mythology, including Greco-Roman mythology. Many planets and alien species in \"Star Trek\", for instance, are named after ancient Roman deities. Several episodes from various \"Star Trek\" television series, such as \"Who Mourns for Adonais\", are directly based on ancient Greek-Roman themes and settings. The series also make references to Ancient Babylon and its mythic folklore. The Klingons and their warrior culture are a representation of the 12th-century Mongols.\n\nMuch of \"Star Wars\" story plots and character developments are based on ancient history, including classical Greece and Rome, such as the fall of the Old Republic in \"Star Wars\", followed by the rise of the Galactic Empire, which parallels the fall of the ancient Roman Republic followed by the rise of the Roman Empire.\n\nA 1983 documentary on the making of \"Star Wars Episode VI: Return of the Jedi\" was hosted by Leonard Nimoy, who also made mention of Lucas's original plan to do two other trilogies preceding and proceeding the original trilogy.\n\nJ. J. Abrams, director and producer of \"Star Trek\" (2009) and \"Star Trek Into Darkness\" (2013) and producer of \"Star Trek Beyond\" (2016), directed and produced \"\" (2015). \"Star Trek\" (2009) and \"Star Wars: The Force Awakens\" (2015) are each the first entries in expected trilogies. These films received favorable critical and commercial response and revived interest for both franchises. In addition to Abrams, actors such as Simon Pegg starred in both series.\n\n\"\" (2002) was poorly received and \"\" (2005) had capped off the prequel trilogy, which overall had a mixed to positive reception.\n\nThe newer films of the two franchises filmed major scenes in the United Arab Emirates. The desert scenes on the planet Jakku in \"Star Wars: The Force Awakens\" (2015) were filmed in the Emirate of Abu Dhabi, while scenes for cities in the film \"Star Trek Beyond\" (2016) were filmed in the Emirate of Dubai.\n\nThe two franchises now offer almost all forms of media ranging from novels, television series, comic books, toys for younger audience, magazines, themed merchandise, board games and video games, as well as fan works. These include canonical and non-canonical works, including works made both by series producers and fans jointly.\n\nDespite the difference in the numbers of films, the profit made by the \"Star Wars\" film series exceed the profit of the \"Star Trek\" film series by almost five times, while the entire franchise outgrosses the other by four times. It is difficult to accurately judge the total worth of each franchise as television series, memorabilia and video games must be taken into account.\n\nScience fiction writer David Brin criticized \"Star Wars\" at the time of the release of \"The Phantom Menace\", arguing that while the \"Star Wars\" movies provide special effects and action/adventure, audiences are not encouraged to engage with their overriding themes. Among his issues with \"Star Wars\" and George Lucas, whom he accused of \"having an agenda\", is that the \"Star Wars\" galaxy is too \"elitist\", with arbitrary rulers on both the evil and good sides, replacing one another without any involvement of the population. He criticizes both sides of the Galactic Civil War as part of the \"same genetically superior royal family\". He finds the \"Star Wars\" universe flawed with additional forms of absolutism, such as justified emotions leading a good person to evil - for example citing the idea that Luke Skywalker killing Palpatine would somehow turn him to the dark side, despite the act potentially saving millions of lives.\n\nAmong the many other flaws he sees with \"Star Wars\" is that Anakin Skywalker becomes a hero in the ending of \"Return of the Jedi\" simply because he saved his son's life, while the atrocities he committed during his time in power go largely ignored. In contrast, he argues that, despite its flaws, \"Star Trek\" is \"democratic\" and follows genuine issues and strong questioning.\n\nWilliam Shatner argues that \"Star Trek\" is superior to \"Star Wars\". According to him, \"\"Star Trek\" had relationships and conflict among the relationships and stories that involved humanity and philosophical questions.\" Shatner believes that \"Star Wars\" was only better than \"Star Trek\" in terms of special effects, and that once J.J. Abrams became involved, \"Star Trek\" was able to \"supersede \"Star Wars\" on every level\".\n\nTim Russ, who played Tuvok on \"\", claims that it is difficult to find common enough elements to be able to compare the two. Among those common elements are their similar settings of unique characters and technologies. He echoed Shatner that \"Star Trek\" reflects common human issues, the morals of exploration and considers ethical questions. \"Star Wars\" in his view is a classic medieval tale dressed up as action-adventure, and that embraces the Eastern philosophy of inner-strength. Russ concludes that despite both their success and popularity, \"Star Trek\" comes out as the better of the two, as it is set in \"our\" galaxy and therefore people can relate better to it, whereas \"Star Wars\" takes place in another galaxy. He acknowledged that he could be biased.\n\nJeremy Bulloch is best known for his role as Boba Fett in the original \"Star Wars\" trilogy. He is a huge fan of \"Star Trek: The Original Series\". He argued that while both franchises are popular, \"Star Wars\" comes out as the superior, for its soundtracks and special effects.\n\nContrasting the focus of the two franchises, contributor J.C. Herzthe of \"The New York Times\" argued, \"\"Trek\" fandom revolves around technology because the \"Star Trek\" universe was founded on ham-fisted dialogue and \"Gong Show\"-caliber acting. But the fictional science has always been brilliant. The science in \"Star Wars\" is nonsense, and everyone knows it. But no one cares because \"Star Wars\" isn't about science. It's epic drama. It's about those incredibly well-developed characters and the moral decisions they face. People don't get into debates about how the second Death Star works. They get into debates about the ethics of blowing it up.\"\n\nJohn Wenzel of \"The Denver Post\" highlighted two differences in approach, noting the \"swashbuckling\" and \"gunslinger\" style of \"Star Wars\" compared with \"Star Trek\"s \"broader themes of utopian living, justice and identity\" and that the spiritual aspect of \"Star Wars\" contrasts with the balance of emotion and logic seen in \"Star Trek\".\n\nBillionaire Peter Thiel told Dowd \"I'm a capitalist. \"Star Wars\" is the capitalist show. \"Star Trek\" is the communist one\". He further stated \"There is no money in \"Star Trek\" because you just have the transporter machine that can make anything you need. The whole plot of \"Star Wars\" starts with Han Solo having this debt that he owes and so the plot in \"Star Wars\" is driven by money.\"\n\nArchived footage in \"Trek Nation\" showed Gene Roddenberry saying, \"I like \"Star Wars\". It was young King Arthur growing up, slaying the evil emperor finally. There's nothing wrong with that kind of entertainment - everything doesn't have to create a philosophy for you - for your whole life. You can also have fun.\"\n\nThe two franchises have a \"symbiotic relationship\" stated Shatner, who credits \"Star Wars\" for launching the \"Star Trek\" films. He repeated this sentiment at a 2016 \"Star Trek\" fan convention in Las Vegas by stating \"\"Star Wars\" created \"Star Trek\"\". He clarified this statement by explaining that at the time of the release of the first \"Star Wars\" film (\"A New Hope\"), Paramount, then under new management, was struggling to come up with something that could compete with it. A \"Star Trek\" relaunch was the choice. Since then, public interest has returned to \"Star Trek\". \"It was \"Star Wars\" that thrust \"Star Trek\" into the people of Paramount's consciousness\" Shatner stated.\n\nThe documentary \"Trek Nation\" features interviews where both Lucas and Roddenberry praise each other's respective franchises, with the former stating that \"Star Trek\" was an influence while writing the original screenplay for \"Star Wars\". He explained that while both franchises were so \"far out\", \"Star Trek\" produced a fanbase that \"softened up the entertainment arena\" so that \"Star Wars\" could \"come along and stand on its shoulders.\" This is also acknowledged by Shatner, who went as far as to call \"Star Wars\" a \"derivative\" of \"Star Trek\".\n\nA few references to \"Star Wars\" have been inserted into \"Star Trek\" films. For fleeting moments, one can see ships and droids from \"Star Wars\" in both \"Star Trek\" (2009) and \"Star Trek Into Darkness\" (2013). Some \"Star Trek\" films and television episodes used the \"Star Wars\" animation shop, Industrial Light & Magic, for their special effects.\n\nWhen Roddenberry was honored at a \"Star Trek\" convention late in life, a congratulatory letter from Lucas was presented by an actor dressed as Darth Vader. A few years earlier, Roddenberry had contributed an entry in honor of \"Star Wars\" and Lucas at a convention honoring the latter.\n\nWilliam Shatner was a presenter at Lucas' American Film Institute Lifetime Achievement Award ceremony in 2007 and did a comical stage performance honoring Lucas.\n\nAt a live concert, Shatner dressed as an imperial stormtrooper singing \"Girl Crush\" alongside Carrie Underwood and Brad Paisley.\n\nIn 2011, Shatner and Carrie Fisher posted a series of humorous YouTube videos satirizing each other's franchises.\n\nIn a 2016 interview, Shatner commented that Captain Kirk and Princess Leia eloping and running off into the sunset would be the \"perfect union\" between \"Star Trek\" and \"Star Wars\".\n\nShatner has also posted a number of humorous tweets on his Twitter account mocking \"Star Wars\". Amongst them were commemorating the 35th anniversary of the poorly received \"Star Wars Holiday Special\". It was then that \"Star Wars\" actor Peter Mayhew posted a \"retaliation\" tweet congratulating Shatner for the directing of \"\", another poorly received film.\n\nBoth franchises are set to grow throughout the next decade.\n\n\"Star Trek\" was rebooted with a series of feature films starting with the \"Star Trek\" reboot (2009), which was followed by \"Star Trek Into Darkness\" (2013) and \"Star Trek Beyond\" (2016) and a number of sequels are set to follow. A new television series based in the original timeline, subtitled \"\", serving as a prequel to the original series, debuted on CBS All Access, an online streaming platform, in 2017.\n\n\"Star Wars\" picked up from where \"Return of the Jedi\" left off, with \"\" the first in the new trilogy, and \"\" following two years later. Additionally, more spin-off media is also underway after the debut of \"Star Wars Rebels\", a television series set in between the \"Star Wars\" prequels and the original trilogy, and an anthology of stand-alone \"Star Wars\" films, starting with \"Rogue One\", which was released in December 2016, and \"\" following in May 2018.\n\nAside from official works by the producers of \"Star Trek\" and \"Star Wars\", many fan films and webisodes set in the two universes of the franchises are also constantly produced and posted on the Internet by fans, but are not officially considered canon in relation to either franchise.\n\n"}
{"id": "16759434", "url": "https://en.wikipedia.org/wiki?curid=16759434", "title": "Comparison of the Amundsen and Scott Expeditions", "text": "Comparison of the Amundsen and Scott Expeditions\n\nBetween December 1911 and January 1912, both Roald Amundsen (leading his South Pole expedition) and Robert Falcon Scott (leading the Terra Nova Expedition) reached the South Pole within a month of each other. But while Scott and his four companions died on the return journey, Amundsen's party managed to reach the geographic south pole first and subsequently return to their base camp at Framheim without loss of life, suggesting that they were better prepared for the expedition. The contrasting fates of the two teams seeking the same prize at the same time invites comparison.\n\nThe outcomes of the two expeditions were as follows.\n\nHistorically, several factors have been discussed and many contributing factors claimed, including:\n\nSullivan states that it was the last factor that probably was decisive. he states \"Man is a poor beast of burden, as was shown in the terrible experience of Scott, Shackleton, and Wilson in their thrust to the south of 1902–3. However, Scott relied chiefly on man-hauling in 1911–12 because ponies could not ascend the glacier midway to the Pole. The Norwegians correctly estimated that dog teams could go all the way. Furthermore, they used a simple plan, based on their native skill with skis and on dog-driving methods that were tried and true. In a similar fashion to the way the moon was reached by expending a succession of rocket stages and then casting each aside; the Norwegians used the same strategy, sacrificing the weaker animals along the journey to feed the other animals and the men themselves.\"\n\nScott and his financial backers saw the expedition as having a scientific basis, while also wishing to reach the pole. However, it was recognised by all involved that the South Pole was the primary objective (\"The Southern Journey involves the most important object of the Expedition\" – Scott), and had priority in terms of resources, such as the best ponies and all the dogs and motor sledges as well as involvement of the vast majority of the expedition personnel. Scott and his team knew the expedition would be judged on his attainment of the pole (\"The ... public will gauge the result of the scientific work of the expedition largely in accordance with the success or failure of the main object\" – Scott). He was prepared to make a second attempt the following year (1912–13) if this attempt failed and had Indian Army mules and additional dogs delivered in anticipation. In fact the mules were used by the team that discovered the dead bodies of Scott, Henry Robertson Bowers, and Edward Adrian Wilson in November 1912, but proved even less useful than the ponies, according to Cherry-Garrard.\n\nAmundsen's expedition was planned to reach the South Pole. This was a plan he conceived in 1909. Amundsen's expedition did conduct geographical work under Kristian Prestrud who conducted an expedition to King Edward VII Land while Amundsen was undertaking his attempt at the pole.\n\nAmundsen camped on the Ross Ice Shelf at the Bay of Whales which is 60 miles (96 km) closer to the pole than Scott's camp (which was 350 miles west of Amundsen). Amundsen had deduced that, as the Trans-Antarctic Mountains ran northwest to southeast then if he were to meet a mountain range on his route then the time spent at the high altitude of the Antarctic plateau would be less than Scott's.\nScott's base was at Cape Evans on Ross Island, with access to the Trans-Antarctic mountain range to the west, and was a better base for geological exploration. He had based his previous expedition in the same area. However, he knew it to be poor as a route to the pole as he had to start before sea ice melted and had suffered delay in returning while waiting for the sea ice to freeze. They also had to make detours around Ross Island and its known crevassed areas which meant a longer journey. The crossing of the Ross Ice Shelf was an onerous task for the ponies. Scott had advanced considerable stores across the ice shelf the year before to allow the ponies to carry lighter loads over the early passage across the ice. Even so, he had to delay the departure of the ponies until 1 November rather than 24 October when the dogs and motor sledges set off.\nConsequently, the Motor Party spent 6 days at the Mount Hooper Depot waiting for Scott to arrive.\n\nThe major comparison between Scott and Amundsen has focused on the choice of draft transport —dog versus pony/man-hauling. In fact Scott took dogs, ponies and three \"motor sledges\". Scott spent nearly seven times the amount of money on his motor sledges than on the dogs and horses combined. They were therefore a vital part of the expedition. Unfortunately, Scott decided to leave behind the engineer, Lieutenant Commander Reginald William Skelton who had created and trialled the motor sledges. This was due to the selection of Lieutenant E.R.G.R. \"Teddy\" Evans as the expedition's second in command. As Evans was junior in rank to Skelton, he insisted that Skelton could not come on the expedition. Scott agreed to this request and Skelton's experience and knowledge was lost. One of the original three motor sledges was a failure even before the expedition set out; the heavy sledge was lost through thin ice on unloading it from the ship. The two remaining motor sledges failed relatively early in the main expedition because of repeated faults. Skelton's experience might have been valuable in overcoming the failures.\n\nScott had used dogs on his first (Discovery) expedition and felt they had failed. On that journey, Scott, Shackleton, and Wilson started with three sledges and 13 dogs. But on that expedition, the men had not properly understood how to travel on snow with the use of dogs. The party had skis but were too inexperienced to make good use of them. As a result, the dogs travelled so fast that the men could not keep up with them. The Discovery expedition had to increase their loads to slow the dogs down. Additionally, the dogs were fed Norwegian dried fish, which did not agree with them and soon they began to deteriorate. The whole team of dogs eventually died (and were eaten), and the men took over hauling the sleds.\n\nScott's opinion was reinforced by Shackleton's experience on his Nimrod expedition that got to within of the pole. Shackleton used ponies. Scott planned to use ponies only to the base of the Beardmore Glacier (one-quarter of the total journey) and man-haul the rest of the journey. Scott's team had developed snow shoes for his ponies, and trials showed they could significantly increase daily progress. However, Lawrence Oates, whom Scott had made responsible for the ponies, was reluctant to use the snow shoes and Scott failed to insist on their use.\n\nThere was plenty of evidence that dogs could succeed in the achievements of William Speirs Bruce in his Arctic, Antarctic, and Scottish National Antarctic Expedition, Amundsen in the \"Gjøa\" North West passage expedition, Fridtjof Nansen's crossing of Greenland, Robert Peary's three attempts at the North Pole, Eivind Astrup's work supporting Peary, Frederick Cook's discredited North Pole expedition, and Otto Sverdrup's explorations of Ellesmere Island. Moreover, Scott ignored the direct advice he received (while attending trials of the motor sledges in Norway) from Nansen, the most famous explorer of the day, who told Scott to take \"dogs, dogs and more dogs\".\n\nAt the time of the events, the expert view in England had been that dogs were of dubious value as a means of Antarctic transport. Broadly speaking, Scott saw two ways in which dogs may be used—they may be taken with the idea of bringing them all back safe and sound, or they may be treated as pawns in the game, from which the best value is to be got regardless of their lives. He stated that if, and only if, the comparison was made with a dog sledge journey which aimed to preserve the dogs' lives, 'I am inclined to state my belief that in the polar regions properly organised parties of men will perform as extended journeys as teams of dogs.' On the other hand, if the lives of the dogs were to be sacrificed, then 'the dog-team is invested with a capacity for work which is beyond the emulation of men. To appreciate this is a matter of simple arithmetic'. But efficiency notwithstanding, he expressed \"reluctance\" to use dogs in this way: \"One cannot calmly contemplate the murder of animals which possess such intelligence and individuality, which have frequently such endearing qualities, and which very possibly one has learnt to regard as friends and companions.\"\n\nAmundsen, by contrast, took an entirely utilitarian approach. Amundsen planned from the start to have weaker animals killed to feed the other animals and the men themselves. He expressed the opinion that it was less cruel to feed and work dogs correctly before shooting them, than it would be to starve and overwork them to the point of collapse. Amundsen and his team had similar affection for their dogs as those expressed above by the English, but they \"also had agreed to shrink from nothing in order to achieve our goal\". The British thought such a procedure was distasteful, though they were willing to eat their ponies.\n\nAmundsen had used the opportunity of learning from the Inuit while on his \"Gjøa\" North West passage expedition of 1905. He recruited experienced dog drivers. To make the most of the dogs he paced them and deliberately kept daily mileages shorter than he need have for 75 percent of the journey, and his team spent up to 16 hours a day resting. His dogs could eat seals and penguins hunted in the Antarctic while Scott's pony fodder had to be brought all the way from England in their ship. It has been later shown that seal meat with the blubber attached is the ideal food for a sledge dog. Amundsen went with 52 dogs, and came back with 11.\n\nWhat Scott did not realise is a sledge dog, if it is to do the same work as a man, will require the same amount of food. Furthermore, when sledge dogs are given insufficient food they become difficult to handle. The advantage of the sledge dog is its greater mobility. Not only were the Norwegians accustomed to skiing, which enabled them to keep up with their dogs, but they also understood how to feed them and not overwork them.\n\nScott took the Norwegian pilot and skier Tryggve Gran to the Antarctic on the recommendation of Nansen to train his expedition to ski, but although a few of his party began to learn, he made no arrangements for compulsory training for the full party. Gran (possibly because he was Norwegian) was not included in the South Pole party, which could have made a difference. Gran was, one year later, the first to locate the deceased Scott and his remaining companions in their tent just some 18 km (11 miles) short of One Ton depot, that might have saved their lives had they reached it.\n\nScott would subsequently complain in his diary, while well into his journey and therefore too late to take any corrective action and after over 10 years since the Discovery expedition, that \"Skis are the thing, and here are my tiresome fellow countrymen too prejudiced to have prepared themselves for the event\".\n\nAmundsen on his side recruited a team of well experienced skiers, all Norwegians who had skied from an early age. He also recruited a champion skier, Olav Bjaaland, as the front runner. The Amundsen party gained weight on their return travel from the South Pole.\n\nScott and Shackleton's experience in 1903 and 1907 gave them first-hand experience of average conditions in Antarctica. Simpson, Scott's meteorologist 1910–1912, charted the weather during their expedition, often taking two readings a day. On their return to the Ross Ice Shelf, Scott's group experienced prolonged low temperatures from 27 February until 10 March which have only been matched once in 15 years of current records. The exceptional severity of the weather meant they failed to make the daily distances they needed to get to the next depot. This was a serious position as they were short of fuel and food. When Scott, Wilson, and Bowers died (Petty Officer Edgar Evans and Lawrence Oates had died earlier during the return from the South Pole) they were short of One-Ton Depot, which was from Corner Camp, where they would have been safe.\n\nOn the other hand, Cherry-Garrard had travelled nearly in the same area, during the same time period and same temperatures, using a dog team. Scott also blamed \"a prolonged blizzard\". But while there is evidence to support the low temperatures, there is only evidence for a \"normal\" two- to four-day blizzard, and not the ten days that Scott claims.\n\nDuring depot laying in February 1911, Roald Amundsen had his first (and last) of his route marked like a Norwegian ski course using marker flags initially every eight miles. He added to this by using food containers painted black, resulting in a marker every mile. From 82 degrees on, Amundsen built a cairn every three miles with a note inside recording the cairn's position, the distance to the next depot, and direction to the next cairn. In order not to miss a depot considering the snow and great distances, Amundsen took precautions. Each depot laid out up to 85 degrees (laid out every degree of latitude) had a line of bamboo flags laid out transversely every half-mile for five miles on either side of the depot, ensuring that the returning party could locate the designated depot.\n\nScott relied on depots much less frequently laid out. For one distance where Amundsen laid seven depots, Scott laid only two. Routes were marked by the walls made at lunch and evening stops to protect the ponies. Depots had a single flag. As a result, Scott has much concern recorded in his diaries over route finding, and experienced close calls about finding depots. It is also clear that Scott's team did not travel on several days, because the swirling snow hid their three-month-old outward tracks. With better depot and route marking they would have been able to travel on more days with a following wind which would have filled the sail attached to their sledge, and so travel further, and might have reached safety.\n\nBy the time they arrived at the pole, the health of Scott's team had significantly deteriorated, whereas Amundsen's team actually gained weight during the expedition. While Scott's team managed to maintain the scheduled pace for most of the return leg, and hence was virtually always on full rations, their condition continued to worsen rapidly. (The only delay occurred when they were held for four days by a blizzard, and had to open their summit rations early as a consequence.)\n\nApsley Cherry-Garrard in his analysis of the expedition estimated that even under optimistic assumptions the summit rations contained only a little more than half the calories actually required for the man-hauling of sledges. A carefully planned 2006 re-enactment of both Amundsen's and Scott's travels, sponsored by the BBC, confirmed Cherry-Garrard's theory. The British team had to abort their tour due to the severe weight loss of all members. The experts hinted that Scott's reports of unusually bad surfaces and weather conditions might in part have been due to their exhausted state which made them feel the sledge weights and the chill more severely.\n\nScott's calculations for the supply requirements were based on a number of expeditions, both by members of his team (e.g., Wilson's trip with Cherry-Garrard and Bowers to the Emperor penguin colony which had each man on a different type of experimental ration), and by Shackleton. Apparently, Scott didn't take the strain of prolonged man-hauling at high altitudes sufficiently into account.\n\nSince the rations contained no B and C vitamins, the only source of these vitamins during the trek was from the slaughter of ponies or dogs. This made the men progressively malnourished, manifested most clearly in the form of scurvy.\n\nScott also had to fight with a shortage of fuel due to leakage from stored fuel cans which used leather washers. This was a phenomenon that had been noticed previously by other expeditions, but Scott took no measures to prevent it. Amundsen, in contrast, had learned the lesson and had his fuel cans soldered closed. A fuel depot he left on Betty's Knoll was found 50 years later still full.\n\nDehydration may also have been a factor. Amundsen's team had plenty of fuel due to better planning and soldered fuel cans. Scott had a shortage of fuel and was unable to melt as much water as Amundsen. At the same time Scott's team were more physically active in man-hauling the sledges.\n\nIt has been said (by the present-day explorer Ranulph Fiennes amongst others) that Scott's team was appropriately dressed for man-hauling in their woolen and wind-proof clothing, and as Amundsen was skiing it was appropriate he wore furs. Skiing at the pace of a dog team is a strenuous activity. Yet Amundsen never complained about the clothing being too hot. That is because the furs are worn loosely so air circulates and sweat evaporates. Scott's team, on the other hand, made regular complaints about the cold.\n\nAmundsen's team did initially have problems with their boots. However, the depot-laying trips of January and February 1911 and an abortive departure to the South Pole on 8 September 1911 allowed changes to be made before it was too late.\n\nScott's team suffered regularly from snow blindness and sometimes this affected over half the team at any one time. By contrast, there was no recorded case of snow blindness during the whole of Amundsen's expedition. On the return journey, Amundsen's team rested during the \"day\" (when the sun was in front of them) and travelled during the \"night\" (when the sun was behind them) to minimise the effects of snow blindness.\n\nIn 1921, 'Teddy' Evans wrote in his book \"South with Scott\" that Scott had left the following written orders at Cape Evans.\n\nHe did however place a lesser importance upon this journey than that of replenishing the food rations at One Ton Depot.\n\nHe continued his instructions in the next paragraph \"You will of course understand that whilst the object of your third journey is important, that of the second is vital. At all hazards three X.S. units of provision must be got to One Ton Camp by the date named (19th January), and if the dogs are unable to perform this task, a man party must be organised.\" with that qualification he closed his notes regarding his instructions for the dogs.\n\nExpedition member Apsley Cherry-Garrard did not mention Scott's order in his 1922 book \"The Worst Journey in the World\". However, in the 1948 preface to his book, he discusses Scott's order. Cherry-Garrard writes that he and Edward Atkinson reached Cape Evans on 28 January. Scott had estimated Atkinson would reach camp by 13 January. Atkinson, now the senior officer discovered that the dog handler Cecil Meares had resigned from the expedition and that neither Meares nor anyone else had resupplied dog food to the depots. Cherry-Garrard also wrote \"In my opinion he [Atkinson] would not have been fit to take out the dogs in the first week of February\".\n\nOn 13 February, Atkinson set off on the first lap southwards to Hut Point with the dog assistant, Dimitri Gerov, and the dogs to avoid being cut off by disintegrating sea ice. Atkinson and Gerov were still at Hut Point when, on 19 February, Tom Crean arrived on foot from the Barrier and reported that Lt Edward Evans was lying seriously ill in a tent some to the south, and in urgent need of rescue. Atkinson decided that this mission was his priority, and set out with the dogs to bring Evans back. This was achieved; the party was back at Hut Point on 22 February.\n\nAtkinson sent a note back to the Cape Evans base camp requesting either the meteorologist Wright or Cherry-Garrard to take over the task of meeting Scott with the dogs. Chief meteorologist Simpson was unwilling to release Wright from his scientific work, and Atkinson therefore selected Apsley Cherry-Garrard. It was still not in Atkinson's mind that Cherry-Garrard's was a relief mission, and according to Cherry-Garrard's account, told him to \"use his judgement\" as to what to do in the event of not meeting the polar party by One Ton, and that Scott's orders were that the dogs must not be risked. Cherry-Garrard left with Gerov and the dogs on 26 February, carrying extra rations for the polar party to be added to the depot and 24 days' of dog food. They arrived at One Ton Depot on 4 March and did not proceed further south. Instead, he and Gerov, after waiting there for Scott for several days, apparently mostly in blizzard conditions (although no blizzard was recorded by Scott some 100 miles further south until 10 March), they returned to Hut Point on 16 March, in poor physical condition and without news of the polar party.\n\nOn the return journey from the pole, Scott reached the 82.30°S meeting point for the dog teams three days ahead of schedule, around 27 February 1912. Scott's diary for that day notes \"We are naturally always discussing possibility of meeting dogs, where and when, etc. It is a critical position. We may find ourselves in safety at the next depot, but there is a horrid element of doubt.\" By 10 March it became clear that the dog teams were not coming: \"The dogs which would have been our salvation have evidently failed. Meares [the dog-driver] had a bad trip home I suppose. It's a miserable jumble.\"\n\nAround 25 March, awaiting death in his tent at latitude 79.30°S, Scott speculated, in a farewell letter to his expedition treasurer Sir Edgar Speyer, that he had overshot the meeting point with the dog relief teams, writing \"We very nearly came through, and it's a pity to have missed it, but lately I have felt that we have overshot our mark. No-one is to blame and I hope no attempt will be made to suggest that we had lacked support.\" (Farewell letter to Sir Edgar Speyer, cited from Karen May 2012.)\n\n"}
{"id": "1833848", "url": "https://en.wikipedia.org/wiki?curid=1833848", "title": "Cross-reference", "text": "Cross-reference\n\nThe term cross-reference can refer to either:\n\nIn a document, especially those authored in a Content management system,\na cross-reference has two major aspects:\n\nThe visible form contains text, graphics, and other indications that:\n\nThe technical mechanism that resides within the system:\n\nIf the cross reference mechanism is well designed, the reader will be able to follow each cross reference to the referenced content whether the content is presented in print or electronically.\n\nAn author working in a content management system is responsible for identifying subjects of interest that cross documents, and creating appropriate systems of cross references to support readers who seek to understand those subjects. For an individual cross reference, an author should ensure that location and content of the target of the cross reference are clearly identified, and the reader can easily determine how to follow the cross reference in each medium in which publication is supported.\n\nContent strategy practitioners (known as content strategists) specialize in planning content to meet business needs, taking into account the processes for creating and maintaining the content, and the systems that support the content.\n\n"}
{"id": "46842976", "url": "https://en.wikipedia.org/wiki?curid=46842976", "title": "Diels–Kranz numbering", "text": "Diels–Kranz numbering\n\nDiels–Kranz (DK) numbering is the standard system for referencing the works of the ancient Greek pre-Socratic philosophers, based on the collection of quotations from and reports of their work, \"Die Fragmente der Vorsokratiker\" (The Fragments of the Pre-Socratics), by Hermann Alexander Diels. The \"Fragmente\" was first published in 1903, was later revised and expanded three times by Diels, and was finally revised in a fifth edition (1934–7) by Walther Kranz and again in a sixth edition (1952). In Diels-Kranz, each passage, or item, is assigned a number which is used to uniquely identify the ancient personality with which it is concerned, and the type of item given. Diels-Kranz is used in academia to cite pre-Socratic philosophers, and the system also encompasses Sophists and pre-Homeric poets such as Orpheus.\n\nStephanus pagination is the comparable system for referring to Plato, and Bekker numbering is the comparable system for referring to Aristotle.\n\nThe works of the pre-Socratics have not survived extant to the present day. Our knowledge of them exists only through references in the works of later philosophers (known as doxography) in the form of quotations and paraphrases. For example, our knowledge of Thales of Miletus comes largely from the works of Aristotle, who lived centuries after him. Another interesting example of such a source is Hippolytus of Rome, whose polemic \"Refutation of All Heresies\" is a source of many direct quotations of Heraclitus as well as of other philosophers, thereby perpetuating the work of those he was refuting.\n\nThese quotations, paraphrases and other references to pre-Socratic philosophers were collected by Diels and Kranz in their book, which became a standard text in modern pre-Socratic education and scholarship. Because of its influence, Diels-Kranz numbering became the standard way of referencing the material: in literature, conferences, and even in conversation.\n\nThe number corresponding to an item was made up of three parts:\n\n\nWhy, take the case of Thales, Theodorus. While he was studying the stars and looking upwards, he fell into a pit, and a neat, witty Thracian servant girl jeered at him, they say, because he was so eager to know the things in the sky that he could not see what was there before him at his very feet.\n\nThe above text has a DK number of 11A9, since it refers to Thales who is, as mentioned above, chapter 11's subject. The source is \"Theaetetus\" (one of Plato's dialogues), and gives an account of Thales' life, hence it is a \"testimonium\", represented by the letter \"A\". Finally, it is the ninth item in its chapter, giving it the overall number of DK 11A9.\n\nSometimes, the chapter (personality) number may simply be replaced by the name, which can be helpful in cases where the former is the same as the passage number, to avoid ambiguity. For example:\n\nThose who seek for gold dig up much earth and find a little.\n\nRather than \"22B22\" the above may also instead be referred to as \"Heraclitus B22\" as it is a direct transmission of the words of Heraclitus (thus, B) and is the 22nd item in the chapter about Heraclitus (whose chapter number is also 22) in the \"Fragmente\".\n\nThe following table gives the Diels-Kranz numbering of Pre-Socratic philosophers. Note that the numbering scheme presented is that of the fifth edition of \"Die Fragmente der Vorsokratiker\", the first to be revised by Kranz. The fifth edition's numbering is the scheme which has since gained the most traction in modern Pre-Socratic scholarship, and it is the one used consistently throughout this article. It should not be confused with the numberings given in other versions, which changed frequently depending on the particular edition of the \"Fragmente\".\n\nMost entries (78) are concerned with a single, named individual, while the remaining minority of entries (12) have more complex context. Of these latter, eight (10, 19, 39, 46, 53-56) are each concerned with groups of named personalties, who typically have a clear relationship of some kind to justify their association in each entry. Two entries (58, 79) are devoted not to individuals, but to schools of thought (Pythagoreanism and Sophism), and the last two (89, 90) reproduce contemporaneous anonymous texts. Although \"the Seven Sages of Greece\" implies a clearly defined set of seven people, historical disagreement renders intractable the problem of exactly who they were, with multiple sources suggesting several different candidates. If one takes the Seven Sages as a group of seven and includes the later Iamblichus, Diels-Kranz encompasses 106 named personalities and two anonymous authors. The chapter on Sophism is concerned with the named sophists who take up most of the rest of the scheme, and \nper Freeman with regard to the chapter on Pythagoreanism, a catalogue due to Iamblichus lists 218 named men and 17 named women as Pythagoreans, along with other probable, anonymous adherents.\n\nIn several cases, the personalities listed are so obscure that they are merely mentioned by name in other sources, commonly with hints as to their geographical and philosophical associations, and without even surviving \"paraphrases\" of any of their ideas, or what they might have written. That is, these more obscure personalities survive in the historical record only as names cited by others, and so came to be included in Diels-Kranz for the sake of scholarly completeness.\n\n\n"}
{"id": "358999", "url": "https://en.wikipedia.org/wiki?curid=358999", "title": "Difference feminism", "text": "Difference feminism\n\nTaking for granted an equal moral status as persons, difference feminism asserts that there are differences between men and women but that no value judgment can be placed upon them.\n\nThe term \"difference feminism\" developed during the \"equality-versus-difference debate\" in American feminism in the 1980s and 1990s, but subsequently fell out of favor and use. In the 1990s feminists addressed the binary logic of \"difference\" versus \"equality\" and moved on from it, notably with postmodern and/or deconstructionist approaches that either dismantled or did not depend on that dichotomy.\n\nDifference feminism did not require a commitment to essentialism. Most strains of difference feminism did not argue that there was a biological, inherent, ahistorical, or otherwise \"essential\" link between womanhood and traditionally feminine values, habits of mind (often called \"ways of knowing\"), or personality traits. These feminists simply sought to recognize that, in the present, women and men are significantly different and to explore the devalued \"feminine\" characteristics.\n\nSome strains of difference feminism, for example Mary Daly's, argue not just that women and men were different, and had different values or different ways of knowing, but that women and their values were superior to men's. This viewpoint does not require essentialism, although there is ongoing debate about whether Daly's feminism is essentialist.\n\nDifference feminism was developed by feminists in the 1980s, in part as a reaction to popular liberal feminism (also known as \"equality feminism\"), which emphasized the similarities between women and men in order to argue for equal treatment for women. Difference feminism, although it still aimed at equality between men and women, emphasized the differences between men and women and argued that identicality or sameness are not necessary in order for men and women, and masculine and feminine values, to be treated equally. Liberal feminism aimed to make society and law gender-neutral, since it saw recognition of gender difference as a barrier to rights and participation within liberal democracy, while difference feminism held that gender-neutrality harmed women \"whether by impelling them to imitate men, by depriving society of their distinctive contributions, or by letting them participate in society only on terms that favor men\".\n\nDifference feminism drew on earlier nineteenth-century strains of thought, for example the work of German writer Elise Oelsner, which held that not only should women be allowed into formerly male-only spheres and institutions (e.g. public life, science) but that those institutions should also be expected to change in a way that recognizes the value of traditionally devalued feminine ethics (like care [see ethics of care]). On the latter point, many feminists have re-read the phrase \"difference feminism\" in a way that asks \"what difference does feminism make?\" (e.g. to the practice of science) rather than \"what differences are there between men and women\"?\n\nSome have argued that the thought of certain prominent second-wave feminists, like psychologist Carol Gilligan and radical feminist theologian Mary Daly, is \"essentialist.\" In philosophy essentialism is the belief that \"(at least some) objects have (at least some) essential properties.\" In the case of sexual politics essentialism is taken to mean that \"women\" and \"men\" have fixed essences or essential properties (e.g. behavioral or personality traits) that cannot be changed. However, essentialist interpretations of Daly and Gilligan have been questioned by some feminist scholars, who argue that charges of \"essentialism\" are often used more as terms of abuse than as theoretical critiques based on evidence, and do not accurately reflect Gilligan or Daly's views.\n\n"}
{"id": "8366559", "url": "https://en.wikipedia.org/wiki?curid=8366559", "title": "Difference theory", "text": "Difference theory\n\nDifference theory has roots in the studies of John Gumperz, who examined differences in cross-cultural communication. While difference theory deals with cross-gender communication, the male and female genders are often presented as being two separate cultures, hence the relevance of Gumperz's studies. In her development of the difference theory, Deborah Tannen drew on the work of Daniel Maltz and Ruth Borker, in particular their 1982 paper, \"A Cultural Approach to Male-Female Miscommunication\", which itself drew on the work of Gumperz. Mary Talbot makes reference to the term \"gender-specific culture\" in her critique of the difference theory, and this idea of genders being culturally separated is embodied by the 1992 publication \"Men Are from Mars, Women Are from Venus\". Difference theory is often compared with dominance theory and deficit theory, and together with the more contemporary dynamic theory they make up four of the theories most widely referred to and compared in the study of language and gender.\n\nThe reason for the popularity of Tannen's book \"You Just Don't Understand\", and the resultant popularisation of difference theory, is generally attributed to the style of Tannen's work, in which she adopts a neutral position on differences in genderlect by making no value-judgements about use of language by either gender. Talbot comments that this means the book provides explanations for domestic disputes without \"pointing the finger\" at anyone.\n\nDifference theory as postulated by Tannen is generally summarised into six categories, each of which pairs contrasting uses of language by males and females.\n\nTannen states that, for men, the world is a competitive place in which conversation and speech are used to build status, whereas for women the world is a network of connections, and that they use language to seek and offer support. In demonstrating this, Tannen uses the example of her husband and herself, who at one point had jobs in different cities. She remarks that whenever someone commented on this, she interpreted it as being an offer of sympathy or support. Her husband, on the other hand, took such comments as being criticisms and attempts to put him down. Tannen remarks that this displays the different approaches that women and men take in terms of status and support. Furthermore, men are also more likely to interrupt to get their point across and hence gain status.\n\nWomen seek comfort and sympathy for their problems, whilst men will seek a solution to the problem.\n\nTannen states that men's conversation is message-oriented, i.e. based upon communicating information. For women, conversation is much more important for building relationships and strengthening social links.\n\nMen will use direct imperatives (\"close the door\", \"switch on the light\") when speaking to others. Women encourage the use of superpolite forms, however (\"let's\", \"would you mind if ...?\").\n\nTannen asserts that most women avoid conflict in language at all costs, and instead attempt to resolve disagreements without any direct confrontation, to maintain positive connection and rapport. Men, on the other hand, are more likely to use confrontation as a way of resolving differences and thereby negotiating status. Tannen supports this view by making reference to the work of Walter J. Ong, whose 1981 publication, \"Fighting for Life\", asserts that \"expressed adversativeness\" is more an element of male culture than female culture. Tannen stresses that both forms of communication are valid ways of creating involvement and forming bonds.\n\nDifference theory asserts that in general men favour independence, while women are more likely to seek intimacy. Tannen demonstrates this with the example of a husband making a decision without consulting his wife. She theorises that he does so because he doesn't want to feel a loss of independence that would come from saying, \"Let me consult this with my wife first.\" Women, by contrast, like to demonstrate that they have to consult with their partner, as this is seen to be proof of the intimacy of the relationship. Tannen asserts that women, seeing the world as a network of connections and relationships, view intimacy as key to achieving consensus and avoiding the appearance of superiority, whereas men, who are more likely to view the world in terms of status, see independence as being key to establishing their status. Tannen also clarifies that while both men and women seek independence and intimacy, men tend to be focused on the former, while women tend to focus on the latter.\n\nGeneral criticisms are that Tannen's observations are largely anecdotal and cannot be said for all conjugal conversations, let alone mixed-gender interactions as a whole.\n\n\n"}
{"id": "1902180", "url": "https://en.wikipedia.org/wiki?curid=1902180", "title": "Digital reference", "text": "Digital reference\n\nDigital reference (or virtual reference) is a service by which a library reference service is conducted online, and the reference transaction is a computer-mediated communication. It is the remote, NextNextcomputer-mediated delivery of reference information provided by library professionals to users who cannot access or do not want face-to-face communication. Virtual reference service is most often an extension of a library's existing reference service program. The word \"reference\" in this context refers to the task of providing assistance to library users in finding information, answering questions, and otherwise fulfilling users’ information needs. Reference work often but not always involves using reference works, such as dictionaries, encyclopedias, etc. This form of reference work expands reference services from the physical reference desk to a \"virtual\" reference desk where the patron could be writing from home, work or a variety of other locations.\n\nThe terminology surrounding virtual reference services may involve multiple terms used for the same definition. The preferred term for remotely delivered, computer-mediated reference services is \"virtual reference\", with the secondary non-preferred term \"digital reference\" having gone out of use in recent years. \"Chat reference\" is often used interchangeably with virtual reference, although it represents only one aspect of virtual reference. Virtual reference includes the use of both synchronous (i.e., IM, videoconferencing) and asynchronous communication (i.e., texting and email). Here, \"synchronous virtual reference\" refers to any real-time computer-mediated communication between patron and information professional. Asynchronous virtual reference is all computer-mediated communication that is sent and received at different times.\n\nThe earliest digital reference services were launched in the mid-1980s, primarily by academic and medical libraries, and provided by e-mail. These early-adopter libraries launched digital reference services for two main reasons: to extend the hours that questions could be submitted to the reference desk, and to explore the potential of campus-wide networks, which at that time was a new technology.\n\nWith the advent of the graphical World Wide Web, libraries quickly adopted webforms for question submission. Since then, the percentage of questions submitted to services via webforms has outstripped the percentage submitted via email.\n\nIn the early- to mid-1990s, digital reference services began to appear that were not affiliated with any library. These digital reference services are often referred to as \"AskA\" services. Examples of AskA services are the Internet Public Library, Ask Dr. Math, and Ask Joan of Art.\n\nProviding remote-based services for patrons has been a steady practice of libraries over the years. For example, before the widespread use of chat software, reference questions were often answered via phone, fax, email and audio conferencing. Email is the oldest type of virtual reference service used by libraries. Library services in America and the UK are just now gaining visibility in their use of virtual reference services using chat software. However, a survey in America revealed that by 2001 over 200 libraries were using chat reference services. \nThe rapid global proliferation of information technology (IT) often leaves libraries at a disadvantage in terms of keeping their services current. However, libraries are always striving to understand their user demographics in order to provide the best possible services. Therefore, libraries continue to take notes from current cyberculture and are continually incorporating a diversified range of interactive technologies in their service repertoires. Virtual reference represents only one small part of a larger library mission to meet the needs of a new generation, sometimes referred to as the \"Google Generation\", of users who have grown up with the internet. For instance, virtual reference may be used in conjunction with embedded Web 2.0 (online social media such as Facebook, YouTube, blogs, del.icio.us, Flickr, etc.) applications in a library's suite of online services. As technological innovations continue, libraries will be watching to find new, more personalized ways of interacting with remote reference users.\n\nThe range of cost-per-transaction of reference interactions has been found to be large, due to the differences in librarian salaries and infrastructural costs required by reference interviews.\n\nWebforms are created for digital reference services in order to help the patron be more productive in asking their question. This document helps the librarian locate exactly what the patron is asking for. Creation of webforms requires design consideration. Because webforms substitute for the reference interview, receiving as much information as possible from the patron is a key function.\n\nAspects commonly found within webforms:\n\n\nSeveral applications exist for providing chat-based reference. Some of these applications are: QuestionPoint, OmniReference, Tutor.com, LibraryH3lp, AspiringKidz.com, and Vienova.com. These applications bear a resemblance to commercial help desk applications. These applications possess functionality such as: chat, co-browsing of webpages, webpage and document pushing, customization of pre-scripted messages, storage of chat transcripts, and statistical reporting.\n\nInstant messaging (IM) services are used by some libraries as a low-cost means of offering chat-based reference, since most IM services are free. Utilizing IM for reference services allows a patron to contact the library from any location via the internet. This service is like the traditional reference interview because it is a live interaction between the patron and the librarian. On the other side the reference interview is different because the conversation does not float away but instead is in print on the screen for the librarian to review if needed to better understand the patron. IM reference services may be for the use of in-house patrons as well as patrons unable to go to the library. If library computers support IM chat programs, patrons may IM from within the library to avoid losing their use of a computer or avoid making embarrassing questions public.\n\nSuccessful IM reference services will:\n\nAt times, IM becomes challenging because of lack of non-verbal cues such as eye contact, and the perceived time pressure. Moreover, formulating the question online without the give and take of nonverbal cues and face to face conversation presents an added obstacle. In addition, to provide effective reference service through IM, it is important to meet higher level of information literacy standards. These standards include evaluating the information and its source, synthesizing the information to create new ideas or products, and understanding the societal, legal, and economic issues surrounding its use.\n\nThe article Live, Digital Reference Marketplace by Buff Hirko contains a comparison of the features of applications for chat-based reference.\n\nSee the entries in the Library Success Wiki's Online Reference Section, including software recommended for web-based chat reference, IM reference, SMS (text messaging) reference, and other types like digital audio or video reference.\n\nVirtual service software programs offered by libraries are often unique, and tailored to the individual library's needs. However, each program may have several distinct features. A knowledge base is a chunk of information that users can access independently. An example of this is a serialized listing of frequently asked questions (FAQ) that a user can read and use at his or her leisure.\n\nOnline chat, or instant messaging (IM) has become a very popular Web-based feature. Instant messaging is a real time conversation that utilizes typed text instead of language. Users may feel a sense of satisfaction with the use of this tool because of their personalized interaction with staff.\n\nThe use of electronic mail (email) in responding to reference questions in libraries has been in use for years. Also, in some cases with the IM feature, a question may be asked that cannot be resolved in online chat. In this instance the staff member may document the inquiring patron’s email address and will the user a response.\n\nWith the increase in use of text messaging (Short Message Service or SMS), some libraries are also adopting text messaging in their virtual reference services. Librarians can use mobile phones, text-to-instant messaging or web-based services to respond to reference questions via text messaging.\n\nCo-browsing, or cooperative browsing, is a virtual reference function that involves interactive control of a user’s web browser. This function enables the librarian to see what the patron has on his or her computer screen. Several types of co-browsing have been offered in mobile devices of late; libraries may have software that incorporates dual modes of co-browsing in a variety of formats. For instance, it is possible to browse on a mobile device within and between documents (such as Word), webpages, and images.\n\nVirtual reference services are growing in popularity in the UK with more institutions accepting queries via email, instant messaging and other chat based services. A study of the use of virtual reference within UK academic institutions showed that 25% currently offer a form of virtual reference, with 54% of academic institutions surveyed considering adding this service.\n\nUK public libraries were instrumental in some of the first steps towards UK-wide internet collaboration amongst libraries with the EARL Consortium (Electronic Access to Resources in Libraries) in 1995, in a time where internet access was a rare commodity for both library staff and the public. Resources were collated and lines of communication opened between libraries across the UK, paving the way for services all over the world to follow suit. There are now a number of area-specific reference services across the UK including Ask A Librarian (UK-wide, established in 1997), Ask Cymru (Welsh and English language service), Enquire (Government funded through the People's Network, also UK-wide), and Ask Scotland. Ask Scotland was created by the Scottish Government's advisory body on libraries, SLIC (Scottish Library and Information Council), and funded by the Public Library Quality Improvement Fund (PLQIF) in June 2009. It uses the Online Computer Library Center's QuestionPoint software.\n\nThe definition formulated by the American Library Association's (ALA) 2004 MARS Digital Reference Guidelines Ad Hoc Committee contains three components:\n\n\nIn January 2011 QuestionPoint and the American Library Association were in talks about offering a National Ask A Librarian service across the whole United States of America. At present the Ask services in the US are run at a local level.\n\nIn Europe some countries offer services in both their own national language and in English. European countries include: Finland, the Netherlands (in Dutch only), Denmark, and France.\n\nOther countries which offer virtual reference services include: Australia, New Zealand, Canada, and the state of Colorado in the United States.\n\nA collaboration between UK and Australian library services, entitled Chasing the Sun, has been initiated using QuestionPoint software so that an all-hours digital reference chat service can be offered. Targeted at health libraries where reference queries from health professionals could occur at any time of the day or night due to medical emergencies, the collaboration between the two countries means that someone will be on hand to field the query at any time. Although the UK libraries involved are currently based in England the programme may expand to other countries and health services if successful.\n\n\n\n\nThe following provide software and technology infrastructure for digital/virtual reference.\n\n\n\n\n\n"}
{"id": "500948", "url": "https://en.wikipedia.org/wiki?curid=500948", "title": "Field guide", "text": "Field guide\n\nA field guide is a book designed to help the reader identify wildlife (plants or animals) or other objects of natural occurrence (e.g. minerals). It is generally designed to be brought into the 'field' or local area where such objects exist to help distinguish between similar objects. Field guides are often designed to help users distinguish animals and plants that may be similar in appearance but are not necessarily closely related.\n\nIt will typically include a description of the objects covered, together with paintings or photographs and an index. More serious and scientific field identification books, including those intended for students, will probably include identification keys to assist with identification, but the publicly accessible field guide is more often a browsable picture guide organized by family, colour, shape, location or other descriptors.\n\nPopular interests in identifying things in nature probably were strongest in bird and plant guides. Perhaps the first popular field guide to plants in the United States was the 1893 \"How to Know the Wildflowers\" by \"Mrs. William Starr Dana\" (Frances Theodora Parsons). In 1890, Florence Merriam published \"Birds Through an Opera-Glass\", describing 70 common species. Focused on living birds observed in the field, the book is considered the first in the tradition of modern, illustrated bird guides. In 1902, now writing as Florence Merriam Bailey (having married the zoologist Vernon Bailey), she published \"Handbook of Birds of the Western United States\". By contrast, the \"Handbook\" is designed as a comprehensive reference for the lab rather a portable book for the field. It was arranged by taxonomic order and had clear descriptions of species size, distribution, feeding, and nesting habits.\n\nFrom this point into the 1930s, features of field guides were introduced by Chester A. Reed and others such as changing the size of the book to fit the pocket, including colour plates, and producing guides in uniform editions that covered subjects such as garden and woodland flowers, mushrooms, insects, and dogs.\n\nIn 1934, Roger Tory Peterson, using his fine skill as an artist, changed the way modern field guides approached identification. Using color plates with paintings of similar species together – and marked with arrows showing the differences – people could use his bird guide in the field to compare species quickly to make identification easier. This technique, the \"Peterson Identification System\", was used in most of Peterson's Field Guides from animal tracks to seashells and has been widely adopted by other publishers and authors as well.\n\nToday, each field guide has its own range, focus and organization. Specialist publishers such as Croom Helm, along with organisations like the Audubon Society, the RSPB, the Field Studies Council, National Geographic, HarperCollins, and many others all produce quality field guides.\n\nIt is somewhat difficult to generalise about how field guides are intended to be used, because this varies from one guide to another, partly depending on how expert the targeted reader is expected to be.\n\nFor general public use, the main function of a field guide is to help the reader identify a bird, plant, rock, butterfly or other natural object down to at least the popular naming level. To this end some field guides employ simple keys and other techniques: the reader is usually encouraged to scan illustrations looking for a match, and to compare similar-looking choices using information on their differences. Guides are often designed to first lead readers to the appropriate section of the book, where the choices are not so overwhelming in number.\n\nGuides for students often introduce the concept of identification keys. Plant field guides such as \"Newcomb's Wildflower Guide\" (which is limited in scope to the wildflowers of northeastern North America) frequently have an abbreviated key that helps limit the search. Insect guides tend to limit identification to Order or Family levels rather than individual species, due to their diversity.\n\nMany taxa show variability and it is often difficult to capture the constant features using a small number of photographs. Illustrations by artists or post processing of photographs help in emphasising specific features needed to for reliable identification. Peterson introduced the idea of lines to point to these key features. He also noted the advantages of illustrations over photographs:\n\nField guides aid in improving the state of knowledge of various taxa. By making the knowledge of experienced museum specialists available to amateurs, they increase the gathering of information by amateurs from a wider geographic area and increasing the communication of these findings to the specialists.\n\n"}
{"id": "597476", "url": "https://en.wikipedia.org/wiki?curid=597476", "title": "Info", "text": "Info\n\nInfo is shorthand for \"information\". It may also refer to:\n\n\n"}
{"id": "17878314", "url": "https://en.wikipedia.org/wiki?curid=17878314", "title": "Information source", "text": "Information source\n\nAn information source is a person, thing, or place from which information comes, arises, or is obtained. Information souces can be known as primary or secondary. That source might then inform a person about something or provide knowledge about it. Information sources are divided into separate distinct categories, primary, secondary, tertiary, and so on.\n\n"}
{"id": "9549311", "url": "https://en.wikipedia.org/wiki?curid=9549311", "title": "Integrative and Comparative Biology", "text": "Integrative and Comparative Biology\n\nIntegrative and Comparative Biology is the scientific journal for the Society for Integrative and Comparative Biology (formerly the American Society of Zoologists). Prior to volume 42 (2002), the journal was known as American Zoologist .\n\n\n"}
{"id": "5995840", "url": "https://en.wikipedia.org/wiki?curid=5995840", "title": "L. G. Pine", "text": "L. G. Pine\n\nLeslie Gilbert Pine (22 December 1907 – 15 May 1987) was a British author, lecturer, and researcher in the areas of genealogy, nobility, history, heraldry and animal welfare. He was born in 1907 in Bristol, England and died in Bury St. Edmunds, Suffolk in 1987. He was the son of Lilian Grace Beswetherick and Henry Moorshead Pine (a tea merchant).\n\nFrom 1935 to 1940 he served as an assistant editor at Burke's Peerage Ltd. During World War II he was an officer in the Royal Air Force intelligence branch, serving in North Africa, Italy, Greece, and India; he retired with the rank of Squadron Leader. After the war and until 1960, he was Burke's executive director. Pine edited \"Burke's Peerage,\" 1949-1959; \"Burke's Landed Gentry (of Great Britain),\" 1952; \"Burke's Landed Gentry (of Ireland),\" 1958; and, \"Burke's Distinguished Families of America,\" 1939, 1947. He also edited \"The International Year Book and Statesmen's Who's Who,\" 1953-1960; \"Author's and Writer's Who's Who,\" 1948, 1960; \"Who's Who in Music,\" 1949; and, \"Who's Who in the Free Churches,\" 1951.\n\nA graduate of London University, he became a Barrister-at-Law, Inner Temple, in 1953. Pine was a member of the International Institute of Genealogy and Heraldry, Fellow of the Society of Antiquaries of Scotland, a Fellow of the Ancient Monuments Society, a Life Fellow of the Institute of Journalists, a Freeman of the City of London, and a Liveryman of the Glaziers' Company. In 1959 he was the unsuccessful Conservative candidate for Bristol Central.\n\nHe was managing editor of a British hunting magazine, \"Shooting Times\", from 1960 to 1964. He later authored an important book highly critical of sport hunting, \"After Their Blood\", in which he wrote: \"It is our duty as men and women of God’s redeemed creation to try not to increase the suffering of the world, but to lessen it. To get rid of bloodsports will be a great step toward this end.\"\n\nIn 1948 Leslie Pine married Grace V. Griffin (20 August 1914- ). Their only child, Richard Pine, was born in London on 21 August 1949.\n\nHis books include:\n\n\nPine is also the primary contributor to the article \"genealogy\" in \"Encyclopædia Britannica\".\n\n"}
{"id": "6487324", "url": "https://en.wikipedia.org/wiki?curid=6487324", "title": "Leishu", "text": "Leishu\n\nThe leishu () is a genre of reference books historically compiled in China and other countries of the Sinosphere. The term is generally translated as \"encyclopedia\", although the \"leishu\" are quite different from the modern notion of encyclopedia.\n\nThe \"leishu\" are composed of sometimes lengthy citations from other works, and often contain copies of entire works, not just excerpts. The works are classified by a systematic set of categories, which are further divided into subcategories. \"Leishu\" may be considered anthologies, but are encyclopedic in the sense that they may comprise the entire realm of knowledge at the time of compilation.\n\nApproximately 600 \"leishu\" were compiled from the early third century until the eighteenth century, of which 200 have survived. The largest \"leishu\" ever compiled was the 1408 \"Yongle Dadian\", containing 370 million Chinese characters, and the largest ever printed was the \"Gujin Tushu Jicheng\", containing 100 million characters and 852,408 pages.\n\nThe genre first appeared in the early third century. The earliest known was the \"Huanglan\" (\"Emperor's mirror\"). Sponsored by the emperor of Cao Wei, it was compiled around 220, but has since been lost. However, the term \"leishu\" was not used until the Song dynasty (960–1279).\n\nIn later imperial China dynasties, such as the Ming and Qing, emperors sponsored monumental projects to compile all known human knowledge into a single \"leishu\", in which entire works, rather than excerpts, were copied and classified by category. The largest \"leishu\" ever compiled, on the order of the Yongle Emperor of Ming, was the \"Yongle Dadian\" containing a total of 370 million Chinese characters. The project involved 2,169 scholars, who worked for four years under general editor Yao Guangxiao. It was completed in 1408, but never printed, as the imperial treasury had run out of money.\n\nThe \"Qinding Gujin Tushu Jicheng\" (Imperially approved synthesis of books and illustrations past and present) is by far the largest \"leishu\" ever printed, containing 100 million characters and 852,408 pages. It was compiled by a team of scholars led by Chen Menglei, and printed between 1726 and 1728, during the Qing dynasty.\n\nThe \"riyong leishu\" (encyclopedias for daily use), containing practical information for people who were literate but below the Confucian elite, were also compiled in the later imperial era. Today, they provide scholars with valuable information on non-elite culture and attitudes.\n\nAccording to Jean-Pierre Diény, the Jiaqing reign (1796–1820) of the Qing dynasty saw the end of the publication of \"leishu\".\n\nOther countries of the Sinosphere also adopted the genre of \"leishu\". In 1712, the \"Sancai Tuhui\", a richly illustrated \"leishu\" compiled by Ming scholar Wang Qi (王圻) in the early 17th century, was printed in Japan as \"Wakan Sansai Zue\". The Japanese version was edited by Terajima Ryōan (寺島良安), a physician born in Osaka.\n\nThe \"leishu\" have played an important role in the preservation of ancient works, many of which have been lost, only preserved completely or partially as part of a \"leishu\" compilation. The 7th-century \"Yiwen Leiju\" is especially valuable. It contains excerpts from 1,400 pre-7th century works, 90% of which have been otherwise lost. Even though the \"Yongle Dadian\" is itself largely lost, the remnants still contain 385 complete books that have been otherwise lost. The \"leishu\" also provide a unique view of the transmission of knowledge and education, and an easy way to locate traditional materials on any given subject.\n\nApproximately 600 \"leishu\" were compiled, from the Cao Wei period (early third century) until the 18th century, of which 200 have survived. Among the most important, in chronological order, are:\n\n\n"}
{"id": "6908619", "url": "https://en.wikipedia.org/wiki?curid=6908619", "title": "Museum of Comparative Zoology", "text": "Museum of Comparative Zoology\n\nThe Museum of Comparative Zoology, full name \"The Louis Agassiz Museum of Comparative Zoology\", often abbreviated simply to \"MCZ\", is the zoology museum located on the grounds of Harvard University in Cambridge, Massachusetts. It is one of three natural history research museums at Harvard whose public face is the Harvard Museum of Natural History. Harvard MCZ's collections consist of some 21 million specimens, of which several thousand are on rotating display at the public museum. The current director of the Museum of Comparative Zoology is James Hanken, the Louis Agassiz Professor of Zoology at Harvard University.\n\nMany of the exhibits in the public museum have not only zoological interest but also historical significance. Past exhibits have included a fossil sand dollar which was found by Charles Darwin in 1834, Captain Cook's mamo, and two pheasants that once belonged to George Washington, now on loan to Mount Vernon in Virginia.\n\nThe Harvard Museum of Natural History is physically connected to the Peabody Museum of Archaeology and Ethnology; for visitors, one admission ticket grants access to both museums. The research collections of the Museum of Comparative Zoology are not open to the public.\n\nThe Museum of Comparative Zoology was founded in 1859 through the efforts of zoologist Louis Agassiz, and the museum used to be referred to as \"The Agassiz\" after its founder. Agassiz designed the collection to illustrate the variety and comparative relationships of animal life.\n\nThe Radcliffe Zoological Laboratory was created in 1894 when Radcliffe College rented a space on the fifth floor of the Museum of Comparative Zoology at Harvard University to convert into a women's laboratory. Prior to this acquisition, Radcliffe science laboratories were taught using inadequate facilities, converting spaces such as bathrooms in old houses into physics laboratories, which Harvard professors often refused to teach in.The laboratory space was converted from an office or storage closet, and was sandwiched between other invertebrate storage rooms on the fifth floor.\n\nThe museum comprises twelve departments: Biological Oceanography, Entomology, Herpetology, Ichthyology, Invertebrate Paleontology, Invertebrate Zoology, Mammalogy, Marine invertebrates, Malacology, Ornithology, Population Genetics, and Vertebrate Paleontology. The Ernst Mayr Library and its archives join in supporting the work of the museum. The Ernst Mayr Library is a founding member of the Biodiversity Heritage Library.\n\nThe museum publishes two journals: the \"Bulletin of the Museum of Comparative Zoology at Harvard College\", first published in 1869, and \"Breviora\", first published in 1956.\n\nIn contrast to numerous more modern museums, the Harvard Museum of Natural History has many hundreds of stuffed animals on display, from the collections of the Museum of Comparative Zoology. Notable exhibits include whale skeletons, the largest turtle shell ever found (eight feet long), \"the Harvard mastodon\", a long \"Kronosaurus\" skeleton, the skeleton of a dodo, and a coelacanth preserved in fluid. The two-story Great Mammal Hall was renovated in 2009 in celebration of the 150th anniversary of founding of the Museum of Comparative Zoology.\n\nNew and changing exhibitions in the Harvard Museum of Natural History include \"Evolution\" (2008); \"The Language of Color\" (2008 to 2013); \"Arthropods: Creatures that Rule\" (2006); \"New England Forests\" (2011); and \"Mollusks: Shelled Masters of the Marine Realm\" (2012).\n\n"}
{"id": "1091767", "url": "https://en.wikipedia.org/wiki?curid=1091767", "title": "Non-well-founded set theory", "text": "Non-well-founded set theory\n\nNon-well-founded set theories are variants of axiomatic set theory that allow sets to contain themselves and otherwise violate the rule of well-foundedness. In non-well-founded set theories, the foundation axiom of ZFC is replaced by axioms implying its negation.\n\nThe study of non-well-founded sets was initiated by Dmitry Mirimanoff in a series of papers between 1917 and 1920, in which he formulated the distinction between well-founded and non-well-founded sets; he did not regard well-foundedness as an axiom. Although a number of axiomatic systems of non-well-founded sets were proposed afterwards, they did not find much in the way of applications until Peter Aczel’s hyperset theory in 1988.\n\nThe theory of non-well-founded sets has been applied in the logical modelling of non-terminating computational processes in computer science (process algebra and final semantics), linguistics and natural language semantics (situation theory), philosophy (work on the Liar Paradox), and in a different setting, non-standard analysis.\n\nIn 1917, Dmitry Mirimanoff introduced the concept of well-foundedness of a set:\n\nIn ZFC, there is no infinite descending ∈-sequence by the axiom of regularity. In fact, the axiom of regularity is often called the \"foundation axiom\" since it can be proved within ZFC (that is, ZFC without the axiom of regularity) that well-foundedness implies regularity. In variants of ZFC without the axiom of regularity, the possibility of non-well-founded sets with set-like ∈-chains arises. For example, a set \"A\" such that \"A\" ∈ \"A\" is non-well-founded.\n\nAlthough Mirimanoff also introduced a notion of isomorphism between possibly non-well-founded sets, he considered neither an axiom of foundation nor of anti-foundation. In 1926, Paul Finsler introduced the first axiom that allowed non-well-founded sets. After Zermelo adopted Foundation into his own system in 1930 (from previous work of von Neumann 1925–1929) interest in non-well-founded sets waned for decades. An early non-well-founded set theory was Willard Van Orman Quine’s New Foundations, although it is not merely ZF with a replacement for Foundation.\n\nSeveral proofs of the independence of Foundation from the rest of ZF were published in 1950s particularly by Paul Bernays (1954), following an announcement of the result in earlier paper of his from 1941, and by Ernst Specker who gave a different proof in his Habilitationsschrift of 1951, proof which was published in 1957. Then in 1957 Rieger's theorem was published, which gave a general method for such proof to be carried out, rekindling some interest in non-well-founded axiomatic systems. The next axiom proposal came in a 1960 congress talk of Dana Scott (never published as a paper), proposing an alternative axiom now called SAFA. Another axiom proposed in the late 1960s was Maurice Boffa's axiom of superuniversality, described by Aczel as the highpoint of research of its decade. Boffa's idea was to make foundation fail as badly as it can (or rather, as extensionality permits): Boffa's axiom implies that every extensional set-like relation is isomorphic to the elementhood predicate on a transitive class.\n\nA more recent approach to non-well-founded set theory, pioneered by M. Forti and F. Honsell in the 1980s, borrows from computer science the concept of a bisimulation. Bisimilar sets are considered indistinguishable and thus equal, which leads to a strengthening of the axiom of extensionality. In this context, axioms contradicting the axiom of regularity are known as anti-foundation axioms, and a set that is not necessarily well-founded is called a hyperset.\n\nFour mutually independent anti-foundation axioms are well-known, sometimes abbreviated by the first letter in the following list:\nThey essentially correspond to four different notions of equality for non-well-founded sets. The first of these, AFA, is based on accessible pointed graphs (apg) and states that two hypersets are equal if and only if they can be pictured by the same apg. Within this framework, it can be shown that the so-called Quine atom, formally defined by Q={Q}, exists and is unique.\n\nEach of the axioms given above extends the universe of the previous, so that: V ⊆ A ⊆ S ⊆ F ⊆ B. In the Boffa universe, the distinct Quine atoms form a proper class.\n\nIt is worth emphasizing that hyperset theory is an extension of classical set theory rather than a replacement: the well-founded sets within a hyperset domain conform to classical set theory.\n\nAczel’s hypersets were extensively used by Jon Barwise and John Etchemendy in their 1987 book \"The Liar\", on the liar's paradox; The book is also good introduction to the topic of non-well-founded sets.\n\nBoffa’s superuniversality axiom has found application as a basis for axiomatic nonstandard analysis.\n\n\n\n"}
{"id": "2673834", "url": "https://en.wikipedia.org/wiki?curid=2673834", "title": "Numeronym", "text": "Numeronym\n\nA numeronym is a number-based word.\n\nMost commonly, a numeronym is a word where a number is used to form an abbreviation (albeit not an acronym or an initialism). Pronouncing the letters and numbers may sound similar to the full word: \"K9\" for \"canine\" (phonetically: \"kay\" + \"nine\").\n\nAlternatively, the letters between the first and last are replaced with a number representing the number of letters omitted, such as \"i18n\" for \"internationalization\". Sometimes the last letter is also counted and omitted. These word shortenings are sometimes called \"alphanumeric acronyms\", \"alphanumeric abbreviations\", or \"numerical contractions\".\n\nAccording to Tex Texin, the first numeronym of this kind was \"S12n\", the electronic mail account name given to Digital Equipment Corporation (DEC) employee Jan Scherpenhuizen by a system administrator because his surname was too long to be an account name. By 1985, colleagues who found Jan's name unpronounceable often referred to him verbally as \"S12n\" (\"ess-twelve-en\"). The use of such numeronyms became part of DEC corporate culture.\n\nA number may also denote how many times the character before or after it is repeated. This is typically used to represent a name or phrase in which several consecutive words start with the same letter, as in W3 (World Wide Web) or W3C (World Wide Web Consortium).\n\nSome numeronyms are composed entirely of numbers, such as \"212\" for \"New Yorker\", \"4-1-1\" for \"information\", \"9-1-1\" for \"help\", and \"101\" for \"basic introduction to a subject\". Words of this type have existed for decades, including those in 10-code, which has been in use since before World War II.\n\nChapter or title numbers of some jurisdictions' statutes have become numeronyms, for example 5150 and 187 from California's penal code. Largely because the production of many American movies and television programs are based in California, usage of these terms has spread beyond its original location and user population.\n\nThe concept of incorporating numbers into words can also be found in Leet-speak, where numbers are frequently substituted for orthographically similar letters (e.g. \"H4CK3D\" for \"HACKED\").\n\nAnne H. Soukhanov, editor of the new \"Microsoft Encarta College Dictionary\", gives the original meaning of the term as \"a telephone number that spells a word or a name\" on a telephone dial.\n\nWhere words have multiple meanings, abbreviations such as these are almost always used to refer to their computing sense; for example, \"G11n\" for \"globalization\" refers to software preparedness for global distribution, and not the social trend of globalization. In some cases, the use of appropriate case makes it easier to distinguish between letters such as uppercase I/i and lower case L/l.\n\n"}
{"id": "3653726", "url": "https://en.wikipedia.org/wiki?curid=3653726", "title": "Psc (military)", "text": "Psc (military)\n\npsc is a post-nominal for \"passed Staff College\" in the Commonwealth militaries of Britain, Bangladesh, Indian, Sri Lanka and Pakistan. It indicates that an officer has undertaken the staff officer course at a Staff College.\n\nThe practice originated in the British Army where the initials \"psc\" appeared in the service lists denoting that the officer had attended the Staff College, Camberley. Royal Navy offers who attended the staff course at Royal Naval College, Greenwich also used the qualification. Since the 1997 amalgamation of staff training officers now receive the letters psc(j) from the Joint Services Command and Staff College.\n\nPSC is used for Bangladeshi Army officers who have attended the Defence Services Command & Staff College (DSCSC), Bangladesh.\n\nInitials psc is used by officers who attended the Defence Services Staff College, Wellington, India.\n\nIn Pakistan initials psc is used by officers who attended the Command and Staff College, Quetta.\n\nOfficers graduated from the Malaysian Armed Forces Staff College, Kuala Lumpur use the initials psc.\n\nIn Sri Lanka, the initials psc are used by Army, Navy and Air Force officers who have gained the Pass Staff College status from a recognized a staff college such as the Defence Services Command and Staff College and the Sri Lanka Air Force Junior Command & Staff College. Such officers are eligible to wear the psc badge.\n"}
{"id": "20110874", "url": "https://en.wikipedia.org/wiki?curid=20110874", "title": "Reference", "text": "Reference\n\nReference is a relation between objects in which one object designates, or acts as a means by which to connect to or link to, another object. The first object in this relation is said to \"refer to\" the second object. It is called a \"name\" for the second object. The second object, the one to which the first object refers, is called the \"referent\" of the first object. A name is usually a phrase or expression, or some other symbolic representation. Its referent may be anything – a material object, a person, an event, an activity, or an abstract concept.\n\nReferences can take on many forms, including: a thought, a sensory perception that is audible (onomatopoeia), visual (text), olfactory, or tactile, emotional state, relationship with other, spacetime coordinate, symbolic or alpha-numeric, a physical object or an energy projection. In some cases, methods are used that intentionally hide the reference from some observers, as in cryptography.\n\nReferences feature in many spheres of human activity and knowledge, and the term adopts shades of meaning particular to the contexts in which it is used. Some of them are described in the sections below.\n\nThe word \"reference\" is derived from Middle English \"referren\", from Middle French \"référer\", from Latin \"referre\", \"to carry back\", formed from the prefix \"re\"- and \"ferre\", \"to bear\". A number of words derive from the same root, including \"refer\", \"referee\", \"referential\", \"referent\", \"referendum\".\n\nThe verb \"refer (to)\" and its derivatives may carry the sense of \"link to\" or \"connect to\", as in the meanings of \"reference\" described in this article. Another sense is \"consult\"; this is reflected in such expressions as reference work, reference desk, job reference, etc.\n\nIn semantics, reference is generally construed as the relationships between nouns or pronouns and objects that are named by them. Hence, the word \"John\" refers to the person John. The word \"it\" refers to some previously specified object. The object referred to is called the \"referent\" of the word. Sometimes the word-object relation is called \"denotation\"; the word denotes the object. The converse relation, the relation from object to word, is called \"exemplification\"; the object exemplifies what the word denotes. In syntactic analysis, if a word refers to a previous word, the previous word is called the \"antecedent\".\n\nGottlob Frege argued that reference cannot be treated as identical with meaning: \"Hesperus\" (an ancient Greek name for the evening star) and \"Phosphorus\" (an ancient Greek name for the morning star) both refer to Venus, but the astronomical fact that '\"Hesperus\" is \"Phosphorus\"' can still be informative, even if the \"meanings\" of \"Hesperus\" and \"Phosphorus\" are already known. This problem led Frege to distinguish between the sense and reference of a word. Some cases seem to be too complicated to be classified within this framework; the acceptance of the notion of secondary reference may be necessary to fill the gap. See also Opaque context.\n\nThe very concept of the linguistic sign is the combination of content and expression, the former of which may refer entities in the world or refer more abstract concepts, e.g. thought.\nCertain parts of speech exist only to express reference, namely anaphora such as pronouns. The subset of reflexives expresses co-reference of two participants in a sentence. These could be the agent (actor) and patient (acted on), as in \"The man washed himself\", the theme and recipient, as in \"I showed Mary to herself\", or various other possible combinations.\n\nIn computer science, references are data types that refer to an object elsewhere in memory and are used to construct a wide variety of data structures, such as linked lists. Generally, a reference is a value that enables a program to directly access the particular data item. Most programming languages support some form of reference. For the specific type of reference used in the C++ language, see reference (C++).\n\nThe notion of reference is also important in relational database theory; see referential integrity.\n\nReferences to many types of printed matter may come in an electronic or machine-readable form. For books, there exists the ISBN and for journal articles, the Digital object identifier (DOI) is gaining relevance. Information on the Internet may be referred to by a Uniform Resource Identifier (URI).\n\nIn terms of mental processing, a self-reference is used in psychology to establish identification with a mental state during self-analysis. This seeks to allow the individual to develop own frames of reference in a greater state of immediate awareness. However, it can also lead to circular reasoning, preventing evolution of thought.\n\nAccording to Perceptual Control Theory (PCT), a reference condition is the state toward which a control system's output tends to alter a controlled quantity. The main proposition is that \"All behavior is oriented all of the time around the control of certain quantities with respect to specific reference conditions.\"\n\nIn academics and scholarship, an author-title-date information in bibliographies and footnotes, specifying complete works of other people. Copying of material by another author without proper citation or without required permissions is plagiarism.\n\nKeeping a diary allows an individual to use references for personal organization, whether or not anyone else understands the systems of reference used. However, scholars have studied methods of reference because of their key role in communication and co-operation between \"different\" people, and also because of misunderstandings that can arise. Modern academic study of reference has been developing since the 19th century.\n\nIn scholarship, a reference may be a citation of a text that has been used in the creation of a piece of work such as an essay, report, or oration. Its primary purpose is to allow people who read such work to examine the author's sources, either for validity or to learn more about the subject. Such items are often listed at the end of an article or book in a section marked \"Bibliography\" or \"References\". A bibliographical section often contains works not cited by the author, but used as background reading or listed as potentially useful to the reader. A reference section contains only those works cited by the author(s) in the main text.\n\nIn patent law, a reference is a document that can be used to show the state of knowledge at a given time and that therefore may make a claimed invention obvious or anticipated. Examples of references are patents of any country, magazine articles, Ph.D. theses that are indexed and thus accessible to those interested in finding information about the subject matter, and to some extent Internet material that is similarly accessible.\n\nIn art, a reference is an item from which a work is based. This may include:\nAnother example of reference is samples of various musical works being incorporated into a new one.\n\n\n"}
{"id": "8912106", "url": "https://en.wikipedia.org/wiki?curid=8912106", "title": "Reference scenario", "text": "Reference scenario\n\nA reference scenario is an imagined situation where a library patron brings a question to a librarian and there is then a conversation, called in the field a reference interview, where the librarian works to help the patron find what he or she wants. These scenarios are used in training future librarians how to help patrons. Basically, a scenario is as short as a couple of sentences, including a question and a situation that underlies that question.\n\nA great deal of reference teaching puts students to researching the answers to made-up questions. This focuses the student on learning about the reference sources at hand by using them to answer those questions. Scenarios are something different. They focus the student on the interaction with patrons. In class practice sessions, one student can be the patron and the other the librarian, as long as the one practicing as the librarian doesn't know the whole scenario in advance.\n\nScenarios are valued because often the question asked is not the end of the patron's information hunt, but the start. Patrons often start by voicing a question that they think the library can answer, rather than the question they are really seeking to answer. Or they pose a question that the librarian doesn't understand. Reference librarian skills are very much about mediating a gap between what the patron wants and what the library can provide. This can involve the librarian making him or herself a partner in the patron's search, teaching them what the library really has to offer, or even just clarifying a confusing word: Does the patron want information about soaps to clean with or soaps as in soap operas?\n\n\n"}
{"id": "23499848", "url": "https://en.wikipedia.org/wiki?curid=23499848", "title": "Secondary source", "text": "Secondary source\n\nIn scholarship, a secondary source is a document or recording that relates or discusses information originally presented elsewhere. A secondary source contrasts with a primary source, which is an original source of the information being discussed; a primary source can be a person with direct knowledge of a situation, or a document created by such a person. \n\nA secondary source is one that gives information about a primary source. In this source, the original information is selected, modified and arranged in a suitable format. Secondary sources involve generalization, analysis, interpretation, or evaluation of the original information. \nThe most accurate classification for any given source is not always obvious. \"Primary\" and \"secondary\" are relative terms, and some sources may be classified as primary or secondary, depending on how they are used. A third level, the tertiary source, such as an encyclopedia or dictionary, resembles a secondary source in that it contains analysis, but attempts to provide a broad introductory overview of a topic.\n\nInformation can be taken from a wide variety of objects, but this classification system is only useful for a class of sources that are called symbolic sources. Symbolic sources are sources that are intended to communicate information to someone. Common symbolic sources include written documents such as letters and notes, but not, for example, bits of broken pottery and scraps of food excavated from a midden, regardless of how much information can be extracted from an ancient trash heap, or how little can be extracted from a written document. \n\nMany sources can be considered either primary or secondary, depending on the context in which they are used. Moreover, the distinction between \"primary\" and \"secondary\" sources is subjective and contextual, so that precise definitions are difficult to make. For example, if a historical text discusses old documents to derive a new historical conclusion, it is considered to be a primary source for the new conclusion, but a secondary source of information found in the old documents. Other examples in which a source can be both primary and secondary include an obituary or a survey of several volumes of a journal counting the frequency of articles on a certain topic.\n\nWhether a source is regarded as primary or secondary in a given context may change, depending upon the present state of knowledge within the field. For example, if a document refers to the contents of a previous but undiscovered letter, that document may be considered \"primary\", since it is the closest known thing to an original source, but if the letter is later found, it may then be considered \"secondary\".\n\nAttempts to map or model scientific and scholarly communication need the concepts of primary, secondary and further \"levels\". One such model is the UNISIST model of information dissemination. Within such a model these concepts are defined in relation to each other, and the acceptance of this way of defining the concepts are connected to the acceptance of the model.\n\nSome other modern languages use more than one word for the English word \"source\". German usually uses \"Sekundärliteratur\" (\"secondary literature\") for secondary sources for historical facts, leaving \"Sekundärquelle\" (\"secondary source\") to historiography. A \"Sekundärquelle\" is a source which can tell about a lost \"Primärquelle\" (\"primary source\"), such as a letter quoting from minutes which are no longer known to exist, and so cannot be consulted by the historian.\n\nIn general, secondary sources are self-described as review articles or meta-analysis.\n\nPrimary source materials are typically defined as \"original research papers written by the scientists who actually conducted the study.\" An example of primary source material is the Purpose, Methods, Results, Conclusions sections of a research paper (in IMRAD style) in a scientific journal by the authors who conducted the study. In some fields, a secondary source may include a summary of the literature in the Introduction of a scientific paper, a description of what is known about a disease or treatment in a chapter in a reference book, or a synthesis written to review available literature. A survey of previous work in the field in a primary peer-reviewed source is secondary source information. This allows secondary sourcing of recent findings in areas where full review articles have not yet been published.\n\nA book review that contains the judgment of the reviewer about the book is a primary source for the reviewer's opinion, and a secondary source for the contents of the book. A summary of the book within a review is a secondary source.\n\nIn library and information sciences, secondary sources are generally regarded as those sources that summarize or add commentary to primary sources in the context of the particular information or idea under study.\n\nAn important use of secondary sources in the field of mathematics has been to make difficult mathematical ideas and proofs from primary sources more accessible to the public; in other sciences tertiary sources are expected to fulfill the introductory role.\n\nSecondary sources in history and humanities are usually books or scholarly journals, from the perspective of a later interpreter, especially by a later scholar. In the humanities, a peer reviewed article is always a secondary source.\nThe delineation of sources as primary and secondary first arose in the field of historiography, as historians attempted to identify and classify the sources of historical writing. In scholarly writing, an important objective of classifying sources is to determine the independence and reliability of sources. In original scholarly writing, historians rely on primary sources, read in the context of the scholarly interpretations.\n\nFollowing the Rankean model established by German scholarship in the 19th century, historians use archives of primary sources. Most undergraduate research projects rely on secondary source material, with perhaps snippets of primary sources.\n\nIn the legal field, source classification is important because the persuasiveness of a source usually depends upon its history. Primary sources may include cases, constitutions, statutes, administrative regulations, and other sources of binding legal authority, while secondary legal sources may include books, the headnotes of case reports, articles, and encyclopedias. Legal writers usually prefer to cite primary sources because only primary sources are authoritative and precedential, while secondary sources are only persuasive at best.\n\n\"A secondary source is a record or statement of an event or circumstance made by a non-eyewitness or by someone not closely connected with the event or circumstances, recorded or stated verbally either at or sometime after the event, or by an eye-witness at a time after the event when the fallibility of memory is an important factor.\" Consequently, according to this definition, a first-hand account written long after the event \"when the fallibility of memory is an important factor\" is a secondary source, even though it may be the first published description of that event.\n\nAn autobiography can be a secondary source in history or the humanities when used for information about topics other than its subject. For example, many first hand accounts of events in World War I written in the post-war years were influenced by the then prevailing perception of the war which was significantly different from contemporary opinion.\n\n\n"}
{"id": "4106285", "url": "https://en.wikipedia.org/wiki?curid=4106285", "title": "Self-referential encoding", "text": "Self-referential encoding\n\nEvery day, people are presented with endless amounts of information, and in an effort to help keep track and organize this information, people must be able to recognize, differentiate and store information. One way to do that is to organize information as it pertains to the self. The overall concept of self-reference suggests that people interpret incoming information in relation to themselves, using their self-concept as a background for new information. Examples include being able to attribute personality traits to oneself or to identify recollected episodes as being personal memories of the past. The implications of self-referential processing are evident in many psychological phenomena. For example, the \"cocktail party effect\" notes that people attend to the sound of their names even during other conversation or more prominent, distracting noise. Also, people tend to evaluate things related to themselves more positively (This is thought to be an aspect of implicit self-esteem). For example, people tend to prefer their own initials over other letters. The self-reference effect (SRE) has received the most attention through investigations into memory. The concepts of self-referential encoding and the SRE rely on the notion that relating information to the self during the process of encoding it in memory facilitates recall, hence the effect of self-reference on memory. In essence, researchers have investigated the potential mnemonic properties of self-reference.\n\nResearch includes investigations into self-schema, self-concept and self-awareness as providing the foundation for self-reference's role in memory. Multiple explanations for the self-reference effect in memory exist, leading to a debate about the underlying processes involved in the self-reference effect. In addition, through the exploration of the self-reference effect, other psychological concepts have been discovered or supported, including simulation theory and the effect.\nAfter researchers developed a concrete understanding of the self-reference effect, many expanded their investigations to consider the self-reference effect in particular groups like those with autism spectrum disorders or those experiencing depression.\n\nSelf-knowledge can be categorized by structures in memory or schemata. A self-schema is a set of facts or beliefs that one has about themselves. For any given trait, an individual may or may not be \"schematic\"; that is, the individual may or may not think about themselves as to where they stand on that trait. For example, people who think of themselves as very overweight or who identify themselves to a greater extent based on their body weight would be considered \"schematic\" on the attribute of body weight. Thus, many everyday events, such as going out for a meal or discussing a friend's eating habits, could induce thoughts about the self. When people relate information to something that has to do with the self, it facilitates memory. Self-descriptive adjectives that fit into one's self-schema are easier to remember than adjectives not viewed as related to the self. Thus, the self-schema is an aspect of oneself that is used as an encoding structure that brings upon memory of information consistent with one's self-schema. Memories that are elaborate and well encoded are usually the result of self-referent correlations during the process of remembering. During the process of encoding, trait representations are encoded in long term memory either directly or indirectly. When they are directly encoded, it is in terms of relating to the self, and when it is indirectly encoded it is done through spouts of episodic information instead of information about the self.\n\nSelf-schema is often used as somewhat of a database for encoding personal data. The self-schema is also used by paying selective attention to outside information and internalizing that information more deeply in one's memory depending on how much that information relates to their schema. When self-schema is engaged, traits that go along with one's view of themselves are better remembered and recalled. These traits are also often recalled much better when processed with respect to the self. Similarly, items that are encoded with the self are based on one's self-schema. Processing the information should balance out when recalled for individuals who have a self-schema that goes along with the information.\n\nSelf-schemas do not necessarily only involve individual traits. People self-categorize at different levels that range from more personal to more social. Self-schemas have three main categories which play a role: the personal self, the relational self, and the collective self. The personal self deals with individual level characteristics, the relational self deals with intimate relationship partners, and the collective self deals with group identities, relating to self-important social groups to which one belongs (e.g., one's family or university). Information that is related to any type of self-schema, including group-related knowledge structures facilitates memory.\n\nIn order for the self to be an effective encoding mechanism, it must be a uniform, consistent, well-developed schema. It has been shown that identity exploration leads to the development of self-knowledge which facilitates self-judgments. Identity exploration led to shorter decision times, higher confidence ratings and more intrusions in memory tasks. Previous researchers hypothesized that words compatible with a person's self-schema are easily accessible in memory and are more likely than incompatible words to intrude on a schema-irrelevant memory task. In one experiment, when participants were asked to decide if certain adjectives were \"like me\" or \"not like me,\" they made the decisions faster when the words were compatible with their self-schema.\n\nHowever, despite the existence of the self-reference effect when considering schemata consistent adjectives, the connection between the self and memory can lead to a larger number of mistakes in recognition, commonly referred to as false alarms. Rogers et al. (1979) found that people are more likely to falsely recognize adjectives they had previously designated to be self-descriptive. Expanding on this, Strube et al. (1986) found that false alarms occurred more for self-schema consistent content, presumably because the presence of such words in the schema makes them more accessible in memory.\n\nIn addition to investigating the self-reference effect in regards to schemata consistent information, Strube et al. discussed how counter schemata information relates to this framework. They noted that the pattern of making correct decisions more rapidly did not hold when considering words that countered a person's self-schema, presumably because they were difficult to integrate into memory due to lack of a preexisting structure. That is, they lacked the organizational structure of encoding because they did not fall into the \"like me\" category, and elaboration would not work because prior connections to the adjective did not exist.\n\nTwo of the most common functions of the self receiving significant attention in research are the self-acting to organize the individual's understanding of the social environment, and the self functioning to regulate behavior through self-evaluation. The concept of self-awareness is considered to be the foundational principle for both functions of the self. Some research presents self-awareness in terms of self-focused attention whereas Hull and Levy suggest that self-awareness refers to the encoding of information based on its relevance to the self. Based on the latter interpretation of self-awareness, individuals must identify the aspects of situations that are relevant to themselves and their behavior will be shaped accordingly. Hull and Levy suggest that self-awareness corresponds to the encoding of information cued by self-symbolic stimuli, and examine the idea of self-awareness as a method of encoding. They structured an investigation that examined self-referent encoding in individuals with different levels of self-awareness, predicting that individuals with higher levels of self-consciousness would encode self-relevant information more deeply than other information, and that they would encode it more deeply than individuals with low levels of self-consciousness. The results of their investigation supported their hypothesis that self-focused attention is not enough to explain the role of self-awareness on attribution. Their results suggest that self-awareness leads to increased sensitivity to the situationally defined meanings of behavior, and therefore organizes the individual's understanding of the social environment. The research presented by Hull and Levy led to future research on the encoding of information associated with self-awareness.\n\nIn later research, Hull and colleagues examined the associations between self-referential encoding, self-consciousness and the extent to which a stimulus is consistent with self-knowledge. They first assumed that the encoding of a stimulus is facilitated if an individual's working memory already contains information consistent with the stimulus, and suggested that self-consciousness as an encoding mechanism relies on an individual's self-knowledge. It is known that situational and dispositional factors may activate certain pools of knowledge, moving them into working memory, and guiding the processing of certain stimulus information.\n\nIn order to better understand the idea of activating information in memory, Hull et al. presented an example of how information is activated. They referred to the sentence \"The robber took the money from the bank\". In English, the word bank has two applicable meanings in the context of this sentence (monetary institution and river shore). However, the monetary institution meaning of the word is more highly activated in this context due to the addition of the words robber and money to the sentence, because they are associatively relevant and therefore pull the monetary institution definition for bank into working memory. Once information is added to working memory, meanings and associations are more easily drawn. Therefore, the meaning of this example sentence is almost universally understood.\n\nIn reference to self-consciousness and self-reference, the connection between self-consciousness and self-referent encoding relies on such information activation. Research suggests that self-consciousness activates knowledge relating to the self, thereby guiding the processing of self-relevant information. Three experiments conducted by Hull and colleagues provided evidence that a manipulation of accessible self-knowledge impacts self-referent encoding based on the self-relevance of such information, individual differences in the accessibility of self-knowledge (self-consciousness) impacts perception, and a mediation relationship exists between self-consciousness and individual differences in self-referential encoding.\n\nSimilar to how self-awareness impacts the availability of self-knowledge and the encoding of self-relevant information, through the development of the self-schema, people develop and maintain certain personality characteristics leading to a variety of behavior patterns. Research has been done on the differences between Type A and Type B behavior patterns, focusing on how people in each group respond to environmental information and their interpretation of the performance of others and themselves. It has been found that Type A behavior is characterized by competitive achievement striving, time urgency and hostility, whereas Type B is usually defined as an absence of Type A characteristics. When investigating causal attributions for hypothetical positive and negative outcomes, Strube et al. found that Type A individuals were more self-serving, in that they took greater responsibility for positive than negative effects. Strube and colleagues argued that this could be a result of the fact that schema-consistent information is more easily remembered and the ease with which past successes and failures are recalled, determined by self-schema, would impact attributions. It is reasonable to believe that Type A's might recall successes more easily and hence be more self-serving.\n\nInfluential psychologists Craik and Lockhart laid the groundwork for research focused on self-referential encoding and memory. In 1972 they proposed their Depth of Processing framework which suggests that memory retention depends on how the stimulus material was encoded in memory. Their original research considered structural, phonemic, and semantic encoding tasks, and showed that semantic encoding is the best method to aid in recall. They asked participants to rate 40 descriptive adjectives on one of four tasks; Structural (Big font or small font?), Phonemic (Rhymes with xxx?), Semantic (Means same as xxx?), or Self-reference (Describes you?). This was then followed by an \"incidental recall task\". This is where participants are asked, without prior warning, to recall as many of the words they had seen as possible within a given time limit. Craik and Tulving's original experiment showed that structural and phonemic tasks lead only to \"shallow\" encoding, while the semantic tasks lead to \"deep\" encoding and resulted in better recall.\n\nHowever, in 1977, it was shown that self-relevant or self-descriptive encoding leads to even better recall than semantic tasks. Experts suggest that the call on associative memory required by semantic tasks is what provides the advantage over structural or phonemic tasks, but is not enough to surpass the benefit provided by self-referential encoding. The fact that self-reference was shown to be a stronger memory encoding method than semantic tasks is what led to more significant interest in the field One early and significant experiment aimed to place self-reference on Craik and Lockhart's depth of processing hierarchy, and suggested that self-reference was a more beneficial encoding method than semantic tasks. In this experiment, participants filled out self-ratings on 84 adjectives. Months later, these participants were revisited and were randomly shown 42 of those words. They then had to select the group of 42 \"revisited\" words out of the total original list. The researchers argued that if the \"self\" was involved in memory retrieval, participants would incorrectly recognize words that were more self-descriptive In another experiment, subjects answered yes or no to cue questions about 40 adjective in 4 tasks (structural, phonemic, semantic and self-referential) and later had to recall the adjectives. This experiment validated the strength of self-reference as an encoding method, and indicated it developed a stronger memory trace than the semantic task.\n\nResearchers are implementing a new strategy by developing different encoding tasks that enhance memory very similarly to self-referential encoding. Symons (1990) had findings that went against the norm when he was unable to find evidence of self-schematicity in the self-reference effect. Another finding was that when referencing gender and religion, there was a low memory recall when compared with referencing the self. A meta-analysis by Symons and Johnson (1997) showed self-reference resulting in better memory in comparison to tasks relying on semantic encoding or other-referent encoding. According to Symons and Johnson, self-referencing questions elicit elaboration and organization in memory, both of which creating a deeper encoding and thus facilitate memory.\n\nTheorists that favor the view that the self has a special role believe that the self leads to more in depth processing, leading to easier recall during self-reference tasks. Theorists also promote the self-schema as being one of the sole inhibitors that allow for recall from deep memory. Thorndyke and Hayes-Roth had the goal of focusing on the process made by the active memory schemata. Sex-typed individuals recall trait adjectives that go along with their sex role more quickly than trait adjectives that are not. During the process of free recall, these individuals also showed more patterns for gender clustering than other sexually typed individuals.\n\nAs research on self-referential encoding became more prolific, some psychologists took an opportunity to delineate specific self-referential encoding tasks. It is noted that descriptive tasks are those that require participants to determine if a stimulus word can be classified as \"self-descriptive.\" Autobiographical tasks are those that require participants to use the stimulus word as a cue to recall an autobiographical memory. Results from experiments that differentiated between these types of self-referential encoding found that they both produced better recall than semantic tasks, and neither was more advantageous than the other. However, research does suggest that the two types of self-referential encoding do rely on different processes to facilitate memory. In most experiments discussed, these types of self- referential encoding were not differentiated.\n\nIn a typical self-reference task, adjectives are presented and classified as either self-descriptive or not. For example, in a study by Dobson and Shaw, adjectives about the self that were preselected were given to the participants and they decide whether or not the adjectives are self-descriptive. The basis for making certain judgments, decisions, inferences and decisions is a self-referent encoding task. If two items are classified as self-descriptive there is no reason one trait would not be equally as easy to retrieve as the other on a self-reference task.\n\nWhile a significant amount of research supports the existence of the self-reference effect, the processes behind it are not well understood. However, multiple hypotheses have been introduced, and two main arguments have been developed: the elaborative processing hypothesis and the organizational processing hypothesis. Encodings in reference to the self are so elaborate because of the information one has about the self. Information encoded with the self is better remembered than information encoded with reference to something else.\n\nElaboration refers to the encoding of a single word by forming connections between it and other material already stored in memory. By creating these connections between the stimulus word and other material already in memory, multiple routes for retrieval of the stimulus word are formed. Based on the depth of processing framework, memory retention increases as elaboration during encoding increases. The Elaborative Processing Hypothesis would suggest that any encoding task that leads to the development of the most trace elaboration or associations is the best for memory retention. Additional research on the depth of processing hierarchy suggests that self-reference is the superior method of information encoding. The elaborative hypothesis would suggest this is because self-reference creates the most elaborate trace, due to the many links that can be made between the stimulus and information about the self already in memory.\n\nThe organizational processing hypothesis was proposed by Klein and Kihlstrom. This hypothesis suggests that encoding is best prompted by considering stimulus words in relation to one another. This thought process and relational thinking creates word to word associations. These inter-item associations are paths in memory that can be used during retrieval. Also, the category labels that define the relations between stimulus items can be used as item cues. Evidence of the organizational component of encoding is demonstrated through the clustering of words during recall. Word clustering during recall indicates that relational information was used to store the words in memory. Rogers, Kuiper and Kirker showed that self-referential judgments were more likely to encourage organization than semantic ones. Therefore, they suggested the self-reference effect was likely due to the organizational processing endured by self-referential encoding.\n\nStructural, phonemic and semantic tasks within the depth of processing paradigm require words to be considered individually, and lend themselves to an elaborative approach. As such, it can be argued that self-referential encoding is superior because it leads to an indirect division of words into categories: words that describe me versus words that do not. Due to this connection between self-reference and organizational processing, further research has been done on this area. Klein and Kihlstrom's research suggests first that, like previous research, self-reference led to better recall than semantic and structural encoding. Second, they found that self-referentially encoded words were more clustered in recall than words from other tasks, suggesting higher levels of organizational processing. From this they concluded that the organization, not encoding task, is what makes self-referential encoding superior \n\nPsychologists Einstein and Hunt showed that both elaborative processing and organizational processing facilitate recall. However, their research argues that the effectiveness of either approach depends on how related the stimulus words are to one another. A list of highly related stimulus words would be better encoded using the elaborative method. The relations between the words would be evident to subjects; therefore, they would not gain any additional pathways for retrieval by encoding the words based on their categorical membership. Instead, the other information gained through elaborative processing would be more beneficial. On the other hand, a list of stimulus words with little relation would be better stored to memory through the organizational method. Since the words have no obvious connection to one another, subjects would likely encode them individually, using an elaborative approach. Since relational information wouldn't be readily detected, focusing on it would add to memory by creating new traces for retrieval. Superior recall was better explained by a combination of elaboration and organization.\nUltimately, the exact processes behind self-referential encoding that makes it superior to other encoding tasks are still under debate. Research suggests that if elaborative processing is behind self-referential encoding, a self-referential task should have the same effect as an elaborative task, whereas if organizational processing underlies the self-reference effect self-referential encoding tasks should function like organizational tasks. To test this, Klein and Loftus ran a 3x2 study testing organizational, elaborative and self-referential encoding with lists of 30 related or unrelated words. When participants were asked to memorize the unrelated list, recall and clustering were higher for the organizational task, which produced almost equal results to the self-referential task, suggesting that has an organizational basis. For the list of related words, the elaborative task led to better recall and had matched results to the self-reference task, suggesting an elaborative basis. This research, then, suggests that the self-reference effect cannot be explained by a single type of processing. Instead, self-referential encoding must lead to information in memory that incorporates item specific and relational information.\n\nOverall, the SRE relies on the unique mnemonic aspects of the self. Ultimately, if the research is suggesting that the self has superior elaborative or organizational properties, information related to the self should be more easily remembered and recalled. The research presented suggests that self-referential encoding is superior because it promotes organization and elaboration simultaneously, and provides self-relevant categories that promote recall.\n\nThe field of social brain science is aimed at examining the neural foundations of social behavior. Neuroimaging and neuropsychology have led to the examination of neuroanatomy and its connection to psychological topics. Through this research, neuropsychologists have found a connection between social cognitive functioning and the medial prefrontal cortex (mPFC). In addition, the mPFC has been connected to reflection and introspection about personal mental states. Supporting these findings, it has been shown that damage to the mPFC is connected to impairments with self-reflection, introspection and daydreaming, as well as social competence, but not other areas of functioning. As such, the mPFC has been connected to self-referential processing.\n\nThe research discussed by those focusing on the neuroanatomy of self-referential processing included similar tasks to the memory and depth of processing research discussed previously. When participants were asked to judge adjectives based in whether or not they were self-descriptive, it was noted that the more self-relevant the trait, the stronger the activation of the mPFC. In addition, it was shown that the mPFC was activated during the appraisal of one's own personality traits, as well as during trait retrieval. One study showed that the more activity in the mPFC during self-referential judgments, the more likely the word was to be remembered on a subsequent surprise memory test. These results suggest that the mPFC is involved in both self-referential processing and in creating self-relevant memories.\n\nMedial prefrontal cortex (mPFC) activation occurs during processing of self-relevant information. When self-referent judgment is more relatable and less negative, the mFPC is activated. Finding support clear cut circuits that have high levels of activation when cognitive and emotional aspects of self-reflection are present. The caudate nucleus has not been associated with self-reference before, however, Fossati and colleagues found activity while participants were retrieving self-relevant trait adjectives. The ventral anterior cingulate cortex (vACC) is also a part of the brain that becomes activated when there are signs of self-referencing and processing. The vACC is activated when self-descriptive information is negative. There is also pCC (posterior cingulate cortex) activity seen in neuroimaging studies during self-referential processing.\n\nGiven all of the neurological support for the effect of self-reference on encoding and memory, there is still a debate in the psychological community about whether or not the self-reference effect signifies a special functional role played by the self in cognition. Generally, this question is met by people that have two opposing views on the processes behind self-reference. On one side of the debate, people believe that the self has special mnemonic abilities because it is a unique cognitive structure. On the other side, people support the arguments described above that suggest there is no special structure, but instead, the self-reference effect is simply a part of the standard depth of processing hierarchy. Since the overall hypothesis is the same for both sides of the debate, that self-relevant material leads to enhanced memory, it is difficult to test them using strictly behavioral measures. Therefore, PET and fMRI scans have been used to see the neural marker of self-referential mental activity.\n\nPrevious studies have shown that areas of the left prefrontal cortex are activated during semantic encoding. Therefore, if the self-reference effect works the same way, as part of the depth of processing hierarchy, the same brain region should be activated when judging traits related to the self. However, if the self has unique mnemonic properties, then self-referential tasks should activate brain regions distinct from those activated during semantic tasks. The field is still at is infancy, but future work on this hypothesis might help to settle the debate about the underlying processes of self-referential encoding.\n\nWhile not able to completely settle the debate over the foundation of self-referential processing, studies on the neurological aspect of personality trait judgments did lead to a related, significant result. It has been shown that judging personality traits about oneself and a close friend activated overlapping brain regions, and the activated regions have all been implicated in self-reference. Noting the similarity between making self-judgments and judgments about close others led to the introduction of the simulation theory of empathy. Simulation theory rests on the idea that one can make inferences about others by using the knowledge they have about themselves. In essence, the theory suggests that people use self-reflection to understand or predict the mental state of others. The more similar a person perceives another to be, the more active the mPFC has shown to be, suggesting more deep or intricate self-reference. However, this effect can cause people to make inaccurate judgments about others or to believe that their own opinions are representative of others in general. This misrepresentation is referred to as the false-consensus effect.\n\nIn addition to simulation theory, other expansions of the self-reference effect have been examined. Through studying the self, researchers have found that the self consists of many independent cognitive representations. For example, the personal self composed of individual characteristics is separate from the relational self which is based on relationships with significant others. These two forms of self are again separate from the collective self which corresponds to a particular group identity. Noting the existence of the collective self and the different group identities that combine to form such a self-representation led researchers to question if information stored in reference to a social group identity has the same effects in memory as information stored in reference to the individual self. In essence, researchers questioned if the self-reference effect can be extended to include situations where the self is more socially defined, producing a group-reference effect.\n\nPrevious research supports the idea that the group-reference effect should exist from a theoretical standpoint. First, the self-expansion model argues that individuals incorporate characteristics of their significant others (or other in-group members into the development of their self-concept. From this model, it is reasonable to conclude that characteristics that are common to both oneself and their significant others (or in-group members) would be more accessible. Second, the previous research discussed suggests that the self-reference effect is due to some combination of organizational, elaborative, mental cueing or evaluative properties of self-referential encoding tasks. Given that we have significant stores of knowledge about our social identities, and such collective identities provide an organizational framework, it is reasonable to assume that a group-reference task would operate similar to that of a self-reference task.\n\nIn order to test these claims, Johnson and colleagues aimed to test whether the self-reference effect generalized to group level identities. Their first study was structured to simply assess if group-reference influenced subsequent memory. In their experiment, they used membership at a particular university as the group of reference. They included group-reference, self-reference and semantic tasks. The experiment replicated the self-reference effect, consistent with previous research. In addition, evidence for a group-reference effect was found. Group-referenced encoding produced better recall than the semantic tasks, and the level of recall from the group-referenced task was not significantly different from the self-referenced task.\n\nDespite finding evidence of a group-reference effect, Johnson and colleagues pointed out that people identify with numerous groups, each with unique characteristics. Therefore, in order to reach conclusive evidence of a group-reference effect, alternative group targets need to be considered. In a second experiment by Johnson et al., the group of reference was modified to be the family of the individual. This group has fewer exemplars than the pool of university students, and affective considerations of the family as a group should be strong. No specific instructions or definitions were provided for family, allowing individuals to consider either the group as a whole (prototype) or specific exemplars (group). When the experiment was repeated using family as the group of reference, group-reference produced recall as much as self-reference. The mean number of recall for the group-reference was higher than self-reference. Participants indicated that they considered both the prototype and individual exemplars when responding to the questions, suggesting that the magnitude of the group-reference effect might not be dependent on the number of exemplars in the target group.\nBoth experiments presented by Johnson et al. found evidence for the group-reference effect. However, these conclusions are limited to the target groups of university students and family. Other research included gender (males and females) and religion (Jewish) as the reference groups and the group-reference effect on memory was not as evident. The group-reference recall for these two groups was not significantly more advantageous than the semantic task. Questioning what characteristics of reference groups that lead to the group-reference effect, a meta-analysis of all four group-reference conditions was performed. This analysis found that self-reference emerged as the most powerful encoding device; however, evidence was found to support the existence of a group-reference effect. The size of the reference groups and number of specific, individual exemplars was hypothesized to influence the existence of the group-reference effect. In addition, accessibility and level of knowledge about group members may also impact such an effect. So, while university students is a much larger group than family, individual exemplars may be more readily accessible than those in a religious group. Similarly, different cognitive representations were hypothesized to influence the group-reference effect. When a larger group is considered, people may be more likely to consider a prototype which may lead to fewer elaborations and cues later on. Smaller groups may lead to relying on the prototype and specific exemplars. Finally, desirability judgments that influence later processing may be influenced by self-reference and certain group-reference tasks. Individuals may be more sensitive to evaluative implications for the personal self and some group identities, but not others.\n\nGroups are also a major part of the self; therefore we attribute the role that different groups play in our self-concept also play a role in the self-reference effect. We process information about group members similarly to how we process for ourselves. Recall of remarks referencing our home and our self and group to familiarity of those aspects of our self. Reference to the self and social group and the identity that comes along with being a part of a social group are equally affective for memory. This is especially true when the groups are small, rather than large.\n\nUltimately, the group-reference effect provides evidence to explain the tendency to notice or pay attention to and remember statements made in regard to our home when traveling in a foreign place. Considering the proposal that groups form part of the self, this phenomenon can be considered an extension of the self-reference effect. Similar to the memorable nature of references to a person's individual self, references to social identities are seemed to be privileged in memory as well.\n\nOnce the foundation of research on self-referential encoding was established, psychologists began to explore how the concept applied to different groups of people, and connected to different phenomena.\n\nIndividuals diagnosed with autism spectrum disorders (ASDs) can display a wide range of symptoms. Some of the most common characteristics of individuals with ASDs include impairments with social functioning, language and communication difficulties, repetitive behaviors and restricted interests. In addition, it is often noted that these individuals are more \"self-focused.\" That is, they have difficulty seeing things from another's perspective. Despite being self-focused, though, research has shown that individuals with ASD's often have difficulty identifying or describing their emotions or the emotions of others. When asked to describe their daily experiences, responses from individuals on the autism spectrum tended to focus more on physical descriptions rather than mental and emotional states. In regards to their social interactions and behavior differences, it is thought that these individuals lack top down control, and therefore, their bottom up decisions remain unchecked. This simply suggests that these individuals cannot use their prior knowledge and memory to make sense of new input, but instead react to each new input individually, compiling them to make a whole picture \n\nNoting the difficulty individuals with ASDs experience with self-awareness, it was thought that they might have difficulty with self-related memory processes. Psychologists questioned if these individuals would show the typical self-reference effect in memory. In one Depth of Processing Study, participants were asked questions about the descriptiveness of certain stimulus words. However, unlike previous DOP studies that focused on phonemic, structural, semantic and self-referential tasks, the tasks were altered for this experiment. To test the referential abilities of individuals with ASD's, the encoding tasks were divided into: \"the self,\" asking to what extent a stimulus word described oneself, \"similar close other,\" asking to what extent a stimulus word was descriptive of one's best friend, \"dissimilar non-close other,\" asking to what extent a stimulus word was descriptive of Harry Potter, and a control group that was asked to determine the number of syllables in each word. Following these encoding tasks, participants were given thirty minutes before a surprise memory task. It was found that individuals with ASD's had no impairment in memory for words encoded in the syllable or dissimilar non-close other condition. However, they had decreased memory for words related to the self.\n\nTherefore, while research suggests that self-referentially encoded information is encoded more deeply than other information, the research on individuals with ASD's showed no advantage for memory recognition with self-reference tasks over semantic encoding tasks. This suggests that individuals with ASD's don't preferentially encode self-relevant information. Psychologists have investigated the biological basis for the decreased self-reference effect among individuals with Autism Spectrum Disorders and have suggested that it may be due to less specialized neural activity in the mPFC for those individuals. However, while individuals with ASD's showed smaller self-reference effects than the control group, some evidence of a self-reference effect was evident in some cases. This indicates that self-referent impairments are a matter of degree, not total absence.\n\nLombardo and his colleagues measured empathy among individuals with ASD's, and showed that these individuals scored lower than the control group on all empathy measures. This may be a result of the difficulty for these individuals to understand or take the perspective of others, in conjunction with their difficulty identifying emotions. This has implications for simulation theory, because these individuals are unable to use their self-knowledge to make conclusions about similar others.\n\nUltimately, the research suggests that people with ASD's might benefit from being more self-focused. The better their ability to reflect on themselves, the better the can mentalize with others.\n\nThere are three possible relations between cognitive processes and anxiety and depression. The first is whether cognitive processes are actually caused by the onset of clinically diagnosed symptoms of major depression or just generalized sadness or anxiousness. The second is whether emotional disorders such as depression and anxiety are able to be considered as caused by cognitions. And the third is whether different specific cognitive processes are able to be considered associates of different disorders. Kovacs and Beck (1977) posited a schematic model of depression where an already depressed self was primed by outside prompts that negatively impacted cognitive illusions of the world in the eye of oneself. These prompts only led participants to a more depressive series of emotions and behavior. The results from the study done by Derry and Kuiper supported Beck's theory that a negative self-schema is present in people, especially those with depressive disorder. Depressed individuals attribute depressive adjectives to themselves more than nondepressive adjectives. Those suffering from a more mild case of depression have trouble deciphering between the traits of themselves and others which results in a loss of their self-esteem and their negative self-evaluation. A depressive schema is what causes the negativity reported by those suffering from depression. Kuiper and Derry found that self-referent recall enhancement was limited only to nondepressed content.\n\nGenerally, self-focus is association with negative emotions. In particular private self-focus is more strongly associated with depression than public self-focus. Results from brain-imaging studies shows\nthat during self-referential processing, those with major depressive disorder show greater activation in the medial prefrontal cortex, suggesting that depressed individuals may be exhibiting greater cognitive control than\nnon-depressed individuals when processing self-relevant information.\n"}
{"id": "40849944", "url": "https://en.wikipedia.org/wiki?curid=40849944", "title": "Sources for the historicity of Jesus", "text": "Sources for the historicity of Jesus\n\nChristian sources, such as the New Testament books in the Christian Bible, include detailed stories about Jesus but scholars differ on the historicity of specific episodes described in the Biblical accounts of Jesus. The only two events subject to \"almost universal assent\" are that Jesus was baptized by John the Baptist and was crucified by the order of the Roman Prefect Pontius Pilate.\n\nNon-Christian sources that are used to study and establish the historicity of Jesus include Jewish sources such as Josephus, and Roman sources such as Tacitus. These sources are compared to Christian sources such as the Pauline Epistles and the Synoptic Gospels. These sources are usually independent of each other (e.g. Jewish sources do not draw upon Roman sources), and similarities and differences between them are used in the authentication process.\n\nIn a review of the state of research, the Jewish scholar Amy-Jill Levine stated that \"no single picture of Jesus has convinced all, or even most scholars\" and that all portraits of Jesus are subject to criticism by some group of scholars.\n\nThe writings of the 1st century Romano-Jewish historian Flavius Josephus include references to Jesus and the origins of Christianity. Josephus' \"Antiquities of the Jews\", written around 93–94 CE, includes two references to Jesus in Books and .\n\nOf the two passages, the James passage in Book 20 is used by scholars to support the existence of Jesus, the \"Testimonium Flavianum\" in Book 18 his crucifixion. Josephus' James passage attests to the existence of Jesus as a historical person and that some of his contemporaries considered him the Messiah. According to Bart Ehrman, Josephus' passage about Jesus was altered by a Christian scribe, including the reference to Jesus as the Messiah.\n\nA textual argument against the authenticity of the James passage is that the use of the term \"Christos\" there seems unusual for Josephus. An argument based on the flow of the text in the document is that, given that the mention of Jesus appears in the \"Antiquities\" before that of the John the Baptist, a Christian interpolator may have inserted it to place Jesus in the text before John. A further argument against the authenticity of the James passage is that it would have read well even without a reference to Jesus.\n\nThe passage deals with the death of \"James the brother of Jesus\" in Jerusalem. Whereas the works of Josephus refer to at least twenty different people with the name Jesus, this passage specifies that this Jesus was the one \"who was called Christ\". Louis Feldman states that this passage, above others, indicates that Josephus did say something about Jesus.\n\nModern scholarship has almost universally acknowledged the authenticity of the reference in of the \"Antiquities\" to \"the brother of Jesus, who was called Christ, whose name was James\", and considers it as having the highest level of authenticity among the references of Josephus to Christianity.\n\nThe \"Testimonium Flavianum\" (meaning the testimony of Flavius [Josephus]) is the name given to the passage found in of the \"Antiquities\" in which Josephus describes the condemnation and crucifixion of Jesus at the hands of the Roman authorities. Scholars have differing opinions on the total or partial authenticity of the reference in the passage to the execution of Jesus by Pontius Pilate. The general scholarly view is that while the \"Testimonium Flavianum\" is most likely not authentic in its entirety, it is broadly agreed upon that it originally consisted of an authentic nucleus with a reference to the execution of Jesus by Pilate which was then subject to Christian interpolation. Although the exact nature and extent of the Christian redaction remains unclear, there is broad consensus as to what the original text of the \"Testimonium\" by Josephus would have looked like.\n\nThe references found in \"Antiquities\" have no parallel texts in the other work by Josephus such as the \"Jewish War\", written twenty years earlier, but some scholars have provided explanations for their absence, such as that the \"Antiquities\" covers a longer time period and that during the twenty-year gap between the writing of the \"Jewish Wars\" (c. 70 CE) and \"Antiquities\" (after 90 CE) Christians had become more important in Rome and were hence given attention in the \"Antiquities\".\n\nA number of variations exist between the statements by Josephus regarding the deaths of James and the New Testament accounts. Scholars generally view these variations as indications that the Josephus passages are not interpolations, because a Christian interpolator would more likely have made them correspond to the Christian traditions. Robert Eisenman provides numerous early Christian sources that confirm the Josephus testament, that James was the brother of Jesus.\n\nThe Roman historian and senator Tacitus referred to Christ, his execution by Pontius Pilate and the existence of early Christians in Rome in his final work, \"Annals\" (c. AD 116), . The relevant passage reads: \"called Christians by the populace. Christus, from whom the name had its origin, suffered the extreme penalty during the reign of Tiberius at the hands of one of our procurators, Pontius Pilatus.\"\n\nScholars generally consider Tacitus's reference to the execution of Jesus by Pontius Pilate to be both authentic, and of historical value as an independent Roman source about early Christianity that is in unison with other historical records. William L. Portier has stated that the consistency in the references by Tacitus, Josephus and the letters to Emperor Trajan by Pliny the Younger reaffirm the validity of all three accounts.\n\nTacitus was a patriotic Roman senator and his writings shows no sympathy towards Christians. Andreas Köstenberger and separately Robert E. Van Voorst state that the tone of the passage towards Christians is far too negative to have been authored by a Christian scribe – a conclusion shared by John P. Meier Robert E. Van Voorst states that \"of all Roman writers, Tacitus gives us the most precise information about Christ\".\n\nJohn Dominic Crossan considers the passage important in establishing that Jesus existed and was crucified, and states: \"That he was crucified is as sure as anything historical can ever be, since both Josephus and Tacitus... agree with the Christian accounts on at least that basic fact.\" Bart D. Ehrman states: \"Tacitus's report confirms what we know from other sources, that Jesus was executed by order of the Roman governor of Judea, Pontius Pilate, sometime during Tiberius's reign.\" Eddy and Boyd state that it is now \"firmly established\" that Tacitus provides a non-Christian confirmation of the crucifixion of Jesus.\n\nAlthough the majority of scholars consider it to be genuine, a few scholars question the authenticity of the passage given that Tacitus was born 25 years after Jesus' death.\n\nSome scholars have debated the historical value of the passage given that Tacitus does not reveal the source of his information. Gerd Theissen and Annette Merz argue that Tacitus at times had drawn on earlier historical works now lost to us, and he may have used official sources from a Roman archive in this case; however, if Tacitus had been copying from an official source, some scholars would expect him to have labeled Pilate correctly as a \"prefect\" rather than a \"procurator\". Theissen and Merz state that Tacitus gives us a description of widespread prejudices about Christianity and a few precise details about \"Christus\" and Christianity, the source of which remains unclear. However, Paul R. Eddy has stated that given his position as a senator Tacitus was also likely to have had access to official Roman documents of the time and did not need other sources.\n\nMichael Martin notes that the authenticity of this passage of the Annals has also been disputed on the grounds that Tacitus would not have used the word “messiah” in an authentic Roman document.\n\nWeaver notes that Tacitus spoke of the persecution of Christians, but no other Christian author wrote of this persecution for a hundred years.\n\nHotema notes that this passage was not quoted by any Church father up to the 15th century, although the passage would have been very useful to them in their work; and that the passage refers to the Christians in Rome being a multitude, while at that time the Christian congregation in Rome would actually have been very small.\n\nRichard Carrier has put forward the ideas that the 'Christ, the author of this name, was executed by the procurator Pontius Pilate in the reign of Tiberius' line is a Christian interpolation and that Tacitus wrote about Chrestians not Christians.\n\nScholars have also debated the issue of hearsay in the reference by Tacitus. Charles Guignebert argued that \"So long as there is that possibility [that Tacitus is merely echoing what Christians themselves were saying], the passage remains quite worthless\". R. T. France states that the Tacitus passage is at best just Tacitus repeating what he had heard through Christians. However, Paul R. Eddy has stated that as Rome's preeminent historian, Tacitus was generally known for checking his sources and was not in the habit of reporting gossip. Tacitus was a member of the Quindecimviri sacris faciundis, a council of priests whose duty it was to supervise foreign religious cults in Rome, which as Van Voorst points out, makes it reasonable to suppose that he would have acquired knowledge of Christian origins through his work with that body.\n\nMara (son of Sarapion) was a Stoic philosopher from the Roman province of Syria. Sometime between 73 AD and the 3rd century, Mara wrote a letter to his son (also called Sarapion) which may contain an early non-Christian reference to the crucifixion of Jesus.\n\nThe letter refers to the unjust treatment of \"three wise men\": the murder of Socrates, the burning of Pythagoras, and the execution of \"the wise king\" of the Jews. The author explains that in all three cases the wrongdoing resulted in the future punishment of those responsible by God and that when the wise are oppressed, not only does their wisdom triumph in the end, but God punishes their oppressors.\n\nThe letter includes no Christian themes and the author is presumed to be a pagan. Some scholars see the reference to the execution of the \"wise king\" of the Jews as an early non-Christian reference to Jesus. Criteria that support the non-Christian origin of the letter include the observation that \"king of the Jews\" was not a Christian title, and that the letter's premise that Jesus lives on based on the wisdom of his teachings is in contrast to the Christian concept that Jesus continues to live through his resurrection.\n\nScholars such as Robert Van Voorst see little doubt that the reference to the execution of the \"king of the Jews\" is about the death of Jesus. Others such as Craig A. Evans see less value in the letter, given its uncertain date, and the possible ambiguity in the reference.\n\nThe Roman historian Suetonius (c. 69 – after 122 CE) made references to early Christians and their leader in his work \"Lives of the Twelve Caesars\" (written 121 CE). The references appear in and which describe the lives of Roman Emperors Claudius and Nero. The Nero 16 passage refers to the abuses by Nero and mentions how he inflicted punishment on Christians – which is generally dated to around AD 64. This passage shows the clear contempt of Suetonius for Christians - the same contempt expressed by Tacitus and Pliny the younger in their writings, but does not refer to Jesus himself.\n\nThe earlier passage in Claudius, may include a reference to Jesus, but is subject to debate among scholars. In Suetonius refers to the expulsion of Jews by Claudius and states:\n\nThe reference in Claudius 25 involves the agitations in the Jewish community which led to the expulsion of some Jews from Rome by Claudius, and is likely the same event mentioned in the Acts of the Apostles (). Most historians date this expulsion to around AD 49–50. Suetonius refers to the leader of the Christians as \"Chrestus\", a term also used by used by Tacitus, referred in Latin dictionaries as a (amongst other things) version of 'Christus'. However, the wording used by Suetonius implies that Chrestus was alive at the time of the disturbance and was agitating the Jews in Rome. This weakens the historical value of his reference as a whole, and there is no overall scholarly agreement about its value as a reference to Jesus. However, the confusion of Suetonius also points to the lack of Christian interpolation, for a Christian scribe would not have confused the Jews with Christians.\n\nMost scholars assume that in the reference Jesus is meant and that the disturbances mentioned were due to the spread of Christianity in Rome. However, scholars are divided on the value of the Suetonius' reference. Some scholars such as Craig A. Evans, John Meier and Craig S. Keener see it as a likely reference to Jesus. Others such as Stephen Benko and H. Dixon Slingerland see it as having little or no historical value.\n\nMenahem Stern states Suetonius definitely was referring to Jesus; because he would have added \"a certain\" to Chrestus if he had meant some unknown agitator.\n\nThe Babylonian Talmud in a few cases includes possible references to Jesus using the terms \"Yeshu\", \"Yeshu ha-Notzri\", \"ben Stada\", and \"ben Pandera\". Some of these references probably date back to the Tannaitic period (70–200 CE). In some cases, it is not clear if the references are to Jesus, or other people, and scholars continue to debate their historical value, and exactly which references, if any, may be to Jesus.\n\nRobert Van Voorst states that the scarcity of Jewish references to Jesus is not surprising, given that Jesus was not a prominent issue for the Jews during the first century, and after the devastation caused by the Siege of Jerusalem in the year 70, Jewish scholars were focusing on preserving Judaism itself, rather than paying much attention to Christianity.\n\nRobert Eisenman argues that the derivation of Jesus of Nazareth from \"ha-Notzri\" is impossible on etymological grounds, as it would suggest rather \"the Nazirite\" rather than \"the Nazarene\".\n\nVan Voorst states that although the question of who was referred to in various points in the Talmud remains subject to debate among scholars, in the case of \"Sanhedrin 43a\" (generally considered the most important reference to Jesus in rabbinic literature), Jesus can be confirmed as the subject of the passage, not only from the reference itself, but from the context that surrounds it, and there is little doubt that it refers to the death of Jesus of Nazareth. Christopher M. Tuckett states that if it is accepted that death narrative of Sanhedrin 43a refers to Jesus of Nazareth then it provides evidence of Jesus' existence and execution.\n\nAndreas Kostenberger states that the passage is a Tannaitic reference to the trial and death of Jesus at Passover and is most likely earlier than other references to Jesus in the Talmud. The passage reflects hostility toward Jesus among the rabbis and includes this text:\n\nIt is taught: On the eve of Passover they hung Yeshu and the crier went forth for forty days beforehand declaring that \"[Yeshu] is going to be stoned for practicing witchcraft, for enticing and leading Israel astray. Anyone who knows something to clear him should come forth and exonerate him.\" But no one had anything exonerating for him and they hung him on the eve of Passover. \n\nPeter Schäfer states that there can be no doubt that the narrative of the execution of Jesus in the Talmud refers to Jesus of Nazareth, but states that the rabbinic literature in question are not Tannaitic but from a later Amoraic period and may have drawn on the Christian gospels, and may have been written as responses to them. Bart Ehrman and separately Mark Allan Powell state that given that the Talmud references are quite late, they can give no historically reliable information about the teachings or actions of Jesus during his life.\n\nAnother reference in early second century Rabbinic literature (Tosefta Hullin II 22) refers to Rabbi Eleazar ben Dama who was bitten by a snake, but was denied healing in the name of Jesus by another Rabbi for it was against the law, and thus died. This passage reflects the attitude of Jesus' early Jewish opponents, i.e. that his miracles were based on evil powers.\n\nEddy and Boyd, who question the value of several of the Talmudic references state that the significance of the Talmud to historical Jesus research is that it never denies the existence of Jesus, but accuses him of sorcery, thus indirectly confirming his existence. R. T. France and separately Edgar V. McKnight state that the divergence of the Talmud statements from the Christian accounts and their negative nature indicate that they are about a person who existed. Craig Blomberg states that the denial of the existence of Jesus was never part of the Jewish tradition, which instead accused him of being a sorcerer and magician, as also reflected in other sources such as Celsus. Andreas Kostenberger states that the overall conclusion that can be drawn from the references in the Talmud is that Jesus was a historical person whose existence was never denied by the Jewish tradition, which instead focused on discrediting him.\n\nPliny the Younger (c. 61 – c. 112), the provincial governor of Pontus and Bithynia, wrote to Emperor Trajan \"c\". 112 concerning how to deal with Christians, who refused to worship the emperor, and instead worshiped \"Christus\". Charles Guignebert, who does not doubt that Jesus of the Gospels lived in Gallilee in the 1st century, nevertheless dismisses this letter as acceptable evidence for a historical Jesus.\n\nThallus, of whom very little is known, and none of whose writings survive, wrote a history allegedly around the middle to late first century CE, to which Eusebius referred. Julius Africanus, writing \"c\" 221, links a reference in the third book of the \"History\" to the period of darkness described in the crucifixion accounts in three of the Gospels . It is not known whether Thallus made any mention to the crucifixion accounts; if he did, it would be the earliest noncanonical reference to a gospel episode, but its usefulness in determining the historicity of Jesus is uncertain. The dating of Thallus is dependent on him writing about an event during the 207th Olympiad (49–52 AD), which means he wrote after that date, not near that date. This depends on the text being corrupt, which would mean Thallus could have been writing after the 217th Olympiad (89–92 AD), or even the 167th Olympiad (112–109 BC). He is first referenced by Theophilus, writing around 180 AD, which means Thallus could have written any time between 109 BC and 180 AD. All we know is Thallus mentioned a solar eclipse, and as solar eclipses are not possible at Passover, that would mean Thallus was not talking about the crucifixion of Jesus at all.\n\nPhlegon of Tralles, AD 80–140, similar to Thallus, Julius Africanus mentions a historian named Phlegon who wrote a chronicle of history around AD 140, where he records:\n“Phlegon records that, in the time of Tiberius Caesar, at full moon, there was a full eclipse of the sun from the sixth to the ninth hour.” (Africanus, Chronography, 18:1) Phlegon is also mentioned by Origen (an early church theologian and scholar, born in Alexandria):\n“Now Phlegon, in the thirteenth or fourteenth book, I think, of his Chronicles, not only ascribed to Jesus a knowledge of future events . . . but also testified that the result corresponded to His predictions.” (Origen Against Celsus, Book 2, Chapter 14)\n“And with regard to the eclipse in the time of Tiberius Caesar, in whose reign Jesus appears to have been crucified, and the great earthquakes which then took place … ” (Origen Against Celsus, Book 2, Chapter 33)\n“Jesus, while alive, was of no assistance to himself, but that he arose after death, and exhibited the marks of his punishment, and showed how his hands had been pierced by nails.” (Origen Against Celsus, Book 2, Chapter 59). However, Eusebius in The Chronicon (written in the 4th century AD) records what Phlegon said verbatim. \"Now, in the fourth year of the 202nd Olympiad [32 AD], a great eclipse of the sun occurred at the sixth hour [noon] that excelled every other before it, turning the day into such darkness of night that the stars could be seen in heaven, and the earth moved in Bithynia, toppling many buildings in the city of Nicaea.\" Phlegon never mentions Jesus or the 3 hour darkness. He also mentions a solar eclipse, which can not occur at Passover. Apart from the year (which may be a corruption), this description fits an earthquake and eclipse that occurred in North West Turkey on November, 29 AD.\n\nCelsus writing late in the second century produced the first full-scale attack on Christianity. Celsus' document has not survived but in the third century Origen replied to it, and what is known of Celsus' writing is through the responses of Origen. According to Origen, Celsus accused Jesus of being a magician and a sorcerer. While the statements of Celsus may be seen as valuable, they have little historical value, given that the wording of the original writings can not be examined.\n\nThe Dead Sea Scrolls are first century or older writings that show the language and customs of some Jews of Jesus' time. Scholars such as Henry Chadwick see the similar uses of languages and viewpoints recorded in the New Testament and the Dead Sea Scrolls as valuable in showing that the New Testament portrays the first century period that it reports and is not a product of a later period. However, the relationship between the Dead Sea scrolls and the historicity of Jesus has been the subject of highly controversial theories, and although new theories continue to appear, there is no overall scholarly agreement about their impact on the historicity of Jesus, despite the usefulness of the scrolls in shedding light on first-century Jewish traditions.\n\nThe following sources are disputed, and of limited historical value, but they are at least proof of Christians existing and being known and talked about in the first and second centuries.\n\nThere is a limestone burial box from the 1st century known as the James Ossuary with the Aramaic inscription, \"James, son of Joseph, brother of Jesus.\" The authenticity of the inscription was challenged by the Israel Antiquities Authority, who filed a complaint with the Israeli police. In 2012, the owner of the ossuary was found not guilty, with the judge ruling that the authenticity of the ossuary inscription had not been proven either way. It has been suggested it was a forgery.\n\nVarious books, memoirs and stories were written about Jesus by the early Christians. The most famous are the gospels of Matthew, Mark, Luke and John. All but one of these are believed to have been written within 50–70 years of the death of Jesus, with the Gospel of Mark believed to be the earliest, and the last the Gospel of John. Blainey writes that the oldest surviving record written by an early Christian is a short letter by St Paul: the First Epistle to the Thessalonians, which appeared about 25 years after the death of Jesus. This letter, while important in describing issues for the development of Gentilic Christianity, contains little of significance for understanding the life of the historic Jesus.\n\nBart Ehrman, Robert Eisenman and others critical of traditional Christian views, in assessing the problems involved in conducting historical Jesus research, say the Gospels are full of discrepancies, were written decades after Jesus' death, by authors who had not witnessed any events in Jesus' life. They go on to say the Gospels were authored not by eyewitnesses who were contemporary with the events that they narrate but rather by people who did not know Jesus, see anything he did, or hear anything he taught, and that the authors did not even share a language with Jesus. The accounts they produced are not disinterested; they are narratives produced by Christians who actually believed in Jesus, and were not immune from slanting the stories in light of their biases. Ehrman points out that the texts are widely inconsistent, full of discrepancies and contradictions in both details and larger portraits of who Jesus was.\n\nIn the context of Christian sources, even if all other texts are ignored, the Pauline epistles can provide some information regarding Jesus. This information does not include a narrative of the life of Jesus, and refers to his existence as a person, but adds few specific items apart from his death by crucifixion. This information comes from those letters of Paul whose authenticity is not disputed. Paul was not a companion of Jesus and claims his information comes from the holy spirit acquired after Jesus' death.\n\nOf the thirteen letters that bear Paul's name, seven are considered authentic by almost all scholars, and the others are generally considered pseudepigraphic. The 7 undisputed letters (and their approximate dates) are: 1 Thessalonians (c. 51 CE), Philippians (c. 52–54 CE), Philemon (c. 52–54 CE), 1 Corinthians (c. 53–54 CE), Galatians (c. 55 CE), 2 Corinthians (c. 55–56 CE) and Romans (c. 55–58 CE). The authenticity of these letters is accepted by almost all scholars, and they have been referenced and interpreted by early authors such as Origen and Eusebius.\n\nGiven that the Pauline epistles are generally dated to AD 50 to AD 60, they are the earliest surviving Christian texts that include information about Jesus. These letters were written approximately twenty to thirty years after the generally accepted time period for the death of Jesus, around AD 30–36. The letters were written during a time when Paul recorded encounters with the disciples of Jesus, e.g. states that several years after his conversion Paul went to Jerusalem and stayed with Apostle Peter for fifteen days. During this time, Paul disputed the nature of Jesus' message with Jesus's brother James, concerning the importance of adhering to kosher food restrictions and circumcision, important features of determining Jewish identity.\n\nThe Pauline letters were not intended to provide a narrative of the life of Jesus, but were written as expositions of Christian teachings. In Paul's view, the earthly life of Jesus was of a lower importance than the theology of his death and resurrection,a theme that permeates Pauline writings. However, the Pauline letters clearly indicate that for Paul Jesus was a real person (born of a woman as in Gal 4.4), a Jew (\"born under the law\", Romans 1.3) who had disciples (1 Corinthians 15.5), who was crucified (as in 1 Corinthians 2.2 and Galatians 3.1) and who resurrected from the dead (1 Corinthians 15.20, Romans 1.4 and 6.5, Philippians 3:10–11). And the letters reflect the general concept within the early Gentillic Christian Church that Jesus existed, was crucified and was raised from the dead.\n\nThe references by Paul to Jesus do not in themselves prove the existence of Jesus, but they do establish that the existence of Jesus was the accepted norm within the early Christians (including the Christian community in Jerusalem, given the references to collections there) twenty to thirty years after the death of Jesus, at a time when those who could have been acquainted with him could still be alive.\n\nThe seven Pauline epistles that are widely regarded as authentic include the following information that along with other historical elements are used to study the historicity of Jesus:\n\nThe existence of only these references to Jesus in the Pauline epistles has given rise to criticism of them by G. A. Wells, who is generally accepted as a leader of the movement to deny the historicity of Jesus. When Wells was still denying the existence of Jesus, he criticized the Pauline epistles for not mentioning items such as John the Baptist or Judas or the trial of Jesus and used that argument to conclude that Jesus was not a historical figure.\n\nJames D. G. Dunn addressed Wells' statement and stated that he knew of no other scholar that shared that view, and most other scholars had other and more plausible explanations for the fact that Paul did not include a narrative of the life of Jesus in his letters, which were primarily written as religious documents rather than historical chronicles at a time when the life story of Jesus could have been well known within the early Church. Dunn states that despite Wells' arguments, the theories of the non-existence of Jesus are a \"thoroughly dead thesis\".\n\nWhile Wells no longer denies the existence of Jesus, he has responded to Dunn, stating that his arguments from silence not only apply to Paul but all early Christian authors, and that he still has a low opinion of early Christian texts, maintaining that for Paul Jesus may have existed a good number of decades before.\n\nThe Pauline letters sometimes refer to creeds, or confessions of faith, that predate their writings. For instance reads: \"For what I received I passed on to you as of first importance: that Christ died for our sins according to the Scriptures, that he was buried, that he was raised on the third day according to the Scriptures.\" refers to Romans 1:2 just before it which mentions an existing gospel, and in effect may be treating it as an earlier creed.\n\nOne of the keys to identifying a pre-Pauline tradition is given in \n\nHere Paul refers to others before him who preached the creed. James Dunn states that indicates that in the 30s Paul was taught about the death of Jesus a few years earlier.\n\nThe Pauline letters thus contain Christian creed elements of pre-Pauline origin. The antiquity of the creed has been located by many Biblical scholars to less than a decade after Jesus' death, originating from the Jerusalem apostolic community. Concerning this creed, Campenhausen wrote, \"This account meets all the demands of historical reliability that could possibly be made of such a text,\" whilst A. M. Hunter said, \"The passage therefore preserves uniquely early and verifiable testimony. It meets every reasonable demand of historical reliability.\"\n\nThese creeds date to within a few years of Jesus' death, and developed within the Christian community in Jerusalem. Although embedded within the texts of the New Testament, these creeds are a distinct source for Early Christianity. This indicates that existence and death of Jesus was part of Christian belief a few years after his death and over a decade before the writing of the Pauline epistles.\n\nThe four canonical gospels, Matthew, Mark, Luke, and John, are the main sources for the biography of Jesus' life, the teachings and actions attributed to him. Three of these (Matthew, Mark, and Luke) are known as the synoptic Gospels, from the Greek σύν (syn \"together\") and ὄψις (opsis \"view\"), given that they display a high degree of similarity in content, narrative arrangement, language and paragraph structure. The presentation in the fourth canonical gospel, i.e. John, differs from these three in that it has more of a thematic nature rather than a narrative format. Scholars generally agree that it is impossible to find any direct literary relationship between the synoptic gospels and the Gospel of John.\n\nThe authors of the New Testament generally showed little interest in an absolute chronology of Jesus or in synchronizing the episodes of his life with the secular history of the age. The gospels were primarily written as theological documents in the context of early Christianity with the chronological timelines as a secondary consideration. One manifestation of the gospels being theological documents rather than historical chronicles is that they devote about one third of their text to just seven days, namely the last week of the life of Jesus in Jerusalem. Although the gospels do not provide enough details to satisfy the demands of modern historians regarding exact dates, scholars have used them to reconstruct a number of portraits of Jesus. However, as stated in the gospels do not claim to provide an exhaustive list of the events in the life of Jesus.\n\nScholars have varying degrees of certainty about the historical reliability of the accounts in the gospels, and the only two events whose historicity is the subject of almost universal agreement among scholars are the baptism and crucifixion of Jesus. Scholars such as E.P. Sanders and separately Craig A. Evans go further and assume that two other events in the gospels are historically certain, namely that Jesus called disciples, and caused a controversy at the Temple.\n\nEver since the Augustinian hypothesis, scholars continue to debate the order in which the gospels were written, and how they may have influenced each other, and several hypothesis exist in that regard, e.g. the Markan priority hypothesis holds that the Gospel of Mark was written first c. 70 CE. In this approach, Matthew is placed at being sometime after this date and Luke is thought to have been written between 70 and 100 CE. However, according to the competing, and more popular, Q source hypothesis, the gospels were not independently written, but were derived from a common source called Q. The two-source hypothesis then proposes that the authors of Matthew and Luke drew on the Gospel of Mark as well as on Q.\n\nThe gospels can be seen as having three separate lines: A literary line which looks at it from a textual perspective, secondly a historical line which observes how Christianity started as a renewal movement within Judaism and eventually separated from it, and finally a theological line which analyzes Christian teachings. Within the historical perspective, the gospels are not simply used to establish the existence of Jesus as sources in their own right alone, but their content is compared and contrasted to non-Christian sources, and the historical context, to draw conclusions about the historicity of Jesus.\n\nTwo possible patristic sources that may refer to eye witness encounters with Jesus are the early references of Papias and Quadratus, reported by Eusebius of Caesarea in the 4th century.\n\nThe works of Papias have not survived, but Eusebius quotes him as saying:\n\nRichard Bauckham states that while Papias was collecting his information (c. 90), Aristion and the elder John (who were Jesus' disciples) were still alive and teaching in Asia minor, and Papias gathered information from people who had known them. However, the exact identity of the \"elder John\" is wound up in the debate on the authorship of the Gospel of John, and scholars have differing opinions on that, e.g. Jack Finegan states that Eusebius may have misunderstood what Papias wrote, and the elder John may be a different person from the author of the fourth gospel, yet still a disciple of Jesus. Gary Burge, on the other hand sees confusion on the part of Eusebius and holds the elder John to be different person from the apostle John.\n\nThe letter of Quadratus (possibly the first Christian apologist) to emperor Hadrian (who reigned 117 – 138) is likely to have an early date and is reported by Eusebius in his \"Ecclesiastical History\" 4.3.2 to have stated:\n\nBy \"our Savior\" Quadratus means Jesus and the letter is most likely written before AD 124. Bauckham states that by \"our times\" he may refer to his early life, rather than when he wrote (117–124), which would be a reference contemporary with Papias. Bauckham states that the importance of the statement attributed to Quadratus is that he emphasizes the \"eye witness\" nature of the testimonies to interaction with Jesus. Such \"eye witness statements\" abound in early Christian writings, particularly the pseudonymous Christian Apocrypha, Gospels and Letters, in order to give them credibility.\n\nA number of later Christian texts, usually dating to the second century or later, exist as New Testament apocrypha, among which the gnostic gospels have been of major recent interest among scholars. The 1945 discovery of the Nag Hammadi library created a significant amount of scholarly interest and many modern scholars have since studied the gnostic gospels and written about them. However, the trend among the 21st century scholars has been to accept that while the gnostic gospels may shed light on the progression of early Christian beliefs, they offer very little to contribute to the study of the historicity of Jesus, in that they are rather late writings, usually consisting of sayings (rather than narrative, similar to the hypothesised Q documents), their authenticity and authorship remain questionable, and various parts of them rely on components of the New Testament. The focus of modern research into the historical Jesus has been away from gnostic writings and towards the comparison of Jewish, Greco-Roman and canonical Christian sources.\n\nAs an example, Bart Ehrman states that gnostic writings of the Gospel of Thomas (part of the Nag Hammadi library) have very little value in historical Jesus research, because the author of that gospel placed no importance on the physical experiences of Jesus (e.g. his crucifixion) or the physical existence of believers, and was only interested in the secret teachings of Jesus rather than any physical events. Similarly, the Apocryphon of John (also part of the Nag Hammadi library) has been useful in studying the prevailing attitudes in the second century, and questions of authorship regarding the Book of revelation, given that it refers to , but is mostly about the post ascension teachings of Jesus in a vision, not a narrative of his life. Some scholars such as Edward Arnal contend that the Gospel of Thomas continues to remain useful for understanding how the teachings of Jesus were transmitted among early Christians, and sheds light on the development of early Christianity.\n\nThere is overlap between the sayings of Jesus in the apocryphal texts and canonical Christian writings, and those not present in the canonical texts are called agrapha. There are at least 225 agrapha but most scholars who have studied them have drawn negative conclusions about the authenticity of most of them and see little value in using them for historical Jesus research. Robert Van Voorst states that the vast majority of the agrapha are certainly inauthentic. Scholars differ on the number of authentic agrapha, some estimating as low as seven as authentic, others as high as 18 among the more than 200, rendering them of little value altogether. While research on apocryphal texts continues, the general scholarly opinion holds that they have little to offer to the study of the historicity of Jesus given that they are often of uncertain origin, and almost always later documents of lower value.\n\n\n"}
{"id": "3423601", "url": "https://en.wikipedia.org/wiki?curid=3423601", "title": "Stumpers-L", "text": "Stumpers-L\n\nThe Stumpers-L electronic mailing list, was a resource available for librarians and others to discuss reference questions which they were unable to answer using available resources. It was succeeded by the similar Project Wombat.\n\nStumpers-L began in 1992, created by Ann Feeney, a library school graduate student at Rosary College in River Forest, Illinois, in the United States. It was moved to Concordia University, Chicago, then back to Rosary, which was then renamed Dominican University. From 2002 to 2005 it was maintained by the Dominican University Graduate School of Library and Information Science program. At the end of 2005 Dominican University ceased hosting the list. A replacement list, known as Project Wombat, commenced in January 2006, and is hosted by Project Gutenberg.\n\nOriginally the Stumpers-L archive was a gopher resource, but migrated to the World Wide Web once the web became more universally used in the mid-1990s.\n\nTypical Stumpers-L topics include:\n\nA book of Stumpers-L questions and answers was published in 1998 by Random House, edited by Fred Shapiro of Yale and titled \"Stumpers! Answers to Hundreds of Questions That Stumped The Experts\" (). Shapiro was an active member; other prominent members include Barbara and David P. Mikkelson, the co-editors of \"Snopes.com.\n\nThe unofficial mascot of the Stumpers-L list is the wombat.\n\n"}
{"id": "26681002", "url": "https://en.wikipedia.org/wiki?curid=26681002", "title": "Text annotation", "text": "Text annotation\n\nText Annotation is the practice and the result of adding a note or gloss to a text, which may include highlights or underlining, comments, footnotes, tags, and links. Text annotations can include notes written for a reader's private purposes, as well as shared annotations written for the purposes of collaborative writing and editing, commentary, or social reading and sharing. In some fields, text annotation is comparable to metadata insofar as it is added post hoc and provides information about a text without fundamentally altering that original text. Text annotations are sometimes referred to as marginalia, though some reserve this term specifically for hand-written notes made in the margins of books or manuscripts. Annotations are extremely useful and help to develop knowledge of English literature.\n\nThis article covers both private and socially shared text annotations, including hand-written and information technology-based annotation. For information on annotation of Web content, including images and other non-textual content, see also Web annotation.\n\nText annotation may be as old as writing on media, where it was possible to produce an additional copy with a reasonable effort. It became a prominent activity around 1000 AD in Talmudic commentaries and Arabic rhetorics treaties. In the Medieval era, scribes who copied manuscripts often made marginal annotations that then circulated with the manuscripts and were thus shared with the community; sometimes annotations were copied over to new versions when such manuscripts were later recopied.\n\nWith the rise of the printing press and the relative ease of circulating and purchasing individual (rather than shared) copies of texts, the prevalence of socially shared annotations declined and text annotation became a more private activity consisting of a reader interacting with a text. Annotations made on shared copies of texts (such as library books) are sometimes seen as devaluing the text, or as an act of defacement. Thus, print technologies support the circulation of annotations primarily as formal scholarly commentary or textual footnotes or endnotes rather than marginal, handwritten comments made by private readers, though handwritten comments or annotations were common in collaborative writing or editing.\n\nComputer-based technologies have provided new opportunities for individual and socially shared text annotations that support multiple purposes, including readers’ individual reading goals, learning, social reading, writing and editing, and other practices. Text annotation in Information Technology (IT) systems raises technical issues of access, linkage, and storage that are generally not relevant to paper-based text annotation, and thus research and development of such systems often addresses these areas.\n\nText annotations can serve a variety of functions for both private and public reading and communication practices. In their article \"From the Margins to the Center: The Future of Annotation,\" scholars Joanna Wolfe and Christine Neuwirth identify four primary functions that text annotations commonly serve in the modern era, including: (1)\"facilitat[ing] reading and later writing tasks,\" which includes annotations that support reading for both personal and professional purposes; (2)\"eavesdrop[ping] on the insights of other readers,\" which involves sharing of annotations; (3)\"provid[ing] feedback to writers or promote communication with collaborators,\" which can include personal, professional, and education-related feedback; and (4)\"call[ing] attention to topics and important passages,\" for which scholarly annotations, footnotes, and call-outs often function. Regarding the ways that annotations can support individual reading tasks, Catherine Marshall points out that the ways that readers annotate texts depends on the purpose, motivation, and context of reading. Readers may annotate to help interpret a text, to call attention to a section for future reference or reading, to support memory and recall, to help focus attention on the text as they read, to work out a problem related to the text, or create annotations not specifically related to the text at all.\n\nEducational research in text annotation has examined the role that both private and shared text annotations can play in supporting learning goals and communication. Much educational research examines how students’ private annotation of texts supports comprehension and memory; for example, research indicates that annotating texts causes more in-depth processing of information, which results in greater recall of information.\n\nOther areas of educational research investigate the benefits of socially shared text annotations for collaborative learning, both for paper-based and IT-based annotation sharing. For example, studies by Joanna Wolfe have investigated the benefits of exposure to others’ annotations on student readers and writers. In a 2000 study, Wolfe found that exposing students to others’ annotations influenced their perceptions of the annotators, which in turn shaped their responses to the material and their written products. In a later study, Wolfe found that viewing others’ written comments on a paper text, especially pairs of annotations that present opposing responses to the text, can help students engage in the type of critical reading and stance-taking necessary for effective argumentative writing.\n\nWhile shared annotations can benefit individual readers, it is important to note that, \"since the 1920s, literacy theory has increasingly emphasized the importance of social factors in the development of literacy.\" Thus, shared annotations can not only help one to better understand the content of a particular text, but may also aid in the acquirement of literacy skills. For example, a mother may leave marks inside a book to draw the attention of her child to a particular theme or concept; thanks to the development of audio annotations, parents may now leave notes for children who are just starting to read and may struggle with textual annotations.\n\nMore recent research in the effects of shared text annotations has focused on the learning applications for web-based annotation systems, some of which were developed based on design recommendations from studies outlined above. For example, Ananda Gunawardena, Aaron Tan, and David Kaufer conducted a pilot study to examine whether annotating documents in Classroom Salon, a web-based annotation and social reading platform, encouraged active reading, error detection, and collaboration in a computer science course at Carnegie Mellon University. This study suggested a correlation between students’ overall performance in the course and their ability to identify errors in a text that they annotated in Classroom Salon; it also found that students were likely to change their annotations in response to annotations made by others in the course.\n\nSimilarly, the web-based annotation tool HyLighter was used in a first-year writing course and shown to improve the development of students’ mental models of texts, including supporting reading comprehension, critical thinking, and the ability to develop a thesis. The collaboration with peers and experts around a shared text improved these skills and brought the communities’ understanding closer together.\n\nA meta-analysis of empirical studies into the higher-education uses of social annotation (SA) tools indicates such tools have been tested in several courses, among them English, sport psychology, and hypermedia. Studies have indicated that social annotation functions, including commenting, information sharing, and highlighting, can support instruction designed to foster collaborative learning and communication, as well as reading comprehension, metacognition, and critical analysis. Several studies indicated that students enjoyed using social annotation tools, and that it improved motivation in the course.\n\nText annotations have long been used in writing and revision processes as a way for reviewers to suggest changes and communicate about a text. In book publishing, for example, the collaboration of authors and editors to develop and revise a manuscript frequently involves exchanges of both in-line revisions or notes as well as marginal annotations. Similarly, copyeditors often make marginal annotations or notes that explain or suggest revisions or are directed at the author as questions or suggestions (commonly called \"queries\"). Asynchronous collaborative writing and document development often depend on text annotations as a way not only to suggest revisions but also to exchange ideas during document development or to facilitate group decision making, though such processes are often complicated by the use of different communication technologies (such as phone calls or emails as well as document sharing) for distinct tasks. Text annotations can also function to allow group or community members to communicate about a shared text, such as a doctor annotating a patient's chart.\n\nMuch research into the functionality and design of collaborative IT-based writing systems, which often support text annotation, has occurred in the area of computer-supported cooperative work.\n\nResearch in the design and development of annotation systems uses specific terminology to refer to distinct structural components of annotations and also distinguishes among options for digital annotation displays.\n\nThe structural components of any annotation can be roughly divided into three primary elements: a \"body\", an \"anchor\", and a \"marker\". The body of an annotation includes reader-generated symbols and text, such as handwritten commentary or stars in the margin. The anchor is what indicates the extent of the original text to which the body of the annotation refers; it may include circles around sections, brackets, highlights, underlines, and so on. Annotations may be anchored to very broad stretches of text (such as an entire document) or very narrow sections (such as a specific letter, word, or phrase). The marker is the visual appearance of the anchor, such as whether it is a grey underline or a yellow highlight. An annotation that has a body (such as a comment in the margin) but no specific anchor has no marker.\n\nIT-based annotation systems utilize a variety of display options for annotations, including:\nAnnotation interfaces may also allow highlighting or underlining, as well as threaded discussions. Sharing and communicating through annotations anchored to specific documents is sometimes referred to as \"anchored discussion\".\n\nIT-based annotation systems include standalone and client-server systems. In the 1980s and 1990s, a number of such systems were built in the context of libraries, patent offices, and legal text processing. Their design led researchers to produce taxonomies of annotation forms. Text annotation research has taken place at several institutions, including Xerox research centers in Palo Alto and Grenoble (France), the Hitachi Central Research Lab (in particular for annotation of patents), and in relation with the construction of the new French National Library between 1989 and 1995 at the Institut de Recherche en Informatique de Toulouse and in the company AIS (Advanced Innovation Systems).\n\nAnnotation functionality has been present in text processing software for many years through inline notes displayed as pop-ups, footnotes, and endnotes; however, it is only recently that functionality for displaying annotations as marginalia has appeared in programs such as OpenOffice.org/LibreOffice Writer and Microsoft Word. Personal or standalone annotation include word processing software that supports embedded or anchored text annotations as well as Adobe Acrobat, which in addition to commenting allows highlights, stamps, and other types of markup.\n\nTim Berners-Lee had already implemented the concept of directly editing web documents in 1990 in WorldWideWeb, the first web browser, but later ported versions removed this collaborative ability. An early version of NCSA Mosaic in 1993 also included a collaborative annotation capability, though it was quickly removed. Web Distributed Authoring and Versioning, WebDAV, was then reintroduced as an extension.\n\nA different approach to distributed authoring consists in first gathering many annotations from a wide public, and then integrate them all in order to produce a further version of a document. This approach was pioneered by Stet, the system put in place to gather comments on drafts of version 3 of the GNU General Public License. This system arose after a specific requirement, which it served egregiously, but was not so easily configurable as to be convenient for annotating any other document on the web. The co-ment system uses annotation interface concepts similar to Stet's, but it is based on an entirely new implementation, using Django/Python on the server side and various AJAX libraries such as JQuery on the client side. Both Stet and co-ment are licensed under the GNU Affero General Public License.\n\nSince 2011, the non-profit Hypothes Is Project has offered the free, open web annotation service Hypothes.is. The service features annotation via a Chrome extension, bookmarklet or proxy server, as well as integration into a LMS or CMS. Both webpages and PDFs can be annotated. Other web-based text annotation systems are collaborative software for distributed text editing and versioning, which also feature annotation and commenting interfaces. For example, HyLighter supports synchronous and asynchronous interactions, general commenting, comment tagging, threaded discussions and comment filtering. Other annotation tools under these category are more focused on NLP tasks as Named-entity recognition, relationship extraction or normalization. Some tools support manual tagging of data or automatic annotations via supervised learning.\n\nSpecialized Web-based text annotations exist in the context of scientific publication, either for refereeing or post-publication. The on-line journal PLoS ONE, published by the Public Library of Science, has developed its own Web-based system where scientists and the public can comment on published articles. The annotations are displayed as pop-ups with an anchor in the text.\n\n\n"}
{"id": "54226143", "url": "https://en.wikipedia.org/wiki?curid=54226143", "title": "The Crime Book", "text": "The Crime Book\n\nThe Crime Book (Big Ideas Simply Explained) is a non-fiction volume co-authored by American crime writers Cathy Scott, Shanna Hogan, Rebecca Morris, Canadian author and historian Lee Mellor, and United Kingdom author Michael Kerrigan, with a foreword for the U.S. edition by Scott and the U.K. edition by crime-fiction author Peter James. It was released by DK Books under its Big Ideas Learning imprint in May 2017.\n\nThe publisher describes \"The Crime Book\" as a guide to criminology that explores the most infamous cases of all time, from serial killers to mob hits to war crimes and more.\n\nIt includes a variety of crimes committed by more than 100 of the world's most notorious criminals. From Jack the Ripper to Jeffrey Dahmer, the book is a study of international true-crime history that covers shocking stories through infographics and research that lays out key facts and details. It examines the science, psychology and sociology of criminal behavior. It profiles of villains, victims and detectives. Each clue is listed for readers to follow investigations from start to finish, and studies the police and detective work for each case.\n\nIn a Q&A article for CrimeCon's blog with Scott, the author described the crimes detailed in the book as having \"such diversity that there is something for everyone. ... I can’t think of one crime that’s not represented in The Crime Book. It runs the gamut—from nonviolent cons to gangland-style criminals, to white-collar offenders—with a complete representation starting with the first known homicide committed against a Neanderthal man. Simply put, you can’t make this stuff up.\"\n\n\"Rolling Stone\" magazine's description, in an August 2017 interview with co-author Scott about the book, wrote that it is \"an encyclopedic treatment of the topic (that) makes for excellent companion reading. A compelling compilation of human trickery and awfulness, it covers crimes from arson, art forgery and kidnapping to bank robbery, drug trafficking and, of course, murder, with many of the entries accompanied by helpful illustrations.\"\n\n\"Reader's Digest\" listed it as one of its \"Best New Books You Should Read This April,\" describing it as \"everything you ever wanted to know about some of the most audacious, hideous, hilarious and mysterious acts of crime in one explosive book, filled with graphs, illustrations, quotes and timelines. This highly addictive encyclopedia of crime ... is a trivia goldmine and a helpful guide allowing you to put events into context.\"\n\n\"Culture Magazine\" in Germany had this to say: \"The level of expertise is quite high,\" noting that the book \"is lushly illustrated, readable and entertaining.\"\n\nIn its review, \"Crime Fiction Lover\" wrote that \"a crack team of true-crime experts helped put it together.\"\n\n\"Crimespree Magazine\" wrote, \"The crimes covered are all over from serial killers to gangsters and outlaws to kidnappers and elderly Brit bank robbers. This is a great book.\"\n\n"}
{"id": "5928902", "url": "https://en.wikipedia.org/wiki?curid=5928902", "title": "The Map Library", "text": "The Map Library\n\nThe Map Library is a project of The Map Maker Trust charity, and supported by Map Maker Ltd., for the supplying of free GIS data. The project website also hosts free conversion software for raster and vector files. As of November 2008, the only data sets available were for the continents of Africa and Central America.\n\nFrom the website...\n\nThe project data is managed in part by two pieces of software, each supporting different file formats and conversions.\n\n\nThe project uses data from NASA mapping projects, Famine Early Warning Systems Network, and the National Geospatial-Intelligence Agency.\n\n"}
{"id": "151042", "url": "https://en.wikipedia.org/wiki?curid=151042", "title": "Viz.", "text": "Viz.\n\nThe abbreviation viz. (or viz without a full stop), short for the Latin , which itself is a contraction from Latin of videre licet meaning \"it is permitted to see\", is used as a synonym for \"namely\", \"that is to say\", \"to wit\", or \"as follows\". It is typically used to introduce examples or further details to illustrate a point. For example: \"all types of data viz. text, audio, video, pictures, graphics etc. can be transmitted through networking\".\n\n\"Viz.\" is shorthand for the adverb \"\". It uses Tironian notes, a system of Latin shorthand developed . It comprises the first two letters, \"vi\", followed by the last two, \"et\", using the z-shaped Tironian \"et\", historically written ⁊, a common contraction for \"et\" in Latin shorthand in Ancient Rome and medieval Europe.\n\n\"Viz.\" is an abbreviation of \"videlicet\", which itself is a contraction from Latin of \"videre licet\" meaning \"it is permitted to see\". The spelling \"viz.\" is the continuation of an abbreviation using Tironian \"et\" (\"vi⁊\"), the \"z\" replacing the \"⁊\" once the latter had fallen out of common use.\n\nIn contradistinction to i.e. and e.g., viz. is used to indicate a detailed description of something stated before, and when it precedes a list of group members, it implies (near) completeness.\n\n\n\nA similar expression is scilicet (from earlier \"scire licet\"), abbreviated as \"sc.\", which is Latin for \"it is permitted to know\". \"Sc.\" provides a parenthetic clarification, removes an ambiguity, or supplies a word omitted in preceding text,  while \"viz.\" is usually used to elaborate or detail text which precedes it.\n\nIn legal usage, \"scilicet\" appears abbreviated as \"ss.\" or, in a caption, as §, where it provides a statement of venue and is read as \"to wit\". \"Scilicet\" can be read as \"namely\", \"to wit\", or \"that is to say\", or pronounced or anglicized as .\n\n"}
{"id": "411562", "url": "https://en.wikipedia.org/wiki?curid=411562", "title": "Xmas", "text": "Xmas\n\nXmas is a common abbreviation of the word \"Christmas\". It is sometimes pronounced , but \"Xmas\", and variants such as \"Xtemass\", originated as handwriting abbreviations for the typical pronunciation . The \"X\" comes from the Greek letter \"Chi\", which is the first letter of the Greek word \"Χριστός\", which in English is \"Christ\".\nThe \"-mas\" part is from the Latin-derived Old English word for \"Mass\".\n\nThere is a common misconception that the word \"Xmas\" stems from a secular attempt to remove the religious tradition from Christmas by taking the \"Christ\" out of \"Christmas\", but its use dates back to the 16th century.\n\n\"Xmas\" is deprecated by some modern style guides, including those at the \"New York Times\", \"The Times\", \"The Guardian\", and the BBC. Millicent Fenwick, in the 1948 \"Vogue's Book of Etiquette\", states that \"'Xmas' should never be used\" in greeting cards. \"The Cambridge Guide to Australian English Usage\" states that the spelling should be considered informal and restricted to contexts where concision is valued, such as headlines and greeting cards. \"The Christian Writer's Manual of Style\", while acknowledging the ancient and respectful use of \"Xmas\" in the past, states that the spelling should never be used in formal writing.\n\nEarly use of \"Xmas\" includes Bernard Ward's \"History of St. Edmund's college, Old Hall\" (originally published circa 1755). An earlier version, \"X'temmas\", dates to 1551. Around 1100 the term was written as \"Xp̄es mæsse\" in the \"Anglo-Saxon Chronicle\". \"Xmas\" is found in a letter from George Woodward in 1753. Lord Byron used the term in 1811, as did Samuel Coleridge (1801) and Lewis Carroll (1864). In the United States, the fifth American edition of William Perry's \"Royal Standard English Dictionary\", published in Boston in 1800, included in its list of \"Explanations of Common Abbreviations, or Contraction of Words\" the entry: \"Xmas. Christmas.\" Oliver Wendell Holmes, Jr. used the term in a letter dated 1923. Since at least the late 19th century, \"Xmas\" has been in use in various other English-language nations. Quotations with the word can be found in texts first written in Canada, and the word has been used in Australia, and in the Caribbean. \"Merriam-Webster's Dictionary of English Usage\" stated that modern use of the term is largely limited to advertisements, headlines and banners, where its conciseness is valued. The association with commerce \"has done nothing for its reputation\", according to the dictionary.\n\nIn the United Kingdom, the former Church of England Bishop of Blackburn, Alan Chesters, recommended to his clergy that they avoid the spelling.\nIn the United States, in 1977 New Hampshire Governor Meldrim Thomson sent out a press release saying that he wanted journalists to keep the \"Christ\" in Christmas, and not call it Xmas—which he called a \"pagan\" spelling of Christmas.\n\nThe abbreviation of Christmas as \"Xmas\" is the source of disagreement among Christians who observe the holiday. \n\nThe December 1957 \"News and Views\" published by the Church League of America, a conservative organization co-founded in 1937 by George Washington Robnett, attacked the use of Xmas in an article titled \"X=The Unknown Quantity\". The claims were picked up later by Gerald L. K. Smith, who in December 1966 claimed that Xmas was a \"blasphemous omission of the name of Christ\" and that \"'X' is referred to as being symbolical of the unknown quantity.\" Smith further argued that Jews introduced Santa Claus to suppress the New Testament accounts of Jesus, and that the United Nations, at the behest of \"world Jewry\", had \"outlawed the name of Christ\". There is, however, a well documented history of use of \"Χ\" (actually a chi) as an abbreviation for \"Christ\" (Χριστός) and possibly also a symbol of the cross. The abbreviation appears on many Orthodox Christian religious icons.\n\nDennis Bratcher, writing for a website for Christians, states \"there are always those who loudly decry the use of the abbreviation 'Xmas' as some kind of blasphemy against Christ and Christianity\". Among them are evangelist Franklin Graham and CNN journalist Roland S. Martin. Graham stated in an interview: \"for us as Christians, this is one of the most holy of the holidays, the birth of our savior Jesus Christ. And for people to take Christ out of Christmas. They're happy to say merry Xmas. Let's just take Jesus out. And really, I think, a war against the name of Jesus Christ.\" Roland Martin likewise relates the use of \"Xmas\" to his growing concerns of increasing commercialization and secularization of one of Christianity's highest holy days. Bratcher posits that those who dislike abbreviating the word are unfamiliar with a long history of Christians using X in place of \"Christ\" for various purposes.\n\nThe word \"Christ\" and its compounds, including \"Christmas\", have been abbreviated in English for at least the past 1,000 years, long before the modern \"Xmas\" was commonly used. \"Christ\" was often written as \"Xρ\" or \"Xt\"; there are references in the \"Anglo-Saxon Chronicle\" as far back as 1021. This X and P arose as the uppercase forms of the Greek letters χ (Ch) and ρ (R) used in ancient abbreviations for Χριστος (Greek for \"Christ\"). The labarum, an amalgamation of the two Greek letters rendered as ☧, is a symbol often used to represent Christ in Catholic, Protestant, and Orthodox Christian Churches.\n\nThe \"Oxford English Dictionary\" (\"OED\") and the \"OED Supplement\" have cited usages of \"X-\" or \"Xp-\" for \"Christ-\" as early as 1485. The terms \"Xtian\" and less commonly \"Xpian\" have also been used for \"Christian\". The \"OED\" further cites usage of \"Xtianity\" for \"Christianity\" from 1634. According to \"Merriam-Webster's Dictionary of English Usage\", most of the evidence for these words comes from \"educated Englishmen who knew their Greek\".\n\nIn ancient Christian art, χ and χρ are abbreviations for Christ's name. In many manuscripts of the \"New Testament\" and icons, Χ is an abbreviation for Χριστος, as is XC (the first and last letters in Greek, using the lunate sigma); compare IC for Jesus in Greek.\n\nOther proper names containing the name \"Christ\" besides those mentioned above are sometimes abbreviated similarly, either as \"X\" or \"Xt\", both of which have been used historically, e.g., \"Xtopher\" or \"Xopher\" for \"Christopher\", or \"Xtina\" or \"Xina\" for the name \"Christina\".\n\nIn the 17th and 18th centuries, \"Xene\" and \"Exene\" were common spellings for the given name Christine. The American singer Christina Aguilera has sometimes gone by the name \"Xtina\". Similarly, Exene Cervenka has been a noted American singer-songwriter since 1977.\n\nThis usage of \"X\" to spell the syllable \"kris\" (rather than the sounds \"ks\") has extended to \"xtal\" for \"crystal\", and on florists' signs to \"xant\" for \"chrysanthemum\", even though these words are not etymologically related to \"Christ\": \"crystal\" comes from a Greek word meaning \"ice\" (and not even using the letter χ), and \"chrysanthemum\" comes from Greek words meaning \"golden flower\", while \"Christ\" comes from a Greek word meaning \"anointed\".\n\nIn the animated television series \"Futurama\", which is set in the 31st century, Xmas is the official name for the day formerly known as Christmas (which, in the episode \"Xmas Story\", is said to have become an \"archaic pronunciation\").\n\nIn the American version of the board game \"Monopoly\", players can draw a card from the Community Chest which reads: \"Xmas fund matures. Collect $100\".\n\n\n"}
