{"id": "192230", "url": "https://en.wikipedia.org/wiki?curid=192230", "title": "Almanac", "text": "Almanac\n\nAn almanac (also spelled \"almanack\" and \"almanach\") is an annual publication listing a set of events forthcoming in the next year.\n\nIt includes information like weather forecasts, farmers' planting dates, tide tables, and other tabular data often arranged according to the calendar. Celestial figures and various statistics are found in almanacs, such as the rising and setting times of the Sun and Moon, dates of eclipses, hours of high and low tides, and religious festivals.\n\nA calendar, which is a system for time keeping, in written form is usually produced as a most simple almanac: it includes additional information about the day of the week on which a particular day falls, major holidays, the phases of the moon, earthquake hazard levels etc. The set of events noted in an almanac are selected in view of a more or less specific group of readers e.g. farmers, sailors, astronomers or others.\n\nThe etymology of the word is unclear. It is suggested the word \"almanac\" derives from a Greek word meaning \"calendar\". However, that word appears only once in antiquity, by Eusebius who quotes Porphyry as to the Coptic Egyptian use of astrological charts (\"almenichiaká\"). The earliest almanacs were calendars that included agricultural, astronomical, or meteorological data. But it is highly unlikely Roger Bacon received the word from this etymology: \"Notwithstanding the suggestive sound and use of this word (of which however the real form is very uncertain), the difficulties of connecting it historically either with the Spanish Arabic manākh, or with Medieval Latin almanach without Arabic intermediation, seem insurmountable.\"\n\nThe earliest documented use of the word in any language is in Latin in 1267 by Roger Bacon, where it meant a set of tables detailing movements of heavenly bodies including the moon.\n\nOne etymology report says \"The ultimate source of the word is obscure. Its first syllable, al-, and its general relevance to medieval science and technology, strongly suggest an Arabic origin, but no convincing candidate has been found\". Another report similarly says of \"almanac\": \"First seen in Roger Bacon. Apparently from Spanish Arabic, \"al-manakh\", but this is not an Arabic word...The word remains a puzzle.\" The \"Oxford English Dictionary\" similarly says \"the word has no etymon in Arabic\" but indirect circumstantial evidence \"points to a Spanish Arabic \"al-manākh\"\".\n\nThe reason why the proposed Arabic word is speculatively spelled \"al-manākh\" is that the spelling occurred as \"almanach\", as well as almanac (and Roger Bacon used both spellings). The earliest use of the word was in the context of astronomy calendars.\n\nThe prestige of the Tables of Toledo and other medieval Arabic astronomy works at the time of the word's emergence in the West, together with the absence of the word in Arabic, suggest it may have been invented in the West and is pseudo-Arabic. At that time in the West, it would have been prestigious to attach an Arabic appellation to a set of astronomical tables. Also around that time, prompted by that motive, the Latin writer Pseudo-Geber wrote under an Arabic pseudonym. (The later alchemical word \"alkahest\" is known to be pseudo-Arabic.)\n\nThe earlier texts considered to be almanacs have been found in the Near East, dating back to the middle of the second millennium BC. They have been called generally hemerologies, from the Greek \"hēmerā\", meaning \"day\". Among them is the so-called Babylonian Almanac, which lists favorable and unfavorable days with advice on what to do on each of them. Successive variants and versions aimed at different readership have been found. Egyptian lists of good and bad moments, three times each day, have also been found. Many of these prognostics were connected with celestial events. The flooding of the Nile valley, a most important event in ancient Egypt, was expected to occur at the summer solstice but as the civil calendar had exactly 365 days, over the centuries the date was drifting in the calendar. The first heliacal rising of Sirius was used for its prediction and this practice, the observation of some star and its connecting to some event apparently spread.\n\nThe Greek almanac, known as parapegma, has existed in the form an inscribed stone on which the days of the month were indicated by movable pegs inserted into bored holes, hence the name. There were also written texts and according to Diogenes Laërtius, \"Parapegma\" was the title of a book by Democritus. Ptolemy, the Alexandrian astronomer (2nd century) wrote a treatise, \"Phaseis\"—\"phases of fixed stars and collection of weather-changes\" is the translation of its full title—the core of which is a \"parapegma\", a list of dates of seasonally regular weather changes, first appearances and last appearances of stars or constellations at sunrise or sunset, and solar events such as solstices, all organized according to the solar year. With the astronomical computations were expected weather phenomena, composed as a digest of observations made by various authorities of the past. \"Parapegmata\" had been composed for centuries.\n\nPtolemy believed that astronomical phenomena caused the changes in seasonal weather; his explanation of why there was not an exact correlation of these events was that the physical influences of other heavenly bodies also came into play. Hence for him, weather prediction was a special division of astrology.\n\nThe origins of the almanac can be connected to ancient Babylonian astronomy, when tables of planetary periods were produced in order to predict lunar and planetary phenomena. Similar treatises called Zij were later composed in medieval Islamic astronomy.\n\nThe modern almanac differs from Babylonian, Ptolemaic and Zij tables in the sense that \"the entries found in the almanacs give directly the positions of the celestial bodies and need no further computation\", in contrast to the more common \"auxiliary astronomical tables\" based on Ptolemy's \"Almagest\". The earliest known almanac in this modern sense is the \"Almanac of Azarqueil\" written in 1088 by Abū Ishāq Ibrāhīm al-Zarqālī (Latinized as Arzachel) in Toledo, al-Andalus. The work provided the true daily positions of the sun, moon and planets for four years from 1088 to 1092, as well as many other related tables. A Latin translation and adaptation of the work appeared as the \"Tables of Toledo\" in the 12th century and the \"Alfonsine tables\" in the 13th century.\nAfter almanacs were devised, people still saw little difference between predicting the movements of the stars and tides, and predicting the future in the divination sense. Early almanacs therefore contained general horoscopes, as well as the more concrete information. In 1150 Solomon Jarchus created such an almanac considered to be among the first modern almanacs. Copies of 12th century almanacs are found in the British Museum, and in the Universities of Oxford and Cambridge. In 1300, Petrus de Dacia created an almanac (Savilian Library, Oxford). This was the same year Roger Bacon, OFM, produced his as well. In 1327 Walter de Elvendene created an almanac and later on John Somers of Oxford, in 1380. In 1386 Nicholas de Lynne, Oxford produced an almanac. In 1457 the first printed almanac was published at Mainz, by Gutenberg (eight years before the famous Bible). Regio-Montanus produced an almanac in 1472 (Nuremberg, 1472), which was continued in print for several centuries in many editions. In 1497 the \"Sheapheard’s Kalendar\", translated from French (Richard Pynson) became the first English printed almanac.\n\nBy the second half of the 16th century, yearly almanacs were being produced in England by men such as Anthony Askham, Thomas Buckminster, John Dade and Gabriel Frende. In the 17th century, English almanacs were bestsellers, second only to the Bible; by the middle of the century, 400,000 almanacs were being produced annually (a complete listing can be found in the English Short Title Catalogue). Until its deregulation in 1775, the Stationers' Company maintained a lucrative monopoly over almanac publication in England. Richard Allestree (who is not the same as this Richard Allestree) wrote one of the more popular English almanacs, producing yearly volumes from 1617 to 1643, but his is by no means the earliest or the longest-running almanac.\n\nIn British America, William Pierce of Harvard College published the first American almanac entitled, \"An Almanac for New England for the year 1639\" Cambridge, Massachusetts. Harvard became the first center for the annual publication of almanacs with various editors including Samuel Danforth, Oakes, Cheever, Chauncey, Dudley, Foster, et alia. An almanac maker going under the pseudonym of Poor Richard, Knight of the Burnt Island began to publish \"Poor Robin's Almanack\" one of the first comic almanacs that parodied these horoscopes in its 1664 issue, saying \"This month we may expect to hear of the Death of some Man, Woman, or Child, either in Kent or Christendom.\" Other noteworthy comic almanacs include those published from 1687-1702 by John Tully of Saybrook, Connecticut.\n\nThe most important early American almanacs were made from 1726-1775 by Nathaniel Ames of Dedham, Massachusetts. A few years later James Franklin began publishing the Rhode-Island Almanack beginning in 1728. Five years later his brother Benjamin Franklin began publishing \"Poor Richard's Almanack\" from 1733-1758. Benjamin Banneker, a free African American, composed a series of almanacs from 1792-1797.\n\nCurrently published almanacs such as \"Whitaker's Almanack\" have expanded their scope and contents beyond that of their historical counterparts. Modern almanacs include a comprehensive presentation of statistical and descriptive data covering the entire world. Contents also include discussions of topical developments and a summary of recent historical events. Other currently published almanacs (ca. 2006) include \"TIME Almanac with Information Please\", \"World Almanac and Book of Facts\", \"The Farmer's Almanac\" and \"The Old Farmer's Almanac\" and The Almanac for Farmers & City Folk. The \"Inverness Almanac\" ,\" an almanac/literary journal, was published in West Marin, California, from 2015 to 2016. In 2007, Harrowsmith Country Life Magazine launched a Canadian Almanac, written in Canada, with all-Canadian content. The nonprofit agrarian organization the Greenhorns currently publishes \"The New Farmer's Almanac\" as a resource for young farmers.\n\nMajor topics covered by almanacs (reflected by their tables of contents) include: geography, government, demographics, agriculture, economics and business, health and medicine, religion, mass media, transportation, science and technology, sport, and awards/prizes.\n\nOther examples include \"The Almanac of American Politics\" published by Columbia Books & Information Services, \"The Almanac of American Literature\", and \"The Almanac of British Politics\".\n\nFrom 1985 to 1990, approximately 53 per cent of all almanac sales sold in the United States were sold through the \"Where in the World Is Carmen Sandiego? (1985)\" computer game pack that included a complimentary \"World Almanac and Book of Facts.\"\n\nThe GPS almanac, as part of the data transmitted by each GPS satellite, contains coarse orbit and status information for all satellites in the constellation, an ionospheric model, and information to relate GPS derived time to Coordinated Universal Time (UTC). Hence the GPS almanac provide a similar goal as the ancient Babylonian almanac, to find celestial bodies.\n\n\n\n"}
{"id": "624684", "url": "https://en.wikipedia.org/wiki?curid=624684", "title": "Annotation", "text": "Annotation\n\nAn annotation is a metadatum (e.g. a post, explanation, markup) attached to location or other data.\n\nTextual scholarship is a discipline that often uses the technique of annotation to describe or add additional historical context to texts and physical documents.\n\nStudents often highlight passages in books in order to refer back to key phrases easily, or add marginalia to aid studying. One educational technique when analyzing prose literature is to have students or teachers circle the names of characters and put rectangular boxes around phrases identifying the setting of a given scene.\n\nAnnotated bibliographies add commentary on the relevance or quality of each source, in addition to the usual bibliographic information that merely identifies the source.\n\nFrom a cognitive perspective annotation has an important role in learning and instruction. As part of guided noticing it involves highlighting, naming or labelling and commenting aspects of visual representations to help focus learners' attention on specific visual aspects. In other words, it means the assignment of typological representations (culturally meaningful categories), to topological representations (e.g. images). This is especially important when experts, such as medical doctors, interpret visualizations in detail and explain their interpretations to others, for example by means of digital technology. Here, annotation can be a way to establish common ground between interactants with different levels of knowledge. The value of annotation has been empirically confirmed, for example, in a study which shows that in computer-based teleconsultations the integration of image annotation and speech leads to significantly improved knowledge exchange compared with the use of images and speech without annotation.\n\nMarkup languages like XML and HTML annotate text in a way that is syntactically distinguishable from that text. They can be used to add information about the desired visual presentation, or machine-readable semantic information, as in the semantic web.\n\nThe \"annotate\" function (also known as \"blame\" or \"praise\") used in source control systems such as Git, Team Foundation Server and Subversion determines who committed changes to the source code into the repository. This outputs a copy of the source code where each line is annotated with the name of the last contributor to edit that line (and possibly a revision number). This can help establish blame in the event a change caused a malfunction, or identify the author of brilliant code.\n\nA special case is the Java programming language, where annotations can be used as a special form of syntactic metadata in the source code. Classes, methods, variables, parameters and packages may be annotated. The annotations can be embedded in class files generated by the compiler and may be retained by the Java virtual machine and thus influence the run-time behaviour of an application. It is possible to create meta-annotations out of the existing ones in Java.\n\nSince the 1980s, molecular biology and bioinformatics have created the need for DNA annotation. DNA annotation or genome annotation is the process of identifying the locations of genes and all of the coding regions in a genome and determining what those genes do. An annotation (irrespective of the context) is a note added by way of explanation or commentary. Once a genome is sequenced, it needs to be annotated to make sense of it.\n\nIn the digital imaging community the term annotation is commonly used for visible metadata superimposed on an image without changing the underlying master image, such as sticky notes, virtual laser pointers, circles, arrows, and black-outs (cf. redaction).\n\nIn the medical imaging community, an annotation is often referred to as a region of interest and is encoded in DICOM format.\n\nIn the United States, legal publishers such as Thomson West and Lexis Nexis publish annotated versions of statutes, providing information about court cases that have interpreted the statutes. Both the federal United States Code and state statutes are subject to interpretation by the courts, and the annotated statutes are valuable tools in legal research.\n\nIn linguistics, annotations include comments and metadata; these non-transcriptional annotations are also non-linguistic. A collection of texts with linguistic annotations is known as a corpus (plural \"corpora\"). The Linguistic Annotation Wiki describes tools and formats for creating and managing linguistic annotations.\n\n"}
{"id": "398493", "url": "https://en.wikipedia.org/wiki?curid=398493", "title": "BRD (Germany)", "text": "BRD (Germany)\n\nBRD (; English: Federal Republic of Germany); () is an unofficial abbreviation commonly used between 1968 and 1990 by the communist regime of the German Democratic Republic (East Germany) to refer to the Federal Republic of Germany, informally known at the time as West Germany. The East German regime previously used the term \"German Federal Republic\" to refer to its western counterpart.\n\nUnlike the English equivalent FRG, which was used as an IOC country code and a FIFA trigramme, the use of \"BRD\" was strongly discouraged by the authorities of the Federal Republic of Germany itself, because it was considered to be a derogatory communist term. The term was not banned by law, but its use was discouraged or forbidden in schools in Western Germany. After German reunification, the country is usually referred to simply as Germany (\"\"), and hence the need for abbreviations is greatly diminished. The most widely used abbreviation for West Germany was its ISO 3166-1 alpha-2 country code \"DE\", which has remained the country code of reunified Germany.\n\nThe official name was and is \"Bundesrepublik Deutschland\" (\"Federal Republic of Germany\"). The name, even though in the beginning referring only to the republic established in the Trizone, was to reflect a name for all of Germany, therefore it was particularly to include the term \"Deutschland\" (\"Germany\"). This corresponded to the spirit of the then West German constitution, the Basic Law, allowing all states or \"Länder\", then under Allied control, to join the new Federal Republic. In 1949 the original eleven states in the Trizone and West Berlin did so. However the latter was prevented by Allied objection on account of the city being a quadripartite allied occupation area. The Saarland joined with effect from 1 January 1957, while the \"new states\" of the East did so with effect from 3 October 1990, including reunited Berlin.\n\nTherefore, the term Germany had an importance as part of the official name, which is reflected in the naming conventions which developed in the Cold War. Starting in June 1949 the abbreviation was sometimes used in the Federal Republic of Germany without any special connotations. The initialism \"BRD\" began to enter into such regular usage in West German scientific and ministerial circles, that it was added to the western edition of the German language dictionary Duden in 1967. The German Democratic Republic at first used the name \"Westdeutschland\" or \"West Germany\" (abbreviated \"WD\") for the Federal Republic of Germany, but since the 1950s the East German government insisted on calling West Germany \"Deutsche Bundesrepublik\" or \"German Federal Republic\" (abbreviated \"DBR\"), because they also considered East Germany part of Germany, and thus would not permit the West German government to use the name \"Germany\".\n\nThis changed in 1968 with the new constitution of the German Democratic Republic. The communists no longer strove for German reunification, and the name \"BRD\" was introduced as a propaganda counter-term to the term \"DDR\", trying to express the equality of the states. Conversely, the West would speak of the \"sogenannte DDR\" or \"so-called 'DDR'\" when it had to be belittled.\n\nAt that time, the initialism \"BRD\" had been adopted by \"Neues Deutschland\", the ruling Socialist Unity Party's daily newspaper, while East German official sources adopted that initialism as standard in 1973.\n\nThe East German decision to abandon the idea of a single German nation was accompanied by omitting the terms \"Deutschland\" (\"Germany\") and \"deutsch\" (\"German\") in a number of terms, for example:\n\n\nHowever, the ruling party's full name, \"Sozialistische Einheitspartei Deutschlands\" or \"Socialist Unity Party of Germany\" remained unchanged, as did that of its newspaper \"Neues Deutschland\" (\"New Germany\") .\nTherefore, using the abbreviation \"BRD\" fitted perfectly into the official East German policy of downplaying the concept of a united Germany. In 1974, the GDR had replaced the vehicle registration code \"D\", hitherto shared with the Federal Republic, for \"DDR\" and demanded that West Germany recognise the division by likewise accepting \"BRD\". \nThis was rejected by the West, where some motorists displayed bumper stickers with the slogan \"BRD - Nein Danke!\" (\"BRD? No Thanks!\"). Thus in the West the initialism became even more objectionable and using it was often considered either unreflecting or even expressing naïve Communist sympathies.\nAs a result, the initialism reached only occasional frequency in West German parlance. In order to be precise West Germans increasingly used the terms \"Bundesrepublik\" or \"Bundesgebiet\" (\"Federal Republic\", or \"Federal Territory\") to refer to the country and \"Bundesbürger\" (\"Federal Citizen[s]\") as to its citizens, with the pertaining adjective \"bundesdeutsch\" (federally German).\n\nTo distance themselves from the term \"BRD\", until German reunification, the government of the Federal Republic of Germany and media sometimes used the abbreviations \"BR Deutschland,\" \"BR-Dt.\", \"BRDt.\",\nWest Germany had always claimed to be \"the\" Germany, and did not like the comparison to \"DDR\", or two separate German states. This claim was also reflected in the Hallstein Doctrine determining its foreign and interior policy until the early 1970s. Named after Walter Hallstein, State secretary at the Foreign Office, this was a key doctrine in the foreign policy of West Germany after 1955, which prescribed that the Federal Republic of Germany would not establish or maintain diplomatic relations with any state that recognised the GDR. Although this changed after 1973, with the Federal Republic no longer asserting an exclusive mandate over the whole of Germany, West Germany only established \"de facto\" diplomatic relations with East Germany. Under the terms of the Basic Treaty in 1972, Bonn and East Berlin exchanged \"permanent missions\", headed by \"permanent representatives\", rather than \"de jure\" embassies headed by ambassadors. Similarly, relations with the GDR were not conducted through the Foreign Office, but through a separate Federal Ministry for Intra-German Relations, to which the East German mission was accredited.\n\nIn 1965 the Federal Minister of All-German Affairs (later Intra-German Relations) issued the \"Directives for the appellation of Germany\" recommending that the use of \"BRD\" be avoided. On 31 May 1974 the heads of the federal and state governments recommended that the full name should always be used in official publications. In November 1979 the federal government informed the Bundestag that the West German public broadcasters ARD and ZDF agreed not to use the initialism.\n\nUnder the West German federal system, the states were generally responsible for school education, and by the 1970s, some of them had either already recommended omitting the initialism, or, in the case of Bavaria, forbidden it. Similarly, a decree by the educational authorities in the state of Schleswig-Holstein of 4 October 1976 declared the term to be \"nicht wünschenswert\" or \"undesirable\". The conference of all the states ministers for school education decided on 12 February 1981 to not print the initialism in books, maps, and atlases for schools. with pupils being required to write \"Bundesrepublik Deutschland\" in full and use of the term being deemed an error. The different usages were so ingrained that one could deduce a person's or source's political leaning from the name used for West Germany, with far-left movements in the country using \"BRD\".\n\nHowever, as the Association for the German Language found, this debate on the initialism had little influence on changing the West German parlance with the usage of the initialism - in any event limited - unaffected by the debate.\n\nA similar ideological question was the question whether to use \"Berlin (West)\" (the officially preferred name) or \"West Berlin\", and even whether to write \"West Berlin\" in German as two hyphenated words - \"West-Berlin\" - or as one word - \"Westberlin\".\n\nMost Westerners called the Western sectors \"Berlin\", unless further distinction was necessary. The West German Federal government initially called West Berlin \"Groß-Berlin\" or \"Greater Berlin\", but changed this \"Berlin (West)\", although it also used the hyphenated \"West-Berlin\". However, the East German government commonly referred to it as \"Westberlin\". Starting from 31 May 1961, East Berlin was officially called \"Berlin, Hauptstadt der DDR\" (Berlin, Capital of the GDR), replacing the formerly used term \"Democratic Berlin\", or simply \"Berlin\", by East Germany, and \"Berlin (Ost)\" by the West German Federal government. Other names used by West German media included \"Ost-Berlin\" and \"Ostberlin\" (both meaning \"East Berlin\") as well as \"Ostsektor\" or \"Eastern Sector\". These different naming conventions for the divided parts of Berlin, when followed by individuals, governments, or media, commonly indicated their political leanings, with the centre-right \"Frankfurter Allgemeine Zeitung\" using \"Ost-Berlin\" and the centre-left \"Süddeutsche Zeitung\" using \"Ostberlin\".\n\nThe naming of the German Democratic Republic was also a controversial issue, West Germans at first preferring the names \"Mitteldeutschland\" (\"Middle Germany\") and \"Sowjetische Besatzungszone\" (Soviet Occupation Zone) abbreviated as \"SBZ\". This only changed under Willy Brandt when West German authorities started using the official name, \"Deutsche Demokratische Republik\" or \"DDR\", but many conservative German newspapers, like \"Bild\", owned by the Springer company, always wrote \"DDR\" in scare quotes until 1 August 1989.\n\nIn 1995, a disagreement arose between reunified Germany and newly independent Slovakia, as Germany objected to the use of the Slovak language name \"Nemecká spolková republika\" (literally \"German Federal Republic\") owing to its Cold War connotations, instead of \"Spolková republika Nemecko\". This was almost identical to the equivalent \"Spolková republika Německo\" in Czech, a language closely related to Slovak, but the Slovak authorities claimed that \"Federal Republic of Germany\" could not be translated grammatically into Slovak. However, the Slovak government had used it until the previous year, leading to suggestions in the Bratislava newspaper \"Narodna Obroda\" that they were using \"German Federal Republic\" to show their displeasure with German attitudes to the country.\n"}
{"id": "17077434", "url": "https://en.wikipedia.org/wiki?curid=17077434", "title": "Comparative Toxicogenomics Database", "text": "Comparative Toxicogenomics Database\n\nThe Comparative Toxicogenomics Database (CTD) is a public website and research tool launched in November 2004 that curates scientific data describing relationships between chemicals/drugs, genes/proteins, diseases, taxa, phenotypes, GO annotations, pathways, and interaction modules.\nThe database is maintained by the Department of Biological Sciences at North Carolina State University.\n\nThe Comparative Toxicogenomics Database (CTD) is a public website and research tool that curates scientific data describing relationships between chemicals, genes/proteins, diseases, taxa, phenotypes, GO annotations, pathways, and interaction modules, launched on November 12, 2004. \nThe database is maintained by the Department of Biological Sciences at North Carolina State University.\n\nOne of the primary goals of CTD is to advance the understanding of the effects of environmental chemicals on human health on the genetic level, a field called toxicogenomics.\n\nThe etiology of many chronic diseases involves interactions between environmental factors and genes that modulate important physiological processes. Chemicals are an important component of the environment. Conditions such as asthma, cancer, diabetes, hypertension, immunodeficiency, and Parkinson's disease are known to be influenced by the environment; however, the molecular mechanisms underlying these correlations are not well understood. CTD may help resolve these mechanisms. The most up-to-date extensive list of peer-reviewed scientific articles about CTD is available at their publications page\n\nCTD is a unique resource where biocurators read the scientific literature and manually curate four types of core data:\n\n\nBy integrating the above four data sets, CTD automatically constructs putative chemical-gene-phenotype-disease networks to illuminate molecular mechanisms underlying environmentally-influenced diseases.\n\nThese inferred relationships are statistically scored and ranked and can be used by scientists and computational biologists to generate and verify testable hypotheses about toxicogenomic mechanisms and how they relate to human health.\n\nUsers can search CTD to explore scientific data for chemicals, genes, diseases, or interactions between any of these three concepts. Currently, CTD integrates toxicogenomic data for vertebrates and invertebrates.\n\nCTD integrates data from or hyperlinks to these databases:\n\n"}
{"id": "1809113", "url": "https://en.wikipedia.org/wiki?curid=1809113", "title": "Comparative biology", "text": "Comparative biology\n\nComparative biology uses natural variation and disparity to understand the patterns of life at all levels—from genes to communities—and the critical role of organisms in ecosystems. Comparative biology is a cross-lineage approach to understanding the phylogenetic history of individuals or higher taxa and the mechanisms and patterns that drives it. Comparative biology encompasses Evolutionary Biology, Systematics, Neontology, Paleontology, Ethology, Anthropology, and Biogeography as well as historical approaches to Developmental biology, Genomics, Physiology, Ecology and many other areas of the biological sciences.The comparative approach also has numerous applications in human health, genetics, biomedicine, and conservation biology. The biological relationships (phylogenies, pedigree) are important for comparative analyses and usually represented by a phylogenetic tree or cladogram to differentiate those features with single origins (Homology) from those with multiple origins (Homoplasy).\n\n"}
{"id": "2051798", "url": "https://en.wikipedia.org/wiki?curid=2051798", "title": "Comparative contextual analysis", "text": "Comparative contextual analysis\n\nComparative contextual analysis is a methodology for comparative research where contextual interrogation precedes any analysis of similarity and difference. It is a thematic process directed and designed to explore relationships of agency rather than institutional or structural frameworks. See structure and agency and theory of structuration.\n\n\n"}
{"id": "4481195", "url": "https://en.wikipedia.org/wiki?curid=4481195", "title": "Comparative cultural studies", "text": "Comparative cultural studies\n\nComparative cultural studies is a contextual approach to the study of culture in a global and intercultural context. Focus is placed on the theory, method, and application of the study process(es) rather than on the \"what\" of the object(s) of study.\n\nIn comparative cultural studies, selected tenets of comparative literature are merged with selected tenets of the field of cultural studies (including culture theories, (radical) constructivism, communication theories, and systems theories) with the objective to study culture and culture products (including but not restricted to literature, communication, media, art, etc.). This is performed in a contextual and relational construction and with a plurality of methods and approaches, interdisciplinary, and, if and when required, including teamwork. In comparative cultural studies, it is the processes of communicative action(s) in culture and the how of these processes that constitute the main objectives of research and study. However, scholarship in comparative cultural studies does not exclude textual analysis proper of other established fields of study. In comparative cultural studies, ideally, the framework of and methodologies available in the systemic and empirical study of culture are favored. Scholarship in comparative cultural studies includes the theoretical, as well as methodological and applied postulate to move and to dialogue between cultures, languages, literature, and disciplines: attention to other cultures against essentialist notions and practices and beyond the paradigm of the nation-state is a basic and founding element of the framework and its application.\n\n\n"}
{"id": "9435784", "url": "https://en.wikipedia.org/wiki?curid=9435784", "title": "Comparative physiology", "text": "Comparative physiology\n\nComparative physiology is a subdiscipline of physiology that studies and exploits the diversity of functional characteristics of various kinds of organisms. It is closely related to evolutionary physiology and environmental physiology. Many universities offer undergraduate courses that cover comparative aspects of animal physiology. According to Clifford Ladd Prosser, \"Comparative Physiology\nis not so much a defined discipline as a viewpoint, a philosophy.\"\n\nOriginally, physiology focused primarily on human beings, in large part from a desire to improve medical practices. When physiologists first began comparing different species it was sometimes out of simple curiosity to understand how organisms work but also stemmed from a desire to discover basic physiological principles. This use of specific organisms convenient to study specific questions is known as the Krogh Principle.\n\nC. Ladd Prosser, a founder of modern comparative physiology, outlined a broad agenda for comparative physiology in his 1950 edited volume (see summary and discussion in Garland and Carter):\n\n1. To describe how different kinds of animals meet their needs.\n\n2. The use of physiological information to reconstruct phylogenetic relationships of organisms.\n\n3. To elucidate how physiology mediates interactions between organisms and their environments.\n\n4. To identify \"model systems\" for studying particular physiological functions.\n\n5. To use the \"kind of animal\" as an experimental variable.\n\nComparative physiologists often study organisms that live in \"extreme\" environments (e.g., deserts) because they expect to find especially clear examples of evolutionary adaptation. One example is the study of water balance in desert-inhabiting mammals, which have been found to exhibit kidney specializations.\n\nSimilarly, comparative physiologists have been attracted to \"unusual\" organisms, such as very large or small ones. As an example, of the latter, hummingbirds have been studied. As another example, giraffe have been studied because of their long necks and the expectation that this would lead to specializations related to the regulation of blood pressure. More generally, ectothermic vertebrates have been studied to determine how blood acid-base balance and pH change as body temperature changes.\n\nIn the United States, research in comparative physiology is funded by both the National Institutes of Health and the National Science Foundation.\n\nA number of scientific societies feature sections on comparative physiology, including:\n\nKnut Schmidt-Nielsen (1915–2007) was a major figure in vertebrate comparative physiology, serving on the faculty at Duke University for many years and training a large number of students (obituary). He also authored several books, including an influential text, all known for their accessible writing style.\n\nGrover C. Stephens (1925–2003) was a well-known invertebrate comparative physiologist, serving on the faculty of the University of Minnesota until becoming the founding chairman of the Department of Organismic Biology at the University of California at Irvine in 1964. He was the mentor for numerous graduate students, many of whom have gone on to further build the field (obituary). He authored several books and in addition to being an accomplished biologist was also an accomplished pianist and philosopher.\n\n\n\n"}
{"id": "380406", "url": "https://en.wikipedia.org/wiki?curid=380406", "title": "Comparative psychology", "text": "Comparative psychology\n\nComparative psychology refers to the scientific study of the behavior and mental processes of non-human animals, especially as these relate to the phylogenetic history, adaptive significance, and development of behavior. Research in this area addresses many different issues, uses many different methods and explores the behavior of many different species from insects to primates.\n\nComparative psychology is sometimes assumed to emphasize cross-species comparisons, including those between humans and animals. However, some researchers feel that direct comparisons should not be the sole focus of comparative psychology and that intense focus on a single organism to understand its behavior is just as desirable; if not more so. Donald Dewsbury reviewed the works of several psychologists and their definitions and concluded that the object of comparative psychology is to establish principles of generality focusing on both proximate and ultimate causation. \n\nUsing a comparative approach to behavior allows one to evaluate the target behavior from four different, complementary perspectives, developed by Niko Tinbergen. First, one may ask how pervasive the behavior is across species (i.e. how common is the behavior between animal species?). Second, one may ask how the behavior contributes to the lifetime reproductive success of the individuals demonstrating the behavior (i.e. does the behavior result in animals producing more offspring than animals not displaying the behavior)? Theories addressing the ultimate causes of behavior are based on the answers to these two questions.\n\nThird, what mechanisms are involved in the behavior (i.e. what physiological, behavioral, and environmental components are necessary and sufficient for the generation of the behavior)? Fourth, a researcher may ask about the development of the behavior within an individual (i.e. what maturational, learning, social experiences must an individual undergo in order to demonstrate a behavior)? Theories addressing the proximate causes of behavior are based on answers to these two questions. For more details see Tinbergen's four questions.\n\nThe 9th century scholar al-Jahiz wrote works on the social organization and communication methods of animals like ants. The 11th century Arabic writer Ibn al-Haytham (Alhazen) wrote the \"Treatise on the Influence of Melodies on the Souls of Animals\", an early treatise dealing with the effects of music on an imals. In the treatise, he demonstrates how a camel's pace could be hastened or retarded with the use of music, and shows other examples of how music can affect animal behavior, experimenting with horses, birds and reptiles. Through to the 19th century, a majority of scholars in the Western world continued to believe that music was a distinctly human phenomenon, but experiments since then have vindicated Ibn al-Haytham's view that music does indeed have an effect on animals.\n\nCharles Darwin was central in the development of comparative psychology; it is thought that psychology should be spoken in terms of \"pre-\" and \"post-Darwin\" because his contributions were so influential. theory led to several hypotheses, one being that the factors that set humans apart, such as higher mental, moral and spiritual faculties, could be accounted for by evolutionary principles. In response to the vehement opposition to Darwinism was the \"anecdotal movement\" led by George Romanes who set out to demonstrate that animals possessed a \"rudimentary human mind\". Romanes is most famous for two major flaws in his work: his focus on anecdotal observations and entrenched anthropomorphism.\n\nNear the end of the 19th century, several scientists existed whose work was also very influential. Douglas Alexander Spalding was called the \"first experimental biologist\", and worked mostly with birds; studying instinct, imprinting, and visual and auditory development. Jacques Loeb emphasized the importance of objectively studying behavior, Sir John Lubbock is credited with first using mazes and puzzle devices to study learning and Conwy Lloyd Morgan is thought to be \"the first ethologist in the sense in which we presently use the word\".\n\nThroughout the long history of comparative psychology, repeated attempts have been made to enforce a more disciplined approach, in which similar studies are carried out on animals of different species, and the results interpreted in terms of their different phylogenetic or ecological backgrounds. Behavioral ecology in the 1970s gave a more solid base of knowledge against which a true comparative psychology could develop. However, the broader use of the term \"comparative psychology\" is enshrined in the names of learned societies and academic journals, not to mention in the minds of psychologists of other specialisms, so the label of the field is never likely to disappear completely.\n\nA persistent question with which comparative psychologists have been faced is the relative intelligence of different species of animal. Indeed, some early attempts at a genuinely comparative psychology involved evaluating how well animals of different species could learn different tasks. These attempts floundered; in retrospect it can be seen that they were not sufficiently sophisticated, either in their analysis of the demands of different tasks, or in their choice of species to compare. However, the definition of \"intelligence\" in comparative psychology is deeply affected by anthropomorphism, and focuses on simple tasks, complex problems, reversal learning, learning sets, and delayed alternation are plagued with practical and theoretical problems. In the literature, \"intelligence\" is defined as whatever is closest to human performance and neglects behaviors that humans are usually incapable of (e.g. echolocation). Specifically, comparative researchers encounter problems associated with individual differences, differences in motivation, differences in reinforcement, differences in sensory function, differences in motor capacities, and species-typical preparedness (i.e. some species have evolved to acquire some behaviors quicker than other behaviors).\n\nA wide variety of species have been studied by comparative psychologists. However, a small number have dominated the scene. Ivan Pavlov's early work used dogs; although they have been the subject of occasional studies, since then they have not figured prominently. Increasing interest in the study of abnormal animal behavior has led to a return to the study of most kinds of domestic animal. Thorndike began his studies with cats, but American comparative psychologists quickly shifted to the more economical rat, which remained the almost invariable subject for the first half of the 20th century and continues to be used.\n\nSkinner introduced the use of pigeons, and they continue to be important in some fields. There has always been interest in studying various species of primate; important contributions to social and developmental psychology were made by Harry F. Harlow's studies of maternal deprivation in rhesus monkeys. Cross-fostering studies have shown similarities between human infants and infant chimpanzees. Kellogg and Kellogg (1933) aimed to look at heredity and environmental effects of young primates. They found that a cross-fostered chimpanzee named Gua was better at recognizing human smells and clothing and that the Kelloggs' infant (Donald) recognised humans better by their faces. The study ended 9 months after it had begun, after the infant began to imitate the noises of Gua.\n\nNonhuman primates have also been used to show the development of language in comparison with human development. For example, Gardner (1967) successfully taught the female chimpanzee Washoe 350 words in American Sign Language. Washoe subsequently passed on some of this teaching to her adopted offspring, Loulis. A criticism of Washoe's acquisition of sign language focused on the extent to which she actually understood what she was signing. Her signs may have just based on an association to get a reward, such as food or a toy. Other studies concluded that apes do not understand linguistic input, but may form an intended meaning of what is being communicated. All great apes have been reported to have the capacity of allospecific symbolic production.\n\nInterest in primate studies has increased with the rise in studies of animal cognition. Other animals thought to be intelligent have also been increasingly studied. Examples include various species of corvid, parrots — especially the grey parrot — and dolphins. Alex (Avian Learning EXperiment) is a well known case study (1976–2007) which was developed by Pepperberg, who found that the African gray parrot Alex did not only mimic vocalisations but understood the concepts of same and different between objects. The study of non-human mammals has also included the study of dogs. Due to their domestic nature and personalities, dogs have lived closely with humans, and parallels in communication and cognitive behaviours have therefore been recognised and further researched. Joly-Mascheroni and colleagues (2008) demonstrated that dogs may be able to catch human yawns and suggested a level of empathy in dogs, a point that is strongly debated. Pilley and Reid found that a Border Collie named Chaser was able to successfully identify and retrieve 1022 distinct objects/toys.\n\nResearchers who study animal cognition are interested in understanding the mental processes that control complex behavior, and much of their work parallels that of cognitive psychologists working with humans. For example, there is extensive research with animals on attention, categorization, concept formation, memory, spatial cognition, and time estimation. Much research in these and other areas is related directly or indirectly to behaviors important to survival in natural settings, such as navigation, tool use, and numerical competence. Thus, comparative psychology and animal cognition are heavily overlapping research categories.\n\nVeterinary surgeons recognize that the psychological state of a captive or domesticated animal must be taken into account if its behavior and health are to be understood and optimized.\n\nCommon causes of disordered behavior in captive or pet animals are lack of stimulation, inappropriate stimulation, or overstimulation. These conditions can lead to disorders, unpredictable and unwanted behavior, and sometimes even physical symptoms and diseases. For example, rats who are exposed to loud music for a long period will ultimately develop unwanted behaviors that have been compared with human psychosis, like biting their owners.\n\nThe way dogs behave when understimulated is widely believed to depend on the breed as well as on the individual animal's character. For example, huskies have been known to ruin gardens and houses if they are not allowed enough activity. Dogs are also prone to psychological damage if they are subjected to violence. If they are treated very badly, they may become dangerous.\n\nThe systematic study of disordered animal behavior draws on research in comparative psychology, including the early work on conditioning and instrumental learning, but also on ethological studies of natural behavior. However, at least in the case of familiar domestic animals, it also draws on the accumulated experience of those who have worked closely with the animals.\n\nThe relationship between humans and animals has long been of interest to anthropologists as one pathway to an understanding the evolution of human behavior. Similarities between the behavior of humans and animals have sometimes been used in an attempt to understand the evolutionary significance of particular behaviors. Differences in the treatment of animals have been said to reflect a society's understanding of human nature and the place of humans and animals in the scheme of things. Domestication has been of particular interest. For example, it has been argued that, as animals became domesticated, humans treated them as property and began to see them as inferior or fundamentally different from humans.\nIngold remarks that in all societies children have to learn to differentiate and separate themselves from others. In this process, strangers may be seen as \"not people,\" and like animals. Ingold quoted Sigmund Freud: \"Children show no trace of arrogance which urges adult civilized men to draw a hard-and-fast line between their own nature and that of all other animals. Children have no scruples over allowing animals to rank as their full equals.\" With maturity however, humans find it hard to accept that they themselves are animals, so they categorize, separating humans from animals, and animals into wild animals and tame animals, and tame animals into house pets and livestock. Such divisions can be seen as similar to categories of humans: who is part of a human community and someone who isn't, that is, the outsider.\n\n\"The New York Times\" ran an article that showed the psychological benefits of animals, more specifically of children with their pets. It's been proven that having a pet does in fact improve kids' social skills. In the article, Dr. Sue Doescher, a psychologist involved in the study, stated, \"It made the children more cooperative and sharing.\" It was also shown that these kids were more confident with themselves and able to be more empathic with other children.\n\nFurthermore, in an edition of \"Social Science and Medicine\" it was stated, \"A random survey of 339 residents from Perth, Western Australia were selected from three suburbs and interviewed by telephone. Pet ownership was found to be positively associated with some forms of social contact and interaction, and with perceptions of neighborhood friendliness. After adjustment for demographic variables, pet owners scored higher on social capital and civic engagement scales.\" Results like these let us know that owning a pet provides opportunities for neighborly interaction, among many other chances for socialization among people.\n\nNoted comparative psychologists, in this broad sense, include:\n\nMany of these were active in fields other than animal psychology; this is characteristic of comparative psychologists.\n\nFields of psychology and other disciplines that draw upon, or overlap with, comparative psychology include:\n\n\n"}
{"id": "2466507", "url": "https://en.wikipedia.org/wiki?curid=2466507", "title": "Comparative sociology", "text": "Comparative sociology\n\nComparative sociology involves comparison of the social processes between nation states, or across different types of society (for example capitalist and socialist). There are two main approaches to comparative sociology: some seek similarity across different countries and cultures whereas others seek variance. For example, structural Marxists have attempted to use comparative methods to discover the general processes that underlie apparently different social orderings in different societies. The danger of this approach is that the different social contexts are overlooked in the search for supposed universal structures.\n\nOne sociologist who employed comparative methods to understand variance was Max Weber, whose studies attempted to show how differences between cultures explained the different social orderings that had emerged (see for example \"The Protestant Ethic and the Spirit of Capitalism\" and Sociology of religion).\n\nThere is some debate within sociology regarding whether the label of 'comparative' is suitable. Emile Durkheim argued in \"The Rules of Sociological Method\" (1895) that all sociological research was in fact comparative since social phenomenon are always held to be typical, representative or unique, all of which imply some sort of comparison. In this sense, all sociological analysis is comparative and it has been suggested that what is normally referred to as comparative research, may be more appropriately called cross-national research.\n\n"}
{"id": "11797804", "url": "https://en.wikipedia.org/wiki?curid=11797804", "title": "Comparison (grammar)", "text": "Comparison (grammar)\n\nComparison is a feature in the morphology or syntax of some languages, whereby adjectives and adverbs are inflected or modified to indicate the relative degree of the property defined by the adjective or adverb. The comparative expresses a comparison between two (or more) entities or groups of entities in quality, quantity, or degree; the superlative is the form of an adverb or adjective that is the greatest degree of a given descriptor.\n\nThe grammatical category associated with comparison of adjectives and adverbs is degree of comparison. The usual degrees of comparison are the \"positive\", which simply denotes a property (as with the English words \"big\" and \"fully\"); the \"comparative\", which indicates \"greater degree (as \"bigger\" and \"more fully\"); and the \"superlative\", which indicates \"greatest degree (as \"biggest\" and \"most fully\"). Some languages have forms indicating a very large degree of a particular quality (called elative in Semitic linguistics). Other languages (e.g. English) can express lesser degree, e.g. \"beautiful\", \"less beautiful\", \"least beautiful\".\n\nThe comparative is frequently associated with adjectives and adverbs because these words take the \"-er\" suffix or modifying word \"more\" or \"less\" (e.g., \"faster\", \"more intelligent\", \"less wasteful\"); it can also, however, appear when no adjective or adverb is present, for instance with nouns (e.g., \"more men than women\"). One preposition, \"near\", also has a superlative form, as in \"Find the restaurant nearest your house\".\n\nComparatives and superlatives may be formed morphologically, by inflection, as with the English and German \"-er\" and \"-(e)st\" forms, or syntactically, as with the English \"more...\" and \"most...\" and the French \"plus...\" and \"le plus...\" forms. Common adjectives and adverbs often produce irregular forms, such as \"better\" and \"best\" (from \"good\") and \"less\" and \"least\" (from \"little/few\") in English, and \"meilleur\" (from \"bon\") and \"mieux\" (from the adverb \"bien\") in French.\n\nMost if not all languages have some means of forming the comparative, although these means can vary significantly from one language to the next.\n\nComparatives are often used with a conjunction or other grammatical means to indicate with what the comparison is being made, as with \"than\" in English, \"als\" in German, etc. In Russian and Greek (Ancient, Koine and Modern) this can be done by placing the compared noun in the genitive case. With superlatives, the class of things being considered for comparison may be indicated, as in \"the best swimmer out of all the girls\".\n\nLanguages also possess other structures for comparing adjectives and adverbs; English examples include \"as... as\" and \"less/least...\".\n\nА few languages apply comparison to nouns and even verbs. One such language is Bulgarian, where expressions like \"по̀ човек (po chovek), най човек (nay chovek), по-малко човек (po malko chovek)\" (literally \"more person\", \"most person\", \"less person\" but normally \"better kind of a person\", \"best kind of person\", \"not that good kind of a person\") and \"по̀ обичам (po obicham), най-малко обичам (nay malko obicham)\" (\"I like more\", \"I like the least\") are quite usual.\n\nIn many languages, including English, traditional grammar requires the comparative form to be used when exactly two things are being considered, even in constructions where the superlative would be used when considering a larger number. For instance, \"May the better man win\" would be considered correct if there are only two individuals competing. However, this rule is not always observed in informal usage; the form \"May the best man win\" will often be used in that situation, as it would if there were three or more competitors involved.\n\nIn some contexts, such as advertising or political speeches, absolute and relative comparatives are intentionally employed in a way that invites a comparison, and yet the basis of comparison is not established. This is a common rhetorical device used to create an implication of significance where one may not actually be present. Although such usage is common, it is sometimes considered ungrammatical.\n\nFor example:\n\nEnglish has two parallel systems of comparison, a morphological one formed using the suffixes \"-er\" (the \"comparative\") and \"-est\" (the \"superlative\"), with some irregular forms; and a syntactic one, formed with the adverbs \"more\" and \"most\".\n\nAs a general rule, words with one syllable require the suffix (except for the four words: fun, real, right, wrong), words with three or more syllables require \"more\" or \"most\", and words with two syllables may use one system or the other; which words use which system is a matter of idiom. Some adjectives, \"e.g.\" 'polite', can use either form, with different frequencies according to context.\n\nMorphological comparison uses the suffixes \"-er\" (the \"comparative\") and \"-est\" (the \"superlative\"). These inflections are of Germanic origin and are cognate with the Latin suffixes -\"ior\" and -\"issimus\" and Ancient Greek -\"īōn\" and -\"istos\". They are typically added to shorter words, words of Anglo-Saxon origin, and borrowed words which have been fully assimilated into the English vocabulary. Usually the words which take these inflections have fewer than three syllables.\n\nThis system also contains a number of irregular forms, some of which, like \"good\", \"better\", and \"best\", contain suppletive forms. These irregular forms include:\n\nThe second system of comparison in English appends the grammatical particles \"more\" and \"most\", themselves the irregular comparatives of \"many\" and \"much\", to the adjective or adverb being modified. This series can be compared to a system containing the diminutives \"less\" and \"least\".\n\nThis system is most commonly used with words of French or Latin derivation; with adjectives and adverbs formed with suffixes other than \"-ly\" (e.g., \"beautiful\"); and with longer, technical, or infrequently used words. For example:\n\nSome adjectives, the absolute or ungradable adjectives do not appear to logically allow degrees. Some qualities are either \"present\" or \"absent\", such as being Cretaceous or igneous, so it appears illogical to call anything \"very Cretaceous\", or to characterize something as \"more igneous\" than something else.\n\nSome grammarians object to the use of the superlative or comparative with words such as \"full\", \"complete\", \"unique\", or \"empty\", which by definition already denote either a totality, an absence, or an absolute. However, such words are routinely and frequently qualified in contemporary speech and writing. This type of usage conveys more of a figurative than a literal meaning, because in a strictly literal sense, something cannot be more or less unique or empty to a greater or lesser degree.\n\nMany prescriptive grammars and style guides include adjectives for inherently superlative qualities to be ungradable. Thus, they reject expressions such as \"more perfect\", \"most unique\", and \"most parallel\" as illogical pleonasms: after all, if something is unique, it is one of a kind, so nothing can be \"very unique\", or \"more unique\" than something else.\n\nOther style guides argue that terms like \"perfect\" and \"parallel\" never apply \"exactly\" to things in real life, so they are commonly used to mean \"nearly perfect\", \"nearly parallel\", and so on; in this sense, \"more perfect\" (\"i.e.\", more nearly perfect, closer to perfect) and \"more parallel\" (\"i.e.\", more nearly parallel, closer to parallel) are meaningful.\n\nIn most Balto-Slavic languages (such as Czech, Polish, Lithuanian and Latvian), the comparative and superlative forms are also declinable adjectives.\n\nIn Bulgarian, comparative and superlative forms are formed with the clitics \"по-\" (\"more\") and \"най-\" (\"most\"):\n\nIn Czech, Polish, Slovak and Slovene, comparative is formed from the base form of an adjective with a suffix and superlative is formed with a circumfix (equivalent to adding a prefix to the comparative).\n\nIn Russian, comparative and superlative forms are usually formed with a suffix:\n\nIn contrast to English, the relative and the superlative are joined into the same degree (the superlative), which can be of two kinds: comparative (e.g. \"the most beautiful\") and absolute (e.g. \"very beautiful\").\n\nFrench: The superlative is created from the comparative by inserting the definitive article (la, le, or les), or the possessive article (\"mon\", \"ton\", \"son\", etc.), before \"plus\" or \"moins\" and the adjective determining the noun. For instance: \"Elle est la plus belle femme\" → (she is the most beautiful woman); \"Cette ville est la moins chère de France\" → (this town is the least expensive in France); \"C'est sa plus belle robe\" → (It is her most beautiful dress). It can also be created with the suffix \"-issime\" but only with certain words, for example: \"C'est un homme richissime\" → (That is the most rich man). Its use is often rare and ironic.\n\nPortuguese and Italian distinguish comparative superlative \"(superlativo relativo)\" and absolute superlative \"(superlativo absoluto/assoluto).\nFor the comparative superlative they use the words \"mais\" and \"più\" between the article and the adjective, like \"most\" in English.\nFor the absolute superlative they either use \"muito\"/\"molto\" and the adjective or modify the adjective by taking away the final vowel and adding \"issimo\" (singular masculine), \"issima\" (singular feminine), \"íssimos\"/\"issimi\" (plural masculine), or \"íssimas\"/\"issime\" (plural feminine). For example:\nThere are some irregular forms for some words ending in \"-re\" and \"-le\" (deriving from Latin words ending in \"-er\" and \"-ilis\") that have a superlative form similar to the Latin one. In the first case words lose the ending \"-re\" and they gain the endings \"errimo\" (singular masculine), \"errima\" (singular feminine), \"érrimos\"/\"errimi\" (plural masculine), or \"érrimas\"/\"errime\" (plural feminine); in the second case words lose the \"-l\"/\"-le\" ending and gain \"ílimo\"/\"illimo\" (singular masculine), \"ílima\"/\"illima\" (singular feminine), \"ílimos\"/\"illimi\" (plural masculine), or \"íli\nRomanian, similar to Portuguese and Italian, distinguishes comparative and absolute superlatives. The comparative uses the word \"mai\" before the adjective, which operates like \"more\" or \"-er\" in English. For example: \"luminos\" → bright, \"mai luminos\" → brighter. To weaken the adjective, the word \"puțin\" (little) is added between \"mai\" and the adjective, for example \"mai puțin luminos\" → less bright. For absolute superlatives, the gender-dependent determinant \"cel\" precedes \"mai,\" conjugated as \"cel / cei\" for male singular / plural and \"cea / cele\" for female singular / plural. For example: \"cea mai luminoasă stea\" → the brightest star; \"cele mai frumoase fete\" → the most beautiful girls; \"cel mai mic morcov\" → the smallest carrot.\n\nScottish Gaelic: When comparing one entity to another in the present or the future tense, the adjective is changed by adding an \"e\" to the end and \"i\" before the final consonant(s) if the final vowel is broad. Then, the adjective is preceded by \"nas\" to say \"more,\" and \"as\" to say \"most.\" (The word \"na\" is used to mean \"than\".) Adjectives that begin with \"f\" are lenited. and \"as\" use different syntax constructions. For example:\nTha mi nas àirde na mo pheathraichean.\" → I am taller than my sisters.\nIs mi as àirde.\" → I am the tallest.\n\nAs in English, some forms are irregular, i.e. nas fheàrr (better), nas miosa (worse), etc.\n\nIn other tenses, \"nas\" is replaced by \"na bu\" and \"as\" by \"a bu,\" both of which lenite the adjective if possible. If the adjective begins with a vowel or an \"f\" followed by a vowel, the word \"bu\" is reduced to \"b\"'. For example:\n\n\nWelsh is similar to English in many respects. The ending \"-af\" is added onto regular adjectives in a similar manner to the English \"-est\", and with (most) long words \"mwyaf\" precedes it, as in the English \"most\". Also, many of the most common adjectives are irregular. Unlike English, however, when comparing just two things, the superlative \"must\" be used, e.g. of two people - \"John ydy'r talaf\" (John is the tallest).\n\nIn Akkadian cuneiform, (on a 12 paragraph clay tablet), from the time period of the 1350 BC Amarna letters (a roughly 20-year body of letters), two striking examples of the superlative extend the common grammatical use. The first is the numeral \"10,\" as well as \"7 and 7.\" The second is a verb-spacement adjustment.\n\nThe term \"7 and 7\" means 'over and over'. The phrase itself is a superlative, but an addition to some of the Amarna letters adds \"more\" at the end of the phrase (EA 283, \"Oh to see the King-(pharaoh)):\" \"... I fall at the feet of the king, my lord. I fall at the feet of the king, my lord, 7 and 7 times\" more, \"...\". The word 'more' is Akkadian \"mila\", and by Moran is 'more' or 'overflowing'. The meaning in its letter context is \"...over and over again, overflowing,\" (as 'gushingly', or 'obsequiously', as an underling of the king).\n\nThe numeral 10 is used for \"ten times greater\" in EA 19, \"Love and Gold\", one of King Tushratta's eleven letters to the Pharaoh-(Amenhotep IV-\"Akhenaton\"). The following quote using 10, also closes out the small paragraph by the second example of the superlative, where the verb that ends the last sentence is spread across the letter in s-p-a-c-i-n-g, to accentuate the last sentence, and the verb itself (i.e. the relational kingly topic of the paragraph):\n\nThe actual last paragraph line contains three words: 'may it be', 'flourish', and 'us'. The verb flourish (from napāhu?, \"to light up, to rise\"), uses: -e-le-né-ep-pi-, and the spaces. The other two words on the line, are made from two characters, and then one: \"...may it be, flourish-our (relations).\"\n\nIn Estonian, the superlative form can usually be formed in two ways. One is a periphrastic construction with \"kõige\" followed by the comparative form. This form exists for all adjectives. For example: the comparative form of \"sinine\" 'blue' is \"sinisem\" and therefore the periphrastic superlative form is \"kõige sinisem\". There is also a synthetic (\"short\") superlative form, which is formed by adding \"-m\" to the end of the plural partitive case. For \"sinine\" the plural partitive form is \"siniseid\" and so \"siniseim\" is the short superlative. The short superlative does not exist for all adjectives and, in contrast to the \"kõige\"-form, has a lot of exceptions.\n\n"}
{"id": "10485277", "url": "https://en.wikipedia.org/wiki?curid=10485277", "title": "Comparison microscope", "text": "Comparison microscope\n\nA comparison microscope is a device used to analyze side-by-side specimens. It consists of two microscopes connected by an optical bridge, which results in a split view window enabling two separate objects to be viewed simultaneously. This avoids the observer having to rely on memory when comparing two objects under a conventional microscope.\n\nIn the 1920s forensic ballistics was waiting at its inception. In 1929, using a comparison microscope adapted for the purpose by Calvin Goddard and his partner Phillip Gravelle used similar techniques to absolve the Chicago Police Department of participation in the St. Valentine's Day Massacre.\n\n Philip O. Gravelle, a chemist, developed a comparison microscope for use in the identification of fired bullets and cartridge cases with the support and guidance of forensic ballistics pioneer Calvin Goddard. It was a significant advance in the science of firearms identification in forensic science. The firearm from which a bullet or cartridge case has been fired is identified by the comparison of the unique striae left on the bullet or cartridge case from the worn, machined metal of the barrel, breach block, extractor, or firing pin in the gun. It was Gravelle who mistrusted his memory. \"As long as he could inspect only one bullet at a time with his microscope, and had to keep the picture of it in his memory until he placed the comparison bullet under the microscope, scientific precision could not be attained. He therefore developed the comparison microscope and Goddard made it work.\" Calvin Goddard perfected the comparison microscope and subsequently popularized its use.Sir Sydney Smith also appreciated the idea, emphasizing its importance in forensic science and firearms identification. He took the comparison microscope to Scotland and introduced it to the European scientists for firearms identification and other forensic science needs.\n\nThe modern instrument has many optical, mechanical and electronic refinements, including fiber optic illumination, video capabilities, digital imaging, automatic exposure for conventional photography, etc. Despite this evolution, however, the basic tools and techniques have remained unchanged which are to determine whether or not ammunition components were fired by a single firearm based on unique and reproducible microscopic and class characteristics, or to reach a \"no conclusion\" result if insufficient marks are present.\n\nSince, ballistic identification has benefited from a long series of structural, scientific and technological advances, law enforcement agencies have established forensic laboratories and researchers have learned much more about how to match bullets and cartridge cases to the guns used to fire them, and comparison microscopes have become more sophisticated. By the end of the 1980s, ballistic identification was an established sub-specialty of forensic science.\n\nVisualization tools have also been developed to allows the firearms examiner to verify the degree of similarity between any two tool-marks in question. These are designed to simulate the operation of the comparison microscope but is capable of rendering a 2D view of the 3D surfaces in a manner similar to that of the conventional comparison microscope.\n\nThe prevalence of hand-gun related crime in the United States compared to most other developed countries provided the impetus for the development of the comparison microscope. As with most firearms, the fired ammunition components may acquire sufficient unique and reproducible microscopic marks to be identifiable as having been fired by a single firearm. Making these comparisons is correctly referred to as firearms identification, or sometimes called as \"ballistics\".\n\nHistorically, and currently, this forensic discipline ultimately requires a microscopic side-by-side comparison of fired bullets or cartridge cases, one pair at a time, by a forensic examiner to confirm or eliminate the two items as having been fired by a single firearm. For this purpose, the traditional tool of the firearms examiner has been what is often called the ballistics comparison microscope.\n\nThe interior of a gun's barrel is machined to have grooves (called rifling) that force the bullet to rotate as it travels along it. These grooves and their counterpart, called \"lands\" imprint groove and land impressions on the surface of the bullet. Together with these land and groove impressions, imperfections on the barrel surface are incidentally transferred to the bullet's surface. Because these imperfections are randomly generated, during manufacture or due to use, they are unique to each barrel. These patterns or imperfections, therefore, amount to a \"signature\" that each barrel imprints on each of the bullets fired through it. It is this \"signature\" on the bullets imparted due to the unique imperfections on the barrel that enable the validation and identification of bullets as having originated from a particular gun. Comparison microscope is used to analyze the matching of the microscopic impressions found on the surface of bullets and casings.\n\nWhen a firearm or a bullet or cartridge case are recovered from a crime scene, forensic examiners compare the ballistic fingerprint of the recovered bullet or cartridge case with the ballistic fingerprint of a second bullet or cartridge case test-fired from the recovered firearm. If the ballistic fingerprint on the test-fired bullet or cartridge case matches the ballistic fingerprint on the recovered bullet or cartridge case, investigators know that the recovered bullet or cartridge case was also fired from the recovered gun. A confirmed link between a specific firearm and a bullet or cartridge case recovered from a crime scene constitutes a valuable lead, because investigators may be able to connect the firearm to a person, who may then become either a suspect or a source of information helpful to the investigation.\n\nForensic innovator Calvin Goddard offered ballistic identification evidence in 1921 to help secure convictions of accused murderers and anarchists Nicola Sacco and Bartolomeo Vanzetti. On April 8, 1927, Sacco and Vanzetti were finally sentenced to death in the electric chair. A worldwide outcry arose and Governor Alvin T. Fuller finally agreed to postpone the executions and set up a committee to reconsider the case. By this time, firearms examination had improved considerably, and it was now known that a semi-automatic pistol could be traced by several different methods if both bullet and casing were recovered from the scene. Automatic pistols could now be traced by unique markings of the rifling on the bullet, by firing pin indentations on the fired primer, or by unique ejector and extractor marks on the casing. The committee appointed to review the case used the services of Calvin Goddard in 1927.\nGoddard used Philip Gravelle's newly invented comparison microscope and helixometer, a hollow, lighted magnifier probe used to inspect gun barrels, to make an examination of Sacco's .32 Colt, the bullet that killed Berardelli, and the spent casings recovered from the scene of the crime. In the presence of one of the defense experts, he fired a bullet from Sacco's gun into a wad of cotton and then put the ejected casing on the comparison microscope next to casings found at the scene. Then he looked at them carefully. The first two casings from the robbery did not match Sacco's gun, but the third one did. Even the defense expert agreed that the two cartridges had been fired from the same gun. The second original defense expert also concurred. The committee upheld the convictions.\nIn October 1961, ballistics tests were run with improved technology using Sacco's Colt automatic. The results confirmed that the bullet that killed the victim, Berardelli in 1920 came from the same .32 Colt Auto taken from the pistol in Sacco's possession. Subsequent investigations in 1983 also supported Goddard's findings.\n\nColonel Goddard was the key forensic expert in solving the 1929 St. Valentine's Day Massacre in which seven gangsters were killed by rival Al Capone mobsters dressed as Chicago police officers. It also led to the establishment of the United States' first independent criminological laboratory, which was located at Northwestern University and headed by Goddard. At this new lab, ballistics, fingerprinting, blood analysis and trace evidence were all brought under one roof.\nIn 1929, using a comparison microscope adapted for the ballistics comparison by his partner, Phillip Gravelle, Goddard used similar techniques to absolve the Chicago Police Department of participation in the St. Valentine's Day Massacre. The case of Sacco and Vanzetti, which took place in Bridgewater, Massachusetts, is responsible for popularizing the use of the comparison microscope for bullet comparison. Forensic expert Calvin Goddard's conclusions were upheld when the evidence was re-examined in 1961.\n\n\n"}
{"id": "1266596", "url": "https://en.wikipedia.org/wiki?curid=1266596", "title": "Comparison of Dewey and Library of Congress subject classification", "text": "Comparison of Dewey and Library of Congress subject classification\n\nThis is a conversion chart showing how the Dewey Decimal and Library of Congress Classification systems organize resources by concept, in part for the purpose of assigning . These two systems account for over 95% of the classification in United States libraries, and are used widely around the world.\n\nThe chart includes all ninety-nine second level (two-digit) DDC classes (040 is not assigned), and should include all second level (two-digit) LCC classes. Where a class in one system maps to several classes in other system, it will be listed multiple times (e.g. DDC class 551).\n\nAdditional information on these classification plans is available at:\n\n\n"}
{"id": "31994535", "url": "https://en.wikipedia.org/wiki?curid=31994535", "title": "Comparison of Nazism and Stalinism", "text": "Comparison of Nazism and Stalinism\n\nA number of authors have carried out comparisons of Nazism and Stalinism, in which they have considered the similarities and differences of the two ideologies and political systems, what relationship existed between the two regimes, and why both of them came to prominence at the same time. During the 20th century, the comparison of Stalinism and Nazism was made on the topics of totalitarianism, ideology, and personality cult. Both regimes were seen in contrast to the liberal West, with an emphasis on the similarities between the two. The American political scientists Zbigniew Brzezinski, Hannah Arendt and Carl Friedrich and historian Robert Conquest were prominent advocates of applying the \"totalitarian\" concept to compare Nazism and Stalinism.\n\nOne of the first scholars to publish a comparative study of Nazi Germany and Stalin’s Soviet Union was Hannah Arendt. In her 1951 work, \"The Origins of Totalitarianism\", Arendt puts forward the idea of totalitarianism as a distinct type of political movement and form of government, which “differs essentially from other forms of political oppression known to us such as despotism, tyranny and dictatorship.” Furthermore, Arendt distinguishes between a totalitarian movement (such as a political party with totalitarian aims) and a totalitarian government. Not all totalitarian movements succeed in creating totalitarian governments once they gain power. In Arendt’s view, although many totalitarian movements existed in Europe in the 1920s and 1930s, only the governments of Stalin and Hitler succeeded in fully implementing their totalitarian aims. \n\nArendt traced the origin of totalitarian movements to the nineteenth century, focusing especially on antisemitism and imperialism. She emphasized the connection between the rise of European nation-states and the growth of antisemitism, which was due to the fact that the Jews represented an “inter-European, non-national element in a world of growing or existing nations.” Conspiracy theories abounded, and the Jews were accused of being part of various international schemes to ruin European nations. Small antisemitic political parties formed in response to this perceived Jewish threat, and, according to Arendt, these were the first political organizations in Europe that claimed to represent the interests of the whole nation as opposed to the interests of a class or other social group. The later totalitarian movements would copy or inherit this claim to speak for the whole nation, with the implication that any opposition to them constituted treason.\n\nEuropean imperialism of the nineteenth century also paved the way for totalitarianism, by legitimizing the concept of endless expansion. After Europeans had engaged in imperialist expansion on other continents, political movements developed which aimed to copy the methods of imperialism on the European continent itself. Arendt refers specifically to the “pan-movements” of pan-Germanism and pan-Slavism, which promised continental empires to nations that had little hope of overseas expansion. According to Arendt, “Nazism and Bolshevism owe more to Pan-Germanism and Pan-Slavism (respectively) than to any other ideology or political movement.”\n\nArendt argues that both the Nazi and Bolshevik movements “recruited their members from [a] mass of apparently indifferent people whom all other parties had given up,” and who “had reason to be equally hostile to all parties.” For this reason, totalitarian movements did not need to use debate or persuasion, and did not need to refute the arguments of the other parties. Their target audience did not have to be persuaded to despise the other parties or the democratic system, because it consisted of people who already despised mainstream politics. As a result, totalitarian movements were free to use violence and terror against their opponents without fear that this might alienate their own supporters. Instead of arguing against their opponents, they adopted deterministic views of human behavior and presented opposing ideas as “originating in deep natural, social, or psychological sources beyond the control of the individual and therefore beyond the power of reason.” The Nazis in particular, during the years before their rise to power, engaged in “killing small socialist functionaries or influential members of opposing parties” both as a means to intimidate opponents and as a means of demonstrating to their supporters that they were a party of action, “different from the ‘idle talkers’ of other parties.”\n\nTotalitarian governments make extensive use of propaganda, and are often characterized by having a strong distinction between what they tell their own supporters and the propaganda they produce for others. Arendt distinguishes these two categories as \"indoctrination\" and \"propaganda\". Indoctrination consists of the message that a totalitarian government promotes internally, to the members of the ruling party and that segment of the population which supports the government. Propaganda consists of the message that a totalitarian government seeks to promote in the outside world, and also among those parts of its own society which may not support the government. Thus, “the necessities for propaganda are always dictated by the outside world,” while the opportunities for indoctrination depend on “the totalitarian governments’ isolation and security from outside interference.” \n\nThe type of indoctrination used by the Soviets and the Nazis was characterized by claims of “scientific” truth, and appeals to “objective laws of nature.” Both movements took a deterministic view of human society and claimed that their ideologies were based on scientific discoveries regarding race (in the case of the Nazis) or the forces governing human history (in the case of the Soviets). Arendt identifies this as being in certain ways similar to modern advertising, in which companies claim that scientific research shows their products to be superior, but more generally she argues that it is an extreme version of “that obsession with science which has characterized the Western world since the rise of mathematics and physics in the sixteenth century.” By their use of pseudoscience as the main justification for their actions, Nazism and Stalinism are distinguished from earlier historical despotic regimes, who appealed instead to religion or sometimes did not try to justify themselves at all. According to Arendt, totalitarian governments did not merely use these appeals to supposed scientific laws as propaganda to manipulate others. Rather, totalitarian leaders like Hitler and Stalin genuinely believed that they were acting in accordance with immutable natural laws, to such an extent that they were willing to sacrifice the self-interest of their regimes for the sake of enacting those supposed laws. For instance, the Nazis treated the inhabitants of occupied territories with extreme brutality and planned to depopulate Eastern Europe in order to make way for colonists from the German “master race,” despite the fact that this actively harmed their war effort. Stalin repeatedly purged the Communist Party of people who deviated even slightly from the party line, even when this weakened the party or the Soviet government, because he believed that they represented the interests of “dying classes” and their demise was historically inevitable.\n\nArendt also identifies the central importance of an all-powerful leader in totalitarian movements. As in other areas, she distinguishes between totalitarian leaders (such as Hitler and Stalin) and non-totalitarian dictators or autocratic leaders. The totalitarian leader does not rise to power by personally using violence or through any special organizational skills, but rather by controlling appointments of personnel within the party, so that all other prominent party members owe their positions to him. With loyalty to the leader becoming the primary criterion for promotion, ambitious party members compete with each other in trying to express their loyalty, and a cult of personality develops around the leader. Even when the leader is not particularly competent and the members of his inner circle are aware of his deficiencies, they remain committed to him out of fear that without him the entire power structure would collapse.\n\nOnce in power, according to Arendt, totalitarian movements face a major dilemma: they built their support on the basis of anger against the status quo and on impossible or dishonest promises, but now they have become the new status quo and are expected to carry out their promises. They deal with this problem by engaging in a constant struggle against external and internal enemies, real or imagined, so as to enable them to say that, in a sense, they have not yet gained the power they need to fulfill their promises. According to Arendt, totalitarian governments must be constantly fighting enemies in order to survive. This explains their apparently irrational behavior, for example when Hitler continued to make territorial demands even after he was offered everything he asked for in the Munich Agreement, or when Stalin unleashed the Great Terror despite the fact that he faced no significant internal opposition.\n\nArendt points out the widespread use of concentration camps by totalitarian governments, arguing that they are the most important manifestation of the need to find enemies to fight against, and are therefore “more essential to the preservation of the regime’s power than any of its other institutions.” Although forced labor was commonly imposed on inmates of concentration camps, Arendt argues that their primary purpose was not any kind of material gain for the regime: “The only permanent economic function of the camps has been the financing of their own supervisory apparatus; thus from the economic point of view the concentration camps exist mostly for their own sake.” The Nazis in particular carried this to the point of “open anti-utility,” by expending large sums of money, resources and manpower – during a war – for the purpose of building and staffing extermination camps and transporting people to them. This sets apart the concentration camps of totalitarian regimes from older human institutions that bear some similarity to them, such as slavery. Slaves were abused and killed for the sake of profit; concentration camp inmates were abused and killed because a totalitarian government needed to justify its existence. Finally, Arendt points out that concentration camps under both Hitler and Stalin included large numbers of inmates who were innocent of any crime – not only in the ordinary sense of the word, but even by the standards of the regimes themselves. That is to say, most of the inmates had not actually committed any action against the regime.\n\nThroughout her analysis, Arendt emphasized the modernity and novelty of the governmental structures set up by Stalin and Hitler, arguing that they represented “an entirely new form of government” which is likely to manifest itself again in various other forms in the future. She also cautioned against the belief that future totalitarian movements would necessarily share the ideological foundations of Nazism or Stalinism, writing that “all ideologies contain totalitarian elements.”\n\nThe totalitarian paradigm in the comparative study of Nazi Germany and the Soviet Union was further developed by Carl Friedrich and Zbigniew Brzezinski, who wrote extensively on this topic both individually and in collaboration. Similar to Hannah Arendt, they state that “totalitarian dictatorship is a new phenomenon; there has never been anything quite like it before.” Friedrich and Brzezinski classify totalitarian dictatorship as a type of autocracy, but argue that it is different in important ways from most other historical autocracies. In particular, it is distinguished by a reliance on modern technology and mass legitimation. Unlike Arendt, Friedrich and Brzezinski apply the notion of totalitarian dictatorship not only to the regimes of Hitler and Stalin, but also to the USSR throughout its entire existence, as well as the regime of Benito Mussolini in Italy and the People’s Republic of China under Mao Zedong.\n\nCarl Friedrich noted that the “possibility of equating the dictatorship of Stalin in the Soviet Union and that of Hitler in Germany” has been a deeply controversial topic and a subject of debate almost from the beginning of those dictatorships. Various other aspects of the two regimes have also been the subject of intense scholarly debate, such as whether Nazi and Stalinist ideologies were genuinely believed and pursued by the respective governments, or whether the ideologies were merely convenient justifications for dictatorial rule. Friedrich himself argues in favor of the former view.\n\nFriedrich and Brzezinski argue that Nazism and Stalinism are not only similar to each other, but also represent a continuation or a return to the tradition of European absolute monarchy on certain levels. In the absolute monarchies of the seventeenth and eighteenth centuries, the monarch ultimately held all decisional power, and was considered accountable only to God. In Stalinism and Nazism, the leader likewise held all real power, and was considered accountable only to various intangible entities such as “the people”, “the masses” or “the Volk.” Thus the common feature of autocracies – whether monarchical or totalitarian – is the concentration of power in the hands of a leader who cannot be held accountable by any legal mechanisms, and who is supposed to be the embodiment of the will of an abstract entity. Friedrich and Brzezinski also identify other features common to all autocracies, such as “the oscillation between tight and loose control.” The regime alternates between periods of intense repression and periods of relative freedom, often represented by different leaders. This depends in part on the personal character of different leaders, but Friedrich and Brzezinski believe that there is also an underlying political cycle, in which rising discontent leads to increased repression up to the point at which the opposition is eliminated, then controls are relaxed until the next time that popular dissatisfaction begins to grow.\n\nThus, placing Stalinism and Nazism within the broader historical tradition of autocratic government, Friedrich and Brzezinski hold that “totalitarian dictatorship, in a sense, is the adaptation of autocracy to twentieth-century industrial society.” However, at the same time, they insist that totalitarian dictatorship is a “\"novel\" type of autocracy” and argue that twentieth century totalitarian regimes (such as those of Hitler and Stalin) had more in common with each other than with any other form of government, including historical autocracies of the past. Totalitarianism can only exist after the creation of modern technology, because such technology is essential for propaganda, for surveillance of the population, and for the operation of a secret police. Furthermore, when speaking of the differences and similarities between fascist and communist regimes, Friedrich and Brzezinski insist that the two kinds of totalitarian governments are “basically alike” but “not wholly alike” – they are more similar to each other than to other forms of government, but they are not the same. Among the major differences between them, Friedrich and Brzezinski identify in particular the fact that communists seek “the world revolution of the proletariat,” while fascists wish to “establish the imperial predominance of a particular nation or race.” \n\nIn terms of the similarities between Nazism and Stalinism, Friedrich lists five main aspects that they hold in common: First, an official ideology that is supposed to be followed by all members of society, at least passively, and which promises to serve as a perfect guide towards some ultimate goal. Second, a single political party, composed of the most enthusiastic supporters of the official ideology, representing an elite group within society (no more than 10 percent of the population), and organized along strictly regimented lines. Third, “a technologically conditioned near-complete monopoly of control of all means of effective armed combat” in the hands of the party or its representatives. Fourth, a similar monopoly held by the party over the mass media and all technological forms of communication. Fifth, “a system of terroristic police control” that is not only used to defend the regime against real enemies, but also to persecute various groups of people who are only suspected of being enemies or who may potentially become enemies in the future.\n\nTwo first pillars of any totalitarian government, according to Friedrich and Brzezinski, are the dictator and the Party. The dictator, whether Stalin, Hitler or Mussolini, holds supreme power. Friedrich and Brzezinski explicitly reject the claim that the Party, or any other institution, could provide a significant counterweight to the power of the dictator in Nazism or Stalinism. The dictator needs the Party in order to be able to rule, so he may be careful not to make decisions that would go directly against the wishes of other leading Party members, but ultimate authority rests with him and not with them. Like Arendt, Friedrich and Brzezinski also identify the cult of personality surrounding the leader as an essential element of a totalitarian dictatorship, and reference Stalin’s personality cult in particular. They also draw attention to the fact that Hitler and Stalin were expected to provide ideological direction for their governments and not merely practical leadership. Friedrich and Brzezinski write that “unlike military dictators in the past, but like certain types of primitive chieftains, the totalitarian dictator is both ruler and high priest.” That is to say, he not only governs, but also provides the principles on which his government is to be based. This is partly due to the way that totalitarian governments arise. They come about when a militant ideological movement seizes power, so the first leader of a totalitarian government is usually the ideologue who built the movement that seized power, and subsequent leaders try to emulate him.\n\nThe totalitarian dictator needs loyal lieutenants to carry out his orders faithfully and with a reasonable degree of efficiency. Friedrich and Brzezinski identify parallels between the men in Hitler and Stalin’s entourage, arguing that both dictators used similar people to perform similar tasks. Thus, for example, Martin Bormann and Georgy Malenkov were both capable administrators and bureaucrats, while Heinrich Himmler and Lavrentiy Beria were ruthless secret police chiefs responsible for suppressing any potential challenge to the dictator’s power. Both Hitler and Stalin promoted rivalry and distrust among their lieutenants so as to ensure that none of them would become powerful enough to challenge the dictator himself. This is the cause of an important weakness of the totalitarian regimes: the problem of succession. Friedrich points out that neither the Nazi nor the Stalinist government ever established any official line of succession or any mechanism to decide who would replace the dictator after his death. The dictator, being the venerated “father of the people,” was regarded as irreplaceable. There could never be any heir apparent, because such an heir would have been a threat to the power of the dictator while he was alive. Thus the dictator’s inevitable death would always leave behind a major power vacuum and cause a political crisis. In the case of the Nazi regime, since Hitler died mere days before the final defeat of Germany in the war, this never became a major issue. In the case of the USSR, Stalin’s death led to a prolonged power struggle.\n\nFriedrich and Brzezinski also identify key similarities between the Nazi and Stalinist political parties, which set them apart from other types of political parties. Both the Nazi Party and the CPSU under Stalin had very strict membership requirements and did not accept members on the basis of mere agreement with the Party’s ideology and goals. Rather, they strictly tested potential members, in a manner similar to exclusive clubs, and often engaged in political purges of the membership, expelling large numbers of people from their ranks (and sometimes arresting and executing those expelled, such as in the Great Purge or the Night of the Long Knives). Thus, the totalitarian party cultivates the idea that to be a member is a privilege which needs to be earned, and total obedience to the leader is required in order to maintain this privilege. While both Nazism and Stalinism required party members to display such total loyalty in practice, they differed in the way they dealt with it in theory. Nazism openly proclaimed the hierarchical ideal of absolute obedience to the Führer as one of its key ideological principles (the \"Führerprinzip\"). Stalinism, meanwhile, denied that it did anything similar, and claimed instead to uphold democratic principles, with the Party Congress (made up of elected delegates) supposedly being the highest authority. However, Stalinist elections typically featured only a single candidate, and the Party Congress met very rarely and simply approved Stalin’s decisions. Thus, regardless of the differences in their underlying ideological claims, the Nazi and Stalinist parties were organized in practice along similar lines, with a rigid hierarchy and centralized leadership.\n\nEach totalitarian party and dictator is supported by a specific totalitarian ideology. Friedrich and Brzezinski argue, in agreement with Arendt, that Nazi and Stalinist leaders really believed in their respective ideologies and did not merely use them as tools to gain power. Several major policies, such as the Stalinist collectivization of agriculture or the Nazi “final solution”, cannot be explained by anything other than a genuine commitment to achieve ideological goals, even at great cost. The ideologies were different and their goals were different, but what they had in common was a utopian commitment to reshaping the world, and a determination to fight by any means necessary against a real or imagined enemy. This stereotyped enemy could be described as “the fat rich Jew or the Jewish Bolshevik” for the Nazis, or “the war-mongering, atom-bomb-wielding American Wallstreeter” for the Soviets.\n\nAccording to Friedrich and Brzezinski, the most important difference between Nazi and Stalinist ideology lies in the degree of universality involved. Stalinism, and communist ideology in general, is universal in its appeal and addresses itself to all the “workers of the world.” Nazism, on the other hand, and fascist ideology in general, can only address itself to one particular race or nation – the “master race” that is destined to dominate all others. Therefore, “in communism social justice appears to be the ultimate value, unless it be the classless society that is its essential condition; in fascism, the highest value is dominion, eventually world dominion, and the strong and pure nation-race is \"its\" essential condition, as seen by its ideology.” This means that fascist or Nazi movements from different countries will be natural enemies, rather than natural allies, as they each seek to extend the dominion of their own nation at the expense of others. Friedrich and Brzezinski see this as a weakness inherent in fascist and Nazi ideology, while communist universalism is a source of ideological strength for Stalinism.\n\nFriedrich and Brzezinski also draw attention to the symbols used by Nazis and Stalinists to represent themselves. The Soviet Union adopted the hammer and sickle, a newly-created symbol, “invented by the leaders of the movement and pointing to the future.” Meanwhile, Nazi Germany used the swastika, “a ritual symbol of uncertain origin, quite common in primitive societies.” Thus, one is trying to project itself as being oriented towards a radically new future, while the other is appealing to a mythical heroic past.\n\nTotalitarian dictatorships maintain themselves in power through the use of propaganda and terror, which Friedrich and Brzezinski believe to be closely connected. Terror may be enforced with arrests and executions of dissenters, but it can also take more subtle forms, such as the threat of losing one’s job, social stigma and defamation. “Terror” can refer to any widespread method used to intimidate people into submission as a matter of daily life. According to Friedrich and Brzezinski, the most effective terror is invisible to the people it affects. They simply develop a habit of acting in a conformist manner and not questioning authority, without necessarily being aware that this is what they are doing. Thus, terror creates a society dominated by apparent consensus, where the vast majority of the population appears to support the government. Propaganda is then used to maintain this appearance of popular consent. \n\nTotalitarian propaganda is one of the features that distinguishes totalitarian regimes as modern forms of government and separates them from older autocracies, since a totalitarian government holds complete control over all means of communication (not only public communication such as the mass media, but also private communication such as letters and telephone calls, which are strictly monitored). The methods of propaganda were very similar in the Stalinist USSR and in Nazi Germany. Both Joseph Goebbels and Soviet propagandists sought to demonize their enemies and present a picture of a united people standing behind its leader to confront foreign threats. In both cases there was no attempt to convey complex ideological nuances to the masses, with the message being instead about a simplistic struggle between good and evil. Both Nazi and Stalinist regimes produced two very different sets of propaganda – one for internal consumption and one for potential sympathizers in other countries. And both regimes would sometimes radically change their propaganda line as they made peace with a former enemy or got into a war with a former ally. Yet, paradoxically, a totalitarian government’s complete control over communications renders that government highly misinformed. With no way for anyone to express criticism, the dictator has no way of knowing how much support he actually has among the general populace. With all government policies always declared successful in propaganda, officials are unable to determine what actually worked and what didn’t. Both Stalinism and Nazism suffered from this problem, especially during the war between them. As the war turned against Germany, there was growing opposition to Hitler’s rule, including within the ranks of the military, but Hitler was never aware of this until it was too late (see: 20 July plot). In 1948, during the early days of the Berlin Blockade, the Soviet leadership apparently believed that the population of West Berlin was sympathetic to Soviet Communism and that they would request to join the Soviet zone. Given enough time, the gap between real public opinion and what the totalitarian government believes about public opinion can grow so wide that the government is no longer able to even produce effective propaganda, because it does not know what the people actually think and so it does not know what to tell them. Friedrich and Brzezinski refer to this as the “ritualization of propaganda”: the totalitarian regime continues to produce propaganda as a political ritual, with little real impact on public opinion.\n\nThe totalitarian use of mass arrests, executions and concentration camps – also noted by Arendt – was analyzed at length by Friedrich and Brzezinski. They hold that “totalitarian terror maintains, in institutionalized form, the civil war that originally produced the totalitarian movement and by means of which the regime is able to proceed with its program, first of social disintegration and then of social reconstruction.” Both Stalinism and Nazism saw themselves as engaging in a life-or-death struggle against implacable enemies. But to declare that the struggle had been won would have meant to declare that most of the totalitarian features of the government were no longer needed. A secret police force, for instance, has no reason to exist if there are no dangerous traitors who need to be found. Thus the struggle, or “civil war” against internal enemies, must be institutionalized and must continue indefinitely. In the Stalinist USSR, the repressive apparatus was eventually turned against members of the Communist Party itself in the Great Purge and the show trials that accompanied it. Nazism, by contrast, had a much shorter lifespan in power, and Nazi terror generally maintained an outward focus, with the extermination of the Jews always given top priority. The Nazis did not turn inward towards purging their own party except in a limited way on two occasions (the Night of the Long Knives and the aftermath of the 20 July plot). \n\nThe peak of totalitarian terror was reached with the Nazi concentration camps. These ranged from labor camps to extermination camps, and they are described by Friedrich and Brzezinski as aiming to “eliminate all actual, potential, and imagined enemies of the regime.” As the field of Holocaust studies was still in its early stages at the time of their writing, they do not describe the conditions in detail, but do refer to the camps as involving “extreme viciousness.” They also compare these camps with the Soviet Gulag system, and highlight the use of concentration camps as a method of punishment and execution by Nazi and Stalinist regimes alike. However, unlike Hannah Arendt, who held that the Gulag camps served no economic purpose, Friedrich and Brzezinski argue that they provided an important source of cheap labor for the Stalinist economy.\n\nThe comparative study of Nazism and Stalinism was carried further by other groups of scholars, such as Moshe Lewin and Ian Kershaw together with their collaborators. Writing after the dissolution of the USSR, Lewin and Kershaw take a longer historical perspective and regard Nazism and Stalinism not so much as examples of a new type of society (like Arendt, Friedrich and Brzezinski did), but more as historical “anomalies” – unusual deviations from the typical path of development that most industrial societies are expected to follow. Therefore, the task of comparing Nazism and Stalinism is, to them, a task of explaining why Germany and Russia (along with other countries) deviated from the historical norm. At the outset, Lewin and Kershaw identify similarities between the historical situations in Germany and Russia prior to the First World War and during that war. Both countries were ruled by authoritarian monarchies, who were under pressure to make concessions to popular demands. Both countries had “powerful bureaucracies and strong military traditions.” Both had “powerful landowning classes,” while also being in the process of rapid industrialization and modernization. And both countries had expansionist foreign policies with a particular interest in Central and Eastern Europe. Lewin and Kershaw do not claim that these factors made Stalinism or Nazism inevitable, but rather that they help to explain why the Stalinist and Nazi regimes developed similar features.\n\nIan Kershaw admitted that Stalinism and Nazism are comparable in “the nature and extent of their inhumanity,” but noted that the two regimes were different in a number of aspects Lewin and Kershaw question the usefulness of grouping the Stalinist and Nazi regimes together under a “totalitarian” category, saying that it remains an open question whether the similarities between them are greater or smaller than the differences. In particular, they criticize what they see as the ideologically-motivated attempt to determine which regime killed more people, saying that apologists of each regime are trying to defend their side by claiming the other was responsible for more deaths.\n\nLewin and Kershaw place the cult of personality at the center of their comparison of Nazism and Stalinism, writing that both regimes “represented a new genre of political system centred upon the artificial construct of a leadership cult – the ‘heroic myth’ of the ‘great leader’, no longer a king or emperor but a ‘man of the people.” With regard to Stalinism, they emphasize its bureaucratic character, and its “merging of the most modern with the most archaic traits” by combining modern technology and the latest methods of administration and propaganda with the ancient practice of arbitrary rule by a single man. They compare this with the Prussian military tradition in Germany, which had been called “bureaucratic absolutism” in the eighteenth century, and which played a significant role in the organization of the Nazi state in the twentieth century.\n\nKershaw agrees with Mommsen that there was a fundamental difference between Nazism and Stalinism regarding the importance of the leader. Stalinism had an absolute leader, but he was not essential. He could be replaced by another. Nazism, on the other hand, was a “classic charismatic leadership movement,” defined entirely by its leader. Stalinism had an ideology which existed independently of Stalin. But for Nazism, “Hitler \"was\" ideological orthodoxy” – Nazi ideals were by definition whatever Hitler said they were. In Stalinism, the bureaucratic apparatus was the foundation of the system, while in Nazism, the person of the leader was the foundation.\n\nMoshe Lewin also focuses on the comparison between the personality cults of Hitler and Stalin, and their respective roles in Nazi Germany and the Soviet Union. He refers to them as the “Hitler myth” and the “Stalin myth,” and argues that they served different functions within their two regimes. The function of the “Hitler myth” was to legitimize Nazi rule. The function of the “Stalin myth” was to legitimize not Soviet rule itself, but Stalin’s leadership within the Party. Stalin’s personality cult existed precisely because Stalin knew that he was replaceable, and feared that he might be replaced, and so needed to bolster his authority as much as possible. While the “Hitler myth” was essential to Nazi Germany, the “Stalin myth” was essential only to Stalin, not to the Soviet Union itself.\n\nTogether with fellow historian Hans Mommsen, Lewin argues that the Stalinist and Nazi regimes featured an “intrinsic structural contradiction” which led to “inherent self-destructiveness”: they depended on a highly organized state bureaucracy which was trying to set up complex rules and procedures for every aspect of life, yet this bureaucracy was under the complete personal control of a despot who made policy decisions as he saw fit, routinely changing his mind on major issues, without any regard for the rules and institutions which his own bureaucracy had set up. The bureaucracy and the leader needed each other, but also undermined each other with their different priorities. Mommsen sees this as being a much greater problem in Nazi Germany than in Stalin’s Soviet Union, as the Nazis inherited large parts of the traditional German bureaucracy, while the Soviets largely built their own bureaucracy from the ground up. He argues that many of the irrational features of the Nazi regime – such as wasting resources on exterminating undesirable populations instead of using those resources in the war effort – were caused by the dysfunction of the Nazi state rather than by fanatical commitment to Nazi ideology. In accordance with the Führerprinzip, all decisional power in the Nazi state ultimately rested with Hitler. But Hitler often issued only vague and general directives, forcing other Nazi leaders lower down in the hierarchy to guess what precisely the Führer wanted. This confusion produced competition between Nazi officials, as each of them attempted to prove that he was a more dedicated Nazi than his rivals, by engaging in ever more extreme policies. This competition to please Hitler was, according to Mommsen, the real cause of Nazi irrationality. Hitler was aware of it, and deliberately encouraged it out of a “social-darwinist conviction that the best man would ultimately prevail.” Mommsen argues that this represents a structural difference between the regimes of Hitler and Stalin. In spite of its purges, Stalin’s regime was more effective in building a stable bureaucracy, such that it was possible for the system to sustain itself and continue even without Stalin. The Nazi regime, on the other hand, was much more personalized and depended entirely on Hitler, being unable to build any lasting institutions.\n\nKershaw also saw major personal differences between Stalin and Hitler and their respective styles of rule. He describes Stalin as “a committee man, chief oligarch, man of the machine” and a “creature of his party,” who came to power only thanks to his party and his ability to manipulate the levers of power within that party. Hitler, by contrast, came to power based on his charisma and mass appeal, and in the Nazi regime it was the leader that created the party instead of the other way around. According to Kershaw, “Stalin was a highly interventionist dictator, sending a stream of letters and directives determining or interfering with policy,” while Hitler “was a non-interventionist dictator as far as government administration was concerned,” preferring to involve himself in military affairs and plans for conquest rather than the daily routine of government work, and giving only broad verbal instructions to his subordinates regarding civilian affairs, which they were expected to translate into policy. Furthermore, although both regimes featured all-pervasive cults of personality, there was a qualitative difference between those cults. Stalin’s personality cult was “superimposed upon the Marxist-Leninist ideology and Communist Party,” and could be abandoned (or replaced with a personality cult around some other leader) without major changes to the regime. On the other hand, “the ‘Hitler myth’ was structurally indispensable to, in fact the very basis of, and scarcely distinguishable from, the Nazi Movement and its \"Weltanschauung\".” The belief in the person of Adolf Hitler as the unique savior of the German nation was the very foundation of Nazism, to such an extent that Nazism found it impossible to even imagine a successor to Hitler. Thus, in Kershaw’s analysis, Stalinism was a fundamentally bureaucratic system while Nazism was the embodiment of “charismatic authority” as described by Max Weber. Stalinism could exist without its leader. Nazism could not. \n\nThe topic of comparisons between Nazism and Stalinism was also studied in the 1990s and 2000s by historians Henry Rousso, Nicolas Werth and Philippe Burrin.\n\nRousso defends the work of Carl Friedrich by pointing out that Friedrich himself had only said that Stalinism and Nazism were comparable, not that they were identical. Rousso also argues that the popularity of the concept of totalitarianism (the way that large numbers of people have come to routinely refer to certain governments as “totalitarian”) should be seen as evidence that the concept is useful, that it really describes a specific type of government which is different from other dictatorships. At the same time, however, Rousso notes that the concept of totalitarianism is descriptive rather than analytical: the regimes described as totalitarian do not have a common origin and did not arise in similar ways. Nazism is unique among totalitarian regimes in having taken power in “a country endowed with an advanced industrial economy and with a system of political democracy (and an even older political pluralism).” All other examples of totalitarianism (including the Stalinist regime) took power, according to Rousso, “in an agrarian economy, in a poor society without a tradition of political pluralism, not to mention democracy, and where diverse forms of tyranny had traditionally prevailed.” He sees this as a weakness of the concept of totalitarianism, because it merely describes the similarities between Stalinism and Nazism without dealing with the very different ways they came to power. On the other hand, Rousso agrees with Hannah Arendt that “totalitarian regimes constitute something new in regard to classical tyranny, authoritarian regimes, or other forms of ancient and medieval dictatorships,” and he says that the main strength of the concept of totalitarianism is the way it highlights this inherent novelty of the regimes involved.\n\nNicolas Werth and Philippe Burrin have worked together on comparative assessments of Stalinism and Nazism, with Werth covering the Stalinist regime and Burrin covering Nazi Germany. One of the topics they have studied is the question of how much power the dictator really held in the two regimes. Werth identifies two main historiographical approaches in the study of the Stalinist regime: Those who emphasize the power and control exercised by Joseph Stalin himself, attributing most of the actions of the Soviet government to deliberate plans and decisions made by him, and those who argue that Stalin had no pre-determined course of action in mind, that he was reacting to events as they unfolded, and that the Soviet bureaucracy had its own agenda which often differed from Stalin’s wishes. Werth regards these as two mistaken extremes, one making Stalin seem all-powerful, the other making him seem like a weak dictator. But he believes that the competing perspectives are useful in drawing attention to the tension between two different forms of organization in the Stalinist USSR: an “administrative system of command,” bureaucratic and resistant to change but effective in running the Soviet state, and the strategy of “running the country in a crudely despotic way by Stalin and his small cadre of directors.” Thus, Werth agrees with Lewin that there was an inherent conflict between the priorities of the Soviet bureaucracy and Stalin’s accumulation of absolute power in his own hands. According to Werth, it was this unresolved and unstated conflict that led to the Great Purge and to the use of terror by Stalin’s regime against its own party and state cadres.\n\nIn studying similar issues with regard to the Nazi regime, Philippe Burrin draws attention to the debate between the “Intentionalist” and “Functionalist” schools of thought, which dealt with the question of whether the Nazi regime represented an extension of Hitler’s autocratic will, faithfully obeying his wishes, or whether it was an essentially chaotic and uncontrollable system that functioned on its own with little direct input from the Führer. Like Kershaw and Lewin, Burrin says that the relationship between the leader and his party’s ideology was different in Nazism compared to Stalinism: “One can rightly state that Nazism cannot be dissociated from Hitlerism, something that is difficult to affirm for Bolshevism and Stalinism.” Unlike Stalin, who inherited an existing system with an existing ideology and presented himself as the heir to the Leninist political tradition, Hitler created both his movement and its ideology by himself, claiming to be “someone sent by Providence, a Messiah whom the German people had been expecting for centuries, even for two thousand years, as Heinrich Himmler enjoyed saying.” Thus, there could be no real conflict between the Party and the leader in Nazi Germany, because the Nazi Party’s entire reason for existence was to support and follow Hitler. However, there was a potential for division between the leader and the state bureaucracy, due to the way that Nazism came to power – as part of an alliance with traditional conservative elites, industrialists, and the army. Unlike the USSR, Nazi Germany did not build its own state, but rather inherited the state machinery of the previous government. This provided the Nazis with an immediate supply of capable and experienced managers and military commanders, but on the other hand it also meant that the Nazi regime had to rely on the cooperation of people who had not been Nazis prior to Hitler’s rise to power, and whose loyalty was questionable. It was only during the war, when Nazi Germany conquered large territories and had to create Nazi administrations for them, that brand new Nazi bureaucracies were created without any input or participation from traditional German elites. This produced a surprising difference between Nazism and Stalinism: When the Stalinist USSR conquered territory, it created smaller copies of itself and installed them as the governments of the occupied countries. When Nazi Germany conquered territory, on the other hand, it did not attempt to create copies of the German government back home. Instead, it experimented with different power structures and policies, often reflecting a “far more ample Nazification of society than what the balance of power authorized in the Reich.”\n\nAnother major topic investigated by Werth and Burrin was the violence and terror employed by the regimes of Hitler and Stalin. Werth reports that the Stalinist USSR underwent an “extraordinary brutalization of the relations between state and society” for the purpose of rapid modernization and industrialization, to “gain one hundred years in one decade, and to metamorphose the country into a great industrial power.” This transformation was accomplished at the cost of massive violence and a sociopolitical regression into what Werth calls “military-feudal exploitation.” The types of violence employed by the Stalinist regime included loss of civil rights, mass arrests, deportations of entire ethnic groups from one part of the USSR to another, forced labor in the Gulag, mass executions (especially during the Great Terror of 1937-38), and most of all the great famine of 1932-33, known as the Holodomor. All levels of Soviet society were affected by Stalinist repression, from the top to the bottom. At the top, high-ranking members of the Communist Party were arrested and executed under the claim that they had plotted against Stalin (and in some cases they were forced to confess to imaginary crimes in show trials). At the bottom, the peasantry suffered the Holodomor famine (especially in Ukraine), and even outside of the famine years they were faced with very high grain quotas.\n\nWerth identifies four categories of people that became the targets of Stalinist violence in the USSR. He lists them from smallest to largest. The first and smallest group consisted of many of Stalin’s former comrades-in-arms, who had participated in the revolution and were known as “Old Bolsheviks.” They were dangerous to Stalin because they had known him before his rise to power and could expose the many false claims made by his personality cult. The second group consisted of mid-level Communist Party officials, who were subject to mass arrests and executions in the late 1930s, particularly during the Great Purge. Eliminating them served a dual purpose: It helped Stalin to centralize power in the Kremlin (as opposed to regional centers), and it also provided him with “corrupt officials” that he could blame for earlier repressions and unpopular policies. Werth draws parallels between this and the old Tsarist tradition of blaming “bad bureaucrats” – rather than the Tsar – for unpopular government actions. The third group was made up of ordinary citizens from all walks of life who resorted to petty crime in order to provide for themselves in the face of worsening living standards (for example by taking home some wheat from the fields or tools from the factory). This type of petty crime became very widespread, and was often punished as if it were intentional sabotage motivated by political opposition to the USSR. The fourth and largest category consisted of ethnic groups that were subject to deportation, famine, or arbitrary arrests under the suspicion of being collectively disloyal to Stalin or to the Soviet state. This included the Holodomor famine directed at the Ukrainians, the deportation of ethnic groups suspected of pro-German sympathies (such as the Volga Germans, the Crimean Tatars, the Chechens and others), and eventually also persecution of ethnic Jews, especially as Stalin grew increasingly antisemitic near the end of his life.\n\nBurrin’s study of violence carried out by the Nazi regime begins with the observation that “violence is at the heart of Nazism,” and that Nazi violence is “established as a doctrine and exalted in speech.” This marks a point of difference between Nazism and Stalinism, according to Burrin. In Stalinism, there was a gulf between ideology and reality when it came to violence. The Soviet regime continuously denied that it was repressive, proclaimed itself a defender of peace, and sought to conceal all the evidence to the contrary. In Nazism, on the other hand, “doctrine and reality were fused from the start.” Nazism not only practiced violent repression and war, but advocated it in principle as well, considering war to be a positive force in human civilization and openly seeking ”living space” and the domination of the European continent by ethnic Germans.\n\nBurrin identifies three motivations for Nazi violence: political repression, exclusion and social repression, and racial politics. The first of these, political repression, is common in many dictatorships. The Nazis aimed to eliminate their real or imagined political opponents, first in the Reich and later in the occupied territories during the war. Some of these opponents were executed, while others were imprisoned in concentration camps. The first targets of political repression, immediately after Hitler’s rise to power in 1933, were the parties of the Left in general and the Communists in particular. Then, after the mid-1930s, repression was extended to members of the clergy, and later to the conservative opposition as well (especially after the failed attempt to assassinate Hitler in 1944). The death penalty was used on a wide scale, even before the war. During the war, political repression was greatly expanded both inside Germany and especially in the newly occupied territories. Political prisoners in the concentration camps numbered only about 25,000 at the beginning of the war. By January 1945 they had swelled to 714,211 – most of them non-Germans accused of plotting against the Reich.\n\nThe second type of Nazi violence, motivated by exclusion and social repression, was the violence aimed at purging German society of people whose lifestyle was considered incompatible with the social norms of the Nazi regime (even if the people involved were racially pure and able-bodied). Such people were divided into two categories: homosexuals and “asocials.” The “asocials” were only vaguely defined, and included “Gypsies, tramps, beggars, prostitutes, alcoholics, the jobless who refused any employment, and those who left their work frequently or for no reason.”\n\nThe third and final type of Nazi violence, by far the most extensive, was violence motivated by Nazi racial policies. This was aimed both inward, to cleanse the “Aryan race” of “degenerate” elements and life unworthy of life, as well as outward, to seek the extermination of “inferior races”. Germans considered physically or mentally unfit were among the first victims. One of the first laws of the Nazi regime mandated the forced sterilization of people suffering from physical handicaps or who had psychiatric conditions deemed to be hereditary. Later, sterilization was replaced by murder of the mentally ill and of people with severe disabilities, as part of a “euthanasia” program called Aktion T4. Burrin notes that this served no practical political purpose – the people being murdered could not have possibly been political opponents of the regime – so the motivation was purely a matter of racial ideology. The most systematic and by far the most large-scale acts of Nazi violence, however, were directed at “racially inferior” non-German populations. As laid out in \"Generalplan Ost\", the Nazis wished to eliminate most of the Slavic populations of Eastern Europe, partly through deportation and partly through murder, in order to secure land for ethnic German settlement and colonization. But even more urgently, the Nazis wished to exterminate the Jews of Europe, whom they regarded as the implacable racial enemy of the Germans. This culminated in the Holocaust, the Nazi genocide of the Jews. Unlike in the case of all other target populations, the Jews were to be exterminated completely, with no individual exceptions for any reason.\n\nIn \"Beyond Totalitarianism: Stalinism and Nazism Compared\", editors Michael Geyer and Sheila Fitzpatrick disputed the concept of totalitarianism, noting that the term entered political discourse first as a term of self-description by the Italian Fascists and was only later used as a framework to compare Nazi Germany with the Soviet Union. They argued that the totalitarian states were not as monolithic or as ideology-driven as they seemed. Geyer and Fitzpatrick describe Nazi Germany and the Stalinist USSR as “immensely powerful, threatening, and contagious dictatorships” who “shook the world in their antagonism.” Without calling them totalitarian, they identified their common features, including genocide, an all-powerful party, a charismatic leader, and pervasive invasion of privacy. However, they argue that Stalinism and Nazism did not represent a new and unique type of government, but rather that they can be placed in the broader context of the turn to dictatorship in Europe in the interwar period. The reason they appear extraordinary is because they were the “most prominent, most hard-headed, and most violent” of the European dictatorships of the 20th century. They are comparable because of their “shock and awe” and sheer ruthlessness, but underneath superficial similarities they were fundamentally different and that “when it comes to one-on-one comparison, the two societies and regimes may as well have hailed from different worlds.”\n\nAccording to Geyer and Fitzpatrick, the similarities between Nazism and Stalinism stem from the fact that they were both “ideology driven” and sought to subordinate all aspects of life to their respective ideologies. The differences stem from the fact that their ideologies were opposed to each other and regarded each other as enemies. Another major difference is that Stalin created a stable and long-lasting regime, while Nazi Germany had a “short-lived, explosive nature.” Notably, the stable state created by Stalinism was based on an entirely new elite, while Nazism, despite having the support of the traditional elite, failed to achieve stability.\n\nHowever, the two regimes did borrow ideas from one another, especially regarding propaganda techniques (most of all in architecture and cinema), but also in terms of state surveillance and antisemitism. At the same time, they both vigorously denied borrowing anything from each other. While their methods of propaganda were similar, the content was different. For instance, Soviet wartime propaganda revolved around the idea of resisting imperial aggression, while Nazi propaganda was about wars of racial conquest. Geyer and Fitzpatrick also take note of the fact that both Stalinism and Nazism sought to create a New Man, an “entirely modern, illiberal, and self-fashioned personage,” even though they had different visions about what being a “New Man” would mean.\n\nAmong the other authors contributing to the volume edited by Geyer and Fitzpatrick, David Hoffmann and Annette Timm discuss biopolitics and the pro-natalist policies of the Nazi and Stalinist regimes. Both governments were highly concerned over low fertility rates in their respective populations, and applied extensive and intrusive social engineering techniques to increase the number of births. Reproductive policies in the Soviet Union and Nazi Germany were administered through their health care systems—both regimes saw health care as a key pillar to their designs to develop a new society. While the Soviet Union had to design a public health care system from scratch, Nazi Germany built upon the pre-existing public health care system in Germany that had existed since 1883, when Otto von Bismarck's legislation had created the world's first national public health care program. The Nazis centralized the German health care system in order to enforce Nazi ideological components upon it, and replaced existing voluntary and government welfare agencies with new ones that were devoted to racial hygiene and other components of Nazi ideology.\n\nThe Nazi and Stalinist attempt to control family size was not unique, as many other European states practiced eugenics at this time, and the Stalinist and Nazi ideals were vastly different. In fact, they had more in common with third parties than with each other: Nazi Germany’s policies were rather similar to those in Scandinavia at the time, while the USSR’s policies resembled those in Catholic countries.The common point between Nazi and Stalinist practices was the connection of reproduction policies with the ideological goals of the state — \"part of the project of a rational, hypermodern vision for the re-organization of society\". There were nevertheless substantial differences between the two regimes' approaches. Stalin's Soviet Union never officially supported eugenics as the Nazis did—the Soviet government called eugenics a \"fascist science\"—although there were in fact Soviet eugenicists. The two regimes also had different approaches to the relationship between family and paid labor—Nazism promoted the male single-breadwinner family while Stalinism promoted the dual-wage-earner household.\n\nIn another contribution to the same volume, Christian Gerlach and Nicolas Werth discuss the topic of mass violence, and the way that it was used by both Stalinism and Nazism. Both Stalin's Soviet Union and Nazi Germany were violent societies where mass violence was accepted by the state, such as in the Great Terror of 1937 to 1938 in the Soviet Union and the Holocaust in Nazi Germany and its occupied territories in World War II.\n\nBoth the Stalinist Soviet Union and Nazi Germany utilized internment camps led by agents of the state – the NKVD in the Soviet Union and the SS in Nazi Germany. They also both engaged in violence against minorities based on xenophobia – the xenophobic violence of the Nazis was outspoken but rationalized as being against \"asocial\" elements while the xenophobic violence of the Stalinists was disguised as being against \"anti-soviet\", \"counter-revolutionary\" and \"socially harmful\" elements – a term which often targeted diaspora nationalities. The Stalinist Soviet Union established \"special settlements\" where the \"socially harmful\" or \"socially dangerous\" who included ex-convicts, criminals, vagrants, the disenfranchised and \"declassed elements\" were expelled to. These \"special settlements\" were largely in Siberia, the far north, the Urals, or other inhospitable territories. In July 1933, the Soviet Union made a mass arrest of 5000 Romani people effectively on the basis of their ethnicity, who were deported that month to the \"special settlements\" in Western Siberia. In 1935, the Soviet Union arrested 160,000 homeless people and juvenile delinquents and sent many of them to NKVD labor colonies where they did forced labor.\n\nThe Nazi regime was founded upon a racialist view of politics and envisioned the deportation or extermination of the majority of the population of Eastern Europe in order to open up “living space” for ethnic German settlers. This was mainly intended to be carried out after an eventual German victory in the war, but steps had already started being taken while the war was still ongoing. For instance, by the end of 1942, the Nazis had deported 365,000 Poles and Jews from their original homes in western Poland (now German-annexed) and into the General Government. A further 194,000 Poles were internally displaced (not deported to another territory but expelled from their homes). The Nazis had also deported 100,000 persons from Alsace, Lorraine, and Luxembourg, as well as 54,000 Slovenians.\n\nStalinism in practice in the Soviet Union pursued ethnic deportations from the 1930s to the early 1950s, with a total of 3 million Soviet citizens being subjected to ethnic-based resettlement. The first major ethnic deportation took place from December 1932 to January 1933, during which some 60,000 Kuban Cossacks were collectively criminally charged as a whole with association with resistance to socialism and affiliation with Ukrainian nationalism. From 1935 to 1936, the Soviet Union deported Soviet citizens of Polish and German origins living in the western districts of Ukraine, and Soviet citizens of Finnish origins living on the Finland-Soviet Union border. These deportations from 1935 to 1936 affected tens of thousands of families. From September to October 1937, Soviet authorities deported the Korean minority from its Far Eastern region that bordered on Japanese-controlled Korea. Soviet authorities claimed the territory was \"rich soil for the Japanese to till\" – implying a Soviet suspicion that the Koreans could potentially join forces with the Japanese to unite the land with Japanese-held Korea. Over 170,000 Koreans were deported to remote parts of Soviet Central Asia from September to October 1937. These ethnically-based deportations reflected a new trend in Stalinist policy, a \"Soviet xenophobia\" based on ideological grounds that suspected that these people were susceptible to foreign influence, and which was also based on a resurgent Russian nationalism.\n\nAfter Nazi Germany declared war on the Soviet Union in 1941, the Soviet Union initiated another major round of ethnic deportations. The first group targeted were Soviet Germans. Between September 1941 and February 1942, 900,000 people – over 70 percent of the entire Soviet German community – were deported to Kazakhstan and Siberia in mass operations. A second wave of mass deportations took place between November 1943 and May 1944, in which Soviet authorities expelled six ethnic groups (the Balkars, Chechens, Crimean Tatars, Ingush, Karachai, and Kalmyks) that together numbered 900,000. There were also smaller-scale operations involving ethnic cleansing of diaspora minorities during and after World War II, in which tens of thousands of Crimean Bulgarians, Greeks, Iranians, Khemshils, Kurds, and Meskhetian Turks were deported from the Black Sea and Transcaucasian border regions.\n\nTwo ethnic groups that were specifically targeted for persecution by Stalin's Soviet Union were the Chechens and the Ingush. Unlike the other nationalities that could be suspected of connection to foreign states which shared their ethnic background, the Chechens and the Ingush were completely indigenous people of the Soviet Union. Rather than being accused of collaboration with foreign enemies, these two ethnic groups were considered to have cultures which did not fit in with Soviet culture – such as accusing Chechens of being associated with “banditism” – and the authorities claimed that the Soviet Union had to intervene in order to “remake” and “reform” these cultures. In practice this meant heavily armed punitive operations carried out against Chechen “bandits” that failed to achieve forced assimilation, culminating in an ethnic cleansing operation in 1944, which involved the arrests and deportation of over 500,000 Chechens and Ingush from the Caucasus to Central Asia and Kazakhstan. The deportations of the Chechens and Ingush also involved the outright massacre of thousands of people, and severe conditions placed upon the deportees – they were put in unsealed train cars, with little to no food for a four-week journey during which many died from hunger and exhaustion.\n\nThe main difference between Nazi and Stalinist deportations was in their purpose: while Nazi Germany sought ethnic cleansing to allow settlement by Germans into the cleansed territory, Stalin's Soviet Union pursued ethnic cleansing in order to remove minorities from strategically important areas.\n\nOther historians and political scientists have also made comparisons between Nazism and Stalinism as part of their work.\n\nStanley Payne, in his work on fascism, said that although the Nazi Party was ideologically opposed to communism, Adolf Hitler and other Nazi leaders frequently expressed recognition that only in Soviet Russia were their revolutionary and ideological counterparts to be found. Both placed a major emphasis on creating a \"party-army,\" with the regular armed forces controlled by the party. In the case of the Soviet Union this was done through the political commissars, while Nazi Germany introduced a roughly equivalent leadership role for \"National Socialist Guidance Officers\" in 1943.\n\nFrançois Furet, in his work on communism, noted that Hitler personally admired Soviet leader Joseph Stalin, and on numerous occasions publicly praised Stalin for seeking to purify the Communist Party of the Soviet Union of Jewish influences, especially by purging Jewish communists such as Leon Trotsky, Grigory Zinoviev, Lev Kamenev and Karl Radek.\n\nRichard Pipes draws attention to Stalin and his antisemitism in a parallel with Nazi antisemitism. He notes that soon after the 1917 October Revolution, the Soviet Union undertook practices to break up Jewish culture, religion and language. In the fall of 1918, the Soviet Communist Party set up the Jewish section Yevsektsiya, with a stated mission of “destruction of traditional Jewish life, the Zionist movement, and Hebrew culture.” By 1919, the Bolsheviks began to confiscate Jewish properties, Hebrew schools, libraries, books, and synagogues in accordance with newly imposed anti-religious laws, turning their buildings into \"Communist centers, clubs or restaurants.\" After Joseph Stalin rose to power, antisemitism continued to be endemic throughout Russia, although official Soviet policy condemned it. On August 12, 1952, Stalin's personal antisemitism became more visible, as he ordered the execution of the most prominent Yiddish authors in the Soviet Union, in an event known as the \"Night of the Murdered Poets\". Shortly before his death, Stalin also organized the anti-Semitic campaign known as the Doctors' plot.\n\nA number of research institutions are focusing on the analysis of fascism/Nazism and Stalinism/communism, and the comparative approach, including the Hannah Arendt Institute for the Research on Totalitarianism in Germany, the Institute for the Study of Totalitarian Regimes in the Czech Republic and the Institute of National Remembrance in Poland.\n\nIn comparing the deaths caused by both Stalin and Hitler's policies, some historians have asserted that archival evidence released after the collapse of the USSR confirms that Stalin did not kill more people than Hitler. American historian Timothy D. Snyder, for example, after assessing such data, says that while the Nazi regime killed approximately 11 million non-combatants (which rises to above 12 million if \"foreseeable deaths from deportation, hunger, and sentences in concentration camps are included\"), Stalin's deliberately killed about 6 million (rising to 9 million if foreseeable deaths arising from policies are taken into account). Australian historian and archival researcher Stephen G. Wheatcroft posits that \"The Stalinist regime was consequently responsible for about a million purposive killings, and through its criminal neglect and irresponsibility it was probably responsible for the premature deaths of about another two million more victims amongst the repressed population, i.e. in the camps, colonies, prisons, exile, in transit and in the POW camps for Germans. These are clearly much lower figures than those for whom Hitler's regime was responsible.\" Wheatcroft also says that, unlike Hitler, Stalin's \"purposive killings\" fit more closely into the category of \"execution\" than \"murder\", given he thought the accused were indeed guilty of crimes against the state and insisted on documentation, whereas Hitler simply wanted to kill Jews and communists because of who they were, and insisted on no documentation and was indifferent at even a pretence of legality for these actions.\n\nKristen R. Ghodsee, an ethnographer of post-Cold War Eastern Europe, contends that the efforts to institutionalize the \"double genocide thesis\", or the moral equivalence between the Nazi Holocaust (race murder) and the victims of communism (class murder), and in particular the recent push at the beginning of the global financial crisis for commemoration of the latter in Europe, can be seen as the response by economic and political elites to fears of a leftist resurgence in the face of devastated economies and extreme inequalities in both the East and West as the result of neoliberal capitalism. She notes that any discussion of the achievements under communism, including literacy, education, women’s rights, and social security is usually silenced, and any discourse on the subject of communism is focused almost exclusively on Stalin's crimes and the \"double genocide thesis\", an intellectual paradigm summed up as such: \"1) any move towards redistribution and away from a completely free market is seen as communist; 2) anything communist inevitably leads to class murder; and 3) class murder is the moral equivalent of the Holocaust.\" By linking all leftist and socialist ideals to the excesses of Stalinism, Ghodsee concludes, the elites in the West hope to discredit and marginalize all political ideologies that could \"threaten the primacy of private property and free markets.\"\n\nThe comparison of Stalinism and Nazism remains a neglected field of academic study.\n\nThe comparison of Nazism and Stalinism has long provoked political controversy, and it led to the historians' dispute within Germany in the 1980s.\n\nIn the 1920s, the Social Democratic Party of Germany (SPD), under the leadership of Chancellor Hermann Müller, adopted the view that \"red equals brown\", i.e. that the communists and Nazis posed an equal danger to liberal democracy. In 1930, Kurt Schumacher said that the two movements enabled each other. He argued that the Communist Party of Germany, which was staunchly Stalinist, were \"red-painted Nazis.\" This comparison was mirrored by the social fascism theory advanced by the Soviet government and the Comintern (including the Communist Party of Germany), which accused social democracy of enabling fascism and went as far as to call social democrats \"social fascists.\" After the 1939 Molotov–Ribbentrop Pact was announced, \"The New York Times\" published an editorial arguing that \"Hitlerism is brown communism, Stalinism is red fascism.\"\n\nMarxist theories of fascism have seen fascism as a form of reaction to socialism and a feature of capitalism. Several modern historians have tried to pay more attention to the economic, political and ideological differences between these two regimes than to their similarities. \n\nThe 2008 Prague Declaration on European Conscience and Communism, initiated by the Czech government and signed by figures such as Václav Havel, called for \"a common approach regarding crimes of totalitarian regimes, inter alia Communist regimes\" and for\nThe Communist Party of Greece opposes the Prague Declaration and has criticized \"the new escalation of the anti-communist hysteria led by the EU council, the European Commission and the political staff of the bourgeois class in the European Parliament.\" The Communist Party of Britain opined that the Prague Declaration \"is a rehash of the persistent attempts by reactionary historians to equate Soviet Communism and Hitlerite Fascism, echoing the old slanders of British authors George Orwell and Robert Conquest.\"\n\nThe 2008 documentary film \"The Soviet Story\", commissioned by the Union for Europe of the Nations group in the European Parliament, published archival records which listed thousands of German Jews who were arrested in the Soviet Union by the NKVD (People's Commissariat for Internal Affairs) from 1937 to 1941 and handed over to Gestapo or SS officials in Germany. These German Jews had originally sought asylum in the USSR. The documentary film accuses Stalin's regime of being an accomplice in Hitler's Holocaust by arresting these asylum seekers and sending them back to Germany.\n\nSince 2009, the European Union has officially commemorated the European Day of Remembrance for Victims of Stalinism and Nazism, proclaimed by the European Parliament in 2008 and endorsed by the Organization for Security and Co-operation in Europe in 2009, and officially known as the Black Ribbon Day in some countries (including Canada).\n\nThe former President of the European Parliament and Christian Democratic Union member, Hans-Gert Pöttering, argued that \"both totalitarian systems (Stalinism and Nazism) are comparable and terrible.\"\n\nIn some Eastern European countries the denial of both Nazi and Communist crimes has been explicitly outlawed, and Czech foreign minister Karel Schwarzenberg has argued that \"there is a fundamental concern here that totalitarian systems be measured by the same standard.\" However, the European Commission rejected calls for similar EU-wide legislation, due to the lack of consensus among member states.\n\nA statement adopted by Russia's legislature said that comparisons of Nazism and Stalinism are \"blasphemous towards all of the anti-fascist movement veterans, Holocaust victims, concentration camp prisoners and tens of millions of people ... who sacrificed their lives for the sake of the fight against the Nazis' anti-human racial theory.\"\n\nBritish journalist Seumas Milne posits that the impact of the post-Cold War narrative that Stalin and Hitler were twin evils, and therefore Communism is as monstrous as Nazism, \"has been to relativise the unique crimes of Nazism, bury those of colonialism and feed the idea that any attempt at radical social change will always lead to suffering, killing and failure.\"\n\n\n"}
{"id": "32639223", "url": "https://en.wikipedia.org/wiki?curid=32639223", "title": "Comparison of online charity donation services in the United Kingdom", "text": "Comparison of online charity donation services in the United Kingdom\n\nThe page is a comparison of notable online charity donation services in the UK.\n\nThe table below gives examples of the various transaction fees for a £10 donation using each organisation, assuming they claim back the tax for the charity using gift aid. (Charities may also be charged set-up fees and monthly fees as detailed above.)\n\n\n"}
{"id": "327803", "url": "https://en.wikipedia.org/wiki?curid=327803", "title": "Crossword abbreviations", "text": "Crossword abbreviations\n\nCryptic crosswords often use abbreviations to clue individual letters or short fragments of the overall solution. These include:\n\n\nThe abbreviation is not always a short form of the word used in the clue. For example:\n\n\nTaking this one stage further, the clue word can hint at the word or words to be abbreviated rather than giving the word itself. For example:\n\n\nMore obscure clue words of this variety include:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "7931", "url": "https://en.wikipedia.org/wiki?curid=7931", "title": "Dictionary", "text": "Dictionary\n\nA dictionary, sometimes known as a wordbook, is a collection of words in one or more specific languages, often arranged alphabetically (or by radical and stroke for ideographic languages), which may include information on definitions, usage, etymologies, pronunciations, translation, etc. or a book of words in one language with their equivalents in another, sometimes known as a lexicon. It is a lexicographical reference that shows inter-relationships among the data.\n\nA broad distinction is made between general and specialized dictionaries. Specialized dictionaries include words in specialist fields, rather than a complete range of words in the language. Lexical items that describe concepts in specific fields are usually called terms instead of words, although there is no consensus whether lexicology and terminology are two different fields of study. In theory, general dictionaries are supposed to be semasiological, mapping word to definition, while specialized dictionaries are supposed to be onomasiological, first identifying concepts and then establishing the terms used to designate them. In practice, the two approaches are used for both types. There are other types of dictionaries that do not fit neatly into the above distinction, for instance bilingual (translation) dictionaries, dictionaries of synonyms (thesauri), and rhyming dictionaries. The word dictionary (unqualified) is usually understood to refer to a general purpose monolingual dictionary.\n\nThere is also a contrast between \"prescriptive\" or \"descriptive\" dictionaries; the former reflect what is seen as correct use of the language while the latter reflect recorded actual use. Stylistic indications (e.g. \"informal\" or \"vulgar\") in many modern dictionaries are also considered by some to be less than objectively descriptive.\n\nAlthough the first recorded dictionaries date back to Sumerian times (these were bilingual dictionaries), the systematic study of dictionaries as objects of scientific interest themselves is a 20th-century enterprise, called lexicography, and largely initiated by Ladislav Zgusta. The birth of the new discipline was not without controversy, the practical dictionary-makers being sometimes accused by others of \"astonishing\" lack of method and critical-self reflection.\n\nThe oldest known dictionaries were Akkadian Empire cuneiform tablets with bilingual Sumerian–Akkadian wordlists, discovered in Ebla (modern Syria) and dated roughly 2300 BCE. The early 2nd millennium BCE \"Urra=hubullu\" glossary is the canonical Babylonian version of such bilingual Sumerian wordlists. A Chinese dictionary, the c. 3rd century BCE \"Erya\", was the earliest surviving monolingual dictionary; although some sources cite the c. 800 BCE Shizhoupian as a \"dictionary\", modern scholarship considers it a calligraphic compendium of Chinese characters from Zhou dynasty bronzes. Philitas of Cos (fl. 4th century BCE) wrote a pioneering vocabulary \"Disorderly Words\" (Ἄτακτοι γλῶσσαι, \"\") which explained the meanings of rare Homeric and other literary words, words from local dialects, and technical terms. Apollonius the Sophist (fl. 1st century CE) wrote the oldest surviving Homeric lexicon. The first Sanskrit dictionary, the Amarakośa, was written by Amara Sinha c. 4th century CE. Written in verse, it listed around 10,000 words. According to the \"Nihon Shoki\", the first Japanese dictionary was the long-lost 682 CE \"Niina\" glossary of Chinese characters. The oldest existing Japanese dictionary, the c. 835 CE \"Tenrei Banshō Meigi\", was also a glossary of written Chinese. In \"Frahang-i Pahlavig\", Aramaic heterograms are listed together with their translation in Middle Persian language and phonetic transcription in Pazand alphabet. A 9th-century CE Irish dictionary, Sanas Cormaic, contained etymologies and explanations of over 1,400 Irish words. In India around 1320, Amir Khusro compiled the Khaliq-e-bari which mainly dealt with Hindustani and Persian words.\nArabic dictionaries were compiled between the 8th and 14th centuries CE, organizing words in rhyme order (by the last syllable), by alphabetical order of the radicals, or according to the alphabetical order of the first letter (the system used in modern European language dictionaries). The modern system was mainly used in specialist dictionaries, such as those of terms from the Qur'an and hadith, while most general use dictionaries, such as the \"Lisan al-`Arab\" (13th century, still the best-known large-scale dictionary of Arabic) and \"al-Qamus al-Muhit\" (14th century) listed words in the alphabetical order of the radicals. The \"Qamus al-Muhit\" is the first handy dictionary in Arabic, which includes only words and their definitions, eliminating the supporting examples used in such dictionaries as the \"Lisan\" and the \"Oxford English Dictionary\".\nIn medieval Europe, glossaries with equivalents for Latin words in vernacular or simpler Latin were in use (e.g. the Leiden Glossary). The \"Catholicon\" (1287) by Johannes Balbus, a large grammatical work with an alphabetical lexicon, was widely adopted. It served as the basis for several bilingual dictionaries and was one of the earliest books (in 1460) to be printed. In 1502 Ambrogio Calepino's \"Dictionarium\" was published, originally a monolingual Latin dictionary, which over the course of the 16th century was enlarged to become a multilingual glossary. In 1532 Robert Estienne published the \"Thesaurus linguae latinae\" and in 1572 his son Henri Estienne published the \"Thesaurus linguae graecae\", which served up to the 19th century as the basis of Greek lexicography. The first monolingual dictionary written in Europe was the Spanish, written by Sebastián Covarrubias' \"Tesoro de la lengua castellana o española\", published in 1611 in Madrid, Spain. In 1612 the first edition of the \"Vocabolario degli Accademici della Crusca\", for Italian, was published. It served as the model for similar works in French and English. In 1690 in Rotterdam was published, posthumously, the \"Dictionnaire Universel\" by Antoine Furetière for French. In 1694 appeared the first edition of the \"Dictionnaire de l'Académie française\". Between 1712 and 1721 was published the \"Vocabulario portughez e latino\" written by Raphael Bluteau. The Real Academia Española published the first edition of the \"Diccionario de la lengua española\" in 1780, but their \"Diccionario de Autoridades\", which included quotes taken from literary works, was published in 1726. The \"Totius Latinitatis lexicon\" by Egidio Forcellini was firstly published in 1777; it has formed the basis of all similar works that have since been published.\n\nThe first edition of \"A Greek-English Lexicon\" by Henry George Liddell and Robert Scott appeared in 1843; this work remained the basic dictionary of Greek until the end of the 20th century. And in 1858 was published the first volume of the Deutsches Wörterbuch by the Brothers Grimm; the work was completed in 1961. Between 1861 and 1874 was published the \"Dizionario della lingua italiana\" by Niccolò Tommaseo. Between 1862 and 1874 was published the six volumes of \"A magyar nyelv szótára\" (Dictionary of Hungarian Language) by Gergely Czuczor and János Fogarasi. Émile Littré published the Dictionnaire de la langue française between 1863 and 1872. In the same year 1863 appeared the first volume of the \"Woordenboek der Nederlandsche Taal\" which was completed in 1998. Also in 1863 Vladimir Ivanovich Dahl published the \"Explanatory Dictionary of the Living Great Russian Language\". The Duden dictionary dates back to 1880, and is currently the prescriptive source for the spelling of German. The decision to start work on the \"Svenska Akademiens ordbok\" was taken in 1787.\n\nThe earliest dictionaries in the English language were glossaries of French, Spanish or Latin words along with their definitions in English. The word \"dictionary\" was invented by an Englishman called John of Garland in 1220 — he had written a book \"Dictionarius\" to help with Latin \"diction\". An early non-alphabetical list of 8000 English words was the \"Elementarie\", created by Richard Mulcaster in 1582.\n\nThe first purely English alphabetical dictionary was \"A Table Alphabeticall\", written by English schoolteacher Robert Cawdrey in 1604. The only surviving copy is found at the Bodleian Library in Oxford. This dictionary, and the many imitators which followed it, was seen as unreliable and nowhere near definitive. Philip Stanhope, 4th Earl of Chesterfield was still lamenting in 1754, 150 years after Cawdrey's publication, that it is \"a sort of disgrace to our nation, that hitherto we have had no… standard of our language; our dictionaries at present being more properly what our neighbors the Dutch and the Germans call theirs, word-books, than dictionaries in the superior sense of that title.\" \n\nIn 1616, John Bullokar described the history of the dictionary with his \"English Expositor\". \"Glossographia\" by Thomas Blount, published in 1656, contains more than 10,000 words along with their etymologies or histories. Edward Phillips wrote another dictionary in 1658, entitled \"The New World of English Words: Or a General Dictionary\" which boldly plagiarized Blount's work, and the two denounced each other. This created more interest in the dictionaries. John Wilkins' 1668 essay on philosophical language contains a list of 11,500 words with careful distinctions, compiled by William Lloyd. Elisha Coles published his \"English Dictionary\" in 1676.\n\nIt was not until Samuel Johnson's \"A Dictionary of the English Language\" (1755) that a more reliable English dictionary was produced. Many people today mistakenly believe that Johnson wrote the first English dictionary: a testimony to this legacy. By this stage, dictionaries had evolved to contain textual references for most words, and were arranged alphabetically, rather than by topic (a previously popular form of arrangement, which meant all animals would be grouped together, etc.). Johnson's masterwork could be judged as the first to bring all these elements together, creating the first \"modern\" dictionary.\n\nJohnson's dictionary remained the English-language standard for over 150 years, until the Oxford University Press began writing and releasing the \"Oxford English Dictionary\" in short fascicles from 1884 onwards. It took nearly 50 years to complete this huge work, and they finally released the complete \"OED\" in twelve volumes in 1928. It remains the most comprehensive and trusted English language dictionary to this day, with revisions and updates added by a dedicated team every three months. One of the main contributors to this modern dictionary was an ex-army surgeon, William Chester Minor, a convicted murderer who was confined to an asylum for the criminally insane.\n\nIn 1806, American Noah Webster published his first dictionary, \"\". In 1807 Webster began compiling an expanded and fully comprehensive dictionary, \"An American Dictionary of the English Language;\" it took twenty-seven years to complete. To evaluate the etymology of words, Webster learned twenty-six languages, including Old English (Anglo-Saxon), German, Greek, Latin, Italian, Spanish, French, Hebrew, Arabic, and Sanskrit.\n\nWebster completed his dictionary during his year abroad in 1825 in Paris, France, and at the University of Cambridge. His book contained seventy thousand words, of which twelve thousand had never appeared in a published dictionary before. As a spelling reformer, Webster believed that English spelling rules were unnecessarily complex, so his dictionary introduced American English spellings, replacing \"colour\" with \"color\", substituting \"wagon\" for \"waggon\", and printing \"center\" instead of \"centre\". He also added American words, like \"skunk\" and \"squash\", that did not appear in British dictionaries. At the age of seventy, Webster published his dictionary in 1828; it sold 2500 copies. In 1840, the second edition was published in two volumes.\n\nIn a general dictionary, each word may have multiple meanings. Some dictionaries include each separate meaning in the order of most common usage while others list definitions in historical order, with the oldest usage first.\n\nIn many languages, words can appear in many different forms, but only the undeclined or unconjugated form appears as the headword in most dictionaries. Dictionaries are most commonly found in the form of a book, but some newer dictionaries, like StarDict and the \"New Oxford American Dictionary\" are dictionary software running on PDAs or computers. There are also many online dictionaries accessible via the Internet.\n\nAccording to the \"Manual of Specialized Lexicographies\", a specialized dictionary, also referred to as a technical dictionary, is a dictionary that focuses upon a specific subject field. Following the description in \"The Bilingual LSP Dictionary\", lexicographers categorize specialized dictionaries into three types: A multi-field dictionary broadly covers several subject fields (e.g. a business dictionary), a single-field dictionary narrowly covers one particular subject field (e.g. law), and a sub-field dictionary covers a more specialized field (e.g. constitutional law). For example, the 23-language Inter-Active Terminology for Europe is a multi-field dictionary, the American National Biography is a single-field, and the African American National Biography Project is a sub-field dictionary. In terms of the coverage distinction between \"minimizing dictionaries\" and \"maximizing dictionaries\", multi-field dictionaries tend to minimize coverage across subject fields (for instance, \"Oxford Dictionary of World Religions\" and \"Yadgar Dictionary of Computer and Internet Terms\") whereas single-field and sub-field dictionaries tend to maximize coverage within a limited subject field (\"The Oxford Dictionary of English Etymology\").\n\nAnother variant is the glossary, an alphabetical list of defined terms in a specialized field, such as medicine (medical dictionary).\n\nThe simplest dictionary, a defining dictionary, provides a core glossary of the simplest meanings of the simplest concepts. From these, other concepts can be explained and defined, in particular for those who are first learning a language. In English, the commercial defining dictionaries typically include only one or two meanings of under 2000 words. With these, the rest of English, and even the 4000 most common English idioms and metaphors, can be defined.\n\nLexicographers apply two basic philosophies to the defining of words: \"prescriptive\" or \"descriptive\". Noah Webster, intent on forging a distinct identity for the American language, altered spellings and accentuated differences in meaning and pronunciation of some words. This is why American English now uses the spelling \"color\" while the rest of the English-speaking world prefers \"colour\". (Similarly, British English subsequently underwent a few spelling changes that did not affect American English; see further at American and British English spelling differences.)\n\nLarge 20th-century dictionaries such as the \"Oxford English Dictionary\" (OED) and \"Webster's Third\" are descriptive, and attempt to describe the actual use of words. Most dictionaries of English now apply the descriptive method to a word's definition, and then, outside of the definition itself, and information alerting readers to attitudes which may influence their choices on words often considered vulgar, offensive, erroneous, or easily confused. \"Merriam-Webster\" is subtle, only adding italicized notations such as, \"sometimes offensive\" or \"stand\" (nonstandard). \"American Heritage\" goes further, discussing issues separately in numerous \"usage notes.\" \"Encarta\" provides similar notes, but is more prescriptive, offering warnings and admonitions against the use of certain words considered by many to be offensive or illiterate, such as, \"an offensive term for...\" or \"a taboo term meaning...\".\n\nBecause of the widespread use of dictionaries in schools, and their acceptance by many as language authorities, their treatment of the language does affect usage to some degree, with even the most descriptive dictionaries providing conservative continuity. In the long run, however, the meanings of words in English are primarily determined by usage, and the language is being changed and created every day. As Jorge Luis Borges says in the prologue to \"El otro, el mismo\": \"It is often forgotten that (dictionaries) are artificial repositories, put together well after the languages they define. The roots of language are irrational and of a magical nature.\"\n\nSometimes the same dictionary can be descriptive in some domains and prescriptive in others. For example, according to Ghil'ad Zuckermann, the \"Oxford English-Hebrew Dictionary\" is \"at war with itself\": whereas its coverage (lexical items) and glosses (definitions) are descriptive and colloquial, its vocalization is prescriptive. This internal conflict results in absurd sentences such as \"hi taharóg otí kshetiré me asíti lamkhonít\" (she'll tear me apart when she sees what I've done to the car). Whereas \"hi taharóg otí\", literally 'she will kill me', is colloquial, me (a variant of ma 'what') is archaic, resulting in a combination that is unutterable in real life.\n\nA historical dictionary is a specific kind of descriptive dictionary which describes the development of words and senses over time, usually using citations to original source material to support its conclusions.\n\nIn contrast to traditional dictionaries, which are designed to be used by human beings, dictionaries for natural language processing (NLP) are built to be used by computer programs. The final user is a human being but the direct user is a program. Such a dictionary does not need to be able to be printed on paper. The structure of the content is not linear, ordered entry by entry but has the form of a complex network (see Diathesis alternation). Because most of these dictionaries are used to control machine translations or cross-lingual information retrieval (CLIR) the content is usually multilingual and usually of huge size. In order to allow formalized exchange and merging of dictionaries, an ISO standard called Lexical Markup Framework (LMF) has been defined and used among the industrial and academic community.\n\n\nIn many languages, such as the English language, the pronunciation of some words is not consistently apparent from their spelling. In these languages, dictionaries usually provide the pronunciation. For example, the definition for the word \"dictionary\" might be followed by the International Phonetic Alphabet spelling . American English dictionaries often use their ownpronunciation respelling systems with diacritics, for example \"dictionary\" is respelled as \"dĭk′shə-nĕr′ē\" in the American Heritage Dictionary. The IPA is more commonly used within the British Commonwealth countries. Yet others use their own pronunciation respelling systems without diacritics: for example, \"dictionary\" may be respelled as . Some online or electronic dictionaries provide audio recordings of words being spoken.\n\nHistories and descriptions of the dictionaries of other languages on Wikipedia include:\n\n\nThe age of the Internet brought online dictionaries to the desktop and, more recently, to the smart phone. David Skinner in 2013 noted that \"Among the top ten lookups on Merriam-Webster Online at this moment are 'holistic, pragmatic, caveat, esoteric' and 'bourgeois.' Teaching users about words they don’t already know has been, historically, an aim of lexicography, and modern dictionaries do this well.\"\nThere exist a number of websites which operate as online dictionaries, usually with a specialized focus. Some of them have exclusively user driven content, often consisting of neologisms. Some of the more notable examples include:\n\n\n\n"}
{"id": "46842976", "url": "https://en.wikipedia.org/wiki?curid=46842976", "title": "Diels–Kranz numbering", "text": "Diels–Kranz numbering\n\nDiels–Kranz (DK) numbering is the standard system for referencing the works of the ancient Greek pre-Socratic philosophers, based on the collection of quotations from and reports of their work, \"Die Fragmente der Vorsokratiker\" (The Fragments of the Pre-Socratics), by Hermann Alexander Diels. The \"Fragmente\" was first published in 1903, was later revised and expanded three times by Diels, and was finally revised in a fifth edition (1934–7) by Walther Kranz and again in a sixth edition (1952). In Diels-Kranz, each passage, or item, is assigned a number which is used to uniquely identify the ancient personality with which it is concerned, and the type of item given. Diels-Kranz is used in academia to cite pre-Socratic philosophers, and the system also encompasses Sophists and pre-Homeric poets such as Orpheus.\n\nStephanus pagination is the comparable system for referring to Plato, and Bekker numbering is the comparable system for referring to Aristotle.\n\nThe works of the pre-Socratics have not survived extant to the present day. Our knowledge of them exists only through references in the works of later philosophers (known as doxography) in the form of quotations and paraphrases. For example, our knowledge of Thales of Miletus comes largely from the works of Aristotle, who lived centuries after him. Another interesting example of such a source is Hippolytus of Rome, whose polemic \"Refutation of All Heresies\" is a source of many direct quotations of Heraclitus as well as of other philosophers, thereby perpetuating the work of those he was refuting.\n\nThese quotations, paraphrases and other references to pre-Socratic philosophers were collected by Diels and Kranz in their book, which became a standard text in modern pre-Socratic education and scholarship. Because of its influence, Diels-Kranz numbering became the standard way of referencing the material: in literature, conferences, and even in conversation.\n\nThe number corresponding to an item was made up of three parts:\n\n\nWhy, take the case of Thales, Theodorus. While he was studying the stars and looking upwards, he fell into a pit, and a neat, witty Thracian servant girl jeered at him, they say, because he was so eager to know the things in the sky that he could not see what was there before him at his very feet.\n\nThe above text has a DK number of 11A9, since it refers to Thales who is, as mentioned above, chapter 11's subject. The source is \"Theaetetus\" (one of Plato's dialogues), and gives an account of Thales' life, hence it is a \"testimonium\", represented by the letter \"A\". Finally, it is the ninth item in its chapter, giving it the overall number of DK 11A9.\n\nSometimes, the chapter (personality) number may simply be replaced by the name, which can be helpful in cases where the former is the same as the passage number, to avoid ambiguity. For example:\n\nThose who seek for gold dig up much earth and find a little.\n\nRather than \"22B22\" the above may also instead be referred to as \"Heraclitus B22\" as it is a direct transmission of the words of Heraclitus (thus, B) and is the 22nd item in the chapter about Heraclitus (whose chapter number is also 22) in the \"Fragmente\".\n\nThe following table gives the Diels-Kranz numbering of Pre-Socratic philosophers. Note that the numbering scheme presented is that of the fifth edition of \"Die Fragmente der Vorsokratiker\", the first to be revised by Kranz. The fifth edition's numbering is the scheme which has since gained the most traction in modern Pre-Socratic scholarship, and it is the one used consistently throughout this article. It should not be confused with the numberings given in other versions, which changed frequently depending on the particular edition of the \"Fragmente\".\n\nMost entries (78) are concerned with a single, named individual, while the remaining minority of entries (12) have more complex context. Of these latter, eight (10, 19, 39, 46, 53-56) are each concerned with groups of named personalties, who typically have a clear relationship of some kind to justify their association in each entry. Two entries (58, 79) are devoted not to individuals, but to schools of thought (Pythagoreanism and Sophism), and the last two (89, 90) reproduce contemporaneous anonymous texts. Although \"the Seven Sages of Greece\" implies a clearly defined set of seven people, historical disagreement renders intractable the problem of exactly who they were, with multiple sources suggesting several different candidates. If one takes the Seven Sages as a group of seven and includes the later Iamblichus, Diels-Kranz encompasses 106 named personalities and two anonymous authors. The chapter on Sophism is concerned with the named sophists who take up most of the rest of the scheme, and \nper Freeman with regard to the chapter on Pythagoreanism, a catalogue due to Iamblichus lists 218 named men and 17 named women as Pythagoreans, along with other probable, anonymous adherents.\n\nIn several cases, the personalities listed are so obscure that they are merely mentioned by name in other sources, commonly with hints as to their geographical and philosophical associations, and without even surviving \"paraphrases\" of any of their ideas, or what they might have written. That is, these more obscure personalities survive in the historical record only as names cited by others, and so came to be included in Diels-Kranz for the sake of scholarly completeness.\n\n\n"}
{"id": "1902180", "url": "https://en.wikipedia.org/wiki?curid=1902180", "title": "Digital reference", "text": "Digital reference\n\nDigital reference (or virtual reference) is a service by which a library reference service is conducted online, and the reference transaction is a computer-mediated communication. It is the remote, NextNextcomputer-mediated delivery of reference information provided by library professionals to users who cannot access or do not want face-to-face communication. Virtual reference service is most often an extension of a library's existing reference service program. The word \"reference\" in this context refers to the task of providing assistance to library users in finding information, answering questions, and otherwise fulfilling users’ information needs. Reference work often but not always involves using reference works, such as dictionaries, encyclopedias, etc. This form of reference work expands reference services from the physical reference desk to a \"virtual\" reference desk where the patron could be writing from home, work or a variety of other locations.\n\nThe terminology surrounding virtual reference services may involve multiple terms used for the same definition. The preferred term for remotely delivered, computer-mediated reference services is \"virtual reference\", with the secondary non-preferred term \"digital reference\" having gone out of use in recent years. \"Chat reference\" is often used interchangeably with virtual reference, although it represents only one aspect of virtual reference. Virtual reference includes the use of both synchronous (i.e., IM, videoconferencing) and asynchronous communication (i.e., texting and email). Here, \"synchronous virtual reference\" refers to any real-time computer-mediated communication between patron and information professional. Asynchronous virtual reference is all computer-mediated communication that is sent and received at different times.\n\nThe earliest digital reference services were launched in the mid-1980s, primarily by academic and medical libraries, and provided by e-mail. These early-adopter libraries launched digital reference services for two main reasons: to extend the hours that questions could be submitted to the reference desk, and to explore the potential of campus-wide networks, which at that time was a new technology.\n\nWith the advent of the graphical World Wide Web, libraries quickly adopted webforms for question submission. Since then, the percentage of questions submitted to services via webforms has outstripped the percentage submitted via email.\n\nIn the early- to mid-1990s, digital reference services began to appear that were not affiliated with any library. These digital reference services are often referred to as \"AskA\" services. Examples of AskA services are the Internet Public Library, Ask Dr. Math, and Ask Joan of Art.\n\nProviding remote-based services for patrons has been a steady practice of libraries over the years. For example, before the widespread use of chat software, reference questions were often answered via phone, fax, email and audio conferencing. Email is the oldest type of virtual reference service used by libraries. Library services in America and the UK are just now gaining visibility in their use of virtual reference services using chat software. However, a survey in America revealed that by 2001 over 200 libraries were using chat reference services. \nThe rapid global proliferation of information technology (IT) often leaves libraries at a disadvantage in terms of keeping their services current. However, libraries are always striving to understand their user demographics in order to provide the best possible services. Therefore, libraries continue to take notes from current cyberculture and are continually incorporating a diversified range of interactive technologies in their service repertoires. Virtual reference represents only one small part of a larger library mission to meet the needs of a new generation, sometimes referred to as the \"Google Generation\", of users who have grown up with the internet. For instance, virtual reference may be used in conjunction with embedded Web 2.0 (online social media such as Facebook, YouTube, blogs, del.icio.us, Flickr, etc.) applications in a library's suite of online services. As technological innovations continue, libraries will be watching to find new, more personalized ways of interacting with remote reference users.\n\nThe range of cost-per-transaction of reference interactions has been found to be large, due to the differences in librarian salaries and infrastructural costs required by reference interviews.\n\nWebforms are created for digital reference services in order to help the patron be more productive in asking their question. This document helps the librarian locate exactly what the patron is asking for. Creation of webforms requires design consideration. Because webforms substitute for the reference interview, receiving as much information as possible from the patron is a key function.\n\nAspects commonly found within webforms:\n\n\nSeveral applications exist for providing chat-based reference. Some of these applications are: QuestionPoint, OmniReference, Tutor.com, LibraryH3lp, AspiringKidz.com, and Vienova.com. These applications bear a resemblance to commercial help desk applications. These applications possess functionality such as: chat, co-browsing of webpages, webpage and document pushing, customization of pre-scripted messages, storage of chat transcripts, and statistical reporting.\n\nInstant messaging (IM) services are used by some libraries as a low-cost means of offering chat-based reference, since most IM services are free. Utilizing IM for reference services allows a patron to contact the library from any location via the internet. This service is like the traditional reference interview because it is a live interaction between the patron and the librarian. On the other side the reference interview is different because the conversation does not float away but instead is in print on the screen for the librarian to review if needed to better understand the patron. IM reference services may be for the use of in-house patrons as well as patrons unable to go to the library. If library computers support IM chat programs, patrons may IM from within the library to avoid losing their use of a computer or avoid making embarrassing questions public.\n\nSuccessful IM reference services will:\n\nAt times, IM becomes challenging because of lack of non-verbal cues such as eye contact, and the perceived time pressure. Moreover, formulating the question online without the give and take of nonverbal cues and face to face conversation presents an added obstacle. In addition, to provide effective reference service through IM, it is important to meet higher level of information literacy standards. These standards include evaluating the information and its source, synthesizing the information to create new ideas or products, and understanding the societal, legal, and economic issues surrounding its use.\n\nThe article Live, Digital Reference Marketplace by Buff Hirko contains a comparison of the features of applications for chat-based reference.\n\nSee the entries in the Library Success Wiki's Online Reference Section, including software recommended for web-based chat reference, IM reference, SMS (text messaging) reference, and other types like digital audio or video reference.\n\nVirtual service software programs offered by libraries are often unique, and tailored to the individual library's needs. However, each program may have several distinct features. A knowledge base is a chunk of information that users can access independently. An example of this is a serialized listing of frequently asked questions (FAQ) that a user can read and use at his or her leisure.\n\nOnline chat, or instant messaging (IM) has become a very popular Web-based feature. Instant messaging is a real time conversation that utilizes typed text instead of language. Users may feel a sense of satisfaction with the use of this tool because of their personalized interaction with staff.\n\nThe use of electronic mail (email) in responding to reference questions in libraries has been in use for years. Also, in some cases with the IM feature, a question may be asked that cannot be resolved in online chat. In this instance the staff member may document the inquiring patron’s email address and will the user a response.\n\nWith the increase in use of text messaging (Short Message Service or SMS), some libraries are also adopting text messaging in their virtual reference services. Librarians can use mobile phones, text-to-instant messaging or web-based services to respond to reference questions via text messaging.\n\nCo-browsing, or cooperative browsing, is a virtual reference function that involves interactive control of a user’s web browser. This function enables the librarian to see what the patron has on his or her computer screen. Several types of co-browsing have been offered in mobile devices of late; libraries may have software that incorporates dual modes of co-browsing in a variety of formats. For instance, it is possible to browse on a mobile device within and between documents (such as Word), webpages, and images.\n\nVirtual reference services are growing in popularity in the UK with more institutions accepting queries via email, instant messaging and other chat based services. A study of the use of virtual reference within UK academic institutions showed that 25% currently offer a form of virtual reference, with 54% of academic institutions surveyed considering adding this service.\n\nUK public libraries were instrumental in some of the first steps towards UK-wide internet collaboration amongst libraries with the EARL Consortium (Electronic Access to Resources in Libraries) in 1995, in a time where internet access was a rare commodity for both library staff and the public. Resources were collated and lines of communication opened between libraries across the UK, paving the way for services all over the world to follow suit. There are now a number of area-specific reference services across the UK including Ask A Librarian (UK-wide, established in 1997), Ask Cymru (Welsh and English language service), Enquire (Government funded through the People's Network, also UK-wide), and Ask Scotland. Ask Scotland was created by the Scottish Government's advisory body on libraries, SLIC (Scottish Library and Information Council), and funded by the Public Library Quality Improvement Fund (PLQIF) in June 2009. It uses the Online Computer Library Center's QuestionPoint software.\n\nThe definition formulated by the American Library Association's (ALA) 2004 MARS Digital Reference Guidelines Ad Hoc Committee contains three components:\n\n\nIn January 2011 QuestionPoint and the American Library Association were in talks about offering a National Ask A Librarian service across the whole United States of America. At present the Ask services in the US are run at a local level.\n\nIn Europe some countries offer services in both their own national language and in English. European countries include: Finland, the Netherlands (in Dutch only), Denmark, and France.\n\nOther countries which offer virtual reference services include: Australia, New Zealand, Canada, and the state of Colorado in the United States.\n\nA collaboration between UK and Australian library services, entitled Chasing the Sun, has been initiated using QuestionPoint software so that an all-hours digital reference chat service can be offered. Targeted at health libraries where reference queries from health professionals could occur at any time of the day or night due to medical emergencies, the collaboration between the two countries means that someone will be on hand to field the query at any time. Although the UK libraries involved are currently based in England the programme may expand to other countries and health services if successful.\n\n\n\n\nThe following provide software and technology infrastructure for digital/virtual reference.\n\n\n\n\n\n"}
{"id": "1054566", "url": "https://en.wikipedia.org/wiki?curid=1054566", "title": "Ditloid", "text": "Ditloid\n\nA ditloid is a type of word puzzle, in which a phrase, quotation, date, or fact must be deduced from the numbers and abbreviated letters in the clue. Common words such as 'the', 'in', 'a', 'an', 'of', 'to', etc. are not normally abbreviated. The name 'ditloid' was given by the \"Daily Express\" newspaper, originating from the clue: 1 = DitLoID ≡ \"1 Day in the Life of Ivan Denisovich\".\n\nWill Shortz originated the current form of this puzzle and first published it in the May–June 1981 issue of \"Games\" magazine, calling it the Equation Analysis Test. In its annual 1981 issue of \"What's hot and what's not,\" \"Us\" magazine named the Equation Analysis Test in the \"what's hot\" category – the only nonperson so recognized. Shortz reports:\nSome anonymous person had retyped the puzzle from \"Games\" (word for word, except for my byline),\nphotocopied it, and passed it along. This page was then rephotocopied ad infinitum, like a chain letter,\nand circulated around the country. \"Games\" readers who hadn't seen the original even started sending\nit back to \"Games\" as something the magazine ought to consider publishing!\nShortz based the puzzle on the Formula Analysis Test - Revised Form published in Morgan Worthy's 1975 book \"AHA! A Puzzle Approach to Creative Thinking\" (Chicago: Nelson Hall). Worthy's equations were in a different format, for example:\n\nWorthy gives the source of his inspiration and speculates about the perennial popularity\nof this puzzle:\nI got the idea for linguistic equations from graffiti someone had\nwritten in the form of an obscene formula on a restroom wall at the\nUniversity of Florida. When the answer suddenly came to me, I realized\nthe format was a good one for eliciting the \"aha effect\". After that I\nused such items as exercise material when teaching workshops on\ncreative thinking.\nMy guess is that one reason a person enjoys linguistic equations is\nthat the answer hits him or her all at once rather than being solved in\nan incremental fashion. It is similar to what happens when we suddenly\nsee an embedded figure pop into focus; the satisfaction is visceral\nrather than just intellectual. My experience was that people often had\nthe answer to an item come to them when they were not consciously\nthinking about the puzzles, but relaxed, such as in the shower or about\nto fall asleep.\nAnother factor is that with well-written items, success does not hinge\non obscure information. Ideally, a person should never have to feel, \"I\ncould never have gotten that one no matter how long I worked on it.\"\nThere is something ego enhancing about knowing you have the answer\ninside and just need to find it.\n"}
{"id": "33487458", "url": "https://en.wikipedia.org/wiki?curid=33487458", "title": "Guide to information sources", "text": "Guide to information sources\n\nA Guide to information sources (or a bibliographic guide, a literature guide, a guide to reference materials, a subject gateway, etc.) is a kind of metabibliography. Ideally it is not just a listing of bibliographies, reference works and other information sources, but more like a textbook introducing users to the information sources in a given field (in general).\n\nSuch guides may have many different forms: Comprehensive or highly selective, printed or electronic sources, annoteted listings or written chapters etc.\n\nOften used as curriculum tools for bibliographic instruction, the guides help library users find materials or help those unfamiliar with a discipline understand the key sources.\n\nAby, Stephen H., Nalen, James & Fielding, Lori (2005). Sociology; a guide to reference and information sources. 3rd ed. Westport, Conn.: Libraries Unlimited.\n\nAdams, Stephen R. (2005). \"Information Sources in Patents\"; 2nd ed. (Guides to Information Sources). München: K. G. Saur \n\nBlewett, Daniel K (2008). American military history; a guide to reference and information sources. 2nd ed. Westport, CT : Libraries Unlimited.\n\nJacoby, JoAnn & Kibbee, Josephine Z. (2007). Cultural anthropology; a guide to reference and information sources. 2nd ed. Westport, Conn.: Libraries Unlimited.\n\nSchmidt, Diane & Bell, George H. (2003). Guide to reference and information sources in the zoological sciences. Westport, Conn. : Libraries Unlimited.\n\nO'Hare, Christine (2007). \"Business Information Sources\". London: Library Assn Pub Ltd\n\nOstwald, W (1919). Die chemische Literatur und die Organisation der Wissenschaft. Leipzig : W. Ostwald & C. Drucker. (This is considered the first \"guide to information sources\").\n\nStebbins, Leslie F. (2006). Student guide to research in the digital age; how to locate and evaluate information sources. Westport, Conn.: Libraries Unlimited.\n\nWebb, W. H. et al. (Ed.). (1986). Sources of information in the social sciences. A Guide to the literature. 3. ed. Chicago : American Library Association.\n\nZell, Hans M. (ed.). (2003). The African studies companion; a guide to information sources. 3rd rev. and expanded ed. Glais Bheinn : Hans Zell.\n\n\n"}
{"id": "4491358", "url": "https://en.wikipedia.org/wiki?curid=4491358", "title": "Handbook", "text": "Handbook\n\nA handbook is a type of reference work, or other collection of instructions, that is intended to provide ready reference. The term originally applied to a small or portable book containing information useful for its owner, but the Oxford English Dictionary defines the current sense as \"any book...giving information such as facts on a particular subject, guidance in some art or occupation, instructions for operating a machine, or information for tourists.\" \n\nA handbook is sometimes referred to as a vade mecum (Latin, \"go with me\") or pocket reference. It may also be referred to as an enchiridion.\n\nHandbooks may deal with any topic, and are generally compendiums of information in a particular field or about a particular technique. They are designed to be easily consulted and provide quick answers in a certain area. For example, the MLA Handbook for Writers of Research Papers is a reference for how to cite works in MLA style, among other things. Examples of engineering handbooks include \"Perry's Chemical Engineers' Handbook\", \"Marks Standard Handbook for Mechanical Engineers\", and the \"CRC Handbook of Chemistry and Physics\".\n\n"}
{"id": "10260905", "url": "https://en.wikipedia.org/wiki?curid=10260905", "title": "I (pronoun)", "text": "I (pronoun)\n\nThe pronoun I is the first-person singular nominative case personal pronoun in Modern English. It is used to refer to one's self and is capitalized, although other pronouns, such as \"he\" or \"she\", are not capitalized.\n\nThe grammatical variants of \"I\" are \"me\", \"my\", \"mine\", and \"myself\".\n\nEnglish \"I\" originates from Old English (OE) \"ic\". Its predecessor \"ic\" had in turn originated from the continuation of Proto-Germanic *\"ik\", and \"ek\"; the asterisk denotes an unattested form, \"ek\" was attested in the Elder Futhark inscriptions (in some cases notably showing the variant \"eka\"; see also ek erilaz). Linguists assume \"ik\" to have developed from the unstressed variant of \"ek\". Variants of \"ic\" were used in various English dialects up until the 1600s.\n\nGermanic cognates are: Old Frisian \"ik\", Old Norse \"ek\" (Danish, Norwegian \"jeg\", Swedish \"jag\", Icelandic ég), Old High German \"ih\" (German \"ich\") and Gothic \"ik\" and in Dutch also \"ik\".\n\nThe Proto-Germanic root came, in turn, from the Proto Indo-European language (PIE). The reconstructed PIE pronoun is *\"egō, egóm\", with cognates including\nSanskrit \"aham\", Hittite \"uk\", Latin \"ego\", Greek \"egō\", Old Slavonic \"azъ\" and Alviri-Vidari (an Iranian language) \"az\".\n\nThe oblique forms are formed from a stem \"*me-\" (English \"me\"), the plural from \"*wei-\" (English \"we\"), the oblique plurals from \"*ns-\" (English \"us\") and from Proto-Germanic \"*unseraz\", PIE \"*no-s-ero-\" (\"our, ours\").\n\n\"I\" (and only this form of the pronoun) is the only pronoun that is always capitalized in English. This practice became established in the late 15th century, though lowercase \"i\" was sometimes found as late as the 17th century.\n\nLike the other English personal pronouns \"we\" (\"us\"), \"he\" (\"him\"), \"she\" (\"her\"), and \"they\" (\"them\"), the pronoun \"I\" has several singular case forms.\nThese are: \n\nThere are some situations in which only the nominative form (\"I\") is grammatically correct and others in which only the accusative form (\"me\") is correct. There are also situations in which one form is used in informal style (and was often considered ungrammatical by older prescriptive grammars) and the other form is preferred in formal style.\n\nIn all varieties of standard English, the nominative form \"I\" is used exclusively when it is the whole subject of an \"explicit\" verb, e.g. \nnot \nWith other pronouns, such as \"we\" (strictly speaking when used as a personal determiner), there may be exceptions to this in some varieties of English.\n\nIn all varieties of standard English, the accusative form \"me\" is used exclusively when it is the whole direct or indirect object of a verb or preposition. The accusative \"me\" is also required in a number of constructions such as \"Silly me!\"\n\nIn many situations, both the nominative \"I\" and the accusative \"me\" are encountered.\n\nWhen the pronoun is used as a subjective predicative complement, the nominative \"I\" is sometimes encountered in (very) formal style:\nBut this is often seen as hypercorrect and may be unacceptable, as in:\n\"Me\" is usually preferred as a subjective predicate, especially in informal style:\nThe nominative \"I\" is more common in this role when it is followed by a relative clause:\nthough even here \"me\" is more common in non-formal style:\n\nFollowing \"as\" or \"than\" (without a following explicit verb), the accusative form is common:\nHowever, where it is possible to think of the pronoun as the subject of an implicit verb and \"than\" or \"as\" as a conjunction, the nominative \"I\" is found in formal style:\n\nIn Australian English, British English and Irish English, many speakers have an unstressed form of \"my\" that is identical to \"me\" (see archaic and non-standard forms of English personal pronouns).\n\nThe above applies when the pronoun stands alone as the subject or object.\nIn some varieties English (particularly formal English), those rules also apply in coordinative constructions such as \"you and I\". So the correct form is \n\nIn some varieties of non-standard informal English, the accusative is sometimes used when the pronoun is part of a coordinative \"subject\" construction, as in\nThis is highly stigmatized.\n\nOn the other hand, the use of the nominative \"I\" in coordinative constructions like \"you and I\"where \"me\" would be used in a non-coordinative object is less stigmatized – and in some cases so widespread as to be considered a variety of standard English: \n\n\n\n\n"}
{"id": "190975", "url": "https://en.wikipedia.org/wiki?curid=190975", "title": "Ibid.", "text": "Ibid.\n\nIbid is an abbreviation for the Latin word \"ibīdem\", meaning \"in the same place\", commonly used in an endnote, footnote, bibliography citation, or scholarly reference to refer to the source cited in the preceding note or list item. This is similar to \"īdem\", literally meaning \"the same\", abbreviated \"Id.\", which is commonly used in legal citation.\n\nIbid. may also be used in the Harvard (name-date) system for in-text references where there has been a close previous citation from the same source material. The previous reference should be immediately visible, e.g. within the same paragraph or page. Some academic publishers now prefer that \"ibid.\" not be italicized, as it is a commonly found term.\n\nSince ibid. is an abbreviation where the last two letters of the word are omitted, it takes a full stop (period) in both British and American usage.\n\nReference 2 is the same as reference 1: E. Vijh, \"Latin for Dummies\" on page 23, whereas reference 3 refers to the same work but at a different location, namely page 29. Intervening entries require a reference to the original citation in the form Ibid. <citation #>, as in reference 5.\n\n\n\n"}
{"id": "17878314", "url": "https://en.wikipedia.org/wiki?curid=17878314", "title": "Information source", "text": "Information source\n\nAn information source is a person, thing, or place from which information comes, arises, or is obtained. Information souces can be known as primary or secondary. That source might then inform a person about something or provide knowledge about it. Information sources are divided into separate distinct categories, primary, secondary, tertiary, and so on.\n\n"}
{"id": "15293025", "url": "https://en.wikipedia.org/wiki?curid=15293025", "title": "Informationsdienst Wissenschaft", "text": "Informationsdienst Wissenschaft\n\nInformationsdienst Wissenschaft e.V. or idw (The Science Information Service) operates an Internet platform, which bundles the press reports and dates of important events from about 1,000 scientific institutions, including universities, technical colleges, governmental and non-governmental research institutes and institutes to support research or scientific administration. idw (a registered charitable society) also operates an expert broker, the idw expert finder, which is exclusively for journalists. This makes idw one of the most comprehensive sources of science news in the German-speaking area. Foreign journalists and institutions (mostly European) now use idw as well. \n\nThe two main objectives of idw are:\n\nThe information in idw can be accessed free of charge - either directly on idw’s www pages, or by using an individually configurable RSS feed or as an e-mail subscriber. Any user can request the information covering the topics and regions which interest him. All idw services can be used free of cost - the current news ticker, the science calendar, research in the archive (which contains more than 350,000 press releases), and the list of institutions linked to idw. idw also provides journalists with instruments for contacting experts, and maintains a database with science photos.\nThe members' press offices have various possibilities of communicating with journalists. Membership is only offered to German or foreign institutions which perform research or teaching, or which support science or are active in science in some other way.\n\nThe original idea of idw was to provide experts for journalists. Using the American ProfNet as example, the press officers of Universitaet Bayreuth, the Ruhr University Bochum and the Clausthal University of Technology, in collaboration with Computing Centre of Clausthal University of Technology/TU Clausthal, developed a concept for a German language network, by means of the new media. The concept was technically implemented by the staff of the Computing Centre of the Clausthal University of Technology. A total of nine staff members in Bayreuth, Bochum and Clausthal are responsible for programming, maintaining and developing the idw operating system, for user services and further development of the content.\n\nThe initial phase (1996–1999) was guaranteed by project support from the Federal Ministry for Education and Research (BMBF). The technical development of the idw was supported by the Ministry, together with the Stifterverband fuer die Deutsche Wissenschaft (Donor Association for German Science). idw has been working closely for years with the initiative Wissenschaft im Dialog (Science in Dialogue). idw has been economically independent since 2000 and is financed by contributions from member institutions. It has been organised as a registered charitable society (gemeinnütziger e. V.) since 2002.\n\nidw has developed as a recognised and accepted source for German language science and for science journalism. It has become an instrument for public relations work for scientific institutions. \nAbout 37,000 subscribers (figure for June 2018) receive regular reports from idw, including some 7,900 journalists. About 1,000 institutions publish their press reports and dates of important events via idw.\n\n"}
{"id": "30795401", "url": "https://en.wikipedia.org/wiki?curid=30795401", "title": "Liar paradox in early Islamic tradition", "text": "Liar paradox in early Islamic tradition\n\nMany early Islamic philosophers and logicians discussed the liar paradox. Their work on the subject began in the 10th century and continued to Athīr al-Dīn al-Abharī and Nasir al-Din al-Tusi of the middle 13th century and beyond. Although the Liar paradox has been well known in Greek and Latin traditions, the works of Arabic scholars have only recently been translated into English.\n\nEach group of early Islamic philosophers discussed different problems presented by the paradox. They pioneered unique solutions that were not influenced by Western ideas.\n\nAthīr al-Dīn Mufaḍḍal (b. ʿUmar Abharī, d. 663/1264) was a Persian philosopher, astronomer and mathematician from the city of Abhar in Persia. There is some speculation that his works on the Liar paradox could have been known to Western logicians, and in particular to Thomas Bradwardine.\n\nHe analyzed the Liar sentence as follows:\n\nIn other words, Athīr says that if the Liar sentence is false, which means that the Liar falsely declares that all he says at the moment is false, then the Liar sentence is true; and, if the Liar sentence is true, which means that the Liar truthfully declares that all he says at the moment is false, then the Liar sentence is false. In any case, the Liar sentence is both true and false at the same time, which is a paradox.\n\nAthīr offers the following solution for the paradox:\n\nAccording to the traditional idealization that presumably was used by Athīr, the sentence as an universal proposition is false only, when \"either it has a counter-instance or its subject term is empty\".\n\n\nThe Liar sentence, however, has neither an empty subject nor counter-instance. This fact creates obstacles for Athīr's view, who must show what is unique about the Liar sentence, and how the Liar sentence still could be only true or false in view of the \"true\" and \"false\" conditions set up in the universal proposition's description. Athīr tries to solve the paradox by applying to it the laws of negation of a conjunction and negation of a disjunction.\n\nAhmed Alwishah, who has a Ph.D. in Islamic Philosophy and David Sanson, who has a Ph.D. in Philosophy explain that Athīr actually claims that:\n\n(1) \"It is not the case that, if the Liar Sentence is not both true and false, then it is true.\"\n\nAlwishah and Sanson continue:\n\"The general principle behind (1) is clear enough: the negation of a conjunction does not entail the negation of a conjunct; so from not both true and false you cannot infer not false and so true. Abharī appears to be saying that the Liar rests on an elementary scope fallacy! But, of course, Abharī is not entitled to (1). In some cases, the negation of a conjunction does entail the negation of a conjunct: 'not both P and P' for example, entails 'not P'. As a general rule, the negation of a conjunction entails the negation of each conjunct whenever the conjuncts are logically equivalent, i.e., whenever the one follows from the other and vice verse. So Abharī is entitled to (1) only if he is entitled to assume that ‘The Liar Sentence is true’ and ‘The Liar Sentence is false’ are not logically equivalent.\"\n\nThe Liar sentence is a universal proposition (The Liar says All I say ...), so \"if it is (non–vacuously) false it must have a counter–instance\". But in this case scenario, when the only thing that the liar is saying is the single sentence declaring that what he is saying at the moment is false, the only available counter–instance is the Liar sentence itself. When staging the paradox Abharī said: \"if it is not true, then it is necessary that one of his sentences at this moment is true, as long as he utters something. But, he says nothing at this moment other than this sentence. Thus, this sentence is necessarily true and false\" So the explanation provided by Abharī himself demonstrates that both \"'The Liar Sentence is false' and 'The Liar Sentence is true' are logically equivalent. If they are logically equivalent, then, contrary to (1), the negation of the conjunction does entail the negation of each conjunct. Abharī’s 'solution; therefore fails.\"\n\nNaṣīr al-Dīn al-Ṭūsī was a Persian polymath and prolific writer: an astronomer, biologist, chemist, mathematician, philosopher, physician, physicist, scientist, theologian and Marja Taqleed. He adhered to the Ismaili, and subsequently Twelver Shī‘ah Islamic belief systems. The Arab scholar Ibn Khaldun (1332–1406) considered Tusi to be the greatest of the later Persian scholars.\n\nṬūsī's work on the paradox begins with a discussion of the paradox and the solution offered by Abharī, with which Ṭūsī disagrees. As Alwishah and Sanson point out \"Ṭūsī argues that whatever fancy thing (conjunction, conditional) Abharī wants to identify as the truth condition for the Liar Sentence, it will not matter, because pace Abharī, we can generate the paradox without inferring, from the negation of a complex truth condition, the negation of one of its parts. We can argue directly that its being false entails the negation of its being false, and so entails its being true.\"\n\nṬūsī then prepares a stage for his own solution of the Liar paradox, writing that:\nHe does not see a reason that could prevent a declarative sentence to declare something about another declarative sentence.\n\nWith an example of two declarative sentences, (D1) \"It is false\" and (D2) \"Zayd is sitting\", Ṭūsī explains how one declarative sentence (D1) can declare another declarative sentence (D2) to be false: \"It is false that Zayd is sitting\". There is no paradox in the above two declarative sentences because they have different subjects. To generate a paradox a declarative sentence must declare something about itself. If (D1) falsely declares itself to be not (D1) then this false declaration referencing to itself as being \"false\" creates a paradox.\n\nṬūsī writes: \n\nThe above conclusions are very important to the history of Liar Paradox. Alwishah and Sanson point out: \"It is hard to overemphasize how remarkable this passage is. The contemporary reader will be familiar with the idea that the Liar Paradox is a paradox of selfreference. But Ṭūsī is, as far as we know, the first person to express this idea. This passage has no precedent in any tradition. Ṭūsī has performed three remarkable feats in short order. First, his Liar Sentence is singular: its subject is itself, and it declares itself to be false. Gone, then, is the choice between universal or particular Liar Sentence, and the associated problem of adding further assumptions to generate a genuine paradox. Second, he has characterized the paradox as one of self-reference. Third, he has identified a key assumption that might be responsible for generating the entire problem: the assumption that a declarative sentence, by its nature, can declare-something-about anything.\"\n\nRecognizing that, if a declarative sentence that declares itself being false, is false, this does not necessitate it being true. Ṭūsī says that it would be absurd to say that this declarative sentence is true only because it is not false. Ṭūsī writes:\n\nṬūsī then interprets the definitions of \"true\" and \"false\", in an attempt to prove that those definitions should not be taken into consideration when dealing with a declarative sentence that declares itself, as its own subject, to be false.\n\nAl-Baghdādī's definition of \"truth\" and \"falsity\" says that: \"truth is an agreement with the subject, and falsity is the opposite of that\". Ṭūsī argues that this definition cannot be applied to a declarative sentence that declares its own subject to be false because then there are at least two opposite parts that are in disagreement with each other. The same subject cannot be in disagreement with itself. Therefore a self–referenced declarative sentence that declares itself to be false is neither false nor true, and truth/falsity definitions are not applicable to those sentences.\n\nṬūsī stopped short from offering a solution for the Liar sentences discussed by Āmidī \"All that I say at this moment is false\". This sentence presents a different case scenario because it can be interpreted as declaring something about itself, and something about another sentence. The solution for this paradox is absent from Ṭūsī's papers.\n"}
{"id": "51388883", "url": "https://en.wikipedia.org/wiki?curid=51388883", "title": "Life spans of home appliances", "text": "Life spans of home appliances\n\nThis page lists the average life spans of home appliances (major and small).\n\n"}
{"id": "6908619", "url": "https://en.wikipedia.org/wiki?curid=6908619", "title": "Museum of Comparative Zoology", "text": "Museum of Comparative Zoology\n\nThe Museum of Comparative Zoology, full name \"The Louis Agassiz Museum of Comparative Zoology\", often abbreviated simply to \"MCZ\", is the zoology museum located on the grounds of Harvard University in Cambridge, Massachusetts. It is one of three natural history research museums at Harvard whose public face is the Harvard Museum of Natural History. Harvard MCZ's collections consist of some 21 million specimens, of which several thousand are on rotating display at the public museum. The current director of the Museum of Comparative Zoology is James Hanken, the Louis Agassiz Professor of Zoology at Harvard University.\n\nMany of the exhibits in the public museum have not only zoological interest but also historical significance. Past exhibits have included a fossil sand dollar which was found by Charles Darwin in 1834, Captain Cook's mamo, and two pheasants that once belonged to George Washington, now on loan to Mount Vernon in Virginia.\n\nThe Harvard Museum of Natural History is physically connected to the Peabody Museum of Archaeology and Ethnology; for visitors, one admission ticket grants access to both museums. The research collections of the Museum of Comparative Zoology are not open to the public.\n\nThe Museum of Comparative Zoology was founded in 1859 through the efforts of zoologist Louis Agassiz, and the museum used to be referred to as \"The Agassiz\" after its founder. Agassiz designed the collection to illustrate the variety and comparative relationships of animal life.\n\nThe Radcliffe Zoological Laboratory was created in 1894 when Radcliffe College rented a space on the fifth floor of the Museum of Comparative Zoology at Harvard University to convert into a women's laboratory. Prior to this acquisition, Radcliffe science laboratories were taught using inadequate facilities, converting spaces such as bathrooms in old houses into physics laboratories, which Harvard professors often refused to teach in.The laboratory space was converted from an office or storage closet, and was sandwiched between other invertebrate storage rooms on the fifth floor.\n\nThe museum comprises twelve departments: Biological Oceanography, Entomology, Herpetology, Ichthyology, Invertebrate Paleontology, Invertebrate Zoology, Mammalogy, Marine invertebrates, Malacology, Ornithology, Population Genetics, and Vertebrate Paleontology. The Ernst Mayr Library and its archives join in supporting the work of the museum. The Ernst Mayr Library is a founding member of the Biodiversity Heritage Library.\n\nThe museum publishes two journals: the \"Bulletin of the Museum of Comparative Zoology at Harvard College\", first published in 1869, and \"Breviora\", first published in 1956.\n\nIn contrast to numerous more modern museums, the Harvard Museum of Natural History has many hundreds of stuffed animals on display, from the collections of the Museum of Comparative Zoology. Notable exhibits include whale skeletons, the largest turtle shell ever found (eight feet long), \"the Harvard mastodon\", a long \"Kronosaurus\" skeleton, the skeleton of a dodo, and a coelacanth preserved in fluid. The two-story Great Mammal Hall was renovated in 2009 in celebration of the 150th anniversary of founding of the Museum of Comparative Zoology.\n\nNew and changing exhibitions in the Harvard Museum of Natural History include \"Evolution\" (2008); \"The Language of Color\" (2008 to 2013); \"Arthropods: Creatures that Rule\" (2006); \"New England Forests\" (2011); and \"Mollusks: Shelled Masters of the Marine Realm\" (2012).\n\n"}
{"id": "1091767", "url": "https://en.wikipedia.org/wiki?curid=1091767", "title": "Non-well-founded set theory", "text": "Non-well-founded set theory\n\nNon-well-founded set theories are variants of axiomatic set theory that allow sets to contain themselves and otherwise violate the rule of well-foundedness. In non-well-founded set theories, the foundation axiom of ZFC is replaced by axioms implying its negation.\n\nThe study of non-well-founded sets was initiated by Dmitry Mirimanoff in a series of papers between 1917 and 1920, in which he formulated the distinction between well-founded and non-well-founded sets; he did not regard well-foundedness as an axiom. Although a number of axiomatic systems of non-well-founded sets were proposed afterwards, they did not find much in the way of applications until Peter Aczel’s hyperset theory in 1988.\n\nThe theory of non-well-founded sets has been applied in the logical modelling of non-terminating computational processes in computer science (process algebra and final semantics), linguistics and natural language semantics (situation theory), philosophy (work on the Liar Paradox), and in a different setting, non-standard analysis.\n\nIn 1917, Dmitry Mirimanoff introduced the concept of well-foundedness of a set:\n\nIn ZFC, there is no infinite descending ∈-sequence by the axiom of regularity. In fact, the axiom of regularity is often called the \"foundation axiom\" since it can be proved within ZFC (that is, ZFC without the axiom of regularity) that well-foundedness implies regularity. In variants of ZFC without the axiom of regularity, the possibility of non-well-founded sets with set-like ∈-chains arises. For example, a set \"A\" such that \"A\" ∈ \"A\" is non-well-founded.\n\nAlthough Mirimanoff also introduced a notion of isomorphism between possibly non-well-founded sets, he considered neither an axiom of foundation nor of anti-foundation. In 1926, Paul Finsler introduced the first axiom that allowed non-well-founded sets. After Zermelo adopted Foundation into his own system in 1930 (from previous work of von Neumann 1925–1929) interest in non-well-founded sets waned for decades. An early non-well-founded set theory was Willard Van Orman Quine’s New Foundations, although it is not merely ZF with a replacement for Foundation.\n\nSeveral proofs of the independence of Foundation from the rest of ZF were published in 1950s particularly by Paul Bernays (1954), following an announcement of the result in earlier paper of his from 1941, and by Ernst Specker who gave a different proof in his Habilitationsschrift of 1951, proof which was published in 1957. Then in 1957 Rieger's theorem was published, which gave a general method for such proof to be carried out, rekindling some interest in non-well-founded axiomatic systems. The next axiom proposal came in a 1960 congress talk of Dana Scott (never published as a paper), proposing an alternative axiom now called SAFA. Another axiom proposed in the late 1960s was Maurice Boffa's axiom of superuniversality, described by Aczel as the highpoint of research of its decade. Boffa's idea was to make foundation fail as badly as it can (or rather, as extensionality permits): Boffa's axiom implies that every extensional set-like relation is isomorphic to the elementhood predicate on a transitive class.\n\nA more recent approach to non-well-founded set theory, pioneered by M. Forti and F. Honsell in the 1980s, borrows from computer science the concept of a bisimulation. Bisimilar sets are considered indistinguishable and thus equal, which leads to a strengthening of the axiom of extensionality. In this context, axioms contradicting the axiom of regularity are known as anti-foundation axioms, and a set that is not necessarily well-founded is called a hyperset.\n\nFour mutually independent anti-foundation axioms are well-known, sometimes abbreviated by the first letter in the following list:\nThey essentially correspond to four different notions of equality for non-well-founded sets. The first of these, AFA, is based on accessible pointed graphs (apg) and states that two hypersets are equal if and only if they can be pictured by the same apg. Within this framework, it can be shown that the so-called Quine atom, formally defined by Q={Q}, exists and is unique.\n\nEach of the axioms given above extends the universe of the previous, so that: V ⊆ A ⊆ S ⊆ F ⊆ B. In the Boffa universe, the distinct Quine atoms form a proper class.\n\nIt is worth emphasizing that hyperset theory is an extension of classical set theory rather than a replacement: the well-founded sets within a hyperset domain conform to classical set theory.\n\nAczel’s hypersets were extensively used by Jon Barwise and John Etchemendy in their 1987 book \"The Liar\", on the liar's paradox; The book is also good introduction to the topic of non-well-founded sets.\n\nBoffa’s superuniversality axiom has found application as a basis for axiomatic nonstandard analysis.\n\n\n\n"}
{"id": "2673834", "url": "https://en.wikipedia.org/wiki?curid=2673834", "title": "Numeronym", "text": "Numeronym\n\nA numeronym is a number-based word.\n\nMost commonly, a numeronym is a word where a number is used to form an abbreviation (albeit not an acronym or an initialism). Pronouncing the letters and numbers may sound similar to the full word: \"K9\" for \"canine\" (phonetically: \"kay\" + \"nine\").\n\nAlternatively, the letters between the first and last are replaced with a number representing the number of letters omitted, such as \"i18n\" for \"internationalization\". Sometimes the last letter is also counted and omitted. These word shortenings are sometimes called \"alphanumeric acronyms\", \"alphanumeric abbreviations\", or \"numerical contractions\".\n\nAccording to Tex Texin, the first numeronym of this kind was \"S12n\", the electronic mail account name given to Digital Equipment Corporation (DEC) employee Jan Scherpenhuizen by a system administrator because his surname was too long to be an account name. By 1985, colleagues who found Jan's name unpronounceable often referred to him verbally as \"S12n\" (\"ess-twelve-en\"). The use of such numeronyms became part of DEC corporate culture.\n\nA number may also denote how many times the character before or after it is repeated. This is typically used to represent a name or phrase in which several consecutive words start with the same letter, as in W3 (World Wide Web) or W3C (World Wide Web Consortium).\n\nSome numeronyms are composed entirely of numbers, such as \"212\" for \"New Yorker\", \"4-1-1\" for \"information\", \"9-1-1\" for \"help\", and \"101\" for \"basic introduction to a subject\". Words of this type have existed for decades, including those in 10-code, which has been in use since before World War II.\n\nChapter or title numbers of some jurisdictions' statutes have become numeronyms, for example 5150 and 187 from California's penal code. Largely because the production of many American movies and television programs are based in California, usage of these terms has spread beyond its original location and user population.\n\nThe concept of incorporating numbers into words can also be found in Leet-speak, where numbers are frequently substituted for orthographically similar letters (e.g. \"H4CK3D\" for \"HACKED\").\n\nAnne H. Soukhanov, editor of the new \"Microsoft Encarta College Dictionary\", gives the original meaning of the term as \"a telephone number that spells a word or a name\" on a telephone dial.\n\nWhere words have multiple meanings, abbreviations such as these are almost always used to refer to their computing sense; for example, \"G11n\" for \"globalization\" refers to software preparedness for global distribution, and not the social trend of globalization. In some cases, the use of appropriate case makes it easier to distinguish between letters such as uppercase I/i and lower case L/l.\n\n"}
{"id": "24673687", "url": "https://en.wikipedia.org/wiki?curid=24673687", "title": "Polymath (disambiguation)", "text": "Polymath (disambiguation)\n\nA polymath is a person whose expertise spans a significant number of different subject areas and who has extraordinarily broad and comprehensive knowledge.\n\nPolymath may also refer to:\n\n"}
{"id": "177891", "url": "https://en.wikipedia.org/wiki?curid=177891", "title": "Primary source", "text": "Primary source\n\nIn the study of history as an academic discipline, a primary source (also called an original source) is an artifact, document, diary, manuscript, autobiography, recording, or any other source of information that was created at the time under study. It serves as an original source of information about the topic. Similar definitions can be used in library science, and other areas of scholarship, although different fields have somewhat different definitions. In journalism, a primary source can be a person with direct knowledge of a situation, or a document written by such a person.\n\nPrimary sources are distinguished from secondary sources, which cite, comment on, or build upon primary sources. Generally, accounts written after the fact with the benefit (and possible distortions) of hindsight are secondary. A secondary source may also be a primary source depending on how it is used. For example, a memoir would be considered a primary source in research concerning its author or about his or her friends characterized within it, but the same memoir would be a secondary source if it were used to examine the culture in which its author lived. \"Primary\" and \"secondary\" should be understood as relative terms, with sources categorized according to specific historical contexts and what is being studied.\n\nIn scholarly writing, an important objective of classifying sources is to determine their independence and reliability. In contexts such as historical writing, it is almost always advisable to use primary sources and that \"if none are available, it is only with great caution that [the author] may proceed to make use of secondary sources.\" Sreedharan believes that primary sources have the most direct connection to the past and that they \"speak for themselves\" in ways that cannot be captured through the filter of secondary sources.\n\nIn scholarly writing, the objective of classifying sources is to determine the independence and reliability of sources. Though the terms \"primary source\" and \"secondary source\" originated in historiography as a way to trace the history of historical ideas, they have been applied to many other fields. For example, these ideas may be used to trace the history of scientific theories, literary elements and other information that is passed from one author to another.\n\nIn scientific literature, a primary source is the original publication of a scientist's new data, results and theories. In political history, primary sources are documents such as official reports, speeches, pamphlets, posters, or letters by participants, official election returns and eyewitness accounts. In the history of ideas or intellectual history, the main primary sources are books, essays and letters written by intellectuals; these intellectuals may include historians, whose books and essays are therefore considered primary sources for the intellectual historian, though they are secondary sources in their own topical fields. In religious history, the primary sources are religious texts and descriptions of religious ceremonies and rituals.\n\nA study of cultural history could include fictional sources such as novels or plays. In a broader sense primary sources also include artifacts like photographs, newsreels, coins, paintings or buildings created at the time. Historians may also take archaeological artifacts and oral reports and interviews into consideration. Written sources may be divided into three types.\n\n\nIn historiography, when the study of history is subject to historical scrutiny, a secondary source becomes a primary source. For a biography of a historian, that historian's publications would be primary sources. Documentary films can be considered a secondary source or primary source, depending on how much the filmmaker modifies the original sources.\n\nThe Lafayette College Library, provides a synopsis of primary sources in several areas of study:\n\"The definition of a primary source varies depending upon the academic discipline and the context in which it is used.<br>\n\nAlthough many primary sources remain in private hands, others are located in archives, libraries, museums, historical societies, and special collections. These can be public or private. Some are affiliated with universities and colleges, while others are government entities. Materials relating to one area might be spread over a large number of different institutions. These can be distant from the original source of the document. For example, the Huntington Library in California houses a large number of documents from the United Kingdom.\n\nIn the US, digital copies of primary sources can be retrieved from a number of places. The Library of Congress maintains several digital collections where they can be retrieved. Some examples are American Memory and Chronicling America. The National Archives and Records Administration also has digital collections in Digital Vaults. The Digital Public Library of America searches across the digitized primary source collections of many libraries, archives, and museums. The Internet Archive also has primary source materials in many formats.\n\nIn the UK, the National Archives provides a consolidated search of its own catalogue and a wide variety of other archives listed on the Access to Archives index. Digital copies of various classes of documents at the National Archives (including wills) are available from DocumentsOnline. Most of the available documents relate to England and Wales. Some digital copies of primary sources are available from the National Archives of Scotland. Many County Record Offices collections are included in Access to Archives, while others have their own on-line catalogues. Many County Record Offices will supply digital copies of documents.\n\nIn other regions, Europeana has digitized materials from across Europe while the World Digital Library and Flickr Commons have items from all over the world. Trove has primary sources from Australia.\n\nMost primary source materials are not digitized and may only be represented online with a record or finding aid. Both digitized and not digitized materials can be found through catalogs such as WorldCat, the Library of Congress catalog, the National Archives catalog, and so on.\n\nHistory as an academic discipline is based on primary sources, as evaluated by the community of scholars, who report their findings in books, articles and papers. Arthur Marwick says \"Primary sources are absolutely fundamental to history.\" Ideally, a historian will use all available primary sources that were created by the people involved at the time being studied. In practice some sources have been destroyed, while others are not available for research. Perhaps the only eyewitness reports of an event may be memoirs, autobiographies, or oral interviews taken years later. Sometimes the only evidence relating to an event or person in the distant past was written or copied decades or centuries later. Manuscripts that are sources for classical texts can be copies of documents, or fragments of copies of documents. This is a common problem in classical studies, where sometimes only a summary of a book or letter has survived. Potential difficulties with primary sources have the result that history is usually taught in schools using secondary sources.\n\nHistorians studying the modern period with the intention of publishing an academic article prefer to go back to available primary sources and to seek new (in other words, forgotten or lost) ones. Primary sources, whether accurate or not, offer new input into historical questions and most modern history revolves around heavy use of archives and special collections for the purpose of finding useful primary sources. A work on history is not likely to be taken seriously as scholarship if it only cites secondary sources, as it does not indicate that original research has been done.\n\nHowever, primary sources – particularly those from before the 20th century – may have hidden challenges. \"Primary sources, in fact, are usually fragmentary, ambiguous and very difficult to analyse and interpret.\" Obsolete meanings of familiar words and social context are among the traps that await the newcomer to historical studies. For this reason, the interpretation of primary texts is typically taught as part of an advanced college or postgraduate history course, although advanced self-study or informal training is also possible.\n\nThe following questions are asked about primary sources:\n\nIn many fields and contexts, such as historical writing, it is almost always advisable to use primary sources if possible, and \"if none are available, it is only with great caution that [the author] may proceed to make use of secondary sources.\" In addition, primary sources avoid the problem inherent in secondary sources in which each new author may distort and put a new spin on the findings of prior cited authors.\n\nHowever, a primary source is not necessarily more of an authority or better than a secondary source. There can be bias and tacit unconscious views which twist historical information.\n\nParticipants and eyewitnesses may misunderstand events or distort their reports, deliberately or not, to enhance their own image or importance. Such effects can increase over time, as people create a narrative that may not be accurate. For any source, primary or secondary, it is important for the researcher to evaluate the amount and direction of bias. As an example, a government report may be an accurate and unbiased description of events, but it may be censored or altered for propaganda or cover-up purposes. The facts can be distorted to present the opposing sides in a negative light. Barristers are taught that evidence in a court case may be truthful but may still be distorted to support or oppose the position of one of the parties.\n\nMany sources can be considered either primary or secondary, depending on the context in which they are examined. Moreover, the distinction between \"primary\" and \"secondary\" sources is subjective and contextual, so that precise definitions are difficult to make. A book review, when it contains the opinion of the reviewer about the book rather than a summary of the book, becomes a primary source.\n\nIf a historical text discusses old documents to derive a new historical conclusion, it is considered to be a primary source for the new conclusion. Examples in which a source can be both primary and secondary include an obituary or a survey of several volumes of a journal counting the frequency of articles on a certain topic.\n\nWhether a source is regarded as primary or secondary in a given context may change, depending upon the present state of knowledge within the field. For example, if a document refers to the contents of a previous but undiscovered letter, that document may be considered \"primary\", since it is the closest known thing to an original source; but if the letter is later found, it may then be considered \"secondary\"\n\nIn some instances, the reason for identifying a text as the \"primary source\" may devolve from the fact that no copy of the original source material exists, or that it is the oldest extant source for the information cited.\n\nHistorians must occasionally contend with forged documents that purport to be primary sources. These forgeries have usually been constructed with a fraudulent purpose, such as promulgating legal rights, supporting false pedigrees, or promoting particular interpretations of historic events. The investigation of documents to determine their authenticity is called diplomatics.\n\nFor centuries, Popes used the forged Donation of Constantine to bolster the Papacy's secular power. Among the earliest forgeries are false Anglo-Saxon charters, a number of 11th- and 12th-century forgeries produced by monasteries and abbeys to support a claim to land where the original document had been lost or never existed. One particularly unusual forgery of a primary source was perpetrated by Sir Edward Dering, who placed false monumental brasses in a parish church. In 1986, Hugh Trevor-Roper \"authenticated\" the Hitler Diaries, which were later proved to be forgeries. Recently, forged documents have been placed within the UK National Archives in the hope of establishing a false provenance. However, historians dealing with recent centuries rarely encounter forgeries of any importance.\n\n</div>\n\n\n\n"}
{"id": "25407", "url": "https://en.wikipedia.org/wiki?curid=25407", "title": "Recursion", "text": "Recursion\n\nRecursion occurs when a thing is defined in terms of itself or of its type. Recursion is used in a variety of disciplines ranging from linguistics to logic. The most common application of recursion is in mathematics and computer science, where a function being defined is applied within its own definition. While this apparently defines an infinite number of instances (function values), it is often done in such a way that no loop or infinite chain of references can occur.\n\nIn mathematics and computer science, a class of objects or methods exhibit recursive behavior when they can be defined by two properties:\n\nFor example, the following is a recursive definition of a person's ancestors:\n\nThe Fibonacci sequence is a classic example of recursion:\n\nformula_1\n\nformula_2\n\nformula_3\n\nMany mathematical axioms are based upon recursive rules. For example, the formal definition of the natural numbers by the Peano axioms can be described as: \"0 is a natural number, and each natural number has a successor, which is also a natural number.\" By this base case and recursive rule, one can generate the set of all natural numbers.\n\nRecursively defined mathematical objects include functions, sets, and especially fractals.\n\nThere are various more tongue-in-cheek \"definitions\" of recursion; see recursive humor.\n\nRecursion is the process a procedure goes through when one of the steps of the procedure involves invoking the procedure itself. A procedure that goes through recursion is said to be 'recursive'.\n\nTo understand recursion, one must recognize the distinction between a procedure and the running of a procedure. A procedure is a set of steps based on a set of rules. The running of a procedure involves actually following the rules and performing the steps. An analogy: a procedure is like a written recipe; running a procedure is like actually preparing the meal.\n\nRecursion is related to, but not the same as, a reference within the specification of a procedure to the execution of some other procedure. For instance, a recipe might refer to cooking vegetables, which is another procedure that in turn requires heating water, and so forth. However, a recursive procedure is where (at least) one of its steps calls for a new instance of the very same procedure, like a sourdough recipe calling for some dough left over from the last time the same recipe was made. This immediately creates the possibility of an endless loop; recursion can only be properly used in a definition if the step in question is skipped in certain cases so that the procedure can complete, like a sourdough recipe that also tells you how to get some starter dough in case you've never made it before. Even if properly defined, a recursive procedure is not easy for humans to perform, as it requires distinguishing the new from the old (partially executed) invocation of the procedure; this requires some administration of how far various simultaneous instances of the procedures have progressed. For this reason recursive definitions are very rare in everyday situations. An example could be the following procedure to find a way through a maze. Proceed forward until reaching either an exit or a branching point (a dead end is considered a branching point with 0 branches). If the point reached is an exit, terminate. Otherwise try each branch in turn, using the procedure recursively; if every trial fails by reaching only dead ends, return on the path that led to this branching point and report failure. Whether this actually defines a terminating procedure depends on the nature of the maze: it must not allow loops. In any case, executing the procedure requires carefully recording all currently explored branching points, and which of their branches have already been exhaustively tried.\n\nLinguist Noam Chomsky among many others has argued that the lack of an upper bound on the number of grammatical sentences in a language, and the lack of an upper bound on grammatical sentence length (beyond practical constraints such as the time available to utter one), can be explained as the consequence of recursion in natural language. This can be understood in terms of a recursive definition of a syntactic category, such as a sentence. A sentence can have a structure in which what follows the verb is another sentence: \"Dorothy thinks witches are dangerous\", in which the sentence \"witches are dangerous\" occurs in the larger one. So a sentence can be defined recursively (very roughly) as something with a structure that includes a noun phrase, a verb, and optionally another sentence. This is really just a special case of the mathematical definition of recursion.\n\nThis provides a way of understanding the creativity of language—the unbounded number of grammatical sentences—because it immediately predicts that sentences can be of arbitrary length: \"Dorothy thinks that Toto suspects that Tin Man said that...\". There are many structures apart from sentences that can be defined recursively, and therefore many ways in which a sentence can embed instances of one category inside another. Over the years, languages in general have proved amenable to this kind of analysis.\n\nRecently, however, the generally accepted idea that recursion is an essential property of human language has been challenged by Daniel Everett on the basis of his claims about the Pirahã language. Andrew Nevins, David Pesetsky and Cilene Rodrigues are among many who have argued against this. Literary self-reference can in any case be argued to be different in kind from mathematical or logical recursion.\n\nRecursion plays a crucial role not only in syntax, but also in natural language semantics. The word \"and\", for example, can be construed as a function that can apply to sentence meanings to create new sentences, and likewise for noun phrase meanings, verb phrase meanings, and others. It can also apply to intransitive verbs, transitive verbs, or ditransitive verbs. In order to provide a single denotation for it that is suitably flexible, \"and\" is typically defined so that it can take any of these different types of meanings as arguments. This can be done by defining it for a simple case in which it combines sentences, and then defining the other cases recursively in terms of the simple one. \n\nA recursive grammar is a formal grammar that contains recursive production rules.\n\nRecursion is sometimes used humorously in computer science, programming, philosophy, or mathematics textbooks, generally by giving a circular definition or self-reference, in which the putative recursive step does not get closer to a base case, but instead leads to an infinite regress. It is not unusual for such books to include a joke entry in their glossary along the lines of:\n\nA variation is found on page 269 in the index of some editions of Brian Kernighan and Dennis Ritchie's book \"The C Programming Language\"; the index entry recursively references itself (\"recursion 86, 139, 141, 182, 202, 269\"). The earliest version of this joke was in \"Software Tools\" by Kernighan and Plauger, and also appears in \"The UNIX Programming Environment\" by Kernighan and Pike. It did not appear in the first edition of \"The C Programming Language\".\n\nAnother joke is that \"To understand recursion, you must understand recursion.\" In the English-language version of the Google web search engine, when a search for \"recursion\" is made, the site suggests \"Did you mean: \"recursion\".\" An alternative form is the following, from Andrew Plotkin: \"If you already know what recursion is, just remember the answer. Otherwise, find someone who is standing closer to Douglas Hofstadter than you are; then ask him or her what recursion is.\"\n\nRecursive acronyms can also be examples of recursive humor. PHP, for example, stands for \"PHP Hypertext Preprocessor\", WINE stands for \"WINE Is Not an Emulator.\" and GNU stands for \"GNU's not Unix\".\n\nThe canonical example of a recursively defined set is given by the natural numbers:\n\nAnother interesting example is the set of all \"true reachable\" propositions in an axiomatic system.\n\n\nThis set is called 'true reachable propositions' because in non-constructive approaches to the foundations of mathematics, the set of true propositions may be larger than the set recursively constructed from the axioms and rules of inference. See also Gödel's incompleteness theorems.\n\nFinite subdivision rules are a geometric form of recursion, which can be used to create fractal-like images. A subdivision rule starts with a collection of polygons labelled by finitely many labels, and then each polygon is subdivided into smaller labelled polygons in a way that depends only on the labels of the original polygon. This process can be iterated. The standard `middle thirds' technique for creating the Cantor set is a subdivision rule, as is barycentric subdivision.\n\nA function may be partly defined in terms of itself. A familiar example is the Fibonacci number sequence: \"F\"(\"n\") = \"F\"(\"n\" − 1) + \"F\"(\"n\" − 2). For such a definition to be useful, it must lead to non-recursively defined values, in this case \"F\"(0) = 0 and \"F\"(1) = 1.\n\nA famous recursive function is the Ackermann function, which—unlike the Fibonacci sequence—cannot easily be expressed without recursion.\n\nApplying the standard technique of proof by cases to recursively defined sets or functions, as in the preceding sections, yields structural induction, a powerful generalization of mathematical induction widely used to derive proofs in mathematical logic and computer science.\n\nDynamic programming is an approach to optimization that restates a multiperiod or multistep optimization problem in recursive form. The key result in dynamic programming is the Bellman equation, which writes the value of the optimization problem at an earlier time (or earlier step) in terms of its value at a later time (or later step).\n\nIn set theory, this is a theorem guaranteeing that recursively defined functions exist. Given a set \"X\", an element \"a\" of \"X\" and a function formula_7, the theorem states that there is a unique function formula_8 (where formula_4 denotes the set of natural numbers including zero) such that\nfor any natural number \"n\".\n\nTake two functions formula_8 and formula_13 such that:\n\nwhere \"a\" is an element of \"X\".\n\nIt can be proved by mathematical induction that formula_18 for all natural numbers \"n\":\n\nBy induction, formula_18 for all formula_25.\n\nA common method of simplification is to divide a problem into subproblems of the same type. As a computer programming technique, this is called divide and conquer and is key to the design of many important algorithms. Divide and conquer serves as a top-down approach to problem solving, where problems are solved by solving smaller and smaller instances. A contrary approach is dynamic programming. This approach serves as a bottom-up approach, where problems are solved by solving larger and larger instances, until the desired size is reached.\n\nA classic example of recursion is the definition of the factorial function, given here in C code:\n\nIf not having reached the base case and returning with value every instantiation of the above function creates a new instance of the function, passing to it an input reduced by (), and returns the result of this (recursive) call, multiplied by its own value of , analogously to the mathematical definition of the factorial.\n\nRecursion in computer programming is exemplified when a function is defined in terms of simpler, often smaller versions of itself. The solution to the problem is then devised by combining the solutions obtained from the simpler versions of the problem. One example application of recursion is in parsers for programming languages. The great advantage of recursion is that an infinite set of possible sentences, designs or other data can be defined, parsed or produced by a finite computer program.\n\nRecurrence relations are equations to define one or more sequences recursively. Some specific kinds of recurrence relation can be \"solved\" to obtain a non-recursive definition.\n\nUse of recursion in an algorithm has both advantages and disadvantages. The main advantage is usually simplicity. The main disadvantage is often that the algorithm may require large amounts of memory if the depth of the recursion is very large.\n\nThe Russian Doll or Matryoshka Doll is a physical artistic example of the recursive concept.\n\nRecursion has been used in paintings since Giotto's \"Stefaneschi Triptych\", made in 1320. Its central panel contains the kneeling figure of Cardinal Stefaneschi, holding up the triptych itself as an offering.\n\nM. C. Escher's \"Print Gallery\" (1956) is a print which depicts a distorted city which contains a gallery which recursively contains the picture, and so \"ad infinitum\".\n\n\n"}
{"id": "20110874", "url": "https://en.wikipedia.org/wiki?curid=20110874", "title": "Reference", "text": "Reference\n\nReference is a relation between objects in which one object designates, or acts as a means by which to connect to or link to, another object. The first object in this relation is said to \"refer to\" the second object. It is called a \"name\" for the second object. The second object, the one to which the first object refers, is called the \"referent\" of the first object. A name is usually a phrase or expression, or some other symbolic representation. Its referent may be anything – a material object, a person, an event, an activity, or an abstract concept.\n\nReferences can take on many forms, including: a thought, a sensory perception that is audible (onomatopoeia), visual (text), olfactory, or tactile, emotional state, relationship with other, spacetime coordinate, symbolic or alpha-numeric, a physical object or an energy projection. In some cases, methods are used that intentionally hide the reference from some observers, as in cryptography.\n\nReferences feature in many spheres of human activity and knowledge, and the term adopts shades of meaning particular to the contexts in which it is used. Some of them are described in the sections below.\n\nThe word \"reference\" is derived from Middle English \"referren\", from Middle French \"référer\", from Latin \"referre\", \"to carry back\", formed from the prefix \"re\"- and \"ferre\", \"to bear\". A number of words derive from the same root, including \"refer\", \"referee\", \"referential\", \"referent\", \"referendum\".\n\nThe verb \"refer (to)\" and its derivatives may carry the sense of \"link to\" or \"connect to\", as in the meanings of \"reference\" described in this article. Another sense is \"consult\"; this is reflected in such expressions as reference work, reference desk, job reference, etc.\n\nIn semantics, reference is generally construed as the relationships between nouns or pronouns and objects that are named by them. Hence, the word \"John\" refers to the person John. The word \"it\" refers to some previously specified object. The object referred to is called the \"referent\" of the word. Sometimes the word-object relation is called \"denotation\"; the word denotes the object. The converse relation, the relation from object to word, is called \"exemplification\"; the object exemplifies what the word denotes. In syntactic analysis, if a word refers to a previous word, the previous word is called the \"antecedent\".\n\nGottlob Frege argued that reference cannot be treated as identical with meaning: \"Hesperus\" (an ancient Greek name for the evening star) and \"Phosphorus\" (an ancient Greek name for the morning star) both refer to Venus, but the astronomical fact that '\"Hesperus\" is \"Phosphorus\"' can still be informative, even if the \"meanings\" of \"Hesperus\" and \"Phosphorus\" are already known. This problem led Frege to distinguish between the sense and reference of a word. Some cases seem to be too complicated to be classified within this framework; the acceptance of the notion of secondary reference may be necessary to fill the gap. See also Opaque context.\n\nThe very concept of the linguistic sign is the combination of content and expression, the former of which may refer entities in the world or refer more abstract concepts, e.g. thought.\nCertain parts of speech exist only to express reference, namely anaphora such as pronouns. The subset of reflexives expresses co-reference of two participants in a sentence. These could be the agent (actor) and patient (acted on), as in \"The man washed himself\", the theme and recipient, as in \"I showed Mary to herself\", or various other possible combinations.\n\nIn computer science, references are data types that refer to an object elsewhere in memory and are used to construct a wide variety of data structures, such as linked lists. Generally, a reference is a value that enables a program to directly access the particular data item. Most programming languages support some form of reference. For the specific type of reference used in the C++ language, see reference (C++).\n\nThe notion of reference is also important in relational database theory; see referential integrity.\n\nReferences to many types of printed matter may come in an electronic or machine-readable form. For books, there exists the ISBN and for journal articles, the Digital object identifier (DOI) is gaining relevance. Information on the Internet may be referred to by a Uniform Resource Identifier (URI).\n\nIn terms of mental processing, a self-reference is used in psychology to establish identification with a mental state during self-analysis. This seeks to allow the individual to develop own frames of reference in a greater state of immediate awareness. However, it can also lead to circular reasoning, preventing evolution of thought.\n\nAccording to Perceptual Control Theory (PCT), a reference condition is the state toward which a control system's output tends to alter a controlled quantity. The main proposition is that \"All behavior is oriented all of the time around the control of certain quantities with respect to specific reference conditions.\"\n\nIn academics and scholarship, an author-title-date information in bibliographies and footnotes, specifying complete works of other people. Copying of material by another author without proper citation or without required permissions is plagiarism.\n\nKeeping a diary allows an individual to use references for personal organization, whether or not anyone else understands the systems of reference used. However, scholars have studied methods of reference because of their key role in communication and co-operation between \"different\" people, and also because of misunderstandings that can arise. Modern academic study of reference has been developing since the 19th century.\n\nIn scholarship, a reference may be a citation of a text that has been used in the creation of a piece of work such as an essay, report, or oration. Its primary purpose is to allow people who read such work to examine the author's sources, either for validity or to learn more about the subject. Such items are often listed at the end of an article or book in a section marked \"Bibliography\" or \"References\". A bibliographical section often contains works not cited by the author, but used as background reading or listed as potentially useful to the reader. A reference section contains only those works cited by the author(s) in the main text.\n\nIn patent law, a reference is a document that can be used to show the state of knowledge at a given time and that therefore may make a claimed invention obvious or anticipated. Examples of references are patents of any country, magazine articles, Ph.D. theses that are indexed and thus accessible to those interested in finding information about the subject matter, and to some extent Internet material that is similarly accessible.\n\nIn art, a reference is an item from which a work is based. This may include:\nAnother example of reference is samples of various musical works being incorporated into a new one.\n\n\n"}
{"id": "25727", "url": "https://en.wikipedia.org/wiki?curid=25727", "title": "Reference work", "text": "Reference work\n\nA reference work is a book or periodical (or its electronic equivalent) to which one can refer for information. The information is intended to be found quickly when needed. Reference works are usually \"referred\" to for particular pieces of information, rather than read beginning to end. The writing style used in these works is informative; the authors avoid use of the first person, and emphasize facts. Many reference works are compiled by a team of contributors whose work is coordinated by one or more editors rather than by an individual author. Indices are commonly provided in many types of reference work. Updated editions are usually published as needed, in some cases annually (e.g. \"Whitaker's Almanack\", \"Who's Who\"). Reference works include dictionaries, thesauruses, encyclopedias, almanacs, bibliographies, and catalogs (e.g. catalogs of libraries, museums or the works of individual artists). Many reference works are available in electronic form and can be obtained as application software, CD-ROMs, DVDs, or online through the Internet.\n\nA reference work is useful to its users if they attribute some degree of trust.\n\nIn comparison, a reference book or reference-only book in a library is one that may only be used in the library and may not be borrowed from the library. Many such books are reference works (in the first sense), which are, usually, used briefly or photocopied from, and therefore, do not need to be borrowed. Keeping reference books in the library assures that they will always be available for use on demand. Some reference-only books are too valuable to permit borrowers to take them out. Reference-only items may be shelved in a reference collection located separately from circulating items. Some libraries consist entirely, or to a large extent, of books which may not be borrowed.\n\nAn electronic resource is a piece of information that is stored electronically, which is usually found on a computer, including information that is available on the internet. Libraries offer numerous types of electronic resources, such as subject research guides, indices, electronic books and texts, electronic journals, library catalogs, reference sources, statistical sources, sound recordings, and image databases.\n\n\nSheehy's Guide is less international in its scope than Walford: \"It seems that Walford is a somewhat better balanced work than Winchell, and is certainly much more comprehensive\"--\"American Reference Books Annual\", quoted in Walford, A. J. (1981) \"Walford's Concise Guide to Reference Material\". London: Library Association ; p. 19.\n"}
{"id": "12153317", "url": "https://en.wikipedia.org/wiki?curid=12153317", "title": "Sixpenny Library", "text": "Sixpenny Library\n\nErnest Benn Limited’s Sixpenny Library is a complete series of reference books published in the late 1920s and early 1930s. The library included over one hundred and eighty volumes. The series was edited by William Rose, who solicited current authorities in such areas as history, literature, religion, psychology, science, and economics. Some contributing authors were Hilaire Belloc, Maurice Baring, J.B. Priestley, Sir (later Lord) Robert Baden-Powell, Sir Oliver Lodge, S.V Keeling and Sir Ernest Benn himself. \"The Spectator\", in November 1927, after announcing some the latest additions to \"Messrs Benn's excellent Sixpenny Library\" devoted a further paragraph to his contribution on Trade (both of which are free to read online). Partial lists of the books published in the series can be found here and here.\n\nThe books were praised by critics for their excellence, brevity, and inexpensive price.\n"}
{"id": "4159251", "url": "https://en.wikipedia.org/wiki?curid=4159251", "title": "Source text", "text": "Source text\n\nA source text is a text (sometimes oral) from which information or ideas are derived. In translation, a source text is the original text that is to be translated into another language.\n\nIn historiography, distinctions are commonly made between three kinds of source texts:\n\nPrimary sources are firsthand written evidence of history made at the time of the event by someone who was present. They have been described as those sources closest to the origin of the information or idea under study. These types of sources have been said to provide researchers with \"direct, unmediated information about the object of study.\" Primary sources are sources which, usually, are recorded by someone who participated in, witnessed, or lived through the event. These are also usually authoritative and fundamental documents concerning the subject under consideration. This includes published original accounts, published original works, or published original research. They may contain original research or new information not previously published elsewhere. They have been distinguished from secondary sources, which often cite, comment on, or build upon primary sources. They serve as an original source of information or new ideas about the topic. \"Primary\" and \"secondary\", however, are relative terms, and any given source may be classified as primary or secondary, depending on how it is used. Physical objects can be primary sources.\n\nSecondary sources are written accounts of history based upon the evidence from primary sources. These are sources which, usually, are accounts, works, or research that analyze, assimilate, evaluate, interpret, and/or synthesize primary sources. These are not as authoritative and are supplemental documents concerning the subject under consideration. These documents or people summarize other material, usually primary source material. They are academics, journalists, and other researchers, and the papers and books they produce. This includes published accounts, published works, or published research. For example a history book drawing upon diary and newspaper records. \n\nTertiary sources are compilations based upon primary and secondary sources. These are sources which, on average, do not fall into the above two levels. They consist of generalized research of a specific subject under consideration. Tertiary sources are analyzed, assimilated, evaluated, interpreted, and/or synthesized from secondary sources, also. These are not authoritative and are just supplemental documents concerning the subject under consideration. These are often meant to present known information in a convenient form with no claim to originality. Common examples are encyclopedias and textbooks.\n\nThe distinction between \"primary source\" and \"secondary source\" is standard in historiography, while the distinction between these sources and \"tertiary sources\" is more peripheral, and is more relevant to the scholarly research work than to the published content itself.\n\nBelow are types of sources that most generally, but not absolutely, fall into a certain level. The letters after an item describes \"generally\" the type it is (though this can vary pending the exact source). \"P\" is for Primary sources, \"S\" is for Secondary sources, and \"T\" is for Tertiary sources. (ed., those with \"?\"s are indeterminate.)\nIn translation, a source text (ST) is a text written in a given source language which is to be, or has been, translated into another language. In translation the source text (ST) is transformed into a target text (TT), written in a given target language. According to Jeremy Munday's definition of translation, \"the process of translation between two different written languages involves the changing of an original written text (the source text or ST) in the original verbal language (the source language or SL) into a written text (the target text or TT) in a different verbal language (the target language or TL)\". \n\nTranslation scholars including Eugene Nida and Peter Newmark have represented the different approaches to translation as falling broadly into source-text-oriented or target-text-oriented categories.\n"}
{"id": "40849944", "url": "https://en.wikipedia.org/wiki?curid=40849944", "title": "Sources for the historicity of Jesus", "text": "Sources for the historicity of Jesus\n\nChristian sources, such as the New Testament books in the Christian Bible, include detailed stories about Jesus but scholars differ on the historicity of specific episodes described in the Biblical accounts of Jesus. The only two events subject to \"almost universal assent\" are that Jesus was baptized by John the Baptist and was crucified by the order of the Roman Prefect Pontius Pilate.\n\nNon-Christian sources that are used to study and establish the historicity of Jesus include Jewish sources such as Josephus, and Roman sources such as Tacitus. These sources are compared to Christian sources such as the Pauline Epistles and the Synoptic Gospels. These sources are usually independent of each other (e.g. Jewish sources do not draw upon Roman sources), and similarities and differences between them are used in the authentication process.\n\nIn a review of the state of research, the Jewish scholar Amy-Jill Levine stated that \"no single picture of Jesus has convinced all, or even most scholars\" and that all portraits of Jesus are subject to criticism by some group of scholars.\n\nThe writings of the 1st century Romano-Jewish historian Flavius Josephus include references to Jesus and the origins of Christianity. Josephus' \"Antiquities of the Jews\", written around 93–94 CE, includes two references to Jesus in Books and .\n\nOf the two passages, the James passage in Book 20 is used by scholars to support the existence of Jesus, the \"Testimonium Flavianum\" in Book 18 his crucifixion. Josephus' James passage attests to the existence of Jesus as a historical person and that some of his contemporaries considered him the Messiah. According to Bart Ehrman, Josephus' passage about Jesus was altered by a Christian scribe, including the reference to Jesus as the Messiah.\n\nA textual argument against the authenticity of the James passage is that the use of the term \"Christos\" there seems unusual for Josephus. An argument based on the flow of the text in the document is that, given that the mention of Jesus appears in the \"Antiquities\" before that of the John the Baptist, a Christian interpolator may have inserted it to place Jesus in the text before John. A further argument against the authenticity of the James passage is that it would have read well even without a reference to Jesus.\n\nThe passage deals with the death of \"James the brother of Jesus\" in Jerusalem. Whereas the works of Josephus refer to at least twenty different people with the name Jesus, this passage specifies that this Jesus was the one \"who was called Christ\". Louis Feldman states that this passage, above others, indicates that Josephus did say something about Jesus.\n\nModern scholarship has almost universally acknowledged the authenticity of the reference in of the \"Antiquities\" to \"the brother of Jesus, who was called Christ, whose name was James\", and considers it as having the highest level of authenticity among the references of Josephus to Christianity.\n\nThe \"Testimonium Flavianum\" (meaning the testimony of Flavius [Josephus]) is the name given to the passage found in of the \"Antiquities\" in which Josephus describes the condemnation and crucifixion of Jesus at the hands of the Roman authorities. Scholars have differing opinions on the total or partial authenticity of the reference in the passage to the execution of Jesus by Pontius Pilate. The general scholarly view is that while the \"Testimonium Flavianum\" is most likely not authentic in its entirety, it is broadly agreed upon that it originally consisted of an authentic nucleus with a reference to the execution of Jesus by Pilate which was then subject to Christian interpolation. Although the exact nature and extent of the Christian redaction remains unclear, there is broad consensus as to what the original text of the \"Testimonium\" by Josephus would have looked like.\n\nThe references found in \"Antiquities\" have no parallel texts in the other work by Josephus such as the \"Jewish War\", written twenty years earlier, but some scholars have provided explanations for their absence, such as that the \"Antiquities\" covers a longer time period and that during the twenty-year gap between the writing of the \"Jewish Wars\" (c. 70 CE) and \"Antiquities\" (after 90 CE) Christians had become more important in Rome and were hence given attention in the \"Antiquities\".\n\nA number of variations exist between the statements by Josephus regarding the deaths of James and the New Testament accounts. Scholars generally view these variations as indications that the Josephus passages are not interpolations, because a Christian interpolator would more likely have made them correspond to the Christian traditions. Robert Eisenman provides numerous early Christian sources that confirm the Josephus testament, that James was the brother of Jesus.\n\nThe Roman historian and senator Tacitus referred to Christ, his execution by Pontius Pilate and the existence of early Christians in Rome in his final work, \"Annals\" (c. AD 116), . The relevant passage reads: \"called Christians by the populace. Christus, from whom the name had its origin, suffered the extreme penalty during the reign of Tiberius at the hands of one of our procurators, Pontius Pilatus.\"\n\nScholars generally consider Tacitus's reference to the execution of Jesus by Pontius Pilate to be both authentic, and of historical value as an independent Roman source about early Christianity that is in unison with other historical records. William L. Portier has stated that the consistency in the references by Tacitus, Josephus and the letters to Emperor Trajan by Pliny the Younger reaffirm the validity of all three accounts.\n\nTacitus was a patriotic Roman senator and his writings shows no sympathy towards Christians. Andreas Köstenberger and separately Robert E. Van Voorst state that the tone of the passage towards Christians is far too negative to have been authored by a Christian scribe – a conclusion shared by John P. Meier Robert E. Van Voorst states that \"of all Roman writers, Tacitus gives us the most precise information about Christ\".\n\nJohn Dominic Crossan considers the passage important in establishing that Jesus existed and was crucified, and states: \"That he was crucified is as sure as anything historical can ever be, since both Josephus and Tacitus... agree with the Christian accounts on at least that basic fact.\" Bart D. Ehrman states: \"Tacitus's report confirms what we know from other sources, that Jesus was executed by order of the Roman governor of Judea, Pontius Pilate, sometime during Tiberius's reign.\" Eddy and Boyd state that it is now \"firmly established\" that Tacitus provides a non-Christian confirmation of the crucifixion of Jesus.\n\nAlthough the majority of scholars consider it to be genuine, a few scholars question the authenticity of the passage given that Tacitus was born 25 years after Jesus' death.\n\nSome scholars have debated the historical value of the passage given that Tacitus does not reveal the source of his information. Gerd Theissen and Annette Merz argue that Tacitus at times had drawn on earlier historical works now lost to us, and he may have used official sources from a Roman archive in this case; however, if Tacitus had been copying from an official source, some scholars would expect him to have labeled Pilate correctly as a \"prefect\" rather than a \"procurator\". Theissen and Merz state that Tacitus gives us a description of widespread prejudices about Christianity and a few precise details about \"Christus\" and Christianity, the source of which remains unclear. However, Paul R. Eddy has stated that given his position as a senator Tacitus was also likely to have had access to official Roman documents of the time and did not need other sources.\n\nMichael Martin notes that the authenticity of this passage of the Annals has also been disputed on the grounds that Tacitus would not have used the word “messiah” in an authentic Roman document.\n\nWeaver notes that Tacitus spoke of the persecution of Christians, but no other Christian author wrote of this persecution for a hundred years.\n\nHotema notes that this passage was not quoted by any Church father up to the 15th century, although the passage would have been very useful to them in their work; and that the passage refers to the Christians in Rome being a multitude, while at that time the Christian congregation in Rome would actually have been very small.\n\nRichard Carrier has put forward the ideas that the 'Christ, the author of this name, was executed by the procurator Pontius Pilate in the reign of Tiberius' line is a Christian interpolation and that Tacitus wrote about Chrestians not Christians.\n\nScholars have also debated the issue of hearsay in the reference by Tacitus. Charles Guignebert argued that \"So long as there is that possibility [that Tacitus is merely echoing what Christians themselves were saying], the passage remains quite worthless\". R. T. France states that the Tacitus passage is at best just Tacitus repeating what he had heard through Christians. However, Paul R. Eddy has stated that as Rome's preeminent historian, Tacitus was generally known for checking his sources and was not in the habit of reporting gossip. Tacitus was a member of the Quindecimviri sacris faciundis, a council of priests whose duty it was to supervise foreign religious cults in Rome, which as Van Voorst points out, makes it reasonable to suppose that he would have acquired knowledge of Christian origins through his work with that body.\n\nMara (son of Sarapion) was a Stoic philosopher from the Roman province of Syria. Sometime between 73 AD and the 3rd century, Mara wrote a letter to his son (also called Sarapion) which may contain an early non-Christian reference to the crucifixion of Jesus.\n\nThe letter refers to the unjust treatment of \"three wise men\": the murder of Socrates, the burning of Pythagoras, and the execution of \"the wise king\" of the Jews. The author explains that in all three cases the wrongdoing resulted in the future punishment of those responsible by God and that when the wise are oppressed, not only does their wisdom triumph in the end, but God punishes their oppressors.\n\nThe letter includes no Christian themes and the author is presumed to be a pagan. Some scholars see the reference to the execution of the \"wise king\" of the Jews as an early non-Christian reference to Jesus. Criteria that support the non-Christian origin of the letter include the observation that \"king of the Jews\" was not a Christian title, and that the letter's premise that Jesus lives on based on the wisdom of his teachings is in contrast to the Christian concept that Jesus continues to live through his resurrection.\n\nScholars such as Robert Van Voorst see little doubt that the reference to the execution of the \"king of the Jews\" is about the death of Jesus. Others such as Craig A. Evans see less value in the letter, given its uncertain date, and the possible ambiguity in the reference.\n\nThe Roman historian Suetonius (c. 69 – after 122 CE) made references to early Christians and their leader in his work \"Lives of the Twelve Caesars\" (written 121 CE). The references appear in and which describe the lives of Roman Emperors Claudius and Nero. The Nero 16 passage refers to the abuses by Nero and mentions how he inflicted punishment on Christians – which is generally dated to around AD 64. This passage shows the clear contempt of Suetonius for Christians - the same contempt expressed by Tacitus and Pliny the younger in their writings, but does not refer to Jesus himself.\n\nThe earlier passage in Claudius, may include a reference to Jesus, but is subject to debate among scholars. In Suetonius refers to the expulsion of Jews by Claudius and states:\n\nThe reference in Claudius 25 involves the agitations in the Jewish community which led to the expulsion of some Jews from Rome by Claudius, and is likely the same event mentioned in the Acts of the Apostles (). Most historians date this expulsion to around AD 49–50. Suetonius refers to the leader of the Christians as \"Chrestus\", a term also used by used by Tacitus, referred in Latin dictionaries as a (amongst other things) version of 'Christus'. However, the wording used by Suetonius implies that Chrestus was alive at the time of the disturbance and was agitating the Jews in Rome. This weakens the historical value of his reference as a whole, and there is no overall scholarly agreement about its value as a reference to Jesus. However, the confusion of Suetonius also points to the lack of Christian interpolation, for a Christian scribe would not have confused the Jews with Christians.\n\nMost scholars assume that in the reference Jesus is meant and that the disturbances mentioned were due to the spread of Christianity in Rome. However, scholars are divided on the value of the Suetonius' reference. Some scholars such as Craig A. Evans, John Meier and Craig S. Keener see it as a likely reference to Jesus. Others such as Stephen Benko and H. Dixon Slingerland see it as having little or no historical value.\n\nMenahem Stern states Suetonius definitely was referring to Jesus; because he would have added \"a certain\" to Chrestus if he had meant some unknown agitator.\n\nThe Babylonian Talmud in a few cases includes possible references to Jesus using the terms \"Yeshu\", \"Yeshu ha-Notzri\", \"ben Stada\", and \"ben Pandera\". Some of these references probably date back to the Tannaitic period (70–200 CE). In some cases, it is not clear if the references are to Jesus, or other people, and scholars continue to debate their historical value, and exactly which references, if any, may be to Jesus.\n\nRobert Van Voorst states that the scarcity of Jewish references to Jesus is not surprising, given that Jesus was not a prominent issue for the Jews during the first century, and after the devastation caused by the Siege of Jerusalem in the year 70, Jewish scholars were focusing on preserving Judaism itself, rather than paying much attention to Christianity.\n\nRobert Eisenman argues that the derivation of Jesus of Nazareth from \"ha-Notzri\" is impossible on etymological grounds, as it would suggest rather \"the Nazirite\" rather than \"the Nazarene\".\n\nVan Voorst states that although the question of who was referred to in various points in the Talmud remains subject to debate among scholars, in the case of \"Sanhedrin 43a\" (generally considered the most important reference to Jesus in rabbinic literature), Jesus can be confirmed as the subject of the passage, not only from the reference itself, but from the context that surrounds it, and there is little doubt that it refers to the death of Jesus of Nazareth. Christopher M. Tuckett states that if it is accepted that death narrative of Sanhedrin 43a refers to Jesus of Nazareth then it provides evidence of Jesus' existence and execution.\n\nAndreas Kostenberger states that the passage is a Tannaitic reference to the trial and death of Jesus at Passover and is most likely earlier than other references to Jesus in the Talmud. The passage reflects hostility toward Jesus among the rabbis and includes this text:\n\nIt is taught: On the eve of Passover they hung Yeshu and the crier went forth for forty days beforehand declaring that \"[Yeshu] is going to be stoned for practicing witchcraft, for enticing and leading Israel astray. Anyone who knows something to clear him should come forth and exonerate him.\" But no one had anything exonerating for him and they hung him on the eve of Passover. \n\nPeter Schäfer states that there can be no doubt that the narrative of the execution of Jesus in the Talmud refers to Jesus of Nazareth, but states that the rabbinic literature in question are not Tannaitic but from a later Amoraic period and may have drawn on the Christian gospels, and may have been written as responses to them. Bart Ehrman and separately Mark Allan Powell state that given that the Talmud references are quite late, they can give no historically reliable information about the teachings or actions of Jesus during his life.\n\nAnother reference in early second century Rabbinic literature (Tosefta Hullin II 22) refers to Rabbi Eleazar ben Dama who was bitten by a snake, but was denied healing in the name of Jesus by another Rabbi for it was against the law, and thus died. This passage reflects the attitude of Jesus' early Jewish opponents, i.e. that his miracles were based on evil powers.\n\nEddy and Boyd, who question the value of several of the Talmudic references state that the significance of the Talmud to historical Jesus research is that it never denies the existence of Jesus, but accuses him of sorcery, thus indirectly confirming his existence. R. T. France and separately Edgar V. McKnight state that the divergence of the Talmud statements from the Christian accounts and their negative nature indicate that they are about a person who existed. Craig Blomberg states that the denial of the existence of Jesus was never part of the Jewish tradition, which instead accused him of being a sorcerer and magician, as also reflected in other sources such as Celsus. Andreas Kostenberger states that the overall conclusion that can be drawn from the references in the Talmud is that Jesus was a historical person whose existence was never denied by the Jewish tradition, which instead focused on discrediting him.\n\nPliny the Younger (c. 61 – c. 112), the provincial governor of Pontus and Bithynia, wrote to Emperor Trajan \"c\". 112 concerning how to deal with Christians, who refused to worship the emperor, and instead worshiped \"Christus\". Charles Guignebert, who does not doubt that Jesus of the Gospels lived in Gallilee in the 1st century, nevertheless dismisses this letter as acceptable evidence for a historical Jesus.\n\nThallus, of whom very little is known, and none of whose writings survive, wrote a history allegedly around the middle to late first century CE, to which Eusebius referred. Julius Africanus, writing \"c\" 221, links a reference in the third book of the \"History\" to the period of darkness described in the crucifixion accounts in three of the Gospels . It is not known whether Thallus made any mention to the crucifixion accounts; if he did, it would be the earliest noncanonical reference to a gospel episode, but its usefulness in determining the historicity of Jesus is uncertain. The dating of Thallus is dependent on him writing about an event during the 207th Olympiad (49–52 AD), which means he wrote after that date, not near that date. This depends on the text being corrupt, which would mean Thallus could have been writing after the 217th Olympiad (89–92 AD), or even the 167th Olympiad (112–109 BC). He is first referenced by Theophilus, writing around 180 AD, which means Thallus could have written any time between 109 BC and 180 AD. All we know is Thallus mentioned a solar eclipse, and as solar eclipses are not possible at Passover, that would mean Thallus was not talking about the crucifixion of Jesus at all.\n\nPhlegon of Tralles, AD 80–140, similar to Thallus, Julius Africanus mentions a historian named Phlegon who wrote a chronicle of history around AD 140, where he records:\n“Phlegon records that, in the time of Tiberius Caesar, at full moon, there was a full eclipse of the sun from the sixth to the ninth hour.” (Africanus, Chronography, 18:1) Phlegon is also mentioned by Origen (an early church theologian and scholar, born in Alexandria):\n“Now Phlegon, in the thirteenth or fourteenth book, I think, of his Chronicles, not only ascribed to Jesus a knowledge of future events . . . but also testified that the result corresponded to His predictions.” (Origen Against Celsus, Book 2, Chapter 14)\n“And with regard to the eclipse in the time of Tiberius Caesar, in whose reign Jesus appears to have been crucified, and the great earthquakes which then took place … ” (Origen Against Celsus, Book 2, Chapter 33)\n“Jesus, while alive, was of no assistance to himself, but that he arose after death, and exhibited the marks of his punishment, and showed how his hands had been pierced by nails.” (Origen Against Celsus, Book 2, Chapter 59). However, Eusebius in The Chronicon (written in the 4th century AD) records what Phlegon said verbatim. \"Now, in the fourth year of the 202nd Olympiad [32 AD], a great eclipse of the sun occurred at the sixth hour [noon] that excelled every other before it, turning the day into such darkness of night that the stars could be seen in heaven, and the earth moved in Bithynia, toppling many buildings in the city of Nicaea.\" Phlegon never mentions Jesus or the 3 hour darkness. He also mentions a solar eclipse, which can not occur at Passover. Apart from the year (which may be a corruption), this description fits an earthquake and eclipse that occurred in North West Turkey on November, 29 AD.\n\nCelsus writing late in the second century produced the first full-scale attack on Christianity. Celsus' document has not survived but in the third century Origen replied to it, and what is known of Celsus' writing is through the responses of Origen. According to Origen, Celsus accused Jesus of being a magician and a sorcerer. While the statements of Celsus may be seen as valuable, they have little historical value, given that the wording of the original writings can not be examined.\n\nThe Dead Sea Scrolls are first century or older writings that show the language and customs of some Jews of Jesus' time. Scholars such as Henry Chadwick see the similar uses of languages and viewpoints recorded in the New Testament and the Dead Sea Scrolls as valuable in showing that the New Testament portrays the first century period that it reports and is not a product of a later period. However, the relationship between the Dead Sea scrolls and the historicity of Jesus has been the subject of highly controversial theories, and although new theories continue to appear, there is no overall scholarly agreement about their impact on the historicity of Jesus, despite the usefulness of the scrolls in shedding light on first-century Jewish traditions.\n\nThe following sources are disputed, and of limited historical value, but they are at least proof of Christians existing and being known and talked about in the first and second centuries.\n\nThere is a limestone burial box from the 1st century known as the James Ossuary with the Aramaic inscription, \"James, son of Joseph, brother of Jesus.\" The authenticity of the inscription was challenged by the Israel Antiquities Authority, who filed a complaint with the Israeli police. In 2012, the owner of the ossuary was found not guilty, with the judge ruling that the authenticity of the ossuary inscription had not been proven either way. It has been suggested it was a forgery.\n\nVarious books, memoirs and stories were written about Jesus by the early Christians. The most famous are the gospels of Matthew, Mark, Luke and John. All but one of these are believed to have been written within 50–70 years of the death of Jesus, with the Gospel of Mark believed to be the earliest, and the last the Gospel of John. Blainey writes that the oldest surviving record written by an early Christian is a short letter by St Paul: the First Epistle to the Thessalonians, which appeared about 25 years after the death of Jesus. This letter, while important in describing issues for the development of Gentilic Christianity, contains little of significance for understanding the life of the historic Jesus.\n\nBart Ehrman, Robert Eisenman and others critical of traditional Christian views, in assessing the problems involved in conducting historical Jesus research, say the Gospels are full of discrepancies, were written decades after Jesus' death, by authors who had not witnessed any events in Jesus' life. They go on to say the Gospels were authored not by eyewitnesses who were contemporary with the events that they narrate but rather by people who did not know Jesus, see anything he did, or hear anything he taught, and that the authors did not even share a language with Jesus. The accounts they produced are not disinterested; they are narratives produced by Christians who actually believed in Jesus, and were not immune from slanting the stories in light of their biases. Ehrman points out that the texts are widely inconsistent, full of discrepancies and contradictions in both details and larger portraits of who Jesus was.\n\nIn the context of Christian sources, even if all other texts are ignored, the Pauline epistles can provide some information regarding Jesus. This information does not include a narrative of the life of Jesus, and refers to his existence as a person, but adds few specific items apart from his death by crucifixion. This information comes from those letters of Paul whose authenticity is not disputed. Paul was not a companion of Jesus and claims his information comes from the holy spirit acquired after Jesus' death.\n\nOf the thirteen letters that bear Paul's name, seven are considered authentic by almost all scholars, and the others are generally considered pseudepigraphic. The 7 undisputed letters (and their approximate dates) are: 1 Thessalonians (c. 51 CE), Philippians (c. 52–54 CE), Philemon (c. 52–54 CE), 1 Corinthians (c. 53–54 CE), Galatians (c. 55 CE), 2 Corinthians (c. 55–56 CE) and Romans (c. 55–58 CE). The authenticity of these letters is accepted by almost all scholars, and they have been referenced and interpreted by early authors such as Origen and Eusebius.\n\nGiven that the Pauline epistles are generally dated to AD 50 to AD 60, they are the earliest surviving Christian texts that include information about Jesus. These letters were written approximately twenty to thirty years after the generally accepted time period for the death of Jesus, around AD 30–36. The letters were written during a time when Paul recorded encounters with the disciples of Jesus, e.g. states that several years after his conversion Paul went to Jerusalem and stayed with Apostle Peter for fifteen days. During this time, Paul disputed the nature of Jesus' message with Jesus's brother James, concerning the importance of adhering to kosher food restrictions and circumcision, important features of determining Jewish identity.\n\nThe Pauline letters were not intended to provide a narrative of the life of Jesus, but were written as expositions of Christian teachings. In Paul's view, the earthly life of Jesus was of a lower importance than the theology of his death and resurrection,a theme that permeates Pauline writings. However, the Pauline letters clearly indicate that for Paul Jesus was a real person (born of a woman as in Gal 4.4), a Jew (\"born under the law\", Romans 1.3) who had disciples (1 Corinthians 15.5), who was crucified (as in 1 Corinthians 2.2 and Galatians 3.1) and who resurrected from the dead (1 Corinthians 15.20, Romans 1.4 and 6.5, Philippians 3:10–11). And the letters reflect the general concept within the early Gentillic Christian Church that Jesus existed, was crucified and was raised from the dead.\n\nThe references by Paul to Jesus do not in themselves prove the existence of Jesus, but they do establish that the existence of Jesus was the accepted norm within the early Christians (including the Christian community in Jerusalem, given the references to collections there) twenty to thirty years after the death of Jesus, at a time when those who could have been acquainted with him could still be alive.\n\nThe seven Pauline epistles that are widely regarded as authentic include the following information that along with other historical elements are used to study the historicity of Jesus:\n\nThe existence of only these references to Jesus in the Pauline epistles has given rise to criticism of them by G. A. Wells, who is generally accepted as a leader of the movement to deny the historicity of Jesus. When Wells was still denying the existence of Jesus, he criticized the Pauline epistles for not mentioning items such as John the Baptist or Judas or the trial of Jesus and used that argument to conclude that Jesus was not a historical figure.\n\nJames D. G. Dunn addressed Wells' statement and stated that he knew of no other scholar that shared that view, and most other scholars had other and more plausible explanations for the fact that Paul did not include a narrative of the life of Jesus in his letters, which were primarily written as religious documents rather than historical chronicles at a time when the life story of Jesus could have been well known within the early Church. Dunn states that despite Wells' arguments, the theories of the non-existence of Jesus are a \"thoroughly dead thesis\".\n\nWhile Wells no longer denies the existence of Jesus, he has responded to Dunn, stating that his arguments from silence not only apply to Paul but all early Christian authors, and that he still has a low opinion of early Christian texts, maintaining that for Paul Jesus may have existed a good number of decades before.\n\nThe Pauline letters sometimes refer to creeds, or confessions of faith, that predate their writings. For instance reads: \"For what I received I passed on to you as of first importance: that Christ died for our sins according to the Scriptures, that he was buried, that he was raised on the third day according to the Scriptures.\" refers to Romans 1:2 just before it which mentions an existing gospel, and in effect may be treating it as an earlier creed.\n\nOne of the keys to identifying a pre-Pauline tradition is given in \n\nHere Paul refers to others before him who preached the creed. James Dunn states that indicates that in the 30s Paul was taught about the death of Jesus a few years earlier.\n\nThe Pauline letters thus contain Christian creed elements of pre-Pauline origin. The antiquity of the creed has been located by many Biblical scholars to less than a decade after Jesus' death, originating from the Jerusalem apostolic community. Concerning this creed, Campenhausen wrote, \"This account meets all the demands of historical reliability that could possibly be made of such a text,\" whilst A. M. Hunter said, \"The passage therefore preserves uniquely early and verifiable testimony. It meets every reasonable demand of historical reliability.\"\n\nThese creeds date to within a few years of Jesus' death, and developed within the Christian community in Jerusalem. Although embedded within the texts of the New Testament, these creeds are a distinct source for Early Christianity. This indicates that existence and death of Jesus was part of Christian belief a few years after his death and over a decade before the writing of the Pauline epistles.\n\nThe four canonical gospels, Matthew, Mark, Luke, and John, are the main sources for the biography of Jesus' life, the teachings and actions attributed to him. Three of these (Matthew, Mark, and Luke) are known as the synoptic Gospels, from the Greek σύν (syn \"together\") and ὄψις (opsis \"view\"), given that they display a high degree of similarity in content, narrative arrangement, language and paragraph structure. The presentation in the fourth canonical gospel, i.e. John, differs from these three in that it has more of a thematic nature rather than a narrative format. Scholars generally agree that it is impossible to find any direct literary relationship between the synoptic gospels and the Gospel of John.\n\nThe authors of the New Testament generally showed little interest in an absolute chronology of Jesus or in synchronizing the episodes of his life with the secular history of the age. The gospels were primarily written as theological documents in the context of early Christianity with the chronological timelines as a secondary consideration. One manifestation of the gospels being theological documents rather than historical chronicles is that they devote about one third of their text to just seven days, namely the last week of the life of Jesus in Jerusalem. Although the gospels do not provide enough details to satisfy the demands of modern historians regarding exact dates, scholars have used them to reconstruct a number of portraits of Jesus. However, as stated in the gospels do not claim to provide an exhaustive list of the events in the life of Jesus.\n\nScholars have varying degrees of certainty about the historical reliability of the accounts in the gospels, and the only two events whose historicity is the subject of almost universal agreement among scholars are the baptism and crucifixion of Jesus. Scholars such as E.P. Sanders and separately Craig A. Evans go further and assume that two other events in the gospels are historically certain, namely that Jesus called disciples, and caused a controversy at the Temple.\n\nEver since the Augustinian hypothesis, scholars continue to debate the order in which the gospels were written, and how they may have influenced each other, and several hypothesis exist in that regard, e.g. the Markan priority hypothesis holds that the Gospel of Mark was written first c. 70 CE. In this approach, Matthew is placed at being sometime after this date and Luke is thought to have been written between 70 and 100 CE. However, according to the competing, and more popular, Q source hypothesis, the gospels were not independently written, but were derived from a common source called Q. The two-source hypothesis then proposes that the authors of Matthew and Luke drew on the Gospel of Mark as well as on Q.\n\nThe gospels can be seen as having three separate lines: A literary line which looks at it from a textual perspective, secondly a historical line which observes how Christianity started as a renewal movement within Judaism and eventually separated from it, and finally a theological line which analyzes Christian teachings. Within the historical perspective, the gospels are not simply used to establish the existence of Jesus as sources in their own right alone, but their content is compared and contrasted to non-Christian sources, and the historical context, to draw conclusions about the historicity of Jesus.\n\nTwo possible patristic sources that may refer to eye witness encounters with Jesus are the early references of Papias and Quadratus, reported by Eusebius of Caesarea in the 4th century.\n\nThe works of Papias have not survived, but Eusebius quotes him as saying:\n\nRichard Bauckham states that while Papias was collecting his information (c. 90), Aristion and the elder John (who were Jesus' disciples) were still alive and teaching in Asia minor, and Papias gathered information from people who had known them. However, the exact identity of the \"elder John\" is wound up in the debate on the authorship of the Gospel of John, and scholars have differing opinions on that, e.g. Jack Finegan states that Eusebius may have misunderstood what Papias wrote, and the elder John may be a different person from the author of the fourth gospel, yet still a disciple of Jesus. Gary Burge, on the other hand sees confusion on the part of Eusebius and holds the elder John to be different person from the apostle John.\n\nThe letter of Quadratus (possibly the first Christian apologist) to emperor Hadrian (who reigned 117 – 138) is likely to have an early date and is reported by Eusebius in his \"Ecclesiastical History\" 4.3.2 to have stated:\n\nBy \"our Savior\" Quadratus means Jesus and the letter is most likely written before AD 124. Bauckham states that by \"our times\" he may refer to his early life, rather than when he wrote (117–124), which would be a reference contemporary with Papias. Bauckham states that the importance of the statement attributed to Quadratus is that he emphasizes the \"eye witness\" nature of the testimonies to interaction with Jesus. Such \"eye witness statements\" abound in early Christian writings, particularly the pseudonymous Christian Apocrypha, Gospels and Letters, in order to give them credibility.\n\nA number of later Christian texts, usually dating to the second century or later, exist as New Testament apocrypha, among which the gnostic gospels have been of major recent interest among scholars. The 1945 discovery of the Nag Hammadi library created a significant amount of scholarly interest and many modern scholars have since studied the gnostic gospels and written about them. However, the trend among the 21st century scholars has been to accept that while the gnostic gospels may shed light on the progression of early Christian beliefs, they offer very little to contribute to the study of the historicity of Jesus, in that they are rather late writings, usually consisting of sayings (rather than narrative, similar to the hypothesised Q documents), their authenticity and authorship remain questionable, and various parts of them rely on components of the New Testament. The focus of modern research into the historical Jesus has been away from gnostic writings and towards the comparison of Jewish, Greco-Roman and canonical Christian sources.\n\nAs an example, Bart Ehrman states that gnostic writings of the Gospel of Thomas (part of the Nag Hammadi library) have very little value in historical Jesus research, because the author of that gospel placed no importance on the physical experiences of Jesus (e.g. his crucifixion) or the physical existence of believers, and was only interested in the secret teachings of Jesus rather than any physical events. Similarly, the Apocryphon of John (also part of the Nag Hammadi library) has been useful in studying the prevailing attitudes in the second century, and questions of authorship regarding the Book of revelation, given that it refers to , but is mostly about the post ascension teachings of Jesus in a vision, not a narrative of his life. Some scholars such as Edward Arnal contend that the Gospel of Thomas continues to remain useful for understanding how the teachings of Jesus were transmitted among early Christians, and sheds light on the development of early Christianity.\n\nThere is overlap between the sayings of Jesus in the apocryphal texts and canonical Christian writings, and those not present in the canonical texts are called agrapha. There are at least 225 agrapha but most scholars who have studied them have drawn negative conclusions about the authenticity of most of them and see little value in using them for historical Jesus research. Robert Van Voorst states that the vast majority of the agrapha are certainly inauthentic. Scholars differ on the number of authentic agrapha, some estimating as low as seven as authentic, others as high as 18 among the more than 200, rendering them of little value altogether. While research on apocryphal texts continues, the general scholarly opinion holds that they have little to offer to the study of the historicity of Jesus given that they are often of uncertain origin, and almost always later documents of lower value.\n\n\n"}
{"id": "3423601", "url": "https://en.wikipedia.org/wiki?curid=3423601", "title": "Stumpers-L", "text": "Stumpers-L\n\nThe Stumpers-L electronic mailing list, was a resource available for librarians and others to discuss reference questions which they were unable to answer using available resources. It was succeeded by the similar Project Wombat.\n\nStumpers-L began in 1992, created by Ann Feeney, a library school graduate student at Rosary College in River Forest, Illinois, in the United States. It was moved to Concordia University, Chicago, then back to Rosary, which was then renamed Dominican University. From 2002 to 2005 it was maintained by the Dominican University Graduate School of Library and Information Science program. At the end of 2005 Dominican University ceased hosting the list. A replacement list, known as Project Wombat, commenced in January 2006, and is hosted by Project Gutenberg.\n\nOriginally the Stumpers-L archive was a gopher resource, but migrated to the World Wide Web once the web became more universally used in the mid-1990s.\n\nTypical Stumpers-L topics include:\n\nA book of Stumpers-L questions and answers was published in 1998 by Random House, edited by Fred Shapiro of Yale and titled \"Stumpers! Answers to Hundreds of Questions That Stumped The Experts\" (). Shapiro was an active member; other prominent members include Barbara and David P. Mikkelson, the co-editors of \"Snopes.com.\n\nThe unofficial mascot of the Stumpers-L list is the wombat.\n\n"}
{"id": "26681002", "url": "https://en.wikipedia.org/wiki?curid=26681002", "title": "Text annotation", "text": "Text annotation\n\nText Annotation is the practice and the result of adding a note or gloss to a text, which may include highlights or underlining, comments, footnotes, tags, and links. Text annotations can include notes written for a reader's private purposes, as well as shared annotations written for the purposes of collaborative writing and editing, commentary, or social reading and sharing. In some fields, text annotation is comparable to metadata insofar as it is added post hoc and provides information about a text without fundamentally altering that original text. Text annotations are sometimes referred to as marginalia, though some reserve this term specifically for hand-written notes made in the margins of books or manuscripts. Annotations are extremely useful and help to develop knowledge of English literature.\n\nThis article covers both private and socially shared text annotations, including hand-written and information technology-based annotation. For information on annotation of Web content, including images and other non-textual content, see also Web annotation.\n\nText annotation may be as old as writing on media, where it was possible to produce an additional copy with a reasonable effort. It became a prominent activity around 1000 AD in Talmudic commentaries and Arabic rhetorics treaties. In the Medieval era, scribes who copied manuscripts often made marginal annotations that then circulated with the manuscripts and were thus shared with the community; sometimes annotations were copied over to new versions when such manuscripts were later recopied.\n\nWith the rise of the printing press and the relative ease of circulating and purchasing individual (rather than shared) copies of texts, the prevalence of socially shared annotations declined and text annotation became a more private activity consisting of a reader interacting with a text. Annotations made on shared copies of texts (such as library books) are sometimes seen as devaluing the text, or as an act of defacement. Thus, print technologies support the circulation of annotations primarily as formal scholarly commentary or textual footnotes or endnotes rather than marginal, handwritten comments made by private readers, though handwritten comments or annotations were common in collaborative writing or editing.\n\nComputer-based technologies have provided new opportunities for individual and socially shared text annotations that support multiple purposes, including readers’ individual reading goals, learning, social reading, writing and editing, and other practices. Text annotation in Information Technology (IT) systems raises technical issues of access, linkage, and storage that are generally not relevant to paper-based text annotation, and thus research and development of such systems often addresses these areas.\n\nText annotations can serve a variety of functions for both private and public reading and communication practices. In their article \"From the Margins to the Center: The Future of Annotation,\" scholars Joanna Wolfe and Christine Neuwirth identify four primary functions that text annotations commonly serve in the modern era, including: (1)\"facilitat[ing] reading and later writing tasks,\" which includes annotations that support reading for both personal and professional purposes; (2)\"eavesdrop[ping] on the insights of other readers,\" which involves sharing of annotations; (3)\"provid[ing] feedback to writers or promote communication with collaborators,\" which can include personal, professional, and education-related feedback; and (4)\"call[ing] attention to topics and important passages,\" for which scholarly annotations, footnotes, and call-outs often function. Regarding the ways that annotations can support individual reading tasks, Catherine Marshall points out that the ways that readers annotate texts depends on the purpose, motivation, and context of reading. Readers may annotate to help interpret a text, to call attention to a section for future reference or reading, to support memory and recall, to help focus attention on the text as they read, to work out a problem related to the text, or create annotations not specifically related to the text at all.\n\nEducational research in text annotation has examined the role that both private and shared text annotations can play in supporting learning goals and communication. Much educational research examines how students’ private annotation of texts supports comprehension and memory; for example, research indicates that annotating texts causes more in-depth processing of information, which results in greater recall of information.\n\nOther areas of educational research investigate the benefits of socially shared text annotations for collaborative learning, both for paper-based and IT-based annotation sharing. For example, studies by Joanna Wolfe have investigated the benefits of exposure to others’ annotations on student readers and writers. In a 2000 study, Wolfe found that exposing students to others’ annotations influenced their perceptions of the annotators, which in turn shaped their responses to the material and their written products. In a later study, Wolfe found that viewing others’ written comments on a paper text, especially pairs of annotations that present opposing responses to the text, can help students engage in the type of critical reading and stance-taking necessary for effective argumentative writing.\n\nWhile shared annotations can benefit individual readers, it is important to note that, \"since the 1920s, literacy theory has increasingly emphasized the importance of social factors in the development of literacy.\" Thus, shared annotations can not only help one to better understand the content of a particular text, but may also aid in the acquirement of literacy skills. For example, a mother may leave marks inside a book to draw the attention of her child to a particular theme or concept; thanks to the development of audio annotations, parents may now leave notes for children who are just starting to read and may struggle with textual annotations.\n\nMore recent research in the effects of shared text annotations has focused on the learning applications for web-based annotation systems, some of which were developed based on design recommendations from studies outlined above. For example, Ananda Gunawardena, Aaron Tan, and David Kaufer conducted a pilot study to examine whether annotating documents in Classroom Salon, a web-based annotation and social reading platform, encouraged active reading, error detection, and collaboration in a computer science course at Carnegie Mellon University. This study suggested a correlation between students’ overall performance in the course and their ability to identify errors in a text that they annotated in Classroom Salon; it also found that students were likely to change their annotations in response to annotations made by others in the course.\n\nSimilarly, the web-based annotation tool HyLighter was used in a first-year writing course and shown to improve the development of students’ mental models of texts, including supporting reading comprehension, critical thinking, and the ability to develop a thesis. The collaboration with peers and experts around a shared text improved these skills and brought the communities’ understanding closer together.\n\nA meta-analysis of empirical studies into the higher-education uses of social annotation (SA) tools indicates such tools have been tested in several courses, among them English, sport psychology, and hypermedia. Studies have indicated that social annotation functions, including commenting, information sharing, and highlighting, can support instruction designed to foster collaborative learning and communication, as well as reading comprehension, metacognition, and critical analysis. Several studies indicated that students enjoyed using social annotation tools, and that it improved motivation in the course.\n\nText annotations have long been used in writing and revision processes as a way for reviewers to suggest changes and communicate about a text. In book publishing, for example, the collaboration of authors and editors to develop and revise a manuscript frequently involves exchanges of both in-line revisions or notes as well as marginal annotations. Similarly, copyeditors often make marginal annotations or notes that explain or suggest revisions or are directed at the author as questions or suggestions (commonly called \"queries\"). Asynchronous collaborative writing and document development often depend on text annotations as a way not only to suggest revisions but also to exchange ideas during document development or to facilitate group decision making, though such processes are often complicated by the use of different communication technologies (such as phone calls or emails as well as document sharing) for distinct tasks. Text annotations can also function to allow group or community members to communicate about a shared text, such as a doctor annotating a patient's chart.\n\nMuch research into the functionality and design of collaborative IT-based writing systems, which often support text annotation, has occurred in the area of computer-supported cooperative work.\n\nResearch in the design and development of annotation systems uses specific terminology to refer to distinct structural components of annotations and also distinguishes among options for digital annotation displays.\n\nThe structural components of any annotation can be roughly divided into three primary elements: a \"body\", an \"anchor\", and a \"marker\". The body of an annotation includes reader-generated symbols and text, such as handwritten commentary or stars in the margin. The anchor is what indicates the extent of the original text to which the body of the annotation refers; it may include circles around sections, brackets, highlights, underlines, and so on. Annotations may be anchored to very broad stretches of text (such as an entire document) or very narrow sections (such as a specific letter, word, or phrase). The marker is the visual appearance of the anchor, such as whether it is a grey underline or a yellow highlight. An annotation that has a body (such as a comment in the margin) but no specific anchor has no marker.\n\nIT-based annotation systems utilize a variety of display options for annotations, including:\nAnnotation interfaces may also allow highlighting or underlining, as well as threaded discussions. Sharing and communicating through annotations anchored to specific documents is sometimes referred to as \"anchored discussion\".\n\nIT-based annotation systems include standalone and client-server systems. In the 1980s and 1990s, a number of such systems were built in the context of libraries, patent offices, and legal text processing. Their design led researchers to produce taxonomies of annotation forms. Text annotation research has taken place at several institutions, including Xerox research centers in Palo Alto and Grenoble (France), the Hitachi Central Research Lab (in particular for annotation of patents), and in relation with the construction of the new French National Library between 1989 and 1995 at the Institut de Recherche en Informatique de Toulouse and in the company AIS (Advanced Innovation Systems).\n\nAnnotation functionality has been present in text processing software for many years through inline notes displayed as pop-ups, footnotes, and endnotes; however, it is only recently that functionality for displaying annotations as marginalia has appeared in programs such as OpenOffice.org/LibreOffice Writer and Microsoft Word. Personal or standalone annotation include word processing software that supports embedded or anchored text annotations as well as Adobe Acrobat, which in addition to commenting allows highlights, stamps, and other types of markup.\n\nTim Berners-Lee had already implemented the concept of directly editing web documents in 1990 in WorldWideWeb, the first web browser, but later ported versions removed this collaborative ability. An early version of NCSA Mosaic in 1993 also included a collaborative annotation capability, though it was quickly removed. Web Distributed Authoring and Versioning, WebDAV, was then reintroduced as an extension.\n\nA different approach to distributed authoring consists in first gathering many annotations from a wide public, and then integrate them all in order to produce a further version of a document. This approach was pioneered by Stet, the system put in place to gather comments on drafts of version 3 of the GNU General Public License. This system arose after a specific requirement, which it served egregiously, but was not so easily configurable as to be convenient for annotating any other document on the web. The co-ment system uses annotation interface concepts similar to Stet's, but it is based on an entirely new implementation, using Django/Python on the server side and various AJAX libraries such as JQuery on the client side. Both Stet and co-ment are licensed under the GNU Affero General Public License.\n\nSince 2011, the non-profit Hypothes Is Project has offered the free, open web annotation service Hypothes.is. The service features annotation via a Chrome extension, bookmarklet or proxy server, as well as integration into a LMS or CMS. Both webpages and PDFs can be annotated. Other web-based text annotation systems are collaborative software for distributed text editing and versioning, which also feature annotation and commenting interfaces. For example, HyLighter supports synchronous and asynchronous interactions, general commenting, comment tagging, threaded discussions and comment filtering. Other annotation tools under these category are more focused on NLP tasks as Named-entity recognition, relationship extraction or normalization. Some tools support manual tagging of data or automatic annotations via supervised learning.\n\nSpecialized Web-based text annotations exist in the context of scientific publication, either for refereeing or post-publication. The on-line journal PLoS ONE, published by the Public Library of Science, has developed its own Web-based system where scientists and the public can comment on published articles. The annotations are displayed as pop-ups with an anchor in the text.\n\n\n"}
{"id": "270906", "url": "https://en.wikipedia.org/wiki?curid=270906", "title": "Three-letter acronym", "text": "Three-letter acronym\n\nA three-letter acronym (TLA), or three-letter abbreviation, is an abbreviation, specifically an acronym, alphabetism, or initialism, consisting of three letters. These are usually the initial letters of the words of the phrase abbreviated, and are written in capital letters (upper case); three-letter abbreviations such as \"etc.\" and \"Mrs.\" are not three-letter acronyms, but \"TLA\" is a TLA (an example of an autological abbreviation).\n\nMost three-letter abbreviations are \"initialisms\": all the letters are pronounced as the names of letters, as in \"APA\" . Some are acronyms pronounced as a word; computed axial tomography, CAT, is almost always pronounced as the animal's name in \"CAT scan\".\n\n\nThe exact phrase \"three-letter acronym\" appeared in the sociology literature in 1975. Three-letter acronyms were used as mnemonics in biological sciences, from 1977 and their practical advantage was promoted by Weber in 1982. They are used in many other fields, but the term TLA is particularly associated with computing. In 1980, the manual for the Sinclair ZX81 home computer used and explained TLA. The specific generation of three-letter acronyms in computing was mentioned in a JPL report of 1982. In 1988, in a paper titled \"On the cruelty of really teaching computer science\", eminent computer scientist Edsger W. Dijkstra wrote \n\"Because no endeavour is respectable these days without a TLA ...\" By 1992 it was in a Microsoft handbook.\n\nThe number of possible three-letter abbreviations (or permutations) using the 26 letters of the alphabet from A to Z (AAA, AAB ... to ZZY, ZZZ) is 26 × 26 × 26 = 17,576. Another 26 × 26 × 10 = 6760 can be produced if the third element is allowed to be a digit 0-9, giving a total of 24,336.\n\nIn English, WWW is the longest possible TLA to pronounce, typically requiring nine syllables. The usefulness of TLAs typically comes from how it is quicker to say the acronym instead of than the phrase they represent, however saying 'WWW' in English requires three times as many syllables than the phrase it is meant to abbreviate (World Wide Web). Consequently, \"www\" is sometimes abbreviated as \"dubdubdub\" in speech.\n\n\n"}
{"id": "58757675", "url": "https://en.wikipedia.org/wiki?curid=58757675", "title": "William Chaffers", "text": "William Chaffers\n\nWilliam Chaffers (28 September 1811 – 12 April 1892) was an English antiquary and writer of reference works on hallmarks, and marks on ceramics. His \"Marks and Monograms on Pottery and Porcelain\", first published in 1863, has appeared in many later editions.\n\nChaffers was the son of William Chaffers and wife Sarah, and was born in Watling Street, London, in 1811; he was descended from a brother of Richard Chaffers (1731–1765), a manufacturer of Liverpool porcelain. He was educated at Margate and at Merchant Taylors' School, where he was entered in 1824.\n\nHe was attracted to antiquarian studies while a clerk in the city of London, by the discovery of Roman and medieval antiquities in the foundations of the Royal Exchange during 1838–9. At the same time he began to concentrate attention upon the study of gold and silver plate and ceramics, especially in regard to the official and other marks by which dates and places of fabrication can be distinguished. In 1863 Chaffers published two important works:\n\nOther publications are \"The Keramic Gallery\", in 2 volumes, with 500 illustrations (1872); a handbook abridged from \"Marks and Monograms\" (1874); \"Gilda Aurifabrorum\", a history of goldsmiths and plate workers and their marks (1883); also a priced catalogue of coins, and other minor catalogues.\n\nHis reputation was furthered in organizing exhibitions of art treasures, at Manchester in 1857, South Kensington in 1862, Leeds in 1869, Dublin in 1872, Wrexham in 1876, and Hanley (at the great Staffordshire exhibition of ceramics) in 1890. Chaffers was elected Fellow of the Society of Antiquaries of London in 1843, and he was a frequent contributor to \"Archæologia\", to \"Notes and Queries\", and to various learned periodicals upon the two subjects of which he had particular knowledge.\n\nIn 1841 he married Charlotte Matilda, daughter of John Hewett. About 1870 he retired from Fitzroy Square to a house in Willesden Lane, and later moved to West Hampstead, where he died on 12 April 1892.\n\nAttribution\n"}
