{"id": "46806415", "url": "https://en.wikipedia.org/wiki?curid=46806415", "title": "Abbreviation (music)", "text": "Abbreviation (music)\n\nAbbreviations in music are of two kinds, namely, abbreviations of terms related to musical expression, and the true musical abbreviations by the help of which certain passages, chords, etc., may be notated in a shortened form, to the greater convenience of both composer and performer. Abbreviations of the first kind are like most abbreviations in language; they consist for the most part of the initial letter or first syllable of the word employed—as for instance, or for the dynamic markings piano and forte, \"cresc.\" for crescendo, \"Ob.\" for oboe, \"Fag.\" for bassoon (). This article is about abbreviations used in music notation. For abbreviations of terms related to musical expression and music in general, see Glossary of musical terminology.\n\nThe continued repetition of a note or chord is expressed by a stroke or strokes across the stem, or above or below the note if it be a whole note or double whole note. The number of strokes denotes the subdivision of the written note into eighth notes, sixteenth notes, etc., unless the word tremolo or tremolando is added, in which case the repetition is as rapid as possible, without regard to the exact number of notes played. (When strokes are added to notes shorter than a quarter note, each beam counts as a stroke.) In the first bar of the example below, the half note with the single stroke across the stem in the \"written\" staff becomes 4 eighth notes in the \"played\" staff. Through the use of 2 strokes across the stem in the second bar, the next full note is expressed as a phrase of 16 sixteenth notes.\n\n\\relative c\" « { \\override Score.TimeSignature #'stencil = ##f } \\new staff { c2:8^\\markup { Written: } <e c g>: | c1:16 | \\time 2/4 f,2:32 | \\time 4/4 f4:8 a:8 c:16 f:16 | b,8:16 d:16 b:16 g:16 a:32 b:32 c:32 d:32 \\bar \"|.\" } \\new staff { c8^\\markup { Played: } c c c <e c g> <e c g> <e c g> <e c g> | c16 c c c c c c c c c c c c c c c | f,32 f f f f f f f f f f f f f f f | f8 f a a c16 c c c f f f f | b, b d d b b g g a32 a a a b b b b c c c c d d d d } »\n</score>\n\nOn bowed instruments the rapid reiteration of a single note is easy, but in piano music an octave or chord becomes necessary to produce a tremolo, the manner of writing and performing of which is seen below.\n\n\\relative c\" « { \\override Score.TimeSignature #'stencil = ##f } \\time 2/4 \\new staff { <g g'>4:16 ^\\markup { \\italic tremolo } <e' c g>:16 | \\repeat tremolo 4 { <c e>16^\\markup { \\italic tremolo } g } | s4 } \\new staff { g'32*8/12 g, g' g, g' g, g' g, g' g, g' g, <c e>32*8/12 g <c e> g <c e> g <c e> g <c e> g <c e> g | <c e>32 g <c e> g <c e> g <c e> g <c e> g <c e> g <c e> g <c e> g | s4 } »\n</score>\n\nIn the abbreviation expressed by strokes, as above, the passage to be abbreviated can contain no note of greater length than an eighth note, but it is possible also to divide a long note into quarter notes, by means of dots (sometimes known as \"divisi\" dots) placed over it, as below.\n\n\\relative c\" « { \\override Score.TimeSignature #'stencil = ##f } \\time 4/4 \\new staff { c2^\\markup { .. } c2^\\markup { .. } | c1^\\markup { ... } | s2 } \\new staff { c4 c c c | c c c c | s2 } »\n</score>\n\nThis is however seldom done, as only a small amount of space is saved. When a long note has to be repeated in the form of triplets or sextuplets, the figure 3 or 6 is usually placed over it in addition to the stroke across the stem, and the note is sometimes, though not necessarily, written dotted.\n\n\\relative c\" « { \\override Score.TimeSignature #'stencil = ##f } \\time 4/4 \\new staff { c4:8^\\markup { \\smaller { \\italic 3 } } c4:8^\\markup { \\smaller { \\italic 3 } } c2:8^\\markup { \\smaller { \\italic 6 } } \\bar \"||\" \\time 4/4 \\omit TupletNumber \\tuplet 3/2 { c4.:8^\\markup { \\smaller { \\italic 3 } } } \\tuplet 3/2 { c4.:8^\\markup { \\smaller { \\italic 3 } } } \\tuplet 6/4 { c4.:16^\\markup { \\smaller { \\italic 6 } } } \\tuplet 6/4 { c4.:16^\\markup { \\smaller { \\italic 6 } } } \\bar \"||\" } \\new staff { \\tuplet 3/2 4 {c8 c c c c c c c c c c c} | \\tuplet 3/2 4 {c8 c c c c c} \\tuplet 6/4 4 {c16 c c c c c c c c c c c} } »\n</score>\n\nThe repetition of a group of two notes is abbreviated by two notes (most often half notes or whole notes) connected by the number of strokes ordinarily used to express eighth notes, sixteenth notes, etc., according to the rate of movement intended, as below. It will be observed that a passage lasting for the value of one half note requires two half notes to express it, on account of the group consisting of two notes.\n\n\\relative c' « { \\override Score.TimeSignature #'stencil = ##f } \\time 4/4 \\new staff { \\repeat tremolo 2 { f8 a } \\repeat tremolo 4 { f16 a} | \\repeat tremolo 4 { f8 a } | s4 } \\new staff { f8 a f a f16 a f a f a f a | f8 a f a f a f a | s4 } »\n</score>\n\nAs seen above, half notes are often written with the strokes beaming the notes together (which is unambiguous as white notes with beams are not otherwise used in music), but with quarter notes and shorter the strokes must be separated from the stems to prevent them being misread as a shorter note value.\n\n\\relative c\" « { \\override Score.TimeSignature #'stencil = ##f } \\time 2/4 \\new staff { \\repeat tremolo 4 { c32 a } \\repeat tremolo 4 { g64 c } \\repeat tremolo 4 { g64 b } | s4 } \\new staff { c32 a c a c a c a g64 [c g c g c g c] g [b g b g b g b] | s4 } »\n</score>\n\nA group of three, four, or more notes is abbreviated by the repetition of the cross strokes without the notes as many times as the group has to be repeated.\n\n\\relative c' « { \\override Score.TimeSignature #'stencil = ##f } \\time 4/4 \\new staff { \\repeat percent 2 { e8[ g c g] } | \\repeat percent 4 { e16 g c g } \\bar \"||\" } \\new staff { e8 g c g e g c g | e16 g c g e g c g e g c g e g c g } »\n</score>\n\nThis can also be written with the notes forming the group are written as a chord, with the necessary number of strokes across the stem. In this case the word \"simili\" or \"segue\" is added, to show that the order of notes in the first group (which must be written out in full) is to be repeated, and to prevent the possibility of mistaking the effect intended for the repetition of the chord as a whole.\n\n\\relative c' « { \\override Score.TimeSignature #'stencil = ##f } \\time 4/4 \\new staff { d16 f a f <d f a>2.:16^\\markup { \\italic simili } | s4 } \\new staff { d16 f a f d f a f d f a f d f a f | s4 } »\n</score>\n\nAnother sign of abbreviation of a group consists of an oblique line with two dots, one on each side; this serves to indicate the repetition of a group of any number of notes of any length. This can even apply to a passage composed of several groups, provided such passage is not more than two bars in length.\n\n\\relative c' « { \\override Score.TimeSignature #'stencil = ##f } \\time 4/4 \\new staff { \\repeat percent 2 { e16 g f e f g a b c g a b c d e f g a g f e d c b a g c b a g a f } \\bar \"||\" } \\new staff { e16 g f e f g a b c g a b c d e f g a g f e d c b a g c b a g a f | e g f e f g a b c g a b c d e f g a g f e d c b a g c b a g a f } »\n</score>\n\nA more usual method of abbreviating the repetition of a passage of the length of the above is to write over it the word \"bis\" (twice), or in some cases \"ter\" (three times), or to enclose it between the dots of an ordinary repeat sign.\n\nPassages intended to be played in octaves are often written as single notes with the words \"coll' ottava\" or \"coll' 8va\" placed above or below them, according as the upper or lower octave is to be added.\n\n« { \\override Score.TimeSignature #'stencil = ##f } \\time 2/4 \\new Staff { c\"8^\\markup { \\smaller { \\italic \"coll' 8\" \\super \\italic \"va\" } } d\" e\" f\" \\bar \"||\" \\clef bass g_\\markup { \\smaller { \\italic \"coll' 8\" \\super \\italic \"va\" } } e c4 \\bar \"||\" s4 } \\new Staff { <c\" c>8 <d\" d> <e\" e> <f\" f> | \\clef bass <g g,> <e e,> <c c,>4 | s4 } »\n</score>\n\nThe word \"8\" (or sometimes \"8 alta\" or \"8 bassa\") written above or below a passage does not add octaves, but merely transposes the passage an octave higher or lower. In clarinet music the word \"chalumeau\" is used to signify that the passage is to be played an octave lower than written.\n\n« { \\override Score.TimeSignature #'stencil = ##f } \\time 4/4 \\new Staff { \\ottava #1 f8 e d c \\ottava #0 b\"^\\markup { \\smaller \\italic loco } a\" g\"4 \\bar \"||\" \\clef bass \\ottava #-1 a,8 b, c, d, \\ottava #0 e_\\markup { \\smaller \\italic loco } f g4 \\bar \"||\" \\clef treble \\ottava #-1 \\set Staff.ottavation = #\"chalumeau\" e8 g c' g f a c' a | g \\ottava #0 e'_\\markup { \\smaller \\italic clar. } g' c\" e\"2 \\bar \"||\" } \\new Staff { f8 e d c b\" a\" g\"4 | \\clef bass a,8 b, c, d, e, f, g,4 | \\clef treble e8 g c' g f a c' a | g e' g' c\" e\"2 } »\n</score>\n\nAll these alterations (which can scarcely be considered abbreviations except that they spare the use of ledger lines) are counteracted, and the passage restored to its usual position, by the ending of the enclosing bracket, the word \"loco\", or in clarinet music \"clarinette\".\n\nIn orchestral music it often happens that certain of the instruments play in unison; when this is the case the parts are sometimes not all written in the score, but the lines belonging to one or more of the instruments are left blank, and the words \"coi violini\" or \"col basso\", etc., are added, to indicate that the instruments in question have to play in unison with the violins or basses, as the case may be, or when two instruments of the same kind, such as first and second violins, have to play in unison, the word \"unisono\" or \"col primo\" is placed instead of the notes in the line belonging to the second. Where two parts are written on one staff in a score the sign \"a 2\" denotes that both play the same notes; and \"a 1\" that the second of the two is resting. The indication \"a 3\" or \"a 4\" at the head of fugues indicates the number of parts or voices in which the fugue is written.\n\nAn abbreviation which is often very troublesome to the conductor occurs in manuscript scores, when a considerable part of the composition is repeated without alteration, and the corresponding number of bars are left vacant, with the remark \"come sopra\" (as above). This is not met with in printed scores.\n\nThere are also abbreviations relating to music analysis, some of which are of great value. In figured bass, for instance, the various chords are expressed by figures, and the several authors in the nineteenth century invented or availed themselves of various methods of shortly expressing the different chords and intervals, particularly using Roman numeral analysis.\n\nGottfried Weber represents an interval by a number with one or two dots before it to express minor or diminished, and one or two after it for major or augmented.\n\nJohann Anton André makes use of a right triangle to express a triad, and a square, for a seventh chord, the inversions being indicated by one, two, or three small vertical lines across their base, and the classification into major, minor, diminished, or augmented by the numbers 1, 2, 3, or 4, placed in the centre.\n"}
{"id": "624684", "url": "https://en.wikipedia.org/wiki?curid=624684", "title": "Annotation", "text": "Annotation\n\nAn annotation is a metadatum (e.g. a post, explanation, markup) attached to location or other data.\n\nTextual scholarship is a discipline that often uses the technique of annotation to describe or add additional historical context to texts and physical documents.\n\nStudents often highlight passages in books in order to refer back to key phrases easily, or add marginalia to aid studying. One educational technique when analyzing prose literature is to have students or teachers circle the names of characters and put rectangular boxes around phrases identifying the setting of a given scene.\n\nAnnotated bibliographies add commentary on the relevance or quality of each source, in addition to the usual bibliographic information that merely identifies the source.\n\nFrom a cognitive perspective annotation has an important role in learning and instruction. As part of guided noticing it involves highlighting, naming or labelling and commenting aspects of visual representations to help focus learners' attention on specific visual aspects. In other words, it means the assignment of typological representations (culturally meaningful categories), to topological representations (e.g. images). This is especially important when experts, such as medical doctors, interpret visualizations in detail and explain their interpretations to others, for example by means of digital technology. Here, annotation can be a way to establish common ground between interactants with different levels of knowledge. The value of annotation has been empirically confirmed, for example, in a study which shows that in computer-based teleconsultations the integration of image annotation and speech leads to significantly improved knowledge exchange compared with the use of images and speech without annotation.\n\nMarkup languages like XML and HTML annotate text in a way that is syntactically distinguishable from that text. They can be used to add information about the desired visual presentation, or machine-readable semantic information, as in the semantic web.\n\nThe \"annotate\" function (also known as \"blame\" or \"praise\") used in source control systems such as Git, Team Foundation Server and Subversion determines who committed changes to the source code into the repository. This outputs a copy of the source code where each line is annotated with the name of the last contributor to edit that line (and possibly a revision number). This can help establish blame in the event a change caused a malfunction, or identify the author of brilliant code.\n\nA special case is the Java programming language, where annotations can be used as a special form of syntactic metadata in the source code. Classes, methods, variables, parameters and packages may be annotated. The annotations can be embedded in class files generated by the compiler and may be retained by the Java virtual machine and thus influence the run-time behaviour of an application. It is possible to create meta-annotations out of the existing ones in Java.\n\nSince the 1980s, molecular biology and bioinformatics have created the need for DNA annotation. DNA annotation or genome annotation is the process of identifying the locations of genes and all of the coding regions in a genome and determining what those genes do. An annotation (irrespective of the context) is a note added by way of explanation or commentary. Once a genome is sequenced, it needs to be annotated to make sense of it.\n\nIn the digital imaging community the term annotation is commonly used for visible metadata superimposed on an image without changing the underlying master image, such as sticky notes, virtual laser pointers, circles, arrows, and black-outs (cf. redaction).\n\nIn the medical imaging community, an annotation is often referred to as a region of interest and is encoded in DICOM format.\n\nIn the United States, legal publishers such as Thomson West and Lexis Nexis publish annotated versions of statutes, providing information about court cases that have interpreted the statutes. Both the federal United States Code and state statutes are subject to interpretation by the courts, and the annotated statutes are valuable tools in legal research.\n\nIn linguistics, annotations include comments and metadata; these non-transcriptional annotations are also non-linguistic. A collection of texts with linguistic annotations is known as a corpus (plural \"corpora\"). The Linguistic Annotation Wiki describes tools and formats for creating and managing linguistic annotations.\n\n"}
{"id": "1837830", "url": "https://en.wikipedia.org/wiki?curid=1837830", "title": "Apocope", "text": "Apocope\n\nIn phonology, apocope () is the loss (elision) of one or more sounds from the end of a word, especially the loss of an unstressed vowel.\n\n\"Apocope\" comes from the Greek () from () \"cutting off\", from () \"away from\" and () \"to cut\".\n\nIn historical linguistics, \"apocope\" is often the loss of an unstressed vowel.\n\n\n\nIn Estonian and the Sami languages, apocopes explain the forms of grammatical cases. For example, a nominative is described as having apocope of the final vowel, but the genitive does not. Throughout its history, however, the genitive case marker has also undergone apocope: Estonian (\"a city\") and (\"of a city\") are derived from and respectively, as can still be seen in the corresponding Finnish word. In the genitive form, the final , while it was being deleted, blocked the loss of . In colloquial Finnish, the final vowel is sometimes omitted from case markers.\n\nSome languages have apocopations that are internalized as mandatory forms. In Spanish and Italian, for example, some adjectives that come before the noun lose the final vowel or syllable if they precede a noun (mainly) in the masculine singular form. In Spanish, some adverbs and cardinal and ordinal numbers have apocopations as well.\n\n\nVarious numerous sorts of informal abbreviations might be classed as apocope:\n\nFor a list of similar apocopations in the English language, see List of English apocopations.\n\nDiminutives in Australian English lists many apocopations.\n\nThe process is also linguistically subsumed under one called \"clipping\", or \"truncation\".\n\n\n\n"}
{"id": "928216", "url": "https://en.wikipedia.org/wiki?curid=928216", "title": "Autological word", "text": "Autological word\n\nAn autological word (also called homological word or autonym) is a word that expresses a property that it also possesses (e.g. the word \"short\" is short, \"noun\" is a noun, \"English\" is English, \"pentasyllabic\" has five syllables, \"word\" is a word). The opposite is a heterological word, one that does not apply to itself (e.g. \"long\" is not long, \"monosyllabic\" has five syllables).\n\nUnlike more general concepts of autology and self-reference, this particular distinction and opposition of \"autological\" and \"heterological words\" is uncommon in linguistics for describing linguistic phenomena or classes of words, but is current in logic and philosophy where it was introduced by Kurt Grelling and Leonard Nelson for describing a semantic paradox, later known as Grelling's paradox or the Grelling–Nelson paradox. \n\nOne source of autological words is ostensive definition: the reference to a class of words by an example of the member of the class, as it were by synecdoche: such as mondegreen, oxymoron, eggcorn, bahuvrihi, etc.\nA word's status as autological may change over time. For example, \"neologism\" was once an autological word but no longer is; similarly, \"protologism\" (a word invented recently by literary theorist Mikhail Epstein) may or may not lose its autological status depending on whether or not it gains wider usage.\n\n\n\n"}
{"id": "2887701", "url": "https://en.wikipedia.org/wiki?curid=2887701", "title": "Bible (screenwriting)", "text": "Bible (screenwriting)\n\nA bible (also known as a story bible, show bible, series bible, or pitch bible) is a reference document used by screenwriters for information on a television series' characters, settings, and other elements.\n\nShow bibles are updated with information on the characters after the information has been established on screen. For example, the \"Frasier\" show bible was \"scrupulously maintained\", and anything established on air — \"the name of Frasier's mother, Niles' favorite professor, Martin's favorite bar...even a list of Maris' [dozens of] food allergies\" — was reflected in the bible. The updated bible then serves as a resource for writers to keep everything within the series consistent. \n\nOther show bibles are used as sales documents to help a television network or studio understand a series, and are sometimes given to new writers when they join the writing staff for the same reason. These types of bibles discuss the backstories of the main characters and the history of the series' fictional universe.\n\nTelevision series often rely on writers' assistants and script coordinators to serve as \"walking bibles\" in remembering details about a series.\n\nIn the United States, writing the show bible of a produced series earns that writer the 24 units of required credit necessary to qualify for membership in the Writers Guild of America.\n\n\n"}
{"id": "27778631", "url": "https://en.wikipedia.org/wiki?curid=27778631", "title": "Breviograph", "text": "Breviograph\n\nA breviograph or brevigraph (from , short, and Greek \"grapho\", to write) is a type of scribal abbreviation in the form of an easily written symbol, character, flourish or stroke, based on a modified letter form to take the place of a common letter combination, especially those occurring at the beginning or end of a word. Breviographs were used frequently by stenographers, law clerks and scriveners, and they were also found in early printed books and tracts. Their use declined after the 17th century.\n\nExamples of breviographs:\n\n\n"}
{"id": "6767022", "url": "https://en.wikipedia.org/wiki?curid=6767022", "title": "Carol Ballard", "text": "Carol Ballard\n\nCarol Ballard is an author of more than 80 non-fiction books. Specializing in informational books for children and teens, her focus is toward the 7- to 14-year-old group.\n\nAfter graduating from Leeds University in plant sciences, Ballard did post-graduate research and was awarded a PhD in Immunology. She has many years experience as a science teacher and co-ordinator and has written articles for teachers on various aspects of science teaching, and teachers' materials for classroon use.\n\nIn addition to her writing, Carol works as a freelance consultant for publishers on educational and scientific matters. She also has her own business, Kite Books, which produces worksheets and teachers' resources.\n\n\n"}
{"id": "744504", "url": "https://en.wikipedia.org/wiki?curid=744504", "title": "Circular reference", "text": "Circular reference\n\nA circular reference is a series of references where the last object references the first, resulting in a closed loop.\nA circular reference is not to be confused with the logical fallacy of a circular argument. Although a circular reference will often be unhelpful and reveal no information, such as two entries in a book index referring to each other, it is not necessarily so that a circular reference is of no use. Dictionaries, for instance, must always ultimately be a circular reference since all words in a dictionary are defined in terms of other words, but a dictionary nevertheless remains a useful reference. Sentences containing circular references can still be meaningful;\n\nis circular but not without meaning. Indeed, it can be argued that self-reference is a necessary consequence of Aristotle's Law of non-contradiction, a fundamental philosophical axiom. In this view, without self-reference, logic and mathematics become impossible, or at least, lack usefulness.\n\nCircular references can appear in computer programming when one piece of code requires the result from another, but that code needs the result from the first. For example:\n\nFunction A will show the time the sun last set based on the current date, which it can obtain by calling Function B. Function B will calculate the date based on the number of times the moon has orbited the earth since the last time Function B was called. So, Function B asks Function C just how many times that is. Function C doesn't know, but can figure it out by calling Function A to get the time the sun last set.\n\nThe entire set of functions is now worthless because none of them can return any useful information whatsoever. This leads to what is technically known as a livelock. It also appears in spreadsheets when two cells require each other's result. For example, if the value in Cell A1 is to be obtained by adding 5 to the value in Cell B1, and the value in Cell B1 is to be obtained by adding 3 to the value in Cell A1, no values can be computed. (Even if the specifications are A1:=B1+5 and B1:=A1-5, there is still a circular reference. It doesn't help that, for instance, A1=3 and B1=-2 would satisfy both formulae, as there are infinitely many other possible values of A1 and B1 that can satisfy both instances.)\n\nA circular reference represents a big problem in computing.\n\nIn ISO Standard SQL circular integrity constraints are implicitly supported within a single table. Between multiple tables circular constraints (e.g. foreign keys) are permitted by defining the constraints as deferrable (See CREATE TABLE for PostgreSQL and DEFERRABLE Constraint Examples for Oracle). In that case the constraint is checked at the end of the transaction not at the time the DML statement is executed. To update a circular reference two statements can be issued in a single transaction that will satisfy both references once the transaction is committed.\n\nA distinction should be made with processes containing a circular reference between those that are incomputable and those that are an iterative calculation with a final output. The latter may fail in spreadsheets not equipped to handle them but are nevertheless still logically valid.\n\nCircular reference in worksheets can be a very useful technique for solving implicit equations such as the Colebrook equation and many others, which might otherwise require tedious Newton-Raphson algorithms in VBA or use of macros.\n\n"}
{"id": "10018490", "url": "https://en.wikipedia.org/wiki?curid=10018490", "title": "Citation Style Language", "text": "Citation Style Language\n\nThe Citation Style Language (CSL) is an open XML-based language to describe the formatting of citations and bibliographies. Reference management programs using CSL include Zotero, Mendeley and Papers.\n\nCSL was created by Bruce D'Arcus for use with OpenOffice.org, and an XSLT-based \"CiteProc\" CSL processor. CSL was further developed in collaboration with Zotero developer Simon Kornblith. Since 2008, the core development team consists of D'Arcus, Frank Bennett and Rintze Zelle.\n\nThe releases of CSL are 0.8 (March 21, 2009), 0.8.1 (February 1, 2010), 1.0 (March 22, 2010), and 1.0.1 (September 3, 2012). CSL 1.0 was a backward-incompatible release, but styles in the 0.8.1 format can be automatically updated to the CSL 1.0 format.\n\nOn its release in 2006, Zotero became the first application to adopt CSL. In 2008 Mendeley was released with CSL support, and in 2011, Papers and Qiqqa gained support for CSL-based citation formatting.\n\n\nThe CSL project maintains a CSL 1.0 style repository, which contains over 9000 styles (more than 1700 unique styles).\n\n"}
{"id": "17077434", "url": "https://en.wikipedia.org/wiki?curid=17077434", "title": "Comparative Toxicogenomics Database", "text": "Comparative Toxicogenomics Database\n\nThe Comparative Toxicogenomics Database (CTD) is a public website and research tool launched in November 2004 that curates scientific data describing relationships between chemicals/drugs, genes/proteins, diseases, taxa, phenotypes, GO annotations, pathways, and interaction modules.\nThe database is maintained by the Department of Biological Sciences at North Carolina State University.\n\nThe Comparative Toxicogenomics Database (CTD) is a public website and research tool that curates scientific data describing relationships between chemicals, genes/proteins, diseases, taxa, phenotypes, GO annotations, pathways, and interaction modules, launched on November 12, 2004. \nThe database is maintained by the Department of Biological Sciences at North Carolina State University.\n\nOne of the primary goals of CTD is to advance the understanding of the effects of environmental chemicals on human health on the genetic level, a field called toxicogenomics.\n\nThe etiology of many chronic diseases involves interactions between environmental factors and genes that modulate important physiological processes. Chemicals are an important component of the environment. Conditions such as asthma, cancer, diabetes, hypertension, immunodeficiency, and Parkinson's disease are known to be influenced by the environment; however, the molecular mechanisms underlying these correlations are not well understood. CTD may help resolve these mechanisms. The most up-to-date extensive list of peer-reviewed scientific articles about CTD is available at their publications page\n\nCTD is a unique resource where biocurators read the scientific literature and manually curate four types of core data:\n\n\nBy integrating the above four data sets, CTD automatically constructs putative chemical-gene-phenotype-disease networks to illuminate molecular mechanisms underlying environmentally-influenced diseases.\n\nThese inferred relationships are statistically scored and ranked and can be used by scientists and computational biologists to generate and verify testable hypotheses about toxicogenomic mechanisms and how they relate to human health.\n\nUsers can search CTD to explore scientific data for chemicals, genes, diseases, or interactions between any of these three concepts. Currently, CTD integrates toxicogenomic data for vertebrates and invertebrates.\n\nCTD integrates data from or hyperlinks to these databases:\n\n"}
{"id": "917868", "url": "https://en.wikipedia.org/wiki?curid=917868", "title": "Comparative genomics", "text": "Comparative genomics\n\nComparative genomics is a field of biological research in which the genomic features of different organisms are compared. The genomic features may include the DNA sequence, genes, gene order, regulatory sequences, and other genomic structural landmarks. In this branch of genomics, whole or large parts of genomes resulting from genome projects are compared to study basic biological similarities and differences as well as evolutionary relationships between organisms. The major principle of comparative genomics is that common features of two organisms will often be encoded within the DNA that is evolutionarily conserved between them. Therefore, comparative genomic approaches start with making some form of alignment of genome sequences and looking for orthologous sequences (sequences that share a common ancestry) in the aligned genomes and checking to what extent those sequences are conserved. Based on these, genome and molecular evolution are inferred and this may in turn be put in the context of, for example, phenotypic evolution or population genetics.\n\nVirtually started as soon as the whole genomes of two organisms became available (that is, the genomes of the bacteria \"Haemophilus influenzae\" and \"Mycoplasma genitalium\") in 1995, comparative genomics is now a standard component of the analysis of every new genome sequence. With the explosion in the number of genome projects due to the advancements in DNA sequencing technologies, particularly the next-generation sequencing methods in late 2000s, this field has become more sophisticated, making it possible to deal with many genomes in a single study. Comparative genomics has revealed high levels of similarity between closely related organisms, such as humans and chimpanzees, and, more surprisingly, similarity between seemingly distantly related organisms, such as humans and the yeast \"Saccharomyces cerevisiae\". It has also showed the extreme diversity of the gene\ncomposition in different evolutionary lineages.\n\n\"See also\": History of genomics\n\nComparative genomics has a root in the comparison of virus genomes in the early 1980s. For example, small RNA viruses infecting animals (picornaviruses) and those infecting plants (cowpea mosaic virus) were compared and turned out to share significant sequence similarity and, in part, the order of their genes. In 1986, the first comparative genomic study at a larger scale was published, comparing the genomes of varicella-zoster virus and Epstein-Barr virus that contained more than 100 genes each.\n\nThe first complete genome sequence of a cellular organism, that of \"Haemophilus influenzae\" Rd, was published in 1995. The second genome sequencing paper was of the small parasitic bacterium \"Mycoplasma genitalium\" published in the same year. Starting from this paper, reports on new genomes inevitably became comparative-genomic studies.\n\nThe first high-resolution whole genome comparison system was developed in 1998 by Art Delcher, Simon Kasif and Steven Salzberg and applied to the comparison of entire highly related microbial organisms with their collaborators at the Institute for Genomic Research (TIGR). The system is called MUMMER and was described in a publication in Nucleic Acids Research in 1999. The system helps researchers to identify large rearrangements, single base mutations, reversals, tandem repeat expansions and other polymorphisms. In bacteria, MUMMER enables the identification of polymorphisms that are responsible for virulence, pathogenicity, and anti-biotic resistance. The system was also applied to the Minimal Organism Project at TIGR and subsequently to many other comparative genomics projects.\n\n\"Saccharomyces cerevisiae\", the baker's yeast, was the first eukaryote to have its complete genome sequence published in 1996. After the publication of the roundworm \"Caenorhabditis elegans\" genome in 1998 and together with the fruit fly \"Drosophila melanogaster\" genome in 2000, Gerald M. Rubin and his team published a paper titled \"Comparative Genomics of the Eukaryotes\", in which they compared the genomes of the eukaryotes \"D. melanogaster\", \"C. elegans\", and \"S. cerevisiae\", as well as the prokaryote \"H. influenzae\". At the same time, Bonnie Berger, Eric Lander, and their team published a paper on whole-genome comparison of human and mouse.\n\nWith the publication of the large genomes of vertebrates in the 2000s, including human, the Japanese pufferfish \"Takifugu rubripes\", and mouse, precomputed results of large genome comparisons have been released for downloading or for visualization in a genome browser. Instead of undertaking their own analyses, most biologists can access these large cross-species comparisons and avoid the impracticality caused by the size of the genomes.\n\nNext-generation sequencing methods, which were first introduced in 2007, have produced an enormous amount of genomic data and have allowed researchers to generate multiple (prokaryotic) draft genome sequences at once. These methods can also quickly uncover single-nucleotide polymorphisms, insertions and deletions by mapping unassembled reads against a well annotated reference genome, and thus provide a list of possible gene differences that may be the basis for any functional variation among strains.\n\nOne character of biology is evolution, evolutionary theory is also the theoretical foundation of comparative genomics, and at the same time the results of comparative genomics unprecedentedly enriched and developed the theory of evolution. When two or more of the genome sequence are compared, one can deduce the evolutionary relationships of the sequences in a phylogenetic tree. Based on a variety of biological genome data and the study of vertical and horizontal evolution processes, one can understand vital parts of the gene structure and its regulatory function.\n\nSimilarity of related genomes is the basis of comparative genomics. If two creatures have a recent common ancestor, the differences between the two species genomes are evolved from the ancestors’ genome. The closer the relationship between two organisms, the higher the similarities between their genomes. If there is close relationship between them, then their genome will display a linear behaviour (synteny), namely some or all of the genetic sequences are conserved. Thus, the genome sequences can be used to identify gene function, by analyzing their homology (sequence similarity) to genes of known function.\n\nOrthologous sequences are related sequences in different species: a gene exists in the original species, the species divided into two species, so genes in new species are orthologous to the sequence in the original species. Paralogous sequences are separated by gene cloning (gene duplication): if a particular gene in the genome is copied, then the copy of the two sequences is paralogous to the original gene. A pair of orthologous sequences is called orthologous pairs (orthologs), a pair of paralogous sequence is called collateral pairs (paralogs). Orthologous pairs usually have the same or similar function, which is not necessarily the case for collateral pairs. In collateral pairs, the sequences tend to evolve into having different functions.\nComparative genomics exploits both similarities and differences in the proteins, RNA, and regulatory regions of different organisms to infer how selection has acted upon these elements. Those elements that are responsible for similarities between different species should be conserved through time (stabilizing selection), while those elements responsible for differences among species should be divergent (positive selection). Finally, those elements that are unimportant to the evolutionary success of the organism will be unconserved (selection is neutral).\n\nOne of the important goals of the field is the identification of the mechanisms of eukaryotic genome evolution. It is however often complicated by the multiplicity of events that have taken place throughout the history of individual lineages, leaving only distorted and superimposed traces in the genome of each living organism. For this reason comparative genomics studies of small model organisms (for example the model Caenorhabditis elegans and closely related Caenorhabditis briggsae) are of great importance to advance our understanding of general mechanisms of evolution.\n\nComputational approaches to genome comparison have recently become a common research topic in computer science. A public collection of case studies and demonstrations is growing, ranging from whole genome comparisons to gene expression analysis. This has increased the introduction of different ideas, including concepts from systems and control, information theory, strings analysis and data mining. It is anticipated that computational approaches will become and remain a standard topic for research and teaching, while multiple courses will begin training students to be fluent in both topics.\n\nComputational tools for analyzing sequences and complete genomes are developing quickly due to the availability of large amount of genomic data. At the same time, comparative analysis tools are progressed and improved. In the challenges about these analyses, it is very important to visualize the comparative results.\n\nVisualization of sequence conservation is a tough task of comparative sequence analysis. As we know, it is highly inefficient to examine the alignment of long genomic regions manually. Internet-based genome browsers provide many useful tools for investigating genomic sequences due to integrating all sequence-based biological information on genomic regions. When we extract large amount of relevant biological data, they can be very easy to use and less time-consuming.\n\n\nAn advantage of using online tools is that these websites are being developed and updated constantly. There are many new settings and content can be used online to improve efficiency.\n\nAgriculture is a field that reaps the benefits of comparative genomics. Identifying the loci of advantageous genes is a key step in breeding crops that are optimized for greater yield, cost-efficiency, quality, and disease resistance. For example, one genome wide association study conducted on 517 rice landraces revealed 80 loci associated with several categories of agronomic performance, such as grain weight, amylose content, and drought tolerance. Many of the loci were previously uncharacterized. Not only is this methodology powerful, it is also quick. Previous methods of identifying loci associated with agronomic performance required several generations of carefully monitored breeding of parent strains, a time consuming effort that is unnecessary for comparative genomic studies.\n\nThe medical field also benefits from the study of comparative genomics. Vaccinology in particular has experienced useful advances in technology due to genomic approaches to problems. In an approach known as reverse vaccinology, researchers can discover candidate antigens for vaccine development by analyzing the genome of a pathogen or a family of pathogens. Applying a comparative genomics approach by analyzing the genomes of several related pathogens can lead to the development of vaccines that are multiprotective. A team of researchers employed such an approach to create a universal vaccine for Group B Streptococcus, a group of bacteria responsible for severe neonatal infection. Comparative genomics can also be used to generate specificity for vaccines against pathogens that are closely related to commensal microorganisms. For example, researchers used comparative genomic analysis of commensal and pathogenic strains of E. coli to identify pathogen specific genes as a basis for finding antigens that result in immune response against pathogenic strains but not commensal ones.\n\nComparative genomics also opens up new avenues in other areas of research. As DNA sequencing technology has become more accessible, the number of sequenced genomes has grown. With the increasing reservoir of available genomic data, the potency of comparative genomic inference has grown as well. A notable case of this increased potency is found in recent primate research. Comparative genomic methods have allowed researchers to gather information about genetic variation, differential gene expression, and evolutionary dynamics in primates that were indiscernible using previous data and methods. The Great Ape Genome Project used comparative genomic methods to investigate genetic variation with reference to the six great ape species, finding healthy levels of variation in their gene pool despite shrinking population size. Another study showed that patterns of DNA methylation, which are a known regulation mechanism for gene expression, differ in the prefrontal cortex of humans versus chimps, and implicated this difference in the evolutionary divergence of the two species.\n\n\n\n"}
{"id": "1826033", "url": "https://en.wikipedia.org/wiki?curid=1826033", "title": "Comparative neuropsychology", "text": "Comparative neuropsychology\n\nComparative neuropsychology refers to an approach used for understanding human brain functions. It involves the direct evaluation of clinical neurological populations by employing experimental methods originally developed for use with nonhuman animals.\n\nOver many decades of animal research, methods were perfected to study the effects of well-defined brain lesions on specific behaviors, and later the tasks were modified for human use. Generally the modifications involve changing the reward from food to money, but standard administration of the tasks in humans still involves minimal instructions, thus necessitating a degree of procedural learning in human and nonhuman animals alike.\n\nCurrently, comparative neuropsychology is used with neurological patients to link specific deficits with localized areas of the brain.\n\nThe comparative neuropsychological approach employs simple tasks that can be mastered without relying upon language skills. Precisely because these simple paradigms do not require linguistic strategies for solution, they are especially useful for working with patients whose language skills are compromised, or whose cognitive skills may be minimal.\n\nComparative neuropsychology contrasts with the traditional approach of using tasks that rely upon linguistic skills, and that were designed to study human cognition. Because important ambiguities about its heuristic value had not been addressed empirically, only recently has comparative neuropsychology become popular for implementation with brain-damaged patients.\n\nWithin the past decade, comparative neuropsychology has had prevalent use as a framework for comparing and contrasting the performances of disparate neurobehavioral populations on similar tasks.\n\nComparative neuropsychology involves the study of brain-behavior relationships by applying experimental paradigms, used extensively in animal laboratories, for testing human clinical populations. Popular paradigms include delayed reaction tasks, discrimination and reversal learning tasks, and matching- and nonmatching-to-sample. These tasks are used to test animals and relate them to human brain functioning. Such tasks were perfected on experimental animals having well defined brain lesions, and adapted for human neurological patients. The comparative aspects of such approach resides in the analogy between animals with brain lesions and human patients with lesions in homologous areas of the brain. One example is represented by the comparison between the brain of laboratory animals (primarily non human primates and mice) with the one of people with damages resulting from alcohol abuse.\n\nGeorge Ettlinger was one of the few who actively combined human and animal research, and he did so consistently throughout his scientific career. Ettinger work focused on the importance of the inferior temporal neocortex in visual discrimination learning and memory in macaque monkeys, and on the importance of ventral temporal lobe in vision. Ettinger animals models carried inferotemporal or latero-ventral prestriate ablation. In 1966 George Ettlinger, together with the psychologist Colin Blakemore and the neurosurgeon Murray Falconer, described the results of a study on correlation between pre-operative intelligence and the severity of mesial temporal sclerosis in temporal lobe specimens excised to treat intractable epilepsy. Such study It is known as a forerunner of what has become one of the potentially most interesting techniques for exploring the relationship between certain aspects of human memory and temporal lobe structures.\n\n\n"}
{"id": "1719952", "url": "https://en.wikipedia.org/wiki?curid=1719952", "title": "Comparative research", "text": "Comparative research\n\nComparative research is a research methodology in the social sciences that aims to make comparisons across different countries or cultures. A major problem in comparative research is that the data sets in different countries may not use the same categories, or define categories differently (for example by using different definitions of poverty).\n\nAs Moutsios argues, cross-cultural and comparative research should be seen as part of the scientific spirit that arose in Greece in the 6th century and the overall appreciation of knowledge and learning that was characteristic of the 5th century. In other words, it is part of the emergence of \"episteme\" and \"philo-sophia\", as a love for knowledge that is independent from material benefits. \"Episteme\", as a form and activity in the field of \"logos\", marked the break of cognitive closure and advanced empirical inquiry, logical argumentation and the search for truth. And the high esteem for intellectual activity gave rise to a genuine curiosity about other cultures – which has lain thereafter at the heart of comparative inquiry.\n\nMoreover, behind the Greek comparative gaze also was the philosophical and political questioning which characterised the life of the democratic \"polis\". Philosophical inquiry, from the Milesians down to the Sophists, questioned the representations and the cognitive traditions of their own people; the inquiry of the traditions of other peoples was, as Herodotus’ \"Histories\" demonstrate, an activity associated with the ethos of philosophical critique that characterised democratic life in Greece. Similarly, questioning of the Greek laws and institutions and its related values and practices (e.g. \"isegoria\" and \"parrhesia\"), as part of Greek politics, is associated with the effort of the first historians to reflect on home institutions through researching those of others.\n\nAccording also to Karl Deutsch, we have been using this form of investigation for over 2,000 years. Comparing things is essential to basic scientific and philosophic inquiry, which has been done for a long time. Most authors are more conservative in their estimate of how long comparative research has been with us. It is largely an empty debate over the definition of the tradition with those questioning whether comparing things counts as comparative research.\n\nTextbooks on this form of study were beginning to appear by the 1880s, but its rise to extreme popularity began after World War II. There are numerous reasons that comparative research has come to take a place of honour in the toolbox of the social scientist. Globalization has been a major factor, increasing the desire and possibility for educational exchanges and intellectual curiosity about other cultures. Information technology has enabled greater production of quantitative data for comparison, and international communications technology has facilitated this information to be easily spread.\n\nComparative research, simply put, is the act of comparing two or more things with a view to discovering something about one or all of the things being compared. This technique often utilizes multiple disciplines in one study. When it comes to method, the majority agreement is that there is no methodology peculiar to comparative research. The multidisciplinary approach is good for the flexibility it offers, yet comparative programs do have a case to answer against the call that their research lacks a \"seamless whole.\" \n\nThere are certainly methods that are far more common than others in comparative studies, however. Quantitative analysis is much more frequently pursued than qualitative, and this is seen by the majority of comparative studies which use quantitative data. The general method of comparing things is the same for comparative research as it is in our everyday practice of comparison. Like cases are treated alike, and different cases are treated differently; the extent of difference determines how differently cases are to be treated. If one is able to sufficiently distinguish two carry the research conclusions will not be very helpful. \n\nSecondary analysis of quantitative data is relatively widespread in comparative research, undoubtedly in part because of the cost of obtaining primary data for such large things as a country's policy environment. This study is generally aggregate data analysis. Comparing large quantities of data (especially government sourced) is prevalent. A typical method of comparing welfare states is to take balance of their levels of spending on social welfare.\n\nIn line with how a lot of theorizing has gone in the last century, comparative research does not tend to investigate \"grand theories,\" such as Marxism. It instead occupies itself with middle-range theories that do not purport to describe our social system in its entirety, but a subset of it. A good example of this is the common research program that looks for differences between two or more social systems, then looks at these differences in relation to some other variable coexisting in those societies to see if it is related. The classic case of this is Esping-Andersen's research on social welfare systems. He noticed there was a difference in types of social welfare systems, and compared them based on their level of decommodification of social welfare goods. He found that he was able to class welfare states into three types, based on their level of decommodification. He further theorized from this that decommodification was based on a combination of class coalitions and mobilization, and regime legacy. Here, Esping-Andersen is using comparative research: he takes many western countries and compares their level of decommodification, then develops a theory of the divergence based on his findings.\n\nComparative research can take many forms. Two key factors are space and time. Spatially, cross-national comparisons are by far the most common, although comparisons within countries, contrasting different areas, cultures or governments also subsist and are very constructive, especially in a country like New Zealand, where policy often changes depending on which race it pertains to. Recurrent interregional studies include comparing similar or different countries or sets of countries, comparing one's own country to others or to the whole world.\n\nThe historical comparative research involves comparing different time-frames. The two main choices within this model are comparing two stages in time (either snapshots or time-series), or just comparing the same thing over time, to see if a policy's effects differ over a stretch of time.\n\nWhen it comes to subject matter of comparative inquiries, many contend there is none unique to it. This may indeed be true, but a brief perusal of comparative endeavours reveals there are some topics more recurrent than others. Determining whether socioeconomic or political factors are more important in explaining government action is a familiar theme. In general, however, the only thing that is certain in comparative research issues is the existence of differences to be analysed.\n\n\n"}
{"id": "2466507", "url": "https://en.wikipedia.org/wiki?curid=2466507", "title": "Comparative sociology", "text": "Comparative sociology\n\nComparative sociology involves comparison of the social processes between nation states, or across different types of society (for example capitalist and socialist). There are two main approaches to comparative sociology: some seek similarity across different countries and cultures whereas others seek variance. For example, structural Marxists have attempted to use comparative methods to discover the general processes that underlie apparently different social orderings in different societies. The danger of this approach is that the different social contexts are overlooked in the search for supposed universal structures.\n\nOne sociologist who employed comparative methods to understand variance was Max Weber, whose studies attempted to show how differences between cultures explained the different social orderings that had emerged (see for example \"The Protestant Ethic and the Spirit of Capitalism\" and Sociology of religion).\n\nThere is some debate within sociology regarding whether the label of 'comparative' is suitable. Emile Durkheim argued in \"The Rules of Sociological Method\" (1895) that all sociological research was in fact comparative since social phenomenon are always held to be typical, representative or unique, all of which imply some sort of comparison. In this sense, all sociological analysis is comparative and it has been suggested that what is normally referred to as comparative research, may be more appropriately called cross-national research.\n\n"}
{"id": "10485277", "url": "https://en.wikipedia.org/wiki?curid=10485277", "title": "Comparison microscope", "text": "Comparison microscope\n\nA comparison microscope is a device used to analyze side-by-side specimens. It consists of two microscopes connected by an optical bridge, which results in a split view window enabling two separate objects to be viewed simultaneously. This avoids the observer having to rely on memory when comparing two objects under a conventional microscope.\n\nIn the 1920s forensic ballistics was waiting at its inception. In 1929, using a comparison microscope adapted for the purpose by Calvin Goddard and his partner Phillip Gravelle used similar techniques to absolve the Chicago Police Department of participation in the St. Valentine's Day Massacre.\n\n Philip O. Gravelle, a chemist, developed a comparison microscope for use in the identification of fired bullets and cartridge cases with the support and guidance of forensic ballistics pioneer Calvin Goddard. It was a significant advance in the science of firearms identification in forensic science. The firearm from which a bullet or cartridge case has been fired is identified by the comparison of the unique striae left on the bullet or cartridge case from the worn, machined metal of the barrel, breach block, extractor, or firing pin in the gun. It was Gravelle who mistrusted his memory. \"As long as he could inspect only one bullet at a time with his microscope, and had to keep the picture of it in his memory until he placed the comparison bullet under the microscope, scientific precision could not be attained. He therefore developed the comparison microscope and Goddard made it work.\" Calvin Goddard perfected the comparison microscope and subsequently popularized its use.Sir Sydney Smith also appreciated the idea, emphasizing its importance in forensic science and firearms identification. He took the comparison microscope to Scotland and introduced it to the European scientists for firearms identification and other forensic science needs.\n\nThe modern instrument has many optical, mechanical and electronic refinements, including fiber optic illumination, video capabilities, digital imaging, automatic exposure for conventional photography, etc. Despite this evolution, however, the basic tools and techniques have remained unchanged which are to determine whether or not ammunition components were fired by a single firearm based on unique and reproducible microscopic and class characteristics, or to reach a \"no conclusion\" result if insufficient marks are present.\n\nSince, ballistic identification has benefited from a long series of structural, scientific and technological advances, law enforcement agencies have established forensic laboratories and researchers have learned much more about how to match bullets and cartridge cases to the guns used to fire them, and comparison microscopes have become more sophisticated. By the end of the 1980s, ballistic identification was an established sub-specialty of forensic science.\n\nVisualization tools have also been developed to allows the firearms examiner to verify the degree of similarity between any two tool-marks in question. These are designed to simulate the operation of the comparison microscope but is capable of rendering a 2D view of the 3D surfaces in a manner similar to that of the conventional comparison microscope.\n\nThe prevalence of hand-gun related crime in the United States compared to most other developed countries provided the impetus for the development of the comparison microscope. As with most firearms, the fired ammunition components may acquire sufficient unique and reproducible microscopic marks to be identifiable as having been fired by a single firearm. Making these comparisons is correctly referred to as firearms identification, or sometimes called as \"ballistics\".\n\nHistorically, and currently, this forensic discipline ultimately requires a microscopic side-by-side comparison of fired bullets or cartridge cases, one pair at a time, by a forensic examiner to confirm or eliminate the two items as having been fired by a single firearm. For this purpose, the traditional tool of the firearms examiner has been what is often called the ballistics comparison microscope.\n\nThe interior of a gun's barrel is machined to have grooves (called rifling) that force the bullet to rotate as it travels along it. These grooves and their counterpart, called \"lands\" imprint groove and land impressions on the surface of the bullet. Together with these land and groove impressions, imperfections on the barrel surface are incidentally transferred to the bullet's surface. Because these imperfections are randomly generated, during manufacture or due to use, they are unique to each barrel. These patterns or imperfections, therefore, amount to a \"signature\" that each barrel imprints on each of the bullets fired through it. It is this \"signature\" on the bullets imparted due to the unique imperfections on the barrel that enable the validation and identification of bullets as having originated from a particular gun. Comparison microscope is used to analyze the matching of the microscopic impressions found on the surface of bullets and casings.\n\nWhen a firearm or a bullet or cartridge case are recovered from a crime scene, forensic examiners compare the ballistic fingerprint of the recovered bullet or cartridge case with the ballistic fingerprint of a second bullet or cartridge case test-fired from the recovered firearm. If the ballistic fingerprint on the test-fired bullet or cartridge case matches the ballistic fingerprint on the recovered bullet or cartridge case, investigators know that the recovered bullet or cartridge case was also fired from the recovered gun. A confirmed link between a specific firearm and a bullet or cartridge case recovered from a crime scene constitutes a valuable lead, because investigators may be able to connect the firearm to a person, who may then become either a suspect or a source of information helpful to the investigation.\n\nForensic innovator Calvin Goddard offered ballistic identification evidence in 1921 to help secure convictions of accused murderers and anarchists Nicola Sacco and Bartolomeo Vanzetti. On April 8, 1927, Sacco and Vanzetti were finally sentenced to death in the electric chair. A worldwide outcry arose and Governor Alvin T. Fuller finally agreed to postpone the executions and set up a committee to reconsider the case. By this time, firearms examination had improved considerably, and it was now known that a semi-automatic pistol could be traced by several different methods if both bullet and casing were recovered from the scene. Automatic pistols could now be traced by unique markings of the rifling on the bullet, by firing pin indentations on the fired primer, or by unique ejector and extractor marks on the casing. The committee appointed to review the case used the services of Calvin Goddard in 1927.\nGoddard used Philip Gravelle's newly invented comparison microscope and helixometer, a hollow, lighted magnifier probe used to inspect gun barrels, to make an examination of Sacco's .32 Colt, the bullet that killed Berardelli, and the spent casings recovered from the scene of the crime. In the presence of one of the defense experts, he fired a bullet from Sacco's gun into a wad of cotton and then put the ejected casing on the comparison microscope next to casings found at the scene. Then he looked at them carefully. The first two casings from the robbery did not match Sacco's gun, but the third one did. Even the defense expert agreed that the two cartridges had been fired from the same gun. The second original defense expert also concurred. The committee upheld the convictions.\nIn October 1961, ballistics tests were run with improved technology using Sacco's Colt automatic. The results confirmed that the bullet that killed the victim, Berardelli in 1920 came from the same .32 Colt Auto taken from the pistol in Sacco's possession. Subsequent investigations in 1983 also supported Goddard's findings.\n\nColonel Goddard was the key forensic expert in solving the 1929 St. Valentine's Day Massacre in which seven gangsters were killed by rival Al Capone mobsters dressed as Chicago police officers. It also led to the establishment of the United States' first independent criminological laboratory, which was located at Northwestern University and headed by Goddard. At this new lab, ballistics, fingerprinting, blood analysis and trace evidence were all brought under one roof.\nIn 1929, using a comparison microscope adapted for the ballistics comparison by his partner, Phillip Gravelle, Goddard used similar techniques to absolve the Chicago Police Department of participation in the St. Valentine's Day Massacre. The case of Sacco and Vanzetti, which took place in Bridgewater, Massachusetts, is responsible for popularizing the use of the comparison microscope for bullet comparison. Forensic expert Calvin Goddard's conclusions were upheld when the evidence was re-examined in 1961.\n\n\n"}
{"id": "31994535", "url": "https://en.wikipedia.org/wiki?curid=31994535", "title": "Comparison of Nazism and Stalinism", "text": "Comparison of Nazism and Stalinism\n\nA number of authors have carried out comparisons of Nazism and Stalinism, in which they have considered the similarities and differences of the two ideologies and political systems, what relationship existed between the two regimes, and why both of them came to prominence at the same time. During the 20th century, the comparison of Stalinism and Nazism was made on the topics of totalitarianism, ideology, and personality cult. Both regimes were seen in contrast to the liberal West, with an emphasis on the similarities between the two. The American political scientists Zbigniew Brzezinski, Hannah Arendt and Carl Friedrich and historian Robert Conquest were prominent advocates of applying the \"totalitarian\" concept to compare Nazism and Stalinism.\n\nOne of the first scholars to publish a comparative study of Nazi Germany and Stalin’s Soviet Union was Hannah Arendt. In her 1951 work, \"The Origins of Totalitarianism\", Arendt puts forward the idea of totalitarianism as a distinct type of political movement and form of government, which “differs essentially from other forms of political oppression known to us such as despotism, tyranny and dictatorship.” Furthermore, Arendt distinguishes between a totalitarian movement (such as a political party with totalitarian aims) and a totalitarian government. Not all totalitarian movements succeed in creating totalitarian governments once they gain power. In Arendt’s view, although many totalitarian movements existed in Europe in the 1920s and 1930s, only the governments of Stalin and Hitler succeeded in fully implementing their totalitarian aims. \n\nArendt traced the origin of totalitarian movements to the nineteenth century, focusing especially on antisemitism and imperialism. She emphasized the connection between the rise of European nation-states and the growth of antisemitism, which was due to the fact that the Jews represented an “inter-European, non-national element in a world of growing or existing nations.” Conspiracy theories abounded, and the Jews were accused of being part of various international schemes to ruin European nations. Small antisemitic political parties formed in response to this perceived Jewish threat, and, according to Arendt, these were the first political organizations in Europe that claimed to represent the interests of the whole nation as opposed to the interests of a class or other social group. The later totalitarian movements would copy or inherit this claim to speak for the whole nation, with the implication that any opposition to them constituted treason.\n\nEuropean imperialism of the nineteenth century also paved the way for totalitarianism, by legitimizing the concept of endless expansion. After Europeans had engaged in imperialist expansion on other continents, political movements developed which aimed to copy the methods of imperialism on the European continent itself. Arendt refers specifically to the “pan-movements” of pan-Germanism and pan-Slavism, which promised continental empires to nations that had little hope of overseas expansion. According to Arendt, “Nazism and Bolshevism owe more to Pan-Germanism and Pan-Slavism (respectively) than to any other ideology or political movement.”\n\nArendt argues that both the Nazi and Bolshevik movements “recruited their members from [a] mass of apparently indifferent people whom all other parties had given up,” and who “had reason to be equally hostile to all parties.” For this reason, totalitarian movements did not need to use debate or persuasion, and did not need to refute the arguments of the other parties. Their target audience did not have to be persuaded to despise the other parties or the democratic system, because it consisted of people who already despised mainstream politics. As a result, totalitarian movements were free to use violence and terror against their opponents without fear that this might alienate their own supporters. Instead of arguing against their opponents, they adopted deterministic views of human behavior and presented opposing ideas as “originating in deep natural, social, or psychological sources beyond the control of the individual and therefore beyond the power of reason.” The Nazis in particular, during the years before their rise to power, engaged in “killing small socialist functionaries or influential members of opposing parties” both as a means to intimidate opponents and as a means of demonstrating to their supporters that they were a party of action, “different from the ‘idle talkers’ of other parties.”\n\nTotalitarian governments make extensive use of propaganda, and are often characterized by having a strong distinction between what they tell their own supporters and the propaganda they produce for others. Arendt distinguishes these two categories as \"indoctrination\" and \"propaganda\". Indoctrination consists of the message that a totalitarian government promotes internally, to the members of the ruling party and that segment of the population which supports the government. Propaganda consists of the message that a totalitarian government seeks to promote in the outside world, and also among those parts of its own society which may not support the government. Thus, “the necessities for propaganda are always dictated by the outside world,” while the opportunities for indoctrination depend on “the totalitarian governments’ isolation and security from outside interference.” \n\nThe type of indoctrination used by the Soviets and the Nazis was characterized by claims of “scientific” truth, and appeals to “objective laws of nature.” Both movements took a deterministic view of human society and claimed that their ideologies were based on scientific discoveries regarding race (in the case of the Nazis) or the forces governing human history (in the case of the Soviets). Arendt identifies this as being in certain ways similar to modern advertising, in which companies claim that scientific research shows their products to be superior, but more generally she argues that it is an extreme version of “that obsession with science which has characterized the Western world since the rise of mathematics and physics in the sixteenth century.” By their use of pseudoscience as the main justification for their actions, Nazism and Stalinism are distinguished from earlier historical despotic regimes, who appealed instead to religion or sometimes did not try to justify themselves at all. According to Arendt, totalitarian governments did not merely use these appeals to supposed scientific laws as propaganda to manipulate others. Rather, totalitarian leaders like Hitler and Stalin genuinely believed that they were acting in accordance with immutable natural laws, to such an extent that they were willing to sacrifice the self-interest of their regimes for the sake of enacting those supposed laws. For instance, the Nazis treated the inhabitants of occupied territories with extreme brutality and planned to depopulate Eastern Europe in order to make way for colonists from the German “master race,” despite the fact that this actively harmed their war effort. Stalin repeatedly purged the Communist Party of people who deviated even slightly from the party line, even when this weakened the party or the Soviet government, because he believed that they represented the interests of “dying classes” and their demise was historically inevitable.\n\nArendt also identifies the central importance of an all-powerful leader in totalitarian movements. As in other areas, she distinguishes between totalitarian leaders (such as Hitler and Stalin) and non-totalitarian dictators or autocratic leaders. The totalitarian leader does not rise to power by personally using violence or through any special organizational skills, but rather by controlling appointments of personnel within the party, so that all other prominent party members owe their positions to him. With loyalty to the leader becoming the primary criterion for promotion, ambitious party members compete with each other in trying to express their loyalty, and a cult of personality develops around the leader. Even when the leader is not particularly competent and the members of his inner circle are aware of his deficiencies, they remain committed to him out of fear that without him the entire power structure would collapse.\n\nOnce in power, according to Arendt, totalitarian movements face a major dilemma: they built their support on the basis of anger against the status quo and on impossible or dishonest promises, but now they have become the new status quo and are expected to carry out their promises. They deal with this problem by engaging in a constant struggle against external and internal enemies, real or imagined, so as to enable them to say that, in a sense, they have not yet gained the power they need to fulfill their promises. According to Arendt, totalitarian governments must be constantly fighting enemies in order to survive. This explains their apparently irrational behavior, for example when Hitler continued to make territorial demands even after he was offered everything he asked for in the Munich Agreement, or when Stalin unleashed the Great Terror despite the fact that he faced no significant internal opposition.\n\nArendt points out the widespread use of concentration camps by totalitarian governments, arguing that they are the most important manifestation of the need to find enemies to fight against, and are therefore “more essential to the preservation of the regime’s power than any of its other institutions.” Although forced labor was commonly imposed on inmates of concentration camps, Arendt argues that their primary purpose was not any kind of material gain for the regime: “The only permanent economic function of the camps has been the financing of their own supervisory apparatus; thus from the economic point of view the concentration camps exist mostly for their own sake.” The Nazis in particular carried this to the point of “open anti-utility,” by expending large sums of money, resources and manpower – during a war – for the purpose of building and staffing extermination camps and transporting people to them. This sets apart the concentration camps of totalitarian regimes from older human institutions that bear some similarity to them, such as slavery. Slaves were abused and killed for the sake of profit; concentration camp inmates were abused and killed because a totalitarian government needed to justify its existence. Finally, Arendt points out that concentration camps under both Hitler and Stalin included large numbers of inmates who were innocent of any crime – not only in the ordinary sense of the word, but even by the standards of the regimes themselves. That is to say, most of the inmates had not actually committed any action against the regime.\n\nThroughout her analysis, Arendt emphasized the modernity and novelty of the governmental structures set up by Stalin and Hitler, arguing that they represented “an entirely new form of government” which is likely to manifest itself again in various other forms in the future. She also cautioned against the belief that future totalitarian movements would necessarily share the ideological foundations of Nazism or Stalinism, writing that “all ideologies contain totalitarian elements.”\n\nThe totalitarian paradigm in the comparative study of Nazi Germany and the Soviet Union was further developed by Carl Friedrich and Zbigniew Brzezinski, who wrote extensively on this topic both individually and in collaboration. Similar to Hannah Arendt, they state that “totalitarian dictatorship is a new phenomenon; there has never been anything quite like it before.” Friedrich and Brzezinski classify totalitarian dictatorship as a type of autocracy, but argue that it is different in important ways from most other historical autocracies. In particular, it is distinguished by a reliance on modern technology and mass legitimation. Unlike Arendt, Friedrich and Brzezinski apply the notion of totalitarian dictatorship not only to the regimes of Hitler and Stalin, but also to the USSR throughout its entire existence, as well as the regime of Benito Mussolini in Italy and the People’s Republic of China under Mao Zedong.\n\nCarl Friedrich noted that the “possibility of equating the dictatorship of Stalin in the Soviet Union and that of Hitler in Germany” has been a deeply controversial topic and a subject of debate almost from the beginning of those dictatorships. Various other aspects of the two regimes have also been the subject of intense scholarly debate, such as whether Nazi and Stalinist ideologies were genuinely believed and pursued by the respective governments, or whether the ideologies were merely convenient justifications for dictatorial rule. Friedrich himself argues in favor of the former view.\n\nFriedrich and Brzezinski argue that Nazism and Stalinism are not only similar to each other, but also represent a continuation or a return to the tradition of European absolute monarchy on certain levels. In the absolute monarchies of the seventeenth and eighteenth centuries, the monarch ultimately held all decisional power, and was considered accountable only to God. In Stalinism and Nazism, the leader likewise held all real power, and was considered accountable only to various intangible entities such as “the people”, “the masses” or “the Volk.” Thus the common feature of autocracies – whether monarchical or totalitarian – is the concentration of power in the hands of a leader who cannot be held accountable by any legal mechanisms, and who is supposed to be the embodiment of the will of an abstract entity. Friedrich and Brzezinski also identify other features common to all autocracies, such as “the oscillation between tight and loose control.” The regime alternates between periods of intense repression and periods of relative freedom, often represented by different leaders. This depends in part on the personal character of different leaders, but Friedrich and Brzezinski believe that there is also an underlying political cycle, in which rising discontent leads to increased repression up to the point at which the opposition is eliminated, then controls are relaxed until the next time that popular dissatisfaction begins to grow.\n\nThus, placing Stalinism and Nazism within the broader historical tradition of autocratic government, Friedrich and Brzezinski hold that “totalitarian dictatorship, in a sense, is the adaptation of autocracy to twentieth-century industrial society.” However, at the same time, they insist that totalitarian dictatorship is a “\"novel\" type of autocracy” and argue that twentieth century totalitarian regimes (such as those of Hitler and Stalin) had more in common with each other than with any other form of government, including historical autocracies of the past. Totalitarianism can only exist after the creation of modern technology, because such technology is essential for propaganda, for surveillance of the population, and for the operation of a secret police. Furthermore, when speaking of the differences and similarities between fascist and communist regimes, Friedrich and Brzezinski insist that the two kinds of totalitarian governments are “basically alike” but “not wholly alike” – they are more similar to each other than to other forms of government, but they are not the same. Among the major differences between them, Friedrich and Brzezinski identify in particular the fact that communists seek “the world revolution of the proletariat,” while fascists wish to “establish the imperial predominance of a particular nation or race.” \n\nIn terms of the similarities between Nazism and Stalinism, Friedrich lists five main aspects that they hold in common: First, an official ideology that is supposed to be followed by all members of society, at least passively, and which promises to serve as a perfect guide towards some ultimate goal. Second, a single political party, composed of the most enthusiastic supporters of the official ideology, representing an elite group within society (no more than 10 percent of the population), and organized along strictly regimented lines. Third, “a technologically conditioned near-complete monopoly of control of all means of effective armed combat” in the hands of the party or its representatives. Fourth, a similar monopoly held by the party over the mass media and all technological forms of communication. Fifth, “a system of terroristic police control” that is not only used to defend the regime against real enemies, but also to persecute various groups of people who are only suspected of being enemies or who may potentially become enemies in the future.\n\nTwo first pillars of any totalitarian government, according to Friedrich and Brzezinski, are the dictator and the Party. The dictator, whether Stalin, Hitler or Mussolini, holds supreme power. Friedrich and Brzezinski explicitly reject the claim that the Party, or any other institution, could provide a significant counterweight to the power of the dictator in Nazism or Stalinism. The dictator needs the Party in order to be able to rule, so he may be careful not to make decisions that would go directly against the wishes of other leading Party members, but ultimate authority rests with him and not with them. Like Arendt, Friedrich and Brzezinski also identify the cult of personality surrounding the leader as an essential element of a totalitarian dictatorship, and reference Stalin’s personality cult in particular. They also draw attention to the fact that Hitler and Stalin were expected to provide ideological direction for their governments and not merely practical leadership. Friedrich and Brzezinski write that “unlike military dictators in the past, but like certain types of primitive chieftains, the totalitarian dictator is both ruler and high priest.” That is to say, he not only governs, but also provides the principles on which his government is to be based. This is partly due to the way that totalitarian governments arise. They come about when a militant ideological movement seizes power, so the first leader of a totalitarian government is usually the ideologue who built the movement that seized power, and subsequent leaders try to emulate him.\n\nThe totalitarian dictator needs loyal lieutenants to carry out his orders faithfully and with a reasonable degree of efficiency. Friedrich and Brzezinski identify parallels between the men in Hitler and Stalin’s entourage, arguing that both dictators used similar people to perform similar tasks. Thus, for example, Martin Bormann and Georgy Malenkov were both capable administrators and bureaucrats, while Heinrich Himmler and Lavrentiy Beria were ruthless secret police chiefs responsible for suppressing any potential challenge to the dictator’s power. Both Hitler and Stalin promoted rivalry and distrust among their lieutenants so as to ensure that none of them would become powerful enough to challenge the dictator himself. This is the cause of an important weakness of the totalitarian regimes: the problem of succession. Friedrich points out that neither the Nazi nor the Stalinist government ever established any official line of succession or any mechanism to decide who would replace the dictator after his death. The dictator, being the venerated “father of the people,” was regarded as irreplaceable. There could never be any heir apparent, because such an heir would have been a threat to the power of the dictator while he was alive. Thus the dictator’s inevitable death would always leave behind a major power vacuum and cause a political crisis. In the case of the Nazi regime, since Hitler died mere days before the final defeat of Germany in the war, this never became a major issue. In the case of the USSR, Stalin’s death led to a prolonged power struggle.\n\nFriedrich and Brzezinski also identify key similarities between the Nazi and Stalinist political parties, which set them apart from other types of political parties. Both the Nazi Party and the CPSU under Stalin had very strict membership requirements and did not accept members on the basis of mere agreement with the Party’s ideology and goals. Rather, they strictly tested potential members, in a manner similar to exclusive clubs, and often engaged in political purges of the membership, expelling large numbers of people from their ranks (and sometimes arresting and executing those expelled, such as in the Great Purge or the Night of the Long Knives). Thus, the totalitarian party cultivates the idea that to be a member is a privilege which needs to be earned, and total obedience to the leader is required in order to maintain this privilege. While both Nazism and Stalinism required party members to display such total loyalty in practice, they differed in the way they dealt with it in theory. Nazism openly proclaimed the hierarchical ideal of absolute obedience to the Führer as one of its key ideological principles (the \"Führerprinzip\"). Stalinism, meanwhile, denied that it did anything similar, and claimed instead to uphold democratic principles, with the Party Congress (made up of elected delegates) supposedly being the highest authority. However, Stalinist elections typically featured only a single candidate, and the Party Congress met very rarely and simply approved Stalin’s decisions. Thus, regardless of the differences in their underlying ideological claims, the Nazi and Stalinist parties were organized in practice along similar lines, with a rigid hierarchy and centralized leadership.\n\nEach totalitarian party and dictator is supported by a specific totalitarian ideology. Friedrich and Brzezinski argue, in agreement with Arendt, that Nazi and Stalinist leaders really believed in their respective ideologies and did not merely use them as tools to gain power. Several major policies, such as the Stalinist collectivization of agriculture or the Nazi “final solution”, cannot be explained by anything other than a genuine commitment to achieve ideological goals, even at great cost. The ideologies were different and their goals were different, but what they had in common was a utopian commitment to reshaping the world, and a determination to fight by any means necessary against a real or imagined enemy. This stereotyped enemy could be described as “the fat rich Jew or the Jewish Bolshevik” for the Nazis, or “the war-mongering, atom-bomb-wielding American Wallstreeter” for the Soviets.\n\nAccording to Friedrich and Brzezinski, the most important difference between Nazi and Stalinist ideology lies in the degree of universality involved. Stalinism, and communist ideology in general, is universal in its appeal and addresses itself to all the “workers of the world.” Nazism, on the other hand, and fascist ideology in general, can only address itself to one particular race or nation – the “master race” that is destined to dominate all others. Therefore, “in communism social justice appears to be the ultimate value, unless it be the classless society that is its essential condition; in fascism, the highest value is dominion, eventually world dominion, and the strong and pure nation-race is \"its\" essential condition, as seen by its ideology.” This means that fascist or Nazi movements from different countries will be natural enemies, rather than natural allies, as they each seek to extend the dominion of their own nation at the expense of others. Friedrich and Brzezinski see this as a weakness inherent in fascist and Nazi ideology, while communist universalism is a source of ideological strength for Stalinism.\n\nFriedrich and Brzezinski also draw attention to the symbols used by Nazis and Stalinists to represent themselves. The Soviet Union adopted the hammer and sickle, a newly-created symbol, “invented by the leaders of the movement and pointing to the future.” Meanwhile, Nazi Germany used the swastika, “a ritual symbol of uncertain origin, quite common in primitive societies.” Thus, one is trying to project itself as being oriented towards a radically new future, while the other is appealing to a mythical heroic past.\n\nTotalitarian dictatorships maintain themselves in power through the use of propaganda and terror, which Friedrich and Brzezinski believe to be closely connected. Terror may be enforced with arrests and executions of dissenters, but it can also take more subtle forms, such as the threat of losing one’s job, social stigma and defamation. “Terror” can refer to any widespread method used to intimidate people into submission as a matter of daily life. According to Friedrich and Brzezinski, the most effective terror is invisible to the people it affects. They simply develop a habit of acting in a conformist manner and not questioning authority, without necessarily being aware that this is what they are doing. Thus, terror creates a society dominated by apparent consensus, where the vast majority of the population appears to support the government. Propaganda is then used to maintain this appearance of popular consent. \n\nTotalitarian propaganda is one of the features that distinguishes totalitarian regimes as modern forms of government and separates them from older autocracies, since a totalitarian government holds complete control over all means of communication (not only public communication such as the mass media, but also private communication such as letters and telephone calls, which are strictly monitored). The methods of propaganda were very similar in the Stalinist USSR and in Nazi Germany. Both Joseph Goebbels and Soviet propagandists sought to demonize their enemies and present a picture of a united people standing behind its leader to confront foreign threats. In both cases there was no attempt to convey complex ideological nuances to the masses, with the message being instead about a simplistic struggle between good and evil. Both Nazi and Stalinist regimes produced two very different sets of propaganda – one for internal consumption and one for potential sympathizers in other countries. And both regimes would sometimes radically change their propaganda line as they made peace with a former enemy or got into a war with a former ally. Yet, paradoxically, a totalitarian government’s complete control over communications renders that government highly misinformed. With no way for anyone to express criticism, the dictator has no way of knowing how much support he actually has among the general populace. With all government policies always declared successful in propaganda, officials are unable to determine what actually worked and what didn’t. Both Stalinism and Nazism suffered from this problem, especially during the war between them. As the war turned against Germany, there was growing opposition to Hitler’s rule, including within the ranks of the military, but Hitler was never aware of this until it was too late (see: 20 July plot). In 1948, during the early days of the Berlin Blockade, the Soviet leadership apparently believed that the population of West Berlin was sympathetic to Soviet Communism and that they would request to join the Soviet zone. Given enough time, the gap between real public opinion and what the totalitarian government believes about public opinion can grow so wide that the government is no longer able to even produce effective propaganda, because it does not know what the people actually think and so it does not know what to tell them. Friedrich and Brzezinski refer to this as the “ritualization of propaganda”: the totalitarian regime continues to produce propaganda as a political ritual, with little real impact on public opinion.\n\nThe totalitarian use of mass arrests, executions and concentration camps – also noted by Arendt – was analyzed at length by Friedrich and Brzezinski. They hold that “totalitarian terror maintains, in institutionalized form, the civil war that originally produced the totalitarian movement and by means of which the regime is able to proceed with its program, first of social disintegration and then of social reconstruction.” Both Stalinism and Nazism saw themselves as engaging in a life-or-death struggle against implacable enemies. But to declare that the struggle had been won would have meant to declare that most of the totalitarian features of the government were no longer needed. A secret police force, for instance, has no reason to exist if there are no dangerous traitors who need to be found. Thus the struggle, or “civil war” against internal enemies, must be institutionalized and must continue indefinitely. In the Stalinist USSR, the repressive apparatus was eventually turned against members of the Communist Party itself in the Great Purge and the show trials that accompanied it. Nazism, by contrast, had a much shorter lifespan in power, and Nazi terror generally maintained an outward focus, with the extermination of the Jews always given top priority. The Nazis did not turn inward towards purging their own party except in a limited way on two occasions (the Night of the Long Knives and the aftermath of the 20 July plot). \n\nThe peak of totalitarian terror was reached with the Nazi concentration camps. These ranged from labor camps to extermination camps, and they are described by Friedrich and Brzezinski as aiming to “eliminate all actual, potential, and imagined enemies of the regime.” As the field of Holocaust studies was still in its early stages at the time of their writing, they do not describe the conditions in detail, but do refer to the camps as involving “extreme viciousness.” They also compare these camps with the Soviet Gulag system, and highlight the use of concentration camps as a method of punishment and execution by Nazi and Stalinist regimes alike. However, unlike Hannah Arendt, who held that the Gulag camps served no economic purpose, Friedrich and Brzezinski argue that they provided an important source of cheap labor for the Stalinist economy.\n\nThe comparative study of Nazism and Stalinism was carried further by other groups of scholars, such as Moshe Lewin and Ian Kershaw together with their collaborators. Writing after the dissolution of the USSR, Lewin and Kershaw take a longer historical perspective and regard Nazism and Stalinism not so much as examples of a new type of society (like Arendt, Friedrich and Brzezinski did), but more as historical “anomalies” – unusual deviations from the typical path of development that most industrial societies are expected to follow. Therefore, the task of comparing Nazism and Stalinism is, to them, a task of explaining why Germany and Russia (along with other countries) deviated from the historical norm. At the outset, Lewin and Kershaw identify similarities between the historical situations in Germany and Russia prior to the First World War and during that war. Both countries were ruled by authoritarian monarchies, who were under pressure to make concessions to popular demands. Both countries had “powerful bureaucracies and strong military traditions.” Both had “powerful landowning classes,” while also being in the process of rapid industrialization and modernization. And both countries had expansionist foreign policies with a particular interest in Central and Eastern Europe. Lewin and Kershaw do not claim that these factors made Stalinism or Nazism inevitable, but rather that they help to explain why the Stalinist and Nazi regimes developed similar features.\n\nIan Kershaw admitted that Stalinism and Nazism are comparable in “the nature and extent of their inhumanity,” but noted that the two regimes were different in a number of aspects Lewin and Kershaw question the usefulness of grouping the Stalinist and Nazi regimes together under a “totalitarian” category, saying that it remains an open question whether the similarities between them are greater or smaller than the differences. In particular, they criticize what they see as the ideologically-motivated attempt to determine which regime killed more people, saying that apologists of each regime are trying to defend their side by claiming the other was responsible for more deaths.\n\nLewin and Kershaw place the cult of personality at the center of their comparison of Nazism and Stalinism, writing that both regimes “represented a new genre of political system centred upon the artificial construct of a leadership cult – the ‘heroic myth’ of the ‘great leader’, no longer a king or emperor but a ‘man of the people.” With regard to Stalinism, they emphasize its bureaucratic character, and its “merging of the most modern with the most archaic traits” by combining modern technology and the latest methods of administration and propaganda with the ancient practice of arbitrary rule by a single man. They compare this with the Prussian military tradition in Germany, which had been called “bureaucratic absolutism” in the eighteenth century, and which played a significant role in the organization of the Nazi state in the twentieth century.\n\nKershaw agrees with Mommsen that there was a fundamental difference between Nazism and Stalinism regarding the importance of the leader. Stalinism had an absolute leader, but he was not essential. He could be replaced by another. Nazism, on the other hand, was a “classic charismatic leadership movement,” defined entirely by its leader. Stalinism had an ideology which existed independently of Stalin. But for Nazism, “Hitler \"was\" ideological orthodoxy” – Nazi ideals were by definition whatever Hitler said they were. In Stalinism, the bureaucratic apparatus was the foundation of the system, while in Nazism, the person of the leader was the foundation.\n\nMoshe Lewin also focuses on the comparison between the personality cults of Hitler and Stalin, and their respective roles in Nazi Germany and the Soviet Union. He refers to them as the “Hitler myth” and the “Stalin myth,” and argues that they served different functions within their two regimes. The function of the “Hitler myth” was to legitimize Nazi rule. The function of the “Stalin myth” was to legitimize not Soviet rule itself, but Stalin’s leadership within the Party. Stalin’s personality cult existed precisely because Stalin knew that he was replaceable, and feared that he might be replaced, and so needed to bolster his authority as much as possible. While the “Hitler myth” was essential to Nazi Germany, the “Stalin myth” was essential only to Stalin, not to the Soviet Union itself.\n\nTogether with fellow historian Hans Mommsen, Lewin argues that the Stalinist and Nazi regimes featured an “intrinsic structural contradiction” which led to “inherent self-destructiveness”: they depended on a highly organized state bureaucracy which was trying to set up complex rules and procedures for every aspect of life, yet this bureaucracy was under the complete personal control of a despot who made policy decisions as he saw fit, routinely changing his mind on major issues, without any regard for the rules and institutions which his own bureaucracy had set up. The bureaucracy and the leader needed each other, but also undermined each other with their different priorities. Mommsen sees this as being a much greater problem in Nazi Germany than in Stalin’s Soviet Union, as the Nazis inherited large parts of the traditional German bureaucracy, while the Soviets largely built their own bureaucracy from the ground up. He argues that many of the irrational features of the Nazi regime – such as wasting resources on exterminating undesirable populations instead of using those resources in the war effort – were caused by the dysfunction of the Nazi state rather than by fanatical commitment to Nazi ideology. In accordance with the Führerprinzip, all decisional power in the Nazi state ultimately rested with Hitler. But Hitler often issued only vague and general directives, forcing other Nazi leaders lower down in the hierarchy to guess what precisely the Führer wanted. This confusion produced competition between Nazi officials, as each of them attempted to prove that he was a more dedicated Nazi than his rivals, by engaging in ever more extreme policies. This competition to please Hitler was, according to Mommsen, the real cause of Nazi irrationality. Hitler was aware of it, and deliberately encouraged it out of a “social-darwinist conviction that the best man would ultimately prevail.” Mommsen argues that this represents a structural difference between the regimes of Hitler and Stalin. In spite of its purges, Stalin’s regime was more effective in building a stable bureaucracy, such that it was possible for the system to sustain itself and continue even without Stalin. The Nazi regime, on the other hand, was much more personalized and depended entirely on Hitler, being unable to build any lasting institutions.\n\nKershaw also saw major personal differences between Stalin and Hitler and their respective styles of rule. He describes Stalin as “a committee man, chief oligarch, man of the machine” and a “creature of his party,” who came to power only thanks to his party and his ability to manipulate the levers of power within that party. Hitler, by contrast, came to power based on his charisma and mass appeal, and in the Nazi regime it was the leader that created the party instead of the other way around. According to Kershaw, “Stalin was a highly interventionist dictator, sending a stream of letters and directives determining or interfering with policy,” while Hitler “was a non-interventionist dictator as far as government administration was concerned,” preferring to involve himself in military affairs and plans for conquest rather than the daily routine of government work, and giving only broad verbal instructions to his subordinates regarding civilian affairs, which they were expected to translate into policy. Furthermore, although both regimes featured all-pervasive cults of personality, there was a qualitative difference between those cults. Stalin’s personality cult was “superimposed upon the Marxist-Leninist ideology and Communist Party,” and could be abandoned (or replaced with a personality cult around some other leader) without major changes to the regime. On the other hand, “the ‘Hitler myth’ was structurally indispensable to, in fact the very basis of, and scarcely distinguishable from, the Nazi Movement and its \"Weltanschauung\".” The belief in the person of Adolf Hitler as the unique savior of the German nation was the very foundation of Nazism, to such an extent that Nazism found it impossible to even imagine a successor to Hitler. Thus, in Kershaw’s analysis, Stalinism was a fundamentally bureaucratic system while Nazism was the embodiment of “charismatic authority” as described by Max Weber. Stalinism could exist without its leader. Nazism could not. \n\nThe topic of comparisons between Nazism and Stalinism was also studied in the 1990s and 2000s by historians Henry Rousso, Nicolas Werth and Philippe Burrin.\n\nRousso defends the work of Carl Friedrich by pointing out that Friedrich himself had only said that Stalinism and Nazism were comparable, not that they were identical. Rousso also argues that the popularity of the concept of totalitarianism (the way that large numbers of people have come to routinely refer to certain governments as “totalitarian”) should be seen as evidence that the concept is useful, that it really describes a specific type of government which is different from other dictatorships. At the same time, however, Rousso notes that the concept of totalitarianism is descriptive rather than analytical: the regimes described as totalitarian do not have a common origin and did not arise in similar ways. Nazism is unique among totalitarian regimes in having taken power in “a country endowed with an advanced industrial economy and with a system of political democracy (and an even older political pluralism).” All other examples of totalitarianism (including the Stalinist regime) took power, according to Rousso, “in an agrarian economy, in a poor society without a tradition of political pluralism, not to mention democracy, and where diverse forms of tyranny had traditionally prevailed.” He sees this as a weakness of the concept of totalitarianism, because it merely describes the similarities between Stalinism and Nazism without dealing with the very different ways they came to power. On the other hand, Rousso agrees with Hannah Arendt that “totalitarian regimes constitute something new in regard to classical tyranny, authoritarian regimes, or other forms of ancient and medieval dictatorships,” and he says that the main strength of the concept of totalitarianism is the way it highlights this inherent novelty of the regimes involved.\n\nNicolas Werth and Philippe Burrin have worked together on comparative assessments of Stalinism and Nazism, with Werth covering the Stalinist regime and Burrin covering Nazi Germany. One of the topics they have studied is the question of how much power the dictator really held in the two regimes. Werth identifies two main historiographical approaches in the study of the Stalinist regime: Those who emphasize the power and control exercised by Joseph Stalin himself, attributing most of the actions of the Soviet government to deliberate plans and decisions made by him, and those who argue that Stalin had no pre-determined course of action in mind, that he was reacting to events as they unfolded, and that the Soviet bureaucracy had its own agenda which often differed from Stalin’s wishes. Werth regards these as two mistaken extremes, one making Stalin seem all-powerful, the other making him seem like a weak dictator. But he believes that the competing perspectives are useful in drawing attention to the tension between two different forms of organization in the Stalinist USSR: an “administrative system of command,” bureaucratic and resistant to change but effective in running the Soviet state, and the strategy of “running the country in a crudely despotic way by Stalin and his small cadre of directors.” Thus, Werth agrees with Lewin that there was an inherent conflict between the priorities of the Soviet bureaucracy and Stalin’s accumulation of absolute power in his own hands. According to Werth, it was this unresolved and unstated conflict that led to the Great Purge and to the use of terror by Stalin’s regime against its own party and state cadres.\n\nIn studying similar issues with regard to the Nazi regime, Philippe Burrin draws attention to the debate between the “Intentionalist” and “Functionalist” schools of thought, which dealt with the question of whether the Nazi regime represented an extension of Hitler’s autocratic will, faithfully obeying his wishes, or whether it was an essentially chaotic and uncontrollable system that functioned on its own with little direct input from the Führer. Like Kershaw and Lewin, Burrin says that the relationship between the leader and his party’s ideology was different in Nazism compared to Stalinism: “One can rightly state that Nazism cannot be dissociated from Hitlerism, something that is difficult to affirm for Bolshevism and Stalinism.” Unlike Stalin, who inherited an existing system with an existing ideology and presented himself as the heir to the Leninist political tradition, Hitler created both his movement and its ideology by himself, claiming to be “someone sent by Providence, a Messiah whom the German people had been expecting for centuries, even for two thousand years, as Heinrich Himmler enjoyed saying.” Thus, there could be no real conflict between the Party and the leader in Nazi Germany, because the Nazi Party’s entire reason for existence was to support and follow Hitler. However, there was a potential for division between the leader and the state bureaucracy, due to the way that Nazism came to power – as part of an alliance with traditional conservative elites, industrialists, and the army. Unlike the USSR, Nazi Germany did not build its own state, but rather inherited the state machinery of the previous government. This provided the Nazis with an immediate supply of capable and experienced managers and military commanders, but on the other hand it also meant that the Nazi regime had to rely on the cooperation of people who had not been Nazis prior to Hitler’s rise to power, and whose loyalty was questionable. It was only during the war, when Nazi Germany conquered large territories and had to create Nazi administrations for them, that brand new Nazi bureaucracies were created without any input or participation from traditional German elites. This produced a surprising difference between Nazism and Stalinism: When the Stalinist USSR conquered territory, it created smaller copies of itself and installed them as the governments of the occupied countries. When Nazi Germany conquered territory, on the other hand, it did not attempt to create copies of the German government back home. Instead, it experimented with different power structures and policies, often reflecting a “far more ample Nazification of society than what the balance of power authorized in the Reich.”\n\nAnother major topic investigated by Werth and Burrin was the violence and terror employed by the regimes of Hitler and Stalin. Werth reports that the Stalinist USSR underwent an “extraordinary brutalization of the relations between state and society” for the purpose of rapid modernization and industrialization, to “gain one hundred years in one decade, and to metamorphose the country into a great industrial power.” This transformation was accomplished at the cost of massive violence and a sociopolitical regression into what Werth calls “military-feudal exploitation.” The types of violence employed by the Stalinist regime included loss of civil rights, mass arrests, deportations of entire ethnic groups from one part of the USSR to another, forced labor in the Gulag, mass executions (especially during the Great Terror of 1937-38), and most of all the great famine of 1932-33, known as the Holodomor. All levels of Soviet society were affected by Stalinist repression, from the top to the bottom. At the top, high-ranking members of the Communist Party were arrested and executed under the claim that they had plotted against Stalin (and in some cases they were forced to confess to imaginary crimes in show trials). At the bottom, the peasantry suffered the Holodomor famine (especially in Ukraine), and even outside of the famine years they were faced with very high grain quotas.\n\nWerth identifies four categories of people that became the targets of Stalinist violence in the USSR. He lists them from smallest to largest. The first and smallest group consisted of many of Stalin’s former comrades-in-arms, who had participated in the revolution and were known as “Old Bolsheviks.” They were dangerous to Stalin because they had known him before his rise to power and could expose the many false claims made by his personality cult. The second group consisted of mid-level Communist Party officials, who were subject to mass arrests and executions in the late 1930s, particularly during the Great Purge. Eliminating them served a dual purpose: It helped Stalin to centralize power in the Kremlin (as opposed to regional centers), and it also provided him with “corrupt officials” that he could blame for earlier repressions and unpopular policies. Werth draws parallels between this and the old Tsarist tradition of blaming “bad bureaucrats” – rather than the Tsar – for unpopular government actions. The third group was made up of ordinary citizens from all walks of life who resorted to petty crime in order to provide for themselves in the face of worsening living standards (for example by taking home some wheat from the fields or tools from the factory). This type of petty crime became very widespread, and was often punished as if it were intentional sabotage motivated by political opposition to the USSR. The fourth and largest category consisted of ethnic groups that were subject to deportation, famine, or arbitrary arrests under the suspicion of being collectively disloyal to Stalin or to the Soviet state. This included the Holodomor famine directed at the Ukrainians, the deportation of ethnic groups suspected of pro-German sympathies (such as the Volga Germans, the Crimean Tatars, the Chechens and others), and eventually also persecution of ethnic Jews, especially as Stalin grew increasingly antisemitic near the end of his life.\n\nBurrin’s study of violence carried out by the Nazi regime begins with the observation that “violence is at the heart of Nazism,” and that Nazi violence is “established as a doctrine and exalted in speech.” This marks a point of difference between Nazism and Stalinism, according to Burrin. In Stalinism, there was a gulf between ideology and reality when it came to violence. The Soviet regime continuously denied that it was repressive, proclaimed itself a defender of peace, and sought to conceal all the evidence to the contrary. In Nazism, on the other hand, “doctrine and reality were fused from the start.” Nazism not only practiced violent repression and war, but advocated it in principle as well, considering war to be a positive force in human civilization and openly seeking ”living space” and the domination of the European continent by ethnic Germans.\n\nBurrin identifies three motivations for Nazi violence: political repression, exclusion and social repression, and racial politics. The first of these, political repression, is common in many dictatorships. The Nazis aimed to eliminate their real or imagined political opponents, first in the Reich and later in the occupied territories during the war. Some of these opponents were executed, while others were imprisoned in concentration camps. The first targets of political repression, immediately after Hitler’s rise to power in 1933, were the parties of the Left in general and the Communists in particular. Then, after the mid-1930s, repression was extended to members of the clergy, and later to the conservative opposition as well (especially after the failed attempt to assassinate Hitler in 1944). The death penalty was used on a wide scale, even before the war. During the war, political repression was greatly expanded both inside Germany and especially in the newly occupied territories. Political prisoners in the concentration camps numbered only about 25,000 at the beginning of the war. By January 1945 they had swelled to 714,211 – most of them non-Germans accused of plotting against the Reich.\n\nThe second type of Nazi violence, motivated by exclusion and social repression, was the violence aimed at purging German society of people whose lifestyle was considered incompatible with the social norms of the Nazi regime (even if the people involved were racially pure and able-bodied). Such people were divided into two categories: homosexuals and “asocials.” The “asocials” were only vaguely defined, and included “Gypsies, tramps, beggars, prostitutes, alcoholics, the jobless who refused any employment, and those who left their work frequently or for no reason.”\n\nThe third and final type of Nazi violence, by far the most extensive, was violence motivated by Nazi racial policies. This was aimed both inward, to cleanse the “Aryan race” of “degenerate” elements and life unworthy of life, as well as outward, to seek the extermination of “inferior races”. Germans considered physically or mentally unfit were among the first victims. One of the first laws of the Nazi regime mandated the forced sterilization of people suffering from physical handicaps or who had psychiatric conditions deemed to be hereditary. Later, sterilization was replaced by murder of the mentally ill and of people with severe disabilities, as part of a “euthanasia” program called Aktion T4. Burrin notes that this served no practical political purpose – the people being murdered could not have possibly been political opponents of the regime – so the motivation was purely a matter of racial ideology. The most systematic and by far the most large-scale acts of Nazi violence, however, were directed at “racially inferior” non-German populations. As laid out in \"Generalplan Ost\", the Nazis wished to eliminate most of the Slavic populations of Eastern Europe, partly through deportation and partly through murder, in order to secure land for ethnic German settlement and colonization. But even more urgently, the Nazis wished to exterminate the Jews of Europe, whom they regarded as the implacable racial enemy of the Germans. This culminated in the Holocaust, the Nazi genocide of the Jews. Unlike in the case of all other target populations, the Jews were to be exterminated completely, with no individual exceptions for any reason.\n\nIn \"Beyond Totalitarianism: Stalinism and Nazism Compared\", editors Michael Geyer and Sheila Fitzpatrick disputed the concept of totalitarianism, noting that the term entered political discourse first as a term of self-description by the Italian Fascists and was only later used as a framework to compare Nazi Germany with the Soviet Union. They argued that the totalitarian states were not as monolithic or as ideology-driven as they seemed. Geyer and Fitzpatrick describe Nazi Germany and the Stalinist USSR as “immensely powerful, threatening, and contagious dictatorships” who “shook the world in their antagonism.” Without calling them totalitarian, they identified their common features, including genocide, an all-powerful party, a charismatic leader, and pervasive invasion of privacy. However, they argue that Stalinism and Nazism did not represent a new and unique type of government, but rather that they can be placed in the broader context of the turn to dictatorship in Europe in the interwar period. The reason they appear extraordinary is because they were the “most prominent, most hard-headed, and most violent” of the European dictatorships of the 20th century. They are comparable because of their “shock and awe” and sheer ruthlessness, but underneath superficial similarities they were fundamentally different and that “when it comes to one-on-one comparison, the two societies and regimes may as well have hailed from different worlds.”\n\nAccording to Geyer and Fitzpatrick, the similarities between Nazism and Stalinism stem from the fact that they were both “ideology driven” and sought to subordinate all aspects of life to their respective ideologies. The differences stem from the fact that their ideologies were opposed to each other and regarded each other as enemies. Another major difference is that Stalin created a stable and long-lasting regime, while Nazi Germany had a “short-lived, explosive nature.” Notably, the stable state created by Stalinism was based on an entirely new elite, while Nazism, despite having the support of the traditional elite, failed to achieve stability.\n\nHowever, the two regimes did borrow ideas from one another, especially regarding propaganda techniques (most of all in architecture and cinema), but also in terms of state surveillance and antisemitism. At the same time, they both vigorously denied borrowing anything from each other. While their methods of propaganda were similar, the content was different. For instance, Soviet wartime propaganda revolved around the idea of resisting imperial aggression, while Nazi propaganda was about wars of racial conquest. Geyer and Fitzpatrick also take note of the fact that both Stalinism and Nazism sought to create a New Man, an “entirely modern, illiberal, and self-fashioned personage,” even though they had different visions about what being a “New Man” would mean.\n\nAmong the other authors contributing to the volume edited by Geyer and Fitzpatrick, David Hoffmann and Annette Timm discuss biopolitics and the pro-natalist policies of the Nazi and Stalinist regimes. Both governments were highly concerned over low fertility rates in their respective populations, and applied extensive and intrusive social engineering techniques to increase the number of births. Reproductive policies in the Soviet Union and Nazi Germany were administered through their health care systems—both regimes saw health care as a key pillar to their designs to develop a new society. While the Soviet Union had to design a public health care system from scratch, Nazi Germany built upon the pre-existing public health care system in Germany that had existed since 1883, when Otto von Bismarck's legislation had created the world's first national public health care program. The Nazis centralized the German health care system in order to enforce Nazi ideological components upon it, and replaced existing voluntary and government welfare agencies with new ones that were devoted to racial hygiene and other components of Nazi ideology.\n\nThe Nazi and Stalinist attempt to control family size was not unique, as many other European states practiced eugenics at this time, and the Stalinist and Nazi ideals were vastly different. In fact, they had more in common with third parties than with each other: Nazi Germany’s policies were rather similar to those in Scandinavia at the time, while the USSR’s policies resembled those in Catholic countries.The common point between Nazi and Stalinist practices was the connection of reproduction policies with the ideological goals of the state — \"part of the project of a rational, hypermodern vision for the re-organization of society\". There were nevertheless substantial differences between the two regimes' approaches. Stalin's Soviet Union never officially supported eugenics as the Nazis did—the Soviet government called eugenics a \"fascist science\"—although there were in fact Soviet eugenicists. The two regimes also had different approaches to the relationship between family and paid labor—Nazism promoted the male single-breadwinner family while Stalinism promoted the dual-wage-earner household.\n\nIn another contribution to the same volume, Christian Gerlach and Nicolas Werth discuss the topic of mass violence, and the way that it was used by both Stalinism and Nazism. Both Stalin's Soviet Union and Nazi Germany were violent societies where mass violence was accepted by the state, such as in the Great Terror of 1937 to 1938 in the Soviet Union and the Holocaust in Nazi Germany and its occupied territories in World War II.\n\nBoth the Stalinist Soviet Union and Nazi Germany utilized internment camps led by agents of the state – the NKVD in the Soviet Union and the SS in Nazi Germany. They also both engaged in violence against minorities based on xenophobia – the xenophobic violence of the Nazis was outspoken but rationalized as being against \"asocial\" elements while the xenophobic violence of the Stalinists was disguised as being against \"anti-soviet\", \"counter-revolutionary\" and \"socially harmful\" elements – a term which often targeted diaspora nationalities. The Stalinist Soviet Union established \"special settlements\" where the \"socially harmful\" or \"socially dangerous\" who included ex-convicts, criminals, vagrants, the disenfranchised and \"declassed elements\" were expelled to. These \"special settlements\" were largely in Siberia, the far north, the Urals, or other inhospitable territories. In July 1933, the Soviet Union made a mass arrest of 5000 Romani people effectively on the basis of their ethnicity, who were deported that month to the \"special settlements\" in Western Siberia. In 1935, the Soviet Union arrested 160,000 homeless people and juvenile delinquents and sent many of them to NKVD labor colonies where they did forced labor.\n\nThe Nazi regime was founded upon a racialist view of politics and envisioned the deportation or extermination of the majority of the population of Eastern Europe in order to open up “living space” for ethnic German settlers. This was mainly intended to be carried out after an eventual German victory in the war, but steps had already started being taken while the war was still ongoing. For instance, by the end of 1942, the Nazis had deported 365,000 Poles and Jews from their original homes in western Poland (now German-annexed) and into the General Government. A further 194,000 Poles were internally displaced (not deported to another territory but expelled from their homes). The Nazis had also deported 100,000 persons from Alsace, Lorraine, and Luxembourg, as well as 54,000 Slovenians.\n\nStalinism in practice in the Soviet Union pursued ethnic deportations from the 1930s to the early 1950s, with a total of 3 million Soviet citizens being subjected to ethnic-based resettlement. The first major ethnic deportation took place from December 1932 to January 1933, during which some 60,000 Kuban Cossacks were collectively criminally charged as a whole with association with resistance to socialism and affiliation with Ukrainian nationalism. From 1935 to 1936, the Soviet Union deported Soviet citizens of Polish and German origins living in the western districts of Ukraine, and Soviet citizens of Finnish origins living on the Finland-Soviet Union border. These deportations from 1935 to 1936 affected tens of thousands of families. From September to October 1937, Soviet authorities deported the Korean minority from its Far Eastern region that bordered on Japanese-controlled Korea. Soviet authorities claimed the territory was \"rich soil for the Japanese to till\" – implying a Soviet suspicion that the Koreans could potentially join forces with the Japanese to unite the land with Japanese-held Korea. Over 170,000 Koreans were deported to remote parts of Soviet Central Asia from September to October 1937. These ethnically-based deportations reflected a new trend in Stalinist policy, a \"Soviet xenophobia\" based on ideological grounds that suspected that these people were susceptible to foreign influence, and which was also based on a resurgent Russian nationalism.\n\nAfter Nazi Germany declared war on the Soviet Union in 1941, the Soviet Union initiated another major round of ethnic deportations. The first group targeted were Soviet Germans. Between September 1941 and February 1942, 900,000 people – over 70 percent of the entire Soviet German community – were deported to Kazakhstan and Siberia in mass operations. A second wave of mass deportations took place between November 1943 and May 1944, in which Soviet authorities expelled six ethnic groups (the Balkars, Chechens, Crimean Tatars, Ingush, Karachai, and Kalmyks) that together numbered 900,000. There were also smaller-scale operations involving ethnic cleansing of diaspora minorities during and after World War II, in which tens of thousands of Crimean Bulgarians, Greeks, Iranians, Khemshils, Kurds, and Meskhetian Turks were deported from the Black Sea and Transcaucasian border regions.\n\nTwo ethnic groups that were specifically targeted for persecution by Stalin's Soviet Union were the Chechens and the Ingush. Unlike the other nationalities that could be suspected of connection to foreign states which shared their ethnic background, the Chechens and the Ingush were completely indigenous people of the Soviet Union. Rather than being accused of collaboration with foreign enemies, these two ethnic groups were considered to have cultures which did not fit in with Soviet culture – such as accusing Chechens of being associated with “banditism” – and the authorities claimed that the Soviet Union had to intervene in order to “remake” and “reform” these cultures. In practice this meant heavily armed punitive operations carried out against Chechen “bandits” that failed to achieve forced assimilation, culminating in an ethnic cleansing operation in 1944, which involved the arrests and deportation of over 500,000 Chechens and Ingush from the Caucasus to Central Asia and Kazakhstan. The deportations of the Chechens and Ingush also involved the outright massacre of thousands of people, and severe conditions placed upon the deportees – they were put in unsealed train cars, with little to no food for a four-week journey during which many died from hunger and exhaustion.\n\nThe main difference between Nazi and Stalinist deportations was in their purpose: while Nazi Germany sought ethnic cleansing to allow settlement by Germans into the cleansed territory, Stalin's Soviet Union pursued ethnic cleansing in order to remove minorities from strategically important areas.\n\nOther historians and political scientists have also made comparisons between Nazism and Stalinism as part of their work.\n\nStanley Payne, in his work on fascism, said that although the Nazi Party was ideologically opposed to communism, Adolf Hitler and other Nazi leaders frequently expressed recognition that only in Soviet Russia were their revolutionary and ideological counterparts to be found. Both placed a major emphasis on creating a \"party-army,\" with the regular armed forces controlled by the party. In the case of the Soviet Union this was done through the political commissars, while Nazi Germany introduced a roughly equivalent leadership role for \"National Socialist Guidance Officers\" in 1943.\n\nFrançois Furet, in his work on communism, noted that Hitler personally admired Soviet leader Joseph Stalin, and on numerous occasions publicly praised Stalin for seeking to purify the Communist Party of the Soviet Union of Jewish influences, especially by purging Jewish communists such as Leon Trotsky, Grigory Zinoviev, Lev Kamenev and Karl Radek.\n\nRichard Pipes draws attention to Stalin and his antisemitism in a parallel with Nazi antisemitism. He notes that soon after the 1917 October Revolution, the Soviet Union undertook practices to break up Jewish culture, religion and language. In the fall of 1918, the Soviet Communist Party set up the Jewish section Yevsektsiya, with a stated mission of “destruction of traditional Jewish life, the Zionist movement, and Hebrew culture.” By 1919, the Bolsheviks began to confiscate Jewish properties, Hebrew schools, libraries, books, and synagogues in accordance with newly imposed anti-religious laws, turning their buildings into \"Communist centers, clubs or restaurants.\" After Joseph Stalin rose to power, antisemitism continued to be endemic throughout Russia, although official Soviet policy condemned it. On August 12, 1952, Stalin's personal antisemitism became more visible, as he ordered the execution of the most prominent Yiddish authors in the Soviet Union, in an event known as the \"Night of the Murdered Poets\". Shortly before his death, Stalin also organized the anti-Semitic campaign known as the Doctors' plot.\n\nA number of research institutions are focusing on the analysis of fascism/Nazism and Stalinism/communism, and the comparative approach, including the Hannah Arendt Institute for the Research on Totalitarianism in Germany, the Institute for the Study of Totalitarian Regimes in the Czech Republic and the Institute of National Remembrance in Poland.\n\nIn comparing the deaths caused by both Stalin and Hitler's policies, some historians have asserted that archival evidence released after the collapse of the USSR confirms that Stalin did not kill more people than Hitler. American historian Timothy D. Snyder, for example, after assessing such data, says that while the Nazi regime killed approximately 11 million non-combatants (which rises to above 12 million if \"foreseeable deaths from deportation, hunger, and sentences in concentration camps are included\"), Stalin's deliberately killed about 6 million (rising to 9 million if foreseeable deaths arising from policies are taken into account). Australian historian and archival researcher Stephen G. Wheatcroft posits that \"The Stalinist regime was consequently responsible for about a million purposive killings, and through its criminal neglect and irresponsibility it was probably responsible for the premature deaths of about another two million more victims amongst the repressed population, i.e. in the camps, colonies, prisons, exile, in transit and in the POW camps for Germans. These are clearly much lower figures than those for whom Hitler's regime was responsible.\" Wheatcroft also says that, unlike Hitler, Stalin's \"purposive killings\" fit more closely into the category of \"execution\" than \"murder\", given he thought the accused were indeed guilty of crimes against the state and insisted on documentation, whereas Hitler simply wanted to kill Jews and communists because of who they were, and insisted on no documentation and was indifferent at even a pretence of legality for these actions.\n\nKristen R. Ghodsee, an ethnographer of post-Cold War Eastern Europe, contends that the efforts to institutionalize the \"double genocide thesis\", or the moral equivalence between the Nazi Holocaust (race murder) and the victims of communism (class murder), and in particular the recent push at the beginning of the global financial crisis for commemoration of the latter in Europe, can be seen as the response by economic and political elites to fears of a leftist resurgence in the face of devastated economies and extreme inequalities in both the East and West as the result of neoliberal capitalism. She notes that any discussion of the achievements under communism, including literacy, education, women’s rights, and social security is usually silenced, and any discourse on the subject of communism is focused almost exclusively on Stalin's crimes and the \"double genocide thesis\", an intellectual paradigm summed up as such: \"1) any move towards redistribution and away from a completely free market is seen as communist; 2) anything communist inevitably leads to class murder; and 3) class murder is the moral equivalent of the Holocaust.\" By linking all leftist and socialist ideals to the excesses of Stalinism, Ghodsee concludes, the elites in the West hope to discredit and marginalize all political ideologies that could \"threaten the primacy of private property and free markets.\"\n\nThe comparison of Stalinism and Nazism remains a neglected field of academic study.\n\nThe comparison of Nazism and Stalinism has long provoked political controversy, and it led to the historians' dispute within Germany in the 1980s.\n\nIn the 1920s, the Social Democratic Party of Germany (SPD), under the leadership of Chancellor Hermann Müller, adopted the view that \"red equals brown\", i.e. that the communists and Nazis posed an equal danger to liberal democracy. In 1930, Kurt Schumacher said that the two movements enabled each other. He argued that the Communist Party of Germany, which was staunchly Stalinist, were \"red-painted Nazis.\" This comparison was mirrored by the social fascism theory advanced by the Soviet government and the Comintern (including the Communist Party of Germany), which accused social democracy of enabling fascism and went as far as to call social democrats \"social fascists.\" After the 1939 Molotov–Ribbentrop Pact was announced, \"The New York Times\" published an editorial arguing that \"Hitlerism is brown communism, Stalinism is red fascism.\"\n\nMarxist theories of fascism have seen fascism as a form of reaction to socialism and a feature of capitalism. Several modern historians have tried to pay more attention to the economic, political and ideological differences between these two regimes than to their similarities. \n\nThe 2008 Prague Declaration on European Conscience and Communism, initiated by the Czech government and signed by figures such as Václav Havel, called for \"a common approach regarding crimes of totalitarian regimes, inter alia Communist regimes\" and for\nThe Communist Party of Greece opposes the Prague Declaration and has criticized \"the new escalation of the anti-communist hysteria led by the EU council, the European Commission and the political staff of the bourgeois class in the European Parliament.\" The Communist Party of Britain opined that the Prague Declaration \"is a rehash of the persistent attempts by reactionary historians to equate Soviet Communism and Hitlerite Fascism, echoing the old slanders of British authors George Orwell and Robert Conquest.\"\n\nThe 2008 documentary film \"The Soviet Story\", commissioned by the Union for Europe of the Nations group in the European Parliament, published archival records which listed thousands of German Jews who were arrested in the Soviet Union by the NKVD (People's Commissariat for Internal Affairs) from 1937 to 1941 and handed over to Gestapo or SS officials in Germany. These German Jews had originally sought asylum in the USSR. The documentary film accuses Stalin's regime of being an accomplice in Hitler's Holocaust by arresting these asylum seekers and sending them back to Germany.\n\nSince 2009, the European Union has officially commemorated the European Day of Remembrance for Victims of Stalinism and Nazism, proclaimed by the European Parliament in 2008 and endorsed by the Organization for Security and Co-operation in Europe in 2009, and officially known as the Black Ribbon Day in some countries (including Canada).\n\nThe former President of the European Parliament and Christian Democratic Union member, Hans-Gert Pöttering, argued that \"both totalitarian systems (Stalinism and Nazism) are comparable and terrible.\"\n\nIn some Eastern European countries the denial of both Nazi and Communist crimes has been explicitly outlawed, and Czech foreign minister Karel Schwarzenberg has argued that \"there is a fundamental concern here that totalitarian systems be measured by the same standard.\" However, the European Commission rejected calls for similar EU-wide legislation, due to the lack of consensus among member states.\n\nA statement adopted by Russia's legislature said that comparisons of Nazism and Stalinism are \"blasphemous towards all of the anti-fascist movement veterans, Holocaust victims, concentration camp prisoners and tens of millions of people ... who sacrificed their lives for the sake of the fight against the Nazis' anti-human racial theory.\"\n\nBritish journalist Seumas Milne posits that the impact of the post-Cold War narrative that Stalin and Hitler were twin evils, and therefore Communism is as monstrous as Nazism, \"has been to relativise the unique crimes of Nazism, bury those of colonialism and feed the idea that any attempt at radical social change will always lead to suffering, killing and failure.\"\n\n\n"}
{"id": "32639223", "url": "https://en.wikipedia.org/wiki?curid=32639223", "title": "Comparison of online charity donation services in the United Kingdom", "text": "Comparison of online charity donation services in the United Kingdom\n\nThe page is a comparison of notable online charity donation services in the UK.\n\nThe table below gives examples of the various transaction fees for a £10 donation using each organisation, assuming they claim back the tax for the charity using gift aid. (Charities may also be charged set-up fees and monthly fees as detailed above.)\n\n\n"}
{"id": "506639", "url": "https://en.wikipedia.org/wiki?curid=506639", "title": "Compendium", "text": "Compendium\n\nA compendium (plural: compendia) is a concise compilation of a body of knowledge. A compendium may summarize a larger work. In most cases the body of knowledge will concern a specific field of human interest or endeavour (for example: hydrogeology, logology, ichthyology, phytosociology or myrmecology), while a general encyclopedia can be referred to as a \"compendium of all human knowledge\".\n\nThe word compendium arrives from the Latin word \"compenso\", meaning \"to weigh together or balance\". The 21st century has seen the rise of democratized, online compendia in various fields.\nAn example would be the \"Compendium of the Catechism of the Catholic Church\", a concise 598-question-and-answer book which summarises the teachings of the Catholic Faith and Morals.\n\nThe Bible is another example of a compendium—a group of many writings of the prophets and apostles over a period of time, whose books are put together to form the Old Testament and the New Testament.\n\nSome well known literary figures have written their own compendium. An example would be Alexandre Dumas, author of The Three Musketeers, and an enthusiastic gourmand. His compendium on food titled \"From Absinthe to Zest\" serves as an alphabet for food lovers.\n\nThe bestiary, popular in the Middle Ages, is another example of a compendium. Bestiaries cataloged animals and facts about natural history and were particularly popular in England and France around the 12th century.\n\n\n"}
{"id": "590473", "url": "https://en.wikipedia.org/wiki?curid=590473", "title": "Contraction (grammar)", "text": "Contraction (grammar)\n\nA contraction is a shortened version of the written and spoken forms of a word, syllable, or word group, created by omission of internal letters and sounds.\n\nIn linguistic analysis, contractions should not be confused with crasis, abbreviations nor acronyms (including initialisms), with which they share some semantic and phonetic functions, though all three are connoted by the term \"abbreviation\" in loose parlance. Contraction is also distinguished from clipping, where beginnings and endings are omitted.\n\nThe definition overlaps with the term portmanteau (a linguistic \"blend\"), but a distinction can be made between a portmanteau and a contraction by noting that contractions are formed from words that would otherwise appear together in sequence, such as \"do\" and \"not\", whereas a portmanteau word is formed by combining two or more existing words that all relate to a singular concept which the portmanteau describes.\n\nEnglish has a number of contractions, mostly involving the elision of a vowel (which is replaced by an apostrophe in writing), as in \"I'm\" for \"I am\", and sometimes other changes as well, as in \"won't\" for \"will not\" or \"ain't\" for \"am not\". These contractions are commonly used in speech and in informal writing, though tend to be avoided in more formal writing (with limited exceptions, such as the mandatory form of \"o'clock\").\n\nThe main contractions are listed in the following table (for more explanation see English auxiliaries and contractions).\nSome other simplified pronunciations of common word groups, which can often equally be described as cases of elision, may also be considered (non-standard) contractions (not enshrined into the written standard language, but frequently expressed in written form anyway), such as \"wanna\" for \"want to\", \"gonna\" for \"going to\", \"y'all\" for \"you all\", \"ya'll\" for \"ya all\" in the Southern United States and others common forms in colloquial speech.\n\nIn subject–auxiliary inversion, the contracted negative forms behave as if they were auxiliaries themselves, changing place with the subject. For example, the interrogative form of \"He won't go\" is \"Won't he go\", whereas the uncontracted equivalent is \"Will he not go?\", with \"not\" following the subject.\n\nContractions exist in Classical Chinese, some of which are used in modern Chinese.\nContractions also appear in Cantonese, for example, 乜嘢 and 咩.\n\nThe French language has a variety of contractions, similar to English but mandatory, as in \"C'est la vie\" (\"That's life\"), where \"c'est\" stands for \"ce\" + \"est\" (\"that is\"). The formation of these contractions is called elision.\n\nIn general, any monosyllabic word ending in \"e caduc\" (schwa) will contract if the following word begins with a vowel, \"h\" or \"y\" (as \"h\" is silent and absorbed by the sound of the succeeding vowel; \"y\" sounds like \"i\"). In addition to \"ce\" → \"c'-\" (demonstrative pronoun \"that\"), these words are \"que\" → \"qu'-\" (conjunction, relative pronoun, or interrogative pronoun \"that\"), \"ne\" → \"n'-\" (\"not\"), \"se\" → \"s'-\" (\"himself\", \"herself\", \"itself\", \"oneself\" before a verb), \"je\" → \"j'-\" (\"I\"), \"me\" → \"m'-\" (\"me\" before a verb), \"te\" → \"t'- \" (informal singular \"you\" before a verb), \"le\" or \"la\" → \"l'-\" (\"the\"; or \"he/she\", \"it\" before a verb or after an imperative verb and before the word \"y\" or \"en\"), and \"de\" → \"d'-\" (\"of\"). Unlike with English contractions, however, these contractions are mandatory: one would never say (or write) \"*ce est\" or \"*que elle\".\n\n\"Moi\" (\"myself\") and \"toi\" (informal \"yourself\") mandatorily contract to \"m'-\" and \"t'-\" respectively after an imperative verb and before the word \"y\" or \"en\".\n\nIt is also mandatory to avoid the repetition of a sound when the conjunction \"si\" (\"if\") is followed by \"il\" (\"he\", \"it\") or \"ils\" (\"they\"), which begin with the same vowel sound \"i\": \"*si il\" → \"s'il\" (\"if it\", if he\"); \"*si ils\" → \"s'ils\" (\"if they\").\n\nCertain prepositions are also mandatorily merged with masculine and plural direct articles: \"au\" for \"à le\", \"aux\" for \"à les\", \"du\" for \"de le\", and \"des\" for \"de les\". However, the contraction of \"cela\" (demonstrative pronoun \"that\") to \"ça\" is optional and informal.\n\nIn informal speech, a personal pronoun may sometimes be contracted onto a following verb. For example, \"je ne sais pas\" (, \"I don't know\") may be pronounced roughly \"chais pas\" (), with the \"ne\" being completely elided and the of \"je\" being mixed with the of \"sais\". It is also common in informal contexts to contract \"tu\" to \"t'-\" before a vowel, e.g., \"t'as mangé\" for \"tu as mangé\".\n\nIn Modern Hebrew, the prepositional prefixes -בְּ /bə-/ 'in' and -לְ /lə-/ 'to' contract with the definite article prefix -ה (/ha-/) to form the prefixes -ב /ba/ 'in the' and -ל /la/ 'to the'. In colloquial Israeli Hebrew, the preposition את (/ʔet/), which indicates a definite direct object, and the definite article prefix -ה (/ha-/) are often contracted to 'ת (/ta-/) when the former immediately precedes the latter. Thus ראיתי את הכלב (/ʁaˈʔiti ʔet haˈkelev/, \"I saw the dog\") may become ראיתי ת'כלב (/ʁaˈʔiti taˈkelev/).\n\nIn Italian, prepositions merge with direct articles in predictable ways. The prepositions \"a\", \"da\", \"di\", \"in\", \"su\", \"con\" and \"per\" combine with the various forms of the definite article, namely \"il\", \"lo\", \"la\", \"l',\" \"i\", \"gli\", \"gl',\" and \"le\".\n\n\nThe words \"ci\" and \"è\" (form of \"essere\", to be) and the words \"vi\" and \"è\" are contracted into \"c'è\" and \"v'è\" (both meaning \"there is\").\n\nThe words \"dove\" and any word that begins with \"e\" are contracted into one single, deleting the e of the principal word, dove (dov'). Equally \"come\" does be made so.\nAs well other words may be contracted the same these two, like \"quale\", and other ones, etcetera.\n\nSpanish has two mandatory phonetic contractions between prepositions and articles: \"al\" (to the) for \"a el\", and \"del\" (of the) for \"de el\" (not to be confused with \"a él\", meaning \"to him\", and \"de él\", meaning \"his\" or, more literally, \"of him\").\n\nOther contractions were common in writing until the 17th century, the most usual being \"de\" + personal and demonstrative pronouns: \"destas\" for \"de estas\" (of these, fem.), \"daquel\" for \"de aquel\" (of that, masc.), \"dél\" for \"de él\" (of him) etc.; and the feminine article before words beginning with \"a-\": \"l'alma\" for \"la alma\", now \"el alma\" (the soul). Several sets of demonstrative pronouns originated as contractions of \"aquí\" (here) + pronoun, or pronoun + \"otro/a\" (other): \"aqueste\", \"aqueso\", \"estotro\" etc. The modern \"aquel\" (that, masc.) is the only survivor of the first pattern; the personal pronouns \"nosotros\" (we) and \"vosotros\" (pl. you) are remnants of the second. In medieval texts unstressed words very often appear contracted: \"todol\" for \"todo el\" (all the, masc.), \"ques\" for \"que es\" (which is); etc. including with common words, like d'ome (d'home/d'homme) instead de ome (home/homme), and so on.\n\nThough not strictly a contraction, a special form is used when combining con with mí, ti or sí which is written as \"conmigo\" for *\"con mí\" (with me), \"contigo\" for *\"con ti\" (with you sing.), \"consigo\" for *\"con sí\" (with himself/herself/itself/themselves (themself).\n\nFinally, one can hear \"pa\"' for \"para\", deriving as \"pa'l\" for \"para el\", but these forms are only considered appropriate in informal speech.\n\nIn Portuguese, contractions are common and much more numerous than those in Spanish. Several prepositions regularly contract with certain articles and pronouns. For instance, \"de\" (of) and \"por\" (by; formerly \"per\") combine with the definite articles \"o\" and \"a\" (masculine and feminine forms of \"the\" respectively), producing \"do\", \"da\" (of the), \"pelo\", \"pela\" (by the). The preposition \"de\" contracts with the pronouns \"ele\" and \"ela\" (he, she), producing \"dele\", \"dela\" (his, her). In addition, some verb forms contract with enclitic object pronouns: e.g., the verb \"amar\" (to love) combines with the pronoun \"a\" (her), giving \"amá-la\" (to love her).\n\nAnother contraction in portuguese which is similar to English ones is the combination of the pronoun \"da\" with words starting in \"a\", resulting in changing the first letter \"a\" for an apostrophe and joining both words. Examples: \"Estrela d'alva\" (A popular phrase to refer to Venus that means \"Alb star\", as a reference to its brightness) ; \"Caixa d'água\" (water tank).\n\nIn informal, spoken German prepositional phrases, one can often merge the preposition and the article; for example, \"von dem\" becomes \"vom\", \"zu dem\" becomes \"zum\", or \"an das\" becomes \"ans\". Some of these are so common that they are mandatory. In informal speech, \"aufm\" for \"auf dem\", \"unterm\" for \"unter dem\", etc. are also used, but would be considered to be incorrect if written, except maybe in quoted direct speech, in appropriate context and style.\n\nThe pronoun \"es\" often contracts to \"s\" (usually written with the apostrophe) in certain contexts. For example, the greeting \"Wie geht es?\" is usually encountered in the contracted form \"Wie geht's?\".\n\nRegional dialects of German, and various local languages which usually were already used long before today's Standard German was created, do use contractions usually more frequently than German, but varying widely between different local languages. The informally spoken German contractions are observed almost everywhere, most often accompanied by additional ones, such as \"in den\" becoming \"in'n\" (sometimes \"im\") or \"haben wir\" becoming \"hamwer\", \"hammor\", \"hemmer\", or \"hamma\" depending on local intonation preferences. Bavarian German features several more contractions such as \"gesund sind wir\" becoming \"xund samma\" which are schematically applied to all word or combinations of similar sound. (One must remember, however, that German \"wir\" exists alongside Bavarian \"mir\", or \"mia\", with the same meaning.) The Munich-born footballer Franz Beckenbauer has as his catchphrase \"Schau mer mal\" (\"Schauen wir einmal\" - in English \"let's have a look\"). A book about his career had as its title the slightly longer version of the phrase, \"Schau'n Mer Mal\".\n\nSuch features are found in all central and southern language regions. A sample from Berlin: \"Sag einmal, Meister, kann man hier einmal hinein?\" is spoken as \"Samma, Meesta, kamma hier ma rin?\"\n\nSeveral West Central German dialects along the Rhine River have built contraction patterns involving long phrases and entire sentences. In speech, words are often concatenated, and frequently the process of \"liaison\" is used. So, \"[Dat] kriegst Du nicht\" may become \"Kressenit\", or \"Lass mich gehen, habe ich gesagt\" may become \"Lomejon haschjesaat\".\n\nMostly, there are no binding orthographies for local dialects of German, hence writing is left to a great extent to authors and their publishers. Outside quotations, at least, they usually pay little attention to print more than the most commonly spoken contractions, so as not to degrade their readability. The use of apostrophes to indicate omissions is a varying and considerably less frequent process than in English-language publications.\n\nThe use of contractions is not allowed in any form of standard Norwegian spelling, however, it is fairly common to shorten or contract words in spoken language. Yet, the commonness varies from dialect to dialect and from sociolect to sociolect—it depends on the formality etc. of the setting. Some common, and quite drastic, contractions found in Norwegian speech are \"jakke\" for \"jeg har ikke\", meaning \"I do not have\" and \"dække\" for \"det er ikke\", meaning \"there is not\". The most frequently used of these contractions—usually consisting of two or three words contracted into one word, contain short, common and often monosyllabic words like , , , , or . The use of the apostrophe (') is much less common than in English, but is sometimes used in contractions to show where letters have been dropped.\n\nIn extreme cases, long, entire sentences may be written as one word. An example of this is \"Det ordner seg av seg selv\" in standard written Bokmål, meaning \"It will sort itself out\" could become \"dånesæsæsjæl\" (note the letters Å and Æ, and the word \"sjæl\", as an eye dialect spelling of ). R-dropping, being present in the example, is especially common in speech in many areas of Norway , but plays out in different ways, as does elision of word-final phonemes like .\n\nBecause of the many dialects of Norwegian and their widespread use it is often difficult to distinguish between non-standard writing of standard Norwegian and eye dialect spelling. It is almost universally true that these spellings try to convey the way each word is pronounced, but it is rare to see language written that does not adhere to at least some of the rules of the official orthography. Reasons for this include words spelled unphonemically, ignorance of conventional spelling rules, or adaptation for better transcription of that dialect's phonemes.\n\nLatin contains several examples of contractions. One such case is preserved in the verb \"nolo\" (I am unwilling/do not want) which was formed by a contraction of \"non volo\" (\"volo\" meaning “I want”). Similarly this is observed in the first person plural and third person plural forms (nolumus and nolunt respectively).\n\nSome contractions in rapid speech include ～っす (\"-ssu\") for です (\"desu\") and すいません (\"suimasen\") for すみません (\"sumimasen\"). では (\"dewa\") is often contracted to じゃ (\"ja\"). In certain grammatical contexts the particle の (\"no\") is contracted to simply ん (\"n\").\n\nWhen used after verbs ending in the conjunctive form ～て (\"-te\"), certain auxiliary verbs and their derivations are often abbreviated. Examples:\n<nowiki>*</nowiki> this abbreviation is never used in the polite conjugation, to avoid the resultant ambiguity between an abbreviated \"ikimasu\" (go) and the verb \"kimasu\" (come).\n\nThe ending ～なければ (\"-nakereba\") can be contracted to ～なきゃ (\"-nakya\") when it is used to indicate obligation. It is often used without an auxiliary, e.g., 行かなきゃ（いけない） (\"ikanakya (ikenai)\") \"I have to go.\"\n\nOther times, contractions are made to create new words or to give added or altered meaning:\n\nVarious dialects of Japanese also use their own specific contractions which are often unintelligible to speakers of other dialects.\n\nIn the Polish language pronouns have contracted forms which are more prevalent in their colloquial usage. Examples are \"go\" and \"mu\". The non-contracted forms are \"jego\" (unless it is used as a possessive pronoun) and \"jemu\", respectively. The clitic \"-ń\" which stands for \"niego\" (him) as in \"dlań\" (\"dla niego\") is more common in literature. The non-contracted forms are generally used as a means to accentuate.\n\nUyghur, a Turkic language spoken in Central Asia, includes some verbal suffixes that are actually contracted forms of compound verbs (serial verbs). For instance, \"sëtip alidu\" (sell-manage, \"manage to sell\") is usually written and pronounced \"sëtivaldu\", with the two words forming a contraction and the [p] leniting into a [v] or [w].\n\nIn Filipino, most contractions need other words to be contracted correctly. Only words that end with vowels can make a contraction with words like \"at\" and \"ay.\" In this chart, the \"@\" represents any vowel.\n"}
{"id": "1338683", "url": "https://en.wikipedia.org/wiki?curid=1338683", "title": "Corecursion", "text": "Corecursion\n\nIn computer science, corecursion is a type of operation that is dual to recursion. Whereas recursion works analytically, starting on data further from a base case and breaking it down into smaller data and repeating until one reaches a base case, corecursion works synthetically, starting from a base case and building it up, iteratively producing data further removed from a base case. Put simply, corecursive algorithms use the data that they themselves produce, bit by bit, as they become available, and needed, to produce further bits of data. A similar but distinct concept is \"generative recursion\" which may lack a definite \"direction\" inherent in corecursion and recursion.\n\nWhere recursion allows programs to operate on arbitrarily complex data, so long as they can be reduced to simple data (base cases), corecursion allows programs to produce arbitrarily complex and potentially infinite data structures, such as streams, so long as it can be produced from simple data (base cases) in a sequence of \"finite\" steps. Where recursion may not terminate, never reaching a base state, corecursion starts from a base state, and thus produces subsequent steps deterministically, though it may proceed indefinitely (and thus not terminate under strict evaluation), or it may consume more than it produces and thus become non-\"productive\". Many functions that are traditionally analyzed as recursive can alternatively, and arguably more naturally, be interpreted as corecursive functions that are terminated at a given stage, for example recurrence relations such as the factorial.\n\nCorecursion can produce both finite and infinite data structures as results, and may employ self-referential data structures. Corecursion is often used in conjunction with lazy evaluation, to produce only a finite subset of a potentially infinite structure (rather than trying to produce an entire infinite structure at once). Corecursion is a particularly important concept in functional programming, where corecursion and codata allow total languages to work with infinite data structures.\n\nCorecursion can be understood by contrast with recursion, which is more familiar. While corecursion is primarily of interest in functional programming, it can be illustrated using imperative programming, which is done below using the generator facility in Python. In these examples local variables are used, and assigned values imperatively (destructively), though these are not necessary in corecursion in pure functional programming. In pure functional programming, rather than assigning to local variables, these computed values form an invariable sequence, and prior values are accessed by self-reference (later values in the sequence reference earlier values in the sequence to be computed). The assignments simply express this in the imperative paradigm and explicitly specify where the computations happen, which serves to clarify the exposition.\n\nA classic example of recursion is computing the factorial, which is defined recursively by \"0! := 1\" and \"n! := n × (n - 1)!\".\n\nTo \"recursively\" compute its result on a given input, a recursive function calls (a copy of) \"itself\" with a different (\"smaller\" in some way) input and uses the result of this call to construct its result. The recursive call does the same, unless the \"base case\" has been reached. Thus a call stack develops in the process. For example, to compute \"fac(3)\", this recursively calls in turn \"fac(2)\", \"fac(1)\", \"fac(0)\" (\"winding up\" the stack), at which point recursion terminates with \"fac(0) = 1\", and then the stack unwinds in reverse order and the results are calculated on the way back along the call stack to the initial call frame \"fac(3)\" that uses the result of \"fac(2) = 2\" to calculate the final result as \"3 × 2 = 3 × fac(2) =: fac(3)\" and finally return \"fac(3) = 6\". In this example a function returns a single value.\n\nThis stack unwinding can be explicated, defining the factorial \"corecursively\", as an iterator, where one \"starts\" with the case of formula_1, then from this starting value constructs factorial values for increasing numbers \"1, 2, 3...\" as in the above recursive definition with \"time arrow\" reversed, as it were, by reading it \"backwards\" as The corecursive algorithm thus defined produces a \"stream\" of \"all\" factorials. This may be concretely implemented as a generator. Symbolically, noting that computing next factorial value requires keeping track of both \"n\" and \"f\" (a previous factorial value), this can be represented as:\nor in Haskell, \n\nmeaning, \"starting from formula_3, on each step the next values are calculated as formula_4\". This is mathematically equivalent and almost identical to the recursive definition, but the formula_5 emphasizes that the factorial values are being built \"up\", going forwards from the starting case, rather than being computed after first going backwards, \"down\" to the base case, with a formula_6 decrement. Note also that the direct output of the corecursive function does not simply contain the factorial formula_7 values, but also includes for each value the auxiliary data of its index \"n\" in the sequence, so that any one specific result can be selected among them all, as and when needed.\n\nNote the connection with denotational semantics, where the denotations of recursive programs is built up corecursively in this way.\n\nIn Python, a recursive factorial function can be defined as:\n\nThis could then be called for example as codice_1 to compute \"5!\".\n\nA corresponding corecursive generator can be defined as:\n\nThis generates an infinite stream of factorials in order; a finite portion of it can be produced by:\n\nThis could then be called to produce the factorials up to \"5!\" via:\n\nIf we're only interested in a certain factorial, just the last value can be taken, or we can fuse the production and the access into one function,\n\nAs can be readily seen here, this is practically equivalent (just by substituting codice_2 for the only codice_3 there) to the accumulator argument technique for tail recursion, unwound into an explicit loop. Thus it can be said that the concept of corecursion is an explication of the embodiment of iterative computation processes by recursive definitions, where applicable.\n\nIn the same way, the Fibonacci sequence can be represented as:\nNote that because the Fibonacci sequence is a recurrence relation of order 2, the corecursive relation must track two successive terms, with the formula_9 corresponding to shift forward by one step, and the formula_10 corresponding to computing the next term. This can then be implemented as follows (using parallel assignment):\n\nIn Haskell, \n\nTree traversal via a depth-first approach is a classic example of recursion. Dually, breadth-first traversal can very naturally be implemented via corecursion.\n\nWithout using recursion or corecursion specifically, one may traverse a tree by starting at the root node, placing its child nodes in a data structure, then iterating by removing node after node from the data structure while placing each removed node's children back into that data structure. If the data structure is a stack (LIFO), this yields depth-first traversal, and if the data structure is a queue (FIFO), this yields breadth-first traversal.\n\nUsing recursion, a (post-order) depth-first traversal can be implemented by starting at the root node and recursively traversing each child subtree in turn (the subtree based at each child node) – the second child subtree does not start processing until the first child subtree is finished. Once a leaf node is reached or the children of a branch node have been exhausted, the node itself is visited (e.g., the value of the node itself is outputted). In this case, the call stack (of the recursive functions) acts as the stack that is iterated over.\n\nUsing corecursion, a breadth-first traversal can be implemented by starting at the root node, outputting its value, then breadth-first traversing the subtrees – i.e., passing on the \"whole list\" of subtrees to the next step (not a single subtree, as in the recursive approach) – at the next step outputting the value of all of their root nodes, then passing on their child subtrees, etc. In this case the generator function, indeed the output sequence itself, acts as the queue. As in the factorial example (above), where the auxiliary information of the index (which step one was at, \"n\") was pushed forward, in addition to the actual output of \"n\"!, in this case the auxiliary information of the remaining subtrees is pushed forward, in addition to the actual output. Symbolically:\nmeaning that at each step, one outputs the list of values of root nodes, then proceeds to the child subtrees. Generating just the node values from this sequence simply requires discarding the auxiliary child tree data, then flattening the list of lists (values are initially grouped by level (depth); flattening (ungrouping) yields a flat linear list). In Haskell, \nThese can be compared as follows. The recursive traversal handles a \"leaf node\" (at the \"bottom\") as the base case (when there are no children, just output the value), and \"analyzes\" a tree into subtrees, traversing each in turn, eventually resulting in just leaf nodes – actual leaf nodes, and branch nodes whose children have already been dealt with (cut off \"below\"). By contrast, the corecursive traversal handles a \"root node\" (at the \"top\") as the base case (given a node, first output the value), treats a tree as being \"synthesized\" of a root node and its children, then produces as auxiliary output a list of subtrees at each step, which are then the input for the next step – the child nodes of the original root are the root nodes at the next step, as their parents have already been dealt with (cut off \"above\"). Note also that in the recursive traversal there is a distinction between leaf nodes and branch nodes, while in the corecursive traversal there is no distinction, as each node is treated as the root node of the subtree it defines.\n\nNotably, given an infinite tree, the corecursive breadth-first traversal will traverse all nodes, just as for a finite tree, while the recursive depth-first traversal will go down one branch and not traverse all nodes, and indeed if traversing post-order, as in this example (or in-order), it will visit no nodes at all, because it never reaches a leaf. This shows the usefulness of corecursion rather than recursion for dealing with infinite data structures.\n\nIn Python, this can be implemented as follows.\nThe usual post-order depth-first traversal can be defined as:\n\nThis can then be called by codice_4 to print the values of the nodes of the tree in post-order depth-first order.\n\nThe breadth-first corecursive generator can be defined as:\n\nThis can then be called to print the values of the nodes of the tree in breadth-first order:\n\nInitial data types can be defined as being the least fixpoint (up to isomorphism) of some type equation; the isomorphism is then given by an initial algebra. Dually, final (or terminal) data types can be defined as being the greatest fixpoint of a type equation; the isomorphism is then given by a final coalgebra.\n\nIf the domain of discourse is the category of sets and total functions, then final data types may contain infinite, non-wellfounded values, whereas initial types do not. On the other hand, if the domain of discourse is the category of complete partial orders and continuous functions, which corresponds roughly to the Haskell programming language, then final types coincide with initial types, and the corresponding final coalgebra and initial algebra form an isomorphism.\n\nCorecursion is then a technique for recursively defining functions whose range (codomain) is a final data type, dual to the way that ordinary recursion recursively defines functions whose domain is an initial data type.\n\nThe discussion below provides several examples in Haskell that distinguish corecursion. Roughly speaking, if one were to port these definitions to the category of sets, they would still be corecursive. This informal usage is consistent with existing textbooks about Haskell. Also note that the examples used in this article predate the attempts to define corecursion and explain what it is.\n\nThe rule for \"primitive corecursion\" on codata is the dual to that for primitive recursion on data. Instead of descending on the argument by pattern-matching on its constructors (that \"were called up before\", somewhere, so we receive a ready-made datum and get at its constituent sub-parts, i.e. \"fields\"), we ascend on the result by filling-in its \"destructors\" (or \"observers\", that \"will be called afterwards\", somewhere - so we're actually calling a constructor, creating another bit of the result to be observed later on). Thus corecursion \"creates\" (potentially infinite) codata, whereas ordinary recursion \"analyses\" (necessarily finite) data. Ordinary recursion might not be applicable to the codata because it might not terminate. Conversely, corecursion is not strictly necessary if the result type is data, because data must be finite.\n\nIn \"Programming with streams in Coq: a case study: the Sieve of Eratosthenes\" we find\n\nwhere primes \"are obtained by applying the primes operation to the stream (Enu 2)\". Following the above notation, the sequence of primes (with a throwaway 0 prefixed to it) and numbers streams being progressively sieved, can be represented as \nor in Haskell, \n\nThe authors discuss how the definition of codice_5 is not guaranteed always to be \"productive\", and could become stuck e.g. if called with codice_6 as the initial stream.\n\nHere is another example in Haskell. The following definition produces the list of Fibonacci numbers in linear time:\nThis infinite list depends on lazy evaluation; elements are computed on an as-needed basis, and only finite prefixes are ever explicitly represented in memory. This feature allows algorithms on parts of codata to terminate; such techniques are an important part of Haskell programming.\n\nThis can be done in Python as well:\nThe definition of codice_7 can be inlined, leading to this:\n\nThis example employs a self-referential \"data structure\". Ordinary recursion makes use of self-referential \"functions\", but does not accommodate self-referential data. However, this is not essential to the Fibonacci example. It can be rewritten as follows:\n\nThis employs only self-referential \"function\" to construct the result. If it were used with strict list constructor it would be an example of runaway recursion, but with non-strict list constructor this guarded recursion gradually produces an indefinitely defined list.\n\nCorecursion need not produce an infinite object; a corecursive queue is a particularly good example of this phenomenon. The following definition produces a breadth-first traversal of a binary tree in linear time:\n\nThis definition takes an initial tree and produces a list of subtrees. This list serves dual purpose as both the queue and the result ( produces its output notches after its input back-pointer, , along the ). It is finite if and only if the initial tree is finite. The length of the queue must be explicitly tracked in order to ensure termination; this can safely be elided if this definition is applied only to infinite trees. \n\nAnother particularly good example gives a solution to the problem of breadth-first labeling. The function codice_8 visits every node in a binary tree in a breadth first fashion, and replaces each label with an integer, each subsequent integer is bigger than the last by one. This solution employs a self-referential data structure, and the binary tree can be finite or infinite.\n\nAn apomorphism (such as an anamorphism, such as unfold) is a form of corecursion in the same way that a paramorphism (such as a catamorphism, such as fold) is a form of recursion.\n\nThe Coq proof assistant supports corecursion and coinduction using the CoFixpoint command.\n\nCorecursion, referred to as \"circular programming,\" dates at least to , who credits John Hughes and Philip Wadler; more general forms were developed in . The original motivations included producing more efficient algorithms (allowing 1 pass over data in some cases, instead of requiring multiple passes) and implementing classical data structures, such as doubly linked lists and queues, in functional languages.\n\n\n"}
{"id": "7931", "url": "https://en.wikipedia.org/wiki?curid=7931", "title": "Dictionary", "text": "Dictionary\n\nA dictionary, sometimes known as a wordbook, is a collection of words in one or more specific languages, often arranged alphabetically (or by radical and stroke for ideographic languages), which may include information on definitions, usage, etymologies, pronunciations, translation, etc. or a book of words in one language with their equivalents in another, sometimes known as a lexicon. It is a lexicographical reference that shows inter-relationships among the data.\n\nA broad distinction is made between general and specialized dictionaries. Specialized dictionaries include words in specialist fields, rather than a complete range of words in the language. Lexical items that describe concepts in specific fields are usually called terms instead of words, although there is no consensus whether lexicology and terminology are two different fields of study. In theory, general dictionaries are supposed to be semasiological, mapping word to definition, while specialized dictionaries are supposed to be onomasiological, first identifying concepts and then establishing the terms used to designate them. In practice, the two approaches are used for both types. There are other types of dictionaries that do not fit neatly into the above distinction, for instance bilingual (translation) dictionaries, dictionaries of synonyms (thesauri), and rhyming dictionaries. The word dictionary (unqualified) is usually understood to refer to a general purpose monolingual dictionary.\n\nThere is also a contrast between \"prescriptive\" or \"descriptive\" dictionaries; the former reflect what is seen as correct use of the language while the latter reflect recorded actual use. Stylistic indications (e.g. \"informal\" or \"vulgar\") in many modern dictionaries are also considered by some to be less than objectively descriptive.\n\nAlthough the first recorded dictionaries date back to Sumerian times (these were bilingual dictionaries), the systematic study of dictionaries as objects of scientific interest themselves is a 20th-century enterprise, called lexicography, and largely initiated by Ladislav Zgusta. The birth of the new discipline was not without controversy, the practical dictionary-makers being sometimes accused by others of \"astonishing\" lack of method and critical-self reflection.\n\nThe oldest known dictionaries were Akkadian Empire cuneiform tablets with bilingual Sumerian–Akkadian wordlists, discovered in Ebla (modern Syria) and dated roughly 2300 BCE. The early 2nd millennium BCE \"Urra=hubullu\" glossary is the canonical Babylonian version of such bilingual Sumerian wordlists. A Chinese dictionary, the c. 3rd century BCE \"Erya\", was the earliest surviving monolingual dictionary; although some sources cite the c. 800 BCE Shizhoupian as a \"dictionary\", modern scholarship considers it a calligraphic compendium of Chinese characters from Zhou dynasty bronzes. Philitas of Cos (fl. 4th century BCE) wrote a pioneering vocabulary \"Disorderly Words\" (Ἄτακτοι γλῶσσαι, \"\") which explained the meanings of rare Homeric and other literary words, words from local dialects, and technical terms. Apollonius the Sophist (fl. 1st century CE) wrote the oldest surviving Homeric lexicon. The first Sanskrit dictionary, the Amarakośa, was written by Amara Sinha c. 4th century CE. Written in verse, it listed around 10,000 words. According to the \"Nihon Shoki\", the first Japanese dictionary was the long-lost 682 CE \"Niina\" glossary of Chinese characters. The oldest existing Japanese dictionary, the c. 835 CE \"Tenrei Banshō Meigi\", was also a glossary of written Chinese. In \"Frahang-i Pahlavig\", Aramaic heterograms are listed together with their translation in Middle Persian language and phonetic transcription in Pazand alphabet. A 9th-century CE Irish dictionary, Sanas Cormaic, contained etymologies and explanations of over 1,400 Irish words. In India around 1320, Amir Khusro compiled the Khaliq-e-bari which mainly dealt with Hindustani and Persian words.\nArabic dictionaries were compiled between the 8th and 14th centuries CE, organizing words in rhyme order (by the last syllable), by alphabetical order of the radicals, or according to the alphabetical order of the first letter (the system used in modern European language dictionaries). The modern system was mainly used in specialist dictionaries, such as those of terms from the Qur'an and hadith, while most general use dictionaries, such as the \"Lisan al-`Arab\" (13th century, still the best-known large-scale dictionary of Arabic) and \"al-Qamus al-Muhit\" (14th century) listed words in the alphabetical order of the radicals. The \"Qamus al-Muhit\" is the first handy dictionary in Arabic, which includes only words and their definitions, eliminating the supporting examples used in such dictionaries as the \"Lisan\" and the \"Oxford English Dictionary\".\nIn medieval Europe, glossaries with equivalents for Latin words in vernacular or simpler Latin were in use (e.g. the Leiden Glossary). The \"Catholicon\" (1287) by Johannes Balbus, a large grammatical work with an alphabetical lexicon, was widely adopted. It served as the basis for several bilingual dictionaries and was one of the earliest books (in 1460) to be printed. In 1502 Ambrogio Calepino's \"Dictionarium\" was published, originally a monolingual Latin dictionary, which over the course of the 16th century was enlarged to become a multilingual glossary. In 1532 Robert Estienne published the \"Thesaurus linguae latinae\" and in 1572 his son Henri Estienne published the \"Thesaurus linguae graecae\", which served up to the 19th century as the basis of Greek lexicography. The first monolingual dictionary written in Europe was the Spanish, written by Sebastián Covarrubias' \"Tesoro de la lengua castellana o española\", published in 1611 in Madrid, Spain. In 1612 the first edition of the \"Vocabolario degli Accademici della Crusca\", for Italian, was published. It served as the model for similar works in French and English. In 1690 in Rotterdam was published, posthumously, the \"Dictionnaire Universel\" by Antoine Furetière for French. In 1694 appeared the first edition of the \"Dictionnaire de l'Académie française\". Between 1712 and 1721 was published the \"Vocabulario portughez e latino\" written by Raphael Bluteau. The Real Academia Española published the first edition of the \"Diccionario de la lengua española\" in 1780, but their \"Diccionario de Autoridades\", which included quotes taken from literary works, was published in 1726. The \"Totius Latinitatis lexicon\" by Egidio Forcellini was firstly published in 1777; it has formed the basis of all similar works that have since been published.\n\nThe first edition of \"A Greek-English Lexicon\" by Henry George Liddell and Robert Scott appeared in 1843; this work remained the basic dictionary of Greek until the end of the 20th century. And in 1858 was published the first volume of the Deutsches Wörterbuch by the Brothers Grimm; the work was completed in 1961. Between 1861 and 1874 was published the \"Dizionario della lingua italiana\" by Niccolò Tommaseo. Between 1862 and 1874 was published the six volumes of \"A magyar nyelv szótára\" (Dictionary of Hungarian Language) by Gergely Czuczor and János Fogarasi. Émile Littré published the Dictionnaire de la langue française between 1863 and 1872. In the same year 1863 appeared the first volume of the \"Woordenboek der Nederlandsche Taal\" which was completed in 1998. Also in 1863 Vladimir Ivanovich Dahl published the \"Explanatory Dictionary of the Living Great Russian Language\". The Duden dictionary dates back to 1880, and is currently the prescriptive source for the spelling of German. The decision to start work on the \"Svenska Akademiens ordbok\" was taken in 1787.\n\nThe earliest dictionaries in the English language were glossaries of French, Spanish or Latin words along with their definitions in English. The word \"dictionary\" was invented by an Englishman called John of Garland in 1220 — he had written a book \"Dictionarius\" to help with Latin \"diction\". An early non-alphabetical list of 8000 English words was the \"Elementarie\", created by Richard Mulcaster in 1582.\n\nThe first purely English alphabetical dictionary was \"A Table Alphabeticall\", written by English schoolteacher Robert Cawdrey in 1604. The only surviving copy is found at the Bodleian Library in Oxford. This dictionary, and the many imitators which followed it, was seen as unreliable and nowhere near definitive. Philip Stanhope, 4th Earl of Chesterfield was still lamenting in 1754, 150 years after Cawdrey's publication, that it is \"a sort of disgrace to our nation, that hitherto we have had no… standard of our language; our dictionaries at present being more properly what our neighbors the Dutch and the Germans call theirs, word-books, than dictionaries in the superior sense of that title.\" \n\nIn 1616, John Bullokar described the history of the dictionary with his \"English Expositor\". \"Glossographia\" by Thomas Blount, published in 1656, contains more than 10,000 words along with their etymologies or histories. Edward Phillips wrote another dictionary in 1658, entitled \"The New World of English Words: Or a General Dictionary\" which boldly plagiarized Blount's work, and the two denounced each other. This created more interest in the dictionaries. John Wilkins' 1668 essay on philosophical language contains a list of 11,500 words with careful distinctions, compiled by William Lloyd. Elisha Coles published his \"English Dictionary\" in 1676.\n\nIt was not until Samuel Johnson's \"A Dictionary of the English Language\" (1755) that a more reliable English dictionary was produced. Many people today mistakenly believe that Johnson wrote the first English dictionary: a testimony to this legacy. By this stage, dictionaries had evolved to contain textual references for most words, and were arranged alphabetically, rather than by topic (a previously popular form of arrangement, which meant all animals would be grouped together, etc.). Johnson's masterwork could be judged as the first to bring all these elements together, creating the first \"modern\" dictionary.\n\nJohnson's dictionary remained the English-language standard for over 150 years, until the Oxford University Press began writing and releasing the \"Oxford English Dictionary\" in short fascicles from 1884 onwards. It took nearly 50 years to complete this huge work, and they finally released the complete \"OED\" in twelve volumes in 1928. It remains the most comprehensive and trusted English language dictionary to this day, with revisions and updates added by a dedicated team every three months. One of the main contributors to this modern dictionary was an ex-army surgeon, William Chester Minor, a convicted murderer who was confined to an asylum for the criminally insane.\n\nIn 1806, American Noah Webster published his first dictionary, \"\". In 1807 Webster began compiling an expanded and fully comprehensive dictionary, \"An American Dictionary of the English Language;\" it took twenty-seven years to complete. To evaluate the etymology of words, Webster learned twenty-six languages, including Old English (Anglo-Saxon), German, Greek, Latin, Italian, Spanish, French, Hebrew, Arabic, and Sanskrit.\n\nWebster completed his dictionary during his year abroad in 1825 in Paris, France, and at the University of Cambridge. His book contained seventy thousand words, of which twelve thousand had never appeared in a published dictionary before. As a spelling reformer, Webster believed that English spelling rules were unnecessarily complex, so his dictionary introduced American English spellings, replacing \"colour\" with \"color\", substituting \"wagon\" for \"waggon\", and printing \"center\" instead of \"centre\". He also added American words, like \"skunk\" and \"squash\", that did not appear in British dictionaries. At the age of seventy, Webster published his dictionary in 1828; it sold 2500 copies. In 1840, the second edition was published in two volumes.\n\nIn a general dictionary, each word may have multiple meanings. Some dictionaries include each separate meaning in the order of most common usage while others list definitions in historical order, with the oldest usage first.\n\nIn many languages, words can appear in many different forms, but only the undeclined or unconjugated form appears as the headword in most dictionaries. Dictionaries are most commonly found in the form of a book, but some newer dictionaries, like StarDict and the \"New Oxford American Dictionary\" are dictionary software running on PDAs or computers. There are also many online dictionaries accessible via the Internet.\n\nAccording to the \"Manual of Specialized Lexicographies\", a specialized dictionary, also referred to as a technical dictionary, is a dictionary that focuses upon a specific subject field. Following the description in \"The Bilingual LSP Dictionary\", lexicographers categorize specialized dictionaries into three types: A multi-field dictionary broadly covers several subject fields (e.g. a business dictionary), a single-field dictionary narrowly covers one particular subject field (e.g. law), and a sub-field dictionary covers a more specialized field (e.g. constitutional law). For example, the 23-language Inter-Active Terminology for Europe is a multi-field dictionary, the American National Biography is a single-field, and the African American National Biography Project is a sub-field dictionary. In terms of the coverage distinction between \"minimizing dictionaries\" and \"maximizing dictionaries\", multi-field dictionaries tend to minimize coverage across subject fields (for instance, \"Oxford Dictionary of World Religions\" and \"Yadgar Dictionary of Computer and Internet Terms\") whereas single-field and sub-field dictionaries tend to maximize coverage within a limited subject field (\"The Oxford Dictionary of English Etymology\").\n\nAnother variant is the glossary, an alphabetical list of defined terms in a specialized field, such as medicine (medical dictionary).\n\nThe simplest dictionary, a defining dictionary, provides a core glossary of the simplest meanings of the simplest concepts. From these, other concepts can be explained and defined, in particular for those who are first learning a language. In English, the commercial defining dictionaries typically include only one or two meanings of under 2000 words. With these, the rest of English, and even the 4000 most common English idioms and metaphors, can be defined.\n\nLexicographers apply two basic philosophies to the defining of words: \"prescriptive\" or \"descriptive\". Noah Webster, intent on forging a distinct identity for the American language, altered spellings and accentuated differences in meaning and pronunciation of some words. This is why American English now uses the spelling \"color\" while the rest of the English-speaking world prefers \"colour\". (Similarly, British English subsequently underwent a few spelling changes that did not affect American English; see further at American and British English spelling differences.)\n\nLarge 20th-century dictionaries such as the \"Oxford English Dictionary\" (OED) and \"Webster's Third\" are descriptive, and attempt to describe the actual use of words. Most dictionaries of English now apply the descriptive method to a word's definition, and then, outside of the definition itself, and information alerting readers to attitudes which may influence their choices on words often considered vulgar, offensive, erroneous, or easily confused. \"Merriam-Webster\" is subtle, only adding italicized notations such as, \"sometimes offensive\" or \"stand\" (nonstandard). \"American Heritage\" goes further, discussing issues separately in numerous \"usage notes.\" \"Encarta\" provides similar notes, but is more prescriptive, offering warnings and admonitions against the use of certain words considered by many to be offensive or illiterate, such as, \"an offensive term for...\" or \"a taboo term meaning...\".\n\nBecause of the widespread use of dictionaries in schools, and their acceptance by many as language authorities, their treatment of the language does affect usage to some degree, with even the most descriptive dictionaries providing conservative continuity. In the long run, however, the meanings of words in English are primarily determined by usage, and the language is being changed and created every day. As Jorge Luis Borges says in the prologue to \"El otro, el mismo\": \"It is often forgotten that (dictionaries) are artificial repositories, put together well after the languages they define. The roots of language are irrational and of a magical nature.\"\n\nSometimes the same dictionary can be descriptive in some domains and prescriptive in others. For example, according to Ghil'ad Zuckermann, the \"Oxford English-Hebrew Dictionary\" is \"at war with itself\": whereas its coverage (lexical items) and glosses (definitions) are descriptive and colloquial, its vocalization is prescriptive. This internal conflict results in absurd sentences such as \"hi taharóg otí kshetiré me asíti lamkhonít\" (she'll tear me apart when she sees what I've done to the car). Whereas \"hi taharóg otí\", literally 'she will kill me', is colloquial, me (a variant of ma 'what') is archaic, resulting in a combination that is unutterable in real life.\n\nA historical dictionary is a specific kind of descriptive dictionary which describes the development of words and senses over time, usually using citations to original source material to support its conclusions.\n\nIn contrast to traditional dictionaries, which are designed to be used by human beings, dictionaries for natural language processing (NLP) are built to be used by computer programs. The final user is a human being but the direct user is a program. Such a dictionary does not need to be able to be printed on paper. The structure of the content is not linear, ordered entry by entry but has the form of a complex network (see Diathesis alternation). Because most of these dictionaries are used to control machine translations or cross-lingual information retrieval (CLIR) the content is usually multilingual and usually of huge size. In order to allow formalized exchange and merging of dictionaries, an ISO standard called Lexical Markup Framework (LMF) has been defined and used among the industrial and academic community.\n\n\nIn many languages, such as the English language, the pronunciation of some words is not consistently apparent from their spelling. In these languages, dictionaries usually provide the pronunciation. For example, the definition for the word \"dictionary\" might be followed by the International Phonetic Alphabet spelling . American English dictionaries often use their ownpronunciation respelling systems with diacritics, for example \"dictionary\" is respelled as \"dĭk′shə-nĕr′ē\" in the American Heritage Dictionary. The IPA is more commonly used within the British Commonwealth countries. Yet others use their own pronunciation respelling systems without diacritics: for example, \"dictionary\" may be respelled as . Some online or electronic dictionaries provide audio recordings of words being spoken.\n\nHistories and descriptions of the dictionaries of other languages on Wikipedia include:\n\n\nThe age of the Internet brought online dictionaries to the desktop and, more recently, to the smart phone. David Skinner in 2013 noted that \"Among the top ten lookups on Merriam-Webster Online at this moment are 'holistic, pragmatic, caveat, esoteric' and 'bourgeois.' Teaching users about words they don’t already know has been, historically, an aim of lexicography, and modern dictionaries do this well.\"\nThere exist a number of websites which operate as online dictionaries, usually with a specialized focus. Some of them have exclusively user driven content, often consisting of neologisms. Some of the more notable examples include:\n\n\n\n"}
{"id": "46842976", "url": "https://en.wikipedia.org/wiki?curid=46842976", "title": "Diels–Kranz numbering", "text": "Diels–Kranz numbering\n\nDiels–Kranz (DK) numbering is the standard system for referencing the works of the ancient Greek pre-Socratic philosophers, based on the collection of quotations from and reports of their work, \"Die Fragmente der Vorsokratiker\" (The Fragments of the Pre-Socratics), by Hermann Alexander Diels. The \"Fragmente\" was first published in 1903, was later revised and expanded three times by Diels, and was finally revised in a fifth edition (1934–7) by Walther Kranz and again in a sixth edition (1952). In Diels-Kranz, each passage, or item, is assigned a number which is used to uniquely identify the ancient personality with which it is concerned, and the type of item given. Diels-Kranz is used in academia to cite pre-Socratic philosophers, and the system also encompasses Sophists and pre-Homeric poets such as Orpheus.\n\nStephanus pagination is the comparable system for referring to Plato, and Bekker numbering is the comparable system for referring to Aristotle.\n\nThe works of the pre-Socratics have not survived extant to the present day. Our knowledge of them exists only through references in the works of later philosophers (known as doxography) in the form of quotations and paraphrases. For example, our knowledge of Thales of Miletus comes largely from the works of Aristotle, who lived centuries after him. Another interesting example of such a source is Hippolytus of Rome, whose polemic \"Refutation of All Heresies\" is a source of many direct quotations of Heraclitus as well as of other philosophers, thereby perpetuating the work of those he was refuting.\n\nThese quotations, paraphrases and other references to pre-Socratic philosophers were collected by Diels and Kranz in their book, which became a standard text in modern pre-Socratic education and scholarship. Because of its influence, Diels-Kranz numbering became the standard way of referencing the material: in literature, conferences, and even in conversation.\n\nThe number corresponding to an item was made up of three parts:\n\n\nWhy, take the case of Thales, Theodorus. While he was studying the stars and looking upwards, he fell into a pit, and a neat, witty Thracian servant girl jeered at him, they say, because he was so eager to know the things in the sky that he could not see what was there before him at his very feet.\n\nThe above text has a DK number of 11A9, since it refers to Thales who is, as mentioned above, chapter 11's subject. The source is \"Theaetetus\" (one of Plato's dialogues), and gives an account of Thales' life, hence it is a \"testimonium\", represented by the letter \"A\". Finally, it is the ninth item in its chapter, giving it the overall number of DK 11A9.\n\nSometimes, the chapter (personality) number may simply be replaced by the name, which can be helpful in cases where the former is the same as the passage number, to avoid ambiguity. For example:\n\nThose who seek for gold dig up much earth and find a little.\n\nRather than \"22B22\" the above may also instead be referred to as \"Heraclitus B22\" as it is a direct transmission of the words of Heraclitus (thus, B) and is the 22nd item in the chapter about Heraclitus (whose chapter number is also 22) in the \"Fragmente\".\n\nThe following table gives the Diels-Kranz numbering of Pre-Socratic philosophers. Note that the numbering scheme presented is that of the fifth edition of \"Die Fragmente der Vorsokratiker\", the first to be revised by Kranz. The fifth edition's numbering is the scheme which has since gained the most traction in modern Pre-Socratic scholarship, and it is the one used consistently throughout this article. It should not be confused with the numberings given in other versions, which changed frequently depending on the particular edition of the \"Fragmente\".\n\nMost entries (78) are concerned with a single, named individual, while the remaining minority of entries (12) have more complex context. Of these latter, eight (10, 19, 39, 46, 53-56) are each concerned with groups of named personalties, who typically have a clear relationship of some kind to justify their association in each entry. Two entries (58, 79) are devoted not to individuals, but to schools of thought (Pythagoreanism and Sophism), and the last two (89, 90) reproduce contemporaneous anonymous texts. Although \"the Seven Sages of Greece\" implies a clearly defined set of seven people, historical disagreement renders intractable the problem of exactly who they were, with multiple sources suggesting several different candidates. If one takes the Seven Sages as a group of seven and includes the later Iamblichus, Diels-Kranz encompasses 106 named personalities and two anonymous authors. The chapter on Sophism is concerned with the named sophists who take up most of the rest of the scheme, and \nper Freeman with regard to the chapter on Pythagoreanism, a catalogue due to Iamblichus lists 218 named men and 17 named women as Pythagoreans, along with other probable, anonymous adherents.\n\nIn several cases, the personalities listed are so obscure that they are merely mentioned by name in other sources, commonly with hints as to their geographical and philosophical associations, and without even surviving \"paraphrases\" of any of their ideas, or what they might have written. That is, these more obscure personalities survive in the historical record only as names cited by others, and so came to be included in Diels-Kranz for the sake of scholarly completeness.\n\n\n"}
{"id": "358999", "url": "https://en.wikipedia.org/wiki?curid=358999", "title": "Difference feminism", "text": "Difference feminism\n\nTaking for granted an equal moral status as persons, difference feminism asserts that there are differences between men and women but that no value judgment can be placed upon them.\n\nThe term \"difference feminism\" developed during the \"equality-versus-difference debate\" in American feminism in the 1980s and 1990s, but subsequently fell out of favor and use. In the 1990s feminists addressed the binary logic of \"difference\" versus \"equality\" and moved on from it, notably with postmodern and/or deconstructionist approaches that either dismantled or did not depend on that dichotomy.\n\nDifference feminism did not require a commitment to essentialism. Most strains of difference feminism did not argue that there was a biological, inherent, ahistorical, or otherwise \"essential\" link between womanhood and traditionally feminine values, habits of mind (often called \"ways of knowing\"), or personality traits. These feminists simply sought to recognize that, in the present, women and men are significantly different and to explore the devalued \"feminine\" characteristics.\n\nSome strains of difference feminism, for example Mary Daly's, argue not just that women and men were different, and had different values or different ways of knowing, but that women and their values were superior to men's. This viewpoint does not require essentialism, although there is ongoing debate about whether Daly's feminism is essentialist.\n\nDifference feminism was developed by feminists in the 1980s, in part as a reaction to popular liberal feminism (also known as \"equality feminism\"), which emphasized the similarities between women and men in order to argue for equal treatment for women. Difference feminism, although it still aimed at equality between men and women, emphasized the differences between men and women and argued that identicality or sameness are not necessary in order for men and women, and masculine and feminine values, to be treated equally. Liberal feminism aimed to make society and law gender-neutral, since it saw recognition of gender difference as a barrier to rights and participation within liberal democracy, while difference feminism held that gender-neutrality harmed women \"whether by impelling them to imitate men, by depriving society of their distinctive contributions, or by letting them participate in society only on terms that favor men\".\n\nDifference feminism drew on earlier nineteenth-century strains of thought, for example the work of German writer Elise Oelsner, which held that not only should women be allowed into formerly male-only spheres and institutions (e.g. public life, science) but that those institutions should also be expected to change in a way that recognizes the value of traditionally devalued feminine ethics (like care [see ethics of care]). On the latter point, many feminists have re-read the phrase \"difference feminism\" in a way that asks \"what difference does feminism make?\" (e.g. to the practice of science) rather than \"what differences are there between men and women\"?\n\nSome have argued that the thought of certain prominent second-wave feminists, like psychologist Carol Gilligan and radical feminist theologian Mary Daly, is \"essentialist.\" In philosophy essentialism is the belief that \"(at least some) objects have (at least some) essential properties.\" In the case of sexual politics essentialism is taken to mean that \"women\" and \"men\" have fixed essences or essential properties (e.g. behavioral or personality traits) that cannot be changed. However, essentialist interpretations of Daly and Gilligan have been questioned by some feminist scholars, who argue that charges of \"essentialism\" are often used more as terms of abuse than as theoretical critiques based on evidence, and do not accurately reflect Gilligan or Daly's views.\n\n"}
{"id": "2330597", "url": "https://en.wikipedia.org/wiki?curid=2330597", "title": "Errors and omissions excepted", "text": "Errors and omissions excepted\n\nErrors and omissions excepted (E&OE) is a phrase used in an attempt to reduce legal liability for potentially incorrect or incomplete information supplied in a contractually related document such as a quotation or specification.\n\nIt is often applied as a disclaimer in situations in which the information to which it is applied is relatively fast-moving. In legal terms, it seeks to make a statement that information cannot be relied upon, or may have changed by the time of use.\n\nIt is regularly used in accounting, to \"excuse slight mistakes or oversights.\"\n\nIt is also used when a large amount of information is listed against a product, to state that—to the best of the supplier's knowledge—the information is correct, but that they will not be held responsible if an error has been committed.\n"}
{"id": "33487458", "url": "https://en.wikipedia.org/wiki?curid=33487458", "title": "Guide to information sources", "text": "Guide to information sources\n\nA Guide to information sources (or a bibliographic guide, a literature guide, a guide to reference materials, a subject gateway, etc.) is a kind of metabibliography. Ideally it is not just a listing of bibliographies, reference works and other information sources, but more like a textbook introducing users to the information sources in a given field (in general).\n\nSuch guides may have many different forms: Comprehensive or highly selective, printed or electronic sources, annoteted listings or written chapters etc.\n\nOften used as curriculum tools for bibliographic instruction, the guides help library users find materials or help those unfamiliar with a discipline understand the key sources.\n\nAby, Stephen H., Nalen, James & Fielding, Lori (2005). Sociology; a guide to reference and information sources. 3rd ed. Westport, Conn.: Libraries Unlimited.\n\nAdams, Stephen R. (2005). \"Information Sources in Patents\"; 2nd ed. (Guides to Information Sources). München: K. G. Saur \n\nBlewett, Daniel K (2008). American military history; a guide to reference and information sources. 2nd ed. Westport, CT : Libraries Unlimited.\n\nJacoby, JoAnn & Kibbee, Josephine Z. (2007). Cultural anthropology; a guide to reference and information sources. 2nd ed. Westport, Conn.: Libraries Unlimited.\n\nSchmidt, Diane & Bell, George H. (2003). Guide to reference and information sources in the zoological sciences. Westport, Conn. : Libraries Unlimited.\n\nO'Hare, Christine (2007). \"Business Information Sources\". London: Library Assn Pub Ltd\n\nOstwald, W (1919). Die chemische Literatur und die Organisation der Wissenschaft. Leipzig : W. Ostwald & C. Drucker. (This is considered the first \"guide to information sources\").\n\nStebbins, Leslie F. (2006). Student guide to research in the digital age; how to locate and evaluate information sources. Westport, Conn.: Libraries Unlimited.\n\nWebb, W. H. et al. (Ed.). (1986). Sources of information in the social sciences. A Guide to the literature. 3. ed. Chicago : American Library Association.\n\nZell, Hans M. (ed.). (2003). The African studies companion; a guide to information sources. 3rd rev. and expanded ed. Glais Bheinn : Hans Zell.\n\n\n"}
{"id": "1930406", "url": "https://en.wikipedia.org/wiki?curid=1930406", "title": "Impredicativity", "text": "Impredicativity\n\nSomething that is impredicative, in mathematics, logic and philosophy of mathematics, is a self-referencing definition. Roughly speaking, a definition is impredicative if it invokes (mentions or quantifies over) the set being defined, or (more commonly) another set that contains the thing being defined. There is no generally accepted precise definition of what it means to be predicative or impredicative. Authors have given different but related definitions.\n\nThe opposite of impredicativity is predicativity, which essentially entails building stratified (or ramified) theories where quantification over lower levels results in variables of some new type, distinguished from the lower types that the variable ranges over. A prototypical example is intuitionistic type theory, which retains ramification so as to discard impredicativity.\n\nRussell's paradox is a famous example of an impredicative construction—namely the set of all sets that do not contain themselves. The paradox is that such a set cannot exist: If it would exist, the question could be asked whether it contains itself or not — if it does then by definition it should not, and if it does not then by definition it should.\n\nThe greatest lower bound of a set , , also has an impredicative definition: if and only if for all elements of , is less than or equal to , and any less than or equal to all elements of is less than or equal to . This definition quantifies over the set (potentially infinite, depending on the order in question) whose members are the lower bounds of , one of which being the glb itself. Hence predicativism would reject this definition.\n\nThe terms \"predicative\" and \"impredicative\" were introduced by , though the meaning has changed a little since then. \n\nSolomon Feferman provides a historical review of predicativity, connecting it to current outstanding research problems.\n\nThe vicious circle principle was suggested by Henri Poincaré (1905-6, 1908) and Bertrand Russell in the wake of the paradoxes as a requirement on legitimate set specifications. Sets that do not meet the requirement are called \"impredicative\".\n\nThe first modern paradox appeared with Cesare Burali-Forti's 1897 \"A question on transfinite numbers\" and would become known as the Burali-Forti paradox. Cantor had apparently discovered the same paradox in his (Cantor's) \"naive\" set theory and this become known as Cantor's paradox. Russell's awareness of the problem originated in June 1901 with his reading of Frege's treatise of mathematical logic, his 1879 \"Begriffsschrift\"; the offending sentence in Frege is the following:\nIn other words, given the function is the variable and is the invariant part. So why not substitute the value for itself? Russell promptly wrote Frege a letter pointing out that:\nFrege promptly wrote back to Russell acknowledging the problem:\nWhile the problem had adverse personal consequences for both men (both had works at the printers that had to be emended), van Heijenoort observes that \"The paradox shook the logicians' world, and the rumbles are still felt today. ... Russell's paradox, which uses the bare notions of set and element, falls squarely in the field of logic. The paradox was first published by Russell in \"The principles of mathematics\" (1903) and is discussed there in great detail ...\". Russell, after six years of false starts, would eventually answer the matter with his 1908 theory of types by \"propounding his \"axiom of reducibility\". It says that any function is coextensive with what he calls a \"predicative\" function: a function in which the types of apparent variables run no higher than the types of the arguments\". But this \"axiom\" was met with resistance from all quarters.\n\nThe rejection of impredicatively defined mathematical objects (while accepting the natural numbers as classically understood) leads to the position in the philosophy of mathematics known as predicativism, advocated by Henri Poincaré and Hermann Weyl in his \"Das Kontinuum\". Poincaré and Weyl argued that impredicative definitions are problematic only when one or more underlying sets are infinite.\n\nErnst Zermelo in his 1908 \"A new proof of the possibility of a well-ordering\" presents an entire section \"b. \"Objection concerning nonpredicative definition\"\" where he argued against \"Poincaré (1906, p. 307) [who states that] a definition is 'predicative' and logically admissible only if it \"excludes\" all objects that are dependent upon the notion defined, that is, that can in any way be determined by it\". He gives two examples of impredicative definitions – (i) the notion of Dedekind chains and (ii) \"in analysis wherever the maximum or minimum of a previously defined \"completed\" set of numbers is used for further inferences. This happens, for example, in the well-known Cauchy proof of the fundamental theorem of algebra, and up to now it has not occurred to anyone to regard this as something illogical\". He ends his section with the following observation: \"A definition may very well rely upon notions that are equivalent to the one being defined; indeed, in every definition \"definiens\" and \"definiendum\" are equivalent notions, and the strict observance of Poincaré's demand would make every definition, hence all of science, impossible\".\n\nZermelo's example of minimum and maximum of a previously defined \"completed\" set of numbers reappears in Kleene 1952:42-42 where Kleene uses the example of Least upper bound in his discussion of impredicative definitions; Kleene does not resolve this problem. In the next paragraphs he discusses Weyl's attempt in his 1918 \"Das Kontinuum\" (\"The Continuum\") to eliminate impredicative definitions and his failure to retain the \"theorem that an arbitrary non-empty set of real numbers having an upper bound has a least upper bound (cf. also Weyl 1919)\".\n\nRamsey argued that \"impredicative\" definitions can be harmless: for instance, the definition of \"tallest person in the room\" is impredicative, since it depends on a set of things of which it is an element, namely the set of all persons in the room. Concerning mathematics, an example of an impredicative definition is the smallest number in a set, which is formally defined as: if and only if for all elements of , is less than or equal to , and is in .\n\nBurgess (2005) discusses predicative and impredicative theories at some length, in the context of Frege's logic, Peano arithmetic, second order arithmetic, and axiomatic set theory.\n\n\n"}
{"id": "597476", "url": "https://en.wikipedia.org/wiki?curid=597476", "title": "Info", "text": "Info\n\nInfo is shorthand for \"information\". It may also refer to:\n\n\n"}
{"id": "9549311", "url": "https://en.wikipedia.org/wiki?curid=9549311", "title": "Integrative and Comparative Biology", "text": "Integrative and Comparative Biology\n\nIntegrative and Comparative Biology is the scientific journal for the Society for Integrative and Comparative Biology (formerly the American Society of Zoologists). Prior to volume 42 (2002), the journal was known as American Zoologist .\n\n\n"}
{"id": "5995840", "url": "https://en.wikipedia.org/wiki?curid=5995840", "title": "L. G. Pine", "text": "L. G. Pine\n\nLeslie Gilbert Pine (22 December 1907 – 15 May 1987) was a British author, lecturer, and researcher in the areas of genealogy, nobility, history, heraldry and animal welfare. He was born in 1907 in Bristol, England and died in Bury St. Edmunds, Suffolk in 1987. He was the son of Lilian Grace Beswetherick and Henry Moorshead Pine (a tea merchant).\n\nFrom 1935 to 1940 he served as an assistant editor at Burke's Peerage Ltd. During World War II he was an officer in the Royal Air Force intelligence branch, serving in North Africa, Italy, Greece, and India; he retired with the rank of Squadron Leader. After the war and until 1960, he was Burke's executive director. Pine edited \"Burke's Peerage,\" 1949-1959; \"Burke's Landed Gentry (of Great Britain),\" 1952; \"Burke's Landed Gentry (of Ireland),\" 1958; and, \"Burke's Distinguished Families of America,\" 1939, 1947. He also edited \"The International Year Book and Statesmen's Who's Who,\" 1953-1960; \"Author's and Writer's Who's Who,\" 1948, 1960; \"Who's Who in Music,\" 1949; and, \"Who's Who in the Free Churches,\" 1951.\n\nA graduate of London University, he became a Barrister-at-Law, Inner Temple, in 1953. Pine was a member of the International Institute of Genealogy and Heraldry, Fellow of the Society of Antiquaries of Scotland, a Fellow of the Ancient Monuments Society, a Life Fellow of the Institute of Journalists, a Freeman of the City of London, and a Liveryman of the Glaziers' Company. In 1959 he was the unsuccessful Conservative candidate for Bristol Central.\n\nHe was managing editor of a British hunting magazine, \"Shooting Times\", from 1960 to 1964. He later authored an important book highly critical of sport hunting, \"After Their Blood\", in which he wrote: \"It is our duty as men and women of God’s redeemed creation to try not to increase the suffering of the world, but to lessen it. To get rid of bloodsports will be a great step toward this end.\"\n\nIn 1948 Leslie Pine married Grace V. Griffin (20 August 1914- ). Their only child, Richard Pine, was born in London on 21 August 1949.\n\nHis books include:\n\n\nPine is also the primary contributor to the article \"genealogy\" in \"Encyclopædia Britannica\".\n\n"}
{"id": "51388883", "url": "https://en.wikipedia.org/wiki?curid=51388883", "title": "Life spans of home appliances", "text": "Life spans of home appliances\n\nThis page lists the average life spans of home appliances (major and small).\n\n"}
{"id": "4104172", "url": "https://en.wikipedia.org/wiki?curid=4104172", "title": "List of Latin abbreviations", "text": "List of Latin abbreviations\n\nThis is a list of common Latin abbreviations. Nearly all the abbreviations below have been adopted by Modern English. However, with some exceptions (for example, \"versus\" or \"modus operandi\"), most of the Latin referent words and phrases are perceived as foreign to English. In a few cases, English referents have replaced the original Latin ones (e.g., \"rest in peace\" for RIP and \"post script\" for PS).\n\nLatin was once the universal academic language in Europe. From the 18th century authors started using their mother tongues to write books, papers or proceedings. Even when Latin fell out of use, many Latin abbreviations continued to be used due to their precise simplicity and Latin's status as a learnèd language.\n\nIn July 2016, the government of the United Kingdom announced that its websites would no longer use Latin abbreviations.\n\nAll abbreviations are given with full stops, although these are omitted or included as a personal preference in most situations.\n\nWords and abbreviations that have been in general use, but are currently used less often:\n\n\n\n\n"}
{"id": "33447383", "url": "https://en.wikipedia.org/wiki?curid=33447383", "title": "Metabibliography", "text": "Metabibliography\n\nA metabibliography (or biblio-bibliography) is a bibliography of bibliographies.\n\nBibliographies serve the finding of relevant documents. Metabibliographies serve the finding of the relevant bibliographies in which the relevant documents may be found. One might quote Patrick Wilson:\n\n\"For if knowledge is power, power over knowledge is power to increase one's power; and if the stock of writings is thought of mainly as it represents a stock of knowledge, it is natural to propose treating it as a \"resource\" to be subjected to rational control, managemenet and utilization.\" (Wilson, 1968, p. 145).\n\nMetabibliographies are valuable for building reference collections, but usually of less interest to the average user, who rely on bibliographies selected by others.\n\n\n\n"}
{"id": "177891", "url": "https://en.wikipedia.org/wiki?curid=177891", "title": "Primary source", "text": "Primary source\n\nIn the study of history as an academic discipline, a primary source (also called an original source) is an artifact, document, diary, manuscript, autobiography, recording, or any other source of information that was created at the time under study. It serves as an original source of information about the topic. Similar definitions can be used in library science, and other areas of scholarship, although different fields have somewhat different definitions. In journalism, a primary source can be a person with direct knowledge of a situation, or a document written by such a person.\n\nPrimary sources are distinguished from secondary sources, which cite, comment on, or build upon primary sources. Generally, accounts written after the fact with the benefit (and possible distortions) of hindsight are secondary. A secondary source may also be a primary source depending on how it is used. For example, a memoir would be considered a primary source in research concerning its author or about his or her friends characterized within it, but the same memoir would be a secondary source if it were used to examine the culture in which its author lived. \"Primary\" and \"secondary\" should be understood as relative terms, with sources categorized according to specific historical contexts and what is being studied.\n\nIn scholarly writing, an important objective of classifying sources is to determine their independence and reliability. In contexts such as historical writing, it is almost always advisable to use primary sources and that \"if none are available, it is only with great caution that [the author] may proceed to make use of secondary sources.\" Sreedharan believes that primary sources have the most direct connection to the past and that they \"speak for themselves\" in ways that cannot be captured through the filter of secondary sources.\n\nIn scholarly writing, the objective of classifying sources is to determine the independence and reliability of sources. Though the terms \"primary source\" and \"secondary source\" originated in historiography as a way to trace the history of historical ideas, they have been applied to many other fields. For example, these ideas may be used to trace the history of scientific theories, literary elements and other information that is passed from one author to another.\n\nIn scientific literature, a primary source is the original publication of a scientist's new data, results and theories. In political history, primary sources are documents such as official reports, speeches, pamphlets, posters, or letters by participants, official election returns and eyewitness accounts. In the history of ideas or intellectual history, the main primary sources are books, essays and letters written by intellectuals; these intellectuals may include historians, whose books and essays are therefore considered primary sources for the intellectual historian, though they are secondary sources in their own topical fields. In religious history, the primary sources are religious texts and descriptions of religious ceremonies and rituals.\n\nA study of cultural history could include fictional sources such as novels or plays. In a broader sense primary sources also include artifacts like photographs, newsreels, coins, paintings or buildings created at the time. Historians may also take archaeological artifacts and oral reports and interviews into consideration. Written sources may be divided into three types.\n\n\nIn historiography, when the study of history is subject to historical scrutiny, a secondary source becomes a primary source. For a biography of a historian, that historian's publications would be primary sources. Documentary films can be considered a secondary source or primary source, depending on how much the filmmaker modifies the original sources.\n\nThe Lafayette College Library, provides a synopsis of primary sources in several areas of study:\n\"The definition of a primary source varies depending upon the academic discipline and the context in which it is used.<br>\n\nAlthough many primary sources remain in private hands, others are located in archives, libraries, museums, historical societies, and special collections. These can be public or private. Some are affiliated with universities and colleges, while others are government entities. Materials relating to one area might be spread over a large number of different institutions. These can be distant from the original source of the document. For example, the Huntington Library in California houses a large number of documents from the United Kingdom.\n\nIn the US, digital copies of primary sources can be retrieved from a number of places. The Library of Congress maintains several digital collections where they can be retrieved. Some examples are American Memory and Chronicling America. The National Archives and Records Administration also has digital collections in Digital Vaults. The Digital Public Library of America searches across the digitized primary source collections of many libraries, archives, and museums. The Internet Archive also has primary source materials in many formats.\n\nIn the UK, the National Archives provides a consolidated search of its own catalogue and a wide variety of other archives listed on the Access to Archives index. Digital copies of various classes of documents at the National Archives (including wills) are available from DocumentsOnline. Most of the available documents relate to England and Wales. Some digital copies of primary sources are available from the National Archives of Scotland. Many County Record Offices collections are included in Access to Archives, while others have their own on-line catalogues. Many County Record Offices will supply digital copies of documents.\n\nIn other regions, Europeana has digitized materials from across Europe while the World Digital Library and Flickr Commons have items from all over the world. Trove has primary sources from Australia.\n\nMost primary source materials are not digitized and may only be represented online with a record or finding aid. Both digitized and not digitized materials can be found through catalogs such as WorldCat, the Library of Congress catalog, the National Archives catalog, and so on.\n\nHistory as an academic discipline is based on primary sources, as evaluated by the community of scholars, who report their findings in books, articles and papers. Arthur Marwick says \"Primary sources are absolutely fundamental to history.\" Ideally, a historian will use all available primary sources that were created by the people involved at the time being studied. In practice some sources have been destroyed, while others are not available for research. Perhaps the only eyewitness reports of an event may be memoirs, autobiographies, or oral interviews taken years later. Sometimes the only evidence relating to an event or person in the distant past was written or copied decades or centuries later. Manuscripts that are sources for classical texts can be copies of documents, or fragments of copies of documents. This is a common problem in classical studies, where sometimes only a summary of a book or letter has survived. Potential difficulties with primary sources have the result that history is usually taught in schools using secondary sources.\n\nHistorians studying the modern period with the intention of publishing an academic article prefer to go back to available primary sources and to seek new (in other words, forgotten or lost) ones. Primary sources, whether accurate or not, offer new input into historical questions and most modern history revolves around heavy use of archives and special collections for the purpose of finding useful primary sources. A work on history is not likely to be taken seriously as scholarship if it only cites secondary sources, as it does not indicate that original research has been done.\n\nHowever, primary sources – particularly those from before the 20th century – may have hidden challenges. \"Primary sources, in fact, are usually fragmentary, ambiguous and very difficult to analyse and interpret.\" Obsolete meanings of familiar words and social context are among the traps that await the newcomer to historical studies. For this reason, the interpretation of primary texts is typically taught as part of an advanced college or postgraduate history course, although advanced self-study or informal training is also possible.\n\nThe following questions are asked about primary sources:\n\nIn many fields and contexts, such as historical writing, it is almost always advisable to use primary sources if possible, and \"if none are available, it is only with great caution that [the author] may proceed to make use of secondary sources.\" In addition, primary sources avoid the problem inherent in secondary sources in which each new author may distort and put a new spin on the findings of prior cited authors.\n\nHowever, a primary source is not necessarily more of an authority or better than a secondary source. There can be bias and tacit unconscious views which twist historical information.\n\nParticipants and eyewitnesses may misunderstand events or distort their reports, deliberately or not, to enhance their own image or importance. Such effects can increase over time, as people create a narrative that may not be accurate. For any source, primary or secondary, it is important for the researcher to evaluate the amount and direction of bias. As an example, a government report may be an accurate and unbiased description of events, but it may be censored or altered for propaganda or cover-up purposes. The facts can be distorted to present the opposing sides in a negative light. Barristers are taught that evidence in a court case may be truthful but may still be distorted to support or oppose the position of one of the parties.\n\nMany sources can be considered either primary or secondary, depending on the context in which they are examined. Moreover, the distinction between \"primary\" and \"secondary\" sources is subjective and contextual, so that precise definitions are difficult to make. A book review, when it contains the opinion of the reviewer about the book rather than a summary of the book, becomes a primary source.\n\nIf a historical text discusses old documents to derive a new historical conclusion, it is considered to be a primary source for the new conclusion. Examples in which a source can be both primary and secondary include an obituary or a survey of several volumes of a journal counting the frequency of articles on a certain topic.\n\nWhether a source is regarded as primary or secondary in a given context may change, depending upon the present state of knowledge within the field. For example, if a document refers to the contents of a previous but undiscovered letter, that document may be considered \"primary\", since it is the closest known thing to an original source; but if the letter is later found, it may then be considered \"secondary\"\n\nIn some instances, the reason for identifying a text as the \"primary source\" may devolve from the fact that no copy of the original source material exists, or that it is the oldest extant source for the information cited.\n\nHistorians must occasionally contend with forged documents that purport to be primary sources. These forgeries have usually been constructed with a fraudulent purpose, such as promulgating legal rights, supporting false pedigrees, or promoting particular interpretations of historic events. The investigation of documents to determine their authenticity is called diplomatics.\n\nFor centuries, Popes used the forged Donation of Constantine to bolster the Papacy's secular power. Among the earliest forgeries are false Anglo-Saxon charters, a number of 11th- and 12th-century forgeries produced by monasteries and abbeys to support a claim to land where the original document had been lost or never existed. One particularly unusual forgery of a primary source was perpetrated by Sir Edward Dering, who placed false monumental brasses in a parish church. In 1986, Hugh Trevor-Roper \"authenticated\" the Hitler Diaries, which were later proved to be forgeries. Recently, forged documents have been placed within the UK National Archives in the hope of establishing a false provenance. However, historians dealing with recent centuries rarely encounter forgeries of any importance.\n\n</div>\n\n\n\n"}
{"id": "17892", "url": "https://en.wikipedia.org/wiki?curid=17892", "title": "Reference desk", "text": "Reference desk\n\nThe reference desk or information desk of a library is a public service counter where professional librarians provide library users with direction to library materials, advice on library collections and services, and expertise on multiple kinds of information from multiple sources.\n\nLibrary users can consult the staff at the reference desk for help in finding information. Using a structured reference interview, the librarian works with the library user to clarify their needs and determine what information sources will fill them. To borrow a medical analogy, reference librarians diagnose and treat information deficiencies.\n\nThe ultimate help provided may consist of reading material in the form of a book or journal article, instruction in the use of specific searchable information resources such as the library's online catalog or subscription bibliographic/fulltext databases, or simply factual information drawn from the library's print or online reference collection. Information is also provided to patrons through electronic resources. Typically, a reference desk can be consulted either in person, by telephone, through email or online chat, although a library user may be asked to come to the library in person for help with more involved research questions. A staffed and knowledgeable reference desk is an essential part of a library.\n\nThe services that are provided at a reference desk may vary depending on the type of library, its purpose, its resources, and its staff.\n\nReference services did not become commonplace in libraries until the late 1800s. These services initially began in public libraries. At first librarians were hesitant to offer reference services because many libraries did not have a large enough staff to provide the services without other duties being neglected. Beginning in 1883 with the Boston Public Library, libraries began to hire librarians whose primary duty was to provide reference services.\n\nOne of the earliest proponents of references services was Samuel Swett Green. He wrote an article titled \"Personal Relations Between Librarians and Readers\" which had a large impact on the future of reference services.\nthen, it operated to incorporate ... making the following variables relevant in offering reference services: the user's query; the reference librarian; and, the reference sources. Until hitherto the communication between the reference librarian and the user are through direct contact. Hence, Utor (2008), defined reference services as a direct personal assistance to readers seeking information. That is during the traditional era. towards the later decades of 19th century, however, reference and information services witnessed an insidious yet drastic paradigm-shift following the incorporation of information communication technology in reference services (and in library operations, by extension). Thus leading to an entirely new era, otherwise known as digital era with different information technologies coming in to aid the work of a reference librarian; changing information sources, reference processes and communication medium.\n\nResources that are often kept at a library reference desk may include:\n\nServices that are often available at a library reference desk include:\n\nThe librarian who staffs the reference desk can usually do the following by virtue of their professional training and experience:\n\nIn the United States, those who staff library reference desks are usually required to have a master's degree in library science from a program accredited by the American Library Association. However, if there is a lack of qualified applicants, particularly in rural areas, a person with an associate degree, a certificate in library technology, or a bachelor's degree in library science may perform these duties. In many academic libraries, student assistants are used as the primary contact, sometimes at an \"information desk.\"\n\nIn Sri Lanka, librarians at reference desks typically have master's degrees from the Sri Lankan Library Association's accredited programs.\n\nWith the development of the Web, digital reference services are beginning to take over some of the roles of the traditional reference desk in a library. There is disagreement over whether or not this development is desirable or inevitable.\n\n\n"}
{"id": "8912106", "url": "https://en.wikipedia.org/wiki?curid=8912106", "title": "Reference scenario", "text": "Reference scenario\n\nA reference scenario is an imagined situation where a library patron brings a question to a librarian and there is then a conversation, called in the field a reference interview, where the librarian works to help the patron find what he or she wants. These scenarios are used in training future librarians how to help patrons. Basically, a scenario is as short as a couple of sentences, including a question and a situation that underlies that question.\n\nA great deal of reference teaching puts students to researching the answers to made-up questions. This focuses the student on learning about the reference sources at hand by using them to answer those questions. Scenarios are something different. They focus the student on the interaction with patrons. In class practice sessions, one student can be the patron and the other the librarian, as long as the one practicing as the librarian doesn't know the whole scenario in advance.\n\nScenarios are valued because often the question asked is not the end of the patron's information hunt, but the start. Patrons often start by voicing a question that they think the library can answer, rather than the question they are really seeking to answer. Or they pose a question that the librarian doesn't understand. Reference librarian skills are very much about mediating a gap between what the patron wants and what the library can provide. This can involve the librarian making him or herself a partner in the patron's search, teaching them what the library really has to offer, or even just clarifying a confusing word: Does the patron want information about soaps to clean with or soaps as in soap operas?\n\n\n"}
{"id": "56067306", "url": "https://en.wikipedia.org/wiki?curid=56067306", "title": "SDS-PAGE", "text": "SDS-PAGE\n\nSDS-PAGE (sodium dodecyl sulfate–polyacrylamide gel electrophoresis) is a variant of polyacrylamide gel electrophoresis, an analytical method in biochemistry for the separation of charged molecules in mixtures by their molecular masses in an electric field. It uses sodium dodecyl sulfate (SDS) molecules to help identify and isolate protein molecules.\n\nSDS-PAGE is a discontinuous electrophoretic system developed by Ulrich K. Laemmli which is commonly used as a method to separate proteins with molecular masses between 5 and 250 KDa. The publication describing it is the most frequently cited paper by a single author, and the second most cited overall.\n\nSDS-PAGE is an electrophoresis method that allows protein separation by mass. The medium (also referred to as ′matrix′) is a polyacrylamide-based discontinuous gel. In addition, SDS (sodium dodecyl sulfate) is used. About 1.4 grams of SDS bind to a gram of protein, corresponding to one SDS molecule per two amino acids. SDS acts as a surfactant, covering the proteins' intrinsic charge and conferring them very similar charge-to-mass ratios. The intrinsic charges of the proteins are negligible in comparison to the SDS loading, and the positive charges are also greatly reduced in the basic pH range of a separating gel. Upon application of a constant electric field, the protein migrate towards the anode, each with a different speed, depending on its mass. This simple procedure allows precise protein separation by mass.\n\nSDS tends to form spherical micelles in aqueous solutions above a certain concentration called the critical micellar concentration (CMC). Above the critical micellar concentration of 7 to 10 millimolar in solutions, the SDS simultaneously occurs as single molecules (monomer) and as micelles, below the CMC SDS occurs only as monomers in aqueous solutions. At the critical micellar concentration, a micelle consists of about 62 SDS molecules. However, only SDS monomers bind to proteins via hydrophobic interactions, whereas the SDS micelles are anionic on the outside and do not adsorb any protein. SDS is amphipathic in nature, which allows it to unfold both polar and nonpolar sections of protein structure. In SDS concentrations above 0.1 millimolar, the unfolding of proteins begins, and above 1 mM, most proteins are denatured. Due to the strong denaturing effect of SDS and the subsequent dissociation of protein complexes, quaternary structures can generally not be determined with SDS. Exceptions are e.g. proteins that were previously stabilised by covalent cross-linking and the SDS-resistant protein complexes, which are stable even in the presence of SDS (the latter, however, only at room temperature). To denature the SDS-resistant complexes a high activation energy is required, which is achieved by heating. SDS resistance is based on a metastability of the protein fold. Although the native, fully folded, SDS-resistant protein does not have sufficient stability in the presence of SDS, the chemical equilibrium of denaturation at room temperature occurs slowly. Stable protein complexes are characterised not only by SDS resistance but also by stability against proteases and an increased biological half-life.\n\nAlternatively, polyacrylamide gel electrophoresis can also be performed with the cationic surfactants CTAB in a CTAB-PAGE, or 16-BAC in a BAC-PAGE.\n\nThe SDS-PAGE method is composed of gel preparation, sample preparation, electrophoresis, protein staining or western blotting and analysis of the generated banding pattern.\n\nWhen using different buffers in the gel (discontinuous gel electrophoresis), the gels are made up to one day prior to electrophoresis, so that the diffusion does not lead to a mixing of the buffers. The gel is produced by radical polymerisation in a mold consisting of two sealed glass plates with spacers between the glass plates. In a typical mini-gel setting, the spacers have a thickness of 0.75 mm or 1.5 mm, which determines the loading capacity of the gel. For pouring the gel solution, the plates are usually clamped in a stand which temporarily seals the otherwise open underside of the glass plates with the two spacers. For the gel solution, acrylamide is mixed as gel-former (usually 4% V/V in the stacking gel and 10-12 % in the separating gel), methylenebisacrylamide as a cross-linker, stacking or separating gel buffer, water and SDS. By adding the catalyst TEMED and the radical initiator ammonium persulfate (APS) the polymerisation is started. The solution is then poured between the glass plates without creating bubbles. Depending on the amount of catalyst and radical starter and depending on the temperature, the polymerisation lasts between a quarter of an hour and several hours. The lower gel (separating gel) is poured first and covered with a few drops of a barely water-soluble alcohol (usually buffer-saturated butanol or isopropanol), which eliminates bubbles from the meniscus and protects the gel solution of the radical scavenger oxygen. After the polymerisation of the separating gel, the alcohol is discarded and the residual alcohol is removed with filter paper. After addition of APS and TEMED to the stacking gel solution, it is poured on top of the solid separation gel. Afterwards, a suitable sample comb is inserted between the glass plates without creating bubbles. The sample comb is carefully pulled out after polymerisation, leaving pockets for the sample application. For later use of proteins for protein sequencing, the gels are often prepared the day before electrophoresis to reduce reactions of unpolymerised acrylamide with cysteines in proteins.\n\nBy using a gradient mixer, gradient gels with a gradient of acrylamide (usually from 4 to 12%) can be cast, which have a larger separation range of the molecular masses. Commercial gel systems (so-called \"pre-cast gels\") usually use the buffer substance Bis-tris methane with a pH value between 6.4 and 7.2 both in the stacking gel and in the separating gel. These gels are delivered cast and ready-to-use. Since they use only one buffer (continuous gel electrophoresis) and have a nearly neutral pH, they can be stored for several weeks. The more neutral pH slows the hydrolysis and thus the decomposition of the polyacrylamide. Furthermore, there are fewer acrylamide-modified cysteines in the proteins. Due to the constant pH in collecting and separating gel there is no stacking effect. Proteins in BisTris gels can not be stained with ruthenium complexes. This gel system has a comparatively large separation range, which can be varied by using MES or MOPS in the running buffer.\n\nDuring sample preparation, the sample buffer, and thus SDS, is added in excess to the proteins, and the sample is then heated to 95 °C for five minutes, or alternatively 70°C for ten minutes. Heating disrupts the secondary and tertiary structures of the protein by disrupting hydrogen bonds and stretching the molecules. Optionally, disulfide bridges can be cleaved by reduction. For this purpose, reducing thiols such as β-mercaptoethanol (β-ME, 5% by volume), dithiothreitol (DTT, 10 millimolar) or dithioerythritol (DTE, 10 millimolar) are added to the sample buffer. After cooling to room temperature, each sample is pipetted into its own well in the gel, which was previously immersed in electrophoresis buffer in the electrophoresis apparatus.\n\nIn addition to the samples, a molecular-weight size marker is usually loaded onto the gel. This consists of proteins of known sizes and thereby allows the estimation (with an error of ± 10%) of the sizes of the proteins in the actual samples, which migrate in parallel in different tracks of the gel. The size marker is often pipetted into the first or last pocket of a gel.\n\nFor separation, the denatured samples are loaded onto a gel of polyacrylamide, which is placed in an electrophoresis buffer with suitable electrolytes. Thereafter, a voltage (usually around 100 V, 10-20 V per cm gel length) is applied, which causes a migration of negatively charged molecules through the gel in the direction of the positively charged anode. The gel acts like a sieve. Small proteins migrate relatively easily through the mesh of the gel, while larger proteins are more likely to be retained and thereby migrate more slowly through the gel, thereby allowing proteins to be separated by molecular size. The electrophoresis lasts between half an hour to several hours depending on the voltage and length of gel used.\n\nThe fastest-migrating proteins (with a molecular weight of less than 5 KDa) form the buffer front together with the anionic components of the electrophoresis buffer, which also migrate through the gel. The area of the buffer front is made visible by adding the comparatively small, anionic dye bromophenol blue to the sample buffer. Due to the relatively small molecule size of bromophenol blue, it migrates faster than proteins. By optical control of the migrating colored band, the electrophoresis can be stopped before the dye and also the samples have completely migrated through the gel and leave it.\n\nThe most commonly used method is the discontinuous SDS-PAGE. In this method, the proteins migrate first into a collecting gel with neutral pH, in which they are concentrated and then they migrate into a separating gel with basic pH, in which the actual separation takes place. Stacking and separating gels differ by different pore size (4-6 % T and 10-20 % T), ionic strength and pH values (pH 6.8 or pH 8.8). The electrolyte most frequently used is an SDS-containing Tris-glycine-chloride buffer system. At neutral pH, glycine predominantly forms the zwitterionic form, at high pH the glycines lose positive charges and become predominantly anionic. In the collection gel, the smaller, negatively charged chloride ions migrate in front of the proteins (as leading ions) and the slightly larger, negatively and partially positively charged glycinate ions migrate behind the proteins (as initial trailing ions), whereas in the comparatively basic separating gel both ions migrate in front of the proteins. The pH gradient between the stacking and separation gel buffers leads to a stacking effect at the border of the stacking gel to the separation gel, since the glycinate partially loses its slowing positive charges as the pH increases and then, as the former trailing ion, overtakes the proteins and becomes a leading ion, which causes the bands of the different proteins (visible after a staining) to become narrower and sharper - the stacking effect. For the separation of smaller proteins and peptides, the TRIS-Tricine buffer system of Schägger and von Jagow is used due to the higher spread of the proteins in the range of 0.5 to 50 KDa.\n\nAt the end of the electrophoretic separation, all proteins are sorted by size and can then be analyzed by other methods, e. g. protein staining such as Coomassie staining (most common and easy to use), silver staining (highest sensitivity), stains all staining, Amido black 10B staining, Fast green FCF staining, fluorescent stains such as epicocconone stain and SYPRO orange stain, and immunological detection such as the Western Blot. The fluorescent dyes have a comparatively higher linearity between protein quantity and color intensity of about three orders of magnitude above the detection limit, i. e. the amount of protein can be estimated by color intensity. When using the fluorescent protein dye trichloroethanol, a subsequent protein staining is omitted if it was added to the gel solution and the gel was irradiated with UV light after electrophoresis.\n\nProtein staining in the gel creates a documentable banding pattern of the various proteins. Glycoproteins have differential levels of glycosylations and adsorb SDS more unevenly at the glycosylations, resulting in broader and blurred bands. Membrane proteins, because of their transmembrane domain, are often composed of the more hydrophobic amino acids, have lower solubility in aqueous solutions, tend to bind lipids, and tend to precipitate in aqueous solutions due to hydrophobic effects when sufficient amounts of detergent are not present. This precipitation manifests itself for membrane proteins in a SDS-PAGE in \"tailing\" above the band of the transmembrane protein. In this case, more SDS can be used (by using more or more concentrated sample buffer) and the amount of protein in the sample application can be reduced. An overloading of the gel with a soluble protein creates a semicircular band of this protein (e. g. in the marker lane of the image at 66 KDa), allowing other proteins with similar molecular weights to be covered. A low contrast (as in the marker lane of the image) between bands within a lane indicates either the presence of many proteins (low purity) or, if using purified proteins and a low contrast occurs only below one band, it indicates a proteolytic degradation of the protein, which first causes degradation bands, and after further degradation produces a homogeneous color (\"smear\") below a band. The documentation of the banding pattern is usually done by photographing or scanning. For a subsequent recovery of the molecules in individual bands, a gel extraction can be performed.\n\nAfter protein staining and documentation of the banding pattern, the polyacrylamide gel can be dried for archival storage. Proteins can be extracted from it at a later date. The gel is either placed in a drying frame (with or without the use of heat) or in a vacuum dryer. The drying frame consists of two parts, one of which serves as a base for a wet cellophane film to which the gel and a one percent glycerol solution are added. Then a second wet cellophane film is applied bubble-free, the second frame part is put on top and the frame is sealed with clips. The removal of the air bubbles avoids a fragmentation of the gel during drying. The water evaporates through the cellophane film. In contrast to the drying frame, a vacuum dryer generates a vacuum and heats the gel to about 50 °C.\n\nFor a more accurate determination of the molecular weight, the relative migration distances of the individual protein bands are measured in the separating gel. The measurements are usually performed in triplicate for increased accuracy. The relative mobility (called Rf value or Rm value) is the quotient of the distance of the band of the protein and the distance of the buffer front. The distances of the bands and the buffer front are each measured from the beginning of the separation gel. The distance of the buffer front roughly corresponds to the distance of the bromophenol blue contained in the sample buffer. The relative distances of the proteins of the size marker are plotted semi-logarithmically against their known molecular weights. By comparison with the linear part of the generated graph or by a regression analysis, the molecular weight of an unknown protein can be determined by its relative mobility. Bands of proteins with glycosylations can be blurred. Proteins with many basic amino acids (e. g. histones) can lead to an overestimation of the molecular weight or even not migrate into the gel at all, because they move slower in the electrophoresis due to the positive charges or even to the opposite direction. Accordingly, many acidic amino acids can lead to accelerated migration of a protein and an underestimation of its molecular mass.\n\nThe SDS-PAGE in combination with a protein stain is widely used in biochemistry for the quick and exact separation and subsequent analysis of proteins. It has comparatively low instrument and reagent costs and is an easy-to-use method. Because of its low scalability, it is mostly used for analytical purposes and less for preparative purposes, especially when larger amounts of a protein are to be isolated.\n\nAdditionally, SDS-PAGE is used in combination with the western blot for the determination of the presence of a specific protein in a mixture of proteins - or for the analysis of post-translational modifications. Post-translational modifications of proteins can lead to a different relative mobility (i.e. a \"band shift\") or to a change in the binding of a detection antibody used in the western blot (i.e. a band disappears or appears).\n\nIn mass spectrometry of proteins, SDS-PAGE is a widely used method for sample preparation prior to spectrometry, mostly using in-gel digestion. In regards to determining the molecular mass of a protein, the SDS-PAGE is a bit more exact than an analytical ultracentrifugation, but less exact than a mass spectrometry or - ignoring post-translational modifications - a calculation of the protein molecular mass from the DNA sequence.\n\nIn medical diagnostics, SDS-PAGE is used as part of the HIV test and to evaluate proteinuria. In the HIV test, HIV proteins are separated by SDS-PAGE and subsequently detected by Western Blot with HIV-specific antibodies of the patient, if they are present in his blood serum. SDS-PAGE for proteinuria evaluates the levels of various serum proteins in the urine, e.g. Albumin, Alpha-2-macroglobulin and IgG.\n\nSDS-PAGE is the most widely used method for gel electrophoretic separation of proteins. Two-dimensional gel electrophoresis sequentially combines isoelectric focusing or BAC-PAGE with a SDS-PAGE. Native PAGE is used if native protein folding is to be maintained. For separation of membrane proteins, BAC-PAGE or CTAB-PAGE may be used as an alternative to SDS-PAGE. For electrophoretic separation of larger protein complexes, agarose gel electrophoresis can be used, e.g. the SDD-AGE. Some enzymes can be detected via their enzyme activity by zymography.\n\nWhile being one of the more precise and low-cost protein separation and analysis methods, the SDS-PAGE denatures proteins. Where non-denaturing conditions are necessary, proteins are separated by a native PAGE or different chromatographic methods with subsequent photometric quantification, for example affinity chromatography (or even tandem affinity purification), size exclusion chromatography, ion exchange chromatography. Proteins can also be separated by size in a tangential flow filtration or a ultrafiltration. Single proteins can be isolated from a mixture by affinity chromatography or by a pull-down assay. Some historically early and cost effective but crude separation methods usually based upon a series of extractions and precipitations using kosmotropic molecules, for example the ammonium sulfate precipitation and the polyethyleneglycol precipitation.\n\nIn 1948, Arne Tiselius was awarded the Nobel Prize in Chemistry for the discovery of the principle of electrophoresis as the migration of charged and dissolved atoms or molecules in an electric field. The use of a solid matrix (initially paper discs) in a zone electrophoresis improved the separation. The discontinuous electrophoresis of 1964 by L. Ornstein and B. J. Davis made it possible to improve the separation by the stacking effect. The use of cross-linked polyacrylamide hydrogels, in contrast to the previously used paper discs or starch gels, provided a higher stability of the gel and no microbial decomposition. The denaturing effect of SDS in continuous polyacrylamide gels and the consequent improvement in resolution was first described in 1965 by David F. Summers in the working group of James E. Darnell to separate poliovirus proteins. The current variant of the SDS-PAGE was described in 1970 by Ulrich K. Laemmli and initially used to characterise the proteins in the head of bacteriophage T4.\n\n"}
{"id": "1130951", "url": "https://en.wikipedia.org/wiki?curid=1130951", "title": "Scribal abbreviation", "text": "Scribal abbreviation\n\nScribal abbreviations or sigla (singular: siglum) are the abbreviations used by ancient and medieval scribes writing in Latin, and later in Greek and Old Norse. In modern manuscript editing (substantive and mechanical) \"sigla\" are the symbols used to indicate the source manuscript (e.g. variations in text between different such manuscripts) and to identify the copyist(s) of a work. See Critical apparatus.\n\nAbbreviated writing, using \"sigla\", arose partly from the limitations of the workable nature of the materials (stone, metal, parchment, etc.) employed in record-making and partly from their availability. Thus, lapidaries, engravers, and copyists made the most of the available writing space. Scribal abbreviations were infrequent when writing materials were plentiful, but by the 3rd and 4th centuries AD, writing materials were scarce and costly.\n\nDuring the Roman Republic, several abbreviations, known as \"sigla\" (plural of \"siglum\" = symbol or abbreviation), were in common use in inscriptions, and they increased in number during the Roman Empire. Additionally, in this period shorthand entered general usage. The earliest known Western shorthand system was that employed by the Greek historian Xenophon in the memoir of Socrates, and it was called \"notae socratae\". In the late Roman Republic, the Tironian notes were developed possibly by Marcus Tullius Tiro, Cicero's amanuensis, in 63 BC to record information with fewer symbols; Tironian notes include a shorthand/syllabic alphabet notation different from the Latin minuscule hand and square and rustic capital letters. The notation was akin to modern stenographic writing systems. It used symbols for whole words or word roots and grammatical modifier marks, and it could be used to write either whole passages in shorthand or only certain words. In medieval times, the symbols to represent words were widely used; and the initial symbols, as few as 140 according to some sources, were increased to 14,000 by the Carolingians, who used them in conjunction with other abbreviations. However, the alphabet notation had a \"murky existence\" (C. Burnett), as it was often associated with witchcraft and magic, and it was eventually forgotten. Interest in it was rekindled by the Archbishop of Canterbury Thomas Becket in the 12th century and later in the 15th century, when it was rediscovered by Johannes Trithemius, abbot of the Benedictine abbey of Sponheim, in a psalm written entirely in Tironian shorthand and a Ciceronian lexicon, which was discovered in a Benedictine monastery (\"notae benenses\").\n\nTo learn the Tironian note system, scribes required formal schooling in some 4,000 symbols; this later increased to some 5,000 symbols and then to some 13,000 in the medieval period (4th to 15th centuries AD); the meanings of some characters remain uncertain. \"Sigla\" were mostly used in lapidary inscriptions; in some places and historical periods (such as medieval Spain) scribal abbreviations were overused to the extent that some are indecipherable.\n\nThe abbreviations were not constant but changed from region to region. Scribal abbreviations increased in usage and reached their height in the Carolingian Renaissance (8th to 10th centuries). The most common abbreviations, called \"notae communes\", were used across most of Europe, but others appeared in certain regions. In legal documents, legal abbreviations, called \"notae juris\", appear but also capricious abbreviations, which scribes manufactured \"ad hoc\" to avoid repeating names and places in a given document.\n\nScribal abbreviations can be found in epigraphy, sacred and legal manuscripts, written in Latin or in a vernacular tongue (but less frequently and with fewer abbreviations), either calligraphically or not.\n\nIn epigraphy, common abbreviations were comprehended in two observed classes:\n\nBoth forms of abbreviation are called \"suspensions\" (as the scribe suspends the writing of the word). A separate form of abbreviation is by \"contraction\" and was mostly a Christian usage for sacred words, Nomina Sacra; non-Christian sigla usage usually limited the number of letters the abbreviation comprised and omitted no intermediate letter. One practice was rendering an overused, formulaic phrase only as a siglum: DM for \"Dis Manibus\" (\"Dedicated to the Manes\"); IHS from the first three letters of \"ΙΗΣΟΥΣ\"; and RIP for \"requiescat in pace\" (\"Rest in Peace\") because the long-form written usage of the abbreviated phrase, by itself, was rare. According to Trabe, these abbreviations are not really meant to lighten the burden of the scribe but rather to shroud in reverent obscurity the holiest words of the Christian religion.\n\nAnother practice was repeating the abbreviation's final consonant a given number of times to indicate a group of as many persons: AVG denoted \"Augustus\", thus, AVGG denoted \"Augusti duo\"; however, lapidaries took typographic liberties with that rule, and instead of using COSS to denote \"Consulibus duobus\", they invented the CCSS form. Still, when occasion required referring to three or four persons, the complex doubling of the final consonant yielded to the simple plural siglum. To that effect, a \"vinculum\" (overbar) above a letter or a letter-set also was so used, becoming a universal medieval typographic usage. Likewise the \"tilde\" (~), an undulated, curved-end line, came into standard late-medieval usage.\n\nBesides the \"tilde\" and macron marks, above and below letters, modifying cross-bars and extended strokes were employed as scribal abbreviation marks, mostly for prefixes and verb, noun and adjective suffixes. The \"typographic\" abbreviations should not be confused with the \"phrasal\" abbreviations: i.e. (\"id est\" — \"that is\"); loc. cit. (\"loco citato\" — \"in the passage already cited\"); viz. (\"vide licet\" — \"namely\", \"that is to say\", \"in other words\" — formed with \"vi\" and the \"yogh\"-like glyph [Ꝫ], [ꝫ], the siglum for the suffix -et and the conjunction et) and et cetera.\n\nMoreover, besides scribal abbreviations, ancient texts also contained variant typographic characters, including ligatures (e.g. Æ, Œ, etc.), the long s (ſ), and the half r, resembling an Arabic numeral two (\"2\"). The \"u\" and \"v\" characters originated as scribal variants for their respective letters, likewise the \"i\" and \"j\" pair. Modern publishers printing Latin-language works replace variant typography and sigla with full-form Latin spellings; the convention of using \"u\" and \"i\" for vowels and \"v\" and \"j\" for consonants is a late typographic development.\n\nSome ancient and medieval sigla are still used in English and other European languages; the Latin ampersand (&) replaces the conjunction \"and\" in English, \"et\" in Latin and French, and \"y\" in Spanish (but its use in Spanish is frowned upon, since the \"y\" is already smaller and easier to write). The Tironian sign ⁊, resembling the digit seven (\"7\"), represents the conjunction \"et\" and is written only to the x-height; in current Irish language usage, the siglum denotes the conjunction \"agus\" (\"and\"). Other scribal abbreviations in modern typographic use are the percentage sign (%), from the Italian \"per cento\" (\"per hundred\"); the permille sign (‰), from the Italian \"per mille\" (\"per thousand\"); the pound sign (₤, £ and #, all descending from ℔ or lb, \"librum\") and the dollar sign ($), which possibly derives from the Spanish word \"Peso\". The commercial at symbol (@), originally denoting \"at the rate/price of\", is a ligature derived from the English preposition \"at\"; from the 1990s, its use outside commerce became widespread, as part of e-mail addresses.\n\nTypographically, the ampersand (\"&\"), representing the word \"et\", is a space-saving ligature of the letters \"e\" and \"t\", its component graphemes. Since the establishment of movable-type printing in the 15th century, founders have created many such ligatures for each set of record type (font) to communicate much information with fewer symbols. Moreover, during the Renaissance (14th to 17th centuries), when Ancient Greek language manuscripts introduced that tongue to Western Europe, its scribal abbreviations were converted to ligatures in imitation of the Latin scribal writing to which readers were accustomed. Later, in the 16th century, when the culture of publishing included Europe's vernacular languages, Graeco-Roman scribal abbreviations disappeared, an ideologic deletion ascribed to the anti-Latinist Protestant Reformation (1517–1648).\n\nThe common abbreviation \"Xmas,\" for Christmas, is a remnant of an old scribal abbreviation that substituted the Greek letter chi (Χ, resembling Latin X and representing the first letter in the Greek word for Christ, Χριστος) for the word Christ.\n\nAfter the invention of printing, manuscript copying abbreviations continued to be employed in Church Slavonic and are still in use in printed books as well as on icons and inscriptions. Many common long roots and nouns describing sacred persons are abbreviated and written under the special diacritic symbol titlo, as shown in the figure at the right. That corresponds to the Nomina sacra (Latin: \"Sacred names\") tradition of using contractions for certain frequently-occurring names in Greek ecclesiastical texts. However, sigla for personal nouns are restricted to \"good\" beings and the same words, when referring to \"bad\" beings, are spelled out; for example, while \"God\" in the sense of the one true God is abbreviated as \"\", \"god\" referring to \"false\" gods is spelled out. Likewise, the word for \"angel\" is generally abbreviated as \"\", but the word for \"angels\" is spelled out for \"performed by evil angels\" in Psalm 77.\n\nAdriano Cappelli's \"Lexicon Abbreviaturarum\", enumerates the various medieval brachigraphic signs found in Latin and Italian vulgar texts, which originate from the Roman sigla, a symbol to express a word, and Tironian notes. Quite rarely, abbreviations did not carry marks to indicate that an abbreviation has occurred: if they did, they were often copying errors. For example, \"e.g.\" is written with periods, but modern terms, such as \"PC\", may be written in uppercase.\n\nIt should be noted that the original manuscripts were not written in a modern sans-serif or serif font but in Roman capitals, rustic, uncial, insular, Carolingian or blackletter styles. For more, refer to Western calligraphy or a beginner's guide.\n\nAdditionally, the abbreviations employed varied across Europe. In Nordic texts, for instance, two runes were used in text written in the Latin alphabet, which are ᚠ for \"fé\" \"cattle, goods\" and ᛘ for \"maðr\" \"man\".\n\nCappelli divides abbreviations into six overlapping categories:\n\nSuspended terms are those of which only the first part is written, and the last part is substituted by a mark, which can be of two types:\n\nThe largest class of suspensions consists of single letters standing in for words that begin with that letter.\n\nA dot at the baseline after a capital letter may stand for a title if it is used such as in front of names or a person's name in medieval legal documents. However, not all sigla use the beginning of the word.\nFor plural words, the siglum is often doubled: \"F.\" = \"frater\" and \"FF.\" = \"fratres\". Tripled sigla often stand for three: \"DDD\" = \"domini tres\".\n\nLetters lying on their sides, or mirrored (backwards), often indicate female titles, but a mirrored C, Ↄ, stands generally for \"con\" or \"contra\" (the latter sometimes with a macron above, \"Ↄ̄\").\n\nTo avoid confusion with abbreviations and numerals, the latter are often written with a bar above. In some contexts, however, numbers with a line above indicate that number is to be multiplied by a thousand, and several other abbreviations also have a line above them, such as \"ΧΡ\" (Greek letters chi+rho) = \"Christus\" or \"IHS\" = \"Jesus\".\n\nStarting in the 8th or the 9th century, single letter sigla grew less common and were replaced by longer, less-ambiguous sigla, with bars above them.\n\nAbbreviations by contraction have one or more middle letters omitted. They were often represented with a general mark of abbreviation (above), such as a line above. They can be divided into two subtypes:\n\nSuch marks inform the reader of the identity of the missing part of the word without affecting (\"independent\" of) the meaning. Some of them may be interpreted as alternative contextual glyphs of their respective letters.\n\nThe meaning of the marks depends on the letter on which they appear.\n\nA superscript letter generally referred to the letter omitted, but, in some instances, as in the case of vowel letters, it could refer to a missing vowel combined with the letter \"r\", before or after it. It is only in some English dialects that the letter \"r\" before another consonant largely silent and the preceding vowel is \"r-coloured\".\n\nHowever, \"a\", \"i\", and \"o\" above \"g\" meant \"gͣ\" \"gna\", \"gͥ\" \"gni\" and \"gͦ\" \"gno\" respectively. Although in English, the \"g\" is silent in \"gn\", but in other languages, it is pronounced. Vowel letters above \"q\" meant \"qu\" + vowel: \"qͣ\", \"qͤ\", \"qͥ\", \"qͦ\", \"qͧ\".\n\n\nVowels were the most common superscripts, but consonants could be placed above letters without ascenders; the most common were \"c\", e.g. \"nͨ\". A cut \"l\" above an \"n\", \"nᷝ\", meant \"nihil\" for instance.\n\nThese marks are nonalphabetic letters carrying a particular meaning. Several of them continue in modern usage, as in the case of monetary symbols. In Unicode, they are referred to as \"letter-like glyphs\". Additionally, several authors are of the view that the Roman numerals themselves were, for example, nothing less than abbreviations of the words for those numbers. Other examples of symbols still in some use are alchemical and zodiac symbols, which were, in any case, employed only in alchemy and astrology texts, which made their appearance beyond that special context rare.\n\nIn addition to the signs used to signify abbreviations, medieval manuscripts feature some glyphs that are now uncommon but were not sigla.\nMany more ligatures were used to reduce the space occupied, a characteristic that is particularly prominent in blackletter scripts.\nSome such as r rotunda, long s and uncial or insular variants (Insular G), Claudian letters were in common use, as well as letters derived from other scripts such as Nordic runes: thorn (þ=th) and eth (ð=dh).\nAn illuminated manuscript would feature miniatures, decorated initials or \"littera notabilior\", which later resulted in the bicamerality of the script (case distinction).\n\nVarious typefaces have been designed to allow scribal abbreviations and other archaic glyphs to be replicated in print. They include \"record type\", which was first developed in the 1770s to publish Domesday Book and was fairly widely used for the publication of medieval records in Britain until the end of the 19th century.\n\nIn the Unicode Standard v. 5.1 (4 April 2008), 152 medieval and classical glyphs were given specific locations outside of the Private Use Area. Specifically, they are located in the charts \"Combining Diacritical Marks Supplement\" (26 characters), \"Latin Extended Additional\" (10 characters), \"Supplemental Punctuation\" (15 characters), \"Ancient Symbols\" (12 characters) and especially \"Latin Extended-D\" (89 characters).\nThese consist in both precomposed characters and modifiers for other characters, called combining diacritical marks (such as writing in LaTeX or using overstrike in MS Word).\n\nCharacters are \"the smallest components of written language that have semantic value\" but glyphs are \"the shapes that characters can have when they are rendered or displayed\".\n\n\n\n"}
{"id": "28545", "url": "https://en.wikipedia.org/wiki?curid=28545", "title": "Self-reference", "text": "Self-reference\n\nSelf-reference occurs in natural or formal languages when a sentence, idea or formula refers to itself. The reference may be expressed either directly—through some intermediate sentence or formula—or by means of some encoding. In philosophy, it also refers to the ability of a subject to speak of or refer to itself: to have the kind of thought expressed by the first person nominative singular pronoun, the word \"I\" in English.\n\nSelf-reference is studied and has applications in mathematics, philosophy, computer programming, and linguistics. Self-referential statements are sometimes paradoxical, and can also be considered recursive.\n\nIn classical philosophy, paradoxes were created by self-referential concepts such as the omnipotence paradox of asking if it was possible for a being to exist so powerful that it could create a stone that it could not lift. The Epimenides paradox, 'All Cretans are liars' when uttered by an ancient Greek Cretan was one of the first recorded versions. Contemporary philosophy sometimes employs the same technique to demonstrate that a supposed concept is meaningless or ill-defined.\n\nIn mathematics and computability theory, self-reference (also known as Impredicativity) is the key concept in proving limitations of many systems. Gödel's theorem uses it to show that no formal consistent system of mathematics can ever contain all possible mathematical truths, because it cannot prove some truths about its own structure. The halting problem equivalent, in computation theory, shows that there is always some task that a computer cannot perform, namely reasoning about itself. These proofs relate to a long tradition of mathematical paradoxes such as Russell's paradox and Berry's paradox, and ultimately to classical philosophical paradoxes.\n\nIn game theory undefined behaviors can occur where two players must model each other's mental states and behaviors, leading to infinite regress.\n\nIn computer programming, self-reference occurs in reflection, where a program can read or modify its own instructions like any other data. Numerous programming languages support reflection to some extent with varying degrees of expressiveness. Additionally, self-reference is seen in recursion (related to the mathematical recurrence relation) in functional programming, where a code structure refers back to itself during computation. 'Taming' self-reference from potentially paradoxical concepts into well-behaved recursions has been one of the great successes of computer science, and is now used routinely in, for example, writing compilers using the 'meta-language' ML. Using a compiler to compile itself is known as bootstrapping. Self-modifying code is possible to write (programs which operate on themselves), both with assembler and with functional languages such as Lisp, but is generally discouraged in real-world programming. Computing hardware makes fundamental use of self-reference in flip-flops, the basic units of digital memory, which convert potentially paradoxical logical self-relations into memory by expanding their terms over time. Thinking in terms of self-reference is a pervasive part of programmer culture, with many programs and acronyms named self-referentially as a form of humor, such as GNU ('Gnu's not Unix') and PINE ('Pine is not Elm'). The GNU Hurd is named for a pair of mutually self-referential acronyms.\n\nTupper's self-referential formula is a mathematical curiosity which plots an image of its own formula.\n\nThe biology of self-replication is self-referential, as embodied by DNA and RNA replication mechanisms. Models of self-replication are found in the computational Game of life, and have inspired engineering systems such as the RepRap self-replicating 3d printer.\n\nSelf-reference occurs in literature and film when an author refers to his or her own work in the context of the work itself. Examples include Cervantes's \"Don Quixote\", Shakespeare's \"A Midsummer Night's Dream\", \"The Tempest\" and \"Twelfth Night\", Denis Diderot's \"Jacques le fataliste et son maître\", Italo Calvino's \"If on a winter's night a traveler\", many stories by Nikolai Gogol, \"Lost in the Funhouse\" by John Barth, Luigi Pirandello's \"Six Characters in Search of an Author\" and Federico Fellini's \"8½\". Perhaps the earliest example is in Homer's Iliad, where Helen of Troy laments: \"for generations still unborn/we will live in song\" (appearing in the song itself).\n\nSelf-reference in art is closely related to the concepts of breaking the fourth wall and meta-reference, which often involve self-reference. The short stories of Jorge Luis Borges play with self-reference and related paradoxes in many ways. Samuel Beckett's Krapp's Last Tape consists entirely of the protagonist listening to and making recordings of himself, mostly about other recordings. During the 1990s and 2000s filmic self-reference was a popular part of the rubber reality movement, notably in Charlie Kaufman's films Being John Malkovich and Adaptation, the latter pushing the concept arguably to its breaking point as it attempts to portray its own creation.\n\nVarious creation myths invoke self-reference to solve the problem of what created the creator. For example the Egyptian creation myth has a god swallowing his own semen to create himself. Ouroboros is a mythical dragon which eats itself.\n\nThe surrealist painter René Magritte is famous for his self-referential works. His painting \"The Treachery of Images\", includes the words \"this is not a pipe\", the truth of which depends entirely on whether the word \"ceci\" (in English, \"this\") refers to the pipe depicted—or to the painting or the word or sentence itself. M.C. Escher's art also contains many self-referential concepts such as hands drawing themselves.\n\nA word that describes itself is called an \"autological word\" (or \"autonym\"). This generally applies to adjectives, for example sesquipedalian (i.e. \"sesquipedalian\" is a sesquipedalian word), but can also apply to other parts of speech, such as TLA, as a three-letter abbreviation for \"three-letter abbreviation\".\n\nA sentence which inventories its own letters and punctuation marks is called an autogram.\n\nThere is a special case of meta-sentence in which the content of the sentence in the metalanguage and the content of the sentence in the object language are the same. Such a sentence is referring to itself. However some meta-sentences of this type can lead to paradoxes. \"This is a sentence.\" can be considered to be a self-referential meta-sentence which is obviously true. However \"This sentence is false\" is a meta-sentence which leads to a self-referential paradox. Such sentences can lead to problems, for example, in law, where statements bringing laws into existence can contradict one another or themselves. Kurt Gödel claimed to have found such a paradox in the US constitution at his citizenship ceremony.\n\nSelf-reference occasionally occurs in the media when it is required to write about itself, for example the BBC reporting on job cuts at the BBC. Notable encyclopedias may be required to feature articles about themselves, such as Wikipedia's article on Wikipedia.\n\nFumblerules are a list of rules of good grammar and writing, demonstrated through sentences that violate those very rules, such as \"Avoid cliches like the plague\" and \"Don't use no double negatives\". The term was coined in a published list of such rules by William Safire.\n\nSeveral academic disciplines are sometime required to study themselves in forms of self-reference, for example historiography (or \"meta-history\") is history's study of its own past; meta-sociology occurs when sociologists study the power structures in their own academic institutions; and meta-mathematics is the study of mathematics itself as a formal system, using its own methods. In law, self-reference may become an issue when laws are required to regulate the making of new laws, especially around constitutional issues. (The game of nomic begins as a model of this process.) The prefix \"meta\" is often used to denote this type of self-reference.\n\n\n"}
{"id": "858507", "url": "https://en.wikipedia.org/wiki?curid=858507", "title": "Self-referential humor", "text": "Self-referential humor\n\nSelf-referential humor, also known as self-reflexive humor or meta humor, is a type of comedic expression that—either directed toward some other subject, or openly directed toward itself—intentionally alludes to the very person who is expressing the humor in a comedic fashion, or to some specific aspect of that same comedic expression. Self-referential humor expressed discreetly and surrealistically is a form of bathos. In general, self-referential humor often uses hypocrisy, oxymoron, or paradox to create a contradictory or otherwise absurd situation that is humorous to the audience. \n\nSelf-referential humor is sometimes combined with breaking the fourth wall to explicitly make the reference directly to the audience, or make self-reference to an element of the medium that the characters should not be aware of.\n\nOld Comedy of Classical Athens is held to be the first—in the extant sources—form of self-referential comedy. Aristophanes, whose plays form the only remaining fragments of Old Comedy, used fantastical plots, grotesque and inhuman masks and status reversals of characters to slander prominent politicians and court his audience's approval.\n\nRAS syndrome refers to the redundant use of one or more of the words that make up an acronym or initialism with the abbreviation itself, thus in effect repeating one or more words. However, \"RAS\" stands for Redundant Acronym Syndrome; therefore, the full phrase yields \"Redundant Acronym Syndrome syndrome\" and is self-referencing in a comical manner. It also reflects an excessive use of TLAs (Three Letter Acronyms).\n\nMeta has come to be used, particularly in art, to refer to something that is self-referential. Popularised by Douglas Hofstadter who wrote several books on himself and the subject of self-reference, meta-jokes are a popular form of humor.\n\n"}
{"id": "12153317", "url": "https://en.wikipedia.org/wiki?curid=12153317", "title": "Sixpenny Library", "text": "Sixpenny Library\n\nErnest Benn Limited’s Sixpenny Library is a complete series of reference books published in the late 1920s and early 1930s. The library included over one hundred and eighty volumes. The series was edited by William Rose, who solicited current authorities in such areas as history, literature, religion, psychology, science, and economics. Some contributing authors were Hilaire Belloc, Maurice Baring, J.B. Priestley, Sir (later Lord) Robert Baden-Powell, Sir Oliver Lodge, S.V Keeling and Sir Ernest Benn himself. \"The Spectator\", in November 1927, after announcing some the latest additions to \"Messrs Benn's excellent Sixpenny Library\" devoted a further paragraph to his contribution on Trade (both of which are free to read online). Partial lists of the books published in the series can be found here and here.\n\nThe books were praised by critics for their excellence, brevity, and inexpensive price.\n"}
{"id": "40849944", "url": "https://en.wikipedia.org/wiki?curid=40849944", "title": "Sources for the historicity of Jesus", "text": "Sources for the historicity of Jesus\n\nChristian sources, such as the New Testament books in the Christian Bible, include detailed stories about Jesus but scholars differ on the historicity of specific episodes described in the Biblical accounts of Jesus. The only two events subject to \"almost universal assent\" are that Jesus was baptized by John the Baptist and was crucified by the order of the Roman Prefect Pontius Pilate.\n\nNon-Christian sources that are used to study and establish the historicity of Jesus include Jewish sources such as Josephus, and Roman sources such as Tacitus. These sources are compared to Christian sources such as the Pauline Epistles and the Synoptic Gospels. These sources are usually independent of each other (e.g. Jewish sources do not draw upon Roman sources), and similarities and differences between them are used in the authentication process.\n\nIn a review of the state of research, the Jewish scholar Amy-Jill Levine stated that \"no single picture of Jesus has convinced all, or even most scholars\" and that all portraits of Jesus are subject to criticism by some group of scholars.\n\nThe writings of the 1st century Romano-Jewish historian Flavius Josephus include references to Jesus and the origins of Christianity. Josephus' \"Antiquities of the Jews\", written around 93–94 CE, includes two references to Jesus in Books and .\n\nOf the two passages, the James passage in Book 20 is used by scholars to support the existence of Jesus, the \"Testimonium Flavianum\" in Book 18 his crucifixion. Josephus' James passage attests to the existence of Jesus as a historical person and that some of his contemporaries considered him the Messiah. According to Bart Ehrman, Josephus' passage about Jesus was altered by a Christian scribe, including the reference to Jesus as the Messiah.\n\nA textual argument against the authenticity of the James passage is that the use of the term \"Christos\" there seems unusual for Josephus. An argument based on the flow of the text in the document is that, given that the mention of Jesus appears in the \"Antiquities\" before that of the John the Baptist, a Christian interpolator may have inserted it to place Jesus in the text before John. A further argument against the authenticity of the James passage is that it would have read well even without a reference to Jesus.\n\nThe passage deals with the death of \"James the brother of Jesus\" in Jerusalem. Whereas the works of Josephus refer to at least twenty different people with the name Jesus, this passage specifies that this Jesus was the one \"who was called Christ\". Louis Feldman states that this passage, above others, indicates that Josephus did say something about Jesus.\n\nModern scholarship has almost universally acknowledged the authenticity of the reference in of the \"Antiquities\" to \"the brother of Jesus, who was called Christ, whose name was James\", and considers it as having the highest level of authenticity among the references of Josephus to Christianity.\n\nThe \"Testimonium Flavianum\" (meaning the testimony of Flavius [Josephus]) is the name given to the passage found in of the \"Antiquities\" in which Josephus describes the condemnation and crucifixion of Jesus at the hands of the Roman authorities. Scholars have differing opinions on the total or partial authenticity of the reference in the passage to the execution of Jesus by Pontius Pilate. The general scholarly view is that while the \"Testimonium Flavianum\" is most likely not authentic in its entirety, it is broadly agreed upon that it originally consisted of an authentic nucleus with a reference to the execution of Jesus by Pilate which was then subject to Christian interpolation. Although the exact nature and extent of the Christian redaction remains unclear, there is broad consensus as to what the original text of the \"Testimonium\" by Josephus would have looked like.\n\nThe references found in \"Antiquities\" have no parallel texts in the other work by Josephus such as the \"Jewish War\", written twenty years earlier, but some scholars have provided explanations for their absence, such as that the \"Antiquities\" covers a longer time period and that during the twenty-year gap between the writing of the \"Jewish Wars\" (c. 70 CE) and \"Antiquities\" (after 90 CE) Christians had become more important in Rome and were hence given attention in the \"Antiquities\".\n\nA number of variations exist between the statements by Josephus regarding the deaths of James and the New Testament accounts. Scholars generally view these variations as indications that the Josephus passages are not interpolations, because a Christian interpolator would more likely have made them correspond to the Christian traditions. Robert Eisenman provides numerous early Christian sources that confirm the Josephus testament, that James was the brother of Jesus.\n\nThe Roman historian and senator Tacitus referred to Christ, his execution by Pontius Pilate and the existence of early Christians in Rome in his final work, \"Annals\" (c. AD 116), . The relevant passage reads: \"called Christians by the populace. Christus, from whom the name had its origin, suffered the extreme penalty during the reign of Tiberius at the hands of one of our procurators, Pontius Pilatus.\"\n\nScholars generally consider Tacitus's reference to the execution of Jesus by Pontius Pilate to be both authentic, and of historical value as an independent Roman source about early Christianity that is in unison with other historical records. William L. Portier has stated that the consistency in the references by Tacitus, Josephus and the letters to Emperor Trajan by Pliny the Younger reaffirm the validity of all three accounts.\n\nTacitus was a patriotic Roman senator and his writings shows no sympathy towards Christians. Andreas Köstenberger and separately Robert E. Van Voorst state that the tone of the passage towards Christians is far too negative to have been authored by a Christian scribe – a conclusion shared by John P. Meier Robert E. Van Voorst states that \"of all Roman writers, Tacitus gives us the most precise information about Christ\".\n\nJohn Dominic Crossan considers the passage important in establishing that Jesus existed and was crucified, and states: \"That he was crucified is as sure as anything historical can ever be, since both Josephus and Tacitus... agree with the Christian accounts on at least that basic fact.\" Bart D. Ehrman states: \"Tacitus's report confirms what we know from other sources, that Jesus was executed by order of the Roman governor of Judea, Pontius Pilate, sometime during Tiberius's reign.\" Eddy and Boyd state that it is now \"firmly established\" that Tacitus provides a non-Christian confirmation of the crucifixion of Jesus.\n\nAlthough the majority of scholars consider it to be genuine, a few scholars question the authenticity of the passage given that Tacitus was born 25 years after Jesus' death.\n\nSome scholars have debated the historical value of the passage given that Tacitus does not reveal the source of his information. Gerd Theissen and Annette Merz argue that Tacitus at times had drawn on earlier historical works now lost to us, and he may have used official sources from a Roman archive in this case; however, if Tacitus had been copying from an official source, some scholars would expect him to have labeled Pilate correctly as a \"prefect\" rather than a \"procurator\". Theissen and Merz state that Tacitus gives us a description of widespread prejudices about Christianity and a few precise details about \"Christus\" and Christianity, the source of which remains unclear. However, Paul R. Eddy has stated that given his position as a senator Tacitus was also likely to have had access to official Roman documents of the time and did not need other sources.\n\nMichael Martin notes that the authenticity of this passage of the Annals has also been disputed on the grounds that Tacitus would not have used the word “messiah” in an authentic Roman document.\n\nWeaver notes that Tacitus spoke of the persecution of Christians, but no other Christian author wrote of this persecution for a hundred years.\n\nHotema notes that this passage was not quoted by any Church father up to the 15th century, although the passage would have been very useful to them in their work; and that the passage refers to the Christians in Rome being a multitude, while at that time the Christian congregation in Rome would actually have been very small.\n\nRichard Carrier has put forward the ideas that the 'Christ, the author of this name, was executed by the procurator Pontius Pilate in the reign of Tiberius' line is a Christian interpolation and that Tacitus wrote about Chrestians not Christians.\n\nScholars have also debated the issue of hearsay in the reference by Tacitus. Charles Guignebert argued that \"So long as there is that possibility [that Tacitus is merely echoing what Christians themselves were saying], the passage remains quite worthless\". R. T. France states that the Tacitus passage is at best just Tacitus repeating what he had heard through Christians. However, Paul R. Eddy has stated that as Rome's preeminent historian, Tacitus was generally known for checking his sources and was not in the habit of reporting gossip. Tacitus was a member of the Quindecimviri sacris faciundis, a council of priests whose duty it was to supervise foreign religious cults in Rome, which as Van Voorst points out, makes it reasonable to suppose that he would have acquired knowledge of Christian origins through his work with that body.\n\nMara (son of Sarapion) was a Stoic philosopher from the Roman province of Syria. Sometime between 73 AD and the 3rd century, Mara wrote a letter to his son (also called Sarapion) which may contain an early non-Christian reference to the crucifixion of Jesus.\n\nThe letter refers to the unjust treatment of \"three wise men\": the murder of Socrates, the burning of Pythagoras, and the execution of \"the wise king\" of the Jews. The author explains that in all three cases the wrongdoing resulted in the future punishment of those responsible by God and that when the wise are oppressed, not only does their wisdom triumph in the end, but God punishes their oppressors.\n\nThe letter includes no Christian themes and the author is presumed to be a pagan. Some scholars see the reference to the execution of the \"wise king\" of the Jews as an early non-Christian reference to Jesus. Criteria that support the non-Christian origin of the letter include the observation that \"king of the Jews\" was not a Christian title, and that the letter's premise that Jesus lives on based on the wisdom of his teachings is in contrast to the Christian concept that Jesus continues to live through his resurrection.\n\nScholars such as Robert Van Voorst see little doubt that the reference to the execution of the \"king of the Jews\" is about the death of Jesus. Others such as Craig A. Evans see less value in the letter, given its uncertain date, and the possible ambiguity in the reference.\n\nThe Roman historian Suetonius (c. 69 – after 122 CE) made references to early Christians and their leader in his work \"Lives of the Twelve Caesars\" (written 121 CE). The references appear in and which describe the lives of Roman Emperors Claudius and Nero. The Nero 16 passage refers to the abuses by Nero and mentions how he inflicted punishment on Christians – which is generally dated to around AD 64. This passage shows the clear contempt of Suetonius for Christians - the same contempt expressed by Tacitus and Pliny the younger in their writings, but does not refer to Jesus himself.\n\nThe earlier passage in Claudius, may include a reference to Jesus, but is subject to debate among scholars. In Suetonius refers to the expulsion of Jews by Claudius and states:\n\nThe reference in Claudius 25 involves the agitations in the Jewish community which led to the expulsion of some Jews from Rome by Claudius, and is likely the same event mentioned in the Acts of the Apostles (). Most historians date this expulsion to around AD 49–50. Suetonius refers to the leader of the Christians as \"Chrestus\", a term also used by used by Tacitus, referred in Latin dictionaries as a (amongst other things) version of 'Christus'. However, the wording used by Suetonius implies that Chrestus was alive at the time of the disturbance and was agitating the Jews in Rome. This weakens the historical value of his reference as a whole, and there is no overall scholarly agreement about its value as a reference to Jesus. However, the confusion of Suetonius also points to the lack of Christian interpolation, for a Christian scribe would not have confused the Jews with Christians.\n\nMost scholars assume that in the reference Jesus is meant and that the disturbances mentioned were due to the spread of Christianity in Rome. However, scholars are divided on the value of the Suetonius' reference. Some scholars such as Craig A. Evans, John Meier and Craig S. Keener see it as a likely reference to Jesus. Others such as Stephen Benko and H. Dixon Slingerland see it as having little or no historical value.\n\nMenahem Stern states Suetonius definitely was referring to Jesus; because he would have added \"a certain\" to Chrestus if he had meant some unknown agitator.\n\nThe Babylonian Talmud in a few cases includes possible references to Jesus using the terms \"Yeshu\", \"Yeshu ha-Notzri\", \"ben Stada\", and \"ben Pandera\". Some of these references probably date back to the Tannaitic period (70–200 CE). In some cases, it is not clear if the references are to Jesus, or other people, and scholars continue to debate their historical value, and exactly which references, if any, may be to Jesus.\n\nRobert Van Voorst states that the scarcity of Jewish references to Jesus is not surprising, given that Jesus was not a prominent issue for the Jews during the first century, and after the devastation caused by the Siege of Jerusalem in the year 70, Jewish scholars were focusing on preserving Judaism itself, rather than paying much attention to Christianity.\n\nRobert Eisenman argues that the derivation of Jesus of Nazareth from \"ha-Notzri\" is impossible on etymological grounds, as it would suggest rather \"the Nazirite\" rather than \"the Nazarene\".\n\nVan Voorst states that although the question of who was referred to in various points in the Talmud remains subject to debate among scholars, in the case of \"Sanhedrin 43a\" (generally considered the most important reference to Jesus in rabbinic literature), Jesus can be confirmed as the subject of the passage, not only from the reference itself, but from the context that surrounds it, and there is little doubt that it refers to the death of Jesus of Nazareth. Christopher M. Tuckett states that if it is accepted that death narrative of Sanhedrin 43a refers to Jesus of Nazareth then it provides evidence of Jesus' existence and execution.\n\nAndreas Kostenberger states that the passage is a Tannaitic reference to the trial and death of Jesus at Passover and is most likely earlier than other references to Jesus in the Talmud. The passage reflects hostility toward Jesus among the rabbis and includes this text:\n\nIt is taught: On the eve of Passover they hung Yeshu and the crier went forth for forty days beforehand declaring that \"[Yeshu] is going to be stoned for practicing witchcraft, for enticing and leading Israel astray. Anyone who knows something to clear him should come forth and exonerate him.\" But no one had anything exonerating for him and they hung him on the eve of Passover. \n\nPeter Schäfer states that there can be no doubt that the narrative of the execution of Jesus in the Talmud refers to Jesus of Nazareth, but states that the rabbinic literature in question are not Tannaitic but from a later Amoraic period and may have drawn on the Christian gospels, and may have been written as responses to them. Bart Ehrman and separately Mark Allan Powell state that given that the Talmud references are quite late, they can give no historically reliable information about the teachings or actions of Jesus during his life.\n\nAnother reference in early second century Rabbinic literature (Tosefta Hullin II 22) refers to Rabbi Eleazar ben Dama who was bitten by a snake, but was denied healing in the name of Jesus by another Rabbi for it was against the law, and thus died. This passage reflects the attitude of Jesus' early Jewish opponents, i.e. that his miracles were based on evil powers.\n\nEddy and Boyd, who question the value of several of the Talmudic references state that the significance of the Talmud to historical Jesus research is that it never denies the existence of Jesus, but accuses him of sorcery, thus indirectly confirming his existence. R. T. France and separately Edgar V. McKnight state that the divergence of the Talmud statements from the Christian accounts and their negative nature indicate that they are about a person who existed. Craig Blomberg states that the denial of the existence of Jesus was never part of the Jewish tradition, which instead accused him of being a sorcerer and magician, as also reflected in other sources such as Celsus. Andreas Kostenberger states that the overall conclusion that can be drawn from the references in the Talmud is that Jesus was a historical person whose existence was never denied by the Jewish tradition, which instead focused on discrediting him.\n\nPliny the Younger (c. 61 – c. 112), the provincial governor of Pontus and Bithynia, wrote to Emperor Trajan \"c\". 112 concerning how to deal with Christians, who refused to worship the emperor, and instead worshiped \"Christus\". Charles Guignebert, who does not doubt that Jesus of the Gospels lived in Gallilee in the 1st century, nevertheless dismisses this letter as acceptable evidence for a historical Jesus.\n\nThallus, of whom very little is known, and none of whose writings survive, wrote a history allegedly around the middle to late first century CE, to which Eusebius referred. Julius Africanus, writing \"c\" 221, links a reference in the third book of the \"History\" to the period of darkness described in the crucifixion accounts in three of the Gospels . It is not known whether Thallus made any mention to the crucifixion accounts; if he did, it would be the earliest noncanonical reference to a gospel episode, but its usefulness in determining the historicity of Jesus is uncertain. The dating of Thallus is dependent on him writing about an event during the 207th Olympiad (49–52 AD), which means he wrote after that date, not near that date. This depends on the text being corrupt, which would mean Thallus could have been writing after the 217th Olympiad (89–92 AD), or even the 167th Olympiad (112–109 BC). He is first referenced by Theophilus, writing around 180 AD, which means Thallus could have written any time between 109 BC and 180 AD. All we know is Thallus mentioned a solar eclipse, and as solar eclipses are not possible at Passover, that would mean Thallus was not talking about the crucifixion of Jesus at all.\n\nPhlegon of Tralles, AD 80–140, similar to Thallus, Julius Africanus mentions a historian named Phlegon who wrote a chronicle of history around AD 140, where he records:\n“Phlegon records that, in the time of Tiberius Caesar, at full moon, there was a full eclipse of the sun from the sixth to the ninth hour.” (Africanus, Chronography, 18:1) Phlegon is also mentioned by Origen (an early church theologian and scholar, born in Alexandria):\n“Now Phlegon, in the thirteenth or fourteenth book, I think, of his Chronicles, not only ascribed to Jesus a knowledge of future events . . . but also testified that the result corresponded to His predictions.” (Origen Against Celsus, Book 2, Chapter 14)\n“And with regard to the eclipse in the time of Tiberius Caesar, in whose reign Jesus appears to have been crucified, and the great earthquakes which then took place … ” (Origen Against Celsus, Book 2, Chapter 33)\n“Jesus, while alive, was of no assistance to himself, but that he arose after death, and exhibited the marks of his punishment, and showed how his hands had been pierced by nails.” (Origen Against Celsus, Book 2, Chapter 59). However, Eusebius in The Chronicon (written in the 4th century AD) records what Phlegon said verbatim. \"Now, in the fourth year of the 202nd Olympiad [32 AD], a great eclipse of the sun occurred at the sixth hour [noon] that excelled every other before it, turning the day into such darkness of night that the stars could be seen in heaven, and the earth moved in Bithynia, toppling many buildings in the city of Nicaea.\" Phlegon never mentions Jesus or the 3 hour darkness. He also mentions a solar eclipse, which can not occur at Passover. Apart from the year (which may be a corruption), this description fits an earthquake and eclipse that occurred in North West Turkey on November, 29 AD.\n\nCelsus writing late in the second century produced the first full-scale attack on Christianity. Celsus' document has not survived but in the third century Origen replied to it, and what is known of Celsus' writing is through the responses of Origen. According to Origen, Celsus accused Jesus of being a magician and a sorcerer. While the statements of Celsus may be seen as valuable, they have little historical value, given that the wording of the original writings can not be examined.\n\nThe Dead Sea Scrolls are first century or older writings that show the language and customs of some Jews of Jesus' time. Scholars such as Henry Chadwick see the similar uses of languages and viewpoints recorded in the New Testament and the Dead Sea Scrolls as valuable in showing that the New Testament portrays the first century period that it reports and is not a product of a later period. However, the relationship between the Dead Sea scrolls and the historicity of Jesus has been the subject of highly controversial theories, and although new theories continue to appear, there is no overall scholarly agreement about their impact on the historicity of Jesus, despite the usefulness of the scrolls in shedding light on first-century Jewish traditions.\n\nThe following sources are disputed, and of limited historical value, but they are at least proof of Christians existing and being known and talked about in the first and second centuries.\n\nThere is a limestone burial box from the 1st century known as the James Ossuary with the Aramaic inscription, \"James, son of Joseph, brother of Jesus.\" The authenticity of the inscription was challenged by the Israel Antiquities Authority, who filed a complaint with the Israeli police. In 2012, the owner of the ossuary was found not guilty, with the judge ruling that the authenticity of the ossuary inscription had not been proven either way. It has been suggested it was a forgery.\n\nVarious books, memoirs and stories were written about Jesus by the early Christians. The most famous are the gospels of Matthew, Mark, Luke and John. All but one of these are believed to have been written within 50–70 years of the death of Jesus, with the Gospel of Mark believed to be the earliest, and the last the Gospel of John. Blainey writes that the oldest surviving record written by an early Christian is a short letter by St Paul: the First Epistle to the Thessalonians, which appeared about 25 years after the death of Jesus. This letter, while important in describing issues for the development of Gentilic Christianity, contains little of significance for understanding the life of the historic Jesus.\n\nBart Ehrman, Robert Eisenman and others critical of traditional Christian views, in assessing the problems involved in conducting historical Jesus research, say the Gospels are full of discrepancies, were written decades after Jesus' death, by authors who had not witnessed any events in Jesus' life. They go on to say the Gospels were authored not by eyewitnesses who were contemporary with the events that they narrate but rather by people who did not know Jesus, see anything he did, or hear anything he taught, and that the authors did not even share a language with Jesus. The accounts they produced are not disinterested; they are narratives produced by Christians who actually believed in Jesus, and were not immune from slanting the stories in light of their biases. Ehrman points out that the texts are widely inconsistent, full of discrepancies and contradictions in both details and larger portraits of who Jesus was.\n\nIn the context of Christian sources, even if all other texts are ignored, the Pauline epistles can provide some information regarding Jesus. This information does not include a narrative of the life of Jesus, and refers to his existence as a person, but adds few specific items apart from his death by crucifixion. This information comes from those letters of Paul whose authenticity is not disputed. Paul was not a companion of Jesus and claims his information comes from the holy spirit acquired after Jesus' death.\n\nOf the thirteen letters that bear Paul's name, seven are considered authentic by almost all scholars, and the others are generally considered pseudepigraphic. The 7 undisputed letters (and their approximate dates) are: 1 Thessalonians (c. 51 CE), Philippians (c. 52–54 CE), Philemon (c. 52–54 CE), 1 Corinthians (c. 53–54 CE), Galatians (c. 55 CE), 2 Corinthians (c. 55–56 CE) and Romans (c. 55–58 CE). The authenticity of these letters is accepted by almost all scholars, and they have been referenced and interpreted by early authors such as Origen and Eusebius.\n\nGiven that the Pauline epistles are generally dated to AD 50 to AD 60, they are the earliest surviving Christian texts that include information about Jesus. These letters were written approximately twenty to thirty years after the generally accepted time period for the death of Jesus, around AD 30–36. The letters were written during a time when Paul recorded encounters with the disciples of Jesus, e.g. states that several years after his conversion Paul went to Jerusalem and stayed with Apostle Peter for fifteen days. During this time, Paul disputed the nature of Jesus' message with Jesus's brother James, concerning the importance of adhering to kosher food restrictions and circumcision, important features of determining Jewish identity.\n\nThe Pauline letters were not intended to provide a narrative of the life of Jesus, but were written as expositions of Christian teachings. In Paul's view, the earthly life of Jesus was of a lower importance than the theology of his death and resurrection,a theme that permeates Pauline writings. However, the Pauline letters clearly indicate that for Paul Jesus was a real person (born of a woman as in Gal 4.4), a Jew (\"born under the law\", Romans 1.3) who had disciples (1 Corinthians 15.5), who was crucified (as in 1 Corinthians 2.2 and Galatians 3.1) and who resurrected from the dead (1 Corinthians 15.20, Romans 1.4 and 6.5, Philippians 3:10–11). And the letters reflect the general concept within the early Gentillic Christian Church that Jesus existed, was crucified and was raised from the dead.\n\nThe references by Paul to Jesus do not in themselves prove the existence of Jesus, but they do establish that the existence of Jesus was the accepted norm within the early Christians (including the Christian community in Jerusalem, given the references to collections there) twenty to thirty years after the death of Jesus, at a time when those who could have been acquainted with him could still be alive.\n\nThe seven Pauline epistles that are widely regarded as authentic include the following information that along with other historical elements are used to study the historicity of Jesus:\n\nThe existence of only these references to Jesus in the Pauline epistles has given rise to criticism of them by G. A. Wells, who is generally accepted as a leader of the movement to deny the historicity of Jesus. When Wells was still denying the existence of Jesus, he criticized the Pauline epistles for not mentioning items such as John the Baptist or Judas or the trial of Jesus and used that argument to conclude that Jesus was not a historical figure.\n\nJames D. G. Dunn addressed Wells' statement and stated that he knew of no other scholar that shared that view, and most other scholars had other and more plausible explanations for the fact that Paul did not include a narrative of the life of Jesus in his letters, which were primarily written as religious documents rather than historical chronicles at a time when the life story of Jesus could have been well known within the early Church. Dunn states that despite Wells' arguments, the theories of the non-existence of Jesus are a \"thoroughly dead thesis\".\n\nWhile Wells no longer denies the existence of Jesus, he has responded to Dunn, stating that his arguments from silence not only apply to Paul but all early Christian authors, and that he still has a low opinion of early Christian texts, maintaining that for Paul Jesus may have existed a good number of decades before.\n\nThe Pauline letters sometimes refer to creeds, or confessions of faith, that predate their writings. For instance reads: \"For what I received I passed on to you as of first importance: that Christ died for our sins according to the Scriptures, that he was buried, that he was raised on the third day according to the Scriptures.\" refers to Romans 1:2 just before it which mentions an existing gospel, and in effect may be treating it as an earlier creed.\n\nOne of the keys to identifying a pre-Pauline tradition is given in \n\nHere Paul refers to others before him who preached the creed. James Dunn states that indicates that in the 30s Paul was taught about the death of Jesus a few years earlier.\n\nThe Pauline letters thus contain Christian creed elements of pre-Pauline origin. The antiquity of the creed has been located by many Biblical scholars to less than a decade after Jesus' death, originating from the Jerusalem apostolic community. Concerning this creed, Campenhausen wrote, \"This account meets all the demands of historical reliability that could possibly be made of such a text,\" whilst A. M. Hunter said, \"The passage therefore preserves uniquely early and verifiable testimony. It meets every reasonable demand of historical reliability.\"\n\nThese creeds date to within a few years of Jesus' death, and developed within the Christian community in Jerusalem. Although embedded within the texts of the New Testament, these creeds are a distinct source for Early Christianity. This indicates that existence and death of Jesus was part of Christian belief a few years after his death and over a decade before the writing of the Pauline epistles.\n\nThe four canonical gospels, Matthew, Mark, Luke, and John, are the main sources for the biography of Jesus' life, the teachings and actions attributed to him. Three of these (Matthew, Mark, and Luke) are known as the synoptic Gospels, from the Greek σύν (syn \"together\") and ὄψις (opsis \"view\"), given that they display a high degree of similarity in content, narrative arrangement, language and paragraph structure. The presentation in the fourth canonical gospel, i.e. John, differs from these three in that it has more of a thematic nature rather than a narrative format. Scholars generally agree that it is impossible to find any direct literary relationship between the synoptic gospels and the Gospel of John.\n\nThe authors of the New Testament generally showed little interest in an absolute chronology of Jesus or in synchronizing the episodes of his life with the secular history of the age. The gospels were primarily written as theological documents in the context of early Christianity with the chronological timelines as a secondary consideration. One manifestation of the gospels being theological documents rather than historical chronicles is that they devote about one third of their text to just seven days, namely the last week of the life of Jesus in Jerusalem. Although the gospels do not provide enough details to satisfy the demands of modern historians regarding exact dates, scholars have used them to reconstruct a number of portraits of Jesus. However, as stated in the gospels do not claim to provide an exhaustive list of the events in the life of Jesus.\n\nScholars have varying degrees of certainty about the historical reliability of the accounts in the gospels, and the only two events whose historicity is the subject of almost universal agreement among scholars are the baptism and crucifixion of Jesus. Scholars such as E.P. Sanders and separately Craig A. Evans go further and assume that two other events in the gospels are historically certain, namely that Jesus called disciples, and caused a controversy at the Temple.\n\nEver since the Augustinian hypothesis, scholars continue to debate the order in which the gospels were written, and how they may have influenced each other, and several hypothesis exist in that regard, e.g. the Markan priority hypothesis holds that the Gospel of Mark was written first c. 70 CE. In this approach, Matthew is placed at being sometime after this date and Luke is thought to have been written between 70 and 100 CE. However, according to the competing, and more popular, Q source hypothesis, the gospels were not independently written, but were derived from a common source called Q. The two-source hypothesis then proposes that the authors of Matthew and Luke drew on the Gospel of Mark as well as on Q.\n\nThe gospels can be seen as having three separate lines: A literary line which looks at it from a textual perspective, secondly a historical line which observes how Christianity started as a renewal movement within Judaism and eventually separated from it, and finally a theological line which analyzes Christian teachings. Within the historical perspective, the gospels are not simply used to establish the existence of Jesus as sources in their own right alone, but their content is compared and contrasted to non-Christian sources, and the historical context, to draw conclusions about the historicity of Jesus.\n\nTwo possible patristic sources that may refer to eye witness encounters with Jesus are the early references of Papias and Quadratus, reported by Eusebius of Caesarea in the 4th century.\n\nThe works of Papias have not survived, but Eusebius quotes him as saying:\n\nRichard Bauckham states that while Papias was collecting his information (c. 90), Aristion and the elder John (who were Jesus' disciples) were still alive and teaching in Asia minor, and Papias gathered information from people who had known them. However, the exact identity of the \"elder John\" is wound up in the debate on the authorship of the Gospel of John, and scholars have differing opinions on that, e.g. Jack Finegan states that Eusebius may have misunderstood what Papias wrote, and the elder John may be a different person from the author of the fourth gospel, yet still a disciple of Jesus. Gary Burge, on the other hand sees confusion on the part of Eusebius and holds the elder John to be different person from the apostle John.\n\nThe letter of Quadratus (possibly the first Christian apologist) to emperor Hadrian (who reigned 117 – 138) is likely to have an early date and is reported by Eusebius in his \"Ecclesiastical History\" 4.3.2 to have stated:\n\nBy \"our Savior\" Quadratus means Jesus and the letter is most likely written before AD 124. Bauckham states that by \"our times\" he may refer to his early life, rather than when he wrote (117–124), which would be a reference contemporary with Papias. Bauckham states that the importance of the statement attributed to Quadratus is that he emphasizes the \"eye witness\" nature of the testimonies to interaction with Jesus. Such \"eye witness statements\" abound in early Christian writings, particularly the pseudonymous Christian Apocrypha, Gospels and Letters, in order to give them credibility.\n\nA number of later Christian texts, usually dating to the second century or later, exist as New Testament apocrypha, among which the gnostic gospels have been of major recent interest among scholars. The 1945 discovery of the Nag Hammadi library created a significant amount of scholarly interest and many modern scholars have since studied the gnostic gospels and written about them. However, the trend among the 21st century scholars has been to accept that while the gnostic gospels may shed light on the progression of early Christian beliefs, they offer very little to contribute to the study of the historicity of Jesus, in that they are rather late writings, usually consisting of sayings (rather than narrative, similar to the hypothesised Q documents), their authenticity and authorship remain questionable, and various parts of them rely on components of the New Testament. The focus of modern research into the historical Jesus has been away from gnostic writings and towards the comparison of Jewish, Greco-Roman and canonical Christian sources.\n\nAs an example, Bart Ehrman states that gnostic writings of the Gospel of Thomas (part of the Nag Hammadi library) have very little value in historical Jesus research, because the author of that gospel placed no importance on the physical experiences of Jesus (e.g. his crucifixion) or the physical existence of believers, and was only interested in the secret teachings of Jesus rather than any physical events. Similarly, the Apocryphon of John (also part of the Nag Hammadi library) has been useful in studying the prevailing attitudes in the second century, and questions of authorship regarding the Book of revelation, given that it refers to , but is mostly about the post ascension teachings of Jesus in a vision, not a narrative of his life. Some scholars such as Edward Arnal contend that the Gospel of Thomas continues to remain useful for understanding how the teachings of Jesus were transmitted among early Christians, and sheds light on the development of early Christianity.\n\nThere is overlap between the sayings of Jesus in the apocryphal texts and canonical Christian writings, and those not present in the canonical texts are called agrapha. There are at least 225 agrapha but most scholars who have studied them have drawn negative conclusions about the authenticity of most of them and see little value in using them for historical Jesus research. Robert Van Voorst states that the vast majority of the agrapha are certainly inauthentic. Scholars differ on the number of authentic agrapha, some estimating as low as seven as authentic, others as high as 18 among the more than 200, rendering them of little value altogether. While research on apocryphal texts continues, the general scholarly opinion holds that they have little to offer to the study of the historicity of Jesus given that they are often of uncertain origin, and almost always later documents of lower value.\n\n\n"}
{"id": "54226143", "url": "https://en.wikipedia.org/wiki?curid=54226143", "title": "The Crime Book", "text": "The Crime Book\n\nThe Crime Book (Big Ideas Simply Explained) is a non-fiction volume co-authored by American crime writers Cathy Scott, Shanna Hogan, Rebecca Morris, Canadian author and historian Lee Mellor, and United Kingdom author Michael Kerrigan, with a foreword for the U.S. edition by Scott and the U.K. edition by crime-fiction author Peter James. It was released by DK Books under its Big Ideas Learning imprint in May 2017.\n\nThe publisher describes \"The Crime Book\" as a guide to criminology that explores the most infamous cases of all time, from serial killers to mob hits to war crimes and more.\n\nIt includes a variety of crimes committed by more than 100 of the world's most notorious criminals. From Jack the Ripper to Jeffrey Dahmer, the book is a study of international true-crime history that covers shocking stories through infographics and research that lays out key facts and details. It examines the science, psychology and sociology of criminal behavior. It profiles of villains, victims and detectives. Each clue is listed for readers to follow investigations from start to finish, and studies the police and detective work for each case.\n\nIn a Q&A article for CrimeCon's blog with Scott, the author described the crimes detailed in the book as having \"such diversity that there is something for everyone. ... I can’t think of one crime that’s not represented in The Crime Book. It runs the gamut—from nonviolent cons to gangland-style criminals, to white-collar offenders—with a complete representation starting with the first known homicide committed against a Neanderthal man. Simply put, you can’t make this stuff up.\"\n\n\"Rolling Stone\" magazine's description, in an August 2017 interview with co-author Scott about the book, wrote that it is \"an encyclopedic treatment of the topic (that) makes for excellent companion reading. A compelling compilation of human trickery and awfulness, it covers crimes from arson, art forgery and kidnapping to bank robbery, drug trafficking and, of course, murder, with many of the entries accompanied by helpful illustrations.\"\n\n\"Reader's Digest\" listed it as one of its \"Best New Books You Should Read This April,\" describing it as \"everything you ever wanted to know about some of the most audacious, hideous, hilarious and mysterious acts of crime in one explosive book, filled with graphs, illustrations, quotes and timelines. This highly addictive encyclopedia of crime ... is a trivia goldmine and a helpful guide allowing you to put events into context.\"\n\n\"Culture Magazine\" in Germany had this to say: \"The level of expertise is quite high,\" noting that the book \"is lushly illustrated, readable and entertaining.\"\n\nIn its review, \"Crime Fiction Lover\" wrote that \"a crack team of true-crime experts helped put it together.\"\n\n\"Crimespree Magazine\" wrote, \"The crimes covered are all over from serial killers to gangsters and outlaws to kidnappers and elderly Brit bank robbers. This is a great book.\"\n\n"}
{"id": "39726999", "url": "https://en.wikipedia.org/wiki?curid=39726999", "title": "The Rough Guide to True Crime", "text": "The Rough Guide to True Crime\n\nThe Rough Guide to True Crime is a non-fiction paperback reference guide to national and international true crime cases by American crime writer Cathy Scott. It was released in the UK and US in August 2009 by Penguin Books through its Rough Guides imprint.\n\n\"The Rough Guide to True Crime\" is a compilation of a variety of cases, including historic crimes, with sections broken down by the type of offenses and who committed them. It includes black-and-white photos as illustration. Psychological profiles are included throughout by forensic expert Dr. Louis B. Schlesinger, who explains the psychology of serial killers, murderers, hit men and burglars. The book features serial killer Jeffrey Dahmer, mob hitman Richard \"The Iceman\" Kuklinski, John Wayne Glover \"The Granny Killer,\" and British \"Doctor of Death\" Harold Shipman.\n\nScott's story from \"The Rough Guide to True Crime\" about mob enforcer Herbert Blitzstein was selected for inclusion in the July 2012 retrospective of crime writing, \"Masters of True Crime: Chilling Stories of Murder and the Macabre\".\n\nThe author appeared on BlogTalkRadio's \"True Murder\" show and described some of the crimes included in the book that were committed in the 19th century as \"a different time in America, where people like Billy the Kid could walk in and just rob a bank\" and get away with it. And while \"there was nothing glamorous about what they did, they are a part of lore.\"\n\nThe book was featured at BookExpo America 2009's trade fair in DK Publishing's booth in New York City.\n\nIn a review, \"True Crime Book Reviews\" wrote, \"From the Moors murders and Harold Shipman, to the murder of 2pac, this guide illuminates the psychology in play behind the most intriguing crimes in history, from the absurd to the appalling. \"The Rough Guide to True Crime\" explores the best of the haunting genre of True Crime.\"\n\n\n"}
{"id": "270906", "url": "https://en.wikipedia.org/wiki?curid=270906", "title": "Three-letter acronym", "text": "Three-letter acronym\n\nA three-letter acronym (TLA), or three-letter abbreviation, is an abbreviation, specifically an acronym, alphabetism, or initialism, consisting of three letters. These are usually the initial letters of the words of the phrase abbreviated, and are written in capital letters (upper case); three-letter abbreviations such as \"etc.\" and \"Mrs.\" are not three-letter acronyms, but \"TLA\" is a TLA (an example of an autological abbreviation).\n\nMost three-letter abbreviations are \"initialisms\": all the letters are pronounced as the names of letters, as in \"APA\" . Some are acronyms pronounced as a word; computed axial tomography, CAT, is almost always pronounced as the animal's name in \"CAT scan\".\n\n\nThe exact phrase \"three-letter acronym\" appeared in the sociology literature in 1975. Three-letter acronyms were used as mnemonics in biological sciences, from 1977 and their practical advantage was promoted by Weber in 1982. They are used in many other fields, but the term TLA is particularly associated with computing. In 1980, the manual for the Sinclair ZX81 home computer used and explained TLA. The specific generation of three-letter acronyms in computing was mentioned in a JPL report of 1982. In 1988, in a paper titled \"On the cruelty of really teaching computer science\", eminent computer scientist Edsger W. Dijkstra wrote \n\"Because no endeavour is respectable these days without a TLA ...\" By 1992 it was in a Microsoft handbook.\n\nThe number of possible three-letter abbreviations (or permutations) using the 26 letters of the alphabet from A to Z (AAA, AAB ... to ZZY, ZZZ) is 26 × 26 × 26 = 17,576. Another 26 × 26 × 10 = 6760 can be produced if the third element is allowed to be a digit 0-9, giving a total of 24,336.\n\nIn English, WWW is the longest possible TLA to pronounce, typically requiring nine syllables. The usefulness of TLAs typically comes from how it is quicker to say the acronym instead of than the phrase they represent, however saying 'WWW' in English requires three times as many syllables than the phrase it is meant to abbreviate (World Wide Web). Consequently, \"www\" is sometimes abbreviated as \"dubdubdub\" in speech.\n\n\n"}
