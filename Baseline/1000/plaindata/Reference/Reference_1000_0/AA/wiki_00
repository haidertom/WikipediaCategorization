{"id": "1230569", "url": "https://en.wikipedia.org/wiki?curid=1230569", "title": "Acronym Finder", "text": "Acronym Finder\n\nAcronym Finder (AF) is a free online searchable dictionary and database of abbreviations (acronyms, initialisms, and others) and their meanings.\n\nThe entries are classified into categories such as \"Information Technology, Military/Government, Science, Slang/Pop Culture\" etc. It also contains a database of US and Canadian postal codes. For abbreviations with multiple meanings they are listed by popularity with the most common one being listed first. it claims to have over a million \"human-edited\" and verified definitions.\n\nAcronym Finder was registered and the database put online by Michael K. Molloy of Colorado in 1997 but he began compiling it in 1985 working as a computer systems officer for the USAF. Molloy first saw the need of an acronym list while integrating computers at the Randolph Air Force Base in Texas His first acronym list running up-to 30 pages. When he had retired and put AF online in 1997, his list already had 43,000 acronyms. It began mainly as a list of Military/Government abbreviations before expanding to other areas.\n\nMolloy and his wife served as the editors of the website verifying user submissions for abbreviations and adding others they found to the database. Molloy has also provided opinions on abbreviations such as \"MSG\" which Madison Square Garden wanted as a domain name (\"msg.com\") claiming trademark to the abbreviated letters. He stated that MSG also stood for more common things such as monosodium glutamate and message among others. The Garden in the end settled out of court and came to own msg.com.\n\nThe website was maintained under Mountain Data Systems, LLC by Molloy before being sold off and eventually coming under the ownership of Farlex, Inc. publishers of Thefreedictionary.com.\n\nThe website contains a database of meanings and expansions for abbreviations, acronyms, initialisms mainly in English but includes some entries in other languages such as French, German, Spanish etc. as well. It is freely accessible. The entries are further classified into categories such as \"Information Technology, Military/Government, Science, Slang/Pop Culture\" etc. It also contains a database of US and Canadian postal codes which are shown on a Map along with location information. Abbreviations with multiple expansions are listed by popularity with the most common one being presented first, these can be sorted alphabetically as well.\n\nAnyone can contribute to the database by submitting abbreviations and their meanings, these are reviewed by an by editor and categorized before being added to the database. While the database has been described as fairly accurate errors have been found in the meanings and expansions of abbreviations. The website does not list sources for the abbreviations and their meanings but it does identify people who have contributed more than 50 abbreviations to the database.\n\nThe database only contains abbreviations and their expansions and does not list other data such as grammatical category, context, source, field of the abbreviation etc.\n\nFarlex, Inc. the current owner of the website also publishes mobile apps for the Android and iOS operating systems.\n\nAcronym Finder also includes a \"Systematic Buzz Phrase Projector\", a light-hearted tool that randomly generates jargon-like phrases and abbreviations — usually initialisms that would be unpronounceable as acronyms — and meanings from 30 cleverly chosen buzz words.\n\nThe website is supported through advertisements.\n\nThe website is listed as a quick reference tool in directories like Stanford Library, Library of Congress, USC Library. It has been cited as the largest database of acronyms and has been used in computational studies for its database.\n\nListings of abbreviations on the website have also been used as a defense that an abbreviation is in public use and cannot be trademarked. While in some trademark cases citations for AF have been accepted it has been described as an unreliable reference in others.\n\nIt has garnered criticism for the fact that anyone can submit abbreviations to the site and the content is user generated. Mike Molloy the site's original owner had defended that each submission is verified before being added to the database.\n\n"}
{"id": "192230", "url": "https://en.wikipedia.org/wiki?curid=192230", "title": "Almanac", "text": "Almanac\n\nAn almanac (also spelled \"almanack\" and \"almanach\") is an annual publication listing a set of events forthcoming in the next year.\n\nIt includes information like weather forecasts, farmers' planting dates, tide tables, and other tabular data often arranged according to the calendar. Celestial figures and various statistics are found in almanacs, such as the rising and setting times of the Sun and Moon, dates of eclipses, hours of high and low tides, and religious festivals.\n\nA calendar, which is a system for time keeping, in written form is usually produced as a most simple almanac: it includes additional information about the day of the week on which a particular day falls, major holidays, the phases of the moon, earthquake hazard levels etc. The set of events noted in an almanac are selected in view of a more or less specific group of readers e.g. farmers, sailors, astronomers or others.\n\nThe etymology of the word is unclear. It is suggested the word \"almanac\" derives from a Greek word meaning \"calendar\". However, that word appears only once in antiquity, by Eusebius who quotes Porphyry as to the Coptic Egyptian use of astrological charts (\"almenichiaká\"). The earliest almanacs were calendars that included agricultural, astronomical, or meteorological data. But it is highly unlikely Roger Bacon received the word from this etymology: \"Notwithstanding the suggestive sound and use of this word (of which however the real form is very uncertain), the difficulties of connecting it historically either with the Spanish Arabic manākh, or with Medieval Latin almanach without Arabic intermediation, seem insurmountable.\"\n\nThe earliest documented use of the word in any language is in Latin in 1267 by Roger Bacon, where it meant a set of tables detailing movements of heavenly bodies including the moon.\n\nOne etymology report says \"The ultimate source of the word is obscure. Its first syllable, al-, and its general relevance to medieval science and technology, strongly suggest an Arabic origin, but no convincing candidate has been found\". Another report similarly says of \"almanac\": \"First seen in Roger Bacon. Apparently from Spanish Arabic, \"al-manakh\", but this is not an Arabic word...The word remains a puzzle.\" The \"Oxford English Dictionary\" similarly says \"the word has no etymon in Arabic\" but indirect circumstantial evidence \"points to a Spanish Arabic \"al-manākh\"\".\n\nThe reason why the proposed Arabic word is speculatively spelled \"al-manākh\" is that the spelling occurred as \"almanach\", as well as almanac (and Roger Bacon used both spellings). The earliest use of the word was in the context of astronomy calendars.\n\nThe prestige of the Tables of Toledo and other medieval Arabic astronomy works at the time of the word's emergence in the West, together with the absence of the word in Arabic, suggest it may have been invented in the West and is pseudo-Arabic. At that time in the West, it would have been prestigious to attach an Arabic appellation to a set of astronomical tables. Also around that time, prompted by that motive, the Latin writer Pseudo-Geber wrote under an Arabic pseudonym. (The later alchemical word \"alkahest\" is known to be pseudo-Arabic.)\n\nThe earlier texts considered to be almanacs have been found in the Near East, dating back to the middle of the second millennium BC. They have been called generally hemerologies, from the Greek \"hēmerā\", meaning \"day\". Among them is the so-called Babylonian Almanac, which lists favorable and unfavorable days with advice on what to do on each of them. Successive variants and versions aimed at different readership have been found. Egyptian lists of good and bad moments, three times each day, have also been found. Many of these prognostics were connected with celestial events. The flooding of the Nile valley, a most important event in ancient Egypt, was expected to occur at the summer solstice but as the civil calendar had exactly 365 days, over the centuries the date was drifting in the calendar. The first heliacal rising of Sirius was used for its prediction and this practice, the observation of some star and its connecting to some event apparently spread.\n\nThe Greek almanac, known as parapegma, has existed in the form an inscribed stone on which the days of the month were indicated by movable pegs inserted into bored holes, hence the name. There were also written texts and according to Diogenes Laërtius, \"Parapegma\" was the title of a book by Democritus. Ptolemy, the Alexandrian astronomer (2nd century) wrote a treatise, \"Phaseis\"—\"phases of fixed stars and collection of weather-changes\" is the translation of its full title—the core of which is a \"parapegma\", a list of dates of seasonally regular weather changes, first appearances and last appearances of stars or constellations at sunrise or sunset, and solar events such as solstices, all organized according to the solar year. With the astronomical computations were expected weather phenomena, composed as a digest of observations made by various authorities of the past. \"Parapegmata\" had been composed for centuries.\n\nPtolemy believed that astronomical phenomena caused the changes in seasonal weather; his explanation of why there was not an exact correlation of these events was that the physical influences of other heavenly bodies also came into play. Hence for him, weather prediction was a special division of astrology.\n\nThe origins of the almanac can be connected to ancient Babylonian astronomy, when tables of planetary periods were produced in order to predict lunar and planetary phenomena. Similar treatises called Zij were later composed in medieval Islamic astronomy.\n\nThe modern almanac differs from Babylonian, Ptolemaic and Zij tables in the sense that \"the entries found in the almanacs give directly the positions of the celestial bodies and need no further computation\", in contrast to the more common \"auxiliary astronomical tables\" based on Ptolemy's \"Almagest\". The earliest known almanac in this modern sense is the \"Almanac of Azarqueil\" written in 1088 by Abū Ishāq Ibrāhīm al-Zarqālī (Latinized as Arzachel) in Toledo, al-Andalus. The work provided the true daily positions of the sun, moon and planets for four years from 1088 to 1092, as well as many other related tables. A Latin translation and adaptation of the work appeared as the \"Tables of Toledo\" in the 12th century and the \"Alfonsine tables\" in the 13th century.\nAfter almanacs were devised, people still saw little difference between predicting the movements of the stars and tides, and predicting the future in the divination sense. Early almanacs therefore contained general horoscopes, as well as the more concrete information. In 1150 Solomon Jarchus created such an almanac considered to be among the first modern almanacs. Copies of 12th century almanacs are found in the British Museum, and in the Universities of Oxford and Cambridge. In 1300, Petrus de Dacia created an almanac (Savilian Library, Oxford). This was the same year Roger Bacon, OFM, produced his as well. In 1327 Walter de Elvendene created an almanac and later on John Somers of Oxford, in 1380. In 1386 Nicholas de Lynne, Oxford produced an almanac. In 1457 the first printed almanac was published at Mainz, by Gutenberg (eight years before the famous Bible). Regio-Montanus produced an almanac in 1472 (Nuremberg, 1472), which was continued in print for several centuries in many editions. In 1497 the \"Sheapheard’s Kalendar\", translated from French (Richard Pynson) became the first English printed almanac.\n\nBy the second half of the 16th century, yearly almanacs were being produced in England by men such as Anthony Askham, Thomas Buckminster, John Dade and Gabriel Frende. In the 17th century, English almanacs were bestsellers, second only to the Bible; by the middle of the century, 400,000 almanacs were being produced annually (a complete listing can be found in the English Short Title Catalogue). Until its deregulation in 1775, the Stationers' Company maintained a lucrative monopoly over almanac publication in England. Richard Allestree (who is not the same as this Richard Allestree) wrote one of the more popular English almanacs, producing yearly volumes from 1617 to 1643, but his is by no means the earliest or the longest-running almanac.\n\nIn British America, William Pierce of Harvard College published the first American almanac entitled, \"An Almanac for New England for the year 1639\" Cambridge, Massachusetts. Harvard became the first center for the annual publication of almanacs with various editors including Samuel Danforth, Oakes, Cheever, Chauncey, Dudley, Foster, et alia. An almanac maker going under the pseudonym of Poor Richard, Knight of the Burnt Island began to publish \"Poor Robin's Almanack\" one of the first comic almanacs that parodied these horoscopes in its 1664 issue, saying \"This month we may expect to hear of the Death of some Man, Woman, or Child, either in Kent or Christendom.\" Other noteworthy comic almanacs include those published from 1687-1702 by John Tully of Saybrook, Connecticut.\n\nThe most important early American almanacs were made from 1726-1775 by Nathaniel Ames of Dedham, Massachusetts. A few years later James Franklin began publishing the Rhode-Island Almanack beginning in 1728. Five years later his brother Benjamin Franklin began publishing \"Poor Richard's Almanack\" from 1733-1758. Benjamin Banneker, a free African American, composed a series of almanacs from 1792-1797.\n\nCurrently published almanacs such as \"Whitaker's Almanack\" have expanded their scope and contents beyond that of their historical counterparts. Modern almanacs include a comprehensive presentation of statistical and descriptive data covering the entire world. Contents also include discussions of topical developments and a summary of recent historical events. Other currently published almanacs (ca. 2006) include \"TIME Almanac with Information Please\", \"World Almanac and Book of Facts\", \"The Farmer's Almanac\" and \"The Old Farmer's Almanac\" and The Almanac for Farmers & City Folk. The \"Inverness Almanac\" ,\" an almanac/literary journal, was published in West Marin, California, from 2015 to 2016. In 2007, Harrowsmith Country Life Magazine launched a Canadian Almanac, written in Canada, with all-Canadian content. The nonprofit agrarian organization the Greenhorns currently publishes \"The New Farmer's Almanac\" as a resource for young farmers.\n\nMajor topics covered by almanacs (reflected by their tables of contents) include: geography, government, demographics, agriculture, economics and business, health and medicine, religion, mass media, transportation, science and technology, sport, and awards/prizes.\n\nOther examples include \"The Almanac of American Politics\" published by Columbia Books & Information Services, \"The Almanac of American Literature\", and \"The Almanac of British Politics\".\n\nFrom 1985 to 1990, approximately 53 per cent of all almanac sales sold in the United States were sold through the \"Where in the World Is Carmen Sandiego? (1985)\" computer game pack that included a complimentary \"World Almanac and Book of Facts.\"\n\nThe GPS almanac, as part of the data transmitted by each GPS satellite, contains coarse orbit and status information for all satellites in the constellation, an ionospheric model, and information to relate GPS derived time to Coordinated Universal Time (UTC). Hence the GPS almanac provide a similar goal as the ancient Babylonian almanac, to find celestial bodies.\n\n\n\n"}
{"id": "9721437", "url": "https://en.wikipedia.org/wiki?curid=9721437", "title": "Ask a Librarian", "text": "Ask a Librarian\n\nAsk a Librarian is a live virtual reference service that offers online reference assistance to residents in the state of Florida. Ask a Librarian is an official service of the Florida Electronic Library and is administered by the Tampa Bay Library Consortium (TBLC).\n\nParticipating libraries provide users with virtual reference services via live chat software, text messaging, and e-mail forms which users can access through embedded links and widgets on their library’s official website. Live chat and text messaging are available from 10:00 a.m. to midnight EST from Sunday through Thursday, and from 10:00 a.m. to 5:00 p.m. EST on Friday and Saturday; the e-mail form is available to patrons 24 hours per day, seven days per week. As of March 2016, Ask a Librarian has 133 participating institutions including public libraries and library systems, K-12 libraries, and university and college libraries.\n\nAsk a Librarian began as a partnership between the College Center for Library Automation (CCLA) and TBLC in the interest of creating a statewide virtual reference service that would increase the presence of librarians on the internet. In 2002, the organizations successfully applied for a joint grant through the Library Services and Technology Act (LSTA) and were awarded $339,000 for the development and implementation of their pilot project as a service of the Florida Electronic Library.\n\nThe Ask a Librarian virtual reference service was officially activated on July 28, 2003. Tampa Bay-area libraries were the first to participate, with Pasco County libraries joining shortly afterward. Within the first year of operation, nearly 7,500 Floridians had used the service to get answers from a reference librarian. By August 19, 2007, Ask a Librarian had answered its 100,000th reference question. As of February 2011, Ask a Librarian had logged over 274,000 live virtual reference sessions and e-mail questions.\n\nAsk a Librarian has made several efforts meet user reference needs on smartphones and other mobile devices. In October 2010, Ask a Librarian introduced a text messaging service to accompany their traditional chat service. In 2012, the service introduced a mobile-friendly website interface for tablets and phones. In April 2013, the service also launched the Ask A Librarian Mobile App, a mobile-friendly interface geared toward improving the user chat experience on smartphones and tablets.\n\n"}
{"id": "925519", "url": "https://en.wikipedia.org/wiki?curid=925519", "title": "Autogram", "text": "Autogram\n\nAn autogram (Greek: αὐτός = self, γράμμα = letter) is a sentence that describes itself in the sense of providing an inventory of its own characters. They were invented by Lee Sallows, who also coined the word \"autogram\". An essential feature is the use of full cardinal number names such as “one”, “two”, etc., in recording character counts. Autograms are also called ‘self-enumerating’ or ‘self-documenting’ sentences. Often, letter counts only are recorded while punctuation signs are ignored, as in this example:\n\nThe first autogram to be published was composed by Sallows in 1982 and appeared in Douglas Hofstadter's \"Metamagical Themas\" column in \"Scientific American\".\n\nThe task of producing an autogram is perplexing because the object to be described cannot be known until its description is first complete.\n\nA type of autogram that has attracted special interest is the autogramic pangram, a self-enumerating sentence in which every letter of the alphabet occurs at least once. Certain letters do not appear in either of the two autograms above, which are therefore not pangrams. The first ever self-enumerating pangram appeared in a Dutch newspaper and was composed by Rudy Kousbroek. Sallows, who lives in the Netherlands, was challenged by Kousbroek to produce a self-enumerating ‘translation’ of this pangram into English—an impossible-seeming task. This prompted Sallows to construct an electronic Pangram Machine. Eventually the machine succeeded, producing the example below which was published in Scientific American in October 1984:\n\nSallows wondered if one could produce a pangram that counts its letters as percentages of the whole sentence–a particularly difficult task since such percentages usually won't be exact integers. He mentioned the problem to Chris Patuzzo and in late 2015 Patuzzo produced the following solution:\n\nAutograms exist that exhibit extra self-descriptive features. Besides counting each letter, here the total number of letters appearing is also named:\n\nJust as an autogram is a sentence that describes itself, so there exist closed chains of sentences each of which describes its predecessor in the chain. Viewed thus, an autogram is such a chain of length 1. Here follows a chain of length 2:\nA special kind of autogram is the ‘reflexicon’ (short for “reflexive lexicon”), which is a self-descriptive word list that describes its own letter frequencies. The constraints on reflexicons are much tighter than on autograms because the freedom to choose alternative words such as “contains”, “comprises”, “employs”, and so on, is lost. However, a degree of freedom still exists through appending entries to the list that are strictly superfluous.\n\nFor example, “Sixteen e's, six f's, one g, three h's, nine i's, nine n's, five o's, five r's, sixteen s's, five t's, three u's, four v's, one w, four x's” is a reflexicon, but it includes what Sallows calls “dummy text” in the shape of “one g” and “one w”. The latter might equally be replaced with “one #”, where “#” can be any typographical sign not already listed. Sallows has made an extensive computer search and conjectures that there exist but three and only three pure (i.e., no dummy text) English reflexicons.\n\n"}
{"id": "398493", "url": "https://en.wikipedia.org/wiki?curid=398493", "title": "BRD (Germany)", "text": "BRD (Germany)\n\nBRD (; English: Federal Republic of Germany); () is an unofficial abbreviation commonly used between 1968 and 1990 by the communist regime of the German Democratic Republic (East Germany) to refer to the Federal Republic of Germany, informally known at the time as West Germany. The East German regime previously used the term \"German Federal Republic\" to refer to its western counterpart.\n\nUnlike the English equivalent FRG, which was used as an IOC country code and a FIFA trigramme, the use of \"BRD\" was strongly discouraged by the authorities of the Federal Republic of Germany itself, because it was considered to be a derogatory communist term. The term was not banned by law, but its use was discouraged or forbidden in schools in Western Germany. After German reunification, the country is usually referred to simply as Germany (\"\"), and hence the need for abbreviations is greatly diminished. The most widely used abbreviation for West Germany was its ISO 3166-1 alpha-2 country code \"DE\", which has remained the country code of reunified Germany.\n\nThe official name was and is \"Bundesrepublik Deutschland\" (\"Federal Republic of Germany\"). The name, even though in the beginning referring only to the republic established in the Trizone, was to reflect a name for all of Germany, therefore it was particularly to include the term \"Deutschland\" (\"Germany\"). This corresponded to the spirit of the then West German constitution, the Basic Law, allowing all states or \"Länder\", then under Allied control, to join the new Federal Republic. In 1949 the original eleven states in the Trizone and West Berlin did so. However the latter was prevented by Allied objection on account of the city being a quadripartite allied occupation area. The Saarland joined with effect from 1 January 1957, while the \"new states\" of the East did so with effect from 3 October 1990, including reunited Berlin.\n\nTherefore, the term Germany had an importance as part of the official name, which is reflected in the naming conventions which developed in the Cold War. Starting in June 1949 the abbreviation was sometimes used in the Federal Republic of Germany without any special connotations. The initialism \"BRD\" began to enter into such regular usage in West German scientific and ministerial circles, that it was added to the western edition of the German language dictionary Duden in 1967. The German Democratic Republic at first used the name \"Westdeutschland\" or \"West Germany\" (abbreviated \"WD\") for the Federal Republic of Germany, but since the 1950s the East German government insisted on calling West Germany \"Deutsche Bundesrepublik\" or \"German Federal Republic\" (abbreviated \"DBR\"), because they also considered East Germany part of Germany, and thus would not permit the West German government to use the name \"Germany\".\n\nThis changed in 1968 with the new constitution of the German Democratic Republic. The communists no longer strove for German reunification, and the name \"BRD\" was introduced as a propaganda counter-term to the term \"DDR\", trying to express the equality of the states. Conversely, the West would speak of the \"sogenannte DDR\" or \"so-called 'DDR'\" when it had to be belittled.\n\nAt that time, the initialism \"BRD\" had been adopted by \"Neues Deutschland\", the ruling Socialist Unity Party's daily newspaper, while East German official sources adopted that initialism as standard in 1973.\n\nThe East German decision to abandon the idea of a single German nation was accompanied by omitting the terms \"Deutschland\" (\"Germany\") and \"deutsch\" (\"German\") in a number of terms, for example:\n\n\nHowever, the ruling party's full name, \"Sozialistische Einheitspartei Deutschlands\" or \"Socialist Unity Party of Germany\" remained unchanged, as did that of its newspaper \"Neues Deutschland\" (\"New Germany\") .\nTherefore, using the abbreviation \"BRD\" fitted perfectly into the official East German policy of downplaying the concept of a united Germany. In 1974, the GDR had replaced the vehicle registration code \"D\", hitherto shared with the Federal Republic, for \"DDR\" and demanded that West Germany recognise the division by likewise accepting \"BRD\". \nThis was rejected by the West, where some motorists displayed bumper stickers with the slogan \"BRD - Nein Danke!\" (\"BRD? No Thanks!\"). Thus in the West the initialism became even more objectionable and using it was often considered either unreflecting or even expressing naïve Communist sympathies.\nAs a result, the initialism reached only occasional frequency in West German parlance. In order to be precise West Germans increasingly used the terms \"Bundesrepublik\" or \"Bundesgebiet\" (\"Federal Republic\", or \"Federal Territory\") to refer to the country and \"Bundesbürger\" (\"Federal Citizen[s]\") as to its citizens, with the pertaining adjective \"bundesdeutsch\" (federally German).\n\nTo distance themselves from the term \"BRD\", until German reunification, the government of the Federal Republic of Germany and media sometimes used the abbreviations \"BR Deutschland,\" \"BR-Dt.\", \"BRDt.\",\nWest Germany had always claimed to be \"the\" Germany, and did not like the comparison to \"DDR\", or two separate German states. This claim was also reflected in the Hallstein Doctrine determining its foreign and interior policy until the early 1970s. Named after Walter Hallstein, State secretary at the Foreign Office, this was a key doctrine in the foreign policy of West Germany after 1955, which prescribed that the Federal Republic of Germany would not establish or maintain diplomatic relations with any state that recognised the GDR. Although this changed after 1973, with the Federal Republic no longer asserting an exclusive mandate over the whole of Germany, West Germany only established \"de facto\" diplomatic relations with East Germany. Under the terms of the Basic Treaty in 1972, Bonn and East Berlin exchanged \"permanent missions\", headed by \"permanent representatives\", rather than \"de jure\" embassies headed by ambassadors. Similarly, relations with the GDR were not conducted through the Foreign Office, but through a separate Federal Ministry for Intra-German Relations, to which the East German mission was accredited.\n\nIn 1965 the Federal Minister of All-German Affairs (later Intra-German Relations) issued the \"Directives for the appellation of Germany\" recommending that the use of \"BRD\" be avoided. On 31 May 1974 the heads of the federal and state governments recommended that the full name should always be used in official publications. In November 1979 the federal government informed the Bundestag that the West German public broadcasters ARD and ZDF agreed not to use the initialism.\n\nUnder the West German federal system, the states were generally responsible for school education, and by the 1970s, some of them had either already recommended omitting the initialism, or, in the case of Bavaria, forbidden it. Similarly, a decree by the educational authorities in the state of Schleswig-Holstein of 4 October 1976 declared the term to be \"nicht wünschenswert\" or \"undesirable\". The conference of all the states ministers for school education decided on 12 February 1981 to not print the initialism in books, maps, and atlases for schools. with pupils being required to write \"Bundesrepublik Deutschland\" in full and use of the term being deemed an error. The different usages were so ingrained that one could deduce a person's or source's political leaning from the name used for West Germany, with far-left movements in the country using \"BRD\".\n\nHowever, as the Association for the German Language found, this debate on the initialism had little influence on changing the West German parlance with the usage of the initialism - in any event limited - unaffected by the debate.\n\nA similar ideological question was the question whether to use \"Berlin (West)\" (the officially preferred name) or \"West Berlin\", and even whether to write \"West Berlin\" in German as two hyphenated words - \"West-Berlin\" - or as one word - \"Westberlin\".\n\nMost Westerners called the Western sectors \"Berlin\", unless further distinction was necessary. The West German Federal government initially called West Berlin \"Groß-Berlin\" or \"Greater Berlin\", but changed this \"Berlin (West)\", although it also used the hyphenated \"West-Berlin\". However, the East German government commonly referred to it as \"Westberlin\". Starting from 31 May 1961, East Berlin was officially called \"Berlin, Hauptstadt der DDR\" (Berlin, Capital of the GDR), replacing the formerly used term \"Democratic Berlin\", or simply \"Berlin\", by East Germany, and \"Berlin (Ost)\" by the West German Federal government. Other names used by West German media included \"Ost-Berlin\" and \"Ostberlin\" (both meaning \"East Berlin\") as well as \"Ostsektor\" or \"Eastern Sector\". These different naming conventions for the divided parts of Berlin, when followed by individuals, governments, or media, commonly indicated their political leanings, with the centre-right \"Frankfurter Allgemeine Zeitung\" using \"Ost-Berlin\" and the centre-left \"Süddeutsche Zeitung\" using \"Ostberlin\".\n\nThe naming of the German Democratic Republic was also a controversial issue, West Germans at first preferring the names \"Mitteldeutschland\" (\"Middle Germany\") and \"Sowjetische Besatzungszone\" (Soviet Occupation Zone) abbreviated as \"SBZ\". This only changed under Willy Brandt when West German authorities started using the official name, \"Deutsche Demokratische Republik\" or \"DDR\", but many conservative German newspapers, like \"Bild\", owned by the Springer company, always wrote \"DDR\" in scare quotes until 1 August 1989.\n\nIn 1995, a disagreement arose between reunified Germany and newly independent Slovakia, as Germany objected to the use of the Slovak language name \"Nemecká spolková republika\" (literally \"German Federal Republic\") owing to its Cold War connotations, instead of \"Spolková republika Nemecko\". This was almost identical to the equivalent \"Spolková republika Německo\" in Czech, a language closely related to Slovak, but the Slovak authorities claimed that \"Federal Republic of Germany\" could not be translated grammatically into Slovak. However, the Slovak government had used it until the previous year, leading to suggestions in the Bratislava newspaper \"Narodna Obroda\" that they were using \"German Federal Republic\" to show their displeasure with German attitudes to the country.\n"}
{"id": "2887701", "url": "https://en.wikipedia.org/wiki?curid=2887701", "title": "Bible (screenwriting)", "text": "Bible (screenwriting)\n\nA bible (also known as a story bible, show bible, series bible, or pitch bible) is a reference document used by screenwriters for information on a television series' characters, settings, and other elements.\n\nShow bibles are updated with information on the characters after the information has been established on screen. For example, the \"Frasier\" show bible was \"scrupulously maintained\", and anything established on air — \"the name of Frasier's mother, Niles' favorite professor, Martin's favorite bar...even a list of Maris' [dozens of] food allergies\" — was reflected in the bible. The updated bible then serves as a resource for writers to keep everything within the series consistent. \n\nOther show bibles are used as sales documents to help a television network or studio understand a series, and are sometimes given to new writers when they join the writing staff for the same reason. These types of bibles discuss the backstories of the main characters and the history of the series' fictional universe.\n\nTelevision series often rely on writers' assistants and script coordinators to serve as \"walking bibles\" in remembering details about a series.\n\nIn the United States, writing the show bible of a produced series earns that writer the 24 units of required credit necessary to qualify for membership in the Writers Guild of America.\n\n\n"}
{"id": "46407896", "url": "https://en.wikipedia.org/wiki?curid=46407896", "title": "Bibliography of C. Northcote Parkinson", "text": "Bibliography of C. Northcote Parkinson\n\n"}
{"id": "6767022", "url": "https://en.wikipedia.org/wiki?curid=6767022", "title": "Carol Ballard", "text": "Carol Ballard\n\nCarol Ballard is an author of more than 80 non-fiction books. Specializing in informational books for children and teens, her focus is toward the 7- to 14-year-old group.\n\nAfter graduating from Leeds University in plant sciences, Ballard did post-graduate research and was awarded a PhD in Immunology. She has many years experience as a science teacher and co-ordinator and has written articles for teachers on various aspects of science teaching, and teachers' materials for classroon use.\n\nIn addition to her writing, Carol works as a freelance consultant for publishers on educational and scientific matters. She also has her own business, Kite Books, which produces worksheets and teachers' resources.\n\n\n"}
{"id": "46902218", "url": "https://en.wikipedia.org/wiki?curid=46902218", "title": "Citation network", "text": "Citation network\n\nCitation Network is a social network which contains paper sources and linked by co-citation relationships. Egghe & Rousseau once (1990, p. 228) explain \"when a document \"d\" cites a document \"d\", we can show this by an arrow going from the node representing \"d\" to the document representing \"d\". In this way the documents from a collection D form a directed graph, which is called a 'citation graph' or 'citation network' \".\n\nCitation is a reference to a published or unpublished source (not always the original source). More precisely, a citation is an abbreviated alphanumeric expression embedded in the body of an intellectual work that denotes an entry in the bibliographic references section of the work for the purpose of acknowledging the relevance of the works of others to the topic of discussion at the spot where the citation appears. Generally the combination of both the in-body citation and the bibliographic entry constitutes what is commonly thought of as a citation (whereas bibliographic entries by themselves are not). References to single, machine-readable assertions in electronic scientific articles are known as nanopublications, a form of microattribution.\nCitation networks, the principal focus of this study, are one kind of social networks that have been studied quantitatively almost from the moment citation databases first became available. In 1965, Derek J. de Solla Price described the inherent linking characteristic of the SCI in his seminal paper titled \"Networks of Scientific Papers\". The links between citing and cited papers became dynamic when the SCI began to be published online. In 1973, Henry Small published his work on co-citation analysis which became a self-organizing classification system that led to document clustering experiments and eventually what is called \"Research Reviews\".\n\n"}
{"id": "17077434", "url": "https://en.wikipedia.org/wiki?curid=17077434", "title": "Comparative Toxicogenomics Database", "text": "Comparative Toxicogenomics Database\n\nThe Comparative Toxicogenomics Database (CTD) is a public website and research tool launched in November 2004 that curates scientific data describing relationships between chemicals/drugs, genes/proteins, diseases, taxa, phenotypes, GO annotations, pathways, and interaction modules.\nThe database is maintained by the Department of Biological Sciences at North Carolina State University.\n\nThe Comparative Toxicogenomics Database (CTD) is a public website and research tool that curates scientific data describing relationships between chemicals, genes/proteins, diseases, taxa, phenotypes, GO annotations, pathways, and interaction modules, launched on November 12, 2004. \nThe database is maintained by the Department of Biological Sciences at North Carolina State University.\n\nOne of the primary goals of CTD is to advance the understanding of the effects of environmental chemicals on human health on the genetic level, a field called toxicogenomics.\n\nThe etiology of many chronic diseases involves interactions between environmental factors and genes that modulate important physiological processes. Chemicals are an important component of the environment. Conditions such as asthma, cancer, diabetes, hypertension, immunodeficiency, and Parkinson's disease are known to be influenced by the environment; however, the molecular mechanisms underlying these correlations are not well understood. CTD may help resolve these mechanisms. The most up-to-date extensive list of peer-reviewed scientific articles about CTD is available at their publications page\n\nCTD is a unique resource where biocurators read the scientific literature and manually curate four types of core data:\n\n\nBy integrating the above four data sets, CTD automatically constructs putative chemical-gene-phenotype-disease networks to illuminate molecular mechanisms underlying environmentally-influenced diseases.\n\nThese inferred relationships are statistically scored and ranked and can be used by scientists and computational biologists to generate and verify testable hypotheses about toxicogenomic mechanisms and how they relate to human health.\n\nUsers can search CTD to explore scientific data for chemicals, genes, diseases, or interactions between any of these three concepts. Currently, CTD integrates toxicogenomic data for vertebrates and invertebrates.\n\nCTD integrates data from or hyperlinks to these databases:\n\n"}
{"id": "14322444", "url": "https://en.wikipedia.org/wiki?curid=14322444", "title": "Comparative advertising", "text": "Comparative advertising\n\nComparative advertising or advertising war is an advertisement in which a particular product, or service, specifically mentions a competitor by name for the express purpose of showing why the competitor is inferior to the product naming it. Also referred to as \"knocking copy\", it is loosely defined as advertising where “the advertised brand is explicitly compared with one or more competing brands and the comparison is obvious to the audience.”\n\nThis should not be confused with parody advertisements, where a fictional product is being advertised for the purpose of poking fun at the particular advertisement, nor should it be confused with the use of a coined brand name for the purpose of comparing the product without actually naming an actual competitor. (\"Wikipedia tastes better and is less filling than the Encyclopedia Galactica.\")\n\nIn the United States, the Federal Trade Commission (FTC) defined comparative advertising as “advertisement that compares alternative brands on objectively measurable attributes or price, and identifies the alternative brand by name, illustration or other distinctive information.” This definition was used in the case Gillette Australia Pty Ltd v Energizer Australia Pty Ltd. Similarly, the Law Council of Australia recently suggested that comparative advertising refers to “advertising which include reference to a competitor’s trademark in a way which does not impute proprietorship in the mark to the advertiser.”\n\nComparative advertisements could be either indirectly or directly comparative, positive or negative, and seeks “to associate or differentiate the two competing brands”. Different countries apply differing views regarding the laws on comparative advertising.\n\nThe earliest court case concerning comparative advertising dates back to 1910 in the United States – Saxlehner v Wagner. Prior to the 1970s, comparative advertising was deemed unfeasible due to related risks. For instance, comparative advertising could invite misidentification of products, potential legal issues, and may even win public sympathy for their competitors as victims.\n\nIn 1972, the FTC began to encourage advertisers to make comparison with named competitors, with the broad, public welfare objective of creating more informative advertising. The FTC argued that this form of advertising could also stimulate comparison shopping, encourage product improvement and innovation, and foster a positive competitive environment. However, studies have shown that while comparative advertisements had increased since 1960, the relative amount of comparative advertising is still small.\n\nPrior to 1997, many European countries severely limited comparative claims as an advertising practice. For example, in Germany comparisons in advertising had since the 1930's been largely prohibited as an anti-competitive practice, with very limited exceptions for cases where the advertiser had a good reason for presenting a critical claim, and reference to a competitor was necessary in order to present that claim. Importantly, this only applied to \"critical\" claims - claims of equivalence were completely prohibited. A similar approach had been adopted in France, where comparative advertising was commonly seen as disparaging of competitors. However, the legalisation of comparative advertising in France in 1992, opened the door to a general legalisation of comparative advertising through EU law, which had first been proposed by the European Commission in 1978. The result was the adoption of Directive 97/55/EC, which came into force in the year 2000. The relevant provisions are now contained in Directive 2006/114/EC.\n\nThis Directive sets out rules that comparative advertising must comply with in order to be considered permissible. These include the requirements that the comparison concern goods and services that meet the same purpose, that it objectively compare the relevant characteristics of the products concerned and that it not cause confusion or denigrate the trademarks and other distinguishing signs of competitors. The Directive prohibits comparisons that take unfair advantage of the reputation of a competitor's distinguishing marks, or present goods or services as imitations of products covered by a protected trade mark or trade name. Additionally, any comparison aimed at promoting goods bearing a protected designation of origin must refer exclusively to other goods bearing the same designation. Directive 2006/114/EC constitutes a total harmonisation of the rules on comparative advertising, meaning that the Member States are neither allowed to permit comparisons that breach the requirements of the Directive, nor prohibit ones that do. \n\nFurther, while trademark rights can in principle be used to prevent comparative advertising that makes unauthorised use of a competitor's trademark, this is not the case where the comparative advertisement complies with all the requirements of Directive 2006/114/EC. Legitimate comparative advertising must therefore be seen as an exception to the exclusive rights of the trademark proprietor. However, the trademark proprietor can, thanks to the prohibition on taking unfair advantage of a trademark's reputation, oppose the use of their trademark where it is not aimed at distinguishing the products of the advertiser and trademark proprietor and to highlight their differences objectively, but rather at riding on the coat-tails of that mark in order to benefit from its reputation.\n\nThe requirements set out by the Directive have resulted in some controversy. This is particularly true of the \"per se\" prohibition on comparisons presenting goods and services as imitations of trademarked products. In this regard, EU law contrasts starkly with the US approach; the US courts have long held that traders are allowed to the trademarked names of products they have imitated in advertising. In contrast, in L'Oréal and others v. Bellure, the Court of Justice held that smell-alike perfumes marketed through comparison lists breached this condition. This decision was criticised both by the English courts and by scholars, who have considered that this places unjustified limits on advertising acts that are otherwise fully legal, such as copying that does not infringe intellectual property rights. \n\nIn the UK, most of the use of competitor’s registered trademark in a comparative advertisement was an infringement of the registration up till the end of 1994. However, the laws on comparative advertising were harmonized in 2000. The current rules on comparative advertising are regulated by a series of EU Directives. The Business Protection from Misleading Marketing Regulations 2008 implements provisions of Directive (EC) 2006/114 in the UK.\n\nOne of the classic cases of comparative advertising in the UK was the O2 v Hutchison case. The European Court of Justice (ECJ) held that there could have been a trademark infringement when a comparative advertiser used the registered trademark for the advertiser’s own goods and services. It was also held that a trademark proprietor could not prevent a competitor’s use of a sign similar or identical to his mark in a comparative advertisement, which satisfies all the conditions of the Comparative Advertising Directive. If the Advocate General's decision in the O2 case were followed by the ECJ, competitors will not be able to use trademark legislation either to prevent a comparative advertisement through an injunction or to charge in respect of its use. Conversely, in British Airways plc v Ryanair Ltd. a lenient approach was adopted by the UK courts. The use of competitors’ trademarks was no longer restricted for businesses competing within an industry, provided that compliance of the conditions set out in the legislation were performed. This meant that businesses are able to use the trademarks of other companies and trade names to distinguish the relative merits of their own products and services over those of their competitors.\n\nThe FTC and the National Advertising Division of the Council of Better Business Bureaus, Inc. (NAD), govern the laws of comparative advertising in the United States including the treatment of comparative advertising claims. FTC stated that comparative advertising could benefit consumers and encourages comparative advertising, provided that the comparisons are “clearly identified, truthful, and non-deceptive”. Although comparative advertising is encouraged, NAD has stated “claims that expressly or implicitly disparage a competing product should be held to the highest level of scrutiny in order to ensure that they are truthful, accurate, and narrowly drawn.” Another major law is the trademark protective Lanham Act, which states that one could incur liability when the message of the comparative advertisement is untrue or uncertain, but has the intention to deceive consumers through the implied message conveyed.\n\nIn Australia, no specific law governs comparative advertising although certain cases regarding this matter have occurred. Comparative advertising that is truthful, and does not lead to confusion is permitted.\n\nGenerally, Australian advertisers should make sure that the following are complied when exercising comparative advertising to avoid breaches regarding misleading advertising under Australia Consumer Law:\n\n\nThe law in Hong Kong regarding comparative advertising is the law that existed in the UK prior to the enactment of the UK Act 1994. Hong Kong has no legislation exclusively intended at limiting false or misleading advertisements. Still, the Trade Descriptions Ordinance (Cap 362) bans the use of false trade descriptions in advertisements. The tort of trade libel also exists to deal with false or misleading advertisements designed to injure the competitor. Consumer Council may have the authority to publish information with a perspective to amending false or misleading advertisements, while the Association of Accredited Advertising Agencies of Hong Kong have the authority to take action against members who organize advertisements that are inaccurate.\n\nIn Argentina, there is no specific statute dealing with comparative advertising (so it is not forbidden), but there are clear jurisprudential rules based on unfair competition law. If in some manner an advertisement is proven to be unfair or exceeds ethical standards by hiding the truth or omitting some essential aspect of the comparison, it is probable that an injunction will be granted and that the plaintiff will be able to obtain a final decision declaring the advertising illegal.\n\nNumerous cases follow international precedent in referring to the requirements of the European Union Directive on comparative advertising. By following these criteria, Argentine courts have developed standards very similar to European regulation. It is as if the judges wanted to validate the law created by the Courts with an external source. Similar conclusions reached elsewhere indicate the existence of universally accepted principles that accept that comparing products in commercial advertisements should be lawful.\n\nIn Brazil, the allow comparative advertising with certain restrictions. Its primary purpose shall be the clarification or consumer’s protection; it shall have as basic principle the objectiveness of the comparison since subjective data, psychological or emotionally based data does not constitute a valid comparison basis for consumers; the purposed or implemented comparison shall be capable of being supported byevidence; in the case of consumption goods, the comparison shall be made with models manufactured in the same year and no comparison shall be made between products manufactured in different years, unless it is only a reference to show evolution, in which case the evolution shall be clearly demonstrated; there shall be no confusion between the products and competitor’s brands; there shall be no unfair competition, denigration of the product’s image or another company’s product; and there shall be no unreasonable use of the corporate image or goodwill of third parties.\n\nLikewise, the majority of the Brazilian authors is inclined to say that its legitimacy depends to meet certain requirements, which, in general, would be stipulated by Article 3a of Directive 84/450/EEC \n\nIn an early Mercosur's rules through Resolution 126/96.\n\nComparative advertising has been increasingly implemented through the years, and the types of comparative advertising range from comparing a single attribute dimension, comparing an attribute unique to the target and absent in the referent and comparisons involving attributes unique to both brands. The contributing factors to the effectiveness of comparative advertising include believability, which refers to the extent a consumer can rely on the information provided in comparative advertisements, the level of involvement, and the convenience in evaluation, provided by spoon feeding the consumer with information that does not require extra effort in recall.\n\nComparative advertising is generally coupled with negativity, as evidenced by early industry condemnation. Stating reasons such as participation in comparative advertising damaged the honour and credibility of advertising. Studies have suggested that negative information can be stored more effectively, thus generating the impact that any advertisement is purposed for, and more importantly, strong recall. On the contrary, such negativity can either be transferred directly to the brand and the consumer’s impression of the brand, various studies through the years have proven that comparative advertising has been responded to negatively.\n\nComparative advertising has been used effectively by companies like The National Australia Bank (NAB), and its “break up” campaign has made such an impact it has won an award from Cannes, and a substantial increase in its consumer interest. Internationally acclaimed Apple Inc. has effectively utilized its Mac vs PC advertisements as part of its marketing efforts to increase its market share over the years. Such companies prove the academic view that comparative advertising is more successful when used by established brands, justified by the credibility and attention an established brand brings. Other famous examples include L’Oreal SA v Bellure NV and Coca Cola v Pepsi. Comparative advertising has to be executed with caution and deep consideration for the targeted markets as the novelty of the concept affects the effectiveness of the stipulated campaigns.\n\nIn the 1980s, during what has been referred to as the cola wars, soft-drink manufacturer Pepsi ran a series of advertisements where people, caught on hidden camera, in a blind taste test, chose Pepsi over rival Coca-Cola.\nThe use of comparative advertising has been well established in political campaigns, where typically one candidate will run ads where the record of the other candidate is displayed, for the purpose of disparaging the other candidate. The most famous of these type ads, which only ran once on TV, consisted of a child picking daisies in a field, while a voice which sounded like Barry Goldwater performed a countdown to zero before the launch of a nuclear weapon which explodes in a mushroom cloud. The ad, \"Daisy\", was produced by Lyndon B. Johnson's campaign in an attempt to prevent Goldwater from either winning the nomination of his party or being selected.\n\nAnother example took place throughout the late 1980s between the bitter rivals Nintendo and Sega. \"Genesis does what Nintendon't\" immediately became a catchphrase following the release of the Sega Genesis (known as Mega Drive in PAL countries).\n\nA 30-second commercial promoting sustainability, showing soda bottles exploding each time a person makes a drink using his Sodastream machine, was banned in the United Kingdom in 2012. Clearcast, the organization that preapproves TV advertising in the U.K., explained that they \"thought it was a denigration of the bottled drinks market.\" The same ad, crafted by Alex Bogusky, ran in the United States, Sweden, Australia, and other countries. An appeal by Sodastream to reverse Clearcast's decision to censor the commercial was rejected. A similar ad was expected to air during Super Bowl XLVII in February 2013 but was banned by CBS for jabbing at Coke and Pepsi (two of CBS's largest sponsors).\n\nIn 2012, Microsoft's Bing (formerly MSN Search) began to run a campaign about which search engine they prefer as it compared Bing to Google, and that more people preferred Bing over Google. The campaign was titled \"Bing It On\".\n"}
{"id": "1809113", "url": "https://en.wikipedia.org/wiki?curid=1809113", "title": "Comparative biology", "text": "Comparative biology\n\nComparative biology uses natural variation and disparity to understand the patterns of life at all levels—from genes to communities—and the critical role of organisms in ecosystems. Comparative biology is a cross-lineage approach to understanding the phylogenetic history of individuals or higher taxa and the mechanisms and patterns that drives it. Comparative biology encompasses Evolutionary Biology, Systematics, Neontology, Paleontology, Ethology, Anthropology, and Biogeography as well as historical approaches to Developmental biology, Genomics, Physiology, Ecology and many other areas of the biological sciences.The comparative approach also has numerous applications in human health, genetics, biomedicine, and conservation biology. The biological relationships (phylogenies, pedigree) are important for comparative analyses and usually represented by a phylogenetic tree or cladogram to differentiate those features with single origins (Homology) from those with multiple origins (Homoplasy).\n\n"}
{"id": "14308145", "url": "https://en.wikipedia.org/wiki?curid=14308145", "title": "Comparative bullet-lead analysis", "text": "Comparative bullet-lead analysis\n\nComparative bullet-lead analysis (CBLA), also known as compositional bullet-lead analysis, is a now discredited and abandoned forensic technique which used chemistry to link crime scene bullets to ones possessed by suspects on the theory that each batch of lead had a unique elemental makeup.\n\nThe technique was first used after U.S. President John F. Kennedy's assassination in 1963. From the early 1980s through 2004 the US Federal Bureau of Investigation conducted about 2,500 analyses on cases submitted by law-enforcement groups. The results of these analyses had often been questioned by defence lawyers and the press, so the FBI finally asked the United States National Academy of Science's Board on Science, Technology, and Economic Policy to research the scientific merit of the process.\n\nIn 2004 the Board's study was summarized in \"Forensic Analysis: Weighing Bullet Lead Evidence.\" The Board determined that the chemical analyses were being performed correctly and were probably sufficient to determine correlation between two bullets from separate sources (the analysis used plasma-optical emission spectroscopy to identify trace elements in the bullets). The report also concluded that the seven trace elements selected for the analyses (arsenic, antimony, tin, copper, bismuth, silver and cadmium) are acceptable for sample correlation. The report finally concluded that the procedure is the best available method for such correlations. The greatest caveat in the report was that the statistical tests as applied by the FBI could cause confusion and misinterpretation when transmitted to prosecutors or when explained to a trial jury. Because of the significance of this weakness, the report concluded that the analysis should be used with caution. This report helped the FBI decide in 2004 to voluntarily cease offering the analysis to law-enforcement entities. The National Academy of Sciences never required that the FBI stop using the test.\n\n\"CNN PRESENTS Encore Presentation: Reasonable Doubt\" examined the unreliability of this technique. It has been discontinued as of September 1, 2005.\n\nThe U.S. government has fought releasing the list of the estimated 2,500 cases over three decades in which it performed the analysis, which may have led to false convictions. According to the FBI, only 20% of the 2,500 tests performed introduced the CBLA results into evidence at trial.\n\nOn 17 December 2008, Jimmy Ates was released from a Florida prison after serving ten years on the conviction of having murdered his wife, a conviction obtained largely on the strength of a bullet-lead analysis. His conviction was overturned as a consequence of the 2004 report.\n\n"}
{"id": "1719952", "url": "https://en.wikipedia.org/wiki?curid=1719952", "title": "Comparative research", "text": "Comparative research\n\nComparative research is a research methodology in the social sciences that aims to make comparisons across different countries or cultures. A major problem in comparative research is that the data sets in different countries may not use the same categories, or define categories differently (for example by using different definitions of poverty).\n\nAs Moutsios argues, cross-cultural and comparative research should be seen as part of the scientific spirit that arose in Greece in the 6th century and the overall appreciation of knowledge and learning that was characteristic of the 5th century. In other words, it is part of the emergence of \"episteme\" and \"philo-sophia\", as a love for knowledge that is independent from material benefits. \"Episteme\", as a form and activity in the field of \"logos\", marked the break of cognitive closure and advanced empirical inquiry, logical argumentation and the search for truth. And the high esteem for intellectual activity gave rise to a genuine curiosity about other cultures – which has lain thereafter at the heart of comparative inquiry.\n\nMoreover, behind the Greek comparative gaze also was the philosophical and political questioning which characterised the life of the democratic \"polis\". Philosophical inquiry, from the Milesians down to the Sophists, questioned the representations and the cognitive traditions of their own people; the inquiry of the traditions of other peoples was, as Herodotus’ \"Histories\" demonstrate, an activity associated with the ethos of philosophical critique that characterised democratic life in Greece. Similarly, questioning of the Greek laws and institutions and its related values and practices (e.g. \"isegoria\" and \"parrhesia\"), as part of Greek politics, is associated with the effort of the first historians to reflect on home institutions through researching those of others.\n\nAccording also to Karl Deutsch, we have been using this form of investigation for over 2,000 years. Comparing things is essential to basic scientific and philosophic inquiry, which has been done for a long time. Most authors are more conservative in their estimate of how long comparative research has been with us. It is largely an empty debate over the definition of the tradition with those questioning whether comparing things counts as comparative research.\n\nTextbooks on this form of study were beginning to appear by the 1880s, but its rise to extreme popularity began after World War II. There are numerous reasons that comparative research has come to take a place of honour in the toolbox of the social scientist. Globalization has been a major factor, increasing the desire and possibility for educational exchanges and intellectual curiosity about other cultures. Information technology has enabled greater production of quantitative data for comparison, and international communications technology has facilitated this information to be easily spread.\n\nComparative research, simply put, is the act of comparing two or more things with a view to discovering something about one or all of the things being compared. This technique often utilizes multiple disciplines in one study. When it comes to method, the majority agreement is that there is no methodology peculiar to comparative research. The multidisciplinary approach is good for the flexibility it offers, yet comparative programs do have a case to answer against the call that their research lacks a \"seamless whole.\" \n\nThere are certainly methods that are far more common than others in comparative studies, however. Quantitative analysis is much more frequently pursued than qualitative, and this is seen by the majority of comparative studies which use quantitative data. The general method of comparing things is the same for comparative research as it is in our everyday practice of comparison. Like cases are treated alike, and different cases are treated differently; the extent of difference determines how differently cases are to be treated. If one is able to sufficiently distinguish two carry the research conclusions will not be very helpful. \n\nSecondary analysis of quantitative data is relatively widespread in comparative research, undoubtedly in part because of the cost of obtaining primary data for such large things as a country's policy environment. This study is generally aggregate data analysis. Comparing large quantities of data (especially government sourced) is prevalent. A typical method of comparing welfare states is to take balance of their levels of spending on social welfare.\n\nIn line with how a lot of theorizing has gone in the last century, comparative research does not tend to investigate \"grand theories,\" such as Marxism. It instead occupies itself with middle-range theories that do not purport to describe our social system in its entirety, but a subset of it. A good example of this is the common research program that looks for differences between two or more social systems, then looks at these differences in relation to some other variable coexisting in those societies to see if it is related. The classic case of this is Esping-Andersen's research on social welfare systems. He noticed there was a difference in types of social welfare systems, and compared them based on their level of decommodification of social welfare goods. He found that he was able to class welfare states into three types, based on their level of decommodification. He further theorized from this that decommodification was based on a combination of class coalitions and mobilization, and regime legacy. Here, Esping-Andersen is using comparative research: he takes many western countries and compares their level of decommodification, then develops a theory of the divergence based on his findings.\n\nComparative research can take many forms. Two key factors are space and time. Spatially, cross-national comparisons are by far the most common, although comparisons within countries, contrasting different areas, cultures or governments also subsist and are very constructive, especially in a country like New Zealand, where policy often changes depending on which race it pertains to. Recurrent interregional studies include comparing similar or different countries or sets of countries, comparing one's own country to others or to the whole world.\n\nThe historical comparative research involves comparing different time-frames. The two main choices within this model are comparing two stages in time (either snapshots or time-series), or just comparing the same thing over time, to see if a policy's effects differ over a stretch of time.\n\nWhen it comes to subject matter of comparative inquiries, many contend there is none unique to it. This may indeed be true, but a brief perusal of comparative endeavours reveals there are some topics more recurrent than others. Determining whether socioeconomic or political factors are more important in explaining government action is a familiar theme. In general, however, the only thing that is certain in comparative research issues is the existence of differences to be analysed.\n\n\n"}
{"id": "31994535", "url": "https://en.wikipedia.org/wiki?curid=31994535", "title": "Comparison of Nazism and Stalinism", "text": "Comparison of Nazism and Stalinism\n\nA number of authors have carried out comparisons of Nazism and Stalinism, in which they have considered the similarities and differences of the two ideologies and political systems, what relationship existed between the two regimes, and why both of them came to prominence at the same time. During the 20th century, the comparison of Stalinism and Nazism was made on the topics of totalitarianism, ideology, and personality cult. Both regimes were seen in contrast to the liberal West, with an emphasis on the similarities between the two. The American political scientists Zbigniew Brzezinski, Hannah Arendt and Carl Friedrich and historian Robert Conquest were prominent advocates of applying the \"totalitarian\" concept to compare Nazism and Stalinism.\n\nOne of the first scholars to publish a comparative study of Nazi Germany and Stalin’s Soviet Union was Hannah Arendt. In her 1951 work, \"The Origins of Totalitarianism\", Arendt puts forward the idea of totalitarianism as a distinct type of political movement and form of government, which “differs essentially from other forms of political oppression known to us such as despotism, tyranny and dictatorship.” Furthermore, Arendt distinguishes between a totalitarian movement (such as a political party with totalitarian aims) and a totalitarian government. Not all totalitarian movements succeed in creating totalitarian governments once they gain power. In Arendt’s view, although many totalitarian movements existed in Europe in the 1920s and 1930s, only the governments of Stalin and Hitler succeeded in fully implementing their totalitarian aims. \n\nArendt traced the origin of totalitarian movements to the nineteenth century, focusing especially on antisemitism and imperialism. She emphasized the connection between the rise of European nation-states and the growth of antisemitism, which was due to the fact that the Jews represented an “inter-European, non-national element in a world of growing or existing nations.” Conspiracy theories abounded, and the Jews were accused of being part of various international schemes to ruin European nations. Small antisemitic political parties formed in response to this perceived Jewish threat, and, according to Arendt, these were the first political organizations in Europe that claimed to represent the interests of the whole nation as opposed to the interests of a class or other social group. The later totalitarian movements would copy or inherit this claim to speak for the whole nation, with the implication that any opposition to them constituted treason.\n\nEuropean imperialism of the nineteenth century also paved the way for totalitarianism, by legitimizing the concept of endless expansion. After Europeans had engaged in imperialist expansion on other continents, political movements developed which aimed to copy the methods of imperialism on the European continent itself. Arendt refers specifically to the “pan-movements” of pan-Germanism and pan-Slavism, which promised continental empires to nations that had little hope of overseas expansion. According to Arendt, “Nazism and Bolshevism owe more to Pan-Germanism and Pan-Slavism (respectively) than to any other ideology or political movement.”\n\nArendt argues that both the Nazi and Bolshevik movements “recruited their members from [a] mass of apparently indifferent people whom all other parties had given up,” and who “had reason to be equally hostile to all parties.” For this reason, totalitarian movements did not need to use debate or persuasion, and did not need to refute the arguments of the other parties. Their target audience did not have to be persuaded to despise the other parties or the democratic system, because it consisted of people who already despised mainstream politics. As a result, totalitarian movements were free to use violence and terror against their opponents without fear that this might alienate their own supporters. Instead of arguing against their opponents, they adopted deterministic views of human behavior and presented opposing ideas as “originating in deep natural, social, or psychological sources beyond the control of the individual and therefore beyond the power of reason.” The Nazis in particular, during the years before their rise to power, engaged in “killing small socialist functionaries or influential members of opposing parties” both as a means to intimidate opponents and as a means of demonstrating to their supporters that they were a party of action, “different from the ‘idle talkers’ of other parties.”\n\nTotalitarian governments make extensive use of propaganda, and are often characterized by having a strong distinction between what they tell their own supporters and the propaganda they produce for others. Arendt distinguishes these two categories as \"indoctrination\" and \"propaganda\". Indoctrination consists of the message that a totalitarian government promotes internally, to the members of the ruling party and that segment of the population which supports the government. Propaganda consists of the message that a totalitarian government seeks to promote in the outside world, and also among those parts of its own society which may not support the government. Thus, “the necessities for propaganda are always dictated by the outside world,” while the opportunities for indoctrination depend on “the totalitarian governments’ isolation and security from outside interference.” \n\nThe type of indoctrination used by the Soviets and the Nazis was characterized by claims of “scientific” truth, and appeals to “objective laws of nature.” Both movements took a deterministic view of human society and claimed that their ideologies were based on scientific discoveries regarding race (in the case of the Nazis) or the forces governing human history (in the case of the Soviets). Arendt identifies this as being in certain ways similar to modern advertising, in which companies claim that scientific research shows their products to be superior, but more generally she argues that it is an extreme version of “that obsession with science which has characterized the Western world since the rise of mathematics and physics in the sixteenth century.” By their use of pseudoscience as the main justification for their actions, Nazism and Stalinism are distinguished from earlier historical despotic regimes, who appealed instead to religion or sometimes did not try to justify themselves at all. According to Arendt, totalitarian governments did not merely use these appeals to supposed scientific laws as propaganda to manipulate others. Rather, totalitarian leaders like Hitler and Stalin genuinely believed that they were acting in accordance with immutable natural laws, to such an extent that they were willing to sacrifice the self-interest of their regimes for the sake of enacting those supposed laws. For instance, the Nazis treated the inhabitants of occupied territories with extreme brutality and planned to depopulate Eastern Europe in order to make way for colonists from the German “master race,” despite the fact that this actively harmed their war effort. Stalin repeatedly purged the Communist Party of people who deviated even slightly from the party line, even when this weakened the party or the Soviet government, because he believed that they represented the interests of “dying classes” and their demise was historically inevitable.\n\nArendt also identifies the central importance of an all-powerful leader in totalitarian movements. As in other areas, she distinguishes between totalitarian leaders (such as Hitler and Stalin) and non-totalitarian dictators or autocratic leaders. The totalitarian leader does not rise to power by personally using violence or through any special organizational skills, but rather by controlling appointments of personnel within the party, so that all other prominent party members owe their positions to him. With loyalty to the leader becoming the primary criterion for promotion, ambitious party members compete with each other in trying to express their loyalty, and a cult of personality develops around the leader. Even when the leader is not particularly competent and the members of his inner circle are aware of his deficiencies, they remain committed to him out of fear that without him the entire power structure would collapse.\n\nOnce in power, according to Arendt, totalitarian movements face a major dilemma: they built their support on the basis of anger against the status quo and on impossible or dishonest promises, but now they have become the new status quo and are expected to carry out their promises. They deal with this problem by engaging in a constant struggle against external and internal enemies, real or imagined, so as to enable them to say that, in a sense, they have not yet gained the power they need to fulfill their promises. According to Arendt, totalitarian governments must be constantly fighting enemies in order to survive. This explains their apparently irrational behavior, for example when Hitler continued to make territorial demands even after he was offered everything he asked for in the Munich Agreement, or when Stalin unleashed the Great Terror despite the fact that he faced no significant internal opposition.\n\nArendt points out the widespread use of concentration camps by totalitarian governments, arguing that they are the most important manifestation of the need to find enemies to fight against, and are therefore “more essential to the preservation of the regime’s power than any of its other institutions.” Although forced labor was commonly imposed on inmates of concentration camps, Arendt argues that their primary purpose was not any kind of material gain for the regime: “The only permanent economic function of the camps has been the financing of their own supervisory apparatus; thus from the economic point of view the concentration camps exist mostly for their own sake.” The Nazis in particular carried this to the point of “open anti-utility,” by expending large sums of money, resources and manpower – during a war – for the purpose of building and staffing extermination camps and transporting people to them. This sets apart the concentration camps of totalitarian regimes from older human institutions that bear some similarity to them, such as slavery. Slaves were abused and killed for the sake of profit; concentration camp inmates were abused and killed because a totalitarian government needed to justify its existence. Finally, Arendt points out that concentration camps under both Hitler and Stalin included large numbers of inmates who were innocent of any crime – not only in the ordinary sense of the word, but even by the standards of the regimes themselves. That is to say, most of the inmates had not actually committed any action against the regime.\n\nThroughout her analysis, Arendt emphasized the modernity and novelty of the governmental structures set up by Stalin and Hitler, arguing that they represented “an entirely new form of government” which is likely to manifest itself again in various other forms in the future. She also cautioned against the belief that future totalitarian movements would necessarily share the ideological foundations of Nazism or Stalinism, writing that “all ideologies contain totalitarian elements.”\n\nThe totalitarian paradigm in the comparative study of Nazi Germany and the Soviet Union was further developed by Carl Friedrich and Zbigniew Brzezinski, who wrote extensively on this topic both individually and in collaboration. Similar to Hannah Arendt, they state that “totalitarian dictatorship is a new phenomenon; there has never been anything quite like it before.” Friedrich and Brzezinski classify totalitarian dictatorship as a type of autocracy, but argue that it is different in important ways from most other historical autocracies. In particular, it is distinguished by a reliance on modern technology and mass legitimation. Unlike Arendt, Friedrich and Brzezinski apply the notion of totalitarian dictatorship not only to the regimes of Hitler and Stalin, but also to the USSR throughout its entire existence, as well as the regime of Benito Mussolini in Italy and the People’s Republic of China under Mao Zedong.\n\nCarl Friedrich noted that the “possibility of equating the dictatorship of Stalin in the Soviet Union and that of Hitler in Germany” has been a deeply controversial topic and a subject of debate almost from the beginning of those dictatorships. Various other aspects of the two regimes have also been the subject of intense scholarly debate, such as whether Nazi and Stalinist ideologies were genuinely believed and pursued by the respective governments, or whether the ideologies were merely convenient justifications for dictatorial rule. Friedrich himself argues in favor of the former view.\n\nFriedrich and Brzezinski argue that Nazism and Stalinism are not only similar to each other, but also represent a continuation or a return to the tradition of European absolute monarchy on certain levels. In the absolute monarchies of the seventeenth and eighteenth centuries, the monarch ultimately held all decisional power, and was considered accountable only to God. In Stalinism and Nazism, the leader likewise held all real power, and was considered accountable only to various intangible entities such as “the people”, “the masses” or “the Volk.” Thus the common feature of autocracies – whether monarchical or totalitarian – is the concentration of power in the hands of a leader who cannot be held accountable by any legal mechanisms, and who is supposed to be the embodiment of the will of an abstract entity. Friedrich and Brzezinski also identify other features common to all autocracies, such as “the oscillation between tight and loose control.” The regime alternates between periods of intense repression and periods of relative freedom, often represented by different leaders. This depends in part on the personal character of different leaders, but Friedrich and Brzezinski believe that there is also an underlying political cycle, in which rising discontent leads to increased repression up to the point at which the opposition is eliminated, then controls are relaxed until the next time that popular dissatisfaction begins to grow.\n\nThus, placing Stalinism and Nazism within the broader historical tradition of autocratic government, Friedrich and Brzezinski hold that “totalitarian dictatorship, in a sense, is the adaptation of autocracy to twentieth-century industrial society.” However, at the same time, they insist that totalitarian dictatorship is a “\"novel\" type of autocracy” and argue that twentieth century totalitarian regimes (such as those of Hitler and Stalin) had more in common with each other than with any other form of government, including historical autocracies of the past. Totalitarianism can only exist after the creation of modern technology, because such technology is essential for propaganda, for surveillance of the population, and for the operation of a secret police. Furthermore, when speaking of the differences and similarities between fascist and communist regimes, Friedrich and Brzezinski insist that the two kinds of totalitarian governments are “basically alike” but “not wholly alike” – they are more similar to each other than to other forms of government, but they are not the same. Among the major differences between them, Friedrich and Brzezinski identify in particular the fact that communists seek “the world revolution of the proletariat,” while fascists wish to “establish the imperial predominance of a particular nation or race.” \n\nIn terms of the similarities between Nazism and Stalinism, Friedrich lists five main aspects that they hold in common: First, an official ideology that is supposed to be followed by all members of society, at least passively, and which promises to serve as a perfect guide towards some ultimate goal. Second, a single political party, composed of the most enthusiastic supporters of the official ideology, representing an elite group within society (no more than 10 percent of the population), and organized along strictly regimented lines. Third, “a technologically conditioned near-complete monopoly of control of all means of effective armed combat” in the hands of the party or its representatives. Fourth, a similar monopoly held by the party over the mass media and all technological forms of communication. Fifth, “a system of terroristic police control” that is not only used to defend the regime against real enemies, but also to persecute various groups of people who are only suspected of being enemies or who may potentially become enemies in the future.\n\nTwo first pillars of any totalitarian government, according to Friedrich and Brzezinski, are the dictator and the Party. The dictator, whether Stalin, Hitler or Mussolini, holds supreme power. Friedrich and Brzezinski explicitly reject the claim that the Party, or any other institution, could provide a significant counterweight to the power of the dictator in Nazism or Stalinism. The dictator needs the Party in order to be able to rule, so he may be careful not to make decisions that would go directly against the wishes of other leading Party members, but ultimate authority rests with him and not with them. Like Arendt, Friedrich and Brzezinski also identify the cult of personality surrounding the leader as an essential element of a totalitarian dictatorship, and reference Stalin’s personality cult in particular. They also draw attention to the fact that Hitler and Stalin were expected to provide ideological direction for their governments and not merely practical leadership. Friedrich and Brzezinski write that “unlike military dictators in the past, but like certain types of primitive chieftains, the totalitarian dictator is both ruler and high priest.” That is to say, he not only governs, but also provides the principles on which his government is to be based. This is partly due to the way that totalitarian governments arise. They come about when a militant ideological movement seizes power, so the first leader of a totalitarian government is usually the ideologue who built the movement that seized power, and subsequent leaders try to emulate him.\n\nThe totalitarian dictator needs loyal lieutenants to carry out his orders faithfully and with a reasonable degree of efficiency. Friedrich and Brzezinski identify parallels between the men in Hitler and Stalin’s entourage, arguing that both dictators used similar people to perform similar tasks. Thus, for example, Martin Bormann and Georgy Malenkov were both capable administrators and bureaucrats, while Heinrich Himmler and Lavrentiy Beria were ruthless secret police chiefs responsible for suppressing any potential challenge to the dictator’s power. Both Hitler and Stalin promoted rivalry and distrust among their lieutenants so as to ensure that none of them would become powerful enough to challenge the dictator himself. This is the cause of an important weakness of the totalitarian regimes: the problem of succession. Friedrich points out that neither the Nazi nor the Stalinist government ever established any official line of succession or any mechanism to decide who would replace the dictator after his death. The dictator, being the venerated “father of the people,” was regarded as irreplaceable. There could never be any heir apparent, because such an heir would have been a threat to the power of the dictator while he was alive. Thus the dictator’s inevitable death would always leave behind a major power vacuum and cause a political crisis. In the case of the Nazi regime, since Hitler died mere days before the final defeat of Germany in the war, this never became a major issue. In the case of the USSR, Stalin’s death led to a prolonged power struggle.\n\nFriedrich and Brzezinski also identify key similarities between the Nazi and Stalinist political parties, which set them apart from other types of political parties. Both the Nazi Party and the CPSU under Stalin had very strict membership requirements and did not accept members on the basis of mere agreement with the Party’s ideology and goals. Rather, they strictly tested potential members, in a manner similar to exclusive clubs, and often engaged in political purges of the membership, expelling large numbers of people from their ranks (and sometimes arresting and executing those expelled, such as in the Great Purge or the Night of the Long Knives). Thus, the totalitarian party cultivates the idea that to be a member is a privilege which needs to be earned, and total obedience to the leader is required in order to maintain this privilege. While both Nazism and Stalinism required party members to display such total loyalty in practice, they differed in the way they dealt with it in theory. Nazism openly proclaimed the hierarchical ideal of absolute obedience to the Führer as one of its key ideological principles (the \"Führerprinzip\"). Stalinism, meanwhile, denied that it did anything similar, and claimed instead to uphold democratic principles, with the Party Congress (made up of elected delegates) supposedly being the highest authority. However, Stalinist elections typically featured only a single candidate, and the Party Congress met very rarely and simply approved Stalin’s decisions. Thus, regardless of the differences in their underlying ideological claims, the Nazi and Stalinist parties were organized in practice along similar lines, with a rigid hierarchy and centralized leadership.\n\nEach totalitarian party and dictator is supported by a specific totalitarian ideology. Friedrich and Brzezinski argue, in agreement with Arendt, that Nazi and Stalinist leaders really believed in their respective ideologies and did not merely use them as tools to gain power. Several major policies, such as the Stalinist collectivization of agriculture or the Nazi “final solution”, cannot be explained by anything other than a genuine commitment to achieve ideological goals, even at great cost. The ideologies were different and their goals were different, but what they had in common was a utopian commitment to reshaping the world, and a determination to fight by any means necessary against a real or imagined enemy. This stereotyped enemy could be described as “the fat rich Jew or the Jewish Bolshevik” for the Nazis, or “the war-mongering, atom-bomb-wielding American Wallstreeter” for the Soviets.\n\nAccording to Friedrich and Brzezinski, the most important difference between Nazi and Stalinist ideology lies in the degree of universality involved. Stalinism, and communist ideology in general, is universal in its appeal and addresses itself to all the “workers of the world.” Nazism, on the other hand, and fascist ideology in general, can only address itself to one particular race or nation – the “master race” that is destined to dominate all others. Therefore, “in communism social justice appears to be the ultimate value, unless it be the classless society that is its essential condition; in fascism, the highest value is dominion, eventually world dominion, and the strong and pure nation-race is \"its\" essential condition, as seen by its ideology.” This means that fascist or Nazi movements from different countries will be natural enemies, rather than natural allies, as they each seek to extend the dominion of their own nation at the expense of others. Friedrich and Brzezinski see this as a weakness inherent in fascist and Nazi ideology, while communist universalism is a source of ideological strength for Stalinism.\n\nFriedrich and Brzezinski also draw attention to the symbols used by Nazis and Stalinists to represent themselves. The Soviet Union adopted the hammer and sickle, a newly-created symbol, “invented by the leaders of the movement and pointing to the future.” Meanwhile, Nazi Germany used the swastika, “a ritual symbol of uncertain origin, quite common in primitive societies.” Thus, one is trying to project itself as being oriented towards a radically new future, while the other is appealing to a mythical heroic past.\n\nTotalitarian dictatorships maintain themselves in power through the use of propaganda and terror, which Friedrich and Brzezinski believe to be closely connected. Terror may be enforced with arrests and executions of dissenters, but it can also take more subtle forms, such as the threat of losing one’s job, social stigma and defamation. “Terror” can refer to any widespread method used to intimidate people into submission as a matter of daily life. According to Friedrich and Brzezinski, the most effective terror is invisible to the people it affects. They simply develop a habit of acting in a conformist manner and not questioning authority, without necessarily being aware that this is what they are doing. Thus, terror creates a society dominated by apparent consensus, where the vast majority of the population appears to support the government. Propaganda is then used to maintain this appearance of popular consent. \n\nTotalitarian propaganda is one of the features that distinguishes totalitarian regimes as modern forms of government and separates them from older autocracies, since a totalitarian government holds complete control over all means of communication (not only public communication such as the mass media, but also private communication such as letters and telephone calls, which are strictly monitored). The methods of propaganda were very similar in the Stalinist USSR and in Nazi Germany. Both Joseph Goebbels and Soviet propagandists sought to demonize their enemies and present a picture of a united people standing behind its leader to confront foreign threats. In both cases there was no attempt to convey complex ideological nuances to the masses, with the message being instead about a simplistic struggle between good and evil. Both Nazi and Stalinist regimes produced two very different sets of propaganda – one for internal consumption and one for potential sympathizers in other countries. And both regimes would sometimes radically change their propaganda line as they made peace with a former enemy or got into a war with a former ally. Yet, paradoxically, a totalitarian government’s complete control over communications renders that government highly misinformed. With no way for anyone to express criticism, the dictator has no way of knowing how much support he actually has among the general populace. With all government policies always declared successful in propaganda, officials are unable to determine what actually worked and what didn’t. Both Stalinism and Nazism suffered from this problem, especially during the war between them. As the war turned against Germany, there was growing opposition to Hitler’s rule, including within the ranks of the military, but Hitler was never aware of this until it was too late (see: 20 July plot). In 1948, during the early days of the Berlin Blockade, the Soviet leadership apparently believed that the population of West Berlin was sympathetic to Soviet Communism and that they would request to join the Soviet zone. Given enough time, the gap between real public opinion and what the totalitarian government believes about public opinion can grow so wide that the government is no longer able to even produce effective propaganda, because it does not know what the people actually think and so it does not know what to tell them. Friedrich and Brzezinski refer to this as the “ritualization of propaganda”: the totalitarian regime continues to produce propaganda as a political ritual, with little real impact on public opinion.\n\nThe totalitarian use of mass arrests, executions and concentration camps – also noted by Arendt – was analyzed at length by Friedrich and Brzezinski. They hold that “totalitarian terror maintains, in institutionalized form, the civil war that originally produced the totalitarian movement and by means of which the regime is able to proceed with its program, first of social disintegration and then of social reconstruction.” Both Stalinism and Nazism saw themselves as engaging in a life-or-death struggle against implacable enemies. But to declare that the struggle had been won would have meant to declare that most of the totalitarian features of the government were no longer needed. A secret police force, for instance, has no reason to exist if there are no dangerous traitors who need to be found. Thus the struggle, or “civil war” against internal enemies, must be institutionalized and must continue indefinitely. In the Stalinist USSR, the repressive apparatus was eventually turned against members of the Communist Party itself in the Great Purge and the show trials that accompanied it. Nazism, by contrast, had a much shorter lifespan in power, and Nazi terror generally maintained an outward focus, with the extermination of the Jews always given top priority. The Nazis did not turn inward towards purging their own party except in a limited way on two occasions (the Night of the Long Knives and the aftermath of the 20 July plot). \n\nThe peak of totalitarian terror was reached with the Nazi concentration camps. These ranged from labor camps to extermination camps, and they are described by Friedrich and Brzezinski as aiming to “eliminate all actual, potential, and imagined enemies of the regime.” As the field of Holocaust studies was still in its early stages at the time of their writing, they do not describe the conditions in detail, but do refer to the camps as involving “extreme viciousness.” They also compare these camps with the Soviet Gulag system, and highlight the use of concentration camps as a method of punishment and execution by Nazi and Stalinist regimes alike. However, unlike Hannah Arendt, who held that the Gulag camps served no economic purpose, Friedrich and Brzezinski argue that they provided an important source of cheap labor for the Stalinist economy.\n\nThe comparative study of Nazism and Stalinism was carried further by other groups of scholars, such as Moshe Lewin and Ian Kershaw together with their collaborators. Writing after the dissolution of the USSR, Lewin and Kershaw take a longer historical perspective and regard Nazism and Stalinism not so much as examples of a new type of society (like Arendt, Friedrich and Brzezinski did), but more as historical “anomalies” – unusual deviations from the typical path of development that most industrial societies are expected to follow. Therefore, the task of comparing Nazism and Stalinism is, to them, a task of explaining why Germany and Russia (along with other countries) deviated from the historical norm. At the outset, Lewin and Kershaw identify similarities between the historical situations in Germany and Russia prior to the First World War and during that war. Both countries were ruled by authoritarian monarchies, who were under pressure to make concessions to popular demands. Both countries had “powerful bureaucracies and strong military traditions.” Both had “powerful landowning classes,” while also being in the process of rapid industrialization and modernization. And both countries had expansionist foreign policies with a particular interest in Central and Eastern Europe. Lewin and Kershaw do not claim that these factors made Stalinism or Nazism inevitable, but rather that they help to explain why the Stalinist and Nazi regimes developed similar features.\n\nIan Kershaw admitted that Stalinism and Nazism are comparable in “the nature and extent of their inhumanity,” but noted that the two regimes were different in a number of aspects Lewin and Kershaw question the usefulness of grouping the Stalinist and Nazi regimes together under a “totalitarian” category, saying that it remains an open question whether the similarities between them are greater or smaller than the differences. In particular, they criticize what they see as the ideologically-motivated attempt to determine which regime killed more people, saying that apologists of each regime are trying to defend their side by claiming the other was responsible for more deaths.\n\nLewin and Kershaw place the cult of personality at the center of their comparison of Nazism and Stalinism, writing that both regimes “represented a new genre of political system centred upon the artificial construct of a leadership cult – the ‘heroic myth’ of the ‘great leader’, no longer a king or emperor but a ‘man of the people.” With regard to Stalinism, they emphasize its bureaucratic character, and its “merging of the most modern with the most archaic traits” by combining modern technology and the latest methods of administration and propaganda with the ancient practice of arbitrary rule by a single man. They compare this with the Prussian military tradition in Germany, which had been called “bureaucratic absolutism” in the eighteenth century, and which played a significant role in the organization of the Nazi state in the twentieth century.\n\nKershaw agrees with Mommsen that there was a fundamental difference between Nazism and Stalinism regarding the importance of the leader. Stalinism had an absolute leader, but he was not essential. He could be replaced by another. Nazism, on the other hand, was a “classic charismatic leadership movement,” defined entirely by its leader. Stalinism had an ideology which existed independently of Stalin. But for Nazism, “Hitler \"was\" ideological orthodoxy” – Nazi ideals were by definition whatever Hitler said they were. In Stalinism, the bureaucratic apparatus was the foundation of the system, while in Nazism, the person of the leader was the foundation.\n\nMoshe Lewin also focuses on the comparison between the personality cults of Hitler and Stalin, and their respective roles in Nazi Germany and the Soviet Union. He refers to them as the “Hitler myth” and the “Stalin myth,” and argues that they served different functions within their two regimes. The function of the “Hitler myth” was to legitimize Nazi rule. The function of the “Stalin myth” was to legitimize not Soviet rule itself, but Stalin’s leadership within the Party. Stalin’s personality cult existed precisely because Stalin knew that he was replaceable, and feared that he might be replaced, and so needed to bolster his authority as much as possible. While the “Hitler myth” was essential to Nazi Germany, the “Stalin myth” was essential only to Stalin, not to the Soviet Union itself.\n\nTogether with fellow historian Hans Mommsen, Lewin argues that the Stalinist and Nazi regimes featured an “intrinsic structural contradiction” which led to “inherent self-destructiveness”: they depended on a highly organized state bureaucracy which was trying to set up complex rules and procedures for every aspect of life, yet this bureaucracy was under the complete personal control of a despot who made policy decisions as he saw fit, routinely changing his mind on major issues, without any regard for the rules and institutions which his own bureaucracy had set up. The bureaucracy and the leader needed each other, but also undermined each other with their different priorities. Mommsen sees this as being a much greater problem in Nazi Germany than in Stalin’s Soviet Union, as the Nazis inherited large parts of the traditional German bureaucracy, while the Soviets largely built their own bureaucracy from the ground up. He argues that many of the irrational features of the Nazi regime – such as wasting resources on exterminating undesirable populations instead of using those resources in the war effort – were caused by the dysfunction of the Nazi state rather than by fanatical commitment to Nazi ideology. In accordance with the Führerprinzip, all decisional power in the Nazi state ultimately rested with Hitler. But Hitler often issued only vague and general directives, forcing other Nazi leaders lower down in the hierarchy to guess what precisely the Führer wanted. This confusion produced competition between Nazi officials, as each of them attempted to prove that he was a more dedicated Nazi than his rivals, by engaging in ever more extreme policies. This competition to please Hitler was, according to Mommsen, the real cause of Nazi irrationality. Hitler was aware of it, and deliberately encouraged it out of a “social-darwinist conviction that the best man would ultimately prevail.” Mommsen argues that this represents a structural difference between the regimes of Hitler and Stalin. In spite of its purges, Stalin’s regime was more effective in building a stable bureaucracy, such that it was possible for the system to sustain itself and continue even without Stalin. The Nazi regime, on the other hand, was much more personalized and depended entirely on Hitler, being unable to build any lasting institutions.\n\nKershaw also saw major personal differences between Stalin and Hitler and their respective styles of rule. He describes Stalin as “a committee man, chief oligarch, man of the machine” and a “creature of his party,” who came to power only thanks to his party and his ability to manipulate the levers of power within that party. Hitler, by contrast, came to power based on his charisma and mass appeal, and in the Nazi regime it was the leader that created the party instead of the other way around. According to Kershaw, “Stalin was a highly interventionist dictator, sending a stream of letters and directives determining or interfering with policy,” while Hitler “was a non-interventionist dictator as far as government administration was concerned,” preferring to involve himself in military affairs and plans for conquest rather than the daily routine of government work, and giving only broad verbal instructions to his subordinates regarding civilian affairs, which they were expected to translate into policy. Furthermore, although both regimes featured all-pervasive cults of personality, there was a qualitative difference between those cults. Stalin’s personality cult was “superimposed upon the Marxist-Leninist ideology and Communist Party,” and could be abandoned (or replaced with a personality cult around some other leader) without major changes to the regime. On the other hand, “the ‘Hitler myth’ was structurally indispensable to, in fact the very basis of, and scarcely distinguishable from, the Nazi Movement and its \"Weltanschauung\".” The belief in the person of Adolf Hitler as the unique savior of the German nation was the very foundation of Nazism, to such an extent that Nazism found it impossible to even imagine a successor to Hitler. Thus, in Kershaw’s analysis, Stalinism was a fundamentally bureaucratic system while Nazism was the embodiment of “charismatic authority” as described by Max Weber. Stalinism could exist without its leader. Nazism could not. \n\nThe topic of comparisons between Nazism and Stalinism was also studied in the 1990s and 2000s by historians Henry Rousso, Nicolas Werth and Philippe Burrin.\n\nRousso defends the work of Carl Friedrich by pointing out that Friedrich himself had only said that Stalinism and Nazism were comparable, not that they were identical. Rousso also argues that the popularity of the concept of totalitarianism (the way that large numbers of people have come to routinely refer to certain governments as “totalitarian”) should be seen as evidence that the concept is useful, that it really describes a specific type of government which is different from other dictatorships. At the same time, however, Rousso notes that the concept of totalitarianism is descriptive rather than analytical: the regimes described as totalitarian do not have a common origin and did not arise in similar ways. Nazism is unique among totalitarian regimes in having taken power in “a country endowed with an advanced industrial economy and with a system of political democracy (and an even older political pluralism).” All other examples of totalitarianism (including the Stalinist regime) took power, according to Rousso, “in an agrarian economy, in a poor society without a tradition of political pluralism, not to mention democracy, and where diverse forms of tyranny had traditionally prevailed.” He sees this as a weakness of the concept of totalitarianism, because it merely describes the similarities between Stalinism and Nazism without dealing with the very different ways they came to power. On the other hand, Rousso agrees with Hannah Arendt that “totalitarian regimes constitute something new in regard to classical tyranny, authoritarian regimes, or other forms of ancient and medieval dictatorships,” and he says that the main strength of the concept of totalitarianism is the way it highlights this inherent novelty of the regimes involved.\n\nNicolas Werth and Philippe Burrin have worked together on comparative assessments of Stalinism and Nazism, with Werth covering the Stalinist regime and Burrin covering Nazi Germany. One of the topics they have studied is the question of how much power the dictator really held in the two regimes. Werth identifies two main historiographical approaches in the study of the Stalinist regime: Those who emphasize the power and control exercised by Joseph Stalin himself, attributing most of the actions of the Soviet government to deliberate plans and decisions made by him, and those who argue that Stalin had no pre-determined course of action in mind, that he was reacting to events as they unfolded, and that the Soviet bureaucracy had its own agenda which often differed from Stalin’s wishes. Werth regards these as two mistaken extremes, one making Stalin seem all-powerful, the other making him seem like a weak dictator. But he believes that the competing perspectives are useful in drawing attention to the tension between two different forms of organization in the Stalinist USSR: an “administrative system of command,” bureaucratic and resistant to change but effective in running the Soviet state, and the strategy of “running the country in a crudely despotic way by Stalin and his small cadre of directors.” Thus, Werth agrees with Lewin that there was an inherent conflict between the priorities of the Soviet bureaucracy and Stalin’s accumulation of absolute power in his own hands. According to Werth, it was this unresolved and unstated conflict that led to the Great Purge and to the use of terror by Stalin’s regime against its own party and state cadres.\n\nIn studying similar issues with regard to the Nazi regime, Philippe Burrin draws attention to the debate between the “Intentionalist” and “Functionalist” schools of thought, which dealt with the question of whether the Nazi regime represented an extension of Hitler’s autocratic will, faithfully obeying his wishes, or whether it was an essentially chaotic and uncontrollable system that functioned on its own with little direct input from the Führer. Like Kershaw and Lewin, Burrin says that the relationship between the leader and his party’s ideology was different in Nazism compared to Stalinism: “One can rightly state that Nazism cannot be dissociated from Hitlerism, something that is difficult to affirm for Bolshevism and Stalinism.” Unlike Stalin, who inherited an existing system with an existing ideology and presented himself as the heir to the Leninist political tradition, Hitler created both his movement and its ideology by himself, claiming to be “someone sent by Providence, a Messiah whom the German people had been expecting for centuries, even for two thousand years, as Heinrich Himmler enjoyed saying.” Thus, there could be no real conflict between the Party and the leader in Nazi Germany, because the Nazi Party’s entire reason for existence was to support and follow Hitler. However, there was a potential for division between the leader and the state bureaucracy, due to the way that Nazism came to power – as part of an alliance with traditional conservative elites, industrialists, and the army. Unlike the USSR, Nazi Germany did not build its own state, but rather inherited the state machinery of the previous government. This provided the Nazis with an immediate supply of capable and experienced managers and military commanders, but on the other hand it also meant that the Nazi regime had to rely on the cooperation of people who had not been Nazis prior to Hitler’s rise to power, and whose loyalty was questionable. It was only during the war, when Nazi Germany conquered large territories and had to create Nazi administrations for them, that brand new Nazi bureaucracies were created without any input or participation from traditional German elites. This produced a surprising difference between Nazism and Stalinism: When the Stalinist USSR conquered territory, it created smaller copies of itself and installed them as the governments of the occupied countries. When Nazi Germany conquered territory, on the other hand, it did not attempt to create copies of the German government back home. Instead, it experimented with different power structures and policies, often reflecting a “far more ample Nazification of society than what the balance of power authorized in the Reich.”\n\nAnother major topic investigated by Werth and Burrin was the violence and terror employed by the regimes of Hitler and Stalin. Werth reports that the Stalinist USSR underwent an “extraordinary brutalization of the relations between state and society” for the purpose of rapid modernization and industrialization, to “gain one hundred years in one decade, and to metamorphose the country into a great industrial power.” This transformation was accomplished at the cost of massive violence and a sociopolitical regression into what Werth calls “military-feudal exploitation.” The types of violence employed by the Stalinist regime included loss of civil rights, mass arrests, deportations of entire ethnic groups from one part of the USSR to another, forced labor in the Gulag, mass executions (especially during the Great Terror of 1937-38), and most of all the great famine of 1932-33, known as the Holodomor. All levels of Soviet society were affected by Stalinist repression, from the top to the bottom. At the top, high-ranking members of the Communist Party were arrested and executed under the claim that they had plotted against Stalin (and in some cases they were forced to confess to imaginary crimes in show trials). At the bottom, the peasantry suffered the Holodomor famine (especially in Ukraine), and even outside of the famine years they were faced with very high grain quotas.\n\nWerth identifies four categories of people that became the targets of Stalinist violence in the USSR. He lists them from smallest to largest. The first and smallest group consisted of many of Stalin’s former comrades-in-arms, who had participated in the revolution and were known as “Old Bolsheviks.” They were dangerous to Stalin because they had known him before his rise to power and could expose the many false claims made by his personality cult. The second group consisted of mid-level Communist Party officials, who were subject to mass arrests and executions in the late 1930s, particularly during the Great Purge. Eliminating them served a dual purpose: It helped Stalin to centralize power in the Kremlin (as opposed to regional centers), and it also provided him with “corrupt officials” that he could blame for earlier repressions and unpopular policies. Werth draws parallels between this and the old Tsarist tradition of blaming “bad bureaucrats” – rather than the Tsar – for unpopular government actions. The third group was made up of ordinary citizens from all walks of life who resorted to petty crime in order to provide for themselves in the face of worsening living standards (for example by taking home some wheat from the fields or tools from the factory). This type of petty crime became very widespread, and was often punished as if it were intentional sabotage motivated by political opposition to the USSR. The fourth and largest category consisted of ethnic groups that were subject to deportation, famine, or arbitrary arrests under the suspicion of being collectively disloyal to Stalin or to the Soviet state. This included the Holodomor famine directed at the Ukrainians, the deportation of ethnic groups suspected of pro-German sympathies (such as the Volga Germans, the Crimean Tatars, the Chechens and others), and eventually also persecution of ethnic Jews, especially as Stalin grew increasingly antisemitic near the end of his life.\n\nBurrin’s study of violence carried out by the Nazi regime begins with the observation that “violence is at the heart of Nazism,” and that Nazi violence is “established as a doctrine and exalted in speech.” This marks a point of difference between Nazism and Stalinism, according to Burrin. In Stalinism, there was a gulf between ideology and reality when it came to violence. The Soviet regime continuously denied that it was repressive, proclaimed itself a defender of peace, and sought to conceal all the evidence to the contrary. In Nazism, on the other hand, “doctrine and reality were fused from the start.” Nazism not only practiced violent repression and war, but advocated it in principle as well, considering war to be a positive force in human civilization and openly seeking ”living space” and the domination of the European continent by ethnic Germans.\n\nBurrin identifies three motivations for Nazi violence: political repression, exclusion and social repression, and racial politics. The first of these, political repression, is common in many dictatorships. The Nazis aimed to eliminate their real or imagined political opponents, first in the Reich and later in the occupied territories during the war. Some of these opponents were executed, while others were imprisoned in concentration camps. The first targets of political repression, immediately after Hitler’s rise to power in 1933, were the parties of the Left in general and the Communists in particular. Then, after the mid-1930s, repression was extended to members of the clergy, and later to the conservative opposition as well (especially after the failed attempt to assassinate Hitler in 1944). The death penalty was used on a wide scale, even before the war. During the war, political repression was greatly expanded both inside Germany and especially in the newly occupied territories. Political prisoners in the concentration camps numbered only about 25,000 at the beginning of the war. By January 1945 they had swelled to 714,211 – most of them non-Germans accused of plotting against the Reich.\n\nThe second type of Nazi violence, motivated by exclusion and social repression, was the violence aimed at purging German society of people whose lifestyle was considered incompatible with the social norms of the Nazi regime (even if the people involved were racially pure and able-bodied). Such people were divided into two categories: homosexuals and “asocials.” The “asocials” were only vaguely defined, and included “Gypsies, tramps, beggars, prostitutes, alcoholics, the jobless who refused any employment, and those who left their work frequently or for no reason.”\n\nThe third and final type of Nazi violence, by far the most extensive, was violence motivated by Nazi racial policies. This was aimed both inward, to cleanse the “Aryan race” of “degenerate” elements and life unworthy of life, as well as outward, to seek the extermination of “inferior races”. Germans considered physically or mentally unfit were among the first victims. One of the first laws of the Nazi regime mandated the forced sterilization of people suffering from physical handicaps or who had psychiatric conditions deemed to be hereditary. Later, sterilization was replaced by murder of the mentally ill and of people with severe disabilities, as part of a “euthanasia” program called Aktion T4. Burrin notes that this served no practical political purpose – the people being murdered could not have possibly been political opponents of the regime – so the motivation was purely a matter of racial ideology. The most systematic and by far the most large-scale acts of Nazi violence, however, were directed at “racially inferior” non-German populations. As laid out in \"Generalplan Ost\", the Nazis wished to eliminate most of the Slavic populations of Eastern Europe, partly through deportation and partly through murder, in order to secure land for ethnic German settlement and colonization. But even more urgently, the Nazis wished to exterminate the Jews of Europe, whom they regarded as the implacable racial enemy of the Germans. This culminated in the Holocaust, the Nazi genocide of the Jews. Unlike in the case of all other target populations, the Jews were to be exterminated completely, with no individual exceptions for any reason.\n\nIn \"Beyond Totalitarianism: Stalinism and Nazism Compared\", editors Michael Geyer and Sheila Fitzpatrick disputed the concept of totalitarianism, noting that the term entered political discourse first as a term of self-description by the Italian Fascists and was only later used as a framework to compare Nazi Germany with the Soviet Union. They argued that the totalitarian states were not as monolithic or as ideology-driven as they seemed. Geyer and Fitzpatrick describe Nazi Germany and the Stalinist USSR as “immensely powerful, threatening, and contagious dictatorships” who “shook the world in their antagonism.” Without calling them totalitarian, they identified their common features, including genocide, an all-powerful party, a charismatic leader, and pervasive invasion of privacy. However, they argue that Stalinism and Nazism did not represent a new and unique type of government, but rather that they can be placed in the broader context of the turn to dictatorship in Europe in the interwar period. The reason they appear extraordinary is because they were the “most prominent, most hard-headed, and most violent” of the European dictatorships of the 20th century. They are comparable because of their “shock and awe” and sheer ruthlessness, but underneath superficial similarities they were fundamentally different and that “when it comes to one-on-one comparison, the two societies and regimes may as well have hailed from different worlds.”\n\nAccording to Geyer and Fitzpatrick, the similarities between Nazism and Stalinism stem from the fact that they were both “ideology driven” and sought to subordinate all aspects of life to their respective ideologies. The differences stem from the fact that their ideologies were opposed to each other and regarded each other as enemies. Another major difference is that Stalin created a stable and long-lasting regime, while Nazi Germany had a “short-lived, explosive nature.” Notably, the stable state created by Stalinism was based on an entirely new elite, while Nazism, despite having the support of the traditional elite, failed to achieve stability.\n\nHowever, the two regimes did borrow ideas from one another, especially regarding propaganda techniques (most of all in architecture and cinema), but also in terms of state surveillance and antisemitism. At the same time, they both vigorously denied borrowing anything from each other. While their methods of propaganda were similar, the content was different. For instance, Soviet wartime propaganda revolved around the idea of resisting imperial aggression, while Nazi propaganda was about wars of racial conquest. Geyer and Fitzpatrick also take note of the fact that both Stalinism and Nazism sought to create a New Man, an “entirely modern, illiberal, and self-fashioned personage,” even though they had different visions about what being a “New Man” would mean.\n\nAmong the other authors contributing to the volume edited by Geyer and Fitzpatrick, David Hoffmann and Annette Timm discuss biopolitics and the pro-natalist policies of the Nazi and Stalinist regimes. Both governments were highly concerned over low fertility rates in their respective populations, and applied extensive and intrusive social engineering techniques to increase the number of births. Reproductive policies in the Soviet Union and Nazi Germany were administered through their health care systems—both regimes saw health care as a key pillar to their designs to develop a new society. While the Soviet Union had to design a public health care system from scratch, Nazi Germany built upon the pre-existing public health care system in Germany that had existed since 1883, when Otto von Bismarck's legislation had created the world's first national public health care program. The Nazis centralized the German health care system in order to enforce Nazi ideological components upon it, and replaced existing voluntary and government welfare agencies with new ones that were devoted to racial hygiene and other components of Nazi ideology.\n\nThe Nazi and Stalinist attempt to control family size was not unique, as many other European states practiced eugenics at this time, and the Stalinist and Nazi ideals were vastly different. In fact, they had more in common with third parties than with each other: Nazi Germany’s policies were rather similar to those in Scandinavia at the time, while the USSR’s policies resembled those in Catholic countries.The common point between Nazi and Stalinist practices was the connection of reproduction policies with the ideological goals of the state — \"part of the project of a rational, hypermodern vision for the re-organization of society\". There were nevertheless substantial differences between the two regimes' approaches. Stalin's Soviet Union never officially supported eugenics as the Nazis did—the Soviet government called eugenics a \"fascist science\"—although there were in fact Soviet eugenicists. The two regimes also had different approaches to the relationship between family and paid labor—Nazism promoted the male single-breadwinner family while Stalinism promoted the dual-wage-earner household.\n\nIn another contribution to the same volume, Christian Gerlach and Nicolas Werth discuss the topic of mass violence, and the way that it was used by both Stalinism and Nazism. Both Stalin's Soviet Union and Nazi Germany were violent societies where mass violence was accepted by the state, such as in the Great Terror of 1937 to 1938 in the Soviet Union and the Holocaust in Nazi Germany and its occupied territories in World War II.\n\nBoth the Stalinist Soviet Union and Nazi Germany utilized internment camps led by agents of the state – the NKVD in the Soviet Union and the SS in Nazi Germany. They also both engaged in violence against minorities based on xenophobia – the xenophobic violence of the Nazis was outspoken but rationalized as being against \"asocial\" elements while the xenophobic violence of the Stalinists was disguised as being against \"anti-soviet\", \"counter-revolutionary\" and \"socially harmful\" elements – a term which often targeted diaspora nationalities. The Stalinist Soviet Union established \"special settlements\" where the \"socially harmful\" or \"socially dangerous\" who included ex-convicts, criminals, vagrants, the disenfranchised and \"declassed elements\" were expelled to. These \"special settlements\" were largely in Siberia, the far north, the Urals, or other inhospitable territories. In July 1933, the Soviet Union made a mass arrest of 5000 Romani people effectively on the basis of their ethnicity, who were deported that month to the \"special settlements\" in Western Siberia. In 1935, the Soviet Union arrested 160,000 homeless people and juvenile delinquents and sent many of them to NKVD labor colonies where they did forced labor.\n\nThe Nazi regime was founded upon a racialist view of politics and envisioned the deportation or extermination of the majority of the population of Eastern Europe in order to open up “living space” for ethnic German settlers. This was mainly intended to be carried out after an eventual German victory in the war, but steps had already started being taken while the war was still ongoing. For instance, by the end of 1942, the Nazis had deported 365,000 Poles and Jews from their original homes in western Poland (now German-annexed) and into the General Government. A further 194,000 Poles were internally displaced (not deported to another territory but expelled from their homes). The Nazis had also deported 100,000 persons from Alsace, Lorraine, and Luxembourg, as well as 54,000 Slovenians.\n\nStalinism in practice in the Soviet Union pursued ethnic deportations from the 1930s to the early 1950s, with a total of 3 million Soviet citizens being subjected to ethnic-based resettlement. The first major ethnic deportation took place from December 1932 to January 1933, during which some 60,000 Kuban Cossacks were collectively criminally charged as a whole with association with resistance to socialism and affiliation with Ukrainian nationalism. From 1935 to 1936, the Soviet Union deported Soviet citizens of Polish and German origins living in the western districts of Ukraine, and Soviet citizens of Finnish origins living on the Finland-Soviet Union border. These deportations from 1935 to 1936 affected tens of thousands of families. From September to October 1937, Soviet authorities deported the Korean minority from its Far Eastern region that bordered on Japanese-controlled Korea. Soviet authorities claimed the territory was \"rich soil for the Japanese to till\" – implying a Soviet suspicion that the Koreans could potentially join forces with the Japanese to unite the land with Japanese-held Korea. Over 170,000 Koreans were deported to remote parts of Soviet Central Asia from September to October 1937. These ethnically-based deportations reflected a new trend in Stalinist policy, a \"Soviet xenophobia\" based on ideological grounds that suspected that these people were susceptible to foreign influence, and which was also based on a resurgent Russian nationalism.\n\nAfter Nazi Germany declared war on the Soviet Union in 1941, the Soviet Union initiated another major round of ethnic deportations. The first group targeted were Soviet Germans. Between September 1941 and February 1942, 900,000 people – over 70 percent of the entire Soviet German community – were deported to Kazakhstan and Siberia in mass operations. A second wave of mass deportations took place between November 1943 and May 1944, in which Soviet authorities expelled six ethnic groups (the Balkars, Chechens, Crimean Tatars, Ingush, Karachai, and Kalmyks) that together numbered 900,000. There were also smaller-scale operations involving ethnic cleansing of diaspora minorities during and after World War II, in which tens of thousands of Crimean Bulgarians, Greeks, Iranians, Khemshils, Kurds, and Meskhetian Turks were deported from the Black Sea and Transcaucasian border regions.\n\nTwo ethnic groups that were specifically targeted for persecution by Stalin's Soviet Union were the Chechens and the Ingush. Unlike the other nationalities that could be suspected of connection to foreign states which shared their ethnic background, the Chechens and the Ingush were completely indigenous people of the Soviet Union. Rather than being accused of collaboration with foreign enemies, these two ethnic groups were considered to have cultures which did not fit in with Soviet culture – such as accusing Chechens of being associated with “banditism” – and the authorities claimed that the Soviet Union had to intervene in order to “remake” and “reform” these cultures. In practice this meant heavily armed punitive operations carried out against Chechen “bandits” that failed to achieve forced assimilation, culminating in an ethnic cleansing operation in 1944, which involved the arrests and deportation of over 500,000 Chechens and Ingush from the Caucasus to Central Asia and Kazakhstan. The deportations of the Chechens and Ingush also involved the outright massacre of thousands of people, and severe conditions placed upon the deportees – they were put in unsealed train cars, with little to no food for a four-week journey during which many died from hunger and exhaustion.\n\nThe main difference between Nazi and Stalinist deportations was in their purpose: while Nazi Germany sought ethnic cleansing to allow settlement by Germans into the cleansed territory, Stalin's Soviet Union pursued ethnic cleansing in order to remove minorities from strategically important areas.\n\nOther historians and political scientists have also made comparisons between Nazism and Stalinism as part of their work.\n\nStanley Payne, in his work on fascism, said that although the Nazi Party was ideologically opposed to communism, Adolf Hitler and other Nazi leaders frequently expressed recognition that only in Soviet Russia were their revolutionary and ideological counterparts to be found. Both placed a major emphasis on creating a \"party-army,\" with the regular armed forces controlled by the party. In the case of the Soviet Union this was done through the political commissars, while Nazi Germany introduced a roughly equivalent leadership role for \"National Socialist Guidance Officers\" in 1943.\n\nFrançois Furet, in his work on communism, noted that Hitler personally admired Soviet leader Joseph Stalin, and on numerous occasions publicly praised Stalin for seeking to purify the Communist Party of the Soviet Union of Jewish influences, especially by purging Jewish communists such as Leon Trotsky, Grigory Zinoviev, Lev Kamenev and Karl Radek.\n\nRichard Pipes draws attention to Stalin and his antisemitism in a parallel with Nazi antisemitism. He notes that soon after the 1917 October Revolution, the Soviet Union undertook practices to break up Jewish culture, religion and language. In the fall of 1918, the Soviet Communist Party set up the Jewish section Yevsektsiya, with a stated mission of “destruction of traditional Jewish life, the Zionist movement, and Hebrew culture.” By 1919, the Bolsheviks began to confiscate Jewish properties, Hebrew schools, libraries, books, and synagogues in accordance with newly imposed anti-religious laws, turning their buildings into \"Communist centers, clubs or restaurants.\" After Joseph Stalin rose to power, antisemitism continued to be endemic throughout Russia, although official Soviet policy condemned it. On August 12, 1952, Stalin's personal antisemitism became more visible, as he ordered the execution of the most prominent Yiddish authors in the Soviet Union, in an event known as the \"Night of the Murdered Poets\". Shortly before his death, Stalin also organized the anti-Semitic campaign known as the Doctors' plot.\n\nA number of research institutions are focusing on the analysis of fascism/Nazism and Stalinism/communism, and the comparative approach, including the Hannah Arendt Institute for the Research on Totalitarianism in Germany, the Institute for the Study of Totalitarian Regimes in the Czech Republic and the Institute of National Remembrance in Poland.\n\nIn comparing the deaths caused by both Stalin and Hitler's policies, some historians have asserted that archival evidence released after the collapse of the USSR confirms that Stalin did not kill more people than Hitler. American historian Timothy D. Snyder, for example, after assessing such data, says that while the Nazi regime killed approximately 11 million non-combatants (which rises to above 12 million if \"foreseeable deaths from deportation, hunger, and sentences in concentration camps are included\"), Stalin's deliberately killed about 6 million (rising to 9 million if foreseeable deaths arising from policies are taken into account). Australian historian and archival researcher Stephen G. Wheatcroft posits that \"The Stalinist regime was consequently responsible for about a million purposive killings, and through its criminal neglect and irresponsibility it was probably responsible for the premature deaths of about another two million more victims amongst the repressed population, i.e. in the camps, colonies, prisons, exile, in transit and in the POW camps for Germans. These are clearly much lower figures than those for whom Hitler's regime was responsible.\" Wheatcroft also says that, unlike Hitler, Stalin's \"purposive killings\" fit more closely into the category of \"execution\" than \"murder\", given he thought the accused were indeed guilty of crimes against the state and insisted on documentation, whereas Hitler simply wanted to kill Jews and communists because of who they were, and insisted on no documentation and was indifferent at even a pretence of legality for these actions.\n\nKristen R. Ghodsee, an ethnographer of post-Cold War Eastern Europe, contends that the efforts to institutionalize the \"double genocide thesis\", or the moral equivalence between the Nazi Holocaust (race murder) and the victims of communism (class murder), and in particular the recent push at the beginning of the global financial crisis for commemoration of the latter in Europe, can be seen as the response by economic and political elites to fears of a leftist resurgence in the face of devastated economies and extreme inequalities in both the East and West as the result of neoliberal capitalism. She notes that any discussion of the achievements under communism, including literacy, education, women’s rights, and social security is usually silenced, and any discourse on the subject of communism is focused almost exclusively on Stalin's crimes and the \"double genocide thesis\", an intellectual paradigm summed up as such: \"1) any move towards redistribution and away from a completely free market is seen as communist; 2) anything communist inevitably leads to class murder; and 3) class murder is the moral equivalent of the Holocaust.\" By linking all leftist and socialist ideals to the excesses of Stalinism, Ghodsee concludes, the elites in the West hope to discredit and marginalize all political ideologies that could \"threaten the primacy of private property and free markets.\"\n\nThe comparison of Stalinism and Nazism remains a neglected field of academic study.\n\nThe comparison of Nazism and Stalinism has long provoked political controversy, and it led to the historians' dispute within Germany in the 1980s.\n\nIn the 1920s, the Social Democratic Party of Germany (SPD), under the leadership of Chancellor Hermann Müller, adopted the view that \"red equals brown\", i.e. that the communists and Nazis posed an equal danger to liberal democracy. In 1930, Kurt Schumacher said that the two movements enabled each other. He argued that the Communist Party of Germany, which was staunchly Stalinist, were \"red-painted Nazis.\" This comparison was mirrored by the social fascism theory advanced by the Soviet government and the Comintern (including the Communist Party of Germany), which accused social democracy of enabling fascism and went as far as to call social democrats \"social fascists.\" After the 1939 Molotov–Ribbentrop Pact was announced, \"The New York Times\" published an editorial arguing that \"Hitlerism is brown communism, Stalinism is red fascism.\"\n\nMarxist theories of fascism have seen fascism as a form of reaction to socialism and a feature of capitalism. Several modern historians have tried to pay more attention to the economic, political and ideological differences between these two regimes than to their similarities. \n\nThe 2008 Prague Declaration on European Conscience and Communism, initiated by the Czech government and signed by figures such as Václav Havel, called for \"a common approach regarding crimes of totalitarian regimes, inter alia Communist regimes\" and for\nThe Communist Party of Greece opposes the Prague Declaration and has criticized \"the new escalation of the anti-communist hysteria led by the EU council, the European Commission and the political staff of the bourgeois class in the European Parliament.\" The Communist Party of Britain opined that the Prague Declaration \"is a rehash of the persistent attempts by reactionary historians to equate Soviet Communism and Hitlerite Fascism, echoing the old slanders of British authors George Orwell and Robert Conquest.\"\n\nThe 2008 documentary film \"The Soviet Story\", commissioned by the Union for Europe of the Nations group in the European Parliament, published archival records which listed thousands of German Jews who were arrested in the Soviet Union by the NKVD (People's Commissariat for Internal Affairs) from 1937 to 1941 and handed over to Gestapo or SS officials in Germany. These German Jews had originally sought asylum in the USSR. The documentary film accuses Stalin's regime of being an accomplice in Hitler's Holocaust by arresting these asylum seekers and sending them back to Germany.\n\nSince 2009, the European Union has officially commemorated the European Day of Remembrance for Victims of Stalinism and Nazism, proclaimed by the European Parliament in 2008 and endorsed by the Organization for Security and Co-operation in Europe in 2009, and officially known as the Black Ribbon Day in some countries (including Canada).\n\nThe former President of the European Parliament and Christian Democratic Union member, Hans-Gert Pöttering, argued that \"both totalitarian systems (Stalinism and Nazism) are comparable and terrible.\"\n\nIn some Eastern European countries the denial of both Nazi and Communist crimes has been explicitly outlawed, and Czech foreign minister Karel Schwarzenberg has argued that \"there is a fundamental concern here that totalitarian systems be measured by the same standard.\" However, the European Commission rejected calls for similar EU-wide legislation, due to the lack of consensus among member states.\n\nA statement adopted by Russia's legislature said that comparisons of Nazism and Stalinism are \"blasphemous towards all of the anti-fascist movement veterans, Holocaust victims, concentration camp prisoners and tens of millions of people ... who sacrificed their lives for the sake of the fight against the Nazis' anti-human racial theory.\"\n\nBritish journalist Seumas Milne posits that the impact of the post-Cold War narrative that Stalin and Hitler were twin evils, and therefore Communism is as monstrous as Nazism, \"has been to relativise the unique crimes of Nazism, bury those of colonialism and feed the idea that any attempt at radical social change will always lead to suffering, killing and failure.\"\n\n\n"}
{"id": "1996367", "url": "https://en.wikipedia.org/wiki?curid=1996367", "title": "Comparison of web template engines", "text": "Comparison of web template engines\n\nThe following table lists the various Web Template Engines used in Web template systems and a brief rundown of their features.\n\n\n"}
{"id": "1338683", "url": "https://en.wikipedia.org/wiki?curid=1338683", "title": "Corecursion", "text": "Corecursion\n\nIn computer science, corecursion is a type of operation that is dual to recursion. Whereas recursion works analytically, starting on data further from a base case and breaking it down into smaller data and repeating until one reaches a base case, corecursion works synthetically, starting from a base case and building it up, iteratively producing data further removed from a base case. Put simply, corecursive algorithms use the data that they themselves produce, bit by bit, as they become available, and needed, to produce further bits of data. A similar but distinct concept is \"generative recursion\" which may lack a definite \"direction\" inherent in corecursion and recursion.\n\nWhere recursion allows programs to operate on arbitrarily complex data, so long as they can be reduced to simple data (base cases), corecursion allows programs to produce arbitrarily complex and potentially infinite data structures, such as streams, so long as it can be produced from simple data (base cases) in a sequence of \"finite\" steps. Where recursion may not terminate, never reaching a base state, corecursion starts from a base state, and thus produces subsequent steps deterministically, though it may proceed indefinitely (and thus not terminate under strict evaluation), or it may consume more than it produces and thus become non-\"productive\". Many functions that are traditionally analyzed as recursive can alternatively, and arguably more naturally, be interpreted as corecursive functions that are terminated at a given stage, for example recurrence relations such as the factorial.\n\nCorecursion can produce both finite and infinite data structures as results, and may employ self-referential data structures. Corecursion is often used in conjunction with lazy evaluation, to produce only a finite subset of a potentially infinite structure (rather than trying to produce an entire infinite structure at once). Corecursion is a particularly important concept in functional programming, where corecursion and codata allow total languages to work with infinite data structures.\n\nCorecursion can be understood by contrast with recursion, which is more familiar. While corecursion is primarily of interest in functional programming, it can be illustrated using imperative programming, which is done below using the generator facility in Python. In these examples local variables are used, and assigned values imperatively (destructively), though these are not necessary in corecursion in pure functional programming. In pure functional programming, rather than assigning to local variables, these computed values form an invariable sequence, and prior values are accessed by self-reference (later values in the sequence reference earlier values in the sequence to be computed). The assignments simply express this in the imperative paradigm and explicitly specify where the computations happen, which serves to clarify the exposition.\n\nA classic example of recursion is computing the factorial, which is defined recursively by \"0! := 1\" and \"n! := n × (n - 1)!\".\n\nTo \"recursively\" compute its result on a given input, a recursive function calls (a copy of) \"itself\" with a different (\"smaller\" in some way) input and uses the result of this call to construct its result. The recursive call does the same, unless the \"base case\" has been reached. Thus a call stack develops in the process. For example, to compute \"fac(3)\", this recursively calls in turn \"fac(2)\", \"fac(1)\", \"fac(0)\" (\"winding up\" the stack), at which point recursion terminates with \"fac(0) = 1\", and then the stack unwinds in reverse order and the results are calculated on the way back along the call stack to the initial call frame \"fac(3)\" that uses the result of \"fac(2) = 2\" to calculate the final result as \"3 × 2 = 3 × fac(2) =: fac(3)\" and finally return \"fac(3) = 6\". In this example a function returns a single value.\n\nThis stack unwinding can be explicated, defining the factorial \"corecursively\", as an iterator, where one \"starts\" with the case of formula_1, then from this starting value constructs factorial values for increasing numbers \"1, 2, 3...\" as in the above recursive definition with \"time arrow\" reversed, as it were, by reading it \"backwards\" as The corecursive algorithm thus defined produces a \"stream\" of \"all\" factorials. This may be concretely implemented as a generator. Symbolically, noting that computing next factorial value requires keeping track of both \"n\" and \"f\" (a previous factorial value), this can be represented as:\nor in Haskell, \n\nmeaning, \"starting from formula_3, on each step the next values are calculated as formula_4\". This is mathematically equivalent and almost identical to the recursive definition, but the formula_5 emphasizes that the factorial values are being built \"up\", going forwards from the starting case, rather than being computed after first going backwards, \"down\" to the base case, with a formula_6 decrement. Note also that the direct output of the corecursive function does not simply contain the factorial formula_7 values, but also includes for each value the auxiliary data of its index \"n\" in the sequence, so that any one specific result can be selected among them all, as and when needed.\n\nNote the connection with denotational semantics, where the denotations of recursive programs is built up corecursively in this way.\n\nIn Python, a recursive factorial function can be defined as:\n\nThis could then be called for example as codice_1 to compute \"5!\".\n\nA corresponding corecursive generator can be defined as:\n\nThis generates an infinite stream of factorials in order; a finite portion of it can be produced by:\n\nThis could then be called to produce the factorials up to \"5!\" via:\n\nIf we're only interested in a certain factorial, just the last value can be taken, or we can fuse the production and the access into one function,\n\nAs can be readily seen here, this is practically equivalent (just by substituting codice_2 for the only codice_3 there) to the accumulator argument technique for tail recursion, unwound into an explicit loop. Thus it can be said that the concept of corecursion is an explication of the embodiment of iterative computation processes by recursive definitions, where applicable.\n\nIn the same way, the Fibonacci sequence can be represented as:\nNote that because the Fibonacci sequence is a recurrence relation of order 2, the corecursive relation must track two successive terms, with the formula_9 corresponding to shift forward by one step, and the formula_10 corresponding to computing the next term. This can then be implemented as follows (using parallel assignment):\n\nIn Haskell, \n\nTree traversal via a depth-first approach is a classic example of recursion. Dually, breadth-first traversal can very naturally be implemented via corecursion.\n\nWithout using recursion or corecursion specifically, one may traverse a tree by starting at the root node, placing its child nodes in a data structure, then iterating by removing node after node from the data structure while placing each removed node's children back into that data structure. If the data structure is a stack (LIFO), this yields depth-first traversal, and if the data structure is a queue (FIFO), this yields breadth-first traversal.\n\nUsing recursion, a (post-order) depth-first traversal can be implemented by starting at the root node and recursively traversing each child subtree in turn (the subtree based at each child node) – the second child subtree does not start processing until the first child subtree is finished. Once a leaf node is reached or the children of a branch node have been exhausted, the node itself is visited (e.g., the value of the node itself is outputted). In this case, the call stack (of the recursive functions) acts as the stack that is iterated over.\n\nUsing corecursion, a breadth-first traversal can be implemented by starting at the root node, outputting its value, then breadth-first traversing the subtrees – i.e., passing on the \"whole list\" of subtrees to the next step (not a single subtree, as in the recursive approach) – at the next step outputting the value of all of their root nodes, then passing on their child subtrees, etc. In this case the generator function, indeed the output sequence itself, acts as the queue. As in the factorial example (above), where the auxiliary information of the index (which step one was at, \"n\") was pushed forward, in addition to the actual output of \"n\"!, in this case the auxiliary information of the remaining subtrees is pushed forward, in addition to the actual output. Symbolically:\nmeaning that at each step, one outputs the list of values of root nodes, then proceeds to the child subtrees. Generating just the node values from this sequence simply requires discarding the auxiliary child tree data, then flattening the list of lists (values are initially grouped by level (depth); flattening (ungrouping) yields a flat linear list). In Haskell, \nThese can be compared as follows. The recursive traversal handles a \"leaf node\" (at the \"bottom\") as the base case (when there are no children, just output the value), and \"analyzes\" a tree into subtrees, traversing each in turn, eventually resulting in just leaf nodes – actual leaf nodes, and branch nodes whose children have already been dealt with (cut off \"below\"). By contrast, the corecursive traversal handles a \"root node\" (at the \"top\") as the base case (given a node, first output the value), treats a tree as being \"synthesized\" of a root node and its children, then produces as auxiliary output a list of subtrees at each step, which are then the input for the next step – the child nodes of the original root are the root nodes at the next step, as their parents have already been dealt with (cut off \"above\"). Note also that in the recursive traversal there is a distinction between leaf nodes and branch nodes, while in the corecursive traversal there is no distinction, as each node is treated as the root node of the subtree it defines.\n\nNotably, given an infinite tree, the corecursive breadth-first traversal will traverse all nodes, just as for a finite tree, while the recursive depth-first traversal will go down one branch and not traverse all nodes, and indeed if traversing post-order, as in this example (or in-order), it will visit no nodes at all, because it never reaches a leaf. This shows the usefulness of corecursion rather than recursion for dealing with infinite data structures.\n\nIn Python, this can be implemented as follows.\nThe usual post-order depth-first traversal can be defined as:\n\nThis can then be called by codice_4 to print the values of the nodes of the tree in post-order depth-first order.\n\nThe breadth-first corecursive generator can be defined as:\n\nThis can then be called to print the values of the nodes of the tree in breadth-first order:\n\nInitial data types can be defined as being the least fixpoint (up to isomorphism) of some type equation; the isomorphism is then given by an initial algebra. Dually, final (or terminal) data types can be defined as being the greatest fixpoint of a type equation; the isomorphism is then given by a final coalgebra.\n\nIf the domain of discourse is the category of sets and total functions, then final data types may contain infinite, non-wellfounded values, whereas initial types do not. On the other hand, if the domain of discourse is the category of complete partial orders and continuous functions, which corresponds roughly to the Haskell programming language, then final types coincide with initial types, and the corresponding final coalgebra and initial algebra form an isomorphism.\n\nCorecursion is then a technique for recursively defining functions whose range (codomain) is a final data type, dual to the way that ordinary recursion recursively defines functions whose domain is an initial data type.\n\nThe discussion below provides several examples in Haskell that distinguish corecursion. Roughly speaking, if one were to port these definitions to the category of sets, they would still be corecursive. This informal usage is consistent with existing textbooks about Haskell. Also note that the examples used in this article predate the attempts to define corecursion and explain what it is.\n\nThe rule for \"primitive corecursion\" on codata is the dual to that for primitive recursion on data. Instead of descending on the argument by pattern-matching on its constructors (that \"were called up before\", somewhere, so we receive a ready-made datum and get at its constituent sub-parts, i.e. \"fields\"), we ascend on the result by filling-in its \"destructors\" (or \"observers\", that \"will be called afterwards\", somewhere - so we're actually calling a constructor, creating another bit of the result to be observed later on). Thus corecursion \"creates\" (potentially infinite) codata, whereas ordinary recursion \"analyses\" (necessarily finite) data. Ordinary recursion might not be applicable to the codata because it might not terminate. Conversely, corecursion is not strictly necessary if the result type is data, because data must be finite.\n\nIn \"Programming with streams in Coq: a case study: the Sieve of Eratosthenes\" we find\n\nwhere primes \"are obtained by applying the primes operation to the stream (Enu 2)\". Following the above notation, the sequence of primes (with a throwaway 0 prefixed to it) and numbers streams being progressively sieved, can be represented as \nor in Haskell, \n\nThe authors discuss how the definition of codice_5 is not guaranteed always to be \"productive\", and could become stuck e.g. if called with codice_6 as the initial stream.\n\nHere is another example in Haskell. The following definition produces the list of Fibonacci numbers in linear time:\nThis infinite list depends on lazy evaluation; elements are computed on an as-needed basis, and only finite prefixes are ever explicitly represented in memory. This feature allows algorithms on parts of codata to terminate; such techniques are an important part of Haskell programming.\n\nThis can be done in Python as well:\nThe definition of codice_7 can be inlined, leading to this:\n\nThis example employs a self-referential \"data structure\". Ordinary recursion makes use of self-referential \"functions\", but does not accommodate self-referential data. However, this is not essential to the Fibonacci example. It can be rewritten as follows:\n\nThis employs only self-referential \"function\" to construct the result. If it were used with strict list constructor it would be an example of runaway recursion, but with non-strict list constructor this guarded recursion gradually produces an indefinitely defined list.\n\nCorecursion need not produce an infinite object; a corecursive queue is a particularly good example of this phenomenon. The following definition produces a breadth-first traversal of a binary tree in linear time:\n\nThis definition takes an initial tree and produces a list of subtrees. This list serves dual purpose as both the queue and the result ( produces its output notches after its input back-pointer, , along the ). It is finite if and only if the initial tree is finite. The length of the queue must be explicitly tracked in order to ensure termination; this can safely be elided if this definition is applied only to infinite trees. \n\nAnother particularly good example gives a solution to the problem of breadth-first labeling. The function codice_8 visits every node in a binary tree in a breadth first fashion, and replaces each label with an integer, each subsequent integer is bigger than the last by one. This solution employs a self-referential data structure, and the binary tree can be finite or infinite.\n\nAn apomorphism (such as an anamorphism, such as unfold) is a form of corecursion in the same way that a paramorphism (such as a catamorphism, such as fold) is a form of recursion.\n\nThe Coq proof assistant supports corecursion and coinduction using the CoFixpoint command.\n\nCorecursion, referred to as \"circular programming,\" dates at least to , who credits John Hughes and Philip Wadler; more general forms were developed in . The original motivations included producing more efficient algorithms (allowing 1 pass over data in some cases, instead of requiring multiple passes) and implementing classical data structures, such as doubly linked lists and queues, in functional languages.\n\n\n"}
{"id": "1833848", "url": "https://en.wikipedia.org/wiki?curid=1833848", "title": "Cross-reference", "text": "Cross-reference\n\nThe term cross-reference can refer to either:\n\nIn a document, especially those authored in a Content management system,\na cross-reference has two major aspects:\n\nThe visible form contains text, graphics, and other indications that:\n\nThe technical mechanism that resides within the system:\n\nIf the cross reference mechanism is well designed, the reader will be able to follow each cross reference to the referenced content whether the content is presented in print or electronically.\n\nAn author working in a content management system is responsible for identifying subjects of interest that cross documents, and creating appropriate systems of cross references to support readers who seek to understand those subjects. For an individual cross reference, an author should ensure that location and content of the target of the cross reference are clearly identified, and the reader can easily determine how to follow the cross reference in each medium in which publication is supported.\n\nContent strategy practitioners (known as content strategists) specialize in planning content to meet business needs, taking into account the processes for creating and maintaining the content, and the systems that support the content.\n\n"}
{"id": "7931", "url": "https://en.wikipedia.org/wiki?curid=7931", "title": "Dictionary", "text": "Dictionary\n\nA dictionary, sometimes known as a wordbook, is a collection of words in one or more specific languages, often arranged alphabetically (or by radical and stroke for ideographic languages), which may include information on definitions, usage, etymologies, pronunciations, translation, etc. or a book of words in one language with their equivalents in another, sometimes known as a lexicon. It is a lexicographical reference that shows inter-relationships among the data.\n\nA broad distinction is made between general and specialized dictionaries. Specialized dictionaries include words in specialist fields, rather than a complete range of words in the language. Lexical items that describe concepts in specific fields are usually called terms instead of words, although there is no consensus whether lexicology and terminology are two different fields of study. In theory, general dictionaries are supposed to be semasiological, mapping word to definition, while specialized dictionaries are supposed to be onomasiological, first identifying concepts and then establishing the terms used to designate them. In practice, the two approaches are used for both types. There are other types of dictionaries that do not fit neatly into the above distinction, for instance bilingual (translation) dictionaries, dictionaries of synonyms (thesauri), and rhyming dictionaries. The word dictionary (unqualified) is usually understood to refer to a general purpose monolingual dictionary.\n\nThere is also a contrast between \"prescriptive\" or \"descriptive\" dictionaries; the former reflect what is seen as correct use of the language while the latter reflect recorded actual use. Stylistic indications (e.g. \"informal\" or \"vulgar\") in many modern dictionaries are also considered by some to be less than objectively descriptive.\n\nAlthough the first recorded dictionaries date back to Sumerian times (these were bilingual dictionaries), the systematic study of dictionaries as objects of scientific interest themselves is a 20th-century enterprise, called lexicography, and largely initiated by Ladislav Zgusta. The birth of the new discipline was not without controversy, the practical dictionary-makers being sometimes accused by others of \"astonishing\" lack of method and critical-self reflection.\n\nThe oldest known dictionaries were Akkadian Empire cuneiform tablets with bilingual Sumerian–Akkadian wordlists, discovered in Ebla (modern Syria) and dated roughly 2300 BCE. The early 2nd millennium BCE \"Urra=hubullu\" glossary is the canonical Babylonian version of such bilingual Sumerian wordlists. A Chinese dictionary, the c. 3rd century BCE \"Erya\", was the earliest surviving monolingual dictionary; although some sources cite the c. 800 BCE Shizhoupian as a \"dictionary\", modern scholarship considers it a calligraphic compendium of Chinese characters from Zhou dynasty bronzes. Philitas of Cos (fl. 4th century BCE) wrote a pioneering vocabulary \"Disorderly Words\" (Ἄτακτοι γλῶσσαι, \"\") which explained the meanings of rare Homeric and other literary words, words from local dialects, and technical terms. Apollonius the Sophist (fl. 1st century CE) wrote the oldest surviving Homeric lexicon. The first Sanskrit dictionary, the Amarakośa, was written by Amara Sinha c. 4th century CE. Written in verse, it listed around 10,000 words. According to the \"Nihon Shoki\", the first Japanese dictionary was the long-lost 682 CE \"Niina\" glossary of Chinese characters. The oldest existing Japanese dictionary, the c. 835 CE \"Tenrei Banshō Meigi\", was also a glossary of written Chinese. In \"Frahang-i Pahlavig\", Aramaic heterograms are listed together with their translation in Middle Persian language and phonetic transcription in Pazand alphabet. A 9th-century CE Irish dictionary, Sanas Cormaic, contained etymologies and explanations of over 1,400 Irish words. In India around 1320, Amir Khusro compiled the Khaliq-e-bari which mainly dealt with Hindustani and Persian words.\nArabic dictionaries were compiled between the 8th and 14th centuries CE, organizing words in rhyme order (by the last syllable), by alphabetical order of the radicals, or according to the alphabetical order of the first letter (the system used in modern European language dictionaries). The modern system was mainly used in specialist dictionaries, such as those of terms from the Qur'an and hadith, while most general use dictionaries, such as the \"Lisan al-`Arab\" (13th century, still the best-known large-scale dictionary of Arabic) and \"al-Qamus al-Muhit\" (14th century) listed words in the alphabetical order of the radicals. The \"Qamus al-Muhit\" is the first handy dictionary in Arabic, which includes only words and their definitions, eliminating the supporting examples used in such dictionaries as the \"Lisan\" and the \"Oxford English Dictionary\".\nIn medieval Europe, glossaries with equivalents for Latin words in vernacular or simpler Latin were in use (e.g. the Leiden Glossary). The \"Catholicon\" (1287) by Johannes Balbus, a large grammatical work with an alphabetical lexicon, was widely adopted. It served as the basis for several bilingual dictionaries and was one of the earliest books (in 1460) to be printed. In 1502 Ambrogio Calepino's \"Dictionarium\" was published, originally a monolingual Latin dictionary, which over the course of the 16th century was enlarged to become a multilingual glossary. In 1532 Robert Estienne published the \"Thesaurus linguae latinae\" and in 1572 his son Henri Estienne published the \"Thesaurus linguae graecae\", which served up to the 19th century as the basis of Greek lexicography. The first monolingual dictionary written in Europe was the Spanish, written by Sebastián Covarrubias' \"Tesoro de la lengua castellana o española\", published in 1611 in Madrid, Spain. In 1612 the first edition of the \"Vocabolario degli Accademici della Crusca\", for Italian, was published. It served as the model for similar works in French and English. In 1690 in Rotterdam was published, posthumously, the \"Dictionnaire Universel\" by Antoine Furetière for French. In 1694 appeared the first edition of the \"Dictionnaire de l'Académie française\". Between 1712 and 1721 was published the \"Vocabulario portughez e latino\" written by Raphael Bluteau. The Real Academia Española published the first edition of the \"Diccionario de la lengua española\" in 1780, but their \"Diccionario de Autoridades\", which included quotes taken from literary works, was published in 1726. The \"Totius Latinitatis lexicon\" by Egidio Forcellini was firstly published in 1777; it has formed the basis of all similar works that have since been published.\n\nThe first edition of \"A Greek-English Lexicon\" by Henry George Liddell and Robert Scott appeared in 1843; this work remained the basic dictionary of Greek until the end of the 20th century. And in 1858 was published the first volume of the Deutsches Wörterbuch by the Brothers Grimm; the work was completed in 1961. Between 1861 and 1874 was published the \"Dizionario della lingua italiana\" by Niccolò Tommaseo. Between 1862 and 1874 was published the six volumes of \"A magyar nyelv szótára\" (Dictionary of Hungarian Language) by Gergely Czuczor and János Fogarasi. Émile Littré published the Dictionnaire de la langue française between 1863 and 1872. In the same year 1863 appeared the first volume of the \"Woordenboek der Nederlandsche Taal\" which was completed in 1998. Also in 1863 Vladimir Ivanovich Dahl published the \"Explanatory Dictionary of the Living Great Russian Language\". The Duden dictionary dates back to 1880, and is currently the prescriptive source for the spelling of German. The decision to start work on the \"Svenska Akademiens ordbok\" was taken in 1787.\n\nThe earliest dictionaries in the English language were glossaries of French, Spanish or Latin words along with their definitions in English. The word \"dictionary\" was invented by an Englishman called John of Garland in 1220 — he had written a book \"Dictionarius\" to help with Latin \"diction\". An early non-alphabetical list of 8000 English words was the \"Elementarie\", created by Richard Mulcaster in 1582.\n\nThe first purely English alphabetical dictionary was \"A Table Alphabeticall\", written by English schoolteacher Robert Cawdrey in 1604. The only surviving copy is found at the Bodleian Library in Oxford. This dictionary, and the many imitators which followed it, was seen as unreliable and nowhere near definitive. Philip Stanhope, 4th Earl of Chesterfield was still lamenting in 1754, 150 years after Cawdrey's publication, that it is \"a sort of disgrace to our nation, that hitherto we have had no… standard of our language; our dictionaries at present being more properly what our neighbors the Dutch and the Germans call theirs, word-books, than dictionaries in the superior sense of that title.\" \n\nIn 1616, John Bullokar described the history of the dictionary with his \"English Expositor\". \"Glossographia\" by Thomas Blount, published in 1656, contains more than 10,000 words along with their etymologies or histories. Edward Phillips wrote another dictionary in 1658, entitled \"The New World of English Words: Or a General Dictionary\" which boldly plagiarized Blount's work, and the two denounced each other. This created more interest in the dictionaries. John Wilkins' 1668 essay on philosophical language contains a list of 11,500 words with careful distinctions, compiled by William Lloyd. Elisha Coles published his \"English Dictionary\" in 1676.\n\nIt was not until Samuel Johnson's \"A Dictionary of the English Language\" (1755) that a more reliable English dictionary was produced. Many people today mistakenly believe that Johnson wrote the first English dictionary: a testimony to this legacy. By this stage, dictionaries had evolved to contain textual references for most words, and were arranged alphabetically, rather than by topic (a previously popular form of arrangement, which meant all animals would be grouped together, etc.). Johnson's masterwork could be judged as the first to bring all these elements together, creating the first \"modern\" dictionary.\n\nJohnson's dictionary remained the English-language standard for over 150 years, until the Oxford University Press began writing and releasing the \"Oxford English Dictionary\" in short fascicles from 1884 onwards. It took nearly 50 years to complete this huge work, and they finally released the complete \"OED\" in twelve volumes in 1928. It remains the most comprehensive and trusted English language dictionary to this day, with revisions and updates added by a dedicated team every three months. One of the main contributors to this modern dictionary was an ex-army surgeon, William Chester Minor, a convicted murderer who was confined to an asylum for the criminally insane.\n\nIn 1806, American Noah Webster published his first dictionary, \"\". In 1807 Webster began compiling an expanded and fully comprehensive dictionary, \"An American Dictionary of the English Language;\" it took twenty-seven years to complete. To evaluate the etymology of words, Webster learned twenty-six languages, including Old English (Anglo-Saxon), German, Greek, Latin, Italian, Spanish, French, Hebrew, Arabic, and Sanskrit.\n\nWebster completed his dictionary during his year abroad in 1825 in Paris, France, and at the University of Cambridge. His book contained seventy thousand words, of which twelve thousand had never appeared in a published dictionary before. As a spelling reformer, Webster believed that English spelling rules were unnecessarily complex, so his dictionary introduced American English spellings, replacing \"colour\" with \"color\", substituting \"wagon\" for \"waggon\", and printing \"center\" instead of \"centre\". He also added American words, like \"skunk\" and \"squash\", that did not appear in British dictionaries. At the age of seventy, Webster published his dictionary in 1828; it sold 2500 copies. In 1840, the second edition was published in two volumes.\n\nIn a general dictionary, each word may have multiple meanings. Some dictionaries include each separate meaning in the order of most common usage while others list definitions in historical order, with the oldest usage first.\n\nIn many languages, words can appear in many different forms, but only the undeclined or unconjugated form appears as the headword in most dictionaries. Dictionaries are most commonly found in the form of a book, but some newer dictionaries, like StarDict and the \"New Oxford American Dictionary\" are dictionary software running on PDAs or computers. There are also many online dictionaries accessible via the Internet.\n\nAccording to the \"Manual of Specialized Lexicographies\", a specialized dictionary, also referred to as a technical dictionary, is a dictionary that focuses upon a specific subject field. Following the description in \"The Bilingual LSP Dictionary\", lexicographers categorize specialized dictionaries into three types: A multi-field dictionary broadly covers several subject fields (e.g. a business dictionary), a single-field dictionary narrowly covers one particular subject field (e.g. law), and a sub-field dictionary covers a more specialized field (e.g. constitutional law). For example, the 23-language Inter-Active Terminology for Europe is a multi-field dictionary, the American National Biography is a single-field, and the African American National Biography Project is a sub-field dictionary. In terms of the coverage distinction between \"minimizing dictionaries\" and \"maximizing dictionaries\", multi-field dictionaries tend to minimize coverage across subject fields (for instance, \"Oxford Dictionary of World Religions\" and \"Yadgar Dictionary of Computer and Internet Terms\") whereas single-field and sub-field dictionaries tend to maximize coverage within a limited subject field (\"The Oxford Dictionary of English Etymology\").\n\nAnother variant is the glossary, an alphabetical list of defined terms in a specialized field, such as medicine (medical dictionary).\n\nThe simplest dictionary, a defining dictionary, provides a core glossary of the simplest meanings of the simplest concepts. From these, other concepts can be explained and defined, in particular for those who are first learning a language. In English, the commercial defining dictionaries typically include only one or two meanings of under 2000 words. With these, the rest of English, and even the 4000 most common English idioms and metaphors, can be defined.\n\nLexicographers apply two basic philosophies to the defining of words: \"prescriptive\" or \"descriptive\". Noah Webster, intent on forging a distinct identity for the American language, altered spellings and accentuated differences in meaning and pronunciation of some words. This is why American English now uses the spelling \"color\" while the rest of the English-speaking world prefers \"colour\". (Similarly, British English subsequently underwent a few spelling changes that did not affect American English; see further at American and British English spelling differences.)\n\nLarge 20th-century dictionaries such as the \"Oxford English Dictionary\" (OED) and \"Webster's Third\" are descriptive, and attempt to describe the actual use of words. Most dictionaries of English now apply the descriptive method to a word's definition, and then, outside of the definition itself, and information alerting readers to attitudes which may influence their choices on words often considered vulgar, offensive, erroneous, or easily confused. \"Merriam-Webster\" is subtle, only adding italicized notations such as, \"sometimes offensive\" or \"stand\" (nonstandard). \"American Heritage\" goes further, discussing issues separately in numerous \"usage notes.\" \"Encarta\" provides similar notes, but is more prescriptive, offering warnings and admonitions against the use of certain words considered by many to be offensive or illiterate, such as, \"an offensive term for...\" or \"a taboo term meaning...\".\n\nBecause of the widespread use of dictionaries in schools, and their acceptance by many as language authorities, their treatment of the language does affect usage to some degree, with even the most descriptive dictionaries providing conservative continuity. In the long run, however, the meanings of words in English are primarily determined by usage, and the language is being changed and created every day. As Jorge Luis Borges says in the prologue to \"El otro, el mismo\": \"It is often forgotten that (dictionaries) are artificial repositories, put together well after the languages they define. The roots of language are irrational and of a magical nature.\"\n\nSometimes the same dictionary can be descriptive in some domains and prescriptive in others. For example, according to Ghil'ad Zuckermann, the \"Oxford English-Hebrew Dictionary\" is \"at war with itself\": whereas its coverage (lexical items) and glosses (definitions) are descriptive and colloquial, its vocalization is prescriptive. This internal conflict results in absurd sentences such as \"hi taharóg otí kshetiré me asíti lamkhonít\" (she'll tear me apart when she sees what I've done to the car). Whereas \"hi taharóg otí\", literally 'she will kill me', is colloquial, me (a variant of ma 'what') is archaic, resulting in a combination that is unutterable in real life.\n\nA historical dictionary is a specific kind of descriptive dictionary which describes the development of words and senses over time, usually using citations to original source material to support its conclusions.\n\nIn contrast to traditional dictionaries, which are designed to be used by human beings, dictionaries for natural language processing (NLP) are built to be used by computer programs. The final user is a human being but the direct user is a program. Such a dictionary does not need to be able to be printed on paper. The structure of the content is not linear, ordered entry by entry but has the form of a complex network (see Diathesis alternation). Because most of these dictionaries are used to control machine translations or cross-lingual information retrieval (CLIR) the content is usually multilingual and usually of huge size. In order to allow formalized exchange and merging of dictionaries, an ISO standard called Lexical Markup Framework (LMF) has been defined and used among the industrial and academic community.\n\n\nIn many languages, such as the English language, the pronunciation of some words is not consistently apparent from their spelling. In these languages, dictionaries usually provide the pronunciation. For example, the definition for the word \"dictionary\" might be followed by the International Phonetic Alphabet spelling . American English dictionaries often use their ownpronunciation respelling systems with diacritics, for example \"dictionary\" is respelled as \"dĭk′shə-nĕr′ē\" in the American Heritage Dictionary. The IPA is more commonly used within the British Commonwealth countries. Yet others use their own pronunciation respelling systems without diacritics: for example, \"dictionary\" may be respelled as . Some online or electronic dictionaries provide audio recordings of words being spoken.\n\nHistories and descriptions of the dictionaries of other languages on Wikipedia include:\n\n\nThe age of the Internet brought online dictionaries to the desktop and, more recently, to the smart phone. David Skinner in 2013 noted that \"Among the top ten lookups on Merriam-Webster Online at this moment are 'holistic, pragmatic, caveat, esoteric' and 'bourgeois.' Teaching users about words they don’t already know has been, historically, an aim of lexicography, and modern dictionaries do this well.\"\nThere exist a number of websites which operate as online dictionaries, usually with a specialized focus. Some of them have exclusively user driven content, often consisting of neologisms. Some of the more notable examples include:\n\n\n\n"}
{"id": "1054566", "url": "https://en.wikipedia.org/wiki?curid=1054566", "title": "Ditloid", "text": "Ditloid\n\nA ditloid is a type of word puzzle, in which a phrase, quotation, date, or fact must be deduced from the numbers and abbreviated letters in the clue. Common words such as 'the', 'in', 'a', 'an', 'of', 'to', etc. are not normally abbreviated. The name 'ditloid' was given by the \"Daily Express\" newspaper, originating from the clue: 1 = DitLoID ≡ \"1 Day in the Life of Ivan Denisovich\".\n\nWill Shortz originated the current form of this puzzle and first published it in the May–June 1981 issue of \"Games\" magazine, calling it the Equation Analysis Test. In its annual 1981 issue of \"What's hot and what's not,\" \"Us\" magazine named the Equation Analysis Test in the \"what's hot\" category – the only nonperson so recognized. Shortz reports:\nSome anonymous person had retyped the puzzle from \"Games\" (word for word, except for my byline),\nphotocopied it, and passed it along. This page was then rephotocopied ad infinitum, like a chain letter,\nand circulated around the country. \"Games\" readers who hadn't seen the original even started sending\nit back to \"Games\" as something the magazine ought to consider publishing!\nShortz based the puzzle on the Formula Analysis Test - Revised Form published in Morgan Worthy's 1975 book \"AHA! A Puzzle Approach to Creative Thinking\" (Chicago: Nelson Hall). Worthy's equations were in a different format, for example:\n\nWorthy gives the source of his inspiration and speculates about the perennial popularity\nof this puzzle:\nI got the idea for linguistic equations from graffiti someone had\nwritten in the form of an obscene formula on a restroom wall at the\nUniversity of Florida. When the answer suddenly came to me, I realized\nthe format was a good one for eliciting the \"aha effect\". After that I\nused such items as exercise material when teaching workshops on\ncreative thinking.\nMy guess is that one reason a person enjoys linguistic equations is\nthat the answer hits him or her all at once rather than being solved in\nan incremental fashion. It is similar to what happens when we suddenly\nsee an embedded figure pop into focus; the satisfaction is visceral\nrather than just intellectual. My experience was that people often had\nthe answer to an item come to them when they were not consciously\nthinking about the puzzles, but relaxed, such as in the shower or about\nto fall asleep.\nAnother factor is that with well-written items, success does not hinge\non obscure information. Ideally, a person should never have to feel, \"I\ncould never have gotten that one no matter how long I worked on it.\"\nThere is something ego enhancing about knowing you have the answer\ninside and just need to find it.\n"}
{"id": "57442907", "url": "https://en.wikipedia.org/wiki?curid=57442907", "title": "Handbook of Middle American Indians", "text": "Handbook of Middle American Indians\n\nHandbook of Middle American Indians (HMAI) is a sixteen-volume compendium on Mesoamerica , from the prehispanic to the late twentieth century. Volumes on particular topics were published from the 1960s and 1970s under the general editorship of Robert Wauchope. Separate volumes with particular volume editors deal with a number of general topics, including archeology, cultural anthropology, physical anthropology, linguistics, with the last four substantive volumes treating various topics in Mesoamerican ethnohistory, under the editorship of Howard F. Cline. Select volumes have become available in e-book format.\n\nA retrospective review of the HMAI by two anthropologists discusses its history and evaluates it. One review calls it a fundamental work. Another reviewer says \"since the first volume of the HMAI appeared in 1964 is far and away the most comprehensive and erudite coverage of native cultures of any region in the Americas.\" A review in the journal \"Science\" says that \"There can be little doubt that, like the \"Handbook of South American Indians\", this monumental synthesis will provide a sound basis for new generalizations and will stimulate additional research to fill the gaps in knowledge and understanding that will become apparent.\n\nStarting in 1981, six volumes in the Supplement to the Handbook of Middle American Indians were published under the general editorship of Victoria Bricker.\n\nVolume 1. Natural Environment and Early Cultures, Robert C. West, volume editor. 1. Geohistory and Paleogeography of Middle America (Manuel Maldonado-Koerdell); 2. Surface Configuration and Associated Geology of Middle America (Robert C. West); 3. The Hydrography of Middle America (Jorge L. Tamayo, in collaboration with Robert C. West); 4. The American Mediterranean (Albert Collier); 5. Oceanography and Marine Life along the Pacific Coast (Carl L. Hubbs and Gunnar I. Roden); 6. Weather and Climate of Mexico and Central America (Jorge A. Vivo Escoto); 7. Natural Vegetation of Middle America (Philip L. Wagner); 8. The Soils of Middle America and their Relation to Indian Peoples and Cultures (Rayfred L. Stevens); 9. Fauna of Middle America (L. C. Stuart); 10. The Natural Regions of Middle America (Robert C. West); 11. The Primitive Hunters (Luis Aveleyra Arroyo de Anda); 12. The Food-gathering and Incipient Agriculture Stage of Prehistoric Middle America (Richard S. MacNeish); 13. Origins of Agriculture in Middle America (Paul C. Mangelsdorf, Richard S. MacNeish, and Gordon R. Willey); 14. The Patterns of Farming Life and Civilization (Gordon R. Willey, Gordon F. Ekholm, and Rene F. Millon)\n\nVolumes 2-3. Archeology of Southern Mesoamerica, Gordon R. Wiley, volume editor.\n\nVolume 4. ‘’Archeological Frontiers and External Connections G.F. Ekholm and G. R. Wiley, volume editors.\n\nVolume 5. ‘’Linguistics, Norman A. McQuown, volume editor.\n\nVolume 6. Social Anthropology, Manning Nash, volume editor. 1.Introduction, Manning Nash; 2. Indian Population and its Identification, Anselmo Marino Flores; 3.Agricultural Systems and Food Patterns, Angel Palerm; 4. Settlement Patterns, William T. Sanders; 5. Indian Economies, Manning Nash; 6. Contemporary Pottery and Basketry, George M. Foster; 7. Laquer, Katharine D. Jenkins; 8. Textiles and Costume, A.H. Gayton; 9. Drama, Dance and Music, Gertrude Prokosch Kurath; 10. Play: Games, Gossip, and Humor; 11. Kinship and Family, A. Kimball Romney; 12. Compadrinazgo, Robert Ravicz; 13. Local and Territoria Units, Eva Hunt and June Nash; 14. Political and Religious Organizations, Frank Cancian; 15. Levels of Communal Relations, Eric R. Wolf; 16. Annual Cycle and Fiesta Cycle, Ruben E. Reina; 17. Sickness and Social Relations, Richard N. Adams and Arthur J. Rubel; 18. Narrative Folklore, Munro S. Edmonson; 19. Religious Syncretism, William Madsen; 20. Ritual and Mythology, E. Michael Mendelson; 21. Psychological Orientations, Benjamin N. Colby; 22. Ethnic Relationships, Julio de la Fuente; 23. Acculturation, Ralph L. Beals; 24. Nationalization, Richard N. Adams; 25. Directed Change, Robert H. Ewald; 26. Urbanization and Industrialization, Arden R. King\n\nVolumes 7-8, Ethnology, Evan Z. Vogt, volume editor. Volume 7. Introduction (Evon Z. Vogt)Section I: The Maya 2; The Maya: Introduction (Evon Z. Vogt); 3. Guatemalan Highlands (Manning Nash); 4. The Maya of Northwestern Guatemala (Charles Wagley); 5. The Maya of the Midwestern Highlands (Sol Tax and Robert Hinshaw); 6. Eastern Guatemalan Highlands: The Pokomames and Chorti (Ruben E. Reina); 7. Chiapas Highlands (Evon Z. Vogt); 8. The Tzotzil (Robert M. Laughlin); 9. The Tzeltal (Alfonso Villa Rojas); 10. The Tojolabal (Roberta Montagu); 11. Maya Lowlands: The Chontal, Chol, and Kekchi (Alfonso Villa Rojas); 12. The Maya of Yucatan (Alfonso Villa Rojas); 13. The Lacandon (Gertrude Duby and Frans Blom); 14. The Huastec (Robert M. Laughlin); Section II: Southern Mexican Highlands and Adjacent Coastal Regions15. Southern Mexican Highlands and Adjacent Coastal Regions: Introduction (Ralph L. Reals); 16. The Zapotec of Oaxaca (Laura Nader); 17. The Chatino (Gabriel DeCicco); 18. The Mixtec (Robert Ravicz and A. Kimball Romney); 19. The Trique of Oaxaca (Laura Nader);20. The Amuzgo (Robert Ravicz and A. Kimball Romney); 21. The Cuicatec (Roberto J. Weitlaner); 22. The Mixe, Zoque, and Popoluca (George M. Foster); 23. The Huave (A. Richard Diebold, Jr.); 24. The Popoloca (Walter A. Hoppe, Andres Medina, and Roberto J. Weitlaner); 25. The Ichcatec (Walter A. Hoppe and Roberto J. Weitlaner); 26. The Chocho (Walter A. Hoppe and Roberto J. Weitlaner); 27. The Mazatec (Roberto J. Weitlaner and Walter A. Hoppe); 28. The Chinantec (Roberto J. Weitlaner and Howard F. Cline); 29. The Tequistlatec and Tlapanec (D. L. Olmsted); 30. The Cuitlatec (Susana Drucker, Roberto Escalante, and Roberto J. Weitlaner); Volume 8, Section III: Central Mexican Highlands; 31. Central Mexican Highlands: Introduction (Pedro Carrasco); 32. The Nahua (William Madsen); 33. The Totonac (H. R. Harvey and Isabel Kelly); 34. The Otomi (Leonardo Manrique C.); Section IV: Western Mexico 35. The Tarascans (Ralph L. Beals); Section V: Northwest Mexico; 36. Northwest Mexico: Introduction (Edward H. Spicer); 37. The Huichol and Cora (Joseph E. Grimes and Thomas B. Hinton); 38. The Southern Tepehuan and Tepecano (Carroll L. Riley); 39. The Northern Tepehuan (Elman R. Service); 40. The Yaqui and Mayo (Edward H. Spicer); 41. The Tarahumara (Jacob Fried); 42. Contemporary Ethnography of Baja California, Mexico (Roger C. Owen); 43. Remnant Tribes of Sonora: Opata, Pima, Papago, and Seri (Thomas B. Hinton).\n\nVolumes 6 & 7 were reviewed when the appeared. One reviewer highlights several articles, including those by Eric R. Wolf, Angel Palerm, and Willilam Sanders, but he goes on to say \"These volumes are ... more valuable for reference than for reading. Sections dealing with distribution, history, and bibliography are very useful, but sections dealing with social structure or the character of the peoples generally fail to provide integrated analyses indicating the essential features.\"\n\nVolume 9. Physical Anthropology, T.D. Stewart, volume editor.\n\nVolume 10-11. Archeology of Northern Mesoamerica, G. F. Ekholm and Ignacio Bernal, volume editors.\n\nVolumes 12-15, Guide to Ethnohistorical Sources, Howard F. Cline, Volume editor.\n\nVolume 12, Guide to Ethnohistorical Sources, Part 1. (1972) 1.“Introductory Notes on Territorial Divisions of Middle America” , Howard F. Cline, pp. 17–62; 2. “Colonial New Spain, 1519-1786: Historical Notes on the Evolution of Minor Political Jurisdictions”, Peter Gerhard, pp. 63–137; 3. “Viceroyalty to Republics, 1786-1952: Historical Notes on the Evolution of Middle American Political Units,” Howard F. Cline, pp. 138–165; 4.“Ethnohistorical Regions of Middle America,” Howard F. Cline, pp. 166–182; 5.“The \"Relaciones Geográficas\" of the Spanish Indies, 1577-1648,” Howard F. Cline, pp. 183–242; 6.“The Pinturas (Maps) of the Relaciones Geográficas, with Catalogue,” Donald Robertson, pp. 243–278; 7.“The Relaciones Geográficas, 1579-1586: Native Languages,” H.R. Harvey, pp. 279–323; 8.“A Census of the Relaciones Geográficas of New Spain, 1579-1612,” Howard F. Cline, pp. 324–369; 9.“The Relaciones Geográficas of Spain, New Spain, and the Spanish Indies: An Annotated Bibliography,” Howard F. Cline, pp. 370–395; 10.“The Relaciones Geográficas of Mexico and Central America, 1740-1792,” Robert C. West, pp. 396–452.\nVolume 13. Guide to Ethnohistorical Sources, Part 2. (1973) 11, “Published Collections of Documents Relating to Middle American Ethnohistory”, Charles Gibson; 12, “An Introductory Survey of Secular Writings in the European Tradition on Colonial Middle America, 1503-1818,” J. Benedict Warren, pp. 42–137; 13. “Religious Chronicles and Historians: A Summary and Annotated Bibliography,” Ernest J. Burrus, S.J.; 14. “Bernardino de Sahagún, 1499-1590A. “Sahagún and His Works,” Nicolau d’Olwer and Howard F. Cline, 186-206; B. “Sahagún’s “Primeros Memoriales.” Tepepulco, H. B. Nicholson, pp. 207–217; C. “Sahagún’s Materials and Studies,” Howard F. Cline, pp. 218–239; 15. “Antonio de Herrera, 1549-1625,” Manuel Ballesteros Gaibrois, pp. 240–255; 16. “Juan de Torquemada, 1564-1624,” José Alcina Franch, pp. 256–275; 17. “Francisco Javier Clavigero, 1731-1787, “ Charles F. Ronan, S. J., pp. 276–297; 18. “Charles Etienne Brasseur de Bourbourg, 1814-1874,” Carroll Edward Mace, pp. 298–325; 19. “Hubert Howe Bancroft, 1832-1918,” Howard F. Cline, pp. 326–347; 20. “Eduard Georg Seler, 1849-1922,” H. B. Nicholson, pp. 348–369; 21, “Select Nineteenth-Century Mexican Writers on Ethnohistory,” Howard F. Cline, pp. 370–403. Carlos María de Bustamante, José Fernando Ramírez, Manuel Orozco y Berra, Joaquín García Icazbalceta, Alfredo Chavero, Francisco del Paso y Troncoso\n\nVolume 14. Guide to Ethnohistorical Sources Part 3. (1975) 22. “A Survey of Native Middle American Pictorial Manuscripts,” John B. Glass, pp. 3–80; 23. “A Census of Native Middle American Pictorial Manuscripts,” John B. Glass with Donald Robertson, pp. 81–252; 24. “Techialoyan Manuscripts and Paintings with a Catalog,” Donald Robertson, pp. 253–280; 25. “A Census of Middle American Testerian Manuscripts,” John B. Glass, pp. 281–296; 26. “A Catalogue of Falsified Middle American Pictorial Manuscripts,” John B. Glass, pp. 297–309; Illustrations and maps, 1-103\n\nVolume 15. Guide to Ethnohistorical Sources Part 4. (1975) 27A. “Prose Sources in the Native Historical Tradition,” Charles Gibson, pp 312–319; 27B. “A Census of Middle American Prose Manuscripts in the Native Historical Tradition,” Charles Gibson and John B. Glass, pp. 322–400; 28. “A Checklist of Institutional Holdings of Middle American Manuscripts in the Native Historical Tradition,” John B. Glass, pp. 401–472; 29. “The Boturini Collection,” John B. Glass, pp. 473–486; 30. “Middle American Ethnohistory: An Overview,” H. B. Nicholson, pp. 487–505; 31.”Index of Authors, Titles, and Synonyms,” John B. Glass, pp. 506–536; 32. “Annotated References,” John B. Glass, pp. 537–724.\nVolume 16. Handbook of Middle American Indians. Margaret A.L. Harrison, volume editor. (1976) – Bibliography for all volumes.\n\nGeneral Editor, Victoria Bricker\n\n"}
{"id": "1865442", "url": "https://en.wikipedia.org/wiki?curid=1865442", "title": "Hofstadter's law", "text": "Hofstadter's law\n\nHofstadter's law is a self-referential time-related adage, coined by Douglas Hofstadter and named after him.\n\nHofstadter's law was a part of Douglas Hofstadter's 1979 book \"Gödel, Escher, Bach: An Eternal Golden Braid\". The \"law\" is a statement regarding the difficulty of accurately estimating the time it will take to complete tasks of substantial complexity. It is often cited by programmers, especially in discussions of techniques to improve productivity, such as \"The Mythical Man-Month\" or extreme programming. The recursive nature of the law is a reflection of the widely experienced difficulty of estimating complex tasks despite all best efforts, including knowing that the task is complex.\n\nThe law was initially introduced in connection with a discussion of chess-playing computers, where top-level players were continually beating machines, even though the machines outweighed the players in recursive analysis. The intuition was that the players were able to focus on particular positions instead of following every possible line of play to its conclusion. Hofstadter wrote in 1979, \"In the early days of computer chess, people used to estimate that it would be ten years until a computer (or program) was world champion. But after ten years had passed, it seemed that the day a computer would become world champion was still more than ten years away ... This is just one more piece of evidence for the rather recursive Hofstadter's Law:\" Notably, that day did indeed come, when Deep Blue defeated Garry Kasparov in 1997, which indicates that the Law of Accelerating Returns may take effect when the task is repeated, thus counteracting -- and [in some cases] overpowering -- Hofstadter's law.\n\n"}
{"id": "10260905", "url": "https://en.wikipedia.org/wiki?curid=10260905", "title": "I (pronoun)", "text": "I (pronoun)\n\nThe pronoun I is the first-person singular nominative case personal pronoun in Modern English. It is used to refer to one's self and is capitalized, although other pronouns, such as \"he\" or \"she\", are not capitalized.\n\nThe grammatical variants of \"I\" are \"me\", \"my\", \"mine\", and \"myself\".\n\nEnglish \"I\" originates from Old English (OE) \"ic\". Its predecessor \"ic\" had in turn originated from the continuation of Proto-Germanic *\"ik\", and \"ek\"; the asterisk denotes an unattested form, \"ek\" was attested in the Elder Futhark inscriptions (in some cases notably showing the variant \"eka\"; see also ek erilaz). Linguists assume \"ik\" to have developed from the unstressed variant of \"ek\". Variants of \"ic\" were used in various English dialects up until the 1600s.\n\nGermanic cognates are: Old Frisian \"ik\", Old Norse \"ek\" (Danish, Norwegian \"jeg\", Swedish \"jag\", Icelandic ég), Old High German \"ih\" (German \"ich\") and Gothic \"ik\" and in Dutch also \"ik\".\n\nThe Proto-Germanic root came, in turn, from the Proto Indo-European language (PIE). The reconstructed PIE pronoun is *\"egō, egóm\", with cognates including\nSanskrit \"aham\", Hittite \"uk\", Latin \"ego\", Greek \"egō\", Old Slavonic \"azъ\" and Alviri-Vidari (an Iranian language) \"az\".\n\nThe oblique forms are formed from a stem \"*me-\" (English \"me\"), the plural from \"*wei-\" (English \"we\"), the oblique plurals from \"*ns-\" (English \"us\") and from Proto-Germanic \"*unseraz\", PIE \"*no-s-ero-\" (\"our, ours\").\n\n\"I\" (and only this form of the pronoun) is the only pronoun that is always capitalized in English. This practice became established in the late 15th century, though lowercase \"i\" was sometimes found as late as the 17th century.\n\nLike the other English personal pronouns \"we\" (\"us\"), \"he\" (\"him\"), \"she\" (\"her\"), and \"they\" (\"them\"), the pronoun \"I\" has several singular case forms.\nThese are: \n\nThere are some situations in which only the nominative form (\"I\") is grammatically correct and others in which only the accusative form (\"me\") is correct. There are also situations in which one form is used in informal style (and was often considered ungrammatical by older prescriptive grammars) and the other form is preferred in formal style.\n\nIn all varieties of standard English, the nominative form \"I\" is used exclusively when it is the whole subject of an \"explicit\" verb, e.g. \nnot \nWith other pronouns, such as \"we\" (strictly speaking when used as a personal determiner), there may be exceptions to this in some varieties of English.\n\nIn all varieties of standard English, the accusative form \"me\" is used exclusively when it is the whole direct or indirect object of a verb or preposition. The accusative \"me\" is also required in a number of constructions such as \"Silly me!\"\n\nIn many situations, both the nominative \"I\" and the accusative \"me\" are encountered.\n\nWhen the pronoun is used as a subjective predicative complement, the nominative \"I\" is sometimes encountered in (very) formal style:\nBut this is often seen as hypercorrect and may be unacceptable, as in:\n\"Me\" is usually preferred as a subjective predicate, especially in informal style:\nThe nominative \"I\" is more common in this role when it is followed by a relative clause:\nthough even here \"me\" is more common in non-formal style:\n\nFollowing \"as\" or \"than\" (without a following explicit verb), the accusative form is common:\nHowever, where it is possible to think of the pronoun as the subject of an implicit verb and \"than\" or \"as\" as a conjunction, the nominative \"I\" is found in formal style:\n\nIn Australian English, British English and Irish English, many speakers have an unstressed form of \"my\" that is identical to \"me\" (see archaic and non-standard forms of English personal pronouns).\n\nThe above applies when the pronoun stands alone as the subject or object.\nIn some varieties English (particularly formal English), those rules also apply in coordinative constructions such as \"you and I\". So the correct form is \n\nIn some varieties of non-standard informal English, the accusative is sometimes used when the pronoun is part of a coordinative \"subject\" construction, as in\nThis is highly stigmatized.\n\nOn the other hand, the use of the nominative \"I\" in coordinative constructions like \"you and I\"where \"me\" would be used in a non-coordinative object is less stigmatized – and in some cases so widespread as to be considered a variety of standard English: \n\n\n\n\n"}
{"id": "3270420", "url": "https://en.wikipedia.org/wiki?curid=3270420", "title": "Idem", "text": "Idem\n\nidem. is a Latin term meaning \"the same\". It is commonly abbreviated as id.,\nwhich is particularly used in legal citations to denote the previously cited source (compare \"ibid.\"). It is also used in academic citations to replace the name of a repeated author. \n\n\"Id.\" is employed extensively in Canadian legislation and in legal documents of the United States to apply a short description to a section with the same focus as the previous.\n\n\"Id\" is masculine and neuter; ead. (feminine), is the abbreviation for eadem, which also translates to \"the same\". \n\nAs an abbreviation, \"Id.\" always takes a period (or full stop) in both British and American usage (see usage of the full stop in abbreviations). Its first known use dates back to the 14th century.\n\nHere, the first citation refers to the case of \"United States v. Martinez-Fuerte.\" The volume number cited is 428 and the page on which the case begins is 543, and the page number cited to is 545. The \"U.S.\" between the numerical portions of the citation refers to the \"United States Reports\". 1976 refers to the year that the case was published. The second citation references the first citation and automatically incorporates the same reporter and volume number; however, the page number cited is now 547. \"Id.\" refers to the immediately preceding citation, so if the previous citation includes more than one reference, or it is unclear which reference \"Id.\" refers to, its usage is inappropriate.\n\n\nHere, Id. refers to the Executive Order that was mentioned in the previous sentence.\n\n\nIn this example, \"Id\" in the second citation indicates that the author is identical to that of the previous citation. That is, the author of the second citation is also Macgillivray, J. A.\n\n"}
{"id": "597476", "url": "https://en.wikipedia.org/wiki?curid=597476", "title": "Info", "text": "Info\n\nInfo is shorthand for \"information\". It may also refer to:\n\n\n"}
{"id": "17878314", "url": "https://en.wikipedia.org/wiki?curid=17878314", "title": "Information source", "text": "Information source\n\nAn information source is a person, thing, or place from which information comes, arises, or is obtained. Information souces can be known as primary or secondary. That source might then inform a person about something or provide knowledge about it. Information sources are divided into separate distinct categories, primary, secondary, tertiary, and so on.\n\n"}
{"id": "15293025", "url": "https://en.wikipedia.org/wiki?curid=15293025", "title": "Informationsdienst Wissenschaft", "text": "Informationsdienst Wissenschaft\n\nInformationsdienst Wissenschaft e.V. or idw (The Science Information Service) operates an Internet platform, which bundles the press reports and dates of important events from about 1,000 scientific institutions, including universities, technical colleges, governmental and non-governmental research institutes and institutes to support research or scientific administration. idw (a registered charitable society) also operates an expert broker, the idw expert finder, which is exclusively for journalists. This makes idw one of the most comprehensive sources of science news in the German-speaking area. Foreign journalists and institutions (mostly European) now use idw as well. \n\nThe two main objectives of idw are:\n\nThe information in idw can be accessed free of charge - either directly on idw’s www pages, or by using an individually configurable RSS feed or as an e-mail subscriber. Any user can request the information covering the topics and regions which interest him. All idw services can be used free of cost - the current news ticker, the science calendar, research in the archive (which contains more than 350,000 press releases), and the list of institutions linked to idw. idw also provides journalists with instruments for contacting experts, and maintains a database with science photos.\nThe members' press offices have various possibilities of communicating with journalists. Membership is only offered to German or foreign institutions which perform research or teaching, or which support science or are active in science in some other way.\n\nThe original idea of idw was to provide experts for journalists. Using the American ProfNet as example, the press officers of Universitaet Bayreuth, the Ruhr University Bochum and the Clausthal University of Technology, in collaboration with Computing Centre of Clausthal University of Technology/TU Clausthal, developed a concept for a German language network, by means of the new media. The concept was technically implemented by the staff of the Computing Centre of the Clausthal University of Technology. A total of nine staff members in Bayreuth, Bochum and Clausthal are responsible for programming, maintaining and developing the idw operating system, for user services and further development of the content.\n\nThe initial phase (1996–1999) was guaranteed by project support from the Federal Ministry for Education and Research (BMBF). The technical development of the idw was supported by the Ministry, together with the Stifterverband fuer die Deutsche Wissenschaft (Donor Association for German Science). idw has been working closely for years with the initiative Wissenschaft im Dialog (Science in Dialogue). idw has been economically independent since 2000 and is financed by contributions from member institutions. It has been organised as a registered charitable society (gemeinnütziger e. V.) since 2002.\n\nidw has developed as a recognised and accepted source for German language science and for science journalism. It has become an instrument for public relations work for scientific institutions. \nAbout 37,000 subscribers (figure for June 2018) receive regular reports from idw, including some 7,900 journalists. About 1,000 institutions publish their press reports and dates of important events via idw.\n\n"}
{"id": "5995840", "url": "https://en.wikipedia.org/wiki?curid=5995840", "title": "L. G. Pine", "text": "L. G. Pine\n\nLeslie Gilbert Pine (22 December 1907 – 15 May 1987) was a British author, lecturer, and researcher in the areas of genealogy, nobility, history, heraldry and animal welfare. He was born in 1907 in Bristol, England and died in Bury St. Edmunds, Suffolk in 1987. He was the son of Lilian Grace Beswetherick and Henry Moorshead Pine (a tea merchant).\n\nFrom 1935 to 1940 he served as an assistant editor at Burke's Peerage Ltd. During World War II he was an officer in the Royal Air Force intelligence branch, serving in North Africa, Italy, Greece, and India; he retired with the rank of Squadron Leader. After the war and until 1960, he was Burke's executive director. Pine edited \"Burke's Peerage,\" 1949-1959; \"Burke's Landed Gentry (of Great Britain),\" 1952; \"Burke's Landed Gentry (of Ireland),\" 1958; and, \"Burke's Distinguished Families of America,\" 1939, 1947. He also edited \"The International Year Book and Statesmen's Who's Who,\" 1953-1960; \"Author's and Writer's Who's Who,\" 1948, 1960; \"Who's Who in Music,\" 1949; and, \"Who's Who in the Free Churches,\" 1951.\n\nA graduate of London University, he became a Barrister-at-Law, Inner Temple, in 1953. Pine was a member of the International Institute of Genealogy and Heraldry, Fellow of the Society of Antiquaries of Scotland, a Fellow of the Ancient Monuments Society, a Life Fellow of the Institute of Journalists, a Freeman of the City of London, and a Liveryman of the Glaziers' Company. In 1959 he was the unsuccessful Conservative candidate for Bristol Central.\n\nHe was managing editor of a British hunting magazine, \"Shooting Times\", from 1960 to 1964. He later authored an important book highly critical of sport hunting, \"After Their Blood\", in which he wrote: \"It is our duty as men and women of God’s redeemed creation to try not to increase the suffering of the world, but to lessen it. To get rid of bloodsports will be a great step toward this end.\"\n\nIn 1948 Leslie Pine married Grace V. Griffin (20 August 1914- ). Their only child, Richard Pine, was born in London on 21 August 1949.\n\nHis books include:\n\n\nPine is also the primary contributor to the article \"genealogy\" in \"Encyclopædia Britannica\".\n\n"}
{"id": "30795401", "url": "https://en.wikipedia.org/wiki?curid=30795401", "title": "Liar paradox in early Islamic tradition", "text": "Liar paradox in early Islamic tradition\n\nMany early Islamic philosophers and logicians discussed the liar paradox. Their work on the subject began in the 10th century and continued to Athīr al-Dīn al-Abharī and Nasir al-Din al-Tusi of the middle 13th century and beyond. Although the Liar paradox has been well known in Greek and Latin traditions, the works of Arabic scholars have only recently been translated into English.\n\nEach group of early Islamic philosophers discussed different problems presented by the paradox. They pioneered unique solutions that were not influenced by Western ideas.\n\nAthīr al-Dīn Mufaḍḍal (b. ʿUmar Abharī, d. 663/1264) was a Persian philosopher, astronomer and mathematician from the city of Abhar in Persia. There is some speculation that his works on the Liar paradox could have been known to Western logicians, and in particular to Thomas Bradwardine.\n\nHe analyzed the Liar sentence as follows:\n\nIn other words, Athīr says that if the Liar sentence is false, which means that the Liar falsely declares that all he says at the moment is false, then the Liar sentence is true; and, if the Liar sentence is true, which means that the Liar truthfully declares that all he says at the moment is false, then the Liar sentence is false. In any case, the Liar sentence is both true and false at the same time, which is a paradox.\n\nAthīr offers the following solution for the paradox:\n\nAccording to the traditional idealization that presumably was used by Athīr, the sentence as an universal proposition is false only, when \"either it has a counter-instance or its subject term is empty\".\n\n\nThe Liar sentence, however, has neither an empty subject nor counter-instance. This fact creates obstacles for Athīr's view, who must show what is unique about the Liar sentence, and how the Liar sentence still could be only true or false in view of the \"true\" and \"false\" conditions set up in the universal proposition's description. Athīr tries to solve the paradox by applying to it the laws of negation of a conjunction and negation of a disjunction.\n\nAhmed Alwishah, who has a Ph.D. in Islamic Philosophy and David Sanson, who has a Ph.D. in Philosophy explain that Athīr actually claims that:\n\n(1) \"It is not the case that, if the Liar Sentence is not both true and false, then it is true.\"\n\nAlwishah and Sanson continue:\n\"The general principle behind (1) is clear enough: the negation of a conjunction does not entail the negation of a conjunct; so from not both true and false you cannot infer not false and so true. Abharī appears to be saying that the Liar rests on an elementary scope fallacy! But, of course, Abharī is not entitled to (1). In some cases, the negation of a conjunction does entail the negation of a conjunct: 'not both P and P' for example, entails 'not P'. As a general rule, the negation of a conjunction entails the negation of each conjunct whenever the conjuncts are logically equivalent, i.e., whenever the one follows from the other and vice verse. So Abharī is entitled to (1) only if he is entitled to assume that ‘The Liar Sentence is true’ and ‘The Liar Sentence is false’ are not logically equivalent.\"\n\nThe Liar sentence is a universal proposition (The Liar says All I say ...), so \"if it is (non–vacuously) false it must have a counter–instance\". But in this case scenario, when the only thing that the liar is saying is the single sentence declaring that what he is saying at the moment is false, the only available counter–instance is the Liar sentence itself. When staging the paradox Abharī said: \"if it is not true, then it is necessary that one of his sentences at this moment is true, as long as he utters something. But, he says nothing at this moment other than this sentence. Thus, this sentence is necessarily true and false\" So the explanation provided by Abharī himself demonstrates that both \"'The Liar Sentence is false' and 'The Liar Sentence is true' are logically equivalent. If they are logically equivalent, then, contrary to (1), the negation of the conjunction does entail the negation of each conjunct. Abharī’s 'solution; therefore fails.\"\n\nNaṣīr al-Dīn al-Ṭūsī was a Persian polymath and prolific writer: an astronomer, biologist, chemist, mathematician, philosopher, physician, physicist, scientist, theologian and Marja Taqleed. He adhered to the Ismaili, and subsequently Twelver Shī‘ah Islamic belief systems. The Arab scholar Ibn Khaldun (1332–1406) considered Tusi to be the greatest of the later Persian scholars.\n\nṬūsī's work on the paradox begins with a discussion of the paradox and the solution offered by Abharī, with which Ṭūsī disagrees. As Alwishah and Sanson point out \"Ṭūsī argues that whatever fancy thing (conjunction, conditional) Abharī wants to identify as the truth condition for the Liar Sentence, it will not matter, because pace Abharī, we can generate the paradox without inferring, from the negation of a complex truth condition, the negation of one of its parts. We can argue directly that its being false entails the negation of its being false, and so entails its being true.\"\n\nṬūsī then prepares a stage for his own solution of the Liar paradox, writing that:\nHe does not see a reason that could prevent a declarative sentence to declare something about another declarative sentence.\n\nWith an example of two declarative sentences, (D1) \"It is false\" and (D2) \"Zayd is sitting\", Ṭūsī explains how one declarative sentence (D1) can declare another declarative sentence (D2) to be false: \"It is false that Zayd is sitting\". There is no paradox in the above two declarative sentences because they have different subjects. To generate a paradox a declarative sentence must declare something about itself. If (D1) falsely declares itself to be not (D1) then this false declaration referencing to itself as being \"false\" creates a paradox.\n\nṬūsī writes: \n\nThe above conclusions are very important to the history of Liar Paradox. Alwishah and Sanson point out: \"It is hard to overemphasize how remarkable this passage is. The contemporary reader will be familiar with the idea that the Liar Paradox is a paradox of selfreference. But Ṭūsī is, as far as we know, the first person to express this idea. This passage has no precedent in any tradition. Ṭūsī has performed three remarkable feats in short order. First, his Liar Sentence is singular: its subject is itself, and it declares itself to be false. Gone, then, is the choice between universal or particular Liar Sentence, and the associated problem of adding further assumptions to generate a genuine paradox. Second, he has characterized the paradox as one of self-reference. Third, he has identified a key assumption that might be responsible for generating the entire problem: the assumption that a declarative sentence, by its nature, can declare-something-about anything.\"\n\nRecognizing that, if a declarative sentence that declares itself being false, is false, this does not necessitate it being true. Ṭūsī says that it would be absurd to say that this declarative sentence is true only because it is not false. Ṭūsī writes:\n\nṬūsī then interprets the definitions of \"true\" and \"false\", in an attempt to prove that those definitions should not be taken into consideration when dealing with a declarative sentence that declares itself, as its own subject, to be false.\n\nAl-Baghdādī's definition of \"truth\" and \"falsity\" says that: \"truth is an agreement with the subject, and falsity is the opposite of that\". Ṭūsī argues that this definition cannot be applied to a declarative sentence that declares its own subject to be false because then there are at least two opposite parts that are in disagreement with each other. The same subject cannot be in disagreement with itself. Therefore a self–referenced declarative sentence that declares itself to be false is neither false nor true, and truth/falsity definitions are not applicable to those sentences.\n\nṬūsī stopped short from offering a solution for the Liar sentences discussed by Āmidī \"All that I say at this moment is false\". This sentence presents a different case scenario because it can be interpreted as declaring something about itself, and something about another sentence. The solution for this paradox is absent from Ṭūsī's papers.\n"}
{"id": "33447383", "url": "https://en.wikipedia.org/wiki?curid=33447383", "title": "Metabibliography", "text": "Metabibliography\n\nA metabibliography (or biblio-bibliography) is a bibliography of bibliographies.\n\nBibliographies serve the finding of relevant documents. Metabibliographies serve the finding of the relevant bibliographies in which the relevant documents may be found. One might quote Patrick Wilson:\n\n\"For if knowledge is power, power over knowledge is power to increase one's power; and if the stock of writings is thought of mainly as it represents a stock of knowledge, it is natural to propose treating it as a \"resource\" to be subjected to rational control, managemenet and utilization.\" (Wilson, 1968, p. 145).\n\nMetabibliographies are valuable for building reference collections, but usually of less interest to the average user, who rely on bibliographies selected by others.\n\n\n\n"}
{"id": "5629066", "url": "https://en.wikipedia.org/wiki?curid=5629066", "title": "Nomina sacra", "text": "Nomina sacra\n\nIn Christian scribal practice, nomina sacra (singular: nomen sacrum from Latin sacred name) is the abbreviation of several frequently occurring divine names or titles, especially in Greek manuscripts of Holy Scripture. A nomen sacrum consists of two or more letters from the original word spanned by an overline.\n\nMetzger lists 15 such expressions from Greek papyri: the Greek counterparts of \"God\", \"Lord\", \"Jesus\", \"Christ\", \"Son\", \"Spirit\", \"David\", \"Cross\", \"Mother\", \"Father\", \"Israel\", \"Savior\", \"Man\", \"Jerusalem\", and \"Heaven\". These \"nomina sacra\" are all found in Greek manuscripts of the 3rd century and earlier, except \"Mother\", which appears in the 4th.\n\n\"Nomina sacra\" also occur in some form in Latin, Coptic, Armenian (indicated by the \"pativ\"), Gothic, Old Nubian, and Cyrillic (indicated by the \"titlo\").\n\n\"Nomina sacra\" are consistently observed in even the earliest extant Christian writings, along with the codex form rather than the roll, implying that when these were written, in approximately the second century, the practice had already been established for some time. However, it is not known precisely when and how the \"nomina sacra\" first arose.\n\nThe initial system of \"nomina sacra\" apparently consisted of just four or five words, called \"nomina divina\": the Greek words for \"Jesus\", \"Christ\", \"Lord\", \"God\", and possibly \"Spirit\". The practice quickly expanded to a number of other words regarded as sacred.\n\nIn the system of \"nomina sacra\" that came to prevail, abbreviation is by \"contraction\", meaning that the first and last letter (at least) of each word are used. In a few early cases, an alternate practice is seen of abbreviation by \"suspension\", meaning that the initial two letters (at least) of the word are used; e.g., the opening verses of Revelation in write (\"Jesus Christ\") as . Contraction, however, offered the practical advantage of indicating the case of the abbreviated noun.\n\nIt is evident that the use of \"nomina sacra\" was an act of reverence rather than a purely practical space-saving device, as they were employed even where well-established abbreviations of far more frequent words such as \"and\" were avoided, and the \"nomen sacrum\" itself was written with generous spacing. Furthermore, early scribes often distinguished between mundane and sacred occurrences of the same word, e.g. a \"spirit\" vs. the \"Spirit\", and applied \"nomina sacra\" only to the latter (at times necessarily revealing an exegetical choice), although later scribes would mechanically abbreviate all occurrences.\n\nScholars have advanced a number of theories on the origin of the \"nomina sacra\". An obvious parallel that likely offered some inspiration is the Jewish practice of writing the divine name of God, commonly rendered as Jehovah or Yahweh in English, as the Hebrew tetragrammaton (transliterated as YHWH) even in Greek Scriptures. The Septuagint manuscript LXX P.Oxy.VII.1007 uses two Paleo-Hebrew \"yodh's\" with a horizontal line through them for YHWH (an abbreviated form of the Name of God translitered as ). Pavlos Vasileiadis, a Doctor of Theology at the Aristotle University of Thessaloniki, quoting Gerard Gertoux, states that \"the subsequent use of the contracted forms of the original nomina sacra κ[ύριο]ς [()] and θ[εό]ς [()] within Christian manuscripts probably reflects the Jewish practice of replacing the Tetragrammaton by י[הו]ה.\", transliterated in koine Greek as ιά.\n\nGreek culture also employed a number of ways of abbreviating even proper names, though none in quite the same form as the \"nomina sacra\". Inspiration for the contracted forms (using the first and last letter) has also been seen in Revelation, where Jesus speaks of himself as \"the beginning and the end\" and \"the first and the last\" as well \"the Alpha and the Omega\". Greek numerals have been suggested as the origin of the overline spanning the whole \"nomen sacrum\", with the suspended form being simply the ordinary way of writing \"eighteen\", for example.\n\n"}
{"id": "8912106", "url": "https://en.wikipedia.org/wiki?curid=8912106", "title": "Reference scenario", "text": "Reference scenario\n\nA reference scenario is an imagined situation where a library patron brings a question to a librarian and there is then a conversation, called in the field a reference interview, where the librarian works to help the patron find what he or she wants. These scenarios are used in training future librarians how to help patrons. Basically, a scenario is as short as a couple of sentences, including a question and a situation that underlies that question.\n\nA great deal of reference teaching puts students to researching the answers to made-up questions. This focuses the student on learning about the reference sources at hand by using them to answer those questions. Scenarios are something different. They focus the student on the interaction with patrons. In class practice sessions, one student can be the patron and the other the librarian, as long as the one practicing as the librarian doesn't know the whole scenario in advance.\n\nScenarios are valued because often the question asked is not the end of the patron's information hunt, but the start. Patrons often start by voicing a question that they think the library can answer, rather than the question they are really seeking to answer. Or they pose a question that the librarian doesn't understand. Reference librarian skills are very much about mediating a gap between what the patron wants and what the library can provide. This can involve the librarian making him or herself a partner in the patron's search, teaching them what the library really has to offer, or even just clarifying a confusing word: Does the patron want information about soaps to clean with or soaps as in soap operas?\n\n\n"}
{"id": "56067306", "url": "https://en.wikipedia.org/wiki?curid=56067306", "title": "SDS-PAGE", "text": "SDS-PAGE\n\nSDS-PAGE (sodium dodecyl sulfate–polyacrylamide gel electrophoresis) is a variant of polyacrylamide gel electrophoresis, an analytical method in biochemistry for the separation of charged molecules in mixtures by their molecular masses in an electric field. It uses sodium dodecyl sulfate (SDS) molecules to help identify and isolate protein molecules.\n\nSDS-PAGE is a discontinuous electrophoretic system developed by Ulrich K. Laemmli which is commonly used as a method to separate proteins with molecular masses between 5 and 250 KDa. The publication describing it is the most frequently cited paper by a single author, and the second most cited overall.\n\nSDS-PAGE is an electrophoresis method that allows protein separation by mass. The medium (also referred to as ′matrix′) is a polyacrylamide-based discontinuous gel. In addition, SDS (sodium dodecyl sulfate) is used. About 1.4 grams of SDS bind to a gram of protein, corresponding to one SDS molecule per two amino acids. SDS acts as a surfactant, covering the proteins' intrinsic charge and conferring them very similar charge-to-mass ratios. The intrinsic charges of the proteins are negligible in comparison to the SDS loading, and the positive charges are also greatly reduced in the basic pH range of a separating gel. Upon application of a constant electric field, the protein migrate towards the anode, each with a different speed, depending on its mass. This simple procedure allows precise protein separation by mass.\n\nSDS tends to form spherical micelles in aqueous solutions above a certain concentration called the critical micellar concentration (CMC). Above the critical micellar concentration of 7 to 10 millimolar in solutions, the SDS simultaneously occurs as single molecules (monomer) and as micelles, below the CMC SDS occurs only as monomers in aqueous solutions. At the critical micellar concentration, a micelle consists of about 62 SDS molecules. However, only SDS monomers bind to proteins via hydrophobic interactions, whereas the SDS micelles are anionic on the outside and do not adsorb any protein. SDS is amphipathic in nature, which allows it to unfold both polar and nonpolar sections of protein structure. In SDS concentrations above 0.1 millimolar, the unfolding of proteins begins, and above 1 mM, most proteins are denatured. Due to the strong denaturing effect of SDS and the subsequent dissociation of protein complexes, quaternary structures can generally not be determined with SDS. Exceptions are e.g. proteins that were previously stabilised by covalent cross-linking and the SDS-resistant protein complexes, which are stable even in the presence of SDS (the latter, however, only at room temperature). To denature the SDS-resistant complexes a high activation energy is required, which is achieved by heating. SDS resistance is based on a metastability of the protein fold. Although the native, fully folded, SDS-resistant protein does not have sufficient stability in the presence of SDS, the chemical equilibrium of denaturation at room temperature occurs slowly. Stable protein complexes are characterised not only by SDS resistance but also by stability against proteases and an increased biological half-life.\n\nAlternatively, polyacrylamide gel electrophoresis can also be performed with the cationic surfactants CTAB in a CTAB-PAGE, or 16-BAC in a BAC-PAGE.\n\nThe SDS-PAGE method is composed of gel preparation, sample preparation, electrophoresis, protein staining or western blotting and analysis of the generated banding pattern.\n\nWhen using different buffers in the gel (discontinuous gel electrophoresis), the gels are made up to one day prior to electrophoresis, so that the diffusion does not lead to a mixing of the buffers. The gel is produced by radical polymerisation in a mold consisting of two sealed glass plates with spacers between the glass plates. In a typical mini-gel setting, the spacers have a thickness of 0.75 mm or 1.5 mm, which determines the loading capacity of the gel. For pouring the gel solution, the plates are usually clamped in a stand which temporarily seals the otherwise open underside of the glass plates with the two spacers. For the gel solution, acrylamide is mixed as gel-former (usually 4% V/V in the stacking gel and 10-12 % in the separating gel), methylenebisacrylamide as a cross-linker, stacking or separating gel buffer, water and SDS. By adding the catalyst TEMED and the radical initiator ammonium persulfate (APS) the polymerisation is started. The solution is then poured between the glass plates without creating bubbles. Depending on the amount of catalyst and radical starter and depending on the temperature, the polymerisation lasts between a quarter of an hour and several hours. The lower gel (separating gel) is poured first and covered with a few drops of a barely water-soluble alcohol (usually buffer-saturated butanol or isopropanol), which eliminates bubbles from the meniscus and protects the gel solution of the radical scavenger oxygen. After the polymerisation of the separating gel, the alcohol is discarded and the residual alcohol is removed with filter paper. After addition of APS and TEMED to the stacking gel solution, it is poured on top of the solid separation gel. Afterwards, a suitable sample comb is inserted between the glass plates without creating bubbles. The sample comb is carefully pulled out after polymerisation, leaving pockets for the sample application. For later use of proteins for protein sequencing, the gels are often prepared the day before electrophoresis to reduce reactions of unpolymerised acrylamide with cysteines in proteins.\n\nBy using a gradient mixer, gradient gels with a gradient of acrylamide (usually from 4 to 12%) can be cast, which have a larger separation range of the molecular masses. Commercial gel systems (so-called \"pre-cast gels\") usually use the buffer substance Bis-tris methane with a pH value between 6.4 and 7.2 both in the stacking gel and in the separating gel. These gels are delivered cast and ready-to-use. Since they use only one buffer (continuous gel electrophoresis) and have a nearly neutral pH, they can be stored for several weeks. The more neutral pH slows the hydrolysis and thus the decomposition of the polyacrylamide. Furthermore, there are fewer acrylamide-modified cysteines in the proteins. Due to the constant pH in collecting and separating gel there is no stacking effect. Proteins in BisTris gels can not be stained with ruthenium complexes. This gel system has a comparatively large separation range, which can be varied by using MES or MOPS in the running buffer.\n\nDuring sample preparation, the sample buffer, and thus SDS, is added in excess to the proteins, and the sample is then heated to 95 °C for five minutes, or alternatively 70°C for ten minutes. Heating disrupts the secondary and tertiary structures of the protein by disrupting hydrogen bonds and stretching the molecules. Optionally, disulfide bridges can be cleaved by reduction. For this purpose, reducing thiols such as β-mercaptoethanol (β-ME, 5% by volume), dithiothreitol (DTT, 10 millimolar) or dithioerythritol (DTE, 10 millimolar) are added to the sample buffer. After cooling to room temperature, each sample is pipetted into its own well in the gel, which was previously immersed in electrophoresis buffer in the electrophoresis apparatus.\n\nIn addition to the samples, a molecular-weight size marker is usually loaded onto the gel. This consists of proteins of known sizes and thereby allows the estimation (with an error of ± 10%) of the sizes of the proteins in the actual samples, which migrate in parallel in different tracks of the gel. The size marker is often pipetted into the first or last pocket of a gel.\n\nFor separation, the denatured samples are loaded onto a gel of polyacrylamide, which is placed in an electrophoresis buffer with suitable electrolytes. Thereafter, a voltage (usually around 100 V, 10-20 V per cm gel length) is applied, which causes a migration of negatively charged molecules through the gel in the direction of the positively charged anode. The gel acts like a sieve. Small proteins migrate relatively easily through the mesh of the gel, while larger proteins are more likely to be retained and thereby migrate more slowly through the gel, thereby allowing proteins to be separated by molecular size. The electrophoresis lasts between half an hour to several hours depending on the voltage and length of gel used.\n\nThe fastest-migrating proteins (with a molecular weight of less than 5 KDa) form the buffer front together with the anionic components of the electrophoresis buffer, which also migrate through the gel. The area of the buffer front is made visible by adding the comparatively small, anionic dye bromophenol blue to the sample buffer. Due to the relatively small molecule size of bromophenol blue, it migrates faster than proteins. By optical control of the migrating colored band, the electrophoresis can be stopped before the dye and also the samples have completely migrated through the gel and leave it.\n\nThe most commonly used method is the discontinuous SDS-PAGE. In this method, the proteins migrate first into a collecting gel with neutral pH, in which they are concentrated and then they migrate into a separating gel with basic pH, in which the actual separation takes place. Stacking and separating gels differ by different pore size (4-6 % T and 10-20 % T), ionic strength and pH values (pH 6.8 or pH 8.8). The electrolyte most frequently used is an SDS-containing Tris-glycine-chloride buffer system. At neutral pH, glycine predominantly forms the zwitterionic form, at high pH the glycines lose positive charges and become predominantly anionic. In the collection gel, the smaller, negatively charged chloride ions migrate in front of the proteins (as leading ions) and the slightly larger, negatively and partially positively charged glycinate ions migrate behind the proteins (as initial trailing ions), whereas in the comparatively basic separating gel both ions migrate in front of the proteins. The pH gradient between the stacking and separation gel buffers leads to a stacking effect at the border of the stacking gel to the separation gel, since the glycinate partially loses its slowing positive charges as the pH increases and then, as the former trailing ion, overtakes the proteins and becomes a leading ion, which causes the bands of the different proteins (visible after a staining) to become narrower and sharper - the stacking effect. For the separation of smaller proteins and peptides, the TRIS-Tricine buffer system of Schägger and von Jagow is used due to the higher spread of the proteins in the range of 0.5 to 50 KDa.\n\nAt the end of the electrophoretic separation, all proteins are sorted by size and can then be analyzed by other methods, e. g. protein staining such as Coomassie staining (most common and easy to use), silver staining (highest sensitivity), stains all staining, Amido black 10B staining, Fast green FCF staining, fluorescent stains such as epicocconone stain and SYPRO orange stain, and immunological detection such as the Western Blot. The fluorescent dyes have a comparatively higher linearity between protein quantity and color intensity of about three orders of magnitude above the detection limit, i. e. the amount of protein can be estimated by color intensity. When using the fluorescent protein dye trichloroethanol, a subsequent protein staining is omitted if it was added to the gel solution and the gel was irradiated with UV light after electrophoresis.\n\nProtein staining in the gel creates a documentable banding pattern of the various proteins. Glycoproteins have differential levels of glycosylations and adsorb SDS more unevenly at the glycosylations, resulting in broader and blurred bands. Membrane proteins, because of their transmembrane domain, are often composed of the more hydrophobic amino acids, have lower solubility in aqueous solutions, tend to bind lipids, and tend to precipitate in aqueous solutions due to hydrophobic effects when sufficient amounts of detergent are not present. This precipitation manifests itself for membrane proteins in a SDS-PAGE in \"tailing\" above the band of the transmembrane protein. In this case, more SDS can be used (by using more or more concentrated sample buffer) and the amount of protein in the sample application can be reduced. An overloading of the gel with a soluble protein creates a semicircular band of this protein (e. g. in the marker lane of the image at 66 KDa), allowing other proteins with similar molecular weights to be covered. A low contrast (as in the marker lane of the image) between bands within a lane indicates either the presence of many proteins (low purity) or, if using purified proteins and a low contrast occurs only below one band, it indicates a proteolytic degradation of the protein, which first causes degradation bands, and after further degradation produces a homogeneous color (\"smear\") below a band. The documentation of the banding pattern is usually done by photographing or scanning. For a subsequent recovery of the molecules in individual bands, a gel extraction can be performed.\n\nAfter protein staining and documentation of the banding pattern, the polyacrylamide gel can be dried for archival storage. Proteins can be extracted from it at a later date. The gel is either placed in a drying frame (with or without the use of heat) or in a vacuum dryer. The drying frame consists of two parts, one of which serves as a base for a wet cellophane film to which the gel and a one percent glycerol solution are added. Then a second wet cellophane film is applied bubble-free, the second frame part is put on top and the frame is sealed with clips. The removal of the air bubbles avoids a fragmentation of the gel during drying. The water evaporates through the cellophane film. In contrast to the drying frame, a vacuum dryer generates a vacuum and heats the gel to about 50 °C.\n\nFor a more accurate determination of the molecular weight, the relative migration distances of the individual protein bands are measured in the separating gel. The measurements are usually performed in triplicate for increased accuracy. The relative mobility (called Rf value or Rm value) is the quotient of the distance of the band of the protein and the distance of the buffer front. The distances of the bands and the buffer front are each measured from the beginning of the separation gel. The distance of the buffer front roughly corresponds to the distance of the bromophenol blue contained in the sample buffer. The relative distances of the proteins of the size marker are plotted semi-logarithmically against their known molecular weights. By comparison with the linear part of the generated graph or by a regression analysis, the molecular weight of an unknown protein can be determined by its relative mobility. Bands of proteins with glycosylations can be blurred. Proteins with many basic amino acids (e. g. histones) can lead to an overestimation of the molecular weight or even not migrate into the gel at all, because they move slower in the electrophoresis due to the positive charges or even to the opposite direction. Accordingly, many acidic amino acids can lead to accelerated migration of a protein and an underestimation of its molecular mass.\n\nThe SDS-PAGE in combination with a protein stain is widely used in biochemistry for the quick and exact separation and subsequent analysis of proteins. It has comparatively low instrument and reagent costs and is an easy-to-use method. Because of its low scalability, it is mostly used for analytical purposes and less for preparative purposes, especially when larger amounts of a protein are to be isolated.\n\nAdditionally, SDS-PAGE is used in combination with the western blot for the determination of the presence of a specific protein in a mixture of proteins - or for the analysis of post-translational modifications. Post-translational modifications of proteins can lead to a different relative mobility (i.e. a \"band shift\") or to a change in the binding of a detection antibody used in the western blot (i.e. a band disappears or appears).\n\nIn mass spectrometry of proteins, SDS-PAGE is a widely used method for sample preparation prior to spectrometry, mostly using in-gel digestion. In regards to determining the molecular mass of a protein, the SDS-PAGE is a bit more exact than an analytical ultracentrifugation, but less exact than a mass spectrometry or - ignoring post-translational modifications - a calculation of the protein molecular mass from the DNA sequence.\n\nIn medical diagnostics, SDS-PAGE is used as part of the HIV test and to evaluate proteinuria. In the HIV test, HIV proteins are separated by SDS-PAGE and subsequently detected by Western Blot with HIV-specific antibodies of the patient, if they are present in his blood serum. SDS-PAGE for proteinuria evaluates the levels of various serum proteins in the urine, e.g. Albumin, Alpha-2-macroglobulin and IgG.\n\nSDS-PAGE is the most widely used method for gel electrophoretic separation of proteins. Two-dimensional gel electrophoresis sequentially combines isoelectric focusing or BAC-PAGE with a SDS-PAGE. Native PAGE is used if native protein folding is to be maintained. For separation of membrane proteins, BAC-PAGE or CTAB-PAGE may be used as an alternative to SDS-PAGE. For electrophoretic separation of larger protein complexes, agarose gel electrophoresis can be used, e.g. the SDD-AGE. Some enzymes can be detected via their enzyme activity by zymography.\n\nWhile being one of the more precise and low-cost protein separation and analysis methods, the SDS-PAGE denatures proteins. Where non-denaturing conditions are necessary, proteins are separated by a native PAGE or different chromatographic methods with subsequent photometric quantification, for example affinity chromatography (or even tandem affinity purification), size exclusion chromatography, ion exchange chromatography. Proteins can also be separated by size in a tangential flow filtration or a ultrafiltration. Single proteins can be isolated from a mixture by affinity chromatography or by a pull-down assay. Some historically early and cost effective but crude separation methods usually based upon a series of extractions and precipitations using kosmotropic molecules, for example the ammonium sulfate precipitation and the polyethyleneglycol precipitation.\n\nIn 1948, Arne Tiselius was awarded the Nobel Prize in Chemistry for the discovery of the principle of electrophoresis as the migration of charged and dissolved atoms or molecules in an electric field. The use of a solid matrix (initially paper discs) in a zone electrophoresis improved the separation. The discontinuous electrophoresis of 1964 by L. Ornstein and B. J. Davis made it possible to improve the separation by the stacking effect. The use of cross-linked polyacrylamide hydrogels, in contrast to the previously used paper discs or starch gels, provided a higher stability of the gel and no microbial decomposition. The denaturing effect of SDS in continuous polyacrylamide gels and the consequent improvement in resolution was first described in 1965 by David F. Summers in the working group of James E. Darnell to separate poliovirus proteins. The current variant of the SDS-PAGE was described in 1970 by Ulrich K. Laemmli and initially used to characterise the proteins in the head of bacteriophage T4.\n\n"}
{"id": "16122539", "url": "https://en.wikipedia.org/wiki?curid=16122539", "title": "Self-reference puzzle", "text": "Self-reference puzzle\n\nA self-reference puzzle is a type of logical puzzle where the question in the puzzle refers to the attributes of the puzzle itself.\nA common example is that a \"fill in the blanks\" style sentence is given, but what is filled in the blanks can contribute to the sentence itself. An example is \"There are _____ e's in this sentence.\", for which a solution is \"eight\" (since including the \"eight\", there are 8 e's in the sentence).\n\n"}
{"id": "4106285", "url": "https://en.wikipedia.org/wiki?curid=4106285", "title": "Self-referential encoding", "text": "Self-referential encoding\n\nEvery day, people are presented with endless amounts of information, and in an effort to help keep track and organize this information, people must be able to recognize, differentiate and store information. One way to do that is to organize information as it pertains to the self. The overall concept of self-reference suggests that people interpret incoming information in relation to themselves, using their self-concept as a background for new information. Examples include being able to attribute personality traits to oneself or to identify recollected episodes as being personal memories of the past. The implications of self-referential processing are evident in many psychological phenomena. For example, the \"cocktail party effect\" notes that people attend to the sound of their names even during other conversation or more prominent, distracting noise. Also, people tend to evaluate things related to themselves more positively (This is thought to be an aspect of implicit self-esteem). For example, people tend to prefer their own initials over other letters. The self-reference effect (SRE) has received the most attention through investigations into memory. The concepts of self-referential encoding and the SRE rely on the notion that relating information to the self during the process of encoding it in memory facilitates recall, hence the effect of self-reference on memory. In essence, researchers have investigated the potential mnemonic properties of self-reference.\n\nResearch includes investigations into self-schema, self-concept and self-awareness as providing the foundation for self-reference's role in memory. Multiple explanations for the self-reference effect in memory exist, leading to a debate about the underlying processes involved in the self-reference effect. In addition, through the exploration of the self-reference effect, other psychological concepts have been discovered or supported, including simulation theory and the effect.\nAfter researchers developed a concrete understanding of the self-reference effect, many expanded their investigations to consider the self-reference effect in particular groups like those with autism spectrum disorders or those experiencing depression.\n\nSelf-knowledge can be categorized by structures in memory or schemata. A self-schema is a set of facts or beliefs that one has about themselves. For any given trait, an individual may or may not be \"schematic\"; that is, the individual may or may not think about themselves as to where they stand on that trait. For example, people who think of themselves as very overweight or who identify themselves to a greater extent based on their body weight would be considered \"schematic\" on the attribute of body weight. Thus, many everyday events, such as going out for a meal or discussing a friend's eating habits, could induce thoughts about the self. When people relate information to something that has to do with the self, it facilitates memory. Self-descriptive adjectives that fit into one's self-schema are easier to remember than adjectives not viewed as related to the self. Thus, the self-schema is an aspect of oneself that is used as an encoding structure that brings upon memory of information consistent with one's self-schema. Memories that are elaborate and well encoded are usually the result of self-referent correlations during the process of remembering. During the process of encoding, trait representations are encoded in long term memory either directly or indirectly. When they are directly encoded, it is in terms of relating to the self, and when it is indirectly encoded it is done through spouts of episodic information instead of information about the self.\n\nSelf-schema is often used as somewhat of a database for encoding personal data. The self-schema is also used by paying selective attention to outside information and internalizing that information more deeply in one's memory depending on how much that information relates to their schema. When self-schema is engaged, traits that go along with one's view of themselves are better remembered and recalled. These traits are also often recalled much better when processed with respect to the self. Similarly, items that are encoded with the self are based on one's self-schema. Processing the information should balance out when recalled for individuals who have a self-schema that goes along with the information.\n\nSelf-schemas do not necessarily only involve individual traits. People self-categorize at different levels that range from more personal to more social. Self-schemas have three main categories which play a role: the personal self, the relational self, and the collective self. The personal self deals with individual level characteristics, the relational self deals with intimate relationship partners, and the collective self deals with group identities, relating to self-important social groups to which one belongs (e.g., one's family or university). Information that is related to any type of self-schema, including group-related knowledge structures facilitates memory.\n\nIn order for the self to be an effective encoding mechanism, it must be a uniform, consistent, well-developed schema. It has been shown that identity exploration leads to the development of self-knowledge which facilitates self-judgments. Identity exploration led to shorter decision times, higher confidence ratings and more intrusions in memory tasks. Previous researchers hypothesized that words compatible with a person's self-schema are easily accessible in memory and are more likely than incompatible words to intrude on a schema-irrelevant memory task. In one experiment, when participants were asked to decide if certain adjectives were \"like me\" or \"not like me,\" they made the decisions faster when the words were compatible with their self-schema.\n\nHowever, despite the existence of the self-reference effect when considering schemata consistent adjectives, the connection between the self and memory can lead to a larger number of mistakes in recognition, commonly referred to as false alarms. Rogers et al. (1979) found that people are more likely to falsely recognize adjectives they had previously designated to be self-descriptive. Expanding on this, Strube et al. (1986) found that false alarms occurred more for self-schema consistent content, presumably because the presence of such words in the schema makes them more accessible in memory.\n\nIn addition to investigating the self-reference effect in regards to schemata consistent information, Strube et al. discussed how counter schemata information relates to this framework. They noted that the pattern of making correct decisions more rapidly did not hold when considering words that countered a person's self-schema, presumably because they were difficult to integrate into memory due to lack of a preexisting structure. That is, they lacked the organizational structure of encoding because they did not fall into the \"like me\" category, and elaboration would not work because prior connections to the adjective did not exist.\n\nTwo of the most common functions of the self receiving significant attention in research are the self-acting to organize the individual's understanding of the social environment, and the self functioning to regulate behavior through self-evaluation. The concept of self-awareness is considered to be the foundational principle for both functions of the self. Some research presents self-awareness in terms of self-focused attention whereas Hull and Levy suggest that self-awareness refers to the encoding of information based on its relevance to the self. Based on the latter interpretation of self-awareness, individuals must identify the aspects of situations that are relevant to themselves and their behavior will be shaped accordingly. Hull and Levy suggest that self-awareness corresponds to the encoding of information cued by self-symbolic stimuli, and examine the idea of self-awareness as a method of encoding. They structured an investigation that examined self-referent encoding in individuals with different levels of self-awareness, predicting that individuals with higher levels of self-consciousness would encode self-relevant information more deeply than other information, and that they would encode it more deeply than individuals with low levels of self-consciousness. The results of their investigation supported their hypothesis that self-focused attention is not enough to explain the role of self-awareness on attribution. Their results suggest that self-awareness leads to increased sensitivity to the situationally defined meanings of behavior, and therefore organizes the individual's understanding of the social environment. The research presented by Hull and Levy led to future research on the encoding of information associated with self-awareness.\n\nIn later research, Hull and colleagues examined the associations between self-referential encoding, self-consciousness and the extent to which a stimulus is consistent with self-knowledge. They first assumed that the encoding of a stimulus is facilitated if an individual's working memory already contains information consistent with the stimulus, and suggested that self-consciousness as an encoding mechanism relies on an individual's self-knowledge. It is known that situational and dispositional factors may activate certain pools of knowledge, moving them into working memory, and guiding the processing of certain stimulus information.\n\nIn order to better understand the idea of activating information in memory, Hull et al. presented an example of how information is activated. They referred to the sentence \"The robber took the money from the bank\". In English, the word bank has two applicable meanings in the context of this sentence (monetary institution and river shore). However, the monetary institution meaning of the word is more highly activated in this context due to the addition of the words robber and money to the sentence, because they are associatively relevant and therefore pull the monetary institution definition for bank into working memory. Once information is added to working memory, meanings and associations are more easily drawn. Therefore, the meaning of this example sentence is almost universally understood.\n\nIn reference to self-consciousness and self-reference, the connection between self-consciousness and self-referent encoding relies on such information activation. Research suggests that self-consciousness activates knowledge relating to the self, thereby guiding the processing of self-relevant information. Three experiments conducted by Hull and colleagues provided evidence that a manipulation of accessible self-knowledge impacts self-referent encoding based on the self-relevance of such information, individual differences in the accessibility of self-knowledge (self-consciousness) impacts perception, and a mediation relationship exists between self-consciousness and individual differences in self-referential encoding.\n\nSimilar to how self-awareness impacts the availability of self-knowledge and the encoding of self-relevant information, through the development of the self-schema, people develop and maintain certain personality characteristics leading to a variety of behavior patterns. Research has been done on the differences between Type A and Type B behavior patterns, focusing on how people in each group respond to environmental information and their interpretation of the performance of others and themselves. It has been found that Type A behavior is characterized by competitive achievement striving, time urgency and hostility, whereas Type B is usually defined as an absence of Type A characteristics. When investigating causal attributions for hypothetical positive and negative outcomes, Strube et al. found that Type A individuals were more self-serving, in that they took greater responsibility for positive than negative effects. Strube and colleagues argued that this could be a result of the fact that schema-consistent information is more easily remembered and the ease with which past successes and failures are recalled, determined by self-schema, would impact attributions. It is reasonable to believe that Type A's might recall successes more easily and hence be more self-serving.\n\nInfluential psychologists Craik and Lockhart laid the groundwork for research focused on self-referential encoding and memory. In 1972 they proposed their Depth of Processing framework which suggests that memory retention depends on how the stimulus material was encoded in memory. Their original research considered structural, phonemic, and semantic encoding tasks, and showed that semantic encoding is the best method to aid in recall. They asked participants to rate 40 descriptive adjectives on one of four tasks; Structural (Big font or small font?), Phonemic (Rhymes with xxx?), Semantic (Means same as xxx?), or Self-reference (Describes you?). This was then followed by an \"incidental recall task\". This is where participants are asked, without prior warning, to recall as many of the words they had seen as possible within a given time limit. Craik and Tulving's original experiment showed that structural and phonemic tasks lead only to \"shallow\" encoding, while the semantic tasks lead to \"deep\" encoding and resulted in better recall.\n\nHowever, in 1977, it was shown that self-relevant or self-descriptive encoding leads to even better recall than semantic tasks. Experts suggest that the call on associative memory required by semantic tasks is what provides the advantage over structural or phonemic tasks, but is not enough to surpass the benefit provided by self-referential encoding. The fact that self-reference was shown to be a stronger memory encoding method than semantic tasks is what led to more significant interest in the field One early and significant experiment aimed to place self-reference on Craik and Lockhart's depth of processing hierarchy, and suggested that self-reference was a more beneficial encoding method than semantic tasks. In this experiment, participants filled out self-ratings on 84 adjectives. Months later, these participants were revisited and were randomly shown 42 of those words. They then had to select the group of 42 \"revisited\" words out of the total original list. The researchers argued that if the \"self\" was involved in memory retrieval, participants would incorrectly recognize words that were more self-descriptive In another experiment, subjects answered yes or no to cue questions about 40 adjective in 4 tasks (structural, phonemic, semantic and self-referential) and later had to recall the adjectives. This experiment validated the strength of self-reference as an encoding method, and indicated it developed a stronger memory trace than the semantic task.\n\nResearchers are implementing a new strategy by developing different encoding tasks that enhance memory very similarly to self-referential encoding. Symons (1990) had findings that went against the norm when he was unable to find evidence of self-schematicity in the self-reference effect. Another finding was that when referencing gender and religion, there was a low memory recall when compared with referencing the self. A meta-analysis by Symons and Johnson (1997) showed self-reference resulting in better memory in comparison to tasks relying on semantic encoding or other-referent encoding. According to Symons and Johnson, self-referencing questions elicit elaboration and organization in memory, both of which creating a deeper encoding and thus facilitate memory.\n\nTheorists that favor the view that the self has a special role believe that the self leads to more in depth processing, leading to easier recall during self-reference tasks. Theorists also promote the self-schema as being one of the sole inhibitors that allow for recall from deep memory. Thorndyke and Hayes-Roth had the goal of focusing on the process made by the active memory schemata. Sex-typed individuals recall trait adjectives that go along with their sex role more quickly than trait adjectives that are not. During the process of free recall, these individuals also showed more patterns for gender clustering than other sexually typed individuals.\n\nAs research on self-referential encoding became more prolific, some psychologists took an opportunity to delineate specific self-referential encoding tasks. It is noted that descriptive tasks are those that require participants to determine if a stimulus word can be classified as \"self-descriptive.\" Autobiographical tasks are those that require participants to use the stimulus word as a cue to recall an autobiographical memory. Results from experiments that differentiated between these types of self-referential encoding found that they both produced better recall than semantic tasks, and neither was more advantageous than the other. However, research does suggest that the two types of self-referential encoding do rely on different processes to facilitate memory. In most experiments discussed, these types of self- referential encoding were not differentiated.\n\nIn a typical self-reference task, adjectives are presented and classified as either self-descriptive or not. For example, in a study by Dobson and Shaw, adjectives about the self that were preselected were given to the participants and they decide whether or not the adjectives are self-descriptive. The basis for making certain judgments, decisions, inferences and decisions is a self-referent encoding task. If two items are classified as self-descriptive there is no reason one trait would not be equally as easy to retrieve as the other on a self-reference task.\n\nWhile a significant amount of research supports the existence of the self-reference effect, the processes behind it are not well understood. However, multiple hypotheses have been introduced, and two main arguments have been developed: the elaborative processing hypothesis and the organizational processing hypothesis. Encodings in reference to the self are so elaborate because of the information one has about the self. Information encoded with the self is better remembered than information encoded with reference to something else.\n\nElaboration refers to the encoding of a single word by forming connections between it and other material already stored in memory. By creating these connections between the stimulus word and other material already in memory, multiple routes for retrieval of the stimulus word are formed. Based on the depth of processing framework, memory retention increases as elaboration during encoding increases. The Elaborative Processing Hypothesis would suggest that any encoding task that leads to the development of the most trace elaboration or associations is the best for memory retention. Additional research on the depth of processing hierarchy suggests that self-reference is the superior method of information encoding. The elaborative hypothesis would suggest this is because self-reference creates the most elaborate trace, due to the many links that can be made between the stimulus and information about the self already in memory.\n\nThe organizational processing hypothesis was proposed by Klein and Kihlstrom. This hypothesis suggests that encoding is best prompted by considering stimulus words in relation to one another. This thought process and relational thinking creates word to word associations. These inter-item associations are paths in memory that can be used during retrieval. Also, the category labels that define the relations between stimulus items can be used as item cues. Evidence of the organizational component of encoding is demonstrated through the clustering of words during recall. Word clustering during recall indicates that relational information was used to store the words in memory. Rogers, Kuiper and Kirker showed that self-referential judgments were more likely to encourage organization than semantic ones. Therefore, they suggested the self-reference effect was likely due to the organizational processing endured by self-referential encoding.\n\nStructural, phonemic and semantic tasks within the depth of processing paradigm require words to be considered individually, and lend themselves to an elaborative approach. As such, it can be argued that self-referential encoding is superior because it leads to an indirect division of words into categories: words that describe me versus words that do not. Due to this connection between self-reference and organizational processing, further research has been done on this area. Klein and Kihlstrom's research suggests first that, like previous research, self-reference led to better recall than semantic and structural encoding. Second, they found that self-referentially encoded words were more clustered in recall than words from other tasks, suggesting higher levels of organizational processing. From this they concluded that the organization, not encoding task, is what makes self-referential encoding superior \n\nPsychologists Einstein and Hunt showed that both elaborative processing and organizational processing facilitate recall. However, their research argues that the effectiveness of either approach depends on how related the stimulus words are to one another. A list of highly related stimulus words would be better encoded using the elaborative method. The relations between the words would be evident to subjects; therefore, they would not gain any additional pathways for retrieval by encoding the words based on their categorical membership. Instead, the other information gained through elaborative processing would be more beneficial. On the other hand, a list of stimulus words with little relation would be better stored to memory through the organizational method. Since the words have no obvious connection to one another, subjects would likely encode them individually, using an elaborative approach. Since relational information wouldn't be readily detected, focusing on it would add to memory by creating new traces for retrieval. Superior recall was better explained by a combination of elaboration and organization.\nUltimately, the exact processes behind self-referential encoding that makes it superior to other encoding tasks are still under debate. Research suggests that if elaborative processing is behind self-referential encoding, a self-referential task should have the same effect as an elaborative task, whereas if organizational processing underlies the self-reference effect self-referential encoding tasks should function like organizational tasks. To test this, Klein and Loftus ran a 3x2 study testing organizational, elaborative and self-referential encoding with lists of 30 related or unrelated words. When participants were asked to memorize the unrelated list, recall and clustering were higher for the organizational task, which produced almost equal results to the self-referential task, suggesting that has an organizational basis. For the list of related words, the elaborative task led to better recall and had matched results to the self-reference task, suggesting an elaborative basis. This research, then, suggests that the self-reference effect cannot be explained by a single type of processing. Instead, self-referential encoding must lead to information in memory that incorporates item specific and relational information.\n\nOverall, the SRE relies on the unique mnemonic aspects of the self. Ultimately, if the research is suggesting that the self has superior elaborative or organizational properties, information related to the self should be more easily remembered and recalled. The research presented suggests that self-referential encoding is superior because it promotes organization and elaboration simultaneously, and provides self-relevant categories that promote recall.\n\nThe field of social brain science is aimed at examining the neural foundations of social behavior. Neuroimaging and neuropsychology have led to the examination of neuroanatomy and its connection to psychological topics. Through this research, neuropsychologists have found a connection between social cognitive functioning and the medial prefrontal cortex (mPFC). In addition, the mPFC has been connected to reflection and introspection about personal mental states. Supporting these findings, it has been shown that damage to the mPFC is connected to impairments with self-reflection, introspection and daydreaming, as well as social competence, but not other areas of functioning. As such, the mPFC has been connected to self-referential processing.\n\nThe research discussed by those focusing on the neuroanatomy of self-referential processing included similar tasks to the memory and depth of processing research discussed previously. When participants were asked to judge adjectives based in whether or not they were self-descriptive, it was noted that the more self-relevant the trait, the stronger the activation of the mPFC. In addition, it was shown that the mPFC was activated during the appraisal of one's own personality traits, as well as during trait retrieval. One study showed that the more activity in the mPFC during self-referential judgments, the more likely the word was to be remembered on a subsequent surprise memory test. These results suggest that the mPFC is involved in both self-referential processing and in creating self-relevant memories.\n\nMedial prefrontal cortex (mPFC) activation occurs during processing of self-relevant information. When self-referent judgment is more relatable and less negative, the mFPC is activated. Finding support clear cut circuits that have high levels of activation when cognitive and emotional aspects of self-reflection are present. The caudate nucleus has not been associated with self-reference before, however, Fossati and colleagues found activity while participants were retrieving self-relevant trait adjectives. The ventral anterior cingulate cortex (vACC) is also a part of the brain that becomes activated when there are signs of self-referencing and processing. The vACC is activated when self-descriptive information is negative. There is also pCC (posterior cingulate cortex) activity seen in neuroimaging studies during self-referential processing.\n\nGiven all of the neurological support for the effect of self-reference on encoding and memory, there is still a debate in the psychological community about whether or not the self-reference effect signifies a special functional role played by the self in cognition. Generally, this question is met by people that have two opposing views on the processes behind self-reference. On one side of the debate, people believe that the self has special mnemonic abilities because it is a unique cognitive structure. On the other side, people support the arguments described above that suggest there is no special structure, but instead, the self-reference effect is simply a part of the standard depth of processing hierarchy. Since the overall hypothesis is the same for both sides of the debate, that self-relevant material leads to enhanced memory, it is difficult to test them using strictly behavioral measures. Therefore, PET and fMRI scans have been used to see the neural marker of self-referential mental activity.\n\nPrevious studies have shown that areas of the left prefrontal cortex are activated during semantic encoding. Therefore, if the self-reference effect works the same way, as part of the depth of processing hierarchy, the same brain region should be activated when judging traits related to the self. However, if the self has unique mnemonic properties, then self-referential tasks should activate brain regions distinct from those activated during semantic tasks. The field is still at is infancy, but future work on this hypothesis might help to settle the debate about the underlying processes of self-referential encoding.\n\nWhile not able to completely settle the debate over the foundation of self-referential processing, studies on the neurological aspect of personality trait judgments did lead to a related, significant result. It has been shown that judging personality traits about oneself and a close friend activated overlapping brain regions, and the activated regions have all been implicated in self-reference. Noting the similarity between making self-judgments and judgments about close others led to the introduction of the simulation theory of empathy. Simulation theory rests on the idea that one can make inferences about others by using the knowledge they have about themselves. In essence, the theory suggests that people use self-reflection to understand or predict the mental state of others. The more similar a person perceives another to be, the more active the mPFC has shown to be, suggesting more deep or intricate self-reference. However, this effect can cause people to make inaccurate judgments about others or to believe that their own opinions are representative of others in general. This misrepresentation is referred to as the false-consensus effect.\n\nIn addition to simulation theory, other expansions of the self-reference effect have been examined. Through studying the self, researchers have found that the self consists of many independent cognitive representations. For example, the personal self composed of individual characteristics is separate from the relational self which is based on relationships with significant others. These two forms of self are again separate from the collective self which corresponds to a particular group identity. Noting the existence of the collective self and the different group identities that combine to form such a self-representation led researchers to question if information stored in reference to a social group identity has the same effects in memory as information stored in reference to the individual self. In essence, researchers questioned if the self-reference effect can be extended to include situations where the self is more socially defined, producing a group-reference effect.\n\nPrevious research supports the idea that the group-reference effect should exist from a theoretical standpoint. First, the self-expansion model argues that individuals incorporate characteristics of their significant others (or other in-group members into the development of their self-concept. From this model, it is reasonable to conclude that characteristics that are common to both oneself and their significant others (or in-group members) would be more accessible. Second, the previous research discussed suggests that the self-reference effect is due to some combination of organizational, elaborative, mental cueing or evaluative properties of self-referential encoding tasks. Given that we have significant stores of knowledge about our social identities, and such collective identities provide an organizational framework, it is reasonable to assume that a group-reference task would operate similar to that of a self-reference task.\n\nIn order to test these claims, Johnson and colleagues aimed to test whether the self-reference effect generalized to group level identities. Their first study was structured to simply assess if group-reference influenced subsequent memory. In their experiment, they used membership at a particular university as the group of reference. They included group-reference, self-reference and semantic tasks. The experiment replicated the self-reference effect, consistent with previous research. In addition, evidence for a group-reference effect was found. Group-referenced encoding produced better recall than the semantic tasks, and the level of recall from the group-referenced task was not significantly different from the self-referenced task.\n\nDespite finding evidence of a group-reference effect, Johnson and colleagues pointed out that people identify with numerous groups, each with unique characteristics. Therefore, in order to reach conclusive evidence of a group-reference effect, alternative group targets need to be considered. In a second experiment by Johnson et al., the group of reference was modified to be the family of the individual. This group has fewer exemplars than the pool of university students, and affective considerations of the family as a group should be strong. No specific instructions or definitions were provided for family, allowing individuals to consider either the group as a whole (prototype) or specific exemplars (group). When the experiment was repeated using family as the group of reference, group-reference produced recall as much as self-reference. The mean number of recall for the group-reference was higher than self-reference. Participants indicated that they considered both the prototype and individual exemplars when responding to the questions, suggesting that the magnitude of the group-reference effect might not be dependent on the number of exemplars in the target group.\nBoth experiments presented by Johnson et al. found evidence for the group-reference effect. However, these conclusions are limited to the target groups of university students and family. Other research included gender (males and females) and religion (Jewish) as the reference groups and the group-reference effect on memory was not as evident. The group-reference recall for these two groups was not significantly more advantageous than the semantic task. Questioning what characteristics of reference groups that lead to the group-reference effect, a meta-analysis of all four group-reference conditions was performed. This analysis found that self-reference emerged as the most powerful encoding device; however, evidence was found to support the existence of a group-reference effect. The size of the reference groups and number of specific, individual exemplars was hypothesized to influence the existence of the group-reference effect. In addition, accessibility and level of knowledge about group members may also impact such an effect. So, while university students is a much larger group than family, individual exemplars may be more readily accessible than those in a religious group. Similarly, different cognitive representations were hypothesized to influence the group-reference effect. When a larger group is considered, people may be more likely to consider a prototype which may lead to fewer elaborations and cues later on. Smaller groups may lead to relying on the prototype and specific exemplars. Finally, desirability judgments that influence later processing may be influenced by self-reference and certain group-reference tasks. Individuals may be more sensitive to evaluative implications for the personal self and some group identities, but not others.\n\nGroups are also a major part of the self; therefore we attribute the role that different groups play in our self-concept also play a role in the self-reference effect. We process information about group members similarly to how we process for ourselves. Recall of remarks referencing our home and our self and group to familiarity of those aspects of our self. Reference to the self and social group and the identity that comes along with being a part of a social group are equally affective for memory. This is especially true when the groups are small, rather than large.\n\nUltimately, the group-reference effect provides evidence to explain the tendency to notice or pay attention to and remember statements made in regard to our home when traveling in a foreign place. Considering the proposal that groups form part of the self, this phenomenon can be considered an extension of the self-reference effect. Similar to the memorable nature of references to a person's individual self, references to social identities are seemed to be privileged in memory as well.\n\nOnce the foundation of research on self-referential encoding was established, psychologists began to explore how the concept applied to different groups of people, and connected to different phenomena.\n\nIndividuals diagnosed with autism spectrum disorders (ASDs) can display a wide range of symptoms. Some of the most common characteristics of individuals with ASDs include impairments with social functioning, language and communication difficulties, repetitive behaviors and restricted interests. In addition, it is often noted that these individuals are more \"self-focused.\" That is, they have difficulty seeing things from another's perspective. Despite being self-focused, though, research has shown that individuals with ASD's often have difficulty identifying or describing their emotions or the emotions of others. When asked to describe their daily experiences, responses from individuals on the autism spectrum tended to focus more on physical descriptions rather than mental and emotional states. In regards to their social interactions and behavior differences, it is thought that these individuals lack top down control, and therefore, their bottom up decisions remain unchecked. This simply suggests that these individuals cannot use their prior knowledge and memory to make sense of new input, but instead react to each new input individually, compiling them to make a whole picture \n\nNoting the difficulty individuals with ASDs experience with self-awareness, it was thought that they might have difficulty with self-related memory processes. Psychologists questioned if these individuals would show the typical self-reference effect in memory. In one Depth of Processing Study, participants were asked questions about the descriptiveness of certain stimulus words. However, unlike previous DOP studies that focused on phonemic, structural, semantic and self-referential tasks, the tasks were altered for this experiment. To test the referential abilities of individuals with ASD's, the encoding tasks were divided into: \"the self,\" asking to what extent a stimulus word described oneself, \"similar close other,\" asking to what extent a stimulus word was descriptive of one's best friend, \"dissimilar non-close other,\" asking to what extent a stimulus word was descriptive of Harry Potter, and a control group that was asked to determine the number of syllables in each word. Following these encoding tasks, participants were given thirty minutes before a surprise memory task. It was found that individuals with ASD's had no impairment in memory for words encoded in the syllable or dissimilar non-close other condition. However, they had decreased memory for words related to the self.\n\nTherefore, while research suggests that self-referentially encoded information is encoded more deeply than other information, the research on individuals with ASD's showed no advantage for memory recognition with self-reference tasks over semantic encoding tasks. This suggests that individuals with ASD's don't preferentially encode self-relevant information. Psychologists have investigated the biological basis for the decreased self-reference effect among individuals with Autism Spectrum Disorders and have suggested that it may be due to less specialized neural activity in the mPFC for those individuals. However, while individuals with ASD's showed smaller self-reference effects than the control group, some evidence of a self-reference effect was evident in some cases. This indicates that self-referent impairments are a matter of degree, not total absence.\n\nLombardo and his colleagues measured empathy among individuals with ASD's, and showed that these individuals scored lower than the control group on all empathy measures. This may be a result of the difficulty for these individuals to understand or take the perspective of others, in conjunction with their difficulty identifying emotions. This has implications for simulation theory, because these individuals are unable to use their self-knowledge to make conclusions about similar others.\n\nUltimately, the research suggests that people with ASD's might benefit from being more self-focused. The better their ability to reflect on themselves, the better the can mentalize with others.\n\nThere are three possible relations between cognitive processes and anxiety and depression. The first is whether cognitive processes are actually caused by the onset of clinically diagnosed symptoms of major depression or just generalized sadness or anxiousness. The second is whether emotional disorders such as depression and anxiety are able to be considered as caused by cognitions. And the third is whether different specific cognitive processes are able to be considered associates of different disorders. Kovacs and Beck (1977) posited a schematic model of depression where an already depressed self was primed by outside prompts that negatively impacted cognitive illusions of the world in the eye of oneself. These prompts only led participants to a more depressive series of emotions and behavior. The results from the study done by Derry and Kuiper supported Beck's theory that a negative self-schema is present in people, especially those with depressive disorder. Depressed individuals attribute depressive adjectives to themselves more than nondepressive adjectives. Those suffering from a more mild case of depression have trouble deciphering between the traits of themselves and others which results in a loss of their self-esteem and their negative self-evaluation. A depressive schema is what causes the negativity reported by those suffering from depression. Kuiper and Derry found that self-referent recall enhancement was limited only to nondepressed content.\n\nGenerally, self-focus is association with negative emotions. In particular private self-focus is more strongly associated with depression than public self-focus. Results from brain-imaging studies shows\nthat during self-referential processing, those with major depressive disorder show greater activation in the medial prefrontal cortex, suggesting that depressed individuals may be exhibiting greater cognitive control than\nnon-depressed individuals when processing self-relevant information.\n"}
{"id": "858507", "url": "https://en.wikipedia.org/wiki?curid=858507", "title": "Self-referential humor", "text": "Self-referential humor\n\nSelf-referential humor, also known as self-reflexive humor or meta humor, is a type of comedic expression that—either directed toward some other subject, or openly directed toward itself—intentionally alludes to the very person who is expressing the humor in a comedic fashion, or to some specific aspect of that same comedic expression. Self-referential humor expressed discreetly and surrealistically is a form of bathos. In general, self-referential humor often uses hypocrisy, oxymoron, or paradox to create a contradictory or otherwise absurd situation that is humorous to the audience. \n\nSelf-referential humor is sometimes combined with breaking the fourth wall to explicitly make the reference directly to the audience, or make self-reference to an element of the medium that the characters should not be aware of.\n\nOld Comedy of Classical Athens is held to be the first—in the extant sources—form of self-referential comedy. Aristophanes, whose plays form the only remaining fragments of Old Comedy, used fantastical plots, grotesque and inhuman masks and status reversals of characters to slander prominent politicians and court his audience's approval.\n\nRAS syndrome refers to the redundant use of one or more of the words that make up an acronym or initialism with the abbreviation itself, thus in effect repeating one or more words. However, \"RAS\" stands for Redundant Acronym Syndrome; therefore, the full phrase yields \"Redundant Acronym Syndrome syndrome\" and is self-referencing in a comical manner. It also reflects an excessive use of TLAs (Three Letter Acronyms).\n\nMeta has come to be used, particularly in art, to refer to something that is self-referential. Popularised by Douglas Hofstadter who wrote several books on himself and the subject of self-reference, meta-jokes are a popular form of humor.\n\n"}
{"id": "3423601", "url": "https://en.wikipedia.org/wiki?curid=3423601", "title": "Stumpers-L", "text": "Stumpers-L\n\nThe Stumpers-L electronic mailing list, was a resource available for librarians and others to discuss reference questions which they were unable to answer using available resources. It was succeeded by the similar Project Wombat.\n\nStumpers-L began in 1992, created by Ann Feeney, a library school graduate student at Rosary College in River Forest, Illinois, in the United States. It was moved to Concordia University, Chicago, then back to Rosary, which was then renamed Dominican University. From 2002 to 2005 it was maintained by the Dominican University Graduate School of Library and Information Science program. At the end of 2005 Dominican University ceased hosting the list. A replacement list, known as Project Wombat, commenced in January 2006, and is hosted by Project Gutenberg.\n\nOriginally the Stumpers-L archive was a gopher resource, but migrated to the World Wide Web once the web became more universally used in the mid-1990s.\n\nTypical Stumpers-L topics include:\n\nA book of Stumpers-L questions and answers was published in 1998 by Random House, edited by Fred Shapiro of Yale and titled \"Stumpers! Answers to Hundreds of Questions That Stumped The Experts\" (). Shapiro was an active member; other prominent members include Barbara and David P. Mikkelson, the co-editors of \"Snopes.com.\n\nThe unofficial mascot of the Stumpers-L list is the wombat.\n\n"}
{"id": "26681002", "url": "https://en.wikipedia.org/wiki?curid=26681002", "title": "Text annotation", "text": "Text annotation\n\nText Annotation is the practice and the result of adding a note or gloss to a text, which may include highlights or underlining, comments, footnotes, tags, and links. Text annotations can include notes written for a reader's private purposes, as well as shared annotations written for the purposes of collaborative writing and editing, commentary, or social reading and sharing. In some fields, text annotation is comparable to metadata insofar as it is added post hoc and provides information about a text without fundamentally altering that original text. Text annotations are sometimes referred to as marginalia, though some reserve this term specifically for hand-written notes made in the margins of books or manuscripts. Annotations are extremely useful and help to develop knowledge of English literature.\n\nThis article covers both private and socially shared text annotations, including hand-written and information technology-based annotation. For information on annotation of Web content, including images and other non-textual content, see also Web annotation.\n\nText annotation may be as old as writing on media, where it was possible to produce an additional copy with a reasonable effort. It became a prominent activity around 1000 AD in Talmudic commentaries and Arabic rhetorics treaties. In the Medieval era, scribes who copied manuscripts often made marginal annotations that then circulated with the manuscripts and were thus shared with the community; sometimes annotations were copied over to new versions when such manuscripts were later recopied.\n\nWith the rise of the printing press and the relative ease of circulating and purchasing individual (rather than shared) copies of texts, the prevalence of socially shared annotations declined and text annotation became a more private activity consisting of a reader interacting with a text. Annotations made on shared copies of texts (such as library books) are sometimes seen as devaluing the text, or as an act of defacement. Thus, print technologies support the circulation of annotations primarily as formal scholarly commentary or textual footnotes or endnotes rather than marginal, handwritten comments made by private readers, though handwritten comments or annotations were common in collaborative writing or editing.\n\nComputer-based technologies have provided new opportunities for individual and socially shared text annotations that support multiple purposes, including readers’ individual reading goals, learning, social reading, writing and editing, and other practices. Text annotation in Information Technology (IT) systems raises technical issues of access, linkage, and storage that are generally not relevant to paper-based text annotation, and thus research and development of such systems often addresses these areas.\n\nText annotations can serve a variety of functions for both private and public reading and communication practices. In their article \"From the Margins to the Center: The Future of Annotation,\" scholars Joanna Wolfe and Christine Neuwirth identify four primary functions that text annotations commonly serve in the modern era, including: (1)\"facilitat[ing] reading and later writing tasks,\" which includes annotations that support reading for both personal and professional purposes; (2)\"eavesdrop[ping] on the insights of other readers,\" which involves sharing of annotations; (3)\"provid[ing] feedback to writers or promote communication with collaborators,\" which can include personal, professional, and education-related feedback; and (4)\"call[ing] attention to topics and important passages,\" for which scholarly annotations, footnotes, and call-outs often function. Regarding the ways that annotations can support individual reading tasks, Catherine Marshall points out that the ways that readers annotate texts depends on the purpose, motivation, and context of reading. Readers may annotate to help interpret a text, to call attention to a section for future reference or reading, to support memory and recall, to help focus attention on the text as they read, to work out a problem related to the text, or create annotations not specifically related to the text at all.\n\nEducational research in text annotation has examined the role that both private and shared text annotations can play in supporting learning goals and communication. Much educational research examines how students’ private annotation of texts supports comprehension and memory; for example, research indicates that annotating texts causes more in-depth processing of information, which results in greater recall of information.\n\nOther areas of educational research investigate the benefits of socially shared text annotations for collaborative learning, both for paper-based and IT-based annotation sharing. For example, studies by Joanna Wolfe have investigated the benefits of exposure to others’ annotations on student readers and writers. In a 2000 study, Wolfe found that exposing students to others’ annotations influenced their perceptions of the annotators, which in turn shaped their responses to the material and their written products. In a later study, Wolfe found that viewing others’ written comments on a paper text, especially pairs of annotations that present opposing responses to the text, can help students engage in the type of critical reading and stance-taking necessary for effective argumentative writing.\n\nWhile shared annotations can benefit individual readers, it is important to note that, \"since the 1920s, literacy theory has increasingly emphasized the importance of social factors in the development of literacy.\" Thus, shared annotations can not only help one to better understand the content of a particular text, but may also aid in the acquirement of literacy skills. For example, a mother may leave marks inside a book to draw the attention of her child to a particular theme or concept; thanks to the development of audio annotations, parents may now leave notes for children who are just starting to read and may struggle with textual annotations.\n\nMore recent research in the effects of shared text annotations has focused on the learning applications for web-based annotation systems, some of which were developed based on design recommendations from studies outlined above. For example, Ananda Gunawardena, Aaron Tan, and David Kaufer conducted a pilot study to examine whether annotating documents in Classroom Salon, a web-based annotation and social reading platform, encouraged active reading, error detection, and collaboration in a computer science course at Carnegie Mellon University. This study suggested a correlation between students’ overall performance in the course and their ability to identify errors in a text that they annotated in Classroom Salon; it also found that students were likely to change their annotations in response to annotations made by others in the course.\n\nSimilarly, the web-based annotation tool HyLighter was used in a first-year writing course and shown to improve the development of students’ mental models of texts, including supporting reading comprehension, critical thinking, and the ability to develop a thesis. The collaboration with peers and experts around a shared text improved these skills and brought the communities’ understanding closer together.\n\nA meta-analysis of empirical studies into the higher-education uses of social annotation (SA) tools indicates such tools have been tested in several courses, among them English, sport psychology, and hypermedia. Studies have indicated that social annotation functions, including commenting, information sharing, and highlighting, can support instruction designed to foster collaborative learning and communication, as well as reading comprehension, metacognition, and critical analysis. Several studies indicated that students enjoyed using social annotation tools, and that it improved motivation in the course.\n\nText annotations have long been used in writing and revision processes as a way for reviewers to suggest changes and communicate about a text. In book publishing, for example, the collaboration of authors and editors to develop and revise a manuscript frequently involves exchanges of both in-line revisions or notes as well as marginal annotations. Similarly, copyeditors often make marginal annotations or notes that explain or suggest revisions or are directed at the author as questions or suggestions (commonly called \"queries\"). Asynchronous collaborative writing and document development often depend on text annotations as a way not only to suggest revisions but also to exchange ideas during document development or to facilitate group decision making, though such processes are often complicated by the use of different communication technologies (such as phone calls or emails as well as document sharing) for distinct tasks. Text annotations can also function to allow group or community members to communicate about a shared text, such as a doctor annotating a patient's chart.\n\nMuch research into the functionality and design of collaborative IT-based writing systems, which often support text annotation, has occurred in the area of computer-supported cooperative work.\n\nResearch in the design and development of annotation systems uses specific terminology to refer to distinct structural components of annotations and also distinguishes among options for digital annotation displays.\n\nThe structural components of any annotation can be roughly divided into three primary elements: a \"body\", an \"anchor\", and a \"marker\". The body of an annotation includes reader-generated symbols and text, such as handwritten commentary or stars in the margin. The anchor is what indicates the extent of the original text to which the body of the annotation refers; it may include circles around sections, brackets, highlights, underlines, and so on. Annotations may be anchored to very broad stretches of text (such as an entire document) or very narrow sections (such as a specific letter, word, or phrase). The marker is the visual appearance of the anchor, such as whether it is a grey underline or a yellow highlight. An annotation that has a body (such as a comment in the margin) but no specific anchor has no marker.\n\nIT-based annotation systems utilize a variety of display options for annotations, including:\nAnnotation interfaces may also allow highlighting or underlining, as well as threaded discussions. Sharing and communicating through annotations anchored to specific documents is sometimes referred to as \"anchored discussion\".\n\nIT-based annotation systems include standalone and client-server systems. In the 1980s and 1990s, a number of such systems were built in the context of libraries, patent offices, and legal text processing. Their design led researchers to produce taxonomies of annotation forms. Text annotation research has taken place at several institutions, including Xerox research centers in Palo Alto and Grenoble (France), the Hitachi Central Research Lab (in particular for annotation of patents), and in relation with the construction of the new French National Library between 1989 and 1995 at the Institut de Recherche en Informatique de Toulouse and in the company AIS (Advanced Innovation Systems).\n\nAnnotation functionality has been present in text processing software for many years through inline notes displayed as pop-ups, footnotes, and endnotes; however, it is only recently that functionality for displaying annotations as marginalia has appeared in programs such as OpenOffice.org/LibreOffice Writer and Microsoft Word. Personal or standalone annotation include word processing software that supports embedded or anchored text annotations as well as Adobe Acrobat, which in addition to commenting allows highlights, stamps, and other types of markup.\n\nTim Berners-Lee had already implemented the concept of directly editing web documents in 1990 in WorldWideWeb, the first web browser, but later ported versions removed this collaborative ability. An early version of NCSA Mosaic in 1993 also included a collaborative annotation capability, though it was quickly removed. Web Distributed Authoring and Versioning, WebDAV, was then reintroduced as an extension.\n\nA different approach to distributed authoring consists in first gathering many annotations from a wide public, and then integrate them all in order to produce a further version of a document. This approach was pioneered by Stet, the system put in place to gather comments on drafts of version 3 of the GNU General Public License. This system arose after a specific requirement, which it served egregiously, but was not so easily configurable as to be convenient for annotating any other document on the web. The co-ment system uses annotation interface concepts similar to Stet's, but it is based on an entirely new implementation, using Django/Python on the server side and various AJAX libraries such as JQuery on the client side. Both Stet and co-ment are licensed under the GNU Affero General Public License.\n\nSince 2011, the non-profit Hypothes Is Project has offered the free, open web annotation service Hypothes.is. The service features annotation via a Chrome extension, bookmarklet or proxy server, as well as integration into a LMS or CMS. Both webpages and PDFs can be annotated. Other web-based text annotation systems are collaborative software for distributed text editing and versioning, which also feature annotation and commenting interfaces. For example, HyLighter supports synchronous and asynchronous interactions, general commenting, comment tagging, threaded discussions and comment filtering. Other annotation tools under these category are more focused on NLP tasks as Named-entity recognition, relationship extraction or normalization. Some tools support manual tagging of data or automatic annotations via supervised learning.\n\nSpecialized Web-based text annotations exist in the context of scientific publication, either for refereeing or post-publication. The on-line journal PLoS ONE, published by the Public Library of Science, has developed its own Web-based system where scientists and the public can comment on published articles. The annotations are displayed as pop-ups with an anchor in the text.\n\n\n"}
{"id": "5928902", "url": "https://en.wikipedia.org/wiki?curid=5928902", "title": "The Map Library", "text": "The Map Library\n\nThe Map Library is a project of The Map Maker Trust charity, and supported by Map Maker Ltd., for the supplying of free GIS data. The project website also hosts free conversion software for raster and vector files. As of November 2008, the only data sets available were for the continents of Africa and Central America.\n\nFrom the website...\n\nThe project data is managed in part by two pieces of software, each supporting different file formats and conversions.\n\n\nThe project uses data from NASA mapping projects, Famine Early Warning Systems Network, and the National Geospatial-Intelligence Agency.\n\n"}
{"id": "39726999", "url": "https://en.wikipedia.org/wiki?curid=39726999", "title": "The Rough Guide to True Crime", "text": "The Rough Guide to True Crime\n\nThe Rough Guide to True Crime is a non-fiction paperback reference guide to national and international true crime cases by American crime writer Cathy Scott. It was released in the UK and US in August 2009 by Penguin Books through its Rough Guides imprint.\n\n\"The Rough Guide to True Crime\" is a compilation of a variety of cases, including historic crimes, with sections broken down by the type of offenses and who committed them. It includes black-and-white photos as illustration. Psychological profiles are included throughout by forensic expert Dr. Louis B. Schlesinger, who explains the psychology of serial killers, murderers, hit men and burglars. The book features serial killer Jeffrey Dahmer, mob hitman Richard \"The Iceman\" Kuklinski, John Wayne Glover \"The Granny Killer,\" and British \"Doctor of Death\" Harold Shipman.\n\nScott's story from \"The Rough Guide to True Crime\" about mob enforcer Herbert Blitzstein was selected for inclusion in the July 2012 retrospective of crime writing, \"Masters of True Crime: Chilling Stories of Murder and the Macabre\".\n\nThe author appeared on BlogTalkRadio's \"True Murder\" show and described some of the crimes included in the book that were committed in the 19th century as \"a different time in America, where people like Billy the Kid could walk in and just rob a bank\" and get away with it. And while \"there was nothing glamorous about what they did, they are a part of lore.\"\n\nThe book was featured at BookExpo America 2009's trade fair in DK Publishing's booth in New York City.\n\nIn a review, \"True Crime Book Reviews\" wrote, \"From the Moors murders and Harold Shipman, to the murder of 2pac, this guide illuminates the psychology in play behind the most intriguing crimes in history, from the absurd to the appalling. \"The Rough Guide to True Crime\" explores the best of the haunting genre of True Crime.\"\n\n\n"}
{"id": "151042", "url": "https://en.wikipedia.org/wiki?curid=151042", "title": "Viz.", "text": "Viz.\n\nThe abbreviation viz. (or viz without a full stop), short for the Latin , which itself is a contraction from Latin of videre licet meaning \"it is permitted to see\", is used as a synonym for \"namely\", \"that is to say\", \"to wit\", or \"as follows\". It is typically used to introduce examples or further details to illustrate a point. For example: \"all types of data viz. text, audio, video, pictures, graphics etc. can be transmitted through networking\".\n\n\"Viz.\" is shorthand for the adverb \"\". It uses Tironian notes, a system of Latin shorthand developed . It comprises the first two letters, \"vi\", followed by the last two, \"et\", using the z-shaped Tironian \"et\", historically written ⁊, a common contraction for \"et\" in Latin shorthand in Ancient Rome and medieval Europe.\n\n\"Viz.\" is an abbreviation of \"videlicet\", which itself is a contraction from Latin of \"videre licet\" meaning \"it is permitted to see\". The spelling \"viz.\" is the continuation of an abbreviation using Tironian \"et\" (\"vi⁊\"), the \"z\" replacing the \"⁊\" once the latter had fallen out of common use.\n\nIn contradistinction to i.e. and e.g., viz. is used to indicate a detailed description of something stated before, and when it precedes a list of group members, it implies (near) completeness.\n\n\n\nA similar expression is scilicet (from earlier \"scire licet\"), abbreviated as \"sc.\", which is Latin for \"it is permitted to know\". \"Sc.\" provides a parenthetic clarification, removes an ambiguity, or supplies a word omitted in preceding text,  while \"viz.\" is usually used to elaborate or detail text which precedes it.\n\nIn legal usage, \"scilicet\" appears abbreviated as \"ss.\" or, in a caption, as §, where it provides a statement of venue and is read as \"to wit\". \"Scilicet\" can be read as \"namely\", \"to wit\", or \"that is to say\", or pronounced or anglicized as .\n\n"}
{"id": "58757675", "url": "https://en.wikipedia.org/wiki?curid=58757675", "title": "William Chaffers", "text": "William Chaffers\n\nWilliam Chaffers (28 September 1811 – 12 April 1892) was an English antiquary and writer of reference works on hallmarks, and marks on ceramics. His \"Marks and Monograms on Pottery and Porcelain\", first published in 1863, has appeared in many later editions.\n\nChaffers was the son of William Chaffers and wife Sarah, and was born in Watling Street, London, in 1811; he was descended from a brother of Richard Chaffers (1731–1765), a manufacturer of Liverpool porcelain. He was educated at Margate and at Merchant Taylors' School, where he was entered in 1824.\n\nHe was attracted to antiquarian studies while a clerk in the city of London, by the discovery of Roman and medieval antiquities in the foundations of the Royal Exchange during 1838–9. At the same time he began to concentrate attention upon the study of gold and silver plate and ceramics, especially in regard to the official and other marks by which dates and places of fabrication can be distinguished. In 1863 Chaffers published two important works:\n\nOther publications are \"The Keramic Gallery\", in 2 volumes, with 500 illustrations (1872); a handbook abridged from \"Marks and Monograms\" (1874); \"Gilda Aurifabrorum\", a history of goldsmiths and plate workers and their marks (1883); also a priced catalogue of coins, and other minor catalogues.\n\nHis reputation was furthered in organizing exhibitions of art treasures, at Manchester in 1857, South Kensington in 1862, Leeds in 1869, Dublin in 1872, Wrexham in 1876, and Hanley (at the great Staffordshire exhibition of ceramics) in 1890. Chaffers was elected Fellow of the Society of Antiquaries of London in 1843, and he was a frequent contributor to \"Archæologia\", to \"Notes and Queries\", and to various learned periodicals upon the two subjects of which he had particular knowledge.\n\nIn 1841 he married Charlotte Matilda, daughter of John Hewett. About 1870 he retired from Fitzroy Square to a house in Willesden Lane, and later moved to West Hampstead, where he died on 12 April 1892.\n\nAttribution\n"}
