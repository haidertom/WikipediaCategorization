{"id": "3556316", "url": "https://en.wikipedia.org/wiki?curid=3556316", "title": "Apples and oranges", "text": "Apples and oranges\n\nA comparison of apples and oranges occurs when two items or groups of items are compared that cannot be practically compared.\n\nThe idiom, \"comparing apples and oranges\", refers to the apparent differences between items which are popularly thought to be incomparable or incommensurable, such as apples and oranges. The idiom may also be used to indicate that a false analogy has been made between two items, such as where an \"apple\" is faulted for not being a good \"orange\".\n\nThe idiom is not unique to English. In Quebec French, it may take the form \"comparer des pommes avec des oranges\" (to compare apples and oranges), while in European French the idiom says \"comparer des pommes et des poires\" (to compare apples and pears). In Latin American Spanish, it is usually \"comparar papas y boniatos\" (comparing potatoes and sweet potatoes) or commonly for all varieties of Spanish \"comparar peras con manzanas\" (comparing pears and apples). In some other languages the term for 'orange' derives from 'apple', suggesting not only that a direct comparison between the two is possible, but that it is implicitly present in their names. Fruit other than apples and oranges can also be compared; for example, apples and pears are compared in Danish, Dutch, German, Spanish, Swedish, Croatian, Czech, Romanian, Hungarian, Italian, Slovene, Luxembourgish, Serbian, and Turkish. In fact, in the Spanish-speaking world, a common idiom is \"sumar peras con manzanas\", that is, \"to add pears and apples\"; the same thing applies in Italian (\"sommare le mele con le pere\") and Romanian (\"a aduna merele cu perele\"). In Portuguese, the expression is \"comparar laranjas com bananas\" (compare orange to banana). In Czech, the idiom \"míchat jablka s hruškami\" literally means 'to mix apples and pears'.\n\nSome languages use completely different items, such as the Serbian \"Поредити бабе и жабе\" (comparing grandmothers and toads), or the Romanian \"baba şi mitraliera\" (the grandmother and the machine gun); \"vaca şi izmenele\" (the cow and the longjohns); or \"țiganul şi carioca\" (the gypsy and the marker), or the Welsh \"mor wahanol â mêl a menyn\" (as different as honey and butter), while some languages compare dissimilar properties of dissimilar items. For example, an equivalent Danish idiom, \"Hvad er højest, Rundetårn eller et tordenskrald?\" means \"What is highest, the Round Tower or a thunderclap?\", referring to the size of the former and the sound of the latter. In Russian, the phrase \"сравнивать тёплое с мягким\" (to compare warm and soft) is used. In Argentina, a common question is \"¿En qué se parecen el amor y el ojo del hacha?\" (What do love and the eye of an axe have in common?) and emphasizes dissimilarity between two subjects; in Colombia, a similar (though more rude) version is common: \"confundir la mierda con la pomada\" (to confuse shit with ointment). In Polish, the expression \"co ma piernik do wiatraka?\" is used, meaning \"What has (is) gingerbread to a windmill?\". In Chinese, a phrase that has the similar meaning is 风马牛不相及 (fēng mǎ niú bù xiāng jí), literally meaning \"horses and cattles won't mate with each other\", and later used to describe things that are totally unrelated and incomparable.\n\nA number of more exaggerated comparisons are sometimes made, in cases in which the speaker believes the two objects being compared are radically different. For example, \"oranges with orangutans\", \"apples with dishwashers\", and so on. In English, different fruits, such as pears, plums, or lemons are sometimes substituted for oranges in this context.\n\nSometimes the two words sound similar, for example, Romanian \"merele cu perele\" (apples and pears) and the Hungarian \"szezont a fazonnal\" (the season with the fashion).\n\nAt least two tongue-in-cheek scientific studies have been conducted on the subject, each of which concluded that apples can be compared with oranges fairly easily and on a low budget and the two fruits are quite similar.\n\nThe first study, conducted by Scott A. Sandford of the NASA Ames Research Center, used infrared spectroscopy to analyze both apples and oranges. The study, which was published in the satirical science magazine \"Annals of Improbable Research\", concluded: \"[...] the comparing apples and oranges defense should no longer be considered valid. This is a somewhat startling revelation. It can be anticipated to have a dramatic effect on the strategies used in arguments and discussions in the future.\"\n\nA second study, written by Stamford Hospital's surgeon-in-chief James Barone and published in the \"British Medical Journal,\" noted that the phrase \"apples and oranges\" was appearing with increasing frequency in the medical literature, with some notable articles comparing \"Desflurane and propofol\" and \"Salmeterol and ipratropium\" with \"apples and oranges\". The study also found that both apples and oranges were sweet, similar in size, weight, and shape, that both are grown in orchards, and both may be eaten, juiced, and so on. The only significant differences found were in terms of seeds (the study used seedless oranges), the involvement of Johnny Appleseed, and color.\n\nThe \"Annals of Improbable Research\" subsequently noted that the \"earlier investigation was done with more depth, more rigour, and, most importantly, more expensive equipment\" than the \"British Medical Journal\" study.\n\nOn April Fools' Day 2014, \"The Economist\" compared worldwide production of apples and oranges from 1983 to 2013, however noted them to be \"unrelated variables\".\n\nWhile references to comparing apples and oranges are often a rhetorical device, references to adding apples and oranges are made in the case of teaching students the proper uses of units. Here, the admonition not to \"add apples and oranges\" refers to the requirement that two quantities with different units may not be combined by addition, although they may always be combined in ratio form by multiplication, so that multiplying ratios of apples and oranges is allowed. Similarly, the concept of this distinction is often used metaphorically in elementary algebra.\n\nThe admonition is really more of a mnemonic, since in general counts of objects have no intrinsic unit and, for example, a number count of apples may be dimensionless or have dimension \"fruit\"; in either of these two cases, apples and oranges may indeed be added.\n\n"}
{"id": "9721437", "url": "https://en.wikipedia.org/wiki?curid=9721437", "title": "Ask a Librarian", "text": "Ask a Librarian\n\nAsk a Librarian is a live virtual reference service that offers online reference assistance to residents in the state of Florida. Ask a Librarian is an official service of the Florida Electronic Library and is administered by the Tampa Bay Library Consortium (TBLC).\n\nParticipating libraries provide users with virtual reference services via live chat software, text messaging, and e-mail forms which users can access through embedded links and widgets on their library’s official website. Live chat and text messaging are available from 10:00 a.m. to midnight EST from Sunday through Thursday, and from 10:00 a.m. to 5:00 p.m. EST on Friday and Saturday; the e-mail form is available to patrons 24 hours per day, seven days per week. As of March 2016, Ask a Librarian has 133 participating institutions including public libraries and library systems, K-12 libraries, and university and college libraries.\n\nAsk a Librarian began as a partnership between the College Center for Library Automation (CCLA) and TBLC in the interest of creating a statewide virtual reference service that would increase the presence of librarians on the internet. In 2002, the organizations successfully applied for a joint grant through the Library Services and Technology Act (LSTA) and were awarded $339,000 for the development and implementation of their pilot project as a service of the Florida Electronic Library.\n\nThe Ask a Librarian virtual reference service was officially activated on July 28, 2003. Tampa Bay-area libraries were the first to participate, with Pasco County libraries joining shortly afterward. Within the first year of operation, nearly 7,500 Floridians had used the service to get answers from a reference librarian. By August 19, 2007, Ask a Librarian had answered its 100,000th reference question. As of February 2011, Ask a Librarian had logged over 274,000 live virtual reference sessions and e-mail questions.\n\nAsk a Librarian has made several efforts meet user reference needs on smartphones and other mobile devices. In October 2010, Ask a Librarian introduced a text messaging service to accompany their traditional chat service. In 2012, the service introduced a mobile-friendly website interface for tablets and phones. In April 2013, the service also launched the Ask A Librarian Mobile App, a mobile-friendly interface geared toward improving the user chat experience on smartphones and tablets.\n\n"}
{"id": "26334944", "url": "https://en.wikipedia.org/wiki?curid=26334944", "title": "Auxiliary sciences of history", "text": "Auxiliary sciences of history\n\nAuxiliary (or ancillary) sciences of history are scholarly disciplines which help evaluate and use historical sources and are seen as auxiliary for historical research. Many of these areas of study, classification and analysis were originally developed between the 16th and 19th centuries by antiquaries, and would then have been regarded as falling under the broad heading of antiquarianism. \"History\" was at that time regarded as a largely literary skill. However, with the spread of the principles of empirical source-based history championed by the Göttingen School of History in the late 18th century and later by Leopold von Ranke from the mid-19th century onwards, they have been increasingly regarded as falling within the skill-set of the trained historian.\n\nAuxiliary sciences of history include, but are not limited to:\n\n"}
{"id": "46407896", "url": "https://en.wikipedia.org/wiki?curid=46407896", "title": "Bibliography of C. Northcote Parkinson", "text": "Bibliography of C. Northcote Parkinson\n\n"}
{"id": "3181897", "url": "https://en.wikipedia.org/wiki?curid=3181897", "title": "Brand Book", "text": "Brand Book\n\nA Brand Book records all livestock brands registered with an organization. In the U.S. most states have branding laws that require brands to be registered before use. This may be a state agency (usually affiliated with each state's Department of Agriculture) or a private association regulated by the state. Most states with such laws have a Brand Book for the entire state. Texas, an exception, registers brands at the county level. These book are usually provided free to law enforcement personnel and County Extension Agents. Some states have their Brand Books available online.\n\nA typical Brand Book will usually have an image of the brand, the location of the brand on the animal, and the type of animal that will be branded, as well as the owner of the brand. Many Brand Books also record earmarks.\n\nBrand Books are used by law enforcement officials, brand inspectors, and association investigators to record and track livestock movement, deter loss of livestock by straying or theft, and prosecute thieves.\n\n\n"}
{"id": "27778631", "url": "https://en.wikipedia.org/wiki?curid=27778631", "title": "Breviograph", "text": "Breviograph\n\nA breviograph or brevigraph (from , short, and Greek \"grapho\", to write) is a type of scribal abbreviation in the form of an easily written symbol, character, flourish or stroke, based on a modified letter form to take the place of a common letter combination, especially those occurring at the beginning or end of a word. Breviographs were used frequently by stenographers, law clerks and scriveners, and they were also found in early printed books and tracts. Their use declined after the 17th century.\n\nExamples of breviographs:\n\n\n"}
{"id": "1047161", "url": "https://en.wikipedia.org/wiki?curid=1047161", "title": "Chapters and verses of the Bible", "text": "Chapters and verses of the Bible\n\nThe Bible is a compilation of many shorter books written at different times by a variety of authors, and later assembled into the biblical canon. Since the early 13th century, most copies and editions of the Bible present all but the shortest of these books with divisions into chapters, generally a page or so in length. Since the mid-16th century editors have further subdivided each chapter into verses - each consisting of a few short lines or sentences. Sometimes a sentence spans more than one verse, as in the case of , and sometimes there is more than one sentence in a single verse, as in the case of .\n\nAs the chapter and verse divisions did not appear in the original texts, they form part of the paratext of the Bible.\n\nThe Jewish divisions of the Hebrew text differ at various points from those used by Christians. For instance, in Jewish tradition, the ascriptions to many Psalms are regarded as independent verses or parts of the subsequent verses, making 116 more verses, whereas established Christian practice treats each Psalm ascription as independent and unnumbered. Some chapter divisions also occur in different places, e.g. Hebrew Bibles have where Christian translations have .\n\nEarly manuscripts of the biblical texts did not contain the chapter and verse divisions in the numbered form familiar to modern readers. In antiquity Hebrew texts were divided into paragraphs (parashot) that were identified by two letters of the Hebrew alphabet. Peh פ indicated an \"open\" paragraph that began on a new line, while Samekh ס indicated a \"closed\" paragraph that began on the same line after a small space. These two letters begin the Hebrew words open (patuach\") and closed (sagoor\"), and are, themselves, open פ and closed ס. The earliest known copies of the Book of Isaiah from the Dead Sea Scrolls used parashot divisions, although they differ slightly from the Masoretic divisions. (This is different from the use of consecutive letters of the Hebrew alphabet to structure certain poetic compositions, known as acrostics, such as several of the Psalms and most of the Book of Lamentations.)\n\nThe Hebrew Bible was also divided into some larger sections. In Israel the Torah (its first five books) were divided into 154 sections so that they could be read through aloud in weekly worship over the course of three years. In Babylonia it was divided into 53 or 54 sections (Parashat ha-Shavua) so it could be read through in one year. The New Testament was divided into topical sections known as \"kephalaia\" by the fourth century. Eusebius of Caesarea divided the gospels into parts that he listed in tables or \"canons\". Neither of these systems corresponds with modern chapter divisions. (See fuller discussions below.)\n\nChapter divisions, with titles, are also found in the 9th century Tours manuscript, Paris Bibliothèque Nationale MS Lat. 3, the so-called Bible of Rorigo.\n\nArchbishop Stephen Langton and Cardinal Hugo de Sancto Caro developed different schemas for systematic division of the Bible in the early 13th century. It is the system of Archbishop Langton on which the modern chapter divisions are based.\n\nWhile chapter divisions have become nearly universal, editions of the Bible have sometimes been published without them. Such editions, which typically use thematic or literary criteria to divide the biblical books instead, include John Locke's \"Paraphrase and Notes on the Epistles of St. Paul\" (1707), Alexander Campbell's \"The Sacred Writings\" (1826), Daniel Berkeley Updike’s fourteen-volume \"The Holy Bible Containing the Old and New Testaments and the Apocrypha,\" Richard Moulton's \"The Modern Reader's Bible\" (1907), Ernest Sutherland Bates's \"The Bible Designed to Be Read as Living Literature\" (1936), \"The Books of the Bible\" (2007) from the International Bible Society (Biblica), Adam Lewis Greene’s five-volume \"Bibliotheca\" (2014), and the six-volume ESV Reader's Bible (2016) from Crossway Books.\n\nSince at least 916 the Tanakh has contained an extensive system of multiple levels of section, paragraph, and phrasal divisions that were indicated in Masoretic vocalization and cantillation markings. One of the most frequent of these was a special type of punctuation, the \"sof passuq\", symbol for a full stop or sentence break, resembling the colon (:) of English and Latin orthography. With the advent of the printing press and the translation of the Bible into English, Old Testament versifications were made that correspond predominantly with the existing Hebrew full stops, with a few isolated exceptions. Most attribute these to Rabbi Isaac Nathan ben Kalonymus's work for the first Hebrew Bible concordance around 1440.\n\nThe first person to divide New Testament chapters into verses was Italian Dominican biblical scholar Santi Pagnini (1470–1541), but his system was never widely adopted. His verse divisions in the New Testament were far longer than those known today. Robert Estienne created an alternate numbering in his 1551 edition of the Greek New Testament which was also used in his 1553 publication of the Bible in French. Estienne's system of division was widely adopted, and it is this system which is found in almost all modern Bibles. Estienne produced a 1555 Vulgate that is the first Bible to include the verse numbers integrated into the text. Before this work, they were printed in the margins.\n\nThe first English New Testament to use the verse divisions was a 1557 translation by William Whittingham (c. 1524–1579). The first Bible in English to use both chapters and verses was the Geneva Bible published shortly afterwards in 1560. These verse divisions soon gained acceptance as a standard way to notate verses, and have since been used in nearly all English Bibles and the vast majority of those in other languages. (Nevertheless, some Bibles have removed the verse numbering, including the ones noted above that also removed chapter numbers; a recent example of an edition that removed only verses, not chapters, is \"The Message: The Bible in Contemporary Language\" by Eugene H. Peterson.)\n\nThe Hebrew Masoretic text of the Bible notes several different kinds of subdivisions within the biblical books:\n\nMost important are the verse endings. According to the Talmudic tradition, the division of the text into verses is of ancient origin. In Masoretic versions of the Bible, the end of a verse is indicated by a small mark in its final word called a \"silluq\" (which means \"stop\"). Less formally, verse endings are usually also indicated by two horizontal dots following the word with a \"silluq\".\n\nThe Masoretic textual tradition also contains section endings called \"parashot\", which are usually indicated by a space within a line (a \"closed\" section) or a new line beginning (an \"open\" section). The division of the text reflected in the \"parashot\" is usually thematic. Unlike chapters, the \"parashot\" are not numbered, but some of them have special titles.\n\nIn early manuscripts (most importantly in Tiberian Masoretic manuscripts, such as the Aleppo codex), an \"open\" section may also be represented by a blank line, and a \"closed\" section by a new line that is slightly indented (the preceding line may also not be full). These latter conventions are no longer used in Torah scrolls and printed Hebrew Bibles. In this system, the one rule differentiating \"open\" and \"closed\" sections is that \"open\" sections must \"always\" start at the beginning of a new line, while \"closed\" sections \"never\" start at the beginning of a new line.\n\nAnother division of the biblical books found in the Masoretic text is the division of the \"sedarim\". This division is not thematic, but is almost entirely based upon the \"quantity\" of text. For the Torah, this division reflects the triennial cycle of reading that was practiced by the Jews of the Land of Israel.\n\nThe Byzantines also introduced a concept roughly similar to chapter divisions, called \"kephalaia\" (singular \"kephalaion\", literally meaning \"heading\"). This system, which was in place no later than the 5th century, is not identical to the present chapters. Unlike the modern chapters, which tend to be of roughly similar length, the distance from one \"kephalaion\" mark to the next varied greatly in length both within a book and from one book to the next. For example, the Sermon on the Mount, comprising three chapters in the modern system, has but one \"kephalaion\" mark, while the single modern chapter 8 of the Gospel of Matthew has several, one per miracle. Moreover, there were far fewer \"kephalaia\" in the Gospel of John than in the Gospel of Mark, even though the latter is the shorter text. In the manuscripts, the \"kephalaia\" with their numbers, their standard titles (\"titloi\") and their page numbers would be listed at the beginning of each biblical book; in the book's main body, they would be marked only with arrow-shaped or asterisk-like symbols in the margin, not in the text itself.\n\nThe titles usually referred to the first event or the first theological point of the section only, and some \"kephalaia\" are manifestly incomplete if one stops reading at the point where the next \"kephalaion\" begins (for example, the combined accounts of the miracles of the Daughter of Jairus and of the healing of the woman with a haemorrhage gets two marked \"kephalaia\", one titled \"of the daughter of the synagogue ruler\" at the beginning when the ruler approaches Jesus and one titled \"of the woman with the flow of blood\" where the woman enters the picture – well before the ruler's daughter is healed and the storyline of the previous \"kephalaion\" is thus properly concluded). Thus the \"kephalaia\" marks are rather more like a system of bookmarks or links into a continuous text, helping a reader to quickly find one of several well-known episodes, than like a true system of chapter divisions.\n\nCardinal Hugo de Sancto Caro is often given credit for first dividing the Latin Vulgate into chapters in the real sense, but it is the arrangement of his contemporary and fellow cardinal Stephen Langton who in 1205 created the chapter divisions which are used today. They were then inserted into Greek manuscripts of the New Testament in the 16th century. Robert Estienne (Robert Stephanus) was the first to number the verses within each chapter, his verse numbers entering printed editions in 1551 (New Testament) and 1571 (Hebrew Bible).\n\nThe division of the Bible into chapters and verses has received criticism from some traditionalists and modern scholars. Critics state that the text is often divided in an incoherent way, or at inappropriate rhetorical points, and that it encourages citing passages out of context. Nevertheless, the chapter and verse numbers have become indispensable as technical references for Bible study.\n\nSeveral modern publications of the Bible have eliminated numbering of chapters and verses. Biblica published such a version of the NIV in 2007 and 2011. In 2014, Crossway published the ESV Reader's Bible and \"Bibliotheca\" published a modified ASV. Projects such as Icthus also exist which strip chapter and verse numbers from existing translations.\n\nThe number of words can vary depending upon aspects such as whether the Hebrew alphabet in Psalm 119, the superscriptions listed in some of the Psalms, and the subscripts traditionally found at the end of the Pauline epistles, are included.\nExcept where stated, the following apply to the King James Version of the Bible in its modern 66-book Protestant form including the New Testament and the protocanonical Old Testament, not the deuterocanonical books.\n\n\n\n\n\n"}
{"id": "744504", "url": "https://en.wikipedia.org/wiki?curid=744504", "title": "Circular reference", "text": "Circular reference\n\nA circular reference is a series of references where the last object references the first, resulting in a closed loop.\nA circular reference is not to be confused with the logical fallacy of a circular argument. Although a circular reference will often be unhelpful and reveal no information, such as two entries in a book index referring to each other, it is not necessarily so that a circular reference is of no use. Dictionaries, for instance, must always ultimately be a circular reference since all words in a dictionary are defined in terms of other words, but a dictionary nevertheless remains a useful reference. Sentences containing circular references can still be meaningful;\n\nis circular but not without meaning. Indeed, it can be argued that self-reference is a necessary consequence of Aristotle's Law of non-contradiction, a fundamental philosophical axiom. In this view, without self-reference, logic and mathematics become impossible, or at least, lack usefulness.\n\nCircular references can appear in computer programming when one piece of code requires the result from another, but that code needs the result from the first. For example:\n\nFunction A will show the time the sun last set based on the current date, which it can obtain by calling Function B. Function B will calculate the date based on the number of times the moon has orbited the earth since the last time Function B was called. So, Function B asks Function C just how many times that is. Function C doesn't know, but can figure it out by calling Function A to get the time the sun last set.\n\nThe entire set of functions is now worthless because none of them can return any useful information whatsoever. This leads to what is technically known as a livelock. It also appears in spreadsheets when two cells require each other's result. For example, if the value in Cell A1 is to be obtained by adding 5 to the value in Cell B1, and the value in Cell B1 is to be obtained by adding 3 to the value in Cell A1, no values can be computed. (Even if the specifications are A1:=B1+5 and B1:=A1-5, there is still a circular reference. It doesn't help that, for instance, A1=3 and B1=-2 would satisfy both formulae, as there are infinitely many other possible values of A1 and B1 that can satisfy both instances.)\n\nA circular reference represents a big problem in computing.\n\nIn ISO Standard SQL circular integrity constraints are implicitly supported within a single table. Between multiple tables circular constraints (e.g. foreign keys) are permitted by defining the constraints as deferrable (See CREATE TABLE for PostgreSQL and DEFERRABLE Constraint Examples for Oracle). In that case the constraint is checked at the end of the transaction not at the time the DML statement is executed. To update a circular reference two statements can be issued in a single transaction that will satisfy both references once the transaction is committed.\n\nA distinction should be made with processes containing a circular reference between those that are incomputable and those that are an iterative calculation with a final output. The latter may fail in spreadsheets not equipped to handle them but are nevertheless still logically valid.\n\nCircular reference in worksheets can be a very useful technique for solving implicit equations such as the Colebrook equation and many others, which might otherwise require tedious Newton-Raphson algorithms in VBA or use of macros.\n\n"}
{"id": "14322444", "url": "https://en.wikipedia.org/wiki?curid=14322444", "title": "Comparative advertising", "text": "Comparative advertising\n\nComparative advertising or advertising war is an advertisement in which a particular product, or service, specifically mentions a competitor by name for the express purpose of showing why the competitor is inferior to the product naming it. Also referred to as \"knocking copy\", it is loosely defined as advertising where “the advertised brand is explicitly compared with one or more competing brands and the comparison is obvious to the audience.”\n\nThis should not be confused with parody advertisements, where a fictional product is being advertised for the purpose of poking fun at the particular advertisement, nor should it be confused with the use of a coined brand name for the purpose of comparing the product without actually naming an actual competitor. (\"Wikipedia tastes better and is less filling than the Encyclopedia Galactica.\")\n\nIn the United States, the Federal Trade Commission (FTC) defined comparative advertising as “advertisement that compares alternative brands on objectively measurable attributes or price, and identifies the alternative brand by name, illustration or other distinctive information.” This definition was used in the case Gillette Australia Pty Ltd v Energizer Australia Pty Ltd. Similarly, the Law Council of Australia recently suggested that comparative advertising refers to “advertising which include reference to a competitor’s trademark in a way which does not impute proprietorship in the mark to the advertiser.”\n\nComparative advertisements could be either indirectly or directly comparative, positive or negative, and seeks “to associate or differentiate the two competing brands”. Different countries apply differing views regarding the laws on comparative advertising.\n\nThe earliest court case concerning comparative advertising dates back to 1910 in the United States – Saxlehner v Wagner. Prior to the 1970s, comparative advertising was deemed unfeasible due to related risks. For instance, comparative advertising could invite misidentification of products, potential legal issues, and may even win public sympathy for their competitors as victims.\n\nIn 1972, the FTC began to encourage advertisers to make comparison with named competitors, with the broad, public welfare objective of creating more informative advertising. The FTC argued that this form of advertising could also stimulate comparison shopping, encourage product improvement and innovation, and foster a positive competitive environment. However, studies have shown that while comparative advertisements had increased since 1960, the relative amount of comparative advertising is still small.\n\nPrior to 1997, many European countries severely limited comparative claims as an advertising practice. For example, in Germany comparisons in advertising had since the 1930's been largely prohibited as an anti-competitive practice, with very limited exceptions for cases where the advertiser had a good reason for presenting a critical claim, and reference to a competitor was necessary in order to present that claim. Importantly, this only applied to \"critical\" claims - claims of equivalence were completely prohibited. A similar approach had been adopted in France, where comparative advertising was commonly seen as disparaging of competitors. However, the legalisation of comparative advertising in France in 1992, opened the door to a general legalisation of comparative advertising through EU law, which had first been proposed by the European Commission in 1978. The result was the adoption of Directive 97/55/EC, which came into force in the year 2000. The relevant provisions are now contained in Directive 2006/114/EC.\n\nThis Directive sets out rules that comparative advertising must comply with in order to be considered permissible. These include the requirements that the comparison concern goods and services that meet the same purpose, that it objectively compare the relevant characteristics of the products concerned and that it not cause confusion or denigrate the trademarks and other distinguishing signs of competitors. The Directive prohibits comparisons that take unfair advantage of the reputation of a competitor's distinguishing marks, or present goods or services as imitations of products covered by a protected trade mark or trade name. Additionally, any comparison aimed at promoting goods bearing a protected designation of origin must refer exclusively to other goods bearing the same designation. Directive 2006/114/EC constitutes a total harmonisation of the rules on comparative advertising, meaning that the Member States are neither allowed to permit comparisons that breach the requirements of the Directive, nor prohibit ones that do. \n\nFurther, while trademark rights can in principle be used to prevent comparative advertising that makes unauthorised use of a competitor's trademark, this is not the case where the comparative advertisement complies with all the requirements of Directive 2006/114/EC. Legitimate comparative advertising must therefore be seen as an exception to the exclusive rights of the trademark proprietor. However, the trademark proprietor can, thanks to the prohibition on taking unfair advantage of a trademark's reputation, oppose the use of their trademark where it is not aimed at distinguishing the products of the advertiser and trademark proprietor and to highlight their differences objectively, but rather at riding on the coat-tails of that mark in order to benefit from its reputation.\n\nThe requirements set out by the Directive have resulted in some controversy. This is particularly true of the \"per se\" prohibition on comparisons presenting goods and services as imitations of trademarked products. In this regard, EU law contrasts starkly with the US approach; the US courts have long held that traders are allowed to the trademarked names of products they have imitated in advertising. In contrast, in L'Oréal and others v. Bellure, the Court of Justice held that smell-alike perfumes marketed through comparison lists breached this condition. This decision was criticised both by the English courts and by scholars, who have considered that this places unjustified limits on advertising acts that are otherwise fully legal, such as copying that does not infringe intellectual property rights. \n\nIn the UK, most of the use of competitor’s registered trademark in a comparative advertisement was an infringement of the registration up till the end of 1994. However, the laws on comparative advertising were harmonized in 2000. The current rules on comparative advertising are regulated by a series of EU Directives. The Business Protection from Misleading Marketing Regulations 2008 implements provisions of Directive (EC) 2006/114 in the UK.\n\nOne of the classic cases of comparative advertising in the UK was the O2 v Hutchison case. The European Court of Justice (ECJ) held that there could have been a trademark infringement when a comparative advertiser used the registered trademark for the advertiser’s own goods and services. It was also held that a trademark proprietor could not prevent a competitor’s use of a sign similar or identical to his mark in a comparative advertisement, which satisfies all the conditions of the Comparative Advertising Directive. If the Advocate General's decision in the O2 case were followed by the ECJ, competitors will not be able to use trademark legislation either to prevent a comparative advertisement through an injunction or to charge in respect of its use. Conversely, in British Airways plc v Ryanair Ltd. a lenient approach was adopted by the UK courts. The use of competitors’ trademarks was no longer restricted for businesses competing within an industry, provided that compliance of the conditions set out in the legislation were performed. This meant that businesses are able to use the trademarks of other companies and trade names to distinguish the relative merits of their own products and services over those of their competitors.\n\nThe FTC and the National Advertising Division of the Council of Better Business Bureaus, Inc. (NAD), govern the laws of comparative advertising in the United States including the treatment of comparative advertising claims. FTC stated that comparative advertising could benefit consumers and encourages comparative advertising, provided that the comparisons are “clearly identified, truthful, and non-deceptive”. Although comparative advertising is encouraged, NAD has stated “claims that expressly or implicitly disparage a competing product should be held to the highest level of scrutiny in order to ensure that they are truthful, accurate, and narrowly drawn.” Another major law is the trademark protective Lanham Act, which states that one could incur liability when the message of the comparative advertisement is untrue or uncertain, but has the intention to deceive consumers through the implied message conveyed.\n\nIn Australia, no specific law governs comparative advertising although certain cases regarding this matter have occurred. Comparative advertising that is truthful, and does not lead to confusion is permitted.\n\nGenerally, Australian advertisers should make sure that the following are complied when exercising comparative advertising to avoid breaches regarding misleading advertising under Australia Consumer Law:\n\n\nThe law in Hong Kong regarding comparative advertising is the law that existed in the UK prior to the enactment of the UK Act 1994. Hong Kong has no legislation exclusively intended at limiting false or misleading advertisements. Still, the Trade Descriptions Ordinance (Cap 362) bans the use of false trade descriptions in advertisements. The tort of trade libel also exists to deal with false or misleading advertisements designed to injure the competitor. Consumer Council may have the authority to publish information with a perspective to amending false or misleading advertisements, while the Association of Accredited Advertising Agencies of Hong Kong have the authority to take action against members who organize advertisements that are inaccurate.\n\nIn Argentina, there is no specific statute dealing with comparative advertising (so it is not forbidden), but there are clear jurisprudential rules based on unfair competition law. If in some manner an advertisement is proven to be unfair or exceeds ethical standards by hiding the truth or omitting some essential aspect of the comparison, it is probable that an injunction will be granted and that the plaintiff will be able to obtain a final decision declaring the advertising illegal.\n\nNumerous cases follow international precedent in referring to the requirements of the European Union Directive on comparative advertising. By following these criteria, Argentine courts have developed standards very similar to European regulation. It is as if the judges wanted to validate the law created by the Courts with an external source. Similar conclusions reached elsewhere indicate the existence of universally accepted principles that accept that comparing products in commercial advertisements should be lawful.\n\nIn Brazil, the allow comparative advertising with certain restrictions. Its primary purpose shall be the clarification or consumer’s protection; it shall have as basic principle the objectiveness of the comparison since subjective data, psychological or emotionally based data does not constitute a valid comparison basis for consumers; the purposed or implemented comparison shall be capable of being supported byevidence; in the case of consumption goods, the comparison shall be made with models manufactured in the same year and no comparison shall be made between products manufactured in different years, unless it is only a reference to show evolution, in which case the evolution shall be clearly demonstrated; there shall be no confusion between the products and competitor’s brands; there shall be no unfair competition, denigration of the product’s image or another company’s product; and there shall be no unreasonable use of the corporate image or goodwill of third parties.\n\nLikewise, the majority of the Brazilian authors is inclined to say that its legitimacy depends to meet certain requirements, which, in general, would be stipulated by Article 3a of Directive 84/450/EEC \n\nIn an early Mercosur's rules through Resolution 126/96.\n\nComparative advertising has been increasingly implemented through the years, and the types of comparative advertising range from comparing a single attribute dimension, comparing an attribute unique to the target and absent in the referent and comparisons involving attributes unique to both brands. The contributing factors to the effectiveness of comparative advertising include believability, which refers to the extent a consumer can rely on the information provided in comparative advertisements, the level of involvement, and the convenience in evaluation, provided by spoon feeding the consumer with information that does not require extra effort in recall.\n\nComparative advertising is generally coupled with negativity, as evidenced by early industry condemnation. Stating reasons such as participation in comparative advertising damaged the honour and credibility of advertising. Studies have suggested that negative information can be stored more effectively, thus generating the impact that any advertisement is purposed for, and more importantly, strong recall. On the contrary, such negativity can either be transferred directly to the brand and the consumer’s impression of the brand, various studies through the years have proven that comparative advertising has been responded to negatively.\n\nComparative advertising has been used effectively by companies like The National Australia Bank (NAB), and its “break up” campaign has made such an impact it has won an award from Cannes, and a substantial increase in its consumer interest. Internationally acclaimed Apple Inc. has effectively utilized its Mac vs PC advertisements as part of its marketing efforts to increase its market share over the years. Such companies prove the academic view that comparative advertising is more successful when used by established brands, justified by the credibility and attention an established brand brings. Other famous examples include L’Oreal SA v Bellure NV and Coca Cola v Pepsi. Comparative advertising has to be executed with caution and deep consideration for the targeted markets as the novelty of the concept affects the effectiveness of the stipulated campaigns.\n\nIn the 1980s, during what has been referred to as the cola wars, soft-drink manufacturer Pepsi ran a series of advertisements where people, caught on hidden camera, in a blind taste test, chose Pepsi over rival Coca-Cola.\nThe use of comparative advertising has been well established in political campaigns, where typically one candidate will run ads where the record of the other candidate is displayed, for the purpose of disparaging the other candidate. The most famous of these type ads, which only ran once on TV, consisted of a child picking daisies in a field, while a voice which sounded like Barry Goldwater performed a countdown to zero before the launch of a nuclear weapon which explodes in a mushroom cloud. The ad, \"Daisy\", was produced by Lyndon B. Johnson's campaign in an attempt to prevent Goldwater from either winning the nomination of his party or being selected.\n\nAnother example took place throughout the late 1980s between the bitter rivals Nintendo and Sega. \"Genesis does what Nintendon't\" immediately became a catchphrase following the release of the Sega Genesis (known as Mega Drive in PAL countries).\n\nA 30-second commercial promoting sustainability, showing soda bottles exploding each time a person makes a drink using his Sodastream machine, was banned in the United Kingdom in 2012. Clearcast, the organization that preapproves TV advertising in the U.K., explained that they \"thought it was a denigration of the bottled drinks market.\" The same ad, crafted by Alex Bogusky, ran in the United States, Sweden, Australia, and other countries. An appeal by Sodastream to reverse Clearcast's decision to censor the commercial was rejected. A similar ad was expected to air during Super Bowl XLVII in February 2013 but was banned by CBS for jabbing at Coke and Pepsi (two of CBS's largest sponsors).\n\nIn 2012, Microsoft's Bing (formerly MSN Search) began to run a campaign about which search engine they prefer as it compared Bing to Google, and that more people preferred Bing over Google. The campaign was titled \"Bing It On\".\n"}
{"id": "16264661", "url": "https://en.wikipedia.org/wiki?curid=16264661", "title": "Comparative case", "text": "Comparative case\n\nThe comparative case (abbreviated ) is a grammatical case used in languages such as Mari and Chechen to mark a likeness to something. \n\nIt is not to be confused with the comparative degree, a much more widely used paradigm used to signify heightening of adjectives and adverbs.\n\nIn Mari, the comparative case is marked with the suffix -ла ('-la') For example, if something were to taste like fish (кол - 'kol'), the form used would be колла - 'kolla'). It is also used in regard to languages, when denoting the language a person is speaking, writing, or hearing. Then, however, the accentuation varies slightly from the standard case. Usually, the suffix is not stressed. When it is used with languages, however, it is stressed.\n\nIn Chechen, it is marked with the suffix \"-l\". For example, \"sha\" is 'ice', \"shiila\" is 'cold', and \"shal shiila\" is 'cold as ice'.\n\n"}
{"id": "12185843", "url": "https://en.wikipedia.org/wiki?curid=12185843", "title": "Comparative cognition", "text": "Comparative cognition\n\nComparative cognition is the comparative study of the mechanisms and origins of cognition in various species, and is sometimes seen as more general than, or similar to, comparative psychology.\nFrom a biological point of view, work is being done on the brains of fruit flies that should yield techniques precise enough to allow an understanding of the workings of the human brain on a scale appreciative of individual groups of neurons rather than the more regional scale previously used. Similarly, gene activity in the human brain is better understood through examination of the brains of mice by the Seattle-based Allen Institute for Brain Science (see link below), yielding the freely available Allen Brain Atlas. This type of study is related to comparative cognition, but better classified as one of comparative genomics. Increasing emphasis in psychology and ethology on the biological aspects of perception and behavior is bridging the gap between genomics and behavioral analysis.\n\nIn order for scientists to better understand cognitive function across a broad range of species they can systematically compare cognitive abilities between closely and distantly related species Through this process they can determine what kinds of selection pressure has led to different cognitive abilities across a broad range of animals. For example, it has been hypothesized that there is convergent evolution of the higher cognitive functions of corvids and apes, possibly due to both being omnivorous, visual animals that live in social groups.\n\n\n"}
{"id": "2051798", "url": "https://en.wikipedia.org/wiki?curid=2051798", "title": "Comparative contextual analysis", "text": "Comparative contextual analysis\n\nComparative contextual analysis is a methodology for comparative research where contextual interrogation precedes any analysis of similarity and difference. It is a thematic process directed and designed to explore relationships of agency rather than institutional or structural frameworks. See structure and agency and theory of structuration.\n\n\n"}
{"id": "1826033", "url": "https://en.wikipedia.org/wiki?curid=1826033", "title": "Comparative neuropsychology", "text": "Comparative neuropsychology\n\nComparative neuropsychology refers to an approach used for understanding human brain functions. It involves the direct evaluation of clinical neurological populations by employing experimental methods originally developed for use with nonhuman animals.\n\nOver many decades of animal research, methods were perfected to study the effects of well-defined brain lesions on specific behaviors, and later the tasks were modified for human use. Generally the modifications involve changing the reward from food to money, but standard administration of the tasks in humans still involves minimal instructions, thus necessitating a degree of procedural learning in human and nonhuman animals alike.\n\nCurrently, comparative neuropsychology is used with neurological patients to link specific deficits with localized areas of the brain.\n\nThe comparative neuropsychological approach employs simple tasks that can be mastered without relying upon language skills. Precisely because these simple paradigms do not require linguistic strategies for solution, they are especially useful for working with patients whose language skills are compromised, or whose cognitive skills may be minimal.\n\nComparative neuropsychology contrasts with the traditional approach of using tasks that rely upon linguistic skills, and that were designed to study human cognition. Because important ambiguities about its heuristic value had not been addressed empirically, only recently has comparative neuropsychology become popular for implementation with brain-damaged patients.\n\nWithin the past decade, comparative neuropsychology has had prevalent use as a framework for comparing and contrasting the performances of disparate neurobehavioral populations on similar tasks.\n\nComparative neuropsychology involves the study of brain-behavior relationships by applying experimental paradigms, used extensively in animal laboratories, for testing human clinical populations. Popular paradigms include delayed reaction tasks, discrimination and reversal learning tasks, and matching- and nonmatching-to-sample. These tasks are used to test animals and relate them to human brain functioning. Such tasks were perfected on experimental animals having well defined brain lesions, and adapted for human neurological patients. The comparative aspects of such approach resides in the analogy between animals with brain lesions and human patients with lesions in homologous areas of the brain. One example is represented by the comparison between the brain of laboratory animals (primarily non human primates and mice) with the one of people with damages resulting from alcohol abuse.\n\nGeorge Ettlinger was one of the few who actively combined human and animal research, and he did so consistently throughout his scientific career. Ettinger work focused on the importance of the inferior temporal neocortex in visual discrimination learning and memory in macaque monkeys, and on the importance of ventral temporal lobe in vision. Ettinger animals models carried inferotemporal or latero-ventral prestriate ablation. In 1966 George Ettlinger, together with the psychologist Colin Blakemore and the neurosurgeon Murray Falconer, described the results of a study on correlation between pre-operative intelligence and the severity of mesial temporal sclerosis in temporal lobe specimens excised to treat intractable epilepsy. Such study It is known as a forerunner of what has become one of the potentially most interesting techniques for exploring the relationship between certain aspects of human memory and temporal lobe structures.\n\n\n"}
{"id": "31994535", "url": "https://en.wikipedia.org/wiki?curid=31994535", "title": "Comparison of Nazism and Stalinism", "text": "Comparison of Nazism and Stalinism\n\nA number of authors have carried out comparisons of Nazism and Stalinism, in which they have considered the similarities and differences of the two ideologies and political systems, what relationship existed between the two regimes, and why both of them came to prominence at the same time. During the 20th century, the comparison of Stalinism and Nazism was made on the topics of totalitarianism, ideology, and personality cult. Both regimes were seen in contrast to the liberal West, with an emphasis on the similarities between the two. The American political scientists Zbigniew Brzezinski, Hannah Arendt and Carl Friedrich and historian Robert Conquest were prominent advocates of applying the \"totalitarian\" concept to compare Nazism and Stalinism.\n\nOne of the first scholars to publish a comparative study of Nazi Germany and Stalin’s Soviet Union was Hannah Arendt. In her 1951 work, \"The Origins of Totalitarianism\", Arendt puts forward the idea of totalitarianism as a distinct type of political movement and form of government, which “differs essentially from other forms of political oppression known to us such as despotism, tyranny and dictatorship.” Furthermore, Arendt distinguishes between a totalitarian movement (such as a political party with totalitarian aims) and a totalitarian government. Not all totalitarian movements succeed in creating totalitarian governments once they gain power. In Arendt’s view, although many totalitarian movements existed in Europe in the 1920s and 1930s, only the governments of Stalin and Hitler succeeded in fully implementing their totalitarian aims. \n\nArendt traced the origin of totalitarian movements to the nineteenth century, focusing especially on antisemitism and imperialism. She emphasized the connection between the rise of European nation-states and the growth of antisemitism, which was due to the fact that the Jews represented an “inter-European, non-national element in a world of growing or existing nations.” Conspiracy theories abounded, and the Jews were accused of being part of various international schemes to ruin European nations. Small antisemitic political parties formed in response to this perceived Jewish threat, and, according to Arendt, these were the first political organizations in Europe that claimed to represent the interests of the whole nation as opposed to the interests of a class or other social group. The later totalitarian movements would copy or inherit this claim to speak for the whole nation, with the implication that any opposition to them constituted treason.\n\nEuropean imperialism of the nineteenth century also paved the way for totalitarianism, by legitimizing the concept of endless expansion. After Europeans had engaged in imperialist expansion on other continents, political movements developed which aimed to copy the methods of imperialism on the European continent itself. Arendt refers specifically to the “pan-movements” of pan-Germanism and pan-Slavism, which promised continental empires to nations that had little hope of overseas expansion. According to Arendt, “Nazism and Bolshevism owe more to Pan-Germanism and Pan-Slavism (respectively) than to any other ideology or political movement.”\n\nArendt argues that both the Nazi and Bolshevik movements “recruited their members from [a] mass of apparently indifferent people whom all other parties had given up,” and who “had reason to be equally hostile to all parties.” For this reason, totalitarian movements did not need to use debate or persuasion, and did not need to refute the arguments of the other parties. Their target audience did not have to be persuaded to despise the other parties or the democratic system, because it consisted of people who already despised mainstream politics. As a result, totalitarian movements were free to use violence and terror against their opponents without fear that this might alienate their own supporters. Instead of arguing against their opponents, they adopted deterministic views of human behavior and presented opposing ideas as “originating in deep natural, social, or psychological sources beyond the control of the individual and therefore beyond the power of reason.” The Nazis in particular, during the years before their rise to power, engaged in “killing small socialist functionaries or influential members of opposing parties” both as a means to intimidate opponents and as a means of demonstrating to their supporters that they were a party of action, “different from the ‘idle talkers’ of other parties.”\n\nTotalitarian governments make extensive use of propaganda, and are often characterized by having a strong distinction between what they tell their own supporters and the propaganda they produce for others. Arendt distinguishes these two categories as \"indoctrination\" and \"propaganda\". Indoctrination consists of the message that a totalitarian government promotes internally, to the members of the ruling party and that segment of the population which supports the government. Propaganda consists of the message that a totalitarian government seeks to promote in the outside world, and also among those parts of its own society which may not support the government. Thus, “the necessities for propaganda are always dictated by the outside world,” while the opportunities for indoctrination depend on “the totalitarian governments’ isolation and security from outside interference.” \n\nThe type of indoctrination used by the Soviets and the Nazis was characterized by claims of “scientific” truth, and appeals to “objective laws of nature.” Both movements took a deterministic view of human society and claimed that their ideologies were based on scientific discoveries regarding race (in the case of the Nazis) or the forces governing human history (in the case of the Soviets). Arendt identifies this as being in certain ways similar to modern advertising, in which companies claim that scientific research shows their products to be superior, but more generally she argues that it is an extreme version of “that obsession with science which has characterized the Western world since the rise of mathematics and physics in the sixteenth century.” By their use of pseudoscience as the main justification for their actions, Nazism and Stalinism are distinguished from earlier historical despotic regimes, who appealed instead to religion or sometimes did not try to justify themselves at all. According to Arendt, totalitarian governments did not merely use these appeals to supposed scientific laws as propaganda to manipulate others. Rather, totalitarian leaders like Hitler and Stalin genuinely believed that they were acting in accordance with immutable natural laws, to such an extent that they were willing to sacrifice the self-interest of their regimes for the sake of enacting those supposed laws. For instance, the Nazis treated the inhabitants of occupied territories with extreme brutality and planned to depopulate Eastern Europe in order to make way for colonists from the German “master race,” despite the fact that this actively harmed their war effort. Stalin repeatedly purged the Communist Party of people who deviated even slightly from the party line, even when this weakened the party or the Soviet government, because he believed that they represented the interests of “dying classes” and their demise was historically inevitable.\n\nArendt also identifies the central importance of an all-powerful leader in totalitarian movements. As in other areas, she distinguishes between totalitarian leaders (such as Hitler and Stalin) and non-totalitarian dictators or autocratic leaders. The totalitarian leader does not rise to power by personally using violence or through any special organizational skills, but rather by controlling appointments of personnel within the party, so that all other prominent party members owe their positions to him. With loyalty to the leader becoming the primary criterion for promotion, ambitious party members compete with each other in trying to express their loyalty, and a cult of personality develops around the leader. Even when the leader is not particularly competent and the members of his inner circle are aware of his deficiencies, they remain committed to him out of fear that without him the entire power structure would collapse.\n\nOnce in power, according to Arendt, totalitarian movements face a major dilemma: they built their support on the basis of anger against the status quo and on impossible or dishonest promises, but now they have become the new status quo and are expected to carry out their promises. They deal with this problem by engaging in a constant struggle against external and internal enemies, real or imagined, so as to enable them to say that, in a sense, they have not yet gained the power they need to fulfill their promises. According to Arendt, totalitarian governments must be constantly fighting enemies in order to survive. This explains their apparently irrational behavior, for example when Hitler continued to make territorial demands even after he was offered everything he asked for in the Munich Agreement, or when Stalin unleashed the Great Terror despite the fact that he faced no significant internal opposition.\n\nArendt points out the widespread use of concentration camps by totalitarian governments, arguing that they are the most important manifestation of the need to find enemies to fight against, and are therefore “more essential to the preservation of the regime’s power than any of its other institutions.” Although forced labor was commonly imposed on inmates of concentration camps, Arendt argues that their primary purpose was not any kind of material gain for the regime: “The only permanent economic function of the camps has been the financing of their own supervisory apparatus; thus from the economic point of view the concentration camps exist mostly for their own sake.” The Nazis in particular carried this to the point of “open anti-utility,” by expending large sums of money, resources and manpower – during a war – for the purpose of building and staffing extermination camps and transporting people to them. This sets apart the concentration camps of totalitarian regimes from older human institutions that bear some similarity to them, such as slavery. Slaves were abused and killed for the sake of profit; concentration camp inmates were abused and killed because a totalitarian government needed to justify its existence. Finally, Arendt points out that concentration camps under both Hitler and Stalin included large numbers of inmates who were innocent of any crime – not only in the ordinary sense of the word, but even by the standards of the regimes themselves. That is to say, most of the inmates had not actually committed any action against the regime.\n\nThroughout her analysis, Arendt emphasized the modernity and novelty of the governmental structures set up by Stalin and Hitler, arguing that they represented “an entirely new form of government” which is likely to manifest itself again in various other forms in the future. She also cautioned against the belief that future totalitarian movements would necessarily share the ideological foundations of Nazism or Stalinism, writing that “all ideologies contain totalitarian elements.”\n\nThe totalitarian paradigm in the comparative study of Nazi Germany and the Soviet Union was further developed by Carl Friedrich and Zbigniew Brzezinski, who wrote extensively on this topic both individually and in collaboration. Similar to Hannah Arendt, they state that “totalitarian dictatorship is a new phenomenon; there has never been anything quite like it before.” Friedrich and Brzezinski classify totalitarian dictatorship as a type of autocracy, but argue that it is different in important ways from most other historical autocracies. In particular, it is distinguished by a reliance on modern technology and mass legitimation. Unlike Arendt, Friedrich and Brzezinski apply the notion of totalitarian dictatorship not only to the regimes of Hitler and Stalin, but also to the USSR throughout its entire existence, as well as the regime of Benito Mussolini in Italy and the People’s Republic of China under Mao Zedong.\n\nCarl Friedrich noted that the “possibility of equating the dictatorship of Stalin in the Soviet Union and that of Hitler in Germany” has been a deeply controversial topic and a subject of debate almost from the beginning of those dictatorships. Various other aspects of the two regimes have also been the subject of intense scholarly debate, such as whether Nazi and Stalinist ideologies were genuinely believed and pursued by the respective governments, or whether the ideologies were merely convenient justifications for dictatorial rule. Friedrich himself argues in favor of the former view.\n\nFriedrich and Brzezinski argue that Nazism and Stalinism are not only similar to each other, but also represent a continuation or a return to the tradition of European absolute monarchy on certain levels. In the absolute monarchies of the seventeenth and eighteenth centuries, the monarch ultimately held all decisional power, and was considered accountable only to God. In Stalinism and Nazism, the leader likewise held all real power, and was considered accountable only to various intangible entities such as “the people”, “the masses” or “the Volk.” Thus the common feature of autocracies – whether monarchical or totalitarian – is the concentration of power in the hands of a leader who cannot be held accountable by any legal mechanisms, and who is supposed to be the embodiment of the will of an abstract entity. Friedrich and Brzezinski also identify other features common to all autocracies, such as “the oscillation between tight and loose control.” The regime alternates between periods of intense repression and periods of relative freedom, often represented by different leaders. This depends in part on the personal character of different leaders, but Friedrich and Brzezinski believe that there is also an underlying political cycle, in which rising discontent leads to increased repression up to the point at which the opposition is eliminated, then controls are relaxed until the next time that popular dissatisfaction begins to grow.\n\nThus, placing Stalinism and Nazism within the broader historical tradition of autocratic government, Friedrich and Brzezinski hold that “totalitarian dictatorship, in a sense, is the adaptation of autocracy to twentieth-century industrial society.” However, at the same time, they insist that totalitarian dictatorship is a “\"novel\" type of autocracy” and argue that twentieth century totalitarian regimes (such as those of Hitler and Stalin) had more in common with each other than with any other form of government, including historical autocracies of the past. Totalitarianism can only exist after the creation of modern technology, because such technology is essential for propaganda, for surveillance of the population, and for the operation of a secret police. Furthermore, when speaking of the differences and similarities between fascist and communist regimes, Friedrich and Brzezinski insist that the two kinds of totalitarian governments are “basically alike” but “not wholly alike” – they are more similar to each other than to other forms of government, but they are not the same. Among the major differences between them, Friedrich and Brzezinski identify in particular the fact that communists seek “the world revolution of the proletariat,” while fascists wish to “establish the imperial predominance of a particular nation or race.” \n\nIn terms of the similarities between Nazism and Stalinism, Friedrich lists five main aspects that they hold in common: First, an official ideology that is supposed to be followed by all members of society, at least passively, and which promises to serve as a perfect guide towards some ultimate goal. Second, a single political party, composed of the most enthusiastic supporters of the official ideology, representing an elite group within society (no more than 10 percent of the population), and organized along strictly regimented lines. Third, “a technologically conditioned near-complete monopoly of control of all means of effective armed combat” in the hands of the party or its representatives. Fourth, a similar monopoly held by the party over the mass media and all technological forms of communication. Fifth, “a system of terroristic police control” that is not only used to defend the regime against real enemies, but also to persecute various groups of people who are only suspected of being enemies or who may potentially become enemies in the future.\n\nTwo first pillars of any totalitarian government, according to Friedrich and Brzezinski, are the dictator and the Party. The dictator, whether Stalin, Hitler or Mussolini, holds supreme power. Friedrich and Brzezinski explicitly reject the claim that the Party, or any other institution, could provide a significant counterweight to the power of the dictator in Nazism or Stalinism. The dictator needs the Party in order to be able to rule, so he may be careful not to make decisions that would go directly against the wishes of other leading Party members, but ultimate authority rests with him and not with them. Like Arendt, Friedrich and Brzezinski also identify the cult of personality surrounding the leader as an essential element of a totalitarian dictatorship, and reference Stalin’s personality cult in particular. They also draw attention to the fact that Hitler and Stalin were expected to provide ideological direction for their governments and not merely practical leadership. Friedrich and Brzezinski write that “unlike military dictators in the past, but like certain types of primitive chieftains, the totalitarian dictator is both ruler and high priest.” That is to say, he not only governs, but also provides the principles on which his government is to be based. This is partly due to the way that totalitarian governments arise. They come about when a militant ideological movement seizes power, so the first leader of a totalitarian government is usually the ideologue who built the movement that seized power, and subsequent leaders try to emulate him.\n\nThe totalitarian dictator needs loyal lieutenants to carry out his orders faithfully and with a reasonable degree of efficiency. Friedrich and Brzezinski identify parallels between the men in Hitler and Stalin’s entourage, arguing that both dictators used similar people to perform similar tasks. Thus, for example, Martin Bormann and Georgy Malenkov were both capable administrators and bureaucrats, while Heinrich Himmler and Lavrentiy Beria were ruthless secret police chiefs responsible for suppressing any potential challenge to the dictator’s power. Both Hitler and Stalin promoted rivalry and distrust among their lieutenants so as to ensure that none of them would become powerful enough to challenge the dictator himself. This is the cause of an important weakness of the totalitarian regimes: the problem of succession. Friedrich points out that neither the Nazi nor the Stalinist government ever established any official line of succession or any mechanism to decide who would replace the dictator after his death. The dictator, being the venerated “father of the people,” was regarded as irreplaceable. There could never be any heir apparent, because such an heir would have been a threat to the power of the dictator while he was alive. Thus the dictator’s inevitable death would always leave behind a major power vacuum and cause a political crisis. In the case of the Nazi regime, since Hitler died mere days before the final defeat of Germany in the war, this never became a major issue. In the case of the USSR, Stalin’s death led to a prolonged power struggle.\n\nFriedrich and Brzezinski also identify key similarities between the Nazi and Stalinist political parties, which set them apart from other types of political parties. Both the Nazi Party and the CPSU under Stalin had very strict membership requirements and did not accept members on the basis of mere agreement with the Party’s ideology and goals. Rather, they strictly tested potential members, in a manner similar to exclusive clubs, and often engaged in political purges of the membership, expelling large numbers of people from their ranks (and sometimes arresting and executing those expelled, such as in the Great Purge or the Night of the Long Knives). Thus, the totalitarian party cultivates the idea that to be a member is a privilege which needs to be earned, and total obedience to the leader is required in order to maintain this privilege. While both Nazism and Stalinism required party members to display such total loyalty in practice, they differed in the way they dealt with it in theory. Nazism openly proclaimed the hierarchical ideal of absolute obedience to the Führer as one of its key ideological principles (the \"Führerprinzip\"). Stalinism, meanwhile, denied that it did anything similar, and claimed instead to uphold democratic principles, with the Party Congress (made up of elected delegates) supposedly being the highest authority. However, Stalinist elections typically featured only a single candidate, and the Party Congress met very rarely and simply approved Stalin’s decisions. Thus, regardless of the differences in their underlying ideological claims, the Nazi and Stalinist parties were organized in practice along similar lines, with a rigid hierarchy and centralized leadership.\n\nEach totalitarian party and dictator is supported by a specific totalitarian ideology. Friedrich and Brzezinski argue, in agreement with Arendt, that Nazi and Stalinist leaders really believed in their respective ideologies and did not merely use them as tools to gain power. Several major policies, such as the Stalinist collectivization of agriculture or the Nazi “final solution”, cannot be explained by anything other than a genuine commitment to achieve ideological goals, even at great cost. The ideologies were different and their goals were different, but what they had in common was a utopian commitment to reshaping the world, and a determination to fight by any means necessary against a real or imagined enemy. This stereotyped enemy could be described as “the fat rich Jew or the Jewish Bolshevik” for the Nazis, or “the war-mongering, atom-bomb-wielding American Wallstreeter” for the Soviets.\n\nAccording to Friedrich and Brzezinski, the most important difference between Nazi and Stalinist ideology lies in the degree of universality involved. Stalinism, and communist ideology in general, is universal in its appeal and addresses itself to all the “workers of the world.” Nazism, on the other hand, and fascist ideology in general, can only address itself to one particular race or nation – the “master race” that is destined to dominate all others. Therefore, “in communism social justice appears to be the ultimate value, unless it be the classless society that is its essential condition; in fascism, the highest value is dominion, eventually world dominion, and the strong and pure nation-race is \"its\" essential condition, as seen by its ideology.” This means that fascist or Nazi movements from different countries will be natural enemies, rather than natural allies, as they each seek to extend the dominion of their own nation at the expense of others. Friedrich and Brzezinski see this as a weakness inherent in fascist and Nazi ideology, while communist universalism is a source of ideological strength for Stalinism.\n\nFriedrich and Brzezinski also draw attention to the symbols used by Nazis and Stalinists to represent themselves. The Soviet Union adopted the hammer and sickle, a newly-created symbol, “invented by the leaders of the movement and pointing to the future.” Meanwhile, Nazi Germany used the swastika, “a ritual symbol of uncertain origin, quite common in primitive societies.” Thus, one is trying to project itself as being oriented towards a radically new future, while the other is appealing to a mythical heroic past.\n\nTotalitarian dictatorships maintain themselves in power through the use of propaganda and terror, which Friedrich and Brzezinski believe to be closely connected. Terror may be enforced with arrests and executions of dissenters, but it can also take more subtle forms, such as the threat of losing one’s job, social stigma and defamation. “Terror” can refer to any widespread method used to intimidate people into submission as a matter of daily life. According to Friedrich and Brzezinski, the most effective terror is invisible to the people it affects. They simply develop a habit of acting in a conformist manner and not questioning authority, without necessarily being aware that this is what they are doing. Thus, terror creates a society dominated by apparent consensus, where the vast majority of the population appears to support the government. Propaganda is then used to maintain this appearance of popular consent. \n\nTotalitarian propaganda is one of the features that distinguishes totalitarian regimes as modern forms of government and separates them from older autocracies, since a totalitarian government holds complete control over all means of communication (not only public communication such as the mass media, but also private communication such as letters and telephone calls, which are strictly monitored). The methods of propaganda were very similar in the Stalinist USSR and in Nazi Germany. Both Joseph Goebbels and Soviet propagandists sought to demonize their enemies and present a picture of a united people standing behind its leader to confront foreign threats. In both cases there was no attempt to convey complex ideological nuances to the masses, with the message being instead about a simplistic struggle between good and evil. Both Nazi and Stalinist regimes produced two very different sets of propaganda – one for internal consumption and one for potential sympathizers in other countries. And both regimes would sometimes radically change their propaganda line as they made peace with a former enemy or got into a war with a former ally. Yet, paradoxically, a totalitarian government’s complete control over communications renders that government highly misinformed. With no way for anyone to express criticism, the dictator has no way of knowing how much support he actually has among the general populace. With all government policies always declared successful in propaganda, officials are unable to determine what actually worked and what didn’t. Both Stalinism and Nazism suffered from this problem, especially during the war between them. As the war turned against Germany, there was growing opposition to Hitler’s rule, including within the ranks of the military, but Hitler was never aware of this until it was too late (see: 20 July plot). In 1948, during the early days of the Berlin Blockade, the Soviet leadership apparently believed that the population of West Berlin was sympathetic to Soviet Communism and that they would request to join the Soviet zone. Given enough time, the gap between real public opinion and what the totalitarian government believes about public opinion can grow so wide that the government is no longer able to even produce effective propaganda, because it does not know what the people actually think and so it does not know what to tell them. Friedrich and Brzezinski refer to this as the “ritualization of propaganda”: the totalitarian regime continues to produce propaganda as a political ritual, with little real impact on public opinion.\n\nThe totalitarian use of mass arrests, executions and concentration camps – also noted by Arendt – was analyzed at length by Friedrich and Brzezinski. They hold that “totalitarian terror maintains, in institutionalized form, the civil war that originally produced the totalitarian movement and by means of which the regime is able to proceed with its program, first of social disintegration and then of social reconstruction.” Both Stalinism and Nazism saw themselves as engaging in a life-or-death struggle against implacable enemies. But to declare that the struggle had been won would have meant to declare that most of the totalitarian features of the government were no longer needed. A secret police force, for instance, has no reason to exist if there are no dangerous traitors who need to be found. Thus the struggle, or “civil war” against internal enemies, must be institutionalized and must continue indefinitely. In the Stalinist USSR, the repressive apparatus was eventually turned against members of the Communist Party itself in the Great Purge and the show trials that accompanied it. Nazism, by contrast, had a much shorter lifespan in power, and Nazi terror generally maintained an outward focus, with the extermination of the Jews always given top priority. The Nazis did not turn inward towards purging their own party except in a limited way on two occasions (the Night of the Long Knives and the aftermath of the 20 July plot). \n\nThe peak of totalitarian terror was reached with the Nazi concentration camps. These ranged from labor camps to extermination camps, and they are described by Friedrich and Brzezinski as aiming to “eliminate all actual, potential, and imagined enemies of the regime.” As the field of Holocaust studies was still in its early stages at the time of their writing, they do not describe the conditions in detail, but do refer to the camps as involving “extreme viciousness.” They also compare these camps with the Soviet Gulag system, and highlight the use of concentration camps as a method of punishment and execution by Nazi and Stalinist regimes alike. However, unlike Hannah Arendt, who held that the Gulag camps served no economic purpose, Friedrich and Brzezinski argue that they provided an important source of cheap labor for the Stalinist economy.\n\nThe comparative study of Nazism and Stalinism was carried further by other groups of scholars, such as Moshe Lewin and Ian Kershaw together with their collaborators. Writing after the dissolution of the USSR, Lewin and Kershaw take a longer historical perspective and regard Nazism and Stalinism not so much as examples of a new type of society (like Arendt, Friedrich and Brzezinski did), but more as historical “anomalies” – unusual deviations from the typical path of development that most industrial societies are expected to follow. Therefore, the task of comparing Nazism and Stalinism is, to them, a task of explaining why Germany and Russia (along with other countries) deviated from the historical norm. At the outset, Lewin and Kershaw identify similarities between the historical situations in Germany and Russia prior to the First World War and during that war. Both countries were ruled by authoritarian monarchies, who were under pressure to make concessions to popular demands. Both countries had “powerful bureaucracies and strong military traditions.” Both had “powerful landowning classes,” while also being in the process of rapid industrialization and modernization. And both countries had expansionist foreign policies with a particular interest in Central and Eastern Europe. Lewin and Kershaw do not claim that these factors made Stalinism or Nazism inevitable, but rather that they help to explain why the Stalinist and Nazi regimes developed similar features.\n\nIan Kershaw admitted that Stalinism and Nazism are comparable in “the nature and extent of their inhumanity,” but noted that the two regimes were different in a number of aspects Lewin and Kershaw question the usefulness of grouping the Stalinist and Nazi regimes together under a “totalitarian” category, saying that it remains an open question whether the similarities between them are greater or smaller than the differences. In particular, they criticize what they see as the ideologically-motivated attempt to determine which regime killed more people, saying that apologists of each regime are trying to defend their side by claiming the other was responsible for more deaths.\n\nLewin and Kershaw place the cult of personality at the center of their comparison of Nazism and Stalinism, writing that both regimes “represented a new genre of political system centred upon the artificial construct of a leadership cult – the ‘heroic myth’ of the ‘great leader’, no longer a king or emperor but a ‘man of the people.” With regard to Stalinism, they emphasize its bureaucratic character, and its “merging of the most modern with the most archaic traits” by combining modern technology and the latest methods of administration and propaganda with the ancient practice of arbitrary rule by a single man. They compare this with the Prussian military tradition in Germany, which had been called “bureaucratic absolutism” in the eighteenth century, and which played a significant role in the organization of the Nazi state in the twentieth century.\n\nKershaw agrees with Mommsen that there was a fundamental difference between Nazism and Stalinism regarding the importance of the leader. Stalinism had an absolute leader, but he was not essential. He could be replaced by another. Nazism, on the other hand, was a “classic charismatic leadership movement,” defined entirely by its leader. Stalinism had an ideology which existed independently of Stalin. But for Nazism, “Hitler \"was\" ideological orthodoxy” – Nazi ideals were by definition whatever Hitler said they were. In Stalinism, the bureaucratic apparatus was the foundation of the system, while in Nazism, the person of the leader was the foundation.\n\nMoshe Lewin also focuses on the comparison between the personality cults of Hitler and Stalin, and their respective roles in Nazi Germany and the Soviet Union. He refers to them as the “Hitler myth” and the “Stalin myth,” and argues that they served different functions within their two regimes. The function of the “Hitler myth” was to legitimize Nazi rule. The function of the “Stalin myth” was to legitimize not Soviet rule itself, but Stalin’s leadership within the Party. Stalin’s personality cult existed precisely because Stalin knew that he was replaceable, and feared that he might be replaced, and so needed to bolster his authority as much as possible. While the “Hitler myth” was essential to Nazi Germany, the “Stalin myth” was essential only to Stalin, not to the Soviet Union itself.\n\nTogether with fellow historian Hans Mommsen, Lewin argues that the Stalinist and Nazi regimes featured an “intrinsic structural contradiction” which led to “inherent self-destructiveness”: they depended on a highly organized state bureaucracy which was trying to set up complex rules and procedures for every aspect of life, yet this bureaucracy was under the complete personal control of a despot who made policy decisions as he saw fit, routinely changing his mind on major issues, without any regard for the rules and institutions which his own bureaucracy had set up. The bureaucracy and the leader needed each other, but also undermined each other with their different priorities. Mommsen sees this as being a much greater problem in Nazi Germany than in Stalin’s Soviet Union, as the Nazis inherited large parts of the traditional German bureaucracy, while the Soviets largely built their own bureaucracy from the ground up. He argues that many of the irrational features of the Nazi regime – such as wasting resources on exterminating undesirable populations instead of using those resources in the war effort – were caused by the dysfunction of the Nazi state rather than by fanatical commitment to Nazi ideology. In accordance with the Führerprinzip, all decisional power in the Nazi state ultimately rested with Hitler. But Hitler often issued only vague and general directives, forcing other Nazi leaders lower down in the hierarchy to guess what precisely the Führer wanted. This confusion produced competition between Nazi officials, as each of them attempted to prove that he was a more dedicated Nazi than his rivals, by engaging in ever more extreme policies. This competition to please Hitler was, according to Mommsen, the real cause of Nazi irrationality. Hitler was aware of it, and deliberately encouraged it out of a “social-darwinist conviction that the best man would ultimately prevail.” Mommsen argues that this represents a structural difference between the regimes of Hitler and Stalin. In spite of its purges, Stalin’s regime was more effective in building a stable bureaucracy, such that it was possible for the system to sustain itself and continue even without Stalin. The Nazi regime, on the other hand, was much more personalized and depended entirely on Hitler, being unable to build any lasting institutions.\n\nKershaw also saw major personal differences between Stalin and Hitler and their respective styles of rule. He describes Stalin as “a committee man, chief oligarch, man of the machine” and a “creature of his party,” who came to power only thanks to his party and his ability to manipulate the levers of power within that party. Hitler, by contrast, came to power based on his charisma and mass appeal, and in the Nazi regime it was the leader that created the party instead of the other way around. According to Kershaw, “Stalin was a highly interventionist dictator, sending a stream of letters and directives determining or interfering with policy,” while Hitler “was a non-interventionist dictator as far as government administration was concerned,” preferring to involve himself in military affairs and plans for conquest rather than the daily routine of government work, and giving only broad verbal instructions to his subordinates regarding civilian affairs, which they were expected to translate into policy. Furthermore, although both regimes featured all-pervasive cults of personality, there was a qualitative difference between those cults. Stalin’s personality cult was “superimposed upon the Marxist-Leninist ideology and Communist Party,” and could be abandoned (or replaced with a personality cult around some other leader) without major changes to the regime. On the other hand, “the ‘Hitler myth’ was structurally indispensable to, in fact the very basis of, and scarcely distinguishable from, the Nazi Movement and its \"Weltanschauung\".” The belief in the person of Adolf Hitler as the unique savior of the German nation was the very foundation of Nazism, to such an extent that Nazism found it impossible to even imagine a successor to Hitler. Thus, in Kershaw’s analysis, Stalinism was a fundamentally bureaucratic system while Nazism was the embodiment of “charismatic authority” as described by Max Weber. Stalinism could exist without its leader. Nazism could not. \n\nThe topic of comparisons between Nazism and Stalinism was also studied in the 1990s and 2000s by historians Henry Rousso, Nicolas Werth and Philippe Burrin.\n\nRousso defends the work of Carl Friedrich by pointing out that Friedrich himself had only said that Stalinism and Nazism were comparable, not that they were identical. Rousso also argues that the popularity of the concept of totalitarianism (the way that large numbers of people have come to routinely refer to certain governments as “totalitarian”) should be seen as evidence that the concept is useful, that it really describes a specific type of government which is different from other dictatorships. At the same time, however, Rousso notes that the concept of totalitarianism is descriptive rather than analytical: the regimes described as totalitarian do not have a common origin and did not arise in similar ways. Nazism is unique among totalitarian regimes in having taken power in “a country endowed with an advanced industrial economy and with a system of political democracy (and an even older political pluralism).” All other examples of totalitarianism (including the Stalinist regime) took power, according to Rousso, “in an agrarian economy, in a poor society without a tradition of political pluralism, not to mention democracy, and where diverse forms of tyranny had traditionally prevailed.” He sees this as a weakness of the concept of totalitarianism, because it merely describes the similarities between Stalinism and Nazism without dealing with the very different ways they came to power. On the other hand, Rousso agrees with Hannah Arendt that “totalitarian regimes constitute something new in regard to classical tyranny, authoritarian regimes, or other forms of ancient and medieval dictatorships,” and he says that the main strength of the concept of totalitarianism is the way it highlights this inherent novelty of the regimes involved.\n\nNicolas Werth and Philippe Burrin have worked together on comparative assessments of Stalinism and Nazism, with Werth covering the Stalinist regime and Burrin covering Nazi Germany. One of the topics they have studied is the question of how much power the dictator really held in the two regimes. Werth identifies two main historiographical approaches in the study of the Stalinist regime: Those who emphasize the power and control exercised by Joseph Stalin himself, attributing most of the actions of the Soviet government to deliberate plans and decisions made by him, and those who argue that Stalin had no pre-determined course of action in mind, that he was reacting to events as they unfolded, and that the Soviet bureaucracy had its own agenda which often differed from Stalin’s wishes. Werth regards these as two mistaken extremes, one making Stalin seem all-powerful, the other making him seem like a weak dictator. But he believes that the competing perspectives are useful in drawing attention to the tension between two different forms of organization in the Stalinist USSR: an “administrative system of command,” bureaucratic and resistant to change but effective in running the Soviet state, and the strategy of “running the country in a crudely despotic way by Stalin and his small cadre of directors.” Thus, Werth agrees with Lewin that there was an inherent conflict between the priorities of the Soviet bureaucracy and Stalin’s accumulation of absolute power in his own hands. According to Werth, it was this unresolved and unstated conflict that led to the Great Purge and to the use of terror by Stalin’s regime against its own party and state cadres.\n\nIn studying similar issues with regard to the Nazi regime, Philippe Burrin draws attention to the debate between the “Intentionalist” and “Functionalist” schools of thought, which dealt with the question of whether the Nazi regime represented an extension of Hitler’s autocratic will, faithfully obeying his wishes, or whether it was an essentially chaotic and uncontrollable system that functioned on its own with little direct input from the Führer. Like Kershaw and Lewin, Burrin says that the relationship between the leader and his party’s ideology was different in Nazism compared to Stalinism: “One can rightly state that Nazism cannot be dissociated from Hitlerism, something that is difficult to affirm for Bolshevism and Stalinism.” Unlike Stalin, who inherited an existing system with an existing ideology and presented himself as the heir to the Leninist political tradition, Hitler created both his movement and its ideology by himself, claiming to be “someone sent by Providence, a Messiah whom the German people had been expecting for centuries, even for two thousand years, as Heinrich Himmler enjoyed saying.” Thus, there could be no real conflict between the Party and the leader in Nazi Germany, because the Nazi Party’s entire reason for existence was to support and follow Hitler. However, there was a potential for division between the leader and the state bureaucracy, due to the way that Nazism came to power – as part of an alliance with traditional conservative elites, industrialists, and the army. Unlike the USSR, Nazi Germany did not build its own state, but rather inherited the state machinery of the previous government. This provided the Nazis with an immediate supply of capable and experienced managers and military commanders, but on the other hand it also meant that the Nazi regime had to rely on the cooperation of people who had not been Nazis prior to Hitler’s rise to power, and whose loyalty was questionable. It was only during the war, when Nazi Germany conquered large territories and had to create Nazi administrations for them, that brand new Nazi bureaucracies were created without any input or participation from traditional German elites. This produced a surprising difference between Nazism and Stalinism: When the Stalinist USSR conquered territory, it created smaller copies of itself and installed them as the governments of the occupied countries. When Nazi Germany conquered territory, on the other hand, it did not attempt to create copies of the German government back home. Instead, it experimented with different power structures and policies, often reflecting a “far more ample Nazification of society than what the balance of power authorized in the Reich.”\n\nAnother major topic investigated by Werth and Burrin was the violence and terror employed by the regimes of Hitler and Stalin. Werth reports that the Stalinist USSR underwent an “extraordinary brutalization of the relations between state and society” for the purpose of rapid modernization and industrialization, to “gain one hundred years in one decade, and to metamorphose the country into a great industrial power.” This transformation was accomplished at the cost of massive violence and a sociopolitical regression into what Werth calls “military-feudal exploitation.” The types of violence employed by the Stalinist regime included loss of civil rights, mass arrests, deportations of entire ethnic groups from one part of the USSR to another, forced labor in the Gulag, mass executions (especially during the Great Terror of 1937-38), and most of all the great famine of 1932-33, known as the Holodomor. All levels of Soviet society were affected by Stalinist repression, from the top to the bottom. At the top, high-ranking members of the Communist Party were arrested and executed under the claim that they had plotted against Stalin (and in some cases they were forced to confess to imaginary crimes in show trials). At the bottom, the peasantry suffered the Holodomor famine (especially in Ukraine), and even outside of the famine years they were faced with very high grain quotas.\n\nWerth identifies four categories of people that became the targets of Stalinist violence in the USSR. He lists them from smallest to largest. The first and smallest group consisted of many of Stalin’s former comrades-in-arms, who had participated in the revolution and were known as “Old Bolsheviks.” They were dangerous to Stalin because they had known him before his rise to power and could expose the many false claims made by his personality cult. The second group consisted of mid-level Communist Party officials, who were subject to mass arrests and executions in the late 1930s, particularly during the Great Purge. Eliminating them served a dual purpose: It helped Stalin to centralize power in the Kremlin (as opposed to regional centers), and it also provided him with “corrupt officials” that he could blame for earlier repressions and unpopular policies. Werth draws parallels between this and the old Tsarist tradition of blaming “bad bureaucrats” – rather than the Tsar – for unpopular government actions. The third group was made up of ordinary citizens from all walks of life who resorted to petty crime in order to provide for themselves in the face of worsening living standards (for example by taking home some wheat from the fields or tools from the factory). This type of petty crime became very widespread, and was often punished as if it were intentional sabotage motivated by political opposition to the USSR. The fourth and largest category consisted of ethnic groups that were subject to deportation, famine, or arbitrary arrests under the suspicion of being collectively disloyal to Stalin or to the Soviet state. This included the Holodomor famine directed at the Ukrainians, the deportation of ethnic groups suspected of pro-German sympathies (such as the Volga Germans, the Crimean Tatars, the Chechens and others), and eventually also persecution of ethnic Jews, especially as Stalin grew increasingly antisemitic near the end of his life.\n\nBurrin’s study of violence carried out by the Nazi regime begins with the observation that “violence is at the heart of Nazism,” and that Nazi violence is “established as a doctrine and exalted in speech.” This marks a point of difference between Nazism and Stalinism, according to Burrin. In Stalinism, there was a gulf between ideology and reality when it came to violence. The Soviet regime continuously denied that it was repressive, proclaimed itself a defender of peace, and sought to conceal all the evidence to the contrary. In Nazism, on the other hand, “doctrine and reality were fused from the start.” Nazism not only practiced violent repression and war, but advocated it in principle as well, considering war to be a positive force in human civilization and openly seeking ”living space” and the domination of the European continent by ethnic Germans.\n\nBurrin identifies three motivations for Nazi violence: political repression, exclusion and social repression, and racial politics. The first of these, political repression, is common in many dictatorships. The Nazis aimed to eliminate their real or imagined political opponents, first in the Reich and later in the occupied territories during the war. Some of these opponents were executed, while others were imprisoned in concentration camps. The first targets of political repression, immediately after Hitler’s rise to power in 1933, were the parties of the Left in general and the Communists in particular. Then, after the mid-1930s, repression was extended to members of the clergy, and later to the conservative opposition as well (especially after the failed attempt to assassinate Hitler in 1944). The death penalty was used on a wide scale, even before the war. During the war, political repression was greatly expanded both inside Germany and especially in the newly occupied territories. Political prisoners in the concentration camps numbered only about 25,000 at the beginning of the war. By January 1945 they had swelled to 714,211 – most of them non-Germans accused of plotting against the Reich.\n\nThe second type of Nazi violence, motivated by exclusion and social repression, was the violence aimed at purging German society of people whose lifestyle was considered incompatible with the social norms of the Nazi regime (even if the people involved were racially pure and able-bodied). Such people were divided into two categories: homosexuals and “asocials.” The “asocials” were only vaguely defined, and included “Gypsies, tramps, beggars, prostitutes, alcoholics, the jobless who refused any employment, and those who left their work frequently or for no reason.”\n\nThe third and final type of Nazi violence, by far the most extensive, was violence motivated by Nazi racial policies. This was aimed both inward, to cleanse the “Aryan race” of “degenerate” elements and life unworthy of life, as well as outward, to seek the extermination of “inferior races”. Germans considered physically or mentally unfit were among the first victims. One of the first laws of the Nazi regime mandated the forced sterilization of people suffering from physical handicaps or who had psychiatric conditions deemed to be hereditary. Later, sterilization was replaced by murder of the mentally ill and of people with severe disabilities, as part of a “euthanasia” program called Aktion T4. Burrin notes that this served no practical political purpose – the people being murdered could not have possibly been political opponents of the regime – so the motivation was purely a matter of racial ideology. The most systematic and by far the most large-scale acts of Nazi violence, however, were directed at “racially inferior” non-German populations. As laid out in \"Generalplan Ost\", the Nazis wished to eliminate most of the Slavic populations of Eastern Europe, partly through deportation and partly through murder, in order to secure land for ethnic German settlement and colonization. But even more urgently, the Nazis wished to exterminate the Jews of Europe, whom they regarded as the implacable racial enemy of the Germans. This culminated in the Holocaust, the Nazi genocide of the Jews. Unlike in the case of all other target populations, the Jews were to be exterminated completely, with no individual exceptions for any reason.\n\nIn \"Beyond Totalitarianism: Stalinism and Nazism Compared\", editors Michael Geyer and Sheila Fitzpatrick disputed the concept of totalitarianism, noting that the term entered political discourse first as a term of self-description by the Italian Fascists and was only later used as a framework to compare Nazi Germany with the Soviet Union. They argued that the totalitarian states were not as monolithic or as ideology-driven as they seemed. Geyer and Fitzpatrick describe Nazi Germany and the Stalinist USSR as “immensely powerful, threatening, and contagious dictatorships” who “shook the world in their antagonism.” Without calling them totalitarian, they identified their common features, including genocide, an all-powerful party, a charismatic leader, and pervasive invasion of privacy. However, they argue that Stalinism and Nazism did not represent a new and unique type of government, but rather that they can be placed in the broader context of the turn to dictatorship in Europe in the interwar period. The reason they appear extraordinary is because they were the “most prominent, most hard-headed, and most violent” of the European dictatorships of the 20th century. They are comparable because of their “shock and awe” and sheer ruthlessness, but underneath superficial similarities they were fundamentally different and that “when it comes to one-on-one comparison, the two societies and regimes may as well have hailed from different worlds.”\n\nAccording to Geyer and Fitzpatrick, the similarities between Nazism and Stalinism stem from the fact that they were both “ideology driven” and sought to subordinate all aspects of life to their respective ideologies. The differences stem from the fact that their ideologies were opposed to each other and regarded each other as enemies. Another major difference is that Stalin created a stable and long-lasting regime, while Nazi Germany had a “short-lived, explosive nature.” Notably, the stable state created by Stalinism was based on an entirely new elite, while Nazism, despite having the support of the traditional elite, failed to achieve stability.\n\nHowever, the two regimes did borrow ideas from one another, especially regarding propaganda techniques (most of all in architecture and cinema), but also in terms of state surveillance and antisemitism. At the same time, they both vigorously denied borrowing anything from each other. While their methods of propaganda were similar, the content was different. For instance, Soviet wartime propaganda revolved around the idea of resisting imperial aggression, while Nazi propaganda was about wars of racial conquest. Geyer and Fitzpatrick also take note of the fact that both Stalinism and Nazism sought to create a New Man, an “entirely modern, illiberal, and self-fashioned personage,” even though they had different visions about what being a “New Man” would mean.\n\nAmong the other authors contributing to the volume edited by Geyer and Fitzpatrick, David Hoffmann and Annette Timm discuss biopolitics and the pro-natalist policies of the Nazi and Stalinist regimes. Both governments were highly concerned over low fertility rates in their respective populations, and applied extensive and intrusive social engineering techniques to increase the number of births. Reproductive policies in the Soviet Union and Nazi Germany were administered through their health care systems—both regimes saw health care as a key pillar to their designs to develop a new society. While the Soviet Union had to design a public health care system from scratch, Nazi Germany built upon the pre-existing public health care system in Germany that had existed since 1883, when Otto von Bismarck's legislation had created the world's first national public health care program. The Nazis centralized the German health care system in order to enforce Nazi ideological components upon it, and replaced existing voluntary and government welfare agencies with new ones that were devoted to racial hygiene and other components of Nazi ideology.\n\nThe Nazi and Stalinist attempt to control family size was not unique, as many other European states practiced eugenics at this time, and the Stalinist and Nazi ideals were vastly different. In fact, they had more in common with third parties than with each other: Nazi Germany’s policies were rather similar to those in Scandinavia at the time, while the USSR’s policies resembled those in Catholic countries.The common point between Nazi and Stalinist practices was the connection of reproduction policies with the ideological goals of the state — \"part of the project of a rational, hypermodern vision for the re-organization of society\". There were nevertheless substantial differences between the two regimes' approaches. Stalin's Soviet Union never officially supported eugenics as the Nazis did—the Soviet government called eugenics a \"fascist science\"—although there were in fact Soviet eugenicists. The two regimes also had different approaches to the relationship between family and paid labor—Nazism promoted the male single-breadwinner family while Stalinism promoted the dual-wage-earner household.\n\nIn another contribution to the same volume, Christian Gerlach and Nicolas Werth discuss the topic of mass violence, and the way that it was used by both Stalinism and Nazism. Both Stalin's Soviet Union and Nazi Germany were violent societies where mass violence was accepted by the state, such as in the Great Terror of 1937 to 1938 in the Soviet Union and the Holocaust in Nazi Germany and its occupied territories in World War II.\n\nBoth the Stalinist Soviet Union and Nazi Germany utilized internment camps led by agents of the state – the NKVD in the Soviet Union and the SS in Nazi Germany. They also both engaged in violence against minorities based on xenophobia – the xenophobic violence of the Nazis was outspoken but rationalized as being against \"asocial\" elements while the xenophobic violence of the Stalinists was disguised as being against \"anti-soviet\", \"counter-revolutionary\" and \"socially harmful\" elements – a term which often targeted diaspora nationalities. The Stalinist Soviet Union established \"special settlements\" where the \"socially harmful\" or \"socially dangerous\" who included ex-convicts, criminals, vagrants, the disenfranchised and \"declassed elements\" were expelled to. These \"special settlements\" were largely in Siberia, the far north, the Urals, or other inhospitable territories. In July 1933, the Soviet Union made a mass arrest of 5000 Romani people effectively on the basis of their ethnicity, who were deported that month to the \"special settlements\" in Western Siberia. In 1935, the Soviet Union arrested 160,000 homeless people and juvenile delinquents and sent many of them to NKVD labor colonies where they did forced labor.\n\nThe Nazi regime was founded upon a racialist view of politics and envisioned the deportation or extermination of the majority of the population of Eastern Europe in order to open up “living space” for ethnic German settlers. This was mainly intended to be carried out after an eventual German victory in the war, but steps had already started being taken while the war was still ongoing. For instance, by the end of 1942, the Nazis had deported 365,000 Poles and Jews from their original homes in western Poland (now German-annexed) and into the General Government. A further 194,000 Poles were internally displaced (not deported to another territory but expelled from their homes). The Nazis had also deported 100,000 persons from Alsace, Lorraine, and Luxembourg, as well as 54,000 Slovenians.\n\nStalinism in practice in the Soviet Union pursued ethnic deportations from the 1930s to the early 1950s, with a total of 3 million Soviet citizens being subjected to ethnic-based resettlement. The first major ethnic deportation took place from December 1932 to January 1933, during which some 60,000 Kuban Cossacks were collectively criminally charged as a whole with association with resistance to socialism and affiliation with Ukrainian nationalism. From 1935 to 1936, the Soviet Union deported Soviet citizens of Polish and German origins living in the western districts of Ukraine, and Soviet citizens of Finnish origins living on the Finland-Soviet Union border. These deportations from 1935 to 1936 affected tens of thousands of families. From September to October 1937, Soviet authorities deported the Korean minority from its Far Eastern region that bordered on Japanese-controlled Korea. Soviet authorities claimed the territory was \"rich soil for the Japanese to till\" – implying a Soviet suspicion that the Koreans could potentially join forces with the Japanese to unite the land with Japanese-held Korea. Over 170,000 Koreans were deported to remote parts of Soviet Central Asia from September to October 1937. These ethnically-based deportations reflected a new trend in Stalinist policy, a \"Soviet xenophobia\" based on ideological grounds that suspected that these people were susceptible to foreign influence, and which was also based on a resurgent Russian nationalism.\n\nAfter Nazi Germany declared war on the Soviet Union in 1941, the Soviet Union initiated another major round of ethnic deportations. The first group targeted were Soviet Germans. Between September 1941 and February 1942, 900,000 people – over 70 percent of the entire Soviet German community – were deported to Kazakhstan and Siberia in mass operations. A second wave of mass deportations took place between November 1943 and May 1944, in which Soviet authorities expelled six ethnic groups (the Balkars, Chechens, Crimean Tatars, Ingush, Karachai, and Kalmyks) that together numbered 900,000. There were also smaller-scale operations involving ethnic cleansing of diaspora minorities during and after World War II, in which tens of thousands of Crimean Bulgarians, Greeks, Iranians, Khemshils, Kurds, and Meskhetian Turks were deported from the Black Sea and Transcaucasian border regions.\n\nTwo ethnic groups that were specifically targeted for persecution by Stalin's Soviet Union were the Chechens and the Ingush. Unlike the other nationalities that could be suspected of connection to foreign states which shared their ethnic background, the Chechens and the Ingush were completely indigenous people of the Soviet Union. Rather than being accused of collaboration with foreign enemies, these two ethnic groups were considered to have cultures which did not fit in with Soviet culture – such as accusing Chechens of being associated with “banditism” – and the authorities claimed that the Soviet Union had to intervene in order to “remake” and “reform” these cultures. In practice this meant heavily armed punitive operations carried out against Chechen “bandits” that failed to achieve forced assimilation, culminating in an ethnic cleansing operation in 1944, which involved the arrests and deportation of over 500,000 Chechens and Ingush from the Caucasus to Central Asia and Kazakhstan. The deportations of the Chechens and Ingush also involved the outright massacre of thousands of people, and severe conditions placed upon the deportees – they were put in unsealed train cars, with little to no food for a four-week journey during which many died from hunger and exhaustion.\n\nThe main difference between Nazi and Stalinist deportations was in their purpose: while Nazi Germany sought ethnic cleansing to allow settlement by Germans into the cleansed territory, Stalin's Soviet Union pursued ethnic cleansing in order to remove minorities from strategically important areas.\n\nOther historians and political scientists have also made comparisons between Nazism and Stalinism as part of their work.\n\nStanley Payne, in his work on fascism, said that although the Nazi Party was ideologically opposed to communism, Adolf Hitler and other Nazi leaders frequently expressed recognition that only in Soviet Russia were their revolutionary and ideological counterparts to be found. Both placed a major emphasis on creating a \"party-army,\" with the regular armed forces controlled by the party. In the case of the Soviet Union this was done through the political commissars, while Nazi Germany introduced a roughly equivalent leadership role for \"National Socialist Guidance Officers\" in 1943.\n\nFrançois Furet, in his work on communism, noted that Hitler personally admired Soviet leader Joseph Stalin, and on numerous occasions publicly praised Stalin for seeking to purify the Communist Party of the Soviet Union of Jewish influences, especially by purging Jewish communists such as Leon Trotsky, Grigory Zinoviev, Lev Kamenev and Karl Radek.\n\nRichard Pipes draws attention to Stalin and his antisemitism in a parallel with Nazi antisemitism. He notes that soon after the 1917 October Revolution, the Soviet Union undertook practices to break up Jewish culture, religion and language. In the fall of 1918, the Soviet Communist Party set up the Jewish section Yevsektsiya, with a stated mission of “destruction of traditional Jewish life, the Zionist movement, and Hebrew culture.” By 1919, the Bolsheviks began to confiscate Jewish properties, Hebrew schools, libraries, books, and synagogues in accordance with newly imposed anti-religious laws, turning their buildings into \"Communist centers, clubs or restaurants.\" After Joseph Stalin rose to power, antisemitism continued to be endemic throughout Russia, although official Soviet policy condemned it. On August 12, 1952, Stalin's personal antisemitism became more visible, as he ordered the execution of the most prominent Yiddish authors in the Soviet Union, in an event known as the \"Night of the Murdered Poets\". Shortly before his death, Stalin also organized the anti-Semitic campaign known as the Doctors' plot.\n\nA number of research institutions are focusing on the analysis of fascism/Nazism and Stalinism/communism, and the comparative approach, including the Hannah Arendt Institute for the Research on Totalitarianism in Germany, the Institute for the Study of Totalitarian Regimes in the Czech Republic and the Institute of National Remembrance in Poland.\n\nIn comparing the deaths caused by both Stalin and Hitler's policies, some historians have asserted that archival evidence released after the collapse of the USSR confirms that Stalin did not kill more people than Hitler. American historian Timothy D. Snyder, for example, after assessing such data, says that while the Nazi regime killed approximately 11 million non-combatants (which rises to above 12 million if \"foreseeable deaths from deportation, hunger, and sentences in concentration camps are included\"), Stalin's deliberately killed about 6 million (rising to 9 million if foreseeable deaths arising from policies are taken into account). Australian historian and archival researcher Stephen G. Wheatcroft posits that \"The Stalinist regime was consequently responsible for about a million purposive killings, and through its criminal neglect and irresponsibility it was probably responsible for the premature deaths of about another two million more victims amongst the repressed population, i.e. in the camps, colonies, prisons, exile, in transit and in the POW camps for Germans. These are clearly much lower figures than those for whom Hitler's regime was responsible.\" Wheatcroft also says that, unlike Hitler, Stalin's \"purposive killings\" fit more closely into the category of \"execution\" than \"murder\", given he thought the accused were indeed guilty of crimes against the state and insisted on documentation, whereas Hitler simply wanted to kill Jews and communists because of who they were, and insisted on no documentation and was indifferent at even a pretence of legality for these actions.\n\nKristen R. Ghodsee, an ethnographer of post-Cold War Eastern Europe, contends that the efforts to institutionalize the \"double genocide thesis\", or the moral equivalence between the Nazi Holocaust (race murder) and the victims of communism (class murder), and in particular the recent push at the beginning of the global financial crisis for commemoration of the latter in Europe, can be seen as the response by economic and political elites to fears of a leftist resurgence in the face of devastated economies and extreme inequalities in both the East and West as the result of neoliberal capitalism. She notes that any discussion of the achievements under communism, including literacy, education, women’s rights, and social security is usually silenced, and any discourse on the subject of communism is focused almost exclusively on Stalin's crimes and the \"double genocide thesis\", an intellectual paradigm summed up as such: \"1) any move towards redistribution and away from a completely free market is seen as communist; 2) anything communist inevitably leads to class murder; and 3) class murder is the moral equivalent of the Holocaust.\" By linking all leftist and socialist ideals to the excesses of Stalinism, Ghodsee concludes, the elites in the West hope to discredit and marginalize all political ideologies that could \"threaten the primacy of private property and free markets.\"\n\nThe comparison of Stalinism and Nazism remains a neglected field of academic study.\n\nThe comparison of Nazism and Stalinism has long provoked political controversy, and it led to the historians' dispute within Germany in the 1980s.\n\nIn the 1920s, the Social Democratic Party of Germany (SPD), under the leadership of Chancellor Hermann Müller, adopted the view that \"red equals brown\", i.e. that the communists and Nazis posed an equal danger to liberal democracy. In 1930, Kurt Schumacher said that the two movements enabled each other. He argued that the Communist Party of Germany, which was staunchly Stalinist, were \"red-painted Nazis.\" This comparison was mirrored by the social fascism theory advanced by the Soviet government and the Comintern (including the Communist Party of Germany), which accused social democracy of enabling fascism and went as far as to call social democrats \"social fascists.\" After the 1939 Molotov–Ribbentrop Pact was announced, \"The New York Times\" published an editorial arguing that \"Hitlerism is brown communism, Stalinism is red fascism.\"\n\nMarxist theories of fascism have seen fascism as a form of reaction to socialism and a feature of capitalism. Several modern historians have tried to pay more attention to the economic, political and ideological differences between these two regimes than to their similarities. \n\nThe 2008 Prague Declaration on European Conscience and Communism, initiated by the Czech government and signed by figures such as Václav Havel, called for \"a common approach regarding crimes of totalitarian regimes, inter alia Communist regimes\" and for\nThe Communist Party of Greece opposes the Prague Declaration and has criticized \"the new escalation of the anti-communist hysteria led by the EU council, the European Commission and the political staff of the bourgeois class in the European Parliament.\" The Communist Party of Britain opined that the Prague Declaration \"is a rehash of the persistent attempts by reactionary historians to equate Soviet Communism and Hitlerite Fascism, echoing the old slanders of British authors George Orwell and Robert Conquest.\"\n\nThe 2008 documentary film \"The Soviet Story\", commissioned by the Union for Europe of the Nations group in the European Parliament, published archival records which listed thousands of German Jews who were arrested in the Soviet Union by the NKVD (People's Commissariat for Internal Affairs) from 1937 to 1941 and handed over to Gestapo or SS officials in Germany. These German Jews had originally sought asylum in the USSR. The documentary film accuses Stalin's regime of being an accomplice in Hitler's Holocaust by arresting these asylum seekers and sending them back to Germany.\n\nSince 2009, the European Union has officially commemorated the European Day of Remembrance for Victims of Stalinism and Nazism, proclaimed by the European Parliament in 2008 and endorsed by the Organization for Security and Co-operation in Europe in 2009, and officially known as the Black Ribbon Day in some countries (including Canada).\n\nThe former President of the European Parliament and Christian Democratic Union member, Hans-Gert Pöttering, argued that \"both totalitarian systems (Stalinism and Nazism) are comparable and terrible.\"\n\nIn some Eastern European countries the denial of both Nazi and Communist crimes has been explicitly outlawed, and Czech foreign minister Karel Schwarzenberg has argued that \"there is a fundamental concern here that totalitarian systems be measured by the same standard.\" However, the European Commission rejected calls for similar EU-wide legislation, due to the lack of consensus among member states.\n\nA statement adopted by Russia's legislature said that comparisons of Nazism and Stalinism are \"blasphemous towards all of the anti-fascist movement veterans, Holocaust victims, concentration camp prisoners and tens of millions of people ... who sacrificed their lives for the sake of the fight against the Nazis' anti-human racial theory.\"\n\nBritish journalist Seumas Milne posits that the impact of the post-Cold War narrative that Stalin and Hitler were twin evils, and therefore Communism is as monstrous as Nazism, \"has been to relativise the unique crimes of Nazism, bury those of colonialism and feed the idea that any attempt at radical social change will always lead to suffering, killing and failure.\"\n\n\n"}
{"id": "32639223", "url": "https://en.wikipedia.org/wiki?curid=32639223", "title": "Comparison of online charity donation services in the United Kingdom", "text": "Comparison of online charity donation services in the United Kingdom\n\nThe page is a comparison of notable online charity donation services in the UK.\n\nThe table below gives examples of the various transaction fees for a £10 donation using each organisation, assuming they claim back the tax for the charity using gift aid. (Charities may also be charged set-up fees and monthly fees as detailed above.)\n\n\n"}
{"id": "1996367", "url": "https://en.wikipedia.org/wiki?curid=1996367", "title": "Comparison of web template engines", "text": "Comparison of web template engines\n\nThe following table lists the various Web Template Engines used in Web template systems and a brief rundown of their features.\n\n\n"}
{"id": "590473", "url": "https://en.wikipedia.org/wiki?curid=590473", "title": "Contraction (grammar)", "text": "Contraction (grammar)\n\nA contraction is a shortened version of the written and spoken forms of a word, syllable, or word group, created by omission of internal letters and sounds.\n\nIn linguistic analysis, contractions should not be confused with crasis, abbreviations nor acronyms (including initialisms), with which they share some semantic and phonetic functions, though all three are connoted by the term \"abbreviation\" in loose parlance. Contraction is also distinguished from clipping, where beginnings and endings are omitted.\n\nThe definition overlaps with the term portmanteau (a linguistic \"blend\"), but a distinction can be made between a portmanteau and a contraction by noting that contractions are formed from words that would otherwise appear together in sequence, such as \"do\" and \"not\", whereas a portmanteau word is formed by combining two or more existing words that all relate to a singular concept which the portmanteau describes.\n\nEnglish has a number of contractions, mostly involving the elision of a vowel (which is replaced by an apostrophe in writing), as in \"I'm\" for \"I am\", and sometimes other changes as well, as in \"won't\" for \"will not\" or \"ain't\" for \"am not\". These contractions are commonly used in speech and in informal writing, though tend to be avoided in more formal writing (with limited exceptions, such as the mandatory form of \"o'clock\").\n\nThe main contractions are listed in the following table (for more explanation see English auxiliaries and contractions).\nSome other simplified pronunciations of common word groups, which can often equally be described as cases of elision, may also be considered (non-standard) contractions (not enshrined into the written standard language, but frequently expressed in written form anyway), such as \"wanna\" for \"want to\", \"gonna\" for \"going to\", \"y'all\" for \"you all\", \"ya'll\" for \"ya all\" in the Southern United States and others common forms in colloquial speech.\n\nIn subject–auxiliary inversion, the contracted negative forms behave as if they were auxiliaries themselves, changing place with the subject. For example, the interrogative form of \"He won't go\" is \"Won't he go\", whereas the uncontracted equivalent is \"Will he not go?\", with \"not\" following the subject.\n\nContractions exist in Classical Chinese, some of which are used in modern Chinese.\nContractions also appear in Cantonese, for example, 乜嘢 and 咩.\n\nThe French language has a variety of contractions, similar to English but mandatory, as in \"C'est la vie\" (\"That's life\"), where \"c'est\" stands for \"ce\" + \"est\" (\"that is\"). The formation of these contractions is called elision.\n\nIn general, any monosyllabic word ending in \"e caduc\" (schwa) will contract if the following word begins with a vowel, \"h\" or \"y\" (as \"h\" is silent and absorbed by the sound of the succeeding vowel; \"y\" sounds like \"i\"). In addition to \"ce\" → \"c'-\" (demonstrative pronoun \"that\"), these words are \"que\" → \"qu'-\" (conjunction, relative pronoun, or interrogative pronoun \"that\"), \"ne\" → \"n'-\" (\"not\"), \"se\" → \"s'-\" (\"himself\", \"herself\", \"itself\", \"oneself\" before a verb), \"je\" → \"j'-\" (\"I\"), \"me\" → \"m'-\" (\"me\" before a verb), \"te\" → \"t'- \" (informal singular \"you\" before a verb), \"le\" or \"la\" → \"l'-\" (\"the\"; or \"he/she\", \"it\" before a verb or after an imperative verb and before the word \"y\" or \"en\"), and \"de\" → \"d'-\" (\"of\"). Unlike with English contractions, however, these contractions are mandatory: one would never say (or write) \"*ce est\" or \"*que elle\".\n\n\"Moi\" (\"myself\") and \"toi\" (informal \"yourself\") mandatorily contract to \"m'-\" and \"t'-\" respectively after an imperative verb and before the word \"y\" or \"en\".\n\nIt is also mandatory to avoid the repetition of a sound when the conjunction \"si\" (\"if\") is followed by \"il\" (\"he\", \"it\") or \"ils\" (\"they\"), which begin with the same vowel sound \"i\": \"*si il\" → \"s'il\" (\"if it\", if he\"); \"*si ils\" → \"s'ils\" (\"if they\").\n\nCertain prepositions are also mandatorily merged with masculine and plural direct articles: \"au\" for \"à le\", \"aux\" for \"à les\", \"du\" for \"de le\", and \"des\" for \"de les\". However, the contraction of \"cela\" (demonstrative pronoun \"that\") to \"ça\" is optional and informal.\n\nIn informal speech, a personal pronoun may sometimes be contracted onto a following verb. For example, \"je ne sais pas\" (, \"I don't know\") may be pronounced roughly \"chais pas\" (), with the \"ne\" being completely elided and the of \"je\" being mixed with the of \"sais\". It is also common in informal contexts to contract \"tu\" to \"t'-\" before a vowel, e.g., \"t'as mangé\" for \"tu as mangé\".\n\nIn Modern Hebrew, the prepositional prefixes -בְּ /bə-/ 'in' and -לְ /lə-/ 'to' contract with the definite article prefix -ה (/ha-/) to form the prefixes -ב /ba/ 'in the' and -ל /la/ 'to the'. In colloquial Israeli Hebrew, the preposition את (/ʔet/), which indicates a definite direct object, and the definite article prefix -ה (/ha-/) are often contracted to 'ת (/ta-/) when the former immediately precedes the latter. Thus ראיתי את הכלב (/ʁaˈʔiti ʔet haˈkelev/, \"I saw the dog\") may become ראיתי ת'כלב (/ʁaˈʔiti taˈkelev/).\n\nIn Italian, prepositions merge with direct articles in predictable ways. The prepositions \"a\", \"da\", \"di\", \"in\", \"su\", \"con\" and \"per\" combine with the various forms of the definite article, namely \"il\", \"lo\", \"la\", \"l',\" \"i\", \"gli\", \"gl',\" and \"le\".\n\n\nThe words \"ci\" and \"è\" (form of \"essere\", to be) and the words \"vi\" and \"è\" are contracted into \"c'è\" and \"v'è\" (both meaning \"there is\").\n\nThe words \"dove\" and any word that begins with \"e\" are contracted into one single, deleting the e of the principal word, dove (dov'). Equally \"come\" does be made so.\nAs well other words may be contracted the same these two, like \"quale\", and other ones, etcetera.\n\nSpanish has two mandatory phonetic contractions between prepositions and articles: \"al\" (to the) for \"a el\", and \"del\" (of the) for \"de el\" (not to be confused with \"a él\", meaning \"to him\", and \"de él\", meaning \"his\" or, more literally, \"of him\").\n\nOther contractions were common in writing until the 17th century, the most usual being \"de\" + personal and demonstrative pronouns: \"destas\" for \"de estas\" (of these, fem.), \"daquel\" for \"de aquel\" (of that, masc.), \"dél\" for \"de él\" (of him) etc.; and the feminine article before words beginning with \"a-\": \"l'alma\" for \"la alma\", now \"el alma\" (the soul). Several sets of demonstrative pronouns originated as contractions of \"aquí\" (here) + pronoun, or pronoun + \"otro/a\" (other): \"aqueste\", \"aqueso\", \"estotro\" etc. The modern \"aquel\" (that, masc.) is the only survivor of the first pattern; the personal pronouns \"nosotros\" (we) and \"vosotros\" (pl. you) are remnants of the second. In medieval texts unstressed words very often appear contracted: \"todol\" for \"todo el\" (all the, masc.), \"ques\" for \"que es\" (which is); etc. including with common words, like d'ome (d'home/d'homme) instead de ome (home/homme), and so on.\n\nThough not strictly a contraction, a special form is used when combining con with mí, ti or sí which is written as \"conmigo\" for *\"con mí\" (with me), \"contigo\" for *\"con ti\" (with you sing.), \"consigo\" for *\"con sí\" (with himself/herself/itself/themselves (themself).\n\nFinally, one can hear \"pa\"' for \"para\", deriving as \"pa'l\" for \"para el\", but these forms are only considered appropriate in informal speech.\n\nIn Portuguese, contractions are common and much more numerous than those in Spanish. Several prepositions regularly contract with certain articles and pronouns. For instance, \"de\" (of) and \"por\" (by; formerly \"per\") combine with the definite articles \"o\" and \"a\" (masculine and feminine forms of \"the\" respectively), producing \"do\", \"da\" (of the), \"pelo\", \"pela\" (by the). The preposition \"de\" contracts with the pronouns \"ele\" and \"ela\" (he, she), producing \"dele\", \"dela\" (his, her). In addition, some verb forms contract with enclitic object pronouns: e.g., the verb \"amar\" (to love) combines with the pronoun \"a\" (her), giving \"amá-la\" (to love her).\n\nAnother contraction in portuguese which is similar to English ones is the combination of the pronoun \"da\" with words starting in \"a\", resulting in changing the first letter \"a\" for an apostrophe and joining both words. Examples: \"Estrela d'alva\" (A popular phrase to refer to Venus that means \"Alb star\", as a reference to its brightness) ; \"Caixa d'água\" (water tank).\n\nIn informal, spoken German prepositional phrases, one can often merge the preposition and the article; for example, \"von dem\" becomes \"vom\", \"zu dem\" becomes \"zum\", or \"an das\" becomes \"ans\". Some of these are so common that they are mandatory. In informal speech, \"aufm\" for \"auf dem\", \"unterm\" for \"unter dem\", etc. are also used, but would be considered to be incorrect if written, except maybe in quoted direct speech, in appropriate context and style.\n\nThe pronoun \"es\" often contracts to \"s\" (usually written with the apostrophe) in certain contexts. For example, the greeting \"Wie geht es?\" is usually encountered in the contracted form \"Wie geht's?\".\n\nRegional dialects of German, and various local languages which usually were already used long before today's Standard German was created, do use contractions usually more frequently than German, but varying widely between different local languages. The informally spoken German contractions are observed almost everywhere, most often accompanied by additional ones, such as \"in den\" becoming \"in'n\" (sometimes \"im\") or \"haben wir\" becoming \"hamwer\", \"hammor\", \"hemmer\", or \"hamma\" depending on local intonation preferences. Bavarian German features several more contractions such as \"gesund sind wir\" becoming \"xund samma\" which are schematically applied to all word or combinations of similar sound. (One must remember, however, that German \"wir\" exists alongside Bavarian \"mir\", or \"mia\", with the same meaning.) The Munich-born footballer Franz Beckenbauer has as his catchphrase \"Schau mer mal\" (\"Schauen wir einmal\" - in English \"let's have a look\"). A book about his career had as its title the slightly longer version of the phrase, \"Schau'n Mer Mal\".\n\nSuch features are found in all central and southern language regions. A sample from Berlin: \"Sag einmal, Meister, kann man hier einmal hinein?\" is spoken as \"Samma, Meesta, kamma hier ma rin?\"\n\nSeveral West Central German dialects along the Rhine River have built contraction patterns involving long phrases and entire sentences. In speech, words are often concatenated, and frequently the process of \"liaison\" is used. So, \"[Dat] kriegst Du nicht\" may become \"Kressenit\", or \"Lass mich gehen, habe ich gesagt\" may become \"Lomejon haschjesaat\".\n\nMostly, there are no binding orthographies for local dialects of German, hence writing is left to a great extent to authors and their publishers. Outside quotations, at least, they usually pay little attention to print more than the most commonly spoken contractions, so as not to degrade their readability. The use of apostrophes to indicate omissions is a varying and considerably less frequent process than in English-language publications.\n\nThe use of contractions is not allowed in any form of standard Norwegian spelling, however, it is fairly common to shorten or contract words in spoken language. Yet, the commonness varies from dialect to dialect and from sociolect to sociolect—it depends on the formality etc. of the setting. Some common, and quite drastic, contractions found in Norwegian speech are \"jakke\" for \"jeg har ikke\", meaning \"I do not have\" and \"dække\" for \"det er ikke\", meaning \"there is not\". The most frequently used of these contractions—usually consisting of two or three words contracted into one word, contain short, common and often monosyllabic words like , , , , or . The use of the apostrophe (') is much less common than in English, but is sometimes used in contractions to show where letters have been dropped.\n\nIn extreme cases, long, entire sentences may be written as one word. An example of this is \"Det ordner seg av seg selv\" in standard written Bokmål, meaning \"It will sort itself out\" could become \"dånesæsæsjæl\" (note the letters Å and Æ, and the word \"sjæl\", as an eye dialect spelling of ). R-dropping, being present in the example, is especially common in speech in many areas of Norway , but plays out in different ways, as does elision of word-final phonemes like .\n\nBecause of the many dialects of Norwegian and their widespread use it is often difficult to distinguish between non-standard writing of standard Norwegian and eye dialect spelling. It is almost universally true that these spellings try to convey the way each word is pronounced, but it is rare to see language written that does not adhere to at least some of the rules of the official orthography. Reasons for this include words spelled unphonemically, ignorance of conventional spelling rules, or adaptation for better transcription of that dialect's phonemes.\n\nLatin contains several examples of contractions. One such case is preserved in the verb \"nolo\" (I am unwilling/do not want) which was formed by a contraction of \"non volo\" (\"volo\" meaning “I want”). Similarly this is observed in the first person plural and third person plural forms (nolumus and nolunt respectively).\n\nSome contractions in rapid speech include ～っす (\"-ssu\") for です (\"desu\") and すいません (\"suimasen\") for すみません (\"sumimasen\"). では (\"dewa\") is often contracted to じゃ (\"ja\"). In certain grammatical contexts the particle の (\"no\") is contracted to simply ん (\"n\").\n\nWhen used after verbs ending in the conjunctive form ～て (\"-te\"), certain auxiliary verbs and their derivations are often abbreviated. Examples:\n<nowiki>*</nowiki> this abbreviation is never used in the polite conjugation, to avoid the resultant ambiguity between an abbreviated \"ikimasu\" (go) and the verb \"kimasu\" (come).\n\nThe ending ～なければ (\"-nakereba\") can be contracted to ～なきゃ (\"-nakya\") when it is used to indicate obligation. It is often used without an auxiliary, e.g., 行かなきゃ（いけない） (\"ikanakya (ikenai)\") \"I have to go.\"\n\nOther times, contractions are made to create new words or to give added or altered meaning:\n\nVarious dialects of Japanese also use their own specific contractions which are often unintelligible to speakers of other dialects.\n\nIn the Polish language pronouns have contracted forms which are more prevalent in their colloquial usage. Examples are \"go\" and \"mu\". The non-contracted forms are \"jego\" (unless it is used as a possessive pronoun) and \"jemu\", respectively. The clitic \"-ń\" which stands for \"niego\" (him) as in \"dlań\" (\"dla niego\") is more common in literature. The non-contracted forms are generally used as a means to accentuate.\n\nUyghur, a Turkic language spoken in Central Asia, includes some verbal suffixes that are actually contracted forms of compound verbs (serial verbs). For instance, \"sëtip alidu\" (sell-manage, \"manage to sell\") is usually written and pronounced \"sëtivaldu\", with the two words forming a contraction and the [p] leniting into a [v] or [w].\n\nIn Filipino, most contractions need other words to be contracted correctly. Only words that end with vowels can make a contraction with words like \"at\" and \"ay.\" In this chart, the \"@\" represents any vowel.\n"}
{"id": "358999", "url": "https://en.wikipedia.org/wiki?curid=358999", "title": "Difference feminism", "text": "Difference feminism\n\nTaking for granted an equal moral status as persons, difference feminism asserts that there are differences between men and women but that no value judgment can be placed upon them.\n\nThe term \"difference feminism\" developed during the \"equality-versus-difference debate\" in American feminism in the 1980s and 1990s, but subsequently fell out of favor and use. In the 1990s feminists addressed the binary logic of \"difference\" versus \"equality\" and moved on from it, notably with postmodern and/or deconstructionist approaches that either dismantled or did not depend on that dichotomy.\n\nDifference feminism did not require a commitment to essentialism. Most strains of difference feminism did not argue that there was a biological, inherent, ahistorical, or otherwise \"essential\" link between womanhood and traditionally feminine values, habits of mind (often called \"ways of knowing\"), or personality traits. These feminists simply sought to recognize that, in the present, women and men are significantly different and to explore the devalued \"feminine\" characteristics.\n\nSome strains of difference feminism, for example Mary Daly's, argue not just that women and men were different, and had different values or different ways of knowing, but that women and their values were superior to men's. This viewpoint does not require essentialism, although there is ongoing debate about whether Daly's feminism is essentialist.\n\nDifference feminism was developed by feminists in the 1980s, in part as a reaction to popular liberal feminism (also known as \"equality feminism\"), which emphasized the similarities between women and men in order to argue for equal treatment for women. Difference feminism, although it still aimed at equality between men and women, emphasized the differences between men and women and argued that identicality or sameness are not necessary in order for men and women, and masculine and feminine values, to be treated equally. Liberal feminism aimed to make society and law gender-neutral, since it saw recognition of gender difference as a barrier to rights and participation within liberal democracy, while difference feminism held that gender-neutrality harmed women \"whether by impelling them to imitate men, by depriving society of their distinctive contributions, or by letting them participate in society only on terms that favor men\".\n\nDifference feminism drew on earlier nineteenth-century strains of thought, for example the work of German writer Elise Oelsner, which held that not only should women be allowed into formerly male-only spheres and institutions (e.g. public life, science) but that those institutions should also be expected to change in a way that recognizes the value of traditionally devalued feminine ethics (like care [see ethics of care]). On the latter point, many feminists have re-read the phrase \"difference feminism\" in a way that asks \"what difference does feminism make?\" (e.g. to the practice of science) rather than \"what differences are there between men and women\"?\n\nSome have argued that the thought of certain prominent second-wave feminists, like psychologist Carol Gilligan and radical feminist theologian Mary Daly, is \"essentialist.\" In philosophy essentialism is the belief that \"(at least some) objects have (at least some) essential properties.\" In the case of sexual politics essentialism is taken to mean that \"women\" and \"men\" have fixed essences or essential properties (e.g. behavioral or personality traits) that cannot be changed. However, essentialist interpretations of Daly and Gilligan have been questioned by some feminist scholars, who argue that charges of \"essentialism\" are often used more as terms of abuse than as theoretical critiques based on evidence, and do not accurately reflect Gilligan or Daly's views.\n\n"}
{"id": "8366559", "url": "https://en.wikipedia.org/wiki?curid=8366559", "title": "Difference theory", "text": "Difference theory\n\nDifference theory has roots in the studies of John Gumperz, who examined differences in cross-cultural communication. While difference theory deals with cross-gender communication, the male and female genders are often presented as being two separate cultures, hence the relevance of Gumperz's studies. In her development of the difference theory, Deborah Tannen drew on the work of Daniel Maltz and Ruth Borker, in particular their 1982 paper, \"A Cultural Approach to Male-Female Miscommunication\", which itself drew on the work of Gumperz. Mary Talbot makes reference to the term \"gender-specific culture\" in her critique of the difference theory, and this idea of genders being culturally separated is embodied by the 1992 publication \"Men Are from Mars, Women Are from Venus\". Difference theory is often compared with dominance theory and deficit theory, and together with the more contemporary dynamic theory they make up four of the theories most widely referred to and compared in the study of language and gender.\n\nThe reason for the popularity of Tannen's book \"You Just Don't Understand\", and the resultant popularisation of difference theory, is generally attributed to the style of Tannen's work, in which she adopts a neutral position on differences in genderlect by making no value-judgements about use of language by either gender. Talbot comments that this means the book provides explanations for domestic disputes without \"pointing the finger\" at anyone.\n\nDifference theory as postulated by Tannen is generally summarised into six categories, each of which pairs contrasting uses of language by males and females.\n\nTannen states that, for men, the world is a competitive place in which conversation and speech are used to build status, whereas for women the world is a network of connections, and that they use language to seek and offer support. In demonstrating this, Tannen uses the example of her husband and herself, who at one point had jobs in different cities. She remarks that whenever someone commented on this, she interpreted it as being an offer of sympathy or support. Her husband, on the other hand, took such comments as being criticisms and attempts to put him down. Tannen remarks that this displays the different approaches that women and men take in terms of status and support. Furthermore, men are also more likely to interrupt to get their point across and hence gain status.\n\nWomen seek comfort and sympathy for their problems, whilst men will seek a solution to the problem.\n\nTannen states that men's conversation is message-oriented, i.e. based upon communicating information. For women, conversation is much more important for building relationships and strengthening social links.\n\nMen will use direct imperatives (\"close the door\", \"switch on the light\") when speaking to others. Women encourage the use of superpolite forms, however (\"let's\", \"would you mind if ...?\").\n\nTannen asserts that most women avoid conflict in language at all costs, and instead attempt to resolve disagreements without any direct confrontation, to maintain positive connection and rapport. Men, on the other hand, are more likely to use confrontation as a way of resolving differences and thereby negotiating status. Tannen supports this view by making reference to the work of Walter J. Ong, whose 1981 publication, \"Fighting for Life\", asserts that \"expressed adversativeness\" is more an element of male culture than female culture. Tannen stresses that both forms of communication are valid ways of creating involvement and forming bonds.\n\nDifference theory asserts that in general men favour independence, while women are more likely to seek intimacy. Tannen demonstrates this with the example of a husband making a decision without consulting his wife. She theorises that he does so because he doesn't want to feel a loss of independence that would come from saying, \"Let me consult this with my wife first.\" Women, by contrast, like to demonstrate that they have to consult with their partner, as this is seen to be proof of the intimacy of the relationship. Tannen asserts that women, seeing the world as a network of connections and relationships, view intimacy as key to achieving consensus and avoiding the appearance of superiority, whereas men, who are more likely to view the world in terms of status, see independence as being key to establishing their status. Tannen also clarifies that while both men and women seek independence and intimacy, men tend to be focused on the former, while women tend to focus on the latter.\n\nGeneral criticisms are that Tannen's observations are largely anecdotal and cannot be said for all conjugal conversations, let alone mixed-gender interactions as a whole.\n\n\n"}
{"id": "58632079", "url": "https://en.wikipedia.org/wiki?curid=58632079", "title": "Encyclopedia of Forensic and Legal Medicine 2nd Edition", "text": "Encyclopedia of Forensic and Legal Medicine 2nd Edition\n\nThe Encyclopedia of Forensic and Legal Medicine 2nd Edition is a reference source and pioneering 4 set encyclopedia of forensics and medico-legal knowledge published by Academic Press, Elsevier in 2016. This has been edited by the renowned British forensic specialist Jason Payne-James and Australian forensic pathologist Roger W. Byard and an international editorial board. \nThis reference work includes more than 300 articles contributed by forensic medicine and forensic science experts from all over the world. The encyclopedia is a complete reference source of articles covering from forensics, criminal investigations, health-care, legal, judicial, ballistics, toxicology,fingerprinting, DNA typing, disaster victim identification to autopsy and postmortem examination.\n\nThe encyclopedia is especially meant for forensic, medical, chemistry, physics, laboratory technologists and anthropology students and specialists such as forensic experts, lawyers, judicial officers, judges, police and investigating offices, nurses, medical officers etc. All the articles of the encyclopedia are available through Science direct and Scopus.\n"}
{"id": "10260905", "url": "https://en.wikipedia.org/wiki?curid=10260905", "title": "I (pronoun)", "text": "I (pronoun)\n\nThe pronoun I is the first-person singular nominative case personal pronoun in Modern English. It is used to refer to one's self and is capitalized, although other pronouns, such as \"he\" or \"she\", are not capitalized.\n\nThe grammatical variants of \"I\" are \"me\", \"my\", \"mine\", and \"myself\".\n\nEnglish \"I\" originates from Old English (OE) \"ic\". Its predecessor \"ic\" had in turn originated from the continuation of Proto-Germanic *\"ik\", and \"ek\"; the asterisk denotes an unattested form, \"ek\" was attested in the Elder Futhark inscriptions (in some cases notably showing the variant \"eka\"; see also ek erilaz). Linguists assume \"ik\" to have developed from the unstressed variant of \"ek\". Variants of \"ic\" were used in various English dialects up until the 1600s.\n\nGermanic cognates are: Old Frisian \"ik\", Old Norse \"ek\" (Danish, Norwegian \"jeg\", Swedish \"jag\", Icelandic ég), Old High German \"ih\" (German \"ich\") and Gothic \"ik\" and in Dutch also \"ik\".\n\nThe Proto-Germanic root came, in turn, from the Proto Indo-European language (PIE). The reconstructed PIE pronoun is *\"egō, egóm\", with cognates including\nSanskrit \"aham\", Hittite \"uk\", Latin \"ego\", Greek \"egō\", Old Slavonic \"azъ\" and Alviri-Vidari (an Iranian language) \"az\".\n\nThe oblique forms are formed from a stem \"*me-\" (English \"me\"), the plural from \"*wei-\" (English \"we\"), the oblique plurals from \"*ns-\" (English \"us\") and from Proto-Germanic \"*unseraz\", PIE \"*no-s-ero-\" (\"our, ours\").\n\n\"I\" (and only this form of the pronoun) is the only pronoun that is always capitalized in English. This practice became established in the late 15th century, though lowercase \"i\" was sometimes found as late as the 17th century.\n\nLike the other English personal pronouns \"we\" (\"us\"), \"he\" (\"him\"), \"she\" (\"her\"), and \"they\" (\"them\"), the pronoun \"I\" has several singular case forms.\nThese are: \n\nThere are some situations in which only the nominative form (\"I\") is grammatically correct and others in which only the accusative form (\"me\") is correct. There are also situations in which one form is used in informal style (and was often considered ungrammatical by older prescriptive grammars) and the other form is preferred in formal style.\n\nIn all varieties of standard English, the nominative form \"I\" is used exclusively when it is the whole subject of an \"explicit\" verb, e.g. \nnot \nWith other pronouns, such as \"we\" (strictly speaking when used as a personal determiner), there may be exceptions to this in some varieties of English.\n\nIn all varieties of standard English, the accusative form \"me\" is used exclusively when it is the whole direct or indirect object of a verb or preposition. The accusative \"me\" is also required in a number of constructions such as \"Silly me!\"\n\nIn many situations, both the nominative \"I\" and the accusative \"me\" are encountered.\n\nWhen the pronoun is used as a subjective predicative complement, the nominative \"I\" is sometimes encountered in (very) formal style:\nBut this is often seen as hypercorrect and may be unacceptable, as in:\n\"Me\" is usually preferred as a subjective predicate, especially in informal style:\nThe nominative \"I\" is more common in this role when it is followed by a relative clause:\nthough even here \"me\" is more common in non-formal style:\n\nFollowing \"as\" or \"than\" (without a following explicit verb), the accusative form is common:\nHowever, where it is possible to think of the pronoun as the subject of an implicit verb and \"than\" or \"as\" as a conjunction, the nominative \"I\" is found in formal style:\n\nIn Australian English, British English and Irish English, many speakers have an unstressed form of \"my\" that is identical to \"me\" (see archaic and non-standard forms of English personal pronouns).\n\nThe above applies when the pronoun stands alone as the subject or object.\nIn some varieties English (particularly formal English), those rules also apply in coordinative constructions such as \"you and I\". So the correct form is \n\nIn some varieties of non-standard informal English, the accusative is sometimes used when the pronoun is part of a coordinative \"subject\" construction, as in\nThis is highly stigmatized.\n\nOn the other hand, the use of the nominative \"I\" in coordinative constructions like \"you and I\"where \"me\" would be used in a non-coordinative object is less stigmatized – and in some cases so widespread as to be considered a variety of standard English: \n\n\n\n\n"}
{"id": "190975", "url": "https://en.wikipedia.org/wiki?curid=190975", "title": "Ibid.", "text": "Ibid.\n\nIbid is an abbreviation for the Latin word \"ibīdem\", meaning \"in the same place\", commonly used in an endnote, footnote, bibliography citation, or scholarly reference to refer to the source cited in the preceding note or list item. This is similar to \"īdem\", literally meaning \"the same\", abbreviated \"Id.\", which is commonly used in legal citation.\n\nIbid. may also be used in the Harvard (name-date) system for in-text references where there has been a close previous citation from the same source material. The previous reference should be immediately visible, e.g. within the same paragraph or page. Some academic publishers now prefer that \"ibid.\" not be italicized, as it is a commonly found term.\n\nSince ibid. is an abbreviation where the last two letters of the word are omitted, it takes a full stop (period) in both British and American usage.\n\nReference 2 is the same as reference 1: E. Vijh, \"Latin for Dummies\" on page 23, whereas reference 3 refers to the same work but at a different location, namely page 29. Intervening entries require a reference to the original citation in the form Ibid. <citation #>, as in reference 5.\n\n\n\n"}
{"id": "1930406", "url": "https://en.wikipedia.org/wiki?curid=1930406", "title": "Impredicativity", "text": "Impredicativity\n\nSomething that is impredicative, in mathematics, logic and philosophy of mathematics, is a self-referencing definition. Roughly speaking, a definition is impredicative if it invokes (mentions or quantifies over) the set being defined, or (more commonly) another set that contains the thing being defined. There is no generally accepted precise definition of what it means to be predicative or impredicative. Authors have given different but related definitions.\n\nThe opposite of impredicativity is predicativity, which essentially entails building stratified (or ramified) theories where quantification over lower levels results in variables of some new type, distinguished from the lower types that the variable ranges over. A prototypical example is intuitionistic type theory, which retains ramification so as to discard impredicativity.\n\nRussell's paradox is a famous example of an impredicative construction—namely the set of all sets that do not contain themselves. The paradox is that such a set cannot exist: If it would exist, the question could be asked whether it contains itself or not — if it does then by definition it should not, and if it does not then by definition it should.\n\nThe greatest lower bound of a set , , also has an impredicative definition: if and only if for all elements of , is less than or equal to , and any less than or equal to all elements of is less than or equal to . This definition quantifies over the set (potentially infinite, depending on the order in question) whose members are the lower bounds of , one of which being the glb itself. Hence predicativism would reject this definition.\n\nThe terms \"predicative\" and \"impredicative\" were introduced by , though the meaning has changed a little since then. \n\nSolomon Feferman provides a historical review of predicativity, connecting it to current outstanding research problems.\n\nThe vicious circle principle was suggested by Henri Poincaré (1905-6, 1908) and Bertrand Russell in the wake of the paradoxes as a requirement on legitimate set specifications. Sets that do not meet the requirement are called \"impredicative\".\n\nThe first modern paradox appeared with Cesare Burali-Forti's 1897 \"A question on transfinite numbers\" and would become known as the Burali-Forti paradox. Cantor had apparently discovered the same paradox in his (Cantor's) \"naive\" set theory and this become known as Cantor's paradox. Russell's awareness of the problem originated in June 1901 with his reading of Frege's treatise of mathematical logic, his 1879 \"Begriffsschrift\"; the offending sentence in Frege is the following:\nIn other words, given the function is the variable and is the invariant part. So why not substitute the value for itself? Russell promptly wrote Frege a letter pointing out that:\nFrege promptly wrote back to Russell acknowledging the problem:\nWhile the problem had adverse personal consequences for both men (both had works at the printers that had to be emended), van Heijenoort observes that \"The paradox shook the logicians' world, and the rumbles are still felt today. ... Russell's paradox, which uses the bare notions of set and element, falls squarely in the field of logic. The paradox was first published by Russell in \"The principles of mathematics\" (1903) and is discussed there in great detail ...\". Russell, after six years of false starts, would eventually answer the matter with his 1908 theory of types by \"propounding his \"axiom of reducibility\". It says that any function is coextensive with what he calls a \"predicative\" function: a function in which the types of apparent variables run no higher than the types of the arguments\". But this \"axiom\" was met with resistance from all quarters.\n\nThe rejection of impredicatively defined mathematical objects (while accepting the natural numbers as classically understood) leads to the position in the philosophy of mathematics known as predicativism, advocated by Henri Poincaré and Hermann Weyl in his \"Das Kontinuum\". Poincaré and Weyl argued that impredicative definitions are problematic only when one or more underlying sets are infinite.\n\nErnst Zermelo in his 1908 \"A new proof of the possibility of a well-ordering\" presents an entire section \"b. \"Objection concerning nonpredicative definition\"\" where he argued against \"Poincaré (1906, p. 307) [who states that] a definition is 'predicative' and logically admissible only if it \"excludes\" all objects that are dependent upon the notion defined, that is, that can in any way be determined by it\". He gives two examples of impredicative definitions – (i) the notion of Dedekind chains and (ii) \"in analysis wherever the maximum or minimum of a previously defined \"completed\" set of numbers is used for further inferences. This happens, for example, in the well-known Cauchy proof of the fundamental theorem of algebra, and up to now it has not occurred to anyone to regard this as something illogical\". He ends his section with the following observation: \"A definition may very well rely upon notions that are equivalent to the one being defined; indeed, in every definition \"definiens\" and \"definiendum\" are equivalent notions, and the strict observance of Poincaré's demand would make every definition, hence all of science, impossible\".\n\nZermelo's example of minimum and maximum of a previously defined \"completed\" set of numbers reappears in Kleene 1952:42-42 where Kleene uses the example of Least upper bound in his discussion of impredicative definitions; Kleene does not resolve this problem. In the next paragraphs he discusses Weyl's attempt in his 1918 \"Das Kontinuum\" (\"The Continuum\") to eliminate impredicative definitions and his failure to retain the \"theorem that an arbitrary non-empty set of real numbers having an upper bound has a least upper bound (cf. also Weyl 1919)\".\n\nRamsey argued that \"impredicative\" definitions can be harmless: for instance, the definition of \"tallest person in the room\" is impredicative, since it depends on a set of things of which it is an element, namely the set of all persons in the room. Concerning mathematics, an example of an impredicative definition is the smallest number in a set, which is formally defined as: if and only if for all elements of , is less than or equal to , and is in .\n\nBurgess (2005) discusses predicative and impredicative theories at some length, in the context of Frege's logic, Peano arithmetic, second order arithmetic, and axiomatic set theory.\n\n\n"}
{"id": "17878314", "url": "https://en.wikipedia.org/wiki?curid=17878314", "title": "Information source", "text": "Information source\n\nAn information source is a person, thing, or place from which information comes, arises, or is obtained. Information souces can be known as primary or secondary. That source might then inform a person about something or provide knowledge about it. Information sources are divided into separate distinct categories, primary, secondary, tertiary, and so on.\n\n"}
{"id": "41908", "url": "https://en.wikipedia.org/wiki?curid=41908", "title": "Key Word in Context", "text": "Key Word in Context\n\nKWIC is an acronym for Key Word In Context, the most common format for concordance lines. The term KWIC was first coined by Hans Peter Luhn. The system was based on a concept called \"keyword in titles\" which was first proposed for Manchester libraries in 1864 by Andrea Crestadoro.\n\nA KWIC index is formed by sorting and aligning the words within an article title to allow each word (except the stop words) in titles to be searchable alphabetically in the index. It was a useful indexing method for technical manuals before computerized full text search became common.\n\nFor example, a search query including all of the words in the title statement of this article (\"KWIC is an acronym for Key Word In Context, the most common format for concordance lines\") and the in English (\"the free encyclopedia\"), searched against this very web page, might yield a KWIC index as follows. A KWIC index usually uses a wide layout to allow the display of maximum 'in context' information (not shown in the following example).\n\nA KWIC index is a special case of a \"permuted index\". This term refers to the fact that it indexes all cyclic permutations of the headings. Books composed of many short sections with their own descriptive headings, most notably collections of manual pages, often ended with a permuted index section, allowing the reader to easily find a section by any word from its heading. This practice, also known as KWOC (“Key Word Out of Context”), is no longer common.\n\n\"Note: The first reference does not show the KWIC index unless you pay to view the paper. The second reference does not even list the paper at all.\"\n\n\n"}
{"id": "5995840", "url": "https://en.wikipedia.org/wiki?curid=5995840", "title": "L. G. Pine", "text": "L. G. Pine\n\nLeslie Gilbert Pine (22 December 1907 – 15 May 1987) was a British author, lecturer, and researcher in the areas of genealogy, nobility, history, heraldry and animal welfare. He was born in 1907 in Bristol, England and died in Bury St. Edmunds, Suffolk in 1987. He was the son of Lilian Grace Beswetherick and Henry Moorshead Pine (a tea merchant).\n\nFrom 1935 to 1940 he served as an assistant editor at Burke's Peerage Ltd. During World War II he was an officer in the Royal Air Force intelligence branch, serving in North Africa, Italy, Greece, and India; he retired with the rank of Squadron Leader. After the war and until 1960, he was Burke's executive director. Pine edited \"Burke's Peerage,\" 1949-1959; \"Burke's Landed Gentry (of Great Britain),\" 1952; \"Burke's Landed Gentry (of Ireland),\" 1958; and, \"Burke's Distinguished Families of America,\" 1939, 1947. He also edited \"The International Year Book and Statesmen's Who's Who,\" 1953-1960; \"Author's and Writer's Who's Who,\" 1948, 1960; \"Who's Who in Music,\" 1949; and, \"Who's Who in the Free Churches,\" 1951.\n\nA graduate of London University, he became a Barrister-at-Law, Inner Temple, in 1953. Pine was a member of the International Institute of Genealogy and Heraldry, Fellow of the Society of Antiquaries of Scotland, a Fellow of the Ancient Monuments Society, a Life Fellow of the Institute of Journalists, a Freeman of the City of London, and a Liveryman of the Glaziers' Company. In 1959 he was the unsuccessful Conservative candidate for Bristol Central.\n\nHe was managing editor of a British hunting magazine, \"Shooting Times\", from 1960 to 1964. He later authored an important book highly critical of sport hunting, \"After Their Blood\", in which he wrote: \"It is our duty as men and women of God’s redeemed creation to try not to increase the suffering of the world, but to lessen it. To get rid of bloodsports will be a great step toward this end.\"\n\nIn 1948 Leslie Pine married Grace V. Griffin (20 August 1914- ). Their only child, Richard Pine, was born in London on 21 August 1949.\n\nHis books include:\n\n\nPine is also the primary contributor to the article \"genealogy\" in \"Encyclopædia Britannica\".\n\n"}
{"id": "51388883", "url": "https://en.wikipedia.org/wiki?curid=51388883", "title": "Life spans of home appliances", "text": "Life spans of home appliances\n\nThis page lists the average life spans of home appliances (major and small).\n\n"}
{"id": "6908619", "url": "https://en.wikipedia.org/wiki?curid=6908619", "title": "Museum of Comparative Zoology", "text": "Museum of Comparative Zoology\n\nThe Museum of Comparative Zoology, full name \"The Louis Agassiz Museum of Comparative Zoology\", often abbreviated simply to \"MCZ\", is the zoology museum located on the grounds of Harvard University in Cambridge, Massachusetts. It is one of three natural history research museums at Harvard whose public face is the Harvard Museum of Natural History. Harvard MCZ's collections consist of some 21 million specimens, of which several thousand are on rotating display at the public museum. The current director of the Museum of Comparative Zoology is James Hanken, the Louis Agassiz Professor of Zoology at Harvard University.\n\nMany of the exhibits in the public museum have not only zoological interest but also historical significance. Past exhibits have included a fossil sand dollar which was found by Charles Darwin in 1834, Captain Cook's mamo, and two pheasants that once belonged to George Washington, now on loan to Mount Vernon in Virginia.\n\nThe Harvard Museum of Natural History is physically connected to the Peabody Museum of Archaeology and Ethnology; for visitors, one admission ticket grants access to both museums. The research collections of the Museum of Comparative Zoology are not open to the public.\n\nThe Museum of Comparative Zoology was founded in 1859 through the efforts of zoologist Louis Agassiz, and the museum used to be referred to as \"The Agassiz\" after its founder. Agassiz designed the collection to illustrate the variety and comparative relationships of animal life.\n\nThe Radcliffe Zoological Laboratory was created in 1894 when Radcliffe College rented a space on the fifth floor of the Museum of Comparative Zoology at Harvard University to convert into a women's laboratory. Prior to this acquisition, Radcliffe science laboratories were taught using inadequate facilities, converting spaces such as bathrooms in old houses into physics laboratories, which Harvard professors often refused to teach in.The laboratory space was converted from an office or storage closet, and was sandwiched between other invertebrate storage rooms on the fifth floor.\n\nThe museum comprises twelve departments: Biological Oceanography, Entomology, Herpetology, Ichthyology, Invertebrate Paleontology, Invertebrate Zoology, Mammalogy, Marine invertebrates, Malacology, Ornithology, Population Genetics, and Vertebrate Paleontology. The Ernst Mayr Library and its archives join in supporting the work of the museum. The Ernst Mayr Library is a founding member of the Biodiversity Heritage Library.\n\nThe museum publishes two journals: the \"Bulletin of the Museum of Comparative Zoology at Harvard College\", first published in 1869, and \"Breviora\", first published in 1956.\n\nIn contrast to numerous more modern museums, the Harvard Museum of Natural History has many hundreds of stuffed animals on display, from the collections of the Museum of Comparative Zoology. Notable exhibits include whale skeletons, the largest turtle shell ever found (eight feet long), \"the Harvard mastodon\", a long \"Kronosaurus\" skeleton, the skeleton of a dodo, and a coelacanth preserved in fluid. The two-story Great Mammal Hall was renovated in 2009 in celebration of the 150th anniversary of founding of the Museum of Comparative Zoology.\n\nNew and changing exhibitions in the Harvard Museum of Natural History include \"Evolution\" (2008); \"The Language of Color\" (2008 to 2013); \"Arthropods: Creatures that Rule\" (2006); \"New England Forests\" (2011); and \"Mollusks: Shelled Masters of the Marine Realm\" (2012).\n\n"}
{"id": "1091767", "url": "https://en.wikipedia.org/wiki?curid=1091767", "title": "Non-well-founded set theory", "text": "Non-well-founded set theory\n\nNon-well-founded set theories are variants of axiomatic set theory that allow sets to contain themselves and otherwise violate the rule of well-foundedness. In non-well-founded set theories, the foundation axiom of ZFC is replaced by axioms implying its negation.\n\nThe study of non-well-founded sets was initiated by Dmitry Mirimanoff in a series of papers between 1917 and 1920, in which he formulated the distinction between well-founded and non-well-founded sets; he did not regard well-foundedness as an axiom. Although a number of axiomatic systems of non-well-founded sets were proposed afterwards, they did not find much in the way of applications until Peter Aczel’s hyperset theory in 1988.\n\nThe theory of non-well-founded sets has been applied in the logical modelling of non-terminating computational processes in computer science (process algebra and final semantics), linguistics and natural language semantics (situation theory), philosophy (work on the Liar Paradox), and in a different setting, non-standard analysis.\n\nIn 1917, Dmitry Mirimanoff introduced the concept of well-foundedness of a set:\n\nIn ZFC, there is no infinite descending ∈-sequence by the axiom of regularity. In fact, the axiom of regularity is often called the \"foundation axiom\" since it can be proved within ZFC (that is, ZFC without the axiom of regularity) that well-foundedness implies regularity. In variants of ZFC without the axiom of regularity, the possibility of non-well-founded sets with set-like ∈-chains arises. For example, a set \"A\" such that \"A\" ∈ \"A\" is non-well-founded.\n\nAlthough Mirimanoff also introduced a notion of isomorphism between possibly non-well-founded sets, he considered neither an axiom of foundation nor of anti-foundation. In 1926, Paul Finsler introduced the first axiom that allowed non-well-founded sets. After Zermelo adopted Foundation into his own system in 1930 (from previous work of von Neumann 1925–1929) interest in non-well-founded sets waned for decades. An early non-well-founded set theory was Willard Van Orman Quine’s New Foundations, although it is not merely ZF with a replacement for Foundation.\n\nSeveral proofs of the independence of Foundation from the rest of ZF were published in 1950s particularly by Paul Bernays (1954), following an announcement of the result in earlier paper of his from 1941, and by Ernst Specker who gave a different proof in his Habilitationsschrift of 1951, proof which was published in 1957. Then in 1957 Rieger's theorem was published, which gave a general method for such proof to be carried out, rekindling some interest in non-well-founded axiomatic systems. The next axiom proposal came in a 1960 congress talk of Dana Scott (never published as a paper), proposing an alternative axiom now called SAFA. Another axiom proposed in the late 1960s was Maurice Boffa's axiom of superuniversality, described by Aczel as the highpoint of research of its decade. Boffa's idea was to make foundation fail as badly as it can (or rather, as extensionality permits): Boffa's axiom implies that every extensional set-like relation is isomorphic to the elementhood predicate on a transitive class.\n\nA more recent approach to non-well-founded set theory, pioneered by M. Forti and F. Honsell in the 1980s, borrows from computer science the concept of a bisimulation. Bisimilar sets are considered indistinguishable and thus equal, which leads to a strengthening of the axiom of extensionality. In this context, axioms contradicting the axiom of regularity are known as anti-foundation axioms, and a set that is not necessarily well-founded is called a hyperset.\n\nFour mutually independent anti-foundation axioms are well-known, sometimes abbreviated by the first letter in the following list:\nThey essentially correspond to four different notions of equality for non-well-founded sets. The first of these, AFA, is based on accessible pointed graphs (apg) and states that two hypersets are equal if and only if they can be pictured by the same apg. Within this framework, it can be shown that the so-called Quine atom, formally defined by Q={Q}, exists and is unique.\n\nEach of the axioms given above extends the universe of the previous, so that: V ⊆ A ⊆ S ⊆ F ⊆ B. In the Boffa universe, the distinct Quine atoms form a proper class.\n\nIt is worth emphasizing that hyperset theory is an extension of classical set theory rather than a replacement: the well-founded sets within a hyperset domain conform to classical set theory.\n\nAczel’s hypersets were extensively used by Jon Barwise and John Etchemendy in their 1987 book \"The Liar\", on the liar's paradox; The book is also good introduction to the topic of non-well-founded sets.\n\nBoffa’s superuniversality axiom has found application as a basis for axiomatic nonstandard analysis.\n\n\n\n"}
{"id": "2673834", "url": "https://en.wikipedia.org/wiki?curid=2673834", "title": "Numeronym", "text": "Numeronym\n\nA numeronym is a number-based word.\n\nMost commonly, a numeronym is a word where a number is used to form an abbreviation (albeit not an acronym or an initialism). Pronouncing the letters and numbers may sound similar to the full word: \"K9\" for \"canine\" (phonetically: \"kay\" + \"nine\").\n\nAlternatively, the letters between the first and last are replaced with a number representing the number of letters omitted, such as \"i18n\" for \"internationalization\". Sometimes the last letter is also counted and omitted. These word shortenings are sometimes called \"alphanumeric acronyms\", \"alphanumeric abbreviations\", or \"numerical contractions\".\n\nAccording to Tex Texin, the first numeronym of this kind was \"S12n\", the electronic mail account name given to Digital Equipment Corporation (DEC) employee Jan Scherpenhuizen by a system administrator because his surname was too long to be an account name. By 1985, colleagues who found Jan's name unpronounceable often referred to him verbally as \"S12n\" (\"ess-twelve-en\"). The use of such numeronyms became part of DEC corporate culture.\n\nA number may also denote how many times the character before or after it is repeated. This is typically used to represent a name or phrase in which several consecutive words start with the same letter, as in W3 (World Wide Web) or W3C (World Wide Web Consortium).\n\nSome numeronyms are composed entirely of numbers, such as \"212\" for \"New Yorker\", \"4-1-1\" for \"information\", \"9-1-1\" for \"help\", and \"101\" for \"basic introduction to a subject\". Words of this type have existed for decades, including those in 10-code, which has been in use since before World War II.\n\nChapter or title numbers of some jurisdictions' statutes have become numeronyms, for example 5150 and 187 from California's penal code. Largely because the production of many American movies and television programs are based in California, usage of these terms has spread beyond its original location and user population.\n\nThe concept of incorporating numbers into words can also be found in Leet-speak, where numbers are frequently substituted for orthographically similar letters (e.g. \"H4CK3D\" for \"HACKED\").\n\nAnne H. Soukhanov, editor of the new \"Microsoft Encarta College Dictionary\", gives the original meaning of the term as \"a telephone number that spells a word or a name\" on a telephone dial.\n\nWhere words have multiple meanings, abbreviations such as these are almost always used to refer to their computing sense; for example, \"G11n\" for \"globalization\" refers to software preparedness for global distribution, and not the social trend of globalization. In some cases, the use of appropriate case makes it easier to distinguish between letters such as uppercase I/i and lower case L/l.\n\n"}
{"id": "49632241", "url": "https://en.wikipedia.org/wiki?curid=49632241", "title": "Self-perpetuation", "text": "Self-perpetuation\n\nSelf-perpetuation, the capability of something to cause itself to continue to exist, is one of the main characteristics of life. Organisms' capability of reproduction leads to self-perpetuation of the species, if not to the individual. Populations self-perpetuate and grow. Entire ecosystems show homeostasis, and thus perpetuate themselves. The slow modifying effect of succession and similar shifts in the composition of the system can, however, not be neglected in the long run. Overall, life's object's capabilities of self-perpetuation are always accompanied by evolution, a perfect steady state of the biological system is never reached. Sexual reproduction is also a form of imperfect self-replication and thus imperfect self-perpetuation because of recombination and mutation. Organisms are not like self-replicating machine but amass random modifications from generation to generation. The property of self-perpetuation in the strict sense thus only applies to life itself.\n\nIn a social context, self-perpetuation is tied to reflexivity and (usually) positive feedback loops:\nDepending on the time scope or the context, self-perpetuation either depends on self-sustainability, or is equivalent to it. While we may talk about the self-sustainability of an ecosystem, this depends amongst other factor on the self-perpetuation of its constituting species.\n\nIn computer science, self-reproducing programs constitute an incomplete metaphor for self-perpetuation. A better analogue can be seen in computer viruses which are actually able to self-reproduce - given a suitable computing environment.\n\n"}
{"id": "28545", "url": "https://en.wikipedia.org/wiki?curid=28545", "title": "Self-reference", "text": "Self-reference\n\nSelf-reference occurs in natural or formal languages when a sentence, idea or formula refers to itself. The reference may be expressed either directly—through some intermediate sentence or formula—or by means of some encoding. In philosophy, it also refers to the ability of a subject to speak of or refer to itself: to have the kind of thought expressed by the first person nominative singular pronoun, the word \"I\" in English.\n\nSelf-reference is studied and has applications in mathematics, philosophy, computer programming, and linguistics. Self-referential statements are sometimes paradoxical, and can also be considered recursive.\n\nIn classical philosophy, paradoxes were created by self-referential concepts such as the omnipotence paradox of asking if it was possible for a being to exist so powerful that it could create a stone that it could not lift. The Epimenides paradox, 'All Cretans are liars' when uttered by an ancient Greek Cretan was one of the first recorded versions. Contemporary philosophy sometimes employs the same technique to demonstrate that a supposed concept is meaningless or ill-defined.\n\nIn mathematics and computability theory, self-reference (also known as Impredicativity) is the key concept in proving limitations of many systems. Gödel's theorem uses it to show that no formal consistent system of mathematics can ever contain all possible mathematical truths, because it cannot prove some truths about its own structure. The halting problem equivalent, in computation theory, shows that there is always some task that a computer cannot perform, namely reasoning about itself. These proofs relate to a long tradition of mathematical paradoxes such as Russell's paradox and Berry's paradox, and ultimately to classical philosophical paradoxes.\n\nIn game theory undefined behaviors can occur where two players must model each other's mental states and behaviors, leading to infinite regress.\n\nIn computer programming, self-reference occurs in reflection, where a program can read or modify its own instructions like any other data. Numerous programming languages support reflection to some extent with varying degrees of expressiveness. Additionally, self-reference is seen in recursion (related to the mathematical recurrence relation) in functional programming, where a code structure refers back to itself during computation. 'Taming' self-reference from potentially paradoxical concepts into well-behaved recursions has been one of the great successes of computer science, and is now used routinely in, for example, writing compilers using the 'meta-language' ML. Using a compiler to compile itself is known as bootstrapping. Self-modifying code is possible to write (programs which operate on themselves), both with assembler and with functional languages such as Lisp, but is generally discouraged in real-world programming. Computing hardware makes fundamental use of self-reference in flip-flops, the basic units of digital memory, which convert potentially paradoxical logical self-relations into memory by expanding their terms over time. Thinking in terms of self-reference is a pervasive part of programmer culture, with many programs and acronyms named self-referentially as a form of humor, such as GNU ('Gnu's not Unix') and PINE ('Pine is not Elm'). The GNU Hurd is named for a pair of mutually self-referential acronyms.\n\nTupper's self-referential formula is a mathematical curiosity which plots an image of its own formula.\n\nThe biology of self-replication is self-referential, as embodied by DNA and RNA replication mechanisms. Models of self-replication are found in the computational Game of life, and have inspired engineering systems such as the RepRap self-replicating 3d printer.\n\nSelf-reference occurs in literature and film when an author refers to his or her own work in the context of the work itself. Examples include Cervantes's \"Don Quixote\", Shakespeare's \"A Midsummer Night's Dream\", \"The Tempest\" and \"Twelfth Night\", Denis Diderot's \"Jacques le fataliste et son maître\", Italo Calvino's \"If on a winter's night a traveler\", many stories by Nikolai Gogol, \"Lost in the Funhouse\" by John Barth, Luigi Pirandello's \"Six Characters in Search of an Author\" and Federico Fellini's \"8½\". Perhaps the earliest example is in Homer's Iliad, where Helen of Troy laments: \"for generations still unborn/we will live in song\" (appearing in the song itself).\n\nSelf-reference in art is closely related to the concepts of breaking the fourth wall and meta-reference, which often involve self-reference. The short stories of Jorge Luis Borges play with self-reference and related paradoxes in many ways. Samuel Beckett's Krapp's Last Tape consists entirely of the protagonist listening to and making recordings of himself, mostly about other recordings. During the 1990s and 2000s filmic self-reference was a popular part of the rubber reality movement, notably in Charlie Kaufman's films Being John Malkovich and Adaptation, the latter pushing the concept arguably to its breaking point as it attempts to portray its own creation.\n\nVarious creation myths invoke self-reference to solve the problem of what created the creator. For example the Egyptian creation myth has a god swallowing his own semen to create himself. Ouroboros is a mythical dragon which eats itself.\n\nThe surrealist painter René Magritte is famous for his self-referential works. His painting \"The Treachery of Images\", includes the words \"this is not a pipe\", the truth of which depends entirely on whether the word \"ceci\" (in English, \"this\") refers to the pipe depicted—or to the painting or the word or sentence itself. M.C. Escher's art also contains many self-referential concepts such as hands drawing themselves.\n\nA word that describes itself is called an \"autological word\" (or \"autonym\"). This generally applies to adjectives, for example sesquipedalian (i.e. \"sesquipedalian\" is a sesquipedalian word), but can also apply to other parts of speech, such as TLA, as a three-letter abbreviation for \"three-letter abbreviation\".\n\nA sentence which inventories its own letters and punctuation marks is called an autogram.\n\nThere is a special case of meta-sentence in which the content of the sentence in the metalanguage and the content of the sentence in the object language are the same. Such a sentence is referring to itself. However some meta-sentences of this type can lead to paradoxes. \"This is a sentence.\" can be considered to be a self-referential meta-sentence which is obviously true. However \"This sentence is false\" is a meta-sentence which leads to a self-referential paradox. Such sentences can lead to problems, for example, in law, where statements bringing laws into existence can contradict one another or themselves. Kurt Gödel claimed to have found such a paradox in the US constitution at his citizenship ceremony.\n\nSelf-reference occasionally occurs in the media when it is required to write about itself, for example the BBC reporting on job cuts at the BBC. Notable encyclopedias may be required to feature articles about themselves, such as Wikipedia's article on Wikipedia.\n\nFumblerules are a list of rules of good grammar and writing, demonstrated through sentences that violate those very rules, such as \"Avoid cliches like the plague\" and \"Don't use no double negatives\". The term was coined in a published list of such rules by William Safire.\n\nSeveral academic disciplines are sometime required to study themselves in forms of self-reference, for example historiography (or \"meta-history\") is history's study of its own past; meta-sociology occurs when sociologists study the power structures in their own academic institutions; and meta-mathematics is the study of mathematics itself as a formal system, using its own methods. In law, self-reference may become an issue when laws are required to regulate the making of new laws, especially around constitutional issues. (The game of nomic begins as a model of this process.) The prefix \"meta\" is often used to denote this type of self-reference.\n\n\n"}
{"id": "4106285", "url": "https://en.wikipedia.org/wiki?curid=4106285", "title": "Self-referential encoding", "text": "Self-referential encoding\n\nEvery day, people are presented with endless amounts of information, and in an effort to help keep track and organize this information, people must be able to recognize, differentiate and store information. One way to do that is to organize information as it pertains to the self. The overall concept of self-reference suggests that people interpret incoming information in relation to themselves, using their self-concept as a background for new information. Examples include being able to attribute personality traits to oneself or to identify recollected episodes as being personal memories of the past. The implications of self-referential processing are evident in many psychological phenomena. For example, the \"cocktail party effect\" notes that people attend to the sound of their names even during other conversation or more prominent, distracting noise. Also, people tend to evaluate things related to themselves more positively (This is thought to be an aspect of implicit self-esteem). For example, people tend to prefer their own initials over other letters. The self-reference effect (SRE) has received the most attention through investigations into memory. The concepts of self-referential encoding and the SRE rely on the notion that relating information to the self during the process of encoding it in memory facilitates recall, hence the effect of self-reference on memory. In essence, researchers have investigated the potential mnemonic properties of self-reference.\n\nResearch includes investigations into self-schema, self-concept and self-awareness as providing the foundation for self-reference's role in memory. Multiple explanations for the self-reference effect in memory exist, leading to a debate about the underlying processes involved in the self-reference effect. In addition, through the exploration of the self-reference effect, other psychological concepts have been discovered or supported, including simulation theory and the effect.\nAfter researchers developed a concrete understanding of the self-reference effect, many expanded their investigations to consider the self-reference effect in particular groups like those with autism spectrum disorders or those experiencing depression.\n\nSelf-knowledge can be categorized by structures in memory or schemata. A self-schema is a set of facts or beliefs that one has about themselves. For any given trait, an individual may or may not be \"schematic\"; that is, the individual may or may not think about themselves as to where they stand on that trait. For example, people who think of themselves as very overweight or who identify themselves to a greater extent based on their body weight would be considered \"schematic\" on the attribute of body weight. Thus, many everyday events, such as going out for a meal or discussing a friend's eating habits, could induce thoughts about the self. When people relate information to something that has to do with the self, it facilitates memory. Self-descriptive adjectives that fit into one's self-schema are easier to remember than adjectives not viewed as related to the self. Thus, the self-schema is an aspect of oneself that is used as an encoding structure that brings upon memory of information consistent with one's self-schema. Memories that are elaborate and well encoded are usually the result of self-referent correlations during the process of remembering. During the process of encoding, trait representations are encoded in long term memory either directly or indirectly. When they are directly encoded, it is in terms of relating to the self, and when it is indirectly encoded it is done through spouts of episodic information instead of information about the self.\n\nSelf-schema is often used as somewhat of a database for encoding personal data. The self-schema is also used by paying selective attention to outside information and internalizing that information more deeply in one's memory depending on how much that information relates to their schema. When self-schema is engaged, traits that go along with one's view of themselves are better remembered and recalled. These traits are also often recalled much better when processed with respect to the self. Similarly, items that are encoded with the self are based on one's self-schema. Processing the information should balance out when recalled for individuals who have a self-schema that goes along with the information.\n\nSelf-schemas do not necessarily only involve individual traits. People self-categorize at different levels that range from more personal to more social. Self-schemas have three main categories which play a role: the personal self, the relational self, and the collective self. The personal self deals with individual level characteristics, the relational self deals with intimate relationship partners, and the collective self deals with group identities, relating to self-important social groups to which one belongs (e.g., one's family or university). Information that is related to any type of self-schema, including group-related knowledge structures facilitates memory.\n\nIn order for the self to be an effective encoding mechanism, it must be a uniform, consistent, well-developed schema. It has been shown that identity exploration leads to the development of self-knowledge which facilitates self-judgments. Identity exploration led to shorter decision times, higher confidence ratings and more intrusions in memory tasks. Previous researchers hypothesized that words compatible with a person's self-schema are easily accessible in memory and are more likely than incompatible words to intrude on a schema-irrelevant memory task. In one experiment, when participants were asked to decide if certain adjectives were \"like me\" or \"not like me,\" they made the decisions faster when the words were compatible with their self-schema.\n\nHowever, despite the existence of the self-reference effect when considering schemata consistent adjectives, the connection between the self and memory can lead to a larger number of mistakes in recognition, commonly referred to as false alarms. Rogers et al. (1979) found that people are more likely to falsely recognize adjectives they had previously designated to be self-descriptive. Expanding on this, Strube et al. (1986) found that false alarms occurred more for self-schema consistent content, presumably because the presence of such words in the schema makes them more accessible in memory.\n\nIn addition to investigating the self-reference effect in regards to schemata consistent information, Strube et al. discussed how counter schemata information relates to this framework. They noted that the pattern of making correct decisions more rapidly did not hold when considering words that countered a person's self-schema, presumably because they were difficult to integrate into memory due to lack of a preexisting structure. That is, they lacked the organizational structure of encoding because they did not fall into the \"like me\" category, and elaboration would not work because prior connections to the adjective did not exist.\n\nTwo of the most common functions of the self receiving significant attention in research are the self-acting to organize the individual's understanding of the social environment, and the self functioning to regulate behavior through self-evaluation. The concept of self-awareness is considered to be the foundational principle for both functions of the self. Some research presents self-awareness in terms of self-focused attention whereas Hull and Levy suggest that self-awareness refers to the encoding of information based on its relevance to the self. Based on the latter interpretation of self-awareness, individuals must identify the aspects of situations that are relevant to themselves and their behavior will be shaped accordingly. Hull and Levy suggest that self-awareness corresponds to the encoding of information cued by self-symbolic stimuli, and examine the idea of self-awareness as a method of encoding. They structured an investigation that examined self-referent encoding in individuals with different levels of self-awareness, predicting that individuals with higher levels of self-consciousness would encode self-relevant information more deeply than other information, and that they would encode it more deeply than individuals with low levels of self-consciousness. The results of their investigation supported their hypothesis that self-focused attention is not enough to explain the role of self-awareness on attribution. Their results suggest that self-awareness leads to increased sensitivity to the situationally defined meanings of behavior, and therefore organizes the individual's understanding of the social environment. The research presented by Hull and Levy led to future research on the encoding of information associated with self-awareness.\n\nIn later research, Hull and colleagues examined the associations between self-referential encoding, self-consciousness and the extent to which a stimulus is consistent with self-knowledge. They first assumed that the encoding of a stimulus is facilitated if an individual's working memory already contains information consistent with the stimulus, and suggested that self-consciousness as an encoding mechanism relies on an individual's self-knowledge. It is known that situational and dispositional factors may activate certain pools of knowledge, moving them into working memory, and guiding the processing of certain stimulus information.\n\nIn order to better understand the idea of activating information in memory, Hull et al. presented an example of how information is activated. They referred to the sentence \"The robber took the money from the bank\". In English, the word bank has two applicable meanings in the context of this sentence (monetary institution and river shore). However, the monetary institution meaning of the word is more highly activated in this context due to the addition of the words robber and money to the sentence, because they are associatively relevant and therefore pull the monetary institution definition for bank into working memory. Once information is added to working memory, meanings and associations are more easily drawn. Therefore, the meaning of this example sentence is almost universally understood.\n\nIn reference to self-consciousness and self-reference, the connection between self-consciousness and self-referent encoding relies on such information activation. Research suggests that self-consciousness activates knowledge relating to the self, thereby guiding the processing of self-relevant information. Three experiments conducted by Hull and colleagues provided evidence that a manipulation of accessible self-knowledge impacts self-referent encoding based on the self-relevance of such information, individual differences in the accessibility of self-knowledge (self-consciousness) impacts perception, and a mediation relationship exists between self-consciousness and individual differences in self-referential encoding.\n\nSimilar to how self-awareness impacts the availability of self-knowledge and the encoding of self-relevant information, through the development of the self-schema, people develop and maintain certain personality characteristics leading to a variety of behavior patterns. Research has been done on the differences between Type A and Type B behavior patterns, focusing on how people in each group respond to environmental information and their interpretation of the performance of others and themselves. It has been found that Type A behavior is characterized by competitive achievement striving, time urgency and hostility, whereas Type B is usually defined as an absence of Type A characteristics. When investigating causal attributions for hypothetical positive and negative outcomes, Strube et al. found that Type A individuals were more self-serving, in that they took greater responsibility for positive than negative effects. Strube and colleagues argued that this could be a result of the fact that schema-consistent information is more easily remembered and the ease with which past successes and failures are recalled, determined by self-schema, would impact attributions. It is reasonable to believe that Type A's might recall successes more easily and hence be more self-serving.\n\nInfluential psychologists Craik and Lockhart laid the groundwork for research focused on self-referential encoding and memory. In 1972 they proposed their Depth of Processing framework which suggests that memory retention depends on how the stimulus material was encoded in memory. Their original research considered structural, phonemic, and semantic encoding tasks, and showed that semantic encoding is the best method to aid in recall. They asked participants to rate 40 descriptive adjectives on one of four tasks; Structural (Big font or small font?), Phonemic (Rhymes with xxx?), Semantic (Means same as xxx?), or Self-reference (Describes you?). This was then followed by an \"incidental recall task\". This is where participants are asked, without prior warning, to recall as many of the words they had seen as possible within a given time limit. Craik and Tulving's original experiment showed that structural and phonemic tasks lead only to \"shallow\" encoding, while the semantic tasks lead to \"deep\" encoding and resulted in better recall.\n\nHowever, in 1977, it was shown that self-relevant or self-descriptive encoding leads to even better recall than semantic tasks. Experts suggest that the call on associative memory required by semantic tasks is what provides the advantage over structural or phonemic tasks, but is not enough to surpass the benefit provided by self-referential encoding. The fact that self-reference was shown to be a stronger memory encoding method than semantic tasks is what led to more significant interest in the field One early and significant experiment aimed to place self-reference on Craik and Lockhart's depth of processing hierarchy, and suggested that self-reference was a more beneficial encoding method than semantic tasks. In this experiment, participants filled out self-ratings on 84 adjectives. Months later, these participants were revisited and were randomly shown 42 of those words. They then had to select the group of 42 \"revisited\" words out of the total original list. The researchers argued that if the \"self\" was involved in memory retrieval, participants would incorrectly recognize words that were more self-descriptive In another experiment, subjects answered yes or no to cue questions about 40 adjective in 4 tasks (structural, phonemic, semantic and self-referential) and later had to recall the adjectives. This experiment validated the strength of self-reference as an encoding method, and indicated it developed a stronger memory trace than the semantic task.\n\nResearchers are implementing a new strategy by developing different encoding tasks that enhance memory very similarly to self-referential encoding. Symons (1990) had findings that went against the norm when he was unable to find evidence of self-schematicity in the self-reference effect. Another finding was that when referencing gender and religion, there was a low memory recall when compared with referencing the self. A meta-analysis by Symons and Johnson (1997) showed self-reference resulting in better memory in comparison to tasks relying on semantic encoding or other-referent encoding. According to Symons and Johnson, self-referencing questions elicit elaboration and organization in memory, both of which creating a deeper encoding and thus facilitate memory.\n\nTheorists that favor the view that the self has a special role believe that the self leads to more in depth processing, leading to easier recall during self-reference tasks. Theorists also promote the self-schema as being one of the sole inhibitors that allow for recall from deep memory. Thorndyke and Hayes-Roth had the goal of focusing on the process made by the active memory schemata. Sex-typed individuals recall trait adjectives that go along with their sex role more quickly than trait adjectives that are not. During the process of free recall, these individuals also showed more patterns for gender clustering than other sexually typed individuals.\n\nAs research on self-referential encoding became more prolific, some psychologists took an opportunity to delineate specific self-referential encoding tasks. It is noted that descriptive tasks are those that require participants to determine if a stimulus word can be classified as \"self-descriptive.\" Autobiographical tasks are those that require participants to use the stimulus word as a cue to recall an autobiographical memory. Results from experiments that differentiated between these types of self-referential encoding found that they both produced better recall than semantic tasks, and neither was more advantageous than the other. However, research does suggest that the two types of self-referential encoding do rely on different processes to facilitate memory. In most experiments discussed, these types of self- referential encoding were not differentiated.\n\nIn a typical self-reference task, adjectives are presented and classified as either self-descriptive or not. For example, in a study by Dobson and Shaw, adjectives about the self that were preselected were given to the participants and they decide whether or not the adjectives are self-descriptive. The basis for making certain judgments, decisions, inferences and decisions is a self-referent encoding task. If two items are classified as self-descriptive there is no reason one trait would not be equally as easy to retrieve as the other on a self-reference task.\n\nWhile a significant amount of research supports the existence of the self-reference effect, the processes behind it are not well understood. However, multiple hypotheses have been introduced, and two main arguments have been developed: the elaborative processing hypothesis and the organizational processing hypothesis. Encodings in reference to the self are so elaborate because of the information one has about the self. Information encoded with the self is better remembered than information encoded with reference to something else.\n\nElaboration refers to the encoding of a single word by forming connections between it and other material already stored in memory. By creating these connections between the stimulus word and other material already in memory, multiple routes for retrieval of the stimulus word are formed. Based on the depth of processing framework, memory retention increases as elaboration during encoding increases. The Elaborative Processing Hypothesis would suggest that any encoding task that leads to the development of the most trace elaboration or associations is the best for memory retention. Additional research on the depth of processing hierarchy suggests that self-reference is the superior method of information encoding. The elaborative hypothesis would suggest this is because self-reference creates the most elaborate trace, due to the many links that can be made between the stimulus and information about the self already in memory.\n\nThe organizational processing hypothesis was proposed by Klein and Kihlstrom. This hypothesis suggests that encoding is best prompted by considering stimulus words in relation to one another. This thought process and relational thinking creates word to word associations. These inter-item associations are paths in memory that can be used during retrieval. Also, the category labels that define the relations between stimulus items can be used as item cues. Evidence of the organizational component of encoding is demonstrated through the clustering of words during recall. Word clustering during recall indicates that relational information was used to store the words in memory. Rogers, Kuiper and Kirker showed that self-referential judgments were more likely to encourage organization than semantic ones. Therefore, they suggested the self-reference effect was likely due to the organizational processing endured by self-referential encoding.\n\nStructural, phonemic and semantic tasks within the depth of processing paradigm require words to be considered individually, and lend themselves to an elaborative approach. As such, it can be argued that self-referential encoding is superior because it leads to an indirect division of words into categories: words that describe me versus words that do not. Due to this connection between self-reference and organizational processing, further research has been done on this area. Klein and Kihlstrom's research suggests first that, like previous research, self-reference led to better recall than semantic and structural encoding. Second, they found that self-referentially encoded words were more clustered in recall than words from other tasks, suggesting higher levels of organizational processing. From this they concluded that the organization, not encoding task, is what makes self-referential encoding superior \n\nPsychologists Einstein and Hunt showed that both elaborative processing and organizational processing facilitate recall. However, their research argues that the effectiveness of either approach depends on how related the stimulus words are to one another. A list of highly related stimulus words would be better encoded using the elaborative method. The relations between the words would be evident to subjects; therefore, they would not gain any additional pathways for retrieval by encoding the words based on their categorical membership. Instead, the other information gained through elaborative processing would be more beneficial. On the other hand, a list of stimulus words with little relation would be better stored to memory through the organizational method. Since the words have no obvious connection to one another, subjects would likely encode them individually, using an elaborative approach. Since relational information wouldn't be readily detected, focusing on it would add to memory by creating new traces for retrieval. Superior recall was better explained by a combination of elaboration and organization.\nUltimately, the exact processes behind self-referential encoding that makes it superior to other encoding tasks are still under debate. Research suggests that if elaborative processing is behind self-referential encoding, a self-referential task should have the same effect as an elaborative task, whereas if organizational processing underlies the self-reference effect self-referential encoding tasks should function like organizational tasks. To test this, Klein and Loftus ran a 3x2 study testing organizational, elaborative and self-referential encoding with lists of 30 related or unrelated words. When participants were asked to memorize the unrelated list, recall and clustering were higher for the organizational task, which produced almost equal results to the self-referential task, suggesting that has an organizational basis. For the list of related words, the elaborative task led to better recall and had matched results to the self-reference task, suggesting an elaborative basis. This research, then, suggests that the self-reference effect cannot be explained by a single type of processing. Instead, self-referential encoding must lead to information in memory that incorporates item specific and relational information.\n\nOverall, the SRE relies on the unique mnemonic aspects of the self. Ultimately, if the research is suggesting that the self has superior elaborative or organizational properties, information related to the self should be more easily remembered and recalled. The research presented suggests that self-referential encoding is superior because it promotes organization and elaboration simultaneously, and provides self-relevant categories that promote recall.\n\nThe field of social brain science is aimed at examining the neural foundations of social behavior. Neuroimaging and neuropsychology have led to the examination of neuroanatomy and its connection to psychological topics. Through this research, neuropsychologists have found a connection between social cognitive functioning and the medial prefrontal cortex (mPFC). In addition, the mPFC has been connected to reflection and introspection about personal mental states. Supporting these findings, it has been shown that damage to the mPFC is connected to impairments with self-reflection, introspection and daydreaming, as well as social competence, but not other areas of functioning. As such, the mPFC has been connected to self-referential processing.\n\nThe research discussed by those focusing on the neuroanatomy of self-referential processing included similar tasks to the memory and depth of processing research discussed previously. When participants were asked to judge adjectives based in whether or not they were self-descriptive, it was noted that the more self-relevant the trait, the stronger the activation of the mPFC. In addition, it was shown that the mPFC was activated during the appraisal of one's own personality traits, as well as during trait retrieval. One study showed that the more activity in the mPFC during self-referential judgments, the more likely the word was to be remembered on a subsequent surprise memory test. These results suggest that the mPFC is involved in both self-referential processing and in creating self-relevant memories.\n\nMedial prefrontal cortex (mPFC) activation occurs during processing of self-relevant information. When self-referent judgment is more relatable and less negative, the mFPC is activated. Finding support clear cut circuits that have high levels of activation when cognitive and emotional aspects of self-reflection are present. The caudate nucleus has not been associated with self-reference before, however, Fossati and colleagues found activity while participants were retrieving self-relevant trait adjectives. The ventral anterior cingulate cortex (vACC) is also a part of the brain that becomes activated when there are signs of self-referencing and processing. The vACC is activated when self-descriptive information is negative. There is also pCC (posterior cingulate cortex) activity seen in neuroimaging studies during self-referential processing.\n\nGiven all of the neurological support for the effect of self-reference on encoding and memory, there is still a debate in the psychological community about whether or not the self-reference effect signifies a special functional role played by the self in cognition. Generally, this question is met by people that have two opposing views on the processes behind self-reference. On one side of the debate, people believe that the self has special mnemonic abilities because it is a unique cognitive structure. On the other side, people support the arguments described above that suggest there is no special structure, but instead, the self-reference effect is simply a part of the standard depth of processing hierarchy. Since the overall hypothesis is the same for both sides of the debate, that self-relevant material leads to enhanced memory, it is difficult to test them using strictly behavioral measures. Therefore, PET and fMRI scans have been used to see the neural marker of self-referential mental activity.\n\nPrevious studies have shown that areas of the left prefrontal cortex are activated during semantic encoding. Therefore, if the self-reference effect works the same way, as part of the depth of processing hierarchy, the same brain region should be activated when judging traits related to the self. However, if the self has unique mnemonic properties, then self-referential tasks should activate brain regions distinct from those activated during semantic tasks. The field is still at is infancy, but future work on this hypothesis might help to settle the debate about the underlying processes of self-referential encoding.\n\nWhile not able to completely settle the debate over the foundation of self-referential processing, studies on the neurological aspect of personality trait judgments did lead to a related, significant result. It has been shown that judging personality traits about oneself and a close friend activated overlapping brain regions, and the activated regions have all been implicated in self-reference. Noting the similarity between making self-judgments and judgments about close others led to the introduction of the simulation theory of empathy. Simulation theory rests on the idea that one can make inferences about others by using the knowledge they have about themselves. In essence, the theory suggests that people use self-reflection to understand or predict the mental state of others. The more similar a person perceives another to be, the more active the mPFC has shown to be, suggesting more deep or intricate self-reference. However, this effect can cause people to make inaccurate judgments about others or to believe that their own opinions are representative of others in general. This misrepresentation is referred to as the false-consensus effect.\n\nIn addition to simulation theory, other expansions of the self-reference effect have been examined. Through studying the self, researchers have found that the self consists of many independent cognitive representations. For example, the personal self composed of individual characteristics is separate from the relational self which is based on relationships with significant others. These two forms of self are again separate from the collective self which corresponds to a particular group identity. Noting the existence of the collective self and the different group identities that combine to form such a self-representation led researchers to question if information stored in reference to a social group identity has the same effects in memory as information stored in reference to the individual self. In essence, researchers questioned if the self-reference effect can be extended to include situations where the self is more socially defined, producing a group-reference effect.\n\nPrevious research supports the idea that the group-reference effect should exist from a theoretical standpoint. First, the self-expansion model argues that individuals incorporate characteristics of their significant others (or other in-group members into the development of their self-concept. From this model, it is reasonable to conclude that characteristics that are common to both oneself and their significant others (or in-group members) would be more accessible. Second, the previous research discussed suggests that the self-reference effect is due to some combination of organizational, elaborative, mental cueing or evaluative properties of self-referential encoding tasks. Given that we have significant stores of knowledge about our social identities, and such collective identities provide an organizational framework, it is reasonable to assume that a group-reference task would operate similar to that of a self-reference task.\n\nIn order to test these claims, Johnson and colleagues aimed to test whether the self-reference effect generalized to group level identities. Their first study was structured to simply assess if group-reference influenced subsequent memory. In their experiment, they used membership at a particular university as the group of reference. They included group-reference, self-reference and semantic tasks. The experiment replicated the self-reference effect, consistent with previous research. In addition, evidence for a group-reference effect was found. Group-referenced encoding produced better recall than the semantic tasks, and the level of recall from the group-referenced task was not significantly different from the self-referenced task.\n\nDespite finding evidence of a group-reference effect, Johnson and colleagues pointed out that people identify with numerous groups, each with unique characteristics. Therefore, in order to reach conclusive evidence of a group-reference effect, alternative group targets need to be considered. In a second experiment by Johnson et al., the group of reference was modified to be the family of the individual. This group has fewer exemplars than the pool of university students, and affective considerations of the family as a group should be strong. No specific instructions or definitions were provided for family, allowing individuals to consider either the group as a whole (prototype) or specific exemplars (group). When the experiment was repeated using family as the group of reference, group-reference produced recall as much as self-reference. The mean number of recall for the group-reference was higher than self-reference. Participants indicated that they considered both the prototype and individual exemplars when responding to the questions, suggesting that the magnitude of the group-reference effect might not be dependent on the number of exemplars in the target group.\nBoth experiments presented by Johnson et al. found evidence for the group-reference effect. However, these conclusions are limited to the target groups of university students and family. Other research included gender (males and females) and religion (Jewish) as the reference groups and the group-reference effect on memory was not as evident. The group-reference recall for these two groups was not significantly more advantageous than the semantic task. Questioning what characteristics of reference groups that lead to the group-reference effect, a meta-analysis of all four group-reference conditions was performed. This analysis found that self-reference emerged as the most powerful encoding device; however, evidence was found to support the existence of a group-reference effect. The size of the reference groups and number of specific, individual exemplars was hypothesized to influence the existence of the group-reference effect. In addition, accessibility and level of knowledge about group members may also impact such an effect. So, while university students is a much larger group than family, individual exemplars may be more readily accessible than those in a religious group. Similarly, different cognitive representations were hypothesized to influence the group-reference effect. When a larger group is considered, people may be more likely to consider a prototype which may lead to fewer elaborations and cues later on. Smaller groups may lead to relying on the prototype and specific exemplars. Finally, desirability judgments that influence later processing may be influenced by self-reference and certain group-reference tasks. Individuals may be more sensitive to evaluative implications for the personal self and some group identities, but not others.\n\nGroups are also a major part of the self; therefore we attribute the role that different groups play in our self-concept also play a role in the self-reference effect. We process information about group members similarly to how we process for ourselves. Recall of remarks referencing our home and our self and group to familiarity of those aspects of our self. Reference to the self and social group and the identity that comes along with being a part of a social group are equally affective for memory. This is especially true when the groups are small, rather than large.\n\nUltimately, the group-reference effect provides evidence to explain the tendency to notice or pay attention to and remember statements made in regard to our home when traveling in a foreign place. Considering the proposal that groups form part of the self, this phenomenon can be considered an extension of the self-reference effect. Similar to the memorable nature of references to a person's individual self, references to social identities are seemed to be privileged in memory as well.\n\nOnce the foundation of research on self-referential encoding was established, psychologists began to explore how the concept applied to different groups of people, and connected to different phenomena.\n\nIndividuals diagnosed with autism spectrum disorders (ASDs) can display a wide range of symptoms. Some of the most common characteristics of individuals with ASDs include impairments with social functioning, language and communication difficulties, repetitive behaviors and restricted interests. In addition, it is often noted that these individuals are more \"self-focused.\" That is, they have difficulty seeing things from another's perspective. Despite being self-focused, though, research has shown that individuals with ASD's often have difficulty identifying or describing their emotions or the emotions of others. When asked to describe their daily experiences, responses from individuals on the autism spectrum tended to focus more on physical descriptions rather than mental and emotional states. In regards to their social interactions and behavior differences, it is thought that these individuals lack top down control, and therefore, their bottom up decisions remain unchecked. This simply suggests that these individuals cannot use their prior knowledge and memory to make sense of new input, but instead react to each new input individually, compiling them to make a whole picture \n\nNoting the difficulty individuals with ASDs experience with self-awareness, it was thought that they might have difficulty with self-related memory processes. Psychologists questioned if these individuals would show the typical self-reference effect in memory. In one Depth of Processing Study, participants were asked questions about the descriptiveness of certain stimulus words. However, unlike previous DOP studies that focused on phonemic, structural, semantic and self-referential tasks, the tasks were altered for this experiment. To test the referential abilities of individuals with ASD's, the encoding tasks were divided into: \"the self,\" asking to what extent a stimulus word described oneself, \"similar close other,\" asking to what extent a stimulus word was descriptive of one's best friend, \"dissimilar non-close other,\" asking to what extent a stimulus word was descriptive of Harry Potter, and a control group that was asked to determine the number of syllables in each word. Following these encoding tasks, participants were given thirty minutes before a surprise memory task. It was found that individuals with ASD's had no impairment in memory for words encoded in the syllable or dissimilar non-close other condition. However, they had decreased memory for words related to the self.\n\nTherefore, while research suggests that self-referentially encoded information is encoded more deeply than other information, the research on individuals with ASD's showed no advantage for memory recognition with self-reference tasks over semantic encoding tasks. This suggests that individuals with ASD's don't preferentially encode self-relevant information. Psychologists have investigated the biological basis for the decreased self-reference effect among individuals with Autism Spectrum Disorders and have suggested that it may be due to less specialized neural activity in the mPFC for those individuals. However, while individuals with ASD's showed smaller self-reference effects than the control group, some evidence of a self-reference effect was evident in some cases. This indicates that self-referent impairments are a matter of degree, not total absence.\n\nLombardo and his colleagues measured empathy among individuals with ASD's, and showed that these individuals scored lower than the control group on all empathy measures. This may be a result of the difficulty for these individuals to understand or take the perspective of others, in conjunction with their difficulty identifying emotions. This has implications for simulation theory, because these individuals are unable to use their self-knowledge to make conclusions about similar others.\n\nUltimately, the research suggests that people with ASD's might benefit from being more self-focused. The better their ability to reflect on themselves, the better the can mentalize with others.\n\nThere are three possible relations between cognitive processes and anxiety and depression. The first is whether cognitive processes are actually caused by the onset of clinically diagnosed symptoms of major depression or just generalized sadness or anxiousness. The second is whether emotional disorders such as depression and anxiety are able to be considered as caused by cognitions. And the third is whether different specific cognitive processes are able to be considered associates of different disorders. Kovacs and Beck (1977) posited a schematic model of depression where an already depressed self was primed by outside prompts that negatively impacted cognitive illusions of the world in the eye of oneself. These prompts only led participants to a more depressive series of emotions and behavior. The results from the study done by Derry and Kuiper supported Beck's theory that a negative self-schema is present in people, especially those with depressive disorder. Depressed individuals attribute depressive adjectives to themselves more than nondepressive adjectives. Those suffering from a more mild case of depression have trouble deciphering between the traits of themselves and others which results in a loss of their self-esteem and their negative self-evaluation. A depressive schema is what causes the negativity reported by those suffering from depression. Kuiper and Derry found that self-referent recall enhancement was limited only to nondepressed content.\n\nGenerally, self-focus is association with negative emotions. In particular private self-focus is more strongly associated with depression than public self-focus. Results from brain-imaging studies shows\nthat during self-referential processing, those with major depressive disorder show greater activation in the medial prefrontal cortex, suggesting that depressed individuals may be exhibiting greater cognitive control than\nnon-depressed individuals when processing self-relevant information.\n"}
{"id": "858507", "url": "https://en.wikipedia.org/wiki?curid=858507", "title": "Self-referential humor", "text": "Self-referential humor\n\nSelf-referential humor, also known as self-reflexive humor or meta humor, is a type of comedic expression that—either directed toward some other subject, or openly directed toward itself—intentionally alludes to the very person who is expressing the humor in a comedic fashion, or to some specific aspect of that same comedic expression. Self-referential humor expressed discreetly and surrealistically is a form of bathos. In general, self-referential humor often uses hypocrisy, oxymoron, or paradox to create a contradictory or otherwise absurd situation that is humorous to the audience. \n\nSelf-referential humor is sometimes combined with breaking the fourth wall to explicitly make the reference directly to the audience, or make self-reference to an element of the medium that the characters should not be aware of.\n\nOld Comedy of Classical Athens is held to be the first—in the extant sources—form of self-referential comedy. Aristophanes, whose plays form the only remaining fragments of Old Comedy, used fantastical plots, grotesque and inhuman masks and status reversals of characters to slander prominent politicians and court his audience's approval.\n\nRAS syndrome refers to the redundant use of one or more of the words that make up an acronym or initialism with the abbreviation itself, thus in effect repeating one or more words. However, \"RAS\" stands for Redundant Acronym Syndrome; therefore, the full phrase yields \"Redundant Acronym Syndrome syndrome\" and is self-referencing in a comical manner. It also reflects an excessive use of TLAs (Three Letter Acronyms).\n\nMeta has come to be used, particularly in art, to refer to something that is self-referential. Popularised by Douglas Hofstadter who wrote several books on himself and the subject of self-reference, meta-jokes are a popular form of humor.\n\n"}
{"id": "4159251", "url": "https://en.wikipedia.org/wiki?curid=4159251", "title": "Source text", "text": "Source text\n\nA source text is a text (sometimes oral) from which information or ideas are derived. In translation, a source text is the original text that is to be translated into another language.\n\nIn historiography, distinctions are commonly made between three kinds of source texts:\n\nPrimary sources are firsthand written evidence of history made at the time of the event by someone who was present. They have been described as those sources closest to the origin of the information or idea under study. These types of sources have been said to provide researchers with \"direct, unmediated information about the object of study.\" Primary sources are sources which, usually, are recorded by someone who participated in, witnessed, or lived through the event. These are also usually authoritative and fundamental documents concerning the subject under consideration. This includes published original accounts, published original works, or published original research. They may contain original research or new information not previously published elsewhere. They have been distinguished from secondary sources, which often cite, comment on, or build upon primary sources. They serve as an original source of information or new ideas about the topic. \"Primary\" and \"secondary\", however, are relative terms, and any given source may be classified as primary or secondary, depending on how it is used. Physical objects can be primary sources.\n\nSecondary sources are written accounts of history based upon the evidence from primary sources. These are sources which, usually, are accounts, works, or research that analyze, assimilate, evaluate, interpret, and/or synthesize primary sources. These are not as authoritative and are supplemental documents concerning the subject under consideration. These documents or people summarize other material, usually primary source material. They are academics, journalists, and other researchers, and the papers and books they produce. This includes published accounts, published works, or published research. For example a history book drawing upon diary and newspaper records. \n\nTertiary sources are compilations based upon primary and secondary sources. These are sources which, on average, do not fall into the above two levels. They consist of generalized research of a specific subject under consideration. Tertiary sources are analyzed, assimilated, evaluated, interpreted, and/or synthesized from secondary sources, also. These are not authoritative and are just supplemental documents concerning the subject under consideration. These are often meant to present known information in a convenient form with no claim to originality. Common examples are encyclopedias and textbooks.\n\nThe distinction between \"primary source\" and \"secondary source\" is standard in historiography, while the distinction between these sources and \"tertiary sources\" is more peripheral, and is more relevant to the scholarly research work than to the published content itself.\n\nBelow are types of sources that most generally, but not absolutely, fall into a certain level. The letters after an item describes \"generally\" the type it is (though this can vary pending the exact source). \"P\" is for Primary sources, \"S\" is for Secondary sources, and \"T\" is for Tertiary sources. (ed., those with \"?\"s are indeterminate.)\nIn translation, a source text (ST) is a text written in a given source language which is to be, or has been, translated into another language. In translation the source text (ST) is transformed into a target text (TT), written in a given target language. According to Jeremy Munday's definition of translation, \"the process of translation between two different written languages involves the changing of an original written text (the source text or ST) in the original verbal language (the source language or SL) into a written text (the target text or TT) in a different verbal language (the target language or TL)\". \n\nTranslation scholars including Eugene Nida and Peter Newmark have represented the different approaches to translation as falling broadly into source-text-oriented or target-text-oriented categories.\n"}
{"id": "54226143", "url": "https://en.wikipedia.org/wiki?curid=54226143", "title": "The Crime Book", "text": "The Crime Book\n\nThe Crime Book (Big Ideas Simply Explained) is a non-fiction volume co-authored by American crime writers Cathy Scott, Shanna Hogan, Rebecca Morris, Canadian author and historian Lee Mellor, and United Kingdom author Michael Kerrigan, with a foreword for the U.S. edition by Scott and the U.K. edition by crime-fiction author Peter James. It was released by DK Books under its Big Ideas Learning imprint in May 2017.\n\nThe publisher describes \"The Crime Book\" as a guide to criminology that explores the most infamous cases of all time, from serial killers to mob hits to war crimes and more.\n\nIt includes a variety of crimes committed by more than 100 of the world's most notorious criminals. From Jack the Ripper to Jeffrey Dahmer, the book is a study of international true-crime history that covers shocking stories through infographics and research that lays out key facts and details. It examines the science, psychology and sociology of criminal behavior. It profiles of villains, victims and detectives. Each clue is listed for readers to follow investigations from start to finish, and studies the police and detective work for each case.\n\nIn a Q&A article for CrimeCon's blog with Scott, the author described the crimes detailed in the book as having \"such diversity that there is something for everyone. ... I can’t think of one crime that’s not represented in The Crime Book. It runs the gamut—from nonviolent cons to gangland-style criminals, to white-collar offenders—with a complete representation starting with the first known homicide committed against a Neanderthal man. Simply put, you can’t make this stuff up.\"\n\n\"Rolling Stone\" magazine's description, in an August 2017 interview with co-author Scott about the book, wrote that it is \"an encyclopedic treatment of the topic (that) makes for excellent companion reading. A compelling compilation of human trickery and awfulness, it covers crimes from arson, art forgery and kidnapping to bank robbery, drug trafficking and, of course, murder, with many of the entries accompanied by helpful illustrations.\"\n\n\"Reader's Digest\" listed it as one of its \"Best New Books You Should Read This April,\" describing it as \"everything you ever wanted to know about some of the most audacious, hideous, hilarious and mysterious acts of crime in one explosive book, filled with graphs, illustrations, quotes and timelines. This highly addictive encyclopedia of crime ... is a trivia goldmine and a helpful guide allowing you to put events into context.\"\n\n\"Culture Magazine\" in Germany had this to say: \"The level of expertise is quite high,\" noting that the book \"is lushly illustrated, readable and entertaining.\"\n\nIn its review, \"Crime Fiction Lover\" wrote that \"a crack team of true-crime experts helped put it together.\"\n\n\"Crimespree Magazine\" wrote, \"The crimes covered are all over from serial killers to gangsters and outlaws to kidnappers and elderly Brit bank robbers. This is a great book.\"\n\n"}
{"id": "270906", "url": "https://en.wikipedia.org/wiki?curid=270906", "title": "Three-letter acronym", "text": "Three-letter acronym\n\nA three-letter acronym (TLA), or three-letter abbreviation, is an abbreviation, specifically an acronym, alphabetism, or initialism, consisting of three letters. These are usually the initial letters of the words of the phrase abbreviated, and are written in capital letters (upper case); three-letter abbreviations such as \"etc.\" and \"Mrs.\" are not three-letter acronyms, but \"TLA\" is a TLA (an example of an autological abbreviation).\n\nMost three-letter abbreviations are \"initialisms\": all the letters are pronounced as the names of letters, as in \"APA\" . Some are acronyms pronounced as a word; computed axial tomography, CAT, is almost always pronounced as the animal's name in \"CAT scan\".\n\n\nThe exact phrase \"three-letter acronym\" appeared in the sociology literature in 1975. Three-letter acronyms were used as mnemonics in biological sciences, from 1977 and their practical advantage was promoted by Weber in 1982. They are used in many other fields, but the term TLA is particularly associated with computing. In 1980, the manual for the Sinclair ZX81 home computer used and explained TLA. The specific generation of three-letter acronyms in computing was mentioned in a JPL report of 1982. In 1988, in a paper titled \"On the cruelty of really teaching computer science\", eminent computer scientist Edsger W. Dijkstra wrote \n\"Because no endeavour is respectable these days without a TLA ...\" By 1992 it was in a Microsoft handbook.\n\nThe number of possible three-letter abbreviations (or permutations) using the 26 letters of the alphabet from A to Z (AAA, AAB ... to ZZY, ZZZ) is 26 × 26 × 26 = 17,576. Another 26 × 26 × 10 = 6760 can be produced if the third element is allowed to be a digit 0-9, giving a total of 24,336.\n\nIn English, WWW is the longest possible TLA to pronounce, typically requiring nine syllables. The usefulness of TLAs typically comes from how it is quicker to say the acronym instead of than the phrase they represent, however saying 'WWW' in English requires three times as many syllables than the phrase it is meant to abbreviate (World Wide Web). Consequently, \"www\" is sometimes abbreviated as \"dubdubdub\" in speech.\n\n\n"}
{"id": "303405", "url": "https://en.wikipedia.org/wiki?curid=303405", "title": "Universal set", "text": "Universal set\n\nIn set theory, a universal set is a set which contains all objects, including itself. In set theory as usually formulated, the conception of a universal set leads to a paradox (Russell's paradox) and is consequently not allowed. However, some non-standard variants of set theory include a universal set.\n\nThere is no standard notation for the universal set of a given set theory. Common symbols include V, U and ξ.\n\nZermelo–Fraenkel set theory and related set theories, which are based on the idea of the cumulative hierarchy, do not allow for the existence of a universal set. It is directly contradicted by the axiom of regularity, and its existence would cause paradoxes which would make the theory inconsistent.\n\nRussell's paradox prevents the existence of a universal set in Zermelo–Fraenkel set theory and other set theories that include Zermelo's axiom of comprehension.\nThis axiom states that, for any formula formula_1 and any set , there exists another set \nthat contains exactly those elements of that satisfy formula_3. If a universal set  existed and the axiom of comprehension could be applied to it, then\nthere would also exist another set formula_4, the set of all sets that do not contain themselves. However, as Bertrand Russell observed, this set is paradoxical. If it contains itself, then it should not contain itself, and vice versa. For this reason, it cannot exist.\n\nA second difficulty with the idea of a universal set concerns the power set of the set of all sets. Because this power set is a set of sets, it would necessarily be a subset of the set of all sets, provided that both exist. However, this conflicts with Cantor's theorem that the power set of any set (whether infinite or not) always has strictly higher cardinality than the set itself.\n\nThe difficulties associated with a universal set can be avoided either by using a variant of set theory in which the axiom of comprehension is restricted in some way, or by using a universal object that is not considered to be a set.\n\nThere are set theories known to be consistent (if the usual set theory is consistent) in which the universal set does exist (and formula_5 is true). In these theories, Zermelo's axiom of comprehension does not hold in general, and the axiom of comprehension of naive set theory is restricted in a different way. A set theory containing a universal set is necessarily a non-well-founded set theory.\nThe most widely studied set theory with a universal set is Willard Van Orman Quine's New Foundations. Alonzo Church and also published work on such set theories. Church speculated that his theory might be extended in a manner consistent with Quine's,\n\nAnother example is positive set theory, where the axiom of comprehension is restricted to hold only for the positive formulas (formulas that do not contain negations). Such set theories are motivated by notions of closure in topology.\n\nThe idea of a universal set seems intuitively desirable in the Zermelo–Fraenkel set theory, particularly because most versions of this theory do allow the use of quantifiers over all sets (see universal quantifier). One way of allowing an object that behaves similarly to a universal set, without creating paradoxes, is to describe and similar large collections as proper classes rather than as sets. One difference between a universal set and a universal class is that the universal class does not contain itself, because proper classes cannot be elements of other classes. Russell's paradox does not apply in these theories because the axiom of comprehension operates on sets, not on classes.\n\nThe category of sets can also be considered to be a universal object that is, again, not itself a set. It has all sets as elements, and also includes arrows for all functions from one set to another. \nAgain, it does not contain itself, because it is not itself a set.\n\n\n"}
{"id": "58757675", "url": "https://en.wikipedia.org/wiki?curid=58757675", "title": "William Chaffers", "text": "William Chaffers\n\nWilliam Chaffers (28 September 1811 – 12 April 1892) was an English antiquary and writer of reference works on hallmarks, and marks on ceramics. His \"Marks and Monograms on Pottery and Porcelain\", first published in 1863, has appeared in many later editions.\n\nChaffers was the son of William Chaffers and wife Sarah, and was born in Watling Street, London, in 1811; he was descended from a brother of Richard Chaffers (1731–1765), a manufacturer of Liverpool porcelain. He was educated at Margate and at Merchant Taylors' School, where he was entered in 1824.\n\nHe was attracted to antiquarian studies while a clerk in the city of London, by the discovery of Roman and medieval antiquities in the foundations of the Royal Exchange during 1838–9. At the same time he began to concentrate attention upon the study of gold and silver plate and ceramics, especially in regard to the official and other marks by which dates and places of fabrication can be distinguished. In 1863 Chaffers published two important works:\n\nOther publications are \"The Keramic Gallery\", in 2 volumes, with 500 illustrations (1872); a handbook abridged from \"Marks and Monograms\" (1874); \"Gilda Aurifabrorum\", a history of goldsmiths and plate workers and their marks (1883); also a priced catalogue of coins, and other minor catalogues.\n\nHis reputation was furthered in organizing exhibitions of art treasures, at Manchester in 1857, South Kensington in 1862, Leeds in 1869, Dublin in 1872, Wrexham in 1876, and Hanley (at the great Staffordshire exhibition of ceramics) in 1890. Chaffers was elected Fellow of the Society of Antiquaries of London in 1843, and he was a frequent contributor to \"Archæologia\", to \"Notes and Queries\", and to various learned periodicals upon the two subjects of which he had particular knowledge.\n\nIn 1841 he married Charlotte Matilda, daughter of John Hewett. About 1870 he retired from Fitzroy Square to a house in Willesden Lane, and later moved to West Hampstead, where he died on 12 April 1892.\n\nAttribution\n"}
