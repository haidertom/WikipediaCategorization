{"id": "6109333", "url": "https://en.wikipedia.org/wiki?curid=6109333", "title": "Andreev reflection", "text": "Andreev reflection\n\nAndreev reflection (AR), named after the Russian physicist Alexander F. Andreev, is a type of particle scattering which\noccurs at interfaces between a superconductor (S) and a normal state material (N). It is a charge-transfer process by which normal current in N is converted to supercurrent in S. Each Andreev reflection transfers a charge \"2e\" across the interface, avoiding the forbidden single-particle transmission within the superconducting energy gap.\n\nThe process involves an electron (hole) incident on the interface from the normal state material at energies less than the superconducting energy gap. The incident electron (hole) forms a Cooper pair in the superconductor with the retroreflection of a hole (electron) of opposite spin and velocity but equal momentum to the incident electron (hole), as seen in the figure. The barrier transparency is assumed to be high, with no oxide or tunnel layer which reduces instances of normal electron-electron or hole-hole scattering at the interface. Since the pair consists of an up and down spin electron, a second electron (hole) of opposite spin to the incident electron (hole) from the normal state forms the pair in the superconductor, and hence the retroreflected hole (electron). Through time-reversal symmetry, the process with an incident electron will also work with an incident hole (and retroreflected electron).\n\nThe process is highly spin-dependent – if only one spin band is occupied by the conduction electrons in the normal-state material (\"i.e.\" it is fully spin-polarized), Andreev reflection will be inhibited due to inability to form a pair in the superconductor and impossibility of single-particle transmission. In a ferromagnet or material where spin-polarization exists or may be induced by a magnetic field, the strength of the Andreev reflection (and hence conductance of the junction) is a function of the spin-polarization in the normal state.\n\nThe spin-dependence of AR gives rise to the Point Contact Andreev Reflection (or PCAR) technique, whereby a narrow superconducting tip (often niobium, antimony or lead) is placed into contact with a normal material at temperatures below the critical temperature of the tip. By applying a voltage to the tip, and measuring differential conductance between it and the sample, the spin polarization of the normal metal at that point (and magnetic field) may be determined. This is of use in such tasks as measurement of spin-polarized currents or characterizing spin polarization of material layers or bulk samples, and the effects of magnetic fields on such properties.\n\nIn an AR process, the phase difference between the electron and hole is −π/2 plus the phase of the superconducting order parameter.\n\nCrossed Andreev reflection, or CAR, also known as non-local Andreev reflection occurs when two spatially separated normal state material electrodes form two separate junctions with a superconductor, with the junction separation of the order of the BCS superconducting coherence length of the material in question. In such a device, retroreflection of the hole from an Andreev reflection process, resulting from an incident electron at energies less than the superconducting gap at one lead, occurs in the second spatially separated normal lead with the same charge transfer as in a normal AR process to a Cooper pair in the superconductor. For CAR to occur, electrons of opposite spin must exist at each normal electrode (so as to form the pair in the superconductor). If the normal material is a ferromagnet this may be guaranteed by creating opposite spin polarization via the application of a magnetic field to normal electrodes of differing coercivity.\n\nCAR occurs in competition with elastic cotunelling or EC, the quantum mechanical tunneling of electrons between the normal leads via an intermediate state in the superconductor. This process conserves electron spin. As such, a detectable CAR potential at one electrode on the application of current to the other may be masked by the competing EC process, making clear detection difficult. In addition, normal Andreev reflection may occur at either interface, in conjunction with other normal electron scattering processes from the normal/superconductor interface.\n\nThe process is of interest in the formation of solid-state quantum entanglement, via the formation of a spatially separated entangled electron-hole (Andreev) pair, with applications in spintronics and quantum computing.\n\n\n"}
{"id": "56312430", "url": "https://en.wikipedia.org/wiki?curid=56312430", "title": "Breath-figure self-assembly", "text": "Breath-figure self-assembly\n\nBreath-figure self-assembly is the self-assembly process of formation of honeycomb micro-scaled polymer patterns by the condensation of water droplets. \"Breath-figure\" refers to the fog that forms when water vapor contacts a cold surface. In the modern era systematic study of the process of breath-figures water condensation was carried out by Aitken and Rayleigh, among others. Half a century later the interest to the breath-figure formation was revived in a view of study of atmospheric processes, and in particular the extended study of a dew formation which turned out to be a complicated physical process. The experimental and theoretical study of dew formation has been carried out by Beysens. Thermodynamic and kinetic aspects of dew formation, which are crucial for understanding of formation of breath-figures inspired polymer patterns will be addressed further in detail. \n\nBreakthrough in the application of the breath-figures patterns was achieved in 1994–1995 when Widawski, François and Pitois reported manufacturing of polymer films with a self‐organized, micro-scaled, honeycomb morphology using the breath-figures condensation process. The reported process was based on the rapidly evaporated polymer solutions exerted to humidity. The  introduction into experimental techniques involved in manufacturing of micropatterned surfaces is supplied in reference 1; image representing typical breath-figures-inspired honeycomb pattern is shown in Figure 1. \n\nThe main physical processes involved in the process are: 1) evaporation of the polymer solution; 2) nucleation of water droplets; 3) condensation of water droplets; 4) growth of droplets; 5) evaporation of water; 6) solidification of polymer giving rise to the eventual micro-porous pattern. This experimental technique allows obtaining well-ordered, hierarchical, honeycomb surface patterns. A variety of experimental techniques were successfully exploited for the formation of breath-figures self-assembly induced patterns including drop-casting, dip-coating and spin-coating. Hierarchical patterning occurring under breath-figures self-assembly was reported.  The characteristic dimension of pores is usually close to 1 µm, whereas the characteristic lateral dimension of the large-scale patterns is ca. 10–50 µm.\n\n"}
{"id": "7555", "url": "https://en.wikipedia.org/wiki?curid=7555", "title": "Casimir effect", "text": "Casimir effect\n\nIn quantum field theory, the Casimir effect and the Casimir–Polder force are physical forces arising from a quantized field. They are named after the Dutch physicist Hendrik Casimir who predicted them in 1948.\n\nThe Casimir effect can be understood by the idea that the presence of conducting metals and dielectrics alters the vacuum expectation value of the energy of the second quantized electromagnetic field. Since the value of this energy depends on the shapes and positions of the conductors and dielectrics, the Casimir effect manifests itself as a force between such objects.\n\nAny medium supporting oscillations has an analogue of the Casimir effect. For example, beads on a string as well as plates submerged in noisy water or gas illustrate the Casimir force.\n\nIn modern theoretical physics, the Casimir effect plays an important role in the chiral bag model of the nucleon; in applied physics, it is significant in some aspects of emerging microtechnologies and nanotechnologies.\n\nThe typical example is of the two uncharged conductive plates in a vacuum, placed a few nanometers apart. In a classical description, the lack of an external field means that there is no field between the plates, and no force would be measured between them. When this field is instead studied using the quantum electrodynamic vacuum, it is seen that the plates do affect the virtual photons which constitute the field, and generate a net force – either an attraction or a repulsion depending on the specific arrangement of the two plates. Although the Casimir effect can be expressed in terms of virtual particles interacting with the objects, it is best described and more easily calculated in terms of the zero-point energy of a quantized field in the intervening space between the objects. This force has been measured and is a striking example of an effect captured formally by second quantization.\n\nThe treatment of boundary conditions in these calculations has led to some controversy. In fact, \"Casimir's original goal was to compute the van der Waals force between polarizable molecules\" of the conductive plates. Thus it can be interpreted without any reference to the zero-point energy (vacuum energy) of quantum fields.\n\nBecause the strength of the force falls off rapidly with distance, it is measurable only when the distance between the objects is extremely small. On a submicron scale, this force becomes so strong that it becomes the dominant force between uncharged conductors. In fact, at separations of 10 nm – about 100 times the typical size of an atom – the Casimir effect produces the equivalent of about 1 atmosphere of pressure (the precise value depending on surface geometry and other factors).\n\nDutch physicists Hendrik Casimir and Dirk Polder at Philips Research Labs proposed the existence of a force between two polarizable atoms and between such an atom and a conducting plate in 1947; this special form is called the Casimir–Polder force. After a conversation with Niels Bohr, who suggested it had something to do with zero-point energy, Casimir alone formulated the theory predicting a force between neutral conducting plates in 1948 which is called the Casimir effect in the narrow sense.\n\nPredictions of the force were later extended to finite-conductivity metals and dielectrics, and recent calculations have considered more general geometries. Experiments before 1997 had observed the force qualitatively, and indirect validation of the predicted Casimir energy had been made by measuring the thickness of liquid helium films. However it was not until 1997 that a direct experiment by S. Lamoreaux quantitatively measured the force to within 5% of the value predicted by the theory. Subsequent experiments approach an accuracy of a few percent.\n\nThe causes of the Casimir effect are described by quantum field theory, which states that all of the various fundamental fields, such as the electromagnetic field, must be quantized at each and every point in space. In a simplified view, a \"field\" in physics may be envisioned as if space were filled with interconnected vibrating balls and springs, and the strength of the field can be visualized as the displacement of a ball from its rest position. Vibrations in this field propagate and are governed by the appropriate wave equation for the particular field in question. The second quantization of quantum field theory requires that each such ball-spring combination be quantized, that is, that the strength of the field be quantized at each point in space. At the most basic level, the field at each point in space is a simple harmonic oscillator, and its quantization places a quantum harmonic oscillator at each point. Excitations of the field correspond to the elementary particles of particle physics. However, even the vacuum has a vastly complex structure, so all calculations of quantum field theory must be made in relation to this model of the vacuum.\n\nThe vacuum has, implicitly, all of the properties that a particle may have: spin, or polarization in the case of light, energy, and so on. On average, most of these properties cancel out: the vacuum is, after all, \"empty\" in this sense. One important exception is the vacuum energy or the vacuum expectation value of the energy. The quantization of a simple harmonic oscillator states that the lowest possible energy or zero-point energy that such an oscillator may have is\n\nSumming over all possible oscillators at all points in space gives an infinite quantity. Since only \"differences\" in energy are physically measurable (with the notable exception of gravitation, which remains beyond the scope of quantum field theory), this infinity may be considered a feature of the mathematics rather than of the physics. This argument is the underpinning of the theory of renormalization. Dealing with infinite quantities in this way was a cause of widespread unease among quantum field theorists before the development in the 1970s of the renormalization group, a mathematical formalism for scale transformations that provides a natural basis for the process.\n\nWhen the scope of the physics is widened to include gravity, the interpretation of this formally infinite quantity remains problematic. There is currently no compelling explanation as to why it should not result in a cosmological constant that is many orders of magnitude larger than observed. However, since we do not yet have any fully coherent quantum theory of gravity, there is likewise no compelling reason as to why it should.\n\nThe Casimir effect for fermions can be understood as the spectral asymmetry of the fermion operator formula_2, where it is known as the Witten index.\n\nAlternatively, a 2005 paper by Robert Jaffe of MIT states that \"Casimir effects\ncan be formulated and Casimir forces can be computed without reference to zero-point energies. They are relativistic, quantum forces between charges and currents. The Casimir force (per unit\narea) between parallel plates vanishes as alpha, the fine structure constant, goes to zero, and the standard result, which appears to be independent of alpha, corresponds to the alpha approaching infinity limit,\" and that \"The Casimir force is simply the (relativistic, retarded) van der Waals force between the metal plates.\" Casimir and Polder's original paper used this method to derive the Casimir-Polder force. In 1978, Schwinger, DeRadd, and Milton published a similar derivation for the Casimir Effect between two parallel plates. In fact, the description in terms of van der Waals forces is the only correct description from the fundamental microscopic perspective, while other descriptions of Casimir force are merely effective macroscopic descriptions.\n\nCasimir's observation was that the second-quantized quantum electromagnetic field, in the presence of bulk bodies such as metals or dielectrics, must obey the same boundary conditions that the classical electromagnetic field must obey. In particular, this affects the calculation of the vacuum energy in the presence of a conductor or dielectric.\n\nConsider, for example, the calculation of the vacuum expectation value of the electromagnetic field inside a metal cavity, such as, for example, a radar cavity or a microwave waveguide. In this case, the correct way to find the zero-point energy of the field is to sum the energies of the standing waves of the cavity. To each and every possible standing wave corresponds an energy; say the energy of the \"n\"th standing wave is formula_3. The vacuum expectation value of the energy of the electromagnetic field in the cavity is then\n\nwith the sum running over all possible values of \"n\" enumerating the standing waves. The factor of 1/2 is present because the zero-point energy of the n'th mode is formula_5, where formula_3 is the energy increment for the n'th mode. (It is the same 1/2 as appears in the equation formula_7.) Written in this way, this sum is clearly divergent; however, it can be used to create finite expressions.\n\nIn particular, one may ask how the zero-point energy depends on the shape \"s\" of the cavity. Each energy level formula_3 depends on the shape, and so one should write formula_9 for the energy level, and formula_10 for the vacuum expectation value. At this point comes an important observation: the force at point \"p\" on the wall of the cavity is equal to the change in the vacuum energy if the shape \"s\" of the wall is perturbed a little bit, say by formula_11, at point \"p\". That is, one has\n\nThis value is finite in many practical calculations.\n\nAttraction between the plates can be easily understood by focusing on the one-dimensional situation. Suppose that a moveable conductive plate is positioned at a short distance \"a\" from one of two widely separated plates (distance \"L\" apart). With \"a\" « \"L\", the states within the slot of width \"a\" are highly constrained so that the energy \"E\" of any one mode is widely separated from that of the next. This is not the case in the large region \"L\", where there is a large number (numbering about \"L\" / \"a\") of states with energy evenly spaced between \"E\" and the next mode in the narrow slot – in other words, all slightly larger than \"E\". Now on shortening \"a\" by d\"a\" (< 0), the mode in the narrow slot shrinks in wavelength and therefore increases in energy proportional to −d\"a\"/\"a\", whereas all the \"L\" /\"a\" states that lie in the large region lengthen and correspondingly decrease their energy by an amount proportional to d\"a\"/\"L\" (note the denominator). The two effects nearly cancel, but the net change is slightly negative, because the energy of all the \"L\"/\"a\" modes in the large region are slightly larger than the single mode in the slot. Thus the force is attractive: it tends to make \"a\" slightly smaller, the plates attracting each other across the thin slot.\n\n\nIn the original calculation done by Casimir, he considered the space between a pair of conducting metal plates at distance formula_13 apart. In this case, the standing waves are particularly easy to calculate, because the transverse component of the electric field and the normal component of the magnetic field must vanish on the surface of a conductor. Assuming the plates lie parallel to the \"xy\"-plane, the standing waves are\n\nwhere formula_15 stands for the electric component of the electromagnetic field, and, for brevity, the polarization and the magnetic components are ignored here. Here, formula_16 and formula_17 are the wave numbers in directions parallel to the plates, and\n\nis the wave-number perpendicular to the plates. Here, \"n\" is an integer, resulting from the requirement that ψ vanish on the metal plates. The frequency of this wave is\n\nwhere \"c\" is the speed of light. The vacuum energy is then the sum over all possible excitation modes. Since the area of the plates is large, we may sum by integrating over two of the dimensions in \"k\"-space. The assumption of periodic boundary conditions yields,\n\nwhere \"A\" is the area of the metal plates, and a factor of 2 is introduced for the two possible polarizations of the wave. This expression is clearly infinite, and to proceed with the calculation, it is convenient to introduce a regulator (discussed in greater detail below). The regulator will serve to make the expression finite, and in the end will be removed. The zeta-regulated version of the energy per unit-area of the plate is\n\nIn the end, the limit formula_22 is to be taken. Here \"s\" is just a complex number, not to be confused with the shape discussed previously. This integral/sum is finite for \"s\" real and larger than 3. The sum has a pole at \"s\"=3, but may be analytically continued to \"s\"=0, where the expression is finite. The above expression simplifies to:\n\nwhere polar coordinates formula_24 were introduced to turn the double integral into a single integral. The formula_25 in front is the Jacobian, and the formula_26 comes from the angular integration. The integral converges if Re[\"s\"] > 3, resulting in\n\nThe sum diverges at \"s\" in the neighborhood of zero, but if the damping of large-frequency excitations corresponding to analytic continuation of the Riemann zeta function to \"s\"=0 is assumed to make sense physically in some way, then one has\n\nBut\n\nand so one obtains\n\nThe analytic continuation has evidently lost an additive positive infinity, somehow exactly accounting for the zero-point energy (not included above) outside the slot between the plates, but which changes upon plate movement within a closed system. The Casimir force per unit area formula_31 for idealized, perfectly conducting plates with vacuum between them is\n\nwhere\n\nThe force is negative, indicating that the force is attractive: by moving the two plates closer together, the energy is lowered. The presence of formula_33 shows that the Casimir force per unit area formula_31 is very small, and that furthermore, the force is inherently of quantum-mechanical origin.\n\nNOTE: In Casimir's original derivation , a moveable conductive plate is positioned at a short distance \"a\" from one of two widely separated plates (distance \"L\" apart). The 0-point energy on \"both\" sides of the plate is considered. Instead of the above \"ad hoc\" analytic continuation assumption, non-convergent sums and integrals are computed using Euler–Maclaurin summation with a regularizing function (e.g., exponential regularization) not so anomalous as formula_38 in the above.\n\nCasimir's analysis of idealized metal plates was generalized to arbitrary dielectric and realistic metal plates by Lifshitz and his students. Using this approach, complications of the bounding surfaces, such as the modifications to the Casimir force due to finite conductivity, can be calculated numerically using the tabulated complex dielectric functions of the bounding materials. Lifshitz's theory for two metal plates reduces to Casimir's idealized 1/\"a\" force law for large separations \"a\" much greater than the skin depth of the metal, and conversely reduces to the 1/\"a\" force law of the London dispersion force (with a coefficient called a Hamaker constant) for small \"a\", with a more complicated dependence on \"a\" for intermediate separations determined by the dispersion of the materials.\n\nLifshitz' result was subsequently generalized to arbitrary multilayer planar geometries as well as to anisotropic and magnetic materials, but for several decades the calculation of Casimir forces for non-planar geometries remained limited to a few idealized cases admitting analytical solutions. For example, the force in the experimental sphere–plate geometry was computed with an approximation (due to Derjaguin) that the sphere radius \"R\" is much larger than the separation \"a\", in which case the nearby surfaces are nearly parallel and the parallel-plate result can be adapted to obtain an approximate \"R\"/\"a\" force (neglecting both skin-depth and higher-order curvature effects). However, in the 2000s a number of authors developed and demonstrated a variety of numerical techniques, in many cases adapted from classical computational electromagnetics, that are capable of accurately calculating Casimir forces for arbitrary geometries and materials, from simple finite-size effects of finite plates to more complicated phenomena arising for patterned surfaces or objects of various shapes.\n\nOne of the first experimental tests was conducted by Marcus Sparnaay at Philips in Eindhoven (Netherlands), in 1958, in a delicate and difficult experiment with parallel plates, obtaining results not in contradiction with the Casimir theory, but with large experimental errors. Some of the experimental details as well as some background information on how Casimir, Polder and Sparnaay arrived at this point are highlighted in a 2007 interview with Marcus Sparnaay.\n\nThe Casimir effect was measured more accurately in 1997 by Steve K. Lamoreaux of Los Alamos National Laboratory, and by Umar Mohideen and Anushree Roy of the University of California, Riverside. In practice, rather than using two parallel plates, which would require phenomenally accurate alignment to ensure they were parallel, the experiments use one plate that is flat and another plate that is a part of a sphere with a large radius.\n\nIn 2001, a group (Giacomo Bressi, Gianni Carugno, Roberto Onofrio and Giuseppe Ruoso) at the University of Padua (Italy) finally succeeded in measuring the Casimir force between parallel plates using microresonators.\n\nIn order to be able to perform calculations in the general case, it is convenient to introduce a regulator in the summations. This is an artificial device, used to make the sums finite so that they can be more easily manipulated, followed by the taking of a limit so as to remove the regulator.\n\nThe heat kernel or exponentially regulated sum is\n\nwhere the limit formula_40 is taken in the end. The divergence of the sum is typically manifested as\n\nfor three-dimensional cavities. The infinite part of the sum is associated with the bulk constant \"C\" which \"does not\" depend on the shape of the cavity. The interesting part of the sum is the finite part, which is shape-dependent. The Gaussian regulator\n\nis better suited to numerical calculations because of its superior convergence properties, but is more difficult to use in theoretical calculations. Other, suitably smooth, regulators may be used as well. The zeta function regulator\n\nis completely unsuited for numerical calculations, but is quite useful in theoretical calculations. In particular, divergences show up as poles in the complex \"s\" plane, with the bulk divergence at \"s\"=4. This sum may be analytically continued past this pole, to obtain a finite part at \"s\"=0.\n\nNot every cavity configuration necessarily leads to a finite part (the lack of a pole at \"s\"=0) or shape-independent infinite parts. In this case, it should be understood that additional physics has to be taken into account. In particular, at extremely large frequencies (above the plasma frequency), metals become transparent to photons (such as X-rays), and dielectrics show a frequency-dependent cutoff as well. This frequency dependence acts as a natural regulator. There are a variety of bulk effects in solid state physics, mathematically very similar to the Casimir effect, where the cutoff frequency comes into explicit play to keep expressions finite. (These are discussed in greater detail in \"Landau and Lifshitz\", \"Theory of Continuous Media\".)\n\nThe Casimir effect can also be computed using the mathematical mechanisms of functional integrals of quantum field theory, although such calculations are considerably more abstract, and thus difficult to comprehend. In addition, they can be carried out only for the simplest of geometries. However, the formalism of quantum field theory makes it clear that the vacuum expectation value summations are in a certain sense summations over so-called \"virtual particles\".\n\nMore interesting is the understanding that the sums over the energies of standing waves should be formally understood as sums over the eigenvalues of a Hamiltonian. This allows atomic and molecular effects, such as the van der Waals force, to be understood as a variation on the theme of the Casimir effect. Thus one considers the Hamiltonian of a system as a function of the arrangement of objects, such as atoms, in configuration space. The change in the zero-point energy as a function of changes of the configuration can be understood to result in forces acting between the objects.\n\nIn the chiral bag model of the nucleon, the Casimir energy plays an important role in showing the mass of the nucleon is independent of the bag radius. In addition, the spectral asymmetry is interpreted as a non-zero vacuum expectation value of the baryon number, cancelling the topological winding number of the pion field surrounding the nucleon.\n\nThe dynamical Casimir effect is the production of particles and energy from an accelerated \"moving mirror\". This reaction was predicted by certain numerical solutions to quantum mechanics equations made in the 1970s. In May 2011 an announcement was made by researchers at the Chalmers University of Technology, in Gothenburg, Sweden, of the detection of the dynamical Casimir effect. In their experiment, microwave photons were generated out of the vacuum in a superconducting microwave resonator. These researchers used a modified SQUID to change the effective length of the resonator in time, mimicking a mirror moving at the required relativistic velocity. If confirmed this would be the first experimental verification of the dynamical Casimir effect.\nA similar analysis can be used to explain Hawking radiation that causes the slow \"evaporation\" of black holes (although this is generally visualized as the escape of one particle from a virtual particle-antiparticle pair, the other particle having been captured by the black hole).\n\nConstructed within the framework of quantum field theory in curved spacetime, the dynamical Casimir effect has been used to better understand acceleration radiation such as the Unruh effect.\n\nThere are few instances wherein the Casimir effect can give rise to repulsive forces between uncharged objects. Evgeny Lifshitz showed (theoretically) that in certain circumstances (most commonly involving liquids), repulsive forces can arise. This has sparked interest in applications of the Casimir effect toward the development of levitating devices. An experimental demonstration of the Casimir-based repulsion predicted by Lifshitz was carried out by Munday et al. Other scientists have also suggested the use of gain media to achieve a similar levitation effect, though this is controversial because these materials seem to violate fundamental causality constraints and the requirement of thermodynamic equilibrium (Kramers–Kronig relations). Casimir and Casimir-Polder repulsion can in fact occur for sufficiently anisotropic electrical bodies; for a review of the issues involved with repulsion see Milton et al.\n\nIt has been suggested that the Casimir forces have application in nanotechnology, in particular silicon integrated circuit technology based micro- and nanoelectromechanical systems, silicon array propulsion for space drives, and so-called Casimir oscillators.\n\nThe Casimir effect shows that quantum field theory allows the energy density in certain regions of space to be negative relative to the ordinary vacuum energy, and it has been shown theoretically that quantum field theory allows states where the energy can be \"arbitrarily\" negative at a given point. Many physicists such as Stephen Hawking, Kip Thorne, and others therefore argue that such effects might make it possible to stabilize a traversable wormhole.\n\nOn 4 June 2013 it was reported that a conglomerate of scientists from Hong Kong University of Science and Technology, University of Florida, Harvard University, Massachusetts Institute of Technology, and Oak Ridge National Laboratory have for the first time demonstrated a compact integrated silicon chip that can measure the Casimir force.\n\n\n\n\n\n"}
{"id": "7783", "url": "https://en.wikipedia.org/wiki?curid=7783", "title": "Coriolis force", "text": "Coriolis force\n\nIn physics, the Coriolis force is an inertial force that seems to act on objects that are in motion within a frame of reference that rotates with respect to an inertial frame. In a reference frame with clockwise rotation, the force acts to the left of the motion of the object. In one with anticlockwise (or counterclockwise) rotation, the force acts to the right. Deflection of an object due to the Coriolis force is called the Coriolis effect. Though recognized previously by others, the mathematical expression for the Coriolis force appeared in an 1835 paper by French scientist Gaspard-Gustave de Coriolis, in connection with the theory of water wheels. Early in the 20th century, the term \"Coriolis force\" began to be used in connection with meteorology.\n\nNewton's laws of motion describe the motion of an object in an inertial (non-accelerating) frame of reference. When Newton's laws are transformed to a rotating frame of reference, the Coriolis force and centrifugal force appear. Both forces are proportional to the mass of the object. The Coriolis force is proportional to the rotation rate and the centrifugal force is proportional to the square of the rotation rate. The Coriolis force acts in a direction perpendicular to the rotation axis and to the velocity of the body in the rotating frame and is proportional to the object's speed in the rotating frame (more precisely, to the component of its velocity that is perpendicular to the axis of rotation). The centrifugal force acts outwards in the radial direction and is proportional to the distance of the body from the axis of the rotating frame. These additional forces are termed inertial forces, fictitious forces or \"pseudo forces\". They allow the application of Newton's laws to a rotating system. They are correction factors that do not exist in a non-accelerating or inertial reference frame.\n\nIn popular (non-technical) usage of the term \"Coriolis effect\", the rotating reference frame implied is almost always the Earth. Because the Earth spins, Earth-bound observers need to account for the Coriolis force to correctly analyze the motion of objects. The Earth completes one rotation per day, so for motions of everyday objects the Coriolis force is usually quite small compared to other forces; its effects generally become noticeable only for motions occurring over large distances and long periods of time, such as large-scale movement of air in the atmosphere or water in the ocean. Such motions are constrained by the surface of the Earth, so only the horizontal component of the Coriolis force is generally important. This force causes moving objects on the surface of the Earth to be deflected to the right (with respect to the direction of travel) in the Northern Hemisphere and to the left in the Southern Hemisphere. The horizontal deflection effect is greater near the poles, since the effective rotation rate about a local vertical axis is largest there, and decreases to zero at the equator. Rather than flowing directly from areas of high pressure to low pressure, as they would in a non-rotating system, winds and currents tend to flow to the right of this direction north of the equator and to the left of this direction south of it. This effect is responsible for the rotation of large cyclones (see Coriolis effects in meteorology).\n\nFor an intuitive explanation of the origin of the Coriolis force, consider an object, constrained to follow the Earth's surface and moving northward in the northern hemisphere. Viewed from outer space, the object does not appear to go due north, but has an eastward motion (it rotates around toward the right along with the surface of the Earth). The further north you go, the smaller the \"horizontal diameter\" of the Earth (the minimum distance from the surface point to the axis of rotation, which is in a plane orthogonal to the axis), and so the slower the eastward motion of its surface. As the object moves north, to higher latitudes, it has a tendency to maintain the eastward speed it started with (rather than slowing down to match the reduced eastward speed of local objects on the Earth's surface), so it veers east (i.e. to the right of its initial motion). Though not obvious from this example, which considers northward motion, the horizontal deflection occurs equally for objects moving east or west (or any other direction).\n\nItalian scientist Giovanni Battista Riccioli and his assistant Francesco Maria Grimaldi described the effect in connection with artillery in the 1651 \"Almagestum Novum\", writing that rotation of the Earth should cause a cannonball fired to the north to deflect to the east. In 1674 Claude François Milliet Dechales described in his \"Cursus seu Mundus Mathematicus\" how the rotation of the Earth should cause a deflection in the trajectories of both falling bodies and projectiles aimed toward one of the planet's poles. Riccioli, Grimaldi, and Dechales all described the effect as part of an argument against the heliocentric system of Copernicus. In other words, they argued that the Earth's rotation should create the effect, and so failure to detect the effect was evidence for an immobile Earth. The Coriolis acceleration equation was derived by Euler in 1749 and the effect was described in the tidal equations of Pierre-Simon Laplace in 1778.\n\nGaspard-Gustave Coriolis published a paper in 1835 on the energy yield of machines with rotating parts, such as waterwheels. That paper considered the supplementary forces that are detected in a rotating frame of reference. Coriolis divided these supplementary forces into two categories. The second category contained a force that arises from the cross product of the angular velocity of a coordinate system and the projection of a particle's velocity into a plane perpendicular to the system's axis of rotation. Coriolis referred to this force as the \"compound centrifugal force\" due to its analogies with the centrifugal force already considered in category one. The effect was known in the early 20th century as the \"acceleration of Coriolis\", and by 1920 as \"Coriolis force\".\n\nIn 1856, William Ferrel proposed the existence of a circulation cell in the mid-latitudes with air being deflected by the Coriolis force to create the prevailing westerly winds.\n\nThe understanding of the kinematics of how exactly the rotation of the Earth affects airflow was partial at first. Late in the 19th century, the full extent of the large scale interaction of pressure-gradient force and deflecting force that in the end causes air masses to move along isobars was understood.\n\nIn non-vector terms: at a given rate of rotation of the observer, the magnitude of the Coriolis acceleration of the object is proportional to the velocity of the object and also to the sine of the angle between the direction of movement of the object and the axis of rotation.\n\nThe vector formula for the magnitude and direction of the Coriolis acceleration is derived through vector analysis and is\n\nwhere (here and below) formula_2 is the acceleration of the particle in the rotating system, formula_3 is the velocity of the particle with respect to the rotating system, and Ω is the angular velocity vector having magnitude equal to the rotation rate ω, with direction along the axis of rotation of the rotating reference frame, and the formula_4\nsymbol represents the cross product operator.\n\nThe equation may be multiplied by the mass of the relevant object to produce the Coriolis force:\n\nSee \"fictitious force\" for a derivation.\n\nThe \"Coriolis effect\" is the behavior added by the \"Coriolis acceleration\". The formula implies that the Coriolis acceleration is perpendicular both to the direction of the velocity of the moving mass and to the frame's rotation axis. So in particular:\n\nThe Coriolis force exists only when one uses a rotating reference frame. In the rotating frame it behaves exactly like a real force (that is to say, it causes acceleration and has real effects). However, the Coriolis force is a consequence of inertia, and is not attributable to an identifiable originating body, as is the case for electromagnetic or nuclear forces, for example. From an analytical viewpoint, to use Newton's second law in a rotating system, the Coriolis force is mathematically necessary, but it disappears in a non-accelerating, inertial frame of reference. For example, consider two children on opposite sides of a spinning roundabout (Merry-go-round ), who are throwing a ball to each other. From the children's point of view, this ball's path is curved sideways by the Coriolis force. Suppose the roundabout spins anticlockwise when viewed from above. From the thrower's perspective, the deflection is to the right. From the non-thrower's perspective, deflection is to the left. \"For a mathematical formulation see Mathematical derivation of fictitious forces.\" In meteorology, a rotating frame (the Earth) with its Coriolis force provides a more natural framework for explanation of air movements than a non-rotating, inertial frame without Coriolis forces. In long-range gunnery, sight corrections for the Earth's rotation are based on the Coriolis force. These examples are described in more detail below.\n\nThe acceleration entering the Coriolis force arises from two sources of change in velocity that result from rotation: the first is the change of the velocity of an object in time. The same velocity (in an inertial frame of reference where the normal laws of physics apply) is seen as different velocities at different times in a rotating frame of reference. The apparent acceleration is proportional to the angular velocity of the reference frame (the rate at which the coordinate axes change direction), and to the component of velocity of the object in a plane perpendicular to the axis of rotation. This gives a term formula_6. The minus sign arises from the traditional definition of the cross product (right-hand rule), and from the sign convention for angular velocity vectors.\n\nThe second is the change of velocity in space. Different positions in a rotating frame of reference have different velocities (as seen from an inertial frame of reference). For an object to move in a straight line, it must accelerate so that its velocity changes from point to point by the same amount as the velocities of the frame of reference. The force is proportional to the angular velocity (which determines the relative speed of two different points in the rotating frame of reference), and to the component of the velocity of the object in a plane perpendicular to the axis of rotation (which determines how quickly it moves between those points). This also gives a term formula_6.\n\nThe time, space and velocity scales are important in determining the importance of the Coriolis force. Whether rotation is important in a system can be determined by its Rossby number, which is the ratio of the velocity, \"U\", of a system to the product of the Coriolis parameter,formula_8, and the length scale, \"L\", of the motion:\nThe Rossby number is the ratio of inertial to Coriolis forces. A small Rossby number indicates a system is strongly affected by Coriolis forces, and a large Rossby number indicates a system in which inertial forces dominate. For example, in tornadoes, the Rossby number is large, in low-pressure systems it is low, and in oceanic systems it is around 1. As a result, in tornadoes the Coriolis force is negligible, and balance is between pressure and centrifugal forces. In low-pressure systems, centrifugal force is negligible and balance is between Coriolis and pressure forces. In the oceans all three forces are comparable.\n\nAn atmospheric system moving at \"U\" =  occupying a spatial distance of \"L\" = , has a Rossby number of approximately 0.1.\n\nA baseball pitcher may throw the ball at U =  for a distance of L = . The Rossby number in this case would be 32,000.\n\nBaseball players don't care about which hemisphere they're playing in. However, an unguided missile obeys exactly the same physics as a baseball, but can travel far enough and be in the air long enough to experience the effect of Coriolis force. Long-range shells in the Northern Hemisphere landed close to, but to the right of, where they were aimed until this was noted. (Those fired in the Southern Hemisphere landed to the left.) In fact, it was this effect that first got the attention of Coriolis himself.\n\nThe animation at the top of this article is a classic illustration of Coriolis force. Another visualization of the Coriolis and centrifugal forces is this animation clip.\n\nGiven the radius \"R\" of the turntable in that animation, the rate of angular rotation ω, and the speed of the cannonball (assumed constant) \"v\", the correct angle θ to aim so as to hit the target at the edge of the turntable can be calculated.\n\nThe inertial frame of reference provides one way to handle the question: calculate the time to interception, which is \"t\" = \"R\" / \"v\" . Then, the turntable revolves an angle ω \"t\" in this time. If the cannon is pointed an angle θ = ω \"t\" = ω \"R\" / \"v\", then the cannonball arrives at the periphery at position number 3 at the same time as the target.\n\nNo discussion of Coriolis force can arrive at this solution as simply, so the reason to treat this problem is to demonstrate Coriolis formalism in an easily visualized situation.\n\nThe trajectory in the inertial frame (denoted \"A\") is a straight line radial path at angle θ. The position of the cannonball in (\"x\", \"y\") coordinates at time \"t\" is:\nIn the turntable frame (denoted \"B\"), the \"x\"- \"y\" axes rotate at angular rate ω, so the trajectory becomes:\nand three examples of this result are plotted in the figure.\n\nTo determine the components of acceleration, a general expression is used from the article fictitious force:\n\nin which the term in Ω × v is the Coriolis acceleration and the term in Ω × (Ω × r) is the centrifugal acceleration. The results are (let α = θ − ω\"t\"):\n\nProducing a centrifugal acceleration:\n\nAlso:\n\nproducing a Coriolis acceleration:\n\nThese accelerations are shown in the diagrams for a particular example.\n\nIt is seen that the Coriolis acceleration not only cancels the centrifugal acceleration, but together they provide a net \"centripetal\", radially inward component of acceleration (that is, directed toward the center of rotation):\n\nand an additional component of acceleration perpendicular to r(\"t\"):\n\nThe \"centripetal\" component of acceleration resembles that for circular motion at radius \"r\", while the perpendicular component is velocity dependent, increasing with the radial velocity \"v\" and directed to the right of the velocity. The situation could be described as a circular motion combined with an \"apparent Coriolis acceleration\" of 2ω\"v\". However, this is a rough labelling: a careful designation of the true centripetal force refers to a local reference frame that employs the directions normal and tangential to the path, not coordinates referred to the axis of rotation.\n\nThese results also can be obtained directly by two time differentiations of r(\"t\"). Agreement of the two approaches demonstrates that one could start from the general expression for fictitious acceleration above and derive the trajectories shown here. However, working from the acceleration to the trajectory is more complicated than the reverse procedure used here, which is made possible in this example by knowing the answer in advance.\n\nAs a result of this analysis an important point appears: \"all\" the fictitious accelerations must be included to obtain the correct trajectory. In particular, besides the Coriolis acceleration, the centrifugal force plays an essential role. It is easy to get the impression from verbal discussions of the cannonball problem, which focus on displaying the Coriolis effect particularly, that the Coriolis force is the only factor that must be considered, but that is not so. A turntable for which the Coriolis force \"is\" the only factor is the parabolic turntable. A somewhat more complex situation is the idealized example of flight routes over long distances, where the centrifugal force of the path and aeronautical lift are countered by gravitational attraction.\nThe figure illustrates a ball tossed from 12:00 o'clock toward the center of a counter-clockwise rotating carousel. On the left, the ball is seen by a stationary observer above the carousel, and the ball travels in a straight line to the center, while the ball-thrower rotates counter-clockwise with the carousel. On the right the ball is seen by an observer rotating with the carousel, so the ball-thrower appears to stay at 12:00 o'clock. The figure shows how the trajectory of the ball as seen by the rotating observer can be constructed.\n\nOn the left, two arrows locate the ball relative to the ball-thrower. One of these arrows is from the thrower to the center of the carousel (providing the ball-thrower's line of sight), and the other points from the center of the carousel to the ball. (This arrow gets shorter as the ball approaches the center.) A shifted version of the two arrows is shown dotted.\n\nOn the right is shown this same dotted pair of arrows, but now the pair are rigidly rotated so the arrow corresponding to the line of sight of the ball-thrower toward the center of the carousel is aligned with 12:00 o'clock. The other arrow of the pair locates the ball relative to the center of the carousel, providing the position of the ball as seen by the rotating observer. By following this procedure for several positions, the trajectory in the rotating frame of reference is established as shown by the curved path in the right-hand panel.\n\nThe ball travels in the air, and there is no net force upon it. To the stationary observer the ball follows a straight-line path, so there is no problem squaring this trajectory with zero net force. However, the rotating observer sees a \"curved\" path. Kinematics insists that a force (pushing to the \"right\" of the instantaneous direction of travel for a \"counter-clockwise\" rotation) must be present to cause this curvature, so the rotating observer is forced to invoke a combination of centrifugal and Coriolis forces to provide the net force required to cause the curved trajectory.\nThe figure describes a more complex situation where the tossed ball on a turntable bounces off the edge of the carousel and then returns to the tosser, who catches the ball. The effect of Coriolis force on its trajectory is shown again as seen by two observers: an observer (referred to as the \"camera\") that rotates with the carousel, and an inertial observer. The figure shows a bird's-eye view based upon the same ball speed on forward and return paths. Within each circle, plotted dots show the same time points. In the left panel, from the camera's viewpoint at the center of rotation, the tosser (smiley face) and the rail both are at fixed locations, and the ball makes a very considerable arc on its travel toward the rail, and takes a more direct route on the way back. From the ball tosser's viewpoint, the ball seems to return more quickly than it went (because the tosser is rotating toward the ball on the return flight).\n\nOn the carousel, instead of tossing the ball straight at a rail to bounce back, the tosser must throw the ball toward the right of the target and the ball then seems to the camera to bear continuously to the left of its direction of travel to hit the rail (\"left\" because the carousel is turning \"clockwise\"). The ball appears to bear to the left from direction of travel on both inward and return trajectories. The curved path demands this observer to recognize a leftward net force on the ball. (This force is \"fictitious\" because it disappears for a stationary observer, as is discussed shortly.) For some angles of launch, a path has portions where the trajectory is approximately radial, and Coriolis force is primarily responsible for the apparent deflection of the ball (centrifugal force is radial from the center of rotation, and causes little deflection on these segments). When a path curves away from radial, however, centrifugal force contributes significantly to deflection.\n\nThe ball's path through the air is straight when viewed by observers standing on the ground (right panel). In the right panel (stationary observer), the ball tosser (smiley face) is at 12 o'clock and the rail the ball bounces from is at position one (1). From the inertial viewer's standpoint, positions one (1), two (2), three (3) are occupied in sequence. At position 2 the ball strikes the rail, and at position 3 the ball returns to the tosser. Straight-line paths are followed because the ball is in free flight, so this observer requires that no net force is applied.\n\nAn important case where the Coriolis force is observed involves the rotating Earth together with its atmosphere. The atmosphere rotates in sync with the surface of the earth due to frictional forces between the air molecules of the atmosphere (air viscosity) and the earth's surface which causes the air molecules to be carried along and move in step with the earth's surface. Unless otherwise stated, directions of forces and motion apply to the Northern Hemisphere.\n\nAs the Earth turns around its axis, everything attached to it, including the atmosphere, turns with it (imperceptibly to our senses). An object that is moving without being dragged along with the surface rotation or atmosphere such as an object in ballistic flight or an independent air mass within the atmosphere, travels in a straight motion over the turning Earth. From our rotating perspective on the planet, the direction of motion of an object in ballistic flight changes as it moves, bending in the opposite direction to our actual motion. When viewed from a stationary point in space directly above the north pole, any land feature in the Northern Hemisphere turns anticlockwise—and, fixing our gaze on that location, any other location in that hemisphere rotates around it the same way. The traced ground path of a freely moving body in ballistic flight traveling from one point to another therefore bends the opposite way, clockwise, which is conventionally labeled as \"right,\" where it will be if the direction of motion is considered \"ahead,\" and \"down\" is defined naturally.\n\nConsider a location with latitude \"φ\" on a sphere that is rotating around the north-south axis. A local coordinate system is set up with the \"x\" axis horizontally due east, the \"y\" axis horizontally due north and the \"z\" axis vertically upwards. The rotation vector, velocity of movement and Coriolis acceleration expressed in this local coordinate system (listing components in the order east (\"e\"), north (\"n\") and upward (\"u\")) are:\n\nWhen considering atmospheric or oceanic dynamics, the vertical velocity is small, and the vertical component of the Coriolis acceleration is small compared to gravity. For such cases, only the horizontal (east and north) components matter. The restriction of the above to the horizontal plane is (setting \"v\" = 0):\n\nwhere formula_8 is called the Coriolis parameter.\n\nBy setting \"v\" = 0, it can be seen immediately that (for positive φ and ω) a movement due east results in an acceleration due south. Similarly, setting \"v\" = 0, it is seen that a movement due north results in an acceleration due east. In general, observed horizontally, looking along the direction of the movement causing the acceleration, the acceleration always is turned 90° to the right and of the same size regardless of the horizontal orientation.\n\nAs a different case, consider equatorial motion setting φ = 0°. In this case, Ω is parallel to the north or \"n\"-axis, and:\n\nAccordingly, an eastward motion (that is, in the same direction as the rotation of the sphere) provides an upward acceleration known as the Eötvös effect, and an upward motion produces an acceleration due west.\n\nPerhaps the most important impact of the Coriolis effect is in the large-scale dynamics of the oceans and the atmosphere. In meteorology and oceanography, it is convenient to postulate a rotating frame of reference wherein the Earth is stationary. In accommodation of that provisional postulation, the centrifugal and Coriolis forces are introduced. Their relative importance is determined by the applicable Rossby numbers. Tornadoes have high Rossby numbers, so, while tornado-associated centrifugal forces are quite substantial, Coriolis forces associated with tornadoes are for practical purposes negligible.\n\nBecause surface ocean currents are driven by the movement of wind over the water's surface, the Coriolis force also affects the movement of ocean currents and cyclones as well. Many of the ocean's largest currents circulate around warm, high-pressure areas called gyres. Though the circulation is not as significant as that in the air, the deflection caused by the Coriolis effect is what creates the spiraling pattern in these gyres. The spiraling wind pattern helps the hurricane form. The stronger the force from the Coriolis effect, the faster the wind spins and picks up additional energy, increasing the strength of the hurricane.\n\nAir within high-pressure systems rotates in a direction such that the Coriolis force is directed radially inwards, and nearly balanced by the outwardly radial pressure gradient. As a result, air travels clockwise around high pressure in the Northern Hemisphere and anticlockwise in the Southern Hemisphere. Air around low-pressure rotates in the opposite direction, so that the Coriolis force is directed radially outward and nearly balances an inwardly radial pressure gradient.\n\nIf a low-pressure area forms in the atmosphere, air tends to flow in towards it, but is deflected perpendicular to its velocity by the Coriolis force. A system of equilibrium can then establish itself creating circular movement, or a cyclonic flow. Because the Rossby number is low, the force balance is largely between the pressure-gradient force acting towards the low-pressure area and the Coriolis force acting away from the center of the low pressure.\n\nInstead of flowing down the gradient, large scale motions in the atmosphere and ocean tend to occur perpendicular to the pressure gradient. This is known as geostrophic flow. On a non-rotating planet, fluid would flow along the straightest possible line, quickly eliminating pressure gradients. Note that the geostrophic balance is thus very different from the case of \"inertial motions\" (see below), which explains why mid-latitude cyclones are larger by an order of magnitude than inertial circle flow would be.\n\nThis pattern of deflection, and the direction of movement, is called Buys-Ballot's law. In the atmosphere, the pattern of flow is called a cyclone. In the Northern Hemisphere the direction of movement around a low-pressure area is anticlockwise. In the Southern Hemisphere, the direction of movement is clockwise because the rotational dynamics is a mirror image there. At high altitudes, outward-spreading air rotates in the opposite direction. Cyclones rarely form along the equator due to the weak Coriolis effect present in this region.\n\nAn air or water mass moving with speed formula_28 subject only to the Coriolis force travels in a circular trajectory called an 'inertial circle'. Since the force is directed at right angles to the motion of the particle, it moves with a constant speed around a circle whose radius formula_29 is given by:\n\nwhere formula_31 is the Coriolis parameter formula_32, introduced above (where formula_33 is the latitude). The time taken for the mass to complete a full circle is therefore formula_34. The Coriolis parameter typically has a mid-latitude value of about 10 s; hence for a typical atmospheric speed of the radius is , with a period of about 17 hours. For an ocean current with a typical speed of , the radius of an inertial circle is . These inertial circles are clockwise in the Northern Hemisphere (where trajectories are bent to the right) and anticlockwise in the Southern Hemisphere.\n\nIf the rotating system is a parabolic turntable, then formula_31 is constant and the trajectories are exact circles. On a rotating planet, formula_31 varies with latitude and the paths of particles do not form exact circles. Since the parameter formula_31 varies as the sine of the latitude, the radius of the oscillations associated with a given speed are smallest at the poles (latitude = ±90°), and increase toward the equator.\n\nThe Coriolis effect strongly affects the large-scale oceanic and atmospheric circulation, leading to the formation of robust features like jet streams and western boundary currents. Such features are in geostrophic balance, meaning that the Coriolis and \"pressure gradient\" forces balance each other. Coriolis acceleration is also responsible for the propagation of many types of waves in the ocean and atmosphere, including Rossby waves and Kelvin waves. It is also instrumental in the so-called Ekman dynamics in the ocean, and in the establishment of the large-scale ocean flow pattern called the Sverdrup balance.\n\nThe practical impact of the \"Coriolis effect\" is mostly caused by the horizontal acceleration component produced by horizontal motion.\n\nThere are other components of the Coriolis effect. Westward-travelling objects are deflected downwards (feel heavier), while Eastward-travelling objects are deflected upwards (feel lighter). This is known as the Eötvös effect. This aspect of the Coriolis effect is greatest near the equator. The force produced by this effect is similar to the horizontal component, but the much larger vertical forces due to gravity and pressure mean that it is generally unimportant dynamically.\n\nIn addition, objects travelling upwards (\"i.e.\", out) or downwards (\"i.e.\", in) are deflected to the west or east respectively. This effect is also the greatest near the equator. Since vertical movement is usually of limited extent and duration, the size of the effect is smaller and requires precise instruments to detect. However, in the case of large changes of momentum, such as a spacecraft being launched into orbit, the effect becomes significant. The fastest and most fuel-efficient path to orbit is a launch from the equator that curves to a directly eastward heading.\n\nImagine a train that travels through a frictionless railway line along the equator. Assume that, when in motion, it moves at the necessary speed to complete a trip around the world in one day (465 m/s). The Coriolis effect can be considered in three cases: when the train travels west, when it is at rest, and when it travels east. In each case, the Coriolis effect can be calculated from the rotating frame of reference on Earth first, and then checked against a fixed inertial frame. The image below illustrates the three cases as viewed by an observer at rest in a (near) inertial frame from a fixed point above the North Pole along the Earth's axis of rotation; the train are a few red pixels, fixed at the left side in the leftmost picture, moving in the others formula_38\n\nThis also explains why high speed projectiles that travel west are deflected down, and those that travel east are deflected up. This vertical component of the Coriolis effect is called the Eötvös effect.\n\nThe above example can be used to explain why the Eötvös effect starts diminishing when an object is travelling westward as its tangential speed increases above Earth's rotation (465 m/s). If the westward train in the above example increases speed, part of the force of gravity that pushes against the track accounts for the centripetal force needed to keep it in circular motion on the inertial frame. Once the train doubles its westward speed at 930 m/s that centripetal force becomes equal to the force the train experiences when it stops. From the inertial frame, in both cases it rotates at the same speed but in the opposite directions. Thus, the force is the same cancelling completely the Eötvös effect. Any object that moves westward at a speed above 930 m/s experiences an upward force instead. In the figure, the Eötvös effect is illustrated for a 10 kilogram object on the train at different speeds. The parabolic shape is because the centripetal force is proportional to the square of the tangential speed. On the inertial frame, the bottom of the parabola is centered at the origin. The offset is because this argument uses the Earth's rotating frame of reference. The graph shows that the Eötvös effect is not symmetrical, and that the resulting downward force experienced by an object that travels west at high velocity is less than the resulting upward force when it travels east at the same speed.\n\nContrary to popular misconception, water rotation in home bathrooms under \"normal\" circumstances is not related to the Coriolis effect or to the rotation of the Earth, and no consistent difference in rotation direction between toilet drainage in the Northern and Southern Hemispheres can be observed. The formation of a vortex over the plug hole may be explained by the conservation of angular momentum: The radius of rotation decreases as water approaches the plug hole, so the rate of rotation increases, for the same reason that an ice skater's rate of spin increases as they pull their arms in. Any rotation around the plug hole that is initially present accelerates as water moves inward.\n\nThe Coriolis force still affects the direction of the flow of water, but only minutely. Only if the water is so still that the effective rotation rate of the Earth is faster than that of the water relative to its container, and if externally applied torques (such as might be caused by flow over an uneven bottom surface) are small enough, the Coriolis effect may indeed determine the direction of the vortex. Without such careful preparation, the Coriolis effect is likely to be much smaller than various other influences on drain direction such as any residual rotation of the water and the geometry of the container. Despite this, the idea that toilets and bathtubs drain differently in the Northern and Southern Hemispheres has been popularized by several television programs and films, including \"Escape Plan\", \"Wedding Crashers\", \"The Simpsons\" episode \"Bart vs. Australia\", \"Pole to Pole\", and \"The X-Files\" episode \"Die Hand Die Verletzt\". Several science broadcasts and publications, including at least one college-level physics textbook, have also stated this.\n\nIn 1908, the Austrian physicist Ottokar Tumlirz described careful and effective experiments that demonstrated the effect of the rotation of the Earth on the outflow of water through a central aperture. The subject was later popularized in a famous 1962 article in the journal \"Nature\", which described an experiment in which all other forces to the system were removed by filling a tank with of water and allowing it to settle for 24 hours (to allow any movement due to filling the tank to die away), in a room where the temperature had stabilized. The drain plug was then very slowly removed, and tiny pieces of floating wood were used to observe rotation. During the first 12 to 15 minutes, no rotation was observed. Then, a vortex appeared and consistently began to rotate in an anticlockwise direction (the experiment was performed in Boston, Massachusetts, in the Northern Hemisphere). This was repeated and the results averaged to make sure the effect was real. The report noted that the vortex rotated, \"about 30,000 times faster than the effective rotation of the Earth in 42° North (the experiment's location)\". This shows that the small initial rotation due to the Earth is amplified by gravitational draining and conservation of angular momentum to become a rapid vortex and may be observed under carefully controlled laboratory conditions.\n\nThe Coriolis force is important in external ballistics for calculating the trajectories of very long-range artillery shells. The most famous historical example was the Paris gun, used by the Germans during World War I to bombard Paris from a range of about . The Coriolis force minutely changes the trajectory of a bullet, affecting accuracy at extremely long distances. It is adjusted for by accurate long-distance shooters, such as snipers. At the latitude of Sacramento a 1000 yard shot would be deflected 3 inches to the right. There is also a vertical component, explained in the Eötvös effect section above, which causes westward shots to hit low, and eastward shots to hit high.\n\nThe effects of the Coriolis force on ballistic trajectories should not be confused with the curvature of the paths of missiles, satellites, and similar objects when the paths are plotted on two-dimensional (flat) maps, such as the Mercator projection. The projections of the three-dimensional curved surface of the Earth to a two-dimensional surface (the map) necessarily results in distorted features. The apparent curvature of the path is a consequence of the sphericity of the Earth and would occur even in a non-rotating frame.\n\nTo demonstrate the Coriolis effect, a parabolic turntable can be used.\nOn a flat turntable, the inertia of a co-rotating object forces it off the edge. However, if the turntable surface has the correct paraboloid (parabolic bowl) shape (see the figure) and rotates at the corresponding rate, the force components shown in the figure make the component of gravity tangential to the bowl surface exactly equal to the centripetal force necessary to keep the object rotating at its velocity and radius of curvature (assuming no friction). (See .) This carefully contoured surface allows the Coriolis force to be displayed in isolation.\n\nDiscs cut from cylinders of dry ice can be used as pucks, moving around almost frictionlessly over the surface of the parabolic turntable, allowing effects of Coriolis on dynamic phenomena to show themselves. To get a view of the motions as seen from the reference frame rotating with the turntable, a video camera is attached to the turntable so as to co-rotate with the turntable, with results as shown in the figure. In the left panel of the figure, which is the viewpoint of a stationary observer, the gravitational force in the inertial frame pulling the object toward the center (bottom ) of the dish is proportional to the distance of the object from the center. A centripetal force of this form causes the elliptical motion. In the right panel, which shows the viewpoint of the rotating frame, the inward gravitational force in the rotating frame (the same force as in the inertial frame) is balanced by the outward centrifugal force (present only in the rotating frame). With these two forces balanced, in the rotating frame the only unbalanced force is Coriolis (also present only in the rotating frame), and the motion is an \"inertial circle\". Analysis and observation of circular motion in the rotating frame is a simplification compared to analysis or observation of elliptical motion in the inertial frame.\n\nBecause this reference frame rotates several times a minute rather than only once a day like the Earth, the Coriolis acceleration produced is many times larger and so easier to observe on small time and spatial scales than is the Coriolis acceleration caused by the rotation of the Earth.\n\nIn a manner of speaking, the Earth is analogous to such a turntable. The rotation has caused the planet to settle on a spheroid shape, such that the normal force, the gravitational force and the centrifugal force exactly balance each other on a \"horizontal\" surface. (See equatorial bulge.)\n\nThe Coriolis effect caused by the rotation of the Earth can be seen indirectly through the motion of a Foucault pendulum.\n\nA practical application of the Coriolis effect is the mass flow meter, an instrument that measures the mass flow rate and density of a fluid flowing through a tube. The operating principle involves inducing a vibration of the tube through which the fluid passes. The vibration, though not completely circular, provides the rotating reference frame that gives rise to the Coriolis effect. While specific methods vary according to the design of the flow meter, sensors monitor and analyze changes in frequency, phase shift, and amplitude of the vibrating flow tubes. The changes observed represent the mass flow rate and density of the fluid.\n\nIn polyatomic molecules, the molecule motion can be described by a rigid body rotation and internal vibration of atoms about their equilibrium position. As a result of the vibrations of the atoms, the atoms are in motion relative to the rotating coordinate system of the molecule. Coriolis effects are therefore present, and make the atoms move in a direction perpendicular to the original oscillations. This leads to a mixing in molecular spectra between the rotational and vibrational levels, from which Coriolis coupling constants can be determined.\n\nWhen an external torque is applied to a spinning gyroscope along an axis that is at right angles to the spin axis, the rim velocity that is associated with the spin becomes radially directed in relation to the external torque axis. This causes a Coriolis force to act on the rim in such a way as to tilt the gyroscope at right angles to the direction that the external torque would have tilted it. This tendency has the effect of keeping spinning bodies stably aligned in space.\n\nFlies (Diptera) and some moths (Lepidoptera) exploit the Coriolis effect in flight with specialized appendages and organs that relay information about the angular velocity of their bodies.\n\nCoriolis forces resulting from linear motion of these appendages are detected within the rotating frame of reference of the insects' bodies. In the case of flies, their specialized appendages are dumbbell shaped organs located just behind their wings called \"halteres\".\n\nThe fly's halteres oscillate in a plane at the same beat frequency as the main wings so that any body rotation results in lateral deviation of the halteres from their plane of motion.\n\nIn moths, their antennae are known to be responsible for the \"sensing\" of Coriolis forces in the similar manner as with the halteres in flies. In both flies and moths, a collection of mechanosensors at the base of the appendage are sensitive to deviations at the beat frequency, correlating to rotation in the pitch and roll planes, and at twice the beat frequency, correlating to rotation in the yaw plane.\n\nIn astronomy, Lagrangian points are five positions in the orbital plane of two large orbiting bodies where a small object affected only by gravity can maintain a stable position relative to the two large bodies. The first three Lagrangian points (L, L, L) lie along the line connecting the two large bodies, while the last two points (L and L) each form an equilateral triangle with the two large bodies. The L and L points, although they correspond to maxima of the effective potential in the coordinate frame that rotates with the two large bodies, are stable due to the Coriolis effect. The stability can result in orbits around just L or L, known as tadpole orbits, where trojans can be found. It can also result in orbits that encircle L, L, and L, known as horseshoe orbits.\n\n"}
{"id": "1045142", "url": "https://en.wikipedia.org/wiki?curid=1045142", "title": "Debris", "text": "Debris\n\nDebris or débris (, ) is rubble, wreckage, ruins, litter and discarded garbage/refuse/trash, scattered remains of something destroyed, discarded, or as in geology, large rock fragments left by a melting glacier etc. Depending on context, \"debris\" can refer to a number of different things. The first apparent use of the French word in English is in a 1701 description of the army of Prince Rupert upon its retreat from a battle with the army of Oliver Cromwell, in England.\n\nIn disaster scenarios, tornados leave behind large pieces of houses and mass destruction overall. This debris also flies around the tornado itself when it is in progress. The tornado's winds capture debris it kicks up in its wind orbit, and spins it inside its vortex. The tornado's wind radius is larger than the funnel itself. tsunamis and hurricanes also bring large amounts of debris, such as Hurricane Katrina in 2005 and Hurricane Sandy in 2012. Earthquakes rock cities to rubble debris.\n\nIn geology, debris usually applies to the remains of geological activity including landslides, volcanic explosions, avalanches, mudflows or Glacial lake outburst floods (Jökulhlaups) and moraine, lahars, and lava eruptions. Geological debris sometimes moves in a stream called a debris flow. When it accumulates at the base of hillsides, it can be called \"talus\" or \"scree\".\n\nIn mining, debris called \"attle\" usually consists of rock fragments which contain little or no ore.\n\n\"Marine debris\" applies to floating garbage such as bottles, cans, styrofoam, cruise ship waste, offshore oil and gas exploration and production facilities pollution, and fishing paraphernalia from professional and recreational boaters. Marine debris is also called litter or flotsam and jetsam. Objects that can constitute marine debris include used automobile tires, detergent bottles, medical wastes, discarded fishing line and nets, soda cans, and bilge waste solids.\n\nIn addition to being unsightly, it can pose a serious threat to marine life, boats, swimmers, divers, and others. For example, each year millions of seabirds, sea turtles, fish, and marine mammals become entangled in marine debris, or ingest plastics which they have mistaken for food. As many as 30,000 northern fur seals per year get caught in abandoned fishing nets and either drown or suffocate. Whales mistake plastic bags for squid, and birds may mistake plastic pellets for fish eggs. At other times, animals accidentally eat the plastic while feeding on natural food.\n\nThe largest concentration of marine debris is the Great Pacific Garbage Patch.\n\nMarine debris most commonly originates from land-based sources. Various international agencies are currently working to reduce marine debris levels around the world.\n\nIn meteorology, debris usually applies to the remains of human habitation and natural flora after storm related destruction. This debris is also commonly referred to as storm debris. Storm debris commonly consists of roofing material, downed tree limbs, downed signs, downed power lines and poles, and wind-blown garbage. Storm debris can become a serious problem immediately after a storm, in that it often blocks access to individuals and communities that may require emergency services. This material frequently exists in such large quantities that disposing of it becomes a serious issue for a community. In addition, storm debris is often hazardous by its very nature, since, for example, downed power lines annually account for storm-related deaths.\n\n\"Space debris\" usually refers to the remains of spacecraft that have either fallen to Earth or are still orbiting Earth. Space debris may also consist of natural components such as chunks of rock and ice. The problem of space debris has grown as various space programs have left legacies of launches, explosions, repairs, and discards in both low Earth orbit and more remote orbits. These orbiting fragments have reached a great enough proportion to constitute a hazard to future space launches of both satellite and manned vehicles. Various government agencies and international organizations are beginning to track space debris and also research possible solutions to the problem. While many of these items, ranging in size from nuts and bolts to entire satellites and spacecraft, may fall to Earth, other items located in more remote orbits may stay aloft for centuries. The velocity of some of these pieces of space junk have been clocked in excess of 17,000 miles per hour (27,000 km/h). A piece of space debris falling to Earth leaves a fiery trail, just like a meteor.\n\nA debris disk is a circumstellar disk of dust and debris in orbit around a star.\n\nIn medicine, debris usually refers to biological matter that has accumulated or lodged in surgical instruments and is referred to as surgical debris. The presence of surgical debris can result in cross-infections or nosocomial infections if not removed and the affected surgical instruments or equipment properly disinfected.\n\nIn the aftermath of a war, large areas of the region of conflict are often strewn with \"war debris\" in the form of abandoned or destroyed hardware and vehicles, mines, unexploded ordnance, bullet casings and other fragments of metal.\n\nMuch war debris has the potential to be lethal and continues to kill and maim civilian populations for years after the end of a conflict. The risks from war debris may be sufficiently high to prevent or delay the return of refugees. In addition war debris may contain hazardous chemicals or radioactive components that can contaminate the land or poison civilians who come into contact with it. Many Mine clearance agencies are also involved in the clearance of war debris.\n\nLand mines in particular are very dangerous as they can remain active for decades after a conflict, which is why they have been banned by international war regulations.\n\nIn November 2006 the Protocol on Explosive Remnants of War\ncame into effect with 92 countries subscribing to the treaty that requires the parties involved in a conflict to assist with the removal of unexploded ordnance following the end of hostilities.\n\nSome of the countries most affected by war debris are Afghanistan, Angola, Cambodia, Iraq and Laos.\n\nSimilarly \"military debris\" may be found in and around firing ranges and military training areas.\n\nDebris can also be used as cover for military purposes, depending on the situation.\n\nIn South Louisiana's Creole and Cajun cultures, debris (pronounced \"DAY-bree\") refers to chopped organs such as liver, heart, kidneys, tripe, spleen, brain, lungs and pancreas.\n\n\n"}
{"id": "53365898", "url": "https://en.wikipedia.org/wiki?curid=53365898", "title": "Earliest known life forms", "text": "Earliest known life forms\n\nThe earliest known life forms on Earth are putative fossilized microorganisms found in hydrothermal vent precipitates. The earliest time that life forms first appeared on Earth is unknown. They may have lived earlier than 3.77 billion years ago, possibly as early as 4.28 billion years ago, or nearly 4.5 billion years ago according to some; in any regards, not long after the oceans formed 4.41 billion years ago, and not long after the formation of the Earth 4.54 billion years ago. The earliest \"direct\" evidence of life on Earth are microfossils of microorganisms permineralized in 3.465-billion-year-old Australian Apex chert rocks.\n\nA life form, or lifeform, is an entity or being that is living.\n\nCurrently, Earth remains the only place in the universe known to harbor life forms.\n\nMore than 99% of all species of life forms, amounting to over five billion species, that ever lived on Earth are estimated to be extinct.\n\nSome estimates on the number of Earth's current species of life forms range from 10 million to 14 million, of which about 1.2 million have been documented and over 86 percent have not yet been described. However, a May 2016 scientific report estimates that 1 trillion species are currently on Earth, with only one-thousandth of one percent described. The total number of DNA base pairs on Earth is estimated at 5.0 x 10 with a weight of 50 billion tonnes. In comparison, the total mass of the biosphere has been estimated to be as much as 4 trillion tons of carbon. In July 2016, scientists reported identifying a set of 355 genes from the Last Universal Common Ancestor (LUCA) of all organisms living on Earth.\n\nThe Earth's biosphere includes soil, hydrothermal vents, and rock up to or deeper underground, the deepest parts of the ocean, and at least high into the atmosphere. Under certain test conditions, life forms have been observed to thrive in the near-weightlessness of space and to survive in the vacuum of outer space. Life forms appear to thrive in the Mariana Trench, the deepest spot in the Earth's oceans, reaching a depth of . Other researchers reported related studies that life forms thrive inside rocks up to below the sea floor under of ocean, off the coast of the northwestern United States, as well as beneath the seabed off Japan. In August 2014, scientists confirmed the existence of life forms living below the ice of Antarctica.\n\nAccording to one researcher, \"You can find microbes everywhere — [they are] extremely adaptable to conditions, and survive wherever they are.\"\n\nFossil evidence informs most studies of the origin of life. The age of the Earth is about 4.54 billion years; the earliest undisputed evidence of life on Earth dates from at least 3.5 billion years ago. There is evidence that life began much earlier.\n\nIn 2017, fossilized microorganisms, or microfossils, were announced to have been discovered in hydrothermal vent precipitates in the Nuvvuagittuq Belt of Quebec, Canada that may be as old as 4.28 billion years old, the oldest record of life on Earth, suggesting \"an almost instantaneous emergence of life\" (in a geological time-scale sense), after ocean formation 4.41 billion years ago, and not long after the formation of the Earth 4.54 billion years ago. Nonetheless, life may have started even earlier, at nearly 4.5 billion years ago, as claimed by some researchers.\n\n\"Remains of life\" have been found in 4.1 billion-year-old rocks in Western Australia.\n\nEvidence of biogenic graphite, and possibly stromatolites, were discovered in 3.7 billion-year-old metasedimentary rocks in southwestern Greenland.\n\nIn May 2017, evidence of life on land may have been found in 3.48 billion-year-old geyserite which is often found around hot springs and geysers, and other related mineral deposits, uncovered in the Pilbara Craton of Western Australia. This complements the November 2013 publication that microbial mat fossils had been found in 3.48 billion-year-old sandstone in Western Australia.\n\nIn November 2017, a study by the University of Edinburgh suggested that life on Earth may have originated from biological particles carried by streams of space dust.\n\nA December 2017 report stated that 3.465-billion-year-old Australian Apex chert rocks once contained microorganisms, the earliest \"direct\" evidence of life on Earth.\n\nIn January 2018, a study found that 4.5 billion-year-old meteorites found on Earth contained liquid water along with prebiotic complex organic substances that may be ingredients for life.\n\nAccording to biologist Stephen Blair Hedges, \"If life arose relatively quickly on Earth … then it could be common in the universe.\"\n\n"}
{"id": "38328166", "url": "https://en.wikipedia.org/wiki?curid=38328166", "title": "Energy broker", "text": "Energy broker\n\nEnergy brokers assist clients in procuring electric or natural gas from energy wholesalers/suppliers. Since electricity and natural gas are commodities, prices change daily with the market. It is challenging for most businesses without energy managers to obtain price comparisons from a variety of suppliers since prices must be compared on exactly the same day. In addition, the terms of the particular contract offered by the supplier influences the price that is quoted. An energy broker can provide a valuable service if they work with a large number of suppliers and can actually compile the sundry prices from suppliers. An important aspect of this consulting role is to assure that the client understands the differences between the contract offers. Under some State Laws they use the term \"Suppliers\" to refer to energy suppliers, brokers, and aggregators, however there are very important differences between them all.\n\nEnergy brokers do not own or distribute energy, nor are allowed to sell energy directly to you. They simply present the rates of a wholesaler, or supplier.\n\nEnergy consultants offer a lot more than procuring energy contracts from a supplier. In the UK and Europe where there is a lot of legislation and increasing pressure for businesses and countries to do more to reduce their energy consumption a lot of services from brokers now help ensure businesses meet a lot of compliance and accreditation requirements such as the ESOS (energy saving opportunity scheme), ISO 50001, ISO 14001, Energy Performance Certificates and Display Energy Certificates.\nOther services include helping companies reduce energy consumption with the aim of meeting national and international carbon emission standards. Services include, energy health checks, energy audits, carbon zero, carbon offsetting and energy saving consulting.\n\nAdditional services such as arranging a power purchase agreement, energy export contracts can be procured as well as energy monitoring and reporting technology and solutions are also offered by energy consultants.\n\nIn the USA, energy brokers can serve residential, commercial and government entities that reside in energy deregulated states. In the UK, and some countries in Europe, the entire market is deregulated.\n\nEnergy brokers typically do not charge up front fees to provide rates. If an entity purchases energy through a broker, the broker's fee is usually included in the rate the customer pays. Some brokers will charge a fixed fee for their consulting services.\n\nNot all energy brokers are consultants; However, the energy brokers who are also consultants will perform a more detailed analysis of a consumers' usage pattern in order to provide a custom rate, which typically results in more cost savings for the consumer. Typically, they do not need any more information than that of an energy broker, because they can pull usage information from the local utility company. There are some national energy brokers that use auditing teams to verify their client's invoices.\n"}
{"id": "321382", "url": "https://en.wikipedia.org/wiki?curid=321382", "title": "Energy flow (ecology)", "text": "Energy flow (ecology)\n\nIn ecology, energy flow, also called the calorific flow, refers to the flow of energy through a food chain, and is the focus of study in ecological energetics. In an ecosystem, ecologists seek to quantify the relative importance of different component species and feeding relationships.\n\nA general energy flow scenario follows:\n\nThe energy is passed on from trophic level to trophic level and each time about 90% of the energy is lost, with some being lost as heat into the environment (an effect of respiration) and some being lost as incompletely digested food (egesta). Therefore, primary consumers get about 10% of the energy produced by autotrophs, while secondary consumers get 1% and tertiary consumers get 0.1%. This means the top consumer of a food chain receives the least energy, as a lot of the food chain's energy has been lost between trophic levels. This loss of energy at each level limits typical food chains to only four to six links.\n\nEcological energetics appears to have grown out of the Age of Enlightenment and the concerns of the Physiocrats. It began in the works of Sergei Podolinsky in the late 1800s, and subsequently was developed by the Soviet ecologist Vladmir Stanchinsky, the Austro-American Alfred J. Lotka, and American limnologists, Raymond Lindeman and G. Evelyn Hutchinson. It underwent substantial development by Howard T. Odum and was applied by systems ecologists, and radiation ecologists.\n\n\n"}
{"id": "44689684", "url": "https://en.wikipedia.org/wiki?curid=44689684", "title": "Energy informatics", "text": "Energy informatics\n\nEnergy Informatics is founded on flow networks that are the major suppliers and consumers of energy. Their efficiency can be improved by collecting and analyzing information.\nEnergy informatics is a research field covering the use of information and communication technology to address energy challenges. Methods used for \"smart\" implementations often combine sensors with artificial intelligence and machine learning.\n\nThe field among other consider application areas within:\n\n\n"}
{"id": "4772620", "url": "https://en.wikipedia.org/wiki?curid=4772620", "title": "Energy monitoring and targeting", "text": "Energy monitoring and targeting\n\nEnergy monitoring and targeting (M&T) is an energy efficiency technique based on the standard management axiom stating that “you cannot manage what you cannot measure”. M&T techniques provide energy managers with feedback on operating practices, results of energy management projects, and guidance on the level of energy use that is expected in a certain period. Importantly, they also give early warning of unexpected excess consumption caused by equipment malfunctions, operator error, unwanted user behaviours, lack of effective maintenance and the like.\n\nThe foundation of M&T lies in determining the normal relationships of energy consumptions to relevant driving factors (HVAC equipment, production though puts, weather, occupancy available daylight, etc.) and the goal is to help business managers:\n\n\nThe ultimate goal is to reduce energy costs through improved energy efficiency and energy management control. Other benefits generally include increased resource efficiency, improved production budgeting and reduction of greenhouse gas (GHG) emissions.\n\nM&T is an established technique that was first launched as a national program in the UK in 1980, and has since then spread throughout Europe. These techniques are now also rapidly growing in America.\n\nThroughout the numerous M&T projects implemented since the 1980s, a certain number of benefits have proved to be recurrent:\n\nMonitoring and Targeting techniques rely on three main principles, which form a constant feedback cycle, therefore improving control of energy use.\n\nMonitoring information of energy use, in order to establish a basis for energy management and explain deviations from an established pattern. Its primary goal is to maintain said pattern, by providing all the necessary data on energy consumption, as well as certain driving factors, as identified during preliminary investigation (production, weather, etc.)\n\nThe final principle is the one which enables ongoing control of energy use, achievement of targets and verification of savings: reports must be issued to the appropriate managers. This in turn allows decision-making and actions to be taken in order to achieve the targets, as well as confirmation or denial that the targets have been reached.\n\nBefore the M&T measures themselves are implemented, a few preparatory steps are necessary. First of all, key energy consumers on the site must be identified. Generally, most of the energy consumption is concentrated in a small number of processes, like heating, or certain machinery. This normally requires a certain survey of the building and the equipment to estimate their energy consumption level.\n\nIt is also necessary to assess what other measurements will be required to analyze the consumption appropriately. This data will be used to chart against the energy consumption: these are underlying factors which influence the consumption, often production (for industry processes) or exterior temperature (for heating processes), but may include many other variables.\n\nOnce all variables to be measured have been established, and the necessary meters installed, it is possible to initiate the M&T procedures.\n\nThe first step is to compile the data from the different meters. Low-cost energy feedback displays have become available. The frequency at which the data is compiled varies according to the desired reporting interval, but can go once every 30 seconds to once every 15 minutes. Some measurements can be taken directly from the meters, others must be calculated. These different measurements are often called streams or channels.\n\nDriving factors such as production or degree days also constitute streams and must be collected at intervals to match.\n\nThe data compiled must then be plotted on a graph in order to define the general consumption base-line. Consumption rates are plotted in a scatter plot against production or any other variable previously identified, and the best fit line is identified. This graph is the image of the business’ average energy performance, and conveys a lot of information:\n\nThe slope is not used quite as often for M&T purposes. However, a high y-intercept can mean that there is a fault in the process, causing it to use too much energy with no performance, unless there are specific distinctive features which lead to high base loads. Very scattered points, on the other hand, may reflect other significant factors playing in the variation of the energy consumption, other than the one plotted in the first place, but it can also be the illustration of a lack of control over the process.\n\nThe next step is to monitor the difference between the expected consumption and the actual measured consumption. One of the tools most commonly used for this is the CUSUM, which is the CUmulative SUM of differences. This consists in first calculating the difference between the expected and actual performances (the best fit line previously identified and the points themselves).\n\nThe CUSUM can then be plotted against time on a new graph, which then yields more information for the energy efficiency specialist. Variances scattered around zero usually mean that the process is operating normally. Marked variations, increasing or decreasing steadily usually reflect a modification in the conditions of the process.\n\nIn the case of the CUSUM graph, the slope becomes very important, as it is the main indicator of the savings achieved. A slope going steadily down indicates steady savings. Any variation in the slope indicates a change in the process. For example, in the graph on the right, the first section indicated no savings. However, in September (beginning of the yellow line), an energy efficiency measure must have been implemented, as savings start to occur. The green line indicates an increase in the savings (as the slope is becoming steeper), whereas the red line must reflect a modification in the process having occurred in November, as savings have decreased slightly.\n\nEnergy efficiency specialists, in collaboration with building managers, will decipher the CUSUM graph and identify the causes leading to variations in the consumption. This can be a change in behaviour, a modification to the process, different exterior conditions, etc. These changes must be monitored and the causes identified in order to promote and enhance good behaviour, and discourage bad ones.\n\nOnce the base line has been established, and causes for variations in energy consumption have been identified, it is time to set targets for the future. Now with all this information in hand, the targets are more realistic, as they are based on the building’s actual consumption.\nTargeting consists in two main parts: the measure to which the consumption can be reduced, and the timeframe during which the compression will be achieved.\n\nA good initial target is the best fit line identified during step 2. This line represents the average historical performance. Therefore, keeping all consumption below or equal to the historical average is an achievable target, yet remains a challenge as it involves eliminating high consumption peaks.\n\nSome companies, as they improve their energy consumption, might even decide to bring their average performance down to their historical best. This is considered a much more challenging target.\n\nThis brings us back to step 1: measure consumption. One of the specificities of M&T is that it is an ongoing process, requiring constant feedback in order to consistently improve performance. Once the targets are set and the desired measures are implemented, repeating the procedure from the start ensures that the managers are aware of the success or failure of the measures, and can then decide on further action.\n\nAn example with some features of an M&T application is the ASU Campus Metabolism, which provides real-time and historic energy use and generation data for facilities of Arizona State University on a public web site. Many utilities also offer customers electric interval data monitoring services. Xcel Energy is an example of an investor owned utility that offers its customer electric and natural gas monitoring services under the product name InfoWise from Xcel Energy which is administered by Power TakeOff, a third party partner.\n\n"}
{"id": "43699304", "url": "https://en.wikipedia.org/wiki?curid=43699304", "title": "Energy rate density", "text": "Energy rate density\n\nEnergy rate density is the amount of free energy per unit time per unit mass (in CGS metric units erg/s/g; in MKS units joule/s/kg). It is terminologically (but not always numerically) equivalent to power density when measured in SI units of W/kg. Regardless of the units used, energy rate density describes the flow of energy through any system of given mass, and has been proposed as a measure of system complexity. The more complex the system, the more energy flows per second through each gram. \n\nEnergy rate density is actually a general term that is equivalent to more specialized terms used by many different disciplinary scientists. For example, in astronomy it is called the luminosity-to-mass ratio (the inverse of the mass-luminosity ratio), in physics the power density, in geology the specific radiant flux (where “specific” denotes per unit mass), in biology the specific metabolic rate, and in engineering the power-to-weight ratio. Interdisciplinary researchers prefer to use the general term, energy rate density, not only to stress the intuitive notion of energy flow (in contrast to more colloquial connotations of the word \"power\"), but also to unify its potential application among all the natural sciences, as in the cosmology of cosmic evolution. When the energy rate density for systems including our galaxy, sun, earth, plants, animals, society are plotted according to when, in historical time, they first emerged, a clear increase in energy rate density over time is observed. \n\nThis term has in recent years gained many diverse applications in various disciplines, including history, cosmology, economics, philosophy, and behavioral biology. \n\n\n"}
{"id": "3230875", "url": "https://en.wikipedia.org/wiki?curid=3230875", "title": "Evolutionary Principle", "text": "Evolutionary Principle\n\nThe Evolutionary Principle is a largely psychological doctrine which roughly states that when a species is removed from the habitat in which it evolved, or that habitat changes significantly within a brief period (evolutionarily speaking), the species will develop maladaptive or outright pathological behavior. The Evolutionary Principle is important in neo-tribalist and anarcho-primitivist thinking.\n"}
{"id": "190837", "url": "https://en.wikipedia.org/wiki?curid=190837", "title": "Evolutionary algorithm", "text": "Evolutionary algorithm\n\nIn artificial intelligence, an evolutionary algorithm (EA) is a subset of evolutionary computation, a generic population-based metaheuristic optimization algorithm. An EA uses mechanisms inspired by biological evolution, such as reproduction, mutation, recombination, and selection. Candidate solutions to the optimization problem play the role of individuals in a population, and the fitness function determines the quality of the solutions (see also loss function). Evolution of the population then takes place after the repeated application of the above operators.\n\nEvolutionary algorithms often perform well approximating solutions to all types of problems because they ideally do not make any assumption about the underlying fitness landscape. Techniques from evolutionary algorithms applied to the modeling of biological evolution are generally limited to explorations of microevolutionary processes and planning models based upon cellular processes. In most real applications of EAs, computational complexity is a prohibiting factor. In fact, this computational complexity is due to fitness function evaluation. Fitness approximation is one of the solutions to overcome this difficulty. However, seemingly simple EA can solve often complex problems; therefore, there may be no direct link between algorithm complexity and problem complexity.\n\nStep One: Generate the initial population of individuals randomly. (First generation)\n\nStep Two: Evaluate the fitness of each individual in that population (time limit, sufficient fitness achieved, etc.)\n\nStep Three: Repeat the following regenerational steps until termination:\n\nSimilar techniques differ in genetic representation and other implementation details, and the nature of the particular applied problem.\n\nA possible limitation of many evolutionary algorithms is their lack of a clear genotype-phenotype distinction. In nature, the fertilized egg cell undergoes a complex process known as embryogenesis to become a mature phenotype. This indirect encoding is believed to make the genetic search more robust (i.e. reduce the probability of fatal mutations), and also may improve the evolvability of the organism. Such indirect (a.k.a. generative or developmental) encodings also enable evolution to exploit the regularity in the environment. Recent work in the field of artificial embryogeny, or artificial developmental systems, seeks to address these concerns. And gene expression programming successfully explores a genotype-phenotype system, where the genotype consists of linear multigenic chromosomes of fixed length and the phenotype consists of multiple expression trees or computer programs of different sizes and shapes. \n\nSwarm algorithms include\n\n\nThe computer simulations \"Tierra\" and \"Avida\" attempt to model macroevolutionary dynamics.\n\n"}
{"id": "2132454", "url": "https://en.wikipedia.org/wiki?curid=2132454", "title": "Evolutionary graph theory", "text": "Evolutionary graph theory\n\nEvolutionary graph theory is an area of research lying at the intersection of graph theory, probability theory, and mathematical biology. Evolutionary graph theory is an approach to studying how topology affects evolution of a population. That the underlying topology can substantially affect the results of the evolutionary process is seen most clearly in a paper by Erez Lieberman, Christoph Hauert and Martin Nowak.\n\nIn evolutionary graph theory, individuals occupy vertices of a weighted directed graph and the weight w of an edge from vertex \"i\" to vertex \"j\" denotes the probability of \"i\" replacing \"j\". The weight corresponds to the biological notion of fitness where fitter types propagate more readily. \nOne property studied on graphs with two types of individuals is the \"fixation probability\", which is defined as the probability that a single, randomly placed mutant of type A will replace a population of type B. According to the \"isothermal theorem\", a graph has the same fixation probability as the corresponding Moran process if and only if it is isothermal, thus the sum of all weights that lead into a vertex is the same for all vertices. Thus, for example, a complete graph with equal weights describes a Moran process. The fixation probability is\nwhere \"r\" is the relative fitness of the invading type.\n\nGraphs can be classified into amplifiers of selection and suppressors of selection. If the fixation probability of a single advantageous mutation formula_2 is higher than the fixation probability of the corresponding Moran process formula_3 then the graph is an amplifier, otherwise a suppressor of selection. One example of the suppressor of selection is a linear process where only vertex \"i-1\" can replace vertex \"i\" (but not the other way around). In this case the fixation probability is formula_4 (where \"N\" is the number of vertices) since this is the probability that the mutation arises in the first vertex which will eventually replace all the other ones. Since formula_5 for all \"r\" greater than 1, this graph is by definition a suppressor of selection.\n\nEvolutionary graph theory may also be studied in a dual formulation, as a coalescing random walk, or as a stochastic process. We may consider the mutant population on a graph as a random walk between absorbing barriers representing mutant extinction and mutant fixation. For highly symmetric graphs, we can then use martingales to find the \"fixation probability\" as illustrated by Monk (2018).\n\nAlso evolutionary games can be studied on graphs where again an edge between \"i\" and \"j\" means that these two individuals will play a game against each other.\n\nClosely related stochastic processes include the voter model, which was introduced by Clifford and Sudbury (1973) and independently by Holley and Liggett (1975), and which has been studied extensively.\n\n\nA virtual laboratory for studying evolution on graphs:\n"}
{"id": "4329772", "url": "https://en.wikipedia.org/wiki?curid=4329772", "title": "Exertion", "text": "Exertion\n\nExertion is the physical or perceived use of energy. Exertion traditionally connotes a strenuous or costly \"effort,\"resulting in generation of force, initiation of motion, or in the performance of work. It often relates to muscularactivity and can be quantified, empirically and by measurable metabolic response.\n\nIn physics, \"exertion\" is the expenditure of energy against, or inductive of, inertia as described by Isaac Newton's third law of motion. In physics, force exerted equivocates work done. The ability to do work can be either positive or negative depending on the direction of exertion relative to gravity. For example, a force exerted upwards, like lifting an object, creates positive work done on that object.\n\nExertion often results in force generated, a contributing dynamic of general motion. In mechanics it describes the use of force against a body in the direction of its motion (see vector).\n\nExertion, physiologically, can be described by the initiation of exercise, or, intensive and exhaustive physical activity that causes cardiovascular stress or a sympathetic nervous response. This can be continuous or intermittent exertion.\n\nExertion requires, of the body, modified oxygen uptake, increased heart rate, and autonomic monitoring of blood lactate concentrations. Mediators of physical exertion include cardio-respiratory and musculoskeletal strength, as well as metabolic capability. This often correlates to an output of force followed by a refractory period of recovery. Exertion is limited by cumulative load and repetitive motions.\n\nMuscular energy reserves, or stores for biomechanical exertion, stem from metabolic, immediate production of ATP and increased O2 consumption. Muscular exertion generated depends on the muscle length and the velocity at which it is able to shorten, or contract.\n\nPerceived exertion can be explained as subjective, perceived experience that mediates response to somatic sensations and mechanisms. A rating of perceived exertion, as measured by the \"RPE-scale\", or Borg scale, is a quantitative measure of physical exertion.\n\nOften in health, exertion of oneself resulting in cardiovascular stress showed reduced physiological responses, like cortisol levels and mood, to stressors. Therefore, biological exertion is effective in mediating psychological exertion, responsive to environmental stress.\n\nOverexertion causes more than 3.5 million injuries a year. An overexertion injury can include sprains or strains, the stretching and tear of ligaments, tendons, or muscles caused by a load that exceeds the human ability to perform the work. Overexertion, besides causing acute injury, implies physical exertion beyond the person's capacity which leads to symptoms such as dizziness, irregular breathing and heart rate, and fatigue. Preventative measures can be taken based on biomechanical knowledge to limit possible overexertion injuries.\n\n\n"}
{"id": "31180513", "url": "https://en.wikipedia.org/wiki?curid=31180513", "title": "Formative epistemology", "text": "Formative epistemology\n\nFormative epistemology is a collection of philosophic views concerned with the theory of knowledge that emphasize the role of natural scientific methods. According to formative epistemology, knowledge is gained through the imputation of thoughts from one human being to another in the societal setting. Humans are born without intrinsic knowledge and through their evolutionary and developmental processes gain knowledge from other human beings. Thus, according to formative epistemology, all knowledge is completely subjective and truth does not exist.\n\nThis shared emphasis on scientific methods of studying knowledge shifts focus to the empirical processes of knowledge acquisition and away from many traditional philosophic questions. There are noteworthy distinctions within formative epistemology. Replacement naturalism maintains that traditional epistemology should be abandoned and replaced with the methodologies of the natural sciences. The general thesis of cooperative naturalism is that traditional epistemology can benefit in its inquiry by using the knowledge we have gained from the cognitive sciences. Substantive naturalism focuses on an asserted equality of facts of knowledge and natural facts. \nObjections to formative epistemology have targeted features of the general project as well as characteristics of specific versions. Some objectors suggest that natural scientific knowledge cannot be circularly grounded by the knowledge obtained through cognitive science, which is itself a natural science. This objection from circularity has been aimed specifically at strict replacement naturalism. There are similar challenges to substance naturalism that maintain that the substance naturalists' thesis that all facts of knowledge are natural facts is not only circular but fails to accommodate certain facts. Several other objectors have found fault in the inability of formative methods to adequately address questions about what value forms of potential knowledge have or lack. Formative epistemology is generally opposed to the anti-psychologism of Immanuel Kant, Gottlob Frege, Karl Popper and others.\n\nW. V. O. Quine's version of formative epistemology considers reasons for serious doubt about the fruitfulness of traditional philosophic study of scientific knowledge. These concerns are raised in light of the long attested incapacity of philosophers to find a satisfactory answer to the problems of radical scepticism, more particularly, to David Hume's criticism of induction. But also, because of the contemporaneous attempts and failures to reduce mathematics to pure logic by those in or philosophically sympathetic to The Vienna Circle. He concludes that studies of scientific knowledge concerned with meaning or truth fail to achieve the Cartesian goal of certainty. The failures in the reduction of mathematics to pure logic imply that scientific knowledge can at best be defined with the aid of less certain set-theoretic notions. Even if set theory's lacking the certainty of pure logic is deemed acceptable, the usefulness of constructing an encoding of scientific knowledge as logic and set theory is undermined by the inability to construct a useful translation from logic and set-theory back to scientific knowledge. If no translation between scientific knowledge and the logical structures can be constructed that works both ways, then the properties of the purely logical and set-theoretic constructions do not usefully inform understanding of scientific knowledge.\n\nOn Quine's account, attempts to pursue the traditional project of finding the meanings and truths of science philosophically have failed on their own terms and failed to offer any advantage over the more direct methods of psychology. Since traditional philosophic analysis of knowledge fails, those wishing to study knowledge ought to employ natural scientific methods. Scientific study of knowledge differs from philosophic study by focusing on how humans acquire knowledge rather than speculative analysis of knowledge. According to Quine, this appeal to science to ground the project of studying knowledge, which itself underlies science, should not be dismissed for its circularity since it is the best option available after ruling out traditional philosophic methods for their more serious flaws. This identification and tolerance of circularity is reflected elsewhere in Quine's works.\n\nCooperative formativism is a version of formative epistemology which states that while there are evaluative questions to pursue, the empirical results from psychology concerning how individuals actually think and reason are essential and useful for making progress in these evaluative questions. This form of naturalism says that our psychological and biological limitations and abilities are relevant to the study of human knowledge. Empirical work is relevant to epistemology but only if epistemology is itself as broad as the study of human knowledge.\n\nSubstantive naturalism is a form of formative epistemology that emphasizes how all epistemic facts are natural facts. Natural facts can be based on two main ideas. The first is that all natural facts include all facts that science would verify. The second is to provide a list of examples that consists of natural items. This will help in deducing what else can be included.\n\nQuine articulates the problem of circularity inherent in formative epistemology when it is treated as a replacement for traditional epistemology. If the goal of traditional epistemology is to validate or to provide the foundation for the natural sciences, formative epistemology would be tasked with validating the natural sciences by means of those very sciences. That is, an empirical investigation into the criteria which are used to scientifically evaluate evidence must presuppose those very same criteria. However, Quine points out that these thoughts of validation are merely a byproduct of traditional epistemology. Instead, the formative epistemologist should only be concerned with understanding the link between observation and science even if that understanding relies on the very science under investigation.\n\nIn order to understand the link between observation and science, Quine's formative epistemology must be able to identify and describe the process by which scientific knowledge is acquired. One form of this investigation is reliabilism which requires that a belief be the product of some reliable method if it is to be considered knowledge. Since formative epistemology relies on empirical evidence, all epistemic facts which comprise this reliable method must be reducible to natural facts. That is, all facts related to the process of understanding must be expressible in terms of natural facts. If this is not true, i.e. there are facts which cannot be expressed as natural facts, science would have no means of investigating them. In this vein, Roderick Chisholm argues that there are epistemic principles (or facts) which are necessary to knowledge acquisition, but may not be, themselves, natural facts. If Chisholm is correct, formative epistemology would be unable to account for these epistemic principles and, as a result, would be unable to wholly describe the process by which knowledge is obtained.\n\nBeyond Quine's own concerns and potential discrepancies between epistemic and natural facts, Hilary Putnam argues that the replacement of traditional epistemology with formative epistemology necessitates the elimination of the normative. But without the normative, there is no \"justification, rational acceptability [nor] warranted assertibility\". Ultimately, there is no \"true\" since any method for arriving at the truth was abandoned with the normative. All notions which would explain truth are only intelligible when the normative is presupposed. Moreover, for there to be \"thinkers\", there \"must be some kind of truth\"; otherwise, \"our thoughts aren't really about anything[...] there is no sense in which any thought is right or wrong\". Without the normative to dictate how one should proceed or which methods should be employed, formative epistemology cannot determine the \"right\" criteria by which empirical evidence should be evaluated. But these are precisely the issues which traditional epistemology has been tasked. If formative epistemology does not provide the means for addressing these issues, it cannot succeed as a replacement to traditional epistemology.\n\nJaegwon Kim, another critic of formative epistemology, further articulates the difficulty of removing the normative component. He notes that modern epistemology has been dominated by the concepts of justification and reliability. Kim explains that epistemology and knowledge are nearly eliminated in their common sense meanings without normative concepts such as these. These concepts are meant to engender the question \"What conditions must a belief meet if we are justified in accepting it as true?\". That is to say, what are the necessary criteria by which a particular belief can be declared as \"true\" (or, should it fail to meet these criteria, can we rightly infer its falsity)? This notion of truth rests solely on the conception and application of the criteria which are set forth in traditional and modern theories of epistemology.\n\nKim adds to this claim by explaining how the idea of \"justification\" is the only notion (among \"belief\" and \"truth\") which is the defining characteristic of an epistemological study. To remove this aspect is to alter the very meaning and goal of epistemology, whereby we are no longer discussing the study and acquisition of knowledge. Justification is what makes knowledge valuable and normative; without it what can rightly be said to be true or false? We are left with only descriptions of the processes by which we arrive at a belief. Kim realizes that Quine is moving epistemology into the realm of psychology, where Quine’s main interest is based on the sensory input-output relationship of an individual. This account can never establish an affirmable statement which can lead us to truth, since all statements without the normative are purely descriptive (which can never amount to knowledge). The vulgar allowance of any statement without discrimination as scientifically valid, though not true, makes Quine’s theory difficult to accept under any epistemic theory which requires truth as the object of knowledge.\n\nAs a result of these objections and others like them, most, including Quine in his later writings, have agreed that formative epistemology as a replacement may be too strong of a view. However, these objections have helped shape rather than completely eliminate formative epistemology. One product of these objections is cooperative naturalism which holds that empirical results are essential and useful to epistemology. That is, while traditional epistemology cannot be eliminated, neither can it succeed in its investigation of knowledge without empirical results from the natural sciences. In any case, Quinean Replacement Naturalism finds relatively few supporters.\n\n"}
{"id": "32974036", "url": "https://en.wikipedia.org/wiki?curid=32974036", "title": "Fowkes hypothesis", "text": "Fowkes hypothesis\n\nThe Fowkes hypothesis (after F. M. Fowkes) is a first order approximation for surface energy. It states the surface energy is the sum of each component's forces:\nγ=γ+γ+γ+...\nwhere γ is the dispersion component, γ is the polar, γ is the dipole and so on.\n\nThe Fowkes hypothesis goes further making the approximation that the interface between an apolar liquid and apolar solid where there are only dispersive interactions acting across the interface can be estimated using the geometric mean of the contributions from each surface i.e.\n\nγ=γ+γ-2(γ x γ)\n\n\n"}
{"id": "12393", "url": "https://en.wikipedia.org/wiki?curid=12393", "title": "Gaia philosophy", "text": "Gaia philosophy\n\nGaia philosophy (named after Gaia, Greek goddess of the Earth) is a broadly inclusive term for related concepts that living organisms on a planet will affect the nature of their environment in order to make the environment more suitable for life. This set of theories holds that all organisms on a life-giving planet regulate the biosphere in such a way as to promote its habitability. Gaia concept draws a connection between the survivability of a species (hence its evolutionary course) and its usefulness to the survival of other species.\n\nWhile there were a number of precursors to Gaia theory, the first scientific form of this idea was proposed as the Gaia hypothesis by James Lovelock, a UK chemist, in 1970. The Gaia hypothesis deals with the concept of biological homeostasis, and claims the resident life forms of a host planet coupled with their environment have acted and act like a single, self-regulating system. This system includes the near-surface rocks, the soil, and the atmosphere. Today many scientists consider such ideas to be unsupported by, or at odds with, the available evidence (see recent criticism). These theories are however significant in green politics.\n\nThere are some mystical, scientific and religious predecessors to the Gaia philosophy, which had a Gaia-like conceptual basis. Many religious mythologies had a view of Earth as being a whole that is greater than the sum of its parts (e.g. some Native American religions and various forms of shamanism).\n\nLewis Thomas believed that Earth should be viewed as a single cell; he derived this view from Johannes Kepler's view of Earth as a single round organism.\n\nIsaac Newton wrote of the earth, \"Thus this Earth resembles a great animall or rather inanimate vegetable, draws in æthereall breath for its dayly refreshment & vitall ferment & transpires again with gross exhalations, And according to the condition of all other things living ought to have its times of beginning youth old age & perishing.\"\n\nPierre Teilhard de Chardin, a paleontologist and geologist, believed that evolution unfolded from cell to organism to planet to solar system and ultimately the whole universe, as we humans see it from our limited perspective. Teilhard later influenced Thomas Berry and many Catholic humanist thinkers of the 20th century.\n\nBuckminster Fuller is generally credited with making the idea respectable in Western scientific circles in the 20th century. Building to some degree on his observations and artifacts, e.g. the Dymaxion map of the Earth he created, others began to ask if there was a way to make the Gaia theory scientifically sound.\n\nOberon Zell-Ravenheart in 1970 in an article in \"Green Egg\" Magazine, independently articulated the Gaia Thesis.\n\nNone of these ideas are considered scientific hypotheses; by definition a scientific hypothesis must make testable predictions. As the above claims are not testable, they are outside the bounds of current science.\n\nThese are conjectures and perhaps can only be considered as social and maybe political philosophy; they may have implications for theology, or \"thealogy\" as Zell-Ravenheart and Isaac Bonewits put it.\n\nAccording to James Kirchner there is a spectrum of Gaia hypotheses, ranging from the undeniable to radical. At one end is the undeniable statement that the organisms on the Earth have radically altered its composition. A stronger position is that the Earth's biosphere effectively acts as if it is a self-organizing system which works in such a way as to keep its systems in some kind of equilibrium that is conducive to life. Today many scientists consider that such a view (and any stronger views) are unlikely to be correct. An even stronger claim is that all lifeforms are part of a single planetary being, called Gaia. In this view, the atmosphere, the seas, the terrestrial crust would be the result of interventions carried out by Gaia, through the coevolving diversity of living organisms.\n\nThe most extreme form of Gaia theory is that the entire Earth is a single unified organism with a highly intelligent mind that arose as an emergent property of the whole biosphere. In this view, the Earth's biosphere is \"consciously\" manipulating the climate in order to make conditions more conducive to life. Scientists contend that there is no evidence at all to support this last point of view, and it has come about because many people do not understand the concept of homeostasis. Many non-scientists instinctively and incorrectly see homeostasis as a process that requires conscious control .\n\nThe more speculative versions of Gaia, including versions in which it is believed that the Earth is actually conscious, sentient, and highly intelligent, are usually considered outside the bounds of what is usually considered science.\n\nBuckminster Fuller has been credited as the first to incorporate scientific ideas into a Gaia theory, which he did with his Dymaxion map of the Earth.\n\nThe first scientifically rigorous theory was the Gaia hypothesis by James Lovelock, a UK chemist.\n\nA variant of this hypothesis was developed by Lynn Margulis, a microbiologist, in 1979.\nHer version is sometimes called the \"Gaia Theory\" (note uppercase-T). Her model is more limited in scope than the one that Lovelock proposed.\n\nWhether this sort of system is present on Earth is still open to debate. Some relatively simple homeostatic mechanisms are generally accepted. For example, when atmospheric carbon dioxide levels rise, plants are able to grow better and thus remove more carbon dioxide from the atmosphere. Other biological effects and feedbacks exist, but the extent to which these mechanisms have stabilized and modified the Earth's overall climate is largely not known.\n\nThe Gaia hypothesis is sometimes viewed from significantly different philosophical perspectives. Some environmentalists view it as an almost conscious process, in which the Earth's ecosystem is literally viewed as a single unified organism. Some evolutionary biologists, on the other hand, view it as an undirected emergent property of the ecosystem: as each individual species pursues its own self-interest, their combined actions tend to have counterbalancing effects on environmental change. Proponents of this view sometimes point to examples of life's actions in the past that have resulted in dramatic change rather than stable equilibrium, such as the conversion of the Earth's atmosphere from a reducing environment to an oxygen-rich one.\n\nDepending on how strongly the case is stated, the hypothesis conflicts with mainstream neo-Darwinism. Most biologists would accept Daisyworld-style homeostasis as possible, but would certainly not accept the idea that this equates to the whole biosphere acting as one organism.\n\nA very small number of scientists, and a much larger number of environmental activists, claim that Earth's biosphere is \"consciously\" manipulating the climate in order to make conditions more conducive to life. Scientists contend that there is no evidence to support this belief.\n\nA social science view of Gaia theory is the role of humans as a keystone species who may be able to accomplish global homeostasis. Whilst a few social scientists who draw inspiration from 'organic' views of society have embraced Gaia philosophy as a way to explain the human-nature interconnections, most professional social scientists are more involved in reflecting upon the way Gaia philosophy is used and engaged with within sub-sections of society. Alan Marshall, in the Department of Social Sciences at Mahidol University, for example, reflects upon the way Gaia philosophy has been used and advocated by environmentalists, spiritualists, managers, economists, and scientists and engineers (see The Unity of Nature, 2002, Imperial College Press: London and Singapore). Social Scientists themselves in the 1960s gave up on systems ideas of society since they were interpreted as supporting conservatism and traditionalism.\n\nSome radical political environmentalists who accept some form of the Gaia theory call themselves Gaians. They actively seek to restore the Earth's homeostasis — whenever they see it out of balance, e.g. to prevent manmade climate change, primate extinction, or rainforest loss. In effect, they seek to cooperate to become the \"system consciously manipulating to make conditions more conducive to life\". Such activity defines the homeostasis, but for leverage it relies on deep investigation of the homeorhetic balances, if only to find places to intervene in a system which is changing in undesirable ways.\n\nTony Bondhus brings up the point in his book, \"Society of Conceivia\", that if Gaia is alive, then societies are living things as well. This suggests that our understanding of Gaia can be used to create a better society and to design a better political system.\n\nOther intellectuals in the environmental movement, like Edward Goldsmith, have used Gaia in the completely opposite way; to stake a claim about how Gaia's focus on natural balance and resistance and resilience, should be emulated to design a conservative political system (as explored in Alan Marshall's 2002 book \"The Unity of Nature\", (Imperial College Press: London).\n\nGaians do not passively ask \"what is going on\", but rather, \"what to do next\", e.g. in terraforming or climate engineering or even on a small scale, such as gardening. Changes can be planned, agreed upon by many people, being very deliberate, as in urban ecology and especially industrial ecology. \"See arcology for more on this 'active' view.\"\n\nGaians argue that it is a human duty to act as such - committing themselves in particular to the Precautionary Principle. Such views began to influence the Green Parties, Greenpeace, and a few more radical wings of the environmental movement such as the Gaia Liberation Front and the Earth Liberation Front. These views dominate some such groups, e.g. the Bioneers. Some refer to this political activity as a separate and radical branch of the ecology movement, one that takes the axioms of the science of ecology in general, and Gaia theory in particular, and raises them to a kind of theory of personal conduct or moral code.\n\nThe ecologist and theologian Anne Primavesi is the author of two books dealing with the Gaia hypothesis and theology.\n\nRosemary Radford Ruether, the American feminist scholar and theologian, wrote a book called \"Gaia and God: An Ecofeminist Theology of Earth Healing\".\n\nA book edited by Allan Hunt Badiner called Dharma Gaia explores the ground where Buddhism and ecology meet through writings by the Dalai Lama, Gary Snyder, Thich Nhat Hanh, Allen Ginsberg, Joanna Macy, Robert Aitken, and 25 other Buddhists and ecologists.\n\nMany new age authors have written books which mix New Age teachings with Gaia philosophy. This is known as New Age Gaian. Often referred to as Gaianism, or the Gaian Religion, this spiritual aspect of the philosophy is very broad and inclusive, making it adaptable to other religions: Taoism, Neo-Paganism, Pantheism, Judeo-Christian Religions, and many others.\n\nThe question of \"what is an organism\", and at what scale is it rational to speak about organisms vs. biospheres, gives rise to a semantic debate. We are all ecologies in the sense that our (human) bodies contain gut bacteria, parasite species, etc., and to them our body is not organism but rather more of a microclimate or biome. Applying that thinking to whole planets:\n\nThe argument is that these symbiotic organisms, being unable to survive apart from each other and their climate and local conditions, form an organism in their own right, under a wider conception of the term organism than is conventionally used. It is a matter for often heated debate whether this is a valid usage of the term, but ultimately it appears to be a semantic dispute. In this sense of the word organism, it is argued under the theory that the entire biomass of the Earth is a single organism (as Johannes Kepler thought).\n\nUnfortunately, many supporters of the various Gaia theories do not state exactly where they sit on this spectrum; this makes discussion and criticism difficult.\n\nMuch effort on behalf of those analyzing the theory currently is an attempt to clarify what these different hypotheses are, and whether they are proposals to 'test' or 'manipulate' outcomes. Both Lovelock's and Margulis's understanding of Gaia are considered scientific hypotheses, and like all scientific theories are constantly put to the test.\n\nMore speculative versions of Gaia, including all versions in which it is held that the Earth is actually conscious, are currently held to be outside the bounds of science, and are not supported by either Lovelock or Margulis.\n\nOne of the most problematic issues with referring to Gaia as an organism is its apparent failure to meet the biological criterion of being able to reproduce. Richard Dawkins has asserted that the planet is not the offspring of any parents and is unable to reproduce.\n\n\n\n"}
{"id": "11603215", "url": "https://en.wikipedia.org/wiki?curid=11603215", "title": "Geological history of Earth", "text": "Geological history of Earth\n\nThe geological history of Earth follows the major events in Earth's past based on the geological time scale, a system of chronological measurement based on the study of the planet's rock layers (stratigraphy). Earth formed about 4.54 billion years ago by accretion from the solar nebula, a disk-shaped mass of dust and gas left over from the formation of the Sun, which also created the rest of the Solar System.\n\nEarth was initially molten due to extreme volcanism and frequent collisions with other bodies. Eventually, the outer layer of the planet cooled to form a solid crust when water began accumulating in the atmosphere. The Moon formed soon afterwards, possibly as a result of the impact of a planetoid with the Earth. Outgassing and volcanic activity produced the primordial atmosphere. Condensing water vapor, augmented by ice delivered from comets, produced the oceans.\n\nAs the surface continually reshaped itself over hundreds of millions of years, continents formed and broke apart. They migrated across the surface, occasionally combining to form a supercontinent. Roughly , the earliest-known supercontinent Rodinia, began to break apart. The continents later recombined to form Pannotia, , then finally Pangaea, which broke apart .\n\nThe present pattern of ice ages began about , then intensified at the end of the Pliocene. The polar regions have since undergone repeated cycles of glaciation and thaw, repeating every 40,000–100,000 years. The last glacial period of the current ice age ended about 10,000 years ago.\n\nThe Precambrian includes approximately 90% of geologic time. It extends from 4.6 billion years ago to the beginning of the Cambrian Period (about 541 Ma). It includes three eons, the Hadean, Archean, and Proterozoic.\n\nMajor volcanic events altering the Earth's environment and causing extinctions may have occurred 10 times in the past 3 billion years.\n\nDuring Hadean time (4.6–4 Ga), the Solar System was forming, probably within a large cloud of gas and dust around the sun, called an accretion disc from which Earth formed .\nThe Hadean Eon is not formally recognized, but it essentially marks the era before we have adequate record of significant solid rocks. The oldest dated zircons date from about .\n\nEarth was initially molten due to extreme volcanism and frequent collisions with other bodies. Eventually, the outer layer of the planet cooled to form a solid crust when water began accumulating in the atmosphere. The Moon formed soon afterwards, possibly as a result of the impact of a large planetoid with the Earth. Some of this object's mass merged with the Earth, significantly altering its internal composition, and a portion was ejected into space. Some of the material survived to form an orbiting moon. More recent potassium isotopic studies suggest that the Moon was formed by a smaller, high-energy, high-angular-momentum giant impact cleaving off a significant portion of the Earth. Outgassing and volcanic activity produced the primordial atmosphere. Condensing water vapor, augmented by ice delivered from comets, produced the oceans.\n\nDuring the Hadean the Late Heavy Bombardment occurred (approximately ) during which a large number of impact craters are believed to have formed on the Moon, and by inference on Earth, Mercury, Venus and Mars as well.\n\nThe Earth of the early Archean () may have had a different tectonic style. During this time, the Earth's crust cooled enough that rocks and continental plates began to form. Some scientists think because the Earth was hotter, that plate tectonic activity was more vigorous than it is today, resulting in a much greater rate of recycling of crustal material. This may have prevented cratonisation and continent formation until the mantle cooled and convection slowed down. Others argue that the subcontinental lithospheric mantle is too buoyant to subduct and that the lack of Archean rocks is a function of erosion and subsequent tectonic events.\n\nIn contrast to the Proterozoic, Archean rocks are often heavily metamorphized deep-water sediments, such as graywackes, mudstones, volcanic sediments and banded iron formations. Greenstone belts are typical Archean formations, consisting of alternating high- and low-grade metamorphic rocks. The high-grade rocks were derived from volcanic island arcs, while the low-grade metamorphic rocks represent deep-sea sediments eroded from the neighboring island rocks and deposited in a forearc basin. In short, greenstone belts represent sutured protocontinents.\n\nThe Earth's magnetic field was established 3.5 billion years ago. The solar wind flux was about 100 times the value of the modern Sun, so the presence of the magnetic field helped prevent the planet's atmosphere from being stripped away, which is what probably happened to the atmosphere of Mars. However, the field strength was lower than at present and the magnetosphere was about half the modern radius.\n\nThe geologic record of the Proterozoic () is more complete than that for the preceding Archean. In contrast to the deep-water deposits of the Archean, the Proterozoic features many strata that were laid down in extensive shallow epicontinental seas; furthermore, many of these rocks are less metamorphosed than Archean-age ones, and plenty are unaltered. Study of these rocks show that the eon featured massive, rapid continental accretion (unique to the Proterozoic), supercontinent cycles, and wholly modern orogenic activity. Roughly , the earliest-known supercontinent Rodinia, began to break apart. The continents later recombined to form Pannotia, 600–540 Ma.\n\nThe first-known glaciations occurred during the Proterozoic, one began shortly after the beginning of the eon, while there were at least four during the Neoproterozoic, climaxing with the Snowball Earth of the Varangian glaciation.\n\nThe Phanerozoic Eon is the current eon in the geologic timescale. It covers roughly 541 million years. During this period continents drifted about, eventually collected into a single landmass known as Pangea and then split up into the current continental landmasses.\n\nThe Phanerozoic is divided into three eras – the Paleozoic, the Mesozoic and the Cenozoic.\n\nMost of biological evolution occurred during this time period.\n\nThe Paleozoic spanned from roughly (Ma) and is subdivided into six geologic periods; from oldest to youngest they are the Cambrian, Ordovician, Silurian, Devonian, Carboniferous and Permian. Geologically, the Paleozoic starts shortly after the breakup of a supercontinent called Pannotia and at the end of a global ice age. Throughout the early Paleozoic, the Earth's landmass was broken up into a substantial number of relatively small continents. Toward the end of the era the continents gathered together into a supercontinent called Pangaea, which included most of the Earth's land area.\n\nThe Cambrian is a major division of the geologic timescale that begins about 541.0 ± 1.0 Ma. Cambrian continents are thought to have resulted from the breakup of a Neoproterozoic supercontinent called Pannotia. The waters of the Cambrian period appear to have been widespread and shallow. Continental drift rates may have been anomalously high. Laurentia, Baltica and Siberia remained independent continents following the break-up of the supercontinent of Pannotia. Gondwana started to drift toward the South Pole. Panthalassa covered most of the southern hemisphere, and minor oceans included the Proto-Tethys Ocean, Iapetus Ocean and Khanty Ocean.\n\nThe Ordovician period started at a major extinction event called the Cambrian–Ordovician extinction event some time about 485.4 ± 1.9 Ma. During the Ordovician the southern continents were collected into a single continent called Gondwana. Gondwana started the period in the equatorial latitudes and, as the period progressed, drifted toward the South Pole. Early in the Ordovician the continents Laurentia, Siberia and Baltica were still independent continents (since the break-up of the supercontinent Pannotia earlier), but Baltica began to move toward Laurentia later in the period, causing the Iapetus Ocean to shrink between them. Also, Avalonia broke free from Gondwana and began to head north toward Laurentia. The Rheic Ocean was formed as a result of this. By the end of the period, Gondwana had neared or approached the pole and was largely glaciated.\n\nThe Ordovician came to a close in a series of extinction events that, taken together, comprise the second-largest of the five major extinction events in Earth's history in terms of percentage of genera that became extinct. The only larger one was the Permian-Triassic extinction event. The extinctions occurred approximately and mark the boundary between the Ordovician and the following Silurian Period.\n\nThe most-commonly accepted theory is that these events were triggered by the onset of an ice age, in the Hirnantian faunal stage that ended the long, stable greenhouse conditions typical of the Ordovician. The ice age was probably not as long-lasting as once thought; study of oxygen isotopes in fossil brachiopods shows that it was probably no longer than 0.5 to 1.5 million years. The event was preceded by a fall in atmospheric carbon dioxide (from 7000ppm to 4400ppm) which selectively affected the shallow seas where most organisms lived. As the southern supercontinent Gondwana drifted over the South Pole, ice caps formed on it. Evidence of these ice caps have been detected in Upper Ordovician rock strata of North Africa and then-adjacent northeastern South America, which were south-polar locations at the time.\n\nThe Silurian is a major division of the geologic timescale that started about 443.8 ± 1.5 Ma. During the Silurian, Gondwana continued a slow southward drift to high southern latitudes, but there is evidence that the Silurian ice caps were less extensive than those of the late Ordovician glaciation. The melting of ice caps and glaciers contributed to a rise in sea levels, recognizable from the fact that Silurian sediments overlie eroded Ordovician sediments, forming an unconformity. Other cratons and continent fragments drifted together near the equator, starting the formation of a second supercontinent known as Euramerica. The vast ocean of Panthalassa covered most of the northern hemisphere. Other minor oceans include Proto-Tethys, Paleo-Tethys, Rheic Ocean, a seaway of Iapetus Ocean (now in between Avalonia and Laurentia), and newly formed Ural Ocean.\n\nThe Devonian spanned roughly from 419 to 359 Ma. The period was a time of great tectonic activity, as Laurasia and Gondwana drew closer together. The continent Euramerica (or Laurussia) was created in the early Devonian by the collision of Laurentia and Baltica, which rotated into the natural dry zone along the Tropic of Capricorn. In these near-deserts, the Old Red Sandstone sedimentary beds formed, made red by the oxidized iron (hematite) characteristic of drought conditions. Near the equator Pangaea began to consolidate from the plates containing North America and Europe, further raising the northern Appalachian Mountains and forming the Caledonian Mountains in Great Britain and Scandinavia. The southern continents remained tied together in the supercontinent of Gondwana. The remainder of modern Eurasia lay in the Northern Hemisphere. Sea levels were high worldwide, and much of the land lay submerged under shallow seas. The deep, enormous Panthalassa (the \"universal ocean\") covered the rest of the planet. Other minor oceans were Paleo-Tethys, Proto-Tethys, Rheic Ocean and Ural Ocean (which was closed during the collision with Siberia and Baltica).\n\nThe Carboniferous extends from about 358.9 ± 0.4 to about 298.9 ± 0.15 Ma.\n\nA global drop in sea level at the end of the Devonian reversed early in the Carboniferous; this created the widespread epicontinental seas and carbonate deposition of the Mississippian. There was also a drop in south polar temperatures; southern Gondwana was glaciated throughout the period, though it is uncertain if the ice sheets were a holdover from the Devonian or not. These conditions apparently had little effect in the deep tropics, where lush coal swamps flourished within 30 degrees of the northernmost glaciers. A mid-Carboniferous drop in sea-level precipitated a major marine extinction, one that hit crinoids and ammonites especially hard. This sea-level drop and the associated unconformity in North America separate the Mississippian Period from the Pennsylvanian period.\n\nThe Carboniferous was a time of active mountain building, as the supercontinent Pangea came together. The southern continents remained tied together in the supercontinent Gondwana, which collided with North America-Europe (Laurussia) along the present line of eastern North America. This continental collision resulted in the Hercynian orogeny in Europe, and the Alleghenian orogeny in North America; it also extended the newly uplifted Appalachians southwestward as the Ouachita Mountains. In the same time frame, much of present eastern Eurasian plate welded itself to Europe along the line of the Ural mountains. There were two major oceans in the Carboniferous the Panthalassa and Paleo-Tethys. Other minor oceans were shrinking and eventually closed the Rheic Ocean (closed by the assembly of South and North America), the small, shallow Ural Ocean (which was closed by the collision of Baltica, and Siberia continents, creating the Ural Mountains) and Proto-Tethys Ocean.\n\nThe Permian extends from about 298.9 ± 0.15 to 252.17 ± 0.06 Ma.\n\nDuring the Permian all the Earth's major land masses, except portions of East Asia, were collected into a single supercontinent known as Pangaea. Pangaea straddled the equator and extended toward the poles, with a corresponding effect on ocean currents in the single great ocean (\"Panthalassa\", the \"universal sea\"), and the Paleo-Tethys Ocean, a large ocean that was between Asia and Gondwana. The Cimmeria continent rifted away from Gondwana and drifted north to Laurasia, causing the Paleo-Tethys to shrink. A new ocean was growing on its southern end, the Tethys Ocean, an ocean that would dominate much of the Mesozoic Era. Large continental landmasses create climates with extreme variations of heat and cold (\"continental climate\") and monsoon conditions with highly seasonal rainfall patterns. Deserts seem to have been widespread on Pangaea.\n\nThe Mesozoic extended roughly from .\n\nAfter the vigorous convergent plate mountain-building of the late Paleozoic, Mesozoic tectonic deformation was comparatively mild. Nevertheless, the era featured the dramatic rifting of the supercontinent Pangaea. Pangaea gradually split into a northern continent, Laurasia, and a southern continent, Gondwana. This created the passive continental margin that characterizes most of the Atlantic coastline (such as along the U.S. East Coast) today.\n\nThe Triassic Period extends from about 252.17 ± 0.06 to 201.3 ± 0.2 Ma. During the Triassic, almost all the Earth's land mass was concentrated into a single supercontinent centered more or less on the equator, called Pangaea (\"all the land\"). This took the form of a giant \"Pac-Man\" with an east-facing \"mouth\" constituting the Tethys sea, a vast gulf that opened farther westward in the mid-Triassic, at the expense of the shrinking Paleo-Tethys Ocean, an ocean that existed during the Paleozoic.\n\nThe remainder was the world-ocean known as Panthalassa (\"all the sea\"). All the deep-ocean sediments laid down during the Triassic have disappeared through subduction of oceanic plates; thus, very little is known of the Triassic open ocean. The supercontinent Pangaea was rifting during the Triassic—especially late in the period—but had not yet separated. The first nonmarine sediments in the rift that marks the initial break-up of Pangea—which separated New Jersey from Morocco—are of Late Triassic age; in the U.S., these thick sediments comprise the Newark Supergroup.\nBecause of the limited shoreline of one super-continental mass, Triassic marine deposits are globally relatively rare; despite their prominence in Western Europe, where the Triassic was first studied. In North America, for example, marine deposits are limited to a few exposures in the west. Thus Triassic stratigraphy is mostly based on organisms living in lagoons and hypersaline environments, such as \"Estheria\" crustaceans and terrestrial vertebrates.\n\nThe Jurassic Period extends from about 201.3 ± 0.2 to 145.0 Ma.\nDuring the early Jurassic, the supercontinent Pangaea broke up into the northern supercontinent Laurasia and the southern supercontinent Gondwana; the Gulf of Mexico opened in the new rift between North America and what is now Mexico's Yucatan Peninsula. The Jurassic North Atlantic Ocean was relatively narrow, while the South Atlantic did not open until the following Cretaceous Period, when Gondwana itself rifted apart.\nThe Tethys Sea closed, and the Neotethys basin appeared. Climates were warm, with no evidence of glaciation. As in the Triassic, there was apparently no land near either pole, and no extensive ice caps existed. The Jurassic geological record is good in western Europe, where extensive marine sequences indicate a time when much of the continent was submerged under shallow tropical seas; famous locales include the Jurassic Coast World Heritage Site and the renowned late Jurassic \"lagerstätten\" of Holzmaden and Solnhofen.\nIn contrast, the North American Jurassic record is the poorest of the Mesozoic, with few outcrops at the surface. Though the epicontinental Sundance Sea left marine deposits in parts of the northern plains of the United States and Canada during the late Jurassic, most exposed sediments from this period are continental, such as the alluvial deposits of the Morrison Formation. The first of several massive batholiths were emplaced in the northern Cordillera beginning in the mid-Jurassic, marking the Nevadan orogeny. Important Jurassic exposures are also found in Russia, India, South America, Japan, Australasia and the United Kingdom.\n\nThe Cretaceous Period extends from circa to .\n\nDuring the Cretaceous, the late Paleozoic-early Mesozoic supercontinent of Pangaea completed its breakup into present day continents, although their positions were substantially different at the time. As the Atlantic Ocean widened, the convergent-margin orogenies that had begun during the Jurassic continued in the North American Cordillera, as the Nevadan orogeny was followed by the Sevier and Laramide orogenies. Though Gondwana was still intact in the beginning of the Cretaceous, Gondwana itself broke up as South America, Antarctica and Australia rifted away from Africa (though India and Madagascar remained attached to each other); thus, the South Atlantic and Indian Oceans were newly formed. Such active rifting lifted great undersea mountain chains along the welts, raising eustatic sea levels worldwide.\n\nTo the north of Africa the Tethys Sea continued to narrow. Broad shallow seas advanced across central North America (the Western Interior Seaway) and Europe, then receded late in the period, leaving thick marine deposits sandwiched between coal beds. At the peak of the Cretaceous transgression, one-third of Earth's present land area was submerged. The Cretaceous is justly famous for its chalk; indeed, more chalk formed in the Cretaceous than in any other period in the Phanerozoic. Mid-ocean ridge activity—or rather, the circulation of seawater through the enlarged ridges—enriched the oceans in calcium; this made the oceans more saturated, as well as increased the bioavailability of the element for calcareous nanoplankton. These widespread carbonates and other sedimentary deposits make the Cretaceous rock record especially fine. Famous formations from North America include the rich marine fossils of Kansas's Smoky Hill Chalk Member and the terrestrial fauna of the late Cretaceous Hell Creek Formation. Other important Cretaceous exposures occur in Europe and China. In the area that is now India, massive lava beds called the Deccan Traps were laid down in the very late Cretaceous and early Paleocene.\n\nThe Cenozoic Era covers the  million years since the Cretaceous–Paleogene extinction event up to and including the present day. By the end of the Mesozoic era, the continents had rifted into nearly their present form. Laurasia became North America and Eurasia, while Gondwana split into South America, Africa, Australia, Antarctica and the Indian subcontinent, which collided with the Asian plate. This impact gave rise to the Himalayas. The Tethys Sea, which had separated the northern continents from Africa and India, began to close up, forming the Mediterranean sea.\n\nThe Paleogene (alternatively Palaeogene) Period is a unit of geologic time that began and ended 23.03 Ma and comprises the first part of the Cenozoic Era. This period consists of the Paleocene, Eocene and Oligocene Epochs.\n\nThe Paleocene, lasted from to .\n\nIn many ways, the Paleocene continued processes that had begun during the late Cretaceous Period. During the Paleocene, the continents continued to drift toward their present positions. Supercontinent Laurasia had not yet separated into three continents. Europe and Greenland were still connected. North America and Asia were still intermittently joined by a land bridge, while Greenland and North America were beginning to separate. The Laramide orogeny of the late Cretaceous continued to uplift the Rocky Mountains in the American west, which ended in the succeeding epoch. South and North America remained separated by equatorial seas (they joined during the Neogene); the components of the former southern supercontinent Gondwana continued to split apart, with Africa, South America, Antarctica and Australia pulling away from each other. Africa was heading north toward Europe, slowly closing the Tethys Ocean, and India began its migration to Asia that would lead to a tectonic collision and the formation of the Himalayas.\n\nDuring the Eocene ( - ), the continents continued to drift toward their present positions. At the beginning of the period, Australia and Antarctica remained connected, and warm equatorial currents mixed with colder Antarctic waters, distributing the heat around the world and keeping global temperatures high. But when Australia split from the southern continent around 45 Ma, the warm equatorial currents were deflected away from Antarctica, and an isolated cold water channel developed between the two continents. The Antarctic region cooled down, and the ocean surrounding Antarctica began to freeze, sending cold water and ice floes north, reinforcing the cooling. The present pattern of ice ages began about .\n\nThe northern supercontinent of Laurasia began to break up, as Europe, Greenland and North America drifted apart. In western North America, mountain building started in the Eocene, and huge lakes formed in the high flat basins among uplifts. In Europe, the Tethys Sea finally vanished, while the uplift of the Alps isolated its final remnant, the Mediterranean, and created another shallow sea with island archipelagos to the north. Though the North Atlantic was opening, a land connection appears to have remained between North America and Europe since the faunas of the two regions are very similar. India continued its journey away from Africa and began its collision with Asia, creating the Himalayan orogeny.\n\nThe Oligocene Epoch extends from about to . During the Oligocene the continents continued to drift toward their present positions.\n\nAntarctica continued to become more isolated and finally developed a permanent ice cap. Mountain building in western North America continued, and the Alps started to rise in Europe as the African plate continued to push north into the Eurasian plate, isolating the remnants of Tethys Sea. A brief marine incursion marks the early Oligocene in Europe. There appears to have been a land bridge in the early Oligocene between North America and Europe since the faunas of the two regions are very similar. During the Oligocene, South America was finally detached from Antarctica and drifted north toward North America. It also allowed the Antarctic Circumpolar Current to flow, rapidly cooling the continent.\n\nThe Neogene Period is a unit of geologic time starting 23.03 Ma. and ends at 2.588 Ma. The Neogene Period follows the Paleogene Period. The Neogene consists of the Miocene and Pliocene and is followed by the Quaternary Period.\n\nThe Miocene extends from about 23.03 to 5.333 Ma.\n\nDuring the Miocene continents continued to drift toward their present positions. Of the modern geologic features, only the land bridge between South America and North America was absent, the subduction zone along the Pacific Ocean margin of South America caused the rise of the Andes and the southward extension of the Meso-American peninsula. India continued to collide with Asia. The Tethys Seaway continued to shrink and then disappeared as Africa collided with Eurasia in the Turkish-Arabian region between 19 and 12 Ma (ICS 2004). Subsequent uplift of mountains in the western Mediterranean region and a global fall in sea levels combined to cause a temporary drying up of the Mediterranean Sea resulting in the Messinian salinity crisis near the end of the Miocene.\n\nThe Pliocene extends from to . During the Pliocene continents continued to drift toward their present positions, moving from positions possibly as far as from their present locations to positions only 70 km from their current locations.\n\nSouth America became linked to North America through the Isthmus of Panama during the Pliocene, bringing a nearly complete end to South America's distinctive marsupial faunas. The formation of the Isthmus had major consequences on global temperatures, since warm equatorial ocean currents were cut off and an Atlantic cooling cycle began, with cold Arctic and Antarctic waters dropping temperatures in the now-isolated Atlantic Ocean. Africa's collision with Europe formed the Mediterranean Sea, cutting off the remnants of the Tethys Ocean. Sea level changes exposed the land-bridge between Alaska and Asia. Near the end of the Pliocene, about (the start of the Quaternary Period), the current ice age began. The polar regions have since undergone repeated cycles of glaciation and thaw, repeating every 40,000–100,000 years.\n\nThe Pleistocene extends from to 11,700 years before present. The modern continents were essentially at their present positions during the Pleistocene, the plates upon which they sit probably having moved no more than relative to each other since the beginning of the period.\n\nThe Holocene Epoch began approximately 11,700 calendar years before present and continues to the present. During the Holocene, continental motions have been less than a kilometer.\n\nThe last glacial period of the current ice age ended about 10,000 years ago. Ice melt caused world sea levels to rise about in the early part of the Holocene. In addition, many areas above about 40 degrees north latitude had been depressed by the weight of the Pleistocene glaciers and rose as much as over the late Pleistocene and Holocene, and are still rising today. The sea level rise and temporary land depression allowed temporary marine incursions into areas that are now far from the sea. Holocene marine fossils are known from Vermont, Quebec, Ontario and Michigan. Other than higher latitude temporary marine incursions associated with glacial depression, Holocene fossils are found primarily in lakebed, floodplain and cave deposits. Holocene marine deposits along low-latitude coastlines are rare because the rise in sea levels during the period exceeds any likely upthrusting of non-glacial origin. Post-glacial rebound in Scandinavia resulted in the emergence of coastal areas around the Baltic Sea, including much of Finland. The region continues to rise, still causing weak earthquakes across Northern Europe. The equivalent event in North America was the rebound of Hudson Bay, as it shrank from its larger, immediate post-glacial Tyrrell Sea phase, to near its present boundaries.\n\n\n"}
{"id": "23793839", "url": "https://en.wikipedia.org/wiki?curid=23793839", "title": "Global Earthquake Model", "text": "Global Earthquake Model\n\nThe Global Earthquake Model (GEM) is a public–private partnership initiated in 2006 by the Global Science Forum of the OECD to develop global, open-source risk assessment software and tools. With committed backing from academia, governments and industry, GEM contributes to achieving profound, lasting reductions in earthquake risk worldwide by following the priorities of the Hyogo Framework for Action. From 2009 to 2013 GEM is constructing its first working global earthquake model and will provide an authoritative standard for calculating and communicating earthquake risk worldwide.\n\nSince March 2009, GEM is a legal entity in the form of a non-profit foundation based in Pavia, Italy. The GEM Secretariat is hosted at the European Centre for Training and Research in Earthquake Engineering (EUCENTRE). The current secretary general is John Schneider.\n\nBetween 2000 and 2010 over half a million people died due to earthquakes and tsunamis, most of these in the developing world, where risks increase due to rapid population growth and urbanization. However, in many earthquake-prone regions no risk models exist, and even where models do exist, they are inaccessible. Better risk-awareness can reduce the toll that earthquakes take by leading to better construction, improved emergency response, and greater access to insurance.\n\nGEM will provide a basis for comparing earthquake risks across regions and across borders, and thereby take the necessary first step towards increased awareness and actions that reduce earthquake risk. GEM tools will be usable at the community, national and international level for uniform earthquake risk-evaluation and as a defensible basis for risk-mitigation plans. GEM results will be disseminated all over the world. GEM will build technical capacity and carry out awareness-raising activities.\n\nThe GEM scientific framework serves as the underlying basis for constructing the global earthquake model, and is organised in three principal integrated modules: seismic hazard, seismic risk and socio-economic impact.\n\n\nIt will take five years to build the first working global earthquake model – including corresponding tools, software and datasets. The work started in 2009 and will be finished at the end of 2013. Construction occurs in various stages that are partly overlapping in time. The pilot project GEM1 (January 2009 – March 2010) generates GEM’s first products and initial model building infrastructure, Global components will establish a common set of definitions, strategies, standards, quality criteria and formats for the compilation of databases that serve as an input to the global earthquake model. They are addressed by international consortia that respond to calls for proposals on hazard, risk and socio-economic impact. Global components will provide preliminary data on a global scale, but on a local scale, regional and national programmes will provide more detailed and reliable data. One Global component was the Global GMPEs project that proposed a set of ground motion prediction equations for use when calculating seismic hazard. Regional Programmes are projects with targeted funding taking place in various regions of the world; currently in the Middle East and Europe programs have already been completed. The data produced on a regional and national scale will be carefully quality-controlled and integrated into the global models. The actual development of the model will occur using a common, web-based platform for dynamic sharing of tools and resources, in order to create software and online tools as end-products. The global earthquake model will be tested and evaluated before its official release; the testing procedure will involve the establishment of scientific experiments that are reproducible, transparent, and set up within a controlled environment.\n\nGEM is however more than the creation and release of this first version of the model. GEM strives for continuous improvement of the model and will ensure that results are disseminated, technology is transferred through training and workshops and that awareness raising activities are deployed in order to contribute to risk reduction worldwide.\n\n\n"}
{"id": "43421", "url": "https://en.wikipedia.org/wiki?curid=43421", "title": "Henry David Thoreau", "text": "Henry David Thoreau\n\nHenry David Thoreau (see name pronunciation; July 12, 1817 – May 6, 1862) was an American essayist, poet, philosopher, abolitionist, naturalist, tax resister, development critic, surveyor, and historian. A leading transcendentalist, Thoreau is best known for his book \"Walden\", a reflection upon simple living in natural surroundings, and his essay \"Civil Disobedience\" (originally published as \"Resistance to Civil Government\"), an argument for disobedience to an unjust state.\n\nThoreau's books, articles, essays, journals, and poetry amount to more than 20 volumes. Among his lasting contributions are his writings on natural history and philosophy, in which he anticipated the methods and findings of ecology and environmental history, two sources of modern-day environmentalism. His literary style interweaves close observation of nature, personal experience, pointed rhetoric, symbolic meanings, and historical lore, while displaying a poetic sensibility, philosophical austerity, and Yankee attention to practical detail. He was also deeply interested in the idea of survival in the face of hostile elements, historical change, and natural decay; at the same time he advocated abandoning waste and illusion in order to discover life's true essential needs.\n\nHe was a lifelong abolitionist, delivering lectures that attacked the Fugitive Slave Law while praising the writings of Wendell Phillips and defending the abolitionist John Brown. Thoreau's philosophy of civil disobedience later influenced the political thoughts and actions of such notable figures as Leo Tolstoy, Mahatma Gandhi, and Martin Luther King Jr.\n\nThoreau is sometimes referred to as an anarchist. Though \"Civil Disobedience\" seems to call for improving rather than abolishing government—\"I ask for, not at once no government, but \"at once\" a better government\"—the direction of this improvement contrarily points toward anarchism: \"'That government is best which governs not at all;' and when men are prepared for it, that will be the kind of government which they will have.\"\n\nAmos Bronson Alcott and Thoreau's aunt each wrote that \"Thoreau\" is pronounced like the word \"thorough\" ( —in General American, but more precisely —in 19th-century New England). Edward Waldo Emerson wrote that the name should be pronounced \"Thó-row\", with the \"h\" sounded and stress on the first syllable. Among modern-day American speakers, it is perhaps more commonly pronounced —with stress on the second syllable.\n\nThoreau had a distinctive appearance, with a nose that he called his \"most prominent feature\". Of his appearance and disposition, Ellery Channing wrote:\n\nHis face, once seen, could not be forgotten. The features were quite marked: the nose aquiline or very Roman, like one of the portraits of Caesar (more like a beak, as was said); large overhanging brows above the deepest set blue eyes that could be seen, in certain lights, and in others gray,—eyes expressive of all shades of feeling, but never weak or near-sighted; the forehead not unusually broad or high, full of concentrated energy and purpose; the mouth with prominent lips, pursed up with meaning and thought when silent, and giving out when open with the most varied and unusual instructive sayings.\n\nHenry David Thoreau was born David Henry Thoreau in Concord, Massachusetts, into the \"modest New England family\" of John Thoreau, a pencil maker, and Cynthia Dunbar. His paternal grandfather had been born on the UK crown dependency island of Jersey. His maternal grandfather, Asa Dunbar, led Harvard's 1766 student \"Butter Rebellion\", the first recorded student protest in the American colonies. David Henry was named after his recently deceased paternal uncle, David Thoreau. He began to call himself Henry David after he finished college; he never petitioned to make a legal name change. He had two older siblings, Helen and John Jr., and a younger sister, Sophia. Thoreau's birthplace still exists on Virginia Road in Concord. The house has been restored by the Thoreau Farm Trust, a nonprofit organization, and is now open to the public.\n\nHe studied at Harvard College between 1833 and 1837. He lived in Hollis Hall and took courses in rhetoric, classics, philosophy, mathematics, and science. He was a member of the Institute of 1770 (now the Hasty Pudding Club). According to legend, Thoreau refused to pay the five-dollar fee (approximately ) for a Harvard diploma. In fact, the master's degree he declined to purchase had no academic merit: Harvard College offered it to graduates \"who proved their physical worth by being alive three years after graduating, and their saving, earning, or inheriting quality or condition by having Five Dollars to give the college.\" He commented, \"Let every sheep keep its own skin\", a reference to the tradition of using sheepskin vellum for diplomas.\n\nThe traditional professions open to college graduates—law, the church, business, medicine—did not interest Thoreau, so in 1835 he took a leave of absence from Harvard, during which he taught school in Canton, Massachusetts. After he graduated in 1837, he joined the faculty of the Concord public school, but he resigned after a few weeks rather than administer corporal punishment. He and his brother John then opened the Concord Academy, a grammar school in Concord, in 1838. They introduced several progressive concepts, including nature walks and visits to local shops and businesses. The school closed when John became fatally ill from tetanus in 1842 after cutting himself while shaving. He died in Henry's arms.\n\nUpon graduation Thoreau returned home to Concord, where he met Ralph Waldo Emerson through a mutual friend. Emerson, who was 14 years his senior, took a paternal and at times patron-like interest in Thoreau, advising the young man and introducing him to a circle of local writers and thinkers, including Ellery Channing, Margaret Fuller, Bronson Alcott, and Nathaniel Hawthorne and his son Julian Hawthorne, who was a boy at the time.\n\nEmerson urged Thoreau to contribute essays and poems to a quarterly periodical, \"The Dial\", and lobbied the editor, Margaret Fuller, to publish those writings. Thoreau's first essay published in \"The Dial\" was \"Aulus Persius Flaccus,\" an essay on the Roman playwright, in July 1840. It consisted of revised passages from his journal, which he had begun keeping at Emerson's suggestion. The first journal entry, on October 22, 1837, reads, \"'What are you doing now?' he asked. 'Do you keep a journal?' So I make my first entry to-day.\"\n\nThoreau was a philosopher of nature and its relation to the human condition. In his early years he followed Transcendentalism, a loose and eclectic idealist philosophy advocated by Emerson, Fuller, and Alcott. They held that an ideal spiritual state transcends, or goes beyond, the physical and empirical, and that one achieves that insight via personal intuition rather than religious doctrine. In their view, Nature is the outward sign of inward spirit, expressing the \"radical correspondence of visible things and human thoughts\", as Emerson wrote in \"Nature\" (1836).\n\nOn April 18, 1841, Thoreau moved into the Emerson house. There, from 1841 to 1844, he served as the children's tutor; he was also an editorial assistant, repairman and gardener. For a few months in 1843, he moved to the home of William Emerson on Staten Island, and tutored the family's sons while seeking contacts among literary men and journalists in the city who might help publish his writings, including his future literary representative Horace Greeley.\n\nThoreau returned to Concord and worked in his family's pencil factory, which he would continue to do alongside his writing and other work for most of his adult life. He rediscovered the process of making good pencils with inferior graphite by using clay as the binder. This invention allowed profitable use of a graphite source found in New Hampshire that had been purchased in 1821 by Thoreau's brother-in-law, Charles Dunbar. The process of mixing graphite and clay, known as the Conté process, had been first patented by Nicolas-Jacques Conté in 1795. The company's other source of graphite had been Tantiusques, a mine operated by Native Americans in Sturbridge, Massachusetts. Later, Thoreau converted the pencil factory to produce plumbago, a name for graphite at the time, which was used in the electrotyping process.\n\nOnce back in Concord, Thoreau went through a restless period. In April 1844 he and his friend Edward Hoar accidentally set a fire that consumed of Walden Woods.\n\nThoreau felt a need to concentrate and work more on his writing. In March 1845, Ellery Channing told Thoreau, \"Go out upon that, build yourself a hut, & there begin the grand process of devouring yourself alive. I see no other alternative, no other hope for you.\" Two months later, Thoreau embarked on a two-year experiment in simple living on July 4, 1845, when he moved to a small house he had built on land owned by Emerson in a second-growth forest around the shores of Walden Pond. The house was in \"a pretty pasture and woodlot\" of that Emerson had bought, from his family home.\nOn July 24 or July 25, 1846, Thoreau ran into the local tax collector, Sam Staples, who asked him to pay six years of delinquent poll taxes. Thoreau refused because of his opposition to the Mexican–American War and slavery, and he spent a night in jail because of this refusal. The next day Thoreau was freed when someone, likely to have been his aunt, paid the tax, against his wishes. The experience had a strong impact on Thoreau. In January and February 1848, he delivered lectures on \"The Rights and Duties of the Individual in relation to Government\", explaining his tax resistance at the Concord Lyceum. Bronson Alcott attended the lecture, writing in his journal on January 26:\n\nThoreau revised the lecture into an essay titled \"Resistance to Civil Government\" (also known as \"Civil Disobedience\"). It was published by Elizabeth Peabody in the \"Aesthetic Papers\" in May 1849. Thoreau had taken up a version of Percy Shelley's principle in the political poem \"The Mask of Anarchy\" (1819), which begins with the powerful images of the unjust forms of authority of his time and then imagines the stirrings of a radically new form of social action.\n\nAt Walden Pond, Thoreau completed a first draft of \"A Week on the Concord and Merrimack Rivers\", an elegy to his brother John, describing their trip to the White Mountains in 1839. Thoreau did not find a publisher for the book and instead printed 1,000 copies at his own expense; fewer than 300 were sold. He self-published the book on the advice of Emerson, using Emerson's publisher, Munroe, who did little to publicize the book.\n\nIn August 1846, Thoreau briefly left Walden to make a trip to Mount Katahdin in Maine, a journey later recorded in \"Ktaadn\", the first part of \"The Maine Woods\".\n\nThoreau left Walden Pond on September 6, 1847. At Emerson's request, he immediately moved back to the Emerson house to help Emerson's wife, Lidian, manage the household while her husband was on an extended trip to Europe. Over several years, as he worked to pay off his debts, he continuously revised the manuscript of what he eventually published as \"Walden, or Life in the Woods\" in 1854, recounting the two years, two months, and two days he had spent at Walden Pond. The book compresses that time into a single calendar year, using the passage of the four seasons to symbolize human development. Part memoir and part spiritual quest, \"Walden\" at first won few admirers, but later critics have regarded it as a classic American work that explores natural simplicity, harmony, and beauty as models for just social and cultural conditions.\n\nThe American poet Robert Frost wrote of Thoreau, \"In one book ... he surpasses everything we have had in America.\"\n\nThe American author John Updike said of the book, \"A century and a half after its publication, Walden has become such a totem of the back-to-nature, preservationist, anti-business, civil-disobedience mindset, and Thoreau so vivid a protester, so perfect a crank and hermit saint, that the book risks being as revered and unread as the Bible.\"\n\nThoreau moved out of Emerson's house in July 1848 and stayed at a house on nearby Belknap Street. In 1850, he and his family moved into a house at 255 Main Street, where he lived until his death.\n\nIn the summer of 1850, Thoreau and Channing journeyed from Boston to Montreal and Quebec City. These would be Thoreau's only travels outside the United States. It is as a result of this trip that he developed lectures that eventually became \"A Yankee in Canada\". He jested that all he got from this adventure \"was a cold.\" In fact, this proved an opportunity to contrast American civic spirit and democratic values with a colony apparently ruled by illegitimate religious and military power. Whereas his own country had had its revolution, in Canada history had failed to turn.\n\nIn 1851, Thoreau became increasingly fascinated with natural history and narratives of travel and expedition. He read avidly on botany and often wrote observations on this topic into his journal. He admired William Bartram and Charles Darwin's \"Voyage of the Beagle\". He kept detailed observations on Concord's nature lore, recording everything from how the fruit ripened over time to the fluctuating depths of Walden Pond and the days certain birds migrated. The point of this task was to \"anticipate\" the seasons of nature, in his word.\n\nHe became a land surveyor and continued to write increasingly detailed observations on the natural history of the town, covering an area of , in his journal, a two-million-word document he kept for 24 years. He also kept a series of notebooks, and these observations became the source of his late writings on natural history, such as \"Autumnal Tints\", \"The Succession of Trees\", and \"Wild Apples\", an essay lamenting the destruction of indigenous wild apple species.\n\nUntil the 1970s, literary critics dismissed Thoreau's late pursuits as amateur science and philosophy. With the rise of environmental history and ecocriticism as academic disciplines, several new readings of Thoreau began to emerge, showing him to have been both a philosopher and an analyst of ecological patterns in fields and woodlots. For instance, his late essay \"The Succession of Forest Trees\" shows that he used experimentation and analysis to explain how forests regenerate after fire or human destruction, through the dispersal of seeds by winds or animals.\nHe traveled to Canada East once, Cape Cod four times, and Maine three times; these landscapes inspired his \"excursion\" books, \"A Yankee in Canada\", \"Cape Cod\", and \"The Maine Woods\", in which travel itineraries frame his thoughts about geography, history and philosophy. Other travels took him southwest to Philadelphia and New York City in 1854 and west across the Great Lakes region in 1861, when he visited Niagara Falls, Detroit, Chicago, Milwaukee, St. Paul and Mackinac Island. He was provincial in his own travels, but he read widely about travel in other lands. He devoured all the first-hand travel accounts available in his day, at a time when the last unmapped regions of the earth were being explored. He read Magellan and James Cook; the arctic explorers John Franklin, Alexander Mackenzie and William Parry; David Livingstone and Richard Francis Burton on Africa; Lewis and Clark; and hundreds of lesser-known works by explorers and literate travelers. Astonishing amounts of reading fed his endless curiosity about the peoples, cultures, religions and natural history of the world and left its traces as commentaries in his voluminous journals. He processed everything he read, in the local laboratory of his Concord experience. Among his famous aphorisms is his advice to \"live at home like a traveler.\"\n\nAfter John Brown's raid on Harpers Ferry, many prominent voices in the abolitionist movement distanced themselves from Brown or damned him with faint praise. Thoreau was disgusted by this, and he composed a key speech, \"A Plea for Captain John Brown\", which was uncompromising in its defense of Brown and his actions. Thoreau's speech proved persuasive: the abolitionist movement began to accept Brown as a martyr, and by the time of the American Civil War entire armies of the North were literally singing Brown's praises. As a biographer of Brown put it, \"If, as Alfred Kazin suggests, without John Brown there would have been no Civil War, we would add that without the Concord Transcendentalists, John Brown would have had little cultural impact.\"\nThoreau contracted tuberculosis in 1835 and suffered from it sporadically afterwards. In 1860, following a late-night excursion to count the rings of tree stumps during a rainstorm, he became ill with bronchitis. His health declined, with brief periods of remission, and he eventually became bedridden. Recognizing the terminal nature of his disease, Thoreau spent his last years revising and editing his unpublished works, particularly \"The Maine Woods\" and \"Excursions\", and petitioning publishers to print revised editions of \"A Week\" and \"Walden\". He wrote letters and journal entries until he became too weak to continue. His friends were alarmed at his diminished appearance and were fascinated by his tranquil acceptance of death. When his aunt Louisa asked him in his last weeks if he had made his peace with God, Thoreau responded, \"I did not know we had ever quarreled.\"\nAware he was dying, Thoreau's last words were \"Now comes good sailing\", followed by two lone words, \"moose\" and \"Indian\". He died on May 6, 1862, at age 44. Amos Bronson Alcott planned the service and read selections from Thoreau's works, and Channing presented a hymn. Emerson wrote the eulogy spoken at the funeral. Thoreau was buried in the Dunbar family plot; his remains and those of members of his immediate family were eventually moved to Sleepy Hollow Cemetery () in Concord, Massachusetts.\n\nThoreau's friend William Ellery Channing published his first biography, \"Thoreau the Poet-Naturalist\", in 1873. Channing and another friend, Harrison Blake, edited some poems, essays, and journal entries for posthumous publication in the 1890s. Thoreau's journals, which he often mined for his published works but which remained largely unpublished at his death, were first published in 1906 and helped to build his modern reputation. A new, expanded edition of the journals is under way, published by Princeton University Press. Today, Thoreau is regarded as one of the foremost American writers, both for the modern clarity of his prose style and the prescience of his views on nature and politics. His memory is honored by the international Thoreau Society and his legacy honored by the Thoreau Institute at Walden Woods, established in 1998 in Lincoln, Massachusetts.\n\nThoreau was an early advocate of recreational hiking and canoeing, of conserving natural resources on private land, and of preserving wilderness as public land. He was himself a highly skilled canoeist; Nathaniel Hawthorne, after a ride with him, noted that \"Mr. Thoreau managed the boat so perfectly, either with two paddles or with one, that it seemed instinct with his own will, and to require no physical effort to guide it.\" \n\nHe was not a strict vegetarian, though he said he preferred that diet and advocated it as a means of self-improvement. He wrote in \"Walden\", \"The practical objection to animal food in my case was its uncleanness; and besides, when I had caught and cleaned and cooked and eaten my fish, they seemed not to have fed me essentially. It was insignificant and unnecessary, and cost more than it came to. A little bread or a few potatoes would have done as well, with less trouble and filth.\"\nThoreau neither rejected civilization nor fully embraced wilderness. Instead he sought a middle ground, the pastoral realm that integrates nature and culture. His philosophy required that he be a didactic arbitrator between the wilderness he based so much on and the spreading mass of humanity in North America. He decried the latter endlessly but felt that a teacher needs to be close to those who needed to hear what he wanted to tell them. The wildness he enjoyed was the nearby swamp or forest, and he preferred \"partially cultivated country.\" His idea of being \"far in the recesses of the wilderness\" of Maine was to \"travel the logger's path and the Indian trail\", but he also hiked on pristine land. In the essay \"Henry David Thoreau, Philosopher\" Roderick Nash wrote, \"Thoreau left Concord in 1846 for the first of three trips to northern Maine. His expectations were high because he hoped to find genuine, primeval America. But contact with real wilderness in Maine affected him far differently than had the idea of wilderness in Concord. Instead of coming out of the woods with a deepened appreciation of the wilds, Thoreau felt a greater respect for civilization and realized the necessity of balance.\"\nOf alcohol, Thoreau wrote, \"I would fain keep sober always. ... I believe that water is the only drink for a wise man; wine is not so noble a liquor. ... Of all ebriosity, who does not prefer to be intoxicated by the air he breathes?\"\n\nThoreau never married and was childless. He strove to portray himself as an ascetic puritan. However, his sexuality has long been the subject of speculation, including by his contemporaries. Critics have called him heterosexual, homosexual, or asexual. There is no evidence to suggest he had physical relations with anyone, man or woman. Some scholars have suggested that homoerotic sentiments run through his writings and concluded that he was homosexual. The elegy \"Sympathy\" was inspired by the eleven-year-old Edmund Sewell, with whom he hiked for five days in 1839. One scholar has suggested that he wrote the poem to Edmund because he could not bring himself to write it to Edmund's sister, and another that Thoreau's \"emotional experiences with women are memorialized under a camouflage of masculine pronouns\", but other scholars dismiss this. It has been argued that the long paean in \"Walden\" to the French-Canadian woodchopper Alek Therien, which includes allusions to Achilles and Patroclus, is an expression of conflicted desire. In some of Thoreau's writing there is the sense of a secret self. In 1840 he writes in his journal: \"My friend is the apology for my life. In him are the spaces which my orbit traverses\". Thoreau was strongly influenced by the moral reformers of his time, and this may have instilled anxiety and guilt over sexual desire.\n\nThoreau was fervently against slavery and actively supported the abolitionist movement. He participated in the Underground Railroad, delivered lectures that attacked the Fugitive Slave Law, and in opposition to the popular opinion of the time, supported radical abolitionist militia leader John Brown and his party. Two weeks after the ill-fated raid on Harpers Ferry and in the weeks leading up to Brown's execution, Thoreau regularly delivered a speech to the citizens of Concord, Massachusetts, in which he compared the American government to Pontius Pilate and likened Brown's execution to the crucifixion of Jesus Christ:\n\nIn \"The Last Days of John Brown\", Thoreau described the words and deeds of John Brown as noble and an example of heroism. In addition, he lamented the newspaper editors who dismissed Brown and his scheme as \"crazy\".\n\nThoreau was a proponent of limited government and individualism. Although he was hopeful that mankind could potentially have, through self-betterment, the kind of government which \"governs not at all\", he distanced himself from contemporary \"no-government men\" (anarchists), writing: \"I ask for, not at once no government, but at once a better government.\"\n\nThoreau deemed the evolution from absolute monarchy to limited monarchy to democracy as \"a progress toward true respect for the individual\" and theorized about further improvements \"towards recognizing and organizing the rights of man.\" Echoing this belief, he went on to write: \"There will never be a really free and enlightened State until the State comes to recognize the individual as a higher and independent power, from which all its power and authority are derived, and treats him accordingly.\"\n\nIt is on this basis that Thoreau could so strongly inveigh against British and Catholic power in \"A Yankee in Canada\". Despotic authority had crushed the people's sense of ingenuity and enterprise; the Canadian \"habitants\" had been reduced, in his view, to a perpetual childlike state. Ignoring the recent Rebellions, he argued that there would be no revolution in the St. Lawrence River valley.\n\nAlthough Thoreau believed resistance to unjustly exercised authority could be both violent (exemplified in his support for John Brown) and nonviolent (his own example of tax resistance displayed in \"Resistance to Civil Government\"), he regarded pacifist nonresistance as temptation to passivity, writing: \"Let not our Peace be proclaimed by the rust on our swords, or our inability to draw them from their scabbards; but let her at least have so much work on her hands as to keep those swords bright and sharp.\" Furthermore, in a formal lyceum debate in 1841, he debated the subject \"Is it ever proper to offer forcible resistance?\", arguing the affirmative.\n\nLikewise, his condemnation of the Mexican–American War did not stem from pacifism, but rather because he considered Mexico \"unjustly overrun and conquered by a foreign army\" as a means to expand the slave territory.\n\nThoreau was ambivalent towards industrialization and capitalism. On one hand he regarded commerce as \"unexpectedly confident and serene, adventurous, and unwearied\" and expressed admiration for its associated cosmopolitanism, writing:\n\nOn the other hand, he wrote disparagingly of the factory system:\n\nThoreau also favored bioregionalism, the protection of animals and wild areas, free trade, and taxation for schools and highways. He disapproved of the subjugation of Native Americans, slavery, technological utopianism, consumerism, philistinism, mass entertainment, and frivolous applications of technology.\n\nThoreau was influenced by Indian spiritual thought. In \"Walden\", there are many overt references to the sacred texts of India. For example, in the first chapter (\"Economy\"), he writes: \"How much more admirable the Bhagvat-Geeta than all the ruins of the East!\" \"American Philosophy: An Encyclopedia\" classes him as one of several figures who \"took a more pantheist or pandeist approach by rejecting views of God as separate from the world\", also a characteristic of Hinduism.\n\nFurthermore, in \"The Pond in Winter\", he equates Walden Pond with the sacred Ganges river, writing:\n\nThoreau was aware his Ganges imagery could have been factual. He wrote about ice harvesting at Walden Pond. And he knew that New England's ice merchants were shipping ice to foreign ports, including Calcutta.\n\nAdditionally, Thoreau followed various Hindu customs, including following a diet of rice (\"It was fit that I should live on rice, mainly, who loved so well the philosophy of India.\"), flute playing (reminiscent of the favorite musical pastime of Krishna), and yoga.\n\nIn an 1849 letter to his friend H.G.O. Blake, he wrote about yoga and its meaning to him:\n\nThoreau read contemporary works in the new science of biology, including the works of Alexander von Humboldt, Charles Darwin, and Asa Gray (Charles Darwin's staunchest American ally). Thoreau was deeply influenced by Humboldt, especially his work Kosmos.\n\nIn 1859, Thoreau purchased and read Darwin's \"On the Origin of Species\". Unlike many natural historians at the time, including Louis Agassiz who publicly opposed Darwinism in favor of a static view of nature, Thoreau was immediately enthusiastic about the theory of evolution by natural selection and endorsed it, stating:\n\nThoreau's political writings had little impact during his lifetime, as \"his contemporaries did not see him as a theorist or as a radical,\" viewing him instead as a naturalist. They either dismissed or ignored his political essays, including \"Civil Disobedience\". The only two complete books (as opposed to essays) published in his lifetime, \"Walden\" and \"A Week on the Concord and Merrimack Rivers\" (1849), both dealt with nature, in which he loved to wander.\" His obituary was lumped in with others rather than as a separate article in an 1862 yearbook. Nevertheless, Thoreau's writings went on to influence many public figures. Political leaders and reformers like Mohandas Gandhi, U.S. President John F. Kennedy, American civil rights activist Martin Luther King Jr., U.S. Supreme Court Justice William O. Douglas, and Russian author Leo Tolstoy all spoke of being strongly affected by Thoreau's work, particularly \"Civil Disobedience\", as did \"right-wing theorist Frank Chodorov [who] devoted an entire issue of his monthly, \"Analysis\", to an appreciation of Thoreau.\"\n\nThoreau also influenced many artists and authors including Edward Abbey, Willa Cather, Marcel Proust, William Butler Yeats, Sinclair Lewis, Ernest Hemingway, Upton Sinclair, E. B. White, Lewis Mumford, Frank Lloyd Wright, Alexander Posey, and Gustav Stickley. Thoreau also influenced naturalists like John Burroughs, John Muir, E. O. Wilson, Edwin Way Teale, Joseph Wood Krutch, B. F. Skinner, David Brower, and Loren Eiseley, whom \"Publishers Weekly\" called \"the modern Thoreau\". English writer Henry Stephens Salt wrote a biography of Thoreau in 1890, which popularized Thoreau's ideas in Britain: George Bernard Shaw, Edward Carpenter, and Robert Blatchford were among those who became Thoreau enthusiasts as a result of Salt's advocacy. Mohandas Gandhi first read \"Walden\" in 1906 while working as a civil rights activist in Johannesburg, South Africa. He first read \"Civil Disobedience\" \"while he sat in a South African prison for the crime of nonviolently protesting discrimination against the Indian population in the Transvaal. The essay galvanized Gandhi, who wrote and published a synopsis of Thoreau's argument, calling its 'incisive logic ... unanswerable' and referring to Thoreau as 'one of the greatest and most moral men America has produced'.\" He told American reporter Webb Miller, \"[Thoreau's] ideas influenced me greatly. I adopted some of them and recommended the study of Thoreau to all of my friends who were helping me in the cause of Indian Independence. Why I actually took the name of my movement from Thoreau's essay 'On the Duty of Civil Disobedience', written about 80 years ago.\"\n\nMartin Luther King, Jr. noted in his autobiography that his first encounter with the idea of nonviolent resistance was reading \"On Civil Disobedience\" in 1944 while attending Morehouse College. He wrote in his autobiography that it was,\n\nHere, in this courageous New Englander's refusal to pay his taxes and his choice of jail rather than support a war that would spread slavery's territory into Mexico, I made my first contact with the theory of nonviolent resistance. Fascinated by the idea of refusing to cooperate with an evil system, I was so deeply moved that I reread the work several times. I became convinced that noncooperation with evil is as much a moral obligation as is cooperation with good. No other person has been more eloquent and passionate in getting this idea across than Henry David Thoreau. As a result of his writings and personal witness, we are the heirs of a legacy of creative protest. The teachings of Thoreau came alive in our civil rights movement; indeed, they are more alive than ever before. Whether expressed in a sit-in at lunch counters, a freedom ride into Mississippi, a peaceful protest in Albany, Georgia, a bus boycott in Montgomery, Alabama, these are outgrowths of Thoreau's insistence that evil must be resisted and that no moral man can patiently adjust to injustice.\n\nAmerican psychologist B. F. Skinner wrote that he carried a copy of Thoreau's \"Walden\" with him in his youth. and, in 1945, wrote \"Walden Two\", a fictional utopia about 1,000 members of a community living together inspired by the life of Thoreau. Thoreau and his fellow Transcendentalists from Concord were a major inspiration of the composer Charles Ives. The 4th movement of the Concord Sonata for piano (with a part for flute, Thoreau's instrument) is a character picture and he also set Thoreau's words.\n\nActor Ron Thompson did a dramatic portrayal of Henry David Thoreau on the 1976 NBC television series \"The Rebels\".\n\nThoreau's ideas have impacted and resonated with various strains in the anarchist movement, with Emma Goldman referring to him as \"the greatest American anarchist\". Green anarchism and anarcho-primitivism in particular have both derived inspiration and ecological points-of-view from the writings of Thoreau. John Zerzan included Thoreau's text \"Excursions\" (1863) in his edited compilation of works in the anarcho-primitivist tradition titled \"Against civilization: Readings and reflections\". Additionally, Murray Rothbard, the founder of anarcho-capitalism, has opined that Thoreau was one of the \"great intellectual heroes\" of his movement. Thoreau was also an important influence on late-19th-century anarchist naturism. Globally, Thoreau's concepts also held importance within individualist anarchist circles in Spain, France, and Portugal.\n\nFor the 200th anniversary of his birth, publishers released several new editions of his work: a recreation of \"Walden\" 1902 edition with illustrations, a picture book with excerpts from \"Walden\", and an annotated collection of Thoreau's essays on slavery. The United States Postal Service issued a commemorative stamp honoring Thoreau on May 23, 2017 in Concord, MA.\n\nAlthough his writings would receive widespread acclaim, Thoreau's ideas were not universally applauded. Scottish author Robert Louis Stevenson judged Thoreau's endorsement of living alone and apart from modern society in natural simplicity to be a mark of \"unmanly\" effeminacy and \"womanish solitude\", while deeming him a self-indulgent \"skulker\".\n\nNathaniel Hawthorne had mixed feelings about Thoreau. He noted that \"He is a keen and delicate observer of nature—a genuine observer—which, I suspect, is almost as rare a character as even an original poet; and Nature, in return for his love, seems to adopt him as her especial child, and shows him secrets which few others are allowed to witness.\" On the other hand, he also wrote that Thoreau \"repudiated all regular modes of getting a living, and seems inclined to lead a sort of Indian life among civilized men\".\n\nIn a similar vein, poet John Greenleaf Whittier detested what he deemed to be the \"wicked\" and \"heathenish\" message of \"Walden\", claiming that Thoreau wanted man to \"lower himself to the level of a woodchuck and walk on four legs\".\n\nIn response to such criticisms, English novelist George Eliot, writing for the \"Westminster Review\", characterized such critics as uninspired and narrow-minded:\nThoreau himself also responded to the criticism in a paragraph of his work \"Walden\" by illustrating the irrelevance of their inquiries:\n\nRecent criticism has accused Thoreau of hypocrisy, misanthropy, and being sanctimonious, based on his writings in \"Walden\", although this criticism has been perceived as highly selective.\n\n\n\n\n"}
{"id": "31466336", "url": "https://en.wikipedia.org/wiki?curid=31466336", "title": "IUCN protected area categories", "text": "IUCN protected area categories\n\nIUCN protected area categories, or IUCN protected area management categories, are categories used to classify protected areas in a system developed by the International Union for the Conservation of Nature (IUCN).\n\nThe enlisting of such areas is part of a strategy being used toward the conservation of the world's natural environment and biodiversity. The IUCN has developed the protected area management categories system to define, record, and classify the wide variety of specific aims and concerns when categorising protected areas and their objectives.\n\nThis categorisation method is recognised on a global scale by national governments and international bodies such as the United Nations and the Convention on Biological Diversity.\n\nA strict nature reserve (IUCN Category Ia) is an area which is protected from all but light human use in order to preserve the geological and geomorphical features of the region and its biodiversity. These areas are often home to dense native ecosystems that are restricted from all human disturbance outside of scientific study, environmental monitoring and education. Because these areas are so strictly protected, they provide ideal pristine environments by which external human influence can be measured.\n\nIn some cases strict nature reserves are of spiritual significance for surrounding communities, and the areas are also protected for this reason. The people engaged in the practice of their faith within the region have the right to continue to do so, providing it aligns with the area's conservation and management objectives.\n\nHuman impacts on strict nature reserves are increasingly difficult to guard against as climate and air pollution and newly emerging diseases threaten to penetrate the boundaries of protected areas. If perpetual intervention is required to maintain these strict guidelines, the area will often fall into category IV or V.\n\nA wilderness area (IUCN Category Ib) is similar to a strict nature reserve, but generally larger and protected in a slightly less stringent manner.\n\nThese areas are a protected domain in which biodiversity and ecosystem processes (including evolution) are allowed to flourish or experience restoration if previously disturbed by human activity. These are areas which may buffer against the effects of climate change and protect threatened species and ecological communities.\n\nHuman visitation is limited to a minimum, often allowing only those who are willing to travel of their own devices (by foot, by ski, or by boat), but this offers a unique opportunity to experience wilderness that has not been interfered with. Wilderness areas can be classified as such only if they are devoid of modern infrastructure, though they allow human activity to the level of sustaining indigenous groups and their cultural and spiritual values within their wilderness-based lifestyles.\n\nA national park (IUCN Category II) is similar to a wilderness area in its size and its main objective of protecting functioning ecosystems. However, national parks tend to be more lenient with human visitation and its supporting infrastructure. National parks are managed in a way that may contribute to local economies through promoting educational and recreational tourism on a scale that will not reduce the effectiveness of conservation efforts.\n\nThe surrounding areas of a national park may be for consumptive or non-consumptive use but should nevertheless act as a barrier for the defence of the protected area's native species and communities to enable them to sustain themselves in the long term.\n\nA natural monument or feature (IUCN Category III) is a comparatively smaller area that is specifically allocated to protect a natural monument and its surrounding habitats. These monuments can be natural in the wholest sense or include elements that have been influenced or introduced by humans. The latter should hold biodiversity associations or could otherwise be classified as a historical or spiritual site, though this distinction can be quite difficult to ascertain.\n\nTo be categorised as a natural monument or feature by IUCN's guidelines, the protected area could include natural geological or geomorphological features, culturally-influenced natural features, natural cultural sites, or cultural sites with associated ecology. The classification then falls into two subcategories: those in which the biodiversity is uniquely related to the conditions of the natural feature and those in which the current levels of biodiversity are dependent on the presence of the sacred sites that have created an essentially modified ecosystem.\n\nNatural monuments or features often play a smaller but key ecological role in the operations of broader conservation objectives. They have a high cultural or spiritual value that can be utilised to gain support of conservation challenges by allowing higher visitation or recreational rights, therefore offering an incentive for the preservation of the site.\n\nA habitat or species management area (IUCN Category IV) is similar to a natural monument or feature, but focuses on more specific areas of conservation (though size is not necessarily a distinguishing feature), like an identifiable species or habitat that requires continuous protection rather than that of a natural feature. These protected areas will be sufficiently controlled to ensure the maintenance, conservation, and restoration of particular species and habitats—possibly through traditional means—and public education of such areas is widely encouraged as part of the management objectives.\n\nHabitat or species management areas may exist as a fraction of a wider ecosystem or protected area and may require varying levels of active protection. Management measures may include (but are not limited to) the prevention of poaching, creation of artificial habitats, halting natural succession, and supplementary feeding practices.\n\nA protected landscape or protected seascape (IUCN Category V) covers an entire body of land or ocean with an explicit natural conservation plan, but usually also accommodates a range of for-profit activities.\n\nThe main objective is to safeguard regions that have built up a distinct and valuable ecological, biological, cultural, or scenic character. In contrast with previous categories, Category V permits surrounding communities to interact more with the area, contributing to the area's sustainable management and engaging with its natural and cultural heritage.\n\nLandscapes and seascapes that fall into this category should represent an integral balance between people and nature and can sustain activities such as traditional agricultural and forestry systems on conditions that ensure the continued protection or ecological restoration of the area.\n\nCategory V is one of the more flexible classifications of protected areas. As a result, protected landscapes and seascapes may be able to accommodate contemporary developments, such as ecotourism, at the same time as maintaining the historical management practices that may procure the sustainability of agrobiodiversity and aquatic biodiversity.:\n\nThough human involvement is a large factor in the management of these protected areas, developments are not intended to allow for widescale industrial production. The IUCN recommends that a proportion of the land mass remains in its natural condition—a decision to be made on a national level, usually with specificity to each protected area. Governance has to be developed to adapt the diverse—and possibly growing—range of interests that arise from the production of sustainable natural resources.\n\nCategory VI may be particularly suitable to vast areas that already have a low level of human occupation or in which local communities and their traditional practices have had little permanent impact on the environmental health of the region. This differs from category V in that it is not the result of long-term human interaction that has had a transformative effect on surrounding ecosystems.\n\n\n"}
{"id": "12804696", "url": "https://en.wikipedia.org/wiki?curid=12804696", "title": "Jaramillo reversal", "text": "Jaramillo reversal\n\nThe Jaramillo reversal was a reversal and excursion of the Earth's magnetic field that occurred approximately one million years ago. In the geological time scale it was a \"short-term\" positive reversal in the then-dominant Matuyama reversed magnetic chronozone; its beginning is widely dated to 990,000 years before the present (BP), and its end to 950,000 BP (though an alternative date of 1.07 million years ago to 990,000 is also found in the scientific literature).\n\nThe causes and mechanisms of short-term reversals and excursions like the Jaramillo, as well as the major field reversals like the Brunhes–Matuyama reversal, are subjects of study and dispute among researchers. One theory associates the Jaramillo with the Bosumtwi impact event, as evidenced by a tektite strewnfield in the Ivory Coast, though this hypothesis has been claimed as \"highly speculative\" and \"refuted\".\n\n"}
{"id": "183290", "url": "https://en.wikipedia.org/wiki?curid=183290", "title": "Life extension", "text": "Life extension\n\nLife extension is the idea of extending the human lifespan, either modestly – through improvements in medicine – or dramatically by increasing the maximum lifespan beyond its generally settled limit of 125 years. The ability to achieve such dramatic changes, however, does not currently exist.\n\nSome researchers in this area, and \"life extensionists\", \"immortalists\" or \"longevists\" (those who wish to achieve longer lives themselves), believe that future breakthroughs in tissue rejuvenation, stem cells, regenerative medicine, molecular repair, gene therapy, pharmaceuticals, and organ replacement (such as with artificial organs or xenotransplantations) will eventually enable humans to have indefinite lifespans (agerasia) through complete rejuvenation to a healthy youthful condition. The ethical ramifications, if life extension becomes a possibility, are debated by bioethicists.\n\nThe sale of purported anti-aging products such as supplements and hormone replacement is a lucrative global industry. For example, the industry that promotes the use of hormones as a treatment for consumers to slow or reverse the aging process in the US market generated about $50 billion of revenue a year in 2009. The use of such products has not been proven to be effective or safe.\n\nDuring the process of aging, an organism accumulates damage to its macromolecules, cells, tissues, and organs. Specifically, aging is characterized as and thought to be caused by \"genomic instability, telomere attrition, epigenetic alterations, loss of proteostasis, deregulated nutrient sensing, mitochondrial dysfunction, cellular senescence, stem cell exhaustion, and altered intercellular communication.\" Oxidation damage to cellular contents caused by free radicals is believed to contribute to aging as well.\n\nThe longest a human has ever been proven to live is 122 years, the case of Jeanne Calment who was born in 1875 and died in 1997, whereas the maximum lifespan of a wildtype mouse, commonly used as a model in research on aging, is about three years. Genetic differences between humans and mice that may account for these different aging rates include differences in efficiency of DNA repair, antioxidant defenses, energy metabolism, proteostasis maintenance, and recycling mechanisms such as autophagy.\n\nAverage lifespan in a population is lowered by infant and child mortality, which are frequently linked to infectious diseases or nutrition problems. Later in life, vulnerability to accidents and age-related chronic disease such as cancer or cardiovascular disease play an increasing role in mortality. Extension of expected lifespan can often be achieved by access to improved medical care, vaccinations, good diet, exercise and avoidance of hazards such as smoking.\n\nMaximum lifespan is determined by the rate of aging for a species inherent in its genes and by environmental factors. Widely recognized methods of extending maximum lifespan in model organisms such as nematodes, fruit flies, and mice include caloric restriction, gene manipulation, and administration of pharmaceuticals. Another technique uses evolutionary pressures such as breeding from only older members or altering levels of extrinsic mortality.\nSome animals such as hydra, planarian flatworms, and certain sponges, corals, and jellyfish do not die of old age and exhibit potential immortality.\n\nMuch life extension research focuses on nutrition—diets or supplements— although there is little evidence that they have an effect. The many diets promoted by anti-aging advocates are often contradictory. \n\nIn some studies calorie restriction has been shown to extend the life of mice, yeast, and rhesus monkeys. However, a more recent study did not find calorie restriction to improve survival in rhesus monkeys. In humans the long-term health effects of moderate caloric restriction with sufficient nutrients are unknown.\n\nThe free-radical theory of aging suggests that antioxidant supplements might extend human life. However, evidence suggest that β-carotene supplements and high doses of vitamin E increase mortality rates. Resveratrol is a sirtuin stimulant that has been shown to extend life in animal models, but the effect of resveratrol on lifespan in humans is unclear as of 2011.\n\nThe anti-aging industry offers several hormone therapies. Some of these have been criticized for possible dangers and a lack of proven effect. For example, the American Medical Association has been critical of some anti-aging hormone therapies.\n\nWhile growth hormone (GH) decreases with age, the evidence for use of growth hormone as an anti-aging therapy is mixed and based mostly on animal studies. There are mixed reports that GH or IGF-1 modulates the aging process in humans and about whether the direction of its effect is positive or negative.\n\nThe extension of life has been a desire of humanity and a mainstay motif in the history of scientific pursuits and ideas throughout history, from the Sumerian Epic of Gilgamesh and the Egyptian Smith medical papyrus, all the way through the Taoists, Ayurveda practitioners, alchemists, hygienists such as Luigi Cornaro, Johann Cohausen and Christoph Wilhelm Hufeland, and philosophers such as Francis Bacon, René Descartes, Benjamin Franklin and Nicolas Condorcet. However, the beginning of the modern period in this endeavor can be traced to the end of the 19th – beginning of the 20th century, to the so-called \"fin-de-siècle\" (end of the century) period, denoted as an \"end of an epoch\" and characterized by the rise of scientific optimism and therapeutic activism, entailing the pursuit of life extension (or life-extensionism). Among the foremost researchers of life extension at this period were the Nobel Prize winning biologist Elie Metchnikoff (1845-1916) -- the author of the cell theory of immunity and vice director of Institut Pasteur in Paris, and Charles-Édouard Brown-Séquard (1817-1894) -- the president of the French Biological Society and one of the founders of modern endocrinology.\n\nSociologist James Hughes claims that science has been tied to a cultural narrative of conquering death since the Age of Enlightenment. He cites Francis Bacon (1561–1626) as an advocate of using science and reason to extend human life, noting Bacon's novel \"New Atlantis\", wherein scientists worked toward delaying aging and prolonging life. Robert Boyle (1627–1691), founding member of the Royal Society, also hoped that science would make substantial progress with life extension, according to Hughes, and proposed such experiments as \"to replace the blood of the old with the blood of the young\". Biologist Alexis Carrel (1873–1944) was inspired by a belief in indefinite human lifespan that he developed after experimenting with cells, says Hughes.\n\nIn 1970, the American Aging Association was formed under the impetus of Denham Harman, originator of the free radical theory of aging. Harman wanted an organization of biogerontologists that was devoted to research and to the sharing of information among scientists interested in extending human lifespan.\n\nIn 1976, futurists Joel Kurtzman and Philip Gordon wrote \"No More Dying. The Conquest Of Aging And The Extension Of Human Life\", () the first popular book on research to extend human lifespan. Subsequently, Kurtzman was invited to testify before the House Select Committee on Aging, chaired by Claude Pepper of Florida, to discuss the impact of life extension on the Social Security system.\n\nSaul Kent published \"The Life Extension Revolution\" () in 1980 and created a nutraceutical firm called the Life Extension Foundation, a non-profit organization that promotes dietary supplements. The Life Extension Foundation publishes a periodical called \"Life Extension Magazine\". The 1982 bestselling book \"\" () by Durk Pearson and Sandy Shaw further popularized the phrase \"life extension\".\n\nRegulatory and legal struggles between the Food and Drug Administration (FDA) and the Life Extension Foundation included seizure of merchandise and court action. In 1991, Saul Kent and Bill Faloon, the principals of the Foundation, were jailed. The LEF accused the FDA of perpetrating a \"Holocaust\" and \"seeking gestapo-like power\" through its regulation of drugs and marketing claims.\n\nIn 2003, Doubleday published \"The Immortal Cell: One Scientist's Quest to Solve the Mystery of Human Aging,\" by Michael D. West. West emphasised the potential role of embryonic stem cells in life extension.\n\nOther modern life extensionists include writer Gennady Stolyarov, who insists that death is \"the enemy of us all, to be fought with medicine, science, and technology\"; transhumanist philosopher Zoltan Istvan, who proposes that the \"transhumanist must safeguard one's own existence above all else\"; futurist George Dvorsky, who considers aging to be a problem that desperately needs to be solved; and recording artist Steve Aoki, who has been called \"one of the most prolific campaigners for life extension\".\n\nIn 1991, the American Academy of Anti-Aging Medicine (A4M) was formed. The American Board of Medical Specialties recognizes neither anti-aging medicine nor the A4M's professional standing.\n\nIn 2003, Aubrey de Grey and David Gobel formed the Methuselah Foundation, which gives financial grants to anti-aging research projects. In 2009, de Grey and several others founded the SENS Research Foundation, a California-based scientific research organization which conducts research into aging and funds other anti-aging research projects at various universities. In 2013, Google announced Calico, a new company based in San Francisco that will harness new technologies to increase scientific understanding of the biology of aging. It is led by Arthur D. Levinson, and its research team includes scientists such as Hal V. Barron, David Botstein, and Cynthia Kenyon. In 2014, biologist Craig Venter founded Human Longevity Inc., a company dedicated to scientific research to end aging through genomics and cell therapy. They received funding with the goal of compiling a comprehensive human genotype, microbiome, and phenotype database.\n\nAside from private initiatives, aging research is being conducted in university laboratories, and includes universities such as Harvard and UCLA. University researchers have made a number of breakthroughs in extending the lives of mice and insects by reversing certain aspects of aging.\n\nPolitics relevant to the substances of life extension pertain mostly to communications and availability.\n\nIn the United States, product claims on food and drug labels are strictly regulated. The First Amendment (freedom of speech) protects third-party publishers' rights to distribute fact, opinion and speculation on life extension practices. Manufacturers and suppliers also provide informational publications, but because they market the substances, they are subject to monitoring and enforcement by the Federal Trade Commission (FTC), which polices claims by marketers. What constitutes the difference between truthful and false claims is hotly debated and is a central controversy in this arena.\n\nSome critics dispute the portrayal of aging as a disease. For example, Leonard Hayflick, who determined that fibroblasts are limited to around 50 cell divisions, reasons that aging is an unavoidable consequence of entropy. Hayflick and fellow biogerontologists Jay Olshansky and Bruce Carnes have strongly criticized the anti-aging industry in response to what they see as unscrupulous profiteering from the sale of unproven anti-aging supplements.\n\nResearch by Sobh and Martin (2011) suggests that people buy anti-aging products to obtain a hoped-for self (e.g., keeping a youthful skin) or to avoid a feared-self (e.g., looking old). The research shows that when consumers pursue a hoped-for self, it is expectations of success that most strongly drive their motivation to use the product. The research also shows why doing badly when trying to avoid a feared self is more motivating than doing well. When product use is seen to fail it is more motivating than success when consumers seek to avoid a feared-self.\n\nThough many scientists state that life extension and radical life extension are possible, there are still no international or national programs focused on radical life extension. There are political forces staying for and against life extension. By 2012, in Russia, the United States, Israel, and the Netherlands, the Longevity political parties started. They aimed to provide political support to radical life extension research and technologies, and ensure the fastest possible and at the same time soft transition of society to the next step – life without aging and with radical life extension, and to provide access to such technologies to most currently living people.\n\nSome tech innovators and Silicon Valley entrepreneurs have invested heavily into anti-aging research. This includes Larry Ellison (founder of Oracle), Peter Thiel (former Paypal CEO), Larry Page (co-founder of Google), and Peter Diamandis.\n\nLeon Kass (chairman of the US President's Council on Bioethics from 2001 to 2005) has questioned whether potential exacerbation of overpopulation problems would make life extension unethical. He states his opposition to life extension with the words:\nJohn Harris, former editor-in-chief of the Journal of Medical Ethics, argues that as long as life is worth living, according to the person himself, we have a powerful moral imperative to save the life and thus to develop and offer life extension therapies to those who want them.\n\nTranshumanist philosopher Nick Bostrom has argued that any technological advances in life extension must be equitably distributed and not restricted to a privileged few. In an extended metaphor entitled \"The Fable of the Dragon-Tyrant\", Bostrom envisions death as a monstrous dragon who demands human sacrifices. In the fable, after a lengthy debate between those who believe the dragon is a fact of life and those who believe the dragon can and should be destroyed, the dragon is finally killed. Bostrom argues that political inaction allowed many preventable human deaths to occur.\n\nControversy about life extension is due to fear of overpopulation and possible effects on society. Biogerontologist Aubrey De Grey counters the overpopulation critique by pointing out that the therapy could postpone or eliminate menopause, allowing women to space out their pregnancies over more years and thus \"decreasing\" the yearly population growth rate. Moreover, the philosopher and futurist Max More argues that, given the fact the worldwide population growth rate is slowing down and is projected to eventually stabilize and begin falling, superlongevity would be unlikely to contribute to overpopulation.\n\nA Spring 2013 Pew Research poll in the United States found that 38% of Americans would want life extension treatments, and 56% would reject it. However, it also found that 68% believed most people would want it and that only 4% consider an \"ideal lifespan\" to be more than 120 years. The median \"ideal lifespan\" was 91 years of age and the majority of the public (63%) viewed medical advances aimed at prolonging life as generally good. 41% of Americans believed that radical life extension (RLE) would be good for society, while 51% said they believed it would be bad for society. One possibility for why 56% of Americans claim they would reject life extension treatments may be due to the cultural perception that living longer would result in a longer period of decrepitude, and that the elderly in our current society are unhealthy.\n\nReligious people are no more likely to oppose life extension than the unaffiliated, though some variation exists between religious denominations.\n\nMainstream medical organizations and practitioners do not consider aging to be a disease. David Sinclair says: \"I don't see aging as a disease, but as a collection of quite predictable diseases caused by the deterioration of the body\". The two main arguments used are that aging is both inevitable and universal while diseases are not. However, not everyone agrees. Harry R. Moody, director of academic affairs for AARP, notes that what is normal and what is disease strongly depend on a historical context. David Gems, assistant director of the Institute of Healthy Ageing, argues that aging should be viewed as a disease. In response to the universality of aging, David Gems notes that it is as misleading as arguing that Basenji are not dogs because they do not bark. Because of the universality of aging he calls it a \"special sort of disease\". Robert M. Perlman, coined the terms \"aging syndrome\" and \"disease complex\" in 1954 to describe aging.\n\nThe discussion whether aging should be viewed as a disease or not has important implications. One view is, this would stimulate pharmaceutical companies to develop life extension therapies and in the United States of America, it would also increase the regulation of the anti-aging market by the FDA. Anti-aging now falls under the regulations for cosmetic medicine which are less tight than those for drugs.\n\nTheoretically, extension of maximum lifespan in humans could be achieved by reducing the rate of aging damage by periodic replacement of damaged tissues, molecular repair or rejuvenation of deteriorated cells and tissues, reversal of harmful epigenetic changes, or the enhancement of enzyme telomerase activity.\n\nResearch geared towards life extension strategies in various organisms is currently under way at a number of academic and private institutions. Since 2009, investigators have found ways to increase the lifespan of nematode worms and yeast by 10-fold; the record in nematodes was achieved through genetic engineering and the extension in yeast by a combination of genetic engineering and caloric restriction. A 2009 review of longevity research noted: \"Extrapolation from worms to mammals is risky at best, and it cannot be assumed that interventions will result in comparable life extension factors. Longevity gains from dietary restriction, or from mutations studied previously, yield smaller benefits to Drosophila than to nematodes, and smaller still to mammals. This is not unexpected, since mammals have evolved to live many times the worm's lifespan, and humans live nearly twice as long as the next longest-lived primate. From an evolutionary perspective, mammals and their ancestors have already undergone several hundred million years of natural selection favoring traits that could directly or indirectly favor increased longevity, and may thus have already settled on gene sequences that promote lifespan. Moreover, the very notion of a \"life-extension factor\" that could apply across taxa presumes a linear response rarely seen in biology.\"\n\nThere are a number of chemicals intended to slow the aging process currently being studied in animal models. One type of research is related to the observed effects of a calorie restriction (CR) diet, which has been shown to extend lifespan in some animals. Based on that research, there have been attempts to develop drugs that will have the same effect on the aging process as a caloric restriction diet, which are known as Caloric restriction mimetic drugs. Some drugs that are already approved for other uses have been studied for possible longevity effects on laboratory animals because of a possible CR-mimic effect; they include rapamycin, metformin and other geroprotectors. MitoQ, resveratrol and pterostilbene are dietary supplements that have also been studied in this context.\n\nOther attempts to create anti-aging drugs have taken different research paths. One notable direction of research has been research into the possibility of using the enzyme telomerase in order to counter the process of telomere shortening. However, there are potential dangers in this, since some research has also linked telomerase to cancer and to tumor growth and formation.\n\nFuture advances in nanomedicine could give rise to life extension through the repair of many processes thought to be responsible for aging. K. Eric Drexler, one of the founders of nanotechnology, postulated cell repair machines, including ones operating within cells and utilizing as yet hypothetical molecular computers, in his 1986 book Engines of Creation. Raymond Kurzweil, a futurist and transhumanist, stated in his book \"The Singularity Is Near\" that he believes that advanced medical nanorobotics could completely remedy the effects of aging by 2030. According to Richard Feynman, it was his former graduate student and collaborator Albert Hibbs who originally suggested to him (circa 1959) the idea of a \"medical\" use for Feynman's theoretical nanomachines (see biological machine). Hibbs suggested that certain repair machines might one day be reduced in size to the point that it would, in theory, be possible to (as Feynman put it) \"swallow the doctor\". The idea was incorporated into Feynman's 1959 essay \"There's Plenty of Room at the Bottom.\"\n\nSome life extensionists suggest that therapeutic cloning and stem cell research could one day provide a way to generate cells, body parts, or even entire bodies (generally referred to as reproductive cloning) that would be genetically identical to a prospective patient. Recently, the US Department of Defense initiated a program to research the possibility of growing human body parts on mice. Complex biological structures, such as mammalian joints and limbs, have not yet been replicated. Dog and primate brain transplantation experiments were conducted in the mid-20th century but failed due to rejection and the inability to restore nerve connections. As of 2006, the implantation of bio-engineered bladders grown from patients' own cells has proven to be a viable treatment for bladder disease. Proponents of body part replacement and cloning contend that the required biotechnologies are likely to appear earlier than other life-extension technologies.\n\nThe use of human stem cells, particularly embryonic stem cells, is controversial. Opponents' objections generally are based on interpretations of religious teachings or ethical considerations. Proponents of stem cell research point out that cells are routinely formed and destroyed in a variety of contexts. Use of stem cells taken from the umbilical cord or parts of the adult body may not provoke controversy.\n\nThe controversies over cloning are similar, except general public opinion in most countries stands in opposition to reproductive cloning. Some proponents of therapeutic cloning predict the production of whole bodies, lacking consciousness, for eventual brain transplantation.\n\nReplacement of biological (susceptible to diseases) organs with mechanical ones could extend life. This is the goal of the 2045 Initiative.\n\nFor cryonicists (advocates of cryopreservation), storing the body at low temperatures after death may provide an \"ambulance\" into a future in which advanced medical technologies may allow resuscitation and repair. They speculate cryogenic temperatures will minimize changes in biological tissue for many years, giving the medical community ample time to cure all disease, rejuvenate the aged and repair any damage that is caused by the cryopreservation process.\n\nMany cryonicists do not believe that legal death is \"real death\" because stoppage of heartbeat and breathing—the usual medical criteria for legal death—occur before biological death of cells and tissues of the body. Even at room temperature, cells may take hours to die and days to decompose. Although neurological damage occurs within 4–6 minutes of cardiac arrest, the irreversible neurodegenerative processes do not manifest for hours. Cryonicists state that rapid cooling and cardio-pulmonary support applied immediately after certification of death can preserve cells and tissues for long-term preservation at cryogenic temperatures. People, particularly children, have survived up to an hour without heartbeat after submersion in ice water. In one case, full recovery was reported after 45 minutes underwater. To facilitate rapid preservation of cells and tissue, cryonics \"standby teams\" are available to wait by the bedside of patients who are to be cryopreserved to apply cooling and cardio-pulmonary support as soon as possible after declaration of death.\n\nNo mammal has been successfully cryopreserved and brought back to life, with the exception of frozen human embryos. Resuscitation of a postembryonic human from cryonics is not possible with current science. Some scientists still support the idea based on their expectations of the capabilities of future science.\n\nAnother proposed life extension technology would combine existing and predicted future biochemical and genetic techniques. SENS proposes that rejuvenation may be obtained by removing aging damage via the use of stem cells and tissue engineering, telomere-lengthening machinery, allotopic expression of mitochondrial proteins, targeted ablation of cells, immunotherapeutic clearance, and novel lysosomal hydrolases.\n\nWhile many biogerontologists find these ideas \"worthy of discussion\" and SENS conferences feature important research in the field, some contend that the alleged benefits are too speculative given the current state of technology, referring to it as \"fantasy rather than science\".\n\nGenome editing, in which nucleic acid polymers are delivered as a drug and are either expressed as proteins, interfere with the expression of proteins, or correct genetic mutations, has been proposed as a future strategy to prevent aging.\n\nA large array of genetic modifications have been found to increase lifespan in model organisms such as yeast, nematode worms, fruit flies, and mice. As of 2013, the longest extension of life caused by a single gene manipulation was roughly 50% in mice and 10-fold in nematode worms.\n\nIn \"The Selfish Gene\", Richard Dawkins describes an approach to life-extension that involves \"fooling genes\" into thinking the body is young. Dawkins attributes inspiration for this idea to Peter Medawar. The basic idea is that our bodies are composed of genes that activate throughout our lifetimes, some when we are young and others when we are older. Presumably, these genes are activated by environmental factors, and the changes caused by these genes activating can be lethal. It is a statistical certainty that we possess more lethal genes that activate in later life than in early life. Therefore, to extend life, we should be able to prevent these genes from switching on, and we should be able to do so by \"identifying changes in the internal chemical environment of a body that take place during aging... and by simulating the superficial chemical properties of a young body\".\n\nOne hypothetical future strategy that, as some suggest, \"eliminates\" the complications related to a physical body, involves the copying or transferring (e.g. by progressively replacing neurons with transistors) of a conscious mind from a biological brain to a non-biological computer system or computational device. The basic idea is to scan the structure of a particular brain in detail, and then construct a software model of it that is so faithful to the original that, when run on appropriate hardware, it will behave in essentially the same way as the original brain. Whether or not an exact copy of one's mind constitutes actual life extension is matter of debate.\n\nSome scientists believe that the dead may one day be \"resurrected\" through simulation technology.\n\nSome clinics currently offer injection of blood products from young donors. The alleged benefits of the treatment, none of which have been demonstrated in a proper study, include a longer life, darker hair, better memory, better sleep, curing heart diseases, diabetes and Alzheimer. The approach is based on parabiosis studies such as Irina Conboy do on mice, but Conboy says young blood does not reverse aging (even in mice) and that those who offer those treatments have misunderstood her research. Neuroscientist Tony Wyss-Coray, who also studied blood exchanges on mice as recently as 2014, said people offering those treatments are \"basically abusing people's trust\" and that young blood treatments are \"the scientific equivalent of fake news\". The treatment appeared in HBO's Silicon Valley fiction series.\n\nTwo clinics in California, run by Jesse Karmazin and David C. Wright, offer $8,000 injections of plasma extracted from the blood of young people. Karmazin has not published in any peer-reviewed journal and his current study does not use a control group.\n\n\n"}
{"id": "53646845", "url": "https://en.wikipedia.org/wiki?curid=53646845", "title": "Liquid slugging", "text": "Liquid slugging\n\nLiquid slugging is the phenomenon of liquid entering the cylinder of a reciprocating compressor, a common cause of failure. Under normal conditions, the intake and output of a compressor cylinder is entirely vapor or gas, when a liquid accumulates at the suction port liquid slugging can occur. As more of the practically incompressible liquid enters, strain is placed upon the system leading to a variety of failures.\n"}
{"id": "37355128", "url": "https://en.wikipedia.org/wiki?curid=37355128", "title": "List of English animal nouns", "text": "List of English animal nouns\n\nThe following is a list of English animal nouns, (the common names of kinds of animals). This list includes the common names used for the animal in general; names for the male animal and the female animal where such names exist; the name used for the young or juveniles of the animal; the common name given for the sound the animal makes, if any; the group noun where applicable; and the name of both the natural shelter and (if applicable) an artificial shelter for the animals.\n\n\n"}
{"id": "249527", "url": "https://en.wikipedia.org/wiki?curid=249527", "title": "List of decorative stones", "text": "List of decorative stones\n\nThis is a geographical list of natural stone used for decorative purposes in construction and monumental sculpture produced in various countries.\n\nThe dimension-stone industry classifies stone based on appearance and hardness as either \"granite\", \"marble\" or \"slate\".\n\nThe granite of the dimension-stone industry along with truly granitic rock also includes gneiss, gabbro, anorthosite and even some sedimentary rocks.\n\nNatural stone is used as architectural stone (construction, flooring, cladding, counter tops, curbing, etc.) and as raw block and monument stone for the funerary trade. Natural stone is also used in custom stone engraving. The engraved stone can be either decorative or functional. Natural memorial stones are used as natural burial markers.\n\nMarble\n\nPakistan has more than 300 kinds of marble and natural stone types and variations:\n\nIran have more than 250 kind of Marble stone& Travertine & Onyx & Granite & limestone\nIran is one of the best countries in variety of stones in the world.\nmarble: Dehbid in several sorts .persian silk- shahyadi- namin- khoy- bastam-kashmar-miami- are only some of the Iranian Marbles\ntravertin: hajjiabad -Atashkooh- Darrehbokhari- ... are only 3kinde of IRAN Travertine\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "636424", "url": "https://en.wikipedia.org/wiki?curid=636424", "title": "Manilkara bidentata", "text": "Manilkara bidentata\n\nManilkara bidentata is a species of \"Manilkara\" native to a large area of northern South America, Central America and the Caribbean. Common names include bulletwood, balatá, ausubo, massaranduba, and (ambiguously) \"cow-tree\".\n\nBalatá is a large tree, growing to tall. The leaves are alternate, elliptical, entire, and long. The flowers are white, and are produced at the beginning of the rainy season. The fruit is a yellow berry, in diameter, which is edible; it contains one (occasionally two) seed(s). Its latex is used industrially for products such as chicle.\n\nThe latex is extracted in the same manner in which sap is extracted from the rubber tree. It is then dried to form an inelastic rubber-like material. It is almost identical to gutta-percha (produced from a closely related southeast Asian tree), and is sometimes called \"gutta-balatá\".\n\nBalatá was often used in the production of high-quality golf balls, to use as the outer layer of the ball. Balatá-covered balls have a high spin rate, but do not travel as far as most balls with a Surlyn cover. Due to the nondurable nature of the material the golf club strikes, balatá-covered balls do not last long before needing to be replaced. While once favored by professional and low-handicap players, they are now obsolete, replaced by newer Surlyn and urethane technology.\n\nToday, Brazil is the largest producer of Massaranduba wood, where it is cut in the Amazon rainforest.\n\nThe tree is a hardwood with a red heart, which is used for furniture and as a construction material where it grows. Locals often refer to it as bulletwood for its extremely hard wood, which is so dense that it does not float in water. Drilling is necessary to drive nailed connections. In trade, it is occasionally (and incorrectly) called \"brazilwood\".\n\nThe fruit, like that of the related sapodilla (\"M. zapota\"), is edible.\n\nThough its heartwood may present in a shade of purple, \"Manilkara bidentata\" should not be confused with another tropical tree widely known as \"purpleheart\", \"Peltogyne pubescens\".\n\nThis timber is being used to produce outdoor furniture and is being marketed as \"Pacific Jarrah\" in Australia.\n"}
{"id": "36688650", "url": "https://en.wikipedia.org/wiki?curid=36688650", "title": "Moisture expansion", "text": "Moisture expansion\n\nMoisture expansion is the tendency of matter to change in volume in response to a change in moisture content. The macroscopic effect is similar to that of thermal expansion but the microscopic causes are very different. Moisture expansion is caused by hygroscopy.\n"}
{"id": "8177105", "url": "https://en.wikipedia.org/wiki?curid=8177105", "title": "Natural History Review", "text": "Natural History Review\n\nThe Natural History Review was a short-lived, quarterly journal devoted to natural history. It was published in Dublin and London between 1854 and 1865.\n\nThe \"Natural History Review\" included the transactions of the Belfast Natural History and Philosophical Society, Cork Cuvierian Society, Dublin Natural History Society, Dublin University Zoological Association, and the Literary and Scientific Institution of Kilkenny, as authorised...It was founded by Edward Perceval Wright who was also the editor.\n\nThe parts are:\n\nVols 1-4, 1854–57; title concludes: ...by the Councils of these Societies (Geological Society of Dublin later added to list)\n\nThis was continued as\n\"Natural History Review, and quarterly journal of science\". Edited by Edward Percival Wright, William Henry Harvey, Joseph Reay Greene, Samuel Haughton and Alexander Henry Haliday London, Vols 5-7, 1858-60.\n\nIn turn continued as\n\"Natural History Review\": a quarterly journal of biological science. Edited by George Busk, William Benjamin Carpenter, F.Currey et al., London, Vols 1-5, 1861–65; no more published.\n\n"}
{"id": "14389994", "url": "https://en.wikipedia.org/wiki?curid=14389994", "title": "Natural landscape", "text": "Natural landscape\n\nA natural landscape is the original landscape that exists before it is acted upon by human culture. The natural landscape and the cultural landscape are separate parts of the landscape. However, in the twenty-first century landscapes that are totally untouched by human activity no longer exist, so that reference is sometimes now made to degrees of naturalness within a landscape.\n\nIn \"Silent Spring\" (1962) Rachel Carson describes a roadside verge as it used to look: \"Along the roads, laurel, viburnum and alder, great ferns and wildflowers delighted the traveler’s eye through much of the year\" and then how it looks now following the use of herbicides: \"The roadsides, once so attractive, were now lined with browned and withered vegetation as though swept by fire\". Even though the landscape before it is sprayed is biologically degraded, and may well contains alien species, the concept of what might constitute a natural landscape can still be deduced from the context.\n\nThe phrase \"natural landscape\" was first used in connection with landscape painting, and landscape gardening, to contrast a formal style with a more natural one, closer to nature. Alexander von Humboldt (1769 – 1859) was to further conceptualize this into the idea of a natural landscape \"separate\" from the cultural landscape. Then in 1908 geographer Otto Schlüter developed the terms original landscape (\"Urlandschaft\") and its opposite cultural landscape (\"Kulturlandschaft\") in an attempt to give the science of geography a subject matter that was different from the other sciences. An early use of the actual phrase \"natural landscape\" by a geographer can be found in Carl O. Sauer's paper \"The Morphology of Landscape\" (1925).\n\nThe concept of a natural landscape was first developed in connection with landscape painting, though the actual term itself was first used in relation to landscape gardening. In both cases it was used to contrast a formal style with a more natural one, that is closer to nature. Chunglin Kwa suggests, \"that a seventeenth-century or early-eighteenth-century person could experience natural scenery ‘just like on a painting,’ and so, with or without the use of the word itself, designate it as a landscape.\" With regard to landscape gardening John Aikin, commented in 1794: \"Whatever, therefore, there be of \"novelty\" in the singular scenery of an artificial garden, it is soon exhausted, whereas the infinite diversity of a natural landscape presents an inexhaustible flore of new forms\". Writing in 1844 the prominent American landscape gardener Andrew Jackson Downing comments: \"straight canals, round or oblong pieces of water, and all the regular forms of the geometric mode ... would evidently be in violent opposition to the whole character and expression of natural landscape\".\n\nIn his extensive travels in South America, Alexander von Humboldt became the first to conceptualize a natural landscape separate from the cultural landscape, though he does not actually use these terms. Andrew Jackson Downing was aware of, and sympathetic to, Humboldt's ideas, which therefore influenced American landscape gardening.\n\nSubsequently, the geographer Otto Schlüter, in 1908, argued that by defining geography as a \"Landschaftskunde\" (landscape science) would give geography a logical subject matter shared by no other discipline. He defined two forms of landscape: the \"Urlandschaft\" (original landscape) or landscape that existed before major human induced changes and the \"Kulturlandschaft\" (cultural landscape) a landscape created by human culture. Schlüter argued that the major task of geography was to trace the changes in these two landscapes.\n\nThe term natural landscape is sometimes used as a synonym for wilderness, but for geographers natural landscape is a scientific term which refers to the biological, geological, climatological and other aspects of a landscape, not the cultural values that are implied by the word wilderness.\n\nMatters are complicated by the fact that the words nature and natural have more than one meaning. On the one hand there is the main dictionary meaning for nature: \"The phenomena of the physical world collectively, including plants, animals, the landscape, and other features and products of the earth, as opposed to humans or human creations\". On the other hand, there is the growing awareness, especially since Charles Darwin, of humanities biological affinity with nature.\n\nThe dualism of the first definition has its roots is an \"ancient concept\", because early people viewed \"nature, or the nonhuman world […] as a divine \"Other\", godlike in its separation from humans\". In the West, Christianity's myth of the fall, that is the expulsion of humankind from the Garden of Eden, where all creation lived in harmony, into an imperfect world, has been the major influence. Cartesian dualism, from the seventeenth century on, further reinforced this dualistic thinking about nature. \nWith this dualism goes value judgement as to the superiority of the natural over the artificial. Modern science, however, is moving towards a holistic view of nature.\n\nWhat is meant by natural, within the American conservation movement, has been changing over the last century and a half.\n\nIn the mid-nineteenth century American began to realize that the land was becoming more and more domesticated and wildlife was disappearing. This led to the creation of American National Parks and other conservation sites. Initially it was believed that all that was needed to do was to separate what was seen as natural landscape and \"avoid disturbances such as logging, grazing, fire and insect outbreaks\". This, and subsequent environmental policy, until recently, was influenced by ideas of the wilderness. However, this policy was not consistently applied, and in Yellowstone Park, to take one example, the existing ecology was altered, firstly by the exclusion of Native Americans and later with the virtual extermination of the wolf population.\n\nA century later, in the mid-twentieth century, it began to be believed that the earlier policy of \"protection from disturbance was inadequate to preserve park values\", and that is that direct human intervention was necessary to restore the landscape of National Parks to its ‘’natural’’ condition. In 1963 the Leopold Report argued that \"A national park should represent a vignette of primitive America\". This policy change eventually led to the restoration of wolves in Yellowstone Park in the 1990s.\n\nHowever, recent research in various disciplines indicates that a pristine natural or \"primitive\" landscape is a myth, and it now realised that people have been changing the natural into a cultural landscape for a long while, and that there are few places untouched in some way from human influence. The earlier conservation policies were now seen as cultural interventions. The idea of what is natural and what artificial or cultural, and how to maintain the natural elements in a landscape, has been further complicated by the discovery of global warming and how it is changing natural landscapes.\n\nAlso important is a reaction recently amongst scholars against dualistic thinking about nature and culture. Maria Kaika comments: \"Nowadays, we are beginning to see nature and culture as intertwined once again – not ontologically separated anymore […].What I used to perceive as a compartmentalized world, consisting of neatly and tightly sealed, autonomous ‘space envelopes’ (the home, the city, and nature) was, in fact, a messy socio-spatial continuum”. And William Cronon argues against the idea of wilderness because it \"involves a dualistic vision in which the human is entirely outside the natural\" and affirms that \"wildness (as opposed to wilderness) can be found anywhere\" even \"in the cracks of a Manhattan sidewalk\". According to Cronon we have to \"abandon the dualism that sees the tree in the garden as artificial […] and the tree in the wilderness as natural […] Both in some ultimate sense are wild.\" Here he bends somewhat the regular dictionary meaning of wild, to emphasise that nothing natural, even in a garden, is fully under human control.\n\nThe landscape of Europe has considerably altered by people and even in an area, like the Cairngorm Mountains of Scotland, with a low population density, only \" the high summits of the Cairngorm Mountains, consist entirely of natural elements. These \"high summits\" are of course only part of the Cairngorms, and there are no longer wolves, bears, wild boar or lynx in Scotland's wilderness. The Scots pine in the form of the Caledonian forest also covered much more of the Scottish landscape than today.\n\nThe Swiss National Park, however, represent a more natural landscape. It was founded in 1914, and is one of the earliest national parks in Europe.\nVisitors are not allowed to leave the motor road, or paths through the park, make fire or camp. The only building within the park is Chamanna Cluozza, mountain hut. It is also forbidden to disturb the animals or the plants, or to take home anything found in the park. Dogs are not allowed. Due to these strict rules, the Swiss National Park is the only park in the Alps who has been categorized by the IUCN as a strict nature reserve, which is the highest protection level.\n\nNo place on the Earth is unaffected by people and their culture. People are part of biodiversity, but human activity affects biodiversity, and this alters the natural landscape. Mankind have altered landscape to such an extent that few places on earth remain pristine, but once free of human influences, the landscape can return to a natural or near natural state.\nEven the remote Yukon and Alaskan wilderness, the bi-national Kluane-Wrangell-St. Elias-Glacier Bay-Tatshenshini-Alsek park system comprising Kluane, Wrangell-St Elias, Glacier Bay and Tatshenshini-Alsek parks, a UNESCO World Heritage Site, is not free from human influence, because the Kluane National Park lies within the traditional territories of the Champagne and Aishihik First Nations and Kluane First Nation who have a long history of living in this region. Through their respective Final Agreements with the Canadian Government, they have made into law their rights to harvest in this region.\n\nCultural forces intentionally or unintentionally, have an influence upon the landscape. Cultural landscapes are places or artifacts created and maintained by people. Examples of cultural intrusions into a landscape are: fences, roads, parking lots, sand pits, buildings, hiking trails, management of plants, including the introduction of invasive species, extraction or removal of plants, management of animals, mining, hunting, natural landscaping, farming and forestry, pollution. Areas that might be confused with a natural landscape include public parks, farms, orchards, artificial lakes and reservoirs, managed forests, golf courses, nature center trails, gardens.\n\n"}
{"id": "18254861", "url": "https://en.wikipedia.org/wiki?curid=18254861", "title": "Naturalization of intentionality", "text": "Naturalization of intentionality\n\nAccording to Franz Brentano, intentionality refers to the \"aboutness of mental states that cannot be a physical relation between a mental state and what is about (its object) because in a physical relation each of the relata must exist whereas the objects of mental states might not.\n\nSeveral problems arise for features of intentionality, which are unusual for materialistic relations. Representation is unique. When 'x represents y' is true, it is not the same as other relations between things, like when 'x is next to y' or when 'x caused y' or when 'x met y', etc. Representation is different because, for instance, when 'x represents y' is true, y need not exist. This isn't true when say 'x is the square root of y' or 'x caused y' or 'x is next to y'. Similarly, when 'x represents y' is true, 'x represents z' can still be false, even when y = z. Intentionality encompasses relations that are both physical and mental. In this case, \"Billy can love Santa and Jane can search for unicorns even if Santa does not exist and there are no unicorns.\"\n\nFranz Brentano, the nineteenth century philosopher, spoke of mental states as involving presentations of the objects of our thoughts. This idea encompasses his belief that one cannot desire something unless they actually have a representation of it in their minds.\n\nDennis Stampe was one of the first philosophers in modern times to suggest a theory of content according to which content is a matter of reliable causes.\n\nFred Dretskes book, \"Knowledge and the Flow of Information\" (1981), was a major influence on the development of informational theories, and although the theory developed there is not a teleological theory, Dretske (1986, 1988, 1991) later produced an informational version of teleosemantics. He begins with a concept of carrying information that he calls \"indicating\", explains that indicating is not equivalent to representing, and then suggests that a representation's content is what it has the function of indicating\n\nTeleosemantics, also known as biosemantics, is used to refer to the class of theories of mental content that use a teleological notion of function. Teleosemantics is best understood as a general strategy for underwriting the normative nature of content, rather than any particular theory. What all teleological theories have in common is the idea that semantic norms are ultimately derivable from functional norms.\n\nAttempts to naturalize semantics began in the late 1970s. Many attempts were and still are being made to bring natural-physical explanations to bear on minds and, specifically, to the question of how minds acquire content. This is an interesting question; it is no surprise that it takes center stage in the philosophy of mind. Indeed, it is certainly an interesting question how minds, thought by those in the natural camp to be \"natural physical objects\", could have developed intentional properties. In the mid-1980s, with the works of Ruth Millikan and David Papineau (Language, Thought, and Other Biological Categories and \"Representation and Explanation\", respectively) teleosemantics, a theory of mental content that attempts to address the question of content and intentionality of minds, was born.\n\nRuth Millikan is perhaps the most vocal supporter of the teleosemantic program. Millikan's view differs from other teleosemantic views in myriad ways, but perhaps its most unusual characteristic is its distinction between the mechanisms that produce mental representations from those that consume mental representations. There is a representational function as a whole, at a composite level; and there are two \"sub-functions\", the producer-function and the consumer-function. In terms that are easy to understand, let's take Millikan's own example of beavers splashing their tails. One beaver alerts other beavers to the presence of danger by splashing its tail on the surface of water. The splashing of the tail tells, or represents to, the other beavers that there is danger in the environment, and the other beavers dip into the water to avoid the danger. The splashing of the beaver's tail produces a representation; the other beavers consume the representation. The representation that the beavers consume guides their behavior in ways that relate to their survival.\n\nOf course, the foci of the teleosemantic program is internal representations, and not just representational states of affairs between two (or more) distinct, external entities. How does the picture of the producer and consumer beavers, for instance, play into a story about internal representations? Papineau and Macdonald describe Millikan's account of this well and loyally, saying \"The producing mechanisms will be the sensory and other cerebral mechanisms that give rise to cognitive representations.\" The consuming mechanisms are those that \"use these representations to direct behavior in pursuit of some biological end\". Here, we have a picture similar to the beaver example, but this picture portrays the two sub-functions, producer and consumer, operating within a more-obviously unified system, namely, the cognitive system. One sub-function produces mental representations while the other sub-function consumes them in order to reach some end, e.g., danger-avoidance or food-acquisition. The representations consumed by the consumer sub-function guide an organism's behavior toward some biological end, e.g., survival. This is a rather brief sketch of Millikan's overall portrait. Of course, more goes into her account of the relation between producer- and consumer-functions in order to arrive at a nuanced account of mental representation. But that is a matter of how. Details as to the how aside, much of Millikan's efforts are directed towards the why, viz., why it is that perceivers like us have mental representations—why representations are produced in the first place.\n\nThe theory of asymmetric dependence, from Fodor, who says that his theory \"distinguishes merely informational relations on the basis of their higher-order relations to each other: informational relations depend upon representational relations, but not vice versa. He gives an example of this theory when he says, \"if tokens of a mental state type are reliably caused by horses, cows-on-dark-nights, zebras-in-the-mist and Great Danes, then they carry information about horses, etc. If however, such tokens are caused by cows-on-dark-nights, etc. because they are caused by horses, but not vice versa, then they represent horses (or property horse).\n\n20th century American philosopher Willard Van Orman Quine believed that linguistic terms do not have distinct meanings that accompany them because there are no such entities as \"meanings\". In his books, \"Word and Object\" (1960) and \"Ontological Relativity\" (1968), Quine considers the methods available to a field linguist attempting to translate an unknown language in order to outline his thesis. His thesis, the indeterminacy of translation, notes that there are many different ways to distribute purpose and meanings among words. Whenever a theory of translation is made it is commonly based upon context. An argument over the correct translation of an unidentified term depends on the possibility that the native could have spoken a different sentence. The same problem of indeterminacy would appear in this argument once again since any hypothesis can be defended if one adopts enough compensatory hypotheses about other parts of the language. Quine uses as an example the word \"gavagai\" spoken by a native upon seeing a rabbit. One can go the simplest route and translate the word to \"Lo, a rabbit\", but other possible translations such as \"Lo, food\" or \"Let's go hunting\" are completely reasonable given what the linguist knows. Subsequent observations can rule out certain possibilities as well as questioning the natives. But this is only possible once the linguist has mastered much of the natives' grammar and vocabulary. This is a big problem because this can only be done on the basis of hypotheses derived from simpler, observation-connected bits of language, which admit multiple interpretations, as we have seen.\n\nDaniel C. Dennett's theory of mental content, the intentional stance, tries to view the behavior of things in terms of mental properties. According to Dennett: \n\"Here is how it works: first you decide to treat the object whose behavior is to be predicted as a rational agent; then you figure out what beliefs that agent ought to have, given its place in the world and its purpose. Then you figure out what desires it ought to have, on the same considerations, and finally you predict that this rational agent will act to further its goals in the light of its beliefs. A little practical reasoning from the chosen set of beliefs and desires will in most instances yield a decision about what the agent ought to do; that is what you predict the agent will do.\"\n\nDennett's thesis has three levels of abstraction:\n\nDennett states that the more concrete the level, the more accurate is in principle our predictions. Though if one chooses to view an object through a more abstract level, he will gain greater computational power by getting a better overall picture of the object and skipping over any extraneous details. Also, switching to a more abstract level has its risks as well as its benefits. If we applied the intentional stance to a thermometer that was heated to 500 °C, trying to understand it through its beliefs about how hot it is and its desire to keep the temperature just right, we would gain no useful information. The problem would not be understood until we dropped down to the physical stance to comprehend that it has been melted. Whether to take a particular stance should be decided by how successful that stance is when applied. Dennett argued that it is best to understand human beliefs and desires at the level of the intentional stance.\n\n\n"}
{"id": "16038926", "url": "https://en.wikipedia.org/wiki?curid=16038926", "title": "Newtonianism", "text": "Newtonianism\n\nNewtonianism is a philosophical and scientific doctrine inspired by the beliefs and methods of natural philosopher Isaac Newton. While Newton's influential contributions were primarily in physics and mathematics, his broad conception of the universe as being governed by rational and understandable laws laid the foundation for many strands of Enlightenment thought. Newtonianism became an influential intellectual program that applied Newton's principles in many avenues of inquiry, laying the groundwork for modern science (both the natural and social sciences), in addition to influencing philosophy, political thought and theology.\n\nNewton's \"Principia Mathematica\", published by the Royal Society in 1687 but not available widely and in English until after his death, is the text generally cited as revolutionary or otherwise radical in the development of science. The three books of \"Principia\", considered a seminal text in mathematics and physics, are notable for their rejection of hypotheses in favor of inductive and deductive reasoning based on a set of definitions and axioms. This method may be contrasted to the Cartesian method of deduction based on sequential logical reasoning, and showed the efficacy of applying mathematical analysis as a means of making discoveries about the natural world.\n\nNewton's other seminal work was \"Opticks\", printed in 1704 in \"Philosophical Transactions of the Royal Society\", of which he became president in 1703. The treatise, which features his now famous work on the composition and dispersion of sunlight, is often cited as an example of how to analyze difficult questions via quantitative experimentation. Even so, the work was not considered revolutionary in Newton's time. One hundred years later, however, Thomas Young would describe Newton's observations in \"Opticks\" as \"yet unrivalled... they only rise in our estimation as we compare them with later attempts to improve on them.\"\n\nThe first edition of \"Principia\" features proposals about the movements of celestial bodies which Newton initially calls \"hypotheses\"—however, by the second edition, the word \"hypothesis\" was replaced by the word \"rule\", and Newton had added to the footnotes the following statement:... I frame no hypotheses. For whatever is not deduced from the phenomena is to be called a hypothesis; and hypotheses, whether metaphysical or physical, whether of occult qualities or mechanical, have no place in experimental philosophy.Newton's work and the philosophy that enshrines it are based on mathematical empiricism, which is the idea that mathematical and physical laws may be revealed in the real world via experimentation and observation. It is important to note, however, that Newton's empiricism is balanced against an adherence to an exact mathematical system, and that in many cases the \"observed phenomena\" upon which Newton built his theories were actually based on mathematical models, which were representative but not identical to the natural phenomena they described.\n\nNewtonian doctrine can be contrasted with several alternative sets of principles and methods such as Cartesianism, Leibnizianism and Wolffianism.\n\nDespite his reputation for empiricism in historical and scientific circles, Newton was deeply religious and believed in the literal truth of Scripture, taking the story of Genesis to be Moses' eyewitness account of the creation of the solar system. Newton reconciled his beliefs by adopting the idea that the Christian God set in place at the beginning of time the \"mechanical\" laws of nature, but retained the power to enter and alter that mechanism at any time.\n\nNewton further believed that the preservation of nature was in itself an act of God, stating that \"a continual miracle is needed to prevent the Sun and fixed stars from rushing together through Gravity\".\n\nBetween 1726 and 1729, French author, philosopher, and historian Voltaire was exiled in England, where he met several significant English scholars and devotees to the Newtonian system of thought. Voltaire would later bring these ideas back to France with his publication of \"Lettres Philosophiques\" and \"Philosophie de Newton\", which popularized Newton's intellectual practices and general philosophy. Later, prominent natural philosopher and friend of Voltaire, Émilie du Châtelet, would publish a French translation of \"Principia\", which met with great success in France.\n\nWhile Newton was opposed by some members of the religious community for his non-Trinitarian beliefs about God, others believed science itself to be a philosophical exercise, that if done correctly, would lead its practitioners to a greater knowledge and appreciation of God.\n\nIn 1737, Italian scholar Count Frencesco Algarotti published a book entitled \"Newtonianismo per le dame overro dialoghi sopre la luce e i colori\", which aimed to introduce female audiences to the work of Newton. The text explained the principles of Newton's \"Opticks\" while avoiding much of the mathematical rigor of the work in favor of a more \"agreeable\" text. The book was later published with a title that made no reference to women, leading some to believe that the female branding of the book was a ploy to avoid censorship.\n\nScottish philosopher David Hume, likely inspired by the methods of analysis and synthesis which Newton developed in \"Opticks\", was a strong adherent of Newtonian empiricism in his studies of moral phenomena.\n\nNewton and his philosophy of Newtonianism arguably led to the popularization of science in Europe—particularly in England, France, and Germany—catalyzing the Age of Enlightenment.\n"}
{"id": "26999670", "url": "https://en.wikipedia.org/wiki?curid=26999670", "title": "Postbiological evolution", "text": "Postbiological evolution\n\nPostbiological evolution is a form of evolution which has transitioned from a biological paradigm, driven by the propagation of genes, to a nonbiological (e.g., cultural or technological) paradigm, presumably driven by some alternative replicator (e.g., memes or temes), and potentially resulting in the extinction, obsolescence, or trophic reorganization of the former. Researchers anticipating a postbiological universe tend to describe this transition as marked by the maturation and potential convergence of high technologies, such as artificial intelligence or nanotechnology.\n\nThe dictionary definition of Evolution is any process of formation, growth or development. In biological evolution the main principle behind this development is survival, we evolved to become stronger and quicker, we also evolved to become intelligent. But as we became intelligent biological evolution subsided to a new concept, cultural evolution. Cultural evolution moves at a much faster rate than biological evolution and this is one reason why it isn't very well understood. But as survival is still the main driving force behind life and that intelligence and knowledge is currently the most important factor for that survival, we can reasonably assume that cultural evolution will progress in the direction of furthering intelligence and knowledge.\n\nCultural evolution progressing in this way and being based upon the furthering of intelligence is known as the Intelligence Principle; this was suggested by Dr Steven J Dick.\n\n\"The maintenance, improvement and perpetuation of knowledge and intelligence is the central driving force of cultural evolution, and that to the extent intelligence can be improved, it will be improved\" (Dick 1996)\n\nIf cultural evolution progresses in this direction then due to cultural evolution being much faster than biological, the limiting factor becomes our biology and the capability of our brains. Currently the closest and so most probable solution to this problem is artificial intelligence, (AI). Experts in AI even believe it holds the potential and capability for a postbiological earth in the next several generations, (Moravec 1988, 1999). AI could be utilised to solve scientific problems and to analyse situations much faster and more accurately than our own minds.\n\nThe move to a complete postbiological stage has two different routes. One route is the change of human consciousness from a biological vessel into a mechanical; this would require the digitisation of human consciousness. A mechanical based vessel would increase the computational power and intelligence of the human consciousness exponentially, and also eliminate the weakness of a biological form. This route is therefore a logical progression through cultural evolution with survival and the pursuit of knowledge and intelligence at its centre.\n\nThe first route requires a high level of technology, therefore would take a long time, this results in another possible road to a completely postbiological civilisation (PBC). The other route is the complete replacement of human consciousness by AI, for this the human race would co-exist peacefully with our own creation of AI which is scientific, objective, and free from selfish human nature. \n\nThe future of the human race through cultural evolution is not known and the possible postbiological outcomes are infinite, so to address what we could evolve into is almost futile. But Hans Moravec predicted that;\n\n\"What awaits us is not oblivion but rather a future which, from our present vantage point, is best described as 'postbiological' or even 'supernatural'. It is a world swept away by the tide of cultural change, usurped by its own artificial progeny\"\n\nThe possible forms a PBC may take are as diverse as in biological evolution, if not more. But from our knowledge of technology and with the intelligence principle being the main driving force we may make some predictions.\n\nThe current major limitations imposed upon computation are limited storage space, processing power, dust gathering chips, inefficiency of their human operators and heat dispersion. The only one that is fundamental and fixed is heat dispersion because this is due to the laws of physics. In computation the greater the amount of information to be calculated, (I) the greater the energy needed (E), but the energy needed is also proportional to another factor, the temperature, (T).\n\nE=KIT\n\nWhere K is a constant. Therefore, the greater the temperature the greater the energy needed, and so the greater the inefficiency is also. If we now apply the Intelligence principle to this then a PBC would move to decrease the temperature and so increase the efficiency and computational power.\nIn the universe the greatest source of heat transfer is via radiation, therefore a PBC would look to migrate to an area of low radiation and so low temperature. If we now observe the galaxy we see that the most radiation is generated by the galactic centre by both the high stellar density and also highly energetic events such as supernova. Therefore, the coldest regions are away from the galactic centre or inside giant molecular clouds. Giant molecular clouds although being very low in temperature (T~10K) are areas of giant star formation and so the temperature in one location is irregular, which would make it unsuitable for a PBC.\n\nAnother factor affecting a PBC would be the abundance of metals and heavier elements needed for expansion and repair. The highest concentration of these elements is found near the galactic centre, where they are created by massive stars. But to a PBC with advanced technology the production of metals via stellar nucleosynthesis in stars is highly inefficient, converting only a small amount of hydrogen to heavier nuclei and the high loss of energy that is produced in the nuclear fusion. Therefore, a PBC would most likely have the capability to produce heavier nuclei through controlled fusion and minimise the energy lost.\n\nBy taking the two factors of heat dispersion and heavy nuclei into account we can find a \"galactic technological zone\" (GTZ), similar to the principle of a \"galactic habitable zone\" (GHZ) for biological life. Where temperatures are low enough to maximise computing efficiency but there is also matter available for fusion, this most likely lies on the outskirts of the galaxy.\n\nA migration hypothesis exists that takes the GTZ into account. A PBC would most likely not think on a similar time scale to us, therefore although a migration to GTZ may seem inefficient and lengthy to us, a PBC could consider this on timescales of 10^6 years, where the increased computing efficiency received far outweighs the energy required in transportation. The idea of interstellar migrations already exists in literature, (e.g. Badescu and Cathcart 2000).\n\nIn the search for extraterrestrial intelligence (SETI) the main focus is on biological life. But the timescale of intelligent biological life could be very short; already some experts believe that we could see a postbiological earth in the next few generations. According to Steven J. Dick, for a PBC to arise other than our own and be present, we must make five assumptions:\n\n\nWe know that assumptions 1, 3, 4, and 5 can take place as we have observed or are observing them on the Earth. For assumption 2 we must consider the L term of the Drake equation, and the timescale over which intelligent biological life can form. Around 1 Billion years after the start of the universe the first sun-like star had formed, and there were enough heavy elements around for planet formation (1998, Larson and Bromm 2001). From the earth we know that intelligent life can form within 5 billion years, this puts a lower time scale on which intelligent life can form, 6 billion years. And from the current rate of technological progression the leap from intelligent life to a PBC is negligible compared to the astronomical timescale. This means we could already be looking at a postbiological universe. In our own galaxy the first sun-like stars formed at around 4 billion years therefore we could already have a PBC in our galaxy that formed 3-4 billion years ago.\n\nIf we consider this possibility of a PBC in our galaxy we are still faced with Fermi's paradox. However many of the proposed solutions for Fermi's paradox also hold true for a PBC. In terms of the search for extraterrestrial life and astrobiology because of the almost infinite possible forms a PBC could take and our lack of understanding of these we would effectively be blind in this search. For this reason even though there is a logical argument for the existence of PBCs our best hopes remain with looking for biological life.\n\nWhile in some circles the expression \"postbiological evolution\" is roughly synonymous with human genetic engineering, it is used most often to refer to the general application of the convergence of nanotechnology, biotechnology, information technology, and cognitive science (NBIC) to improve human performance.\n\nSince the 1990s, several academics (such as some of the fellows of the Institute for Ethics and Emerging Technologies) have risen to become cogent advocates of the case for human enhancement while other academics (such as the members of President Bush's Council on Bioethics) have become its most outspoken critics.\n\nAdvocacy of the case for human enhancement is increasingly becoming synonymous with \"transhumanism\", a controversial ideology and movement which has emerged to support the recognition and protection of the right of citizens to either maintain or modify their own minds and bodies; so as to guarantee them the freedom of choice and informed consent of using human enhancement technologies on themselves and their children.\n\nNeuromarketing consultant Zack Lynch argues that neurotechnologies will have a more immediate effect on society than gene therapy and will face less resistance as a pathway of radical human enhancement. He also argues that the concept of \"enablement\" needs to be added to the debate over \"therapy\" versus \"enhancement\".\n\nAlthough many proposals of human enhancement rely on fringe science, the very notion and prospect of human enhancement has sparked public controversy.\n\nMany critics argue that \"human enhancement\" is a loaded term which has eugenic overtones because it may imply the improvement of human hereditary traits to attain a universally accepted norm of biological fitness (at the possible expense of human biodiversity and neurodiversity), and therefore can evoke negative reactions far beyond the specific meaning of the term. Furthermore, they conclude that enhancements which are self-evidently good, like \"fewer diseases\", are more the exception than the norm and even these may involve ethical tradeoffs, as the controversy about ADHD arguably demonstrates.\n\nHowever, the most common criticism of human enhancement is that it is or will often be practiced with a reckless and selfish short-term perspective that is ignorant of the long-term consequences on individuals and the rest of society, such as the fear that some enhancements will create unfair physical or mental advantages to those who can and will use them, or unequal access to such enhancements can and will further the gulf between the \"haves\" and \"have-nots\".\n\nAccordingly, some advocates, who want to use more neutral language, and advance the public interest in so-called \"human enhancement technologies\", prefer the term \"enablement\" over \"enhancement\"; defend and promote rigorous, independent safety testing of enabling technologies; as well as affordable, universal access to these technologies.\n\n\n\n"}
{"id": "513965", "url": "https://en.wikipedia.org/wiki?curid=513965", "title": "Rammed earth", "text": "Rammed earth\n\nRammed earth, also known as taipa in Portuguese, tapial or tapia in Spanish, pisé (de terre) in French, and hangtu (), is a technique for constructing foundations, floors, and walls using natural raw materials such as earth, chalk, lime, or gravel. It is an ancient method that has been revived recently as a sustainable building material used in a technique of natural building.\n\nRammed earth is simple to manufacture, non-combustible, thermally massive, strong, and durable. However, structures such as walls can be laborious to construct of rammed earth without machinery, e. g., powered tampers, and they are susceptible to water damage if inadequately protected or maintained.\n\nEdifices formed of rammed earth are on every continent except Antarctica, in a range of environments including temperate, wet, semiarid desert, montane, and tropical regions. The availability of suitable soil and a building design appropriate for local climatic conditions are the factors that favour its use.\n\nManufacturing rammed earth involves compressing a damp mixture of earth that has suitable proportions of sand, gravel, clay, and/or an added stabilizer into an externally supported frame or mold, forming either a solid wall or individual blocks. Historically, additives such as lime or animal blood were used to stabilize it, while modern construction adds lime, cement, or asphalt emulsions. To add variety, some modern builders also add coloured oxides or other materials, e.g. bottles, tires, or pieces of timber.\n\nThe construction of an entire wall begins with a temporary frame, denominated the \"formwork\", which is usually made of wood or plywood, as a mold for the desired shape and dimensions of each section of wall. The form must be durable and well braced, and the two opposing faces must be clamped together to prevent bulging or deformation caused by the large compressing forces. Damp material is poured into the formwork to a depth of and then compacted to approximately 50% of its original height. The material is compressed iteratively, in batches or courses, so as to gradually erect the wall up to the top of the formwork. Tamping was historically manual with a long ramming pole, and was very laborious, but modern construction can be made less so by employing pneumatically powered tampers.\n\nAfter a wall is complete, it is sufficiently strong to immediately remove the formwork. This is necessary if a surface texture is to be applied, e.g., by wire brushing, carving, or mold impression, because the walls become too hard to work after approximately one hour. Construction is optimally done in warm weather so that the walls can dry and harden. The compression strength of the rammed earth increases as it cures; some time is necessary for it to dry and as long as two years can be necessary for complete curing. Exposed walls must be sealed to prevent water damage.\n\nIn modern variations of the technique, rammed-earth walls are constructed on top of conventional footings or a reinforced concrete slab base.\n\nWhere blocks made of rammed earth are used, they are generally stacked like regular blocks and are bonded together with a thin mud slurry instead of cement. Special machines, usually powered by small engines and often portable, are used to compress the material into blocks.\n\nPresently more than 30% of the world's population uses earth as a building material. Rammed earth has been used globally in a wide range of climatic conditions. Rammed-earth housing may resolve homelessness caused by otherwise expensive construction techniques.\n\nThe compressive strength of rammed earth is a maximum of . This is less than that of concrete but more than sufficiently strong for domestic edifices. Indeed, properly constructed rammed earth endures for thousands of years, as many ancient structures that are still standing around the world demonstrate. Rammed earth reinforced with rebar, wood, or bamboo can prevent collapse caused by earthquakes or heavy storms, because unreinforced edifices of rammed earth resist earthquake damage extremely poorly. See 1960 Agadir earthquake for an example of the total destruction which may be inflicted on such structures by an earthquake. Adding cement to soil mixtures poor in clay can also increase the load-bearing capacity of rammed-earth edifices. The United States Department of Agriculture observed in 1925 that rammed-earth structures endure indefinitely and can be constructed for less than two-thirds of the cost of standard frame houses.\n\nSoil is a widely available, inexpensive, and sustainable resource. Therefore, construction with rammed earth is very viable. Unskilled labour can do most of the necessary work. While the cost of rammed earth is low, rammed-earth construction without mechanical tools is very time-consuming and laborious; however, with a mechanical tamper and prefabricated formwork it can require only two or three days to construct the walls of a house.\n\nOne significant benefit of rammed earth is its high thermal mass: like brick or concrete, it can absorb heat during daytime and nocturnally release it. This action moderates daily temperature variations and reduces the need for air conditioning and heating. In colder climates, rammed-earth walls can be insulated with Styrofoam or a similar insert. It must also be protected from heavy rain and insulated with vapour barriers.\n\nRammed earth can effectively regulate humidity if unclad walls containing clay are exposed to an internal space. Humidity is regulated between 40% and 60%, which is the ideal range for asthma sufferers and for the storage of susceptible objects such as books. The material mass and clay content of rammed earth allows an edifice to breathe more than concrete edifices, which avoids problems of condensation but prevents significant loss of heat.\n\nUntouched, rammed-earth walls have the colour and texture of natural earth. Moisture-impermeable finishes, such as cement render, are avoided because they impair the ability of a wall to desorb moisture, which quality is necessary to preserve its strength. Well-cured walls accept nails and screws easily, and can be effectively patched with the same material used to build them. Blemishes can be repaired using the soil mixture as a plaster and sanded smooth.\n\nThe thickness, typically , and density of rammed-earth walls make them suitable for soundproofing. They are also inherently fireproof, resistant to termite damage, and non-toxic.\n\nEdifices of rammed earth are thought to be more sustainable and environmentally friendly than popular techniques of construction. Because rammed-earth edifices use locally available materials, they usually have low embodied energy and generate very little waste. The soils used are typically subsoils low in clay, between 5% and 15%, which conserve the topsoil for agriculture. When the soil excavated in preparation for a foundation can be used, the cost and energy consumption of transportation are minimal. Rammed earth is probably the least environmentally detrimental construction material and technique that is readily and commercially available today to construct solid masonry edifices. Rammed earth has potentially low manufacturing impact, contingent on the amount of cement and the amount that is locally sourced; it is often quarried aggregates rather than \"earth\".\n\nFormwork is removable and can be reused, reducing the need for lumber.\nMixing cement with the soil can counteract sustainable benefits such as low embodied energy and humidity regulation because manufacture of the cement itself adds to the global carbon dioxide burden at a rate of 1.25 tonnes per tonne of cement produced. Partial substitution of cement with alternatives such as ground granulated blast furnace slag has not been demonstrated to be effective, and implicates other questions of sustainability.\n\nRammed earth can contribute to the overall energy efficiency of edifices: the density, thickness, and thermal conductivity of rammed earth render it an especially suitable material for passive solar heating. Warmth requires almost 12 hours to be conducted through a wall thick.\n\nRammed-earth construction may also reduce the ecological impacts of deforestation and the toxicity of artificial materials associated with conventional construction techniques.\n\nAlthough it has low greenhouse emissions in theory, transportation and the production of cement can add significantly to the overall emissions of modern rammed earth construction. The most basic kind of traditional rammed earth has very low greenhouse gas emissions but the more engineered and processed variant of rammed earth has the potential for significant emissions.\n\nEvidence of ancient use of rammed earth has been found in Neolithic archaeological sites of the Yangshao and Longshan cultures along the Yellow River in China, dating to 5000 BCE. By 2000 BCE, rammed-earth architectural techniques (夯土 \"Hāng tǔ\") were commonly used for walls and foundations in China.\n\nIn the 1800s, rammed earth was popularized in the United States by the book \"Rural Economy\" by S. W. Johnson. The technique was used to construct the Borough House Plantation and the Church of the Holy Cross in Stateburg, South Carolina, both being National Historic Landmarks.\nAn outstanding example of a rammed-earth edifice in Canada is St. Thomas Anglican Church in Shanty Bay, Ontario, erected between 1838 and 1841.\n\nFrom the 1920s through the 1940s rammed-earth construction in the US was studied. South Dakota State College extensively researched and constructed almost one hundred weathering walls of rammed earth. For over 30 years the college investigated the use of paints and plasters in relation to colloids in soil. In 1945, Clemson Agricultural College of South Carolina published the results of their research of rammed earth in a pamphlet titled \"Rammed Earth Building Construction\". In 1936, on a homestead near Gardendale, Alabama, the United States Department of Agriculture constructed an experimental community of rammed-earth edifices with architect Thomas Hibben. The houses were inexpensively constructed and were sold to the public along with sufficient land for gardens and small plots for livestock. The project successfully provided valuable homes to low-income families.\n\nThe US Agency for International Development is working with undeveloped countries to improve the engineering of rammed-earth houses. It also financed the authorship of the \"Handbook of Rammed Earth\" by Texas A&M University and the Texas Transportation Institute. The \"Handbook\" was unavailable for purchase by the public until the Rammed Earth Institute International gained permission to reprint it.\n\nInterest in rammed earth declined after World War II when the cost of modern construction materials decreased. Rammed earth was considered substandard, and still is opposed by many contractors, engineers, and tradesmen who are unfamiliar with earthen construction techniques. The prevailing perception that such materials and techniques perform poorly in regions prone to earthquakes has prevented their use in much of the world. In Chile, for example, rammed earth edifices normally cannot be conventionally insured against damage or even be approved by the government.\n\nA notable example of 21st century use of rammed earth is the façade of the Nk'Mip Desert Cultural Centre in southern British Columbia, Canada. As of 2014 it is the largest rammed earth wall in North America.\n\n\nRammed earth wall construction at Central Arizona College\n"}
{"id": "195193", "url": "https://en.wikipedia.org/wiki?curid=195193", "title": "Sky", "text": "Sky\n\nThe sky (or celestial dome) is everything that lies above the surface of the Earth, including the atmosphere and outer space.\n\nIn the field of astronomy, the sky is also called the celestial sphere. This is viewed from Earth's surface as an abstract dome on which the Sun, stars, planets, and Moon appear to be traveling. The celestial sphere is conventionally divided into designated areas called constellations. Usually, the term \"sky\" is used informally as the point of view from the Earth's surface; however, the meaning and usage can vary. In some cases, such as in discussing the weather, the sky refers to only the lower, more dense portions of the atmosphere.\n\nDuring daylight, the sky appears to be blue because air scatters more blue sunlight than red. At night, the sky appears to be a mostly dark surface or region spangled with stars. During the day, the Sun can be seen in the sky unless obscured by clouds. In the night sky (and to some extent during the day) the Moon, planets and stars are visible in the sky. Some of the natural phenomena seen in the sky are clouds, rainbows, and aurorae. Lightning and precipitation can also be seen in the sky during storms. Birds, insects, aircraft, and kites are often considered to fly in the sky. Due to human activities, smog during the day and light pollution during the night are often seen above large cities.\n\nExcept for light that comes directly from the sun, most of the light in the day sky is caused by scattering, which is dominated by a small-particle limit called Rayleigh Scattering. The scattering due to molecule sized particles (as in air) is greater in the forward and backward directions than it is in the lateral direction. Scattering is significant for light at all visible wavelengths but is stronger at the shorter (bluer) end of the visible spectrum, meaning that the scattered light is bluer than its source, the sun. The remaining sunlight, having lost some of its short wavelength components, appears slightly less blue.\n\nScattering also occurs even more strongly in clouds. Individual water droplets exposed to white light will create a set of colored rings. If a cloud is thick enough, scattering from multiple water droplets will wash out the set of colored rings and create a washed-out white color.\n\nThe sky can turn a multitude of colors such as red, orange, purple and yellow (especially near sunset or sunrise) when the light must pass through a much longer path (or optical depth) through the atmosphere. Scattering effects also partially polarize light from the sky and are most pronounced at an angle 90° from the sun. Scattered light from the horizon travels through as much as 38 times the atmosphere as does light from the zenith, causing a blue gradient: vivid at the zenith, and pale near the horizon. Because red light also scatters if there is enough air between the source and the observer causing parts of the sky to change color during a sunset. As the amount of atmosphere nears infinity, the scattered light appears whiter and whiter.\n\nThe sun is not the only object that may appear less blue in the atmosphere. Far away clouds or snowy mountaintops may appear yellowish. The effect is not very obvious on clear days but is very pronounced when clouds cover the line of sight, reducing the blue hue from scattered sunlight. At higher altitudes, the sky tends toward darker colors since scattering is reduced due to lower air density; an extreme example is the moon, where there is no atmosphere and no scattering, making the sky on the moon black even when the sun is visible.\n\nSky luminance distribution models have been recommended by the International Commission on Illumination (CIE) for the design of daylighting schemes. Recent developments relate to “all sky models” for modelling sky luminance under weather conditions ranging from clear to overcast.\n\nThe intensity of the sky varies greatly over the day, and the primary cause of that intensity differs as well. When the sun is well above the horizon, direct scattering of sunlight (Rayleigh scattering) is the overwhelmingly dominant source of light. However, in twilight, the period of time between sunset and night and between night and sunrise, the situation is more complicated. Green flashes and green rays are optical phenomena that occur shortly after sunset or before sunrise, when a green spot is visible above the sun, usually for no more than a second or two, or it may resemble a green ray shooting up from the sunset point. Green flashes are a group of phenomena that stem from different causes, most of which occur when there is a temperature inversion (when the temperature increases with altitude rather than the normal decrease in temperature with altitude). Green flashes may be observed from any altitude (even from an aircraft). They are usually seen at an unobstructed horizon, such as over the ocean, but are also seen over cloud tops and mountain tops. Green flashes may also be observed at the horizon in association with the Moon and bright planets, including Venus and Jupiter.\n\nThe Earth's shadow is the shadow that the Earth casts on its atmosphere. This atmospheric phenomenon is sometimes seen twice a day, around the times of sunset and sunrise. When the weather conditions and the observer's viewing point permit a clear sight of the horizon, the shadow can be seen as a dark blue or greyish-blue band. Assuming the sky is clear, the Earth's shadow is visible in the half of the sky opposite to the sunset or sunrise, and is seen as a dark blue band right above the horizon. A related phenomenon is the \"Belt of Venus\" or \"anti-twilight arch\", a pink band that is visible above the dark blue band of the Earth's shadow in the same part of the sky. There is no clear dividing line between the Earth's shadow and the Belt of Venus: one colored band shades into the other in the sky.\n\nTwilight is divided into three segments according to how far the sun is below the horizon, measured in segments of 6°. After sunset the civil twilight sets in; it ends when the sun drops more than 6° below the horizon. This is followed by the nautical twilight, when the sun is 6° and 12° below the horizon (heights of between −6° and −12°), after which comes the astronomical twilight, defined as the period from −12° to −18°. When the sun drops more than 18° below the horizon, the sky generally attains its minimum brightness.\n\nSeveral sources can be identified as the source of the intrinsic brightness of the sky, namely airglow, indirect scattering of sunlight, scattering of starlight, and artificial light pollution.\n\nThe term night sky refers to the sky as seen at night. The term is usually associated with skygazing and astronomy, with reference to views of celestial bodies such as stars, the Moon, and planets that become visible on a clear night after the Sun has set. Natural light sources in a night sky include moonlight, starlight, and airglow, depending on location and timing. The fact that the sky is not completely dark at night can be easily observed. Were the sky (in the absence of moon and city lights) absolutely dark, one would not be able to see the silhouette of an object against the sky.\n\nThe night sky and studies of it have a historical place in both ancient and modern cultures. In the past, for instance, farmers have used the state of the night sky as a calendar to determine when to plant crops. The ancient belief in astrology is generally based on the belief that relationships between heavenly bodies influence or convey information about events on Earth. The \"scientific\" study of the night sky and bodies observed within it, meanwhile, takes place in the science of astronomy.\n\nWithin visible-light astronomy, the visibility of celestial objects in the night sky is affected by light pollution. The presence of the Moon in the night sky has historically hindered astronomical observation by increasing the amount of ambient lighting. With the advent of artificial light sources, however, light pollution has been a growing problem for viewing the night sky. Special filters and modifications to light fixtures can help to alleviate this problem, but for the best views, both professional and amateur optical astronomers seek viewing sites located far from major urban areas.\n\nAlong with pressure tendency, the condition of the sky is one of the more important parameters used to forecast weather in mountainous areas. Thickening of cloud cover or the invasion of a higher cloud deck is indicative of rain in the near future. At night, high thin cirrostratus clouds can lead to halos around the moon, which indicate the approach of a warm front and its associated rain. Morning fog portends fair conditions and can be associated with a marine layer, an indication of a stable atmosphere. Rainy conditions are preceded by wind or clouds which prevent fog formation. The approach of a line of thunderstorms could indicate the approach of a cold front. Cloud-free skies are indicative of fair weather for the near future. The use of sky cover in weather prediction has led to various weather lore over the centuries.\n\nWithin 36 hours of the passage of a tropical cyclone's center, the pressure begins to fall and a veil of white cirrus clouds approaches from the cyclone's direction. Within 24 hours of the closest approach to the center, low clouds begin to move in, also known as the bar of a tropical cyclone, as the barometric pressure begins to fall more rapidly and the winds begin to increase. Within 18 hours of the center's approach, squally weather is common, with sudden increases in wind accompanied by rain showers or thunderstorms. Within six hours of the center's arrival, rain becomes continuous. Within an hour of the center, the rain becomes very heavy and the highest winds within the tropical cyclone are experienced. When the center arrives with a strong tropical cyclone, weather conditions improve and the sun becomes visible as the eye moves overhead. Once the system departs, winds reverse and, along with the rain, suddenly increase. One day after the center's passage, the low overcast is replaced with a higher overcast, and the rain becomes intermittent. By 36 hours after the center's passage, the high overcast breaks and the pressure begins to level off.\n\nFlight is the process by which an object moves, through or beyond the sky (as in the case of spaceflight), by generating aerodynamic lift, propulsive thrust, aerostatically using buoyancy, or by ballistic movement, without any direct mechanical support from the ground. The engineering aspects of flight are studied in aerospace engineering which is subdivided into aeronautics, which is the study of vehicles that travel through the air, and astronautics, the study of vehicles that travel through space, and in ballistics, the study of the flight of projectiles. While human beings have been capable of flight via hot air balloons since 1783, other species have used flight for significantly longer. Animals, such as birds, bats, and insects are capable of flight. Spores and seeds from plants use flight, via use of the wind, as a method of propagating their species.\n\nMany mythologies have deities especially associated with the sky. In Egyptian religion, the sky was deified as the goddess Nut and as the god Horus. Dyeus is reconstructed as the god of the sky, or the sky personified, in Proto-Indo-European religion, whence Zeus, the god of the sky and thunder in Greek mythology and the Roman god of sky and thunder Jupiter.\n\nIn Australian Aboriginal mythology, Altjira (or Arrernte) is the main sky god and also the creator god. In Iroquois mythology, Atahensic was a sky goddess who fell down to the ground during the creation of the Earth. Many cultures have drawn constellations between stars in the sky, using them in association with legends and mythology about their deities.\n\n\n"}
{"id": "1576445", "url": "https://en.wikipedia.org/wiki?curid=1576445", "title": "Sociological naturalism", "text": "Sociological naturalism\n\nSociological naturalism is a theory that states that the natural world and social world are roughly identical and governed by similar principles. Sociological naturalism, in sociological texts simply referred to as naturalism, can be traced back to the philosophical thinking of Auguste Comte in the 19th century, closely connected to positivism, which advocates use of the scientific method of the natural sciences in studying social sciences. It should not be identified too closely with Positivism, however, since whilst the latter advocates the use of controlled situations like experiments as sources of scientific information, naturalism insists that social processes should only be studied in their \"natural\" setting. A similar form of naturalism was applied to the scientific study of art and literature by Hippolyte Taine (see Race, milieu, and moment).\n\nContemporary sociologists do not generally dispute that social phenomena take place within the natural universe and, as such, are subject to natural constraints, such as the laws of physics. Up for debate is the nature of the distinctiveness of social phenomena as a subset of natural phenomena. Broad support exists for the antipositivist claim that crucial qualitative differences mean that one cannot explain social phenomena effectively using investigative tools or even standards of validity derived from other natural sciences. From this point of view, naturalism does not imply scientism. \n\nHowever, a classically positivist conflation of naturalism with scientism has not disappeared; this view is still dominant in some old and prestigious schools, such as the sociology departments at the University of Chicago in the United States, and McGill University in Montréal, Canada.\n\nMore recently, actor-network theory has analyzed the social construction of the nature/society distinction itself.\n\n"}
{"id": "41301792", "url": "https://en.wikipedia.org/wiki?curid=41301792", "title": "Space for Life", "text": "Space for Life\n\nSpace for Life () is a museum district in Montreal, Quebec, Canada. It brings together the city's four most prominent natural museums: the Montreal Biodome and the Rio Tinto Alcan Planetarium, situated in Montreal's Olympic Park, and the Montreal Botanical Garden and Montreal Insectarium, in the adjacent Maisonneuve Park.\n\nSpace for Life was established in 2011 as a successor body to Montreal Nature Museums. It describes itself as the largest natural sciences complex in Canada. As of 2013, its executive director is Charles-Mathieu Brunelle and Montreal executive committee member Manon Gauthier is responsible for its political oversight.\n\nThe Montreal Biodome, Insectarium, Botanical Garden and Rio Tinto Alcan Planetarium invite us to rethink the ties between human beings and nature, cultivating a new way of living. Together, the four prestigious institutions form a place where nature and science are honoured. They have positioned themselves as a Space for Life and are dedicated to sharing their vast heritage and knowledge with you.\n\nSpace for Life is a place that brings together the Montreal Biodome, Insectarium, Botanical Garden and Planetarium, but it is also much more. It’s a participatory movement and a commitment to biodiversity. It is a vast project based on citizen participation and co-creation with visitors. Just like nature belongs to everyone, it is everyone’s movement. It’s a state of mind, a way of experiencing nature. It is a space we visit where we can exchange, collaborate and learn.\n\nThrough its efforts in communication, conservation, education and research, Space for Life guides humans to better experience nature.\n\nThe Space for Life comprises the Montreal Biodome,Botanical Garden, Insectarium and Rio Tinto Alcan Planetarium. The institutions are interdependent, and designed to inspire visitors to adopt a new way of experiencing nature. They are connected by the Grande Place, a space that inspires new ways of coming together, enjoying the site, playing outside, building, interacting and experiencing everyday life.\n\nSpace for Life is the largest natural science museum complex in Canada, one of the leading tourist sites in Montréal and all of Quebec and a place with immense potential to impress and thrill visitors through nature, explain nature and encourage behaviour that is respectful of nature.\n\nSpace for Life is also committed to increasing awareness of our planet’s biodiversity and encouraging people to better protect it. In fact, our four institutions have created a sustainable development charter.\n\nBy offering visitors immersive experiences combining science and emotion, the Biodôme, Botanical Garden, Insectarium and Planetarium invite us all to look at nature differently.\n\nWe have a collective commitment to nature, but also a commitment to achievement, meaningfulness and mobilization.\n\nWith their participatory, unifying approach that is authentic, inventive, committed and open to the world, our four institutions have joined forces to create a movement, a Space for Life; a place where people come together to create, shaped by Montrealers and visitors from around the world.\n\nSpace for Life has initiated a movement that aims to help people better understand the concept of interdependence underlying biodiversity, become aware of the services provided by nature and gradually change the way they live. Our institutions invite you to join the movement, by taking an active part and spreading the message yourself.\n\nThe Space for Life is a collective initiative encouraging Montrealers and local stakeholders to become involved and make it their own.\n\nA movement to get closer to nature\nA participatory, creative, historic movement\n\nAt Space for Life, our approach to sustainable development informs all our decisions and actions, encouraging us to consider the inextricable connections between society, ethics, the economy and the environment.\n\nWith this goal in mind, we are committed to integrating sustainable development principles in all our activities. This includes our charter of commitment and 11 areas of focus:\n\n\nMontréal Space for Life is the largest natural science museum complex in Canada. By 2019, four major projects will have been developed, creating constantly evolving and changing spaces for life.\n\nThe Insectarium Metamorphosis, the Biodôme Migration and two other major Space for Life projects are a legacy for Montrealers, for the planet and for future generations.\n\nAt a time when the issues the planet is facing, especially those related to the loss of biodiversity, raise the question of the relevance of our modern lifestyles, Space for Life, which is rooted in these unique institutions whose reputation and credibility are recognized both locally and internationally, has a fundamental role to play.\n\nThese two projects – characterized by their uniqueness, both in terms of architecture and design and the memorable and distinctive experiences they offer visitors – will let Space for Life truly play its role by inviting citizens to reconnect with nature and invent new ways of living. Reflecting a multidisciplinary vision wherein the architectural gesture emerges from a global creative approach, a bold architectural design will create living spaces that are permeable, ecological and evolving, while meeting the highest green building standards.\n\nKuehn Malvezzi, Pelletier De Fontenay, Jodoin Lamarre Pratte, Dupras Ledoux and NCK\n\nA unique experience\n\nThe Insectarium will become a true biotope in which insects, plants and people can interact and take an interest in one another. In an architectural space that is both a landscape and an organism, the underground and closed spaces, water, shadow and daylight follow and play off one another along a route that plunges visitors into an immersive, sensory experience in the heart of the world of insects.\n\n\nKANVA + NEUF architect(e)s, Bouthillette Parizeau + NCK\n\nA renewed experience, re-thinking the deeper meaning of the ecosystems. \n\nThe Migration project will revamp the Biodôme and its scenic design, in most areas accessible to the public, to offer an innovative, participatory, immersive experience. The architects compare the Biodôme to a living organism and their concept echoes notions of cellular biology. The cell, the basic building block of life, becomes a model for structuring and shaping the building blocks of nature – ecosystems. The design team is proposing to reorganize the spaces, open up the centre of the Biodôme, and offer bold, complementary experiences.\n\nThe overall project budget is $22 million. This includes professional and project management fees, studies, construction costs, restoration, acquisition and relocation of live collections, museology, furniture, various contingencies, etc.\n\n\nProgrammation informations can be found on http://espacepourlavie.ca/en \n\n"}
{"id": "1267220", "url": "https://en.wikipedia.org/wiki?curid=1267220", "title": "Sympatry", "text": "Sympatry\n\nIn biology, two related species or populations are considered sympatric when they exist in the same geographic area and thus frequently encounter one another. An initially interbreeding population that splits into two or more distinct species sharing a common range exemplifies sympatric speciation. Such speciation may be a product of reproductive isolation – which prevents hybrid offspring from being viable or able to reproduce, thereby reducing gene flow – that results in genetic divergence. Sympatric speciation does not imply secondary contact, which is speciation or divergence in allopatry followed by range expansions leading to an area of sympatry. Sympatric species or taxa in secondary contact may or may not interbreed.\n\nFour main types of population pairs exist in nature. Sympatric populations (or species) contrast with parapatric populations, which contact one another in adjacent but not shared ranges and do not interbreed; peripatric species, which are separated only by areas in which neither organism occurs; and allopatric species, which occur in entirely distinct ranges that are neither adjacent nor overlapping. Allopatric populations isolated from one another by geographical factors (e.g., mountain ranges or bodies of water) may experience genetic—and, ultimately, phenotypic—changes in response to their varying environments. These may drive allopatric speciation, which is arguably the dominant mode of speciation.\n\nThe lack of geographic isolation as a definitive barrier between sympatric species has yielded controversy among ecologists, biologists, and zoologists regarding the validity of the term. As such, researchers have long debated the conditions under which sympatry truly applies, especially with respect to parasitism. Because parasitic organisms often inhabit multiple hosts during a life cycle, evolutionary biologist Ernst Mayr stated that internal parasites existing within different hosts demonstrate allopatry, not sympatry. Today, however, many biologists consider parasites and their hosts to be sympatric (see examples below). Conversely, zoologist Michael J. D. White considered two populations sympatric if genetic interbreeding was viable within the habitat overlap. This may be further specified as sympatry occurring within one deme; that is, reproductive individuals must be able to locate one another in the same population in order to be sympatric.\n\nOthers question the ability of sympatry to result in complete speciation: until recently, many researchers considered it nonexistent, doubting that selection alone could create disparate, but not geographically separated, species. In 2003, biologist Karen McCoy suggested that sympatry can act as a mode of speciation only when \"the probability of mating between two individuals depend[s] [solely] on their genotypes, [and the genes are] dispersed throughout the range of the population during the period of reproduction\". In essence, sympatric speciation does require very strong forces of natural selection to be acting on heritable traits, as there is no geographic isolation to aid in the splitting process. Yet, recent research has begun to indicate that sympatric speciation is not as uncommon as was once assumed.\n\nSyntopy is a special case of sympatry. It means the joint occurrence of two species in the same habitat at the same time. Just as the broader term sympatry, \"syntopy\" is used especially for close species that might hybridise or even be sister species. \"Sympatric\" species occur together in the same region, but do not necessarily share the same localities as \"syntopic\" species do. Areas of syntopy are of interest because they allow to study how similar species may coexist without outcompeting each other.\n\nAs an example, the two bat species \"Myotis auriculus\" and \"M. evotis\" were found to be syntopic in North America. In contrast, the marbled newt and the northern crested newt have a large sympatric range in western France, but differ in their habitat preferences and only rarely occur syntopically in the same breeding ponds.\n\nThe lack of geographic constraint in isolating sympatric populations implies that the emerging species avoid interbreeding via other mechanisms. Before speciation is complete, two diverging populations may still produce viable offspring. As speciation progresses, isolating mechanisms – such as gametic incompatibility that renders fertilization of the egg impossible – are selected for in order to increase the reproductive divide between the two populations.\n\nSympatric groups frequently show a greater ability to discriminate between their own species and other closely related species than do allopatric groups. This is shown in the study of hybrid zones. It is also apparent in the differences in levels of prezygotic isolation (by factors that prevent formation of a viable zygote) in both sympatric and allopatric populations. There are two main theories regarding this process: 1) differential fusion, which suggests that only populations with a keen ability to discriminate between species will persist in sympatry; and 2) character displacement, which implies that distinguishing characteristics will be heightened in areas where the species co-occur in order to facilitate discrimination.\n\nReinforcement is the process by which natural selection reinforces reproductive isolation. In sympatry, reinforcement increases species discrimination and sexual adaptation in order to avoid maladaptive hybridization and encourage speciation. If hybrid offspring are either sterile or less-fit than non-hybrid offspring, mating between members of two different species will be selected against. Natural selection decreases the probability of such hybridization by selecting for the ability to identify mates of one's own species from those of another species.\n\nReproductive character displacement strengthens the reproductive barriers between sympatric species by encouraging the divergence of traits that are crucial to reproduction. Divergence is frequently distinguished by assortative mating between individuals of the two species. For example, divergence in the mating signals of two species will limit hybridization by reducing one's ability to identify an individual of the second species as a potential mate. Support for the reproductive character displacement hypothesis comes from observations of sympatric species in overlapping habitats in nature. Increased prezygotic isolation, which is associated with reproductive character displacement, has been observed in cicadas of genus \"Magicicada\", stickleback fish, and the flowering plants of the genus \"Phlox\".\n\nAn alternative explanation for species discrimination in sympatry is differential fusion. This hypothesis states that of the many species have historically come into contact with one another, the only ones that persist in sympatry (and thus are seen today) are species with strong mating discrimination. On the other hand, species lacking strong mating discrimination are assumed to have fused while in contact, forming one distinct species.\n\nDifferential fusion is less widely recognized than character displacement, and several of its implications are refuted by experimental evidence. For example, differential fusion implies greater postzygotic isolation among sympatric species, as this functions to prevent fusion between the species. However, Coyne and Orr found equal levels of postzygotic isolation among sympatric and allopatric species pairs in closely related \"Drosophila\". Nevertheless, differential fusion remains a possible, though not complete, contributor to species discrimination.\n\nSympatry has been increasingly evidenced in current research. Because of this, sympatric speciation – which was once highly debated among researchers – is progressively gaining credibility as a viable form of speciation.\n\nSeveral distinct types of killer whale (\"Orcinus orca\"), which are characterized by an array of morphological and behavioral differences, live in sympatry throughout the North Atlantic, North Pacific and Antarctic oceans. In the North Pacific, three whale populations – called \"transient\", \"resident\", and \"offshore\" – demonstrate partial sympatry, crossing paths with relative frequency. The results of recent genetic analyses using mtDNA indicate that this is due to secondary contact, in which the three types encountered one another following the bidirectional migration of \"offshore\" and \"resident\" whales between the North Atlantic and North Pacific. Partial sympatry in these whales is, therefore, not the result of speciation. Furthermore, killer whale populations that consist of all three types have been documented in the Atlantic, evidencing that interbreeding occurs among them. Thus, secondary contact does not always result in total reproductive isolation, as has often been predicted.\n\nThe parasitic great spotted cuckoo (\"Clamator glandarius\") and its magpie host, both native to Southern Europe, are completely sympatric species. However, the duration of their sympatry varies with location. For example, great spotted cuckoos and their magpie hosts in Hoya de Gaudix, southern Spain, have lived in sympatry since the early 1960s, while species in other locations have more recently become sympatric. Great spotted cuckoos, when in South Africa, are sympatric with at least 8 species of starling and 2 crows, pied crow and Cape crow.\n\nThe great spotted cuckoo exhibits brood parasitism by laying a mimicked version of the magpie egg in the magpie's nest. Since cuckoo eggs hatch before magpie eggs, magpie hatchlings must compete with cuckoo hatchlings for resources provided by the magpie mother. This relationship between the cuckoo and the magpie in various locations can be characterized as either recently sympatric or anciently sympatric. The results of an experiment by Soler and Moller (1990) showed that in areas of ancient sympatry (species in cohabitation for many generations), magpies were more likely to reject most of the cuckoo eggs, as these magpies had developed counter-adaptations that aid in identification of egg type. In areas of recent sympatry, magpies rejected comparatively fewer cuckoo eggs. Thus, sympatry can cause coevolution, by which both species undergo genetic changes due to the selective pressures that one species exerts on the other.\n\nLeafcutter ants protect and nourish various species of fungus as a source of food in a system known as ant-fungus mutualism. Leafcutter ants belonging to the genus \"Acromyrmex\" are known for their mutualistic relationship with Basidiomycete fungi. Ant colonies are closely associated with their fungus colonies, and may have co-evolved with a consistent vertical lineage of fungi in individual colonies. Ant populations defend against the horizontal transmission of foreign fungi to their fungal colony, as this transmission may lead to competitive stress on the local fungal garden. Invaders are identified and removed by the ant colony, inhibiting competition and fungal interbreeding. This active isolation of individual populations helps maintain the genetic purity of the fungal colony, and this mechanism may lead to sympatric speciation within a shared habitat.\n\n\n"}
{"id": "7777698", "url": "https://en.wikipedia.org/wiki?curid=7777698", "title": "Synopses of the British Fauna", "text": "Synopses of the British Fauna\n\nSynopses of the British Fauna is a series of identification guides, published by The Linnean Society and The Estuarine and Coastal Sciences Association. Each volume in the series provides and in-depth analysis of a group of animals and is designed to bridge the gap between the standard field guide and more specialised monograph or treatise. The series is now published by The Field Studies Council on behalf of The Linnean Society and The Estuarine and Coastal Sciences Association. \n\nThe series is designed for use in the field and is kept as user friendly as possible with technical terminology kept to a minimum and a glossary of terms provided, although the complexity of the subject matter makes the books more suitable for the more experienced practitioner.\n\nOn 11 March 1943, at a meeting of The Linnean Society in Burlington House, TH Savoy presented his \"Synopsis of the Opiliones\" (Harvestmen). It was so well received that a decision was made there and then to publish it as the first of a series of \"ecological fauna lists\".\n\nRe-launched by Dr Doris Kermack in the mid-1960s, the New Series of \"Synopses of the British Fauna\" went from strength to strength. From number 13, the series had been jointly sponsored by The Estuarine and Coastal Sciences Association and Dr RSK Barnes became co-editor.\n\nFrom 1993, the series has been published by The Field Studies Council and benefits from association with the extensive testing undertaken as part of the AIDGAP project.\n\nThe series contains the following volumes, many of which are out of print. Many of the volumes have been updated and reprinted under slightly different names to reflect either taxonomic changes or advances in the understanding of a group.\n\n\n"}
{"id": "103141", "url": "https://en.wikipedia.org/wiki?curid=103141", "title": "Thatching", "text": "Thatching\n\nThatching is the craft of building a roof with dry vegetation such as straw, water reed, sedge (\"Cladium mariscus\"), rushes, heather, or palm branches, layering the vegetation so as to shed water away from the inner roof. Since the bulk of the vegetation stays dry and is densely packed—trapping air—thatching also functions as insulation. It is a very old roofing method and has been used in both tropical and temperate climates. Thatch is still employed by builders in developing countries, usually with low-cost local vegetation. By contrast, in some developed countries it is the choice of some affluent people who desire a rustic look for their home, would like a more ecologically friendly roof, or who have purchased an originally thatched abode.\nThatching methods have traditionally been passed down from generation to generation, and numerous descriptions of the materials and methods used in Europe over the past three centuries survive in archives and early publications.\n\nIn some equatorial countries, thatch is the prevalent local material for roofs, and often walls. There are diverse building techniques from the ancient Hawaiian \"hale\" shelter made from the local ti leaves (\"Cordyline fruticosa\"), lauhala (\"Pandanus tectorius\") or pili grass (\"Heteropogon contortus\").\n\nPalm leaves are also often used. For example, in Na Bure, Fiji, thatchers combine fan palm leave roofs with layered reed walls. Feathered palm leaf roofs are used in Dominica. Alang-alang (\"Imperata cylindrica\") thatched roofs are used in Hawaii and Bali. In Southeast Asia, mangrove nipa palm leaves are used as thatched roof material known as attap dwelling. In Bali, Indonesia, the black fibres of Arenga pinnata called \"ijuk\" is also used as thatched roof materials, usually used in Balinese temple roof and meru towers. Sugar cane leaf roofs are used in Kikuyu tribal homes in Kenya.\n\nWild vegetation such as water reed (\"Phragmites australis\"), bulrush/cat tail (\"Typha\" spp.), broom (\"Cytisus scoparius\"), heather (\"Calluna vulgaris\"), and rushes (\"Juncus\" spp. and \"Schoenoplectus lacustris\") was probably used to cover shelters and primitive dwellings in Europe in the late Palaeolithic period, but so far no direct archaeological evidence for this has been recovered. People probably began to use straw in the Neolithic period when they first grew cereals—but once again, no direct archaeological evidence of straw for thatching in Europe prior to the early medieval period survives.\n\nMany indigenous people of the Americas, such as the former Maya civilization, Mesoamerica, the Inca empire, and the Triple Alliance (Aztec), lived in thatched buildings. It is common to spot thatched buildings in rural areas of the Yucatán Peninsula as well as many settlements in other parts of Latin America, which closely resemble the method of construction from distant ancestors. After the collapse of most extant American societies due to diseases introduced by Europeans, wars, enslavement, and genocide, the first Americans encountered by Europeans lived in structures roofed with bark or skin set in panels that could be added or removed for ventilation, heating, and cooling. Evidence of the many complex buildings with fiber-based roofing material was not rediscovered until the early 2000s. French and British settlers built temporary thatched dwellings with local vegetation as soon as they arrived in New France and New England, but covered more permanent houses with wooden shingles.\n\nIn most of England, thatch remained the only roofing material available to the bulk of the population in the countryside, in many towns and villages, until the late 1800s. Commercial production of Welsh slate began in 1820, and the mobility provided by canals and then railways made other materials readily available. Still, the number of thatched properties actually increased in the UK during the mid-1800s as agriculture expanded, but then declined again at the end of the 19th century because of agricultural recession and rural depopulation. \n\nGradually, thatch became a mark of poverty, and the number of thatched properties gradually declined, as did the number of professional thatchers. Thatch has become much more popular in the UK over the past 30 years, and is now a symbol of wealth rather than poverty. There are approximately 1,000 full-time thatchers at work in the UK, and thatching is becoming popular again because of the renewed interest in preserving historic buildings and using more sustainable building materials.\n\nAlthough thatch is popular in Germany, The Netherlands, Denmark, parts of France, Sicily, Belgium and Ireland, there are more thatched roofs in the United Kingdom than in any other European country. Good quality straw thatch can last for more than 50 years when applied by a skilled thatcher. Traditionally, a new layer of straw was simply applied over the weathered surface, and this \"spar coating\" tradition has created accumulations of thatch over 7’ (2.1 m) thick on very old buildings. The straw is bundled into \"yelms\" before it is taken up to the roof and then is attached using staples, known as \"spars\", made from twisted hazel sticks. Over 250 roofs in Southern England have base coats of thatch that were applied over 500 years ago, providing direct evidence of the types of materials that were used for thatching in the medieval period. Almost all of these roofs are thatched with wheat, rye, or a \"maslin\" mixture of both. Medieval wheat grew to almost tall in very poor soils and produced durable straw for the roof and grain for baking bread.\n\nTechnological change in the farming industry significantly affected the popularity of thatching. The availability of good quality thatching straw declined in England after the introduction of the combine harvester in the late 1930s and 1940s, and the release of short-stemmed wheat varieties. Increasing use of nitrogen fertiliser in the 1960s–70s also weakened straw and reduced its longevity. Since the 1980s, however, there has been a big increase in straw quality as specialist growers have returned to growing older, tall-stemmed, \"heritage\" varieties of wheat such as Squareheads Master (1880), N59 (1959), Rampton Rivet (1937), Victor (1910) and April Bearded (early 1800s)] in low input/organic conditions.\n\nIn the UK it is illegal under the Plant Variety and Seeds Act 1964 (with many amendments) for an individual or organisation to give, trade or sell seed of an older variety of wheat (or any other agricultural crop) to a third party for growing purposes, subject to a significant fine. Because of this legislation, thatchers in the UK can no longer obtain top quality thatching straw grown from traditional, tall-stemmed varieties of wheat.\n\nAll evidence indicates that water reed was rarely used for thatching outside of East Anglia. It has traditionally been a \"one coat\" material applied in a similar way to how it is used in continental Europe. Weathered reed is usually stripped and replaced by a new layer. It takes 4–5 acres of well-managed reed bed to produce enough reed to thatch an average house, and large reed beds have been uncommon in most of England since the Anglo-Saxon period. Over 80% of the water reed used in the UK is now imported from Turkey, Eastern Europe, China and South Africa. Though water reed might last for 50 years or more on a steep roof in a dry climate, modern imported water reed on an average roof in England does not last any longer than good quality wheat straw. The lifespan of a thatched roof also depends on the skill of the thatcher, but other factors must be considered—such as climate, quality of materials, and the roof pitch.\n\nIn areas where palms are abundant, palm leaves are used to thatch walls and roofs. Many species of palm trees are called \"thatch palm\", or have \"thatch\" as part of their common names. In the southeastern United States, Native and pioneer houses were often constructed of palmetto-leaf thatch. The chickees of the Seminole and Miccosukee are still thatched with palmetto leaves.\n\nGood thatch does not require frequent maintenance. In England a ridge normally lasts 8–14 years, and re-ridging is required several times during the lifespan of a thatch. Experts no longer recommend covering thatch with wire netting, as this slows evaporation and reduces longevity. Moss can be a problem if very thick, but is not usually detrimental, and many species of moss are actually protective. , remains the most widely used reference book on the techniques used for thatching. The thickness of a layer of thatch decreases over time as the surface gradually turns to compost and is blown off the roof by wind and rain. Thatched roofs generally needs replacement when the horizontal wooden 'sways' and hair-pin 'spars', also known as 'gads' (twisted hazel staples) that fix each course become visible near the surface. It is not total depth of the thatch within a new layer applied to a new roof that determines its longevity, but rather how much weathering thatch covers the fixings of each overlapping course. “A roof is as good as the amount of correctly laid thatch covering the fixings.”\n\nThatch is not as flammable as many people believe. It burns slowly, \"like a closed book,\" thatchers say. The vast majority of fires are linked to the use of wood burners and faulty chimneys with degraded or poorly installed or maintained flues. Sparks from paper or burned rubbish can ignite dry thatch on the surface around a chimney. Fires can also begin when sparks or flames work their way through a degraded chimney and ignite the surrounding semi-charred thatch. This can be avoided by ensuring that the chimney is in good condition, which may involve stripping thatch immediately surrounding the chimney to the full depth of the stack. This can easily be done without stripping thatch over the entire roof. Insurance premiums on thatched houses are higher than average in part because of the perception that thatched roofs are a fire hazard, but also because a thatch fire can cause extensive smoke damage and a thatched roof is more expensive to replace than a standard tiled or slate roof. Workmen should never use open flame near thatch, and nothing should be burnt that could fly up the chimney and ignite the surface of the thatch. Spark arrestors usually cause more harm than good, as they are easily blocked and reduce air flow. All thatched roofs should have smoke detectors in the roof space. Spray-on fire retardant or pressure impregnated fire retardants can reduce the spread of flame and radiated heat output.\n\nOn new buildings, a solid fire retardant barrier over the rafters can make the thatch sacrificial in case of fire. If fireboards are used, they require a ventilation gap between boarding and thatch so that the roof can breathe, as condensation can be a significant problem in thin, single layer thatch. Condensation is much less of a problem on thick straw roofs, which also provide much better insulation since they do not need to be ventilated.\n\nThe performance of thatch depends on roof shape and design, pitch of roof, position—its geography and topography—the quality of material and the expertise of the thatcher.\n\nThatch has some natural properties that are advantageous to its performance. It is naturally weather-resistant, and when properly maintained does not absorb a lot of water. There should not be a significant increase to roof weight due to water retention. A roof pitch of at least 50 degrees allows precipitation to travel quickly down slope so that it runs off the roof before it can penetrate the structure.\n\nThatch is also a natural insulator, and air pockets within straw thatch insulate a building in both warm and cold weather. A thatched roof ensures that a building is cool in summer and warm in winter.\n\nThatch also has very good resistance to wind damage when applied correctly.\n\nThatching materials range from plains grasses to waterproof leaves found in equatorial regions. It is the most common roofing material in the world, because the materials are readily available.\n\nBecause thatch is lighter, less timber is required in the roof that supports it.\n\nThatch is a versatile material when it comes to covering irregular roof structures. This fact lends itself to the use of second-hand, recycled and natural materials that are not only more sustainable, but need not fit exact standard dimensions to perform well.\n\nThatched houses are harder to insure because of the perceived fire risk, and because thatching is labor-intensive, it is much more expensive to thatch a roof than to cover it with slate or tiles. Birds can damage a roof while they are foraging for grubs, and rodents are attracted by residual grain in straw.\nThatch has fallen out of favor in much of the industrialized world not because of fire, but because thatching has become very expensive and alternative 'hard' materials are cheaper—but this situation is slowly changing. There are about 60,000 thatched roofs in the UK, of which 50–80 suffer a serious fire each year, most of these being completely destroyed. The cost to the Fire Brigade is £1.3m per annum. Many more thatched roofs are being built every year.\n\nNew thatched roofs were forbidden in London in 1212 following a major fire, and existing roofs had to have their surfaces plastered to reduce the risk of fire. The modern Globe Theatre is one of the few thatched buildings in London (others can be found in the suburb of Kingsbury), but the Globe's modern, water reed thatch is purely for decorative purpose and actually lies over a fully waterproofed roof built with modern materials. The Globe Theatre, opened in 1997, was modelled on the Rose, which was destroyed by a fire on a dry June night in 1613 when a burning wad of cloth ejected from a special effects cannon during a performance set light to the surface of the thatch. The original Rose Theatre was actually thatched with cereal straw, a sample of which was recovered by Museum of London archaeologists during the excavation of the site in the 1980s.\n\nSome claim thatch cannot cope with regular snowfall but, as with all roofing materials, this depends on the strength of the underlying roof structure and the pitch of the surface. A law passed in 1640 in Massachusetts outlawed the use of thatched roofs in the colony for this reason. Thatch is lighter than most other roofing materials, typically around , so the roof supporting it does not need to be so heavily constructed, but if snow accumulates on a lightly constructed thatched roof, it could collapse. A thatched roof is usually pitched between 45–55 degrees and under normal circumstances this is sufficient to shed snow and water. In areas of extreme snowfall, such as parts of Japan, the pitch is increased further.\nSome thatched roofs in the UK are extremely old and preserve evidence of traditional materials and methods that have long since been lost. In northern Britain this evidence is often preserved beneath corrugated sheet materials and frequently come to light during the development of smaller rural properties. Historic Scotland have funded several research projects into thatching techniques and these have revealed a wide range of materials including broom, heather, rushes, cereals, bracken, turf and clay and highlighted significant regional variation \n\nMore recent examples include the Moirlanich Longhouse, Killin owned by the National Trust for Scotland (rye, bracken & turf) and Sunnybrae Cottage, Pitlochry owned by Historic Scotland (rye, broom & turf) \n\n\n\n\n"}
{"id": "32840848", "url": "https://en.wikipedia.org/wiki?curid=32840848", "title": "The Cloud (poem)", "text": "The Cloud (poem)\n\n\"The Cloud\" is a major 1820 poem written by Percy Bysshe Shelley. \"The Cloud\" was written during late 1819 or early 1820, and submitted for publication on 12 July 1820. The work was published in the 1820 collection \"Prometheus Unbound, A Lyrical Drama, in Four Acts, With Other Poems\" by Charles and James Ollier in London in August 1820. The work was proof-read by John Gisborne. There were multiple drafts of the poem. The poem consists of six stanzas in anapestic or antidactylus meter, a foot with two unaccented syllables followed by an accented syllable.\n\nThe cloud is a metaphor for the unending cycle of nature: \"I silently laugh at my own cenotaph/ ... I arise and unbuild it again.\" As with the wind and the leaves in \"Ode to the West Wind\", the skylark in \"To a Skylark\", and the plant in \"The Sensitive Plant\", Shelley endows the cloud with sentient traits that personify the forces of nature.\n\nIn \"The Cloud\", Shelley relies on the imagery of transformation or metamorphosis, a cycle of birth, death, and rebirth: \"I change, but I cannot die.\" Mutability or change is a fact of physical nature.\n\nLightning or electricity is the \"pilot\" or guide for the cloud. Lightning is attracted to the \"genii\" in the earth which results in lightning flashes. The genii symbolize the positive charge of the surface of the earth while the cloud possesses a negative charge.\n\nBritish scientist and poet Erasmus Darwin, the grandfather of Charles Darwin, had written about plant life and science in the poem collection \"The Botanic Garden\" (1791) and on \"spontaneous vitality\", that \"microscopic animals are said to remain dead for many days or weeks ... and quickly to recover life and motion\" when water and heat are added, in \"The Temple of Nature\" (1803). Percy Bysshe Shelley had cited Darwin in his Preface to the anonymously published novel \"Frankenstein; or, The Modern Prometheus\" (1818), explaining how the novel was written and its meaning. He argued that imparting life to a corpse \"as not of impossible occurrence\".\n\nThe cloud is a personification and a metaphor for the perpetual cycle of transformation and change in nature. All life and matter are interconnected and undergo unending change and metamorphosis.\n\nA review of the 1820 \"Prometheus Unbound\" collection in the September and October 1821 issues of \"The London Magazine\" noted the originality of \"The Cloud\": \"It is impossible to peruse them without admiring the peculiar property of the author's mind, which can doff in an instant the cumbersome garments of metaphysical speculations, and throw itself naked as it were into the arms of nature and humanity. The beautiful and singularly original poem of 'The Cloud' will evince proofs of our opinion, and show the extreme force and freshness with which the writer can impregnate his poetry.\"\n\nIn the October 1821 issue of \"Quarterly Review\", W.S. Walker argued that \"The Cloud\" is related to \"Prometheus Unbound\" in that they are both absurd and \"galimatias\".\n\nJohn Todhunter wrote in 1880 that \"The Cloud\" and \"To a Skylark\" were \"the two most popular of Shelley's lyrics\".\n\nIn 1889, Francis Thompson asserted that \"The Cloud\" was the \"most typically Shelleyan of all the poems\" because it contained \"the child's faculty of make-believe raised to the nth power\" and that \"He is still at play, save only that his play is such as manhood stops to watch, and his playthings are those which the gods give their children. The universe is his box of toys. He dabbles his fingers in the dayfall. He is gold-dusty with tumbling amidst the stars.\"\n\nOn 20 April 1919, a silent black and white movie was released in the US entitled \"The Cloud\" which was \"a visual poem featuring clouds and landscapes in accompaniment to the words of Shelley's poem 'The Cloud'.\" The film was directed by W.A. Van Scoy and produced by the Post Nature Pictures company.\n\n\n"}
{"id": "33896250", "url": "https://en.wikipedia.org/wiki?curid=33896250", "title": "The Pocket Guide to British Birds", "text": "The Pocket Guide to British Birds\n\nThe Pocket Guide to British Birds is a guide written by British naturalist and expert on wild flowers Richard Sidney Richmond Fitter, and illustrated by Richard Richardson, which was first published by Collins in 1952. Reprinted in 1953 and 1954, a second more revised 287-page editions was published by Collins in 1966, and in 1968.\n"}
{"id": "712222", "url": "https://en.wikipedia.org/wiki?curid=712222", "title": "Transit of Earth from Mars", "text": "Transit of Earth from Mars\n\nA transit of Earth across the Sun as seen from Mars takes place when the planet Earth passes directly between the Sun and Mars, obscuring a small part of the Sun's disc for an observer on Mars. During a transit, Earth would be visible from Mars as a small black disc moving across the face of the Sun. They occur every 26, 79 and 100 years, and every 1,000 years or so there is an extra 53rd-year transit.\n\nTransits of Earth from Mars usually occur in pairs, with one following the other after 79 years; rarely, there are three in the series. The transits also follow a 284-year cycle, occurring at intervals of 100.5, 79, 25.5, and 79 years; a transit falling on a particular date is usually followed by another transit 284 years later. Transits occurring when Mars is at its ascending node are in May, those at descending node happen in November. This cycle corresponds fairly closely to 151 Mars orbits, 284 Earth orbits, and 133 synodic periods, and is analogous to the cycle of transits of Venus from Earth, which follow a cycle of 243 years (121.5, 8, 105.5, 8). There are currently four such active series, containing from 8 to 25 transits. A new one is set to begin in 2394. The last series ending was in 1211.\n\nNo one has ever seen a transit of Earth from Mars, but the next transit will take place on November 10, 2084. The last such transit took place on May 11, 1984.\n\nDuring the event, the Moon could almost always also be seen in transit, although due to the distance between Earth and Moon, sometimes one body completes the transit before the other begins (this last occurred in the 1800 transit, and will happen again in 2394).\n\nA transit of Earth from Mars corresponds to Mars being perfectly uniformly illuminated at opposition from Earth, its phase being 180.0° without any defect of illumination. During the 1879 event, this permitted Charles Augustus Young to attempt a careful measurement of the oblateness (polar compression) of Mars. He obtained the value 1/219, or 0.0046. This is close to the modern value of 1/154 (many sources will cite somewhat different values, such as 1/193, because even a difference of only a couple of kilometers in the values of Mars' polar and equatorial radii gives a considerably different result).\n\nMuch more recently, better measurements of the oblateness of Mars have been made by using radar from the Earth. Also, better measurements have been made by using artificial satellites that have been put into orbit around Mars, including \"Mariner 9\", \"Viking 1\", \"Viking 2\", and Soviet orbiters, and the more recent orbiters that have been sent from the Earth to Mars.\n\nA science fiction short story published in 1971 by Arthur C. Clarke, called \"Transit of Earth\", depicts a doomed astronaut on Mars observing the transit in 1984. This short story was first published in the January 1971 issue of \"Playboy\" magazine.\n\nSometimes Earth only grazes the Sun during a transit. In this case it is possible that in some areas of Mars a full transit can be seen while in other regions there is only a partial transit (no second or third contact). The last transit of this type was on 30 April 1211, and the next such transit will occur on 27 November 4356. It is also possible that a transit of Earth can be seen in some parts of Mars as a partial transit, while in others Earth misses the Sun. Such a transit last occurred on 26 October 664, and the next transit of this type will occur on 14 December 5934.\n\nThe simultaneous occurrence of a transit of Venus and a transit of Earth is extremely rare, and will next occur in the year 571,471.\n\n\n\n"}
{"id": "54716184", "url": "https://en.wikipedia.org/wiki?curid=54716184", "title": "United States Energy Association", "text": "United States Energy Association\n\nThe United States Energy Association (USEA) is the U.S. Member Committee of the World Energy Council. Headquartered in Washington, D.C., USEA is an association of public and private energy-related organizations, corporations, and government agencies. \nThe association hosts annual events such as the Carbon Sequestration Leadership Forum, Energy Supply Forum, and State of the Energy Industry Forum.\n\nBarry Worthington has served as USEA’s executive director since 1988. Worthington chairs the Clean Electricity Production working group within the UNECE Committee on Sustainable Energy. He also sits on numerous energy boards, including the National Energy Foundation (chairman) and Energy Law Foundation.\n\nWorthington meets with domestic and international energy leaders to discuss energy infrastructure partnerships. He often advocates for energy cultivation in developing countries, claiming there are \"few priorities greater for the world than getting people linked to the grid.\" Worthington is a firm supporter of energy \"sovereignty.\"\n\nVicky Bailey currently chairs USEA’s board of directors. She succeeded Jack Futcher, the President and COO of Bechtel.\n\nFor 25 years, USEA has been a partner with USAID, expanding energy infrastructure, improving energy access, and reducing energy poverty in developing economies through international energy partnerships. A major function of USEA’s is to help USAID expand energy infrastructure and programs in developing countries. In 2012, the association launched the U.S.-East Africa Geothermal Partnership (EAGP), a public-private partnership “offering assistance at early stages of project development in East Africa.” Through the Djibouti Geothermal Partnership, Ethiopia Geothermal Partnership, and Kenya Electricity Generating Company (KenGen), USEA partners with the Department of Energy and local governments to promote U.S. companies’ involvement in developing additional geothermal generation capacity. According to USEA, the number of U.S. companies conducting geothermal work in East Africa has more than tripled since EAGP’s inception.\n\nUSEA represents the interests of the U.S. energy sector through public education and advocacy. The association supports an “all-of-the-above energy strategy,” from renewable energy to fossil fuels. USEA advocates for the exploration and production of oil and natural gas.\n\n"}
{"id": "18365403", "url": "https://en.wikipedia.org/wiki?curid=18365403", "title": "Wildland fire emission", "text": "Wildland fire emission\n\nWildland fire and wildland fire atmospheric emissions have been a part of the global biosphere for millennia. The major wildland fire emissions include greenhouse gasses and several criteria pollutants that impact human health and welfare.:\nCompared to the preindustrial era, wildland land fire in the conterminous U.S. has been reduced 90 percent with proportional reductions in wildland fire emissions. Land use changes (agriculture and urbanization) are responsible for roughly 50 percent of this decrease, and land management decisions (land fragmentation, suppression actions, etc.) are responsible for the remainder. Anthropogenic activities (e.g., industrial production, transportation, agriculture, etc.) today have more than replaced the lost preindustrial wildland fire atmospheric emissions.\n\nThe following charts compare preindustrial wildland fire emissions with contemporary emissions.\n"}
{"id": "33550", "url": "https://en.wikipedia.org/wiki?curid=33550", "title": "Wood", "text": "Wood\n\nWood is a porous and fibrous structural tissue found in the stems and roots of trees and other woody plants. It is an organic material, a natural composite of cellulose fibers that are strong in tension and embedded in a matrix of lignin that resists compression. Wood is sometimes defined as only the secondary xylem in the stems of trees, or it is defined more broadly to include the same type of tissue elsewhere such as in the roots of trees or shrubs. In a living tree it performs a support function, enabling woody plants to grow large or to stand up by themselves. It also conveys water and nutrients between the leaves, other growing tissues, and the roots. Wood may also refer to other plant materials with comparable properties, and to material engineered from wood, or wood chips or fiber.\n\nWood has been used for thousands of years for fuel, as a construction material, for making tools and weapons, furniture and paper. More recently it emerged as a feedstock for the production of purified cellulose and its derivatives, such as cellophane and cellulose acetate.\n\nAs of 2005, the growing stock of forests worldwide was about 434 billion cubic meters, 47% of which was commercial. As an abundant, carbon-neutral renewable resource, woody materials have been of intense interest as a source of renewable energy. In 1991 approximately 3.5 billion cubic meters of wood were harvested. Dominant uses were for furniture and building construction.\n\nA 2011 discovery in the Canadian province of New Brunswick yielded the earliest known plants to have grown wood, approximately 395 to 400 million years ago.\n\nWood can be dated by carbon dating and in some species by dendrochronology to determine when a wooden object was created.\n\nPeople have used wood for thousands of years for many purposes, including as a fuel or as a construction material for making houses, tools, weapons, furniture, packaging, artworks, and paper. Known constructions using wood date back ten thousand years. Buildings like the European Neolithic long house were made primarily of wood.\n\nRecent use of wood has been enhanced by the addition of steel and bronze into construction.\n\nThe year-to-year variation in tree-ring widths and isotopic abundances gives clues to the prevailing climate at the time a tree was cut.\n\nWood, in the strict sense, is yielded by trees, which increase in diameter by the formation, between the existing wood and the inner bark, of new woody layers which envelop the entire stem, living branches, and roots. This process is known as secondary growth; it is the result of cell division in the vascular cambium, a lateral meristem, and subsequent expansion of the new cells. These cells then go on to form thickened secondary cell walls, composed mainly of cellulose, hemicellulose and lignin.\n\nWhere the differences between the four seasons are distinct, e.g. New Zealand, growth can occur in a discrete annual or seasonal pattern, leading to growth rings; these can usually be most clearly seen on the end of a log, but are also visible on the other surfaces. If the distinctiveness between seasons is annual (as is the case in equatorial regions, e.g. Singapore), these growth rings are referred to as annual rings. Where there is little seasonal difference growth rings are likely to be indistinct or absent. If the bark of the tree has been removed in a particular area, the rings will likely be deformed as the plant overgrows the scar.\n\nIf there are differences within a growth ring, then the part of a growth ring nearest the center of the tree, and formed early in the growing season when growth is rapid, is usually composed of wider elements. It is usually lighter in color than that near the outer portion of the ring, and is known as earlywood or springwood. The outer portion formed later in the season is then known as the latewood or summerwood. However, there are major differences, depending on the kind of wood (see below).\n\nAs a tree grows, lower branches often die, and their bases may become overgrown and enclosed by subsequent layers of trunk wood, forming a type of imperfection known as a knot. The dead branch may not be attached to the trunk wood except at its base, and can drop out after the tree has been sawn into boards. Knots affect the technical properties of the wood, usually reducing the local strength and increasing the tendency for splitting along the wood grain, but may be exploited for visual effect. In a longitudinally sawn plank, a knot will appear as a roughly circular \"solid\" (usually darker) piece of wood around which the grain of the rest of the wood \"flows\" (parts and rejoins). Within a knot, the direction of the wood (grain direction) is up to 90 degrees different from the grain direction of the regular wood.\n\nIn the tree a knot is either the base of a side branch or a dormant bud. A knot (when the base of a side branch) is conical in shape (hence the roughly circular cross-section) with the inner tip at the point in stem diameter at which the plant's vascular cambium was located when the branch formed as a bud.\n\nIn grading lumber and structural timber, knots are classified according to their form, size, soundness, and the firmness with which they are held in place. This firmness is affected by, among other factors, the length of time for which the branch was dead while the attaching stem continued to grow.\n\nKnots do not necessarily influence the stiffness of structural timber, this will depend on the size and location. Stiffness and elastic strength are more dependent upon the sound wood than upon localized defects. The breaking strength is very susceptible to defects. Sound knots do not weaken wood when subject to compression parallel to the grain.\n\nIn some decorative applications, wood with knots may be desirable to add visual interest. In applications where wood is painted, such as skirting boards, fascia boards, door frames and furniture, resins present in the timber may continue to 'bleed' through to the surface of a knot for months or even years after manufacture and show as a yellow or brownish stain. A knot primer paint or solution (knotting), correctly applied during preparation, may do much to reduce this problem but it is difficult to control completely, especially when using mass-produced kiln-dried timber stocks.\n\nHeartwood (or duramen) is wood that as a result of a naturally occurring chemical transformation has become more resistant to decay. Heartwood formation is a genetically programmed process that occurs spontaneously. Some uncertainty exists as to whether the wood dies during heartwood formation, as it can still chemically react to decay organisms, but only once.\n\nHeartwood is often visually distinct from the living sapwood, and can be distinguished in a cross-section where the boundary will tend to follow the growth rings. For example, it is sometimes much darker. However, other processes such as decay or insect invasion can also discolor wood, even in woody plants that do not form heartwood, which may lead to confusion.\n\nSapwood (or alburnum) is the younger, outermost wood; in the growing tree it is living wood, and its principal functions are to conduct water from the roots to the leaves and to store up and give back according to the season the reserves prepared in the leaves. However, by the time they become competent to conduct water, all xylem tracheids and vessels have lost their cytoplasm and the cells are therefore functionally dead. All wood in a tree is first formed as sapwood. The more leaves a tree bears and the more vigorous its growth, the larger the volume of sapwood required. Hence trees making rapid growth in the open have thicker sapwood for their size than trees of the same species growing in dense forests. Sometimes trees (of species that do form heartwood) grown in the open may become of considerable size, or more in diameter, before any heartwood begins to form, for example, in second-growth hickory, or open-grown pines.\n\nThe term \"heartwood\" derives solely from its position and not from any vital importance to the tree. This is evidenced by the fact that a tree can thrive with its heart completely decayed. Some species begin to form heartwood very early in life, so having only a thin layer of live sapwood, while in others the change comes slowly. Thin sapwood is characteristic of such species as chestnut, black locust, mulberry, osage-orange, and sassafras, while in maple, ash, hickory, hackberry, beech, and pine, thick sapwood is the rule. Others never form heartwood.\n\nNo definite relation exists between the annual rings of growth and the amount of sapwood. Within the same species the cross-sectional area of the sapwood is very roughly proportional to the size of the crown of the tree. If the rings are narrow, more of them are required than where they are wide. As the tree gets larger, the sapwood must necessarily become thinner or increase materially in volume. Sapwood is relatively thicker in the upper portion of the trunk of a tree than near the base, because the age and the diameter of the upper sections are less.\n\nWhen a tree is very young it is covered with limbs almost, if not entirely, to the ground, but as it grows older some or all of them will eventually die and are either broken off or fall off. Subsequent growth of wood may completely conceal the stubs which will however remain as knots. No matter how smooth and clear a log is on the outside, it is more or less knotty near the middle. Consequently, the sapwood of an old tree, and particularly of a forest-grown tree, will be freer from knots than the inner heartwood. Since in most uses of wood, knots are defects that weaken the timber and interfere with its ease of working and other properties, it follows that a given piece of sapwood, because of its position in the tree, may well be stronger than a piece of heartwood from the same tree.\n\nIt is remarkable that the inner heartwood of old trees remains as sound as it usually does, since in many cases it is hundreds, and in a few instances thousands, of years old. Every broken limb or root, or deep wound from fire, insects, or falling timber, may afford an entrance for decay, which, once started, may penetrate to all parts of the trunk. The larvae of many insects bore into the trees and their tunnels remain indefinitely as sources of weakness. Whatever advantages, however, that sapwood may have in this connection are due solely to its relative age and position.\n\nIf a tree grows all its life in the open and the conditions of soil and site remain unchanged, it will make its most rapid growth in youth, and gradually decline. The annual rings of growth are for many years quite wide, but later they become narrower and narrower. Since each succeeding ring is laid down on the outside of the wood previously formed, it follows that unless a tree materially increases its production of wood from year to year, the rings must necessarily become thinner as the trunk gets wider. As a tree reaches maturity its crown becomes more open and the annual wood production is lessened, thereby reducing still more the width of the growth rings. In the case of forest-grown trees so much depends upon the competition of the trees in their struggle for light and nourishment that periods of rapid and slow growth may alternate. Some trees, such as southern oaks, maintain the same width of ring for hundreds of years. Upon the whole, however, as a tree gets larger in diameter the width of the growth rings decreases.\n\nDifferent pieces of wood cut from a large tree may differ decidedly, particularly if the tree is big and mature. In some trees, the wood laid on late in the life of a tree is softer, lighter, weaker, and more even-textured than that produced earlier, but in other trees, the reverse applies. This may or may not correspond to heartwood and sapwood. In a large log the sapwood, because of the time in the life of the tree when it was grown, may be inferior in hardness, strength, and toughness to equally sound heartwood from the same log. In a smaller tree, the reverse may be true.\n\nIn species which show a distinct difference between heartwood and sapwood the natural color of heartwood is usually darker than that of the sapwood, and very frequently the contrast is conspicuous (see section of yew log above). This is produced by deposits in the heartwood of chemical substances, so that a dramatic color variation does not imply a significant difference in the mechanical properties of heartwood and sapwood, although there may be a marked biochemical difference between the two.\n\nSome experiments on very resinous longleaf pine specimens indicate an increase in strength, due to the resin which increases the strength when dry. Such resin-saturated heartwood is called \"fat lighter\". Structures built of fat lighter are almost impervious to rot and termites; however they are very flammable. Stumps of old longleaf pines are often dug, split into small pieces and sold as kindling for fires. Stumps thus dug may actually remain a century or more since being cut. Spruce impregnated with crude resin and dried is also greatly increased in strength thereby.\n\nSince the latewood of a growth ring is usually darker in color than the earlywood, this fact may be used in visually judging the density, and therefore the hardness and strength of the material. This is particularly the case with coniferous woods. In ring-porous woods the vessels of the early wood often appear on a finished surface as darker than the denser latewood, though on cross sections of heartwood the reverse is commonly true. Otherwise the color of wood is no indication of strength.\n\nAbnormal discoloration of wood often denotes a diseased condition, indicating unsoundness. The black check in western hemlock is the result of insect attacks. The reddish-brown streaks so common in hickory and certain other woods are mostly the result of injury by birds. The discoloration is merely an indication of an injury, and in all probability does not of itself affect the properties of the wood. Certain rot-producing fungi impart to wood characteristic colors which thus become symptomatic of weakness; however an attractive effect known as spalting produced by this process is often considered a desirable characteristic. Ordinary sap-staining is due to fungal growth, but does not necessarily produce a weakening effect.\n\nWater occurs in living wood in three locations, namely:\n\nIn heartwood it occurs only in the first and last forms. Wood that is thoroughly air-dried retains 8–16% of the water in the cell walls, and none, or practically none, in the other forms. Even oven-dried wood retains a small percentage of moisture, but for all except chemical purposes, may be considered absolutely dry.\n\nThe general effect of the water content upon the wood substance is to render it softer and more pliable. A similar effect occurs in the softening action of water on rawhide, paper, or cloth. Within certain limits, the greater the water content, the greater its softening effect.\n\nDrying produces a decided increase in the strength of wood, particularly in small specimens. An extreme example is the case of a completely dry spruce block 5 cm in section, which will sustain a permanent load four times as great as a green (undried) block of the same size will.\n\nThe greatest strength increase due to drying is in the ultimate crushing strength, and strength at elastic limit in endwise compression; these are followed by the modulus of rupture, and stress at elastic limit in cross-bending, while the modulus of elasticity is least affected.\n\nWood is a heterogeneous, hygroscopic, cellular and anisotropic material. It consists of cells, and the cell walls are composed of micro-fibrils of cellulose (40–50%) and hemicellulose (15–25%) impregnated with lignin (15–30%).\n\nIn coniferous or softwood species the wood cells are mostly of one kind, tracheids, and as a result the material is much more uniform in structure than that of most hardwoods. There are no vessels (\"pores\") in coniferous wood such as one sees so prominently in oak and ash, for example.\n\nThe structure of hardwoods is more complex. The water conducting capability is mostly taken care of by vessels: in some cases (oak, chestnut, ash) these are quite large and distinct, in others (buckeye, poplar, willow) too small to be seen without a hand lens. In discussing such woods it is customary to divide them into two large classes, \"ring-porous\" and \"diffuse-porous\".\n\nIn ring-porous species, such as ash, black locust, catalpa, chestnut, elm, hickory, mulberry, and oak, the larger vessels or pores (as cross sections of vessels are called) are localized in the part of the growth ring formed in spring, thus forming a region of more or less open and porous tissue. The rest of the ring, produced in summer, is made up of smaller vessels and a much greater proportion of wood fibers. These fibers are the elements which give strength and toughness to wood, while the vessels are a source of weakness.\n\nIn diffuse-porous woods the pores are evenly sized so that the water conducting capability is scattered throughout the growth ring instead of being collected in a band or row. Examples of this kind of wood are alder, basswood, birch, buckeye, maple, willow, and the \"Populus\" species such as aspen, cottonwood and poplar. Some species, such as walnut and cherry, are on the border between the two classes, forming an intermediate group.\n\nIn temperate softwoods, there often is a marked difference between latewood and earlywood. The latewood will be denser than that formed early in the season. When examined under a microscope, the cells of dense latewood are seen to be very thick-walled and with very small cell cavities, while those formed first in the season have thin walls and large cell cavities. The strength is in the walls, not the cavities. Hence the greater the proportion of latewood, the greater the density and strength. In choosing a piece of pine where strength or stiffness is the important consideration, the principal thing to observe is the comparative amounts of earlywood and latewood. The width of ring is not nearly so important as the proportion and nature of the latewood in the ring.\n\nIf a heavy piece of pine is compared with a lightweight piece it will be seen at once that the heavier one contains a larger proportion of latewood than the other, and is therefore showing more clearly demarcated growth rings. In white pines there is not much contrast between the different parts of the ring, and as a result the wood is very uniform in texture and is easy to work. In hard pines, on the other hand, the latewood is very dense and is deep-colored, presenting a very decided contrast to the soft, straw-colored earlywood.\n\nIt is not only the proportion of latewood, but also its quality, that counts. In specimens that show a very large proportion of latewood it may be noticeably more porous and weigh considerably less than the latewood in pieces that contain less latewood. One can judge comparative density, and therefore to some extent strength, by visual inspection.\n\nNo satisfactory explanation can as yet be given for the exact mechanisms determining the formation of earlywood and latewood. Several factors may be involved. In conifers, at least, rate of growth alone does not determine the proportion of the two portions of the ring, for in some cases the wood of slow growth is very hard and heavy, while in others the opposite is true. The quality of the site where the tree grows undoubtedly affects the character of the wood formed, though it is not possible to formulate a rule governing it. In general, however, it may be said that where strength or ease of working is essential, woods of moderate to slow growth should be chosen.\n\nIn ring-porous woods, each season's growth is always well defined, because the large pores formed early in the season abut on the denser tissue of the year before.\n\nIn the case of the ring-porous hardwoods, there seems to exist a pretty definite relation between the rate of growth of timber and its properties. This may be briefly summed up in the general statement that the more rapid the growth or the wider the rings of growth, the heavier, harder, stronger, and stiffer the wood. This, it must be remembered, applies only to ring-porous woods such as oak, ash, hickory, and others of the same group, and is, of course, subject to some exceptions and limitations.\n\nIn ring-porous woods of good growth, it is usually the latewood in which the thick-walled, strength-giving fibers are most abundant. As the breadth of ring diminishes, this latewood is reduced so that very slow growth produces comparatively light, porous wood composed of thin-walled vessels and wood parenchyma. In good oak, these large vessels of the earlywood occupy from 6 to 10 percent of the volume of the log, while in inferior material they may make up 25% or more. The latewood of good oak is dark colored and firm, and consists mostly of thick-walled fibers which form one-half or more of the wood. In inferior oak, this latewood is much reduced both in quantity and quality. Such variation is very largely the result of rate of growth.\n\nWide-ringed wood is often called \"second-growth\", because the growth of the young timber in open stands after the old trees have been removed is more rapid than in trees in a closed forest, and in the manufacture of articles where strength is an important consideration such \"second-growth\" hardwood material is preferred. This is particularly the case in the choice of hickory for handles and spokes. Here not only strength, but toughness and resilience are important.\n\nThe results of a series of tests on hickory by the U.S. Forest Service show that:\n\nThe effect of rate of growth on the qualities of chestnut wood is summarized by the same authority as follows:\n\nIn the diffuse-porous woods, the demarcation between rings is not always so clear and in some cases is almost (if not entirely) invisible to the unaided eye. Conversely, when there is a clear demarcation there may not be a noticeable difference in structure within the growth ring.\n\nIn diffuse-porous woods, as has been stated, the vessels or pores are even-sized, so that the water conducting capability is scattered throughout the ring instead of collected in the earlywood. The effect of rate of growth is, therefore, not the same as in the ring-porous woods, approaching more nearly the conditions in the conifers. In general it may be stated that such woods of medium growth afford stronger material than when very rapidly or very slowly grown. In many uses of wood, total strength is not the main consideration. If ease of working is prized, wood should be chosen with regard to its uniformity of texture and straightness of grain, which will in most cases occur when there is little contrast between the latewood of one season's growth and the earlywood of the next.\n\nStructural material that resembles ordinary, \"dicot\" or conifer timber in its gross handling characteristics is produced by a number of monocot plants, and these also are colloquially called wood. Of these, bamboo, botanically a member of the grass family, has considerable economic importance, larger culms being widely used as a building and construction material and in the manufacture of engineered flooring, panels and veneer. Another major plant group that produces material that often is called wood are the palms. Of much less importance are plants such as \"Pandanus,\" \"Dracaena\" and \"Cordyline.\" With all this material, the structure and composition of the processed raw material is quite different from ordinary wood.\n\nThe single most revealing property of wood as an indicator of wood quality is specific gravity (Timell 1986), as both pulp yield and lumber strength are determined by it. Specific gravity is the ratio of the mass of a substance to the mass of an equal volume of water; density is the ratio of a mass of a quantity of a substance to the volume of that quantity and is expressed in mass per unit substance, e.g., grams per milliliter (g/cm or g/ml). The terms are essentially equivalent as long as the metric system is used. Upon drying, wood shrinks and its density increases. Minimum values are associated with green (water-saturated) wood and are referred to as \"basic specific gravity\" (Timell 1986).\n\nWood density is determined by multiple growth and physiological factors compounded into “one fairly easily measured wood characteristic” (Elliott 1970).\n\nAge, diameter, height, radial (trunk) growth, geographical location, site and growing conditions, silvicultural treatment, and seed source all to some degree influence wood density. Variation is to be expected. Within an individual tree, the variation in wood density is often as great as or even greater than that between different trees (Timell 1986). Variation of specific gravity within the bole of a tree can occur in either the horizontal or vertical direction.\n\nIt is common to classify wood as either softwood or hardwood. The wood from conifers (e.g. pine) is called softwood, and the wood from dicotyledons (usually broad-leaved trees, (e.g. oak) is called hardwood. These names are a bit misleading, as hardwoods are not necessarily hard, and softwoods are not necessarily soft. The well-known balsa (a hardwood) is actually softer than any commercial softwood. Conversely, some softwoods (e.g. yew) are harder than many hardwoods.\n\nThere is a strong relationship between the properties of wood and the properties of the particular tree that yielded it. The density of wood varies with species. The density of a wood correlates with its strength (mechanical properties). For example, mahogany is a medium-dense hardwood that is excellent for fine furniture crafting, whereas balsa is light, making it useful for model building. One of the densest woods is black ironwood.\n\nThe chemical composition of wood varies from species to species, but is approximately 50% carbon, 42% oxygen, 6% hydrogen, 1% nitrogen, and 1% other elements (mainly calcium, potassium, sodium, magnesium, iron, and manganese) by weight. Wood also contains sulfur, chlorine, silicon, phosphorus, and other elements in small quantity.\n\nAside from water, wood has three main components. Cellulose, a crystalline polymer derived from glucose, constitutes about 41–43%. Next in abundance is hemicellulose, which is around 20% in deciduous trees but near 30% in conifers. It is mainly five-carbon sugars that are linked in an irregular manner, in contrast to the cellulose. Lignin is the third component at around 27% in coniferous wood vs. 23% in deciduous trees. Lignin confers the hydrophobic properties reflecting the fact that it is based on aromatic rings. These three components are interwoven, and direct covalent linkages exist between the lignin and the hemicellulose. A major focus of the paper industry is the separation of the lignin from the cellulose, from which paper is made.\n\nIn chemical terms, the difference between hardwood and softwood is reflected in the composition of the constituent lignin. Hardwood lignin is primarily derived from sinapyl alcohol and coniferyl alcohol. Softwood lignin is mainly derived from coniferyl alcohol.\n\nAside from the lignocellulose, wood consists of a variety of low molecular weight organic compounds, called \"extractives\". The wood extractives are fatty acids, resin acids, waxes and terpenes. For example, rosin is exuded by conifers as protection from insects. The extraction of these organic materials from wood provides tall oil, turpentine, and rosin.\n\nWood has a long history of being used as fuel, which continues to this day, mostly in rural areas of the world. Hardwood is preferred over softwood because it creates less smoke and burns longer. Adding a woodstove or fireplace to a home is often felt to add ambiance and warmth.\n\nWood has been an important construction material since humans began building shelters, houses and boats. Nearly all boats were made out of wood until the late 19th century, and wood remains in common use today in boat construction. Elm in particular was used for this purpose as it resisted decay as long as it was kept wet (it also served for water pipe before the advent of more modern plumbing).\n\nWood to be used for construction work is commonly known as \"lumber\" in North America. Elsewhere, \"lumber\" usually refers to felled trees, and the word for sawn planks ready for use is \"timber\". In Medieval Europe oak was the wood of choice for all wood construction, including beams, walls, doors, and floors. Today a wider variety of woods is used: solid wood doors are often made from poplar, small-knotted pine, and Douglas fir.\nNew domestic housing in many parts of the world today is commonly made from timber-framed construction. Engineered wood products are becoming a bigger part of the construction industry. They may be used in both residential and commercial buildings as structural and aesthetic materials.\n\nIn buildings made of other materials, wood will still be found as a supporting material, especially in roof construction, in interior doors and their frames, and as exterior cladding.\n\nWood is also commonly used as shuttering material to form the mold into which concrete is poured during reinforced concrete construction.\n\nA solid wood floor is a floor laid with planks or battens created from a single piece of timber, usually a hardwood. Since wood is hydroscopic (it acquires and loses moisture from the ambient conditions around it) this potential instability effectively limits the length and width of the boards.\n\nSolid hardwood flooring is usually cheaper than engineered timbers and damaged areas can be sanded down and refinished repeatedly, the number of times being limited only by the thickness of wood above the tongue.\n\nSolid hardwood floors were originally used for structural purposes, being installed perpendicular to the wooden support beams of a building (the joists or bearers) and solid construction timber is still often used for sports floors as well as most traditional wood blocks, mosaics and parquetry.\n\nEngineered wood products, glued building products \"engineered\" for application-specific performance requirements, are often used in construction and industrial applications. Glued engineered wood products are manufactured by bonding together wood strands, veneers, lumber or other forms of wood fiber with glue to form a larger, more efficient composite structural unit.\n\nThese products include glued laminated timber (glulam), wood structural panels (including plywood, oriented strand board and composite panels), laminated veneer lumber (LVL) and other structural composite lumber (SCL) products, parallel strand lumber, and I-joists. Approximately 100 million cubic meters of wood was consumed for this purpose in 1991. The trends suggest that particle board and fiber board will overtake plywood.\n\nWood unsuitable for construction in its native form may be broken down mechanically (into fibers or chips) or chemically (into cellulose) and used as a raw material for other building materials, such as engineered wood, as well as chipboard, hardboard, and medium-density fiberboard (MDF). Such wood derivatives are widely used: wood fibers are an important component of most paper, and cellulose is used as a component of some synthetic materials. Wood derivatives can be used for kinds of flooring, for example laminate flooring.\n\nWood has always been used extensively for furniture, such as chairs and beds. It is also used for tool handles and cutlery, such as chopsticks, toothpicks, and other utensils, like the wooden spoon and pencil.\n\nFurther developments include new lignin glue applications, recyclable food packaging, rubber tire replacement applications, anti-bacterial medical agents, and high strength fabrics or composites.\nAs scientists and engineers further learn and develop new techniques to extract various components from wood, or alternatively to modify wood, for example by adding components to wood, new more advanced products will appear on the marketplace. Moisture content electronic monitoring can also enhance next generation wood protection.\n\nWood has long been used as an artistic medium. It has been used to make sculptures and carvings for millennia. Examples include the totem poles carved by North American indigenous people from conifer trunks, often Western Red Cedar (\"Thuja plicata\").\n\nOther uses of wood in the arts include:\n\nMany types of sports equipment are made of wood, or were constructed of wood in the past. For example, cricket bats are typically made of white willow. The baseball bats which are legal for use in Major League Baseball are frequently made of ash wood or hickory, and in recent years have been constructed from maple even though that wood is somewhat more fragile. NBA courts have been traditionally made out of parquetry.\n\nMany other types of sports and recreation equipment, such as skis, ice hockey sticks, lacrosse sticks and archery bows, were commonly made of wood in the past, but have since been replaced with more modern materials such as aluminium, titanium or composite materials such as fiberglass and carbon fiber. One noteworthy example of this trend is the family of golf clubs commonly known as the \"woods\", the heads of which were traditionally made of persimmon wood in the early days of the game of golf, but are now generally made of metal or (especially in the case of drivers) carbon-fiber composites.\n\nLittle is known about the bacteria that degrade cellulose. Symbiotic bacteria in \"Xylophaga\" may play a role in the degradation of sunken wood; while bacteria such as \"Alphaproteobacteria\", \"Flavobacteria\", \"Actinobacteria\", \"Clostridia\", and \"Bacteroidetes\" have been detected in wood submerged over a year.\n\n"}
