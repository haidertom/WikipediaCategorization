{"id": "36939819", "url": "https://en.wikipedia.org/wiki?curid=36939819", "title": "Analog observation", "text": "Analog observation\n\nAnalog observation is, in contrast to naturalistic observation, a research tool by which a subject is observed in an artificial setting. Typically, types of settings in which analog observation is utilized include clinical offices or research laboratories, but, by definition, analog observations can be made in any artificial environment, even if the environment is one which the subject is likely to encounter naturally.\n\nAnalog observation is typically divided into two iteration of application: The first iteration primarily studies the effect of manipulation of variables in the subject's environment, including setting and events, on the subject's behavior. The second iteration primarily seeks to observe the subject's behavior in quasi-experimental social situations.\n\n"}
{"id": "25287133", "url": "https://en.wikipedia.org/wiki?curid=25287133", "title": "Anywhere on Earth", "text": "Anywhere on Earth\n\nAnywhere on Earth (AoE) is a calendar designation which indicates that a period expires when the date passes everywhere on Earth. The last place on Earth where any date exists is on Howland and Baker islands, in the time zone (the West side of the International Date Line), and so is the last spot on the globe for any day to exist. Therefore, the day ends AoE when it ends on Howland Island.\n\nThe convention originated in IEEE 802.16 balloting procedures. At this point, many IEEE 802 ballot deadlines are established as the end of day using \"AoE\", for \"Anywhere on Earth\" as a designation. This means that the deadline has not passed if, anywhere on Earth, the deadline date has not yet passed.\n\nNote that the day's end AoE occurs at noon Coordinated Universal Time (UTC) of the following day, Howland and Baker islands being halfway around the world from the prime meridian that is the base reference longitude for UTC.\n\nThus, in standard notation this is:\n\n"}
{"id": "41821574", "url": "https://en.wikipedia.org/wiki?curid=41821574", "title": "Binsey Poplars", "text": "Binsey Poplars\n\n‘Binsey Poplars’ is a poem by Gerard Manley Hopkins (1844–1889), written in 1879. The poem was inspired by the felling of a row of poplar trees near the village of Binsey, northwest of Oxford, England, and overlooking Port Meadow on the bank of the River Thames. The replacements for these trees, running from Binsey north to Godstow, lasted until 2004, when replanting began again.\n\nThe Bodleian Library of Oxford University holds a draft manuscript of the poem, handwritten by Hopkins, acquired in 2013.\n\nThe text of the poem is as follows:\n\n\n"}
{"id": "58685900", "url": "https://en.wikipedia.org/wiki?curid=58685900", "title": "Bioresilience", "text": "Bioresilience\n\nBioresilience refers to the ability of a whole species or an individual of a species to adapt to change. Initially the term applied to changes in the natural environment, but increasingly it is also used for adaptation to anthropogenically induced change.\n\nAlexander von Humboldt was the first to note the resilience of life forms with increasing altitude and the accompanying decreasing prevalence in numbers, and he documented this in the 18th century on the slopes of the volcano Chimborazo.\n\nUnderstanding of bioresilience evolved from research led by The Mountain Institute when establishing two of the national parks that surround Mount Everest, Makalu-Barun National Park in Nepal, and Qomolangma National Nature Preserve in the Tibet Autonomous Region of China. The research documented greater biodiversity at Everest’s base than higher up. There were progressively fewer documented species as the mountain ascended into higher biomes, from subtropical to temperate to alpine to Arctic-like. These fewer species, though, had greater biologic robustness correlating directly with increasing bioresilience. \n\nMonitoring of bioresilience, beginning in the Everest ecosystem but expanding to other mountain ecologies globally is being carried out by the Biomeridian Project at Future Generations University.\n\nThe concept of bioresilience has also been applied to human health to explain aging or chronic diseases decrease the ability of the body to adapt; in such cases, the system becomes rigid and unable to cross different life demands. As the human body loses robustness with age, an individual becomes unable to accommodate new life demands, be they contagions, stress, or events such as injury or even jet lag.\n\nThe importance of resilience in biological systems has been widely recognized in terms of the impacts on life by anthropogenic changes. Accelerating environmental change and continuing loss of genetic resources positions lower biodiversity around the planet threatening ecosystem services. A major mitigating factor will be life forms with higher resilience. \n\nParalleling the work in mountain environments, a growing number of studies is applying the concept of bioresilience to assess the robustness of life in other ecological systems challenged by the Anthropocene. One such study was with the adaptive renewal and natural perturbation in Lake Victoria, the world’s second largest freshwater lake.\n"}
{"id": "5129726", "url": "https://en.wikipedia.org/wiki?curid=5129726", "title": "Casimir pressure", "text": "Casimir pressure\n\nCasimir pressure is created by the Casimir force of virtual particles.\n\nAccording to experiments, the Casimir force formula_1 between two closely spaced neutral parallel plate conductors is directly proportional to their surface area formula_2:\n\nformula_3\n\nTherefore, dividing the magnitude of Casimir force by the area of each conductor, Casimir pressure formula_4 can be found. Because the Casimir force between conductors is attractive, the Casimir pressure in space between the conductors is negative.\n\nBecause virtual particles are physical representations of the zero point energy of physical vacuum, the Casimir pressure is the difference in the density of the zero point energy of empty space inside and outside of cavity made by conductive plates.\n\nSome scientists believe that zero point energy is the dominant energy of the Universe and that the Casimir pressure of this energy is the main cause of the observed accelerated expansion of the Universe. In other words, virtual particles drive the accelerated expansion of the Universe.\n\n"}
{"id": "40626908", "url": "https://en.wikipedia.org/wiki?curid=40626908", "title": "Classical Marxism", "text": "Classical Marxism\n\nClassical Marxism refers to the economic, philosophical and sociological theories expounded by Karl Marx and Friedrich Engels as contrasted with later developments in Marxism, especially Leninism and Marxism–Leninism.\n\nKarl Heinrich Marx (May 5, 1818, Trier, Germany – March 14, 1883, London) was an immensely influential German philosopher, sociologist, political economist and revolutionary socialist. Marx addressed a wide range of issues, including alienation and exploitation of the worker, the capitalist mode of production and historical materialism, although he is most famous for his analysis of history in terms of class struggles, summed up in the opening line of the introduction to \"The Communist Manifesto\": \"The history of all hitherto existing society is the history of class struggles\". The influence of his ideas, already popular during his life, was given added impetus by the victory of the Russian Bolsheviks in the 1917 October Revolution and there are few parts of the world which were not significantly touched by Marxian ideas in the course of the twentieth century.\n\nAs the American Marx scholar Hal Draper remarked: \"[T]here are few thinkers in modern history whose thought has been so badly misrepresented, by Marxists and anti-Marxists alike\".\n\nThe early influences on Marx are often grouped into three categories: German philosophy, English/Scottish political economy and French socialism.\n\nMain influences include Immanuel Kant; G. W. F. Hegel; and Ludwig Feuerbach. \n\nMarx studied under one of Hegel's pupils, Bruno Bauer, a leader of the circle of Young Hegelians to whom Marx attached himself. However, from 1841 he and Engels came to disagree with Bauer and the rest of the Young Hegelians about socialism and also about the usage of Hegel's dialectic and progressively broke away from German idealism and the Young Hegelians. Marx's early writings are thus a response towards Hegel, German idealism and a break with the rest of the Young Hegelians. Marx, \"stood Hegel on his head\", in his own view of his role by turning the idealistic dialectic into a materialistic one, in proposing that material circumstances shape ideas instead of the other way around. In this, Marx was following the lead of Feuerbach. His theory of alienation, developed in the \"Economic and Philosophical Manuscripts of 1844\" (published in 1932), inspired itself from Feuerbach's critique of the alienation of Man in God through the objectivation of all his inherent characteristics (thus man projected on God all qualities which are in fact man's own quality which defines the \"human nature\"). But Marx also criticized Feuerbach for being insufficiently materialistic.\n\nMain influences include Adam Smith and David Ricardo. \n\nMarx built on and critiqued the most well-known political economists of his day, the British classical political economists.\n\nMarx critiqued Smith and Ricardo for not realizing that their economic concepts reflected specifically capitalist institutions, not innate natural properties of human society and could not be applied unchanged to all societies. He proposed a systematic correlation between labour-values and money prices. He claimed that the source of profits under capitalism is value added by workers not paid out in wages. This mechanism operated through the distinction between \"labour power\", which workers freely exchanged for their wages; and \"labour\", over which asset-holding capitalists thereby gained control. This practical and theoretical distinction was Marx's primary insight, and allowed him to develop the concept of \"surplus value\", which distinguished his works from that of Smith and Ricardo.\n\nMain influences include Jean-Jacques Rousseau; Charles Fourier; Henri de Saint-Simon; Pierre-Joseph Proudhon; and Louis Blanc.\n\nRousseau was one of the first modern writers to seriously attack the institution of private property and therefore is sometimes considered a forebear of modern socialism and communism, though Marx rarely mentions Rousseau in his writings.\n\nIn 1833, France was experiencing a number of social problems arising out of the Industrial Revolution. A number of sweeping plans of reform were developed by thinkers on the political left. Among the more grandiose were the plans of Charles Fourier and the followers of Saint-Simon. Fourier wanted to replace modern cities with utopian communities while the Saint-Simonians advocated directing the economy by manipulating credit. Although these programs did not have much support, they did expand the political and social imagination of Marx.\n\nLouis Blanc is perhaps best known for originating the social principle, later adopted by Marx, of how labor and income should be distributed: “From each according to his abilities, to each according to his needs”.\n\nPierre-Joseph Proudhon participated in the February 1848 uprising and the composition of what he termed \"the first republican proclamation\" of the new republic, but he had misgivings about the new government because it was pursuing political reform at the expense of the socio-economic reform, which Proudhon considered basic. Proudhon published his own perspective for reform, \"Solution du problème social\", in which he laid out a program of mutual financial cooperation among workers. He believed this would transfer control of economic relations from capitalists and financiers to workers. It was Proudhon's book \"What Is Property?\" that convinced the young Karl Marx that private property should be abolished.\n\nMain influences includes Friedrich Engels; ancient Greek materialism; Giambattista Vico; and Lewis H. Morgan.\n\nMarx's revision of Hegelianism was also influenced by Engels' book \"The Condition of the Working Class in England\" in 1844, which led Marx to conceive of the historical dialectic in terms of class conflict and to see the modern working class as the most progressive force for revolution.\n\nMarx was influenced by Antique materialism, especially Epicurus (to whom Marx dedicated his thesis, \"The Difference Between the Democritean and Epicurean Philosophy of Nature\", 1841) for his materialism and theory of clinamen which opened up a realm of liberty.\n\nGiambattista Vico propounded a cyclical theory of history, according to which human societies progress through a series of stages from barbarism to civilization and then return to barbarism. In the first stage—called the Age of the Gods—religion, the family and other basic institutions emerge; in the succeeding Age of Heroes, the common people are kept in subjection by a dominant class of nobles; in the final stage—the Age of Men—the people rebel and win equality, but in the process society begins to disintegrate. Vico's influence on Marx is obvious.\n\nMarx drew on Lewis H. Morgan and his social evolution theory. He wrote a collection of notebooks from his reading of Lewis Morgan, but they are regarded as being quite obscure and only available in scholarly editions. (However Engels is much more noticeably influenced by Morgan than Marx).\n\nFriedrich Engels (November 28, 1820, Wuppertal – August 5, 1895, London) was a nineteenth-century German political philosopher. He developed communist theory alongside his better-known collaborator, Karl Marx.\n\nIn 1842, his father sent the young Engels to England to help manage his cotton factory in Manchester. Shocked by the widespread poverty, Engels began writing an account which he published in 1845 as \"The Condition of the Working Class in England in 1844\" ().\n\nIn July 1845, Engels went to England, where he met an Irish working-class woman named Mary Burns (Crosby), with whom he lived until her death in 1863 (Carver 2003:19). Later, Engels lived with her sister Lizzie, marrying her the day before she died in 1877 (Carver 2003:42). These women may have introduced him to the Chartist movement, of whose leaders he met several, including George Harney.\n\nEngels actively participated in the Revolution of 1848, taking part in the uprising at Elberfeld. Engels fought in the Baden campaign against the Prussians (June/July 1849) as the aide-de-camp of August Willich, who commanded a Free Corps in the Baden-Palatinate uprising. \n\nMarx and Engels first met in person in September 1844. They discovered that they had similar views on philosophy and on capitalism and decided to work together, producing a number of works including \"Die heilige Familie\" (\"The Holy Family\"). After the French authorities deported Marx from France in January 1845, Engels and Marx decided to move to Belgium, which then permitted greater freedom of expression than some other countries in Europe. Engels and Marx returned to Brussels in January 1846, where they set up the Communist Correspondence Committee.\n\nIn 1847, Engels and Marx began writing a pamphlet together, based on Engels' \"The Principles of Communism\". They completed the 12,000-word pamphlet in six weeks, writing it in such a manner as to make communism understandable to a wide audience and published it as \"The Communist Manifesto\" in February 1848. In March, Belgium expelled both Engels and Marx. They moved to Cologne, where they began to publish a radical newspaper, the \"Neue Rheinische Zeitung\". By 1849, both Engels and Marx had to leave Germany and moved to London. The Prussian authorities applied pressure on the British government to expel the two men, but Prime Minister Lord John Russell refused. With only the money that Engels could raise, the Marx family lived in extreme poverty. The contributions of Marx and Engels to the formation of Marxist theory have been described as inseparable.\n\nMarx's main ideas included:\n\nMarx believed that class identity was configured in the relations with the mode of production. In other words, a class is a collective of individuals who have a similar relationship with the means of production (as opposed to the more common-sense idea that class is determined by wealth alone, i.e. high class, middle class and poor class).\n\nMarx describes several social classes in capitalist societies, including primarily:\nThe bourgeoisie may be further subdivided into the very wealthy bourgeoisie and the petty bourgeoisie. The petty bourgeoisie are those who employ labor, but may also work themselves. These may be small proprietors, land-holding peasants, or trade workers. Marx predicted that the petty bourgeoisie would eventually be destroyed by the constant reinvention of the means of production and the result of this would be the forced movement of the vast majority of the petty bourgeoisie to the proletariat. Marx also identified the lumpenproletariat, a stratum of society completely disconnected from the means of production.\n\nMarx also describes the communists as separate from the oppressed proletariat. The communists were to be a unifying party among the proletariat; they were educated revolutionaries who could bring the proletariat to revolution and help them establish the democratic dictatorship of the proletariat. According to Marx, the communists would support any true revolution of the proletariat against the bourgeoisie. Thus the communists aide the proletariat in creating the inevitable classless society (Vladimir Lenin takes this concept a step further by stating that only \"professional revolutionaries\" can lead the revolution against the bourgeoisie).\n\nThe Marxist theory of historical materialism understands society as fundamentally determined by the material conditions at any given time—this means the relationships which people enter into with one another in order to fulfill their basic needs, for instance to feed and clothe themselves and their families. In general, Marx and Engels identified five successive stages of the development of these material conditions in Western Europe.\n\n"}
{"id": "36980", "url": "https://en.wikipedia.org/wiki?curid=36980", "title": "Clay", "text": "Clay\n\nClay is a finely-grained natural rock or soil material that combines one or more clay minerals with possible traces of quartz (SiO), metal oxides (AlO , MgO etc.) and organic matter. Geologic clay deposits are mostly composed of phyllosilicate minerals containing variable amounts of water trapped in the mineral structure. Clays are plastic due to particle size and geometry as well as water content, and become hard, brittle and non–plastic upon drying or firing. Depending on the soil's content in which it is found, clay can appear in various colours from white to dull grey or brown to deep orange-red.\n\nAlthough many naturally occurring deposits include both silts and clay, clays are distinguished from other fine-grained soils by differences in size and mineralogy. Silts, which are fine-grained soils that do not include clay minerals, tend to have larger particle sizes than clays. There is, however, some overlap in particle size and other physical properties. The distinction between silt and clay varies by discipline. Geologists and soil scientists usually consider the separation to occur at a particle size of 2 µm (clays being finer than silts), sedimentologists often use 4–5 μm, and colloid chemists use 1 μm. Geotechnical engineers distinguish between silts and clays based on the plasticity properties of the soil, as measured by the soils' Atterberg limits. ISO 14688 grades clay particles as being smaller than 2 μm and silt particles as being larger.\n\nMixtures of sand, silt and less than 40% clay are called loam. Loam makes good soil and is used as a building material.\n\nClay minerals typically form over long periods of time as a result of the gradual chemical weathering of rocks, usually silicate-bearing, by low concentrations of carbonic acid and other diluted solvents. These solvents, usually acidic, migrate through the weathering rock after leaching through upper weathered layers. In addition to the weathering process, some clay minerals are formed through hydrothermal activity. There are two types of clay deposits: primary and secondary. Primary clays form as residual deposits in soil and remain at the site of formation. Secondary clays are clays that have been transported from their original location by water erosion and deposited in a new sedimentary deposit. Clay deposits are typically associated with very low energy depositional environments such as large lakes and marine basins.\n\nDepending on the academic source, there are three or four main groups of clays: kaolinite, montmorillonite-smectite, illite, and chlorite. Chlorites are not always considered to be a clay, sometimes being classified as a separate group within the phyllosilicates. There are approximately 30 different types of \"pure\" clays in these categories, but most \"natural\" clay deposits are mixtures of these different types, along with other weathered minerals.\n\nVarve (or \"varved clay\") is clay with visible annual layers, which are formed by seasonal deposition of those layers and are marked by differences in erosion and organic content. This type of deposit is common in former glacial lakes. When fine sediments are delivered into the calm waters of these glacial lake basins away from the shoreline, they settle to the lake bed. The resulting seasonal layering is preserved in an even distribution of clay sediment banding.\n\nQuick clay is a unique type of marine clay indigenous to the glaciated terrains of Norway, Canada, Northern Ireland, and Sweden. It is a highly sensitive clay, prone to liquefaction, which has been involved in several deadly landslides.\n\nPowder X-ray diffraction can be used to identify clays.\n\nThe physical and reactive chemical properties can be used to help elucidate the composition of clays.\n\nClays exhibit plasticity when mixed with water in certain proportions. However, when dry, clay becomes firm and when fired in a kiln, permanent physical and chemical changes occur. These changes convert the clay into a ceramic material. Because of these properties, clay is used for making pottery, both utilitarian and decorative, and construction products, such as bricks, wall and floor tiles. Different types of clay, when used with different minerals and firing conditions, are used to produce earthenware, stoneware, and porcelain. Prehistoric humans discovered the useful properties of clay. Some of the earliest pottery shards recovered are from central Honshu, Japan. They are associated with the Jōmon culture and deposits they were recovered from have been dated to around 14,000 BC.\n\nClay tablets were the first known writing medium. Scribes wrote by inscribing them with cuneiform script using a blunt reed called a stylus. Purpose-made clay balls were also used as sling ammunition.\n\nClays sintered in fire were the first form of ceramic. Bricks, cooking pots, art objects, dishware, smoking pipes, and even musical instruments such as the ocarina can all be shaped from clay before being fired. Clay is also used in many industrial processes, such as paper making, cement production, and chemical filtering. Until the late 20th century, bentonite clay was widely used as a mold binder in the manufacture of sand castings.\n\nClay, being relatively impermeable to water, is also used where natural seals are needed, such as in the cores of dams, or as a barrier in landfills against toxic seepage (lining the landfill, preferably in combination with geotextiles). (See puddling.)\n\nStudies in the early 21st century have investigated clay's absorption capacities in various applications, such as the removal of heavy metals from waste water and air purification.\n\nTraditional uses of clay as medicine goes back to prehistoric times. An example is Armenian bole, which is used to soothe an upset stomach. Some animals such as parrots and pigs ingest clay for similar reasons. Kaolin clay and attapulgite have been used as anti-diarrheal medicines.\n\nClay as the defining ingredient of loam is one of the oldest building materials on Earth, among other ancient, naturally-occurring geologic materials such as stone and organic materials like wood. Between one-half and two-thirds of the world's population, in both traditional societies as well as developed countries, still live or work in buildings made with clay, often baked into brick, as an essential part of its load-bearing structure. Also a primary ingredient in many natural building techniques, clay is used to create adobe, cob, cordwood, and rammed earth structures and building elements such as wattle and daub, clay plaster, clay render case, clay floors and clay paints and ceramic building material. Clay was used as a mortar in brick chimneys and stone walls where protected from water.\n\n\n\n"}
{"id": "35971482", "url": "https://en.wikipedia.org/wiki?curid=35971482", "title": "Day length fluctuations", "text": "Day length fluctuations\n\nThe length of the day, which has increased over the long term of Earth's history due to tidal effects, is also subject to fluctuations on a shorter scale of time. Exact measurements of time by atomic clocks and satellite laser ranging have revealed that the length of day (LOD) is subject to a number of different changes. These subtle variations have periods that range from a few weeks to a few years. They are attributed to interactions between the dynamic atmosphere and Earth itself. The International Earth Rotation and Reference Systems Service monitors the changes.\n\nIn the absence of external torques, the total angular momentum of Earth as a whole system must be constant. Internal torques are due to relative movements and mass redistribution of Earth's core, mantle, crust, oceans, atmosphere, and cryosphere. In order to keep the total angular momentum constant, a change of the angular momentum in one region must necessarily be balanced by angular momentum changes in the other regions.\n\nCrustal movements (such as continental drift) or polar cap melting are slow secular events. The characteristic coupling time between core and mantle has been estimated to be on the order of ten years, and the so-called 'decade fluctuations' of Earth's rotation rate are thought to result from fluctuations within the core, transferred to the mantle. The length of day (LOD) varies significantly even for time scales from a few years down to weeks (Figure), and the observed fluctuations in the LOD - after eliminating the effects of external torques - are a direct consequence of the action of internal torques. These short term fluctuations are very probably generated by the interaction between the solid Earth and the atmosphere.\n\nAny change of the axial component of the atmospheric angular momentum (AAM) must be accompanied by a corresponding change of the angular momentum of Earth's crust and mantle (due to the law of conservation of angular momentum). Because the moment of inertia of the system mantle-crust is only slightly influenced by atmospheric pressure loading, this mainly requires a change in the angular velocity of the solid Earth; \"i.e.\", a change of LOD. The LOD can presently be measured to a high accuracy over integration times of only a few hours, and general circulation models of the atmosphere allow high precision determination of changes in AAM in the model. A comparison between AAM and LOD shows that they are highly correlated. In particular, one recognizes an annual period of LOD with an amplitude of 0.34 milliseconds, maximizing on February 3, and a semiannual period with an amplitude of 0.29 milliseconds, maximizing on May 8, as well as 10‑day fluctuations of the order of 0.1 milliseconds. Interseasonal fluctuations reflecting El Niño events and quasi-biennial oscillations have also been observed. There is now general agreement that most of the changes in LOD on time scales from weeks to a few years are excited by changes in AAM.\n\nOne means of exchange of angular momentum between the atmosphere and the non gaseous parts of the earth is evaporation and precipitation. Massive quantities of water are in continual flux between the oceans and the atmosphere. As the mass of water (vapour) rises its rotation must slow due to conservation of angular momentum. Equally when if falls as rain, its rate of rotation will increase to conserve angular momentum. Any net global transfer of water mass from oceans to the atmosphere or the opposite implies a change in the speed of rotation of the solid/liquid Earth which will be reflected in LOD. \n\nObservational evidence shows that there is no significant time delay between the change of AAM and its corresponding change of LOD for periods longer than about 10 days. This implies a strong coupling between atmosphere and solid Earth due to surface friction with a time constant of about 7 days, the spin-down time of the Ekman layer. This spin-down time is the characteristic time for the transfer of atmospheric axial angular momentum to Earth's surface and vice versa.\n\nThe zonal wind-component on the ground, which is most effective for the transfer of axial angular momentum between Earth and atmosphere, is the component describing rigid rotation of the atmosphere. The zonal wind of this component has the amplitude \"u\" at the equator relative to the ground, where \"u\" > 0 indicates superrotation and \"u\" < 0 indicates retrograde rotation with respect to the solid Earth. All other wind terms merely redistribute the AAM with latitude, an effect that cancels out when averaged over the globe.\n\nSurface friction allows the atmosphere to 'pick up' angular momentum from Earth in the case of retrograde rotation or release it to Earth in the case of superrotation. Averaging over longer time scales, no exchange of AAM with the solid Earth takes place. Earth and atmosphere are decoupled. This implies that the ground level zonal wind-component responsible for rigid rotation must be zero on the average. Indeed, the observed meridional structure of the climatic mean zonal wind on the ground shows westerly winds (from the west) in middle latitudes beyond about ± 30 latitude and easterly winds (from the east) in low latitudes—the trade winds—as well as near the poles\n(prevailing winds).\nThe atmosphere picks up angular momentum from Earth at low and high latitudes and transfers the same amount to Earth at middle latitudes.\n\nAny short term fluctuation of the rigidly rotating zonal wind-component is then accompanied by a corresponding change in LOD. In order to estimate the order of magnitude of that effect, one may consider the total atmosphere to rotate rigidly with velocity \"u\" (in m/s) without surface friction. Then this value is related to the corresponding change of the length of day Δ\"\" (in milliseconds) as\n\nThe annual component of the change of the length of day of Δ' ≃ 0.34 ms corresponds then to a superrotation of \"u\" ≃ 0.9 m/s, and the semiannual component of Δ' ≃ 0.29 ms to \"u\" ≃ 0.8 m/s.\n"}
{"id": "55895027", "url": "https://en.wikipedia.org/wiki?curid=55895027", "title": "Disposable soma theory of aging", "text": "Disposable soma theory of aging\n\nThe disposable soma theory of aging states that organisms age due to an evolutionary trade-off between growth, reproduction, and DNA repair maintenance. Formulated by Thomas Kirkwood, the disposable soma theory explains that an organism only has a limited amount of resources or \"soma\" that it can allocate to its various cellular processes. Therefore, a greater investment in growth and reproduction would result in reduced investment in DNA repair maintenance, leading to increased cellular damage, shortened telomeres, accumulation of mutations, compromised stem cells, and ultimately, senescence. Although many models, both animal and human, have appeared to support this theory, parts of it are still controversial. \nSpecifically, while the evolutionary trade-off between growth and aging has been well established, \nthe relationship between reproduction and aging is still without scientific consensus, and the cellular mechanisms largely undiscovered.\n\nBritish biologist Thomas Kirkwood first proposed the disposable soma theory of aging in a 1977 \"Nature\" review article. The theory was inspired by Leslie Orgel's Error Catastrophe Theory of Aging, which was published fourteen years earlier, in 1963. Orgel believed that the process of aging arose due to mutations acquired during the replication process, and Kirkwood developed the disposable soma theory in order to mediate Orgel's work with evolutionary genetics.\n\nThe disposable soma theory of aging acts on the premise that there is a tradeoff in resource allocation between somatic maintenance and reproductive investment. Too low an investment in self-repair would be evolutionarily unsound, as the organism would likely die before reproductive age. However, too high an investment in self-repair would also be evolutionarily unsound due to the fact that one's offspring would likely die before reproductive age. Therefore, there is a compromise and resources are partitioned accordingly. However, this compromise is thought to damage somatic repair systems, which can lead to progressive cellular damage and senescence. Repair costs can be categorized into three groups: (1) the costs of increased durability of nonrenewable parts; (2) the costs of maintenance involving cell renewal, and (3) the costs of intracellular maintenance. In a nutshell, aging and decline is essentially a tradeoff for increased reproductive robustness in youth.\n\nMuch research has been done on the antagonistic effects of increased growth on lifespan. Specifically, the hormone insulin-like growth factor 1 (IGF-1), binds to a cell receptor, leading to a phosphorylation cascade. This cascade results in kinases phosphorylating forkhead transcription factor (FOXO), deactivating it. Deactivation of FOXO results in an inability to express genes involved in responding to oxidative stress response, such as antioxidants, chaperones, and heat-shock proteins. \nAdditionally, uptake of IGF-1 stimulates the mTOR pathway, which activates protein synthesis (and therefore growth) through upregulation of the translation-promoting S6K1, and also inhibits autophagy, a process necessary for recycling of damaged cellular products. Decline of autophagy causes neurodegeneration, protein aggregation and premature aging. Lastly, studies have also indicated that the mTOR pathway also alters immune responses and stimulates cyclin-dependent kinase (CDK) inhibitors such as p16 and p21. This leads to alteration of the stem-cell niche and results in stem cell exhaustion, another theorized mechanism of aging.\n\nThe mechanism of why reproduction inhibits lifespan with regards to multicellular organisms is still unclear. Although many models do illustrate an inverse relationship, and the theory makes sense from an evolutionary perspective, the cellular mechanisms have yet to be explored. However, with regards to cellular replication, the progressive shortening of telomeres is a mechanism which limits the amount of generations of a single cell may undergo. Furthermore, in unicellular organisms like \"Saccharomyces cerevisiae\", the formation of extrachromosomal rDNA circles (ERCs) in mother cells (but not daughter cells) upon every subsequent division is an identifiable type of DNA damage that is associated with replication. These ERCs accumulate over time and eventually trigger replicative senescence and death of the mother cell.\n\nThere is a large body of evidence indicating the negative effects of growth on longevity across many species. As a general rule, individuals of a smaller size generally live longer than larger individuals of the same species.\n\nIn dwarf models of mice, such Snell or Ames mice, mutations have arisen, either rendering them incapable of producing IGF-1 or unable to have adequate receptors for IGF-1 uptake. Furthermore, mice injected with growth hormone have been shown to have progressive weight loss, roughing of the coat, curvature of the spine, enlargement of the organs, kidney lesions and increased cancer risk. This effect is also seen in different breeds of dogs, where smaller breeds of dogs typically live significantly longer compared to their larger counterparts. Selectively bred for their small size, smaller dog breeds like the Chihuahua (average lifespan of 15–20 years) have the B/B genotype for the IGF-1 haplotype, reducing the amount of IGF-1 produced. Conversely, large dogs like the Great Dane (average lifespan of 6–8 years) are homozygous for the IGF-1 I allele, which increases the amount of IGF-1 production.\n\nInitially, it was believed that growth hormone actually prolonged lifespan due to a 1990 study that indicated that injection of growth hormone to men over 60 years of age appeared to reverse various biomarkers implicated in aging, such as decreased muscle mass, bone density, skin thickness, and increased adipose tissue. However, a 1999 study found that administering growth hormone also significantly increased mortality rate. Recent genomic studies have confirmed that the genes involved in growth hormone uptake and signaling are largely conserved across a plethora of species, such as yeast, nematodes, fruit flies, mice and humans. These studies have also shown that individuals with Laron syndrome, an autosomal recessive disorder resulting in dwarfism due to defects in growth hormone receptors, have increased lifespan. Additionally, these individuals have much lower incidences of age-related diseases such as type 2 diabetes and cancer. Lastly, human centenarians around the world are disproportionately of short stature, and have low levels of IGF-1.\n\nNumerous studies have found that lifespan is inversely correlated with both the total amount of offspring birthed, as well as the age at which females first gives birth, also known as primiparity. Additionally, it has been found that reproduction is a costly mechanism that alters the metabolism of fat. Lipids invested in reproduction would be unable to be allocated to support mechanisms involved in somatic maintenance.\n\nThe disposable soma theory has been consistent with the majority of animal models. It was found in numerous animal studies that castration or genetic deformities of reproduction organs was correlated with increased lifespan. Moreover, in red squirrels, it was found that females with an early primiparity achieved the highest immediate and lifetime reproductive success. However, it was also found that these same individuals had a decreased median and maximum lifespan. Specifically squirrels who mated earlier had a 22.4% rate of mortality until two years of age compared to a 16.5% rate of mortality in late breeders. In addition, these squirrels had an average maximum lifespan of 1035 days compared to an average maximum lifespan of 1245 days for squirrels that bred later.\n\nIn another study, researchers selectively bred fruit flies over three years to develop two different strains, an early-reproducing strain and a late-reproducing strain. The late-reproducing line had a significantly longer lifespan than the early-reproducing line. Even more telling was that when the researchers introduced a mutation in the ovarian-associated gene \"ovoD1\", resulting in defective oogenesis, the differences in lifespan between the two lines disappeared. The researchers in this case concluded that \"aging has evolved primarily because of the damaging effects of reproduction earlier in life\".\n\nProminent aging researcher Steven Austad also performed a large-scale ecological study on the coast of Georgia in 1993. Austad isolated two opossum populations, one from the predator-infested mainland and one from the predator-absent nearby island of Sapelo. According to the disposable soma theory, a genetically isolated population subject to low environmentally-induced mortality would evolve delayed reproduction and aging. This is because without the pressure of predation, it would be evolutionarily advantageous to allocate more resources to somatic maintenance than reproduction, as early offspring mortality would be low. As predicted, even after controlling for predation, the isolated population had a longer lifespan, delayed primiparity, and reduced aging biomarkers such as tail collagen cross-linking.\n\nIn general, only a few studies exist in human models. It was found that castrated men live longer than their fertile counterparts. Further studies found that in British women, primiparity was earliest in women who died early and latest in women who died at the oldest ages. Furthermore, increased number of children birthed was associated with a decreased lifespan. A final study found that female centenarians were more likely to have children in later life compared average, especially past the age of 40. The researchers discovered that 19.2% of female centenarians had their first child after the age of 40, compared to 5.5% of the rest of the female population.\n\nThere are numerous studies that support cellular damage, often due to a lack of somatic maintenance mechanisms, as a primary determinant for aging, and these studies have given rise to the free radical theory of aging and the DNA damage theory of aging. One study found that the cells of short-living rodents \"in vitro\" show much greater mutation rates and a general lack of genome surveillance compared to human cells and are far more susceptible to oxidative stress. \nOther studies have been conducted on the naked mole rat, a rodent species with remarkable longevity (30 years), capable of outliving the brown rat (3 years) by ten-fold. Additionally, almost no incidence cancer has ever been detected in naked mole rats. Nearly all of the differences found between these two organisms, which are otherwise rather genetically similar, was in somatic maintenance. Naked mole rats were found to have higher levels of superoxide dismutase, a reactive oxygen species clearing antioxidant. In addition, naked mole rats had higher levels of base excision repair, DNA damage response signaling, homologous recombination repair, mismatch repair, nucleotide excision repair, and non-homologous end joining. In fact, many of these processes were near or exceeded human levels. Proteins from naked mole rats were also more resistant to oxidation, misfolding, ubiquitination, and had increased translational fidelity.\n\nFurther studies have been conducted on patients with Hutchinson-Gilford Progeria Syndrome (HGPS), a condition that leads to premature aging. Patients with HGPS typically age about seven times faster than average and usually succumb to the disease in their early teens. Patients with HGPS have cellular defects, specifically in the lamin proteins, which regulate the organization of the lamina and nuclear envelope for mitosis. \nLastly, as mentioned previously, it has been found that the suppression of autophagy is associated with reduced lifespan, while stimulation is associated with extended lifespan. Activated in times of caloric restriction, autophagy is a process that prevents cellular damage through clearance and recycling of damaged proteins and organelles.\n\nOne of the main weaknesses of the disposable soma theory is that it does not postulate any specific cellular mechanisms to which an organism shifts energy to somatic repair over reproduction. Instead, it only offers an evolutionary perspective on why aging may occur due to reproduction. Therefore, parts of it are rather limited outside of the field of evolutionary biology.\n\nCritics have pointed out the supposed inconsistencies of the disposable soma theory due to the observed effects of caloric restriction, which is correlated with increased lifespan. Although it activates autophagy, according to classical disposable soma principles, with less caloric intake, there would less total energy to be distributed towards somatic maintenance, and decreased lifespan would be observed (or at least the positive autophagic effects would be balanced out). However, Kirkwood, alongside his collaborator Darryl P. Shanley, assert that caloric restriction triggers an adaptive mechanism which causes the organism to shift a higher proportion of resources to somatic maintenance, away from reproduction. This theory is supported by multiple studies, which show that caloric restriction typically results in impaired fertility, but leave an otherwise healthy organism. Evolutionarily, an organism would want to delay reproduction to when resources were more plentiful. During a resource-barren period, it would evolutionarily unwise to invest resources in progeny that would be unlikely to survive in famine. Mechanistically, the NAD-dependent deacetylase Sirtuin 1 (SIRT-1) is upregulated during low-nutrient periods. SIRT-1 increases insulin sensitivity, decreases the amount of inflammatory cytokines, stimulates autophagy, and activates FOXO, the aforementioned protein involved in activating stress response genes. SIRT-1 is also found to result in decreased fertility.\n\nIn additional to differential partitioning of energy allocation during caloric restriction, less caloric intake would result in less metabolic waste in the forms of free radicals like hydrogen peroxide, superoxide and hydroxyl radicals, which damage important cellular components, particularly mitochondria. Elevated levels of free radicals in mice has been correlated with neurodegeneration, myocardial injury, severe anemia, and premature death.\n\nAnother primary criticism of the disposable soma theory is that it fails to account for why women tend to live longer than their male counterparts. Afterall, females invest significantly more resources into reproduction and according to the classical disposable soma principles, this would compromise energy diverted to somatic maintenance. However, this can be reconciled with the grandmother hypothesis. The Grandmother Hypothesis states that menopause comes about into older women in order to restrict the time of reproduction as a protective mechanism. This would allow women to live longer and increase the amount of care they could provide to their grandchildren, increasing their evolutionary fitness. And so, although women do invest a greater proportion of resources into reproduction during their fertile years, their overall reproductive period is significantly shorter than men, who are able of reproduction during and even beyond middle age. Additionally, males invest more resources into growth, and have significantly higher levels of IGF-1 compared to females, which is correlated with decreased lifespan. Other variables such as increased testosterone levels in males are not accounted for. Increased testosterone is often associated with reckless behaviour, which may lead to a high accidental death rate.\n\nA few contradicting animal models weaken the validity of the disposable soma theory. This includes studies done on the aforementioned naked mole rats. In these studies, it was found that reproductive naked mole rats actually show significantly increased lifespans compared to non-reproductive individuals, which contradicts the principles of diposable soma. However, although these naked mole rats are mammalian, they are highly atypical in terms of aging studies and may not serve as the best model for humans. For example, naked mole rats have a disproportionately high longevity quotient and live in eusocial societies, where breeding is usually designated to a queen.\n\nThe disposable soma theory is tested disproportionately on female organisms for the relationship between reproduction and aging, as females carry a greater burden in reproduction. Additionally, for the relationship between growth and aging, studies are disproportionately conducted on males, to minimize the hormonal fluctuations that occur with menstrual cycling. Lastly, genetic and environmental factors, rather than reproductive patterns, may explain the variations in human lifespan. For example, studies have shown that poorer individuals, to whom nutritious food and medical care is less accessible, typically have higher birth rates and earlier primiparity.\n\n\n"}
{"id": "57151017", "url": "https://en.wikipedia.org/wiki?curid=57151017", "title": "Drakoo wave energy converter", "text": "Drakoo wave energy converter\n\nThe Drakoo wave energy converter is a technological device that uses the motion of ocean surface waves to generate electricity.\n\nThe Drakoo WEC does not fall under any of the usual wave energy converter classifications: its working principle, based on a twin-chamber oscillating water column system, is to transform waves into a continuous water flow which drives a hydro turbine generator.\n\nafter being patented in 2008, the Drakoo technology has successfully been tested in NTU lab and, subsequently, in the deep wave flume of NAREC in 2012(UK).\n\nOn December 23, 2016, Hann Ocean Energy, the Singapore wave developer company which invented and patented the technology, announced the first successful power production during a trial in its testing facility in Nantong, with a peak power output of 3.8 kW at a wave height of 0.6m.\n\nThe technology continuously increased its performances along the years, reaching firstly a peak output power of 9.3 kW in November 2017 and of 11.2 kW in March 2018. Moreover, during the World Future Energy Summit 2018, in Abu Dhabi, Hann Ocean Energy reported about sales inquiries from the Persian Gulf for the application of the Drakoo WEC for wellhead platforms.\n"}
{"id": "13566263", "url": "https://en.wikipedia.org/wiki?curid=13566263", "title": "Dukhin number", "text": "Dukhin number\n\nThe Dukhin number () is a dimensionless quantity that characterizes the contribution of the surface conductivity to various electrokinetic and electroacoustic effects, as well as to electrical conductivity and permittivity of fluid heterogeneous systems. \n\nIt was introduced by Lyklema in “Fundamentals of Interface and Colloid Science”. A recent IUPAC Technical Report used this term explicitly and detailed several means of measurement in physical systems.\n\nThe Dukhin number is a ratio of the surface conductivity formula_1 to the fluid bulk electrical conductivity K multiplied by particle size \"a\":\n"}
{"id": "944638", "url": "https://en.wikipedia.org/wiki?curid=944638", "title": "Earth's energy budget", "text": "Earth's energy budget\n\nEarth's energy budget accounts for the balance between the energy Earth receives from the Sun, the energy Earth radiates back into outer space after having been distributed throughout the five components of Earth's climate system and having thus powered the so-called Earth’s heat engine. This system is made up of earth's water, ice, atmosphere, rocky crust, and all living things.\n\nQuantifying changes in these amounts is required to accurately model the Earth's climate. \n\nReceived radiation is unevenly distributed over the planet, because the Sun heats equatorial regions more than polar regions. The atmosphere and ocean work non-stop to even out solar heating imbalances through evaporation of surface water, convection, rainfall, winds, and ocean circulation. Earth is very close to being in radiative equilibrium, the situation where the incoming solar energy is balanced by an equal flow of heat to space; under that condition, global temperatures will be \"relatively\" stable. Globally, over the course of the year, the Earth system—land surfaces, oceans, and atmosphere—absorbs and then radiates back to space an average of about 240 watts of solar power per square meter. Anything that increases or decreases the amount of incoming or outgoing energy will change global temperatures in response.\n\nHowever, Earth's energy balance and heat fluxes depend on many factors, such as atmospheric composition (mainly aerosols and greenhouse gases), the albedo (reflectivity) of surface properties, cloud cover and vegetation and land use patterns.\n\nChanges in surface temperature due to Earth's energy budget do not occur instantaneously, due to the inertia of the oceans and the cryosphere. The net heat flux is buffered primarily by becoming part of the ocean's heat content, until a new equilibrium state is established between radiative forcings and the climate response.\n\nIn spite of the enormous transfers of energy into and from the Earth, it maintains a relatively constant temperature because, as a whole, there is little net gain or loss: Earth emits via atmospheric and terrestrial radiation (shifted to longer electromagnetic wavelengths) to space about the same amount of energy as it receives via insolation (all forms of electromagnetic radiation).\n\nTo quantify Earth's \"heat budget\" or \"heat balance\", let the insolation received at the top of the atmosphere be 100 units (100 units = about 1,360 watts per square meter facing the sun), as shown in the accompanying illustration. Called the albedo of Earth, around 35 units are reflected back to space: 27 from the top of clouds, 2 from snow and ice-covered areas, and 6 by other parts of the atmosphere. The 65 remaining units are absorbed: 14 within the atmosphere and 51 by the Earth’s surface. These 51 units are radiated to space in the form of terrestrial radiation: 17 directly radiated to space and 34 absorbed by the atmosphere (19 through latent heat of condensation, 9 via convection and turbulence, and 6 directly absorbed). The 48 units absorbed by the atmosphere (34 units from terrestrial radiation and 14 from insolation) are finally radiated back to space. These 65 units (17 from the ground and 48 from the atmosphere) balance the 65 units absorbed from the sun in order to maintain zero net gain of energy by the Earth.\n\nThe total amount of energy received per second at the top of Earth's atmosphere (TOA) is measured in watts and is given by the solar constant times the cross-sectional area of the Earth. Because the surface area of a sphere is four times the cross-sectional surface area of a sphere (i.e. the area of a circle), the average TOA flux is one quarter of the solar constant and so is approximately 340 W/m². Since the absorption varies with location as well as with diurnal, seasonal and annual variations, the numbers quoted are long-term averages, typically averaged from multiple satellite measurements.\n\nOf the ~340 W/m² of solar radiation received by the Earth, an average of ~77 W/m² is reflected back to space by clouds and the atmosphere and ~23 W/m² is reflected by the surface albedo, leaving ~240 W/m² of solar energy input to the Earth's energy budget. This gives the earth a mean net albedo of 0.29.\n\nThe geothermal heat flux from the Earth's interior is estimated to be 47 terawatts. This comes to 0.087 watt/square metre, which represents only 0.027% of Earth's total energy budget at the surface, which is dominated by 173,000 terawatts of incoming solar radiation.\n\nHuman production of energy is even lower, at an estimated 18 TW.\n\nPhotosynthesis has a larger effect: photosynthetic efficiency turns up to 2% of incoming sunlight into biomass, for a total photosynthetic productivity of earth between ~1500–2250 TW (~1%+/-0.26% solar energy hitting the Earth's surface).\n\nOther minor sources of energy are usually ignored in these calculations, including accretion of interplanetary dust and solar wind, light from stars other than the Sun and the thermal radiation from space. Earlier, Joseph Fourier had claimed that deep space radiation was significant in a paper often cited as the first on the greenhouse effect.\n\nLongwave radiation is usually defined as outgoing infrared energy leaving the planet. However, the atmosphere absorbs parts initially, or cloud cover can reflect radiation. Generally, heat energy is transported between the planet's surface layers (land and ocean) to the atmosphere, transported via evapotranspiration and latent heat fluxes or conduction/convection processes. Ultimately, energy is radiated in the form of longwave infrared radiation back into space.\n\nRecent satellite observations indicate additional precipitation, which is sustained by increased energy leaving the surface through evaporation (the latent heat flux), offsetting increases in longwave flux to the surface.\n\nIf the incoming energy flux is not equal to the outgoing energy flux, net heat is added to or lost by the planet (if the incoming flux is larger or smaller than the outgoing respectively).\n\nAn imbalance must show in something on Earth warming or cooling (depending on the direction of the imbalance), and the ocean being the larger thermal reservoir on Earth, is a prime candidate for measurements.\n\nEarth's energy imbalance measurements provided by Argo floats have detected an accumulation of ocean heat content (OHC). The estimated imbalance was measured during a deep solar minimum of 2005–2010 to be 0.58 ± 0.15 W/m². This level of detail cannot be inferred directly from measurements of surface energy fluxes, which have combined uncertainties of the order of ± 17 W/m².\n\nSeveral satellites indirectly measure the energy absorbed and radiated by Earth and by inference the energy imbalance. The NASA Earth Radiation Budget Experiment (ERBE) project involves three such satellites: the Earth Radiation Budget Satellite (ERBS), launched October 1984; NOAA-9, launched December 1984; and NOAA-10, launched September 1986.\n\nToday NASA's satellite instruments, provided by CERES, part of the NASA's Earth Observing System (EOS), are designed to measure both solar-reflected and Earth-emitted radiation.\n\nThe major atmospheric gases (oxygen and nitrogen) are transparent to incoming sunlight but are also transparent to outgoing thermal (infrared) radiation. However, water vapor, carbon dioxide, methane and other trace gases are opaque to many wavelengths of thermal radiation. The Earth's surface radiates the net equivalent of 17 percent of the incoming solar energy in the form of thermal infrared. However, the amount that directly escapes to space is only about 12 percent of incoming solar energy. The remaining fraction, 5 to 6 percent, is absorbed by the atmosphere by greenhouse gas molecules.\nWhen greenhouse gas molecules absorb thermal infrared energy, their temperature rises. Those gases then radiate an increased amount of thermal infrared energy in all directions. Heat radiated upward continues to encounter greenhouse gas molecules; those molecules also absorb the heat, and their temperature rises and the amount of heat they radiate increases. The atmosphere thins with altitude, and at roughly 5–6 kilometres, the concentration of greenhouse gases in the overlying atmosphere is so thin that heat can escape to space.\n\nBecause greenhouse gas molecules radiate infrared energy in all directions, some of it spreads downward and ultimately returns to the Earth's surface, where it is absorbed. The Earth's surface temperature is thus higher than it would be if it were heated only by direct solar heating. This supplemental heating is the natural greenhouse effect. It is as if the Earth is covered by a blanket that allows high frequency radiation (sunlight) to enter, but slows the rate at which the low frequency infrared radiant energy emitted by the Earth leaves.\n\nA change in the incident radiated portion of the energy budget is referred to as a radiative forcing.\n\nClimate sensitivity is the steady state change in the equilibrium temperature as a result of changes in the energy budget.\n\nClimate forcings are changes that cause temperatures to rise or fall, disrupting the energy balance. Natural climate forcings include changes in the Sun's brightness, Milankovitch cycles (small variations in the shape of Earth's orbit and its axis of rotation that occur over thousands of years) and volcanic eruptions that inject light-reflecting particles as high as the stratosphere. Man-made forcings include particle pollution (aerosols) that absorb and reflect incoming sunlight; deforestation, which changes how the surface reflects and absorbs sunlight; and the rising concentration of atmospheric carbon dioxide and other greenhouse gases, which decreases the rate at which heat is radiated to space.\n\nA forcing can trigger feedbacks that intensify (positive feedback) or weaken (negative feedback) the original forcing. For example, loss of ice at the poles, which makes them less reflective, causes greater absorption of energy and so increases the rate at which the ice melts, is an example of a positive feedback.\n\nThe observed planetary energy imbalance during the recent solar minimum shows that solar forcing of climate, although natural and significant, is overwhelmed by anthropogenic climate forcing.\n\nIn 2012, NASA scientists reported that to stop global warming atmospheric CO content would have to be reduced to 350 ppm or less, assuming all other climate forcings were fixed. The impact of anthropogenic aerosols has not been quantified, but individual aerosol types are thought to have substantial heating and cooling effects.\n\n\n"}
{"id": "4396171", "url": "https://en.wikipedia.org/wiki?curid=4396171", "title": "Earth's rotation", "text": "Earth's rotation\n\nEarth's rotation is the rotation of Planet Earth around its own axis. Earth rotates eastward, in prograde motion. As viewed from the north pole star Polaris, Earth turns counter clockwise.\n\nThe North Pole, also known as the Geographic North Pole or Terrestrial North Pole, is the point in the Northern Hemisphere where Earth's axis of rotation meets its surface. This point is distinct from Earth's North Magnetic Pole. The South Pole is the other point where Earth's axis of rotation intersects its surface, in Antarctica.\n\nEarth rotates once in about 24 hours with respect to the Sun, but once every 23 hours, 56 minutes, and 4 seconds with respect to the stars (see below). Earth's rotation is slowing slightly with time; thus, a day was shorter in the past. This is due to the tidal effects the Moon has on Earth's rotation. Atomic clocks show that a modern day is longer by about 1.7 milliseconds than a century ago, slowly increasing the rate at which UTC is adjusted by leap seconds. Analysis of historical astronomical records shows a slowing trend of about 2.3 milliseconds per century since the 8th century BCE.\n\nAmong the ancient Greeks, several of the Pythagorean school believed in the rotation of the earth rather than the apparent diurnal rotation of the heavens. Perhaps the first was Philolaus (470–385 BCE), though his system was complicated, including a counter-earth rotating daily about a central fire.\n\nA more conventional picture was that supported by Hicetas, Heraclides and Ecphantus in the fourth century BCE who assumed that the earth rotated but did not suggest that the earth revolved about the sun. In the third century BCE, Aristarchus of Samos suggested the sun's central place.\n\nHowever, Aristotle in the fourth century BCE criticized the ideas of Philolaus as being based on theory rather than observation. He established the idea of a sphere of fixed stars that rotated about the earth. This was accepted by most of those who came after, in particular Claudius Ptolemy (2nd century CE), who thought the earth would be devastated by gales if it rotated.\n\nIn 499 CE, the Indian astronomer Aryabhata wrote that the spherical earth rotates about its axis daily, and that the apparent movement of the stars is a relative motion caused by the rotation of the Earth. He provided the following analogy: \"Just as a man in a boat going in one direction sees the stationary things on the bank as moving in the opposite direction, in the same way to a man at Lanka the fixed stars appear to be going westward.\"\n\nIn the 10th century, some Muslim astronomers accepted that the Earth rotates around its axis. According to al-Biruni, Abu Sa'id al-Sijzi (d. circa 1020) invented an astrolabe called \"al-zūraqī\" based on the idea believed by some of his contemporaries \"that the motion we see is due to the Earth's movement and not to that of the sky.\" The prevalence of this view is further confirmed by a reference from the 13th century which states: \"According to the geometers [or engineers] (\"muhandisīn\"), the earth is in constant circular motion, and what appears to be the motion of the heavens is actually due to the motion of the earth and not the stars.\" Treatises were written to discuss its possibility, either as refutations or expressing doubts about Ptolemy's arguments against it. At the Maragha and Samarkand observatories, the Earth's rotation was discussed by Tusi (b. 1201) and Qushji (b. 1403); the arguments and evidence they used resemble those used by Copernicus.\n\nIn medieval Europe, Thomas Aquinas accepted Aristotle's view and so, reluctantly, did John Buridan and Nicole Oresme in the fourteenth century. Not until Nicolaus Copernicus in 1543 adopted a heliocentric world system did the contemporary understanding of earth's rotation begin to be established. Copernicus pointed out that if the movement of the earth is violent, then the movement of the stars must be very much more so. He acknowledged the contribution of the Pythagoreans and pointed to examples of relative motion. For Copernicus this was the first step in establishing the simpler pattern of planets circling a central sun.\n\nTycho Brahe, who produced accurate observations on which Kepler based his laws, used Copernicus's work as the basis of a system assuming a stationary earth. In 1600, William Gilbert strongly supported the earth's rotation in his treatise on the earth's magnetism and thereby influenced many of his contemporaries. Those like Gilbert who did not openly support or reject the motion of the earth about the sun are often called \"semi-Copernicans\". A century after Copernicus, Riccioli disputed the model of a rotating earth due to the lack of then-observable eastward deflections in falling bodies; such deflections would later be called the Coriolis effect. However, the contributions of Kepler, Galileo and Newton gathered support for the theory of the rotation of the Earth.\n\nEarth's rotation implies that the Equator bulges and the geographical poles are flattened. In his \"Principia\", Newton predicted this flattening would occur in the ratio of 1:230, and pointed to the pendulum measurements taken by Richer in 1673 as corroboration of the change in gravity, but initial measurements of meridian lengths by Picard and Cassini at the end of the 17th century suggested the opposite. However, measurements by Maupertuis and the French Geodesic Mission in the 1730s established the oblateness of Earth, thus confirming the positions of both Newton and Copernicus.\n\nIn the Earth's rotating frame of reference, a freely moving body follows an apparent path that deviates from the one it would follow in a fixed frame of reference. Because of the Coriolis effect, falling bodies veer slightly eastward from the vertical plumb line below their point of release, and projectiles veer right in the Northern Hemisphere (and left in the Southern) from the direction in which they are shot. The Coriolis effect is mainly observable at a meteorological scale, where it is responsible for the opposite directions of cyclone rotation in the Northern and Southern hemispheres (anticlockwise and clockwise, respectively).\n\nHooke, following a suggestion from Newton in 1679, tried unsuccessfully to verify the predicted eastward deviation of a body dropped from a height of , but definitive results were obtained later, in the late 18th and early 19th century, by Giovanni Battista Guglielmini in Bologna, Johann Friedrich Benzenberg in Hamburg and Ferdinand Reich in Freiberg, using taller towers and carefully released weights. A ball dropped from a height of departed by from the vertical compared with a calculated value of .\n\nThe most celebrated test of Earth's rotation is the Foucault pendulum first built by physicist Léon Foucault in 1851, which consisted of a lead-filled brass sphere suspended from the top of the Panthéon in Paris. Because of the Earth's rotation under the swinging pendulum, the pendulum's plane of oscillation appears to rotate at a rate depending on latitude. At the latitude of Paris the predicted and observed shift was about clockwise per hour. Foucault pendulums now swing in museums around the world.\n\nEarth's rotation period relative to the Sun (solar noon to solar noon) is its \"true solar day\" or \"apparent solar day\". It depends on the Earth's orbital motion and is thus affected by changes in the eccentricity and inclination of Earth's orbit. Both vary over thousands of years, so the annual variation of the true solar day also varies. Generally, it is longer than the mean solar day during two periods of the year and shorter during another two. The true solar day tends to be longer near perihelion when the Sun apparently moves along the ecliptic through a greater angle than usual, taking about longer to do so. Conversely, it is about shorter near aphelion. It is about longer near a solstice when the projection of the Sun's apparent motion along the ecliptic onto the celestial equator causes the Sun to move through a greater angle than usual. Conversely, near an equinox the projection onto the equator is shorter by about . Currently, the perihelion and solstice effects combine to lengthen the true solar day near by solar seconds, but the solstice effect is partially cancelled by the aphelion effect near when it is only longer. The effects of the equinoxes shorten it near and by and , respectively.\n\nThe average of the true solar day during the course of an entire year is the \"mean solar day\", which contains solar seconds. Currently, each of these seconds is slightly longer than an SI second because Earth's mean solar day is now slightly longer than it was during the 19th century due to tidal friction. The average length of the mean solar day since the introduction of the leap second in 1972 has been about 0 to 2 ms longer than 86,400 SI seconds. Random fluctuations due to core-mantle coupling have an amplitude of about 5 ms. The mean solar second between 1750 and 1892 was chosen in 1895 by Simon Newcomb as the independent unit of time in his Tables of the Sun. These tables were used to calculate the world's ephemerides between 1900 and 1983, so this second became known as the ephemeris second. In 1967 the SI second was made equal to the ephemeris second.\n\nThe apparent solar time is a measure of the Earth's rotation and the difference between it and the mean solar time is known as the equation of time.\n\nEarth's rotation period relative to the fixed stars, called its \"stellar day\" by the International Earth Rotation and Reference Systems Service (IERS), is seconds of mean solar time (UT1) , mean solar days). Earth's rotation period relative to the precessing mean vernal equinox, named \"sidereal day\", is seconds of mean solar time (UT1) , mean solar days). Thus, the sidereal day is shorter than the stellar day by about .\n\nBoth the stellar day and the sidereal day are shorter than the mean solar day by about . The mean solar day in SI seconds is available from the IERS for the periods and .\n\nRecently (1999–2010) the average annual length of the mean solar day in excess of 86,400 SI seconds has varied between and , which must be added to both the stellar and sidereal days given in mean solar time above to obtain their lengths in SI seconds (see Fluctuations in the length of day).\n\nThe angular speed of Earth's rotation in inertial space is radians per SI second (mean solar second). Multiplying by (180°/π radians) × (86,400 seconds/mean solar day) yields 360.9856°/mean solar day, indicating that Earth rotates more than 360° relative to the fixed stars in one solar day. Earth's movement along its nearly circular orbit while it is rotating once around its axis requires that Earth rotate slightly more than once relative to the fixed stars before the mean Sun can pass overhead again, even though it rotates only once (360°) relative to the mean Sun. Multiplying the value in rad/s by Earth's equatorial radius of (WGS84 ellipsoid) (factors of 2π radians needed by both cancel) yields an equatorial speed of , or . Some sources state that Earth's equatorial speed is slightly less, or . This is obtained by dividing Earth's equatorial circumference by . However, the use of only one circumference unwittingly implies only one rotation in inertial space, so the corresponding time unit must be a sidereal hour. This is confirmed by multiplying by the number of sidereal days in one mean solar day, , which yields the equatorial speed in mean solar hours given above of .\n\nThe tangential speed of Earth's rotation at a point on Earth can be approximated by multiplying the speed at the equator by the cosine of the latitude. For example, the Kennedy Space Center is located at latitude 28.59° N, which yields a speed of: cos 28.59° × = \n\nThe Earth's rotation axis moves with respect to the fixed stars (inertial space); the components of this motion are precession and nutation. It also moves with respect to the Earth's crust; this is called polar motion.\n\nPrecession is a rotation of the Earth's rotation axis, caused primarily by external torques from the gravity of the Sun, Moon and other bodies. The polar motion is primarily due to free core nutation and the Chandler wobble.\n\nOver millions of years, the Earth's rotation slowed significantly by tidal acceleration through gravitational interactions with the Moon. In this process, angular momentum is slowly transferred to the Moon at a rate proportional to formula_1, where formula_2 is the orbital radius of the Moon. This process gradually increased the length of day to its current value and resulted in the Moon being tidally locked with the Earth.\n\nThis gradual rotational deceleration is empirically documented with estimates of day lengths obtained from observations of tidal rhythmites and stromatolites; a compilation of these measurements found the length of day to increase steadily from about 21 hours at 600Myr ago to the current 24 hour value. By counting the microscopic lamina that form at higher tides, tidal frequencies (and thus day lengths) can be estimated, much like counting tree rings, though these estimates can be increasingly unreliable at older ages.\n\nThe current rate of tidal deceleration is anomalously high, implying the Earth's rotational velocity must have decreased more slowly in the past. Empirical data tentatively shows a sharp increase in rotational deceleration about 600Myr ago. Some models suggest that the Earth maintained a constant day length of 21 hours throughout much of the Precambrian. This day length corresponds to the semidiurnal resonant period of the thermally-driven atmospheric tide; at this day length, the decelerative lunar torque could have been canceled by an accelerative torque from the atmospheric tide, resulting in no net torque and a constant rotational period. This stabilizing effect could have been broken by a sudden change in global temperature. Recent computational simulations support this hypothesis and suggest the Marinoan or Sturtian glaciations broke this stable configuration about 600Myr ago, citing the resemblance of simulated results and existing paleorotational data.\n\nAdditionally, some large-scale events, such as the 2004 Indian Ocean earthquake, have caused the length of a day to shorten by 3 microseconds by affecting the Earth's moment of inertia. Post-glacial rebound, ongoing since the last Ice age, is also changing the distribution of the Earth's mass thus affecting the moment of inertia of the Earth and, by the conservation of angular momentum, the Earth's rotation period.\n\nThe primary monitoring of the Earth's rotation is performed with very-long-baseline interferometry coordinated with the Global Positioning System, satellite laser ranging, and other satellite techniques. This provides an absolute reference for the determination of universal time, precession, and nutation.\n\nThere are recorded observations of solar and lunar eclipses by Babylonian and Chinese astronomers beginning in the 8th century BCE, as well as from the medieval Islamic world and elsewhere. These observations can be used to determine changes in the Earth's rotation over the last 27 centuries, since the length of the day is a critical parameter in the calculation of the place and time of eclipses. A change in day length of milliseconds per century shows up as a change of hours and thousands of kilometers in eclipse observations. The ancient data are consistent with a shorter day, meaning the Earth was turning faster throughout the past.\n\nThe Earth's original rotation was a vestige of the original angular momentum of the cloud of dust, rocks, and gas that coalesced to form the Solar System. This primordial cloud was composed of hydrogen and helium produced in the Big Bang, as well as heavier elements ejected by supernovas. As this interstellar dust is heterogeneous, any asymmetry during gravitational accretion resulted in the angular momentum of the eventual planet.\n\nHowever, if the giant-impact hypothesis for the origin of the Moon is correct, this primordial rotation rate would have been reset by the Theia impact 4.5 billion years ago. Regardless of the speed and tilt of the Earth's rotation before the impact, it would have experienced a day some five hours long after the impact. Tidal effects would then have slowed this rate to its modern value.\n\n"}
{"id": "40159918", "url": "https://en.wikipedia.org/wiki?curid=40159918", "title": "Ecosystem health", "text": "Ecosystem health\n\nEcosystem health is a metaphor used to describe the condition of an ecosystem. Ecosystem condition can vary as a result of fire, flooding, drought, extinctions, invasive species, climate change, mining, overexploitation in fishing, farming or logging, chemical spills, and a host of other reasons. There is no universally accepted benchmark for a healthy ecosystem, rather the apparent health status of an ecosystem can vary depending upon which health metrics are employed in judging it and which societal aspirations are driving the assessment. Advocates of the health metaphor argue for its simplicity as a communication tool. \"Policy-makers and the public need simple, understandable concepts like health.\" Critics worry that ecosystem health, a \"value-laden construct\", is often \"passed off as science to unsuspecting policy makers and the public.\"\n\nThe health metaphor applied to the environment has been in use at least since the early 1800s and the great American conservationist Aldo Leopold (1887–1948) spoke metaphorically of land health, land sickness, mutilation, and violence when describing land use practices. The term \"ecosystem management\" has been in use at least since the 1950s. The term \"ecosystem health\" has become widespread in the ecological literature, as a general metaphor meaning something good, and as an environmental quality goal in field assessments of rivers, lakes, seas, and forests.\n\nRecently however this metaphor has been subject of quantitative formulation using complex systems concepts such as criticality, meaning that a healthy ecosystem is in some sort of balance between adaptability (randomness) and robustness (order) . Nevertheless the universality of criticality is still under examination and is known as the Criticality Hypothesis, which states that systems in a dynamic regime shifting between order and disorder, attain the highest level of computational capabilities and achieve an optimal trade-off between robustness and flexibility. Recent results in cell and evolutionary biology, neuroscience and computer science have great interest in the criticality hypothesis, emphasizing its role as a viable candidate general law in the realm of adaptive complex systems (see and references therein).\n\nThe term ecosystem health has been employed to embrace some suite of environmental goals deemed desirable. Edward Grumbine's highly cited paper \"What is ecosystem management?\" surveyed ecosystem management and ecosystem health literature and summarized frequently encountered goal statements:\n\nGrumbine describes each of these goals as a \"value statement\" and stresses the role of human values in setting ecosystem management goals.\n\nIt is the last goal mentioned in the survey, accommodating humans, that is most contentious. \"We have observed that when groups of stakeholders work to define … visions, this leads to debate over whether to emphasize ecosystem health or human well-being … Whether the priority is ecosystems or people greatly influences stakeholders' assessment of desirable ecological and social states.\" and, for example, \"For some, wolves are critical to ecosystem health and an essential part of nature, for others they are a symbol of government overreach threatening their livelihoods and cultural values.\"\n\nMeasuring ecosystem health requires extensive goal-driven environmental sampling. For example, a vision for ecosystem health of Lake Superior was developed by a public forum and a series of objectives were prepared for protection of habitat and maintenance of populations of some 70 indigenous fish species. A suite of 80 lake health indicators was developed for the Great Lakes Basin including monitoring native fish species, exotic species, water levels, phosphorus levels, toxic chemicals, phytoplankton, zooplankton, fish tissue contaminants, etc.\n\nSome authors have attempted broad definitions of ecosystem health, such as benchmarking as healthy the historical ecosystem state \"prior to the onset of anthropogenic stress.\" A difficulty is that the historical composition of many human-altered ecosystems is unknown or unknowable. Also, fossil and pollen records indicate that the species that occupy an ecosystem reshuffle through time, so it is difficult to identify one snapshot in time as optimum or \"healthy.\".\n\nA commonly cited broad definition states that a healthy ecosystem has three attributes:\n\nWhile this captures significant ecosystem properties, a generalization is elusive as those properties do not necessarily co-vary in nature. For example, there is not necessarily a clear or consistent relationship between productivity and species richness. Similarly, the relationship between resilience and diversity is complex, and ecosystem stability may depend upon one or a few species rather than overall diversity. And some undesirable ecosystems are highly productive.\n\n\"Resilience is not desirable per se. There can be highly resilient states of ecosystems which are very undesirable from some human perspectives , such as algal-dominated coral reefs.\" Ecological resilience is a \"capacity\" that varies depending upon which properties of the ecosystem are to be studied and depending upon what kinds of disturbances are considered and how they are to be quantified. Approaches to assessing it \"face high uncertainties and still require a considerable amount of empirical and theoretical research.\"\n\nOther authors have sought a numerical index of ecosystem health that would permit quantitative comparisons among ecosystems and within ecosystems over time. One such system employs ratings of the three properties mentioned above: Health = system vigor x system organization x system resilience. Ecologist Glenn Suter argues that such indices employ \"nonsense units,\" the indices have \"no meaning; they cannot be predicted, so they are not applicable to most regulatory problems; they have no diagnostic power; effects of one component are eclipsed by responses of other components, and the reason for a high or low index value is unknown.\"\n\nHealth metrics are determined by stakeholder goals, which drive ecosystem definition. An ecosystem is an abstraction. \"Ecosystems cannot be identified or found in nature. Instead, they must be delimited by an observer. This can be done in many different ways for the same chunk of nature, depending on the specific perspectives of interest.\"\n\nEcosystem definition determines the acceptable range of variability (reference conditions) and determines measurement variables. The latter are used as indicators of ecosystem structure and function, and can be used as indicators of \"health\".\n\nAn indicator is a variable, such as a chemical or biological property, that when measured, is used to infer trends in another (unmeasured) environmental variable or cluster of unmeasured variables (the indicandum). For example, rising mortality rate of canaries in a coal mine is an indicator of rising carbon monoxide levels. Rising chlorophyll-a levels in a lake may signal eutrophication.\n\nEcosystem assessments employ two kinds of indicators, descriptive indicators and normative indicators. \"Indicators can be used descriptively for a scientific purpose or normatively for a political purpose.\"\n\nUsed descriptively, high chlorophyll-a is an indicator of eutrophication, but it may also be used as an ecosystem health indicator. When used as a normative (health) indicator, it indicates a rank on a health scale, a rank that can vary widely depending on societal preferences as to what is desirable. A high chlorophyll-a level in a natural successional wetland might be viewed as healthy whereas a human-impacted wetland with the \"same\" indicator value may be judged unhealthy.\n\nEstimation of ecosystem health has been criticized for intermingling the two types of environmental indicators. A health indicator is a normative indicator, and if conflated with descriptive indicators \"implies that normative values can be measured objectively, which is certainly not true. Thus, implicit values are insinuated to the reader, a situation which has to be avoided.\"\n\nIt can be argued that the very act of selecting indicators of any kind is biased by the observer's perspective but separation of goals from descriptions has been advocated as a step toward transparency: \"A separation of descriptive and normative indicators is essential from the perspective of the philosophy of science … Goals and values cannot be deduced directly from descriptions … a fact that is emphasized repeatedly in the literature of environmental ethics … Hence, we advise always specifying the definition of indicators and propose clearly distinguishing ecological indicators in science from policy indicators used for decision-making processes.\"\n\nAnd integration of multiple, possibly conflicting, normative indicators into a single measure of \"ecosystem health\" is problematic. Using 56 indicators, \"determining environmental status and assessing marine ecosystems health in an integrative way is still one of the grand challenges in marine ecosystems ecology, research and management\"\n\nAnother issue with indicators is validity. Good indicators must have an independently validated high predictive value, that is high sensitivity (high probability of indicating a significant change in the indicandum) and high specificity (low probability of wrongly indicating a change). The reliability of various health metrics has been questioned and \"what combination of measurements should be used to evaluate ecosystems is a matter of current scientific debate.\" Most attempts to identify ecological indicators have been correlative rather than derived from prospective testing of their predictive value and the selection process for many indicators has been based upon weak evidence or has been lacking in evidence.\n\nIn some cases no reliable indicators are known: \"We found no examples of invertebrates successfully used in [forest] monitoring programs. Their richness and abundance ensure that they play significant roles in ecosystem function but thwart focus on a few key species.\" And, \"Reviews of species-based monitoring approaches reveal that no single species, nor even a group of species, accurately reflects entire communities. Understanding the response of a single species may not provide reliable predictions about a group of species even when the group is a few very similar species.\"\n\nA trade-off between human health and the \"health\" of nature has been termed the \"health paradox\" and it illuminates how human values drive perceptions of ecosystem health.\n\nHuman health has benefited by sacrificing the \"health\" of wild ecosystems, such as dismantling and damming of wild valleys, destruction of mosquito-bearing wetlands, diversion of water for irrigation, conversion of wilderness to farmland, timber removal, and extirpation of tigers, whales, ferrets, and wolves.\n\nThere has been an acrimonious schism among conservationists and resource managers over the question of whether to \"ratchet back human domination of the biosphere\" or whether to embrace it. These two perspectives have been characterized as utilitarian vs protectionist.\n\nThe utilitarian view treats human health and well-being as criteria of ecosystem health. For example, destruction of wetlands to control malaria mosquitoes \"resulted in an improvement in ecosystem health.\"\nThe protectionist view treats humans as an invasive species: \"If there was ever a species that qualified as an invasive pest, it is \"Homo sapiens\",\"\n\nProponents of the utilitarian view argue that \"healthy ecosystems are characterized by their capability to sustain healthy human populations,\" and \"healthy ecosystems must be economically viable,\" as it is \"unhealthy\" ecosystems that are likely to result in increases in contamination, infectious diseases, fires, floods, crop failures and fishery collapse.\n\nProtectionists argue that privileging of human health is a conflict of interest as humans have demolished massive numbers of ecosystems to maintain their welfare, also disease and parasitism are historically normal in pre-industrial nature. Diseases and parasites promote ecosystem functioning, driving biodiversity and productivity, and parasites may constitute a significant fraction of ecosystem biomass.\n\nThe very choice of the word \"health\" applied to ecology has been questioned as lacking in neutrality in a BioScience article on responsible use of scientific language: \"Some conservationists fear that these terms could endorse human domination of the planet … and could exacerbate the shifting cognitive baseline whereby humans tend to become accustomed to new and often degraded ecosystems and thus forget the nature of the past.\"\n\nCriticism of ecosystem health largely targets the failure of proponents to explicitly distinguish the normative dimension from the descriptive dimension, and has included the following:\n\nAlternatives have been proposed for the term ecosystem health, including more neutral language such as ecosystem status, ecosystem prognosis, and ecosystem sustainability. Another alternative to the use of a health metaphor is to \"express exactly and clearly the public policy and the management objective\", to employ habitat descriptors and real properties of ecosystems. An example of a policy statement is \"The maintenance of viable natural populations of wildlife and ecological functions always takes precedence over any human use of wildlife.\" An example of a goal is \"Maintain viable populations of all native species in situ.\" An example of a management objective is \"Maintain self-sustaining populations of lake whitefish within the range of abundance observed during 1990-99.\"\n\nKurt Jax presented an ecosystem assessment format that avoids imposing a preconceived notion of normality, that avoids the muddling of normative and descriptive, and that gives serious attention to ecosystem definition. (1) Societal purposes for the ecosystem are negotiated by stakeholders, (2) a functioning ecosystem is defined with emphasis on phenomena relevant to stakeholder goals, (3) benchmark reference conditions and permissible variation of the system are established, (4) measurement variables are chosen for use as indicators, and (5) the time scale and spatial scale of assessment are decided.\n\nEcological health has been used as a medical term in reference to human allergy and multiple chemical sensitivity and as a public health term for programs to modify health risks (diabetes, obesity, smoking, etc.). Human health itself, when viewed in its broadest sense, is viewed as having ecological foundations. It is also an urban planning term in reference to \"green\" cities (composting, recycling), and has been used loosely with regard to various environmental issues, and as the condition of human-disturbed environmental sites. Ecosystem integrity implies a condition of an ecosystem exposed to a minimum of human influence. Ecohealth is the relationship of human health to the environment, including the effect of climate change, wars, food production, urbanization, and ecosystem structure and function. Ecosystem management and ecosystem-based management refer to the sustainable management of ecosystems and in some cases may employ the terms ecosystem health or ecosystem integrity as a goal.\n"}
{"id": "43400705", "url": "https://en.wikipedia.org/wiki?curid=43400705", "title": "Ekeby oak tree", "text": "Ekeby oak tree\n\nThe Ekeby oak tree () is an oak tree in Ekerö outside Stockholm, Sweden, close to Ekebyhov Castle. It is the largest living deciduous tree in Sweden by volume. \n\nThe Ekeby oak is approximately 500 years old. It was declared a natural monument in 1956. There are many old trees around Ekebyhov Castle; the oak, sometimes called \"Ekeröjätten\" (the Ekerö giant) stands alone in a field south of the castle, where it had no competition for space from other trees. It was measured in 2008 as the largest tree by volume in Sweden.\n"}
{"id": "52408383", "url": "https://en.wikipedia.org/wiki?curid=52408383", "title": "Endogenosymbiosis", "text": "Endogenosymbiosis\n\nEndogenosymbiosis is an evolutionary process, proposed by the evolutionary and environmental biologist Roberto Cazzolla Gatti, in which \"gene carriers\" (viruses, retroviruses and bacteriophages) and symbiotic prokaryotic cells (bacteria or archaea) could share parts or all of their genomes in an endogenous symbiotic relationship with their hosts.\n\nThe related process of symbiogenesis or endosymbiosis was proposed by Lynn Margulis in 1967. She argued that the internal symbiosis of bacteria-like organisms had formed organelles like chloroplasts and mitochondria. She proposed that this had created the eukaryotes, and thus driven the expansion of life on Earth. She had argued that this process of symbiotic collaboration had run alongside the classical Darwinian cycle of mutation, natural selection and adaptation.\n\nRoberto Cazzolla Gatti, Ph.D., associate professor at Tomsk State University (Russia), argued in his hypothesis that \"the main likely cause of the evolution of sexual reproduction, the parasitism, also represents the origin of biodiversity\". \nIn other terms, this theory suggests that sexual reproduction acts as a conservative system against the inclusion of new genetic variations into cells' DNA (supported by the DNA repair systems) and, instead, the evolution of species can take place only when this preservative system fails to contrast the inclusion, within the host genome, of hexogen parts of DNA (and RNA) coming from obliged \"parasitic\" elements (viruses and phages) that establish a symbiosis with their hosts. \n\"As two parallel evolutionary lines – Cazzolla Gatti wrote in his original paper – sexual reproduction seems to preserve what the endogenosymbiosis moves to diversify. Following the former process, the species can adapt slowly and indefinitely to the external factors, adjusting themselves, but not 'creating' novelty. The latter process, instead, leads to the speciation due to sudden changes in genes sequences. Not only organelles can be symbiotic with other cells, as suggested Lynn Margulis, but entire pieces of genetic material coming from symbiotic parasites, can be included in the host DNA, changing the gene expression and addressing the speciation process\".\n\nThis idea challenges the canonical natural selection models based on the gradualism of the mutation-adaptation pattern, providing more support to the punctuated equilibrium theory proposed by Stephen Jay Gould and Niles Eldredge.\n\nTwo independent studies provide support for the hypothesis. Jamie E. Henzy and Welkin E. Johnson demonstrated that the complex evolutionary history of the IFIT (Interferon Induced proteins with Tetratricopeptide repeats) family of antiviral genes has been shaped by continuous interactions between mammalian hosts and their many viruses.\n\nDavid Enard and colleagues estimated that viruses have driven close to 30% of all adaptive amino acid changes in the part of the human proteome conserved within mammals. Their results suggest that viruses are one of the most dominant drivers of evolutionary change across mammalian and human proteomes.\n\nPreviously, it was estimated that about 7–8% percent of the entire human genome carry about 100,000 pieces of DNA that came from endogenous retroviruses. This may be an underestimate.\n\nIn 2016 the biologists Sarah R. Bordestein and Seth R. Bordestein reported that genes are frequently transferred between hosts and parasites. Eukaryotic genes are often co-opted by viruses and bacterial genes are commonly found in bacteriophages. The presence of bacteriophages in symbiotic bacteria that obligately reside in eukaryotes may promote eukayotic DNA transfers to bacteriophages.\n"}
{"id": "57977652", "url": "https://en.wikipedia.org/wiki?curid=57977652", "title": "Evolution in fiction", "text": "Evolution in fiction\n\nEvolution has been an important theme in fiction, including speculative evolution in science fiction, since the late 19th century, though it began before Charles Darwin's time, and reflects progressionist and Lamarckist views as well as Darwin's. Darwinian evolution is pervasive in literature, whether taken optimistically in terms of how humanity may evolve towards perfection, or pessimistically in terms of the dire consequences of the interaction of human nature and the struggle for survival. Other themes include the replacement of humanity, either by other species or by intelligent machines.\n\nCharles Darwin's evolution by natural selection, as set out in his 1859 \"On the Origin of Species\", is the dominant theory in modern biology, but it is accompanied as a philosophy and in fiction by two earlier evolutionary theories, progressionism (orthogenesis) and Lamarckism. Progressionism is the view that evolution is progress towards some goal of perfection, and that it is in some way directed towards that goal. Lamarckism, a philosophy that long predates Jean-Baptiste de Lamarck, is the view that evolution is guided by the inheritance of characteristics acquired by use or disuse during an animal's lifetime.\n\nIdeas of progress and evolution were popular, long before Darwinism, in the 18th century, leading to Nicolas-Edme Rétif's allegorical 1781 story \"\" (The Southern Hemisphere Discovery by a Flying Man). \n\nThe evolutionary biologist Kayla M. Hardwick quotes from the 2013 film \"Man of Steel\", where the villain Faora states: \"The fact that you possess a sense of morality, and we do not, gives us an evolutionary advantage. And if history has taught us anything, it is that evolution always wins.\" She points out that the idea that evolution wins is progressionist, while (she argues) the idea that evolution gives evil an advantage over the moral and good, driving the creation of formidable monsters, is a popular science fiction misconception. Hardwick gives as examples of the evolution of \"bad-guy traits\" the Morlocks in H. G. Wells's 1895 \"The Time Machine\", the bugs' caste system in Robert Heinlein's 1959 \"Starship Troopers\", and the effective colonisation by Don Siegel's 1956 \"Invasion of the Body Snatchers\" aliens.\n\nIn French 19th century literature, evolutionary fantasy was Lamarckian, as seen in Camille Flammarion's 1887 \"Lumen\" and his 1894 \"\", J.-H. Rosny's 1887 \"Les Xipéhuz\" and his 1910 \"La mort de la terre\", and Jules Verne's 1901 \"La grande forêt, le village aérien\". The Lamarckist philosopher Henri Bergson's creative evolution driven by the supposed élan vital likely inspired J. D. Beresford's English evolutionary fantasy, his 1911 \"The Hampdenshire Wonder\".\n\nDarwin's version of evolution has been widely explored in fiction, both in fantasies and in imaginative explorations of its grimmer \"survival of the fittest\" effects, with much attention focused on possible human evolution. H. G. Wells's \"The Time Machine\" already mentioned, his 1896 \"The Island of Dr Moreau\", and his 1898 \"The War of the Worlds\" all pessimistically explore the possible dire consequences of the darker sides of human nature in the struggle for survival. More broadly, Joseph Conrad's 1899 \"Heart of Darkness\" and R. L. Stevenson's 1886 \"Dr Jekyll and Mr Hyde\" portray Darwinian thinking in mainstream English literature.\n\nThe evolutionary biologist J. B. S. Haldane wrote an optimistic tale, \"The Last Judgement\", in the 1927 collection \"Possible Worlds\". This influenced Olaf Stapledon's 1930 \"Last and First Men\", which portrays the many species that evolved from humans in a billion-year timeframe. A different take on Darwinism is the idea, popular from the 1950s onwards, that humans will evolve more or less godlike mental capacity, as in Arthur C. Clarke's 1950 \"Childhood's End\" and Brian Aldiss's 1959 \"Galaxies Like Grains of Sand\". Another science fiction theme is the replacement of humanity on Earth by other species or intelligent machines. For instance, Olof Johannesson's 1966 \"The Great Computer\" gives humans the role of enabling intelligent machines to evolve, while Kurt Vonnegut's 1985 \"Galapagos\" is one of several novels to depict a replacement species.\n\n"}
{"id": "113728", "url": "https://en.wikipedia.org/wiki?curid=113728", "title": "Geothermal energy", "text": "Geothermal energy\n\nGeothermal energy is thermal energy generated and stored in the Earth. Thermal energy is the energy that determines the temperature of matter. The geothermal energy of the Earth's crust originates from the original formation of the planet and from radioactive decay of materials (in currently uncertain but possibly roughly equal proportions). The geothermal gradient, which is the difference in temperature between the core of the planet and its surface, drives a continuous conduction of thermal energy in the form of heat from the core to the surface. The adjective \"geothermal\" originates from the Greek roots \"γη (ge)\", meaning earth, and \"θερμος (thermos)\", meaning hot.\n\nEarth's internal heat is thermal energy generated from radioactive decay and continual heat loss from Earth's formation. Temperatures at the core–mantle boundary may reach over 4000 °C (7,200 °F). The high temperature and pressure in Earth's interior cause some rock to melt and solid mantle to behave plastically, resulting in portions of the mantle convecting upward since it is lighter than the surrounding rock. Rock and water is heated in the crust, sometimes up to 370 °C (700 °F).\n\nWith water from hot springs, geothermal energy has been used for bathing since Paleolithic times and for space heating since ancient Roman times, but it is now better known for electricity generation. Worldwide, 11,700 megawatts (MW) of geothermal power was available in 2013. An additional 28 gigawatts of direct geothermal heating capacity is installed for district heating, space heating, spas, industrial processes, desalination and agricultural applications as of 2010.\n\nGeothermal power is cost-effective, reliable, sustainable, and environmentally friendly, but has historically been limited to areas near tectonic plate boundaries. Recent technological advances have dramatically expanded the range and size of viable resources, especially for applications such as home heating, opening a potential for widespread exploitation. Geothermal wells release greenhouse gases trapped deep within the earth, but these emissions are much lower per energy unit than those of fossil fuels.\n\nThe Earth's geothermal resources are theoretically more than adequate to supply humanity's energy needs, but only a very small fraction may be profitably exploited. Drilling and exploration for deep resources is very expensive. Forecasts for the future of geothermal power depend on assumptions about technology, energy prices, subsidies, plate boundary movement and interest rates. Pilot programs like EWEB's customer opt in Green Power Program show that customers would be willing to pay a little more for a renewable energy source like geothermal. But as a result of government assisted research and industry experience, the cost of generating geothermal power has decreased by 25% over the past two decades. In 2001, geothermal energy costs between two and ten US cents per kWh. \n\nHot springs have been used for bathing at least since Paleolithic times. The oldest known spa is a stone pool on China's Lisan mountain built in the Qin Dynasty in the 3rd century BC, at the same site where the Huaqing Chi palace was later built. In the first century AD, Romans conquered \"Aquae Sulis\", now Bath, Somerset, England, and used the hot springs there to feed public baths and underfloor heating. The admission fees for these baths probably represent the first commercial use of geothermal power. The world's oldest geothermal district heating system in Chaudes-Aigues, France, has been operating since the 14th century. The earliest industrial exploitation began in 1827 with the use of geyser steam to extract boric acid from volcanic mud in Larderello, Italy.\n\nIn 1892, America's first district heating system in Boise, Idaho was powered directly by geothermal energy, and was copied in Klamath Falls, Oregon in 1900. The first known building in the world to utilize geothermal energy as its primary heat source was the Hot Lake Hotel in Union County, Oregon, whose construction was completed in 1907. A deep geothermal well was used to heat greenhouses in Boise in 1926, and geysers were used to heat greenhouses in Iceland and Tuscany at about the same time. Charlie Lieb developed the first downhole heat exchanger in 1930 to heat his house. Steam and hot water from geysers began heating homes in Iceland starting in 1943.\n\nIn the 20th century, demand for electricity led to the consideration of geothermal power as a generating source. Prince Piero Ginori Conti tested the first geothermal power generator on 4 July 1904, at the same Larderello dry steam field where geothermal acid extraction began. It successfully lit four light bulbs. Later, in 1911, the world's first commercial geothermal power plant was built there. It was the world's only industrial producer of geothermal electricity until New Zealand built a plant in 1958. In 2012, it produced some 594 megawatts.\n\nLord Kelvin invented the heat pump in 1852, and Heinrich Zoelly had patented the idea of using it to draw heat from the ground in 1912. But it was not until the late 1940s that the geothermal heat pump was successfully implemented. The earliest one was probably Robert C. Webber's home-made 2.2 kW direct-exchange system, but sources disagree as to the exact timeline of his invention. J. Donald Kroeker designed the first commercial geothermal heat pump to heat the Commonwealth Building (Portland, Oregon) and demonstrated it in 1946. Professor Carl Nielsen of Ohio State University built the first residential open loop version in his home in 1948. The technology became popular in Sweden as a result of the 1973 oil crisis, and has been growing slowly in worldwide acceptance since then. The 1979 development of polybutylene pipe greatly augmented the heat pump’s economic viability.\n\nIn 1960, Pacific Gas and Electric began operation of the first successful geothermal electric power plant in the United States at The Geysers in California. The original turbine lasted for more than 30 years and produced 11 MW net power.\n\nThe binary cycle power plant was first demonstrated in 1967 in the USSR and later introduced to the US in 1981. This technology allows the generation of electricity from much lower temperature resources than previously. In 2006, a binary cycle plant in Chena Hot Springs, Alaska, came on-line, producing electricity from a record low fluid temperature of .\n\nThe International Geothermal Association (IGA) has reported that 10,715 megawatts (MW) of geothermal power in 24 countries is online, which was expected to generate 67,246 GWh of electricity in 2010. This represents a 20% increase in online capacity since 2005. IGA projects growth to 18,500 MW by 2015, due to the projects presently under consideration, often in areas previously assumed to have little exploitable resources.\n\nIn 2010, the United States led the world in geothermal electricity production with 3,086 MW of installed capacity from 77 power plants. The largest group of geothermal power plants in the world is located at The Geysers, a geothermal field in California. The Philippines is the second highest producer, with 1,904 MW of capacity online. Geothermal power makes up approximately 27% of Philippine electricity generation.\n\nIn 2016, Indonesia set in third with 1,647 MW online behind USA at 3,450 MW and the Philippines at 1,870 MW, but Indonesia will become second due to an additional online 130 MW at the end of 2016 and 255 MW in 2017. Indonesia's 28,994 MW are the largest geothermal reserves in the world, and it is predicted to overtake the USA in the next decade.\n\nGeothermal electric plants were traditionally built exclusively on the edges of tectonic plates where high temperature geothermal resources are available near the surface. The development of binary cycle power plants and improvements in drilling and extraction technology enable enhanced geothermal systems over a much greater geographical range. Demonstration projects are operational in Landau-Pfalz, Germany, and Soultz-sous-Forêts, France, while an earlier effort in Basel, Switzerland was shut down after it triggered earthquakes. Other demonstration projects are under construction in Australia, the United Kingdom, and the United States of America.\n\nThe thermal efficiency of geothermal electric plants is low, around 10–23%, because geothermal fluids do not reach the high temperatures of steam from boilers. The laws of thermodynamics limits the efficiency of heat engines in extracting useful energy. Exhaust heat is wasted, unless it can be used directly and locally, for example in greenhouses, timber mills, and district heating. System efficiency does not materially affect operational costs as it would for plants that use fuel, but it does affect return on the capital used to build the plant. In order to produce more energy than the pumps consume, electricity generation requires relatively hot fields and specialized heat cycles. Because geothermal power does not rely on variable sources of energy, unlike, for example, wind or solar, its capacity factor can be quite large – up to 96% has been demonstrated. The global average was 73% in 2005.\n\nGeothermal energy comes in either \"vapor-dominated\" or \"liquid-dominated\" forms. Larderello and The Geysers are vapor-dominated. Vapor-dominated sites offer temperatures from 240 to 300 °C that produce superheated steam.\n\nLiquid-dominated reservoirs (LDRs) were more common with temperatures greater than and are found near young volcanoes surrounding the Pacific Ocean and in rift zones and hot spots. \"Flash plants\" are the common way to generate electricity from these sources. Pumps are generally not required, powered instead when the water turns to steam. Most wells generate 2-10 MWe. Steam is separated from liquid via cyclone separators, while the liquid is returned to the reservoir for reheating/reuse. As of 2013, the largest liquid system is Cerro Prieto in Mexico, which generates 750 MWe from temperatures reaching . The Salton Sea field in Southern California offers the potential of generating 2000 MWe.\n\nLower temperature LDRs (120–200 °C) require pumping. They are common in extensional terrains, where heating takes place via deep circulation along faults, such as in the Western US and Turkey. Water passes through a heat exchanger in a Rankine cycle binary plant. The water vaporizes an organic working fluid that drives a turbine. These binary plants originated in the Soviet Union in the late 1960s and predominate in new US plants. Binary plants have no emissions.\n\nLower temperature sources produce the energy equivalent of 100M BBL per year. Sources with temperatures of 30–150 °C are used without conversion to electricity as district heating, greenhouses, fisheries, mineral recovery, industrial process heating and bathing in 75 countries. Heat pumps extract energy from shallow sources at 10–20 °C in 43 countries for use in space heating and cooling. Home heating is the fastest-growing means of exploiting geothermal energy, with global annual growth rate of 30% in 2005 and 20% in 2012.\n\nApproximately 270 petajoules (PJ) of geothermal heating was used in 2004. More than half went for space heating, and another third for heated pools. The remainder supported industrial and agricultural applications. Global installed capacity was 28 GW, but capacity factors tend to be low (30% on average) since heat is mostly needed in winter. Some 88 PJ for space heating was extracted by an estimated 1.3 million geothermal heat pumps with a total capacity of 15 GW.\n\nHeat for these purposes may also be extracted from co-generation at a geothermal electrical plant.\n\nHeating is cost-effective at many more sites than electricity generation. At natural hot springs or geysers, water can be piped directly into radiators. In hot, dry ground, earth tubes or downhole heat exchangers can collect the heat. However, even in areas where the ground is colder than room temperature, heat can often be extracted with a geothermal heat pump more cost-effectively and cleanly than by conventional furnaces. These devices draw on much shallower and colder resources than traditional geothermal techniques. They frequently combine functions, including air conditioning, seasonal thermal energy storage, solar energy collection, and electric heating. Heat pumps can be used for space heating essentially anywhere.\n\nIceland is the world leader in direct applications. Some 92.5% of its homes are heated with geothermal energy, saving Iceland over $100 million annually in avoided oil imports. Reykjavík, Iceland has the world's biggest district heating system, often used to heat pathways and roads to hinder the accumulation of ice. Once known as the most polluted city in the world, it is now one of the cleanest.\n\nEnhanced geothermal systems (EGS) actively inject water into wells to be heated and pumped back out. The water is injected under high pressure to expand existing rock fissures to enable the water to freely flow in and out. The technique was adapted from oil and gas extraction techniques. However, the geologic formations are deeper and no toxic chemicals are used, reducing the possibility of environmental damage. Drillers can employ directional drilling to expand the size of the reservoir.\n\nSmall-scale EGS have been installed in the Rhine Graben at Soultz-sous-Forêts in France and at Landau and Insheim in Germany.\n\nGeothermal power requires no fuel (except for pumps), and is therefore immune to fuel cost fluctuations. However, capital costs are significant. Drilling accounts for over half the costs, and exploration of deep resources entails significant risks. A typical well doublet (extraction and injection wells) in Nevada can support 4.5 megawatts (MW) and costs about $10 million to drill, with a 20% failure rate.\n\nIn total, electrical plant construction and well drilling cost about €2–5 million per MW of electrical capacity, while the break–even price is 0.04–0.10 € per kW·h. Enhanced geothermal systems tend to be on the high side of these ranges, with capital costs above $4 million per MW and break–even above $0.054 per kW·h in 2007. Direct heating applications can use much shallower wells with lower temperatures, so smaller systems with lower costs and risks are feasible. Residential geothermal heat pumps with a capacity of 10 kilowatt (kW) are routinely installed for around $1–3,000 per kilowatt. District heating systems may benefit from economies of scale if demand is geographically dense, as in cities and greenhouses, but otherwise piping installation dominates capital costs. The capital cost of one such district heating system in Bavaria was estimated at somewhat over 1 million € per MW. Direct systems of any size are much simpler than electric generators and have lower maintenance costs per kW·h, but they must consume electricity to run pumps and compressors. Some governments subsidize geothermal projects.\n\nGeothermal power is highly scalable: from a rural village to an entire city.\n\nThe most developed geothermal field in the United States is The Geysers in Northern California.\n\nGeothermal projects have several stages of development. Each phase has associated risks. At the early stages of reconnaissance and geophysical surveys, many projects are cancelled, making that phase unsuitable for traditional lending. Projects moving forward from the identification, exploration and exploratory drilling often trade equity for financing.\n\nThe Earth's internal thermal energy flows to the surface by conduction at a rate of 44.2 terawatts (TW), and is replenished by radioactive decay of minerals at a rate of 30 TW. These power rates are more than double humanity’s current energy consumption from all primary sources, but most of this energy flow is not recoverable. In addition to the internal heat flows, the top layer of the surface to a depth of is heated by solar energy during the summer, and releases that energy and cools during the winter.\n\nOutside of the seasonal variations, the geothermal gradient of temperatures through the crust is 25–30 °C (77–86 °F) per kilometer of depth in most of the world. The conductive heat flux averages 0.1 MW/km. These values are much higher near tectonic plate boundaries where the crust is thinner. They may be further augmented by fluid circulation, either through magma conduits, hot springs, hydrothermal circulation or a combination of these.\n\nA geothermal heat pump can extract enough heat from shallow ground anywhere in the world to provide home heating, but industrial applications need the higher temperatures of deep resources. The thermal efficiency and profitability of electricity generation is particularly sensitive to temperature. The most demanding applications receive the greatest benefit from a high natural heat flux, ideally from using a hot spring. The next best option is to drill a well into a hot aquifer. If no adequate aquifer is available, an artificial one may be built by injecting water to hydraulically fracture the bedrock. This last approach is called hot dry rock geothermal energy in Europe, or enhanced geothermal systems in North America. Much greater potential may be available from this approach than from conventional tapping of natural aquifers.\n\nEstimates of the potential for electricity generation from geothermal energy vary sixfold, from depending on the scale of investments. Upper estimates of geothermal resources assume enhanced geothermal wells as deep as , whereas existing geothermal wells are rarely more than deep. Wells of this depth are now common in the petroleum industry. The deepest research well in the world, the Kola superdeep borehole, is deep.\n\nMyanmar Engineering Society has identified at least 39 locations (in Myanmar) capable of geothermal power production and some of these hydrothermal reservoirs lie quite close to Yangon which is a significant underutilized resource.\n\nAccording to the Geothermal Energy Association (GEA) installed geothermal capacity in the United States grew by 5%, or 147.05 MW, ce the last annual survey in March 2012. This increase came from seven geothermal projects that began production in 2012. GEA also revised its 2011 estimate of installed capacity upward by 128 MW, bringing current installed U.S. geothermal capacity to 3,386 MW.\n\nGeothermal power is considered to be renewable because any projected heat extraction is small compared to the Earth's heat content. The Earth has an internal heat content of 10 joules (3·10 TW·hr), approximately 100 billion times current (2010) worldwide annual energy consumption. About 20% of this is residual heat from planetary accretion, and the remainder is attributed to higher radioactive decay rates that existed in the past. Natural heat flows are not in equilibrium, and the planet is slowly cooling down on geologic timescales. Human extraction taps a minute fraction of the natural outflow, often without accelerating it.\n\nGeothermal power is also considered to be sustainable thanks to its power to sustain the Earth’s intricate ecosystems. By using geothermal sources of energy present generations of humans will not endanger the capability of future generations to use their own resources to the same amount that those energy sources are presently used. Further, due to its low emissions geothermal energy is considered to have excellent potential for mitigation of global warming.\n\nEven though geothermal power is globally sustainable, extraction must still be monitored to avoid local depletion. Over the course of decades, individual wells draw down local temperatures and water levels until a new equilibrium is reached with natural flows. The three oldest sites, at Larderello, Wairakei, and the Geysers have experienced reduced output because of local depletion. Heat and water, in uncertain proportions, were extracted faster than they were replenished. If production is reduced and water is reinjected, these wells could theoretically recover their full potential. Such mitigation strategies have already been implemented at some sites. The long-term sustainability of geothermal energy has been demonstrated at the Lardarello field in Italy since 1913, at the Wairakei field in New Zealand since 1958, and at The Geysers field in California since 1960.\n\nFalling electricity production may be boosted through drilling additional supply boreholes, as at Poihipi and Ohaaki. The Wairakei power station has been running much longer, with its first unit commissioned in November 1958, and it attained its peak generation of 173MW in 1965, but already the supply of high-pressure steam was faltering, in 1982 being derated to intermediate pressure and the station managing 157MW. Around the start of the 21st century it was managing about 150MW, then in 2005 two 8MW isopentane systems were added, boosting the station's output by about 14MW. Detailed data are unavailable, being lost due to re-organisations. One such re-organisation in 1996 causes the absence of early data for Poihipi (started 1996), and the gap in 1996/7 for Wairakei and Ohaaki; half-hourly data for Ohaaki's first few months of operation are also missing, as well as for most of Wairakei's history.\n\nFluids drawn from the deep earth carry a mixture of gases, notably carbon dioxide (), hydrogen sulfide (), methane () and ammonia (). These pollutants contribute to global warming, acid rain, and noxious smells if released. Existing geothermal electric plants emit an average of of per megawatt-hour (MW·h) of electricity, a small fraction of the emission intensity of conventional fossil fuel plants. Plants that experience high levels of acids and volatile chemicals are usually equipped with emission-control systems to reduce the exhaust.\n\nIn addition to dissolved gases, hot water from geothermal sources may hold in solution trace amounts of toxic elements such as mercury, arsenic, boron, and antimony. These chemicals precipitate as the water cools, and can cause environmental damage if released. The modern practice of injecting cooled geothermal fluids back into the Earth to stimulate production has the side benefit of reducing this environmental risk.\n\nDirect geothermal heating systems contain pumps and compressors, which may consume energy from a polluting source. This parasitic load is normally a fraction of the heat output, so it is always less polluting than electric heating. However, if the electricity is produced by burning fossil fuels, then the net emissions of geothermal heating may be comparable to directly burning the fuel for heat. For example, a geothermal heat pump powered by electricity from a combined cycle natural gas plant would produce about as much pollution as a natural gas condensing furnace of the same size. Therefore, the environmental value of direct geothermal heating applications is highly dependent on the emissions intensity of the neighboring electric grid.\n\nPlant construction can adversely affect land stability. Subsidence has occurred in the Wairakei field in New Zealand. In Staufen im Breisgau, Germany, tectonic uplift occurred instead, due to a previously isolated anhydrite layer coming in contact with water and turning into gypsum, doubling its volume.\nEnhanced geothermal systems can trigger earthquakes as part of hydraulic fracturing. The project in Basel, Switzerland was suspended because more than 10,000 seismic events measuring up to 3.4 on the Richter Scale occurred over the first 6 days of water injection.\n\nGeothermal has minimal land and freshwater requirements. Geothermal plants use per gigawatt of electrical production (not capacity) versus and for coal facilities and wind farms respectively. They use of freshwater per MW·h versus over per MW·h for nuclear, coal, or oil.\n\nSome of the legal issues raised by geothermal energy resources include questions of ownership and allocation of the resource, the grant of exploration permits, exploitation rights, royalties, and the extent to which geothermal energy issues have been recognized in existing planning and environmental laws. Other questions concern overlap between geothermal and mineral or petroleum tenements. Broader issues concern the extent to which the legal framework for encouragement of renewable energy assists in encouraging geothermal industry innovation and development.\n\n\n\n"}
{"id": "14329", "url": "https://en.wikipedia.org/wiki?curid=14329", "title": "Historicism", "text": "Historicism\n\nHistoricism is the idea of attributing meaningful significance to space and time, such as historical period, geographical place, and local culture. Historicism tends to be hermeneutical because it values cautious, rigorous, and contextualized interpretation of information; or relativist, because it rejects notions of universal, fundamental and immutable interpretations. The approach varies from individualist theories of knowledge such as empiricism and rationalism, which neglect the role of traditions.\n\nThe term \"historicism\" (\"Historismus\") was coined by German philosopher Karl Wilhelm Friedrich Schlegel. Over time it has developed different and somewhat divergent meanings. Elements of historicism appear in the writings of French essayist Michel de Montaigne (1533–1592) and Italian philosopher G. B. Vico (1668–1744), and became more fully developed with the dialectic of Georg Hegel (1770–1831), influential in 19th-century Europe. The writings of Karl Marx, influenced by Hegel, also include historicism. The term is also associated with the empirical social sciences and with the work of Franz Boas.\n\nHistoricism may be contrasted with reductionist theories—which assumes that all developments can be explained by fundamental principles (such as in economic determinism)—or with theories that posit that historical changes occur as a result of random chance.\n\nThe Austrian-English philosopher Karl Popper condemned historicism along with the determinism and holism which he argued formed its basis. In his \"Poverty of Historicism\", he identified historicism with the opinion that there are \"inexorable laws of historical destiny\", which opinion he warned against. This contrasts with the contextually relative interpretation of historicism for which its proponents argue. Talcott Parsons criticized historicism as a case of idealistic fallacy in \"The Structure of Social Action\" (1937).\n\nPost-structuralism uses the term \"New Historicism\", which has some associations with both anthropology and Hegelianism.\n\nThe theological use of the word denotes the interpretation of biblical prophecy as being related to church history.\n\nHegel viewed the realization of human freedom as the ultimate purpose of history, which could only be achieved through the creation of the perfect state. And this progressive history would only occur through a dialectical process: namely, the tension between the purpose of humankind (freedom), the position that humankind currently finds itself, and mankind's attempt to bend the current world into accord with its nature. However, because humans are often not aware of the goal of both humanity and history, the process of achieving freedom is necessarily one of self-discovery. Hegel also saw the progress toward freedom being conducted by the \"spirit\" (Geist), a seemingly supernatural force that directed all human actions and interactions. Yet Hegel makes clear that the spirit is a mere abstraction, and only comes into existence \"through the activity of finite agents.\" Thus, Hegel's philosophy of history is not necessarily metaphysical, despite the fact that many of Hegel's opponents and interpreters have understood Hegel's philosophy of history as a metaphysical and determinist view of history. For example, Karl Popper in his book \"The Poverty of Historicism\" interpreted Hegel's philosophy of history as metaphysical and deterministic. Popper referred to this \"Hegelian\" philosophy of history as \"Historicism\".\n\nHegel's historicism also suggests that any human society and all human activities such as science, art, or philosophy, are defined by their history. Consequently, their essence can be sought only by understanding said history. The history of any such human endeavor, moreover, not only continues but also reacts against what has gone before; this is the source of Hegel's famous dialectic teaching usually summarized by the slogan \"thesis, antithesis, and synthesis\". (Hegel did not use these terms, although Johann Fichte did.) Hegel's famous aphorism, \"Philosophy is the history of philosophy,\" describes it bluntly.\n\nHegel's position is perhaps best illuminated when contrasted against the atomistic and reductionist opinion of human societies and social activities self-defining on an \"ad hoc\" basis through the sum of dozens of interactions. Yet another contrasting model is the persistent metaphor of a social contract. Hegel considers the relationship between individuals and societies as organic, not atomic: even their social discourse is mediated by language, and language is based on etymology and unique character. It thus preserves the culture of the past in thousands of half-forgotten metaphors. To understand why a person is the way he is, you must examine that person in his society: and to understand that society, you must understand its history, and the forces that influenced it. The \"Zeitgeist\", the \"Spirit of the Age,\" is the concrete embodiment of the most important factors that are acting in human history at any given time. This contrasts with teleological theories of activity, which suppose that the end is the determining factor of activity, as well as those who believe in a tabula rasa, or blank slate, opinion, such that individuals are defined by their interactions.\n\nThese ideas can be interpreted variously. The Right Hegelians, working from Hegel's opinions about the organicism and historically determined nature of human societies, interpreted Hegel's historicism as a justification of the unique destiny of national groups and the importance of stability and institutions. Hegel's conception of human societies as entities greater than the individuals who constitute them influenced nineteenth-century romantic nationalism and its twentieth-century excesses. The Young Hegelians, by contrast, interpreted Hegel's thoughts on societies influenced by social conflict for a doctrine of social progress, and attempted to manipulate these forces to cause various results. Karl Marx's doctrine of \"historical inevitabilities\" and historical materialism is one of the more influential reactions to this part of Hegel's thought. Significantly, Karl Marx's theory of alienation argues that capitalism disrupts traditional relationships between workers and their work.\n\nHegelian historicism is related to his ideas on the means by which human societies progress, specifically the dialectic and his conception of logic as representing the inner essential nature of reality. Hegel attributes the change to the \"modern\" need to interact with the world, whereas ancient philosophers were self-contained, and medieval philosophers were monks. In his History of Philosophy Hegel writes:\nIn modern times things are very different; now we no longer see philosophic individuals who constitute a class by themselves. With the present day all difference has disappeared; philosophers are not monks, for we find them generally in connection with the world, participating with others in some common work or calling. They live, not independently, but in the relation of citizens, or they occupy public offices and take part in the life of the state. Certainly they may be private persons, but if so, their position as such does not in any way isolate them from their other relationship. They are involved in present conditions, in the world and its work and progress. Thus their philosophy is only by the way, a sort of luxury and superfluity. This difference is really to be found in the manner in which outward conditions have taken shape after the building up of the inward world of religion. In modern times, namely, on account of the reconciliation of the worldly principle with itself, the external world is at rest, is brought into order — worldly relationships, conditions, modes of life, have become constituted and organized in a manner which is conformable to nature and rational. We see a universal, comprehensible connection, and with that individuality likewise attains another character and nature, for it is no longer the plastic individuality of the ancients. This connection is of such power that every individuality is under its dominion, and yet at the same time can construct for itself an inward world.\nThis opinion that entanglement in society creates an indissoluble bond with expression, would become an influential question in philosophy, namely, the requirements for individuality. It would be considered by Nietzsche, John Dewey and Michel Foucault directly, as well as in the work of numerous artists and authors. There have been various responses to Hegel's challenge. The Romantic period emphasized the ability of individual genius to transcend time and place, and use the materials from their heritage to fashion works which were beyond determination. The modern would advance versions of John Locke's infinite malleability of the human animal. Post-structuralism would argue that since history is not present, but only the image of history, that while an individual era or power structure might emphasize a particular history, that the contradictions within the story would hinder the very purposes that the history was constructed to advance.\n\nIn the context of anthropology and other sciences which study the past, historicism has a different meaning. Anthropological historicism is associated with the work of Franz Boas. His theory used the diffusionist concept that there were a few \"cradles of civilization\" which grew outwards, and merged it with the idea that societies would adapt to their circumstances, which is called historical particularism. The school of historicism grew in response to unilinear theories that social development represented adaptive fitness, and therefore existed on a continuum. While these theories were espoused by Charles Darwin and many of his students, their application as applied in social Darwinism and general evolution characterized in the theories of Herbert Spencer and Leslie White, historicism was neither anti-selection, nor anti-evolution, as Darwin never attempted nor offered an explanation for cultural evolution. However, it attacked the notion that there was one normative spectrum of development, instead emphasizing how local conditions would create adaptations to the local environment. Julian Steward refuted the viability of globally and universally applicable adaptive standards proposing that culture was honed adaptively in response to the idiosyncrasies of the local environment, the cultural ecology, by specific evolution. What was adaptive for one region might not be so for another. This conclusion has likewise been adopted by modern forms of biological evolutionary theory.\n\nThe primary method of historicism was empirical, namely that there were so many requisite inputs into a society or event, that only by emphasizing the data available could a theory of the source be determined. In this opinion, grand theories are unprovable, and instead intensive field work would determine the most likely explanation and history of a culture, and hence it is named \"historicism.\"\n\nThis opinion would produce a wide range of definition of what, exactly, constituted culture and history, but in each case the only means of explaining it was in terms of the historical particulars of the culture itself.\n\nSince the 1950s, when Jacques Lacan and Foucault argued that each epoch has its own knowledge system, within which individuals are inexorably entangled, many post-structuralists have used \"historicism\" to describe the opinion that all questions must be settled within the cultural and social context in which they are raised. Answers cannot be found by appeal to an external truth, but only within the confines of the norms and forms that phrase the question. This version of historicism holds that there are only the raw texts, markings and artifacts that exist in the present, and the conventions used to decode them. This school of thought is sometimes given the name of \"New Historicism\".\n\nThe same term, \"new historicism\" is also used for a school of literary scholarship which interprets a poem, drama, etc. as an expression of or reaction to the power-structures of its society. Stephen Greenblatt is an example of this school.\n\nWithin the context of 20th-century philosophy, debates continue as to whether ahistorical and immanent methods were sufficient to understand meaning—that is to say, \"what you see is what you get\" positivism—or whether context, background and culture are important beyond the mere need to decode words, phrases and references. While post-structural historicism is relativist in its orientation, that is, it sees each culture as its own frame of reference, a large number of thinkers have embraced the need for historical context, not because culture is self-referential, but because there is no more compressed means of conveying all of the relevant information except through history. This opinion is often seen as deriving from the work of Benedetto Croce. Recent historians using this tradition include Thomas Kuhn.\n\nIn Christianity, the term \"historicism\" refers to the confessional Protestant form of prophetical interpretation which holds that the fulfillment of biblical prophecy has occurred throughout history and continues to occur; as opposed to other methods which limit the time-frame of prophecy-fulfillment to the past or to the future.\n\nThere is also a particular opinion in ecclesiastical history and in the history of dogmas which has been described as historicist by Pope Pius XII in the encyclical \"Humani generis\". \"They add that the history of dogmas consists in the reporting of the various forms in which revealed truth has been clothed, forms that have succeeded one another in accordance with the different teachings and opinions that have arisen over the course of the centuries.\"\n\nThe social theory of Karl Marx, with respect to modern scholarship, has an ambiguous relation to historicism. Critics of Marx have charged his theory with historicism since its very genesis. However, the issue of historicism also finds itself important to many debates within Marxism itself; the charge of historicism has been made against various types of Marxism, typically disparaged by Marxists as \"vulgar\" Marxism.\n\nMarx himself expresses critical concerns with this historicist tendency in his Theses on Feuerbach:\n\nKarl Popper used the term \"historicism\" in his influential books \"The Poverty of Historicism\" and \"The Open Society and Its Enemies\", to mean: \"an approach to the social sciences which assumes that \"historical prediction\" is their primary aim, and which assumes that this aim is attainable by discovering the 'rhythms' or the 'patterns', the 'laws' or the 'trends' that underlie the evolution of history\". Karl Popper wrote with reference to Hegel's theory of history, which he criticized extensively. However, there is wide dispute whether Popper's description of \"historicism\" is an accurate description of Hegel, or more his characterisation of his own philosophical antagonists, including Marxist-Leninist thought, then widely held as posing a challenge to the philosophical basis of the West, as well as theories such as Spengler's which drew predictions about the future course of events from the past.\n\nIn \"The Open Society and Its Enemies\", Popper attacks \"historicism\" and its proponents, among whom (as well as Hegel) he identifies and singles out Plato and Marx—calling them all \"enemies of the open society\". The objection he makes is that historicist positions, by claiming that there is an inevitable and deterministic pattern to history, abrogate the democratic responsibility of each one of us to make our own free contributions to the evolution of society, and hence lead to totalitarianism.\n\nAnother of his targets is what he terms \"moral historicism\", the attempt to infer moral values from the course of history; in Hegel's words, that \"history is the world's court of justice\". This may take the form of conservatism (former might is right), positivism (might is right) or futurism (presumed coming might is right). As against these, Popper says that he does not believe \"that success proves anything or that history is our judge\". Futurism must be distinguished from prophecies that the right will prevail: these attempt to infer history from ethics, rather than ethics from history, and are therefore historicism in the normal sense rather than moral historicism.\n\nHe also attacks what he calls \"Historism\", which he regards as distinct from historicism. By historism, he means the tendency to regard every argument or idea as completely accounted for by its historical context, as opposed to assessing it by its merits. In Popperian terms, the \"New Historicism\" is an example of historism rather than of historicism proper.\n\nLeo Strauss used the term \"historicism\" and reportedly termed it the single greatest threat to intellectual freedom insofar as it denies any attempt to address injustice-pure-and-simple (such is the significance of historicism's rejection of \"natural right\" or \"right by nature\"). Strauss argued that historicism \"rejects political philosophy\" (insofar as this stands or falls by questions of permanent, trans-historical significance) and is based on the belief that \"all human thought, including scientific thought, rests on premises which cannot be validated by human reason and which came from historical epoch to historical epoch.\" Strauss further identified R. G. Collingwood as the most coherent advocate of historicism in the English language. Countering Collingwood's arguments, Strauss warned against historicist social scientists' failure to address real-life problems—most notably that of tyranny—to the extent that they relativize (or \"subjectivize\") all ethical problems by placing their significance strictly in function of particular or ever-changing socio-material conditions devoid of inherent or \"objective\" \"value.\" Similarly, Strauss criticized Eric Voegelin's abandonment of ancient political thought as guide or vehicle in interpreting modern political problems.\n\nIn his books, \"Natural Right and History\" and \"On Tyranny\", Strauss offers a complete critique of historicism as it emerges in the works of Hegel, Marx, and Heidegger. Many believe that Strauss also found historicism in Edmund Burke, Tocqueville, Augustine, and John Stuart Mill. Although it is largely disputed whether Strauss himself was a historicist, he often indicated that historicism grew out of and against Christianity and was a threat to civic participation, belief in human agency, religious pluralism, and, most controversially, an accurate understanding of the classical philosophers and religious prophets themselves. Throughout his work, he warns that historicism, and the understanding of progress that results from it, expose us to tyranny, totalitarianism, and democratic extremism. In his exchange with Alexandre Kojève in \"On Tyranny\", Strauss seems to blame historicism for Nazism and Communism. In a collection of his works by Kenneth Hart entitled \"Jewish Philosophy and the Crisis of Modernity\", he argues that Islam, traditional Judaism, and ancient Greece, share a concern for sacred law that makes them especially susceptible to historicism, and therefore to tyranny. Strauss makes use of Nietzsche's own critique of progress and historicism, although Strauss refers to Nietzsche himself (no less than to Heidegger) as a \"radical historicist\" who articulated a philosophical (if only untenable) justification for historicism.\n\n\n\n"}
{"id": "1159407", "url": "https://en.wikipedia.org/wiki?curid=1159407", "title": "Homeosis", "text": "Homeosis\n\nIn evolutionary developmental biology, homeosis is the transformation of one organ into another, arising from mutation in or misexpression of certain developmentally critical genes, specifically homeotic genes. In animals, these developmental genes specifically control the development of organs on their anteroposterior axis. In plants, however, the developmental genes affected by homeosis may control anything from the development of a stamen or petals to the development of chlorophyll. Homeosis may be caused by mutations in Hox genes, found in animals, or others such as the MADS-box family in plants. Homeosis is a characteristic that has helped insects become as successful and diverse as they are.\n\nHomeotic mutations work by changing segment identity during development. For example, the \"Ultrabithorax\" genotype gives a phenotype wherein metathoracic and first abdominal segments become mesothoracic segments. Another well-known example is \"Antennapedia\": a gain-of-function allele causes legs to develop in the place of antennae.\n\nIn botany, Rolf Sattler has revised the concept of homeosis (replacement) by his emphasis on partial homeosis in addition to complete homeosis; this revision is now widely accepted.\n\nHomeotic mutants in angiosperms are thought to be rare in the wild: in the annual plant \"Clarkia\" (Onagraceae), homeotic mutants are known where the petals are replaced by a second whorl of sepal-like organs, originating via a mutation governed by a single recessive gene. The absence of lethal or deleterious consequences in floral mutants resulting in distinct morphological expressions has been a factor in the evolution of Clarkia, and perhaps also in many other plant groups.\n\nFollowing the work on homeotic mutants by Ed Lewis, the phenomenology of homeosis in animals was further elaborated by discovery of a conserved DNA binding sequence present in many homeotic proteins. \nThus, the 60 amino acid DNA binding protein domain was named the homeodomain, while the 180 bp nucleotide sequence encoding it was named the homeobox. The homeobox gene clusters studied by Ed Lewis were named the Hox genes, although it should be noted that many more homeobox genes are encoded by animal genomes than those in the Hox gene clusters.\n\nThe homeotic-function of certain proteins was first postulated to be that of a \"selector\" as proposed by Antonio Garcia-Bellido. \nBy definition selectors were imagined to be (transcription factor) proteins that stably determined one of two possible cell fates for a cell and its cellular descendants in a tissue. \nWhile most animal homeotic functions are associated with homeobox-containing factors, not all homeotic proteins in animals are encoded by homeobox genes, and further not all homeobox genes are necessarily associated with homeotic functions or (mutant) phenotypes.\nThe concept of homeotic selectors was further elaborated or at least qualified by Michael Akam in a so-called \"post-selector gene\" model that incorporated additional findings and \"walked back\" the \"orthodoxy\" of selector-dependent stable binary switches.\n\nThe concept of tissue compartments is deeply intertwined with the selector model of homeosis because the selector-mediated maintenance of cell fate can be restricted into different organizational units of an animal's body plan.\nIn this context, newer insights into homeotic mechanisms were found by Albert Erives and colleagues by focusing on enhancer DNAs that are co-targeted by homeotic selectors and different combinations of developmental signals.\nThis work identifies a protein biochemical difference between the transcription factors that function as homeotic selectors versus the transcription factors that function as effectors of developmental signaling pathways, such as the Notch signaling pathway and the BMP signaling pathway.\nThis work proposes that homeotic selectors function to \"license\" enhancer DNAs in a restricted tissue compartment so that the enhancers are enabled to read-out developmental signals, which are then integrated via polyglutamine-mediated aggregation.\n\nLike the complex multicellularity seen in animals, the multicellularity of land plants is developmentally organized into tissue and organ units via transcription factor genes with homeotic effects.\nAlthough plants have homeobox-containing genes, plant homeotic factors tend to possess MADS-box DNA binding domains.\nAnimal genomes also possess a small number MADS-box factors.\nThus, in the independent evolution of multicellularity in plants and animals, different eukaryotic transcription factor families were co-opted to serve homeotic functions. \nMADS-domain factors have been proposed to function as co-factors to more specialized factors and thereby help to determine organ identity.\nThis has been proposed to correspond more closely to the interpretation of animal homeotics outlined by Michael Akam.\n\n"}
{"id": "3112323", "url": "https://en.wikipedia.org/wiki?curid=3112323", "title": "Humanistic naturalism", "text": "Humanistic naturalism\n\nHumanistic naturalism is the branch of philosophical naturalism wherein human beings are best able to control and understand the world through use of the scientific method, combined with the social and ethical values of humanism. Concepts of spirituality, intuition, and metaphysics are considered subjectively valuable only, primarily because they are unfalsifiable, and therefore can never progress beyond the realm of personal opinion. A boundary is not drawn between nature and what lies \"beyond\" nature; everything is regarded as a result of explainable processes within nature, with nothing lying outside it. \nThe belief is that all living things are intricate extensions of nature, and therefore deserve some degree of mutual respect from human beings. Naturalists accept the need for adaptation to current change, however it may be, and also that life must feed upon life for survival. However, they also recognize the necessity for a fair exchange of resources between all species. Humanistic naturalists are generally concerned with the ethical aspects of \"worldview naturalism.\"\nIndustry and technology are sometimes regarded as enemies to naturalism, but this is not always the case. For those who do believe in such threats, the thought is that the majority of human history, societies were largely agricultural and hunter-gatherer and lived in relative harmony and balance with nature. With the dawn of the Industrial Revolution, some humanistic naturalists see this balance as being increasingly threatened. This view has some similarities with anarcho-primitivism and other anti-modernist perspectives.\n\n"}
{"id": "4187880", "url": "https://en.wikipedia.org/wiki?curid=4187880", "title": "Interlocus contest evolution", "text": "Interlocus contest evolution\n\nInterlocus contest evolution (ICE) is a process of intergenomic conflict by which different loci within a single genome antagonistically coevolve. ICE supposes that the Red Queen process, which is characterized by a never-ending antagonistic evolutionary arms race, does not only apply to species but also to genes within the genome of a species.\n\nBecause sexual recombination allows different gene loci to evolve semi-autonomously, genes have the potential to coevolve antagonistically. ICE occurs when \"an allelic substitution at one locus selects for a new allele at the interacting locus, and vice versa.\" As a result, ICE can lead to a chain reaction of perpetual gene substitution at antagonistically interacting loci, and no stable equilibrium can be achieved. The rate of evolution thus increases at that locus.\n\nICE is thought to be the dominant mode of evolution for genes controlling social behavior. The ICE process can explain many biological phenomena, including intersexual conflict, parent offspring conflict, and interference competition.\n\nA fundamental conflict between the sexes lies in differences in investment: males generally invest predominantly in fertilization while females invest predominantly in offspring. This conflict manifests itself in many traits associated with sexual reproduction. Genes expressed in only one sex are selectively neutral in the other sex; male- and female-linked genes can therefore be acted upon separated by selection and will evolve semi-autonomously. Thus, one sex of a species may evolve to better itself rather than better the species as a whole, sometimes with negative results for the opposite sex: loci will antagonistically coevolve to enhance male reproductive success at females’ expense on the one hand, and to enhance female resistance to male coercion on the other. This is an example of intralocus sexual conflict, and is unlikely to be resolved fully throughout the genome. However, in some cases this conflict may be resolved by the restriction of the gene’s expression to only the sex that it benefits, resulting in sexual dimorphism.\n\nThe ICE theory can explain the differentiation of the human X- and Y-chromosomes. Semi-autonomous evolution may have promoted genes beneficial to females in the X-chromosome even when detrimental to males, and genes beneficial to males in the Y-chromosome, even when detrimental to females. As the distribution of the X-chromosome is three times as large as the Y-chromosome (the X-chromosome occurs in 3/4 of offspring genes, while the Y-chromosome occurs in only 1/4), the Y-chromosome has a reduced opportunity for rapid evolution. Thus the Y-chromosome has \"shed\" its genes to leave only the essential ones (such as the SRY gene), which gives rise to the differences in the X- and Y-chromosomes.\n\nA father, mother and offspring may differ in the optimal resource allocation to the offspring. This co-evolutionary conflict can be considered in the context of ICE. Selection will favor genes in the male to maximize female investment in the current offspring, no matter the consequences to the female's reproduction later in life, while selection will favor genes in the female that increase her overall lifetime fitness. Genes expressed in the offspring will be selected to produce an intermediary level of resource allocation between the male-benefit and female-benefit loci. This three-way conflict again occurs when parents feed their offspring, as the optimum feeding rate and optimum point in time to discontinue feeding differ between father, mother and offspring.\n\nICE can also explain the theory of interference competition, which is most likely to be associated with opposing sets of genes that determine the outcome of competition between individuals. Different sets of genes may code for signal or receiver phenotypes, such as in the context of threat displays: when a competing male can win more contests by intimidation, rather than by fighting, selection will favor the accumulation of deceitful genes that may not be honest indicators of the male’s fighting capability.\n\nFor example, primitive male elephant seals may have used the lowest frequencies in the threat call of a rival as an indication of body size. The elephant seal's enormous nose may have evolved as a resonating device to amplify low frequencies, illustrating selection that favors the production of low-frequency threat vocalizations. However, this counter-selects for receptor systems that provide an increased threshold required for intimidation, which in turn selects for deeper threat vocalizations. The rapid divergence of threat displays among closely related species provides further evidence in support of the co-evolutionary arms race within the genome of a single species, driven by the ICE process.\n"}
{"id": "144551", "url": "https://en.wikipedia.org/wiki?curid=144551", "title": "Knapping", "text": "Knapping\n\nKnapping is the shaping of flint, chert, obsidian or other conchoidal fracturing stone through the process of lithic reduction to manufacture stone tools, strikers for flintlock firearms, or to produce flat-faced stones for building or facing walls, and flushwork decoration. The original Germanic term \"knopp\" meant strike, shape, or work, so it could theoretically have referred equally well to making a statue or dice. Modern usage is more specific, referring almost exclusively to the hand-tool pressure-flaking process pictured.\n\nFlintknapping or knapping is done in a variety of ways depending on the purpose of the final product. For stone tools and flintlock strikers, chert is worked using a fabricator such as a hammerstone to remove lithic flakes from a nucleus or core of tool stone. Stone tools can then be further refined using wood, bone, and antler tools to perform pressure flaking.\n\nFor building work a hammer or pick is used to split chert nodules supported on the lap. Often the chert nodule will be split in half to create two cherts with a flat circular face for use in walls constructed of lime. More sophisticated knapping is employed to produce near-perfect cubes which are used as bricks.\n\nThere are many different methods of shaping stone into useful tools. Early knappers could have used simple hammers made of wood or antler to shape stone tools. The factors that contribute to the knapping results are varied, but the EPA (exterior platform angle) indeed influences many attributes, such as length, thickness and termination of flakes.\n\n\"Hard hammer\" techniques are used to remove large flakes of stone. Early knappers and hobbyists replicating their methods often use cobbles of very hard stone, such as quartzite. This technique can be used by flintknappers to remove broad flakes that can be made into smaller tools. This method of manufacture is believed to have been used to make some of the earliest stone tools ever found, some of which date from over 2 million years ago.\n\n\"Soft hammer\" techniques are more precise than hard hammer methods of shaping stone. Soft hammer techniques allow a knapper to shape a stone into many different kinds of cutting, scraping, and projectile tools. These \"soft hammer\" techniques also produce longer, thinner flakes, potentially allowing for material conservation or a lighter lithic tool kit to be carried by mobile societies.\n\n\"Pressure flaking\" involves removing narrow flakes along the edge of a stone tool. This technique is often used to do detailed thinning and shaping of a stone tool. Pressure flaking involves putting a large amount of force across a region on the edge of the tool and (hopefully) causing a narrow flake to come off of the stone. Modern hobbyists often use pressure flaking tools with a copper or brass tip, but early knappers could have used antler tines or a pointed wooden punch; traditionalist knappers still use antler tines and copper-tipped tools. The major advantage of using soft metals rather than wood or bone is that the metal punches wear down less and are less likely to break under pressure.\n\nIn cultures that have not adopted metalworking technologies, the production of stone tools by knappers is common, but in modern cultures the making of such tools is the domain of experimental archaeologists and hobbyists. Archaeologists usually undertake the task so that they can better understand how prehistoric stone tools were made.\n\nKnapping is often learned by outdoorsmen.\n\nKnapping \"gun flints\", used by flintlock firearms was formerly a major industry in flint bearing locations, such as Brandon in Suffolk, England and the small towns of Meusnes and Couffy in France. Meusnes has a small museum dedicated to the industry.\n\nIn 1804, during the Napoleonic Wars, Brandon was supplying over 400,000 flints a month for use by the British Army and Navy. Brandon knappers made gun flints for export to Africa as late as the 1960s.\n\nKnapping for building purposes is still a skill that is practiced in the flint-bearing regions of southern England, such as Sussex, Suffolk and Norfolk, and in northern France, especially Brittany and Normandy, where there is a resurgence of the craft due to government funding.\n\nHistorically, flint knappers commonly suffered from silicosis, due to the inhalation of flint dust. This has been called \"the world's first industrial disease\".\n\nWhen gun flint knapping was a large-scale industry in Brandon, silicosis was widely known as \"knappers' rot\". It has been claimed silicosis was responsible for the early death of three-quarters of Brandon gun flint makers. In one workshop, seven of the eight workmen died of the condition before the age of fifty.\n\nModern knappers are advised to work in the open air to reduce the dust hazard, and to wear eye and hand protection. Some modern knappers wear a respirator to guard against dust.\n\nModern American interest in knapping can be traced back to the study of a California Native American called Ishi who lived in the early twentieth century. Ishi taught scholars and academics traditional methods of making stone tools and how to use them for survival in the wild. Early European explorers to the New world were also exposed to flint knapping techniques. Additionally, several pioneering nineteenth-century European experimental knappers are also known and in the late 1960s and early 1970s experimental archaeologist Don Crabtree published texts such as \"Experiments in Flintworking\". François Bordes was an early writer on Old World knapping; he experimented with ways to replicate stone tools found across Western Europe. These authors helped to ignite a small craze in knapping among archaeologists and prehistorians.\n\nEnglish archaeologist Phil Harding is another contemporary expert, whose exposure on the television series Time Team has led to him being a familiar figure in the UK and beyond. Many groups, with members from all walks of life, can now be found across the United States and Europe. These organizations continue to demonstrate and teach various ways of shaping stone tools.\n\n"}
{"id": "18032715", "url": "https://en.wikipedia.org/wiki?curid=18032715", "title": "List of herbaria in Europe", "text": "List of herbaria in Europe\n\nThis is a list of herbaria in Europe, organized first by region where the herbarium is located (using the United Nations geoscheme for Europe), then within each region by size of the collection. For other continents, see List of herbaria.\n\nThe tables below list herbaria located in Eastern Europe as defined by the United Nations geoscheme for Europe.\nThe following table includes herbaria located in countries on the Black Sea, including Bulgaria, Moldova, Romania, and Ukraine.\n\nThe following table includes herbaria located in the Czech Republic.\n\nThe following table includes herbaria located in Hungary.\n\nThe following table includes herbaria located in Poland.\n\nThe following table includes herbaria located in European Russia.\n\nThe tables below list herbaria located in Northern Europe as defined by the United Nations geoscheme for Europe: the British Isles, Baltic states, Scandinavia, and Iceland.\n\nThe following table includes herbaria located in the Baltic states: Estonia, Latvia, and Lithuania.\n\nThe following table includes herbaria located in the British Isles, including Ireland.\n\nThe following table includes herbaria located in Scandinavia, including Denmark and Iceland.\n\nThe tables below list herbaria located in Southern Europe as defined by the United Nations geoscheme for Europe.\n\nThe following table includes herbaria located in the western and southern Balkans, including Albania, Greece and nations formerly part of Yugoslavia.\n\nThe following table includes herbaria located in Italy, including Sicily and Sardinia.\n\nThe following table includes herbaria located in Spain and Portugal, including the Canary Islands.\n\nThe tables below list herbaria located in Western Europe as defined by the United Nations geoscheme for Europe: France, Germany, Austria, Switzerland, and the Low Countries.\n\nThe following table includes herbaria located in Austria and Switzerland, as well as Liechtenstein.\n\nThe following table includes herbaria located in France (including Corsica) and Monaco.\n\nThe following table includes herbaria located in Germany.\n\nThe following table includes herbaria located in Belgium, Luxembourg, Netherlands and Cyprus\n"}
{"id": "58439642", "url": "https://en.wikipedia.org/wiki?curid=58439642", "title": "Margham", "text": "Margham\n\nMargham is an oil and gas field in Dubai, United Arab Emirates (UAE) and the largest onshore gas field in the emirate. The field is managed by Dusup - the Dubai Supply Authority. Condensate production ran at some 25,000 barrels per day in 2010. Margham also has an oil production capability.\n\nProduction at Margham commenced in 1984, with three major gas-bearing formations located up to 10,000 feet below sea level. The field is connected by pipeline to Jebel Ali, where the gas condensate is loaded onto tankers for export. Dry gas is now also sent by pipeline to supply the Dubai grid, with consumption increasing since 2015.\n\nMargham was initially developed as a liquids stripping/gas recycling project (dry gas was pumped back into the reservoir), but now operates as a gas storage facility for Dubai since 2008, allowing Dubai to depend on gas produced from Margham for its elecricity generation and desalination needs. This usage, together with sustainables such as DEWA's Mohammed bin Rashid Al Maktoum Solar Park, means that Dubai has eliminated the use of oil as a domestic energy fuel.\n\nAlthough it is a major producer with ambitions to develop its trading activities to become a major global LNG hub, the UAE is actually a net importer of LNG.\n"}
{"id": "21898316", "url": "https://en.wikipedia.org/wiki?curid=21898316", "title": "Meteorological intelligence", "text": "Meteorological intelligence\n\nMeteorological intelligence is information measured, gathered, compiled, exploited, analyzed and disseminated by meteorologists, climatologists and hydrologists to characterize the current state and/or predict the future state of the atmosphere at a given location and time. Meteorological intelligence is a subset of environmental intelligence and is synonymous with the term weather intelligence. \n\nThe earliest known use of the term \"meteorological intelligence\" in a written document dates to 1854 on pg. 168 of the Eighth Annual Report of the Board of Regents of the Smithsonian Institution. This report discusses the Smithsonian Institution's initiative to transmit meteorological intelligence via telegraph lines. An early reference to \"meteorological intelligence\" in England dates an 1866 issue of The Edinburgh Review which was a prominent Scottish journal during the 19th century (Reeve 1866, pg. 75). \n\nAnother documented, early use of the term dates to 1874 in a historical compilation entitled, \"The American Historical Record\" (Lossing 1874, pg. 125). In this book, Lossing uses the term to refer to weather observations transmitted over telegraph lines for the purpose of studying the nature of storms with the ultimate goal of enhancing public safety through the issuance of storm warnings. This mission was carried out by the Army Signal Service starting in the 1870s who was responsible for communication (via telegraph) of technical intelligence for the army as well as \"meteorological intelligence\" for the general welfare of the country (Ingersoll 1879, pg. 156).\n\nFrom the viewpoint of the intelligence community, the term meteorological intelligence is more limited in its use referring to the use of clandestine or technical means to learn about environmental conditions over enemy territory (Shulsky and Schmitt 2002) as in the North Atlantic weather war. In the military intelligence context, weather information is often referred to as meteorological or environmental intelligence (Hinsley 1990, pg. 420; Platt 1957, pg. 14; U.S. Congress, pg. 164). \n\nWith regard to private sector meteorology, the term meteorological intelligence is a broad term of art that is primarily associated with observed and forecast weather information provided to decision makers in one of a number of weather sensitive business areas including: Energy, forestry, agriculture, telecommunications, transportation, aviation, entertainment, retail and construction (CMOS 2001, pg. 23) . It is considered a key aspect of weather risk management for the legal and insurance industries.\n\n\n\n"}
{"id": "14389994", "url": "https://en.wikipedia.org/wiki?curid=14389994", "title": "Natural landscape", "text": "Natural landscape\n\nA natural landscape is the original landscape that exists before it is acted upon by human culture. The natural landscape and the cultural landscape are separate parts of the landscape. However, in the twenty-first century landscapes that are totally untouched by human activity no longer exist, so that reference is sometimes now made to degrees of naturalness within a landscape.\n\nIn \"Silent Spring\" (1962) Rachel Carson describes a roadside verge as it used to look: \"Along the roads, laurel, viburnum and alder, great ferns and wildflowers delighted the traveler’s eye through much of the year\" and then how it looks now following the use of herbicides: \"The roadsides, once so attractive, were now lined with browned and withered vegetation as though swept by fire\". Even though the landscape before it is sprayed is biologically degraded, and may well contains alien species, the concept of what might constitute a natural landscape can still be deduced from the context.\n\nThe phrase \"natural landscape\" was first used in connection with landscape painting, and landscape gardening, to contrast a formal style with a more natural one, closer to nature. Alexander von Humboldt (1769 – 1859) was to further conceptualize this into the idea of a natural landscape \"separate\" from the cultural landscape. Then in 1908 geographer Otto Schlüter developed the terms original landscape (\"Urlandschaft\") and its opposite cultural landscape (\"Kulturlandschaft\") in an attempt to give the science of geography a subject matter that was different from the other sciences. An early use of the actual phrase \"natural landscape\" by a geographer can be found in Carl O. Sauer's paper \"The Morphology of Landscape\" (1925).\n\nThe concept of a natural landscape was first developed in connection with landscape painting, though the actual term itself was first used in relation to landscape gardening. In both cases it was used to contrast a formal style with a more natural one, that is closer to nature. Chunglin Kwa suggests, \"that a seventeenth-century or early-eighteenth-century person could experience natural scenery ‘just like on a painting,’ and so, with or without the use of the word itself, designate it as a landscape.\" With regard to landscape gardening John Aikin, commented in 1794: \"Whatever, therefore, there be of \"novelty\" in the singular scenery of an artificial garden, it is soon exhausted, whereas the infinite diversity of a natural landscape presents an inexhaustible flore of new forms\". Writing in 1844 the prominent American landscape gardener Andrew Jackson Downing comments: \"straight canals, round or oblong pieces of water, and all the regular forms of the geometric mode ... would evidently be in violent opposition to the whole character and expression of natural landscape\".\n\nIn his extensive travels in South America, Alexander von Humboldt became the first to conceptualize a natural landscape separate from the cultural landscape, though he does not actually use these terms. Andrew Jackson Downing was aware of, and sympathetic to, Humboldt's ideas, which therefore influenced American landscape gardening.\n\nSubsequently, the geographer Otto Schlüter, in 1908, argued that by defining geography as a \"Landschaftskunde\" (landscape science) would give geography a logical subject matter shared by no other discipline. He defined two forms of landscape: the \"Urlandschaft\" (original landscape) or landscape that existed before major human induced changes and the \"Kulturlandschaft\" (cultural landscape) a landscape created by human culture. Schlüter argued that the major task of geography was to trace the changes in these two landscapes.\n\nThe term natural landscape is sometimes used as a synonym for wilderness, but for geographers natural landscape is a scientific term which refers to the biological, geological, climatological and other aspects of a landscape, not the cultural values that are implied by the word wilderness.\n\nMatters are complicated by the fact that the words nature and natural have more than one meaning. On the one hand there is the main dictionary meaning for nature: \"The phenomena of the physical world collectively, including plants, animals, the landscape, and other features and products of the earth, as opposed to humans or human creations\". On the other hand, there is the growing awareness, especially since Charles Darwin, of humanities biological affinity with nature.\n\nThe dualism of the first definition has its roots is an \"ancient concept\", because early people viewed \"nature, or the nonhuman world […] as a divine \"Other\", godlike in its separation from humans\". In the West, Christianity's myth of the fall, that is the expulsion of humankind from the Garden of Eden, where all creation lived in harmony, into an imperfect world, has been the major influence. Cartesian dualism, from the seventeenth century on, further reinforced this dualistic thinking about nature. \nWith this dualism goes value judgement as to the superiority of the natural over the artificial. Modern science, however, is moving towards a holistic view of nature.\n\nWhat is meant by natural, within the American conservation movement, has been changing over the last century and a half.\n\nIn the mid-nineteenth century American began to realize that the land was becoming more and more domesticated and wildlife was disappearing. This led to the creation of American National Parks and other conservation sites. Initially it was believed that all that was needed to do was to separate what was seen as natural landscape and \"avoid disturbances such as logging, grazing, fire and insect outbreaks\". This, and subsequent environmental policy, until recently, was influenced by ideas of the wilderness. However, this policy was not consistently applied, and in Yellowstone Park, to take one example, the existing ecology was altered, firstly by the exclusion of Native Americans and later with the virtual extermination of the wolf population.\n\nA century later, in the mid-twentieth century, it began to be believed that the earlier policy of \"protection from disturbance was inadequate to preserve park values\", and that is that direct human intervention was necessary to restore the landscape of National Parks to its ‘’natural’’ condition. In 1963 the Leopold Report argued that \"A national park should represent a vignette of primitive America\". This policy change eventually led to the restoration of wolves in Yellowstone Park in the 1990s.\n\nHowever, recent research in various disciplines indicates that a pristine natural or \"primitive\" landscape is a myth, and it now realised that people have been changing the natural into a cultural landscape for a long while, and that there are few places untouched in some way from human influence. The earlier conservation policies were now seen as cultural interventions. The idea of what is natural and what artificial or cultural, and how to maintain the natural elements in a landscape, has been further complicated by the discovery of global warming and how it is changing natural landscapes.\n\nAlso important is a reaction recently amongst scholars against dualistic thinking about nature and culture. Maria Kaika comments: \"Nowadays, we are beginning to see nature and culture as intertwined once again – not ontologically separated anymore […].What I used to perceive as a compartmentalized world, consisting of neatly and tightly sealed, autonomous ‘space envelopes’ (the home, the city, and nature) was, in fact, a messy socio-spatial continuum”. And William Cronon argues against the idea of wilderness because it \"involves a dualistic vision in which the human is entirely outside the natural\" and affirms that \"wildness (as opposed to wilderness) can be found anywhere\" even \"in the cracks of a Manhattan sidewalk\". According to Cronon we have to \"abandon the dualism that sees the tree in the garden as artificial […] and the tree in the wilderness as natural […] Both in some ultimate sense are wild.\" Here he bends somewhat the regular dictionary meaning of wild, to emphasise that nothing natural, even in a garden, is fully under human control.\n\nThe landscape of Europe has considerably altered by people and even in an area, like the Cairngorm Mountains of Scotland, with a low population density, only \" the high summits of the Cairngorm Mountains, consist entirely of natural elements. These \"high summits\" are of course only part of the Cairngorms, and there are no longer wolves, bears, wild boar or lynx in Scotland's wilderness. The Scots pine in the form of the Caledonian forest also covered much more of the Scottish landscape than today.\n\nThe Swiss National Park, however, represent a more natural landscape. It was founded in 1914, and is one of the earliest national parks in Europe.\nVisitors are not allowed to leave the motor road, or paths through the park, make fire or camp. The only building within the park is Chamanna Cluozza, mountain hut. It is also forbidden to disturb the animals or the plants, or to take home anything found in the park. Dogs are not allowed. Due to these strict rules, the Swiss National Park is the only park in the Alps who has been categorized by the IUCN as a strict nature reserve, which is the highest protection level.\n\nNo place on the Earth is unaffected by people and their culture. People are part of biodiversity, but human activity affects biodiversity, and this alters the natural landscape. Mankind have altered landscape to such an extent that few places on earth remain pristine, but once free of human influences, the landscape can return to a natural or near natural state.\nEven the remote Yukon and Alaskan wilderness, the bi-national Kluane-Wrangell-St. Elias-Glacier Bay-Tatshenshini-Alsek park system comprising Kluane, Wrangell-St Elias, Glacier Bay and Tatshenshini-Alsek parks, a UNESCO World Heritage Site, is not free from human influence, because the Kluane National Park lies within the traditional territories of the Champagne and Aishihik First Nations and Kluane First Nation who have a long history of living in this region. Through their respective Final Agreements with the Canadian Government, they have made into law their rights to harvest in this region.\n\nCultural forces intentionally or unintentionally, have an influence upon the landscape. Cultural landscapes are places or artifacts created and maintained by people. Examples of cultural intrusions into a landscape are: fences, roads, parking lots, sand pits, buildings, hiking trails, management of plants, including the introduction of invasive species, extraction or removal of plants, management of animals, mining, hunting, natural landscaping, farming and forestry, pollution. Areas that might be confused with a natural landscape include public parks, farms, orchards, artificial lakes and reservoirs, managed forests, golf courses, nature center trails, gardens.\n\n"}
{"id": "2629440", "url": "https://en.wikipedia.org/wiki?curid=2629440", "title": "Natural material", "text": "Natural material\n\nA natural material is any product or physical matter that comes from plants, animals, or the ground. Minerals and the metals that can be extracted from them (without further modification) are also considered to belong into this category. Natural materials are used as building materials and clothing. Types include:\n\n\n"}
{"id": "980435", "url": "https://en.wikipedia.org/wiki?curid=980435", "title": "Naturalistic observation", "text": "Naturalistic observation\n\nNaturalistic observation is, in contrast to analog observation, a research tool in which a subject is observed in its natural habitat without any manipulation by the observer. During naturalistic observation, researchers take great care to avoid interfering with the behavior they are observing by using unobtrusive methods. Naturalistic observation involves two main differences that set it apart from other forms of data gathering. In the context of a naturalistic observation, the environment is in no way being manipulated by the observer nor was it created by the observer.\nNaturalistic observation, as a research tool, comes with both advantages and disadvantages that impact its application. By merely observing at a given instance without any manipulation in its natural context, it makes the behaviors exhibited more credible because they are occurring in a real, typical scenario as opposed to an artificial one generated within a lab. Naturalistic observation also allows for study of events that are deemed unethical to study via experimental models, such as the impact of high school shootings on students attending the high school. Naturalistic observation is used in many techniques, from watching an animal's eating patterns in the forest to observing the behavior of students in a school setting.\n\n"}
{"id": "13360851", "url": "https://en.wikipedia.org/wiki?curid=13360851", "title": "Nature center", "text": "Nature center\n\nA nature center (or nature centre) is an organization with a visitor center or interpretive center designed to educate people about nature and the environment. Usually located within a protected open space, nature centers often have trails through their property. Some are located within a state or city park, and some have special gardens or an arboretum. Their properties can be characterized as nature preserves and wildlife sanctuaries. Nature centers generally display small live animals, such as reptiles, rodents, insects, or fish. There are often museum exhibits and displays about natural history, or preserved mounted animals or nature dioramas. Nature centers are staffed by paid or volunteer naturalists and most offer educational programs to the general public, as well as summer camp, after-school and school group programs.\n\nSome nature centers allow free admission but collect voluntary donations in order to help offset expenses. They usually rely on support from dedicated volunteers.\n\nEnvironmental education centers differ from nature centers in that their museum exhibits and education programs are available mostly by appointment, although casual visitors may be allowed to walk on their grounds.\n\nSome city, state and national parks have facilities similar to nature centers, such as museum exhibits, dioramas and trails, and some offer park nature education programs, usually presented by a park ranger.\n\n"}
{"id": "14272151", "url": "https://en.wikipedia.org/wiki?curid=14272151", "title": "Nature religion", "text": "Nature religion\n\nA nature religion is a religious movement that believes nature and the natural world is an embodiment of divinity, sacredness or spiritual power. Nature religions include indigenous religions practiced in various parts of the world by cultures who consider the environment to be imbued with spirits and other sacred entities. It also includes contemporary Pagan faiths which are primarily concentrated in Europe and North America.\n\nThe term \"nature religion\" was first coined by the American religious studies scholar Catherine Albanese, who used it in her work \"Nature Religion in America: From the Algonkian Indians to the New Age\" (1991) and later went on to use it in other studies. Following on from Albanese's development of the term it has since been used by other academics working in the discipline.\n\nCatherine Albanese described nature religion as \"a symbolic center and the cluster of beliefs, behaviours, and values that encircles it\", deeming it to be useful for shining a light on aspects of history that are rarely viewed as religious.\nIn a paper of his on the subject, the Canadian religious studies scholar Peter Beyer described \"nature religion\" as a \"useful analytical abstraction\" to refer to \"any religious belief or practice in which devotees consider nature to be the embodiment of divinity, sacredness, transcendence, spiritual power, or whatever cognate term one wishes to use\". He went on to note that in this way nature religion was not an \"identifiable religious tradition\" such as Buddhism or Christianity are, but that it instead covers \"a range of religious and quasi-religious movements, groups and social networks whose participants may or may not identify with one of the many constructed religions of global society which referred to many other nature religion.\"\n\nPeter Beyer noted the existence of a series of common characteristics which he believed were shared by different nature religions. He remarked that although \"one must be careful not to overgeneralise\", he suspected that there were a series of features which \"occur sufficiently often\" in those nature religions known to recorded scholarship to constitute a pattern.\n\nThe first of these common characteristics was nature religion's \"comparative resistance to institutionalisation and legitimisation in terms of identifiable socio-religious authorities and organisations\", meaning that nature religionists rarely formed their religious beliefs into large, visible socio-political structures such as churches. Furthermore, Beyer noted, nature religionists often held a \"concomitant distrust of and even eschewing of politically orientated power\". Instead of this, he felt that among nature religious communities, there was \"a valuing of community as non-hierarchical\" and a \"conditional optimism with regard to human capacity and the future.\"\n\nIn the sphere of the environment, Beyer noted that nature religionists held to a \"holistic conception of reality\" and \"a valorisation of physical place as vital aspects of their spiritualities\". Similarly, Beyer noted the individualism which was favoured by nature religionists. He remarked that those adhering to such beliefs typically had respect for \"charismatic and hence purely individual authority\" and place a \"strong emphasis on individual paths\" which led them to believe in \"the equal value of individuals and groups\". Along similar lines, he also commented on the \"strong experiential basis\" to nature religionist beliefs \"where personal experience is a final arbiter of truth or validity\".\n\nIn April 1996, the University of Lancaster in North West England held a conference on contemporary Paganism entitled \"Nature Religion Today: Western Paganism, Shamanism and Esotericism in the 1990s\", and ultimately led to the publication of an academic anthology of the same name two years later. This book, \"Nature Religion Today: Paganism in the Modern World\", was edited by members of the University's Department of Religious Studies, a postgraduate named Joanne Pearson and two professors, Richard H. Roberts and Geoffrey Samuel.\n\nIn his study of Wicca, the Pagan studies scholar Ethan Doyle White expressed the view that the category of \"nature religion\" was problematic from a \"historical perspective\" because it solely emphasises the \"commonalities of belief and attitude to the natural world\" that are found between different religions and in doing so divorces these different belief systems from their distinctive socio-cultural and historical backgrounds.\n\n\n"}
{"id": "22756813", "url": "https://en.wikipedia.org/wiki?curid=22756813", "title": "Nicolas Antoine Boulanger", "text": "Nicolas Antoine Boulanger\n\nNicolas Antoine Boulanger (11 November 1722, Paris – 16 September 1759, Paris) was a French philosopher and man of letters during the Age of Enlightenment.\n\nBorn the son of a paper merchant in Paris, Boulanger studied first mathematics, and later ancient languages. He composed several philosophical works in which he sought to come up with naturalistic explanations for superstitions and religious practices, all of which were published posthumously. His major works were \"Research into the Origins of Oriental Despotism\" («Recherches sur l’origine du despotisme oriental», 1761) and \"Antiquity Unveiled\" («L’Antiquité dévoilée par ses usages», 1766). Boulanger's collected works were published in 1792.\n\nThe German-born Baron d'Holbach (Paul-Henri Thiry, 1723–1789) published his controversial anti-religious work \"Christianity Unveiled\" («Christianisme dévoilé», 1761), using Boulanger's name as his pseudonym, just two years after the philosopher's death. Boulanger also was one of the first modern critics of Paul.\n\nThe Koronian asteroid 7346 Boulanger, discovered in 1993, was named in his honor.\n"}
{"id": "4224324", "url": "https://en.wikipedia.org/wiki?curid=4224324", "title": "Origin of water on Earth", "text": "Origin of water on Earth\n\nThe origin of water on Earth, or the reason that there is clearly more liquid water on Earth than on the other rocky planets of the Solar System, is not completely understood. There exist numerous more or less mutually compatible hypotheses as to how water may have accumulated on Earth's surface over the past 4.5 billion years in sufficient quantity to form oceans.\n\nComets, trans-Neptunian objects, or water-rich meteoroids (protoplanets) from the outer reaches of the asteroid belt colliding with Earth may have brought water to the world's oceans. Asteroids may have been primarily responsible based on several studies, including measurements of the ratio of the hydrogen isotopes deuterium and protium, since similar percentage impurities as in carbon-rich chondrites were found in oceanic water, whereas previous measurement of the isotopes' concentrations in comets and trans-Neptunian objects correspond only slightly to water on Earth. In January 2018, researchers reported that two 4.5 billion-year-old meteorites found on Earth contained liquid water alongside a wide diversity of deuterium-poor organic matter.\n\nLarge-enough planetesimals were heated by the decay of aluminium-26. This could cause water to rise to the surface. Recent studies suggest that water with similar deuterium-to-hydrogen ratio was already available at the time of Earth's formation, as evidenced in ancient eucrite meteorites originating from the asteroid Vesta.\n\nThat Earth's water originated purely from comets is implausible, since a result of measurements of the isotope ratios of deuterium to protium (D/H ratio) in the four comets Halley, Hyakutake, Hale–Bopp, and 67P/Churyumov–Gerasimenko, by researchers such as David Jewitt, is approximately double that of oceanic water. What is, however, unclear is whether these comets are representative of those from the Kuiper belt. According to Alessandro Morbidelli, the largest part of today's water comes from protoplanets formed in the outer asteroid belt that plunged towards Earth, as indicated by the D/H proportions in carbon-rich chondrites. The water in carbon-rich chondrites point to a similar D/H ratio as oceanic water. Nevertheless, mechanisms have been proposed to suggest that the D/H-ratio of oceanic water may have increased significantly throughout Earth's history. Such a proposal is consistent with the possibility that a significant amount of the water on Earth was already present during the planet's early evolution.\n\nRecent measurements of the chemical composition of Moon rocks suggest that Earth was born with its water already present. Investigating lunar samples carried to Earth by the Apollo 15 and 17 missions found a deuterium-to-hydrogen ratio that matched the isotopic ratio in carbonaceous chondrites. The ratio is also similar to that found in water on Earth. The findings suggest a common source of water for both objects. This supports a theory that Jupiter temporarily migrated into the inner Solar System, destabilizing the orbits of water-rich carbonaceous chondrites. As a result, some of the bodies could have fallen inwards and become part of the raw material for making Earth and its neighbors. The discovery of water vapor out-gassing from Ceres provides related information on water-ice content of the asteroid belt.\n\nGradual \"dehydration melting\"—leakage of water stored in hydrate minerals of Earth's rocks—could have formed a portion of its water. Water may also have come from volcanism: water vapor in the atmosphere that originated in volcanic eruptions may have condensed to form rain, slowly filling Earth's oceanic basins.\n\nA sizeable quantity of water would have been in the material that formed Earth. Water molecules would have escaped Earth's gravity more easily when it was less massive during its formation. Hydrogen and helium are expected to leak from the atmosphere continually, but the lack of denser noble gases in the modern atmosphere suggests that something disastrous happened to the early atmosphere.\n\nPart of the young planet is theorized to have been disrupted by the impact which created the Moon, which should have caused melting of one or two large areas. Present composition does not match complete melting and it is hard to melt and mix huge rock masses completely. However, a fair fraction of material should have been vaporized by this impact, creating a rock-vapor atmosphere around the young planet. The rock vapor would have condensed within two thousand years, leaving behind hot volatiles which probably resulted in a heavy carbon dioxide atmosphere with hydrogen and water vapor. Liquid water oceans existed despite the surface temperature of because of the atmospheric pressure of the heavy CO atmosphere. As cooling continued, subduction and dissolving in ocean water removed most CO from the atmosphere but levels oscillated wildly as new surface and mantle cycles appeared.\n\nStudy of zircons has found that liquid water must have existed as long ago as 4.404 ± 0.008 Ga, very soon after the formation of Earth. This requires the presence of an atmosphere. The Cool early Earth theory covers a range from about 4.4 Ga to 4.0 Ga.\n\nIn fact, recent studies of zircons (in the fall of 2008) found in Australian Hadean rock hold minerals that point to the existence of plate tectonics as early as 4 billion years ago. If this holds true, the previous beliefs about the Hadean period are far from correct. That is, rather than a hot, molten surface and atmosphere full of carbon dioxide, Earth's surface would be very much like it is today. The action of plate tectonics traps vast amounts of carbon dioxide, thereby reducing greenhouse effects, and leading to a much cooler surface temperature, and the formation of solid rock, and possibly even life.\n\nSome terrestrial water may have had a biochemical origin, during the Great Oxygenation Event, via redox reactions and photosynthesis.\n\nIn the early 1930s, Cornelis van Niel discovered that sulfide-dependent chemoautotrophic bacteria (purple sulfur bacteria) fix carbon and synthesize water as a byproduct of a photosynthetic pathway using hydrogen sulfide and carbon dioxide:\n\nFew modern organisms use this method of photosynthesis, making their water contribution negligible. But on the hydrogen-sulfide-rich and oxygen-poor early Earth, a small but significant portion of Earth's water may have been synthesized biochemically through this pathway.\n\n"}
{"id": "15654603", "url": "https://en.wikipedia.org/wiki?curid=15654603", "title": "Peterson Identification System", "text": "Peterson Identification System\n\nThe Peterson Identification System is a practical method for the field identification of animals, plants and other natural phenomena. It was devised by ornithologist Roger Tory Peterson in 1934 for the first of his series of \"Field Guide\"s (See Peterson Field Guides.) Peterson devised his system \"so that live birds could be identified readily at a distance by their 'field marks' without resorting to the bird-in-hand characters that the early collectors relied on. During the last half century the binocular and the spotting scope have replaced the shotgun.\" As such, it both reflected and contributed to awareness of the emerging early environmental movement.\n\nCreated for use by amateur naturalists and laymen, rather than specialists, the \"Peterson System\" is essentially a pictorial key based upon readily noticed visual impressions rather than on the technical features of interest to scientists. The technique involves patternistic drawings with arrows that pinpoint the key field comparisons between similar species.\n\nSince the first Peterson \"Field Guide\", the system has been expanded to about three dozen volumes in the series as well as being emulated by many other publishers and authors of field guides. It has become the near-universally accepted standard, first in the United States and Europe and then around the world.\n"}
{"id": "53850852", "url": "https://en.wikipedia.org/wiki?curid=53850852", "title": "Phylosymbiosis", "text": "Phylosymbiosis\n\nIn the field of microbiome research, a group of species is said to show a phylosymbiotic signal if the degree of similarity between the species' microbiomes recapitulates to a significant extent their evolutionary history.\nIn other words, a phylosymbiotic signal among a group of species is evident if their microbiome similarity dendrogram could significantly\nrecapitulate their hosts phylogenic tree. For the analysis of the phylosymbiotic signal to be reliable, environmental differences\nthat could shape the host microbiome should be either eliminated or accounted for.\nOne plausible mechanistic explanation for such phenomena could be, for example, a result of host immune genes that rapidly evolve in a continues arms race with members of its microbiome.\n\n"}
{"id": "24714", "url": "https://en.wikipedia.org/wiki?curid=24714", "title": "Precession", "text": "Precession\n\nPrecession is a change in the orientation of the rotational axis of a rotating body. In an appropriate reference frame it can be defined as a change in the first Euler angle, whereas the third Euler angle defines the rotation itself. In other words, if the axis of rotation of a body is itself rotating about a second axis, that body is said to be precessing about the second axis. A motion in which the second Euler angle changes is called \"nutation\". In physics, there are two types of precession: torque-free and torque-induced.\n\nIn astronomy, \"precession\" refers to any of several slow changes in an astronomical body's rotational or orbital parameters. An important example is the steady change in the orientation of the axis of rotation of the Earth, known as the precession of the equinoxes.\n\nTorque-free precession implies that no external moment (torque) is applied to the body. In torque-free precession, the angular momentum is a constant, but the angular velocity vector changes orientation with time. What makes this possible is a time-varying moment of inertia, or more precisely, a time-varying inertia matrix. The inertia matrix is composed of the moments of inertia of a body calculated with respect to separate coordinate axes (e.g. , , ). If an object is asymmetric about its principal axis of rotation, the moment of inertia with respect to each coordinate direction will change with time, while preserving angular momentum. The result is that the component of the angular velocities of the body about each axis will vary inversely with each axis' moment of inertia.\n\nThe torque-free precession rate of an object with an axis of symmetry, such as a disk, spinning about an axis not aligned with that axis of symmetry can be calculated as follows:\n\nwhere is the precession rate, is the spin rate about the axis of symmetry, is the moment of inertia about the axis of symmetry, is moment of inertia about either of the other two equal perpendicular principal axes, and is the angle between the moment of inertia direction and the symmetry axis.\n\nWhen an object is not perfectly solid, internal vortices will tend to damp torque-free precession, and the rotation axis will align itself with one of the inertia axes of the body.\n\nFor a generic solid object without any axis of symmetry, the evolution of the object's orientation, represented (for example) by a rotation matrix that transforms internal to external coordinates, may be numerically simulated. Given the object's fixed internal moment of inertia tensor and fixed external angular momentum , the instantaneous angular velocity is\nPrecession occurs by repeatedly recalculating and applying a small rotation vector for the short time ; e.g.:\nfor the skew-symmetric matrix . The errors induced by finite time steps tend to increase the rotational kinetic energy:\nthis unphysical tendency can be counteracted by repeatedly applying a small rotation vector perpendicular to both and , noting that\n\nAnother type of torque-free precession can occur when there are multiple reference frames at work. For example, Earth is subject to local torque induced precession due to the gravity of the sun and moon acting on Earth's axis, but at the same time the solar system is moving around the galactic center. As a consequence, an accurate measurement of Earth's axial reorientation relative to objects outside the frame of the moving galaxy (such as distant quasars commonly used as precession measurement reference points) must account for a minor amount of non-local torque-free precession, due to the solar system’s motion.\n\nTorque-induced precession (gyroscopic precession) is the phenomenon in which the axis of a spinning object (e.g., a gyroscope) describes a cone in space when an external torque is applied to it. The phenomenon is commonly seen in a spinning toy top, but all rotating objects can undergo precession. If the speed of the rotation and the magnitude of the external torque are constant, the spin axis will move at right angles to the direction that would intuitively result from the external torque. In the case of a toy top, its weight is acting downwards from its center of mass and the normal force (reaction) of the ground is pushing up on it at the point of contact with the support. These two opposite forces produce a torque which causes the top to precess.\nThe device depicted on the right (or above on mobile devices) is gimbal mounted. From inside to outside there are three axes of rotation: the hub of the wheel, the gimbal axis, and the vertical pivot.\n\nTo distinguish between the two horizontal axes, rotation around the wheel hub will be called \"spinning\", and rotation around the gimbal axis will be called \"pitching\". Rotation around the vertical pivot axis is called \"rotation\".\n\nFirst, imagine that the entire device is rotating around the (vertical) pivot axis. Then, spinning of the wheel (around the wheelhub) is added. Imagine the gimbal axis to be locked, so that the wheel cannot pitch. The gimbal axis has sensors, that measure whether there is a torque around the gimbal axis.\n\nIn the picture, a section of the wheel has been named . At the depicted moment in time, section is at the perimeter of the rotating motion around the (vertical) pivot axis. Section , therefore, has a lot of angular rotating velocity with respect to the rotation around the pivot axis, and as is forced closer to the pivot axis of the rotation (by the wheel spinning further), because of the Coriolis effect, with respect to the vertical pivot axis, tends to move in the direction of the top-left arrow in the diagram (shown at 45°) in the direction of rotation around the pivot axis. Section of the wheel is moving away from the pivot axis, and so a force (again, a Coriolis force) acts in the same direction as in the case of . Note that both arrows point in the same direction.\n\nThe same reasoning applies for the bottom half of the wheel, but there the arrows point in the opposite direction to that of the top arrows. Combined over the entire wheel, there is a torque around the gimbal axis when some spinning is added to rotation around a vertical axis.\n\nIt is important to note that the torque around the gimbal axis arises without any delay; the response is instantaneous.\n\nIn the discussion above, the setup was kept unchanging by preventing pitching around the gimbal axis. In the case of a spinning toy top, when the spinning top starts tilting, gravity exerts a torque. However, instead of rolling over, the spinning top just pitches a little. This pitching motion reorients the spinning top with respect to the torque that is being exerted. The result is that the torque exerted by gravity – via the pitching motion – elicits gyroscopic precession (which in turn yields a counter torque against the gravity torque) rather than causing the spinning top to fall to its side.\n\nPrecession or gyroscopic considerations have an effect on bicycle performance at high speed. Precession is also the mechanism behind gyrocompasses.\n\nPrecession is the change of angular velocity and angular momentum produced by a torque. The general equation that relates the torque to the rate of change of angular momentum is:\n\nwhere formula_7 and formula_8 are the torque and angular momentum vectors respectively.\n\nDue to the way the torque vectors are defined, it is a vector that is perpendicular to the plane of the forces that create it. Thus it may be seen that the angular momentum vector will change perpendicular to those forces. Depending on how the forces are created, they will often rotate with the angular momentum vector, and then circular precession is created.\n\nUnder these circumstances the angular velocity of precession is given by:\n\nwhere is the moment of inertia, is the angular velocity of spin about the spin axis, is the mass, is the acceleration due to gravity and is the perpendicular distance of the spin axis about the axis of precession. The torque vector originates at the center of mass. Using , we find that the period of precession is given by:\n\nWhere is the moment of inertia, is the period of spin about the spin axis, and is the torque. In general, the problem is more complicated than this, however.\n\nThere is an easy way to understand why gyroscopic precession occurs without using any mathematics. The behavior of a spinning object simply obeys laws of inertia by resisting any change in direction. A spinning object possesses a property known as rigidity in space, meaning the spin axis resists any change in orientation. It is the inertia of matter comprising the object as it resists any change in direction that provides this property. Of course, the direction this matter travels constantly changes as the object spins, but any further change in direction is resisted. If a force is applied to the surface of a spinning disc, for example, matter experiences no change in direction at the place the force was applied (or 180 degrees from that place). But 90 degrees before and 90 degrees after that place, matter is forced to change direction. This causes the object to behave as if the force was applied at those places instead. When a force is applied to anything, the object exerts an equal force back but in the opposite direction. Since no actual force was applied 90 degrees before or after, nothing prevents the reaction from taking place, and the object causes itself to move in response. A good way to visualize why this happens is to imagine the spinning object to be a large hollow doughnut filled with water, as described in the book \"Thinking Physics\" by Lewis Epstein. The doughnut is held still while water circulates inside it. As the force is applied, the water inside is caused to change direction 90 degrees before and after that point. The water then exerts its own force against the inner wall of the doughnut and causes the doughnut to rotate as if the force was applied 90 degrees ahead in the direction of rotation. Epstein exaggerates the vertical and horizontal motion of the water by changing the shape of the doughnut from round to square with rounded corners. \n\nNow imagine the object to be a spinning bicycle wheel, held at both ends of its axle in the hands of a subject. The wheel is spinning clock-wise as seen from a viewer to the subject’s right. Clock positions on the wheel are given relative to this viewer. As the wheel spins, the molecules comprising it are traveling exactly horizontal and to the right the instant they pass the 12-o'clock position. They then travel vertically downward the instant they pass 3 o'clock, horizontally to the left at 6 o'clock, vertically upward at 9 o’clock and horizontally to the right again at 12 o'clock. Between these positions, each molecule travels components of these directions. Now imagine the viewer applying a force to the rim of the wheel at 12 o’clock. For this example’s sake, imagine the wheel tilting over when this force is applied; it tilts to the left as seen from the subject holding it at its axle. As the wheel tilts to its new position, molecules at 12 o’clock (where the force was applied) as well as those at 6 o’clock, still travel horizontally; their direction did not change as the wheel was tilting. Nor is their direction different after the wheel settles in its new position; they still move horizontally the instant they pass 12 and 6 o’clock. BUT, molecules passing 3 and 9 o’clock were forced to change direction. Those at 3 o’clock were forced to change from moving straight downward, to downward and to the right as viewed from the subject holding the wheel. Molecules passing 9 o’clock were forced to change from moving straight upward, to upward and to the left. This change in direction is resisted by the inertia of those molecules. And when they experience this change in direction, they exert an equal and opposite force in response AT THOSE LOCATIONS-3 AND 9 O’CLOCK. At 3 o’clock, where they were forced to change from moving straight down to downward and to the right, they exert their own equal and opposite reactive force to the left. At 9 o’clock, they exert their own reactive force to the right, as viewed from the subject holding the wheel. This makes the wheel as a whole react by momentarily rotating counter-clockwise as viewed from directly above. Thus, as the force was applied at 12 o’clock, the wheel behaved as if that force was applied at 3 o’clock, which is 90 degrees ahead in the direction of spin. Or, you can say it behaved as if a force from the opposite direction was applied at 9 o'clock, 90 degrees prior to the direction of spin.\n\nIn summary, when you apply a force to a spinning object to change the direction of its spin axis, you are not changing the direction of the matter comprising the object at the place you applied the force (nor at 180 degrees from it); matter experiences zero change in direction at those places. Matter experiences the maximum change in direction 90 degrees before and 90 degrees beyond that place, and lesser amounts closer to it. The equal and opposite reaction that occurs 90 degrees before and after then causes the object to behave as it does. This principle is demonstrated in helicopters. Helicopter controls are rigged so that inputs to them are transmitted to the rotor blades at points 90 degrees prior to and 90 degrees beyond the point at which the change in aircraft attitude is desired. The effect is dramatically felt on motorcycles. A motorcycle will suddenly lean and turn in the opposite direction the handle bars are turned. \n\nGyro precession causes another phenomenon for spinning objects such as the bicycle wheel in this scenario. If the subject holding the wheel removes a hand from one end of its axle, the wheel will not topple over, but will remain upright, supported at just the other end. However, it will immediately take on an additional motion; it will begin to rotate about a vertical axis, pivoting at the point of support as it continues spinning. If you allowed the wheel to continue rotating, you would have to turn your body in the same direction as the wheel rotated. If the wheel was not spinning, it would obviously topple over and fall when one hand is removed. The initial action of the wheel beginning to topple over is equivalent to applying a force to it at 12 o'clock in the direction toward the unsupported side (or a force at 6 o’clock toward the supported side). When the wheel is spinning, the sudden lack of support at one end of its axle is equivalent to this same force. So, instead of toppling over, the wheel behaves as if a continuous force is being applied to it at 3 or 9 o’clock, depending on the direction of spin and which hand was removed. This causes the wheel to begin pivoting at the one supported end of its axle while remaining upright. Although it pivots at that point, it does so only because of the fact that it is supported there; the actual axis of precessional rotation is located vertically through the wheel, passing through its center of mass. Also, this explanation does not account for the effect of variation in the speed of the spinning object; it only illustrates how the spin axis behaves due to precession. More correctly, the object behaves according to the balance of all forces based on the magnitude of the applied force, mass and rotational speed of the object. Once it is visualized why the wheel remains upright and rotates, it can easily be seen why the axis of a spinning top slowly rotates while the top spins as shown in the illustration on this page. A top behaves exactly like the bicycle wheel due to the force of gravity pulling downward. The point of contact with the surface it spins on is equivalent to the end of the axle the wheel is supported at. As the top's spin slows, the reactive force that keeps it upright due to inertia is overcome by gravity. Once the reason for gyro precession is visualized, the mathematical formulas start to make sense. \n\nThe special and general theories of relativity give three types of corrections to the Newtonian precession, of a gyroscope near a large mass such as Earth, described above. They are:\n\nIn astronomy, precession refers to any of several gravity-induced, slow and continuous changes in an astronomical body's rotational axis or orbital path. Precession of the equinoxes, perihelion precession, changes in the tilt of Earth's axis to its orbit, and the eccentricity of its orbit over tens of thousands of years are all important parts of the astronomical theory of ice ages. \"(See Milankovitch cycles.)\"\n\nAxial precession is the movement of the rotational axis of an astronomical body, whereby the axis slowly traces out a cone. In the case of Earth, this type of precession is also known as the \"precession of the equinoxes\", \"lunisolar precession\", or \"precession of the equator\". Earth goes through one such complete precessional cycle in a period of approximately 26,000 years or 1° every 72 years, during which the positions of stars will slowly change in both equatorial coordinates and ecliptic longitude. Over this cycle, Earth's north axial pole moves from where it is now, within 1° of Polaris, in a circle around the ecliptic pole, with an angular radius of about 23.5°.\n\nThe ancient Greek astronomer Hipparchus (c. 190-120 BC) is claimed to be the earliest known astronomer to recognize and assess the precession of the equinoxes at about 1° per century (which is not far from the actual value for antiquity, 1.38°). Caltech's Swerdlow disputes Hipparchus's knowledge of precession because Hipparchus apparently did not necessarily indicate anything like a motion of the entire sphere of the fixed stars with respect to the equinoxes. In ancient China, the Jin-dynasty scholar-official Yu Xi (fl. 307-345 AD) made a similar discovery centuries later, noting that the position of the Sun during the winter solstice had drifted roughly one degree over the course of fifty years relative to the position of the stars. The precession of Earth's axis was later explained by Newtonian physics. Being an oblate spheroid, Earth has a non-spherical shape, bulging outward at the equator. The gravitational tidal forces of the Moon and Sun apply torque to the equator, attempting to pull the equatorial bulge into the plane of the ecliptic, but instead causing it to precess. The torque exerted by the planets, particularly Jupiter, also plays a role.\n\nThe orbits of planets around the Sun do not really follow an identical ellipse each time, but actually trace out a flower-petal shape because the major axis of each planet's elliptical orbit also precesses within its orbital plane, partly in response to perturbations in the form of the changing gravitational forces exerted by other planets. This is called perihelion precession or apsidal precession.\n\nIn the adjunct image, Earth's apsidal precession is illustrated. As the Earth travels around the Sun, its elliptical orbit rotates gradually over time. The eccentricity of its ellipse and the precession rate of its orbit are exaggerated for visualization. Most orbits in the Solar System have a much smaller eccentricity and precess at a much slower rate, making them nearly circular and stationary.\n\nDiscrepancies between the observed perihelion precession rate of the planet Mercury and that predicted by classical mechanics were prominent among the forms of experimental evidence leading to the acceptance of Einstein's Theory of Relativity (in particular, his General Theory of Relativity), which accurately predicted the anomalies. Deviating from Newton's law, Einstein's theory of gravitation predicts an extra term of , which accurately gives the observed excess turning rate of 43″ every 100 years.\n\nThe gravitational force between the Sun and moon induces the precession in Earth's orbit, which is the major cause of the climate oscillation of Earth that has a period of 19,000 to 23,000 years. It follows that changes in Earth's orbital parameters (e.g., orbital inclination, the angle between Earth's rotation axis and its plane of orbit) is important to the study of Earth's climate, in particular to the study of past ice ages.\n\nOrbital nodes also precess over time.\n\n\n"}
{"id": "19951", "url": "https://en.wikipedia.org/wiki?curid=19951", "title": "Pressure measurement", "text": "Pressure measurement\n\nPressure measurement is the analysis of an applied force by a fluid (liquid or gas) on a surface. Pressure is typically measured in units of force per unit of surface area. Many techniques have been developed for the measurement of pressure and vacuum. Instruments used to measure and display pressure in an integral unit are called pressure gauges or vacuum gauges. A manometer (not to be confused with nanometer) is a good example, as it uses a column of liquid to both measure and indicate pressure. Likewise the widely used Bourdon gauge is a mechanical device, which both measures and indicates and is probably the best known type of gauge.\n\nA vacuum gauge is a pressure gauge used to measure pressures lower than the ambient atmospheric pressure, which is set as the zero point, in negative values (e.g.: −15 psig or −760 mmHg equals total vacuum). Most gauges measure pressure relative to atmospheric pressure as the zero point, so this form of reading is simply referred to as \"gauge pressure\". However, anything greater than total vacuum is technically a form of pressure. For very accurate readings, especially at very low pressures, a gauge that uses total vacuum as the zero point may be used, giving pressure readings in an absolute scale.\n\nOther methods of pressure measurement involve sensors that can transmit the pressure reading to a remote indicator or control system (telemetry).\n\nEveryday pressure measurements, such as for vehicle tire pressure, are usually made relative to ambient air pressure. In other cases measurements are made relative to a vacuum or to some other specific reference. When distinguishing between these zero references, the following terms are used:\n\nThe zero reference in use is usually implied by context, and these words are added only when clarification is needed. Tire pressure and blood pressure are gauge pressures by convention, while atmospheric pressures, deep vacuum pressures, and altimeter pressures must be absolute.\n\nFor most working fluids where a fluid exists in a closed system, gauge pressure measurement prevails. Pressure instruments connected to the system will indicate pressures relative to the current atmospheric pressure. The situation changes when extreme vacuum pressures are measured, then absolute pressures are typically used instead.\n\nDifferential pressures are commonly used in industrial process systems. Differential pressure gauges have two inlet ports, each connected to one of the volumes whose pressure is to be monitored. In effect, such a gauge performs the mathematical operation of subtraction through mechanical means, obviating the need for an operator or control system to watch two separate gauges and determine the difference in readings.\n\nModerate vacuum pressure readings can be ambiguous without the proper context, as they may represent absolute pressure or gauge pressure without a negative sign. Thus a vacuum of 26 inHg gauge is equivalent to an absolute pressure of 30 inHg (typical atmospheric pressure) − 26 inHg = 4 inHg.\n\nAtmospheric pressure is typically about 100 kPa at sea level, but is variable with altitude and weather. If the absolute pressure of a fluid stays constant, the gauge pressure of the same fluid will vary as atmospheric pressure changes. For example, when a car drives up a mountain, the (gauge) tire pressure goes up because atmospheric pressure goes down. The absolute pressure in the tire is essentially unchanged.\n\nUsing atmospheric pressure as reference is usually signified by a \"g\" for gauge after the pressure unit, e.g. 70 psig, which means that the pressure measured is the total pressure minus atmospheric pressure. There are two types of gauge reference pressure: vented gauge (vg) and sealed gauge (sg).\n\nA vented-gauge pressure transmitter, for example, allows the outside air pressure to be exposed to the negative side of the pressure-sensing diaphragm, through a vented cable or a hole on the side of the device, so that it always measures the pressure referred to ambient barometric pressure. Thus a vented-gauge reference pressure sensor should always read zero pressure when the process pressure connection is held open to the air.\n\nA sealed gauge reference is very similar, except that atmospheric pressure is sealed on the negative side of the diaphragm. This is usually adopted on high pressure ranges, such as hydraulics, where atmospheric pressure changes will have a negligible effect on the accuracy of the reading, so venting is not necessary. This also allows some manufacturers to provide secondary pressure containment as an extra precaution for pressure equipment safety if the burst pressure of the primary pressure sensing diaphragm is exceeded.\n\nThere is another way of creating a sealed gauge reference, and this is to seal a high vacuum on the reverse side of the sensing diaphragm. Then the output signal is offset, so the pressure sensor reads close to zero when measuring atmospheric pressure.\n\nA sealed gauge reference pressure transducer will never read exactly zero because atmospheric pressure is always changing and the reference in this case is fixed at 1 bar.\n\nTo produce an absolute pressure sensor, the manufacturer seals a high vacuum behind the sensing diaphragm. If the process-pressure connection of an absolute-pressure transmitter is open to the air, it will read the actual barometric pressure.\n\nThe SI unit for pressure is the pascal (Pa), equal to one newton per square metre (N·m or kg·m·s). This special name for the unit was added in 1971; before that, pressure in SI was expressed in units such as N·m. When indicated, the zero reference is stated in parenthesis following the unit, for example 101 kPa (abs). The pound per square inch (psi) is still in widespread use in the US and Canada, for measuring, for instance, tire pressure. A letter is often appended to the psi unit to indicate the measurement's zero reference; psia for absolute, psig for gauge, psid for differential, although this practice is discouraged by the NIST.\n\nBecause pressure was once commonly measured by its ability to displace a column of liquid in a manometer, pressures are often expressed as a depth of a particular fluid (\"e.g.,\" inches of water). Manometric measurement is the subject of pressure head calculations. The most common choices for a manometer's fluid are mercury (Hg) and water; water is nontoxic and readily available, while mercury's density allows for a shorter column (and so a smaller manometer) to measure a given pressure. The abbreviation \"W.C.\" or the words \"water column\" are often printed on gauges and measurements that use water for the manometer.\n\nFluid density and local gravity can vary from one reading to another depending on local factors, so the height of a fluid column does not define pressure precisely. So measurements in \"millimetres of mercury\" or \"inches of mercury\" can be converted to SI units as long as attention is paid to the local factors of fluid density and gravity. Temperature fluctuations change the value of fluid density, while location can affect gravity.\n\nAlthough no longer preferred, these manometric units are still encountered in many fields. Blood pressure is measured in millimetres of mercury (see torr) in most of the world, central venous pressure and lung pressures in centimeters of water are still common, as in settings for CPAP machines. Natural gas pipeline pressures are measured in inches of water, expressed as \"inches W.C.\" Scuba divers often use a manometric rule of thumb: the pressure exerted by ten meters depth of sea water (\"10 msw\") is approximately equal to one atmosphere. In vacuum systems, the units torr (millimeter of mercury), micron (micrometer of mercury), and inch of mercury (inHg) are most commonly used. Torr and micron usually indicates an absolute pressure, while inHg usually indicates a gauge pressure.\n\nAtmospheric pressures are usually stated using hectopascal (hPa), kilopascal (kPa), millibar (mbar) or atmospheres (atm). In American and Canadian engineering, stress is often measured in kip. Note that stress is not a true pressure since it is not scalar. In the cgs system the unit of pressure was the barye (ba), equal to 1 dyn·cm. In the mts system, the unit of pressure was the pieze, equal to 1 sthene per square metre.\n\nMany other hybrid units are used such as mmHg/cm or grams-force/cm (sometimes as kg/cm without properly identifying the force units). Using the names kilogram, gram, kilogram-force, or gram-force (or their symbols) as a unit of force is prohibited in SI; the unit of force in SI is the newton (N).\n\nStatic pressure is uniform in all directions, so pressure measurements are independent of direction in an immovable (static) fluid. Flow, however, applies additional pressure on surfaces perpendicular to the flow direction, while having little impact on surfaces parallel to the flow direction. This directional component of pressure in a moving (dynamic) fluid is called dynamic pressure. An instrument facing the flow direction measures the sum of the static and dynamic pressures; this measurement is called the total pressure or stagnation pressure. Since dynamic pressure is referenced to static pressure, it is neither gauge nor absolute; it is a differential pressure.\n\nWhile static gauge pressure is of primary importance to determining net loads on pipe walls, dynamic pressure is used to measure flow rates and airspeed. Dynamic pressure can be measured by taking the differential pressure between instruments parallel and perpendicular to the flow. Pitot-static tubes, for example perform this measurement on airplanes to determine airspeed. The presence of the measuring instrument inevitably acts to divert flow and create turbulence, so its shape is critical to accuracy and the calibration curves are often non-linear.\n\n\nMany instruments have been invented to measure pressure, with different advantages and disadvantages. Pressure range, sensitivity, dynamic response and cost all vary by several orders of magnitude from one instrument design to the next. The oldest type is the liquid column (a vertical tube filled with mercury) manometer invented by Evangelista Torricelli in 1643. The U-Tube was invented by Christiaan Huygens in 1661.\n\nHydrostatic gauges (such as the mercury column manometer) compare pressure to the hydrostatic force per unit area at the base of a column of fluid. Hydrostatic gauge measurements are independent of the type of gas being measured, and can be designed to have a very linear calibration. They have poor dynamic response.\n\nPiston-type gauges counterbalance the pressure of a fluid with a spring (for example tire-pressure gauges of comparatively low accuracy) or a solid weight, in which case it is known as a deadweight tester and may be used for calibration of other gauges.\n\nLiquid-column gauges consist of a column of liquid in a tube whose ends are exposed to different pressures. The column will rise or fall until its weight (a force applied due to gravity) is in equilibrium with the pressure differential between the two ends of the tube (a force applied due to fluid pressure). A very simple version is a U-shaped tube half-full of liquid, one side of which is connected to the region of interest while the reference pressure (which might be the atmospheric pressure or a vacuum) is applied to the other. The difference in liquid levels represents the applied pressure. The pressure exerted by a column of fluid of height \"h\" and density \"ρ\" is given by the hydrostatic pressure equation, \"P\" = \"hgρ\". Therefore, the pressure difference between the applied pressure \"P\" and the reference pressure \"P\" in a U-tube manometer can be found by solving . In other words, the pressure on either end of the liquid (shown in blue in the figure) must be balanced (since the liquid is static), and so .\n\nIn most liquid-column measurements, the result of the measurement is the height \"h\", expressed typically in mm, cm, or inches. The \"h\" is also known as the pressure head. When expressed as a pressure head, pressure is specified in units of length and the measurement fluid must be specified. When accuracy is critical, the temperature of the measurement fluid must likewise be specified, because liquid density is a function of temperature. So, for example, pressure head might be written \"742.2 mm\" or \"4.2 in at 59 °F\" for measurements taken with mercury or water as the manometric fluid respectively. The word \"gauge\" or \"vacuum\" may be added to such a measurement to distinguish between a pressure above or below the atmospheric pressure. Both mm of mercury and inches of water are common pressure heads, which can be converted to S.I. units of pressure using unit conversion and the above formulas.\n\nIf the fluid being measured is significantly dense, hydrostatic corrections may have to be made for the height between the moving surface of the manometer working fluid and the location where the pressure measurement is desired, except when measuring differential pressure of a fluid (for example, across an orifice plate or venturi), in which case the density ρ should be corrected by subtracting the density of the fluid being measured.\n\nAlthough any fluid can be used, mercury is preferred for its high density (13.534 g/cm) and low vapour pressure. For low pressure differences, light oil or water are commonly used (the latter giving rise to units of measurement such as inches water gauge and millimetres HO. Liquid-column pressure gauges have a highly linear calibration. They have poor dynamic response because the fluid in the column may react slowly to a pressure change.\n\nWhen measuring vacuum, the working liquid may evaporate and contaminate the vacuum if its vapor pressure is too high. When measuring liquid pressure, a loop filled with gas or a light fluid can isolate the liquids to prevent them from mixing, but this can be unnecessary, for example, when mercury is used as the manometer fluid to measure differential pressure of a fluid such as water. Simple hydrostatic gauges can measure pressures ranging from a few torrs (a few 100 Pa) to a few atmospheres (approximately ).\n\nA single-limb liquid-column manometer has a larger reservoir instead of one side of the U-tube and has a scale beside the narrower column. The column may be inclined to further amplify the liquid movement. Based on the use and structure, following types of manometers are used\n\nA McLeod gauge isolates a sample of gas and compresses it in a modified mercury manometer until the pressure is a few millimetres of mercury. The technique is very slow and unsuited to continual monitoring, but is capable of good accuracy. Unlike other manometer gauges, the McLeod gauge reading is dependent on the composition of the gas, since the interpretation relies on the sample compressing as an ideal gas. Due to the compression process, the McLeod gauge completely ignores partial pressures from non-ideal vapors that condense, such as pump oils, mercury, and even water if compressed enough.\n\n0.1 mPa is the lowest direct measurement of pressure that is possible with current technology. Other vacuum gauges can measure lower pressures, but only indirectly by measurement of other pressure-dependent properties. These indirect measurements must be calibrated to SI units by a direct measurement, most commonly a McLeod gauge.\n\nAneroid gauges are based on a metallic pressure-sensing element that flexes elastically under the effect of a pressure difference across the element. \"Aneroid\" means \"without fluid\", and the term originally distinguished these gauges from the hydrostatic gauges described above. However, aneroid gauges can be used to measure the pressure of a liquid as well as a gas, and they are not the only type of gauge that can operate without fluid. For this reason, they are often called mechanical gauges in modern language. Aneroid gauges are not dependent on the type of gas being measured, unlike thermal and ionization gauges, and are less likely to contaminate the system than hydrostatic gauges. The pressure sensing element may be a Bourdon tube, a diaphragm, a capsule, or a set of bellows, which will change shape in response to the pressure of the region in question. The deflection of the pressure sensing element may be read by a linkage connected to a needle, or it may be read by a secondary transducer. The most common secondary transducers in modern vacuum gauges measure a change in capacitance due to the mechanical deflection. Gauges that rely on a change in capacitance are often referred to as capacitance manometers.\n\nThe Bourdon pressure gauge uses the principle that a flattened tube tends to straighten or regain its circular form in cross-section when pressurized. This change in cross-section may be hardly noticeable, involving moderate stresses within the elastic range of easily workable materials. The strain of the material of the tube is magnified by forming the tube into a C shape or even a helix, such that the entire tube tends to straighten out or uncoil elastically as it is pressurized. Eugène Bourdon patented his gauge in France in 1849, and it was widely adopted because of its superior sensitivity, linearity, and accuracy; Edward Ashcroft purchased Bourdon's American patent rights in 1852 and became a major manufacturer of gauges. Also in 1849, Bernard Schaeffer in Magdeburg, Germany patented a successful diaphragm (see below) pressure gauge, which, together with the Bourdon gauge, revolutionized pressure measurement in industry. But in 1875 after Bourdon's patents expired, his company Schaeffer and Budenberg also manufactured Bourdon tube gauges.\n\nIn practice, a flattened thin-wall, closed-end tube is connected at the hollow end to a fixed pipe containing the fluid pressure to be measured. As the pressure increases, the closed end moves in an arc, and this motion is converted into the rotation of a (segment of a) gear by a connecting link that is usually adjustable. A small-diameter pinion gear is on the pointer shaft, so the motion is magnified further by the gear ratio. The positioning of the indicator card behind the pointer, the initial pointer shaft position, the linkage length and initial position, all provide means to calibrate the pointer to indicate the desired range of pressure for variations in the behavior of the Bourdon tube itself. Differential pressure can be measured by gauges containing two different Bourdon tubes, with connecting linkages.\n\nBourdon tubes measure gauge pressure, relative to ambient atmospheric pressure, as opposed to absolute pressure; vacuum is sensed as a reverse motion. Some aneroid barometers use Bourdon tubes closed at both ends (but most use diaphragms or capsules, see below). When the measured pressure is rapidly pulsing, such as when the gauge is near a reciprocating pump, an orifice restriction in the connecting pipe is frequently used to avoid unnecessary wear on the gears and provide an average reading; when the whole gauge is subject to mechanical vibration, the entire case including the pointer and indicator card can be filled with an oil or glycerin. Tapping on the face of the gauge is not recommended as it will tend to falsify actual readings initially presented by the gauge. The Bourdon tube is separate from the face of the gauge and thus has no effect on the actual reading of pressure. Typical high-quality modern gauges provide an accuracy of ±2% of span, and a special high-precision gauge can be as accurate as 0.1% of full scale.\n\nIn the following illustrations the transparent cover face of the pictured combination pressure and vacuum gauge has been removed and the mechanism removed from the case. This particular gauge is a combination vacuum and pressure gauge used for automotive diagnosis:\n\nStationary parts:\n\nMoving Parts:\n\nA second type of aneroid gauge uses deflection of a flexible membrane that separates regions of different pressure. The amount of deflection is repeatable for known pressures so the pressure can be determined by using calibration. The deformation of a thin diaphragm is dependent on the difference in pressure between its two faces. The reference face can be open to atmosphere to measure gauge pressure, open to a second port to measure differential pressure, or can be sealed against a vacuum or other fixed reference pressure to measure absolute pressure. The deformation can be measured using mechanical, optical or capacitive techniques. Ceramic and metallic diaphragms are used.\nFor absolute measurements, welded pressure capsules with diaphragms on either side are often used.\n\nshape:\n\nIn gauges intended to sense small pressures or pressure differences, or require that an absolute pressure be measured, the gear train and needle may be driven by an enclosed and sealed bellows chamber, called an aneroid, which means \"without liquid\". (Early barometers used a column of liquid such as water or the liquid metal mercury suspended by a vacuum.) This bellows configuration is used in aneroid barometers (barometers with an indicating needle and dial card), altimeters, altitude recording barographs, and the altitude telemetry instruments used in weather balloon radiosondes. These devices use the sealed chamber as a reference pressure and are driven by the external pressure. Other sensitive aircraft instruments such as air speed indicators and rate of climb indicators (variometers) have connections both to the internal part of the aneroid chamber and to an external enclosing chamber.\n\nThese gauges use the attraction of two magnets to translate differential pressure into motion of a dial pointer. As differential pressure increases, a magnet attached to either a piston or rubber diaphragm moves. A rotary magnet that is attached to a pointer then moves in unison. To create different pressure ranges, the spring rate can be increased or decreased.\n\nThe spinning-rotor gauge works by measuring the amount a rotating ball is slowed by the viscosity of the gas being measured. The ball is made of steel and is magnetically levitated inside a steel tube closed at one end and exposed to the gas to be measured at the other. The ball is brought up to speed (about 2500 rad/s), and the speed measured after switching off the drive, by electromagnetic transducers. The range of the instrument is 10 to 10 Pa (10 Pa with less accuracy). It is accurate and stable enough to be used as a secondary standard. The instrument requires some skill and knowledge to use correctly. Various corrections must be applied and the ball must be spun at a pressure well below the intended measurement pressure for five hours before using. It is most useful in calibration and research laboratories where high accuracy is required and qualified technicians are available.\n\n\nGenerally, as a real gas increases in density -which may indicate an increase in pressure- its ability to conduct heat increases. In this type of gauge, a wire filament is heated by running current through it. A thermocouple or resistance thermometer (RTD) can then be used to measure the temperature of the filament. This temperature is dependent on the rate at which the filament loses heat to the surrounding gas, and therefore on the thermal conductivity. A common variant is the Pirani gauge, which uses a single platinum filament as both the heated element and RTD. These gauges are accurate from 10 Torr to 10 Torr, but their calibration is sensitive to the chemical composition of the gases being measured.\n\nA Pirani gauge consist of a metal wire open to the pressure being measured. The wire is heated by a current flowing through it and cooled by the gas surrounding it. If the gas pressure is reduced, the cooling effect will decrease, hence the equilibrium temperature of the wire will increase. The resistance of the wire is a function of its temperature: by measuring the voltage across the wire and the current flowing through it, the resistance (and so the gas pressure) can be determined. This type of gauge was invented by Marcello Pirani.\n\nIn two-wire gauges, one wire coil is used as a heater, and the other is used to measure temperature due to convection. Thermocouple gauges and thermistor gauges work in this manner using thermocouple or thermistor, respectively, to measure the temperature of the heated wire.\n\nIonization gauges are the most sensitive gauges for very low pressures (also referred to as hard or high vacuum). They sense pressure indirectly by measuring the electrical ions produced when the gas is bombarded with electrons. Fewer ions will be produced by lower density gases. The calibration of an ion gauge is unstable and dependent on the nature of the gases being measured, which is not always known. They can be calibrated against a McLeod gauge which is much more stable and independent of gas chemistry.\n\nThermionic emission generate electrons, which collide with gas atoms and generate positive ions. The ions are attracted to a suitably biased electrode known as the collector. The current in the collector is proportional to the rate of ionization, which is a function of the pressure in the system. Hence, measuring the collector current gives the gas pressure. There are several sub-types of ionization gauge.\n\nMost ion gauges come in two types: hot cathode and cold cathode. In the hot cathode version, an electrically heated filament produces an electron beam. The electrons travel through the gauge and ionize gas molecules around them. The resulting ions are collected at a negative electrode. The current depends on the number of ions, which depends on the pressure in the gauge. Hot cathode gauges are accurate from 10 Torr to 10 Torr. The principle behind cold cathode version is the same, except that electrons are produced in the discharge of a high voltage. Cold Cathode gauges are accurate from 10 Torr to 10 Torr. Ionization gauge calibration is very sensitive to construction geometry, chemical composition of gases being measured, corrosion and surface deposits. Their calibration can be invalidated by activation at atmospheric pressure or low vacuum. The composition of gases at high vacuums will usually be unpredictable, so a mass spectrometer must be used in conjunction with the ionization gauge for accurate measurement.\n\nA hot-cathode ionization gauge is composed mainly of three electrodes acting together as a triode, wherein the cathode is the filament. The three electrodes are a collector or plate, a filament, and a grid. The collector current is measured in picoamperes by an electrometer. The filament voltage to ground is usually at a potential of 30 volts, while the grid voltage at 180–210 volts DC, unless there is an optional electron bombardment feature, by heating the grid, which may have a high potential of approximately 565 volts.\n\nThe most common ion gauge is the hot-cathode Bayard–Alpert gauge, with a small ion collector inside the grid. A glass envelope with an opening to the vacuum can surround the electrodes, but usually the nude gauge is inserted in the vacuum chamber directly, the pins being fed through a ceramic plate in the wall of the chamber. Hot-cathode gauges can be damaged or lose their calibration if they are exposed to atmospheric pressure or even low vacuum while hot. The measurements of a hot-cathode ionization gauge are always logarithmic.\n\nElectrons emitted from the filament move several times in back-and-forth movements around the grid before finally entering the grid. During these movements, some electrons collide with a gaseous molecule to form a pair of an ion and an electron (electron ionization). The number of these ions is proportional to the gaseous molecule density multiplied by the electron current emitted from the filament, and these ions pour into the collector to form an ion current. Since the gaseous molecule density is proportional to the pressure, the pressure is estimated by measuring the ion current.\n\nThe low-pressure sensitivity of hot-cathode gauges is limited by the photoelectric effect. Electrons hitting the grid produce x-rays that produce photoelectric noise in the ion collector. This limits the range of older hot-cathode gauges to 10 Torr and the Bayard–Alpert to about 10 Torr. Additional wires at cathode potential in the line of sight between the ion collector and the grid prevent this effect. In the extraction type the ions are not attracted by a wire, but by an open cone. As the ions cannot decide which part of the cone to hit, they pass through the hole and form an ion beam. This ion beam can be passed on to a:\n\nThere are two subtypes of cold-cathode ionization gauges: the Penning gauge (invented by Frans Michel Penning), and the Inverted magnetron, also called a Redhead gauge. The major difference between the two is the position of the anode with respect to the cathode. Neither has a filament, and each may require a DC potential of about 4 kV for operation. Inverted magnetrons can measure down to 1  Torr.\n\nLikewise, cold-cathode gauges may be reluctant to start at very low pressures, in that the near-absence of a gas makes it difficult to establish an electrode current - in particular in Penning gauges, which use an axially symmetric magnetic field to create path lengths for electrons that are of the order of metres. In ambient air, suitable ion-pairs are ubiquitously formed by cosmic radiation; in a Penning gauge, design features are used to ease the set-up of a discharge path. For example, the electrode of a Penning gauge is usually finely tapered to facilitate the field emission of electrons.\n\nMaintenance cycles of cold cathode gauges are, in general, measured in years, depending on the gas type and pressure that they are operated in. Using a cold cathode gauge in gases with substantial organic components, such as pump oil fractions, can result in the growth of delicate carbon films and shards within the gauge that eventually either short-circuit the electrodes of the gauge or impede the generation of a discharge path.\n\nWhen fluid flows are not in equilibrium, local pressures may be higher or lower than the average pressure in a medium. These disturbances propagate from their source as longitudinal pressure variations along the path of propagation. This is also called sound. Sound pressure is the instantaneous local pressure deviation from the average pressure caused by a sound wave. Sound pressure can be measured using a microphone in air and a hydrophone in water. The effective sound pressure is the root mean square of the instantaneous sound pressure over a given interval of time. Sound pressures are normally small and are often expressed in units of microbar.\n\nThe American Society of Mechanical Engineers (ASME) has developed two separate and distinct standards on pressure Measurement, B40.100 and PTC 19.2.\nB40.100 provides guidelines on Pressure Indicated Dial Type and Pressure Digital Indicating Gauges, Diaphragm Seals, Snubbers, and Pressure Limiter Valves.\nPTC 19.2 provides instructions and guidance for the accurate determination of pressure values in support of the ASME Performance Test Codes. The choice of method, instruments, required calculations, and corrections to be applied depends on the purpose of the measurement, the allowable uncertainty, and the characteristics of the equipment being tested.\n\nThe methods for pressure measurement and the protocols used for data transmission are also provided. Guidance is given for setting up the instrumentation and determining the uncertainty of the measurement. Information regarding the instrument type, design, applicable pressure range, accuracy, output, and relative cost is provided. Information is also provided on pressure-measuring devices that are used in field environments i.e., Piston Gauges, Manometers, and Low-Absolute-Pressure (Vacuum) Instruments.\n\nThese methods are designed to assist in the evaluation of measurement uncertainty based on current technology and engineering knowledge, taking into account published instrumentation specifications and measurement and application techniques. This Supplement provides guidance in the use of methods to establish the pressure-measurement uncertainty.\n\n\n\n"}
{"id": "5764764", "url": "https://en.wikipedia.org/wiki?curid=5764764", "title": "Radiation trapping", "text": "Radiation trapping\n\nRadiation trapping, imprisonment of resonance radiation, radiative transfer of spectral lines, line transfer or radiation diffusion is a phenomenon in physics whereby radiation may be \"trapped\" in a system as it is emitted by one atom and absorbed by another.\n"}
{"id": "401062", "url": "https://en.wikipedia.org/wiki?curid=401062", "title": "Shower-curtain effect", "text": "Shower-curtain effect\n\nThe shower-curtain effect in physics describes the phenomenon of how a shower curtain gets blown inward with a running shower. The problem of identifying the cause of this effect has been featured in Scientific American magazine, with several theories given to explain the phenomenon but no definite conclusion.\n\nThe shower-curtain effect may also be used to describe the observation how nearby phase front distortions of an optical wave are more severe than remote distortions of the same amplitude.\n\nAlso called Chimney effect or Stack effect, observes that warm air (from the hot shower) rises out over the shower curtain as cooler air (near the floor) pushes in under the curtain to replace the rising air. By pushing the curtain in towards the shower, the (short range) vortex and Coandă effects become more significant. However, the shower-curtain effect persists when cold water is used, implying that this cannot be the only mechanism at work.\n\nSee also Cooling tower.\n\nThe most popular explanation given for the shower-curtain effect is Bernoulli's principle. Bernoulli's principle states that an increase in velocity results in a decrease in pressure. This theory presumes that the water flowing out of a shower head causes the air through which the water moves to start flowing in the same direction as the water. This movement would be parallel to the plane of the shower curtain. If air is moving across the inside surface of the shower curtain, Bernoulli's principle says the air pressure there will drop. This would result in a pressure differential between the inside and outside, causing the curtain to move inward. It would be strongest when the gap between the bather and the curtain is smallest - resulting in the curtain attaching to the bather.\n\nA computer simulation of a typical bathroom found that none of the above theories pan out in their analysis, but instead found that the spray from the shower-head drives a horizontal vortex. This vortex has a low-pressure zone in the centre, which sucks the curtain.\n\nDavid Schmidt of University of Massachusetts was awarded the 2001 Ig Nobel Prize in Physics for his partial solution to the question of why shower curtains billow inwards. He used a computational fluid dynamics code to achieve the results. Professor Schmidt is adamant that this was done \"for fun\" in his own free time without the use of grants.\n\nThe Coandă effect, also known as \"boundary layer attachment\", is the tendency of a moving fluid to adhere to an adjacent wall.\n\nA hot shower will produce steam that condenses on the shower side of the curtain; lowering the pressure there. In a steady state the steam will be replaced by new steam delivered by the shower but in reality the water temperature will fluctuate and lead to times when the net steam production is negative.\n\nColder dense air outside and hot less dense air inside causes higher air pressure on the outside to force the shower curtain inwards to equalise the air pressure, this can be observed simply when the bathroom door is open allowing cold air into the bathroom.\n\nMany shower curtains come with features to reduce the shower-curtain effect. They may have adhesive suction cups on the bottom edges of the curtain, which are then pushed onto the sides of the shower when in use. Others may have magnets at the bottom, though these are not effective on acrylic or fiberglass tubs.\n\nIt's possible to use a telescopic shower curtain rod to block the curtain on it's lower part and to prevent it from sucking inside.\n\nHanging the curtain rod higher or lower, or especially further away from the shower head, can reduce the effect. A (convex) curved shower rod can also be used to hold the curtain against the inside wall of a tub.\n\nA weight can be attached to a long string and the string attached to the curtain rod in the middle of the curtain (on the inside). Hanging the weight low against the curtain just above the rim of the shower pan or tub makes it an effective billowing deterrent without allowing the weight to hit the pan or tub and damage it.\n\nThere are a few alternative solutions that either attach to the shower curtain directly, attach to the shower rod or attach to the wall.\n\n"}
{"id": "41301792", "url": "https://en.wikipedia.org/wiki?curid=41301792", "title": "Space for Life", "text": "Space for Life\n\nSpace for Life () is a museum district in Montreal, Quebec, Canada. It brings together the city's four most prominent natural museums: the Montreal Biodome and the Rio Tinto Alcan Planetarium, situated in Montreal's Olympic Park, and the Montreal Botanical Garden and Montreal Insectarium, in the adjacent Maisonneuve Park.\n\nSpace for Life was established in 2011 as a successor body to Montreal Nature Museums. It describes itself as the largest natural sciences complex in Canada. As of 2013, its executive director is Charles-Mathieu Brunelle and Montreal executive committee member Manon Gauthier is responsible for its political oversight.\n\nThe Montreal Biodome, Insectarium, Botanical Garden and Rio Tinto Alcan Planetarium invite us to rethink the ties between human beings and nature, cultivating a new way of living. Together, the four prestigious institutions form a place where nature and science are honoured. They have positioned themselves as a Space for Life and are dedicated to sharing their vast heritage and knowledge with you.\n\nSpace for Life is a place that brings together the Montreal Biodome, Insectarium, Botanical Garden and Planetarium, but it is also much more. It’s a participatory movement and a commitment to biodiversity. It is a vast project based on citizen participation and co-creation with visitors. Just like nature belongs to everyone, it is everyone’s movement. It’s a state of mind, a way of experiencing nature. It is a space we visit where we can exchange, collaborate and learn.\n\nThrough its efforts in communication, conservation, education and research, Space for Life guides humans to better experience nature.\n\nThe Space for Life comprises the Montreal Biodome,Botanical Garden, Insectarium and Rio Tinto Alcan Planetarium. The institutions are interdependent, and designed to inspire visitors to adopt a new way of experiencing nature. They are connected by the Grande Place, a space that inspires new ways of coming together, enjoying the site, playing outside, building, interacting and experiencing everyday life.\n\nSpace for Life is the largest natural science museum complex in Canada, one of the leading tourist sites in Montréal and all of Quebec and a place with immense potential to impress and thrill visitors through nature, explain nature and encourage behaviour that is respectful of nature.\n\nSpace for Life is also committed to increasing awareness of our planet’s biodiversity and encouraging people to better protect it. In fact, our four institutions have created a sustainable development charter.\n\nBy offering visitors immersive experiences combining science and emotion, the Biodôme, Botanical Garden, Insectarium and Planetarium invite us all to look at nature differently.\n\nWe have a collective commitment to nature, but also a commitment to achievement, meaningfulness and mobilization.\n\nWith their participatory, unifying approach that is authentic, inventive, committed and open to the world, our four institutions have joined forces to create a movement, a Space for Life; a place where people come together to create, shaped by Montrealers and visitors from around the world.\n\nSpace for Life has initiated a movement that aims to help people better understand the concept of interdependence underlying biodiversity, become aware of the services provided by nature and gradually change the way they live. Our institutions invite you to join the movement, by taking an active part and spreading the message yourself.\n\nThe Space for Life is a collective initiative encouraging Montrealers and local stakeholders to become involved and make it their own.\n\nA movement to get closer to nature\nA participatory, creative, historic movement\n\nAt Space for Life, our approach to sustainable development informs all our decisions and actions, encouraging us to consider the inextricable connections between society, ethics, the economy and the environment.\n\nWith this goal in mind, we are committed to integrating sustainable development principles in all our activities. This includes our charter of commitment and 11 areas of focus:\n\n\nMontréal Space for Life is the largest natural science museum complex in Canada. By 2019, four major projects will have been developed, creating constantly evolving and changing spaces for life.\n\nThe Insectarium Metamorphosis, the Biodôme Migration and two other major Space for Life projects are a legacy for Montrealers, for the planet and for future generations.\n\nAt a time when the issues the planet is facing, especially those related to the loss of biodiversity, raise the question of the relevance of our modern lifestyles, Space for Life, which is rooted in these unique institutions whose reputation and credibility are recognized both locally and internationally, has a fundamental role to play.\n\nThese two projects – characterized by their uniqueness, both in terms of architecture and design and the memorable and distinctive experiences they offer visitors – will let Space for Life truly play its role by inviting citizens to reconnect with nature and invent new ways of living. Reflecting a multidisciplinary vision wherein the architectural gesture emerges from a global creative approach, a bold architectural design will create living spaces that are permeable, ecological and evolving, while meeting the highest green building standards.\n\nKuehn Malvezzi, Pelletier De Fontenay, Jodoin Lamarre Pratte, Dupras Ledoux and NCK\n\nA unique experience\n\nThe Insectarium will become a true biotope in which insects, plants and people can interact and take an interest in one another. In an architectural space that is both a landscape and an organism, the underground and closed spaces, water, shadow and daylight follow and play off one another along a route that plunges visitors into an immersive, sensory experience in the heart of the world of insects.\n\n\nKANVA + NEUF architect(e)s, Bouthillette Parizeau + NCK\n\nA renewed experience, re-thinking the deeper meaning of the ecosystems. \n\nThe Migration project will revamp the Biodôme and its scenic design, in most areas accessible to the public, to offer an innovative, participatory, immersive experience. The architects compare the Biodôme to a living organism and their concept echoes notions of cellular biology. The cell, the basic building block of life, becomes a model for structuring and shaping the building blocks of nature – ecosystems. The design team is proposing to reorganize the spaces, open up the centre of the Biodôme, and offer bold, complementary experiences.\n\nThe overall project budget is $22 million. This includes professional and project management fees, studies, construction costs, restoration, acquisition and relocation of live collections, museology, furniture, various contingencies, etc.\n\n\nProgrammation informations can be found on http://espacepourlavie.ca/en \n\n"}
{"id": "10854000", "url": "https://en.wikipedia.org/wiki?curid=10854000", "title": "Statistical study of energy data", "text": "Statistical study of energy data\n\nEnergy statistics refers to collecting, compiling, analyzing and disseminating data on commodities such as coal, crude oil, natural gas, electricity, or renewable energy sources (biomass, geothermal, wind or solar energy), when they are used for the energy they contain. Energy is the capability of some substances, resulting from their physico-chemical properties, to do work or produce heat. Some energy commodities, called fuels, release their energy content as heat when they burn. This heat could be used to run an internal or external combustion engine.\n\nThe need to have statistics on energy commodities became obvious during the 1973 oil crisis that brought tenfold increase in petroleum prices. Before the crisis, to have accurate data on global energy supply and demand was not deemed critical. Another concern of energy statistics today is a huge gap in energy use between developed and developing countries. As the gap narrows (\"see picture\"), the pressure on energy supply increases tremendously. \n\nThe data on energy and electricity come from three principal sources:\nThe flows of and trade in energy commodities are measured both in physical units (e.g., metric tons), and, when energy balances are calculated, in energy units (e.g., terajoules or tons of oil equivalent). What makes energy statistics specific and different from other fields of economic statistics is the fact that energy commodities undergo greater number of transformations (flows) than other commodities. In these transformations energy is conserved, as defined by and within the limitations of the first and second laws of thermodynamics. \n\n\n\n"}
{"id": "4354345", "url": "https://en.wikipedia.org/wiki?curid=4354345", "title": "Stone wall", "text": "Stone wall\n\nStone walls are a kind of masonry construction that has been used for thousands of years. The first stone walls were constructed by farmers and primitive people by piling loose field stones into a dry stone wall. Later, mortar and plaster were used, especially in the construction of city walls, castles, and other fortifications before and during the Middle Ages. These stone walls are spread throughout the world in different forms. One of the best example is the Cyclopean Wall in Rajgir, India.\n\nStone walls are usually made of local materials varying from limestone and flint to granite and sandstone. However, the quality of building stone varies greatly, both in its endurance to weathering, resistance to water penetration and in its ability to be worked into regular shapes before construction. Worked stone is usually known as ashlar, and is often used for corners in stone buildings. Granite is very resistant to weathering, while some limestones are very weak. Other limestones, such as Portland stone, are more weather-resistant.\n\nLarge structures are usually made of very thick walls, so that castles and cathedrals possess walls which may be up to 12 feet thick. They normally consist of a layered stone exterior and rubble infill.\n\n"}
{"id": "3331283", "url": "https://en.wikipedia.org/wiki?curid=3331283", "title": "Suction cup", "text": "Suction cup\n\nA suction cup, also known as a sucker, is a device or object that uses the negative fluid pressure of air or water to adhere to nonporous surfaces, creating a partial vacuum.\n\nSuction cups are peripherial traits of some animals such as octopuses and squids, and have been reproduced artificially for numerous purposes.\n\nThe working face of the suction cup is made of elastic, flexible material and has a curved surface. When the center of the suction cup is pressed against a flat, non-porous surface, the volume of the space between the suction cup and the flat surface is reduced, which causes the air or water between the cup and the surface to be expelled past the rim of the circular cup. The cavity which develops between the cup and the flat surface has little to no air or water in it because most of the fluid has already been forced out of the inside of the cup, causing a lack of pressure. The pressure difference between the atmosphere on the outside of the cup and the low-pressure cavity on the inside of the cup keeps the cup adhered to the surface.\n\nWhen the user ceases to apply physical pressure to the outside of the cup, the elastic substance of which the cup is made tends to resume its original, curved shape. The length of time for which the suction effect can be maintained depends mainly on how long it takes for air or water to leak back into the cavity between the cup and the surface, equalizing the pressure with the surrounding atmosphere. This depends on the porosity and flatness of the surface and the properties of the cup's rim.\n\nThe force required to detach an ideal suction cup by pulling it directly away from the surface is given by the formula:\nwhere:\n\nThis is derived from the definition of pressure, which is:\nFor example, a suction cup of radius 2.0 cm has an area of formula_3(0.020 m) = 0.0013 square meters. Using the force formula (\"F\" = \"AP\"), the result is\n\"F\" = (0.0013 m)(100,000 Pa) = about 130 newtons.\n\nThe above formula relies on several assumptions:\n\nArtificial suction cups are believed to have first been used in the third century, B.C., and were made out of gourds. They were used to suction \"bad blood\" from internal organs to the surface. Hippocrates is believed to have invented this procedure.\n\nThe first modern suction cup patents were issued by the United States Patent and Trademark Office during the 1860s. TC Roche was awarded U.S. Patent No. 52,748 in 1866 for a \"Photographic Developer Dipping Stick\"; the patent discloses a primitive suction cup means for handling photographic plates during developing procedures. In 1868, Orwell Needham patented a more refined suction cup design, U.S. Patent No. 82,629, calling his invention an \"Atmospheric Knob\" purposed for general use as a handle and drawer opening means.\n\nSuction cups have a number of commercial and industrial applications:\n\nOn May 25, 1981, Dan Goodwin, a.k.a. SpiderDan, scaled Sears Tower, the former world's tallest building, with a pair of suction cups. He went on to scale the Renaissance Center in Dallas, the Bonaventure Hotel in Los Angeles, the World Trade Center in New York City, Parque Central Tower in Caracas, the Nippon TV station in Tokyo, and the Millennium Tower in San Francisco.\n\n"}
{"id": "26821712", "url": "https://en.wikipedia.org/wiki?curid=26821712", "title": "Tsunamis in lakes", "text": "Tsunamis in lakes\n\nA tsunami is defined as a series of water waves caused by the displacement of a large volume of a body of water; in the case of this article the body of water being investigated will be a lake rather than an ocean. Tsunamis in lakes are becoming increasingly important to investigate as a hazard, due to the increasing popularity for recreational uses, and increasing populations that inhabit the shores of lakes. Tsunamis generated in lakes and reservoirs are of high concern because it is associated with a near field source region which means a decrease in warning times to minutes or hours.\n\nInland tsunami hazards can be generated by many different types of earth movement. Some of these include earthquakes in or around lake systems, landslides, debris flow, rock avalanches, and glacier calving. Volcanogenic processes such as gas and mass flow characteristics are discussed in more detail below.\n\nTsunamis in lakes can be generated by fault displacement beneath or around lake systems. Faulting shifts the ground in a vertical motion through reverse, normal or oblique strike slip faulting processes, this displaces the water above causing a tsunami (Figure 1). The reason strike-slip faulting does not cause tsunamis is because there is no vertical displacement within the fault movement, only lateral movement resulting in no displacement of the water. In an enclosed basin such as a lake, tsunamis are referred to as the initial wave produced by coseismic displacement from an earthquake, and the seiche as the harmonic resonance within the lake.\n\nIn order for a tsunami to be generated certain criteria is required:\n\nThese tsunamis are of high damage potential due to being within a lake, making them of a near field source. This means a vast decrease in warning times, resulting in organised emergency evacuations after the generation of the tsunami being virtually impossible, and due to low lying shores even small waves lead to substantial flooding. Planning and education of residents needs to be done beforehand, so that when an earthquake is felt they know to head to higher ground and what routes to take to get there.\n\nLake Tahoe is an example of a lake that is in danger of having a tsunami due to faulting processes. Lake Tahoe in California and Nevada USA lies within an intermountain basin bounded by faults, with most of these faults at the lake bottom or hidden in glaciofluvial deposits. Lake Tahoe has had many prehistoric eruptions and in studies of the lake bottom sediments, a 10m high scarp has displaced the lake bottom sediments, indicating that the water was displaced by the same magnitude, as well as generating a tsunami. A tsunami and seiche in Lake Tahoe can be treated as shallow-water long waves as the maximum water depth is much smaller than the wavelength. This demonstrates the interesting impact that lakes have on the tsunami wave characteristics, as it is very different from ocean tsunami wave characteristics due to the ocean being deeper, and lakes being relatively shallow in comparison. With ocean tsunami waves amplitudes only increase when the tsunami gets close to shore, in lake tsunami waves are generated and stay in a shallow environment.\n\nThis would have a major impact on the 34,000 permanent residences along the lake, not to mention the impact on tourism in the area. Tsunami run-ups would leave areas near the lake inundated due to permanent ground subsidence attributed to the earthquake, with the highest run-ups and amplitudes being attributed to the seiches rather than the actual tsunami. The reason seiches cause so much damage is due to resonance within the bays reflecting the waves where they combine to make larger standing waves. For more information see seiches. Lake Tahoe also experienced a massive collapse of the western edge of the basin that formed McKinney Bay around 50,000 years ago. Is thought to have generated a tsunami/seiche wave with a height approaching .\n\nSub-aerial mass flows (landslides or rapid mass wasting) happen when a large amount of sediment becomes unstable, this can happen for example from the shaking from an earthquake, or saturation of the sediment initiating a sliding layer. This volume of sediment then flows into the lake giving a sudden large displacement of water. Tsunamis generated by sub aerial mass flows are defined in terms of the first initial wave being the tsunami wave and any tsunamis in terms of sub aerial mass flows are characterised into three zones. A splash zone or wave generation zone, this is the region were landslides and water motion are coupled and it extends as far as the landslide travels. Near field area, were the concern is based on the characteristics of the tsunami wave such as amplitude and wavelength which are crucial for predictive purposes. Far field area, the process is influenced mainly by dispersion characteristics and is not often used when investigating tsunamis in lakes, as most lake tsunamis are related only to near field processes.\n\nA modern example of a landslide into a reservoir lake, overtopping a dam, occurred in Italy with the Vajont Dam disaster in 1963. Evidence exists in paleoseismological evidence and other sedimentary core sample proxies of catastrophic rock failures of landslide-triggered lake tsunamis worldwide, including in Lake Geneva during AD 563.\n\nIn the event of the Alpine fault in New Zealand rupturing in the South Island, it is predicted that there would be shaking of approximately magnitude eight in the lake side towns of Queenstown (Lake Wakatipu) and Wanaka (Lake Wanaka). These could possibly cause sub-aerial mass flows that could generate tsunamis within the lakes, this would have a devastating impact on the 28,224 residents (2013 New Zealand census) who occupy these lake towns, not only in the potential losses of life and property, but the damage to the booming tourism industry would take years to rebuild.\n\nThe Otago Regional Council, responsible for the area, has recognised that in such an event, tsunamis could occur in both lakes.\n\nIn this article the focus is on tsunamis generated in lakes by volcanogenic processes in terms of gas build up causing violent lake over turns, with other processes such as pyroclastic flows not accounted for, as it requires more complex modelling . Lake overturns can be incredibly dangerous and occur when gas trapped at the bottom of the lake is heated by rising magma causing an explosion and lake overturn; an example of this is Lake Kivu.\n\nLake Kivu, one of the African Great Lakes, lies on the border between the Democratic Republic of the Congo and Rwanda, and is part of the East Africa Rift. Being part of the rift means it is affected by volcanic activity beneath the lake. This has led to a buildup of methane and carbon dioxide at the bottom of the lake, which can lead to violent limnic eruptions.\n\nLimnic eruptions (also called \"lake over turns\") are due to volcanic interaction with the water at the bottom of the lake that has high gas concentrations, this leads to heating of the lake and this rapid rise in temperature would spark a methane explosion displacing a large amount of water, followed nearly simultaneously by a release of carbon dioxide. This carbon dioxide would suffocate large numbers of people, with a possible tsunami generated from water displaced by the gas explosion affecting all of the 2 million people who occupy the shores of Lake Kivu. This is incredibly important as the warning times for an event such as a lake overturn is incredibly short in the order of minutes and the event itself may not even be noticed. Education of locals and preparation is crucial in this case and a lot of research in this area has been done in order to try to understand what is happening within the lake, in order to try to reduce the effects when this phenomenon does happen.\n\nA lake turn-over in Lake Kivu occurs from one of two scenarios. Either (1) up to another hundred years of gas accumulation leads to gas saturation in the lake, resulting in a spontaneous outburst of gas originating at the depth at which gas saturation has exceeded 100%, or (2) a volcanic or even seismic event triggers a turn-over. In either case a strong vertical lift of a large body of water results in a plume of gas bubbles and water rising up to and through the water surface. As the bubbling water column draws in fresh gas-laden water, the bubbling water column widens and becomes more energetic as a virtual \"chain reaction\" occurs which would look like a watery volcano. Very large volumes of water are displaced, vertically at first, then horizontally away from the centre at surface and horizontally inwards to the bottom of the bubbling water column, feeding in fresh gas-laden water. The speed of the rising column of water increases until it has the potential to rise 25m or more in the centre above lake level. The water column has the potential to widen to well in excess of a kilometre, in a violent disturbance of the whole lake. The watery volcano may take as much as a day to fully develop while it releases upwards of 400 billion cubic metres of gas (~12tcf). Some of these parameters are uncertain, particularly the time taken to release the gas and the height to which the water column can rise. As a secondary effect, particularly if the water column behaves irregularly with a series of surges, the lake surface will both rise by up to several metres and create a series of tsunamis or waves radiating away from the epicentre of the eruption. Surface waters may simultaneously race away from the epicentre at speeds as high as 20-40m/second, slowing as distances from the centre increase. The size of the waves created is unpredictable. Wave heights will be highest if the water column surges periodically, resulting in wave heights is great as 10-20m. This is caused by the ever-shifting pathway that the vertical column takes to the surface. No reliable model exists to predict this overall turnover behaviour. For tsunami precautions it will be necessary for people to move to high ground, at least 20m above lake level. A worse situation may pertain in the Ruzizi River where a surge in lake level would cause flash-flooding of the steeply sloping river valley dropping 700m to Lake Tanganyika, where it is possible that a wall of water from 20-50m high may race down the gorge. Water is not the only problem for residents of the Kivu basin; the more than 400 billion cubic metres of gas released creates a denser-than-air cloud which may blanket the whole valley to a depth of 300m or more. The presence of this opaque gas cloud, which would suffocate any living creatures with its mixture of carbon dioxide and methane laced with hydrogen sulphide, would cause the majority of casualties. Residents would be advised to climb to at least 400m above the lake level to ensure their safety. Strangely the risk of a gas explosion is not great as the gas cloud is only about 20% methane in carbon dioxide, a mixture that is difficult to ignite.\n\nAt 11:24 PM on 21 July 2014, in a period experiencing an earthquake swarm related to the upcoming eruption of Bárðarbunga, an 800m-wide section gave way on the slopes of the Icelandic volcano Askja. Beginning at 350m over water height, it caused a tsunami 20–30 meters high across the caldera, and potentially larger at localized points of impact. Thanks to the late hour, no tourists were present; however, search and rescue observed a steam cloud rising from the volcano, apparently geothermal steam released by the landslide. Whether geothermal activity played a role in the landslide is uncertain. A total of 30-50 million cubic meters was involved in the landslide, raising the caldera's water level by 1–2 meters.\n\nHazard mitigation for tsunamis in lakes is immensely important in the preservation of life, infrastructure and property. In order for hazard management of tsunamis in lakes to function at full capacity there are four aspects that need to be balanced and interacted with each other, these are:\n\n\nWhen all these aspects are taken into consideration and continually managed and maintained, the vulnerability of an area to a tsunami within the lake decreases. This is not because the hazard itself has decreased but the awareness of the people who would be affected makes them more prepared to deal with the situation when it does occur. This reduces recovery and response times for an area, decreasing the amount of disruption and in turn the effect the disaster has on the community.\n\nInvestigation into the phenomena of tsunamis in lakes for this article was restricted by certain limitations. Internationally there has been a fair amount of research into certain lakes but not all lakes that can be affected by the phenomenon have been covered. This is especially true for New Zealand with the possible occurrence of tsunamis in the major lakes recognised as a hazard, but with no further research completed.\n\n\n"}
{"id": "926840", "url": "https://en.wikipedia.org/wiki?curid=926840", "title": "Uniformity of motive", "text": "Uniformity of motive\n\nIn astrobiology, the Uniformity of Motive theory suggests that any civilization in the universe would go through similar technological steps in their development. This theory supports the idea that at some point in their history, advanced alien civilizations would use the electromagnetic medium for communications, and thus would emit radio waves that could be detected by projects such as SETI.\n\nThe fact that no artificial EM band communications have ever been detected supports the Fermi Principle, which in conjunction with the Uniformity of Motive theory, and Occam's razor suggests that a civilization that uses this medium is a unique occurrence in Earth's region of the Milky Way Galaxy and perhaps the universe.\n"}
{"id": "3672150", "url": "https://en.wikipedia.org/wiki?curid=3672150", "title": "Vacuum evaporation", "text": "Vacuum evaporation\n\nVacuum evaporation is the process of causing the pressure in a liquid-filled container to be reduced below the vapor pressure of the liquid, causing the liquid to evaporate at a lower temperature than normal. Although the process can be applied to any type of liquid at any vapor pressure, it is generally used to describe the boiling of water by lowering the container's internal pressure below standard atmospheric pressure and causing the water to boil at room temperature.\n\nThe vacuum evaporation treatment process consists of reducing the interior pressure of the evaporation chamber below atmospheric pressure. This reduces the boiling point of the liquid to be evaporated, thereby reducing or eliminating the need for heat in both the boiling and condensation processes. In addition, there are other technical advantages such as the ability to distill other liquids with high boiling points and avoiding the decomposition of substances that are sensitive to temperature, etc.\n\nWhen the process is applied to food and the water is evaporated and removed, the food can be stored for long periods of time without spoiling. It is also used when boiling a substance at normal temperatures would chemically change the consistency of the product, such as egg whites coagulating when attempting to dehydrate the albumen into a powder.\n\nThis process was invented by Henri Nestlé in 1866, of Nestlé Chocolate fame, although the Shakers were already using a vacuum pan earlier than that (see condensed milk).\n\nThis process is used industrially to make such food products as evaporated milk for milk chocolate, and tomato paste for ketchup.\nIn the sugar industry vacuum evaporation is used in the crystallization of sucrose solutions. Traditionally, this process was performed in batch mode, but nowadays continuous vacuum pans are available.\n\nVacuum evaporators are used in a wide range of industrial sectors to treat industrial wastewater. It represents a clean, safe and very versatile technology having low management costs, which in most cases serves as a zero-discharge treatment system.\n\nVacuum evaporation is also a form of physical vapor deposition used in the semiconductor, microelectronics, and optical industries and in this context is a process of depositing thin films of material onto surfaces. Such a technique consists of pumping a vacuum chamber to pressures of less than 10 torr and heating a material to produce a flux of vapor in order to deposit the material onto a surface. The material to be vaporized is typically heated until its vapor pressure is high enough to produce a flux of several Angstroms per second by using an electrically resistive heater or bombardment by a high voltage beam.\n\n"}
{"id": "143689", "url": "https://en.wikipedia.org/wiki?curid=143689", "title": "Vacuum flask", "text": "Vacuum flask\n\nA vacuum flask (also known as a Dewar flask, Dewar bottle or thermos) is an insulating storage vessel that greatly lengthens the time over which its contents remain hotter or cooler than the flask's surroundings. Invented by Sir James Dewar in 1892, the vacuum flask consists of two flasks, placed one within the other and joined at the neck. The gap between the two flasks is partially evacuated of air, creating a near-vacuum which significantly reduces heat transfer by conduction or convection.\n\nVacuum flasks are used domestically to keep beverages hot or cold for extended periods of time and for many purposes in industry.\n\nThe vacuum flask was designed and invented by Scottish scientist Sir James Dewar in 1892 as a result of his research in the field of cryogenics and is sometimes called a Dewar flask in his honour. While performing experiments in determining the specific heat of the element palladium, Dewar made a brass chamber that he enclosed in another chamber to keep the palladium at its desired temperature. He evacuated the air between the two chambers, creating a partial vacuum to keep the temperature of the contents stable. Through the need for this insulated container James Dewar created the vacuum flask, which became a significant tool for chemical experiments and also became a common household item. The flask was later developed using new materials such as glass and aluminum; however, Dewar refused to patent his invention.\n\nDewar's design was quickly transformed into a commercial item in 1904 as two German glassblowers, Reinhold Burger and Albert Aschenbrenner, discovered that it could be used to keep cold drinks cold and warm drinks warm. The Dewar flask design had never been patented but the German men who discovered the commercial use for the product renamed it \"Thermos,\" and subsequently claimed both the rights to the commercial product and the trademark to the name. In his subsequent attempt to claim the rights to the invention, Dewar instead lost a court case to the company. The manufacturing and performance of the Thermos bottle was significantly improved and refined by the Viennese inventor and merchant Gustav Robert Paalen, who designed various types for domestic use, which he also patented, and distributed widely, through his Thermos Bottle Companies in the United States and Canada. The name later became a genericized trademark after the term \"thermos\" became the household name for such a liquid container. The vacuum flask went on to be used for many different types of scientific experiments and the commercial “Thermos” was transformed into a common item. \"Thermos\" remains a registered trademark in some countries, but it was declared a genericized trademark by court action in the United States in 1963, since it had become colloquially synonymous with vacuum flasks in general. However, there are other vacuum flasks.\n\nAfter the German glassblowers determined the commercial uses for the Dewar flask, the technology was sold to the Thermos company, who used it to mass-produce vacuum flasks for at-home use. Over time, the company expanded the size, shapes and materials of these consumer products, primarily used for carrying coffee on the go and carrying liquids on camping trips to keep them either hot or cold. Eventually other manufacturers produced similar products for consumer use.\n\nThe vacuum flask consists of two vessels, one placed within the other and joined at the neck. The gap between the two vessels is partially evacuated of air, creating a partial-vacuum which reduces heat conduction or convection. Heat transfer by thermal radiation may be minimized by silvering flask surfaces facing the gap but can become problematic if the flask's contents or surroundings are very hot; hence vacuum flasks usually hold contents below the boiling point of water. Most heat transfer occurs through the neck and opening of the flask, where there is no vacuum. Vacuum flasks are usually made of metal, borosilicate glass, foam or plastic and have their opening stoppered with cork or polyethylene plastic. Vacuum flasks are often used as insulated shipping containers.\n\nExtremely large or long vacuum flasks sometimes cannot fully support the inner flask from the neck alone, so additional support is provided by \"spacers\" between the interior and exterior shell. These spacers act as a thermal bridge and partially reduce the insulating properties of the flask around the area where the spacer contacts the interior surface.\n\nSeveral technological applications, such as NMR and MRI machines, rely on the use of double vacuum flasks. These flasks have two vacuum sections. The inner flask contains liquid helium and the outer flask contains liquid nitrogen, with one vacuum section in between. The loss of precious helium is limited in this way.\n\nOther improvements to the vacuum flask include the \"vapour-cooled radiation shield\" and the \"vapour-cooled neck\", both of which help to reduce evaporation from the flask.\n\nIn laboratories and industry, vacuum flasks are often used to hold liquefied gases (often LN2) for flash freezing, sample preparation and other processes where maintaining an extreme low temperature is desired. Larger vacuum flasks store liquids that become gaseous at well below ambient temperature, such as oxygen and nitrogen; in this case the leakage of heat into the extremely cold interior of the bottle results in a slow boiling-off of the liquid so that a narrow unstoppered opening, or a stoppered opening protected by a pressure relief valve, is necessary to prevent pressure from building up and eventually shattering the flask. The insulation of the vacuum flask results in a very slow \"boil\" and thus the contents remain liquid for long periods without refrigeration equipment.\n\nVacuum flasks have been used to house standard cells and ovenized Zener diodes, along with their printed circuit board, in precision voltage-regulating devices used as electrical standards. The flask helped with controlling the Zener temperature over a long time span and was used to reduce variations of the output voltage of the Zener standard owing to temperature fluctuation to within a few parts per million.\n\nOne notable use was by Guildline Instruments, of Canada, in their Transvolt, model 9154B, saturated standard cell, which is an electrical voltage standard. Here a silvered vacuum flask was encased in foam insulation and, using a large glass vacuum plug, held the saturated cell. The output of the device was 1.018 volts and was held to within a few parts per million.\n\nThe principle of the vacuum flask makes it ideal for storing certain types of rocket fuel, and NASA used it extensively in the propellant tanks of the Saturn launch vehicles in the 1960s and 1970s.\n\nThe design and shape of the Dewar flask was used as a model for optical experiments based on the idea that the shape of the two compartments with the space in between is similar to the way the light hits the eye. The vacuum flask has also been part of experiments using it as the capacitor of different chemicals in order to keep them at a consistent temperature.\n\nThe industrial Dewar flask is the base for a device used to passively insulate medical shipments. Most vaccines are sensitive to heat and require a cold chain system to keep them at stable, near freezing temperatures. The Arktek device uses eight one-litre ice blocks to hold vaccines at under 10 °C.\n\nVacuum flasks are at risk of implosion hazard, and glass vessels under vacuum, in particular, may shatter unexpectedly. Chips, scratches or cracks can be a starting point for dangerous vessel failure, especially when the vessel temperature changes rapidly (when hot or cold liquid is added). Proper preparation of the Dewar vacuum flask by tempering prior to use is advised to maintain and optimize the functioning of the unit. Glass vacuum flasks are usually fitted into a metal base with the cylinder contained in or coated with mesh, aluminum or plastic to aid in handling, protect it from physical damage, and contain fragments should they break.\n\nIn addition, cryogenic storage dewars are usually pressurized, and they may explode if pressure relief valves are not used.\n\nThe rate of heat (energy) loss through a vacuum flask can be analyzed thermodynamically, starting from the second TdS relation:\n\nformula_1\n\nformula_2\n\nAssuming constant pressure throughout the process,\n\nformula_3\n\nRearranging the equation in terms of the temperature of the outside surface of the vacuum flask's inner wall,\n\nformula_4\n\nWhere\n\n\nNow consider the general expression for heat loss due to radiation:\n\nformula_5\n\nIn the case of the vacuum flask,\n\nformula_6\n\nSubstituting our earlier expression for T,\n\nformula_7\n\nWhere\n\nAssuming that the outer surface of the inner wall and the inner surface of the outer wall of the vacuum flask are coated with polished silver to minimize heat loss due to radiation, we can say that the rate of heat absorption by the inner surface of the outer wall is equal to the absorptivity of polished silver times the heat radiated by the outer surface of the inner wall,\n\nformula_8\n\nIn order for energy balance to be maintained, the heat lost through the outer surface of the outer wall must be equal to the heat absorbed by the inner surface of the outer wall,\n\nformula_9\n\nSince the absorptivity of polished silver is the same as its emissivity, we can write\n\nformula_10\n\nWe must also consider the rate of heat loss through the lid of the vacuum flask (assuming it is made of polypropylene, a common plastic) where there is no vacuum inside the material. In this area, the three heat transfer modes of conduction, convection, and radiation are present. Therefore, the rate of heat loss through the lid is,\n\nformula_11\n\nformula_12\n\nWhere\n\nNow we have an expression for the total rate of heat loss, which is the sum of the rate of heat loss through the walls of the vacuum flask and the rate of heat loss through the lid,\n\nformula_13\n\nwhere we substitute each of the expressions for each component into the equation.\n\nThe rate of entropy generation of this process can also be calculated, starting from entropy balance:\n\nformula_14\n\nWritten in rate form,\n\nformula_15\n\nAssuming a steady-state process,\n\nformula_16\n\nformula_17\n\nSince there is no heat added to the system,\n\nformula_18\n\n\n"}
{"id": "26830333", "url": "https://en.wikipedia.org/wiki?curid=26830333", "title": "Water-energy nexus", "text": "Water-energy nexus\n\nThere is no formal definition for the water-energy nexus - the concept refers to the relationship between the water used for energy production, including both electricity and sources of fuel such as oil and natural gas, and the energy consumed to extract, purify, deliver, heat/cool, treat and dispose of water (and wastewater) sometimes referred to as the energy intensity (EI). The relationship is not truly a closed loop as the water used for energy production need not be the same water that is processed using that energy, but all forms of energy production require some input of water making the relationship inextricable.\n\nAmong the first studies to evaluate the water and energy relationship was a life-cycle analysis conducted by Peter Gleick in 1994 that highlighted the interdependence and initiated the joint study of water and energy. In 2014 the US Department of Energy (DOE) released their report on the water-energy nexus citing the need for joint water-energy policies and better understanding of the nexus and its susceptibility to climate change as a matter of national security. The hybrid Sankey diagram in the DOE's 2014 water-energy nexus report summarizes water and energy flows in the US by sector, demonstrating interdependence as well as singling out thermoelectric power as the single largest user of water, used mainly for cooling. \n\nAll types of energy generation consume water either to process the raw materials used in the facility, constructing and maintaining the plant, or to just generate the electricity itself. Renewable power sources such as photovoltaic solar and wind power, which require little water to produce energy, require water in processing the raw materials to build. Water can either be \"used\" or \"consumed,\" and can be categorised as fresh, ground, surface, blue, grey or green among others. Water is considered used if it does not reduce the supply of water to downstream users, i.e. water that is taken and returned to the same source (instream use), such as in thermoelectric plants that use water for cooling and are by far the largest users of water. While used water is returned to the system for downstream uses, it has usually been degraded in some way, mainly due to thermal or chemical pollution, and the natural flow has been altered which does not factor into an assessment if only the quantity of water is considered. Water is consumed when it is removed completely from the system, such as by evaporation or consumption by crops or humans. When assessing water use all these factors must be considered as well as spatiotemporal considerations making precise determination of water use very difficult.\n\nSpang et al. (2014) conducted a study looking at the water consumption for energy production (WCEP) internationally that both showed the variation in energy types produced across countries as well as the vast differences in efficiency of energy production per unit of water use (figure 1). Operations of water distribution systems and power distribution systems under emergency conditions of limited power and water availability is an important consideration for improving the overall resilience of the water - energy nexus. Khatavkar and Mays (2017) present a methodology for control of water distribution and power distribution systems under emergency conditions of drought and limited power availability to ascertain at least minimal supply of cooling water to the power plants. Khatavkar and Mays (2017 b) applied an optimization model for water - energy nexus system for a hypothetical regional level system which showed an improved resilience for several contingency scenarios.\n\nIn 2001 operating water systems in the US consumed approximately 3% of the total annual electricity (~75 TWh). The California's State Water Project (SWP) and Central Valley Project (CVP) are together the largest water system in the world with the highest water lift, over 2000 ft. across the Tehachapi mountains, delivering water from the wetter and relatively rural north of the state, to the agriculturally intensive central valley, and finally to the arid and heavily populated south. Consequently, the SWP and CVP are the single largest consumers of electricity in California consuming approximately 5 TWh of electricity each per year. In 2001, 19% of the state’s total electricity use (~48 TWh/year) was used in processing water including end uses, with the urban sector accounting for 65% of this. In addition to electricity, 30% of California’s natural gas consumption was due to water-related processes, mainly residential water heating, and 88 million gallons of diesel was consumed by groundwater pumps for agriculture. The residential sector alone accounted for 48% of the total combined electricity and natural gas consumed for water-related processes in the state.\n\nAccording to the California Public Utilities Commission (CPUC) Energy Division’s Embedded Energy in Water Studies report:\"“'Energy Intensity' refers to the average amount of energy needed to transport or treat water or wastewater on a per unit basis\"\"Energy Intensity is sometimes used synonymously with embedded or embodied energy. In 2005, water deliveries to Southern California were assessed to have an average EI of 12.7 MWh/MG, nearly two-thirds of which was due to transportation. Following the findings that a fifth of California’s electricity is consumed in water-related processes including end-use, the CPUC responded by authorising a statewide study into the relationship between energy and water that was conducted by the California Institute for Energy and Environment (CIEE), and developed programs to save energy through water conservation.\n\nHydroelectricity is a special case of water used for energy production mainly because hydroelectric power generation is regarded as being clean and renewable, and dams (the main source of hydroelectric production) serve multiple purposes besides energy generation, including flood prevention, storage, control and recreation which make justifiable allocation analyses difficult. Furthermore, the impacts of hydroelectric power generation can be hard to quantify both in terms of evaporative consumptive losses and altered quality of water, since damming results in flows that are much colder than for flowing streams. In some cases the moderation of flows can be seen as a rivalry of water use in time may also need to accounted for in impact analysis.\n\n\n"}
{"id": "38714", "url": "https://en.wikipedia.org/wiki?curid=38714", "title": "World", "text": "World\n\nThe world is the planet Earth and all life upon it, including human civilization. In a philosophical context, the \"world\" is the whole of the physical Universe, or an ontological world (the \"world\" of an individual). In a theological context, the \"world\" is the material or the profane sphere, as opposed to the celestial, spiritual, transcendent or sacred spheres. \"End of the world\" scenarios refer to the end of human history, often in religious contexts.\n\nThe history of the world is commonly understood as spanning the major geopolitical developments of about five millennia, from the first civilizations to the present. In terms such as world religion, world language, world government, and world war, the term \"world\" suggests an international or intercontinental scope without necessarily implying participation of every part of the world.\n\nThe world population is the sum of all human populations at any time; similarly, the world economy is the sum of the economies of all societies or countries, especially in the context of globalization. Terms such as \"world championship\", \"gross world product\", and \"world flags\" imply the sum or combination of all sovereign states.\n\nThe English word \"world\" comes from the Old English \"weorold (-uld), weorld, worold (-uld, -eld)\", a compound of \"wer\" \"man\" and \"eld\" \"age,\" which thus means roughly \"Age of Man.\"\nThe Old English is a reflex of the Common Germanic \"*wira-alđiz\", also reflected in Old Saxon \"werold\", Old Dutch \"werilt\", Old High German \"weralt\", Old Frisian \"warld\" and Old Norse \"verǫld\" (whence the Icelandic \"veröld\").\n\nThe corresponding word in Latin is \"mundus\", literally \"clean, elegant\", itself a loan translation of Greek \"cosmos\" \"orderly arrangement.\" While the Germanic word thus reflects a mythological notion of a \"domain of Man\" (compare Midgard), presumably as opposed to the divine sphere on the one hand and the chthonic sphere of the underworld on the other, the Greco-Latin term expresses a notion of creation as an act of establishing order out of chaos.\n\n\"World\" distinguishes the entire planet or population from any particular country or region: \"world affairs\" pertain not just to one place but to the whole world, and \"world history\" is a field of history that examines events from a global (rather than a national or a regional) perspective. \"Earth\", on the other hand, refers to the planet as a physical entity, and distinguishes it from other planets and physical objects.\n\n\"World\" was also classically used to mean the material universe, or the cosmos: \"The worlde is an apte frame of heauen and earthe, and all other naturall thinges contained in them.\" The earth was often described as \"the center of the world\".\n\nThe term can also be used attributively, to mean \"global\", or \"relating to the whole world\", forming usages such as world community or world canonical texts.\n\nBy extension, a \"world\" may refer to any planet or heavenly body, especially when it is thought of as inhabited, especially in the context of science fiction or futurology.\n\n\"World\", in its original sense, when qualified, can also refer to a particular domain of human experience.\n\n\nIn philosophy, the term world has several possible meanings. In some contexts, it refers to everything that makes up reality or the physical universe. In others, it can mean have a specific ontological sense (see world disclosure). While clarifying the concept of world has arguably always been among the basic tasks of Western philosophy, this theme appears to have been raised explicitly only at the start of the twentieth century and has been the subject of continuous debate. The question of what the world is has by no means been settled.\n\nThe traditional interpretation of Parmenides' work is that he argued that the everyday perception of reality of the physical world (as described in \"doxa\") is mistaken, and that the reality of the world is 'One Being' (as described in aletheia): an unchanging, ungenerated, indestructible whole.\n\nIn his Allegory of the Cave, Plato distinguishes between forms and ideas and imagines two distinct worlds: the sensible world and the intelligible world.\n\nIn Georg Wilhelm Friedrich Hegel's philosophy of history, the expression \"Weltgeschichte ist Weltgericht\" (World History is a tribunal that judges the World) is used to assert the view that History is what judges men, their actions and their opinions. Science is born from the desire to transform the World in relation to Man; its final end is technical application.\n\n\"The World as Will and Representation\" is the central work of Arthur Schopenhauer.\nSchopenhauer saw the human will as our one window to the world behind the representation; the Kantian thing-in-itself. He believed, therefore, that we could gain knowledge about the thing-in-itself, something Kant said was impossible, since the rest of the relationship between representation and thing-in-itself could be understood by analogy to the relationship between human will and human body.\n\nTwo definitions that were both put forward in the 1920s, however, suggest the range of available opinion. \"The world is everything that is the case,\" wrote Ludwig Wittgenstein in his influential \"Tractatus Logico-Philosophicus\", first published in 1921. This definition would serve as the basis of logical positivism, with its assumption that there is exactly one world, consisting of the totality of facts, regardless of the interpretations that individual people may make of them.\n\nMartin Heidegger, meanwhile, argued that \"the surrounding world is different for each of us, and notwithstanding that we move about in a common world\". The world, for Heidegger, was that into which we are always already \"thrown\" and with which we, as beings-in-the-world, must come to terms. His conception of \"world disclosure\" was most notably elaborated in his 1927 work \"Being and Time\".\n\nIn response, Sigmund Freud proposed that we do not move about in a common world, but a common thought process. He believed that all the actions of a person are motivated by one thing: lust. This led to numerous theories about reactionary consciousness.\n\nSome philosophers, often inspired by David Lewis, argue that metaphysical concepts such as possibility, probability, and necessity are best analyzed by comparing \"the\" world to a range of possible worlds; a view commonly known as modal realism.\n\nMythological cosmologies often depict the world as centered on an \"axis mundi\" and delimited by a boundary such as a world ocean, a world serpent or similar. In some religions, worldliness (also called carnality) is that which relates to this world as opposed to other worlds or realms.\n\nIn Buddhism, the world means society, as distinct from the monastery. It refers to the material world, and to worldly gain such as wealth, reputation, jobs, and war. The spiritual world would be the path to enlightenment, and changes would be sought in what we could call the psychological realm.\n\nIn Christianity, the term often connotes the concept of the fallen and corrupt world order of human society, in contrast to the World to Come. The world is frequently cited alongside \"the flesh\" and \"the Devil\" as a source of temptation that Christians should flee. Monks speak of striving to be \"\"in\" this world, but not \"of\" this world\"—as Jesus said—and the term \"worldhood\" has been distinguished from \"monkhood\", the former being the status of merchants, princes, and others who deal with \"worldly\" things.\n\nThis view is clearly expressed by king Alfred the Great of England (d. 899) in his famous Preface to the \"Cura Pastoralis\":\nAlthough Hebrew and Greek words meaning \"world\" are used in Scripture with the normal variety of senses, many examples of its use in this particular sense can be found in the teachings of Jesus according to the Gospel of John, e.g. 7:7, 8:23, 12:25, 14:17, 15:18-19, 17:6-25, 18:36. For contrast, a relatively newer concept is Catholic imagination.\n\n\"Contemptus mundi\" is the name given to the recognition that the world, in all its vanity, is nothing more than a futile attempt to hide from God by stifling our desire for the good and the holy. This view has been criticized as a \"pastoral of fear\" by modern historian Jean Delumeau.\n\nDuring the Second Vatican Council, there was a novel attempt to develop a positive theological view of the World, which is illustrated by the pastoral optimism of the constitutions \"Gaudium et spes\", \"Lumen gentium\", \"Unitatis redintegratio\" and \"Dignitatis humanae\".\n\nIn Eastern Christian monasticism or asceticism, the world of mankind is driven by passions. Therefore, the passions of the World are simply called \"the world\". Each of these passions are a link to the world of mankind or order of human society. Each of these passions must be overcome in order for a person to receive salvation (theosis). The process of theosis is a personal relationship with God. This understanding is taught within the works of ascetics like Evagrius Ponticus, and the most seminal ascetic works read most widely by Eastern Christians, the Philokalia and the Ladder of Divine Ascent (the works of Evagrius and John Climacus are also contained within the Philokalia). At the highest level of world transcendence is hesychasm which culminates into the Vision of God.\n\n\"Orbis Catholicus\" is a Latin phrase meaning \"Catholic world\", per the expression Urbi et Orbi, and refers to that area of Christendom under papal supremacy. It is somewhat similar to the phrases secular world, Jewish world and Islamic world.\n\n\"Dunya\" derives from the root word \"dana\" that means to bring near. In that sense, \"dunya\" is \"what is brought near\".\n\nHinduism is an Indian religion and \"dharma\", or a way of life, widely practised in the Indian subcontinent. It includes a number of Indian religious traditions with a loose sense of interconnection, as different from Jainism and Buddhism, and (since medieaval and modern times) Islam and Christianity. Hinduism has been called the oldest religion in the world,\n"}
