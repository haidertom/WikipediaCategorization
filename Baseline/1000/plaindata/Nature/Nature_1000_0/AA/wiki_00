{"id": "71318", "url": "https://en.wikipedia.org/wiki?curid=71318", "title": "100-year flood", "text": "100-year flood\n\nA one-hundred-year flood is a flood event that has a 1% probability of occurring in any given year.\n\nThe 100-year flood is also referred to as the 1% flood, since its annual exceedance probability is 1%. For river systems, the 100-year flood is generally expressed as a flowrate. Based on the expected 100-year flood flow rate, the flood water level can be mapped as an area of inundation. The resulting floodplain map is referred to as the 100-year floodplain. Estimates of the 100-year flood flowrate and other streamflow statistics for any stream in the United States are available. In the UK The Environment Agency publishes a comprehensive map of all areas at risk of a 1 in 100 year flood. Areas near the coast of an ocean or large lake also can be flooded by combinations of tide, storm surge, and waves. Maps of the riverine or coastal 100-year floodplain may figure importantly in building permits, environmental regulations, and flood insurance.\n\nA common misunderstanding is that a 100-year flood is likely to occur only once in a 100-year period. In fact, there is approximately a 63.4% chance of one or more 100-year floods occurring in any 100-year period. On the Danube River at Passau, Germany, the actual intervals between 100-year floods during 1501 to 2013 ranged from 37 to 192 years. The probability P that one or more floods occurring during any period will exceed a given flood threshold can be expressed, using the binomial distribution, as\n\nformula_1\n\nwhere T is the threshold return period (e.g. 100-yr, 50-yr, 25-yr, and so forth), and n is the number of years in the period. The probability of exceedance P is also described as the natural, inherent, or hydrologic risk of failure. However, the expected value of the number of 100-year floods occurring in any 100-year period is 1.\n\nTen-year floods have a 10% chance of occurring in any given year (P =0.10); 500-year have a 0.2% chance of occurring in any given year (P =0.002); etc. The percent chance of an X-year flood occurring in a single year is 100/X. A similar analysis is commonly applied to coastal flooding or rainfall data. The recurrence interval of a storm is rarely identical to that of an associated riverine flood, because of rainfall timing and location variations among different drainage basins.\n\nThe field of extreme value theory was created to model rare events such as 100-year floods for the purposes of civil engineering. This theory is most commonly applied to the maximum or minimum observed stream flows of a given river. In desert areas where there are only ephemeral washes, this method is applied to the maximum observed rainfall over a given period of time (24-hours, 6-hours, or 3-hours). The extreme value analysis only considers the most extreme event observed in a given year. So, between the large spring runoff and a heavy summer rain storm, whichever resulted in more runoff would be considered the extreme event, while the smaller event would be ignored in the analysis (even though both may have been capable of causing terrible flooding in their own right).\n\nThere are a number of assumptions which are made to complete the analysis which determines the 100-year flood. First, the extreme events observed in each year must be independent from year-to-year. In other words, the maximum river flow rate from 1984 cannot be found to be significantly correlated with the observed flow rate in 1985. 1985 cannot be correlated with 1986, and so forth. The second assumption is that the observed extreme events must come from the same probability distribution function. The third assumption is that the probability distribution relates to the largest storm (rainfall or river flow rate measurement) that occurs in any one year. The fourth assumption is that the probability distribution function is stationary, meaning that the mean (average), standard deviation and max/min values are not increasing or decreasing over time. This concept is referred to as stationarity.\n\nThe first assumption is often but not always valid and should be tested on a case by case basis. The second assumption is often valid if the extreme events are observed under similar climate conditions. For example, if the extreme events on record all come from late summer thunder storms (as is the case in the southwest U.S.), or from snow pack melting (as is the case in north-central U.S.), then this assumption should be valid. If, however, there are some extreme events taken from thunder storms, others from snow pack melting, and others from hurricanes, then this assumption is most likely not valid. The third assumption is only a problem when trying to forecast a low, but maximum flow event (for example, an event smaller than a 2-year flood). Since this is not typically a goal in extreme analysis, or in civil engineering design, then the situation rarely presents itself. The final assumption about stationarity is difficult to test from data for a single site because of the large uncertainties in even the longest flood records (see next section). More broadly, substantial evidence of climate change strongly suggests that the probability distribution is also changing and that managing flood risks in the future will become even more difficult. The simplest implication of this is that not all of the historical data are, or can be, considered valid as input into the extreme event analysis.\n\nWhen these assumptions are violated there is an \"unknown\" amount of uncertainty introduced into the reported value of what the 100-year flood means in terms of rainfall intensity, or flood depth. When all of the inputs are known the uncertainty can be measured in the form of a confidence interval. For example, one might say there is a 95% chance that the 100-year flood is greater than X, but less than Y.\n\nDirect statistical analysis to estimate the 100-year riverine flood is possible only at the relatively few locations where an annual series of maximum instantaneous flood discharges has been recorded. In the United States as of 2014, taxpayers have supported such records for at least 60 years at fewer than 2,600 locations, for at least 90 years at fewer than 500, and for at least 120 years at only 11. For comparison, the total area of the nation is about , so there are perhaps 3,000 stream reaches that drain watersheds of and 300,000 reaches that drain . In urban areas, 100-year flood estimates are needed for watersheds as small as . For reaches without sufficient data for direct analysis, 100-year flood estimates are derived from indirect statistical analysis of flood records at other locations in a hydrologically similar region or from other hydrologic models. Similarly for coastal floods, tide gauge data exist for only about 1,450 sites worldwide, of which only about 950 added information to the global data center between January 2010 and March 2016.\nMuch longer records of flood elevations exist at a few locations around the world, such as the Danube River at Passau, Germany, but they must be evaluated carefully for accuracy and completeness before any statistical interpretation.\n\nFor an individual stream reach, the uncertainties in any analysis can be large, so 100-year flood estimates have large individual uncertainties for most stream reaches. For the largest recorded flood at any specific location, or any potentially larger event, the recurrence interval always is poorly known. Spatial variability adds more uncertainty, because a flood peak observed at different locations on the same stream during the same event commonly represents a different recurrence interval at each location. If an extreme storm drops enough rain on one branch of a river to cause a 100-year flood, but no rain falls over another branch, the flood wave downstream from their junction might have a recurrence interval of only 10 years. Conversely, a storm that produces a 25-year flood simultaneously in each branch might form a 100-year flood downstream. During a time of flooding, news accounts necessarily simplify the story by reporting the greatest damage and largest recurrence interval estimated at any location. The public can easily and incorrectly conclude that the recurrence interval applies to all stream reaches in the flood area.\n\nPeak elevations of 14 floods as early as 1501 on the Danube River at Passau, Germany, reveal great variability in the actual intervals between floods. Flood events greater than the 50-year flood occurred at intervals of 4 to 192 years since 1501, and the 50-year flood of 2002 was followed only 11 years later by a 500-year flood. Only half of the intervals between 50- and 100-year floods were within 50 percent of the nominal average interval. Similarly, the intervals between 5-year floods during 1955 to 2007 ranged from 5 months to 16 years, and only half were within 2.5 to 7.5 years.\nIn the United States, the 100-year flood provides the risk basis for flood insurance rates. Complete information on the National Flood Insurance Program (NFIP) is available here. A \"regulatory flood\" or \"base flood\" is routinely established for river reaches through a science-based rule-making process targeted to a 100-year flood at the historical average recurrence interval. In addition to historical flood data, the process accounts for previously established regulatory values, the effects of flood-control reservoirs, and changes in land use in the watershed. Coastal flood hazards have been mapped by a similar approach that includes the relevant physical processes. Most areas where serious floods can occur in the United States have been mapped consistently in this manner. On average nationwide, those 100-year flood estimates are well sufficient for the purposes of the NFIP and offer reasonable estimates of future flood risk, if the future is like the past. Approximately 3% of the U.S. population lives in areas subject to the 1% annual chance coastal flood hazard.\n\nIn theory, removing homes and businesses from areas that flood repeatedly can protect people and reduce insurance losses, but in practice it is difficult for people to retreat from established neighborhoods.\n\n\n"}
{"id": "22508780", "url": "https://en.wikipedia.org/wiki?curid=22508780", "title": "Alternative natural materials", "text": "Alternative natural materials\n\nAlternative natural materials are natural materials like rock or adobe that are not as commonly in use as materials such as wood or iron. Alternative natural materials have many practical uses in areas such as sustainable architecture and engineering. The main purpose of using such materials is to minimize the negative effects that our built environment can have on the planet while increasing the efficiency and adaptability of the structures.\n\nAlternative natural materials have existed for quite some time but often in very basic forms or only as ingredients to a particular material in the past. For example, earth used as a building material for walls of houses has existed for thousands of years. Much more recently, in the 1920s, the United States government promoted rammed earth as a fireproof construction method for building farmhouses. Another more common example is adobe. Adobe homes are prominent in the southwestern U.S. and several Spanish-speaking countries.\n\nStraw bale construction is a more modern concept, but there even exists evidence that straw was used to make homes in African prairies as far back as the Paleolithic times.\nAlternative natural materials, specifically their applications, have only recently made their way into more common use. The ideas of being both green and sustainable in response to global warming and climate change shifted more of a focus onto the materials and methods used to build our cityscape and homes. As environmentally conscious decisions became commonplace, the use of alternative natural materials instead of typical natural materials or man-made materials that rely heavily on natural resources became prominent.\n\nRock is a great alternative to conventional materials which contain chemicals that may be harmful to people, pets or the environment. Rocks have two great characteristics: good thermal mass and thermal insulation. These characteristics make stone a great idea because the temperature in the house stays rather constant thus requiring less air conditioning and other cooling systems. Types of rocks that can be employed are reject stone (pieces of stone that are not able to be used for another task), limestone, and flagstone.\nStraw bales can be used as a basis for walls instead of drywall. Straw provides excellent insulation and fire resistance in a traditional post-and-beam structure, where a wood frame supports the house. These straw walls are about 75% more energy efficient than standard drywalls and because no oxygen can get through the walls, fire cannot spread and there is no chance of combustion.\nIn Asian countries, bamboo is being used for structures like bridges and homes. Bamboo is surprisingly strong and rather flexible and grows incredibly fast, making it a rather abundant material. Although it can be difficult to join corners together, bamboo is immensely strong and makes up for the hardships that can be encountered while building it.\n\nCordwood is a combination of small remnants of firewood and other lumber that usually go to waste. These small blocks of wood can easily be put together to make a structure that, like stone, has great insulation as well as thermal mass. Cordwood provides the rustic look of log cabins without the use of tons of lumber. You can build an entire building with just cordwood or use stones to fill in the walls.\n\nRammed earth is a very abundant material that can be used in place of concrete and brick. Soil is packed tightly into wall molds where it is rammed together and hardened to form a durable wall packing made of nothing more than dirt, stones, and sticks. Rammed Earth also provides great thermal mass, which means great energy savings. In addition, it is very weatherproof and durable enough that it was used in the Great Wall of China.\n\nEarth sheltering is a unique building technique in which buildings are completely constructed on at least one side by some form of Earth whether it be a grass roof, clay walls, or both. This unique system usually includes plenty of windows because of the difficulty involved with using too much electricity in such a house. This adds to the energy efficiency of the house by reducing lighting costs.\n\nPapercrete is an interesting and very new material that is a good substitute for concrete. Papercrete is shredded paper, sand, and cement mixed together that forms a very durable brick-like material. Buildings utilizing papercrete are very well-insulated as well as being termite- and fire-resistant. Papercrete is very cheap as it usually only costs about $0.35 per square foot.\n\nAdobe is an age-old technique that is cheap, easy to obtain, and ideal for hot environments. A mixture of sand, clay, and water is poured into a mold and left in the sun to dry. When dried, it is exceptionally strong and heat-resistant. Adobe doesn’t let much heat through to the inside of the structure, thus providing excellent insulation during the summer to reduce energy costs. Although this clay mixture provides excellent insulation from heat, it is not very waterproof and can be dangerous in earth-quake prone areas due to its tendency to crack easily.\n\nSawdust is a good material to combine with clay or cement mixtures and use for walls. These walls turn out surprisingly sturdy and effectively recycle any trees that may need to be excavated from the building area. Depending what type of sawdust used (hardwood is best) the wood chips in the walls absorb moisture and help prevent cracking during freeze/thaw cycles. Sawdust may be combined with water and frozen to produce a material commonly known as pykrete, which is strong, and less prone to melting than regular ice.\n\nAlthough this is a newer technology there are some buildings that have already employed these materials, as well as other tactics, to make themselves green.\n\n"}
{"id": "3522161", "url": "https://en.wikipedia.org/wiki?curid=3522161", "title": "Askaryan radiation", "text": "Askaryan radiation\n\nThe Askaryan radiation also known as Askaryan effect is the phenomenon whereby a particle traveling faster than the phase velocity of light in a dense dielectric (such as salt, ice or the lunar regolith) produces a shower of secondary charged particles which contain a charge anisotropy and thus emits a cone of coherent radiation in the radio or microwave part of the electromagnetic spectrum. It is similar to the Cherenkov radiation. It is named after Gurgen Askaryan, a Soviet-Armenian physicist who postulated it in 1962.\n\nThe radiation was first observed experimentally in 2000, 38 years after its theoretical prediction. So far the effect has been observed in silica sand, rock salt, ice, and Earth's atmosphere.\n\nThe effect is of primary interest in using bulk matter to detect ultra-high energy neutrinos. The Antarctic Impulse Transient Antenna (ANITA) experiment uses antennas attached to a balloon flying over Antarctica to detect the Askaryan radiation produced as cosmic neutrinos travel through the ice. Several experiments have also used the Moon as a neutrino detector based on detection of the Askaryan radiation.\n\n\n"}
{"id": "31167381", "url": "https://en.wikipedia.org/wiki?curid=31167381", "title": "Asperity (geotechnical engineering)", "text": "Asperity (geotechnical engineering)\n\nIn Geotechnical engineering the term asperity is mostly used for unevenness (\"roughness\") of the surface of a discontinuity, grain, or particle with heights in the range from approximately 0.1 mm to many decimetre. Smaller unevenness is normally considered to be a \"material\" property (often denoted by \"material friction\" or \"basic material friction\").\n\nAn often used definition for \"asperities\" in geotechnical engineering:\nUnevenness of a surface are \"asperities\" if these cause dilation if two blocks with in between a discontinuity with matching \"asperities\" on the two opposing surfaces (i.e. a \"fitting discontinuity\") move relative to each other, under low stress levels that do not cause breaking of the \"asperities\".\n\nMaterials science recognizes asperities ranging from the sub-visual (normally less than 0.1 mm) to the atomic scale.\n\n"}
{"id": "2845506", "url": "https://en.wikipedia.org/wiki?curid=2845506", "title": "Bast fibre", "text": "Bast fibre\n\nBast fibre (also called phloem fibre or skin fibre) is plant fibre collected from the phloem (the \"inner bark\", sometimes called \"skin\") or bast surrounding the stem of certain dicotyledonous plants. They support the conductive cells of the phloem and provide strength to the stem. Some of the economically important bast fibres are obtained from herbs cultivated in agriculture, as for instance flax, hemp, or ramie, but also bast fibres from wild plants, as stinging nettle, and trees such as lime or linden, wisteria, and mulberry have been used in the past. Bast fibres are classified as soft fibres, and are flexible. Fibres from monocotyledonous plants, called \"leaf fibre\", are classified as hard fibres and are stiff.\n\nSince the valuable fibres are located in the phloem, they must often be separated from the xylem material (\"woody core\"), and sometimes also from the epidermis. The process for this is called retting, and can be performed by micro-organisms either on land (nowadays the most important) or in water, or by chemicals (for instance high pH and chelating agents) or by pectinolytic enzymes. In the phloem, bast fibres occur in bundles that are glued together by pectin and calcium ions. More intense retting separates the fibre bundles into elementary fibres, that can be several centimetres long. Often bast fibres have higher tensile strength than other kinds, and are used in high-quality textiles (sometimes in blends with cotton or synthetic fibres), ropes, yarn, paper, composite materials and burlap. An important property of bast fibres is that they contain a special structure, the \"fibre node\", that represents a weak point, and gives flexibility. Seed hairs, such as cotton, do not have nodes.\n\nPlants that have been used for bast fibre include flax (from which linen is made), hemp, jute, kenaf, kudzu, linden, milkweed, nettle, okra, paper mulberry, ramie, and roselle hemp.\n\nBast fibres are processed for use in carpet, yarn, rope, geotextile (netting or matting), traditional carpets, hessian or burlap, paper, sacks, etc. Bast fibres are also used in the non-woven, moulding, and composite technology industries for the manufacturing of non-woven mats and carpets, composite boards as furniture materials, automobile door panels and headliners, etc. From prehistoric times through at least the early twentieth century, bast shoes were woven from bast strips in the forest areas of Eastern Europe.\n\nWhere no other source of tanbark was available, bast has also been used for tanning leather.\n\n"}
{"id": "515382", "url": "https://en.wikipedia.org/wiki?curid=515382", "title": "Beamline", "text": "Beamline\n\nIn accelerator physics, a beamline refers to the trajectory of the beam of accelerated particles, including the overall construction of the path segment (vacuum tube, magnets, diagnostic devices) along a specific path of an accelerator facility. This part is either\n\nBeamlines usually end in experimental stations that utilize particle beams or synchrotron light obtained from a synchrotron, or neutrons from a spallation source or research reactor. Beamlines are used in experiments in particle physics, materials science, chemistry, and molecular biology.\n\nIn particle accelerators the beamline is usually housed in a tunnel and/or underground, cased inside a concrete housing. The beamline is usually a cylindrical metal pipe, typically called a \"beam pipe\", and/or a \"drift tube\", evacuated to a high vacuum so there are few gas molecules in the path for the beam of accelerated particles to hit, which would scatter them before they reach their destination.\n\nThere are specialized devices and equipment on the beamline that are used for producing, maintaining, monitoring, and accelerating the particle beam. These devices may be in proximity or attached to the beamline. These devices include sophisticated transducers, diagnostics (position monitors and wire scanners), lenses, collimators, thermocouples, ion pumps, ion gauges, ion chambers (sometimes called \"beam loss monitors\"), vacuum valves (\"isolation valves\"), and gate valves, to mention a few. There are also water cooling devices to cool the dipole and quadrupole magnets. Positive pressure, such as that provided by compressed air, regulates and controls the vacuum valves and manipulators on the beamline.\n\nIt is imperative to have all beamline sections, magnets, etc., aligned by a survey and alignment crew by using a laser tracker. All beamlines must be within micrometre tolerance. Good alignment helps to prevent beam loss, and beam from colliding with the pipe walls, which creates secondary emissions and/or radiation.\n\nRegarding synchrotrons, \"beamline\" may also refer to the instrumentation that carries beams of synchrotron radiation to an experimental end station, which uses the radiation produced by the bending magnets and insertion devices in the storage ring of a synchrotron radiation facility. A typical application for this kind of beamline is crystallography, although many other utilising synchrotron light exist.\n\nAt a large synchrotron facility there will be many beamlines, each optimised for a particular field of research. The differences will depend on the type of insertion device (which, in turn, determines the intensity and spectral distribution of the radiation); the beam conditioning equipment; and the experimental end station. A typical beamline at a modern synchrotron facility will be 25 to 100 m long from the storage ring to the end station, and may cost up to millions of US dollars. For this reason, a synchrotron facility is often built in stages, with the first few beamlines opening on day one of operation, and other beamlines being added later as the funding permits.\n\nThe beamline elements are located in radiation shielding enclosures, called hutches, which are the size of a small room (cabin). A typical beamline consists of two hutches, an optical hutch for the beam conditioning elements and an experimental hutch, which houses the experiment. Between hutches, the beam travels in a transport tube. Entrance to the hutches is forbidden when the beam shutter is open and radiation can enter the hutch. This is enforced by the use of elaborate safety systems with redundant interlocking functions, which make sure that no one is inside the hutch when the radiation is turned on. The safety system will also shut down the radiation beam if the door to the hutch is accidentally opened when the beam is on. In this case, the beam is dumped, meaning the stored beam is diverted into a target designed to absorb and contain its energy.\n\nElements that are used in beamlines by experimenters for conditioning the radiation beam between the storage ring and the end station include the following:\n\n\nThe combination of beam conditioning devices controls the thermal load (heating caused by the beam) at the end station; the spectrum of radiation incident at the end station; and the focus or collimation of the beam. Devices along the beamline which absorb significant power from the beam may need to be actively cooled by water, or liquid nitrogen. The entire length of a beamline is normally kept under ultra high vacuum conditions.\n\nAlthough the design of a synchrotron radiation beamline may be seen as an application of X-ray optics, there are dedicated tools for modeling the x-ray propagation down the beamline and their interaction with various components. There are ray-tracing codes such as Shadow and McXTrace that treat the x-ray beam in the geometric optics limit, and then there are wave propagation software that takes into account diffraction, and the intrinsic wavelike properties of the radiation. For the purposes of understanding full or partial coherence of the synchrotron radiation, the wave properties need to be taken into account. The codes SRW and Spectra include this possibility.\n\nAn experimental end station in a neutron facility is called a neutron beamline. Superficially, neutron beamlines differ from synchrotron radiation beamlines mostly by the fact that they use neutrons from a research reactor or a spallation source instead of photons. The experiments usually measure neutron scattering from the sample under study.\n\n\n"}
{"id": "58685900", "url": "https://en.wikipedia.org/wiki?curid=58685900", "title": "Bioresilience", "text": "Bioresilience\n\nBioresilience refers to the ability of a whole species or an individual of a species to adapt to change. Initially the term applied to changes in the natural environment, but increasingly it is also used for adaptation to anthropogenically induced change.\n\nAlexander von Humboldt was the first to note the resilience of life forms with increasing altitude and the accompanying decreasing prevalence in numbers, and he documented this in the 18th century on the slopes of the volcano Chimborazo.\n\nUnderstanding of bioresilience evolved from research led by The Mountain Institute when establishing two of the national parks that surround Mount Everest, Makalu-Barun National Park in Nepal, and Qomolangma National Nature Preserve in the Tibet Autonomous Region of China. The research documented greater biodiversity at Everest’s base than higher up. There were progressively fewer documented species as the mountain ascended into higher biomes, from subtropical to temperate to alpine to Arctic-like. These fewer species, though, had greater biologic robustness correlating directly with increasing bioresilience. \n\nMonitoring of bioresilience, beginning in the Everest ecosystem but expanding to other mountain ecologies globally is being carried out by the Biomeridian Project at Future Generations University.\n\nThe concept of bioresilience has also been applied to human health to explain aging or chronic diseases decrease the ability of the body to adapt; in such cases, the system becomes rigid and unable to cross different life demands. As the human body loses robustness with age, an individual becomes unable to accommodate new life demands, be they contagions, stress, or events such as injury or even jet lag.\n\nThe importance of resilience in biological systems has been widely recognized in terms of the impacts on life by anthropogenic changes. Accelerating environmental change and continuing loss of genetic resources positions lower biodiversity around the planet threatening ecosystem services. A major mitigating factor will be life forms with higher resilience. \n\nParalleling the work in mountain environments, a growing number of studies is applying the concept of bioresilience to assess the robustness of life in other ecological systems challenged by the Anthropocene. One such study was with the adaptive renewal and natural perturbation in Lake Victoria, the world’s second largest freshwater lake.\n"}
{"id": "42739", "url": "https://en.wikipedia.org/wiki?curid=42739", "title": "Bubble fusion", "text": "Bubble fusion\n\nBubble fusion is the non-technical name for a nuclear fusion reaction hypothesized to occur inside extraordinarily large collapsing gas bubbles created in a liquid during acoustic cavitation. The more technical name is sonofusion.\n\nThe term was coined in 2002 with the release of a report by Rusi Taleyarkhan and collaborators that claimed to have observed evidence of sonofusion. The claim was quickly surrounded by controversy, including allegations ranging from experimental error to academic fraud. Subsequent publications claiming independent verification of sonofusion were also highly controversial.\n\nEventually, an investigation by Purdue University found that Taleyarkhan had engaged in falsification of independent verification, and had included a student as an author on a paper when he had not participated in the research. He was subsequently stripped of his professorship. One of his funders, the Office of Naval Research reviewed the report by Purdue and barred him from federal funding for 28 months.\n\nUS patent 4,333,796, filed by Hugh Flynn in 1978, appears to be the earliest documented reference to a sonofusion-type reaction.\n\nIn the March 8, 2002 issue of the peer-reviewed journal \"Science\", Rusi P. Taleyarkhan and colleagues at the Oak Ridge National Laboratory (ORNL) reported that acoustic cavitation experiments conducted with deuterated acetone () showed measurements of tritium and neutron output consistent with the occurrence of fusion. The neutron emission was also reported to be coincident with the sonoluminescence pulse, a key indicator that its source was fusion caused by the heat and pressure inside the collapsing bubbles.\n\nThe results were so startling, that the Oak Ridge National Laboratory asked two independent researchers, D. Shapira and M. J. Saltmarsh, to repeat the experiment using more sophisticated neutron detection equipment. They reported that the neutron release was consistent with random coincidence. A rebuttal by Taleyarkhan and the other authors of the original report argued that the Shapira and Saltmarsh report failed to account for significant differences in experimental setup, including over an inch of shielding between the neutron detector and the sonoluminescing acetone. According to Taleyarkhan \"et al.\", when properly considering those differences, the results were consistent with fusion.\n\nAs early as 2002, while experimental work was still in progress, Aaron Galonsky of Michigan State University, in a letter to the journal \"Science\"\nexpressed doubts about the claim made by the Taleyarkhan team. In Galonsky's opinion, the observed neutrons were too high in energy to be from a deuterium-deuterium (d-d) fusion reaction. In their response (published on the same page), the Taleyarkhan team provided detailed counter-arguments and concluded that the energy was \"reasonably close\" to that which was expected from a fusion reaction.\n\nIn February 2005 the documentary series \"Horizon\" commissioned two leading sonoluminescence researchers, Seth Putterman and Kenneth S. Suslick, to reproduce Taleyarkhan's work. Using similar acoustic parameters, deuterated acetone, similar bubble nucleation, and a much more sophisticated neutron detection device, the researchers could find no evidence of a fusion reaction.\n\nIn 2004, new reports of bubble fusion were published by the Taleyarkhan group, claiming that the results of previous experiments had been replicated under more stringent experimental conditions. These results differed from the original results in that fusion was claimed to occur over longer times than previously reported. The original report only claimed neutron emission from the initial bubble collapse following bubble nucleation, whereas this report claimed neutron emission many acoustic cycles later.\n\nIn July 2005, two of Taleyarkhan's students at Purdue University published evidence confirming the previous result. They used the same acoustic chamber, the same deuterated acetone fluid and a similar bubble nucleation system. In this report, no neutron-sonoluminescence coincidence was attempted. An article in \"Nature\" raised issues about the validity of the research and complaints from his Purdue colleagues (see full analysis elsewhere in this page). Charges of misconduct were raised, and Purdue University opened an investigation. It concluded in 2008 that Taleyarkhan's name should have appeared in the author list because of his deep involvement in many steps of the research, that he added one author that had not really participated in the paper just to overcome the criticism of one reviewer, and that this was part of an attempt of \"an effort to falsify the scientific record by assertion of independent confirmation\". The investigation did not address the validity of the experimental results.\n\nIn January 2006, a paper published in the journal \"Physical Review Letters\" by Taleyarkhan in collaboration with researchers from Rensselaer Polytechnic Institute reported statistically significant evidence of fusion.\n\nIn November 2006, in the midst of accusations concerning Taleyarkhan's research standards, two different scientists visited the meta-stable fluids research lab at Purdue University to measure neutrons, using Taleyarkhan's equipment. Dr. Edward R. Forringer and undergraduates David Robbins and Jonathan Martin of LeTourneau University presented two papers at the American Nuclear Society Winter Meeting that reported replication of neutron emission. Their experimental setup was similar to previous experiments in that it used a mixture of deuterated acetone, deuterated benzene, tetrachloroethylene and uranyl nitrate. Notably, however, it operated without an external neutron source and used two types of neutron detectors. They claimed a liquid scintillation detector measured neutron levels at 8 standard deviations above the background level, while plastic detectors measured levels at 3.8 standard deviations above the background. When the same experiment was performed with non-deuterated control liquid, the measurements were within one standard deviation of background, indicating that the neutron production had only occurred during cavitation of the deuterated liquid. William M. Bugg, emeritus physics professor at the University of Tennessee also traveled to Taleyarkhan's lab to repeat the experiment with his equipment. He also reported neutron emission, using plastic neutron detectors. Taleyarkhan claimed these visits counted as independent replications by experts, but Forringer later recognized that he was not an expert, and Bugg later said that Taleyarkhan performed the experiments and he had only watched.\n\nIn March 2006, \"Nature\" published a special report that called into question the validity of the results of the Purdue experiments. The report quotes Brian Naranjo of the University of California, Los Angeles to the effect that neutron energy spectrum reported in the 2006 paper by Taleyarkhan, et al. was statistically inconsistent with neutrons produced by the proposed fusion reaction and instead highly consistent with neutrons produced by the radioactive decay of Californium 252, an isotope commonly used as a laboratory neutron source .\n\nThe response of Taleyarkhan \"et al.\", published in \"Physical Review Letters\", attempts to refute Naranjo's hypothesis as to the cause of the neutrons detected.\n\nTsoukalas, head of the School of Nuclear Engineering at Purdue, and several of his colleagues at Purdue, had convinced Taleyarkhan to move to Purdue and attempt a joint replication. In the 2006 \"Nature\" report they detail several troubling issues when trying to collaborate with Taleyarkhan. He reported positive results from certain set of raw data, but his colleagues had also examined that set and it only contained negative results. He never showed his colleagues the raw data corresponding to the positive results, despite several requests. He moved the equipment from a shared laboratory to his own laboratory, thus impeding review by his colleagues, and he didn't give any advance warning or explanation for the move. Taleyarkhan convinced his colleagues that they shouldn't publish a paper with their negative results. Taleyarkhan then insisted that the university's press release present his experiment as \"peer-reviewed\" and \"independent\", when the co-authors were working in his laboratory under his supervision, and his peers in the faculty were not allowed to review the data. In summary, Taleyarkhan's colleagues at Purdue said he placed obstacles to peer review of his experiments, and they had serious doubts about the validity of the research.\n\n\"Nature\" also revealed that the process of anonymous peer-review had not been followed, and that the journal \"Nuclear Engineering and Design\" was not independent from the authors. Taleyarkhan was co-editor of the journal, and the paper was only peer-reviewed by his co-editor, with Taleyarkhan's knowledge.\n\nIn 2002 Taleyarkhan filed a patent application on behalf of the United States Department of Energy, while working in Oak Ridge. \"Nature\" reported that the patent had been rejected in 2005 by the US Patent Office. The examiner called the experiment a variation of discredited cold fusion, found that there was \"no reputable evidence of record to support any allegations or claims that the invention is capable of operating as indicated\", and found that there was not enough detail for others to replicate the invention. The field of fusion suffered from many flawed claims, thus the examiner asked for additional proof that the radiation was generated from fusion and not from other sources. An appeal was not filed because the Department of Energy had dropped the claim in December 2005.\n\nDoubts among Purdue University's Nuclear Engineering faculty as to whether the positive results reported from sonofusion experiments conducted there were truthful prompted the university to initiate a review of the research, conducted by Purdue's Office of the Vice President for Research. In a March 9, 2006 article entitled \"Evidence for bubble fusion called into question\", \"Nature\" interviewed several of Taleyarkhan's colleagues who suspected something was amiss.\n\nOn February 7, 2007, the Purdue University administration determined that \"the evidence does not support the allegations of research misconduct and that no further investigation of the allegations is warranted\". Their report also stated that \"vigorous, open debate of the scientific merits of this new technology is the most appropriate focus going forward.\" In order to verify that the investigation was properly conducted, House Representative Brad Miller requested full copies of its documents and reports by March 30, 2007. His congressional report concluded that \"Purdue deviated from its own procedures in investigating this case and did not conduct a thorough investigation\"; in response, Purdue announced that it would re-open its investigation.\n\nIn June 2008, a multi-institutional team including Taleyarkhan published a paper in Nuclear Engineering and Design to \"clear up misconceptions generated by a webposting of UCLA which served as the basis for the \"Nature\" article of March 2006\", according to a press release.\n\nOn July 18, 2008, Purdue University announced that a committee with members from five institutions had investigated 12 allegations of research misconduct against Rusi Taleyarkhan. It concluded that two allegations were founded—that Taleyarkhan had claimed independent confirmation of his work when in reality the apparent confirmations were done by Taleyarkhan's former students and was not as \"independent\" as Taleyarkhan implied, and that Taleyarkhan had included a colleague's name on one of his papers who had not actually been involved in the research (\"the sole apparent motivation for the addition of Mr. Butt was a desire to overcome a reviewer's criticism\", the report concluded).\n\nTaleyarkhan's appeal of the report's conclusions was rejected. He said the two allegations of misconduct were trivial administrative issues and had nothing to do with the discovery of bubble nuclear fusion or the underlying science, and that \"all allegations of fraud and fabrication have been dismissed as invalid and without merit — thereby supporting the underlying science and experimental data as being on solid ground\". A researcher questioned by the LA Times said that the report had not clarified whether bubble fusion was real or not, but that the low quality of the papers and the doubts cast by the report had destroyed Taleyarkhan's credibility with the scientific community.\n\nOn August 27, 2008 he was stripped of his named Arden Bement Jr. Professorship, and forbidden to be a thesis advisor for graduate students for at least the next 3 years.\n\nDespite the findings against him, Taleyarkhan received a $185,000 grant from the National Science Foundation between September 2008 and August 2009 to investigate bubble fusion. In 2009 the Office of Naval Research debarred him for 28 months, until September 2011, from receiving U.S. Federal Funding. During that period his name was listed in the 'Excluded Parties List' to prevent him from receiving further grants from any government agency.\n\n\n"}
{"id": "19349161", "url": "https://en.wikipedia.org/wiki?curid=19349161", "title": "Cambrian explosion", "text": "Cambrian explosion\n\nThe Cambrian explosion or Cambrian radiation was an event approximately in the Cambrian period when most major animal phyla appeared in the fossil record. It lasted for about 20–25 million years. It resulted in the divergence of most modern metazoan phyla. The event was accompanied by major diversification of other organisms.\n\nBefore the Cambrian explosion, most organisms were simple, composed of individual cells occasionally organized into colonies. Over the following 70 to 80 million years, the rate of diversification accelerated, and the variety of life began to resemble that of today. Almost all present animal phyla appeared during this period.\n\nThe Cambrian explosion has generated extensive scientific debate.\n\nThe seemingly rapid appearance of fossils in the \"Primordial Strata\" was noted by William Buckland in the 1840s, and in his 1859 book \"On the Origin of Species\", Charles Darwin discussed the then inexplicable lack of earlier fossils as one of the main difficulties for his theory of descent with slow modification through natural selection. The long-running puzzlement about the appearance of the Cambrian fauna, seemingly abruptly, without precursor, centers on three key points: whether there really was a mass diversification of complex organisms over a relatively short period of time during the early Cambrian; what might have caused such rapid change; and what it would imply about the origin of animal life. Interpretation is difficult due to a limited supply of evidence, based mainly on an incomplete fossil record and chemical signatures remaining in Cambrian rocks.\n\nThe first discovered Cambrian fossils were trilobites, described by Edward Lhuyd, the curator of Oxford Museum, in 1698. Although their evolutionary importance was not known, on the basis of their old age, William Buckland (1784–1856) realised that a dramatic step-change in the fossil record had occurred around the base of what we now call the Cambrian. Nineteenth-century geologists such as Adam Sedgwick and Roderick Murchison used the fossils for dating rock strata, specifically for establishing the Cambrian and Silurian periods. By 1859, leading geologists including Roderick Murchison, were convinced that what was then called the lowest Silurian stratum showed the origin of life on Earth, though others, including Charles Lyell, differed. In \"On the Origin of Species\", Charles Darwin considered this sudden appearance of a solitary group of trilobites, with no apparent antecedents, and absence of other fossils, to be \"undoubtedly of the gravest nature\" among the difficulties in his theory of natural selection. He reasoned that earlier seas had swarmed with living creatures, but that their fossils had not been found due to the imperfections of the fossil record. In the sixth edition of his book, he stressed his problem further as:\n\nAmerican paleontologist Charles Walcott, who studied the Burgess Shale fauna, proposed that an interval of time, the \"Lipalian\", was not represented in the fossil record or did not preserve fossils, and that the ancestors of the Cambrian animals evolved during this time.\n\nEarlier fossil evidence has since been found. The earliest claim is that the history of life on earth goes back : Rocks of that age at Warrawoona, Australia, were claimed to contain fossil stromatolites, stubby pillars formed by colonies of microorganisms. Fossils (\"Grypania\") of more complex eukaryotic cells, from which all animals, plants, and fungi are built, have been found in rocks from , in China and Montana. Rocks dating from contain fossils of the Ediacara biota, organisms so large that they are likely multicelled, but very unlike any modern organism. In 1948, Preston Cloud argued that a period of \"eruptive\" evolution occurred in the Early Cambrian, but as recently as the 1970s, no sign was seen of how the 'relatively' modern-looking organisms of the Middle and Late Cambrian arose.\n\nThe intense modern interest in this \"Cambrian explosion\" was sparked by the work of Harry B. Whittington and colleagues, who, in the 1970s, reanalysed many fossils from the Burgess Shale and concluded that several were as complex as, but different from, any living animals. The most common organism, \"Marrella\", was clearly an arthropod, but not a member of any known arthropod class. Organisms such as the five-eyed \"Opabinia\" and spiny slug-like \"Wiwaxia\" were so different from anything else known that Whittington's team assumed they must represent different phyla, seemingly unrelated to anything known today. Stephen Jay Gould's popular 1989 account of this work, \"Wonderful Life\", brought the matter into the public eye and raised questions about what the explosion represented. While differing significantly in details, both Whittington and Gould proposed that all modern animal phyla had appeared almost simultaneously in a rather short span of geological period. This view led to the modernization of Darwin's tree of life and the theory of punctuated equilibrium, which Eldredge and Gould developed in the early 1970s and which views evolution as long intervals of near-stasis \"punctuated\" by short periods of rapid change.\n\nOther analyses, some more recent and some dating back to the 1970s, argue that complex animals similar to modern types evolved well before the start of the Cambrian.\n\nRadiometric dates for much of the Cambrian, obtained by analysis of radioactive elements contained within rocks, have only recently become available, and for only a few regions.\n\nRelative dating (\"A\" was before \"B\") is often assumed sufficient for studying processes of evolution, but this, too, has been difficult, because of the problems involved in matching up rocks of the same age across different continents.\n\nTherefore, dates or descriptions of sequences of events should be regarded with some caution until better data become available.\n\nFossils of organisms' bodies are usually the most informative type of evidence. Fossilization is a rare event, and most fossils are destroyed by erosion or metamorphism before they can be observed. Hence, the fossil record is very incomplete, increasingly so as earlier times are considered. Despite this, they are often adequate to illustrate the broader patterns of life's history. Also, biases exist in the fossil record: different environments are more favourable to the preservation of different types of organism or parts of organisms. Further, only the parts of organisms that were already mineralised are usually preserved, such as the shells of molluscs. Since most animal species are soft-bodied, they decay before they can become fossilised. As a result, although 30-plus phyla of living animals are known, two-thirds have never been found as fossils.\nThe Cambrian fossil record includes an unusually high number of lagerstätten, which preserve soft tissues. These allow paleontologists to examine the internal anatomy of animals, which in other sediments are only represented by shells, spines, claws, etc. – if they are preserved at all. The most significant Cambrian lagerstätten are the early Cambrian Maotianshan shale beds of Chengjiang (Yunnan, China) and Sirius Passet (Greenland); the middle Cambrian Burgess Shale (British Columbia, Canada); and the late Cambrian Orsten (Sweden) fossil beds.\n\nWhile lagerstätten preserve far more than the conventional fossil record, they are far from complete. Because lagerstätten are restricted to a narrow range of environments (where soft-bodied organisms can be preserved very quickly, e.g. by mudslides), most animals are probably not represented; further, the exceptional conditions that create lagerstätten probably do not represent normal living conditions. In addition, the known Cambrian lagerstätten are rare and difficult to date, while Precambrian lagerstätten have yet to be studied in detail.\n\nThe sparseness of the fossil record means that organisms usually exist long before they are found in the fossil record – this is known as the Signor–Lipps effect.\n\nTrace fossils consist mainly of tracks and burrows, but also include coprolites (fossil feces) and marks left by feeding. Trace fossils are particularly significant because they represent a data source that is not limited to animals with easily fossilized hard parts, and reflects organisms' behaviour. Also, many traces date from significantly earlier than the body fossils of animals that are thought to have been capable of making them. While exact assignment of trace fossils to their makers is generally impossible, traces may, for example, provide the earliest physical evidence of the appearance of moderately complex animals (comparable to earthworms).\n\nSeveral chemical markers indicate a drastic change in the environment around the start of the Cambrian. The markers are consistent with a mass extinction, or with a massive warming resulting from the release of methane ice.\nSuch changes may reflect a cause of the Cambrian explosion, although they may also have resulted from an increased level of biological activity – a possible result of the explosion. Despite these uncertainties, the geochemical evidence helps by making scientists focus on theories that are consistent with at least one of the likely environmental changes.\n\nCladistics is a technique for working out the \"family tree\" of a set of organisms. It works by the logic that, if groups B and C have more similarities to each other than either has to group A, then B and C are more closely related to each other than either is to A. Characteristics that are compared may be anatomical, such as the presence of a notochord, or molecular, by comparing sequences of DNA or protein. The result of a successful analysis is a hierarchy of clades – groups whose members are believed to share a common ancestor. The cladistic technique is sometimes problematic, as some features, such as wings or camera eyes, evolved more than once, convergently – this must be taken into account in analyses.\n\nFrom the relationships, it may be possible to constrain the date that lineages first appeared. For instance, if fossils of B or C date to X million years ago and the calculated \"family tree\" says A was an ancestor of B and C, then A must have evolved more than X million years ago.\n\nIt is also possible to estimate how long ago two living clades diverged – i.e. about how long ago their last common ancestor must have lived  – by assuming that DNA mutations accumulate at a constant rate. These \"molecular clocks\", however, are fallible, and provide only a very approximate timing: they are not sufficiently precise and reliable for estimating when the groups that feature in the Cambrian explosion first evolved, and estimates produced by different techniques vary by a factor of two. However, the clocks can give an indication of branching rate, and when combined with the constraints of the fossil record, recent clocks suggest a sustained period of diversification through the Ediacaran and Cambrian.\n\nA phylum is the highest level in the Linnaean system for classifying organisms. Phyla can be thought of as groupings of animals based on general body plan. Despite the seemingly different external appearances of organisms, they are classified into phyla based on their internal and developmental organizations. For example, despite their obvious differences, spiders and barnacles both belong to the phylum Arthropoda, but earthworms and tapeworms, although similar in shape, belong to different phyla. As chemical and genetic testing becomes more accurate, previously hypothesised phyla are often entirely reworked.\n\nA phylum is not a fundamental division of nature, such as the difference between electrons and protons. It is simply a very high-level grouping in a classification system created to describe all currently living organisms. This system is imperfect, even for modern animals: different books quote different numbers of phyla, mainly because they disagree about the classification of a huge number of worm-like species. As it is based on living organisms, it accommodates extinct organisms poorly, if at all.\n\nThe concept of stem groups was introduced to cover evolutionary \"aunts\" and \"cousins\" of living groups, and have been hypothesized based on this scientific theory. A crown group is a group of closely related living animals plus their last common ancestor plus all its descendants. A stem group is a set of offshoots from the lineage at a point earlier than the last common ancestor of the crown group; it is a relative concept, for example tardigrades are living animals that form a crown group in their own right, but Budd (1996) regarded them as also being a stem group relative to the arthropods.\n\nThe term \"Triploblastic\" means consisting of three layers, which are formed in the embryo, quite early in the animal's development from a single-celled egg to a larva or juvenile form. The innermost layer forms the digestive tract (gut); the outermost forms skin; and the middle one forms muscles and all the internal organs except the digestive system. Most types of living animal are triploblastic – the best-known exceptions are Porifera (sponges) and Cnidaria (jellyfish, sea anemones, etc.).\n\nThe bilaterians are animals that have right and left sides at some point in their life histories. This implies that they have top and bottom surfaces and, importantly, distinct front and back ends. All known bilaterian animals are triploblastic, and all known triploblastic animals are bilaterian. Living echinoderms (sea stars, sea urchins, sea cucumbers, etc.) 'look' radially symmetrical (like wheels) rather than bilaterian, but their larvae exhibit bilateral symmetry and some of the earliest echinoderms may have been bilaterally symmetrical. Porifera and Cnidaria are radially symmetrical, not bilaterian, and not triploblastic.\n\nThe term \"Coelomate\" means having a body cavity (coelom) containing the internal organs. Most of the phyla featured in the debate about the Cambrian explosion are coelomates: arthropods, annelid worms, molluscs, echinoderms, and chordates – the noncoelomate priapulids are an important exception. All known coelomate animals are triploblastic bilaterians, but some triploblastic bilaterian animals do not have a coelom – for example flatworms, whose organs are surrounded by unspecialized tissues.\n\nUnderstanding of the Cambrian explosion relies upon knowing what was there beforehand – did the event herald the sudden appearance of a wide range of animals and behaviours, or did such things exist beforehand?\n\nPhylogenetic analysis has been used to support the view that during the Cambrian explosion, metazoans (multi-celled animals) evolved monophyletically from a single common ancestor: flagellated colonial protists similar to modern choanoflagellates.\n\nChanges in the abundance and diversity of some types of fossil have been interpreted as evidence for \"attacks\" by animals or other organisms. Stromatolites, stubby pillars built by colonies of microorganisms, are a major constituent of the fossil record from about , but their abundance and diversity declined steeply after about . This decline has been attributed to disruption by grazing and burrowing animals.\n\nPrecambrian marine diversity was dominated by small fossils known as acritarchs. This term describes almost any small organic walled fossil – from the egg cases of small metazoans to resting cysts of many different kinds of green algae. After appearing around , acritarchs underwent a boom around , increasing in abundance, diversity, size, complexity of shape, and especially size and number of spines. Their increasingly spiny forms in the last 1 billion years may indicate an increased need for defence against predation. Other groups of small organisms from the Neoproterozoic era also show signs of antipredator defenses. A consideration of taxon longevity appears to support an increase in predation pressure around this time.\nIn general, the fossil record shows a very slow appearance of these lifeforms in the Precambrian, with many cyanobacterial species making up much of the underlying sediment.\n\nThe layers of the Doushantuo formation from around \nharbour microscopic fossils that may represent early bilaterians. Some have been described as animal embryos and eggs, although some may represent the remains of giant bacteria.\nAnother fossil, \"Vernanimalcula\", has been interpreted as a coelomate bilaterian,\nbut may simply be an infilled bubble.\n\nThese fossils form the earliest hard-and-fast evidence of animals, as opposed to other predators.\n\nThe traces of organisms moving on and directly underneath the microbial mats that covered the Ediacaran sea floor are preserved from the Ediacaran period, about . They were probably made by organisms resembling earthworms in shape, size, and how they moved. The burrow-makers have never been found preserved, but, because they would need a head and a tail, the burrowers probably had bilateral symmetry – which would in all probability make them bilaterian animals. They fed above the sediment surface, but were forced to burrow to avoid predators.\n\nAround the start of the Cambrian (about ), many new types of traces first appear, including well-known vertical burrows such as \"Diplocraterion\" and \"Skolithos\", and traces normally attributed to arthropods, such as \"Cruziana\" and \"Rusophycus\". The vertical burrows indicate that worm-like animals acquired new behaviours, and possibly new physical capabilities. Some Cambrian trace fossils indicate that their makers possessed hard exoskeletons, although they were not necessarily mineralised.\n\nBurrows provide firm evidence of complex organisms; they are also much more readily preserved than body fossils, to the extent that the absence of trace fossils has been used to imply the genuine absence of large, motile, bottom-dwelling organisms. They provide a further line of evidence to show that the Cambrian explosion represents a real diversification, and is not a preservational artefact.\n\nThis new habit changed the seafloor's geochemistry, and led to decreased oxygen in the ocean and increased CO2-levels in the seas and the atmosphere, resulting in global warming for tens of millions years, and could be responsible for mass extinctions. But as burrowing became established, it allowed an explosion of its own, for as burrowers disturbed the sea floor, they aerated it, mixing oxygen into the toxic muds. This made the bottom sediments more hospitable, and allowed a wider range of organisms to inhabit them – creating new niches and the scope for higher diversity.\n\nAt the start of the Ediacaran period, much of the acritarch fauna, which had remained relatively unchanged for hundreds of millions of years, became extinct, to be replaced with a range of new, larger species, which would prove far more ephemeral. This radiation, the first in the fossil record, is followed soon after by an array of unfamiliar, large, fossils dubbed the Ediacara biota, which flourished for 40 million years until the start of the Cambrian. Most of this \"Ediacara biota\" were at least a few centimeters long, significantly larger than any earlier fossils. The organisms form three distinct assemblages, increasing in size and complexity as time progressed.\n\nMany of these organisms were quite unlike anything that appeared before or since, resembling discs, mud-filled bags, or quilted mattresses – one palæontologist proposed that the strangest organisms should be classified as a separate kingdom, Vendozoa.\n\nAt least some may have been early forms of the phyla at the heart of the \"Cambrian explosion\" debate, having been interpreted as early molluscs (\"Kimberella\"), echinoderms (\"Arkarua\"); and arthropods (\"Spriggina\", \"Parvancorina\"). Still, debate exists about the classification of these specimens, mainly because the diagnostic features that allow taxonomists to classify more recent organisms, such as similarities to living organisms, are generally absent in the ediacarans. However, there seems little doubt that \"Kimberella\" was at least a triploblastic bilaterian animal. These organisms are central to the debate about how abrupt the Cambrian explosion was. If some were early members of the animal phyla seen today, the \"explosion\" looks a lot less sudden than if all these organisms represent an unrelated \"experiment\", and were replaced by the animal kingdom fairly soon thereafter (40M years is \"soon\" by evolutionary and geological standards).\n\nPaul Knauth, a geologist at Arizona State University, maintains that photosynthesizing organisms such as algae may have grown over a 750- to 800-million-year-old formation in Death Valley known as the Beck Spring Dolomite. In the early 1990s, samples from this 1,000-foot thick layer of dolomite revealed that the region housed flourishing mats of photosynthesizing, unicellular life forms which antedated the Cambrian explosion.\n\nMicrofossils have been unearthed from holes riddling the otherwise barren surface of the dolomite. These geochemical and microfossil findings support the idea that during the Precambrian period, complex life evolved both in the oceans and on land. Knauth contends that animals may well have had their origins in freshwater lakes and streams, and not in the oceans.\n\nSome 30 years later, a number of studies have documented an abundance of geochemical and microfossil evidence showing that life covered the continents as far back as 2.2 billion years ago. Many paleobiologists now accept the idea that simple life forms existed on land during the Precambrian, but are opposed to the more radical idea that multicellular life thrived on land more than 600 million years ago.\n\nThe first Ediacaran and lowest Cambrian (Nemakit-Daldynian) skeletal fossils represent tubes and problematic sponge spicules. The oldest sponge spicules are monaxon siliceous, aged around , known from the Doushantou Formation in China and from deposits of the same age in Mongolia, although the interpretation of these fossils as spicules has been challenged. In the late Ediacaran-lowest Cambrian, numerous tube dwellings of enigmatic organisms appeared. It was organic-walled tubes (e.g. \"Saarina\") and chitinous tubes of the sabelliditids (e.g. \"Sokoloviina\", \"Sabellidites\", \"Paleolina\") that prospered up to the beginning of the Tommotian. The mineralized tubes of \"Cloudina\", \"Namacalathus\", \"Sinotubulites\", and a dozen more of the other organisms from carbonate rocks formed near the end of the Ediacaran period from , as well as the triradially symmetrical mineralized tubes of anabaritids (e.g. \"Anabarites\", \"Cambrotubulus\") from uppermost Ediacaran and lower Cambrian. Ediacaran mineralized tubes are often found in carbonates of the stromatolite reefs and thrombolites, i.e. they could live in an environment adverse to the majority of animals.\n\nAlthough they are as hard to classify as most other Ediacaran organisms, they are important in two other ways. First, they are the earliest known calcifying organisms (organisms that built shells from calcium carbonate). Secondly, these tubes are a device to rise over a substrate and competitors for effective feeding and, to a lesser degree, they serve as armor for protection against predators and adverse conditions of environment. Some \"Cloudina\" fossils show small holes in shells. The holes possibly are evidence of boring by predators sufficiently advanced to penetrate shells. A possible \"evolutionary arms race\" between predators and prey is one of the hypotheses that attempt to explain the Cambrian explosion.\n\nIn the lowest Cambrian, the stromatolites were decimated. This allowed animals to begin colonization of warm-water pools with carbonate sedimentation. At first, it was anabaritids and \"Protohertzina\" (the fossilized grasping spines of chaetognaths) fossils. Such mineral skeletons as shells, sclerites, thorns, and plates appeared in uppermost Nemakit-Daldynian; they were the earliest species of halkierids, gastropods, hyoliths and other rare organisms. The beginning of the Tommotian has historically been understood to mark an explosive increase of the number and variety of fossils of molluscs, hyoliths, and sponges, along with a rich complex of skeletal elements of unknown animals, the first archaeocyathids, brachiopods, tommotiids, and others. Also soft-bodied extant phyla such as comb jellies, scalidophorans, entoproctans, horseshoe worms and lobopodians had armored forms. This sudden increase is partially an artefact of missing strata at the Tommotian type section, and most of this fauna in fact began to diversify in a series of pulses through the Nemakit-Daldynian and into the Tommotian.\n\nSome animals may already have had sclerites, thorns, and plates in the Ediacaran (e.g. \"Kimberella\" had hard sclerites, probably of carbonate), but thin carbonate skeletons cannot be fossilized in siliciclastic deposits. Older (~750 Ma) fossils indicate that mineralization long preceded the Cambrian, probably defending small photosynthetic algae from single-celled eukaryotic predators.\n\nTrace fossils (burrows, etc.) are a reliable indicator of what life was around, and indicate a diversification of life around the start of the Cambrian, with the freshwater realm colonized by animals almost as quickly as the oceans.\n\nFossils known as \"small shelly fauna\" have been found in many parts on the world, and date from just before the Cambrian to about 10 million years after the start of the Cambrian (the Nemakit-Daldynian and Tommotian ages; see timeline). These are a very mixed collection of fossils: spines, sclerites (armor plates), tubes, archeocyathids (sponge-like animals), and small shells very like those of brachiopods and snail-like molluscs – but all tiny, mostly 1 to 2 mm long.\n\nWhile small, these fossils are far more common than complete fossils of the organisms that produced them; crucially, they cover the window from the start of the Cambrian to the first lagerstätten: a period of time otherwise lacking in fossils. Hence, they supplement the conventional fossil record and allow the fossil ranges of many groups to be extended.\n\nThe earliest trilobite fossils are about 530 million years old, but the class was already quite diverse and worldwide, suggesting they had been around for quite some time.\nThe fossil record of trilobites began with the appearance of trilobites with mineral exoskeletons – not from the time of their origin.\n\nThe earliest generally accepted echinoderm fossils appeared a little bit later, in the Late Atdabanian; unlike modern echinoderms, these early Cambrian echinoderms were not all radially symmetrical.\n\nThese provide firm data points for the \"end\" of the explosion, or at least indications that the crown groups of modern phyla were represented.\n\nThe Burgess Shale and similar lagerstätten preserve the soft parts of organisms, which provide a wealth of data to aid in the classification of enigmatic fossils. It often preserved complete specimens of organisms only otherwise known from dispersed parts, such as loose scales or isolated mouthparts. Further, the majority of organisms and taxa in these horizons are entirely soft-bodied, hence absent from the rest of the fossil record. Since a large part of the ecosystem is preserved, the ecology of the community can also be tentatively reconstructed.\nHowever, the assemblages may represent a \"museum\": a deep-water ecosystem that is evolutionarily \"behind\" the rapidly diversifying fauna of shallower waters.\n\nBecause the lagerstätten provide a mode and quality of preservation that is virtually absent outside of the Cambrian, many organisms appear completely different from anything known from the conventional fossil record. This led early workers in the field to attempt to shoehorn the organisms into extant phyla; the shortcomings of this approach led later workers to erect a multitude of new phyla to accommodate all the oddballs. It has since been realised that most oddballs diverged from lineages before they established the phyla known today – slightly different designs, which were fated to perish rather than flourish into phyla, as their cousin lineages did.\n\nThe preservational mode is rare in the preceding Ediacaran period, but those assemblages known show no trace of animal life – perhaps implying a genuine absence of macroscopic metazoans.\n\nCrustaceans, one of the four great modern groups of arthropods, are very rare throughout the Cambrian. Convincing crustaceans were once thought to be common in Burgess Shale-type biotas, but none of these individuals can be shown to fall into the crown group of \"true crustaceans\". The Cambrian record of crown-group crustaceans comes from microfossils. The Swedish Orsten horizons contain later Cambrian crustaceans, but only organisms smaller than 2 mm are preserved. This restricts the data set to juveniles and miniaturised adults.\n\nA more informative data source is the organic microfossils of the Mount Cap formation, Mackenzie Mountains, Canada. This late Early Cambrian assemblage () consists of microscopic fragments of arthropods' cuticle, which is left behind when the rock is dissolved with hydrofluoric acid. The diversity of this assemblage is similar to that of modern crustacean faunas. Analysis of fragments of feeding machinery found in the formation shows that it was adapted to feed in a very precise and refined fashion. This contrasts with most other early Cambrian arthropods, which fed messily by shovelling anything they could get their feeding appendages on into their mouths. This sophisticated and specialised feeding machinery belonged to a large (about 30 cm) organism, and would have provided great potential for diversification; specialised feeding apparatus allows a number of different approaches to feeding and development, and creates a number of different approaches to avoid being eaten.\n\nAfter an extinction at the Cambrian–Ordovician boundary, another radiation occurred, which established the taxa that would dominate the Palaeozoic.\n\nDuring this radiation, the total number of orders doubled, and families tripled, increasing marine diversity to levels typical of the Palaeozoic, and disparity to levels approximately equivalent to today's.\n\nThe event lasted for about the next 20–25 million years. Different authors break the explosion down into stages in different ways.\n\nEd Landing recognizes three stages: Stage 1, spanning the Ediacaran-Cambrian boundary, corresponds to a diversification of biomineralizing animals and of deep and complex burrows; Stage 2, corresponding to the radiation of molluscs and stem-group Brachiopods (hyoliths and tommotiids), which apparently arose in intertidal waters; and Stage 3, seeing the Atdabanian diversification of trilobites in deeper waters, but little change in the intertidal realm.\n\nGraham Budd synthesises various schemes to produce a compatible view of the SSF record of the Cambrian explosion, divided slightly differently into four intervals: a \"Tube world\", lasting from , spanning the Ediacaran-Cambrian boundary, dominated by Cloudina, Namacalathus and pseudoconodont-type elements; a \"Sclerite world\", seeing the rise of halkieriids, tommotiids, and hyoliths, lasting to the end of the Fortunian (c. 525 Ma); a brachiopod world, perhaps corresponding to the as yet unratified Cambrian Stage 2; and Trilobite World, kicking off in Stage 3.\n\nComplementary to the shelly fossil record, trace fossils can be divided into five subdivisions: \"Flat world\" (late Ediacaran), with traces restricted to the sediment surface; Protreozoic III (after Jensen), with increasing complexity; \"pedum\" world, initiated at the base of the Cambrian with the base of the \"T.pedum\" zone (see discussion at Cambrian#Dating the Cambrian); \"Rusophycus\" world, spanning and thus corresponding exactly to the periods of Sclerite World and Brachiopod World under the SSF paradigm; and \"Cruziana\" world, with an obvious correspondence to Trilobite World.\n\nThere is strong evidence for species of Cnidaria and Porifera existing in the Ediacaran and possible members of Porifera even before that during the Cryogenian. Bryozoans don't appear in the fossil record until after the Cambrian, in the Lower Ordovician.\n\nThe fossil record as Darwin knew it seemed to suggest that the major metazoan groups appeared in a few million years of the early to mid-Cambrian, and even in the 1980s, this still appeared to be the case.\n\nHowever, evidence of Precambrian Metazoa is gradually accumulating. If the Ediacaran \"Kimberella\" was a mollusc-like protostome (one of the two main groups of coelomates), the protostome and deuterostome lineages must have split significantly before (deuterostomes are the other main group of coelomates). Even if it is not a protostome, it is widely accepted as a bilaterian. Since fossils of rather modern-looking cnidarians (jellyfish-like organisms) have been found in the Doushantuo lagerstätte, the cnidarian and bilaterian lineages must have diverged well over .\n\nTrace fossils and predatory borings in \"Cloudina\" shells provide further evidence of Ediacaran animals. Some fossils from the Doushantuo formation have been interpreted as embryos and one (\"Vernanimalcula\") as a bilaterian coelomate, although these interpretations are not universally accepted. Earlier still, predatory pressure has acted on stromatolites and acritarchs since around .\n\nSome say that the evolutionary change was accelerated by an order of magnitude, but the presence of Precambrian animals somewhat dampens the \"bang\" of the explosion; not only was the appearance of animals gradual, but their evolutionary radiation (\"diversification\") may also not have been as rapid as once thought. Indeed, statistical analysis shows that the Cambrian explosion was no faster than any of the other radiations in animals' history. However, it does seem that some innovations linked to the explosion – such as resistant armour – only evolved once in the animal lineage; this makes a lengthy Precambrian animal lineage harder to defend. Further, the conventional view that all the phyla arose in the Cambrian is flawed; while the phyla may have diversified in this time period, representatives of the crown groups of many phyla do not appear until much later in the Phanerozoic. Further, the mineralised phyla that form the basis of the fossil record may not be representative of other phyla, since most mineralised phyla originated in a benthic setting. The fossil record is consistent with a Cambrian explosion that was limited to the benthos, with pelagic phyla evolving much later.\n\nEcological complexity among marine animals increased in the Cambrian, as well later in the Ordovician. However, recent research has overthrown the once-popular idea that disparity was exceptionally high throughout the Cambrian, before subsequently decreasing. In fact, disparity remains relatively low throughout the Cambrian, with modern levels of disparity only attained after the early Ordovician radiation.\n\nThe diversity of many Cambrian assemblages is similar to today's, and at a high (class/phylum) level, diversity is thought by some to have risen relatively smoothly through the Cambrian, stabilizing somewhat in the Ordovician. This interpretation, however, glosses over the astonishing and fundamental pattern of basal polytomy and phylogenetic telescoping at or near the Cambrian boundary, as seen in most major animal lineages. Thus Harry Blackmore Whittington's questions regarding the abrupt nature of the Cambrian explosion remain, and have yet to be satisfactorily answered.\n\nBudd and Mann suggested that the Cambrian explosion was the result of a type of survivorship bias called the \"Push of the past\". As groups at their origin tend to go extinct, it follows that any long-lived group would have experienced an unusually rapid rate of diversification early on, creating the illusion of a general speed-up in diversification rates. However, rates of diversification could remain at background levels and still generate this sort of effect in the surviving lineages.\n\nDespite the evidence that moderately complex animals (triploblastic bilaterians) existed before and possibly long before the start of the Cambrian, it seems that the pace of evolution was exceptionally fast in the early Cambrian. Possible explanations for this fall into three broad categories: environmental, developmental, and ecological changes. Any explanation must explain both the timing and magnitude of the explosion.\n\nEarth's earliest atmosphere contained no free oxygen (O); the oxygen that animals breathe today, both in the air and dissolved in water, is the product of billions of years of photosynthesis. Cyanobacteria were the first organisms to evolve the ability to photosynthesize, introducing a steady supply of oxygen into the environment. Initially, oxygen levels did not increase substantially in the atmosphere. The oxygen quickly reacted with iron and other minerals in the surrounding rock and ocean water. Once a saturation point was reached for the reactions in rock and water, oxygen was able to exist as a gas in its diatomic form. Oxygen levels in the atmosphere increased substantially afterward. As a general trend, the concentration of oxygen in the atmosphere has risen gradually over about the last 2.5 billion years.\n\nOxygen levels seem to have a positive correlation with diversity in eukaryotes well before the Cambrian period. The last common ancestor of all extant eukaryotes is thought to have lived around 1.8 billion years ago. Around 800 million years ago, there was a notable increase in the complexity and number of eukaryotes species in the fossil record. Before the spike in diversity, eukaryotes are thought to have lived in highly sulfuric environments. Sulfide interferes with mitochondrial function in aerobic organisms, limiting the amount of oxygen that could be used to drive metabolism. Oceanic sulfide levels decreased around 800 million years ago, which supports the importance of oxygen in eukaryotic diversity.\n\nThe shortage of oxygen might well have prevented the rise of large, complex animals. The amount of oxygen an animal can absorb is largely determined by the area of its oxygen-absorbing surfaces (lungs and gills in the most complex animals; the skin in less complex ones); but, the amount needed is determined by its volume, which grows faster than the oxygen-absorbing area if an animal's size increases equally in all directions. An increase in the concentration of oxygen in air or water would increase the size to which an organism could grow without its tissues becoming starved of oxygen. However, members of the Ediacara biota reached metres in length tens of millions of years before the Cambrian explosion. Other metabolic functions may have been inhibited by lack of oxygen, for example the construction of tissue such as collagen, required for the construction of complex structures, or to form molecules for the construction of a hard exoskeleton. However, animals are not affected when similar oceanographic conditions occur in the Phanerozoic; there is no convincing correlation between oxygen levels and evolution, so oxygen may have been no more a prerequisite to complex life than liquid water or primary productivity.\n\nThe amount of ozone (O) required to shield Earth from biologically lethal UV radiation, wavelengths from 200 to 300 nanometers (nm), is believed to have been in existence around the Cambrian explosion. The presence of the ozone layer may have enabled the development of complex life and life on land, as opposed to life being restricted to the water.\n\nIn the late Neoproterozoic (extending into the early Ediacaran period), the Earth suffered massive glaciations in which most of its surface was covered by ice. This may have caused a mass extinction, creating a genetic bottleneck; the resulting diversification may have given rise to the Ediacara biota, which appears soon after the last \"Snowball Earth\" episode.\nHowever, the snowball episodes occurred a long time before the start of the Cambrian, and it is hard to see how so much diversity could have been caused by even a series of bottlenecks; the cold periods may even have \"delayed\" the evolution of large size organisms.\n\nNewer research suggests that volcanically active midocean ridges caused a massive and sudden surge of the calcium concentration in the oceans, making it possible for marine organisms to build skeletons and hard body parts.\nAlternatively a high influx of ions could have been provided by the widespread erosion that produced Powell's Great Unconformity.\n\nAn increase of calcium may also have been caused by erosion of the Transgondwanan Supermountain that existed at the time the explosion. The roots of the mountain are preserved in present-day East Africa as an orogen.\n\nA range of theories are based on the concept that minor modifications to animals' development as they grow from embryo to adult may have been able to cause very large changes in the final adult form. The Hox genes, for example, control which organs individual regions of an embryo will develop into. For instance, if a certain \"Hox\" gene is expressed, a region will develop into a limb; if a different Hox gene is expressed in that region (a minor change), it could develop into an eye instead (a phenotypically major change).\n\nSuch a system allows a large range of disparity to appear from a limited set of genes, but such theories linking this with the explosion struggle to explain why the origin of such a development system should by itself lead to increased diversity or disparity. Evidence of Precambrian metazoans combines with molecular data to show that much of the genetic architecture that could feasibly have played a role in the explosion was already well established by the Cambrian.\n\nThis apparent paradox is addressed in a theory that focuses on the physics of development. It is proposed that the emergence of simple multicellular forms provided a changed context and spatial scale in which novel physical processes and effects were mobilized by the products of genes that had previously evolved to serve unicellular functions. Morphological complexity (layers, segments, lumens, appendages) arose, in this view, by self-organization.\n\nHorizontal gene transfer has also been identified as a possible factor in the rapid acquisition of the biochemical capability of biomineralization among organisms during this period, based on evidence that the gene for a critical protein in the process was originally transferred from a bacterium into sponges.\n\nThese focus on the interactions between different types of organism. Some of these hypotheses deal with changes in the food chain; some suggest arms races between predators and prey, and others focus on the more general mechanisms of coevolution. Such theories are well suited to explaining why there was a rapid increase in both disparity and diversity, but they must explain why the \"explosion\" happened when it did.\n\nEvidence for such an extinction includes the disappearance from the fossil record of the Ediacara biota and shelly fossils such as \"Cloudina\", and the accompanying perturbation in the record.\n\nMass extinctions are often followed by adaptive radiations as existing clades expand to occupy the ecospace emptied by the extinction. However, once the dust had settled, overall disparity and diversity returned to the pre-extinction level in each of the Phanerozoic extinctions.\n\nAndrew Parker has proposed that predator-prey relationships changed dramatically after eyesight evolved. Prior to that time, hunting and evading were both close-range affairs – smell, vibration, and touch were the only senses used. When predators could see their prey from a distance, new defensive strategies were needed. Armor, spines, and similar defenses may also have evolved in response to vision. He further observed that, where animals lose vision in unlighted environments such as caves, diversity of animal forms tends to decrease. Nevertheless, many scientists doubt that vision could have caused the explosion. Eyes may well have evolved long before the start of the Cambrian. It is also difficult to understand why the evolution of eyesight would have caused an explosion, since other senses, such as smell and pressure detection, can detect things at a greater distance in the sea than sight can; but the appearance of these other senses apparently did not cause an evolutionary explosion.\n\nThe ability to avoid or recover from predation often makes the difference between life and death, and is therefore one of the strongest components of natural selection. The pressure to adapt is stronger on the prey than on the predator: if the predator fails to win a contest, it loses a meal; if the prey is the loser, it loses its life.\n\nBut, there is evidence that predation was rife long before the start of the Cambrian, for example in the increasingly spiny forms of acritarchs, the holes drilled in \"Cloudina\" shells, and traces of burrowing to avoid predators. Hence, it is unlikely that the \"appearance\" of predation was the trigger for the Cambrian \"explosion\", although it may well have exhibited a strong influence on the body forms that the \"explosion\" produced. However, the intensity of predation does appear to have increased dramatically during the Cambrian as new predatory \"tactics\" (such as shell-crushing) emerged. This rise of predation during the Cambrian was confirmed by the temporal pattern of the median predator ratio at the scale of genus, in fossil communities covering the Cambrian and Ordovician periods, but this pattern is not correlated to diversification rate. This lack of correlation between predator ratio and diversification over the Cambrian and Ordovician suggests that predators did not trigger the large evolutionary radiation of animals during this interval. Thus the role of predators as triggerers of diversification may have been limited to the very beginning of the \"Cambrian explosion\".\n\nGeochemical evidence strongly indicates that the total mass of plankton has been similar to modern levels since early in the Proterozoic. Before the start of the Cambrian, their corpses and droppings were too small to fall quickly towards the seabed, since their drag was about the same as their weight. This meant they were destroyed by scavengers or by chemical processes before they reached the sea floor.\n\nMesozooplankton are plankton of a larger size. Early Cambrian specimens filtered microscopic plankton from the seawater. These larger organisms would have produced droppings and corpses that were large enough to fall fairly quickly. This provided a new supply of energy and nutrients to the mid-levels and bottoms of the seas, which opened up a huge range of new possible ways of life. If any of these remains sank uneaten to the sea floor they could be buried; this would have taken some carbon out of circulation, resulting in an increase in the concentration of breathable oxygen in the seas (carbon readily combines with oxygen).\n\nThe initial herbivorous mesozooplankton were probably larvae of benthic (seafloor) animals. A larval stage was probably an evolutionary innovation driven by the increasing level of predation at the seafloor during the Ediacaran period.\n\nMetazoans have an amazing ability to increase diversity through coevolution. This means that an organism's traits can lead to traits evolving in other organisms; a number of responses are possible, and a different species can potentially emerge from each one. As a simple example, the evolution of predation may have caused one organism to develop a defence, while another developed motion to flee. This would cause the predator lineage to split into two species: one that was good at chasing prey, and another that was good at breaking through defences. Actual coevolution is somewhat more subtle, but, in this fashion, great diversity can arise: three quarters of living species are animals, and most of the rest have formed by coevolution with animals.\n\nEvolving organisms inevitably change the environment they evolve in. The Devonian colonization of land had planet-wide consequences for sediment cycling and ocean nutrients, and was likely linked to the Devonian mass extinction. A similar process may have occurred on smaller scales in the oceans, with, for example, the sponges filtering particles from the water and depositing them in the mud in a more digestible form; or burrowing organisms making previously unavailable resources available for other organisms.\n\nThe explosion may not have been a significant evolutionary event. It may represent a threshold being crossed: for example a threshold in genetic complexity that allowed a vast range of morphological forms to be employed. This genetic threshold may have a correlation to the amount of oxygen available to organisms. Using oxygen for metabolism produces much more energy than anaerobic processes. Organisms that use more oxygen have the opportunity to produce more complex proteins, providing a template for further evolution. These proteins translate into larger, more complex structures that allow organisms better to adapt to their environments. With the help of oxygen, genes that code for these proteins could contribute to the expression of complex traits more efficiently. Access to a wider range of structures and functions would allow organisms to evolve in different directions, increasing the number of niches that could be inhabited. Furthermore, organisms had the opportunity to become more specialized in their own niches.\n\nThe \"Cambrian explosion\" can be viewed as two waves of metazoan expansion into empty niches: first, a coevolutionary rise in diversity as animals explored niches on the Ediacaran sea floor, followed by a second expansion in the early Cambrian as they became established in the water column. The rate of diversification seen in the Cambrian phase of the explosion is unparalleled among marine animals: it affected all metazoan clades of which Cambrian fossils have been found. Later radiations, such as those of fish in the Silurian and Devonian periods, involved fewer taxa, mainly with very similar body plans. Although the recovery from the Permian-Triassic extinction started with about as few animal species as the Cambrian explosion, the recovery produced far fewer significantly new types of animals.\n\nWhatever triggered the early Cambrian diversification opened up an exceptionally wide range of previously unavailable ecological niches. When these were all occupied, limited space existed for such wide-ranging diversifications to occur again, because strong competition existed in all niches and incumbents usually had the advantage. If a wide range of empty niches had continued, clades would be able to continue diversifying and become disparate enough for us to recognise them as different phyla; when niches are filled, lineages will continue to resemble one another long after they diverge, as limited opportunity exists for them to change their life-styles and forms.\n\nThere were two similar explosions in the evolution of land plants: after a cryptic history beginning about , land plants underwent a uniquely rapid adaptive radiation during the Devonian period, about . Furthermore, Angiosperms (flowering plants) originated and rapidly diversified during the Cretaceous period.\n\n\nTimeline References:\n\n"}
{"id": "7555", "url": "https://en.wikipedia.org/wiki?curid=7555", "title": "Casimir effect", "text": "Casimir effect\n\nIn quantum field theory, the Casimir effect and the Casimir–Polder force are physical forces arising from a quantized field. They are named after the Dutch physicist Hendrik Casimir who predicted them in 1948.\n\nThe Casimir effect can be understood by the idea that the presence of conducting metals and dielectrics alters the vacuum expectation value of the energy of the second quantized electromagnetic field. Since the value of this energy depends on the shapes and positions of the conductors and dielectrics, the Casimir effect manifests itself as a force between such objects.\n\nAny medium supporting oscillations has an analogue of the Casimir effect. For example, beads on a string as well as plates submerged in noisy water or gas illustrate the Casimir force.\n\nIn modern theoretical physics, the Casimir effect plays an important role in the chiral bag model of the nucleon; in applied physics, it is significant in some aspects of emerging microtechnologies and nanotechnologies.\n\nThe typical example is of the two uncharged conductive plates in a vacuum, placed a few nanometers apart. In a classical description, the lack of an external field means that there is no field between the plates, and no force would be measured between them. When this field is instead studied using the quantum electrodynamic vacuum, it is seen that the plates do affect the virtual photons which constitute the field, and generate a net force – either an attraction or a repulsion depending on the specific arrangement of the two plates. Although the Casimir effect can be expressed in terms of virtual particles interacting with the objects, it is best described and more easily calculated in terms of the zero-point energy of a quantized field in the intervening space between the objects. This force has been measured and is a striking example of an effect captured formally by second quantization.\n\nThe treatment of boundary conditions in these calculations has led to some controversy. In fact, \"Casimir's original goal was to compute the van der Waals force between polarizable molecules\" of the conductive plates. Thus it can be interpreted without any reference to the zero-point energy (vacuum energy) of quantum fields.\n\nBecause the strength of the force falls off rapidly with distance, it is measurable only when the distance between the objects is extremely small. On a submicron scale, this force becomes so strong that it becomes the dominant force between uncharged conductors. In fact, at separations of 10 nm – about 100 times the typical size of an atom – the Casimir effect produces the equivalent of about 1 atmosphere of pressure (the precise value depending on surface geometry and other factors).\n\nDutch physicists Hendrik Casimir and Dirk Polder at Philips Research Labs proposed the existence of a force between two polarizable atoms and between such an atom and a conducting plate in 1947; this special form is called the Casimir–Polder force. After a conversation with Niels Bohr, who suggested it had something to do with zero-point energy, Casimir alone formulated the theory predicting a force between neutral conducting plates in 1948 which is called the Casimir effect in the narrow sense.\n\nPredictions of the force were later extended to finite-conductivity metals and dielectrics, and recent calculations have considered more general geometries. Experiments before 1997 had observed the force qualitatively, and indirect validation of the predicted Casimir energy had been made by measuring the thickness of liquid helium films. However it was not until 1997 that a direct experiment by S. Lamoreaux quantitatively measured the force to within 5% of the value predicted by the theory. Subsequent experiments approach an accuracy of a few percent.\n\nThe causes of the Casimir effect are described by quantum field theory, which states that all of the various fundamental fields, such as the electromagnetic field, must be quantized at each and every point in space. In a simplified view, a \"field\" in physics may be envisioned as if space were filled with interconnected vibrating balls and springs, and the strength of the field can be visualized as the displacement of a ball from its rest position. Vibrations in this field propagate and are governed by the appropriate wave equation for the particular field in question. The second quantization of quantum field theory requires that each such ball-spring combination be quantized, that is, that the strength of the field be quantized at each point in space. At the most basic level, the field at each point in space is a simple harmonic oscillator, and its quantization places a quantum harmonic oscillator at each point. Excitations of the field correspond to the elementary particles of particle physics. However, even the vacuum has a vastly complex structure, so all calculations of quantum field theory must be made in relation to this model of the vacuum.\n\nThe vacuum has, implicitly, all of the properties that a particle may have: spin, or polarization in the case of light, energy, and so on. On average, most of these properties cancel out: the vacuum is, after all, \"empty\" in this sense. One important exception is the vacuum energy or the vacuum expectation value of the energy. The quantization of a simple harmonic oscillator states that the lowest possible energy or zero-point energy that such an oscillator may have is\n\nSumming over all possible oscillators at all points in space gives an infinite quantity. Since only \"differences\" in energy are physically measurable (with the notable exception of gravitation, which remains beyond the scope of quantum field theory), this infinity may be considered a feature of the mathematics rather than of the physics. This argument is the underpinning of the theory of renormalization. Dealing with infinite quantities in this way was a cause of widespread unease among quantum field theorists before the development in the 1970s of the renormalization group, a mathematical formalism for scale transformations that provides a natural basis for the process.\n\nWhen the scope of the physics is widened to include gravity, the interpretation of this formally infinite quantity remains problematic. There is currently no compelling explanation as to why it should not result in a cosmological constant that is many orders of magnitude larger than observed. However, since we do not yet have any fully coherent quantum theory of gravity, there is likewise no compelling reason as to why it should.\n\nThe Casimir effect for fermions can be understood as the spectral asymmetry of the fermion operator formula_2, where it is known as the Witten index.\n\nAlternatively, a 2005 paper by Robert Jaffe of MIT states that \"Casimir effects\ncan be formulated and Casimir forces can be computed without reference to zero-point energies. They are relativistic, quantum forces between charges and currents. The Casimir force (per unit\narea) between parallel plates vanishes as alpha, the fine structure constant, goes to zero, and the standard result, which appears to be independent of alpha, corresponds to the alpha approaching infinity limit,\" and that \"The Casimir force is simply the (relativistic, retarded) van der Waals force between the metal plates.\" Casimir and Polder's original paper used this method to derive the Casimir-Polder force. In 1978, Schwinger, DeRadd, and Milton published a similar derivation for the Casimir Effect between two parallel plates. In fact, the description in terms of van der Waals forces is the only correct description from the fundamental microscopic perspective, while other descriptions of Casimir force are merely effective macroscopic descriptions.\n\nCasimir's observation was that the second-quantized quantum electromagnetic field, in the presence of bulk bodies such as metals or dielectrics, must obey the same boundary conditions that the classical electromagnetic field must obey. In particular, this affects the calculation of the vacuum energy in the presence of a conductor or dielectric.\n\nConsider, for example, the calculation of the vacuum expectation value of the electromagnetic field inside a metal cavity, such as, for example, a radar cavity or a microwave waveguide. In this case, the correct way to find the zero-point energy of the field is to sum the energies of the standing waves of the cavity. To each and every possible standing wave corresponds an energy; say the energy of the \"n\"th standing wave is formula_3. The vacuum expectation value of the energy of the electromagnetic field in the cavity is then\n\nwith the sum running over all possible values of \"n\" enumerating the standing waves. The factor of 1/2 is present because the zero-point energy of the n'th mode is formula_5, where formula_3 is the energy increment for the n'th mode. (It is the same 1/2 as appears in the equation formula_7.) Written in this way, this sum is clearly divergent; however, it can be used to create finite expressions.\n\nIn particular, one may ask how the zero-point energy depends on the shape \"s\" of the cavity. Each energy level formula_3 depends on the shape, and so one should write formula_9 for the energy level, and formula_10 for the vacuum expectation value. At this point comes an important observation: the force at point \"p\" on the wall of the cavity is equal to the change in the vacuum energy if the shape \"s\" of the wall is perturbed a little bit, say by formula_11, at point \"p\". That is, one has\n\nThis value is finite in many practical calculations.\n\nAttraction between the plates can be easily understood by focusing on the one-dimensional situation. Suppose that a moveable conductive plate is positioned at a short distance \"a\" from one of two widely separated plates (distance \"L\" apart). With \"a\" « \"L\", the states within the slot of width \"a\" are highly constrained so that the energy \"E\" of any one mode is widely separated from that of the next. This is not the case in the large region \"L\", where there is a large number (numbering about \"L\" / \"a\") of states with energy evenly spaced between \"E\" and the next mode in the narrow slot – in other words, all slightly larger than \"E\". Now on shortening \"a\" by d\"a\" (< 0), the mode in the narrow slot shrinks in wavelength and therefore increases in energy proportional to −d\"a\"/\"a\", whereas all the \"L\" /\"a\" states that lie in the large region lengthen and correspondingly decrease their energy by an amount proportional to d\"a\"/\"L\" (note the denominator). The two effects nearly cancel, but the net change is slightly negative, because the energy of all the \"L\"/\"a\" modes in the large region are slightly larger than the single mode in the slot. Thus the force is attractive: it tends to make \"a\" slightly smaller, the plates attracting each other across the thin slot.\n\n\nIn the original calculation done by Casimir, he considered the space between a pair of conducting metal plates at distance formula_13 apart. In this case, the standing waves are particularly easy to calculate, because the transverse component of the electric field and the normal component of the magnetic field must vanish on the surface of a conductor. Assuming the plates lie parallel to the \"xy\"-plane, the standing waves are\n\nwhere formula_15 stands for the electric component of the electromagnetic field, and, for brevity, the polarization and the magnetic components are ignored here. Here, formula_16 and formula_17 are the wave numbers in directions parallel to the plates, and\n\nis the wave-number perpendicular to the plates. Here, \"n\" is an integer, resulting from the requirement that ψ vanish on the metal plates. The frequency of this wave is\n\nwhere \"c\" is the speed of light. The vacuum energy is then the sum over all possible excitation modes. Since the area of the plates is large, we may sum by integrating over two of the dimensions in \"k\"-space. The assumption of periodic boundary conditions yields,\n\nwhere \"A\" is the area of the metal plates, and a factor of 2 is introduced for the two possible polarizations of the wave. This expression is clearly infinite, and to proceed with the calculation, it is convenient to introduce a regulator (discussed in greater detail below). The regulator will serve to make the expression finite, and in the end will be removed. The zeta-regulated version of the energy per unit-area of the plate is\n\nIn the end, the limit formula_22 is to be taken. Here \"s\" is just a complex number, not to be confused with the shape discussed previously. This integral/sum is finite for \"s\" real and larger than 3. The sum has a pole at \"s\"=3, but may be analytically continued to \"s\"=0, where the expression is finite. The above expression simplifies to:\n\nwhere polar coordinates formula_24 were introduced to turn the double integral into a single integral. The formula_25 in front is the Jacobian, and the formula_26 comes from the angular integration. The integral converges if Re[\"s\"] > 3, resulting in\n\nThe sum diverges at \"s\" in the neighborhood of zero, but if the damping of large-frequency excitations corresponding to analytic continuation of the Riemann zeta function to \"s\"=0 is assumed to make sense physically in some way, then one has\n\nBut\n\nand so one obtains\n\nThe analytic continuation has evidently lost an additive positive infinity, somehow exactly accounting for the zero-point energy (not included above) outside the slot between the plates, but which changes upon plate movement within a closed system. The Casimir force per unit area formula_31 for idealized, perfectly conducting plates with vacuum between them is\n\nwhere\n\nThe force is negative, indicating that the force is attractive: by moving the two plates closer together, the energy is lowered. The presence of formula_33 shows that the Casimir force per unit area formula_31 is very small, and that furthermore, the force is inherently of quantum-mechanical origin.\n\nNOTE: In Casimir's original derivation , a moveable conductive plate is positioned at a short distance \"a\" from one of two widely separated plates (distance \"L\" apart). The 0-point energy on \"both\" sides of the plate is considered. Instead of the above \"ad hoc\" analytic continuation assumption, non-convergent sums and integrals are computed using Euler–Maclaurin summation with a regularizing function (e.g., exponential regularization) not so anomalous as formula_38 in the above.\n\nCasimir's analysis of idealized metal plates was generalized to arbitrary dielectric and realistic metal plates by Lifshitz and his students. Using this approach, complications of the bounding surfaces, such as the modifications to the Casimir force due to finite conductivity, can be calculated numerically using the tabulated complex dielectric functions of the bounding materials. Lifshitz's theory for two metal plates reduces to Casimir's idealized 1/\"a\" force law for large separations \"a\" much greater than the skin depth of the metal, and conversely reduces to the 1/\"a\" force law of the London dispersion force (with a coefficient called a Hamaker constant) for small \"a\", with a more complicated dependence on \"a\" for intermediate separations determined by the dispersion of the materials.\n\nLifshitz' result was subsequently generalized to arbitrary multilayer planar geometries as well as to anisotropic and magnetic materials, but for several decades the calculation of Casimir forces for non-planar geometries remained limited to a few idealized cases admitting analytical solutions. For example, the force in the experimental sphere–plate geometry was computed with an approximation (due to Derjaguin) that the sphere radius \"R\" is much larger than the separation \"a\", in which case the nearby surfaces are nearly parallel and the parallel-plate result can be adapted to obtain an approximate \"R\"/\"a\" force (neglecting both skin-depth and higher-order curvature effects). However, in the 2000s a number of authors developed and demonstrated a variety of numerical techniques, in many cases adapted from classical computational electromagnetics, that are capable of accurately calculating Casimir forces for arbitrary geometries and materials, from simple finite-size effects of finite plates to more complicated phenomena arising for patterned surfaces or objects of various shapes.\n\nOne of the first experimental tests was conducted by Marcus Sparnaay at Philips in Eindhoven (Netherlands), in 1958, in a delicate and difficult experiment with parallel plates, obtaining results not in contradiction with the Casimir theory, but with large experimental errors. Some of the experimental details as well as some background information on how Casimir, Polder and Sparnaay arrived at this point are highlighted in a 2007 interview with Marcus Sparnaay.\n\nThe Casimir effect was measured more accurately in 1997 by Steve K. Lamoreaux of Los Alamos National Laboratory, and by Umar Mohideen and Anushree Roy of the University of California, Riverside. In practice, rather than using two parallel plates, which would require phenomenally accurate alignment to ensure they were parallel, the experiments use one plate that is flat and another plate that is a part of a sphere with a large radius.\n\nIn 2001, a group (Giacomo Bressi, Gianni Carugno, Roberto Onofrio and Giuseppe Ruoso) at the University of Padua (Italy) finally succeeded in measuring the Casimir force between parallel plates using microresonators.\n\nIn order to be able to perform calculations in the general case, it is convenient to introduce a regulator in the summations. This is an artificial device, used to make the sums finite so that they can be more easily manipulated, followed by the taking of a limit so as to remove the regulator.\n\nThe heat kernel or exponentially regulated sum is\n\nwhere the limit formula_40 is taken in the end. The divergence of the sum is typically manifested as\n\nfor three-dimensional cavities. The infinite part of the sum is associated with the bulk constant \"C\" which \"does not\" depend on the shape of the cavity. The interesting part of the sum is the finite part, which is shape-dependent. The Gaussian regulator\n\nis better suited to numerical calculations because of its superior convergence properties, but is more difficult to use in theoretical calculations. Other, suitably smooth, regulators may be used as well. The zeta function regulator\n\nis completely unsuited for numerical calculations, but is quite useful in theoretical calculations. In particular, divergences show up as poles in the complex \"s\" plane, with the bulk divergence at \"s\"=4. This sum may be analytically continued past this pole, to obtain a finite part at \"s\"=0.\n\nNot every cavity configuration necessarily leads to a finite part (the lack of a pole at \"s\"=0) or shape-independent infinite parts. In this case, it should be understood that additional physics has to be taken into account. In particular, at extremely large frequencies (above the plasma frequency), metals become transparent to photons (such as X-rays), and dielectrics show a frequency-dependent cutoff as well. This frequency dependence acts as a natural regulator. There are a variety of bulk effects in solid state physics, mathematically very similar to the Casimir effect, where the cutoff frequency comes into explicit play to keep expressions finite. (These are discussed in greater detail in \"Landau and Lifshitz\", \"Theory of Continuous Media\".)\n\nThe Casimir effect can also be computed using the mathematical mechanisms of functional integrals of quantum field theory, although such calculations are considerably more abstract, and thus difficult to comprehend. In addition, they can be carried out only for the simplest of geometries. However, the formalism of quantum field theory makes it clear that the vacuum expectation value summations are in a certain sense summations over so-called \"virtual particles\".\n\nMore interesting is the understanding that the sums over the energies of standing waves should be formally understood as sums over the eigenvalues of a Hamiltonian. This allows atomic and molecular effects, such as the van der Waals force, to be understood as a variation on the theme of the Casimir effect. Thus one considers the Hamiltonian of a system as a function of the arrangement of objects, such as atoms, in configuration space. The change in the zero-point energy as a function of changes of the configuration can be understood to result in forces acting between the objects.\n\nIn the chiral bag model of the nucleon, the Casimir energy plays an important role in showing the mass of the nucleon is independent of the bag radius. In addition, the spectral asymmetry is interpreted as a non-zero vacuum expectation value of the baryon number, cancelling the topological winding number of the pion field surrounding the nucleon.\n\nThe dynamical Casimir effect is the production of particles and energy from an accelerated \"moving mirror\". This reaction was predicted by certain numerical solutions to quantum mechanics equations made in the 1970s. In May 2011 an announcement was made by researchers at the Chalmers University of Technology, in Gothenburg, Sweden, of the detection of the dynamical Casimir effect. In their experiment, microwave photons were generated out of the vacuum in a superconducting microwave resonator. These researchers used a modified SQUID to change the effective length of the resonator in time, mimicking a mirror moving at the required relativistic velocity. If confirmed this would be the first experimental verification of the dynamical Casimir effect.\nA similar analysis can be used to explain Hawking radiation that causes the slow \"evaporation\" of black holes (although this is generally visualized as the escape of one particle from a virtual particle-antiparticle pair, the other particle having been captured by the black hole).\n\nConstructed within the framework of quantum field theory in curved spacetime, the dynamical Casimir effect has been used to better understand acceleration radiation such as the Unruh effect.\n\nThere are few instances wherein the Casimir effect can give rise to repulsive forces between uncharged objects. Evgeny Lifshitz showed (theoretically) that in certain circumstances (most commonly involving liquids), repulsive forces can arise. This has sparked interest in applications of the Casimir effect toward the development of levitating devices. An experimental demonstration of the Casimir-based repulsion predicted by Lifshitz was carried out by Munday et al. Other scientists have also suggested the use of gain media to achieve a similar levitation effect, though this is controversial because these materials seem to violate fundamental causality constraints and the requirement of thermodynamic equilibrium (Kramers–Kronig relations). Casimir and Casimir-Polder repulsion can in fact occur for sufficiently anisotropic electrical bodies; for a review of the issues involved with repulsion see Milton et al.\n\nIt has been suggested that the Casimir forces have application in nanotechnology, in particular silicon integrated circuit technology based micro- and nanoelectromechanical systems, silicon array propulsion for space drives, and so-called Casimir oscillators.\n\nThe Casimir effect shows that quantum field theory allows the energy density in certain regions of space to be negative relative to the ordinary vacuum energy, and it has been shown theoretically that quantum field theory allows states where the energy can be \"arbitrarily\" negative at a given point. Many physicists such as Stephen Hawking, Kip Thorne, and others therefore argue that such effects might make it possible to stabilize a traversable wormhole.\n\nOn 4 June 2013 it was reported that a conglomerate of scientists from Hong Kong University of Science and Technology, University of Florida, Harvard University, Massachusetts Institute of Technology, and Oak Ridge National Laboratory have for the first time demonstrated a compact integrated silicon chip that can measure the Casimir force.\n\n\n\n\n\n"}
{"id": "7807", "url": "https://en.wikipedia.org/wiki?curid=7807", "title": "Cavitation", "text": "Cavitation\n\nCavitation is the formation of vapour cavities in a liquid, small liquid-free zones (\"bubbles\" or \"voids\"), that are the consequence of forces acting upon the liquid. It usually occurs when a liquid is subjected to rapid changes of pressure that cause the formation of cavities in the liquid where the pressure is relatively low. When subjected to higher pressure, the voids implode and can generate an intense shock wave.\n\nCavitation is a significant cause of wear in some engineering contexts. Collapsing voids that implode near to a metal surface cause cyclic stress through repeated implosion. This results in surface fatigue of the metal causing a type of wear also called \"cavitation\". The most common examples of this kind of wear are to pump impellers, and bends where a sudden change in the direction of liquid occurs. Cavitation is usually divided into two classes of behavior: inertial (or transient) cavitation and non-inertial cavitation.\n\nInertial cavitation is the process where a void or bubble in a liquid rapidly collapses, producing a shock wave. Inertial cavitation occurs in nature in the strikes of mantis shrimps and pistol shrimps, as well as in the vascular tissues of plants. In man-made objects, it can occur in control valves, pumps, propellers and impellers.\n\nNon-inertial cavitation is the process in which a bubble in a fluid is forced to oscillate in size or shape due to some form of energy input, such as an acoustic field. Such cavitation is often employed in ultrasonic cleaning baths and can also be observed in pumps, propellers, etc.\n\nSince the shock waves formed by collapse of the voids are strong enough to cause significant damage to moving parts, cavitation is usually an undesirable phenomenon. It is very often specifically avoided in the design of machines such as turbines or propellers, and eliminating cavitation is a major field in the study of fluid dynamics. However, it is sometimes useful and does not cause damage when the bubbles collapse away from machinery, such as in supercavitation.\n\nInertial cavitation was first observed in the late 19th century, considering the collapse of a spherical void within a liquid. When a volume of liquid is subjected to a sufficiently low pressure, it may rupture and form a cavity. This phenomenon is coined \"cavitation inception\" and may occur behind the blade of a rapidly rotating propeller or on any surface vibrating in the liquid with sufficient amplitude and acceleration. A fast-flowing river can cause cavitation on rock surfaces, particularly when there is a drop-off, such as on a waterfall.\n\nOther ways of generating cavitation voids involve the local deposition of energy, such as an intense focused laser pulse (optic cavitation) or with an electrical discharge through a spark. Vapor gases evaporate into the cavity from the surrounding medium; thus, the cavity is not a perfect vacuum, but has a relatively low gas pressure. Such a low-pressure bubble in a liquid begins to collapse due to the higher pressure of the surrounding medium. As the bubble collapses, the pressure and temperature of the vapor within increases. The bubble eventually collapses to a minute fraction of its original size, at which point the gas within dissipates into the surrounding liquid via a rather violent mechanism which releases a significant amount of energy in the form of an acoustic shock wave and as visible light. At the point of total collapse, the temperature of the vapor within the bubble may be several thousand kelvin, and the pressure several hundred atmospheres.\n\nInertial cavitation can also occur in the presence of an acoustic field. Microscopic gas bubbles that are generally present in a liquid will be forced to oscillate due to an applied acoustic field. If the acoustic intensity is sufficiently high, the bubbles will first grow in size and then rapidly collapse. Hence, inertial cavitation can occur even if the rarefaction in the liquid is insufficient for a Rayleigh-like void to occur. High-power ultrasonics usually utilize the inertial cavitation of microscopic vacuum bubbles for treatment of surfaces, liquids, and slurries.\n\nThe physical process of cavitation inception is similar to boiling. The major difference between the two is the thermodynamic paths that precede the formation of the vapor. Boiling occurs when the local temperature of the liquid reaches the saturation temperature, and further heat is supplied to allow the liquid to sufficiently phase change into a gas. Cavitation inception occurs when the local pressure falls sufficiently far below the saturated vapor pressure, a value given by the tensile strength of the liquid at a certain temperature.\n\nIn order for cavitation inception to occur, the cavitation \"bubbles\" generally need a surface on which they can nucleate. This surface can be provided by the sides of a container, by impurities in the liquid, or by small undissolved microbubbles within the liquid. It is generally accepted that hydrophobic surfaces stabilize small bubbles. These pre-existing bubbles start to grow unbounded when they are exposed to a pressure below the threshold pressure, termed Blake's threshold.\n\nThe vapor pressure here differs from the meteorological definition of vapor pressure, which describes the partial pressure of water in the atmosphere at some value less than 100% saturation. Vapor pressure as relating to cavitation refers to the vapor pressure in equilibrium conditions and can therefore be more accurately defined as the equilibrium (or saturated) vapor pressure.\n\nNon-inertial cavitation is the process in which small bubbles in a liquid are forced to oscillate in the presence of an acoustic field, when the intensity of the acoustic field is insufficient to cause total bubble collapse. This form of cavitation causes significantly less erosion than inertial cavitation, and is often used for the cleaning of delicate materials, such as silicon wafers.\n\nHydrodynamic cavitation describes the process of vaporisation, bubble generation and bubble implosion which occurs in a flowing liquid as a result of a decrease and subsequent increase in local pressure. Cavitation will only occur if the local pressure declines to some point below the saturated vapor pressure of the liquid and subsequent recovery above the vapor pressure. If the recovery pressure is not above the vapor pressure then flashing is said to have occurred. In pipe systems, cavitation typically occurs either as the result of an increase in the kinetic energy (through an area constriction) or an increase in the pipe elevation.\n\nHydrodynamic cavitation can be produced by passing a liquid through a constricted channel at a specific flow velocity or by mechanical rotation of an object through a liquid. In the case of the constricted channel and based on the specific (or unique) geometry of the system, the combination of pressure and kinetic energy can create the hydrodynamic cavitation cavern downstream of the local constriction generating high energy cavitation bubbles.\n\nThe process of bubble generation, and the subsequent growth and collapse of the cavitation bubbles, results in very high energy densities and in very high local temperatures and local pressures at the surface of the bubbles for a very short time. The overall liquid medium environment, therefore, remains at ambient conditions. When uncontrolled, cavitation is damaging; by controlling the flow of the cavitation, however, the power can be harnessed and non-destructive. Controlled cavitation can be used to enhance chemical reactions or propagate certain unexpected reactions because free radicals are generated in the process due to disassociation of vapors trapped in the cavitating bubbles.\n\nOrifices and venturi are reported to be widely used for generating cavitation. A venturi has an inherent advantage over an orifice because of its smooth converging and diverging sections, such that it can generate a higher flow velocity at the throat for a given pressure drop across it. On the other hand, an orifice has an advantage that it can accommodate a greater number of holes (larger perimeter of holes) in a given cross sectional area of the pipe.\n\nThe cavitation phenomenon can be controlled to enhance the performance of high-speed marine vessels and projectiles, as well as in material processing technologies, in medicine, etc. Controlling the cavitating flows in liquids can be achieved only by advancing the mathematical foundation of the cavitation processes. These processes are manifested in different ways, the most common ones and promising for control being bubble cavitation and supercavitation. The first exact classical solution should perhaps be credited to the well- known solution by H. Helmholtz in 1868. The earliest distinguished studies of academic type on the theory of a cavitating flow with free boundaries and supercavitation were published in the book \"Jets, wakes and cavities\" followed by \"Theory of jets of ideal fluid\". Widely used in these books was the well-developed theory of conformal mappings of functions of a complex variable, allowing one to derive a large number of exact solutions of plane problems. Another venue combining the existing exact solutions with approximated and heuristic models was explored in the work \"Hydrodynamics of Flows with Free Boundaries\" that refined the applied calculation techniques based on the principle of cavity expansion independence, theory of pulsations and stability of elongated axisymmetric cavities, etc. and in \"Dimensionality and similarity methods in the problems of the hydromechanics of vessels\".\n\nA natural continuation of these studies was recently presented in \"The Hydrodynamics of Cavitating Flows\" – an encyclopedic work encompassing all the best advances in this domain for the last three decades, and blending the classical methods of mathematical research with the modern capabilities of computer technologies. These include elaboration of nonlinear numerical methods of solving 3D cavitation problems, refinement of the known plane linear theories, development of asymptotic theories of axisymmetric and nearly axisymmetric flows, etc. As compared to the classical approaches, the new trend is characterized by expansion of the theory into the 3D flows. It also reflects a certain correlation with current works of an applied character on the hydrodynamics of supercavitating bodies.\n\nHydrodynamic cavitation can also improve some industrial processes. For instance, cavitated corn slurry shows higher yields in ethanol production compared to uncavitated corn slurry in dry milling facilities.\n\nThis is also used in the mineralization of bio-refractory compounds which otherwise would need extremely high temperature and pressure conditions since free radicals are generated in the process due to the dissociation of vapors trapped in the cavitating bubbles, which results in either the intensification of the chemical reaction or may even result in the propagation of certain reactions not possible under otherwise ambient conditions.\n\nIn industry, cavitation is often used to homogenize, or mix and break down, suspended particles in a colloidal liquid compound such as paint mixtures or milk. Many industrial mixing machines are based upon this design principle. It is usually achieved through impeller design or by forcing the mixture through an annular opening that has a narrow entrance orifice with a much larger exit orifice. In the latter case, the drastic decrease in pressure as the liquid accelerates into a larger volume induces cavitation. This method can be controlled with hydraulic devices that control inlet orifice size, allowing for dynamic adjustment during the process, or modification for different substances. The surface of this type of mixing valve, against which surface the cavitation bubbles are driven causing their implosion, undergoes tremendous mechanical and thermal localized stress; they are therefore often constructed of super-hard or tough materials such as stainless steel, Stellite, or even polycrystalline diamond (PCD).\n\nCavitating water purification devices have also been designed, in which the extreme conditions of cavitation can break down pollutants and organic molecules. Spectral analysis of light emitted in sonochemical reactions reveal chemical and plasma-based mechanisms of energy transfer. The light emitted from cavitation bubbles is termed sonoluminescence.\n\nUse of this technology has been tried successfully in alkali refining of vegetable oils.\n\nHydrophobic chemicals are attracted underwater by cavitation as the pressure difference between the bubbles and the liquid water forces them to join together. This effect may assist in protein folding.\n\nCavitation plays an important role for the destruction of kidney stones in shock wave lithotripsy. Currently, tests are being conducted as to whether cavitation can be used to transfer large molecules into biological cells (sonoporation). Nitrogen cavitation is a method used in research to lyse cell membranes while leaving organelles intact.\n\nCavitation plays a key role in non-thermal, non-invasive fractionation of tissue for treatment of a variety of diseases and can be used to open the blood-brain barrier to increase uptake of neurological drugs in the brain.\n\nCavitation also plays a role in HIFU, a thermal non-invasive treatment methodology for cancer. \n\nUltrasound sometimes is used to increase bone formation, for instance in post-surgical applications.\nUltrasound treatments or exposure can create cavitation that potentially may \"result in a syndrome involving manifestations of nausea, headache, tinnitus, pain, dizziness, and fatigue.\".\n\nIt has been suggested that the sound of \"cracking\" knuckles derives from the collapse of cavitation in the synovial fluid within the joint. Movements that cause cracking expand the joint space, thus reducing pressure to the point of cavitation. It remains controversial whether this is associated with clinically significant joint injury such as osteoarthritis. Some physicians say that osteoarthritis is caused by cracking knuckles regularly, as this causes wear and tear and may cause the bone to weaken. The implication being that, it is not the \"bubbles popping,\" but rather, the bones rubbing together, that causes osteoarthritis.\n\nIn industrial cleaning applications, cavitation has sufficient power to overcome the particle-to-substrate adhesion forces, loosening contaminants. The threshold pressure required to initiate cavitation is a strong function of the pulse width and the power input. This method works by generating controlled acoustic cavitation in the cleaning fluid, picking up and carrying contaminant particles away so that they do not reattach to the material being cleaned.\n\nCavitation has been applied to egg pasteurization. A hole-filled rotor produces cavitation bubbles, heating the liquid from within. Equipment surfaces stay cooler than the passing liquid, so eggs don't harden as they did on the hot surfaces of older equipment. The intensity of cavitation can be adjusted, making it possible to tune the process for minimum protein damage.\n\nCavitation is, in many cases, an undesirable occurrence. In devices such as propellers and pumps, cavitation causes a great deal of noise, damage to components, vibrations, and a loss of efficiency. Cavitation has also become a concern in the renewable energy sector as it may occur on the blade surface of tidal stream turbines.\n\nWhen the cavitation bubbles collapse, they force energetic liquid into very small volumes, thereby creating spots of high temperature and emitting shock waves, the latter of which are a source of noise. The noise created by cavitation is a particular problem for military submarines, as it increases the chances of being detected by passive sonar.\n\nAlthough the collapse of a small cavity is a relatively low-energy event, highly localized collapses can erode metals, such as steel, over time. The pitting caused by the collapse of cavities produces great wear on components and can dramatically shorten a propeller's or pump's lifetime.\n\nAfter a surface is initially affected by cavitation, it tends to erode at an accelerating pace. The cavitation pits increase the turbulence of the fluid flow and create crevices that act as nucleation sites for additional cavitation bubbles. The pits also increase the components' surface area and leave behind residual stresses. This makes the surface more prone to stress corrosion.\n\nMajor places where cavitation occurs are in pumps, on propellers, or at restrictions in a flowing liquid.\n\nAs an impeller's (in a pump) or propeller's (as in the case of a ship or submarine) blades move through a fluid, low-pressure areas are formed as the fluid accelerates around and moves past the blades. The faster the blade moves, the lower the pressure around it can become. As it reaches vapor pressure, the fluid vaporizes and forms small bubbles of gas. This is cavitation. When the bubbles collapse later, they typically cause very strong local shock waves in the fluid, which may be audible and may even damage the blades.\n\nCavitation in pumps may occur in two different forms:\n\nSuction cavitation occurs when the pump suction is under a low-pressure/high-vacuum condition where the liquid turns into a vapor at the eye of the pump impeller. This vapor is carried over to the discharge side of the pump, where it no longer sees vacuum and is compressed back into a liquid by the discharge pressure. This imploding action occurs violently and attacks the face of the impeller. An impeller that has been operating under a suction cavitation condition can have large chunks of material removed from its face or very small bits of material removed, causing the impeller to look spongelike. Both cases will cause premature failure of the pump, often due to bearing failure. Suction cavitation is often identified by a sound like gravel or marbles in the pump casing.\n\nCommon causes of suction cavitation can include clogged filters, pipe blockage on the suction side, poor piping design, pump running too far right on the pump curve, or conditions not meeting NPSH (net positive suction head) requirements.\n\nIn automotive applications, a clogged filter in a hydraulic system (power steering, power brakes) can cause suction cavitation making a noise that rises and falls in synch with engine RPM. It is fairly often a high pitched whine, like set of nylon gears not quite meshing correctly.\n\nDischarge cavitation occurs when the pump discharge pressure is extremely high, normally occurring in a pump that is running at less than 10% of its best efficiency point. The high discharge pressure causes the majority of the fluid to circulate inside the pump instead of being allowed to flow out the discharge. As the liquid flows around the impeller, it must pass through the small clearance between the impeller and the pump housing at extremely high flow velocity. This flow velocity causes a vacuum to develop at the housing wall (similar to what occurs in a venturi), which turns the liquid into a vapor. A pump that has been operating under these conditions shows premature wear of the impeller vane tips and the pump housing. In addition, due to the high pressure conditions, premature failure of the pump's mechanical seal and bearings can be expected. Under extreme conditions, this can break the impeller shaft.\n\nDischarge cavitation in joint fluid is thought to cause the popping sound produced by bone joint cracking, for example by deliberately cracking one's knuckles.\n\nSince all pumps require well-developed inlet flow to meet their potential, a pump may not perform or be as reliable as expected due to a faulty suction piping layout such as a close-coupled elbow on the inlet flange. When poorly developed flow enters the pump impeller, it strikes the vanes and is unable to follow the impeller passage. The liquid then separates from the vanes causing mechanical problems due to cavitation, vibration and performance problems due to turbulence and poor filling of the impeller. This results in premature seal, bearing and impeller failure, high maintenance costs, high power consumption, and less-than-specified head and/or flow.\n\nTo have a well-developed flow pattern, pump manufacturer's manuals recommend about (10 diameters?) of straight pipe run upstream of the pump inlet flange. Unfortunately, piping designers and plant personnel must contend with space and equipment layout constraints and usually cannot comply with this recommendation. Instead, it is common to use an elbow close-coupled to the pump suction which creates a poorly developed flow pattern at the pump suction.\n\nWith a double-suction pump tied to a close-coupled elbow, flow distribution to the impeller is poor and causes reliability and performance shortfalls. The elbow divides the flow unevenly with more channeled to the outside of the elbow. Consequently, one side of the double-suction impeller receives more flow at a higher flow velocity and pressure while the starved side receives a highly turbulent and potentially damaging flow. This degrades overall pump performance (delivered head, flow and power consumption) and causes axial imbalance which shortens seal, bearing and impeller life.\nTo overcome cavitation:\nIncrease suction pressure if possible.\nDecrease liquid temperature if possible.\nThrottle back on the discharge valve to decrease flow-rate.\nVent gases off the pump casing.\n\nCavitation can occur in control valves. If the actual pressure drop across the valve as defined by the upstream and downstream pressures in the system is greater than the sizing calculations allow, pressure drop flashing or cavitation may occur. The change from a liquid state to a vapor state results from the increase in flow velocity at or just downstream of the greatest flow restriction which is normally the valve port. To maintain a steady flow of liquid through a valve the flow velocity must be greatest at the vena contracta or the point where the cross sectional area is the smallest. This increase in flow velocity is accompanied by a substantial decrease in the fluid pressure which is partially recovered downstream as the area increases and flow velocity decreases. This pressure recovery is never completely to the level of the upstream pressure. If the pressure at the vena contracta drops below the vapor pressure of the fluid bubbles will form in the flow stream. If the pressure recovers after the valve to a pressure that is once again above the vapor pressure, then the vapor bubbles will collapse and cavitation will occur.\n\nWhen water flows over a dam spillway, the irregularities on the spillway surface will cause small areas of flow separation in a high-speed flow, and, in these regions, the pressure will be lowered. If the flow velocities are high enough the pressure may fall to below the local vapor pressure of the water and vapor bubbles will form. When these are carried downstream into a high pressure region the bubbles collapse giving rise to high pressures and possible cavitation damage.\n\nExperimental investigations show that the damage on concrete chute and tunnel spillways can start at clear water flow velocities of between 12 and 15 m/s, and, up to flow velocities of 20 m/s, it may be possible to protect the surface by streamlining the boundaries, improving the surface finishes or using resistant materials.\n\nWhen some air is present in the water the resulting mixture is compressible and this damps the high pressure caused\nby the bubble collapses. If the flow velocities near the spillway invert are sufficiently high, aerators (or aeration devices) must be introduced to prevent cavitation. Although these have been installed for some years, the mechanisms of air entrainment at the aerators and the slow movement of the air away from the spillway surface are still challenging.\n\nThe spillway aeration device design is based upon a small deflection of the spillway bed (or sidewall) such as a ramp and offset to deflect the high flow velocity flow away from the spillway surface. In the cavity formed below the nappe, a local subpressure beneath the nappe is produced by which air is sucked into the flow. The complete design includes the deflection device (ramp, offset) and the air supply system.\n\nSome larger diesel engines suffer from cavitation due to high compression and undersized cylinder walls. Vibrations of the cylinder wall induce alternating low and high pressure in the coolant against the cylinder wall. The result is pitting of the cylinder wall, which will eventually let cooling fluid leak into the cylinder and combustion gases to leak into the coolant.\n\nIt is possible to prevent this from happening with the use of chemical additives in the cooling fluid that form a protective layer on the cylinder wall. This layer will be exposed to the same cavitation, but rebuilds itself. Additionally a regulated overpressure in the cooling system (regulated and maintained by the coolant filler cap spring pressure) prevents the forming of cavitation.\n\nFrom about the 1980s, new designs of smaller gasoline engines also displayed cavitation phenomena. One answer to the need for smaller and lighter engines was a smaller coolant volume and a correspondingly higher coolant flow velocity. This gave rise to rapid changes in flow velocity and therefore rapid changes of static pressure in areas of high heat transfer. Where resulting vapor bubbles collapsed against a surface, they had the effect of first disrupting protective oxide layers (of cast aluminium materials) and then repeatedly damaging the newly formed surface, preventing the action of some types of corrosion inhibitor (such as silicate based inhibitors). A final problem was the effect that increased material temperature had on the relative electrochemical reactivity of the base metal and its alloying constituents. The result was deep pits that could form and penetrate the engine head in a matter of hours when the engine was running at high load and high speed. These effects could largely be avoided by the use of organic corrosion inhibitors or (preferably) by designing the engine head in such a way as to avoid certain cavitation inducing conditions.\n\nSome hypotheses relating to diamond formation posit a possible role for cavitation—namely cavitiation in the kimberlite pipes providing the extreme pressure needed to change pure carbon into the rare allotrope that is diamond.\n\nThe loudest three sounds ever recorded, during the 1883 eruption of Krakatoa, are now understood as the bursts of three huge cavitation bubbles, each larger than the last, formed in the volcano's throat. Rising magma, filled with dissolved gasses and under immense pressure, encountered a different magma that compressed easily, allowing bubbles to grow and combine. \n\nThere exist macroscopic white lamellae inside quartz and other minerals in the Bohemian Massif and even at another places in whole of the world like wavefronts generated by a meteorite impact according to the Rajlich's Hypothesis. The hypothetical wavefronts are composed of many microcavities. Their origin is seen in a physical phenomenon of ultrasonic cavitation, which is well known from the technical practice.\n\nCavitation occurs in the xylem of vascular plants when the tension of water within the xylem exceeds atmospheric pressure. The sap vaporizes locally so that either the vessel elements or tracheids are filled with water vapor. Plants are able to repair cavitated xylem in a number of ways. For plants less than 50 cm tall, root pressure can be sufficient to redissolve the vapor. Larger plants direct solutes into the xylem via \"ray cells\", or in tracheids, via osmosis through bordered pits. Solutes attract water, the pressure rises and vapor can redissolve. In some trees, the sound of the cavitation is audible, particularly in summer, when the rate of evapotranspiration is highest. Some deciduous trees have to shed leaves in the autumn partly because cavitation increases as temperatures decrease.\n\nJust as cavitation bubbles form on a fast-spinning boat propeller, they may also form on the tails and fins of aquatic animals. This primarily occurs near the surface of the ocean, where the ambient water pressure is low.\n\nCavitation may limit the maximum swimming speed of powerful swimming animals like dolphins and tuna. Dolphins may have to restrict their speed because collapsing cavitation bubbles on their tail are painful. Tuna have bony fins without nerve endings and do not feel pain from cavitation. They are slowed down when cavitation bubbles create a vapor film around their fins. Lesions have been found on tuna that are consistent with cavitation damage.\n\nSome sea animals have found ways to use cavitation to their advantage when hunting prey. The pistol shrimp snaps a specialized claw to create cavitation, which can kill small fish. The mantis shrimp (of the \"smasher\" variety) uses cavitation as well in order to stun, smash open, or kill the shellfish that it feasts upon.\n\nThresher sharks use 'tail slaps' to debilitate their small fish prey and cavitation bubbles have been seen rising from the apex of the tail arc.\n\nIn the last half-decade, coastal erosion in the form of inertial cavitation has been generally accepted. Bubbles in an incoming wave are forced into cracks in the cliff being eroded. Varying pressure decompresses some vapor pockets which subsequently implode. The resulting pressure peaks can blast apart fractions of the rock.\n\n\n\n"}
{"id": "6111", "url": "https://en.wikipedia.org/wiki?curid=6111", "title": "Chemical vapor deposition", "text": "Chemical vapor deposition\n\nChemical vapor deposition (CVD) is a deposition method used to produce high quality, high-performance, solid materials, typically under vacuum. The process is often used in the semiconductor industry to produce thin films.\n\nIn typical CVD, the wafer (substrate) is exposed to one or more volatile precursors, which react and/or decompose on the substrate surface to produce the desired deposit. Frequently, volatile by-products are also produced, which are removed by gas flow through the reaction chamber.\n\nMicrofabrication processes widely use CVD to deposit materials in various forms, including: monocrystalline, polycrystalline, amorphous, and epitaxial. These materials include: silicon (dioxide, carbide, nitride, oxynitride), carbon (fiber, nanofibers, nanotubes, diamond and graphene), fluorocarbons, filaments, tungsten, titanium nitride and various high-k dielectrics.\n\nCVD is practiced in a variety of formats. These processes generally differ in the means by which chemical reactions are initiated.\n\nMost modern CVD is either LPCVD or UHVCVD.\n\nCVD is commonly used to deposit conformal films and augment substrate surfaces in ways that more traditional surface modification techniques are not capable of. CVD is extremely useful in the process of atomic layer deposition at depositing extremely thin layers of material. A variety of applications for such films exist. Gallium arsenide is used in some integrated circuits (ICs) and photovoltaic devices. Amorphous polysilicon is used in photovoltaic devices. Certain carbides and nitrides confer wear-resistance. Polymerization by CVD, perhaps the most versatile of all applications, allows for super-thin coatings which possess some very desirable qualities, such as lubricity, hydrophobicity and weather-resistance to name a few. CVD of metal-organic frameworks, a class of crystalline nanoporous materials, has recently been demonstrated. Applications for these films are anticipated in gas sensing and low-k dielectrics\nCVD techniques are adventageous for membrane coatings as well, such as those in desalination or water treatment, as these coatings can be sufficiently uniform (conformal) and thin that they do not clog membrane pores.\n\nPolycrystalline silicon is deposited from trichlorosilane (SiHCl) or silane (SiH), using the following reactions:\n\nThis reaction is usually performed in LPCVD systems, with either pure silane feedstock, or a solution of silane with 70–80% nitrogen. Temperatures between 600 and 650 °C and pressures between 25 and 150 Pa yield a growth rate between 10 and 20 nm per minute. An alternative process uses a hydrogen-based solution. The hydrogen reduces the growth rate, but the temperature is raised to 850 or even 1050 °C to compensate. Polysilicon may be grown directly with doping, if gases such as phosphine, arsine or diborane are added to the CVD chamber. Diborane increases the growth rate, but arsine and phosphine decrease it.\n\nSilicon dioxide (usually called simply \"oxide\" in the semiconductor industry) may be deposited by several different processes. Common source gases include silane and oxygen, dichlorosilane (SiClH) and nitrous oxide (NO), or tetraethylorthosilicate (TEOS; Si(OCH)). The reactions are as follows:\n\nThe choice of source gas depends on the thermal stability of the substrate; for instance, aluminium is sensitive to high temperature. Silane deposits between 300 and 500 °C, dichlorosilane at around 900 °C, and TEOS between 650 and 750 °C, resulting in a layer of \"low- temperature oxide\" (LTO). However, silane produces a lower-quality oxide than the other methods (lower dielectric strength, for instance), and it deposits nonconformally. Any of these reactions may be used in LPCVD, but the silane reaction is also done in APCVD. CVD oxide invariably has lower quality than thermal oxide, but thermal oxidation can only be used in the earliest stages of IC manufacturing.\n\nOxide may also be grown with impurities (alloying or \"doping\"). This may have two purposes. During further process steps that occur at high temperature, the impurities may diffuse from the oxide into adjacent layers (most notably silicon) and dope them. Oxides containing 5–15% impurities by mass are often used for this purpose. In addition, silicon dioxide alloyed with phosphorus pentoxide (\"P-glass\") can be used to smooth out uneven surfaces. P-glass softens and reflows at temperatures above 1000 °C. This process requires a phosphorus concentration of at least 6%, but concentrations above 8% can corrode aluminium. Phosphorus is deposited from phosphine gas and oxygen:\n\nGlasses containing both boron and phosphorus (borophosphosilicate glass, BPSG) undergo viscous flow at lower temperatures; around 850 °C is achievable with glasses containing around 5 weight % of both constituents, but stability in air can be difficult to achieve. Phosphorus oxide in high concentrations interacts with ambient moisture to produce phosphoric acid. Crystals of BPO can also precipitate from the flowing glass on cooling; these crystals are not readily etched in the standard reactive plasmas used to pattern oxides, and will result in circuit defects in integrated circuit manufacturing.\n\nBesides these intentional impurities, CVD oxide may contain byproducts of the deposition. TEOS produces a relatively pure oxide, whereas silane introduces hydrogen impurities, and dichlorosilane introduces chlorine.\n\nLower temperature deposition of silicon dioxide and doped glasses from TEOS using ozone rather than oxygen has also been explored (350 to 500 °C). Ozone glasses have excellent conformality but tend to be hygroscopic – that is, they absorb water from the air due to the incorporation of silanol (Si-OH) in the glass. Infrared spectroscopy and mechanical strain as a function of temperature are valuable diagnostic tools for diagnosing such problems.\n\nSilicon nitride is often used as an insulator and chemical barrier in manufacturing ICs. The following two reactions deposit silicon nitride from the gas phase:\n\nSilicon nitride deposited by LPCVD contains up to 8% hydrogen. It also experiences strong tensile stress, which may crack films thicker than 200 nm. However, it has higher resistivity and dielectric strength than most insulators commonly available in microfabrication (10 Ω·cm and 10 MV/cm, respectively).\n\nAnother two reactions may be used in plasma to deposit SiNH:\n\nThese films have much less tensile stress, but worse electrical properties (resistivity 10 to 10 Ω·cm, and dielectric strength 1 to 5 MV/cm).\n\nCVD for tungsten is achieved from tungsten hexafluoride (WF), which may be deposited in two ways:\n\nOther metals, notably aluminium and copper, can be deposited by CVD. , commercially cost-effective CVD for copper did not exist, although volatile sources exist, such as Cu(hfac). Copper is typically deposited by electroplating. Aluminum can be deposited from triisobutylaluminium (TIBAL) and related organoaluminium compounds.\n\nCVD for molybdenum, tantalum, titanium, nickel is widely used. These metals can form useful silicides when deposited onto silicon. Mo, Ta and Ti are deposited by LPCVD, from their pentachlorides. Nickel, molybdenum, and tungsten can be deposited at low temperatures from their carbonyl precursors. In general, for an arbitrary metal \"M\", the chloride deposition reaction is as follows:\n\nwhereas the carbonyl decomposition reaction can happen spontaneously under thermal treatment or acoustic cavitation and is as follows:\n\nthe decomposition of metal carbonyls is often violently precipitated by moisture or air, where oxygen reacts with the metal precursor to form metal or metal oxide along with carbon dioxide.\n\nNiobium(V) oxide layers can be produced by the thermal decomposition of niobium(V) ethoxide with the loss of diethyl ether according to the equation:\n\nMany variations of CVD can be utilized to synthesize graphene. Although many advancements have been made, the processes listed below are not commercially viable yet.\nThe most popular carbon source used to produce graphene is methane gas. Less popular choices include petroleum asphalt, notable for being inexpensive but more difficult to work with.\nThe use of catalyst is viable in changing the physical process of graphene production. Notable examples include iron nanoparticles, nickel foam, and gallium vapor. These catalysts can either be used in situ during graphene buildup, or situated at some distance away at the deposition area. Some catalysts require another step to remove them from the sample material.\n\nThe direct growth of high-quality, large single-crystalline domains of graphene on a dielectric substrate is of vital importance for applications in electronics and optoelectronics. Combining the advantages of both catalytic CVD and the ultra-flat dielectric substrate, gaseous catalyst-assisted CVD paves the way for synthesizing high-quality graphene for device applications while avoiding the transfer process.\nPhysical conditions such as surrounding pressure, temperature, carrier gas, and chamber material play a big role in production of graphene.\n\nMost systems use LPCVD with pressures ranging from 1 to 1500 Pa. However, some still use APCVD. Low pressures are used more commonly as they help prevent unwanted reactions and produce more uniform thickness of deposition on the substrate.\n\nOn the other hand, temperatures used range from 800–1050 °C. High temperatures translate to an increase of the rate of reaction. Caution has to be exercised as high temperatures do pose higher danger levels in addition to greater energy costs.\nHydrogen gas and inert gases such as argon are flowed into the system. These gases act as a carrier, enhancing surface reaction and improving reaction rate, thereby increasing deposition of graphene onto the substrate.\nStandard quartz tubing and chambers are used in CVD of graphene. Quartz is chosen because it has a very high melting point and is chemically inert. In other words, quartz does not interfere with any physical or chemical reactions regardless of the conditions.\nRaman spectroscopy, X-ray spectroscopy, transmission electron microscopy (TEM), and scanning electron microscopy (SEM) are used to examine and characterize the graphene samples.\n\nRaman spectroscopy is used to characterize and identify the graphene particles; X-ray spectroscopy is used to characterize chemical states; TEM is used to provide fine details regarding the internal composition of graphene; SEM is used to examine the surface and topography.\n\nSometimes, atomic force microscopy (AFM) is used to measure local properties such as friction and magnetism.\n\nCold wall CVD technique can be used to study the underlying surface science involved in graphene nucleation and growth as it allows unprecedented control of process parameters like gas flow rates, temperature and pressure as demonstrated in a recent study. The study was carried out in a home-built vertical cold wall system utilizing resistive heating by passing direct current through the substrate. It provided conclusive insight into a typical surface-mediated nucleation and growth mechanism involved in two-dimensional materials grown using catalytic CVD under conditions sought out in the semiconductor industry.\n\nIn spite of graphene's exciting electronic and thermal properties, it is unsuitable as a transistor for future digital devices, due to the absence of a bandgap between the conduction and valence bands. This makes it impossible to switch between on and off states with respect to electron flow. Scaling things down, graphene nanoribbons of less than 10 nm in width do exhibit electronic bandgaps and are therefore potential candidates for digital devices. Precise control over their dimensions, and hence electronic properties, however, represents a challenging goal, and the ribbons typically possess rough edges that are detrimental to their performance.\n\nCVD can be used to produce a synthetic diamond by creating the circumstances necessary for carbon atoms in a gas to settle on a substrate in crystalline form. CVD of diamonds has received much attention in the materials sciences because it allows many new applications that had previously been considered too expensive. CVD diamond growth typically occurs under low pressure (1–27 kPa; 0.145–3.926 psi; 7.5–203 Torr) and involves feeding varying amounts of gases into a chamber, energizing them and providing conditions for diamond growth on the substrate. The gases always include a carbon source, and typically include hydrogen as well, though the amounts used vary greatly depending on the type of diamond being grown. Energy sources include hot filament, microwave power, and arc discharges, among others. The energy source is intended to generate a plasma in which the gases are broken down and more complex chemistries occur. The actual chemical process for diamond growth is still under study and is complicated by the very wide variety of diamond growth processes used.\n\nUsing CVD, films of diamond can be grown over large areas of substrate with control over the properties of the diamond produced. In the past, when high pressure high temperature (HPHT) techniques were used to produce a diamond, the result was typically very small free standing diamonds of varying sizes. With CVD diamond growth areas of greater than fifteen centimeters (six inches) diameter have been achieved and much larger areas are likely to be successfully coated with diamond in the future. Improving this process is key to enabling several important applications.\n\nThe growth of diamond directly on a substrate allows the addition of many of diamond's important qualities to other materials. Since diamond has the highest thermal conductivity of any bulk material, layering diamond onto high heat producing electronics (such as optics and transistors) allows the diamond to be used as a heat sink. Diamond films are being grown on valve rings, cutting tools, and other objects that benefit from diamond's hardness and exceedingly low wear rate. In each case the diamond growth must be carefully done to achieve the necessary adhesion onto the substrate. Diamond's very high scratch resistance and thermal conductivity, combined with a lower coefficient of thermal expansion than Pyrex glass, a coefficient of friction close to that of Teflon (polytetrafluoroethylene) and strong lipophilicity would make it a nearly ideal non-stick coating for cookware if large substrate areas could be coated economically.\n\nCVD growth allows one to control the properties of the diamond produced. In the area of diamond growth, the word \"diamond\" is used as a description of any material primarily made up of sp3-bonded carbon, and there are many different types of diamond included in this. By regulating the processing parameters—especially the gases introduced, but also including the pressure the system is operated under, the temperature of the diamond, and the method of generating plasma—many different materials that can be considered diamond can be made. Single crystal diamond can be made containing various dopants. Polycrystalline diamond consisting of grain sizes from several nanometers to several micrometers can be grown. Some polycrystalline diamond grains are surrounded by thin, non-diamond carbon, while others are not. These different factors affect the diamond's hardness, smoothness, conductivity, optical properties and more.\n\nCommercially, mercury cadmium telluride is of continuing interest for detection of infrared radiation. Consisting of an alloy of CdTe and HgTe, this material can be prepared from the dimethyl derivatives of the respective elements.\n\n\n"}
{"id": "1170166", "url": "https://en.wikipedia.org/wiki?curid=1170166", "title": "Chirality (chemistry)", "text": "Chirality (chemistry)\n\nChirality is a geometric property of some molecules and ions. A chiral molecule/ion is non-superposable on its mirror image. The presence of an asymmetric carbon center is one of several structural features that induce chirality in organic and inorganic molecules. The term \"chirality\" is derived from the Ancient Greek word for hand, χεῖρ (\"kheir\").\n\nThe mirror images of a chiral molecule or ion are called enantiomers or optical isomers. Individual enantiomers are often designated as either right-handed or left-handed. Chirality is an essential consideration when discussing the stereochemistry in organic and inorganic chemistry. The concept is of great practical importance because most biomolecules and pharmaceuticals are chiral.\n\nChiral molecules and ions are described by various ways of designating their absolute configuration, which codify either the entity's geometry or its ability to rotate plane-polarized light, a common technique in studying chirality.\n\nChirality is based on molecular symmetry elements. Specifically, a chiral compound can contain no improper axis of rotation (S), which includes planes of symmetry and inversion center. Chiral molecules are always dissymmetric (lacking S) but not always asymmetric (lacking all symmetry elements except the trivial identity). Asymmetric molecules are always chiral.\n\nIn general, chiral molecules have point chirality at a single \"stereogenic\" atom, which has four different substituents. The two enantiomers of such compounds are said to have different absolute configurations at this center. This center is thus stereogenic (i.e., a grouping within a molecular entity that may be considered a focus of stereoisomerism). The stereogenic atom (also known as the \"stereocenter\") is usually carbon, as in many biological molecules. However a stereocenter can coincide with any atom, including metals (as in many chiral coordination compounds), phosphorus, or sulfur. The low barrier of nitrogen inversion make most \"N\"-chiral amines (NRR′R″) impossible to resolve, but \"P\"-chiral phosphines (PRR′R″) as well as S-chiral sulfoxides (OSRR′) are optically stable.\n\nWhile the presence of a stereogenic atom describes the great majority of chiral molecules, many variations and exceptions exist. For instance it is not necessary for the chiral substance to have a stereogenic atom. Examples include 1-bromo-3-chloro-5-fluoroadamantane, methylethylphenyltetrahedrane, certain calixarenes and fullerenes, which have inherent chirality. The C-symmetric species 1,1′-bi-2-naphthol (BINOL), 1,3-dichloroallene have axial chirality. (\"E\")-cyclooctene and many ferrocenes have planar chirality.\n\nWhen the optical rotation for an enantiomer is too low for practical measurement, the species is said to exhibit cryptochirality.\n\nEven isotopic differences must be considered when examining chirality. Illustrative is the derivative of benzyl alcohol PhCHDOH, which is chiral. The \"S\" enantiomer has [\"α\"] = +0.715°.\n\nMany biologically active molecules are chiral, including the naturally occurring amino acids (the building blocks of proteins) and sugars. In biological systems, most of these compounds are of the same chirality: most amino acids are levorotatory () and sugars are dextrorotatory (). Typical naturally occurring proteins are made of -amino acids and are known as \"left-handed proteins\"; the comparably rarer -amino acids produce \"right-handed proteins\".\n\nThe origin of this homochirality in biology is the subject of much debate. Most scientists believe that Earth life's \"choice\" of chirality was purely random, and that if carbon-based life forms exist elsewhere in the universe, their chemistry could theoretically have opposite chirality. However, there is some suggestion that early amino acids could have formed in comet dust. In this case, circularly polarised radiation (which makes up 17% of stellar radiation) could have caused the selective destruction of one chirality of amino acids, leading to a selection bias which ultimately resulted in all life on Earth being homochiral.\n\nEnzymes, which are chiral, often distinguish between the two enantiomers of a chiral substrate. One could imagine an enzyme as having a glove-like cavity that binds a substrate. If this glove is right-handed, then one enantiomer will fit inside and be bound, whereas the other enantiomer will have a poor fit and is unlikely to bind.\n\n-forms of amino acids tend to be tasteless, whereas -forms tend to taste sweet. Spearmint leaves contain the -enantiomer of the chemical carvone or \"R\"-(−)-carvone and caraway seeds contain the -enantiomer or \"S\"-(+)-carvone. These smell different to most people because our olfactory receptors are chiral.\n\nChirality is important in context of ordered phases as well, for example the addition of a small amount of an optically active molecule to a nematic phase (a phase that has long range orientational order of molecules) transforms that phase to a chiral nematic phase (or cholesteric phase). Chirality in context of such phases in polymeric fluids has also been studied in this context.\n\nChirality is a symmetry property, not a characteristic of any part of the periodic table. Thus many inorganic materials, molecules, and ions are chiral. Quartz is an example from the mineral kingdom. Such noncentric materials are of interest for applications in nonlinear optics.\n\nIn the areas of coordination chemistry and organometallic chemistry, chirality is pervasive and of practical importance. A famous example is tris(bipyridine)ruthenium(II) complex in which the three bipyridine ligands adopt a chiral propeller-like arrangement. The two enantiomers of complexes such as [Ru(2,2′-bipyridine)] may be designated as Λ (capital lambda, the Greek version of \"L\") for a left-handed twist of the propeller described by the ligands, and Δ (capital delta, Greek \"D\") for a right-handed twist (pictured).\n\nChiral ligands confer chirality to a metal complex, as illustrated by metal-amino acid complexes. If the metal exhibits catalytic properties, its combination with a chiral ligand is the basis of asymmetric catalysis.\n\nThe term \"optical activity\" is derived from the interaction of chiral materials with polarized light. In a solution, the (−)-form, or levorotatory form, of an optical isomer rotates the plane of a beam of linearly polarized light counterclockwise. The (+)-form, or dextrorotatory form, of an optical isomer does the opposite. The rotation of light is measured using a polarimeter and is expressed as the optical rotation.\n\n\nThe rotation of plane polarized light by chiral substances was first observed by Jean-Baptiste Biot in 1815, and gained considerable importance in the sugar industry, analytical chemistry, and pharmaceuticals. Louis Pasteur deduced in 1848 that this phenomenon has a molecular basis. The term \"chirality\" itself was coined by Lord Kelvin in 1894. Different enantiomers or diastereomers of a compound were formerly called optical isomers due to their different optical properties. At one time, chirality was thought to be associated with organic chemistry, but this misconception was overthrown by the resolution of a purely inorganic compound, hexol, by Alfred Werner.\n\n\n"}
{"id": "421129", "url": "https://en.wikipedia.org/wiki?curid=421129", "title": "Cob (material)", "text": "Cob (material)\n\nCob, cobb or clom (in Wales) is a natural building material made from subsoil, water, fibrous organic material (typically straw), and sometimes lime. The contents of subsoil naturally vary, and if it does not contain the right mixture it can be modified with sand or clay. Cob is fireproof, resistant to seismic activity, and inexpensive. It can be used to create artistic, sculptural forms, and its use has been revived in recent years by the natural building and sustainability movements.\n\nIn technical building and engineering documents such as the Uniform Building Code, cob may be referred to as an \"unburned clay masonry\" when used in a structural context. It might also be referred to as an \"aggregate\" in non-structural contexts, such as a \"clay and sand aggregate\" or more simply an \"organic aggregate,\" such as where the cob is an insulating filler between post and beam construction.\n\n\"Cob\" is an English term attested to around the year 1600 for an ancient building material that has been used for building since prehistoric times. The etymology of \"cob\" and \"cobbing\" is unclear, but in several senses means to \"beat\" or \"strike\", which is how cob material is applied to a wall.\n\nSome of the oldest man-made structures in Afghanistan are composed of rammed earth and cob. Cobwork (\"tabya\") was used in the Maghreb and al-Andalus in the 11th and 12th centuries, and was described in detail by Ibn Khaldun in the 14th century.\n\nCob material is known by many names including \"adobe\", \"lump clay\", \"puddled clay\", \"chalk mud\", \"wichert\", \"clay daubins\", \"swish\" (Asante Twi), \"torchis\" (French), \"bauge\" (French), \"bousille\" (French mud with moss), and \"cat and clay\".\n\nCob structures can be found in a variety of climates across the globe. European examples include:\n\n\nMany old cob buildings can be found in Africa, the Middle East, and many parts of the southwestern United States. A number of cob cottages survive from mid-19th-century New Zealand.\n\nTraditionally, English cob was made by mixing the clay-based subsoil with sand, straw and water using oxen to trample it. English soils contain varying amounts of chalk, and cob made with significant amounts of chalk are called \"chalk cob\" or \"wychert\". The earthen mixture was then ladled onto a stone foundation in courses and trodden onto the wall by workers in a process known as \"cobbing\". The construction would progress according to the time required for the prior course to dry. After drying, the walls would be trimmed and the next course built, with lintels for later openings such as doors and windows being placed as the wall takes shape.\n\nThe walls of a cob house are generally about thick, and windows were correspondingly deep-set, giving the homes a characteristic internal appearance. The thick walls provided excellent thermal mass which was easy to keep warm in winter and cool in summer. Walls with a high thermal mass value act as a thermal buffer inside the home.\nThe material has a long life-span even in rainy and/or humid climates, provided a tall foundation and large roof overhang are present.\n\nCob is fireproof, while \"fire cob\" (cob without straw or fiber) is a refractory material (the same material, essentially, as unfired common red brick), and historically, has been used to make chimneys, fireplaces, forges and crucibles. Without fiber, however, cob loses most of its tensile strength.\n\nWhen Kevin McCabe constructed a two-story, four bedroom cob house in England, UK in 1994, it was reputedly the first cob residence built in the country in 70 years. His techniques remained very traditional; the only innovations he made were using a tractor to mix the cob and adding sand or shillet, a gravel of crushed shale, to reduce shrinkage.\n\nIn 2000-01, a modern, four bedroom cob house in Worcestershire, England, UK, designed by Associated Architects, was sold for £999,000. Cobtun House was erected in 2001 and won the Royal Institute of British Architects' Sustainable Building of the Year award in 2005. The total construction cost was £300,000, but the metre-thick outer cob wall cost only £20,000.\n\nIn the Pacific Northwest of the United States there has been a resurgence of cob construction, both as an alternative building practice and one desired for its form, function, and cost effectiveness. Pat Hennebery, Tracy Calvert, Elke Cole, and the Cobworks workshops erected more than ten cob houses in the Southern Gulf Islands of British Columbia, Canada.\n\nIn 2010, Sota Construction Services in Pittsburgh, Pennsylvania, United States completed construction on its new 7,500 square foot corporate headquarters, which featured exterior cob walls along with other energy saving features like radiant heat flooring, a rooftop solar panel array, and daylighting. The cob walls, in conjunction with the other sustainable features, enabled the edifice to earn a LEED Platinum rating in 2012, and it also received one of the highest scores by percentage of total points earned in any LEED category.\n\nIn 2007, Ann and Gord Baird began constructing a two-storey cob house in Victoria, British Columbia, Canada for an estimated $210,000 CDN. The home of 2,150 square feet includes heated floors, solar panels, and a southern exposure to enable passive solar heating.\n\nWelsh architect Ianto Evans and researcher Linda Smiley refined the construction technique known as \"Oregon Cob\" in the 1980s and 1990s. Oregon Cob integrates the variation of wall layup technique which uses loaves of mud mixed with sand and straw with a rounded architectural stylism. They are experimenting with a mixture of cob and straw bale denominated \"balecob\".\n\n\n\n"}
{"id": "665333", "url": "https://en.wikipedia.org/wiki?curid=665333", "title": "Cymdeithas Edward Llwyd", "text": "Cymdeithas Edward Llwyd\n\nCymdeithas Edward Llwyd (English: Edward Llwyd Society) is a Welsh natural history organization whose name commemorates the great Welsh natural historian, geographer and linguist Edward Llwyd.\n\nThe Cymdeithas Edward Llwyd organizes regular country walks throughout Wales in sites of interest of the Welsh environment, including SSI's & post-industrial landscapes. These are Welsh-language walking groups, although learners are just as welcome.\n\nThey also organize a variety of Nature & Environmental activities, including lectures, publications on Welsh Nature & Environment & conservation work.\n\n"}
{"id": "35971482", "url": "https://en.wikipedia.org/wiki?curid=35971482", "title": "Day length fluctuations", "text": "Day length fluctuations\n\nThe length of the day, which has increased over the long term of Earth's history due to tidal effects, is also subject to fluctuations on a shorter scale of time. Exact measurements of time by atomic clocks and satellite laser ranging have revealed that the length of day (LOD) is subject to a number of different changes. These subtle variations have periods that range from a few weeks to a few years. They are attributed to interactions between the dynamic atmosphere and Earth itself. The International Earth Rotation and Reference Systems Service monitors the changes.\n\nIn the absence of external torques, the total angular momentum of Earth as a whole system must be constant. Internal torques are due to relative movements and mass redistribution of Earth's core, mantle, crust, oceans, atmosphere, and cryosphere. In order to keep the total angular momentum constant, a change of the angular momentum in one region must necessarily be balanced by angular momentum changes in the other regions.\n\nCrustal movements (such as continental drift) or polar cap melting are slow secular events. The characteristic coupling time between core and mantle has been estimated to be on the order of ten years, and the so-called 'decade fluctuations' of Earth's rotation rate are thought to result from fluctuations within the core, transferred to the mantle. The length of day (LOD) varies significantly even for time scales from a few years down to weeks (Figure), and the observed fluctuations in the LOD - after eliminating the effects of external torques - are a direct consequence of the action of internal torques. These short term fluctuations are very probably generated by the interaction between the solid Earth and the atmosphere.\n\nAny change of the axial component of the atmospheric angular momentum (AAM) must be accompanied by a corresponding change of the angular momentum of Earth's crust and mantle (due to the law of conservation of angular momentum). Because the moment of inertia of the system mantle-crust is only slightly influenced by atmospheric pressure loading, this mainly requires a change in the angular velocity of the solid Earth; \"i.e.\", a change of LOD. The LOD can presently be measured to a high accuracy over integration times of only a few hours, and general circulation models of the atmosphere allow high precision determination of changes in AAM in the model. A comparison between AAM and LOD shows that they are highly correlated. In particular, one recognizes an annual period of LOD with an amplitude of 0.34 milliseconds, maximizing on February 3, and a semiannual period with an amplitude of 0.29 milliseconds, maximizing on May 8, as well as 10‑day fluctuations of the order of 0.1 milliseconds. Interseasonal fluctuations reflecting El Niño events and quasi-biennial oscillations have also been observed. There is now general agreement that most of the changes in LOD on time scales from weeks to a few years are excited by changes in AAM.\n\nOne means of exchange of angular momentum between the atmosphere and the non gaseous parts of the earth is evaporation and precipitation. Massive quantities of water are in continual flux between the oceans and the atmosphere. As the mass of water (vapour) rises its rotation must slow due to conservation of angular momentum. Equally when if falls as rain, its rate of rotation will increase to conserve angular momentum. Any net global transfer of water mass from oceans to the atmosphere or the opposite implies a change in the speed of rotation of the solid/liquid Earth which will be reflected in LOD. \n\nObservational evidence shows that there is no significant time delay between the change of AAM and its corresponding change of LOD for periods longer than about 10 days. This implies a strong coupling between atmosphere and solid Earth due to surface friction with a time constant of about 7 days, the spin-down time of the Ekman layer. This spin-down time is the characteristic time for the transfer of atmospheric axial angular momentum to Earth's surface and vice versa.\n\nThe zonal wind-component on the ground, which is most effective for the transfer of axial angular momentum between Earth and atmosphere, is the component describing rigid rotation of the atmosphere. The zonal wind of this component has the amplitude \"u\" at the equator relative to the ground, where \"u\" > 0 indicates superrotation and \"u\" < 0 indicates retrograde rotation with respect to the solid Earth. All other wind terms merely redistribute the AAM with latitude, an effect that cancels out when averaged over the globe.\n\nSurface friction allows the atmosphere to 'pick up' angular momentum from Earth in the case of retrograde rotation or release it to Earth in the case of superrotation. Averaging over longer time scales, no exchange of AAM with the solid Earth takes place. Earth and atmosphere are decoupled. This implies that the ground level zonal wind-component responsible for rigid rotation must be zero on the average. Indeed, the observed meridional structure of the climatic mean zonal wind on the ground shows westerly winds (from the west) in middle latitudes beyond about ± 30 latitude and easterly winds (from the east) in low latitudes—the trade winds—as well as near the poles\n(prevailing winds).\nThe atmosphere picks up angular momentum from Earth at low and high latitudes and transfers the same amount to Earth at middle latitudes.\n\nAny short term fluctuation of the rigidly rotating zonal wind-component is then accompanied by a corresponding change in LOD. In order to estimate the order of magnitude of that effect, one may consider the total atmosphere to rotate rigidly with velocity \"u\" (in m/s) without surface friction. Then this value is related to the corresponding change of the length of day Δ\"\" (in milliseconds) as\n\nThe annual component of the change of the length of day of Δ' ≃ 0.34 ms corresponds then to a superrotation of \"u\" ≃ 0.9 m/s, and the semiannual component of Δ' ≃ 0.29 ms to \"u\" ≃ 0.8 m/s.\n"}
{"id": "19873073", "url": "https://en.wikipedia.org/wiki?curid=19873073", "title": "Defensible space (fire control)", "text": "Defensible space (fire control)\n\nA defensible space, in the context of fire control, is a natural and/or landscaped area around a structure that has been maintained and designed to reduce fire danger. The practice is sometimes called firescaping. \"Defensible space\" is also used in the context of wildfires, especially in the wildland-urban interface (WUI). This defensible space reduces the risk that fire will spread from one area to another, or to a structure, and provides firefighters access and a safer area from which to defend a threatened area. Firefighters sometimes do not attempt to protect structures without adequate defensible space, as it is less safe and less likely to succeed.\n\n\nThe term defensible space in landscape (\"firescape\") use refers to the zone surrounding a structure. Often the location is in the wildland–urban interface. This area need not be devoid of vegetation by using naturally fire resistive plants that are spaced, pruned and trimmed, and irrigated, to minimize the fuel mass available to ignite and also to hamper the spread of a fire.\n\n\nAn important component is ongoing maintenance of the fire-resistant landscaping for reduced fuel loads and fire fighting access. Fire resistive plants that are not maintained can desiccate, die, or amass deadwood debris, and become fire assistive. Irrigation systems and pruning can help maintain a plant's fire resistance. Maintaining access roads and driveways clear of side and low-hanging vegetation can allow large fire equipment to reach properties and structures.\nSome agencies recommend clearing combustible vegetation at minimum horizontal 10 ft from roads and driveways a vertical of 13 ft 6 inches above them. Considering the plant material involved is important to not create unintended consequences to habitat integrity and unnecessary aesthetic issues. Street signs, and homes clearly identified with the numerical address, assist access also.\n\nThe unintended negative consequences of erosion and native habitat loss can result from some unskillful defensible space applications. The disturbance of the soil surface, such as garden soil cultivation in and firebreaks beyond native landscape zones areas, destroys the native plant cover and exposes open soil, accelerating invasive species of plants (\"invasive exotics\") spreading and replacing native habitats.\n\nIn suburban and wildland–urban interface areas, the vegetation clearance and brush removal ordinances of municipalities for defensible space can result in mistaken excessive clearcutting of native and non-invasive introduced shrubs and perennials that exposes the soil to more light and less competition for invasive plant species, and also to erosion and landslides. Negative aesthetic consequences to natural and landscaped areas can be minimized with integrated and balanced defensible space practices.\n\n\n"}
{"id": "55895027", "url": "https://en.wikipedia.org/wiki?curid=55895027", "title": "Disposable soma theory of aging", "text": "Disposable soma theory of aging\n\nThe disposable soma theory of aging states that organisms age due to an evolutionary trade-off between growth, reproduction, and DNA repair maintenance. Formulated by Thomas Kirkwood, the disposable soma theory explains that an organism only has a limited amount of resources or \"soma\" that it can allocate to its various cellular processes. Therefore, a greater investment in growth and reproduction would result in reduced investment in DNA repair maintenance, leading to increased cellular damage, shortened telomeres, accumulation of mutations, compromised stem cells, and ultimately, senescence. Although many models, both animal and human, have appeared to support this theory, parts of it are still controversial. \nSpecifically, while the evolutionary trade-off between growth and aging has been well established, \nthe relationship between reproduction and aging is still without scientific consensus, and the cellular mechanisms largely undiscovered.\n\nBritish biologist Thomas Kirkwood first proposed the disposable soma theory of aging in a 1977 \"Nature\" review article. The theory was inspired by Leslie Orgel's Error Catastrophe Theory of Aging, which was published fourteen years earlier, in 1963. Orgel believed that the process of aging arose due to mutations acquired during the replication process, and Kirkwood developed the disposable soma theory in order to mediate Orgel's work with evolutionary genetics.\n\nThe disposable soma theory of aging acts on the premise that there is a tradeoff in resource allocation between somatic maintenance and reproductive investment. Too low an investment in self-repair would be evolutionarily unsound, as the organism would likely die before reproductive age. However, too high an investment in self-repair would also be evolutionarily unsound due to the fact that one's offspring would likely die before reproductive age. Therefore, there is a compromise and resources are partitioned accordingly. However, this compromise is thought to damage somatic repair systems, which can lead to progressive cellular damage and senescence. Repair costs can be categorized into three groups: (1) the costs of increased durability of nonrenewable parts; (2) the costs of maintenance involving cell renewal, and (3) the costs of intracellular maintenance. In a nutshell, aging and decline is essentially a tradeoff for increased reproductive robustness in youth.\n\nMuch research has been done on the antagonistic effects of increased growth on lifespan. Specifically, the hormone insulin-like growth factor 1 (IGF-1), binds to a cell receptor, leading to a phosphorylation cascade. This cascade results in kinases phosphorylating forkhead transcription factor (FOXO), deactivating it. Deactivation of FOXO results in an inability to express genes involved in responding to oxidative stress response, such as antioxidants, chaperones, and heat-shock proteins. \nAdditionally, uptake of IGF-1 stimulates the mTOR pathway, which activates protein synthesis (and therefore growth) through upregulation of the translation-promoting S6K1, and also inhibits autophagy, a process necessary for recycling of damaged cellular products. Decline of autophagy causes neurodegeneration, protein aggregation and premature aging. Lastly, studies have also indicated that the mTOR pathway also alters immune responses and stimulates cyclin-dependent kinase (CDK) inhibitors such as p16 and p21. This leads to alteration of the stem-cell niche and results in stem cell exhaustion, another theorized mechanism of aging.\n\nThe mechanism of why reproduction inhibits lifespan with regards to multicellular organisms is still unclear. Although many models do illustrate an inverse relationship, and the theory makes sense from an evolutionary perspective, the cellular mechanisms have yet to be explored. However, with regards to cellular replication, the progressive shortening of telomeres is a mechanism which limits the amount of generations of a single cell may undergo. Furthermore, in unicellular organisms like \"Saccharomyces cerevisiae\", the formation of extrachromosomal rDNA circles (ERCs) in mother cells (but not daughter cells) upon every subsequent division is an identifiable type of DNA damage that is associated with replication. These ERCs accumulate over time and eventually trigger replicative senescence and death of the mother cell.\n\nThere is a large body of evidence indicating the negative effects of growth on longevity across many species. As a general rule, individuals of a smaller size generally live longer than larger individuals of the same species.\n\nIn dwarf models of mice, such Snell or Ames mice, mutations have arisen, either rendering them incapable of producing IGF-1 or unable to have adequate receptors for IGF-1 uptake. Furthermore, mice injected with growth hormone have been shown to have progressive weight loss, roughing of the coat, curvature of the spine, enlargement of the organs, kidney lesions and increased cancer risk. This effect is also seen in different breeds of dogs, where smaller breeds of dogs typically live significantly longer compared to their larger counterparts. Selectively bred for their small size, smaller dog breeds like the Chihuahua (average lifespan of 15–20 years) have the B/B genotype for the IGF-1 haplotype, reducing the amount of IGF-1 produced. Conversely, large dogs like the Great Dane (average lifespan of 6–8 years) are homozygous for the IGF-1 I allele, which increases the amount of IGF-1 production.\n\nInitially, it was believed that growth hormone actually prolonged lifespan due to a 1990 study that indicated that injection of growth hormone to men over 60 years of age appeared to reverse various biomarkers implicated in aging, such as decreased muscle mass, bone density, skin thickness, and increased adipose tissue. However, a 1999 study found that administering growth hormone also significantly increased mortality rate. Recent genomic studies have confirmed that the genes involved in growth hormone uptake and signaling are largely conserved across a plethora of species, such as yeast, nematodes, fruit flies, mice and humans. These studies have also shown that individuals with Laron syndrome, an autosomal recessive disorder resulting in dwarfism due to defects in growth hormone receptors, have increased lifespan. Additionally, these individuals have much lower incidences of age-related diseases such as type 2 diabetes and cancer. Lastly, human centenarians around the world are disproportionately of short stature, and have low levels of IGF-1.\n\nNumerous studies have found that lifespan is inversely correlated with both the total amount of offspring birthed, as well as the age at which females first gives birth, also known as primiparity. Additionally, it has been found that reproduction is a costly mechanism that alters the metabolism of fat. Lipids invested in reproduction would be unable to be allocated to support mechanisms involved in somatic maintenance.\n\nThe disposable soma theory has been consistent with the majority of animal models. It was found in numerous animal studies that castration or genetic deformities of reproduction organs was correlated with increased lifespan. Moreover, in red squirrels, it was found that females with an early primiparity achieved the highest immediate and lifetime reproductive success. However, it was also found that these same individuals had a decreased median and maximum lifespan. Specifically squirrels who mated earlier had a 22.4% rate of mortality until two years of age compared to a 16.5% rate of mortality in late breeders. In addition, these squirrels had an average maximum lifespan of 1035 days compared to an average maximum lifespan of 1245 days for squirrels that bred later.\n\nIn another study, researchers selectively bred fruit flies over three years to develop two different strains, an early-reproducing strain and a late-reproducing strain. The late-reproducing line had a significantly longer lifespan than the early-reproducing line. Even more telling was that when the researchers introduced a mutation in the ovarian-associated gene \"ovoD1\", resulting in defective oogenesis, the differences in lifespan between the two lines disappeared. The researchers in this case concluded that \"aging has evolved primarily because of the damaging effects of reproduction earlier in life\".\n\nProminent aging researcher Steven Austad also performed a large-scale ecological study on the coast of Georgia in 1993. Austad isolated two opossum populations, one from the predator-infested mainland and one from the predator-absent nearby island of Sapelo. According to the disposable soma theory, a genetically isolated population subject to low environmentally-induced mortality would evolve delayed reproduction and aging. This is because without the pressure of predation, it would be evolutionarily advantageous to allocate more resources to somatic maintenance than reproduction, as early offspring mortality would be low. As predicted, even after controlling for predation, the isolated population had a longer lifespan, delayed primiparity, and reduced aging biomarkers such as tail collagen cross-linking.\n\nIn general, only a few studies exist in human models. It was found that castrated men live longer than their fertile counterparts. Further studies found that in British women, primiparity was earliest in women who died early and latest in women who died at the oldest ages. Furthermore, increased number of children birthed was associated with a decreased lifespan. A final study found that female centenarians were more likely to have children in later life compared average, especially past the age of 40. The researchers discovered that 19.2% of female centenarians had their first child after the age of 40, compared to 5.5% of the rest of the female population.\n\nThere are numerous studies that support cellular damage, often due to a lack of somatic maintenance mechanisms, as a primary determinant for aging, and these studies have given rise to the free radical theory of aging and the DNA damage theory of aging. One study found that the cells of short-living rodents \"in vitro\" show much greater mutation rates and a general lack of genome surveillance compared to human cells and are far more susceptible to oxidative stress. \nOther studies have been conducted on the naked mole rat, a rodent species with remarkable longevity (30 years), capable of outliving the brown rat (3 years) by ten-fold. Additionally, almost no incidence cancer has ever been detected in naked mole rats. Nearly all of the differences found between these two organisms, which are otherwise rather genetically similar, was in somatic maintenance. Naked mole rats were found to have higher levels of superoxide dismutase, a reactive oxygen species clearing antioxidant. In addition, naked mole rats had higher levels of base excision repair, DNA damage response signaling, homologous recombination repair, mismatch repair, nucleotide excision repair, and non-homologous end joining. In fact, many of these processes were near or exceeded human levels. Proteins from naked mole rats were also more resistant to oxidation, misfolding, ubiquitination, and had increased translational fidelity.\n\nFurther studies have been conducted on patients with Hutchinson-Gilford Progeria Syndrome (HGPS), a condition that leads to premature aging. Patients with HGPS typically age about seven times faster than average and usually succumb to the disease in their early teens. Patients with HGPS have cellular defects, specifically in the lamin proteins, which regulate the organization of the lamina and nuclear envelope for mitosis. \nLastly, as mentioned previously, it has been found that the suppression of autophagy is associated with reduced lifespan, while stimulation is associated with extended lifespan. Activated in times of caloric restriction, autophagy is a process that prevents cellular damage through clearance and recycling of damaged proteins and organelles.\n\nOne of the main weaknesses of the disposable soma theory is that it does not postulate any specific cellular mechanisms to which an organism shifts energy to somatic repair over reproduction. Instead, it only offers an evolutionary perspective on why aging may occur due to reproduction. Therefore, parts of it are rather limited outside of the field of evolutionary biology.\n\nCritics have pointed out the supposed inconsistencies of the disposable soma theory due to the observed effects of caloric restriction, which is correlated with increased lifespan. Although it activates autophagy, according to classical disposable soma principles, with less caloric intake, there would less total energy to be distributed towards somatic maintenance, and decreased lifespan would be observed (or at least the positive autophagic effects would be balanced out). However, Kirkwood, alongside his collaborator Darryl P. Shanley, assert that caloric restriction triggers an adaptive mechanism which causes the organism to shift a higher proportion of resources to somatic maintenance, away from reproduction. This theory is supported by multiple studies, which show that caloric restriction typically results in impaired fertility, but leave an otherwise healthy organism. Evolutionarily, an organism would want to delay reproduction to when resources were more plentiful. During a resource-barren period, it would evolutionarily unwise to invest resources in progeny that would be unlikely to survive in famine. Mechanistically, the NAD-dependent deacetylase Sirtuin 1 (SIRT-1) is upregulated during low-nutrient periods. SIRT-1 increases insulin sensitivity, decreases the amount of inflammatory cytokines, stimulates autophagy, and activates FOXO, the aforementioned protein involved in activating stress response genes. SIRT-1 is also found to result in decreased fertility.\n\nIn additional to differential partitioning of energy allocation during caloric restriction, less caloric intake would result in less metabolic waste in the forms of free radicals like hydrogen peroxide, superoxide and hydroxyl radicals, which damage important cellular components, particularly mitochondria. Elevated levels of free radicals in mice has been correlated with neurodegeneration, myocardial injury, severe anemia, and premature death.\n\nAnother primary criticism of the disposable soma theory is that it fails to account for why women tend to live longer than their male counterparts. Afterall, females invest significantly more resources into reproduction and according to the classical disposable soma principles, this would compromise energy diverted to somatic maintenance. However, this can be reconciled with the grandmother hypothesis. The Grandmother Hypothesis states that menopause comes about into older women in order to restrict the time of reproduction as a protective mechanism. This would allow women to live longer and increase the amount of care they could provide to their grandchildren, increasing their evolutionary fitness. And so, although women do invest a greater proportion of resources into reproduction during their fertile years, their overall reproductive period is significantly shorter than men, who are able of reproduction during and even beyond middle age. Additionally, males invest more resources into growth, and have significantly higher levels of IGF-1 compared to females, which is correlated with decreased lifespan. Other variables such as increased testosterone levels in males are not accounted for. Increased testosterone is often associated with reckless behaviour, which may lead to a high accidental death rate.\n\nA few contradicting animal models weaken the validity of the disposable soma theory. This includes studies done on the aforementioned naked mole rats. In these studies, it was found that reproductive naked mole rats actually show significantly increased lifespans compared to non-reproductive individuals, which contradicts the principles of diposable soma. However, although these naked mole rats are mammalian, they are highly atypical in terms of aging studies and may not serve as the best model for humans. For example, naked mole rats have a disproportionately high longevity quotient and live in eusocial societies, where breeding is usually designated to a queen.\n\nThe disposable soma theory is tested disproportionately on female organisms for the relationship between reproduction and aging, as females carry a greater burden in reproduction. Additionally, for the relationship between growth and aging, studies are disproportionately conducted on males, to minimize the hormonal fluctuations that occur with menstrual cycling. Lastly, genetic and environmental factors, rather than reproductive patterns, may explain the variations in human lifespan. For example, studies have shown that poorer individuals, to whom nutritious food and medical care is less accessible, typically have higher birth rates and earlier primiparity.\n\n\n"}
{"id": "29247528", "url": "https://en.wikipedia.org/wiki?curid=29247528", "title": "Earth's shadow", "text": "Earth's shadow\n\nEarth's shadow or Earth shadow is the shadow that Earth itself casts onto its atmosphere and into outer space, toward the antisolar point. During twilight (both early dusk and late dawn), the shadow's visible fringe (sometimes called the dark segment or twilight wedge) appears in a clear sky as a dark and diffused band low above the horizon.\n\nEarth's shadow cast onto the atmosphere can be viewed during the \"civil\" stage of twilight, assuming the sky is clear and the horizon is relatively unobstructed. The shadow's fringe appears as a dark bluish to purplish band that stretches over 180° of the horizon opposite the Sun, i.e. in the eastern sky at dusk and in the western sky at dawn. Before sunrise, Earth's shadow appears to recede as the Sun rises; after sunset, the shadow appears to rise as the Sun sets.\n\nEarth's shadow is best seen when the horizon is low, such as over the sea, and when the sky conditions are clear. In addition, the higher the observer's elevation is to view the horizon, the sharper the shadow appears.\n\nA related phenomenon in the same part of the sky is the Belt of Venus, or anti-twilight arch, a pinkish band visible above the bluish shade of Earth's shadow, named after the planet Venus which, when visible, is typically located in this region of the sky.\nNo defined line divides the Earth's shadow and the Belt of Venus; one colored band blends into the other in the sky.\n\nThe Belt of Venus is quite a different phenomenon from the afterglow, which appears in the geometrically opposite part of the sky.\n\nWhen the Sun is near the horizon around sunset or sunrise, the sunlight appears reddish. This is because the light rays are penetrating an especially thick layer of the atmosphere, which works as a filter, scattering all but the longer (redder) wavelengths. \n\nFrom the observer's perspective, the red sunlight directly illuminates small particles in the lower atmosphere in the sky opposite of the Sun. The red light is backscattered to the observer, which is the reason why the Belt of Venus appears pink. \n\nThe lower the setting Sun descends, the less defined the boundary between Earth's shadow and the Belt of Venus appears. This is because the setting Sun now illuminates a thinner part of the upper atmosphere. There the red light is not scattered because fewer particles are present, and the eye only sees the \"normal\" (usual) blue sky, which is due to Rayleigh scattering from air molecules. Eventually, both Earth's shadow and the Belt of Venus dissolve into the darkness of the night sky.\n\nEarth's shadow is as curved as the planet is, and its umbra extends into outer space. (The antumbra, however, extends indefinitely.) When the Sun, Earth, and the Moon are aligned perfectly (or nearly so), with Earth between the Sun and the Moon, Earth's shadow falls onto the lunar surface facing the night side of the planet, such that the shadow gradually darkens the full Moon, causing a lunar eclipse. \n\nEven during a total lunar eclipse, a small amount of sunlight however still reaches the Moon. This indirect sunlight has been refracted as it passed through Earth's atmosphere. The air molecules and particulates in Earth's atmosphere scatter the shorter wavelengths of this sunlight; thus, the longer wavelengths of reddish light reaches the Moon, in the same way that light at sunset or sunrise appears reddish. This weak red illumination gives the eclipsed Moon a dimly reddish or copper color.\n\n\n"}
{"id": "52275835", "url": "https://en.wikipedia.org/wiki?curid=52275835", "title": "Earth symbol", "text": "Earth symbol\n\nA variety of symbols or iconographic conventions are used to represent Earth, either in the sense of planet Earth, or the inhabited world. Representations of the globe of Earth, either with an indication of the shape of the continents or with a representation of meridians and parallels remains a common pictographic convention to express the notion of \"worldwide, global\". The modern astronomical symbol for Earth as a planet uses either a stylized globus cruciger () or a circle with a cross (representing the equator and one meridian; ().\n\nThe earliest type of symbols are allegories, personifications or deifications, mostly in the form of an Earth goddess (in the case of Egyptian mythology a god, Geb).\n\nThe classical element \"Earth\" is represented by the trigram of three broken lines in the I Ching ( U+2637). The Western (early modern) alchemical symbol is a downward-pointing triangle bisected by a horizontal line.\nOther symbols for the earth in alchemy or mysticism include the square and the serpent.\n\nIn the Roman period, the globe, a representation of the spherical Earth, became the main symbol representing the concept.\nThe globe depicted the \"universe\" (pictured as the celestial sphere) as well as the Earth.\nThe globus cruciger is the globe surmounted by a Christian cross, held by Byzantine Emperors on the one hand to represent the Christian ecumene, on the other hand the Akakia represented the mortal nature of all men.\nIn the medieval period, the known world was also represented by the T-and-O figure, representing an extremely simplified world map of the three classical continents of the Old World, viz. \"Asia, Europe\" and \"Africa\".\nUnicode has introduced a four characters representing the \"globe\" in the Miscellaneous Symbols and Pictographs block:\n\n"}
{"id": "44689684", "url": "https://en.wikipedia.org/wiki?curid=44689684", "title": "Energy informatics", "text": "Energy informatics\n\nEnergy Informatics is founded on flow networks that are the major suppliers and consumers of energy. Their efficiency can be improved by collecting and analyzing information.\nEnergy informatics is a research field covering the use of information and communication technology to address energy challenges. Methods used for \"smart\" implementations often combine sensors with artificial intelligence and machine learning.\n\nThe field among other consider application areas within:\n\n\n"}
{"id": "514231", "url": "https://en.wikipedia.org/wiki?curid=514231", "title": "Frequency-dependent selection", "text": "Frequency-dependent selection\n\nFrequency-dependent selection is an evolutionary process by which the fitness of a phenotype depends on its frequency relative to other phenotypes in a given population.\n\n\nFrequency-dependent selection is usually the result of interactions between species (predation, parasitism, or competition), or between genotypes within species (usually competitive or symbiotic), and has been especially frequently discussed with relation to anti-predator adaptations. Frequency-dependent selection can lead to polymorphic equilibria, which result from interactions among genotypes within species, in the same way that multi-species equilibria require interactions between species in competition (e.g. where \"α\" parameters in Lotka-Volterra competition equations are non-zero).\n\nThe first explicit statement of frequency-dependent selection appears to have been by Edward Bagnall Poulton in 1884, on the way that predators could maintain color polymorphisms in their prey.\n\nPerhaps the best known early modern statement of the principle is Bryan Clarke's 1962 paper on apostatic selection (a synonym of negative frequency-dependent selection). Clarke discussed predator attacks on polymorphic British snails, citing Luuk Tinbergen's classic work on searching images as support that predators such as birds tended to specialize in common forms of palatable species. Clarke later argued that frequency-dependent balancing selection could explain molecular polymorphisms (often in the absence of heterosis) in opposition to the neutral theory of molecular evolution.\n\nAnother example is plant self-incompatibility alleles. When two plants share the same incompatibility allele, they are unable to mate. Thus, a plant with a new (and therefore, rare) allele has more success at mating, and its allele spreads quickly through the population .\n\nIn human pathogens, such as the flu virus, once a particular strain has become common, most individuals have developed an immune response to that strain. But a rare, novel strain of the flu virus is able to spread quickly to almost any individual, causing continual evolution of viral strains.\n\nThe major histocompatibility complex (MHC) is involved in the recognition of foreign antigens and cells. Frequency-dependent selection may explain the high degree of polymorphism in the MHC.\n\nIn behavioral ecology, negative frequency-dependent selection often maintains multiple behavioral strategies within a species. A classic example is the Hawk-Dove model of interactions among individuals in a population. In a population with two traits A and B, being one form is better when most members are the other form. As another example, male common side-blotched lizards have three morphs, which either defend large territories and maintain large harems of females, defend smaller territories and keep one female, or mimic females in order to sneak matings from the other two morphs. These three morphs participate in a rock paper scissors sort of interaction such that no one morph completely outcompetes the other two. Another example occurs in the scaly-breasted munia, where certain individuals become scroungers and others become producers.\n\nPositive frequency-dependent selection gives an advantage to common phenotypes. A good example is warning coloration in aposematic species. Predators are more likely to remember a common color pattern that they have already encountered frequently than one that is rare. This means that new mutants or migrants that have color patterns other than the common type are eliminated from the population by differential predation. Positive frequency-dependent selection provides the basis for Müllerian mimicry, as described by Fritz Müller , because all species involved are aposematic and share the benefit of a common, honest signal to potential predators.\n\nAnother, rather complicated example occurs in the Batesian mimicry complex between a harmless mimic, the scarlet kingsnake (\"Lampropeltis elapsoides\"), and the model, the eastern coral snake (\"Micrurus fulvius\"), in locations where the model and mimic were in deep sympatry, the phenotype of the scarlet kingsnake was quite variable due to relaxed selection. But where the pattern was rare, the predator population was not 'educated', so the pattern brought no benefit. The scarlet kingsnake was much less variable on the allopatry/sympatry border of the model and mimic, most probably due to increased selection since the eastern coral snake is rare, but present, on this border. Therefore, the coloration is only advantageous once it has become common.\n\n\n"}
{"id": "180236", "url": "https://en.wikipedia.org/wiki?curid=180236", "title": "Greisen–Zatsepin–Kuzmin limit", "text": "Greisen–Zatsepin–Kuzmin limit\n\nThe Greisen–Zatsepin–Kuzmin limit (GZK limit) is a theoretical upper limit on the energy of cosmic ray protons traveling from other galaxies through the intergalactic medium to our galaxy. The limit is , or about 8 joules. The limit is set by slowing-interactions of the protons with the microwave background radiation over long distances (~160 million light-years). The limit is at the same order of magnitude as the upper limit for energy at which cosmic rays have experimentally been detected. For example, one extreme-energy cosmic ray has been detected which appeared to possess a record (50 joules) of energy (about the same as the kinetic energy of a 35 mph baseball).\n\nThe GZK limit is derived under the assumption that ultra-high energy cosmic rays are protons. Measurements by the largest cosmic-ray observatory, the Pierre Auger Observatory, suggest that most ultra-high energy cosmic rays are heavier elements. In this case, the argument behind the GZK limit does not apply in the originally simple form and there is no fundamental contradiction in observing cosmic rays with energies that violate the limit.\n\nIn the past, the apparent violation of the GZK limit has inspired cosmologists and theoretical physicists to suggest other ways that circumvent the limit. These theories propose that ultra-high energy cosmic rays are produced nearby our galaxy or that Lorentz covariance is violated in such a way that protons do not lose energy on their way to our galaxy.\n\nThe limit was independently computed in 1966 by Kenneth Greisen, Vadim Kuzmin, and Georgiy Zatsepin, based on interactions between cosmic rays and the photons of the cosmic microwave background radiation (CMB). They predicted that cosmic rays with energies over the threshold energy of would interact with cosmic microwave background photons formula_1, relatively blueshifted by the speed of the cosmic rays, to produce pions via the formula_2 resonance,\n\nor\n\nPions produced in this manner proceed to decay in the standard pion channels—ultimately to photons for neutral pions, and photons, positrons, and various neutrinos for positive pions. Neutrons decay also to similar products, so that ultimately the energy of any cosmic ray proton is drained off by production of high energy photons plus (in some cases) high energy electron/positron pairs and neutrino pairs.\n\nThe pion production process begins at a higher energy than ordinary electron-positron pair production (lepton production) from protons impacting the CMB, which starts at cosmic ray proton energies of only about . However, pion production events drain 20% of the energy of a cosmic ray proton as compared with only 0.1% of its energy for electron positron pair production. This factor of 200 is from two sources: the pion has only about ~130 times the mass of the leptons, but the extra energy appears as different kinetic energies of the pion or leptons, and results in relatively more kinetic energy transferred to a heavier product pion, in order to conserve momentum. The much larger total energy losses from pion production result in the pion production process becoming the limiting one to high energy cosmic ray travel, rather than the lower-energy light-lepton production process.\n\nThe pion production process continues until the cosmic ray energy falls below the pion production threshold. Due to the mean path associated with this interaction, extragalactic cosmic rays traveling over distances larger than () and with energies greater than this threshold should never be observed on Earth. This distance is also known as GZK horizon.\n\nA number of observations have been made by the largest cosmic ray experiments Akeno Giant Air Shower Array, High Resolution Fly's Eye Cosmic Ray Detector, the Pierre Auger Observatory and Telescope Array Project that appeared to show cosmic rays with energies above this limit (called extreme-energy cosmic rays, or EECRs). The observation of these particles was the so-called GZK paradox or cosmic ray paradox.\n\nThese observations appear to contradict the predictions of special relativity and particle physics as they are presently understood. However, there are a number of possible explanations for these observations that may resolve this inconsistency.\n\nAnother suggestion involves ultra-high energy weakly interacting particles (for instance, neutrinos) which might be created at great distances and later react locally to give rise to the particles observed. In the proposed Z-burst model, an ultra-high energy cosmic neutrino collides with a relic anti-neutrino in our galaxy and annihilates to hadrons. This process proceeds via a (virtual) Z-boson:\n\nformula_8\n\nThe cross section for this process becomes large if the center of mass energy of the neutrino antineutrino pair is equal to the Z-boson mass (such a peak in the cross section is called \"resonance\"). Assuming that the relic anti-neutrino is at rest, the energy of the incident cosmic neutrino has to be:\n\nformula_9\n\nwhere formula_10 is the mass of the Z-boson and formula_11 the mass of the neutrino.\n\nA number of exotic theories have been advanced to explain the AGASA observations, including doubly special relativity. However, it is now established that standard doubly special relativity does not predict any GZK suppression (or GZK cutoff), contrary to models of Lorentz symmetry violation involving an absolute rest frame. Other possible theories involve a relation with dark matter, decays of exotic super-heavy particles beyond those known in the Standard Model.\n\nA suppression of the cosmic ray flux which can be explained with the GZK limit has been confirmed by the latest generation of cosmic ray observatories. A former claim by the AGASA experiment that there is no suppression was overruled. It remains controversial, whether the suppression is due to the GZK effect. The GZK limit only applies if ultra-high energy cosmic rays are mostly protons.\n\nIn July 2007, during the 30th International Cosmic Ray Conference in Mérida, Yucatán, México, the High Resolution Fly's Eye Experiment (HiRes) and the Pierre Auger Observatory (Auger) presented their results on ultra-high-energy cosmic rays. HiRes observed a suppression in the UHECR spectrum at just the right energy, observing only 13 events with an energy above the threshold, while expecting 43 with no suppression. This was interpreted as the first observation of the GZK limit. Auger confirmed the flux suppression, but did not claim it to be the GZK limit: instead of the 30 events necessary to confirm the AGASA results, Auger saw only two, which are believed to be heavy nuclei events. The flux suppression was previously brought into question when the AGASA experiment found no suppression in their spectrum. According to Alan Watson, spokesperson for the Auger Collaboration, AGASA results have been shown to be incorrect, possibly due to the systematic shift in energy assignment.\n\nIn 2010 and the following years, both the Pierre Auger Observatory and HiRes confirmed again a flux suppression, in case of the Pierre Auger Observatory the effect is statistically significant at the level of 20 standard deviations.\n\nAfter the flux suppression was established, a heated debate ensued whether cosmic rays that violate the GZK limit are protons. The Pierre Auger Observatory, the world's largest observatory, found with high statistical significance that ultra-high energy cosmic rays are not purely protons, but a mixture of elements which is getting heavier with increasing energy.\nThe Telescope Array Project, a joint effort from members of the HiRes and AGASA collaborations, agrees with the former HiRes result that these cosmic rays look like protons. The claim is based on data with lower statistical significance, however. The area covered by Telescope Array is about one third of the area covered by the Pierre Auger Observatory, and the latter has been running for a longer time.\n\nThe controversy was partially resolved in 2017, when a joint working group formed by members of both experiments presented a report at the 35th International Cosmic Ray Conference. According to the report, the raw experimental results are not in contradiction with each other. The different interpretations are mainly based on the use of different theoretical models (Telescope Array uses an outdated model for its interpretation), and the fact that Telescope Array has not collected enough events yet to distinguish the pure proton hypothesis from the mixed-nuclei hypothesis.\n\nEUSO, which was scheduled to fly on the International Space Station (ISS) in 2009, was designed to use the atmospheric-fluorescence technique to monitor a huge area and boost the statistics of UHECRs considerably. EUSO is to make a deep survey of UHECR-induced extensive air showers (EASs) from space, extending the measured energy spectrum well beyond the GZK-cutoff. It is to search for the origin of UHECRs, determine the nature of the origin of UHECRs, make an all-sky survey of the arrival direction of UHECRs, and seek to open the astronomical window on the extreme-energy universe with neutrinos. The fate of the EUSO Observatory is still unclear since NASA is considering early retirement of the ISS.\n\nLaunched in June 2008, the Fermi Gamma-ray Space Telescope (formerly GLAST) will also provide data that will help resolve these inconsistencies.\n\nIn November 2007, researchers at the Pierre Auger Observatory announced that they had evidence that UHECRs appear to come from the active galactic nuclei (AGNs) of energetic galaxies powered by matter swirling onto a supermassive black hole. The cosmic rays were detected and traced back to the AGNs using the Véron-Cetty-Véron catalog. These results are reported in the journal \"Science\". Nevertheless, the strength of the correlation with AGNs from this particular catalog for the Auger data recorded after 2007 has been slowly diminishing.\n\n"}
{"id": "2169038", "url": "https://en.wikipedia.org/wiki?curid=2169038", "title": "Homochirality", "text": "Homochirality\n\nHomochirality is a uniformity of chirality, or handedness. Objects are \"chiral\" when they cannot be superposed on their mirror images. For example, the left and right hands of a human are approximately mirror images of each other but are not their own mirror images, so they are \"chiral\". In biology, 19 of the 20 natural amino acids are homochiral, being -chiral (left-handed), while sugars are -chiral (right-handed). \"Homochirality\" can also refer to \"enantiomerically pure\" substances in which all the constituents are the same enantiomer (a right-handed or left-handed version of an atom or molecule), but some sources discourage this use of the term.\n\nIt is unclear whether homochirality has a purpose, however, it appears to be a form of information storage. One suggestion is that it reduces entropy barriers in the formation of large organized molecules. It has been experimentally verified that amino acids form large aggregates in larger abundance from enantiopure substrates than from racemic ones.\n\nIt is not clear whether homochirality emerged before or after life, and many mechanisms for its origin have been proposed. Some of these models propose three distinct steps: \"mirror-symmetry breaking\" creates a minute enantiomeric imbalance, \"chiral amplification\" builds on this imbalance, and \"chiral transmission\" is the transfer of chirality from one set of molecules to another.\n\nAmino acids are the building blocks of peptides and enzymes while sugar-peptide chains are the backbone of RNA and DNA. In biological organisms, amino acids appear almost exclusively in the left-handed form (-amino acids) and sugars in the right-handed form (R-sugars). Since the enzymes catalyze reactions, they enforce homochirality on a great variety of other chemicals, including hormones, toxins, fragrances and food flavors. Glycine is achiral, as are some other non-proteinogenic amino acids are either achiral (such as dimethylglycine) or of the enantiomeric form.\n\nBiological organisms easily discriminate between molecules with different chiralities. This can affect physiological reactions such as smell and taste. Carvone, a terpenoid found in essential oils, smells like mint in its L-form and caraway in its R-form. Limonene tastes like lemons when right-handed and oranges when left-handed.\n\nHomochirality also affects the response to drugs. Thalidomide, in its left-handed form, cures morning sickness; in its right-handed form, it causes birth defects. Unfortunately, even if a pure left-handed version is administered, some of it can convert to the right-handed form in the patient. Many drugs are available as both a racemic mixture (equal amounts of both chiralities) and an enantiopure drug (only one chirality). Depending on the manufacturing process, enantiopure forms can be more expensive to produce than stereochemical mixtures.\n\nChiral preferences can also be found at a macroscopic level. Snail shells can be right-turning or left-turning helices, but one form or the other is strongly preferred in a given species. In the edible snail \"Helix pomatia\", only one out of 20,000 is left-helical. The coiling of plants can have a preferred chirality and even the chewing motion of cows has a 10% excess in one direction.\n\nKnown mechanisms for the production of non-racemic mixtures from racemic starting materials include: asymmetric physical laws, such as the electroweak interaction; asymmetric environments, such as those caused by circularly polarized light, quartz crystals, or the Earth's rotation; and statistical fluctuations during racemic synthesis. Once established, chirality would be selected for. A small enantiomeric excess can be amplified into a large one by asymmetric autocatalysis, such as in the Soai reaction. In asymmetric autocatalysis, the catalyst is a chiral molecule, which means that a chiral molecule is catalysing its own production. An initial enantiomeric excess, such as can be produced by polarized light, then allows the more abundant enantiomer to outcompete the other.\n\nOne supposition is that the discovery of an enantiomeric imbalance in molecules in the Murchison meteorite supports an extraterrestrial origin of homochirality: there is evidence for the existence of circularly polarized light originating from Mie scattering on aligned interstellar dust particles which may trigger the formation of an enantiomeric excess within chiral material in space. Interstellar and near-stellar magnetic fields can align dust particles in this fashion. Another speculation (the Vester-Ulbricht hypothesis) suggests that fundamental chirality of physical processes such as that of the beta decay (see Parity violation) leads to slightly different half-lives of biologically relevant molecules. Homochirality may also result from spontaneous absolute asymmetric synthesis.\n\nIt is also possible that homochirality is simply a result of the natural autoamplification process of life—that either the formation of life as preferring one chirality or the other was a chance rare event which happened to occur with the chiralities we observe, or that all chiralities of life emerged rapidly but due to catastrophic events and strong competition, the other unobserved chiral preferences were wiped out by the preponderance and metabolic, enantiomeric enrichment from the 'winning' chirality choices. The emergence of chirality consensus as a natural autoamplification process has been associated with the 2nd law of thermodynamics.\n\nIn 1953, Charles Frank proposed a model to demonstrate that homochirality is a consequence of autocatalysis. In his model the and enantiomers of a chiral molecule are autocatalytically produced from an achiral molecule A\n\nwhile suppressing each other through a reaction that he called \"mutual antagonism\"\nIn this model the racemic state is unstable in the sense that the slightest enantiomeric excess will be amplified to a completely homochiral state. This can be shown by computing the reaction rates from the law of mass action:\nwhere formula_2 is the rate constant for the autocatalytic reactions, formula_3 is the rate constant for mutual antagonism reaction, and the concentration of A is kept constant for simplicity. By defining the enantiomeric excess formula_4 as\nwe can compute the rate of change of enatiomeric excess using chain rule from the rate of change of the concentrations of enantiomeres and .\nLinear stability analysis of this equation shows that the racemic state formula_7 is unstable. Starting from almost everywhere in the concentration space, the system evolves to a homochiral state.\n\nIt is generally understood that autocatalysis alone does not yield to homochirality, and the presence of the mutually antagonistic relationship between the two enantiomers is necessary for the instability of the racemic mixture. However, recent studies show that homochirality could be achieved from autocatalysis in the absence of the mutually antagonistic relationship, but the underlying mechanism for symmetry-breaking is different.\n\nThere are several laboratory experiments that demonstrate how a small amount of one enantiomer at the start of a reaction can lead to a large excess of a single enantiomer as the product. For example, the Soai reaction is autocatalytic. If the reaction is started with some of one of the product enantiomers already present, the product acts as an enantioselective catalyst for production of more of that same enantiomer. The initial presence of just 0.2 equivalent one enantiomer can lead to up to 93% enantiomeric excess of the product.\n\nAnother study concerns the proline catalyzed aminoxylation of propionaldehyde by nitrosobenzene. In this system, a small enantiomeric excess of catalyst leads to a large enantiomeric excess of product.\n\nSerine octamer clusters are also contenders. These clusters of 8 serine molecules appear in mass spectrometry with an unusual homochiral preference, however there is no evidence that such clusters exist under non-ionizing conditions and amino acid phase behavior is far more prebiotically relevant. The recent observation that partial sublimation of a 10% enantioenriched sample of leucine results in up to 82% enrichment in the sublimate shows that enantioenrichment of amino acids could occur in space. Partial sublimation processes can take place on the surface of meteors where large variations in temperature exist. This finding may have consequences for the development of the Mars Organic Detector scheduled for launch in 2013 which aims to recover trace amounts of amino acids from the Mars surface exactly by a sublimation technique.\n\nA high asymmetric amplification of the enantiomeric excess of sugars are also present in the amino acid catalyzed asymmetric formation of carbohydrates\n\nOne classic study involves an experiment that takes place in the laboratory. When sodium chlorate is allowed to crystallize from water and the collected crystals examined in a polarimeter, each crystal turns out to be chiral and either the form or the form. In an ordinary experiment the amount of crystals collected equals the amount of crystals (corrected for statistical effects). However, when the sodium chlorate solution is stirred during the crystallization process the crystals are either exclusively or exclusively . In 32 consecutive crystallization experiments 14 experiments deliver -crystals and 18 others -crystals. The explanation for this symmetry breaking is unclear but is related to autocatalysis taking place in the nucleation process.\n\nIn a related experiment, a crystal suspension of a racemic amino acid derivative continuously stirred, results in a 100% crystal phase of one of the enantiomers because the enantiomeric pair is able to equilibrate in solution (compare with dynamic kinetic resolution).\n\nMany strategies in asymmetric synthesis are built on chiral transmission. Especially important is the so-called organocatalysis of organic reactions by proline for example in Mannich reactions.\n\nThere exists no theory elucidating correlations among -amino acids. If one takes, for example, alanine, which has a small methyl group, and phenylalanine, which has a larger benzyl group, a simple question is in what aspect, -alanine resembles -phenylalanine more than -phenylalanine, and what kind of mechanism causes the selection of all -amino acids. Because it might be possible that alanine was and phenylalanine was .\n\nIt was reported in 2004 that excess racemic ,-asparagine (Asn), which spontaneously forms crystals of either isomer during recrystallization, induces asymmetric resolution of a co-existing racemic amino acid such as arginine (Arg), aspartic acid (Asp), glutamine (Gln), histidine (His), leucine (Leu), methionine (Met), phenylalanine (Phe), serine (Ser), valine (Val), tyrosine (Tyr), and tryptophan (Trp). The enantiomeric excess of these amino acids was correlated almost linearly with that of the inducer, i.e., Asn. When recrystallizations from a mixture of 12 ,-amino acids (Ala, Asp, Arg, Glu, Gln, His, Leu, Met, Ser, Val, Phe, and Tyr) and excess ,-Asn were made, all amino acids with the same configuration with Asn were preferentially co-crystallized. It was incidental whether the enrichment took place in - or -Asn, however, once the selection was made, the co-existing amino acid with the same configuration at the α-carbon was preferentially involved because of thermodynamic stability in the crystal formation. The maximal ee was reported to be 100%. Based on these results, it is proposed that a mixture of racemic amino acids causes spontaneous and effective optical resolution, even if asymmetric synthesis of a single amino acid does not occur without an aid of an optically active molecule.\n\nThis is the first study elucidating reasonably the formation of chirality from racemic amino acids with experimental evidences.\n\nThis term was introduced by Kelvin in 1904, the year that he published his Baltimore Lecture of 1884. Kelvin used the term homochirality as a relationship between two molecules, i.e. two molecule are homochiral if they have the same chirality. Recently, however, homochiral has been used in the same sense as enantiomerically pure. This is permitted in some journals (but not encouraged), its meaning changing into the preference of a process or system for a single optical isomer in a pair of isomers in these journals.\n\n\n"}
{"id": "41856558", "url": "https://en.wikipedia.org/wiki?curid=41856558", "title": "Incomplete Nature", "text": "Incomplete Nature\n\nIncomplete Nature: How Mind Emerged from Matter is a 2011 book by biological anthropologist Terrence Deacon. The book covers topics in biosemiotics, philosophy of mind, and the origins of life. Broadly, the book seeks to naturalistically explain \"aboutness\", that is, concepts like intentionality, meaning, normativity, purpose, and function; which Deacon groups together and labels as ententional phenomena.\n\nDeacon's first book, \"The Symbolic Species\" focused on the evolution of human language. In that book, Deacon notes that much of the mystery surrounding language origins comes from a profound confusion on the nature of semiotic processes themselves. Accordingly, the focus of \"Incomplete Nature\" shifts from human origins to the origin of life and semiosis. \"Incomplete Nature\" can be viewed as a sizable contribution to the growing body of work positing that the problem of consciousness and the problem of the origin of life are inexorably linked. Deacon tackles these two linked problems by going back to basics. The book expands upon the classical conceptions of work and information in order to give an account of ententionality that is consistent with eliminative materialism and yet does not seek to explain away or pass off as epiphenominal the non-physical properties of life.\n\nA central thesis of the book is that absence can still be efficacious. Deacon makes the claim that just as the concept of zero revolutionized mathematics, thinking about life, mind, and other ententional phenomena in terms of constraints (i.e., what is absent) can similarly help us overcome the artificial dichotomy of the mind body problem. A good example of this concept is the hole that defines the hub of a wagon wheel. The hole itself is not a physical thing, but rather a source of constraint that helps to restrict the conformational possibilities of the wheel's components, such that, on a global scale, the property of rolling emerges. Constraints which produce emergent phenomena may not be a process which can be understood by looking at the make-up of the constituents of a pattern. Emergent phenomena are difficult to study because their complexity does not necessarily decompose into parts. When a pattern is broken down, the constraints are no longer at work; there is no hole, no absence to notice. Imagine a hub, a hole for an axle, produced only when the wheel is rolling, thus breaking the wheel may not show you how the hub emerges.\n\nDeacon notes that the apparent patterns of causality exhibited by living systems seem to be in some ways the inverse of the causal patterns of non-living systems. In an attempt to find a solution to the philosophical problems associated with teleological explanations, Deacon returns to Aristotle's four causes and attempts to modernize them with thermodynamic concepts.\n\nOrthograde changes are caused internally. They are spontaneous changes. That is, orthograde changes are generated by the spontaneous elimination of asymmetries in a thermodynamic system in disequilibrium. Because orthograde changes are driven by the internal geometry of a changing system, orthograde causes can be seen as analogous to Aristotle's formal cause. More loosely, Aristotle's final cause can also be considered orthograde, because goal oriented actions are caused from within.\n\nContragrade changes are imposed from the outside. They are non-spontaneous changes. Contragrade change is induced when one thermodynamic system interacts with the orthograde changes of another thermodynamic system. The interaction drives the first system into a higher energy, more asymmetrical state. Contragrade changes do work. Because contragrade changes are driven by external interactions with another changing system, contragrade causes can be seen as analogous to Aristotle's efficient cause.\n\nMuch of the book is devoted to expanding upon the ideas of classical thermodynamics, with an extended discussion about how consistently far from equilibrium systems can interact and combine to produce novel emergent properties. \nDeacon defines three hierarchically nested levels of thermodynamic systems: Homeodynamic systems combine to produce morphodynamic systems which combine to produce teleodynamic systems. Teleodynamic systems can be further combined to produce higher orders of self organization.\n\nHomeodynamic systems are essentially equivalent to classical thermodynamic systems like a gas under pressure or solute in solution, but the term serves to emphasize that homeodynamics is an abstract process that can be realized in forms beyond the scope of classic thermodynamics. For example, the diffuse brain activity normally associated with emotional states can be considered to be a homeodynamic system because there is a general state of equilibrium which its components (neural activity) distribute towards. In general, a homeodynamic system is any collection of components that will spontaneously eliminate constraints by rearranging the parts until a maximum entropy state (disorderliness) is achieved.\n\nA morphodynamic system consists of a coupling of two homeodynamic systems such that the constraint dissipation of each complements the other, producing macroscopic order out of microscopic interactions. Morphodynamic systems require constant perturbation to maintain their structure, so they are relatively rare in nature. The paradigm example of a morphodynamic system is a Rayleigh–Bénard cell. Other common examples are snowflake formation, whirlpools and the stimulated emission of laser light.\nMaximum entropy production: The organized structure of a morphodynamic system forms to facilitate maximal entropy production. In the case of a Rayleigh–Bénard cell, heat at the base of the liquid produces an uneven distribution of high energy molecules which will tend to diffuse towards the surface. As the temperature of the heat source increases, density effects come into play. Simple diffusion can no longer dissipate energy as fast as it is added and so the bottom of the liquid becomes hot and more buoyant than the cooler, denser liquid at the top. The bottom of the liquid begins to rise, and the top begins to sink - producing convection currents.\n\nTwo systems: The significant heat differential on the liquid produces two homeodynamic systems. The first is a diffusion system, where high energy molecules on the bottom collide with lower energy molecules on the top until the added kinetic energy from the heat source is evenly distributed. The second is a convection system, where the low density fluid on the bottom mixes with the high density fluid on the top until the density becomes evenly distributed. The second system arises when there is too much energy to be effectively dissipated by the first, and once both systems are in place, they will begin to interact.\n\nSelf organization: The convection creates currents in the fluid that disrupt the pattern of heat diffusion from bottom to top. Heat begins to diffuse into the denser areas of current, irrespective of the vertical location of these denser portions of fluid. The areas of the fluid where diffusion is occurring most rapidly will be the most viscous because molecules are rubbing against each other in opposite directions. The convection currents will shun these areas in favor of parts of the fluid where they can flow more easily. And so the fluid spontaneously segregates itself into cells where high energy, low density fluid flows up from the center of the cell and cooler, denser fluid flows down along the edges, with diffusion effects dominating in the area between the center and the edge of each cell.\n\nSynergy and constraint: What is notable about morphodynamic processes is that order spontaneously emerges explicitly because the ordered system that results is more efficient at increasing entropy than a chaotic one. In the case of the Rayleigh–Bénard cell, neither diffusion nor convection on their own will produce as much entropy as both effects coupled together. When both effects are brought into interaction, they constrain each other into a particular geometric form because that form facilitates minimal interference between the two processes. The orderly hexagonal form is stable as long as the energy differential persists, and yet the orderly form more effectively degrades the energy differential than any other form. This is why morphodynamic processes in nature are usually so short lived. They are self organizing, but also self undermining.\n\nA teleodynamic system consists of coupling two morphodynamic systems such that the self undermining quality of each is constrained by the other. Each system prevents the other from dissipating all of the energy available, and so long term organizational stability is obtained. Deacon claims that we should pinpoint the moment when two morphodynamic systems reciprocally constrain each other as the point when ententional qualities like function, purpose and normativity emerge.\n\nDeacon explores the properties of teleodynamic systems by describing a chemically plausible model system called an autogen. Deacon emphasizes that the specific autogen he describes is not a proposed description of the first life form, but rather a description of the kinds of thermodynamic synergies that the first living creature likely possessed.\n\nReciprocal catalysis: An autogen consists of two self catalyzing cyclical morphodynamic chemical reactions, similar to a chemoton. In one reaction, organic molecules react in a looped series, the products of one reaction becoming the reactants for the next. This looped reaction is self amplifying, producing more and more reactants until all the substrate is consumed. A side product of this reciprocally catalytic loop is a lipid that can be used as a reactant in a second reaction. This second reaction creates a boundary (either a microtubule or some other closed capsid like structure), that serves to contain the first reaction. The boundary limits diffusion; it keeps all of the necessary catalysts in close proximity to each other. In addition, the boundary prevents the first reaction from completely consuming all of the available substrate in the environment.\n\nThe first self: Unlike an isolated morphodynamic process whose organization rapidly eliminates the energy gradient necessary to maintain its structure, a teleodynamic process is self-limiting and self preserving. The two reactions complement each other, and ensure that neither ever runs to equilibrium - that is completion, cessation, and death. So, in a teleodynamic system there will be structures that embody a preliminary sketch of a biological function. The internal reaction network functions to create the substrates for the boundary reaction, and the boundary reaction functions to protect and constrain the internal reaction network. Either process in isolation would be abiotic but together they create a system with a normative status dependent on the functioning of its component parts.\n\nAs with other concepts in the book, in his discussion of work Deacon seeks to generalize the Newtonian conception of work such that the term can be used to describe and differentiate mental phenomena - to describe \"that which makes daydreaming effortless but metabolically equivalent problem solving difficult.\" Work is generally described as \"activity that is necessary to overcome resistance to change. Resistance can be either active or passive, and so work can be directed towards enacting change that wouldn't otherwise occur or preventing change that would happen in its absence.\" Using the terminology developed earlier in the book, work can be considered to be \"the organization of differences between orthograde processes such that a locus of contragrade process is created. Or, more simply, work is a spontaneous change inducing a non-spontaneous change to occur.\"\n\nA thermodynamic systems capacity to do work depends less upon the total energy of the system and more upon the geometric distribution of its components. A glass of water at 20 degrees Celsius will have the same amount of energy as a glass divided in half with the top fluid at 30 degrees and the bottom at 10, but only in the second glass will the top half have the capacity to do work upon the bottom. This is because work occurs at both macroscopic and microscopic levels. Microscopically, there is constant work being performed on one molecule by another when they collide. But the potential for this microscopic work to additively sum to macroscopic work depends on there being an asymmetric distribution of particle speeds, so that the average collision pushes in a focused direction. Microscopic work is necessary but not sufficient for macroscopic work. A global property of asymmetric distribution is also required.\n\nBy recognizing that asymmetry is a general property of work - that work is done as asymmetric systems spontaneously tend towards symmetry, Deacon abstracts the concept of work and applies it to systems whose symmetries are vastly more complex than those covered by classical thermodynamics. In a morphodynamic system, the tendency towards symmetry produces not global equilibrium, but a complex geometric form like a hexagonal Benard cell or the resonant frequency of a flute. This tendency towards convolutedly symmetric forms can be harnessed to do work on other morphodynamic systems, if the systems are properly coupled.\n\nResonance example: A good example of morphodynamic work is the induced resonance that can be observed by singing or playing a flute next to a string instrument like a harp or guitar. The vibrating air emitted from the flute will interact with the taut strings. If any of the strings are tuned to a resonant frequency that matches the note being played, they too will begin to vibrate and emit sound.\n\nContragrade change: When energy is added to the flute by blowing air into it, there is a spontaneous (orthograde) tendency for the system to dissipate the added energy by inducing the air within the flute to vibrate at a specific frequency. This orthograde morphodynamic form generation can be used to induce contragrade change in the system coupled to it - the taught string. Playing the flute does work on the string by causing it to enter a high energy state that could not be reached spontaneously in an uncoupled state.\n\nStructure and form: Importantly, this is not just the macro scale propagation of random micro vibrations from one system to another. The global geometric structure of the system is essential. The total energy transferred from the flute to the string matters far less than the patterns it takes in transit. That is, the amplitude of the coupled note is irrelevant, what matters is its frequency. Notes that have a higher or lower frequency than the resonant frequency of the string will not be able to do morphodynamic work.\n\nWork is generally defined to be the interaction of two orthograde changing systems such that contragrade change is produced. In teleodynamic systems, the spontaneous orthograde tendency is not to equilibriate (as in homeodynamic systems), nor to self simplify (as in morphodynamic systems) but rather to tend towards self-preservation. Living organisms spontaneously tend to heal, to reproduce and to pursue resources towards these ends. Teleodynamic work acts on these tendencies and pushes them in a contragrade, non-spontaneous direction. \nEvolution as work: Natural selection, or perhaps more accurately, adaptation, can be considered to be a ubiquitous form of teleodynamic work. The othograde self-preservation and reproduction tendencies of individual organisms tends to undermine those same tendencies in conspecifics. This competition produces a constraint that tends to mold organisms into forms that are more adapted to their environments – forms that would otherwise not spontaneously persist.\n\nFor example, in a population of New Zealand wrybill who make a living by searching for grubs under rocks, those that have a bent beak gain access to more calories. Those with bent beaks are able to better provide for their young, and at the same time they remove a disproportionate quantity of grubs from their environment, making it more difficult for those with straight beaks to provide for their own young. Throughout their lives, all the wrybills in the population do work to structure the form of the next generation. The increased efficiency of the bent beak causes that morphology to dominate the next generation. Thus an asymmetry of beak shape distribution is produced in the population - an asymmetry produced by teleodynamic work.\n\nThought as work: Mental problem solving can also be considered teleodynamic work. Thought forms are spontaneously generated, and task of problem solving is the task of molding those forms to fit the context of the problem at hand. Deacon makes the link between evolution as teleodynamic work and thought as teleodynamic work explicit. \"The experience of being sentient is what it feels like to \"be\" evolution.\"\n\nBy conceiving of work in this way, Deacon claims \"we can begin to discern \"a basis for a form of causal openness\" in the universe.\" While increases in complexity in no way alter the laws of physics, by juxtaposing systems together, pathways of spontaneous change can be made available that were inconceivably improbable prior to the systems coupling. The causal power of any complex living system lies not solely in the underlying quantum mechanics but also in the global arrangement of its components. A careful arrangement of parts can constrain possibilities such that phenomena that were formerly impossibly rare can become improbably common.\n\nOne of the central purposes of Incomplete Nature is to articulate a theory of biological information. The first formal theory of information was articulated by Claude Shannon in 1948 in his work A Mathematical Theory of Communication. Shannon's work is widely credited with ushering in the information age, but somewhat paradoxically, it was completely silent on questions of meaning and reference, i.e., what the information is \"about.\" As an engineer, Shannon was concerned with the challenge of reliably transmitting a message from one location to another. The meaning and content of the message was largely irrelevant. So, while Shannon information theory has been essential for the development of devices like computers, it has left open many philosophical questions regarding the nature of information. Incomplete Nature seeks to answer some of these questions.\n\nShannon's key insight was to recognize a link between entropy and information. Entropy is often defined as a measurement of disorder, or randomness, but this can be misleading. For Shannon's purposes, the entropy of a system is the number of possible states that the system has the capacity to be in. Any one of these potential states can constitute a message. For example, a typewritten page can bear as many different messages as there are different combinations of characters that can be arranged on the page. The information content of a message can only be understood against the background context of all of the messages that could have been sent, but weren't. Information is produced by a reduction of entropy in the message medium. \nShannon's information based conception of entropy should be distinguished from the more classic thermodynamic conception of entropy developed by Ludwig Boltzmann and others at the end of the nineteenth century. While Shannon entropy is static and has to do with the set of all possible messages/states that a signal bearing system might take, Boltzmann entropy has to do with the tendency of all dynamic systems to tend towards equilibrium. That is, there are many more ways for a collection of particles to be well mixed than to be segregated based on velocity, mass, or any other property. Boltzmann entropy is central to the theory of work developed earlier in the book because entropy dictates the direction in which a system will spontaneously tend.\n\nDeacon's addition to Shannon information theory is to propose a method for describing not just how a message is transmitted, but also how it is interpreted. Deacon weaves together Shannon entropy and Boltzmann entropy in order to develop a theory of interpretation based in teleodynamic work. Interpretation is inherently normative. Data becomes information when it has significance for its interpreter. Thus interpretive systems are teleodynamic - the interpretive process is designed to perpetuate itself. \"The interpretation of something as information indirectly reinforces the capacity to do this again.\"\n\n"}
{"id": "53384839", "url": "https://en.wikipedia.org/wiki?curid=53384839", "title": "International Union for Vacuum Science, Technique and Applications", "text": "International Union for Vacuum Science, Technique and Applications\n\nThe International Union for Vacuum Science, Technique, and Applications (IUVSTA) is a union of 33 science and technology national member societies whose role is to stimulate international collaboration in the fields of vacuum science, technique and applications, and related multi-disciplinary topics.\n\nIUVSTA is a Member Scientific Associate of the International Council for Science (ICSU).\n\nFounded in 1958, IUVSTA is an interdisciplinary union which represents several thousands of physicists, chemists, materials scientists, engineers and technologists who are active in basic and applied research, development, manufacturing, sales and education. IUVSTA finances advanced scientific workshops, international schools and technical courses, worldwide.\n\nIUVSTA comprises member societies from the following countries:\nArgentina, Australia, Austria, Belgium, Brazil, Bulgaria, China, Croatia, Czech Republic, Finland, France, Germany, Hungary, India, Israel, Iran, Italy, Japan, Korea, Mexico, Netherlands, Pakistan, Philippines, Poland, Portugal, Russian Federation, Slovakia, Slovenia, Spain, Sweden, Switzerland, United Kingdom, and USA.\n\nThe main purposes of the IUVSTA are to organize and sponsor international conferences and educational activities, as well as to facilitate research and technological developments in the field of vacuum science and its applications.\n\nThe history and structure of the Union are described in two articles in scientific journals.\n\nIUVSTA has nine technical divisions:\n\n\n"}
{"id": "41566158", "url": "https://en.wikipedia.org/wiki?curid=41566158", "title": "Janet Lembke", "text": "Janet Lembke\n\nJanet Lembke (2 March 1933 – 3 September 2013), \"née\" Janet Nutt, was an American author, essayist, naturalist, translator and scholar. She was born in Cleveland, Ohio during the Great Depression, graduated in 1953 from Middlebury College, Vermont, with a degree in Classics, and her knowledge of the classical Greek and Latin worldview, from Homer to Virgil, informed her life and work. A Certified Virginia Master Gardener, she lived in Virginia and North Carolina, drawing inspiration from both locales. She was recognized for her creative view of natural cycles, agriculture and of animals, both domestic and wild, with whom we share the natural environment. Referred to as an \"acclaimed Southern naturalist,\" she was equally (as The Chicago Tribune described her) a \"classicist, a noted Oxford University Press translator of the works of Sophocles, Euripides and Aeschylus\". She received a grant from the National Endowment for the Arts to translate Virgil's Georgics, having already translated Euripides' \"Electra\" and \"Hecuba\", and Aeschylus's \"Persians\" and \"Suppliants\".\n\nJanet Lembke's first book was \"Bronze and Iron: Old Latin Poetry from Its Beginnings to 100 B.C.\" (1973), but beyond translations and essays about classics, there were more than a dozen books on nature, works for which the author acquired a base of admirers. Her articles were printed in The New York Times, \"Sierra Magazine\" (The Sierra Club), Oxford American, Audubon, Raleigh News and Observer, Southern Review and other publications. The writing style was eclectic and personal, meditative and detailed, and though she was at least once accused of \"taking poetic license too far\" in her translation of \"Georgics\", readers were often charmed and seduced by her way of weaving scientific fact, history and culture, with personal anecdote, mythological allusion and poetic feeling. \"The author's ability to pull together disparate elements in her writing is impressive, and her passionate connection with the natural world is displayed in line after line,\" wrote The New York Times. Novelist Annie Proulx expressed a similar perception, observing that \"Lembke's writing tacks between three points: the stuff of her late-twentieth-century life; the tangle of creature and plant in every dimension of tide and river flow; and the haunting, connecting wires of mythos that still knot us to the ancient beginnings.\"\n\nAmong Janet Lembke's noted titles were \"Because the Cat Purrs: How We Relate to Other Species and Why It Matters\" (2008); \"Skinny Dipping: And Other Immersions in Water, Myth, and Being Human\" (2004); \"Dangerous Birds\" (1996); \"River Time\" (1997); \"Despicable Species: On Cowbirds, Kudzu, Hornworms, and Other Scourges\" (1999); and \"The Quality of Life: Living Well, Dying Well\" (2004)-- a sober and unflinching account of the death of the author's mother. At the time of her own death at age 80 in Staunton, Virginia, Janet Lembke was working on a memoir, \"I Married an Arsonist\". She had married twice, and had four children and six grandchildren.\n\nThere is a repository of archived materials (\"The Janet Lembke Papers, 1966 - 2008\"), including notes and correspondence by the author, at the Jackson Library of the University of North Carolina in Greensboro, NC.\n\n\n\n\n"}
{"id": "144551", "url": "https://en.wikipedia.org/wiki?curid=144551", "title": "Knapping", "text": "Knapping\n\nKnapping is the shaping of flint, chert, obsidian or other conchoidal fracturing stone through the process of lithic reduction to manufacture stone tools, strikers for flintlock firearms, or to produce flat-faced stones for building or facing walls, and flushwork decoration. The original Germanic term \"knopp\" meant strike, shape, or work, so it could theoretically have referred equally well to making a statue or dice. Modern usage is more specific, referring almost exclusively to the hand-tool pressure-flaking process pictured.\n\nFlintknapping or knapping is done in a variety of ways depending on the purpose of the final product. For stone tools and flintlock strikers, chert is worked using a fabricator such as a hammerstone to remove lithic flakes from a nucleus or core of tool stone. Stone tools can then be further refined using wood, bone, and antler tools to perform pressure flaking.\n\nFor building work a hammer or pick is used to split chert nodules supported on the lap. Often the chert nodule will be split in half to create two cherts with a flat circular face for use in walls constructed of lime. More sophisticated knapping is employed to produce near-perfect cubes which are used as bricks.\n\nThere are many different methods of shaping stone into useful tools. Early knappers could have used simple hammers made of wood or antler to shape stone tools. The factors that contribute to the knapping results are varied, but the EPA (exterior platform angle) indeed influences many attributes, such as length, thickness and termination of flakes.\n\n\"Hard hammer\" techniques are used to remove large flakes of stone. Early knappers and hobbyists replicating their methods often use cobbles of very hard stone, such as quartzite. This technique can be used by flintknappers to remove broad flakes that can be made into smaller tools. This method of manufacture is believed to have been used to make some of the earliest stone tools ever found, some of which date from over 2 million years ago.\n\n\"Soft hammer\" techniques are more precise than hard hammer methods of shaping stone. Soft hammer techniques allow a knapper to shape a stone into many different kinds of cutting, scraping, and projectile tools. These \"soft hammer\" techniques also produce longer, thinner flakes, potentially allowing for material conservation or a lighter lithic tool kit to be carried by mobile societies.\n\n\"Pressure flaking\" involves removing narrow flakes along the edge of a stone tool. This technique is often used to do detailed thinning and shaping of a stone tool. Pressure flaking involves putting a large amount of force across a region on the edge of the tool and (hopefully) causing a narrow flake to come off of the stone. Modern hobbyists often use pressure flaking tools with a copper or brass tip, but early knappers could have used antler tines or a pointed wooden punch; traditionalist knappers still use antler tines and copper-tipped tools. The major advantage of using soft metals rather than wood or bone is that the metal punches wear down less and are less likely to break under pressure.\n\nIn cultures that have not adopted metalworking technologies, the production of stone tools by knappers is common, but in modern cultures the making of such tools is the domain of experimental archaeologists and hobbyists. Archaeologists usually undertake the task so that they can better understand how prehistoric stone tools were made.\n\nKnapping is often learned by outdoorsmen.\n\nKnapping \"gun flints\", used by flintlock firearms was formerly a major industry in flint bearing locations, such as Brandon in Suffolk, England and the small towns of Meusnes and Couffy in France. Meusnes has a small museum dedicated to the industry.\n\nIn 1804, during the Napoleonic Wars, Brandon was supplying over 400,000 flints a month for use by the British Army and Navy. Brandon knappers made gun flints for export to Africa as late as the 1960s.\n\nKnapping for building purposes is still a skill that is practiced in the flint-bearing regions of southern England, such as Sussex, Suffolk and Norfolk, and in northern France, especially Brittany and Normandy, where there is a resurgence of the craft due to government funding.\n\nHistorically, flint knappers commonly suffered from silicosis, due to the inhalation of flint dust. This has been called \"the world's first industrial disease\".\n\nWhen gun flint knapping was a large-scale industry in Brandon, silicosis was widely known as \"knappers' rot\". It has been claimed silicosis was responsible for the early death of three-quarters of Brandon gun flint makers. In one workshop, seven of the eight workmen died of the condition before the age of fifty.\n\nModern knappers are advised to work in the open air to reduce the dust hazard, and to wear eye and hand protection. Some modern knappers wear a respirator to guard against dust.\n\nModern American interest in knapping can be traced back to the study of a California Native American called Ishi who lived in the early twentieth century. Ishi taught scholars and academics traditional methods of making stone tools and how to use them for survival in the wild. Early European explorers to the New world were also exposed to flint knapping techniques. Additionally, several pioneering nineteenth-century European experimental knappers are also known and in the late 1960s and early 1970s experimental archaeologist Don Crabtree published texts such as \"Experiments in Flintworking\". François Bordes was an early writer on Old World knapping; he experimented with ways to replicate stone tools found across Western Europe. These authors helped to ignite a small craze in knapping among archaeologists and prehistorians.\n\nEnglish archaeologist Phil Harding is another contemporary expert, whose exposure on the television series Time Team has led to him being a familiar figure in the UK and beyond. Many groups, with members from all walks of life, can now be found across the United States and Europe. These organizations continue to demonstrate and teach various ways of shaping stone tools.\n\n"}
{"id": "8746727", "url": "https://en.wikipedia.org/wiki?curid=8746727", "title": "Level of support for evolution", "text": "Level of support for evolution\n\nThe level of support for evolution among scientists, the public and other groups is a topic that frequently arises in the creation-evolution controversy and touches on educational, religious, philosophical, scientific and political issues. The subject is especially contentious in countries where significant levels of non-acceptance of evolution by general society exist although evolution is taught at school and university.\n\nNearly all (around 97%) of the scientific community accepts evolution as the dominant scientific theory of biological diversity. Scientific associations have strongly rebutted and refuted the challenges to evolution proposed by intelligent design proponents.\n\nThere are religious sects and denominations in several countries for whom the theory of evolution is in conflict with creationism that is central to their beliefs, and who therefore reject it: in the United States, South Africa, India, South Korea, Singapore, the Philippines, and Brazil, with smaller followings in the United Kingdom, the Republic of Ireland, Japan, Italy, Germany, Israel, Australia, New Zealand, and Canada.\n\nSeveral publications discuss the subject of acceptance, including a document produced by the United States National Academy of Sciences.\n\nThe vast majority of the scientific community and academia supports evolutionary theory as the only explanation that can fully account for observations in the fields of biology, paleontology, molecular biology, genetics, anthropology, and others. A 1991 Gallup poll found that about 5% of American scientists (including those with training outside biology) identified themselves as creationists.\n\nAdditionally, the scientific community considers intelligent design, a neo-creationist offshoot, to be unscientific, pseudoscience, or junk science. The U.S. National Academy of Sciences has stated that intelligent design \"and other claims of supernatural intervention in the origin of life\" are not science because they cannot be tested by experiment, do not generate any predictions, and propose no new hypotheses of their own. In September 2005, 38 Nobel laureates issued a statement saying \"Intelligent design is fundamentally unscientific; it cannot be tested as scientific theory because its central conclusion is based on belief in the intervention of a supernatural agent.\" In October 2005, a coalition representing more than 70,000 Australian scientists and science teachers issued a statement saying \"intelligent design is not science\" and calling on \"all schools not to teach Intelligent Design (ID) as science, because it fails to qualify on every count as a scientific theory\".\n\nIn 1986, an \"amicus curiae\" brief, signed by 72 US Nobel Prize winners, 17 state academies of science and 7 other scientific societies, asked the US Supreme Court in \"Edwards v. Aguillard\", to reject a Louisiana state law requiring that where evolutionary science was taught in public schools, creation science must also be taught. The brief also stated that the term \"creation science\" as used by the law embodied religious dogma, and that \"teaching religious ideas mislabeled as science is detrimental to scientific education\". This was the largest collection of Nobel Prize winners to sign a petition up to that point. According to anthropologists Almquist and Cronin, the brief is the \"clearest statement by scientists in support of evolution yet produced.\"\n\nThere are many scientific and scholarly organizations from around the world that have issued statements in support of the theory of evolution. The American Association for the Advancement of Science, the world's largest general scientific society with more than 130,000 members and over 262 affiliated societies and academies of science including over 10 million individuals, has made several statements and issued several press releases in support of evolution. The prestigious United States National Academy of Sciences, which provides science advice to the nation, has published several books supporting evolution and criticising creationism and intelligent design.\n\nThere is a notable difference between the opinion of scientists and that of the general public in the United States. A 2009 poll by Pew Research Center found that \"Nearly all scientists (97%) say humans and other living things have evolved over time – 87% say evolution is due to natural processes, such as natural selection. The dominant position among scientists – that living things have evolved due to natural processes – is shared by only about a third (32%) of the public.\"\n\nOne of the earliest resolutions in support of evolution was issued by the American Association for the Advancement of Science in 1922, and readopted in 1929.\n\nAnother early effort to express support for evolution by scientists was organized by Nobel Prize–winning American biologist Hermann J. Muller in 1966. Muller circulated a petition entitled \"Is Biological Evolution a Principle of Nature that has been well established by Science?\" in May 1966:\n\nThis manifesto was signed by 177 of the leading American biologists, including George G. Simpson of Harvard University, Nobel Prize Winner Peter Agre of Duke University, Carl Sagan of Cornell, John Tyler Bonner of Princeton, Nobel Prize Winner George Beadle, President of the University of Chicago, and Donald F. Kennedy of Stanford University, formerly head of the United States Food and Drug Administration.\n\nThis was followed by the passing of a resolution by the American Association for the Advancement of Science (AAAS) in the fall of 1972 that stated, in part, \"the theory of creation ... is neither scientifically grounded nor capable of performing the rules required of science theories\". The United States National Academy of Sciences also passed a similar resolution in the fall of 1972. A statement on evolution called \"A Statement Affirming Evolution as a Principle of Science.\" was signed by Nobel Prize Winner Linus Pauling, Isaac Asimov, George G. Simpson, Caltech Biology Professor Norman H. Horowitz, Ernst Mayr, and others, and published in 1977. The governing board of the American Geological Institute issued a statement supporting resolution in November 1981.\nShortly thereafter, the AAAS passed another resolution supporting evolution and disparaging efforts to teach creationism in science classes.\n\nTo date, there are no scientifically peer-reviewed research articles that disclaim evolution listed in the scientific and medical journal search engine Pubmed.\n\nThe Discovery Institute announced that over 700 scientists had expressed support for intelligent design as of February 8, 2007. This prompted the National Center for Science Education to produce a \"light-hearted\" petition called \"Project Steve\" in support of evolution. Only scientists named \"Steve\" or some variation (such as Stephen, Stephanie, and Stefan) are eligible to sign the petition. It is intended to be a \"tongue-in-cheek parody\" of the lists of alleged \"scientists\" supposedly supporting creationist principles that creationist organizations produce. The petition demonstrates that there are more scientists who accept evolution with a name like \"Steve\" alone (over 1370) than there are in total who support intelligent design. This is, again, why the percentage of scientists who support evolution has been estimated by Brian Alters to be about 99.9 percent.\n\nMany creationists act as evangelists and their organizations are registered as tax-free religious organizations. Creationists have claimed that they represent the interests of true Christians, and evolution is associated only with atheism.\n\nHowever, not all religious organizations find support for evolution incompatible with their religious faith. For example, 12 of the plaintiffs opposing the teaching of creation science in the influential \"McLean v. Arkansas\" court case were clergy representing Methodist, Episcopal, African Methodist Episcopal, Catholic, Southern Baptist, Reform Jewish, and Presbyterian groups. There are several religious organizations that have issued statements advocating the teaching of evolution in public schools. In addition, the Archbishop of Canterbury, Dr. Rowan Williams, issued statements in support of evolution in 2006. The Clergy Letter Project is a signed statement by 12,808 (as of 28 May 2012) American Christian clergy of different denominations rejecting creationism organized in 2004. Molleen Matsumura of the National Center for Science Education found, of Americans in the twelve largest Christian denominations, at least 77% belong to churches that support evolution education (and that at one point, this figure was as high as 89.6%). These religious groups include the Catholic Church, as well as various denominations of Protestantism, including the United Methodist Church, National Baptist Convention, USA, Evangelical Lutheran Church in America, Presbyterian Church (USA), National Baptist Convention of America, African Methodist Episcopal Church, the Episcopal Church, and others. A figure closer to about 71% is presented by the analysis of Walter B. Murfin and David F. Beck.\n\nMichael Shermer argued in Scientific American in October 2006 that evolution supports concepts like family values, avoiding lies, fidelity, moral codes and the rule of law. Shermer also suggests that evolution gives more support to the notion of an omnipotent creator, rather than a tinkerer with limitations based on a human model.\n\nThe Ahmadiyya Movement universally accepts evolution and actively promotes it. Mirza Tahir Ahmad, Fourth Caliph of the Ahmadiyya Muslim Community has stated in his magnum opus \"Revelation, Rationality, Knowledge & Truth\" that evolution did occur but only through God being the One who brings it about. It does not occur itself, according to the Ahmadiyya Muslim Community. The Ahmadis do not believe Adam was the first human on earth, but merely the first prophet to receive a revelation of God.\n\nA fundamental part of `Abdul-Bahá's teachings on evolution is the belief that all life came from the same origin: \"the origin of all material life is one...\" He states that from this sole origin, the complete diversity of life was generated: \"Consider the world of created beings, how varied and diverse they are in species, yet with one sole origin\" He explains that a slow, gradual process led to the development of complex entities:\n\nThe 1950 encyclical \"Humani generis\" advocated scepticism towards evolution without explicitly rejecting it; this was substantially amended by Pope John-Paul II in 1996 in an address to the Pontifical Academy of Sciences in which he said, \"Today, almost half a century after publication of the encyclical, new knowledge has led to the recognition of the theory of evolution as more than a hypothesis.\" Between 2000 and 2002 the International Theological Commission found that \"Converging evidence from many studies in the physical and biological sciences furnishes mounting support for some theory of evolution to account for the development and diversification of life on earth, while controversy continues over the pace and mechanisms of evolution.\" This statement was published by the Vatican on July 2004 by the authority of Cardinal Ratzinger (who became Pope Benedict XVI) who was the president of the Commission at the time.\n\nThe Magisterium has not made an authoritative statement on intelligent design, and has permitted arguments on both sides of the issue. In 2005, Cardinal Christoph Schönborn of Vienna appeared to endorse intelligent design when he denounced philosophically materialist interpretations of evolution. In an op-ed in the New York Times he said \"Evolution in the sense of common ancestry might be true, but evolution in the neo-Darwinian sense - an unguided, unplanned process of random variation and natural selection - is not.\" This common line of reasoning among fundamentalist theologians is flawed, as evolution by natural selection is not random at all; only mutations occur in a stochastic manner, while natural selection establishes genes which aid survival in a particular environment.\n\nIn the January 16–17 2006 edition of the official Vatican newspaper \"L'Osservatore Romano\", University of Bologna evolutionary biology Professor Fiorenzo Facchini wrote an article agreeing with the judge's ruling in \"Kitzmiller v. Dover\" and stating that intelligent design was unscientific. Jesuit Father George Coyne, former director of the Vatican Observatory, has also denounced intelligent design.\n\nHindus believe in the concept of evolution of life on Earth. The concepts of Dashavatara—different incarnations of God starting from simple organisms and progressively becoming complex beings—and Day and Night of Brahma are generally cited as instances of Hindu acceptance of evolution.\n\nIn the United States, many Protestant denominations promote creationism, preach against evolution, and sponsor lectures and debates on the subject. Denominations that explicitly advocate creationism instead of evolution or \"Darwinism\" include the Assemblies of God, the Free Methodist Church, Lutheran Church–Missouri Synod, Pentecostal Churches, Seventh-day Adventist Churches, Wisconsin Evangelical Lutheran Synod, Christian Reformed Church, Southern Baptist Convention, and the Pentecostal Oneness churches. Jehovah's Witnesses produce gap creationism and day-age creationism literature to refute evolution but reject the \"creationist\" label, which they consider to apply only to Young Earth creationism.\n\nA common complaint of creationists is that evolution is of no value, has never been used for anything, and will never be of any use. According to many creationists, nothing would be lost by getting rid of evolution, and science and industry might even benefit.\n\nIn fact, evolution is being put to practical use in industry and widely used on a daily basis by researchers in medicine, biochemistry, molecular biology, and genetics to both formulate hypotheses about biological systems for the purposes of experimental design, as well as to rationalise observed data and prepare applications. As of August 2017 there are 487,558 scientific papers in PubMed that mention 'evolution'. Pharmaceutical companies utilize biological evolution in their development of new products, and also use these medicines to combat evolving bacteria and viruses.\n\nBecause of the perceived value of evolution in applications, there have been some expressions of support for evolution on the part of corporations. In Kansas, there has been some widespread concern in the corporate and academic communities that a move to weaken the teaching of evolution in schools will hurt the state's ability to recruit the best talent, particularly in the biotech industry. Paul Hanle of the Biotechnology Institute warned that the United States risks falling behind in the biotechnology race with other nations if it does not do a better job of teaching evolution. James McCarter of Divergence Incorporated stated that the work of 2001 Nobel Prize winner Leland Hartwell relied heavily on the use of evolutionary knowledge and predictions, both of which have significant implications for the treatment of cancers. Furthermore, McCarter concluded that 47 of the last 50 Nobel Prizes in medicine or physiology depended on an understanding of evolutionary theory (according to McCarter's unspecified personal criteria).\n\nThere are also many educational organizations that have issued statements in support of the theory of evolution.\n\nRepeatedly, creationists and intelligent design advocates have lost suits in US courts. Here is a list of important court cases in which creationists have suffered setbacks:\n\n\nThere does not appear to be significant correlation between believing in evolution and understanding evolutionary science. In some countries, creationist beliefs (or a lack of support for evolutionary theory) are relatively widespread, even garnering a majority of public opinion. A study published in \"Science\" compared attitudes about evolution in the United States, 32 European countries (including Turkey) and Japan. The only country where acceptance of evolution was lower than in the United States was Turkey (25%). Public acceptance of evolution was most widespread (at over 80% of the population) in Iceland, Denmark and Sweden.\nAccording to the PEW research center, Afghanistan has the lowest acceptance of evolution in the Muslim countries. Only 26% of people in Afghanistan accept evolution. 62% deny human evolution and believe that humans have always existed in their present form..\n\nAccording to a 2014 poll produced by the Pew Research Center, 71% of people in Argentina believe \"humans and other living things evolved over time\" while 23% believe they have \"always existed in the present form.\"\nAccording to the PEW research, 56 percent of Armenians deny human evolution & claim that humans have always existed in their present and only 34 percent of Armenians accept human evolution.\n\nA 2009 poll showed that almost a quarter of Australians believe \"the biblical account of human origins\" over the Darwinian account. 42 percent of Australians believe in a \"wholly scientific\" explanation for the origins of life, while 32 percent believe in an evolutionary process \"guided by God\".\n\nA 2010 survey conducted by Auspoll and the Australian Academy of Science found that 79% of Australians believe in evolution (71% believe it is currently occurring, 8% believe in evolution but do not think it is currently occurring), 11% were not sure and 10% stated they do not believe in evolution.\n\nAccording to a 2014 poll by the Pew Research Center, 44% of people in Bolivia believe \"humans and other living things evolved over time\" while 39% believe they have \"always existed in the present form.\"\n\nIn a 2010 poll, 59% of respondents said they believe in theistic evolution, or evolution guided by God. A further 8% believe in evolution without divine intervention, while 25% were creationists. Support for creationism was stronger among the poor and the least educated. According to a 2014 poll produced by the Pew Research Center, 66% of Brazilians agree that humans evolved over time and 29% think they have always existed in the present form.\n\nIn a 2012 poll, 61% of Canadians believe that humans evolved from less advanced life forms, while 22% believe that God created human beings in their present form within the last 10,000 years.\n\nAccording to a 2014 poll by the Pew Research Center, 69% of people in Chile believe \"humans and other living things evolved over time\" while 26% believe they have \"always existed in the present form.\"\n\nAccording to a 2014 poll by the Pew Research Center, 59% of people in Colombia believe \"humans and other living things evolved over time\" while 35% believe they have \"always existed in the present form.\"\n\nAccording to a 2014 poll by the Pew Research Center, 56% of people in Costa Rica believe \"humans and other living things evolved over time\" while 38% believe they have \"always existed in the present form.\"\n\nAccording to the PEW research center,the Czech Republic has the highest acceptance of evolution in Eastern Europe. 83 percent people in the Czech Republic believe that humans evolved over time.\n\nAccording to a 2014 poll by the Pew Research Center, 41% of people in Dominican Republic believe \"humans and other living things evolved over time\" while 56% believe they have \"always existed in the present form.\"\n\nAccording to a 2014 poll by the Pew Research Center, 50% of people in Ecuador believe \"humans and other living things evolved over time\" while 44% believe they have \"always existed in the present form.\"\n\nAccording to a 2014 poll by the Pew Research Center, 46% of people in El Salvador believe \"humans and other living things evolved over time\" while 45% believe they have \"always existed in the present form.\"\n\nAccording to a 2014 poll by the Pew Research Center, 55% of people in Guatemala believe \"humans and other living things evolved over time\" while 38% believe they have \"always existed in the present form.\"\n\nAccording to a 2014 poll by the Pew Research Center, 49% of people in Honduras believe \"humans and other living things evolved over time\" while 45% believe they have \"always existed in the present form.\"\n\nAccording to the PEW research center, Kazakhstan has the highest acceptance of evolution in the Muslim countries. 79% of \npeople in Kazakhstan acceptance the theory of evolution.\n\nAmong those who had heard of Charles Darwin and knew something about the theory of evolution, 77% of people in India agree that enough scientific evidence exists to support Charles Darwin’s Theory of Evolution. Also, 85% of God believing Indians who know about evolution agree that life on earth evolved over time as a result of natural selection.\n\nIn a survey carried among 10 major nations, the highest proportion that agreed that evolutionary theories alone should be taught in schools was in India, at 49%.\n\nIn a recent survey conducted across 12 states in India, public acceptence of evolution stood at 65.5% across Indian population. Highest acceptence was found in Delhi, Maharashtra and Kerala (all above 78%) while the least was found to be in Haryana (41.3%). Males were marginally more likely to accept the evolution compared with females (72% vs. 69%), and non-religious people compared with religious people (74% vs. 67%). Surprisingly people who identified as ‘rightists’ accepted the evolution more than those who identified themselves as ‘leftists’ (66% vs. 61%) in political spectra. The study also identified teachers and students (over 73%) as most likely to accept evolution while employed adults (59%) least. While at the international level, the trend is quite clear that religiosity is inversely proportional to public acceptance of evolution, situation in India was strikingly opposite. Lead author, Dr. Felix Bast from Central University of Punjab conjectured possible reason for high public acceptance of evolution in India despite the fact of high religiosity is that Hinduism does not conflict Darwin’s theory of evolution to a large extent. According to 2011 census, Hindus encompass 80.3% of Indian population. Many concepts of Vedas and Hinduism support the scientific consensus of geology, climate science and evolution to a large extent. For example, according to Rigveda, the age of earth is 1.97 billion years, which is very old compared with that of creation myth propounded by Abrahamic religions (according to creationism-also called Intelligent Design, the age of earth is around 6000 years). Current scientific consensus of the age of earth is 4.543 billion years. A number of evolutionary biologists in the past as well were baffled about the surprising similarity between evolutionary theory and Hinduism. British evolutionary biologist JBS Haldane, for instance, suggested that Hindu concept of \"dashavatara\"- the ten incarnations of lord Vishnu- is a rough idea of vertebrate evolution (fish-the vertebrate to tortoise-reptile to boar-mammal to man). Vedic concepts of \"pralaya\" and \"mahapralaya\" too surprisingly capture the cyclic nature of global climate (glacial-interglacial cycles).\n\nA 2009 survey conducted by the McGill researchers and their international collaborators found that 85% of Indonesian high school students agreed with the statement, \"Millions of fossils show that life has existed for billions of years and changed over time.\"\n\nThe theory of evolution is a 'hard sell' in schools in Israel. More than half of Israeli Jews accept the human evolution while more than 40% deny human evolution & claim that humans have always existed in their present form. \n\nAccording to a 2014 poll by the Pew Research Center, 64% of people in Mexico believe \"humans and other living things evolved over time\" while 32% believe they have \"always existed in the present form.\"\n\nAccording to a 2014 poll by the Pew Research Center, 47% of people in Nicaragua believe \"humans and other living things evolved over time\" while 48% believe they have \"always existed in the present form.\"\n\nAccording to a 2008 Norstat poll for NRK, 59% of the Norwegian population fully accept evolution, 24% somewhat agree with the theory, 4% somewhat disagree with the theory while 8% do not accept evolution. 4% did not know.\n\nA 2009 survey conducted by the McGill researchers and their international collaborators found that 86% of Pakistani high school students agreed with the statement, \"Millions of fossils show that life has existed for billions of years and changed over time.\"\n\nAccording to a 2014 poll by the Pew Research Center, 61% of people in Panama believe \"humans and other living things evolved over time\" while 34% believe they have \"always existed in the present form.\"\n\nAccording to a 2014 poll by the Pew Research Center, 59% of people in Paraguay believe \"humans and other living things evolved over time\" while 30% believe they have \"always existed in the present form.\"\n\nAccording to a 2014 poll by the Pew Research Center, 51% of people in Peru believe \"humans and other living things evolved over time\" while 39% believe they have \"always existed in the present form.\"\n\nA 2006 UK poll on the \"origin and development of life\" asked participants to choose between three different explanations for the origin of life: 22% chose (Young Earth) creationism, 17% opted for intelligent design (\"certain features of living things are best explained by the intervention of a supernatural being, e.g. God\"), 48% selected evolution theory (with a divine role explicitly excluded) and the rest did not know. A 2009 poll found that only 38% of Britons believe God played no role in evolution. In a 2012 poll, 69% of Britons believe that humans evolved from less advanced life forms, while 17% believe that God created human beings in their present forms within the last 10,000 years.\n\nUS courts have ruled in favor of teaching evolution in science classrooms, and against teaching creationism, in numerous cases such as Edwards v. Aguillard, Hendren v. Campbell, McLean v. Arkansas and Kitzmiller v. Dover Area School District.\n\nA prominent organization in the United States behind the intelligent design movement is the Discovery Institute, which, through its Center for Science and Culture, conducts a number of public relations and lobbying campaigns aimed at influencing the public and policy makers in order to advance its position in academia. The Discovery Institute claims that because there is a significant lack of public support for evolution, that public schools should, as their campaign states, \"Teach the Controversy\", although there is no controversy over the validity of evolution within the scientific community.\n\nThe US has one of the highest levels of public belief in biblical or other religious accounts of the origins of life on earth among industrialized countries. However according to the PEW research center, 62 percent of adults in the United States accept human evolution and while 34 percent of adults believe that humans have always existed in their present form. The poll involved over 35,000 adults in the United States. However acceptance of evolution varies per state. For example the State of Vermont has the highest acceptance of evolution of any other State in the United States. 79% people in Vermont accept human evolution. Mississippi has the lowest acceptance of evolution of any other State in the United States.\nA 2017 Gallup creationism survey found that 38% of adults in the United States inclined to the view that \"God created humans in their present form at one time within the last 10,000 years\" when asked for their views on the origin and development of human beings, which was noted as being at the lowest level in 35 years. 19% believed that \"human beings have developed over millions of years from less advanced forms of life, but God had no part in this process\", despite 49% of respondents indicating they believed in evolution. Belief in creationism is inversely correlated to education; only 22% of those with post-graduate degrees believe in strict creationism. (The level of support for strict creationism could be even lower when poll results are adjusted after comparison with other polls with questions that more specifically account for uncertainty and ambivalence.A 2000 poll for People for the American Way found 70% of the American public felt that evolution was compatible with a belief in God.\n\nA 2005 Pew Research Center poll found that 70% of evangelical Christians believed that living organisms have not changed since their creation, but only 31% of Catholics and 32% of mainline Protestants shared this opinion. A 2005 Harris Poll estimated that 63% of liberals and 37% of conservatives agreed that humans and other primates have a common ancestry.\n\nAccording to a 2014 poll produced by the Pew Research Center, 74% of people in Uruguay believe \"humans and other living things evolved over time\" while 20% believe they have \"always existed in the present form.\"\n\nAccording to a 2014 poll by the Pew Research Center, 63% of people in Venezuela believe \"humans and other living things evolved over time\" while 33% believe they have \"always existed in the present form.\"\n\nThe level of assent that evolution garners has changed with time. The trends in acceptance of evolution can be estimated.\n\nThe level of support for evolution in different communities has varied with time. Darwin's theory had convinced almost every naturalist within 20 years of its publication in 1858, and was making serious inroads with the public and the more liberal clergy. It had reached such extremes, that by 1880, one\nAmerican religious weekly publication estimated that \"perhaps a quarter, perhaps a half of the educated ministers in our leading Evangelical denominations\" felt \"that the story of the creation and fall of man, told in Genesis, is no more the record of actual occurrences than is the parable of the Prodigal Son.\"\n\nBy the late 19th century, many of the most conservative Christians accepted an ancient earth, and life on earth before Eden. Victorian Era Creationists were more akin to people who subscribe to theistic evolution today. Even fervent anti-evolutionist Scopes Trial prosecutor William Jennings Bryan interpreted the \"days\" of Genesis as ages of the earth, and acknowledged that biochemical evolution took place, drawing the line only at the story of Adam and Eve's creation. Prominent pre-World War II creationist Harry Rimmer allowed an Old Earth by slipping millions of years into putative gaps in the Genesis account, and claimed that the Noachian Flood was only a local phenomenon.\n\nIn the decades of the 20th century, George McCready Price and a tiny group of Seventh-day Adventist followers were among the very few believers in a Young Earth and a worldwide flood, which Price championed in his \"new catastrophism\" theories. It was not until the publication of John C. Whitcomb, Jr., and Henry M. Morris’s book \"Genesis Flood\" in 1961 that Price's idea was revived. In the last few decades, many creationists have adopted Price's beliefs, becoming progressively more strict biblical literalists.\n\nIn a 1991 Gallup poll, 47% of the US population, and 25% of college graduates agreed with the statement, \"God created man pretty much in his present form at one time within the last 10,000 years.\"\n\nFourteen years\nlater, in 2005, Gallup found that 53% of Americans expressed the belief that \"God created human beings in their present form exactly the way the Bible describes it.\" About 2/3 (65.5%) of those surveyed thought that creationism was definitely or probably true. In 2005 a Newsweek poll discovered that 80 percent of the American public thought that \"God created the universe.\" and the Pew Research Center reported that \"nearly two-thirds of Americans say that creationism should be taught alongside evolution in public schools.\" Ronald Numbers commented on that with \"Most surprising of all was the discovery that large numbers of high-school biology teachers — from 30% in Illinois and 38% in Ohio to a whopping 69% in Kentucky — supported the teaching of creationism.\"\n\nThe National Center for Science Education reports that from 1985 to 2005, the number of Americans unsure about evolution increased from 7% to 21%, while the number rejecting evolution declined from 48% to 39%. Jon Miller of Michigan State University has found in his polls that the number of Americans who accept evolution has declined from 45% to 40% from 1985 to 2005.\n\nIn light of these somewhat contradictory results, it is difficult to know for sure what is happening to public opinion on evolution in the US. It does not appear that either side is making unequivocal progress. It does appear that uncertainty about the issue is increasing, however.\n\nAnecdotal evidence is that creationism is becoming more of an issue in the UK as well. One report in 2006 was that UK students are increasingly arriving ill-prepared to participate in medical studies or other advanced education.\n\nThe level of support for creationism among relevant scientists is minimal. In 2007 the Discovery Institute reported that about 600 scientists signed their \"A Scientific Dissent from Darwinism\" list, up from 100 in 2001. The actual statement of the Scientific Dissent from Darwinism is a relatively mild one that expresses skepticism about the absoluteness of 'Darwinism' (and is in line with the falsifiability required of scientific theories) to explain all features of life, and does not in any way represent an absolute denial or rejection of evolution. By contrast, a tongue-in-cheek response known as Project Steve, a list restricted to scientists named Steve, Stephanie etc. who agree that evolution is \"a vital, well-supported, unifying principle of the biological sciences,\" has 1,382 signatories . People with these names make up approximately 1% of the total U.S. population.\n\nThe United States National Science Foundation statistics on US yearly science graduates demonstrate that from 1987 to 2001, the number of biological science graduates increased by 59% while the number of geological science graduates decreased by 20.5%. However, the number of geology graduates in 2001 was only 5.4% of the number of graduates in the biological sciences, while it was 10.7% of the number of biological science graduates in 1987. The Science Resources Statistics Division of the National Science Foundation estimated that in 1999, there were 955,300 biological scientists in the US (about 1/3 of who hold graduate degrees). There were also 152,800 earth scientists in the US as well.\n\nA large fraction of the Darwin Dissenters have specialties unrelated to research on evolution; of the dissenters, three-quarters are not biologists. As of 2006, the dissenter list was expanded to include non-US scientists.\n\nSome researchers are attempting to understand the factors that affect people's acceptance of evolution. Studies have yielded inconsistent results, explains associate professor of education at Ohio State University, David Haury. He recently performed a study that found people are likely to reject evolution if they have feelings of uncertainty, regardless of how well they understand evolutionary theory. Haury believes that teachers need to show students that their intuitive feelings may be misleading (for example, using the Wason selection task), and thus to exercise caution when relying on them as they judge the rational merits of ideas.\n\n\n"}
{"id": "146118", "url": "https://en.wikipedia.org/wiki?curid=146118", "title": "List of long-distance footpaths", "text": "List of long-distance footpaths\n\nThis is a list of some long-distance footpaths used for walking and hiking.\n\n\n\n\nThe merit of hiking trails in Hong Kong is that hikers can enjoy scenery of both the sea and highland.\n\nTranscaucasian Trail:\n\nA long distance trail in the caucasus has been a lingering idea for trekkers and hikers for many years since they started hiking remote parts of the Caucasus. \nMany sections of the TCT already exist, used by local community members and shepherds for centuries. These trail cross long valleys and traverse mammoth mountains to connect mountain villages together. Unfortunately, in recent years many of these trails have fallen into disrepair, and while many trails are known to locals, they are difficult to navigate for visitors and tourists. \nIn 2015, two Peace Corps volunteers, Paul Stephens and Jeff Haack, mapped and charted known routes in The Republic of Georgia. During this time they succeeded in locating many connections between known trails and publicizing the concept of the trail. In 2016, Tom Allen and Alessandro Mambelli scouted new trail routes in Armenia while the first trail building project began in Svaneti, Georgia. In 2017, the trail building expanded to Dilijan National Park in Armenia while trail building continued in the Svaneti region. \nToday, over 300 km of trail has been improved and marked in Georgia and Armenia. Many 7-10 day guided hikes are available on the TCT this summer. Over the next 5 years, the trail will be expanded to connect all of the sections and create even longer hikes.\nThe TCT can serve many purposes in the Caucasus region. For one, the natural diversity of the area needs to be protected. This habitat fosters many species of animal and provides unique ecosystems created by the mountains. \nMore information about the trail can be found at transcaucasiantrail.org . \nDonations can be sustaining or one-time.\n\n\n\n\n\nHkakabo Razi Trail, climbing the highest peak in Myanmar, in Khakaborazi National Park, and various footpaths in Putao\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSee: \n\n\n\n\n\n\nSee: List of long-distance hiking tracks in Australia\n\n\n\n\n\n\n"}
{"id": "822008", "url": "https://en.wikipedia.org/wiki?curid=822008", "title": "Lists of invasive species", "text": "Lists of invasive species\n\nThese are lists of invasive species by country or region. A species is regarded as invasive if it has been introduced by human action to a location, area, or region where it did not previously occur naturally (i.e., is not a native species), becomes capable of establishing a breeding population in the new location without further intervention by humans, and becomes a pest in the new location, threatening agriculture and/or the local biodiversity.\n\nThe term invasive species refers to a subset of those species defined as introduced species, for which see List of introduced species.\n\n\n\n"}
{"id": "16554664", "url": "https://en.wikipedia.org/wiki?curid=16554664", "title": "Living systems", "text": "Living systems\n\nLiving systems are open self-organizing life forms that interact with their environment. These systems are maintained by flows of information, energy and matter.\n\nSome scientists have proposed in the last few decades that a general living systems theory is required to explain the nature of life. Such a general theory, arising out of the ecological and biological sciences, attempts to map general principles for how all living systems work. Instead of examining phenomena by attempting to break things down into components, a general living systems theory explores phenomena in terms of dynamic patterns of the relationships of organisms with their environment.\n\nLiving systems theory is a general theory about the existence of all living systems, their structure, interaction, behavior and development. This work is created by James Grier Miller, which was intended to formalize the concept of life. According to Miller's original conception as spelled out in his magnum opus \"Living Systems\", a \"living system\" must contain each of twenty \"critical subsystems\", which are defined by their functions and visible in numerous systems, from simple cells to organisms, countries, and societies. In \"Living Systems\" Miller provides a detailed look at a number of systems in order of increasing size, and identifies his subsystems in each. \nMiller considers living systems as a subset of all systems. Below the level of living systems, he defines space and time, matter and energy, information and entropy, levels of organization, and physical and conceptual factors, and above living systems ecological, planetary and solar systems, galaxies, etc.\n\nLiving systems according to Parent (1996) are by definition \"open self-organizing systems that have the special characteristics of life and interact with their environment. This takes place by means of information and material-energy exchanges. Living systems can be as simple as a single cell or as complex as a supranational organization such as the European Union. Regardless of their complexity, they each depend upon the same essential twenty subsystems (or processes) in order to survive and to continue the propagation of their species or types beyond a single generation\".\n\nMiller said that systems exist at eight \"nested\" hierarchical levels: cell, organ, organism, group, organization, community, society, and supranational system. At each level, a system invariably comprises twenty critical subsystems, which process matter–energy or information except for the first two, which process both matter–energy and information: reproducer and boundary.\n\nThe processors of matter–energy are: \n\nThe processors of information are:\n\nJames Grier Miller in 1978 wrote a 1,102-page volume to present his living systems theory. He constructed a general theory of living systems by focusing on concrete systems—nonrandom accumulations of matter–energy in physical space–time organized into interacting, interrelated subsystems or components. Slightly revising the original model a dozen years later, he distinguished eight \"nested\" hierarchical levels in such complex structures. Each level is \"nested\" in the sense that each higher level contains the next lower level in a nested fashion.\n\nHis central thesis is that the systems in existence at all eight levels are open systems composed of twenty critical subsystems that process inputs, throughputs, and outputs of various forms of matter–energy and information. Two of these subsystems—reproducer and boundary—process both matter–energy and information. Eight of them process only matter–energy. The other ten process information only. \nAll nature is a continuum. The endless complexity of life is organized into patterns which repeat themselves—theme and variations—at each level of system. These similarities and differences are proper concerns for science. From the ceaseless streaming of protoplasm to the many-vectored activities of supranational systems, there are continuous flows through living systems as they maintain their highly organized steady states.\nSeppänen (1998) says that Miller applied general systems theory on a broad scale to describe all aspects of living systems.\n\nMiller's theory posits that the mutual interrelationship of the components of a system extends across the hierarchical levels. Examples: Cells and organs of a living system thrive on the food the organism obtains from its suprasystem; the member countries of a supranational system reap the benefits accrued from the communal activities to which each one contributes. Miller says that his eclectic theory \"ties together past discoveries from many disciplines and provides an outline into which new findings can be fitted\".\n\nMiller says the concepts of space, time, matter, energy, and information are essential to his theory because the living systems exist in space and are made of matter and energy organized by information. Miller's theory of living systems employs two sorts of spaces: physical or geographical space, and conceptual or abstracted spaces. Time is the fundamental \"fourth dimension\" of the physical space–time continuum/spiral. Matter is anything that has mass and occupies physical space. Mass and energy are equivalent as one can be converted into the other. Information refers to the degrees of freedom that exist in a given situation to choose among signals, symbols, messages, or patterns to be transmitted.\n\nOther relevant concepts are system, structure, process, type, level, echelon, suprasystem, subsystem, transmissions, and steady state. A system can be conceptual, concrete or abstracted. The structure of a system is the arrangement of the subsystems and their components in three-dimensional space at any point of time. Process, which can be reversible or irreversible, refers to change over time of matter–energy or information in a system. Type defines living systems with similar characteristics. Level is the position in a hierarchy of systems. Many complex living systems, at various levels, are organized into two or more echelons. The suprasystem of any living system is the next higher system in which it is a subsystem or component. The totality of all the structures in a system which carry out a particular process is a subsystem. Transmissions are inputs and outputs in concrete systems. Because living systems are open systems, with continually altering fluxes of matter–energy and information, many of their equilibria are dynamic—situations identified as steady states or flux equilibria.\n\nMiller identifies the comparable matter–energy and information processing critical subsystems. Elaborating on the eight hierarchical levels, he defines society, which constitutes the seventh hierarchy, as \"a large, living, concrete system with [community] and lower am levels of living systems as subsystems and components\". Society may include small, primitive, totipotential communities; ancient city–states, and kingdoms; as well as modern nation–states and empires that are not supranational systems. Miller provides general descriptions of each of the subsystems that fit all eight levels.\n\nA supranational system, in Miller's view, \"is composed of two or more societies, some or all of whose processes are under the control of a decider that is superordinate to their highest echelons\". However, he contends that no supranational system with all its twenty subsystems under control of its decider exists today. The absence of a supranational decider precludes the existence of a concrete supranational system. Miller says that studying a supranational system is problematical because its subsystems\n...tend to consist of few components besides the decoder. These systems do little matter-energy processing. The power of component societies [nations] today is almost always greater than the power of supranational deciders. Traditionally, theory at this level has been based upon intuition and study of history rather than data collection. Some quantitative research is now being done, and construction of global-system models and simulations is currently burgeoning.\n\nAt the supranational system level, Miller's emphasis is on international organizations, associations, and groups comprising representatives of societies (nation–states). Miller identifies the subsystems at this level to suit this emphasis. Thus, for example, the reproducer is \"any multipurpose supranational system which creates a single purpose supranational organization\" (p. 914); and the boundary is the \"supranational forces, usually located on or near supranational borders, which defend, guard, or police them\" (p. 914).\n\nNot just those specialized in international communication, but all communication science scholars could pay particular attention to the major contributions of living systems theory (LST) to social systems approaches that Bailey has pointed out:\n\nBailey says that LST, perhaps the \"most integrative\" social systems theory, has made many more contributions that may be easily overlooked, such as: providing a detailed analysis of types of systems; making a distinction between concrete and abstracted systems; discussion of physical space and time; placing emphasis on information processing; providing an analysis of entropy; recognition of totipotential systems, and partipotential systems; providing an innovative approach to the structure–process issue; and introducing the concept of joint subsystem—a subsystem that belongs to two systems simultaneously; of dispersal—lateral, outward, upward, and downward; of inclusion—inclusion of something from the environment that is not part of the system; of artifact—an animal-made or human-made inclusion; of adjustment process, which combats stress in a system; and of critical subsystems, which carry out processes that all living systems need to survive.\n\nLST's analysis of the twenty interacting subsystems, Bailey adds, clearly distinguishing between matter–energy-processing and information-processing, as well as LST's analysis of the eight interrelated system levels, enables us to understand how social systems are linked to biological systems. LST also analyzes the irregularities or \"organizational pathologies\" of systems functioning (e.g., system stress and strain, feedback irregularities, information–input overload). It explicates the role of entropy in social research while it equates negentropy with information and order. It emphasizes both structure and process, as well as their interrelations.\n\nIt omits the analysis of subjective phenomena, and it overemphasizes concrete Q-analysis (correlation of objects) to the virtual exclusion of R-analysis (correlation of variables). By asserting that societies (ranging from totipotential communities to nation-states and non-supranational systems) have greater control over their subsystem components than supranational systems have, it dodges the issue of transnational power over the contemporary social systems. Miller's supranational system bears no resemblance to the modern world-system that Immanuel Wallerstein (1974) described, although both of them were looking at the same living (dissipative) structure.\n\n\n\n"}
{"id": "1852572", "url": "https://en.wikipedia.org/wiki?curid=1852572", "title": "Marangoni effect", "text": "Marangoni effect\n\nThe Marangoni effect (also called the Gibbs–Marangoni effect) is the mass transfer along an interface between two fluids due to a gradient of the surface tension. In the case of temperature dependence, this phenomenon may be called thermo-capillary convection (or Bénard–Marangoni convection).\n\nThis phenomenon was first identified in the so-called \"tears of wine\" by physicist James Thomson (Lord Kelvin's brother) in 1855. The general effect is named after Italian physicist Carlo Marangoni, who studied it for his doctoral dissertation at the University of Pavia and published his results in 1865. A complete theoretical treatment of the subject was given by J. Willard Gibbs in his work \"On the Equilibrium of Heterogeneous Substances\" (1875-8).\n\nSince a liquid with a high surface tension pulls more strongly on the surrounding liquid than one with a low surface tension, the presence of a gradient in surface tension will naturally cause the liquid to flow away from regions of low surface tension. The surface tension gradient can be caused by concentration gradient or by a temperature gradient (surface tension is a function of temperature).\n\nAs an example, wine may exhibit a visible effect called \"tears\", as shown in the photograph. The effect is a consequence of the fact that alcohol has a lower surface tension and higher volatility than water. The water/alcohol solution rises up the surface of the glass due to capillary action. Alcohol evaporates from the film leaving behind liquid with a higher surface tension (more water, less alcohol). This region with a lower concentration of alcohol (greater surface tension) pulls on the surrounding fluid more strongly than the regions with a higher alcohol concentration (lower in the glass). The result is the liquid is pulled up until its own weight exceeds the force of the effect, and the liquid drips back down the vessel's walls. This can also be easily demonstrated by spreading a thin film of water on a smooth surface and then allowing a drop of alcohol to fall on the center of the film. The liquid will rush out of the region where the drop of alcohol fell.\n\nThe Marangoni number, a dimensionless value, can be used to characterize the relative effects of surface tension and viscous forces.\n\nA very detailed mathematical treatment of this from the point of view of the Navier–Stokes equations and the equations of thermodynamics can be found in the first third of Subrahmanyan Chandrasekhar's Hydrodynamic and Hydromagnetic Stability originally published in 1961 by Oxford, and republished by Dover in 1981.\n\nUnder earth conditions, the effect of gravity causing natural convection in a system with a temperature gradient along a fluid/fluid interface is usually much stronger than the Marangoni effect. Many experiments (ESA MASER 1-3) have been conducted under microgravity conditions aboard sounding rockets to observe the Marangoni effect without the influence of gravity. Research on heat pipes performed on the International Space Station revealed that whilst heat pipes exposed to a temperature gradient on Earth cause the inner fluid to evaporate at one end and migrate along the pipe, thus drying the hot end, in space (where the effects of gravity can be ignored) the opposite happens and the hot end of the pipe is flooded with liquid. This is due to the Marangoni effect, together with capillary action. The fluid is drawn to the hot end of the tube by capillary action. But the bulk of the liquid still ends up as a droplet a short distance away from the hottest part of the tube, explained by Marangoni flow. The temperature gradients in axial and radial directions makes the fluid flow away from the hot end and the walls of the tube, towards the center axis. The liquid forms a droplet with a small contact area with the tube walls, a thin film circulating liquid between the cooler droplet and the liquid at the hot end.\n\nThe effect of the Marangoni effect on heat transfer in the presence of gas bubbles on the heating surface (e.g., in subcooled nucleate boiling) has long been ignored, but it is currently a topic of ongoing research interest because of its potential fundamental importance to the understanding of heat transfer in boiling.\n\nA familiar example is in soap films: the Marangoni effect \"stabilizes\" soap films. Another instance of the Marangoni effect appears in the behavior of convection cells, the so-called Bénard cells.\n\nOne important application of the Marangoni effect is the use for drying silicon wafers after a wet processing step during the manufacture of integrated circuits. Liquid spots left on the wafer surface can cause oxidation that damages components on the wafer. To avoid spotting, an alcohol vapor (IPA) or other organic compound in gas, vapor, or aerosol form is blown through a nozzle over the wet wafer surface (or at the meniscus formed between the cleaning liquid and wafer as the wafer is lifted from an immersion bath), and the subsequent Marangoni effect causes a surface-tension gradient in the liquid allowing gravity more easily to pull the liquid completely off the wafer surface, effectively leaving a dry wafer surface.\n\nA similar phenomenon has been creatively utilized to self-assemble nanoparticles into ordered arrays and to grow ordered nanotubes. An alcohol containing nanoparticles is spread on the substrate, followed by blowing the substrate with a humid air flow. The alcohol is evaporated under the flow. Simultaneously, water condenses and forms microdroplets on the substrate. Meanwhile, the nanoparticles in alcohol are transferred into the microdroplets and finally form numerous coffee rings on the substrate after drying.\n\nThe Marangoni effect is also important to the fields of welding, crystal growth and electron beam melting of metals.\n\n\n"}
{"id": "2277747", "url": "https://en.wikipedia.org/wiki?curid=2277747", "title": "Metal clay", "text": "Metal clay\n\nMetal clay is a crafting medium consisting of very small particles of metal such as silver, gold, bronze, or copper mixed with an organic binder and water for use in making jewelry, beads and small sculptures. Originating in Japan in 1990, metal clay can be shaped just like any soft clay, by hand or using molds. After drying, the clay can be fired in a variety of ways such as in a kiln, with a handheld gas torch, or on a gas stove, depending on the type of clay and the metal in it. The binder burns away, leaving the pure sintered metal. Shrinkage of between 8% and 30% occurs (depending on the product used). Alloys such as bronze, sterling silver, and steel also are available.\n\nMetal clay first came out in Japan in 1990 to allow craft jewelry makers to make sophisticated looking jewelry without the years of study needed to make fine jewelry.\n\nFine silver metal clay results in objects containing 99.9% pure silver, which is suitable for enameling. Lump metal clay is sold in sealed packets to keep it moist and workable. The silver versions are also available as a softer paste in a pre-filled syringe which can be used to produce extruded forms, in small jars of slip and as paper-like sheets, from which most of the moisture has been removed. Common brands of silver metal clay include Precious Metal Clay (PMC) and Art Clay Silver (ACS).\n\nMetal clay artists looking for more strength in their silver creations can also mix PMC fine silver clay with an equal part of PMC Sterling clay. The firing of this alloy is found to be up to for two hours.\n\nAnother available alloy, EZ960 Sterling Silver Metal Clay was invented by Bill Struve from Metal Adventures, the inventor of BRONZclay™ and COPPRclay™. Because the clay is a sterling silver alloy, one of its best attributes is its post firing strength, in comparison to fine silver. This clay is fired open shelf on a raised hard ceramic kiln shelf at for 2 hours, full ramp. No carbon required. Its shrinkage rate is smaller than other clays, at 10–11%.\n\nPMC was developed in the early 1990s in Japan by metallurgist Masaki Morikawa. As a solid-phase sintered product of a precious metal powder used to form a precious metal article, the material consists of microscopic particles of pure silver or fine gold and a water-soluble, non-toxic, organic binder that burns off during firing. Success was first achieved with gold and later duplicated with silver. \nThe PMC brand includes the following products:\n\nACS was developed by AIDA Chemical Industries, also a Japanese company. ACS followed PMC Standard with their Art Clay Original clay (more like PMC+ than PMC Standard), which allows the user to fire with a handheld torch or on a gas hob. Owing to subtle differences in the binder and suggested firing times, this clay shrinks less than the PMC versions, approximately 8–10%.\n\nFurther developments introduced the Art Clay Slow Dry, a clay with a longer working time. Art Clay 650 and Art Clay 650 Slow Dry soon followed; both clays can be fired at , allowing the user to combine the clay with glass and sterling silver, which are affected negatively by the higher temperatures needed to fire the first generation clays. AIDA also manufacturers Oil Paste, a product used only on fired metal clay or milled fine silver, and Overlay Paste, which is designed for drawing designs on glass and porcelain.\n\nIn 2006 AIDA introduced the Art Clay Gold Paste, a more economical way to work with gold. The paste is painted onto the fired silver clay, then refired in a kiln, or with a torch or gas stove. When fired, it bonds with the silver, giving a 22-carat gold accent. The same year also saw Art Clay Slow Tarnish introduced, a clay that tarnishes less rapidly than the other metal clays.\n\nLump metal clay in bronze was introduced in 2008 by Metal Adventures Inc. and in 2009 by Prometheus. Lump metal clays in copper were introduced in 2009 by Metal Adventures Inc. and Aida. Because of the lower cost, the bronze and copper metal clays are used by artists more often than the gold and silver metal clays in the American market place. The actual creation time of a bronze or copper piece is also far greater than that of its silver counterpart. Base metal clays, such as bronze, copper, and steel metal clays are best fired in the absence of oxygen to eliminate the oxidation of the metal by atmospheric oxygen. A means to accomplish this –- to place the pieces in activated carbon inside a container – was developed by Bill Struve.\n\nMetal clays are also available as dry powders to which water is added to hydrate and kneaded to attain a clay consistency. One advantage to the powders are their unlimited shelf life. The first silver clay in powder form was released in 2006 as Silver Smiths' Metal Clay Powder. In the following years base metal clays by Hadar Jacobson and Goldie World released several variation containing copper, brass, and even steel.\n\n"}
{"id": "19053", "url": "https://en.wikipedia.org/wiki?curid=19053", "title": "Mineral", "text": "Mineral\n\nA mineral is a naturally occurring chemical compound, usually of crystalline form and not produced by life processes. A mineral has one specific chemical composition, whereas a rock can be an aggregate of different minerals or mineraloids. The study of minerals is called mineralogy.\n\nMinerals are classified by variety, species, series and group, in order of increasing generality. As of November 2018, there are more than 5,500 known mineral \"species\"; 5,389 of these have been approved by the International Mineralogical Association (IMA). \n\nMinerals are distinguished by various chemical and physical properties. Differences in chemical composition and crystal structure distinguish the various species, which were determined by the mineral's geological environment when formed. Changes in the temperature, pressure, or bulk composition of a rock mass cause changes in its minerals. Within a mineral species there may be variation in physical properties or minor amounts of impurities that are recognized by mineralogists or wider society as a mineral \"variety\", for example amethyst, a purple variety of the mineral species quartz.\n\nMinerals can be described by their various physical properties, which are related to their chemical structure and composition. Common distinguishing characteristics include crystal structure and habit, hardness, lustre, diaphaneity, colour, streak, tenacity, cleavage, fracture, parting, specific gravity, magnetism, taste or smell, radioactivity, and reaction to acid.\n\nMinerals are classified by key chemical constituents; the two dominant systems are the Dana classification and the Strunz classification. Silicon and oxygen constitute approximately 75% of the Earth's crust, which translates directly into the predominance of silicate minerals. The silicate minerals compose over 90% of the Earth's crust. The silicate class of minerals is subdivided into six subclasses by the degree of polymerization in the chemical structure. All silicate minerals have a base unit of a [SiO] silica tetrahedron—that is, a silicon cation coordinated by four oxygen anions, which gives the shape of a tetrahedron. These tetrahedra can be polymerized to give the subclasses: orthosilicates (no polymerization, thus single tetrahedra), disilicates (two tetrahedra bonded together), cyclosilicates (rings of tetrahedra), inosilicates (chains of tetrahedra), phyllosilicates (sheets of tetrahedra), and tectosilicates (three-dimensional network of tetrahedra). Other important mineral groups include the native elements, sulfides, oxides, halides, carbonates, sulfates, and phosphates.\n\nOne definition of a mineral encompasses the following criteria:\n\n\nThe first three general characteristics are less debated than the last two.\n\nMineral classification schemes and their definitions are evolving to match recent advances in mineral science. Recent changes have included the addition of an organic class, in both the new Dana and the Strunz classification schemes. The organic class includes a very rare group of minerals with hydrocarbons. The IMA Commission on New Minerals and Mineral Names adopted in 2009 a hierarchical scheme for the naming and classification of mineral groups and group names and established seven commissions and four working groups to review and classify minerals into an official listing of their published names. According to these new rules, \"mineral species can be grouped in a number of different ways, on the basis of chemistry, crystal structure, occurrence, association, genetic history, or resource, for example, depending on the purpose to be served by the classification.\"\n\nThe Nickel (1995) exclusion of biogenic substances was not universally adhered to. For example, Lowenstam (1981) stated that \"organisms are capable of forming a diverse array of minerals, some of which cannot be formed inorganically in the biosphere.\" The distinction is a matter of classification and less to do with the constituents of the minerals themselves. Skinner (2005) views all solids as potential minerals and includes biominerals in the mineral kingdom, which are those that are created by the metabolic activities of organisms. Skinner expanded the previous definition of a mineral to classify \"element or compound, amorphous or crystalline, formed through \"biogeochemical \" processes,\" as a mineral.\n\nRecent advances in high-resolution genetics and X-ray absorption spectroscopy are providing revelations on the biogeochemical relations between microorganisms and minerals that may make Nickel's (1995) biogenic mineral exclusion obsolete and Skinner's (2005) biogenic mineral inclusion a necessity. For example, the IMA-commissioned \"Working Group on Environmental Mineralogy and Geochemistry \" deals with minerals in the hydrosphere, atmosphere, and biosphere. The group's scope includes mineral-forming microorganisms, which exist on nearly every rock, soil, and particle surface spanning the globe to depths of at least 1600 metres below the sea floor and 70 kilometres into the stratosphere (possibly entering the mesosphere). Biogeochemical cycles have contributed to the formation of minerals for billions of years. Microorganisms can precipitate metals from solution, contributing to the formation of ore deposits. They can also catalyze the dissolution of minerals.\n\nPrior to the International Mineralogical Association's listing, over 60 biominerals had been discovered, named, and published. These minerals (a sub-set tabulated in Lowenstam (1981)) are considered minerals proper according to the Skinner (2005) definition. These biominerals are not listed in the International Mineral Association official list of mineral names, however, many of these biomineral representatives are distributed amongst the 78 mineral classes listed in the Dana classification scheme. Another rare class of minerals (primarily biological in origin) include the mineral liquid crystals that have properties of both liquids and crystals. To date, over 80,000 liquid crystalline compounds have been identified.\n\nThe Skinner (2005) definition of a mineral takes this matter into account by stating that a mineral can be crystalline or amorphous, the latter group including liquid crystals. Although biominerals and liquid mineral crystals, are not the most common form of minerals, they help to define the limits of what constitutes a mineral proper. The formal Nickel (1995) definition explicitly mentioned crystallinity as a key to defining a substance as a mineral. A 2011 article defined icosahedrite, an aluminium-iron-copper alloy as mineral; named for its unique natural icosahedral symmetry, it is a quasicrystal. Unlike a true crystal, quasicrystals are ordered but not periodic.\n\nMinerals are not equivalent to rocks. A rock is an aggregate of one or more minerals or mineraloids. Some rocks, such as limestone or quartzite, are composed primarily of one mineral—calcite or aragonite in the case of limestone, and quartz in the latter case. Other rocks can be defined by relative abundances of key (essential) minerals; a granite is defined by proportions of quartz, alkali feldspar, and plagioclase feldspar. The other minerals in the rock are termed accessory, and do not greatly affect the bulk composition of the rock. Rocks can also be composed entirely of non-mineral material; coal is a sedimentary rock composed primarily of organically derived carbon.\n\nIn rocks, some mineral species and groups are much more abundant than others; these are termed the rock-forming minerals. The major examples of these are quartz, the feldspars, the micas, the amphiboles, the pyroxenes, the olivines, and calcite; except for the last one, all of these minerals are silicates. Overall, around 150 minerals are considered particularly important, whether in terms of their abundance or aesthetic value in terms of collecting.\n\nCommercially valuable minerals and rocks are referred to as industrial minerals. For example, muscovite, a white mica, can be used for windows (sometimes referred to as isinglass), as a filler, or as an insulator. Ores are minerals that have a high concentration of a certain element, typically a metal. Examples are cinnabar (HgS), an ore of mercury, sphalerite (ZnS), an ore of zinc, or cassiterite (SnO), an ore of tin. Gems are minerals with an ornamental value, and are distinguished from non-gems by their beauty, durability, and usually, rarity. There are about 20 mineral species that qualify as gem minerals, which constitute about 35 of the most common gemstones. Gem minerals are often present in several varieties, and so one mineral can account for several different gemstones; for example, ruby and sapphire are both corundum, AlO.\n\nMinerals are classified by variety, species, series and group, in order of increasing generality. The basic level of definition is that of mineral species, each of which is distinguished from the others by unique chemical and physical properties. For example, quartz is defined by its formula, SiO, and a specific crystalline structure that distinguishes it from other minerals with the same chemical formula (termed polymorphs). When there exists a range of composition between two minerals species, a mineral series is defined. For example, the biotite series is represented by variable amounts of the endmembers phlogopite, siderophyllite, annite, and eastonite. In contrast, a mineral group is a grouping of mineral species with some common chemical properties that share a crystal structure. The pyroxene group has a common formula of XY(Si,Al)O, where X and Y are both cations, with X typically bigger than Y; the pyroxenes are single-chain silicates that crystallize in either the orthorhombic or monoclinic crystal systems. Finally, a mineral variety is a specific type of mineral species that differs by some physical characteristic, such as colour or crystal habit. An example is amethyst, which is a purple variety of quartz.\n\nTwo common classifications, Dana and Strunz, are used for minerals; both rely on composition, specifically with regards to important chemical groups, and structure. James Dwight Dana, a leading geologist of his time, first published his \"System of Mineralogy\" in 1837; as of 1997, it is in its eighth edition. The Dana classification assigns a four-part number to a mineral species. Its class number is based on important compositional groups; the type gives the ratio of cations to anions in the mineral, and the last two numbers group minerals by structural similarity within a given type or class. The less commonly used Strunz classification, named for German mineralogist Karl Hugo Strunz, is based on the Dana system, but combines both chemical and structural criteria, the latter with regards to distribution of chemical bonds.\n\n, 5,389 mineral species are approved by the IMA. They are most commonly named after a person (45%), followed by discovery location (23%); names based on chemical composition (14%) and physical properties (8%) are the two other major groups of mineral name etymologies.\n\nThe word \"species\" (from the Latin \"species\", \"a particular sort, kind, or type with distinct look, or appearance\") comes from the classification scheme in \"Systema Naturae\" by Carl Linnaeus. He divided the natural world into three kingdoms – plants, animals, and minerals – and classified each with the same hierarchy. In descending order, these were Phylum, Class, Order, Family, Tribe, Genus, and Species.\n\nThe abundance and diversity of minerals is controlled directly by their chemistry, in turn dependent on elemental abundances in the Earth. The majority of minerals observed are derived from the Earth's crust. Eight elements account for most of the key components of minerals, due to their abundance in the crust. These eight elements, summing to over 98% of the crust by weight, are, in order of decreasing abundance: oxygen, silicon, aluminium, iron, magnesium, calcium, sodium and potassium. Oxygen and silicon are by far the two most important – oxygen composes 47% of the crust by weight, and silicon accounts for 28%.\n\nThe minerals that form are directly controlled by the bulk chemistry of the parent body. For example, a magma rich in iron and magnesium will form mafic minerals, such as olivine and the pyroxenes; in contrast, a more silica-rich magma will crystallize to form minerals that incorporate more SiO, such as the feldspars and quartz. In a limestone, calcite or aragonite (both CaCO) form because the rock is rich in calcium and carbonate. A corollary is that a mineral will not be found in a rock whose bulk chemistry does not resemble the bulk chemistry of a given mineral with the exception of trace minerals. For example, kyanite, AlSiO forms from the metamorphism of aluminium-rich shales; it would not likely occur in aluminium-poor rock, such as quartzite.\n\nThe chemical composition may vary between end member species of a solid solution series. For example, the plagioclase feldspars comprise a continuous series from sodium-rich end member albite (NaAlSiO) to calcium-rich anorthite (CaAlSiO) with four recognized intermediate varieties between them (given in order from sodium- to calcium-rich): oligoclase, andesine, labradorite, and bytownite. Other examples of series include the olivine series of magnesium-rich forsterite and iron-rich fayalite, and the wolframite series of manganese-rich hübnerite and iron-rich ferberite.\n\nChemical substitution and coordination polyhedra explain this common feature of minerals. In nature, minerals are not pure substances, and are contaminated by whatever other elements are present in the given chemical system. As a result, it is possible for one element to be substituted for another. Chemical substitution will occur between ions of a similar size and charge; for example, K will not substitute for Si because of chemical and structural incompatibilities caused by a big difference in size and charge. A common example of chemical substitution is that of Si by Al, which are close in charge, size, and abundance in the crust. In the example of plagioclase, there are three cases of substitution. Feldspars are all framework silicates, which have a silicon-oxygen ratio of 2:1, and the space for other elements is given by the substitution of Si by Al to give a base unit of [AlSiO]; without the substitution, the formula would be charge-balanced as SiO, giving quartz. The significance of this structural property will be explained further by coordination polyhedra. The second substitution occurs between Na and Ca; however, the difference in charge has to accounted for by making a second substitution of Si by Al.\n\nCoordination polyhedra are geometric representations of how a cation is surrounded by an anion. In mineralogy, coordination polyhedra are usually considered in terms of oxygen, due its abundance in the crust. The base unit of silicate minerals is the silica tetrahedron – one Si surrounded by four O. An alternate way of describing the coordination of the silicate is by a number: in the case of the silica tetrahedron, the silicon is said to have a coordination number of 4. Various cations have a specific range of possible coordination numbers; for silicon, it is almost always 4, except for very high-pressure minerals where the compound is compressed such that silicon is in six-fold (octahedral) coordination with oxygen. Bigger cations have a bigger coordination numbers because of the increase in relative size as compared to oxygen (the last orbital subshell of heavier atoms is different too). Changes in coordination numbers leads to physical and mineralogical differences; for example, at high pressure, such as in the mantle, many minerals, especially silicates such as olivine and garnet, will change to a perovskite structure, where silicon is in octahedral coordination. Other examples are the aluminosilicates kyanite, andalusite, and sillimanite (polymorphs, since they share the formula AlSiO), which differ by the coordination number of the Al; these minerals transition from one another as a response to changes in pressure and temperature. In the case of silicate materials, the substitution of Si by Al allows for a variety of minerals because of the need to balance charges.\n\nChanges in temperature and pressure and composition alter the mineralogy of a rock sample. Changes in composition can be caused by processes such as weathering or metasomatism (hydrothermal alteration). Changes in temperature and pressure occur when the host rock undergoes tectonic or magmatic movement into differing physical regimes. Changes in thermodynamic conditions make it favourable for mineral assemblages to react with each other to produce new minerals; as such, it is possible for two rocks to have an identical or a very similar bulk rock chemistry without having a similar mineralogy. This process of mineralogical alteration is related to the rock cycle. An example of a series of mineral reactions is illustrated as follows.\n\nOrthoclase feldspar (KAlSiO) is a mineral commonly found in granite, a plutonic igneous rock. When exposed to weathering, it reacts to form kaolinite (AlSiO(OH), a sedimentary mineral, and silicic acid):\n\nUnder low-grade metamorphic conditions, kaolinite reacts with quartz to form pyrophyllite (AlSiO(OH)):\n\nAs metamorphic grade increases, the pyrophyllite reacts to form kyanite and quartz:\n\nAlternatively, a mineral may change its crystal structure as a consequence of changes in temperature and pressure without reacting. For example, quartz will change into a variety of its SiO polymorphs, such as tridymite and cristobalite at high temperatures, and coesite at high pressures.\n\nClassifying minerals ranges from simple to difficult. A mineral can be identified by several physical properties, some of them being sufficient for full identification without equivocation. In other cases, minerals can only be classified by more complex optical, chemical or X-ray diffraction analysis; these methods, however, can be costly and time-consuming. Physical properties applied for classification include crystal structure and habit, hardness, lustre, diaphaneity, colour, streak, cleavage and fracture, and specific gravity. Other less general tests include fluorescence, phosphorescence, magnetism, radioactivity, tenacity (response to mechanical induced changes of shape or form), piezoelectricity and reactivity to dilute acids.\n\nCrystal structure results from the orderly geometric spatial arrangement of atoms in the internal structure of a mineral. This crystal structure is based on regular internal atomic or ionic arrangement that is often expressed in the geometric form that the crystal takes. Even when the mineral grains are too small to see or are irregularly shaped, the underlying crystal structure is always periodic and can be determined by X-ray diffraction. Minerals are typically described by their symmetry content. Crystals are restricted to 32 point groups, which differ by their symmetry. These groups are classified in turn into more broad categories, the most encompassing of these being the six crystal families.\n\nThese families can be described by the relative lengths of the three crystallographic axes, and the angles between them; these relationships correspond to the symmetry operations that define the narrower point groups. They are summarized below; a, b, and c represent the axes, and α, β, γ represent the angle opposite the respective crystallographic axis (e.g. α is the angle opposite the a-axis, viz. the angle between the b and c axes):\n\nThe hexagonal crystal family is also split into two crystal \"systems\" – the trigonal, which has a three-fold axis of symmetry, and the hexagonal, which has a six-fold axis of symmetry.\n\nChemistry and crystal structure together define a mineral. With a restriction to 32 point groups, minerals of different chemistry may have identical crystal structure. For example, halite (NaCl), galena (PbS), and periclase (MgO) all belong to the hexaoctahedral point group (isometric family), as they have a similar stoichiometry between their different constituent elements. In contrast, polymorphs are groupings of minerals that share a chemical formula but have a different structure. For example, pyrite and marcasite, both iron sulfides, have the formula FeS; however, the former is isometric while the latter is orthorhombic. This polymorphism extends to other sulfides with the generic AX formula; these two groups are collectively known as the pyrite and marcasite groups.\n\nPolymorphism can extend beyond pure symmetry content. The aluminosilicates are a group of three minerals – kyanite, andalusite, and sillimanite – which share the chemical formula AlSiO. Kyanite is triclinic, while andalusite and sillimanite are both orthorhombic and belong to the dipyramidal point group. These differences arise corresponding to how aluminium is coordinated within the crystal structure. In all minerals, one aluminium ion is always in six-fold coordination with oxygen. Silicon, as a general rule, is in four-fold coordination in all minerals; an exception is a case like stishovite (SiO, an ultra-high pressure quartz polymorph with rutile structure). In kyanite, the second aluminium is in six-fold coordination; its chemical formula can be expressed as AlAlSiO, to reflect its crystal structure. Andalusite has the second aluminium in five-fold coordination (AlAlSiO) and sillimanite has it in four-fold coordination (AlAlSiO).\n\nDifferences in crystal structure and chemistry greatly influence other physical properties of the mineral. The carbon allotropes diamond and graphite have vastly different properties; diamond is the hardest natural substance, has an adamantine lustre, and belongs to the isometric crystal family, whereas graphite is very soft, has a greasy lustre, and crystallises in the hexagonal family. This difference is accounted for by differences in bonding. In diamond, the carbons are in sp hybrid orbitals, which means they form a framework where each carbon is covalently bonded to four neighbours in a tetrahedral fashion; on the other hand, graphite is composed of sheets of carbons in sp hybrid orbitals, where each carbon is bonded covalently to only three others. These sheets are held together by much weaker van der Waals forces, and this discrepancy translates to large macroscopic differences.\n\nTwinning is the intergrowth of two or more crystals of a single mineral species. The geometry of the twinning is controlled by the mineral's symmetry. As a result, there are several types of twins, including contact twins, reticulated twins, geniculated twins, penetration twins, cyclic twins, and polysynthetic twins. Contact, or simple twins, consist of two crystals joined at a plane; this type of twinning is common in spinel. Reticulated twins, common in rutile, are interlocking crystals resembling netting. Geniculated twins have a bend in the middle that is caused by start of the twin. Penetration twins consist of two single crystals that have grown into each other; examples of this twinning include cross-shaped staurolite twins and Carlsbad twinning in orthoclase. Cyclic twins are caused by repeated twinning around a rotation axis. This type of twinning occurs around three, four, five, six, or eight-fold axes, and the corresponding patterns are called threelings, fourlings, fivelings, sixlings, and eightlings. Sixlings are common in aragonite. Polysynthetic twins are similar to cyclic twins through the presence of repetitive twinning; however, instead of occurring around a rotational axis, polysynthetic twinning occurs along parallel planes, usually on a microscopic scale.\n\nCrystal habit refers to the overall shape of crystal. Several terms are used to describe this property. Common habits include acicular, which describes needlelike crystals as in natrolite, bladed, dendritic (tree-pattern, common in native copper), equant, which is typical of garnet, prismatic (elongated in one direction), and tabular, which differs from bladed habit in that the former is platy whereas the latter has a defined elongation. Related to crystal form, the quality of crystal faces is diagnostic of some minerals, especially with a petrographic microscope. Euhedral crystals have a defined external shape, while anhedral crystals do not; those intermediate forms are termed subhedral.\n\nThe hardness of a mineral defines how much it can resist scratching. This physical property is controlled by the chemical composition and crystalline structure of a mineral. A mineral's hardness is not necessarily constant for all sides, which is a function of its structure; crystallographic weakness renders some directions softer than others. An example of this property exists in kyanite, which has a Mohs hardness of 5½ parallel to [001] but 7 parallel to [100].\n\nThe most common scale of measurement is the ordinal Mohs hardness scale. Defined by ten indicators, a mineral with a higher index scratches those below it. The scale ranges from talc, a phyllosilicate, to diamond, a carbon polymorph that is the hardest natural material. The scale is provided below:\n\nLustre indicates how light reflects from the mineral's surface, with regards to its quality and intensity. There are numerous qualitative terms used to describe this property, which are split into metallic and non-metallic categories. Metallic and sub-metallic minerals have high reflectivity like metal; examples of minerals with this lustre are galena and pyrite. Non-metallic lustres include: adamantine, such as in diamond; vitreous, which is a glassy lustre very common in silicate minerals; pearly, such as in talc and apophyllite; resinous, such as members of the garnet group; silky which is common in fibrous minerals such as asbestiform chrysotile.\n\nThe diaphaneity of a mineral describes the ability of light to pass through it. Transparent minerals do not diminish the intensity of light passing through them. An example of a transparent mineral is muscovite (potassium mica); some varieties are sufficiently clear to have been used for windows. Translucent minerals allow some light to pass, but less than those that are transparent. Jadeite and nephrite (mineral forms of jade are examples of minerals with this property). Minerals that do not allow light to pass are called opaque.\n\nThe diaphaneity of a mineral depends on the thickness of the sample. When a mineral is sufficiently thin (e.g., in a thin section for petrography), it may become transparent even if that property is not seen in a hand sample. In contrast, some minerals, such as hematite or pyrite, are opaque even in thin-section.\n\nColour is the most obvious property of a mineral, but it is often non-diagnostic. It is caused by electromagnetic radiation interacting with electrons (except in the case of incandescence, which does not apply to minerals). Two broad classes of elements (idiochromatic and allochromatic) are defined with regards to their contribution to a mineral's colour: Idiochromatic elements are essential to a mineral's composition; their contribution to a mineral's colour is diagnostic. Examples of such minerals are malachite (green) and azurite (blue). In contrast, allochromatic elements in minerals are present in trace amounts as impurities. An example of such a mineral would be the ruby and sapphire varieties of the mineral corundum.\nThe colours of pseudochromatic minerals are the result of interference of light waves. Examples include labradorite and bornite.\n\nIn addition to simple body colour, minerals can have various other distinctive optical properties, such as play of colours, asterism, chatoyancy, iridescence, tarnish, and pleochroism. Several of these properties involve variability in colour. Play of colour, such as in opal, results in the sample reflecting different colours as it is turned, while pleochroism describes the change in colour as light passes through a mineral in a different orientation. Iridescence is a variety of the play of colours where light scatters off a coating on the surface of crystal, cleavage planes, or off layers having minor gradations in chemistry. In contrast, the play of colours in opal is caused by light refracting from ordered microscopic silica spheres within its physical structure. Chatoyancy (\"cat's eye\") is the wavy banding of colour that is observed as the sample is rotated; asterism, a variety of chatoyancy, gives the appearance of a star on the mineral grain. The latter property is particularly common in gem-quality corundum.\nThe streak of a mineral refers to the colour of a mineral in powdered form, which may or may not be identical to its body colour. The most common way of testing this property is done with a streak plate, which is made out of porcelain and coloured either white or black. The streak of a mineral is independent of trace elements or any weathering surface. A common example of this property is illustrated with hematite, which is coloured black, silver, or red in hand sample, but has a cherry-red to reddish-brown streak. Streak is more often distinctive for metallic minerals, in contrast to non-metallic minerals whose body colour is created by allochromatic elements. Streak testing is constrained by the hardness of the mineral, as those harder than 7 powder the \"streak plate\" instead.\n\nBy definition, minerals have a characteristic atomic arrangement. Weakness in this crystalline structure causes planes of weakness, and the breakage of a mineral along such planes is termed cleavage. The quality of cleavage can be described based on how cleanly and easily the mineral breaks; common descriptors, in order of decreasing quality, are \"perfect\", \"good\", \"distinct\", and \"poor\". In particularly transparent minerals, or in thin-section, cleavage can be seen as a series of parallel lines marking the planar surfaces when viewed from the side. Cleavage is not a universal property among minerals; for example, quartz, consisting of extensively interconnected silica tetrahedra, does not have a crystallographic weakness which would allow it to cleave. In contrast, micas, which have perfect basal cleavage, consist of sheets of silica tetrahedra which are very weakly held together.\n\nAs cleavage is a function of crystallography, there are a variety of cleavage types. Cleavage occurs typically in either one, two, three, four, or six directions. Basal cleavage in one direction is a distinctive property of the micas. Two-directional cleavage is described as prismatic, and occurs in minerals such as the amphiboles and pyroxenes. Minerals such as galena or halite have cubic (or isometric) cleavage in three directions, at 90°; when three directions of cleavage are present, but not at 90°, such as in calcite or rhodochrosite, it is termed rhombohedral cleavage. Octahedral cleavage (four directions) is present in fluorite and diamond, and sphalerite has six-directional dodecahedral cleavage.\n\nMinerals with many cleavages might not break equally well in all of the directions; for example, calcite has good cleavage in three directions, but gypsum has perfect cleavage in one direction, and poor cleavage in two other directions. Angles between cleavage planes vary between minerals. For example, as the amphiboles are double-chain silicates and the pyroxenes are single-chain silicates, the angle between their cleavage planes is different. The pyroxenes cleave in two directions at approximately 90°, whereas the amphiboles distinctively cleave in two directions separated by approximately 120° and 60°. The cleavage angles can be measured with a contact goniometer, which is similar to a protractor.\n\nParting, sometimes called \"false cleavage\", is similar in appearance to cleavage but is instead produced by structural defects in the mineral, as opposed to systematic weakness. Parting varies from crystal to crystal of a mineral, whereas all crystals of a given mineral will cleave if the atomic structure allows for that property. In general, parting is caused by some stress applied to a crystal. The sources of the stresses include deformation (e.g. an increase in pressure), exsolution, or twinning. Minerals that often display parting include the pyroxenes, hematite, magnetite, and corundum.\n\nWhen a mineral is broken in a direction that does not correspond to a plane of cleavage, it is termed to have been fractured. There are several types of uneven fracture. The classic example is conchoidal fracture, like that of quartz; rounded surfaces are created, which are marked by smooth curved lines. This type of fracture occurs only in very homogeneous minerals. Other types of fracture are fibrous, splintery, and hackly. The latter describes a break along a rough, jagged surface; an example of this property is found in native copper.\n\nTenacity is related to both cleavage and fracture. Whereas fracture and cleavage describes the surfaces that are created when a mineral is broken, tenacity describes how resistant a mineral is to such breaking. Minerals can be described as brittle, ductile, malleable, sectile, flexible, or elastic.\n\nSpecific gravity numerically describes the density of a mineral. The dimensions of density are mass divided by volume with units: kg/m or g/cm. Specific gravity measures how much water a mineral sample displaces. Defined as the quotient of the mass of the sample and difference between the weight of the sample in air and its corresponding weight in water, specific gravity is a unitless ratio. Among most minerals, this property is not diagnostic. Rock forming minerals – typically silicates or occasionally carbonates – have a specific gravity of 2.5–3.5.\n\nHigh specific gravity is a diagnostic property of a mineral. A variation in chemistry (and consequently, mineral class) correlates to a change in specific gravity. Among more common minerals, oxides and sulfides tend to have a higher specific gravity as they include elements with higher atomic mass. A generalization is that minerals with metallic or adamantine lustre tend to have higher specific gravities than those having a non-metallic to dull lustre. For example, hematite, FeO, has a specific gravity of 5.26 while galena, PbS, has a specific gravity of 7.2–7.6, which is a result of their high iron and lead content, respectively. A very high specific gravity becomes very pronounced in native metals; kamacite, an iron-nickel alloy common in iron meteorites has a specific gravity of 7.9, and gold has an observed specific gravity between 15 and 19.3.\n\nOther properties can be used to diagnose minerals. These are less general, and apply to specific minerals.\n\nDropping dilute acid (often 10% HCl) onto a mineral aids in distinguishing carbonates from other mineral classes. The acid reacts with the carbonate ([CO]) group, which causes the affected area to effervesce, giving off carbon dioxide gas. This test can be further expanded to test the mineral in its original crystal form or powdered form. An example of this test is done when distinguishing calcite from dolomite, especially within rocks (limestone and dolostone respectively). Calcite immediately effervesces in acid, whereas acid must be applied to powdered dolomite (often to a scratched surface in a rock), for it to effervesce. Zeolite minerals will not effervesce in acid; instead, they become frosted after 5–10 minutes, and if left in acid for a day, they dissolve or become a silica gel.\n\nWhen tested, magnetism is a very conspicuous property of minerals. Among common minerals, magnetite exhibits this property strongly, and magnetism is also present, albeit not as strongly, in pyrrhotite and ilmenite. Some minerals exhibit electrical properties – for example, quartz is piezoelectric – but electrical properties are rarely used as diagnostic criteria for minerals because of incomplete data and natural variation.\n\nMinerals can also be tested for taste or smell. Halite, NaCl, is table salt; its potassium-bearing counterpart, sylvite, has a pronounced bitter taste. Sulfides have a characteristic smell, especially as samples are fractured, reacting, or powdered.\n\nRadioactivity is a rare property; minerals may be composed of radioactive elements. They could be a defining constituent, such as uranium in uraninite, autunite, and carnotite, or as trace impurities. In the latter case, the decay of a radioactive element damages the mineral crystal; the result, termed a \"radioactive halo\" or \"pleochroic halo\", is observable with various techniques, such as thin-section petrography.\n\nAs the composition of the Earth's crust is dominated by silicon and oxygen, silicate elements are by far the most important class of minerals in terms of rock formation and diversity. However, non-silicate minerals are of great economic importance, especially as ores.\n\nNon-silicate minerals are subdivided into several other classes by their dominant chemistry, which includes native elements, sulfides, halides, oxides and hydroxides, carbonates and nitrates, borates, sulfates, phosphates, and organic compounds. Most non-silicate mineral species are rare (constituting in total 8% of the Earth's crust), although some are relatively common, such as calcite, pyrite, magnetite, and hematite. There are two major structural styles observed in non-silicates: close-packing and silicate-like linked tetrahedra. close-packed structures is a way to densely pack atoms while minimizing interstitial space. Hexagonal close-packing involves stacking layers where every other layer is the same (\"ababab\"), whereas cubic close-packing involves stacking groups of three layers (\"abcabcabc\"). Analogues to linked silica tetrahedra include SO (sulfate), PO (phosphate), AsO (arsenate), and VO (vanadate). The non-silicates have great economic importance, as they concentrate elements more than the silicate minerals do.\n\nThe largest grouping of minerals by far are the silicates; most rocks are composed of greater than 95% silicate minerals, and over 90% of the Earth's crust is composed of these minerals. The two main constituents of silicates are silicon and oxygen, which are the two most abundant elements in the Earth's crust. Other common elements in silicate minerals correspond to other common elements in the Earth's crust, such as aluminium, magnesium, iron, calcium, sodium, and potassium. Some important rock-forming silicates include the feldspars, quartz, olivines, pyroxenes, amphiboles, garnets, and micas.\n\nThe base unit of a silicate mineral is the [SiO] tetrahedron. In the vast majority of cases, silicon is in four-fold or tetrahedral coordination with oxygen. In very high-pressure situations, silicon will be in six-fold or octahedral coordination, such as in the perovskite structure or the quartz polymorph stishovite (SiO). In the latter case, the mineral no longer has a silicate structure, but that of rutile (TiO), and its associated group, which are simple oxides. These silica tetrahedra are then polymerized to some degree to create various structures, such as one-dimensional chains, two-dimensional sheets, and three-dimensional frameworks. The basic silicate mineral where no polymerization of the tetrahedra has occurred requires other elements to balance out the base 4- charge. In other silicate structures, different combinations of elements are required to balance out the resultant negative charge. It is common for the Si to be substituted by Al because of similarity in ionic radius and charge; in those cases, the [AlO] tetrahedra form the same structures as do the unsubstituted tetrahedra, but their charge-balancing requirements are different.\n\nThe degree of polymerization can be described by both the structure formed and how many tetrahedral corners (or coordinating oxygens) are shared (for aluminium and silicon in tetrahedral sites). Orthosilicates (or nesosilicates) have no linking of polyhedra, thus tetrahedra share no corners. Disilicates (or sorosilicates) have two tetrahedra sharing one oxygen atom. Inosilicates are chain silicates; single-chain silicates have two shared corners, whereas double-chain silicates have two or three shared corners. In phyllosilicates, a sheet structure is formed which requires three shared oxygens; in the case of double-chain silicates, some tetrahedra must share two corners instead of three as otherwise a sheet structure would result. Framework silicates, or tectosilicates, have tetrahedra that share all four corners. The ring silicates, or cyclosilicates, only need tetrahedra to share two corners to form the cyclical structure.\n\nThe silicate subclasses are described below in order of decreasing polymerization.\n\nTectosilicates, also known as framework silicates, have the highest degree of polymerization. With all corners of a tetrahedra shared, the silicon:oxygen ratio becomes 1:2. Examples are quartz, the feldspars, feldspathoids, and the zeolites. Framework silicates tend to be particularly chemically stable as a result of strong covalent bonds.\n\nForming 12% of the Earth's crust, quartz (SiO) is the most abundant mineral species. It is characterized by its high chemical and physical resistivity. Quartz has several polymorphs, including tridymite and cristobalite at high temperatures, high-pressure coesite, and ultra-high pressure stishovite. The latter mineral can only be formed on Earth by meteorite impacts, and its structure has been composed so much that it had changed from a silicate structure to that of rutile (TiO). The silica polymorph that is most stable at the Earth's surface is α-quartz. Its counterpart, β-quartz, is present only at high temperatures and pressures (changes to α-quartz below 573 °C at 1 bar). These two polymorphs differ by a \"kinking\" of bonds; this change in structure gives β-quartz greater symmetry than α-quartz, and they are thus also called high quartz (β) and low quartz (α).\n\nFeldspars are the most abundant group in the Earth's crust, at about 50%. In the feldspars, Al substitutes for Si, which creates a charge imbalance that must be accounted for by the addition of cations. The base structure becomes either [AlSiO] or [AlSiO] There are 22 mineral species of feldspars, subdivided into two major subgroups – alkali and plagioclase – and two less common groups – celsian and banalsite. The alkali feldspars are most commonly in a series between potassium-rich orthoclase and sodium-rich albite; in the case of plagioclase, the most common series ranges from albite to calcium-rich anorthite. Crystal twinning is common in feldspars, especially polysynthetic twins in plagioclase and Carlsbad twins in alkali feldspars. If the latter subgroup cools slowly from a melt, it forms exsolution lamellae because the two components – orthoclase and albite – are unstable in solid solution. Exsolution can be on a scale from microscopic to readily observable in hand-sample; perthitic texture forms when Na-rich feldspar exsolve in a K-rich host. The opposite texture (antiperthitic), where K-rich feldspar exsolves in a Na-rich host, is very rare.\n\nFeldspathoids are structurally similar to feldspar, but differ in that they form in Si-deficient conditions, which allows for further substitution by Al. As a result, feldspathoids cannot be associated with quartz. A common example of a feldspathoid is nepheline ((Na, K)AlSiO); compared to alkali feldspar, nepheline has an AlO:SiO ratio of 1:2, as opposed to 1:6 in the feldspar. Zeolites often have distinctive crystal habits, occurring in needles, plates, or blocky masses. They form in the presence of water at low temperatures and pressures, and have channels and voids in their structure. Zeolites have several industrial applications, especially in waste water treatment.\n\nPhyllosilicates consist of sheets of polymerized tetrahedra. They are bound at three oxygen sites, which gives a characteristic silicon:oxygen ratio of 2:5. Important examples include the mica, chlorite, and the kaolinite-serpentine groups. The sheets are weakly bound by van der Waals forces or hydrogen bonds, which causes a crystallographic weakness, in turn leading to a prominent basal cleavage among the phyllosilicates. In addition to the tetrahedra, phyllosilicates have a sheet of octahedra (elements in six-fold coordination by oxygen) that balance out the basic tetrahedra, which have a negative charge (e.g. [SiO]) These tetrahedra (T) and octahedra (O) sheets are stacked in a variety of combinations to create phyllosilicate groups. Within an octahedral sheet, there are three octahedral sites in a unit structure; however, not all of the sites may be occupied. In that case, the mineral is termed dioctahedral, whereas in other case it is termed trioctahedral.\n\nThe kaolinite-serpentine group consists of T-O stacks (the 1:1 clay minerals); their hardness ranges from 2 to 4, as the sheets are held by hydrogen bonds. The 2:1 clay minerals (pyrophyllite-talc) consist of T-O-T stacks, but they are softer (hardness from 1 to 2), as they are instead held together by van der Waals forces. These two groups of minerals are subgrouped by octahedral occupation; specifically, kaolinite and pyrophyllite are dioctahedral whereas serpentine and talc trioctahedral.\n\nMicas are also T-O-T-stacked phyllosilicates, but differ from the other T-O-T and T-O-stacked subclass members in that they incorporate aluminium into the tetrahedral sheets (clay minerals have Al in octahedral sites). Common examples of micas are muscovite, and the biotite series. The chlorite group is related to mica group, but a brucite-like (Mg(OH)) layer between the T-O-T stacks.\n\nBecause of their chemical structure, phyllosilicates typically have flexible, elastic, transparent layers that are electrical insulators and can be split into very thin flakes. Micas can be used in electronics as insulators, in construction, as optical filler, or even cosmetics. Chrysotile, a species of serpentine, is the most common mineral species in industrial asbestos, as it is less dangerous in terms of health than the amphibole asbestos.\n\nInosilicates consist of tetrahedra repeatedly bonded in chains. These chains can be single, where a tetrahedron is bound to two others to form a continuous chain; alternatively, two chains can be merged to create double-chain silicates. Single-chain silicates have a silicon:oxygen ratio of 1:3 (e.g. [SiO]), whereas the double-chain variety has a ratio of 4:11, e.g. [SiO]. Inosilicates contain two important rock-forming mineral groups; single-chain silicates are most commonly pyroxenes, while double-chain silicates are often amphiboles. Higher-order chains exist (e.g. three-member, four-member, five-member chains, etc.) but they are rare.\n\nThe pyroxene group consists of 21 mineral species. Pyroxenes have a general structure formula of XY(SiO), where X is an octahedral site, while Y can vary in coordination number from six to eight. Most varieties of pyroxene consist of permutations of Ca, Fe and Mg to balance the negative charge on the backbone. Pyroxenes are common in the Earth's crust (about 10%) and are a key constituent of mafic igneous rocks.\n\nAmphiboles have great variability in chemistry, described variously as a \"mineralogical garbage can\" or a \"mineralogical shark swimming a sea of elements\". The backbone of the amphiboles is the [SiO]; it is balanced by cations in three possible positions, although the third position is not always used, and one element can occupy both remaining ones. Finally, the amphiboles are usually hydrated, that is, they have a hydroxyl group ([OH]), although it can be replaced by a fluoride, a chloride, or an oxide ion. Because of the variable chemistry, there are over 80 species of amphibole, although variations, as in the pyroxenes, most commonly involve mixtures of Ca, Fe and Mg. Several amphibole mineral species can have an asbestiform crystal habit. These asbestos minerals form long, thin, flexible, and strong fibres, which are electrical insulators, chemically inert and heat-resistant; as such, they have several applications, especially in construction materials. However, asbestos are known carcinogens, and cause various other illnesses, such as asbestosis; amphibole asbestos (anthophyllite, tremolite, actinolite, grunerite, and riebeckite) are considered more dangerous than chrysotile serpentine asbestos.\n\nCyclosilicates, or ring silicates, have a ratio of silicon to oxygen of 1:3. Six-member rings are most common, with a base structure of [SiO]; examples include the tourmaline group and beryl. Other ring structures exist, with 3, 4, 8, 9, 12 having been described. Cyclosilicates tend to be strong, with elongated, striated crystals.\n\nTourmalines have a very complex chemistry that can be described by a general formula XYZ(BO)TOVW. The TO is the basic ring structure, where T is usually Si, but substitutable by Al or B. Tourmalines can be subgrouped by the occupancy of the X site, and from there further subdivided by the chemistry of the W site. The Y and Z sites can accommodate a variety of cations, especially various transition metals; this variability in structural transition metal content gives the tourmaline group greater variability in colour. Other cyclosilicates include beryl, AlBeSiO, whose varieties include the gemstones emerald (green) and aquamarine (bluish). Cordierite is structurally similar to beryl, and is a common metamorphic mineral.\n\nSorosilicates, also termed disilicates, have tetrahedron-tetrahedron bonding at one oxygen, which results in a 2:7 ratio of silicon to oxygen. The resultant common structural element is the [SiO] group. The most common disilicates by far are members of the epidote group. Epidotes are found in variety of geologic settings, ranging from mid-ocean ridge to granites to metapelites. Epidotes are built around the structure [(SiO)(SiO)] structure; for example, the mineral \"species\" epidote has calcium, aluminium, and ferric iron to charge balance: CaAl(Fe, Al)(SiO)(SiO)O(OH). The presence of iron as Fe and Fe helps understand oxygen fugacity, which in turn is a significant factor in petrogenesis.\n\nOther examples of sorosilicates include lawsonite, a metamorphic mineral forming in the blueschist facies (subduction zone setting with low temperature and high pressure), vesuvianite, which takes up a significant amount of calcium in its chemical structure.\n\nOrthosilicates consist of isolated tetrahedra that are charge-balanced by other cations. Also termed nesosilicates, this type of silicate has a silicon:oxygen ratio of 1:4 (e.g. SiO). Typical orthosilicates tend to form blocky equant crystals, and are fairly hard. Several rock-forming minerals are part of this subclass, such as the aluminosilicates, the olivine group, and the garnet group.\n\nThe aluminosilicates –bkyanite, andalusite, and sillimanite, all AlSiO – are structurally composed of one [SiO] tetrahedron, and one Al in octahedral coordination. The remaining Al can be in six-fold coordination (kyanite), five-fold (andalusite) or four-fold (sillimanite); which mineral forms in a given environment is depend on pressure and temperature conditions. In the olivine structure, the main olivine series of (Mg, Fe)SiO consist of magnesium-rich forsterite and iron-rich fayalite. Both iron and magnesium are in octahedral by oxygen. Other mineral species having this structure exist, such as tephroite, MnSiO. The garnet group has a general formula of XY(SiO), where X is a large eight-fold coordinated cation, and Y is a smaller six-fold coordinated cation. There are six ideal endmembers of garnet, split into two group. The pyralspite garnets have Al in the Y position: pyrope (MgAl(SiO)), almandine (FeAl(SiO)), and spessartine (MnAl(SiO)). The ugrandite garnets have Ca in the X position: uvarovite (CaCr(SiO)), grossular (CaAl(SiO)) and andradite (CaFe(SiO)). While there are two subgroups of garnet, solid solutions exist between all six end-members.\n\nOther orthosilicates include zircon, staurolite, and topaz. Zircon (ZrSiO) is useful in geochronology as the Zr can be substituted by U; furthermore, because of its very resistant structure, it is difficult to reset it as a chronometer. Staurolite is a common metamorphic intermediate-grade index mineral. It has a particularly complicated crystal structure that was only fully described in 1986. Topaz (AlSiO(F, OH), often found in granitic pegmatites associated with tourmaline, is a common gemstone mineral.\n\nNative elements are those that are not chemically bonded to other elements. This mineral group includes native metals, semi-metals, and non-metals, and various alloys and solid solutions. The metals are held together by metallic bonding, which confers distinctive physical properties such as their shiny metallic lustre, ductility and malleability, and electrical conductivity. Native elements are subdivided into groups by their structure or chemical attributes.\n\nThe gold group, with a cubic close-packed structure, includes metals such as gold, silver, and copper. The platinum group is similar in structure to the gold group. The iron-nickel group is characterized by several iron-nickel alloy species. Two examples are kamacite and taenite, which are found in iron meteorites; these species differ by the amount of Ni in the alloy; kamacite has less than 5–7% nickel and is a variety of native iron, whereas the nickel content of taenite ranges from 7–37%. Arsenic group minerals consist of semi-metals, which have only some metallic traits; for example, they lack the malleability of metals. Native carbon occurs in two allotropes, graphite and diamond; the latter forms at very high pressure in the mantle, which gives it a much stronger structure than graphite.\n\nThe sulfide minerals are chemical compounds of one or more metals or semimetals with a sulfur; tellurium, arsenic, or selenium can substitute for the sulfur. Sulfides tend to be soft, brittle minerals with a high specific gravity. Many powdered sulfides, such as pyrite, have a sulfurous smell when powdered. Sulfides are susceptible to weathering, and many readily dissolve in water; these dissolved minerals can be later redeposited, which creates enriched secondary ore deposits. Sulfides are classified by the ratio of the metal or semimetal to the sulfur, such as M:S equal to 2:1, or 1:1. Many sulfide minerals are economically important as metal ores; examples include sphalerite (ZnS), an ore of zinc, galena (PbS), an ore of lead, cinnabar (HgS), an ore of mercury, and molybdenite (MoS, an ore of molybdenum. Pyrite (FeS), is the most commonly occurring sulfide, and can be found in most geological environments. It is not, however, an ore of iron, but can be instead oxidized to produce sulfuric acid. Related to the sulfides are the rare sulfosalts, in which a metallic element is bonded to sulfur and a semimetal such as antimony, arsenic, or bismuth. Like the sulfides, sulfosalts are typically soft, heavy, and brittle minerals.\n\nOxide minerals are divided into three categories: simple oxides, hydroxides, and multiple oxides. Simple oxides are characterized by O as the main anion and primarily ionic bonding. They can be further subdivided by the ratio of oxygen to the cations. The periclase group consists of minerals with a 1:1 ratio. Oxides with a 2:1 ratio include cuprite (CuO) and water ice. Corundum group minerals have a 2:3 ratio, and includes minerals such as corundum (AlO), and hematite (FeO). Rutile group minerals have a ratio of 1:2; the eponymous species, rutile (TiO) is the chief ore of titanium; other examples include cassiterite (SnO; ore of tin), and pyrolusite (MnO; ore of manganese). In hydroxides, the dominant anion is the hydroxyl ion, OH. Bauxites are the chief aluminium ore, and are a heterogeneous mixture of the hydroxide minerals diaspore, gibbsite, and bohmite; they form in areas with a very high rate of chemical weathering (mainly tropical conditions). Finally, multiple oxides are compounds of two metals with oxygen. A major group within this class are the spinels, with a general formula of XYO. Examples of species include spinel (MgAlO), chromite (FeCrO), and magnetite (FeO). The latter is readily distinguishable by its strong magnetism, which occurs as it has iron in two oxidation states (FeFeO), which makes it a multiple oxide instead of a single oxide.\n\nThe halide minerals are compounds in which a halogen (fluorine, chlorine, iodine, or bromine) is the main anion. These minerals tend to be soft, weak, brittle, and water-soluble. Common examples of halides include halite (NaCl, table salt), sylvite (KCl), fluorite (CaF). Halite and sylvite commonly form as evaporites, and can be dominant minerals in chemical sedimentary rocks. Cryolite, NaAlF, is a key mineral in the extraction of aluminium from bauxites; however, as the only significant occurrence at Ivittuut, Greenland, in a granitic pegmatite, was depleted, synthetic cryolite can be made from fluorite.\n\nThe carbonate minerals are those in which the main anionic group is carbonate, [CO]. Carbonates tend to be brittle, many have rhombohedral cleavage, and all react with acid. Due to the last characteristic, field geologists often carry dilute hydrochloric acid to distinguish carbonates from non-carbonates. The reaction of acid with carbonates, most commonly found as the polymorph calcite and aragonite (CaCO), relates to the dissolution and precipitation of the mineral, which is a key in the formation of limestone caves, features within them such as stalactite and stalagmites, and karst landforms. Carbonates are most often formed as biogenic or chemical sediments in marine environments. The carbonate group is structurally a triangle, where a central C cation is surrounded by three O anions; different groups of minerals form from different arrangements of these triangles. The most common carbonate mineral is calcite, which is the primary constituent of sedimentary limestone and metamorphic marble. Calcite, CaCO, can have a high magnesium impurity. Under high-Mg conditions, its polymorph aragonite will form instead; the marine geochemistry in this regard can be described as an aragonite or calcite sea, depending on which mineral preferentially forms. Dolomite is a double carbonate, with the formula CaMg(CO). Secondary dolomitization of limestone is common, in which calcite or aragonite are converted to dolomite; this reaction increases pore space (the unit cell volume of dolomite is 88% that of calcite), which can create a reservoir for oil and gas. These two mineral species are members of eponymous mineral groups: the calcite group includes carbonates with the general formula XCO, and the dolomite group constitutes minerals with the general formula XY(CO).\n\nThe sulfate minerals all contain the sulfate anion, [SO]. They tend to be transparent to translucent, soft, and many are fragile. Sulfate minerals commonly form as evaporites, where they precipitate out of evaporating saline waters. Sulfates can also be found in hydrothermal vein systems associated with sulfides, or as oxidation products of sulfides. Sulfates can be subdivided into anhydrous and hydrous minerals. The most common hydrous sulfate by far is gypsum, CaSO⋅2HO. It forms as an evaporite, and is associated with other evaporites such as calcite and halite; if it incorporates sand grains as it crystallizes, gypsum can form desert roses. Gypsum has very low thermal conductivity and maintains a low temperature when heated as it loses that heat by dehydrating; as such, gypsum is used as an insulator in materials such as plaster and drywall. The anhydrous equivalent of gypsum is anhydrite; it can form directly from seawater in highly arid conditions. The barite group has the general formula XSO, where the X is a large 12-coordinated cation. Examples include barite (BaSO), celestine (SrSO), and anglesite (PbSO); anhydrite is not part of the barite group, as the smaller Ca is only in eight-fold coordination.\n\nThe phosphate minerals are characterized by the tetrahedral [PO] unit, although the structure can be generalized, and phosphorus is replaced by antimony, arsenic, or vanadium. The most common phosphate is the apatite group; common species within this group are fluorapatite (Ca(PO)F), chlorapatite (Ca(PO)Cl) and hydroxylapatite (Ca(PO)(OH)). Minerals in this group are the main crystalline constituents of teeth and bones in vertebrates. The relatively abundant monazite group has a general structure of ATO, where T is phosphorus or arsenic, and A is often a rare-earth element (REE). Monazite is important in two ways: first, as a REE \"sink\", it can sufficiently concentrate these elements to become an ore; secondly, monazite group elements can incorporate relatively large amounts of uranium and thorium, which can be used in monazite geochronology to date the rock based on the decay of the U and Th to lead.\n\nThe Strunz classification includes a class for . These rare compounds contain organic carbon, but can be formed by a geologic process. For example, whewellite, CaCO⋅HO is an oxalate that can be deposited in hydrothermal ore veins. While hydrated calcium oxalate can be found in coal seams and other sedimentary deposits involving organic matter, the hydrothermal occurrence is not considered to be related to biological activity.\n\nIt has been suggested that biominerals could be important indicators of extraterrestrial life and thus could play an important role in the search for past or present life on the planet Mars. Furthermore, organic components (biosignatures) that are often associated with biominerals are believed to play crucial roles in both pre-biotic and biotic reactions.\n\nOn January 24, 2014, NASA reported that current studies by the \"Curiosity\" and \"Opportunity\" rovers on Mars will now be searching for evidence of ancient life, including a biosphere based on autotrophic, chemotrophic and/or chemolithoautotrophic microorganisms, as well as ancient water, including fluvio-lacustrine environments (plains related to ancient rivers or lakes) that may have been habitable. The search for evidence of habitability, taphonomy (related to fossils), and organic carbon on the planet Mars is now a primary NASA objective.\n\n\n\n"}
{"id": "980435", "url": "https://en.wikipedia.org/wiki?curid=980435", "title": "Naturalistic observation", "text": "Naturalistic observation\n\nNaturalistic observation is, in contrast to analog observation, a research tool in which a subject is observed in its natural habitat without any manipulation by the observer. During naturalistic observation, researchers take great care to avoid interfering with the behavior they are observing by using unobtrusive methods. Naturalistic observation involves two main differences that set it apart from other forms of data gathering. In the context of a naturalistic observation, the environment is in no way being manipulated by the observer nor was it created by the observer.\nNaturalistic observation, as a research tool, comes with both advantages and disadvantages that impact its application. By merely observing at a given instance without any manipulation in its natural context, it makes the behaviors exhibited more credible because they are occurring in a real, typical scenario as opposed to an artificial one generated within a lab. Naturalistic observation also allows for study of events that are deemed unethical to study via experimental models, such as the impact of high school shootings on students attending the high school. Naturalistic observation is used in many techniques, from watching an animal's eating patterns in the forest to observing the behavior of students in a school setting.\n\n"}
{"id": "52634071", "url": "https://en.wikipedia.org/wiki?curid=52634071", "title": "Nature-based solutions", "text": "Nature-based solutions\n\nNature-based solutions (NBS or NbS) refers to the sustainable management and use of nature for tackling socio-environmental challenges. The challenges include issues such as climate change, water security, water pollution, food security, human health, and disaster risk management. \n\nA definition by the European Union states that these solutions are \"inspired and supported by nature, which are cost-effective, simultaneously provide environmental, social and economic benefits and help build resilience. The Nature-based Solutions Initiative meanwhile defines them as \"actions that work with and enhance nature so as to help people adapt to change and disasters\". Such solutions bring more, and more diverse, nature and natural features and processes into cities, landscapes and seascapes, through locally adapted, resource-efficient and systemic interventions\". With NBS, healthy, resilient and diverse ecosystems (whether natural, managed or newly created) can provide solutions for the benefit of societies and overall biodiversity.\n\nFor instance, the restoration or protection of mangroves along coastlines utilizes a nature-based solution to accomplish several things. Mangroves moderate the impact of waves and wind on coastal settlements or cities and sequester CO . They also provide safe nurseries for marine life that can be the basis for sustaining populations of fish that local populations may depend on. Additionally, the mangrove forests can help control coastal erosion resulting from sea level rise. Similarly, in cities green roofs or walls are nature-based solutions that can be used to moderate the impact of high temperatures, capture storm water, abate pollution, and act as carbon sinks, while enhancing biodiversity.\n\nConservation approaches and environment management initiatives have been carried out for decades. What is new is that the benefits of such nature-based solutions to human well-being have been articulated well more recently. Even if the term itself is still being framed, examples of nature-based solutions can be found all over the world, and imitated. Nature-based solutions are on their way to being mainstreamed in national and international policies and programmes (e.g. climate change policy, law, infrastructure investment and financing mechanisms). For example, the theme for World Water Day 2018 was \"Nature for water\" and by UN-Water's accompanying UN World Water Development Report had the title \"Nature-based Solutions for Water\".\n\nSocieties increasingly face challenges such as climate change, urbanization, jeopardized food security and water resource provision, and disaster risk. One approach to answer these challenges is to singularly rely on technological strategies. An alternative approach is to manage the (socio-)ecological systems in a comprehensive way in order to sustain and potentially increase the delivery of ecosystem services to humans. In this context, nature-based solutions (NBS) have recently been put forward by practitioners and quickly thereafter by policymakers. These solutions stress the sustainable use of nature in solving coupled environmental-social-economic challenges. \n\nWhile ecosystem services are often valued in terms of immediate benefits to human well-being and economy, NBS focus on the benefits to people and the environment itself, to allow for sustainable solutions that are able to respond to environmental change and hazards in the long-term. NBS go beyond the traditional biodiversity conservation and management principles by \"re-focusing\" the debate on humans and specifically integrating societal factors such as human well-being and poverty reduction, socio-economic development, and governance principles. \n\nWith respect to water issues, NBS can achieve the following, according to the World Water Development Report 2018 by UN-Water: \n\nIn this sense, NBS are strongly connected to ideas such as natural systems agriculture, natural solutions, ecosystem-based approaches, adaptation services, natural infrastructure, green infrastructure and ecological engineering. For instance, ecosystem-based approaches are increasingly promoted for climate change adaptation and mitigation by organisations like United Nations Environment Programme and non-governmental organisations such as The Nature Conservancy. These organisations refer to \"policies and measures that take into account the role of ecosystem services in reducing the vulnerability of society to climate change, in a multi-sectoral and multi-scale approach\".\n\nLikewise, natural infrastructure is defined as a \"strategically planned and managed network of natural lands, such as forests and wetlands, working landscapes, and other open spaces that conserves or enhances ecosystem values and functions and provides associated benefits to human populations\"; and green infrastructure refers to an \"interconnected network of green spaces that conserves natural systems and provides assorted benefits to human populations\".\n\nSimilarly, the concept of ecological engineering generally refers to \"protecting, restoring (i.e. ecosystem restoration) or modifying ecological systems to increase the quantity, quality and sustainability of particular services they provide, or to build new ecological systems that provide services that would otherwise be provided through more conventional engineering, based on non-renewable resources\".\n\nThe International Union for the Conservation of Nature (IUCN) defines NBS as actions to protect, sustainably manage, and restore natural or modified ecosystems, that address societal challenges effectively and adaptively, simultaneously providing human well-being and biodiversity benefits, with climate change, food security, disaster risks, water security, social and economic development as well as human health being the common societal challenges.\n\nIUCN proposes to consider NBS as an umbrella concept. Categories and examples of NBS approaches according to IUCN include:\n\nThe general objective of NBS is clear, namely the sustainable management and use of nature for tackling societal challenges. However, different stakeholders view NBS from other perspectives. For instance, IUCN defines NBS as \"actions to protect, sustainably manage and restore natural or modified ecosystems, which address societal challenges effectively and adaptively, while simultaneously providing human well-being and biodiversity benefits\". This framing puts the need for well-managed and restored ecosystems at the heart of NBS, with the overarching goal of \"Supporting the achievement of society's development goals and safeguard human well-being in ways that reflect cultural and societal values and enhance the resilience of ecosystems, their capacity for renewal and the provision of services\". \n\nIn the context of the ongoing political debate on jobs and growth (main drivers of the current EU policy agenda), the European Commission underlines that NBS can transform environmental and societal challenges into innovation opportunities, by turning natural capital into a source for green growth and sustainable development. In their view, NBS to societal challenges are \"solutions that are inspired and supported by nature, which are cost-effective, simultaneously provide environmental, social and economic benefits and help build resilience. Such solutions bring more, and more diverse, nature and natural features and processes into cities, landscapes and seascapes, through locally adapted, resource-efficient and systemic interventions.\" \n\nThis framing is somewhat broader, and puts economy and social assets at the heart of NBS as importantly as sustaining environmental conditions. It shares similarities with the definition proposed by Maes and Jacobs (2015) defining NBS as \"any transition to a use of ES with decreased input of non-renewable natural capital and increased investment in renewable natural processes\". In their view, development and evaluation of NBS spans three basic requirements: (1) decrease of fossil fuel input per produced unit; (2) lowering of systemic trade-offs and increasing synergies between ES; and (3) increasing labor input and jobs. Here, nature is seen as a tool to inspire more systemic solutions to societal problems.\n\nWhatever definition used, promoting sustainability and the increased role of natural, self-sustained processes relying on biodiversity, are inherent to NBS. They constitute actions easily seen as positive for a wide range of stakeholders, as they bring about benefits at environmental, economic and social levels. As a consequence, the concept of NBS is gaining acceptance outside the conservation community (e.g. urban planning) and is now on its way to be mainstreamed into policies and programmes (climate change policy, law, infrastructure investment and financing mechanisms).\n\nIn 2015, the European network BiodivERsA mobilized a range of scientists, research donors and stakeholders and proposed a typology characterizing NBS along two gradients. 1. \"how much engineering of biodiversity and ecosystems is involved in NBS\", and 2. \"how many ecosystem services and stakeholder groups are targeted by a given NBS\". The typology highlights that NBS can involve very different actions on ecosystems (from protection to management and even creation of new ecosystems) and is based on the assumption that the higher the number of services and stakeholder groups targeted, the lower the capacity to maximize the delivery of each service and simultaneously fulfil the specific needs of all stakeholder groups. As such, three types of NBS are distinguished (Figure 2):\n\nType 1 NBS consists of no or minimal intervention in ecosystems, with the objectives of maintaining or improving the delivery of a range of ES both inside and outside of these conserved ecosystems. Examples include the protection of mangroves in coastal areas to limit risks associated to extreme weather conditions and provide benefits and opportunities to local populations; and the establishment of marine protected areas to conserve biodiversity within these areas while exporting biomass into fishing grounds. This type of NBS is connected to, for example, the concept of biosphere reserves which incorporates core protected areas for nature conservation and buffer zones and transition areas where people live and work in a sustainable way.\n\nType 2 NBS corresponds to management approaches that develop sustainable and multifunctional ecosystems and landscapes (extensively or intensively managed). These types improve the delivery of selected ES compared to what would be obtained with a more conventional intervention. Examples include innovative planning of agricultural landscapes to increase their multi-functionality; and approaches for enhancing tree species and genetic diversity to increase forest resilience to extreme events. This type of NBS is strongly connected to concepts like natural systems agriculture, agro-ecology, and evolutionary-orientated forestry.\n\nType 3 NBS consists of managing ecosystems in very extensive ways or even creating new ecosystems (e.g., artificial ecosystems with new assemblages of organisms for green roofs and walls to mitigate city warming and clean polluted air). Type 3 is linked to concepts like green and blue infrastructures and objectives like restoration of heavily degraded or polluted areas and greening cities.\n\nType 1 and 2 would typically fall within the IUCN NBS framework, whereas Type 2 and moreover Type 3 are often exemplified by EC for turning natural capital into a source for green growth and sustainable development.\n\nHybrid solutions exist along this gradient both in space and time. For instance, at landscape scale, mixing protected and managed areas could be needed to fulfil multi-functionality and sustainability goals. Similarly, a constructed wetland can be developed as a type 3 but, when well established, may subsequently be preserved and surveyed as a type 1.\n\nDemonstrating the benefits of nature and healthy ecosystems and showcasing the return on investment they can offer is necessary in order to increase awareness, but also to provide support and guidance on how to implement NBS. A large number of initiatives around the world already highlight the effectiveness of NBS approaches to address a wide range of societal challenges.\n\nThe following table shows examples from around the world:\n\nIn 2018, The Hindu reported that the East Kolkata wetlands, the world's largest organic sewage treatment facility had been used to clean the sewage of Kolkata in an organic manner by using algae for several decades. In use since the 1930s, the natural system was discovered by Dhrubajyoti Ghosh, an ecologist and a municipal engineer in the 1970s while working in the region. Ghosh worked for decades to protect the wetlands. It had been a practice in Kolkata, one of the five largest cities in India, for the municipal authorities to pump sewage into shallow ponds (\"bheris\"). Under the heat of the tropical sun, algae proliferated in them, converting the sewage into clean water, which in turn was used by villagers to grow paddy and vegetables. This system has been in use in the region since the 1930s and treats 750 million litres of wastewater per day, giving livelihood to 100,000 people in the vicinity. For his work, Ghosh was included in the UN Global 500 Roll of Honour in 1990 and received the Luc Hoffmann award in 2016.\n\nThere is currently no accepted basis on which a government agency, municipality or private company can systematically assess the efficiency, effectiveness and sustainability of a particular nature-based solution. However, a series of principles are proposed to guide effective and appropriate implementation, and thus to upscale NBS in practice. For example, NBS embrace and are not meant to replace nature conservation norms. Also, NBS are determined by site-specific natural and cultural contexts that include traditional, local and scientific knowledge. NBS are an integral part of the overall design of policies, and measure or actions, to address a specific challenges. Finally, NBS can be implemented alone or in an integrated manner with other solutions to societal challenges (e.g. technological and engineering solutions) and they are applied at the landscape scale.\n\nImplementing NBS requires political, economic, and scientific challenges to be tackled. First and foremost, private sector investment is needed, not to replace but to supplement traditional sources of capital such as public funding or philanthropy. The challenge is therefore to provide a robust evidence base for the contribution of nature to economic growth and jobs, and to demonstrate the economic viability of these solutions – compared to technological ones – on a timescale compatible with that of global change. Furthermore, it requires measures like adaptation of economic subsidy schemes, and the creation of opportunities for conservation finance, to name a few. Indeed, such measures will be needed to scale up NBS interventions, and strengthen their impact in mitigating the world's most pressing challenges.\n\nSince 2016, the EU is supporting a multi-stakeholder dialogue platform (called ThinkNature) to promote the co-design, testing and deployment of improved and innovative NBS in an integrated way. Creation of such science-policy-business-society interfaces could promote the market uptake of NBS. The project is part of the EU’s Horizon 2020 – Research and Innovation programme, and will last for 3 years. There are a total of 17 international partners involved, including the Technical University of Crete (Project Leader), the University of Helsinki and BiodivERsA.\n\nIn 2017, as part of the Presidency of the Estonian Republic of the Council of the European Union, a conference called “Nature-based Solutions: From Innovation to Common-use” was organized by the Ministry of the Environment of Estonia and the University of Tallinn. This conference aimed to strengthen synergies among various recent initiatives and programs related to NBS launched by the European Commission and by the EU Member States, focusing on policy and governance of NBS, and on research and innovation.\n\nIn recognition of the importance of natural ecosystems for mitigation and adaptation, the Paris Agreement calls on all Parties to acknowledge “the importance of the conservation and enhancement, as appropriate, of sinks and reservoirs of the greenhouse gases” and to “note the importance of ensuring the integrity of all ecosystems, including oceans, and the protection of biodiversity, recognized by some cultures as Mother Earth”. It then includes in its Articles several references to nature-based solutions. For example, Article 5.2 encourages Parties to adopt “…policy approaches and positive incentives for activities relating to reducing emissions from deforestation and forest degradation, and the role of conservation and sustainable management of forests and enhancement of forest carbon stocks in developing countries; and alternative policy approaches, such as joint mitigation and adaptation approaches for the integral and sustainable management of forests, while reaffirming the importance of incentivizing, as appropriate, non-carbon benefits associated with such approaches”. Article 7.1 further encourages Parties to build the resilience of socioeconomic and ecological systems, including through economic diversification and sustainable management of natural resources. In total, the Agreement refers to nature (ecosystems, natural resources, forests) in 13 distinct places. An in-depth analysis of all Nationally Determined Contributions submitted to UNFCCC, revealed that around 130 NDCs or 65% of signatories commit to nature-based solutions in their climate pledges, suggesting broad consensus for the role of nature in helping meet climate change goals. However, high-level commitments rarely translate into robust, measurable actions on-the-ground.\n\nThe term NBS was put forward by practitioners in the late 2000s (in particular the International Union for the Conservation of Nature and the World Bank) and thereafter by policymakers in Europe (most notably the European Commission). \n\nThe term \"nature-based solutions\" was first used in the late 2000s. It was used in the context of finding new solutions to mitigate and adapt to climate change effects, whilst simultaneously protecting biodiversity and improving sustainable livelihoods. \n\nThe IUCN referred to NBS in a position paper for the United Nations Framework Convention on Climate Change. The term was also adopted by European policymakers, in particular by the European Commission in a report stressing that NBS can offer innovative means to create jobs and growth as part of a green economy. The term started to make appearances in the mainstream media around the time of the Global Climate Action Summit in California in September 2018 \n\n\n"}
{"id": "1548703", "url": "https://en.wikipedia.org/wiki?curid=1548703", "title": "Nature worship", "text": "Nature worship\n\nNature worship is any of a variety of religious, spiritual and devotional practices that focus on the worship of the nature spirits considered to be behind the natural phenomena visible throughout nature. A nature deity can be in charge of nature, a place, a biotope, the biosphere, the cosmos, or the universe. Nature worship is often considered the primitive source of modern religious beliefs and can be found in theism, panentheism, pantheism, deism, polytheism, animism, totemism, shamanism, paganism. Common to most forms of nature worship is a spiritual focus on the individual's connection and influence on some aspects of the natural world and reverence towards it.\n"}
{"id": "774575", "url": "https://en.wikipedia.org/wiki?curid=774575", "title": "Outgassing", "text": "Outgassing\n\nOutgassing (sometimes called offgassing, particularly when in reference to indoor air quality) is the release of a gas that was dissolved, trapped, frozen or absorbed in some material. Outgassing can include sublimation and evaporation (which are phase transitions of a substance into a gas), as well as desorption, seepage from cracks or internal volumes, and gaseous products of slow chemical reactions. Boiling is generally thought of as a separate phenomenon from outgassing because it consists of a phase transition of a liquid into a vapor of the same substance.\n\nOutgassing is a challenge to creating and maintaining clean high-vacuum environments. NASA and ESA maintains a list of low-outgassing materials to be used for spacecraft, as outgassing products can condense onto optical elements, thermal radiators, or solar cells and obscure them. Materials not normally considered absorbent can release enough light-weight molecules to interfere with industrial or scientific vacuum processes. Moisture, sealants, lubricants, and adhesives are the most common sources, but even metals and glasses can release gases from cracks or impurities. The rate of outgassing increases at higher temperatures because the vapor pressure and rate of chemical reaction increases. For most solid materials, the method of manufacture and preparation can reduce the level of outgassing significantly. Cleaning of surfaces, or heating of individual components or the entire assembly (a process called \"bake-out\") can drive off volatiles.\n\nNASA's Stardust spaceprobe suffered reduced image quality due to an unknown contaminant that had condensed on the CCD sensor of the navigation camera. A similar problem affected the Cassini spaceprobe's Narrow Angle Camera, but was corrected by repeatedly heating the system to 4 °C. A comprehensive characterisation of outgassing effects using mass spectrometers could be obtained for ESA's Rosetta spacecraft.\n\nNatural outgassing is commonplace in comets.\n\nOutgassing is a possible source of many tenuous atmospheres of terrestrial planets or moons. Many materials are volatile relative to the extreme vacuum of space, such as around the Moon, and may evaporate or even boil at ambient temperature. Materials on the lunar surface have completely outgassed and been ripped away by solar winds long ago, but volatile materials may remain at depth. Once released, gases almost always are less dense than the surrounding rocks and sand and seep toward the surface. The lunar atmosphere probably originates from outgassing of warm material below the surface. At the Earth's tectonic divergent boundaries where new crust is being created, helium and carbon dioxide are some of the volatiles being outgassed from mantle magma.\n\nOutgassing can be significant if it collects in a closed environment where air is stagnant or recirculated. For example, new car smell consists of outgassed chemicals released by heat in a closed automobile. Even a nearly odorless material such as wood may build up a strong smell if kept in a closed box for months. There is some concern that plasticizers and solvents released from many industrial products, especially plastics, may be harmful to human health. Long-term exposure to solvent vapors can cause chronic solvent-induced encephalopathy (CSE). Outgassing toxic gases are of great concern in the design of submarines and space stations, which must have self-contained recirculated atmospheres.\n\nThe outgassing of small pockets of air near the surface of setting concrete can lead to permanent holes in the structure (called bugholes) that may compromise its structural integrity.\n\n\n"}
{"id": "18106574", "url": "https://en.wikipedia.org/wiki?curid=18106574", "title": "Political naturalism", "text": "Political naturalism\n\nPolitical naturalism is a minor political ideology and legal system which believes that there is a natural law, just and obvious to all, that crosses ideologies, faiths and personal thinking, that naturally guaranties justice. It is inspired by sociological naturalism, and scientific naturalism's belief that the precision of natural sciences can be applied to social sciences, and hence to practical social activities like politics and law.\n\nIt may be seen as a natural law-based version of legalism/constitutionalism (especially of prescriptive constitutionalism, in the way it tries, idealistically, to make a constitution how it should justly be), and it bears relation with many constitutional monarchies (as in that system they too believe in rule of the law and in certain things who are naturally correct (like monarchy, monarchic institutions and traditions.\n\nThe roots of this legal political ideology may be found in positive visions of natural law (like John Locke's and Rousseau's, and even in the Founding Fathers of the United States. The Catholic German Centre Party politician and diplomat Karl Friedrich von Savigny also thought so.\n\nIts main modern thinker is Egyptian legal scholar and creator of the Egyptian Civil Code Al-Razzak Al-Sanhuri. Through the Egyptian Code, many other Arab constitutions (in monarchist and pre-dictatorships Iraq and Libya and modern Qatar) ended up including political naturalist laws, and Al-Sanhuri himself wrote the Syrian and Jordanian civil codes and the Kuwaiti commercial code.\n"}
{"id": "48191844", "url": "https://en.wikipedia.org/wiki?curid=48191844", "title": "Scandiobabingtonite", "text": "Scandiobabingtonite\n\nScandiobabingtonite was first discovered in the Montecatini granite quarry near Baveno, Italy in a pegmatite cavity. Though found in pegmatites, the crystals of scandiobabingtonite are sub-millimeter sized, and are tabular shaped. Scandiobabingtonite was the sixth naturally occurring mineral discovered with the rare earth element scandium, and grows around babingtonite, with which it is isostructural, hence the namesake. It is also referred to as scandian babingtonite. The ideal chemical formula for scandiobabingtonite is Ca(Fe,Mn)ScSiO(OH).\n\nScandiobabingtonite is found in association with orthoclase, quartz, light blue albite, stilbite, fluorite, and mica. When found with these minerals, the scandiobabingtonite crystals are emplanted on the surface of the other minerals. It also occurs as growth around green-black prismatic crystals of babingtonite. The samples of scandiobabingtonite that have been discovered have shown that they start out growing from a seed of babingtonite crystal. This is how scandiobabingtonite gets its chemical structure. The starting seed of babingtonite is still present in the center of the resulting crystal and can be detected with optical and chemical studies. Scandiobabingtonite is a uniquely rare mineral, as it occurs in very small amounts in few locations around the world. It is one of thirteen naturally occurring minerals where scandium is a dominant member. The other scandium minerals are bazzite, cascandite, hetftetjernite, jervisite, juonniite, kolbeckite, kristiansenite, magbasite, oftedalite, pretulite, thortveitite, and titanowodginite. Scandium can also concentrate in other minerals, such as in ferromagnesian minerals, aluminum phosphate minerals, meteoric minerals, and other minerals containing rare earth elements, but it occurs in trace amounts.\n\nScandiobabingtonite is a colorless or lightly gray-green colored transparent mineral with a glassy or vitreous luster. It exhibits a hardness of 6 on the Mohs hardness scale. Scandiobabingtonite occurs as short, prismatic crystals that are slightly elongated on the [001] axis which gives it a tabular or platy shape. Its crystals are characterized by the {010}, {001}, {110}, {1-10}, and {101} faces. Scandiobabingtonite is brittle and shows perfect cleavage along the {001} and {1-10} planes. The measured density is 3.24 g/cm.\n\nScandiobabingtonite is biaxial positive, which means it will refract light along two axes. It exhibits a 2V=64(2)°, strong dispersion with r>v, and displays strong pleochroism with colors ranging from pink (γ') to green(α'). The extinction angle along the (110) is 6°. Z:Φ=-250°, ρ=47°; Y:Φ=146°, ρ=75°; X:Φ=42°, ρ=47°.\n\nScandiobabingtonite is isostructural with babingtonite, and has the same chemical properties as well. It is an inosilicate with 5-periodic single chains. Scandium replaces the Fe in babingtonite in six-fold coordination. The empirical chemical formula for scandiobabingtonite is (Ca,Na)(Fe,Mn)(Sc,Sn,Fe)SiO(OH). Simplified, the formula is Ca(Fe,Mn)ScSiO(OH)\n\nScandiobabingtonite is in the triclinic crystal system, with space group P1. The unit cell dimensions are a=7.536(2) Å, b=11.734(2) Å, c=6.748(2) Å, α=91.70(2)°, β=93.86(2)°, γ=104.53(2)°. These dimensions are almost identical to those of babingtonite. The difference in dimensions is caused by the replacement of iron with scandium in the Fe-centered octahedra. The Fe-O distance measures as 2.048 Å, while the Sc-O distance is 2.092 Å. This equates to a slightly larger octahedra in scandiobabingtonite than babingtonite.\n\nList of Minerals\n"}
{"id": "195193", "url": "https://en.wikipedia.org/wiki?curid=195193", "title": "Sky", "text": "Sky\n\nThe sky (or celestial dome) is everything that lies above the surface of the Earth, including the atmosphere and outer space.\n\nIn the field of astronomy, the sky is also called the celestial sphere. This is viewed from Earth's surface as an abstract dome on which the Sun, stars, planets, and Moon appear to be traveling. The celestial sphere is conventionally divided into designated areas called constellations. Usually, the term \"sky\" is used informally as the point of view from the Earth's surface; however, the meaning and usage can vary. In some cases, such as in discussing the weather, the sky refers to only the lower, more dense portions of the atmosphere.\n\nDuring daylight, the sky appears to be blue because air scatters more blue sunlight than red. At night, the sky appears to be a mostly dark surface or region spangled with stars. During the day, the Sun can be seen in the sky unless obscured by clouds. In the night sky (and to some extent during the day) the Moon, planets and stars are visible in the sky. Some of the natural phenomena seen in the sky are clouds, rainbows, and aurorae. Lightning and precipitation can also be seen in the sky during storms. Birds, insects, aircraft, and kites are often considered to fly in the sky. Due to human activities, smog during the day and light pollution during the night are often seen above large cities.\n\nExcept for light that comes directly from the sun, most of the light in the day sky is caused by scattering, which is dominated by a small-particle limit called Rayleigh Scattering. The scattering due to molecule sized particles (as in air) is greater in the forward and backward directions than it is in the lateral direction. Scattering is significant for light at all visible wavelengths but is stronger at the shorter (bluer) end of the visible spectrum, meaning that the scattered light is bluer than its source, the sun. The remaining sunlight, having lost some of its short wavelength components, appears slightly less blue.\n\nScattering also occurs even more strongly in clouds. Individual water droplets exposed to white light will create a set of colored rings. If a cloud is thick enough, scattering from multiple water droplets will wash out the set of colored rings and create a washed-out white color.\n\nThe sky can turn a multitude of colors such as red, orange, purple and yellow (especially near sunset or sunrise) when the light must pass through a much longer path (or optical depth) through the atmosphere. Scattering effects also partially polarize light from the sky and are most pronounced at an angle 90° from the sun. Scattered light from the horizon travels through as much as 38 times the atmosphere as does light from the zenith, causing a blue gradient: vivid at the zenith, and pale near the horizon. Because red light also scatters if there is enough air between the source and the observer causing parts of the sky to change color during a sunset. As the amount of atmosphere nears infinity, the scattered light appears whiter and whiter.\n\nThe sun is not the only object that may appear less blue in the atmosphere. Far away clouds or snowy mountaintops may appear yellowish. The effect is not very obvious on clear days but is very pronounced when clouds cover the line of sight, reducing the blue hue from scattered sunlight. At higher altitudes, the sky tends toward darker colors since scattering is reduced due to lower air density; an extreme example is the moon, where there is no atmosphere and no scattering, making the sky on the moon black even when the sun is visible.\n\nSky luminance distribution models have been recommended by the International Commission on Illumination (CIE) for the design of daylighting schemes. Recent developments relate to “all sky models” for modelling sky luminance under weather conditions ranging from clear to overcast.\n\nThe intensity of the sky varies greatly over the day, and the primary cause of that intensity differs as well. When the sun is well above the horizon, direct scattering of sunlight (Rayleigh scattering) is the overwhelmingly dominant source of light. However, in twilight, the period of time between sunset and night and between night and sunrise, the situation is more complicated. Green flashes and green rays are optical phenomena that occur shortly after sunset or before sunrise, when a green spot is visible above the sun, usually for no more than a second or two, or it may resemble a green ray shooting up from the sunset point. Green flashes are a group of phenomena that stem from different causes, most of which occur when there is a temperature inversion (when the temperature increases with altitude rather than the normal decrease in temperature with altitude). Green flashes may be observed from any altitude (even from an aircraft). They are usually seen at an unobstructed horizon, such as over the ocean, but are also seen over cloud tops and mountain tops. Green flashes may also be observed at the horizon in association with the Moon and bright planets, including Venus and Jupiter.\n\nThe Earth's shadow is the shadow that the Earth casts on its atmosphere. This atmospheric phenomenon is sometimes seen twice a day, around the times of sunset and sunrise. When the weather conditions and the observer's viewing point permit a clear sight of the horizon, the shadow can be seen as a dark blue or greyish-blue band. Assuming the sky is clear, the Earth's shadow is visible in the half of the sky opposite to the sunset or sunrise, and is seen as a dark blue band right above the horizon. A related phenomenon is the \"Belt of Venus\" or \"anti-twilight arch\", a pink band that is visible above the dark blue band of the Earth's shadow in the same part of the sky. There is no clear dividing line between the Earth's shadow and the Belt of Venus: one colored band shades into the other in the sky.\n\nTwilight is divided into three segments according to how far the sun is below the horizon, measured in segments of 6°. After sunset the civil twilight sets in; it ends when the sun drops more than 6° below the horizon. This is followed by the nautical twilight, when the sun is 6° and 12° below the horizon (heights of between −6° and −12°), after which comes the astronomical twilight, defined as the period from −12° to −18°. When the sun drops more than 18° below the horizon, the sky generally attains its minimum brightness.\n\nSeveral sources can be identified as the source of the intrinsic brightness of the sky, namely airglow, indirect scattering of sunlight, scattering of starlight, and artificial light pollution.\n\nThe term night sky refers to the sky as seen at night. The term is usually associated with skygazing and astronomy, with reference to views of celestial bodies such as stars, the Moon, and planets that become visible on a clear night after the Sun has set. Natural light sources in a night sky include moonlight, starlight, and airglow, depending on location and timing. The fact that the sky is not completely dark at night can be easily observed. Were the sky (in the absence of moon and city lights) absolutely dark, one would not be able to see the silhouette of an object against the sky.\n\nThe night sky and studies of it have a historical place in both ancient and modern cultures. In the past, for instance, farmers have used the state of the night sky as a calendar to determine when to plant crops. The ancient belief in astrology is generally based on the belief that relationships between heavenly bodies influence or convey information about events on Earth. The \"scientific\" study of the night sky and bodies observed within it, meanwhile, takes place in the science of astronomy.\n\nWithin visible-light astronomy, the visibility of celestial objects in the night sky is affected by light pollution. The presence of the Moon in the night sky has historically hindered astronomical observation by increasing the amount of ambient lighting. With the advent of artificial light sources, however, light pollution has been a growing problem for viewing the night sky. Special filters and modifications to light fixtures can help to alleviate this problem, but for the best views, both professional and amateur optical astronomers seek viewing sites located far from major urban areas.\n\nAlong with pressure tendency, the condition of the sky is one of the more important parameters used to forecast weather in mountainous areas. Thickening of cloud cover or the invasion of a higher cloud deck is indicative of rain in the near future. At night, high thin cirrostratus clouds can lead to halos around the moon, which indicate the approach of a warm front and its associated rain. Morning fog portends fair conditions and can be associated with a marine layer, an indication of a stable atmosphere. Rainy conditions are preceded by wind or clouds which prevent fog formation. The approach of a line of thunderstorms could indicate the approach of a cold front. Cloud-free skies are indicative of fair weather for the near future. The use of sky cover in weather prediction has led to various weather lore over the centuries.\n\nWithin 36 hours of the passage of a tropical cyclone's center, the pressure begins to fall and a veil of white cirrus clouds approaches from the cyclone's direction. Within 24 hours of the closest approach to the center, low clouds begin to move in, also known as the bar of a tropical cyclone, as the barometric pressure begins to fall more rapidly and the winds begin to increase. Within 18 hours of the center's approach, squally weather is common, with sudden increases in wind accompanied by rain showers or thunderstorms. Within six hours of the center's arrival, rain becomes continuous. Within an hour of the center, the rain becomes very heavy and the highest winds within the tropical cyclone are experienced. When the center arrives with a strong tropical cyclone, weather conditions improve and the sun becomes visible as the eye moves overhead. Once the system departs, winds reverse and, along with the rain, suddenly increase. One day after the center's passage, the low overcast is replaced with a higher overcast, and the rain becomes intermittent. By 36 hours after the center's passage, the high overcast breaks and the pressure begins to level off.\n\nFlight is the process by which an object moves, through or beyond the sky (as in the case of spaceflight), by generating aerodynamic lift, propulsive thrust, aerostatically using buoyancy, or by ballistic movement, without any direct mechanical support from the ground. The engineering aspects of flight are studied in aerospace engineering which is subdivided into aeronautics, which is the study of vehicles that travel through the air, and astronautics, the study of vehicles that travel through space, and in ballistics, the study of the flight of projectiles. While human beings have been capable of flight via hot air balloons since 1783, other species have used flight for significantly longer. Animals, such as birds, bats, and insects are capable of flight. Spores and seeds from plants use flight, via use of the wind, as a method of propagating their species.\n\nMany mythologies have deities especially associated with the sky. In Egyptian religion, the sky was deified as the goddess Nut and as the god Horus. Dyeus is reconstructed as the god of the sky, or the sky personified, in Proto-Indo-European religion, whence Zeus, the god of the sky and thunder in Greek mythology and the Roman god of sky and thunder Jupiter.\n\nIn Australian Aboriginal mythology, Altjira (or Arrernte) is the main sky god and also the creator god. In Iroquois mythology, Atahensic was a sky goddess who fell down to the ground during the creation of the Earth. Many cultures have drawn constellations between stars in the sky, using them in association with legends and mythology about their deities.\n\n\n"}
{"id": "13680444", "url": "https://en.wikipedia.org/wiki?curid=13680444", "title": "Streaming vibration current", "text": "Streaming vibration current\n\nThe streaming vibration current (SVI) and the associated streaming vibration potential is an electric signal that arises when an acoustic wave propagates through a porous body in which the pores are filled with fluid.\n\nStreaming vibration current was experimentally observed in 1948 by M. Williams. A theoretical model was developed some 30 years later by Dukhin and coworkers. This effect opens another possibility for characterizing the electric properties of the surfaces in porous bodies.\n\n"}
{"id": "448321", "url": "https://en.wikipedia.org/wiki?curid=448321", "title": "Thermoelectric effect", "text": "Thermoelectric effect\n\nThe thermoelectric effect is the direct conversion of temperature differences to electric voltage and vice versa via a thermocouple. A thermoelectric device creates voltage when there is a different temperature on each side. Conversely, when a voltage is applied to it, heat is transferred from one side to the other, creating a temperature difference. At the atomic scale, an applied temperature gradient causes charge carriers in the material to diffuse from the hot side to the cold side.\n\nThis effect can be used to generate electricity, measure temperature or change the temperature of objects. Because the direction of heating and cooling is determined by the polarity of the applied voltage, thermoelectric devices can be used as temperature controllers.\n\nThe term \"thermoelectric effect\" encompasses three separately identified effects: the Seebeck effect, Peltier effect, and Thomson effect. The Seebeck and Peltier effects are different manifestations of the same physical process; textbooks may refer to this process as the Peltier–Seebeck effect (the separation derives from the independent discoveries by French physicist Jean Charles Athanase Peltier and Baltic German physicist Thomas Johann Seebeck). The Thomson effect is an extension of the Peltier–Seebeck model and is credited to Lord Kelvin.\n\nJoule heating, the heat that is generated whenever a current is passed through a resistive material, is related, though it is not generally termed a thermoelectric effect. The Peltier–Seebeck and Thomson effects are thermodynamically reversible, whereas Joule heating is not.\n\nThe Seebeck effect is the conversion of heat directly into electricity at the junction of different types of wire. Originally discovered in 1794 by Italian scientist Alessandro Volta, it is named after the Baltic German physicist Thomas Johann Seebeck, who in 1821 independently rediscovered it. It was observed that a compass needle would be deflected by a closed loop formed by two different metals joined in two places, with a temperature difference between the joints. This was because the electron energy levels in each metal shifted differently and a potential difference between the junctions created an electrical current and therefore a magnetic field around the wires. Seebeck did not recognize that there was an electric current involved, so he called the phenomenon \"thermomagnetic effect\". Danish physicist Hans Christian Ørsted rectified the oversight and coined the term \"thermoelectricity\".\n\nThe Seebeck effect is a classic example of an electromotive force (emf) and leads to measurable currents or voltages in the same way as any other emf. Electromotive forces modify Ohm's law by generating currents even in the absence of voltage differences (or vice versa); the local current density is given by\n\nwhere formula_2 is the local voltage, and formula_3 is the local conductivity. In general, the Seebeck effect is described locally by the creation of an electromotive field\n\nwhere formula_5 is the Seebeck coefficient (also known as thermopower), a property of the local material, and formula_6 is the temperature gradient.\n\nThe Seebeck coefficients generally vary as function of temperature and depend strongly on the composition of the conductor. For ordinary materials at room temperature, the Seebeck coefficient may range in value from −100 μV/K to +1,000 μV/K (see Seebeck coefficient article for more information).\n\nIf the system reaches a steady state, where formula_7, then the voltage gradient is given simply by the emf: formula_8. This simple relationship, which does not depend on conductivity, is used in the thermocouple to measure a temperature difference; an absolute temperature may be found by performing the voltage measurement at a known reference temperature. A metal of unknown composition can be classified by its thermoelectric effect if a metallic probe of known composition is kept at a constant temperature and held in contact with the unknown sample that is locally heated to the probe temperature. It is used commercially to identify metal alloys. Thermocouples in series form a thermopile. Thermoelectric generators are used for creating power from heat differentials.\nThe Peltier effect is the presence of heating or cooling at an electrified junction of two different conductors and is named after French physicist Jean Charles Athanase Peltier, who discovered it in 1834. When a current is made to flow through a junction between two conductors, A and B, heat may be generated or removed at the junction. The Peltier heat generated at the junction per unit time is\n\nwhere formula_10 and formula_11 are the Peltier coefficients of conductors A and B, and formula_12 is the electric current (from A to B). The total heat generated is not determined by the Peltier effect alone, as it may also be influenced by Joule heating and thermal-gradient effects (see below).\n\nThe Peltier coefficients represent how much heat is carried per unit charge. Since charge current must be continuous across a junction, the associated heat flow will develop a discontinuity if formula_10 and formula_11 are different. The Peltier effect can be considered as the back-action counterpart to the Seebeck effect (analogous to the back-emf in magnetic induction): if a simple thermoelectric circuit is closed, then the Seebeck effect will drive a current, which in turn (by the Peltier effect) will always transfer heat from the hot to the cold junction. The close relationship between Peltier and Seebeck effects can be seen in the direct connection between their coefficients: formula_15 (see below).\n\nA typical Peltier heat pump involves multiple junctions in series, through which a current is driven. Some of the junctions lose heat due to the Peltier effect, while others gain heat. Thermoelectric heat pumps exploit this phenomenon, as do thermoelectric cooling devices found in refrigerators.\n\nIn different materials, the Seebeck coefficient is not constant in temperature, and so a spatial gradient in temperature can result in a gradient in the Seebeck coefficient. If a current is driven through this gradient, then a continuous version of the Peltier effect will occur. This Thomson effect was predicted and subsequently observed in 1851 by Lord Kelvin (William Thomson). It describes the heating or cooling of a current-carrying conductor with a temperature gradient.\n\nIf a current density formula_16 is passed through a homogeneous conductor, the Thomson effect predicts a heat production rate per unit volume\n\nwhere formula_6 is the temperature gradient, and formula_19 is the Thomson coefficient. The Thomson coefficient is related to the Seebeck coefficient as formula_20 (see below). This equation, however, neglects Joule heating and ordinary thermal conductivity (see full equations below).\n\nOften, more than one of the above effects is involved in the operation of a real thermoelectric device. The Seebeck effect, Peltier effect, and Thomson effect can be gathered together in a consistent and rigorous way, described here; the effects of Joule heating and ordinary heat conduction are included as well. As stated above, the Seebeck effect generates an electromotive force, leading to the current equation\n\nTo describe the Peltier and Thomson effects the flow of energy must be considered. To start, the dynamic case where both temperature and charge may be varying with time can be considered. The full thermoelectric equation for the energy accumulation, formula_22, is\n\nwhere formula_24 is the thermal conductivity. The first term is the Fourier's heat conduction law, and the second term shows the energy carried by currents. The third term, formula_25, is the heat added from an external source (if applicable).\n\nIn the case where the material has reached a steady state, the charge and temperature distributions are stable, so one must have both formula_26 and formula_27. Using these facts and the second Thomson relation (see below), the heat equation then can be simplified to\n\nThe middle term is the Joule heating, and the last term includes both Peltier (formula_29 at junction) and Thomson (formula_29 in thermal gradient) effects. Combined with the Seebeck equation for formula_16, this can be used to solve for the steady-state voltage and temperature profiles in a complicated system.\n\nIf the material is not in a steady state, a complete description will also need to include dynamic effects such as relating to electrical capacitance, inductance, and heat capacity.\n\nIn 1854, Lord Kelvin found relationships between the three coefficients, implying that the Thomson, Peltier, and Seebeck effects are different manifestations of one effect (uniquely characterized by the Seebeck coefficient).\n\nThe first Thomson relation is\n\nwhere formula_33 is the absolute temperature, formula_19 is the Thomson coefficient, formula_35 is the Peltier coefficient, and formula_5 is the Seebeck coefficient. This relationship is easily shown given that the Thomson effect is a continuous version of the Peltier effect. Using the second relation (described next), the first Thomson relation becomes formula_20.\n\nThe second Thomson relation is\n\nThis relation expresses a subtle and fundamental connection between the Peltier and Seebeck effects. It was not satisfactorily proven until the advent of the Onsager relations, and it is worth noting that this second Thomson relation is only guaranteed for a time-reversal symmetric material; if the material is placed in a magnetic field or is itself magnetically ordered (ferromagnetic, antiferromagnetic, etc.), then the second Thomson relation does not take the simple form shown here.\n\nThe Thomson coefficient is unique among the three main thermoelectric coefficients because it is the only one directly measurable for individual materials. The Peltier and Seebeck coefficients can only be easily determined for pairs of materials; hence, it is difficult to find values of absolute Seebeck or Peltier coefficients for an individual material.\n\nIf the Thomson coefficient of a material is measured over a wide temperature range, it can be integrated using the Thomson relations to determine the absolute values for the Peltier and Seebeck coefficients. This needs to be done only for one material, since the other values can be determined by measuring pairwise Seebeck coefficients in thermocouples containing the reference material and then adding back the absolute Seebeck coefficient of the reference material. For more details on absolute Seebeck coefficient determination, see Seebeck coefficient.\n\nThe Seebeck effect is used in thermoelectric generators, which function like heat engines, but are less bulky, have no moving parts, and are typically more expensive and less efficient. They have a use in power plants for converting waste heat into additional electrical power (a form of energy recycling) and in automobiles as automotive thermoelectric generators (ATGs) for increasing fuel efficiency. Space probes often use radioisotope thermoelectric generators with the same mechanism but using radioisotopes to generate the required heat difference.\nRecent uses include stove fans, body-heat—powered lighting and a smartwatch powered by body heat.\n\nThe Peltier effect can be used to create a refrigerator that is compact and has no circulating fluid or moving parts. Such refrigerators are useful in applications where their advantages outweigh the disadvantage of their very low efficiency. The Peltier effect is also used by many thermal cyclers, laboratory devices used to amplify DNA by the polymerase chain reaction (PCR). PCR requires the cyclic heating and cooling of samples to specified temperatures. The inclusion of many thermocouples in a small space enables many samples to be amplified in parallel.\n\nThermocouples and thermopiles are devices that use the Seebeck effect to measure the temperature difference between two objects.\nThermocouples are often used to measure high temperatures, holding the temperature of one junction constant or measuring it independently (cold junction compensation). Thermopiles use many thermocouples electrically connected in series, for sensitive measurements of very small temperature difference.\n\n\n\n"}
{"id": "20847621", "url": "https://en.wikipedia.org/wiki?curid=20847621", "title": "Universal Darwinism", "text": "Universal Darwinism\n\nUniversal Darwinism (also known as generalized Darwinism, universal selection theory,\nor Darwinian metaphysics) refers to a variety of approaches that extend the theory of Darwinism beyond its original domain of biological evolution on Earth. Universal Darwinism aims to formulate a generalized version of the mechanisms of variation, selection and heredity proposed by Charles Darwin, so that they can apply to explain evolution in a wide variety of other domains, including psychology, economics, culture, medicine, computer science and physics.\n\nAt the most fundamental level, Charles Darwin's theory of evolution states that organisms evolve and adapt to their environment by an iterative process. This process can be conceived as an evolutionary algorithm that searches the space of possible forms (the fitness landscape) for the ones that are best adapted. The process has three components:\n\n\nAfter those fit variants are retained, they can again undergo variation, either directly or in their offspring, starting a new round of the iteration. The overall mechanism is similar to the problem-solving procedures of trial-and-error or generate-and-test: evolution can be seen as searching for the best solution for the problem of how to survive and reproduce by generating new trials, testing how well they perform, eliminating the failures, and retaining the successes.\n\nThe generalization made in \"universal\" Darwinism is to replace \"organism\" by any recognizable pattern, phenomenon, or system. The first requirement is that the pattern can \"survive\" (maintain, be retained) long enough or \"reproduce\" (replicate, be copied) sufficiently frequently so as not to disappear immediately. This is the heredity component: the information in the pattern must be retained or passed on. The second requirement is that during survival and reproduction variation (small changes in the pattern) can occur. The final requirement is that there is a selective \"preference\" so that certain variants tend to survive or reproduce \"better\" than others. If these conditions are met, then, by the logic of natural selection, the pattern will evolve towards more adapted forms.\n\nExamples of patterns that have been postulated to undergo variation and selection, and thus adaptation, are genes, ideas (memes), theories, technologies, neurons and their connections, words, computer programs, firms, antibodies, institutions, law and judicial systems, quantum states and even whole universes.\n\nConceptually, \"evolutionary theorizing about cultural, social, and economic phenomena\" preceded Darwin, but was still lacking the concept of natural selection. Darwin himself, together with subsequent 19th century thinkers such as Herbert Spencer, Thorstein Veblen, James Mark Baldwin and William James, was quick to apply the idea of selection to other domains, such as language, psychology, society, and culture. However, this evolutionary tradition was largely banned from the social sciences in the beginning of the 20th century, in part because of the bad reputation of social Darwinism, an attempt to use Darwinism to justify social inequality.\n\nStarting in the 1950s, Donald T. Campbell was one of the first and most influential authors to revive the tradition, and to formulate a generalized Darwinian algorithm directly applicable to phenomena outside of biology. In this, he was inspired by William Ross Ashby's view of self-organization and intelligence as fundamental processes of selection. His aim was to explain the development of science and other forms of knowledge by focusing on the variation and selection of ideas and theories, thus laying the basis for the domain of evolutionary epistemology. In the 1990s, Campbell's formulation of the mechanism of \"blind-variation-and-selective-retention\" (BVSR) was further developed and extended to other domains under the labels of \"universal selection theory\" or \"universal selectionism\" by his disciples Gary Cziko, Mark Bickhard, and Francis Heylighen.\n\nRichard Dawkins may have first coined the term \"universal Darwinism\" in 1983 to describe his conjecture that any possible life forms existing outside the solar system would evolve by natural selection just as they do on Earth. This conjecture was also presented in 1983 in a paper entitled “The Darwinian Dynamic” that dealt with the evolution of order in living systems and certain nonliving physical systems. It was suggested “that ‘life’, wherever it might exist in the universe, evolves according to the same dynamical law” termed the Darwinian dynamic. Henry Plotkin in his 1997 book on Darwin machines makes the link between universal Darwinism and Campbell's evolutionary epistemology. Susan Blackmore, in her 1999 book \"The Meme Machine\", devotes a chapter titled 'Universal Darwinism' to a discussion of the applicability of the Darwinian process to a wide range of scientific subject matters.\n\nThe philosopher of mind Daniel Dennett, in his 1995 book \"Darwin's Dangerous Idea\", developed the idea of a Darwinian process, involving variation, selection and retention, as a generic algorithm that is substrate-neutral and could be applied to many fields of knowledge outside of biology. He described the idea of natural selection as a \"universal acid\" that cannot be contained in any vessel, as it seeps through the walls and spreads ever further, touching and transforming ever more domains. He notes in particular the field of memetics in the social sciences.\n\nIn agreement with Dennett's prediction, over the past decades the Darwinian perspective has spread ever more widely, in particular across the social sciences as the foundation for numerous schools of study including memetics, evolutionary economics, evolutionary psychology, evolutionary anthropology, neural Darwinism, and evolutionary linguistics. Researchers have postulated Darwinian processes as operating at the foundations of physics, cosmology and chemistry via the theories of quantum Darwinism, observation selection effects and cosmological natural selection. Similar mechanisms are extensively applied in computer science in the domains of genetic algorithms and evolutionary computation, which develop solutions to complex problems via a process of variation and selection.\n\nAuthor D. B. Kelley has formulated one of the most all-encompassing approaches to Universal Darwinism. In his 2013 book \"The Origin of Everything\", he holds that natural selection involves not the preservation of favored races in the struggle for life, as shown by Darwin, but the preservation of favored systems in contention for existence. The fundamental mechanism behind all such stability and evolution is therefore what Kelley calls \"survival of the fittest systems.\" Because all systems are cyclical, the Darwinian processes of iteration, variation and selection are operative not only among species but among all natural phenomena both large-scale and small. Kelley thus maintains that, since the Big Bang especially, the universe has evolved from a highly chaotic state to one that is now highly ordered with many stable phenomena, naturally selected.\nThe following approaches can all be seen as exemplifying a generalization of Darwinian ideas outside of their original domain of biology. These \"Darwinian extensions\" can be grouped in two categories, depending on whether they discuss implications of biological (genetic) evolution in other disciplines (e.g. medicine or psychology), or discuss processes of variation and selection of entities other than genes (e.g. computer programs, firms or ideas). However, there is no strict separation possible, since most of these approaches (e.g. in sociology, psychology and linguistics) consider both genetic and non-genetic (e.g. cultural) aspects of evolution, as well as the interactions between them (see e.g. gene-culture coevolution).\n\n\n\n\n"}
{"id": "31040618", "url": "https://en.wikipedia.org/wiki?curid=31040618", "title": "Zeeman energy", "text": "Zeeman energy\n\nZeeman energy, or the external field energy, is the potential energy of a magnetised body in an external magnetic field. It is named after the Dutch physicist Pieter Zeeman, primarily known by the Zeeman effect. In SI units, it is given by\n\nwhere H is the external field, M the local magnetisation, and the integral is done over the volume of the body. This is the statistical average (over a \nunit volume macroscopic sample) of a corresponding microscopic Hamiltonial (energy) for each individual magnetic moment m, which is however experiencing a \"local\" induction B:\n\n"}
