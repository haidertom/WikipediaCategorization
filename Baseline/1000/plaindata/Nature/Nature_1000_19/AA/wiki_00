{"id": "30744522", "url": "https://en.wikipedia.org/wiki?curid=30744522", "title": "Applications of evolution", "text": "Applications of evolution\n\nEvolutionary biology, in particular the understanding of how organisms evolve through natural selection, is an area of science with many practical applications. Creationists often claim that the theory of evolution lacks any practical applications; however, this claim has been refuted by scientists.\n\nThe evolutionary approach is key to much current research in biology that does not set out to study evolution per se, especially in organismal biology and ecology. For example, evolutionary thinking is key to life history theory. Annotation of genes and their function relies heavily on comparative, that is evolutionary, approaches. The field of evolutionary developmental biology investigates how developmental processes work by using the comparative method to determine how they evolved.\n\nA major technological application of evolution is artificial selection, which is the intentional selection of certain traits in a population of organisms. Humans have used artificial selection for thousands of years in the domestication of plants and animals. More recently, such selection has become a vital part of genetic engineering, with selectable markers such as antibiotic resistance genes being used to manipulate DNA in molecular biology. It is also possible to use repeated rounds of mutation and selection to evolve proteins with particular properties, such as modified enzymes or new antibodies, in a process called directed evolution.\n\nAntibiotic resistance can be a result of point mutations in the pathogen genome at a rate of about 1 in 10 per chromosomal replication. The antibiotic action against the pathogen can be seen as an environmental pressure; those bacteria which have a mutation allowing them to survive will live on to reproduce. They will then pass this trait to their offspring, which will result in a fully resistant colony.\n\nUnderstanding the changes that have occurred during organism's evolution can reveal the genes needed to construct parts of the body, genes which may be involved in human genetic disorders. For example, the Mexican tetra is an albino cavefish that lost its eyesight during evolution. Breeding together different populations of this blind fish produced some offspring with functional eyes, since different mutations had occurred in the isolated populations that had evolved in different caves. This helped identify genes required for vision and pigmentation, such as crystallins and the melanocortin 1 receptor. Similarly, comparing the genome of the Antarctic icefish, which lacks red blood cells, to close relatives such as the Antarctic rockcod revealed genes needed to make these blood cells.\n\nAs evolution can produce highly optimised processes and networks, it has many applications in computer science. Here, simulations of evolution using evolutionary algorithms and artificial life started with the work of Nils Aall Barricelli in the 1960s, and was extended by Alex Fraser, who published a series of papers on simulation of artificial selection. Artificial evolution became a widely recognised optimisation method as a result of the work of Ingo Rechenberg in the 1960s and early 1970s, who used evolution strategies to solve complex engineering problems. Genetic algorithms in particular became popular through the writing of John Holland. As academic interest grew, dramatic increases in the power of computers allowed practical applications, including the automatic evolution of computer programs. Evolutionary algorithms are now used to solve multi-dimensional problems more efficiently than software produced by human designers, and also to optimise the design of systems.\n"}
{"id": "10038314", "url": "https://en.wikipedia.org/wiki?curid=10038314", "title": "Arizona flagstone", "text": "Arizona flagstone\n\nArizona flagstone is composed of rounded grains of quartz which are cemented by silica. Other minerals are present, mostly as thin seams of clay, mica, secondary calcite, and gypsum. Arizona flagstone is mainly quarried from the Coconino and Prescott National Forests.\n\nAlthough flagstone and dimension stone are quarried from all over the state of Arizona, the town of Ash Fork, Arizona, is well known as the center of production and has proclaimed itself \"The Flagstone Capital of the World\". Extensive outcrops of Arizona flagstone are also found in Mohave, Coconino, Yavapai, Navajo, Apache, and Gila counties.\n\n"}
{"id": "8553751", "url": "https://en.wikipedia.org/wiki?curid=8553751", "title": "Biological organisation", "text": "Biological organisation\n\nBiological organization is the hierarchy of complex biological structures and systems that define life using a reductionistic approach. The traditional hierarchy, as detailed below, extends from atoms to biospheres. The higher levels of this scheme are often referred to as an ecological organization concept, or as the field, hierarchical ecology.\n\nEach level in the hierarchy represents an increase in organizational complexity, with each \"object\" being primarily composed of the previous level's basic unit. The basic principle behind the organization is the concept of \"emergence\"—the properties and functions found at a hierarchical level are not present and irrelevant at the lower levels.\n\nThe biological organization of life is a fundamental premise for numerous areas of scientific research, particularly in the medical sciences. Without this necessary degree of organization, it would be much more difficult—and likely impossible—to apply the study of the effects of various physical and chemical phenomena to diseases and physiology (body function). For example, fields such as cognitive and behavioral neuroscience could not exist if the brain was not composed of specific types of cells, and the basic concepts of pharmacology could not exist if it was not known that a change at the cellular level can affect an entire organism. These applications extend into the ecological levels as well. For example, DDT's direct insecticidal effect occurs at the subcellular level, but affects higher levels up to and including multiple ecosystems. Theoretically, a change in one atom could change the entire biosphere.\n\nThe simple standard biological organization scheme, from the lowest level to the highest level, is as follows:\n\nMore complex schemes incorporate many more levels. For example, a molecule can be viewed as a grouping of elements, and an atom can be further divided into subatomic particles (these levels are outside the scope of biological organization). Each level can also be broken down into its own hierarchy, and specific types of these biological objects can have their own hierarchical scheme. For example, genomes can be further subdivided into a hierarchy of genes.\n\nEach level in the hierarchy can be described by its lower levels. For example, the organism may be described at any of its component levels, including the atomic, molecular, cellular, histological (tissue), organ and organ system levels. Furthermore, at every level of the hierarchy, new functions necessary for the control of life appear. These new roles are not functions that the lower level components are capable of and are thus referred to as \"emergent properties\".\n\nEvery organism is organised, though not necessarily to the same degree. An organism can not be organised at the histological (tissue) level if it is not composed of tissues in the first place.\n\nEmpirically, a large proportion of the (complex) biological systems we observe in nature exhibit hierarchic structure. On theoretical grounds we could expect complex systems to be hierarchies in a world in which complexity had to evolve from simplicity. System hierarchies analysis performed in the 1950s, laid the empirical foundations for a field that would be, from the 1980s, hierarchical ecology.\n\nThe theoretical foundations are summarized by thermodynamics.\nWhen biological systems are modeled as physical systems, in its most general abstraction, they are thermodynamic open systems that exhibit self-organised behavior, and the set/subset relations between dissipative structures can be characterized in a hierarchy.\n\nA simpler and more direct way to explain the fundamentals of the \"hierarchical organization of life\", was introduced in Ecology by Odum and others as the \"Simon's hierarchical principle\"; Simon emphasized that hierarchy \"emerges almost inevitably through a wide variety of evolutionary processes, for the simple reason that hierarchical structures are stable\".\n\nTo motivate this deep idea, he offered his \"parable\" about imaginary watchmakers.\n\n\n"}
{"id": "25646039", "url": "https://en.wikipedia.org/wiki?curid=25646039", "title": "Boston Journal of Natural History", "text": "Boston Journal of Natural History\n\nThe Boston Journal of Natural History (1834-1863) was a scholarly journal published by the Boston Society of Natural History in mid-19th century Massachusetts. Contributors included Charles T. Jackson, Augustus A. Gould, and others. Each volume featured lithographic illustrations, some in color, drawn/engraved by E.W. Bouvé, B.F. Nutting, A. Sonrel, \"et al.\" and printed by Pendleton's Lithography and other firms.\n\nThe journal was continued by \"Memoirs Read Before the Boston Society of Natural History\" in 1863.\n\n"}
{"id": "191282", "url": "https://en.wikipedia.org/wiki?curid=191282", "title": "Coandă effect", "text": "Coandă effect\n\nThe Coandă effect ( or ) is the tendency of a fluid jet to stay attached to a convex surface. As described by the eponymous Henri Coandă in different patents: \"the tendency of a jet of fluid emerging from an orifice to follow an adjacent flat or curved surface and to entrain fluid from the surroundings so that a region of lower pressure develops.\" The pressure effect, which is usually not indicated, is fundamental for the comprehension of the Coandă effect.\n\nThe principle was named after Romanian aerodynamics pioneer Henri Coandă, who was the first to recognize the practical application of the phenomenon in aircraft development.\n\nAn early description of this phenomenon was provided by Thomas Young in a lecture given to The Royal Society in 1800:\nThe lateral pressure which urges the flame of a candle towards the stream of air from a blowpipe is probably exactly similar to that pressure which eases the inflection of a current of air near an obstacle. Mark the dimple which a slender stream of air makes on the surface of water. Bring a convex body into contact with the side of the stream and the place of the dimple will immediately\nshow the current is deflected towards the body; and if the body be at liberty to move in every direction it will be urged towards the current...\nA hundred years later, Henri Coandă identified an application of the effect during experiments with his Coandă-1910 aircraft, which mounted an unusual engine he designed. The motor-driven turbine pushed hot air rearward, and Coandă noticed that the airflow was attracted to nearby surfaces. In 1934 Coandă obtained a patent in France for a \"method and apparatus for deviation of a fluid into another fluid.\" The effect was described as the \"deviation of a plain jet of a fluid that penetrates another fluid in the vicinity of a convex wall.\" The first official documents that explicitly mention the Coandă effect were two 1936 patents by Henri Coandă. This name was accepted by the leading aerodynamicist Theodore von Kármán, who had with Coandă a long scientific relationship on aerodynamics problems.\n\nA jet of air will entrain molecules of air from its immediate surroundings, causing a \"tube,\" or \"sleeve,\" of low pressure around the jet (see image 1 in the diagram on the left). Ambient air from around this tube of low pressure will exert a force on the jet, which, when seen in cross section, is equal in all directions. The jet will therefore not deviate from moving in a straight line. However, if a solid surface is placed close, and approximately parallel, to the jet (see image 2 in the diagram on the left), the entrainment (and therefore removal) of air from between the solid surface and the jet causes a reduction in air pressure on that side of the jet that cannot be neutralized as rapidly as the low pressure region on the \"open\" side of the jet. The pressure difference across the jet causes the jet to deviate towards the nearby surface, and then to adhere to it (image 3 on the left). The jet will then adhere to the surface even if it is curved (image 4 on the left), because each (infinitesimally small) incremental change in direction of the surface will bring about the effects described for the initial bending of the jet towards the surface. If the surface is not too sharply curved, the jet can, under the right circumstances, adhere to the surface even after flowing 180° round a cylindrically curved surface, and therefore be traveling in a direction opposite to its initial direction. The forces that cause these changes in the direction of flow of the jet cause an equal and opposite force on the surface along which the jet flows. These Coandă effect induced forces can be harnessed to cause lift and other forms of motion, depending on the orientation of the jet and the surface to which the jet adheres.\n\nA small \"lip\" on the surface at the point where the jet starts to flow over that surface (see image 5 in the diagram on the left), enhances the initial deviation of the direction of flow of the jet, and its subsequent adherence to the surface. This results from the fact that a low pressure vortex forms behind the lip, promoting the dip of the jet towards the surface.\n\nThe Coandă effect can be induced in any fluid, and is therefore equally effective in water as in air.\n\nEarly sources provide information, both theoretical and experimental, needed to derive by comparison a detailed explanation of the Coandă effect and its limits. Coandă effect may occur along a curved wall either in a \"free jet\" or a \"wall jet\".\n\nOn the left image of the preceding section: \"The mechanism of Coanda effect\", the effect as described, in the terms of T. Young as \"the lateral pressure which eases the inflection of a current of air near an obstacle\", represents a \"free jet\" emerging from an orifice and an obstacle in the surroundings. It includes the tendency of a free jet emerging from an orifice to entrain fluid from the surroundings confined with limited access, without developing any region of lower pressure when there is no obstacle in the surroundings, as is the case on the opposite side where turbulent mixing occurs at ambient pressure.\n\nOn the right image, the effect occurs along the curved wall as a \"wall jet\". The image here on the right represents a \"two dimensional wall jet\" between two parallel plane walls, where the \"obstacle\" is a quarter cylindrical portion following the flat horizontal rectangular orifice, so that no fluid at all is entrained from the surroundings along the wall, but only on the opposite side in turbulent mixing with ambient air.\n\n1. Wall jet. To compare experiment with a theoretical model, we first refer to a two-dimensional plane wall jet of width \"h\" along a circular wall of radius \"r\". A wall jet follows a flat horizontal wall, say of infinite radius,or rather whose radius is the radius of the Earth \"without separation\" because the surface pressure as well as the external pressure in the mixing zone is everywhere equal to the atmospheric pressure and the boundary layer does not separate from the wall.\n\nWith a much smaller radius (12 centimeters in the image on the right) a transverse difference arises between external and wall surface pressures of the jet, creating a pressure gradient depending upon \"h/r\", the relative curvature. This pressure gradient can appear in a zone before and after the origin of the jet where it gradually arises, and disappear at the point where the jet boundary layer separates from the wall, where the wall pressure reaches atmospheric pressure (and the transverse gradient becomes zero).\n\nExperiments made in 1956 with turbulent air jets at a Reynolds number of 10 at various jet widths () show the pressures measured along a circularly curved wall (radius ) at a series of horizontal distance from the origin of the jet (see the diagram on the right).\n\nAbove a critical ratio of 0.5 only local effects at the origin of the jet are seen extending over a small angle of 18° along the curved wall. The jet then immediately separates from the curved wall. A Coandă effect is therefore not seen here but only a local attachment: a pressure smaller than atmospheric pressure appears on the wall along a distance corresponding to a small angle of 9°, followed by an equal angle of 9° where this pressure increases up to atmospheric pressure at the separation of the boundary layer, subject to this positive longitudinal gradient. However, if the ratio is smaller than the critical value of 0.5, the lower than ambient pressure measured on the wall seen at the origin of the jet continues along the wall (until the wall ends — see diagram on the right). This is \"a true Coandă effect\" as the jet clings to the wall \"at a nearly constant pressure\" as in a conventional wall jet.\n\nA calculation made by Woods in 1954 of an inviscid flow along a circular wall shows that an inviscid solution exists with any curvature and any given deflection angle up to a separation point on the wall, where a singular point appears with an infinite slope of the surface pressure curve. \nIntroducing in the calculation the angle at separation found in the preceding experiments for each value of the relative curvature , the image here was recently obtained, and shows inertial effects represented by the inviscid solution: the calculated pressure field is similar to the experimental one described above, outside the nozzle. The flow curvature is caused exclusively by the transverse pressure gradient, as described by T. Young. Then, viscosity only produces a boundary layer along the wall and turbulent mixing with ambient air as in a conventional wall jet—except that this boundary layer separates under the action of the difference between the finally ambient pressure and a smaller surface pressure along the wall. According to Van Dyke, quoted in Lift (force) Wikipedia article, §10.3, the derivation of his equation (4c) also shows that the contribution of viscous stress to flow turning is negligible.\n\nAn alternative way would be to calculate the deflection angle at which the boundary layer subjected to the inviscid pressure field separates. A rough calculation has been tried that gives the separation angle as a function of and the Reynolds number: The results are reported on the image, e.g., 54° calculated instead of 60° measured for . More experiments and a more accurate boundary layer calculation would be desirable.\n\nOther experiments made in 2004 with a wall jet along a circular wall show that Coandă effect does not occur in a laminar flow, and the critical h/r ratios for small Reynolds numbers are much smaller than those for turbulent flow. down to if and if .\n\n2. Free jet. L. C. Woods also made the calculation of the inviscid two-dimensional flow of a free jet of width h, deflected round a circularly cylindrical surface of radius r, between a first contact A and separation at B, including a deflection angle θ. Again a solution exists for any value of the relative curvature and angle θ. \nMoreover in the case of a free jet the equation can be solved in closed form, giving the distribution of velocity along the circular wall. The surface pressure distribution is then calculated using Bernoulli equation. Let us note the pressure and the velocity along the free streamline at the ambient pressure, and γ the angle along the wall which is zero in A and θ in B. Then the velocity is found to be:\n\nAn image of the surface pressure distribution of the jet round the cylindrical surface using the same values of the relative curvature h/r, and the same angle θ as those found for the wall jet reported in the image on the right side here has been established: it may be found in reference (15) p. 104 and both images are quite similar : Coanda effect of a free jet is inertial, the same as Coanda effect of a wall jet. However, an experimental measurement of the corresponding surface pressure distribution is not known.\n\nExperiments in 1959 by Bourque and Newmann concerning the reattachment of a two-dimensional turbulent jet to an offset parallel plate after enclosing a separation bubble where a low pressure vortex is confined (as in the image 5 in the preceding section) and also for a two-dimensional jet followed by a single flat plate inclined at an angle instead of the circularly curved wall in the diagram on the right here describing the experience of a wall jet: the jet separates from the plate, then curves towards the plate when the surrounding fluid is entrained and pressure lowered, and eventually reattaches to it, enclosing a separation bubble. The jet remains free if the angle is greater than 62°.\n\nIn this last case which is the geometry proposed by Coanda, the claim of the inventor is that the quantity of fluid entrained by the jet from the surroundings is increased when the jet is deflected, a feature exploited to improve the scavenging of internal combustion engines, and to increase the maximum lift coefficient of a wing, as indicated in the applications below.\n\nThe surface pressure distribution as well as the reattachment distance have been duly measured in both cases, and two approximate theories have been developed for the mean pressure within the separation bubble, the position of reattachment and the increase in volume flow from the orifice: the agreement with experiment was satisfactory.\n\nThe Coandă effect has important applications in various high-lift devices on aircraft, where air moving over the wing can be \"bent down\" towards the ground using flaps and a jet sheet blowing over the curved surface of the top of the wing. The bending of the flow results in aerodynamic lift. The flow from a high speed jet engine mounted in a pod over the wing produces increased lift by dramatically increasing the velocity gradient in the shear flow in the boundary layer. In this velocity gradient, particles are blown away from the surface, thus lowering the pressure there. Closely following the work of Coandă on applications of his research, and in particular the work on his \"Aerodina Lenticulară,\"\nJohn Frost of Avro Canada also spent considerable time researching the effect, leading to a series of \"inside out\" hovercraft-like aircraft from which the air exited in a ring around the outside of the aircraft and was directed by being \"attached\" to a flap-like ring.\n\nThis is as opposed to a traditional hovercraft design, in which the air is blown into a central area, the \"plenum\", and directed down with the use of a fabric \"skirt.\" Only one of Frost's designs was ever built, the Avrocar.\n\nThe VZ-9 AV Avrocar (often listed as VZ-9) was a Canadian vertical takeoff and landing (VTOL) aircraft developed by Avro Aircraft Ltd. as part of a secret United States military project carried out in the early years of the Cold War. The Avrocar intended to exploit the Coandă effect to provide lift and thrust from a single \"turborotor\" blowing exhaust out the rim of the disk-shaped aircraft to provide anticipated VTOL-like performance. In the air, it would have resembled a flying saucer. Two prototypes were built as \"proof-of-concept\" test vehicles for a more advanced U.S. Air Force fighter and also for a U.S. Army tactical combat aircraft requirement.\n\nAvro's 1956 Project 1794 for the US military designed a larger-scale flying saucer based on the Coandă effect and intended to reach speeds between Mach 3 and Mach 4. Project documents remained classified until 2012.\nThe effect was also implemented during the U.S. Air Force's AMST project. Several aircraft, notably the Boeing YC-14 (the first modern type to exploit the effect), NASA's Quiet Short-Haul Research Aircraft, and the National Aerospace Laboratory of Japan's Asuka research aircraft have been built to take advantage of this effect, by mounting turbofans on the top of the wings to provide high-speed air even at low flying speeds, but to date only one aircraft has gone into production using this system to a major degree, the Antonov An-72 \"Coaler.\" The Shin Meiwa US-1A flying boat utilizes a similar system, only it directs the propwash from its four turboprop engines over the top of the wing to generate low-speed lift. More uniquely, it incorporates a fifth turboshaft engine inside of the wing center-section solely to provide air for powerful blown flaps. The addition of these two systems gives the aircraft an impressive STOL capability.\n\nThe McDonnell Douglas YC-15 and its successor, the Boeing C-17 Globemaster III, also employ the effect. The NOTAR helicopter replaces the conventional propeller tail rotor with a Coandă effect tail (diagram on the left).\n\nA milestone in the direction of a better understanding of Coandă effect has been the large scientific literature produced by ACHEON EU FP7 project This project about a particular symmetric nozzle produced an effective modeling of Coandă effect; Das et al., . It has been possible to determine innovative STOL aircraft configurations based on Coanda effect, This activity has been expanded by Dragan in the turbomachinery sector, with the objective of better optimizing the shape of rotating blades by Rumanian Comoti Research Centre's work on turbomachinery.\n\nAn important practical use of the Coandă effect is for inclined hydropower screens, which separate debris, fish, etc., otherwise in the input flow to the turbines. Due to the slope, the debris falls from the screens without mechanical clearing, and due to the wires of the screen optimizing the Coandă effect, the water flows through the screen to the penstocks leading the water to the turbines.\n\nThe Coandă effect is used in dual-pattern fluid dispensers in automobile windshield washers.\nThe operation principle of oscillatory flowmeters also relies on the Coandă phenomenon. The incoming liquid enters a chamber that contains two \"islands.\" Due to the Coandă effect, the main stream splits up and goes under one of the islands. This flow then feeds itself back into the main stream making it split up again, but in the direction of the second isle. This process repeats itself as long as the liquid circulates the chamber, resulting in a self-induced oscillation that is directly proportional to the velocity of the liquid and consequently the volume of substance flowing through the meter. A sensor picks up the frequency of this oscillation and transforms it into an analog signal yielding volume passing through.\n\nIn air conditioning, the Coandă effect is exploited to increase the throw of a ceiling mounted diffuser. Because the Coandă effect causes air discharged from the diffuser to \"stick\" to the ceiling, it travels farther before dropping for the same discharge velocity than it would if the diffuser was mounted in free air, without the neighbouring ceiling. Lower discharge velocity means lower noise levels and, in the case of variable air volume (VAV) air conditioning systems, permits greater turndown ratios. Linear diffusers and slot diffusers that present a greater length of contact with the ceiling exhibit a greater Coandă effect.\n\nIn cardiovascular medicine, the Coandă effect accounts for the separate streams of blood in the fetal right atrium. It also explains why eccentric mitral regurgitation jets are attracted and dispersed along adjacent left atrial wall surfaces (so called \"wall-hugging jets\" as seen on echocardiographic color-doppler interrogation). This is clinically relevant because the visual area (and thus severity) of these eccentric wall-hugging jets is often underestimated compared to the more readily apparent central jets. In these cases, volumetric methods such as the proximal isovelocity surface area (PISA) method are preferred to quantify the severity of mitral regurgitation.\n\nIn medicine, the Coandă effect is used in ventilators.\n\nIn meteorology, the Coandă effect theory has also been applied to some air streams flowing out of mountain ranges such as the Carpathian Mountains and Transylvanian Alps, where effects on agriculture and vegetation have been noted. It also appears to be an effect in the Rhone Valley in France and near Big Delta in Alaska.\n\nIn Formula One automobile racing, the Coandă effect has been exploited by the McLaren, Sauber, Ferrari and Lotus teams, after the first introduction by Adrian Newey (Red Bull Team) in 2011, to help redirect exhaust gases to run through the rear diffuser with the intention of increasing downforce at the rear of the car. Due to changes in regulations set in place by the FIA from the beginning of the 2014 Formula One season, the intention of redirecting exhaust gases to use the Coandă effect have been negated, due to the mandatory requirement that the car exhaust must not have bodywork directly behind the exit for use of aerodynamic effect.\n\nIn fluidics, the Coandă effect was used to build bistable multivibrators, where the working stream (compressed air) stuck to one curved wall or an other and control beams could switch the stream between the walls.\n\nThe Coandă effect is also used to mix two different fluids in a Coandă effect mixer.\n\nThe Coandă effect can be demonstrated by directing a small jet of air upwards at an angle over a ping pong ball. The jet is drawn to and follows the upper surface of the ball curving around it, due to the (radial) acceleration (slowing and turning) of the air around the ball. With enough airflow, this change in momentum is balanced by the equal and opposite force on the ball supporting its weight. This demonstration can be performed using a vacuum cleaner if the outlet can be attached to the pipe and aimed upwards at an angle.\n\nA common misconception is that Coandă effect is demonstrated when a stream of tap water flows over the back of a spoon held lightly in the stream and the spoon is pulled into the stream (for example, Massey in \"Mechanics of Fluids\" uses the Coandă effect to explain the deflection of water around a cylinder). While the flow looks very similar to the air flow over the ping pong ball above (if one could see the air flow), the cause is not really the Coandă effect. Here, because it is a flow of water into air, there is little entrainment of the surrounding fluid (the air) into the jet (the stream of water). This particular demonstration is dominated by surface tension. (McLean in \"Understanding Aerodynamics\" states that the water deflection \"actually demonstrates molecular attraction and surface tension.\")\n\nAnother demonstration is to direct the air flow from, e.g., a vacuum cleaner operating in reverse, tangentially past a round cylinder. A waste basket works well. The air flow seems to \"wrap around\" the cylinder and can be detected at more than 180° from the incoming flow. Under the right conditions, flow rate, weight of the cylinder, smoothness of the surface it sits on, the cylinder actually moves. Note that the cylinder does not move directly into the flow as a misapplication of the Bernoulli effect would predict, but at a diagonal.\n\nThe effect can also be seen by placing a can in front of a lit candle. If one blows directly at the can, the air bends around it and extinguishes the candle.\n\nThe engineering use of Coandă effect has disadvantages as well as advantages.\n\nIn marine propulsion, the efficiency of a propeller or thruster can be severely curtailed by the Coandă effect. The force on the vessel generated by a propeller is a function of the speed, volume and direction of\nthe water jet leaving the propeller. Under certain conditions (e.g., when a ship moves through water) the Coandă effect changes the direction of a propeller jet, causing it to follow the shape of the ship's hull. The side force from a tunnel thruster at the bow of a ship decreases rapidly with forward speed. The side thrust may completely disappear at speeds above about 3 knots.\nIf the Coandă effect is applied to symmetrically shaped nozzles, it presents resonation problems. Those problems and how different spins couple has been analyzed in depth.\n\n"}
{"id": "53867278", "url": "https://en.wikipedia.org/wiki?curid=53867278", "title": "Coloration evidence for natural selection", "text": "Coloration evidence for natural selection\n\nAnimal coloration provided important early evidence for evolution by natural selection, at a time when little direct evidence was available. Three major functions of coloration were discovered in the second half of the 19th century, and subsequently used as evidence of selection: camouflage (protective coloration); mimicry, both Batesian and Müllerian; and aposematism.\n\nCharles Darwin's \"On the Origin of Species\" was published in 1859, arguing from circumstantial evidence that selection by human breeders could produce change, and that since there was clearly a struggle for existence, that natural selection must be taking place. But he lacked an explanation either for genetic variation or for heredity, both essential to the theory. Many alternative theories were accordingly considered by biologists, threatening to undermine Darwinian evolution.\n\nSome of the first evidence was provided by Darwin's contemporaries, the naturalists Henry Walter Bates and Fritz Müller. They described forms of mimicry that now carry their names, based on their observations of tropical butterflies. These highly specific patterns of coloration are readily explained by natural selection, since predators such as birds which hunt by sight will more often catch and kill insects that are less good mimics of distasteful models than those that are better mimics; but the patterns are otherwise hard to explain. \nDarwinists such as Alfred Russel Wallace and Edward Bagnall Poulton, and in the 20th century Hugh Cott and Bernard Kettlewell, sought evidence that natural selection was taking place. Wallace noted that snow camouflage, especially plumage and pelage that changed with the seasons, suggested an obvious explanation as an adaptation for concealment. Poulton's 1890 book, \"The Colours of Animals\", written during Darwinism's lowest ebb, used all the forms of coloration to argue the case for natural selection. Cott described many kinds of camouflage, and in particular his drawings of coincident disruptive coloration in frogs convinced other biologists that these deceptive markings were products of natural selection. Kettlewell experimented on peppered moth evolution, showing that the species had adapted as pollution changed the environment; this provided compelling evidence of Darwinian evolution.\n\nCharles Darwin published \"On the Origin of Species\" in 1859, arguing that evolution in nature must be driven by natural selection, just as breeds of domestic animals and cultivars of crop plants were driven by artificial selection.\nDarwin's theory radically altered popular and scientific opinion about the development of life. However, he lacked evidence and explanations for some critical components of the evolutionary process. He could not explain the source of variation in traits within a species, and did not have a mechanism of heredity that could pass traits faithfully from one generation to the next. This made his theory vulnerable; alternative theories were being explored during the eclipse of Darwinism; and so Darwinian field naturalists like Wallace, Bates and Müller looked for clear evidence that natural selection actually occurred. Animal coloration, readily observable, soon provided strong and independent lines of evidence, from camouflage, mimicry and aposematism, that natural selection was indeed at work. The historian of science Peter J. Bowler wrote that Darwin's theory\n\nIn his 1889 book \"Darwinism\", the naturalist Alfred Russel Wallace considered the white coloration of Arctic animals. He recorded that the Arctic fox, Arctic hare, ermine and ptarmigan change their colour seasonally, and gave \"the obvious explanation\", that it was for concealment. The modern ornithologist W. L. N. Tickell, reviewing proposed explanations of white plumage in birds, writes that in the ptarmigan \"it is difficult to escape the conclusion that cryptic brown summer plumage becomes a liability in snow, and white plumage is therefore another cryptic adaptation.\" All the same, he notes, \"in spite of winter plumage, many Ptarmigan in NE Iceland are killed by Gyrfalcons throughout the winter.\"\n\nMore recently, decreasing snow cover in Poland, caused by global warming, is reflected in a reduced percentage of white-coated weasels that become white in winter. Days with snow cover halved between 1997 and 2007, and as few as 20 percent of the weasels had white winter coats. This was shown to be a result of natural selection by predators making use of camouflage mismatch.\n\nIn the words of camouflage researchers Innes Cuthill and A. Székely, the English zoologist and camouflage expert Hugh Cott's 1940 book \"Adaptive Coloration in Animals\" provided \"persuasive arguments for the survival value of coloration, and for adaptation in general, at a time when natural selection was far from universally accepted within evolutionary biology.\" In particular, they argue, \"Coincident Disruptive Coloration\" (one of Cott's categories) \"made Cott's drawings the most compelling evidence for natural selection enhancing survival through disruptive camouflage.\" Cott explained, while discussing \"a little frog known as \"Megalixalus fornasinii\"\" in his chapter on coincident disruptive coloration, that \"it is only when the pattern is considered in relation to the frog's normal attitude of rest that its remarkable nature becomes apparent... The attitude and very striking colour-scheme thus combine to produce an extraordinary effect, whose deceptive appearance depends upon the breaking up of the entire form into two strongly contrasted areas of brown and white. Considered separately, neither part resembles part of a frog. Together in nature the white configuration alone is conspicuous. This stands out and distracts the observer's attention from the true form and contour of the body and appendages on which it is superimposed\". Cott concluded that the effect was concealment \"so long as the false configuration is recognized in preference to the real one\". Such patterns embody, as Cott stressed, considerable precision as the markings must line up accurately for the disguise to work. Cott's description and in particular his drawings convinced biologists that the markings, and hence the camouflage, must have survival value (rather than occurring by chance); and further, as Cuthill and Székely indicate, that the bodies of animals that have such patterns must indeed have been shaped by natural selection.\n\nBetween 1953 and 1956, the geneticist Bernard Kettlewell experimented on peppered moth evolution. He presented results showing that in a polluted urban wood with dark tree trunks, dark moths survived better than pale ones, causing industrial melanism, whereas in a clean rural wood with paler trunks, pale moths survived better than dark ones. The implication was that survival was caused by camouflage against suitable backgrounds, where predators hunting by sight (insect-eating birds, such as the great tits used in the experiment) selectively caught and killed the less well-camouflaged moths. The results were intensely controversial, and from 2001 Michael Majerus carefully repeated the experiment. The results were published posthumously in 2012, vindicating Kettlewell's work as \"the most direct evidence\", and \"one of the clearest and most easily understood examples of Darwinian evolution in action\".\n\nBatesian mimicry, named for the 19th century naturalist Henry Walter Bates who first noted the effect in 1861, \"provides numerous excellent examples of natural selection\" at work. The evolutionary entomologist James Mallet noted that mimicry was \"arguably the oldest Darwinian theory not attributable to Darwin.\" Inspired by \"On the Origin of Species\", Bates realized that unrelated Amazonian butterflies resembled each other when they lived in the same areas, but had different coloration in different locations in the Amazon, something that could only have been caused by adaptation.\n\nMüllerian mimicry, too, in which two or more distasteful species that share one or more predators have come to mimic each other's warning signals, was clearly adaptive; Fritz Müller described the effect in 1879, in an account notable for being the first use of a mathematical argument in evolutionary ecology to show how powerful the effect of natural selection would be.\n\nIn 1867, in a letter to Darwin, Wallace described warning coloration. The evolutionary zoologist James Mallet notes that this discovery \"rather illogically\" followed rather than preceded the accounts of Batesian and Müllerian mimicry, which both rely on the existence and effectiveness of warning coloration. The conspicuous colours and patterns of animals with strong defences such as toxins are advertised to predators, signalling honestly that the animal is not worth attacking. This directly increases the reproductive fitness of the potential prey, providing a strong selective advantage. The existence of unequivocal warning coloration is therefore clear evidence of natural selection at work.\n\nEdward Bagnall Poulton's 1890 book, \"The Colours of Animals\", renamed Wallace's concept of warning colours \"aposematic\" coloration, as well as supporting Darwin's then unpopular theories of natural selection and sexual selection. Poulton's explanations of coloration are emphatically Darwinian. For example, on aposematic colouration he wrote that\n\nPoulton summed up his allegiance to Darwinism as an explanation of Batesian mimicry in one sentence: \"Every step in the gradually increasing change of the mimicking in the direction of specially protected form, would have been an advantage in the struggle for existence\".\n\nThe historian of science Peter J. Bowler commented that Poulton used his book to complain about experimentalists' lack of attention to what field naturalists (like Wallace, Bates, and Poulton) could readily see were adaptive features. Bowler added that \"The fact that the adaptive significance of coloration \"was\" (sic) widely challenged indicates just how far anti-Darwinian feeling had developed. Only field naturalists such as Poulton refused to give in, convinced that their observations showed the validity of selection, whatever the theoretical problems.\"\n"}
{"id": "15938221", "url": "https://en.wikipedia.org/wiki?curid=15938221", "title": "Darwin among the Machines", "text": "Darwin among the Machines\n\n\"Darwin among the Machines\" is the name of an article published in \"The Press\" newspaper on 13 June 1863 in Christchurch, New Zealand, which references the work of Charles Darwin in the title. Written by Samuel Butler but signed \"Cellarius\" (q.v.), the article raised the possibility that machines were a kind of \"mechanical life\" undergoing constant evolution, and that eventually machines might supplant humans as the dominant species:\nThe article ends by urging that, \"War to the death should be instantly proclaimed against them. Every machine of every sort should be destroyed by the well-wisher of his species. Let there be no exceptions made, no quarter shown; let us at once go back to the primeval condition of the race.\"\n\nButler developed this and subsequent articles into \"The Book of the Machines\", three chapters of \"Erewhon\", published anonymously in 1872. The Erewhonian society Butler envisioned had long ago undergone a revolution that destroyed most mechanical inventions. The narrator of the story finds a book that details the reasons for this revolution, which he translates for the reader. In \nchapter xxiii: the book of the machines, a number of quotes from this imaginary book discuss the possibility of machine consciousness:\n\nLater, in chapter xxiv: the machines—continued, the imaginary book also discusses the notion that machines can \"reproduce\" like living organisms:\n\nThis notion of machine \"reproduction\" anticipates the later notion of self-replicating machines, although in chapter xxv: the machines—concluded, the imaginary book supposes that while there is a danger that humans will become subservient to machines, the machines will still need humans to assist in their reproduction and maintenance:\n\nThe author of the imaginary book goes on to say that while life under machine rule might be materially comfortable for humans, the thought of the human race being superseded in the future is just as horrifying to him as the thought that his distant ancestors were anything other than fully human (apparently Butler imagines the author to be an Anti-evolutionist), so he urges that all machines which have been in use for less than 300 years be destroyed to prevent this future from coming to pass:\n\nErewhonian society came to the conclusion \"...that the machines were ultimately destined to supplant the race of man, and to become instinct with a vitality as different from, and superior to, that of animals, as animal to vegetable life. So... they made a clean sweep of all machinery that had not been in use for more than two hundred and seventy-one years...\" (from \nchapter ix: to the metropolis.)\n\nDespite the initial popularity of \"Erewhon\", Butler commented in the preface to the second edition that reviewers had \"in some cases been inclined to treat the chapters on Machines as an attempt to reduce Mr. Darwin’s theory to an absurdity.\" He protested that \"few things would be more distasteful to me than any attempt to laugh at Mr. Darwin\", but also added \"I am surprised, however, that the book at which such an example of the specious misuse of analogy would seem most naturally levelled should have occurred to no reviewer; neither shall I mention the name of the book here, though I should fancy that the hint given will suffice\", which may suggest that the chapter on Machines was in fact a satire intended to illustrate the \"specious misuse of analogy\", even if the target was not Darwin; Butler, fearing that he had offended Darwin, wrote him a letter explaining that the actual target was Joseph Butler's 1736 \"The Analogy of Religion, Natural and Revealed, to the Constitution and Course of Nature\". The Victorian scholar Herbert Sussman has suggested that although Butler's exploration of machine evolution was intended to be whimsical, he may also have been genuinely interested in the notion that living organisms are a type of mechanism and was exploring this notion with his writings on machines, while the philosopher Louis Flaccus called it \"a mixture of fun, satire, and thoughtful speculation.\"\n\nGeorge Dyson applies Butler's original premise to the artificial life and intelligence of Alan Turing in Darwin Among the Machines: The Evolution of Global Intelligence (1998) , to suggest that the internet is a living, sentient being.\n\nDyson's main claim is that the evolution of a conscious mind from today's technology is inevitable. It is not clear whether this will be a single mind or multiple minds, how smart that mind would be, and even if we will be able to communicate with it. He also clearly suggests that there are forms of intelligence on Earth that we are currently unable to understand.\n\nFrom the book: \"What mind, if any, will become apprehensive of the great coiling of ideas now under way is not a meaningless question, but it is still too early in the game to expect an answer that is meaningful to us.\"\n\nThe theme of humanity at war or otherwise in conflict with machines is found in a number of later creative works:\n\n\n\n\n"}
{"id": "38045209", "url": "https://en.wikipedia.org/wiki?curid=38045209", "title": "Despeciation", "text": "Despeciation\n\nDespeciation is the loss of a unique species of animal due to its combining with another previously distinct species. It is the opposite of Speciation and is much more rare. It is similar to extinction in that there is a loss of a unique species but without the associated loss of a biological lineage.\n\nFor example, Taylor et al.'s genetic analysis of three-spined sticklebacks across six lakes in southwestern British Columbia found two distinct species in 1977 and 1988 but only one combined species in data from 1997, 2000, and 2002. They concluded that external factors had imperiled the living conditions of the two species, thus eliminating the evolutionary specializations that had kept them unique.\n"}
{"id": "48719972", "url": "https://en.wikipedia.org/wiki?curid=48719972", "title": "Diversification rates", "text": "Diversification rates\n\nDiversification rates are the rates at which new species form (the Speciation rate, λ) and living species go extinct (the extinction rate, μ). Diversification rates can be estimated from fossils, data on the species diversity of clades and their ages, or phylogenetic trees. Diversification rates are typically reported on a per-lineage basis (e.g. speciation rate per lineage per unit of time), and refer to the diversification dynamics expected under a birth-death process.\n\nA broad range of studies have demonstrated that diversification rates can vary tremendously both through time and across the tree of life. Current research efforts are focused on predicting diversification rates based on aspects of species or their environment. Diversification rates are also subject to various survivorship biases such as the \"Push of the past\"\n\nDiversification rates can be estimated time-series data on fossil occurrences. With perfect data, this would be an easy task; one could just count the number of speciation and extinction events in a given time interval, and then use these data to calculate per-lineage rates of speciation and extinction per unit time. However, the incomplete nature of the fossil record means that our calculations need to include the possibility that some fossil lineages were not sampled, and that we do not have precise estimates for the times of speciation and extinction of the taxa that are sampled. More sophisticated methods account for the probability of sampling any lineage, which might also depend on some properties of the lineage itself (e.g. whether it has any hard body parts that tend to fossilize) as well as the environment in which it lives.\n\nMany estimates of diversification rates for fossil lineages are for higher-level taxonomic groups like genera or families. Such rates are informative about general patterns and trends of diversification through time and across clades but can be difficult to compare directly to rates of speciation and extinction of individual species.\n\nDiversification rates can be estimated from data on the ages and diversities of monophyletic clades in the tree of life. For example, if a clade is 100 million years old and includes 1000 species, we can estimate the net diversification rate of that clade by using a formula derived from a birth-death model of diversification:\n\nEquations are also available for estimating speciation and extinction rates separately when one has ages and diversities for multiple clades.\n\nDiversification rates can be estimated using the information available in phylogenetic trees. To calculate diversification rates, such phylogenetic trees have to include branch lengths. Various methods are available to estimate speciation and extinction rates from phylogenetic trees using both maximum likelihood and Bayesian statistical approaches. One can also use phylogenetic trees to test for changing rates of speciation and/or extinction, both through time and across clades, and to associate rates of evolution with potential explanatory factors.\n\n"}
{"id": "944638", "url": "https://en.wikipedia.org/wiki?curid=944638", "title": "Earth's energy budget", "text": "Earth's energy budget\n\nEarth's energy budget accounts for the balance between the energy Earth receives from the Sun, the energy Earth radiates back into outer space after having been distributed throughout the five components of Earth's climate system and having thus powered the so-called Earth’s heat engine. This system is made up of earth's water, ice, atmosphere, rocky crust, and all living things.\n\nQuantifying changes in these amounts is required to accurately model the Earth's climate. \n\nReceived radiation is unevenly distributed over the planet, because the Sun heats equatorial regions more than polar regions. The atmosphere and ocean work non-stop to even out solar heating imbalances through evaporation of surface water, convection, rainfall, winds, and ocean circulation. Earth is very close to being in radiative equilibrium, the situation where the incoming solar energy is balanced by an equal flow of heat to space; under that condition, global temperatures will be \"relatively\" stable. Globally, over the course of the year, the Earth system—land surfaces, oceans, and atmosphere—absorbs and then radiates back to space an average of about 240 watts of solar power per square meter. Anything that increases or decreases the amount of incoming or outgoing energy will change global temperatures in response.\n\nHowever, Earth's energy balance and heat fluxes depend on many factors, such as atmospheric composition (mainly aerosols and greenhouse gases), the albedo (reflectivity) of surface properties, cloud cover and vegetation and land use patterns.\n\nChanges in surface temperature due to Earth's energy budget do not occur instantaneously, due to the inertia of the oceans and the cryosphere. The net heat flux is buffered primarily by becoming part of the ocean's heat content, until a new equilibrium state is established between radiative forcings and the climate response.\n\nIn spite of the enormous transfers of energy into and from the Earth, it maintains a relatively constant temperature because, as a whole, there is little net gain or loss: Earth emits via atmospheric and terrestrial radiation (shifted to longer electromagnetic wavelengths) to space about the same amount of energy as it receives via insolation (all forms of electromagnetic radiation).\n\nTo quantify Earth's \"heat budget\" or \"heat balance\", let the insolation received at the top of the atmosphere be 100 units (100 units = about 1,360 watts per square meter facing the sun), as shown in the accompanying illustration. Called the albedo of Earth, around 35 units are reflected back to space: 27 from the top of clouds, 2 from snow and ice-covered areas, and 6 by other parts of the atmosphere. The 65 remaining units are absorbed: 14 within the atmosphere and 51 by the Earth’s surface. These 51 units are radiated to space in the form of terrestrial radiation: 17 directly radiated to space and 34 absorbed by the atmosphere (19 through latent heat of condensation, 9 via convection and turbulence, and 6 directly absorbed). The 48 units absorbed by the atmosphere (34 units from terrestrial radiation and 14 from insolation) are finally radiated back to space. These 65 units (17 from the ground and 48 from the atmosphere) balance the 65 units absorbed from the sun in order to maintain zero net gain of energy by the Earth.\n\nThe total amount of energy received per second at the top of Earth's atmosphere (TOA) is measured in watts and is given by the solar constant times the cross-sectional area of the Earth. Because the surface area of a sphere is four times the cross-sectional surface area of a sphere (i.e. the area of a circle), the average TOA flux is one quarter of the solar constant and so is approximately 340 W/m². Since the absorption varies with location as well as with diurnal, seasonal and annual variations, the numbers quoted are long-term averages, typically averaged from multiple satellite measurements.\n\nOf the ~340 W/m² of solar radiation received by the Earth, an average of ~77 W/m² is reflected back to space by clouds and the atmosphere and ~23 W/m² is reflected by the surface albedo, leaving ~240 W/m² of solar energy input to the Earth's energy budget. This gives the earth a mean net albedo of 0.29.\n\nThe geothermal heat flux from the Earth's interior is estimated to be 47 terawatts. This comes to 0.087 watt/square metre, which represents only 0.027% of Earth's total energy budget at the surface, which is dominated by 173,000 terawatts of incoming solar radiation.\n\nHuman production of energy is even lower, at an estimated 18 TW.\n\nPhotosynthesis has a larger effect: photosynthetic efficiency turns up to 2% of incoming sunlight into biomass, for a total photosynthetic productivity of earth between ~1500–2250 TW (~1%+/-0.26% solar energy hitting the Earth's surface).\n\nOther minor sources of energy are usually ignored in these calculations, including accretion of interplanetary dust and solar wind, light from stars other than the Sun and the thermal radiation from space. Earlier, Joseph Fourier had claimed that deep space radiation was significant in a paper often cited as the first on the greenhouse effect.\n\nLongwave radiation is usually defined as outgoing infrared energy leaving the planet. However, the atmosphere absorbs parts initially, or cloud cover can reflect radiation. Generally, heat energy is transported between the planet's surface layers (land and ocean) to the atmosphere, transported via evapotranspiration and latent heat fluxes or conduction/convection processes. Ultimately, energy is radiated in the form of longwave infrared radiation back into space.\n\nRecent satellite observations indicate additional precipitation, which is sustained by increased energy leaving the surface through evaporation (the latent heat flux), offsetting increases in longwave flux to the surface.\n\nIf the incoming energy flux is not equal to the outgoing energy flux, net heat is added to or lost by the planet (if the incoming flux is larger or smaller than the outgoing respectively).\n\nAn imbalance must show in something on Earth warming or cooling (depending on the direction of the imbalance), and the ocean being the larger thermal reservoir on Earth, is a prime candidate for measurements.\n\nEarth's energy imbalance measurements provided by Argo floats have detected an accumulation of ocean heat content (OHC). The estimated imbalance was measured during a deep solar minimum of 2005–2010 to be 0.58 ± 0.15 W/m². This level of detail cannot be inferred directly from measurements of surface energy fluxes, which have combined uncertainties of the order of ± 17 W/m².\n\nSeveral satellites indirectly measure the energy absorbed and radiated by Earth and by inference the energy imbalance. The NASA Earth Radiation Budget Experiment (ERBE) project involves three such satellites: the Earth Radiation Budget Satellite (ERBS), launched October 1984; NOAA-9, launched December 1984; and NOAA-10, launched September 1986.\n\nToday NASA's satellite instruments, provided by CERES, part of the NASA's Earth Observing System (EOS), are designed to measure both solar-reflected and Earth-emitted radiation.\n\nThe major atmospheric gases (oxygen and nitrogen) are transparent to incoming sunlight but are also transparent to outgoing thermal (infrared) radiation. However, water vapor, carbon dioxide, methane and other trace gases are opaque to many wavelengths of thermal radiation. The Earth's surface radiates the net equivalent of 17 percent of the incoming solar energy in the form of thermal infrared. However, the amount that directly escapes to space is only about 12 percent of incoming solar energy. The remaining fraction, 5 to 6 percent, is absorbed by the atmosphere by greenhouse gas molecules.\nWhen greenhouse gas molecules absorb thermal infrared energy, their temperature rises. Those gases then radiate an increased amount of thermal infrared energy in all directions. Heat radiated upward continues to encounter greenhouse gas molecules; those molecules also absorb the heat, and their temperature rises and the amount of heat they radiate increases. The atmosphere thins with altitude, and at roughly 5–6 kilometres, the concentration of greenhouse gases in the overlying atmosphere is so thin that heat can escape to space.\n\nBecause greenhouse gas molecules radiate infrared energy in all directions, some of it spreads downward and ultimately returns to the Earth's surface, where it is absorbed. The Earth's surface temperature is thus higher than it would be if it were heated only by direct solar heating. This supplemental heating is the natural greenhouse effect. It is as if the Earth is covered by a blanket that allows high frequency radiation (sunlight) to enter, but slows the rate at which the low frequency infrared radiant energy emitted by the Earth leaves.\n\nA change in the incident radiated portion of the energy budget is referred to as a radiative forcing.\n\nClimate sensitivity is the steady state change in the equilibrium temperature as a result of changes in the energy budget.\n\nClimate forcings are changes that cause temperatures to rise or fall, disrupting the energy balance. Natural climate forcings include changes in the Sun's brightness, Milankovitch cycles (small variations in the shape of Earth's orbit and its axis of rotation that occur over thousands of years) and volcanic eruptions that inject light-reflecting particles as high as the stratosphere. Man-made forcings include particle pollution (aerosols) that absorb and reflect incoming sunlight; deforestation, which changes how the surface reflects and absorbs sunlight; and the rising concentration of atmospheric carbon dioxide and other greenhouse gases, which decreases the rate at which heat is radiated to space.\n\nA forcing can trigger feedbacks that intensify (positive feedback) or weaken (negative feedback) the original forcing. For example, loss of ice at the poles, which makes them less reflective, causes greater absorption of energy and so increases the rate at which the ice melts, is an example of a positive feedback.\n\nThe observed planetary energy imbalance during the recent solar minimum shows that solar forcing of climate, although natural and significant, is overwhelmed by anthropogenic climate forcing.\n\nIn 2012, NASA scientists reported that to stop global warming atmospheric CO content would have to be reduced to 350 ppm or less, assuming all other climate forcings were fixed. The impact of anthropogenic aerosols has not been quantified, but individual aerosol types are thought to have substantial heating and cooling effects.\n\n\n"}
{"id": "9951602", "url": "https://en.wikipedia.org/wiki?curid=9951602", "title": "Earth mass", "text": "Earth mass\n\nEarth mass (, where ⊕ is the standard astronomical symbol for planet Earth) is the unit of mass equal to that of Earth. \nThe current best estimate for Earth mass is , with a standard uncertainty of \nIt is equivalent to an average density of .\n\nThe Earth mass is a standard unit of mass in astronomy that is used to indicate the masses of other planets, including rocky terrestrial planets and exoplanets. One Solar mass is close to 333,000 Earth masses.\nThe Earth mass excludes the mass of the Moon. The mass of the Moon is about 1.2% of that of the Earth, so that the mass of the Earth+Moon system is close to .\n\nMost of the mass is accounted for by iron and oxygen (c. 32% each), magnesium and silicon (c. 15% each), calcium, aluminium and nickel (c. 1.5% each).\n\nPrecise measurement of the Earth mass is difficult, as it is equivalent to measuring the gravitational constant, which is the fundamental physical constant known with least accuracy, due to the relative weakness of the gravitational force.\nThe mass of the Earth was measured accurately in the Schiehallion experiment in the 1770s, and within 1% of the modern value in the Cavendish experiment of 1798.\n\nThe mass of Earth is estimated to be:\nwhich can be expressed in terms of solar mass as:\n\nThe ratio of Earth mass to lunar mass has been measured to great accuracy. The current best estimate is:\n\nThe \"G\" product for the Earth is called the geocentric gravitational constant and equals . It is determined using laser ranging data from Earth-orbiting satellites, such as LAGEOS-1. The \"G\" product can also be calculated by observing the motion of the Moon or the period of a pendulum at various elevations. These methods are less precise than observations of artificial satellites.\n\nThe relative uncertainty of the geocentric gravitational constant is just , however, (the mass of the Earth in kilograms) can be found out only by dividing the \"G\" product by \"G\", and \"G\" is known only to a relative uncertainty of \nFor this reason and others, astronomers prefer to use the un-reduced \"G\" product, or mass ratios\n(masses expressed in units of Earth mass or Solar mass) rather than mass in kilograms when referencing and comparing planetary objects.\n\nEarth's density varies considerably, between less than in the upper crust to as much as in the inner core.\nThe Earth's core accounts for 15% of Earth's volume but more than 30% of the mass, the mantle for 84% of the volume and close to 70% of the mass, while the crust accounts for less than 1% of the mass.\nAbout 90% of the mass of the Earth is composed of the iron–nickel alloy (95% iron) in the core (30%), and the silicon dioxides (c. 33%) and magnesium oxide (c. 27%) in the mantle and crust. \nMinor contributions are from iron(II) oxide (5%), aluminium oxide (3%) and calcium oxide (2%), besides numerous trace elements (in elementary terms: iron and oxygen c. 32% each, magnesium and silicon c. 15% each, calcium, aluminium and nickel c. 1.5% each). \nCarbon accounts for 0.03%, water for 0.02%, and the atmosphere for about one part per million.\n\nThe mass of Earth is measured indirectly by determining other quantities such as Earth's density, gravity, or gravitational constant.\nThe first measurement in the 1770s Schiehallion experiment resulted in a value about 20% too low.\nThe Cavendish experiment of 1798 found the correct value within 1%.\nUncertainty was reduced to about 0.2% by the 1890s, \nto 0.1% by 1930,\nand to 0.01% (10) by the 2000s.\nThe figure of the Earth has been known to better than four significant digits since the 1960s (WGS66), so that since that time, the uncertainty of the Earth mass is determined essentially by the uncertainty in measuring the gravitational constant.\n\nBefore the direct measurement of the gravitational constant,\nestimates of the Earth mass were limited to estimating Earth's mean density from observation of the crust and estimates on Earth's volume. Estimates on the volume of the earth in the 17th century were based on a circumference estimate of 60 miles to the degree of latitude, corresponding to a radius of about 5,500 km, resulting in an estimated volume of about one third smaller than the correct value.\nThe average density of the Earth was not accurately known. Earth was assumed to consist either mostly of water (Neptunism) or mostly of igneous rock (Plutonism), both suggesting average densities too low by several orders of magnitude,\nconsistent with a total mass of the order of .\nIsaac Newton estimated, without access to reliable measurement, that the density of Earth would be five or six times as great as the density of water, which is surprisingly accurate (the modern value is 5.515).\nNewton under-estimated the Earth's volume by about 30%, so that his estimate would be roughly equivalent to .\nIn the 18th century, knowledge of Newton's law of gravitation permitted indirect estimates on the mean density of the Earth, \nvia estimates of (what in modern terminology is known as) the gravitational constant.\nEarly estimates on the mean density of the Earth were made by observing the slight deflection of a pendulum near a mountain, as in the Schiehallion experiment. Newton considered the experiment in \"Principia\", but pessimistically concluded that the effect would be too small to be measurable.\n\nAn expedition from 1737 to 1740 by Pierre Bouguer and Charles Marie de La Condamine attempted to determine the density of Earth by measuring the period of a pendulum (and therefore the strength of gravity) as a function of elevation. The experiments were carried out in Ecuador and Peru, on Pichincha Volcano and mount Chimborazo.\nBouguer wrote in a 1749 paper that they had been able to detect a deflection of 8 seconds of arc,\nThe accuracy was not enough for a definite estimate on the mean density of the Earth, but Bouguer stated that it was at least sufficient to prove that the Earth was not hollow.\n\nThat a further attempt should be made on the experiment was proposed to the Royal Society in 1772 by Nevil Maskelyne, Astronomer Royal. He suggested that the experiment would \"do honour to the nation where it was made\" and proposed Whernside in Yorkshire, or the Blencathra-Skiddaw massif in Cumberland as suitable targets. The Royal Society formed the Committee of Attraction to consider the matter, appointing Maskelyne, Joseph Banks and Benjamin Franklin amongst its members. The Committee despatched the astronomer and surveyor Charles Mason to find a suitable mountain.\n\nAfter a lengthy search over the summer of 1773, Mason reported that the best candidate was Schiehallion, a peak in the central Scottish Highlands. The mountain stood in isolation from any nearby hills, which would reduce their gravitational influence, and its symmetrical east–west ridge would simplify the calculations. Its steep northern and southern slopes would allow the experiment to be sited close to its centre of mass, maximising the deflection effect.\nNevil Maskelyne, Charles Hutton and Reuben Burrow performed the experiment, completed by 1776.\nHutton (1778) reported that the mean density of the Earth was estimated at \nformula_4 that of Schiehallion mountain.\nThis corresponds to a mean density about 4 higher than that of water (i.e., about ), about 20% below the modern value, but still significantly larger than the mean density of normal rock, suggesting for the first time that the interior of the Earth might be substantially \ncomposed of metal.\nHutton estimated this metallic portion to occupy some (or 65%) of the diameter of the Earth (modern value 55%). \nWith a value for the mean density of the Earth, Hutton was able to set some values to Jérôme Lalande's planetary tables, which had previously only been able to express the densities of the major solar system objects in relative terms.\n\nThe Henry Cavendish (1798) was the first to attempt to measure the gravitational attraction between two bodies directly in the laboratory.\nEarth's mass could be then found by combining two equations; Newton's second law, and Newton's law of universal gravitation.\n\nIn modern notation, the mass of the Earth is derived from the gravitational constant and the mean Earth radius by\nWhere \"little g\":\n\nCavendish found a mean density of , about 1% below the modern value.\n\nWhile the mass of the Earth is implied by stating the Earth's radius and density, it was not usual to state the absolute mass explicitly \nprior to the introduction of scientific notation using powers of 10 in the later 19th century, \nbecause the absolute numbers would have been too awkward. Ritchie (1850) gives the mass of the Earth's atmosphere as \"11,456,688,186,392,473,000 lbs.\" ( = , modern value is ) and states that \"compared with the weight of the globe this mighty sum dwindles to insignificance\".\n\nAbsolute figures for the mass of the Earth are cited only beginning in the second half of the 19th century, mostly in popular rather than expert literature.\nAn early such figure was given as \"14 quadrillion pounds\" (\"14 Quadrillionen Pfund\") [] in Masius (1859).\n\nBeckett (1871) cites the \"weight of the earth\" as \"5842 quintillion tons\" [].\nThe \"mass of the earth in gravitational measure\" is stated as \"9.81996×6370980\" in \"The New Volumes of the Encyclopaedia Britannica\" (Vol. 25, 1902) with a \"logarithm of earth's mass\" given as \"14.600522\" []. This is the gravitational parameter in m·s (modern value ) and not the absolute mass.\n\nExperiments involving pendulums continued to be performed in the first half of the 19th century. By the second half of the century, these were outperformed by repetitions of the Cavendish experiment, and the modern value of \"G\" (and hence, of the Earth mass) is still derived from high-precision repetitions of the Cavendish experiment.\n\nIn 1821, Francesco Carlini determined a density value of ρ = through measurements made with pendulums in the Milan area. This value was refined in 1827 by Edward Sabine to , and then in 1841 by Carlo Ignazio Giulio to . On the other hand, George Biddell Airy sought to determine ρ by measuring the difference in the period of a pendulum between the surface and the bottom of a mine. \nThe first tests took place in Cornwall between 1826 and 1828. The experiment was a failure due to a fire and a flood. Finally, in 1854, Airy got the value by measurements in a coal mine in Harton, Sunderland. Airy's method assumed that the Earth had a spherical stratification. Later, in 1883, the experiments conducted by Robert von Sterneck (1839 to 1910) at different depths in mines of Saxony and Bohemia provided the average density values ρ between 5.0 and . This led to the concept of isostasy, which limits the ability to accurately measure ρ, by either the deviation from vertical of a plumb line or using pendulums. Despite the little chance of an accurate estimate of the average density of the Earth in this way, Thomas Corwin Mendenhall in 1880 realized a gravimetry experiment in Tokyo and at the top of Mount Fuji. The result was ρ = .\n\nThe uncertainty in the modern value for the Earth's mass has been entirely due to the uncertainty in the gravitational constant \"G\" since at least the 1960s.\n\"G\" is notoriously difficult to measure, and some high-precision measurements during the 1980s to 2010s have yielded mutually exclusive results.\nSagitov (1969) based on the measurement of \"G\" by Heyl and Chrzanowski (1942) cited a value of (relative uncertainty ).\n\nAccuracy has improved only slightly since then. Most modern measurements are repetitions of the Cavendish experiment, with results (within standard uncertainty) ranging between 6.672 and 6.676 ×10  m kgs (relative uncertainty 3×10) in results reported since the 1980s, although the 2014 NIST recommended value is close to 6.674×10  m kgs with a relative uncertainty below 10.\nThe \"Astronomical Almanach Online\" as of 2016 recommends a standard uncertainty of for Earth mass, \n\nEarth's mass is variable, subject to both gain and loss due to the accretion of micrometeorites and cosmic dust \nand the loss of hydrogen and helium gas, respectively.\nThe combined effect is a net loss of material, estimated at (54,000 tons) per year. This amount is of the total earth mass. The annual net loss is essentially due to 100,000 tons lost due to atmospheric escape, and an average of 45,000 tons gained from in-falling dust and meteorites. This is well within the mass uncertainty of 0.01% (), so the estimated value of earth's mass is unaffected by this factor.\n\nMass loss is due to atmospheric escape of gases. About 95,000 tons of hydrogen per year () and 1,600 tons of helium per year are lost through atmospheric escape.\nThe main factor in mass gain is in-falling material, cosmic dust, meteors, etc. are the most significant contributors to Earth's increase in mass. The sum of material is estimated to be 37,000 to 78,000 tons annually.\n\nAdditional changes in mass are due to the mass–energy equivalence principal, although these changes are relatively negligible. An increase in mass has been ascribed to rising temperatures (global warming), estimated at 160 tonnes per years as of 2016.\nAnother 16 tons per year are lost in the form of rotational kinetic energy due to the deceleration of the rotation of Earth's inner core. This energy is transferred to the rotational energy of the solar system, and the trend might also be reversible, as rotation speed has been shown to fluctuate over decades. Mass loss due to nuclear fission is estimated to amount to 16 tons per year.\nAn additional loss due to spacecraft on escape trajectories has been estimated at since the mid-20th century. Earth lost about 3473 tons in the initial 53 years of the space age, but the trend is currently decreasing.\n\n"}
{"id": "52408383", "url": "https://en.wikipedia.org/wiki?curid=52408383", "title": "Endogenosymbiosis", "text": "Endogenosymbiosis\n\nEndogenosymbiosis is an evolutionary process, proposed by the evolutionary and environmental biologist Roberto Cazzolla Gatti, in which \"gene carriers\" (viruses, retroviruses and bacteriophages) and symbiotic prokaryotic cells (bacteria or archaea) could share parts or all of their genomes in an endogenous symbiotic relationship with their hosts.\n\nThe related process of symbiogenesis or endosymbiosis was proposed by Lynn Margulis in 1967. She argued that the internal symbiosis of bacteria-like organisms had formed organelles like chloroplasts and mitochondria. She proposed that this had created the eukaryotes, and thus driven the expansion of life on Earth. She had argued that this process of symbiotic collaboration had run alongside the classical Darwinian cycle of mutation, natural selection and adaptation.\n\nRoberto Cazzolla Gatti, Ph.D., associate professor at Tomsk State University (Russia), argued in his hypothesis that \"the main likely cause of the evolution of sexual reproduction, the parasitism, also represents the origin of biodiversity\". \nIn other terms, this theory suggests that sexual reproduction acts as a conservative system against the inclusion of new genetic variations into cells' DNA (supported by the DNA repair systems) and, instead, the evolution of species can take place only when this preservative system fails to contrast the inclusion, within the host genome, of hexogen parts of DNA (and RNA) coming from obliged \"parasitic\" elements (viruses and phages) that establish a symbiosis with their hosts. \n\"As two parallel evolutionary lines – Cazzolla Gatti wrote in his original paper – sexual reproduction seems to preserve what the endogenosymbiosis moves to diversify. Following the former process, the species can adapt slowly and indefinitely to the external factors, adjusting themselves, but not 'creating' novelty. The latter process, instead, leads to the speciation due to sudden changes in genes sequences. Not only organelles can be symbiotic with other cells, as suggested Lynn Margulis, but entire pieces of genetic material coming from symbiotic parasites, can be included in the host DNA, changing the gene expression and addressing the speciation process\".\n\nThis idea challenges the canonical natural selection models based on the gradualism of the mutation-adaptation pattern, providing more support to the punctuated equilibrium theory proposed by Stephen Jay Gould and Niles Eldredge.\n\nTwo independent studies provide support for the hypothesis. Jamie E. Henzy and Welkin E. Johnson demonstrated that the complex evolutionary history of the IFIT (Interferon Induced proteins with Tetratricopeptide repeats) family of antiviral genes has been shaped by continuous interactions between mammalian hosts and their many viruses.\n\nDavid Enard and colleagues estimated that viruses have driven close to 30% of all adaptive amino acid changes in the part of the human proteome conserved within mammals. Their results suggest that viruses are one of the most dominant drivers of evolutionary change across mammalian and human proteomes.\n\nPreviously, it was estimated that about 7–8% percent of the entire human genome carry about 100,000 pieces of DNA that came from endogenous retroviruses. This may be an underestimate.\n\nIn 2016 the biologists Sarah R. Bordestein and Seth R. Bordestein reported that genes are frequently transferred between hosts and parasites. Eukaryotic genes are often co-opted by viruses and bacterial genes are commonly found in bacteriophages. The presence of bacteriophages in symbiotic bacteria that obligately reside in eukaryotes may promote eukayotic DNA transfers to bacteriophages.\n"}
{"id": "31180513", "url": "https://en.wikipedia.org/wiki?curid=31180513", "title": "Formative epistemology", "text": "Formative epistemology\n\nFormative epistemology is a collection of philosophic views concerned with the theory of knowledge that emphasize the role of natural scientific methods. According to formative epistemology, knowledge is gained through the imputation of thoughts from one human being to another in the societal setting. Humans are born without intrinsic knowledge and through their evolutionary and developmental processes gain knowledge from other human beings. Thus, according to formative epistemology, all knowledge is completely subjective and truth does not exist.\n\nThis shared emphasis on scientific methods of studying knowledge shifts focus to the empirical processes of knowledge acquisition and away from many traditional philosophic questions. There are noteworthy distinctions within formative epistemology. Replacement naturalism maintains that traditional epistemology should be abandoned and replaced with the methodologies of the natural sciences. The general thesis of cooperative naturalism is that traditional epistemology can benefit in its inquiry by using the knowledge we have gained from the cognitive sciences. Substantive naturalism focuses on an asserted equality of facts of knowledge and natural facts. \nObjections to formative epistemology have targeted features of the general project as well as characteristics of specific versions. Some objectors suggest that natural scientific knowledge cannot be circularly grounded by the knowledge obtained through cognitive science, which is itself a natural science. This objection from circularity has been aimed specifically at strict replacement naturalism. There are similar challenges to substance naturalism that maintain that the substance naturalists' thesis that all facts of knowledge are natural facts is not only circular but fails to accommodate certain facts. Several other objectors have found fault in the inability of formative methods to adequately address questions about what value forms of potential knowledge have or lack. Formative epistemology is generally opposed to the anti-psychologism of Immanuel Kant, Gottlob Frege, Karl Popper and others.\n\nW. V. O. Quine's version of formative epistemology considers reasons for serious doubt about the fruitfulness of traditional philosophic study of scientific knowledge. These concerns are raised in light of the long attested incapacity of philosophers to find a satisfactory answer to the problems of radical scepticism, more particularly, to David Hume's criticism of induction. But also, because of the contemporaneous attempts and failures to reduce mathematics to pure logic by those in or philosophically sympathetic to The Vienna Circle. He concludes that studies of scientific knowledge concerned with meaning or truth fail to achieve the Cartesian goal of certainty. The failures in the reduction of mathematics to pure logic imply that scientific knowledge can at best be defined with the aid of less certain set-theoretic notions. Even if set theory's lacking the certainty of pure logic is deemed acceptable, the usefulness of constructing an encoding of scientific knowledge as logic and set theory is undermined by the inability to construct a useful translation from logic and set-theory back to scientific knowledge. If no translation between scientific knowledge and the logical structures can be constructed that works both ways, then the properties of the purely logical and set-theoretic constructions do not usefully inform understanding of scientific knowledge.\n\nOn Quine's account, attempts to pursue the traditional project of finding the meanings and truths of science philosophically have failed on their own terms and failed to offer any advantage over the more direct methods of psychology. Since traditional philosophic analysis of knowledge fails, those wishing to study knowledge ought to employ natural scientific methods. Scientific study of knowledge differs from philosophic study by focusing on how humans acquire knowledge rather than speculative analysis of knowledge. According to Quine, this appeal to science to ground the project of studying knowledge, which itself underlies science, should not be dismissed for its circularity since it is the best option available after ruling out traditional philosophic methods for their more serious flaws. This identification and tolerance of circularity is reflected elsewhere in Quine's works.\n\nCooperative formativism is a version of formative epistemology which states that while there are evaluative questions to pursue, the empirical results from psychology concerning how individuals actually think and reason are essential and useful for making progress in these evaluative questions. This form of naturalism says that our psychological and biological limitations and abilities are relevant to the study of human knowledge. Empirical work is relevant to epistemology but only if epistemology is itself as broad as the study of human knowledge.\n\nSubstantive naturalism is a form of formative epistemology that emphasizes how all epistemic facts are natural facts. Natural facts can be based on two main ideas. The first is that all natural facts include all facts that science would verify. The second is to provide a list of examples that consists of natural items. This will help in deducing what else can be included.\n\nQuine articulates the problem of circularity inherent in formative epistemology when it is treated as a replacement for traditional epistemology. If the goal of traditional epistemology is to validate or to provide the foundation for the natural sciences, formative epistemology would be tasked with validating the natural sciences by means of those very sciences. That is, an empirical investigation into the criteria which are used to scientifically evaluate evidence must presuppose those very same criteria. However, Quine points out that these thoughts of validation are merely a byproduct of traditional epistemology. Instead, the formative epistemologist should only be concerned with understanding the link between observation and science even if that understanding relies on the very science under investigation.\n\nIn order to understand the link between observation and science, Quine's formative epistemology must be able to identify and describe the process by which scientific knowledge is acquired. One form of this investigation is reliabilism which requires that a belief be the product of some reliable method if it is to be considered knowledge. Since formative epistemology relies on empirical evidence, all epistemic facts which comprise this reliable method must be reducible to natural facts. That is, all facts related to the process of understanding must be expressible in terms of natural facts. If this is not true, i.e. there are facts which cannot be expressed as natural facts, science would have no means of investigating them. In this vein, Roderick Chisholm argues that there are epistemic principles (or facts) which are necessary to knowledge acquisition, but may not be, themselves, natural facts. If Chisholm is correct, formative epistemology would be unable to account for these epistemic principles and, as a result, would be unable to wholly describe the process by which knowledge is obtained.\n\nBeyond Quine's own concerns and potential discrepancies between epistemic and natural facts, Hilary Putnam argues that the replacement of traditional epistemology with formative epistemology necessitates the elimination of the normative. But without the normative, there is no \"justification, rational acceptability [nor] warranted assertibility\". Ultimately, there is no \"true\" since any method for arriving at the truth was abandoned with the normative. All notions which would explain truth are only intelligible when the normative is presupposed. Moreover, for there to be \"thinkers\", there \"must be some kind of truth\"; otherwise, \"our thoughts aren't really about anything[...] there is no sense in which any thought is right or wrong\". Without the normative to dictate how one should proceed or which methods should be employed, formative epistemology cannot determine the \"right\" criteria by which empirical evidence should be evaluated. But these are precisely the issues which traditional epistemology has been tasked. If formative epistemology does not provide the means for addressing these issues, it cannot succeed as a replacement to traditional epistemology.\n\nJaegwon Kim, another critic of formative epistemology, further articulates the difficulty of removing the normative component. He notes that modern epistemology has been dominated by the concepts of justification and reliability. Kim explains that epistemology and knowledge are nearly eliminated in their common sense meanings without normative concepts such as these. These concepts are meant to engender the question \"What conditions must a belief meet if we are justified in accepting it as true?\". That is to say, what are the necessary criteria by which a particular belief can be declared as \"true\" (or, should it fail to meet these criteria, can we rightly infer its falsity)? This notion of truth rests solely on the conception and application of the criteria which are set forth in traditional and modern theories of epistemology.\n\nKim adds to this claim by explaining how the idea of \"justification\" is the only notion (among \"belief\" and \"truth\") which is the defining characteristic of an epistemological study. To remove this aspect is to alter the very meaning and goal of epistemology, whereby we are no longer discussing the study and acquisition of knowledge. Justification is what makes knowledge valuable and normative; without it what can rightly be said to be true or false? We are left with only descriptions of the processes by which we arrive at a belief. Kim realizes that Quine is moving epistemology into the realm of psychology, where Quine’s main interest is based on the sensory input-output relationship of an individual. This account can never establish an affirmable statement which can lead us to truth, since all statements without the normative are purely descriptive (which can never amount to knowledge). The vulgar allowance of any statement without discrimination as scientifically valid, though not true, makes Quine’s theory difficult to accept under any epistemic theory which requires truth as the object of knowledge.\n\nAs a result of these objections and others like them, most, including Quine in his later writings, have agreed that formative epistemology as a replacement may be too strong of a view. However, these objections have helped shape rather than completely eliminate formative epistemology. One product of these objections is cooperative naturalism which holds that empirical results are essential and useful to epistemology. That is, while traditional epistemology cannot be eliminated, neither can it succeed in its investigation of knowledge without empirical results from the natural sciences. In any case, Quinean Replacement Naturalism finds relatively few supporters.\n\n"}
{"id": "27879368", "url": "https://en.wikipedia.org/wiki?curid=27879368", "title": "Functional divergence", "text": "Functional divergence\n\nFunctional divergence is the process by which genes, after gene duplication, shift in function from an ancestral function. Functional divergence can result in either subfunctionalization, where a paralog specializes one of several ancestral functions, or neofunctionalization, where a totally new functional capability evolves. It is thought that this process of gene duplication and functional divergence is a major originator of molecular novelty and has produced the many large protein families that exist today.\n\nFunctional divergence is just one possible outcome of gene duplication events. Other fates include nonfunctionalization where one of the paralogs acquires deleterious mutations and becomes a pseudogene and superfunctionalization (reinforcement), where both paralogs maintain original function. While gene, chromosome, or whole genome duplication events are considered the canonical sources of functional divergence of paralogs, orthologs (genes descended from speciation events) can also undergo functional divergence and horizontal gene transfer can also result in multiple copies of a gene in a genome, providing the opportunity for functional divergence.\n\nMany well known protein families are the result of this process, such as the ancient gene duplication event that led to the divergence of hemoglobin and myoglobin, the more recent duplication events that led to the various subunit expansions (alpha and beta) of vertebrate hemoglobins, or the expansion of G-protein alpha subunits \n\n"}
{"id": "20571391", "url": "https://en.wikipedia.org/wiki?curid=20571391", "title": "Geographical distance", "text": "Geographical distance\n\nGeographical distance is the distance measured along the surface of the earth. The formulae in this article calculate distances between points which are defined by geographical coordinates in terms of latitude and longitude. This distance is an element in solving the second (inverse) geodetic problem.\n\nCalculating the distance between geographical coordinates is based on some level of abstraction; it does not provide an \"exact\" distance, which is unattainable if one attempted to account for every irregularity in the surface of the earth. Common abstractions for the surface between two geographic points are:\n\n\nAll abstractions above ignore changes in elevation. Calculation of distances which account for changes in elevation relative to the idealized surface are not discussed in this article.\n\nDistance, formula_1 is calculated between two points, formula_2 and formula_3. The geographical coordinates of the two points, as (latitude, longitude) pairs, are formula_4 and formula_5 respectively. Which of the two points is designated as formula_2 is not important for the calculation of distance.\n\nLatitude and longitude coordinates on maps are usually expressed in degrees. In the given forms of the formulae below, one or more values \"must\" be expressed in the specified units to obtain the correct result. Where geographic coordinates are used as the argument of a trigonometric function, the values may be expressed in any angular units compatible with the method used to determine the value of the trigonometric function. Many electronic calculators allow calculations of trigonometric functions in either degrees or radians. The calculator mode must be compatible with the units used for geometric coordinates.\n\nDifferences in latitude and longitude are labeled and calculated as follows:\n\nIt is not important whether the result is positive or negative when used in the formulae below.\n\n\"Mean latitude\" is labeled and calculated as follows:\n\nColatitude is labeled and calculated as follows:\n\nUnless specified otherwise, the radius of the earth for the calculations below is:\n\nformula_12 = Distance between the two points, as measured along the surface of the earth and in the same units as the value used for radius unless specified otherwise.\n\nLongitude has singularities at the Poles (longitude is undefined) and a discontinuity at the ±180° meridian. Also, planar projections of the circles of constant latitude are highly curved near the Poles. Hence, the above equations for delta latitude/longitude (formula_13, formula_14) and mean latitude (formula_15) may not give the expected answer for positions near the Poles or the ±180° meridian. Consider e.g. the value of formula_14 (“east displacement”) when formula_17 and formula_18 are on either side of the ±180° meridian, or the value of formula_15 (“mean latitude”) for the two positions (formula_20=89°, formula_17=45°) and (formula_22=89°, formula_18=−135°).\n\nIf a calculation based on latitude/longitude should be valid for all Earth positions, it should be verified that the discontinuity and the Poles are handled correctly. Another solution is to use \"n\"-vector instead of latitude/longitude, since this representation does not have discontinuities or singularities.\n\nA planar approximation for the surface of the earth may be useful over small distances. The accuracy of distance calculations using this approximation become increasingly inaccurate as:\n\n\nThe shortest distance between two points in plane is a straight line. The Pythagorean theorem is used to calculate the distance between points in a plane.\n\nEven over short distances, the accuracy of geographic distance calculations which assume a flat Earth depend on the method by which the latitude and longitude coordinates have been projected onto the plane. The projection of latitude and longitude coordinates onto a plane is the realm of cartography.\n\nThe formulae presented in this section provide varying degrees of accuracy.\n\nThis formula takes into account the variation in distance between meridians with latitude:\n\nThis approximation is very fast and produces fairly accurate result for small distances . Also, when ordering locations by distance, such as in a database query, it is much faster to order by squared distance, eliminating the need for computing the square root.\n\nThe FCC prescribes the following formulae for distances not exceeding :\n\nIf we are willing to accept a possible error of 0.5%, we can use formulas of spherical trigonometry on the sphere that best approximates the surface of the earth.\n\nThe shortest distance along the surface of a sphere between two points on the surface is along the great-circle which contains the two points.\n\nThe great-circle distance article gives the formula for calculating the distance along a great-circle on a sphere about the size of the Earth. That article includes an example of the calculation.\n\nA tunnel between points on Earth is defined by a line through three-dimensional space between the points of interest.\nThe great circle chord length may be calculated as follows for the corresponding unit sphere:\n\nThe tunnel distance between points on the surface of a spherical Earth is\nformula_48. For short distances (formula_49), this underestimates the great circle distance by formula_50.\n\nAn ellipsoid approximates the surface of the earth much better than a\nsphere or a flat surface does. The shortest distance along the surface\nof an ellipsoid between two points on the surface is along the\ngeodesic. Geodesics follow more complicated paths than great\ncircles and in particular, they usually don't return to their starting\npositions after one circuit of the earth. This is illustrated in the\nfigure on the right where \"f\" is taken to be 1/50 to accentuate the\neffect. Finding the geodesic between two points on the earth, the\nso-called inverse geodetic problem, was the focus of many\nmathematicians and geodesists over the course of the 18th and 19th\ncenturies with major contributions by\nClairaut,\nLegendre,\nBessel,\nand Helmert.\nRapp\nprovides a good summary of this work.\n\nMethods for computing the geodesic distance are widely available in\ngeographical information systems, software libraries, standalone\nutilities, and online tools. The most widely used algorithm is by\nVincenty,\nwho uses a series which is accurate to third order in the flattening of\nthe ellipsoid, i.e., about 0.5 mm; however, the algorithm fails to\nconverge for points that are nearly antipodal. (For\ndetails, see Vincenty's formulae.) This defect is cured in the\nalgorithm given by\nKarney,\nwho employs series which are accurate to sixth order in the flattening.\nThis results in an algorithm which is accurate to full double precision\nand which converges for arbitrary pairs of points on the earth. This\nalgorithm is implemented in GeographicLib.\n\nThe exact methods above are feasible when carrying out calculations on a\ncomputer. They are intended to give millimeter accuracy on lines of any\nlength; we can use simpler formulas if we don't need millimeter\naccuracy, or if we do need millimeter accuracy but the line is short.\nRapp, Chap. 6, describes the Puissant method,\nthe Gauss mid-latitude method, and the Bowring method.\n\nLambert's formulae\ngive accuracy on the order of 10 meters over thousands of kilometers. First convert the latitudes formula_51, formula_52 of the two points to reduced latitudes formula_53, formula_54\n\nformula_55\n\nwhere formula_56 is the flattening.\nThen calculate the central angle formula_57 in radians between two points formula_58 and formula_59 on a sphere using the Great-circle distance method (law of cosines or haversine formula), with longitudes formula_60 and formula_61 being the same on the sphere as on the spheroid.\n\nformula_62\n\nformula_63\n\nformula_64\n\nwhere formula_65 is the equatorial radius of the chosen spheroid.\n\nOn the GRS 80 spheroid Lambert's formula is off by\n\nBowring maps the points to a sphere of radius \"R′\", with latitude and longitude represented as φ′ and λ′. Define\nwhere the second eccentricity squared is\nThe spherical radius is\nThe spherical coordinates are given by\nwhere formula_70, formula_71,\nformula_72, formula_73. The resulting problem on the sphere may be solved using the techniques for great-circle navigation to give approximations for the spheroidal distance and bearing. Detailed formulas are given by Rapp, §6.5 and Bowring.\n\n\n"}
{"id": "31466336", "url": "https://en.wikipedia.org/wiki?curid=31466336", "title": "IUCN protected area categories", "text": "IUCN protected area categories\n\nIUCN protected area categories, or IUCN protected area management categories, are categories used to classify protected areas in a system developed by the International Union for the Conservation of Nature (IUCN).\n\nThe enlisting of such areas is part of a strategy being used toward the conservation of the world's natural environment and biodiversity. The IUCN has developed the protected area management categories system to define, record, and classify the wide variety of specific aims and concerns when categorising protected areas and their objectives.\n\nThis categorisation method is recognised on a global scale by national governments and international bodies such as the United Nations and the Convention on Biological Diversity.\n\nA strict nature reserve (IUCN Category Ia) is an area which is protected from all but light human use in order to preserve the geological and geomorphical features of the region and its biodiversity. These areas are often home to dense native ecosystems that are restricted from all human disturbance outside of scientific study, environmental monitoring and education. Because these areas are so strictly protected, they provide ideal pristine environments by which external human influence can be measured.\n\nIn some cases strict nature reserves are of spiritual significance for surrounding communities, and the areas are also protected for this reason. The people engaged in the practice of their faith within the region have the right to continue to do so, providing it aligns with the area's conservation and management objectives.\n\nHuman impacts on strict nature reserves are increasingly difficult to guard against as climate and air pollution and newly emerging diseases threaten to penetrate the boundaries of protected areas. If perpetual intervention is required to maintain these strict guidelines, the area will often fall into category IV or V.\n\nA wilderness area (IUCN Category Ib) is similar to a strict nature reserve, but generally larger and protected in a slightly less stringent manner.\n\nThese areas are a protected domain in which biodiversity and ecosystem processes (including evolution) are allowed to flourish or experience restoration if previously disturbed by human activity. These are areas which may buffer against the effects of climate change and protect threatened species and ecological communities.\n\nHuman visitation is limited to a minimum, often allowing only those who are willing to travel of their own devices (by foot, by ski, or by boat), but this offers a unique opportunity to experience wilderness that has not been interfered with. Wilderness areas can be classified as such only if they are devoid of modern infrastructure, though they allow human activity to the level of sustaining indigenous groups and their cultural and spiritual values within their wilderness-based lifestyles.\n\nA national park (IUCN Category II) is similar to a wilderness area in its size and its main objective of protecting functioning ecosystems. However, national parks tend to be more lenient with human visitation and its supporting infrastructure. National parks are managed in a way that may contribute to local economies through promoting educational and recreational tourism on a scale that will not reduce the effectiveness of conservation efforts.\n\nThe surrounding areas of a national park may be for consumptive or non-consumptive use but should nevertheless act as a barrier for the defence of the protected area's native species and communities to enable them to sustain themselves in the long term.\n\nA natural monument or feature (IUCN Category III) is a comparatively smaller area that is specifically allocated to protect a natural monument and its surrounding habitats. These monuments can be natural in the wholest sense or include elements that have been influenced or introduced by humans. The latter should hold biodiversity associations or could otherwise be classified as a historical or spiritual site, though this distinction can be quite difficult to ascertain.\n\nTo be categorised as a natural monument or feature by IUCN's guidelines, the protected area could include natural geological or geomorphological features, culturally-influenced natural features, natural cultural sites, or cultural sites with associated ecology. The classification then falls into two subcategories: those in which the biodiversity is uniquely related to the conditions of the natural feature and those in which the current levels of biodiversity are dependent on the presence of the sacred sites that have created an essentially modified ecosystem.\n\nNatural monuments or features often play a smaller but key ecological role in the operations of broader conservation objectives. They have a high cultural or spiritual value that can be utilised to gain support of conservation challenges by allowing higher visitation or recreational rights, therefore offering an incentive for the preservation of the site.\n\nA habitat or species management area (IUCN Category IV) is similar to a natural monument or feature, but focuses on more specific areas of conservation (though size is not necessarily a distinguishing feature), like an identifiable species or habitat that requires continuous protection rather than that of a natural feature. These protected areas will be sufficiently controlled to ensure the maintenance, conservation, and restoration of particular species and habitats—possibly through traditional means—and public education of such areas is widely encouraged as part of the management objectives.\n\nHabitat or species management areas may exist as a fraction of a wider ecosystem or protected area and may require varying levels of active protection. Management measures may include (but are not limited to) the prevention of poaching, creation of artificial habitats, halting natural succession, and supplementary feeding practices.\n\nA protected landscape or protected seascape (IUCN Category V) covers an entire body of land or ocean with an explicit natural conservation plan, but usually also accommodates a range of for-profit activities.\n\nThe main objective is to safeguard regions that have built up a distinct and valuable ecological, biological, cultural, or scenic character. In contrast with previous categories, Category V permits surrounding communities to interact more with the area, contributing to the area's sustainable management and engaging with its natural and cultural heritage.\n\nLandscapes and seascapes that fall into this category should represent an integral balance between people and nature and can sustain activities such as traditional agricultural and forestry systems on conditions that ensure the continued protection or ecological restoration of the area.\n\nCategory V is one of the more flexible classifications of protected areas. As a result, protected landscapes and seascapes may be able to accommodate contemporary developments, such as ecotourism, at the same time as maintaining the historical management practices that may procure the sustainability of agrobiodiversity and aquatic biodiversity.:\n\nThough human involvement is a large factor in the management of these protected areas, developments are not intended to allow for widescale industrial production. The IUCN recommends that a proportion of the land mass remains in its natural condition—a decision to be made on a national level, usually with specificity to each protected area. Governance has to be developed to adapt the diverse—and possibly growing—range of interests that arise from the production of sustainable natural resources.\n\nCategory VI may be particularly suitable to vast areas that already have a low level of human occupation or in which local communities and their traditional practices have had little permanent impact on the environmental health of the region. This differs from category V in that it is not the result of long-term human interaction that has had a transformative effect on surrounding ecosystems.\n\n\n"}
{"id": "4187880", "url": "https://en.wikipedia.org/wiki?curid=4187880", "title": "Interlocus contest evolution", "text": "Interlocus contest evolution\n\nInterlocus contest evolution (ICE) is a process of intergenomic conflict by which different loci within a single genome antagonistically coevolve. ICE supposes that the Red Queen process, which is characterized by a never-ending antagonistic evolutionary arms race, does not only apply to species but also to genes within the genome of a species.\n\nBecause sexual recombination allows different gene loci to evolve semi-autonomously, genes have the potential to coevolve antagonistically. ICE occurs when \"an allelic substitution at one locus selects for a new allele at the interacting locus, and vice versa.\" As a result, ICE can lead to a chain reaction of perpetual gene substitution at antagonistically interacting loci, and no stable equilibrium can be achieved. The rate of evolution thus increases at that locus.\n\nICE is thought to be the dominant mode of evolution for genes controlling social behavior. The ICE process can explain many biological phenomena, including intersexual conflict, parent offspring conflict, and interference competition.\n\nA fundamental conflict between the sexes lies in differences in investment: males generally invest predominantly in fertilization while females invest predominantly in offspring. This conflict manifests itself in many traits associated with sexual reproduction. Genes expressed in only one sex are selectively neutral in the other sex; male- and female-linked genes can therefore be acted upon separated by selection and will evolve semi-autonomously. Thus, one sex of a species may evolve to better itself rather than better the species as a whole, sometimes with negative results for the opposite sex: loci will antagonistically coevolve to enhance male reproductive success at females’ expense on the one hand, and to enhance female resistance to male coercion on the other. This is an example of intralocus sexual conflict, and is unlikely to be resolved fully throughout the genome. However, in some cases this conflict may be resolved by the restriction of the gene’s expression to only the sex that it benefits, resulting in sexual dimorphism.\n\nThe ICE theory can explain the differentiation of the human X- and Y-chromosomes. Semi-autonomous evolution may have promoted genes beneficial to females in the X-chromosome even when detrimental to males, and genes beneficial to males in the Y-chromosome, even when detrimental to females. As the distribution of the X-chromosome is three times as large as the Y-chromosome (the X-chromosome occurs in 3/4 of offspring genes, while the Y-chromosome occurs in only 1/4), the Y-chromosome has a reduced opportunity for rapid evolution. Thus the Y-chromosome has \"shed\" its genes to leave only the essential ones (such as the SRY gene), which gives rise to the differences in the X- and Y-chromosomes.\n\nA father, mother and offspring may differ in the optimal resource allocation to the offspring. This co-evolutionary conflict can be considered in the context of ICE. Selection will favor genes in the male to maximize female investment in the current offspring, no matter the consequences to the female's reproduction later in life, while selection will favor genes in the female that increase her overall lifetime fitness. Genes expressed in the offspring will be selected to produce an intermediary level of resource allocation between the male-benefit and female-benefit loci. This three-way conflict again occurs when parents feed their offspring, as the optimum feeding rate and optimum point in time to discontinue feeding differ between father, mother and offspring.\n\nICE can also explain the theory of interference competition, which is most likely to be associated with opposing sets of genes that determine the outcome of competition between individuals. Different sets of genes may code for signal or receiver phenotypes, such as in the context of threat displays: when a competing male can win more contests by intimidation, rather than by fighting, selection will favor the accumulation of deceitful genes that may not be honest indicators of the male’s fighting capability.\n\nFor example, primitive male elephant seals may have used the lowest frequencies in the threat call of a rival as an indication of body size. The elephant seal's enormous nose may have evolved as a resonating device to amplify low frequencies, illustrating selection that favors the production of low-frequency threat vocalizations. However, this counter-selects for receptor systems that provide an increased threshold required for intimidation, which in turn selects for deeper threat vocalizations. The rapid divergence of threat displays among closely related species provides further evidence in support of the co-evolutionary arms race within the genome of a single species, driven by the ICE process.\n"}
{"id": "12804696", "url": "https://en.wikipedia.org/wiki?curid=12804696", "title": "Jaramillo reversal", "text": "Jaramillo reversal\n\nThe Jaramillo reversal was a reversal and excursion of the Earth's magnetic field that occurred approximately one million years ago. In the geological time scale it was a \"short-term\" positive reversal in the then-dominant Matuyama reversed magnetic chronozone; its beginning is widely dated to 990,000 years before the present (BP), and its end to 950,000 BP (though an alternative date of 1.07 million years ago to 990,000 is also found in the scientific literature).\n\nThe causes and mechanisms of short-term reversals and excursions like the Jaramillo, as well as the major field reversals like the Brunhes–Matuyama reversal, are subjects of study and dispute among researchers. One theory associates the Jaramillo with the Bosumtwi impact event, as evidenced by a tektite strewnfield in the Ivory Coast, though this hypothesis has been claimed as \"highly speculative\" and \"refuted\".\n\n"}
{"id": "148578", "url": "https://en.wikipedia.org/wiki?curid=148578", "title": "List of artificial radiation belts", "text": "List of artificial radiation belts\n\nArtificial radiation belts are radiation belts that have been created by high altitude nuclear explosions. \n\nThe table above only lists those high-altitude nuclear explosions for which a reference exists in the open (unclassified) English-language scientific literature to persistent artificial radiation belts resulting from the explosion.\n\nThe Starfish Prime radiation belt had, by far, the greatest intensity and duration of any of the artificial radiation belts.\n\nThe Starfish Prime radiation belt damaged the United Kingdom Satellite Ariel 1 and the United States satellites, Traac, Transit 4B, Injun I and Telstar I.  It also damaged the Soviet satellite Cosmos V.  All of these satellites failed completely within several months of the Starfish detonation. \n\nTelstar I lasted the longest of the satellites damaged by the Starfish Prime radiation, with its complete failure occurring on February 21, 1963.\n\nIn Los Alamos Scientific Laboratory report LA-6405, Herman Hoerlin gave the following explanation of the history of the original Argus experiment and of how the nuclear detonations lead to the development of artificial radiation belts.\n\nIn 2010, the United States Defense Threat Reduction Agency issued a report that had been written in support of the United States Commission to Assess the Threat to the United States from Electromagnetic Pulse Attack. The report, entitled \"Collateral Damage to Satellites from an EMP Attack,\" discusses in great detail the historical events that caused artificial radiation belts and their effects on many satellites that were then in orbit. The same report also projects the effects of one or more present-day high altitude nuclear explosions upon the formation of artificial radiation belts and the probable resulting effects on satellites that are currently in orbit.\n\n"}
{"id": "18032715", "url": "https://en.wikipedia.org/wiki?curid=18032715", "title": "List of herbaria in Europe", "text": "List of herbaria in Europe\n\nThis is a list of herbaria in Europe, organized first by region where the herbarium is located (using the United Nations geoscheme for Europe), then within each region by size of the collection. For other continents, see List of herbaria.\n\nThe tables below list herbaria located in Eastern Europe as defined by the United Nations geoscheme for Europe.\nThe following table includes herbaria located in countries on the Black Sea, including Bulgaria, Moldova, Romania, and Ukraine.\n\nThe following table includes herbaria located in the Czech Republic.\n\nThe following table includes herbaria located in Hungary.\n\nThe following table includes herbaria located in Poland.\n\nThe following table includes herbaria located in European Russia.\n\nThe tables below list herbaria located in Northern Europe as defined by the United Nations geoscheme for Europe: the British Isles, Baltic states, Scandinavia, and Iceland.\n\nThe following table includes herbaria located in the Baltic states: Estonia, Latvia, and Lithuania.\n\nThe following table includes herbaria located in the British Isles, including Ireland.\n\nThe following table includes herbaria located in Scandinavia, including Denmark and Iceland.\n\nThe tables below list herbaria located in Southern Europe as defined by the United Nations geoscheme for Europe.\n\nThe following table includes herbaria located in the western and southern Balkans, including Albania, Greece and nations formerly part of Yugoslavia.\n\nThe following table includes herbaria located in Italy, including Sicily and Sardinia.\n\nThe following table includes herbaria located in Spain and Portugal, including the Canary Islands.\n\nThe tables below list herbaria located in Western Europe as defined by the United Nations geoscheme for Europe: France, Germany, Austria, Switzerland, and the Low Countries.\n\nThe following table includes herbaria located in Austria and Switzerland, as well as Liechtenstein.\n\nThe following table includes herbaria located in France (including Corsica) and Monaco.\n\nThe following table includes herbaria located in Germany.\n\nThe following table includes herbaria located in Belgium, Luxembourg, Netherlands and Cyprus\n"}
{"id": "24694906", "url": "https://en.wikipedia.org/wiki?curid=24694906", "title": "List of invasive species in Asia", "text": "List of invasive species in Asia\n\nThis is a list of invasive species in Asia. A species is regarded as invasive if it has been introduced by human action to a location, area, or region where it did not previously occur naturally (i.e., is not a native species), becomes capable of establishing a breeding population in the new location without further intervention by humans, and becomes a \"pest\" in the new location, directly threatening agriculture and/or the local biodiversity.\n\nThe term invasive species refers to a subset of those species defined as introduced species. If a species has been introduced but remains local, and is not problematic to agriculture or to the local biodiversity, then it cannot be considered to be an invasive species and does not belong on this list.\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "24694990", "url": "https://en.wikipedia.org/wiki?curid=24694990", "title": "List of invasive species in Europe", "text": "List of invasive species in Europe\n\nThis is a list of invasive species in Europe. A species is regarded as invasive if it has become introduced to a location, area, or region where it did not previously occur naturally (i.e., is not a native species) and becomes capable of establishing a breeding population in the new location. An invasive species will be one that thrives in its new environment and negatively influences the ecology and biodiversity of that ecosystem.\n\nThe term invasive species refers to a subset of those species defined as introduced species. If a species has been introduced but remains local, and is not problematic to agriculture or to the local biodiversity, then it cannot be considered to be invasive, and does not belong on this list.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "24695021", "url": "https://en.wikipedia.org/wiki?curid=24695021", "title": "List of invasive species in North America", "text": "List of invasive species in North America\n\nThis is a list of invasive species in North America. A species is regarded as invasive if it has been introduced by human action to a location, area, or region where it did not previously occur naturally (i.e., is not a native species), becomes capable of establishing a breeding population in the new location without further intervention by humans, and becomes a pest in the new location, directly threatening human industry, such as agriculture, or the local biodiversity.\n\nThe term \"invasive species\" refers to a subset of those species defined as introduced species. If a species has been introduced, but remains local, and is not problematic for human industry or the local biodiversity, then it is not considered invasive, and does not belong on this list.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "146118", "url": "https://en.wikipedia.org/wiki?curid=146118", "title": "List of long-distance footpaths", "text": "List of long-distance footpaths\n\nThis is a list of some long-distance footpaths used for walking and hiking.\n\n\n\n\nThe merit of hiking trails in Hong Kong is that hikers can enjoy scenery of both the sea and highland.\n\nTranscaucasian Trail:\n\nA long distance trail in the caucasus has been a lingering idea for trekkers and hikers for many years since they started hiking remote parts of the Caucasus. \nMany sections of the TCT already exist, used by local community members and shepherds for centuries. These trail cross long valleys and traverse mammoth mountains to connect mountain villages together. Unfortunately, in recent years many of these trails have fallen into disrepair, and while many trails are known to locals, they are difficult to navigate for visitors and tourists. \nIn 2015, two Peace Corps volunteers, Paul Stephens and Jeff Haack, mapped and charted known routes in The Republic of Georgia. During this time they succeeded in locating many connections between known trails and publicizing the concept of the trail. In 2016, Tom Allen and Alessandro Mambelli scouted new trail routes in Armenia while the first trail building project began in Svaneti, Georgia. In 2017, the trail building expanded to Dilijan National Park in Armenia while trail building continued in the Svaneti region. \nToday, over 300 km of trail has been improved and marked in Georgia and Armenia. Many 7-10 day guided hikes are available on the TCT this summer. Over the next 5 years, the trail will be expanded to connect all of the sections and create even longer hikes.\nThe TCT can serve many purposes in the Caucasus region. For one, the natural diversity of the area needs to be protected. This habitat fosters many species of animal and provides unique ecosystems created by the mountains. \nMore information about the trail can be found at transcaucasiantrail.org . \nDonations can be sustaining or one-time.\n\n\n\n\n\nHkakabo Razi Trail, climbing the highest peak in Myanmar, in Khakaborazi National Park, and various footpaths in Putao\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSee: \n\n\n\n\n\n\nSee: List of long-distance hiking tracks in Australia\n\n\n\n\n\n\n"}
{"id": "1301543", "url": "https://en.wikipedia.org/wiki?curid=1301543", "title": "List of natural history museums", "text": "List of natural history museums\n\nThis is a list of natural history museums, also known as museums of natural history, i.e. museums whose exhibits focus on the subject of natural history, including such topics as animals, plants, ecosystems, geology, paleontology, and climatology.\n\nSome museums feature natural-history collections in addition to other collections, such as ones related to history, art and science. In addition, nature centers often include natural-history exhibits.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "16964856", "url": "https://en.wikipedia.org/wiki?curid=16964856", "title": "List of types of limestone", "text": "List of types of limestone\n\nThis is a list of types of limestone arranged according to location. It includes both formal stratigraphic unit names and less formal designations.\n\n\n\n\n\n\n\n\n\nEngland:\nScotland:\nWales:\n\n\n\nThis section is a list of generic types of limestone\n\n\n"}
{"id": "14997569", "url": "https://en.wikipedia.org/wiki?curid=14997569", "title": "Location of Earth", "text": "Location of Earth\n\nKnowledge of the location of Earth has been shaped by 400 years of telescopic observations, and has expanded radically in the last century. Initially, Earth was believed to be the center of the Universe, \nwhich consisted only of those planets visible with the naked eye and an outlying sphere of fixed stars. After the acceptance of the heliocentric model in the 17th century, observations by William Herschel and others showed that the Sun lay within a vast, disc-shaped galaxy of stars. By the 20th century, observations of spiral nebulae revealed that our galaxy was one of billions in an expanding universe, grouped into clusters and superclusters. By the end of the 20th century, the overall structure of the visible universe was becoming clearer, with superclusters forming into a vast web of filaments and voids. Superclusters, filaments and voids are the largest coherent structures in the Universe that we can observe. At still larger scales (over 1000 megaparsecs) the Universe becomes homogeneous meaning that all its parts have on average the same density, composition and structure.\n\nSince there is believed to be no \"center\" or \"edge\" of the Universe, there is no particular reference point with which to plot the overall location of the Earth in the universe. Because the observable universe is defined as that region of the Universe visible to terrestrial observers, Earth is, by definition, the center of Earth's observable universe. Reference can be made to the Earth's position with respect to specific structures, which exist at various scales. It is still undetermined whether the Universe is infinite. There have been numerous hypotheses that our universe may be only one such example within a higher multiverse; however, no direct evidence of any sort of multiverse has ever been observed, and some have argued that the hypothesis is not falsifiable.\n\n"}
{"id": "1155556", "url": "https://en.wikipedia.org/wiki?curid=1155556", "title": "McLeod gauge", "text": "McLeod gauge\n\nA McLeod gauge is a scientific instrument used to measure very low pressures, down to 10 Torr. It was invented in 1874 by Herbert McLeod (1841–1923). McLeod gauges were once commonly found attached to equipment that operates under vacuum, such as a lyophilizer. Today, however, these gauges have largely been replaced by electronic vacuum gauges.\n\nThe design of a McLeod gauge is somewhat similar to that of a mercury-column manometer. Typically it is filled with mercury. If used incorrectly, this mercury can escape and contaminate the vacuum system attached to the gauge.\nMcLeod gauges operate by taking in a sample volume of gas from a vacuum chamber, then compressing it by tilting and infilling with mercury. The pressure in this smaller volume is then measured by a mercury manometer, and knowing the compression ratio (the ratio of the initial and final volumes), the pressure of the original vacuum can be determined by applying Boyle's law.\n\nThis method is fairly accurate for non-condensible gases, such as oxygen and nitrogen. However, condensible gases, such as water vapour, ammonia, carbon dioxide, and pump-oil vapors may be in gaseous form in the low pressure of the vacuum chamber, but will condense when compressed by the McLeod gauge. The result is an erroneous reading, showing a pressure much lower than actually present. A cold trap may be used in conjunction with a McLeod gauge to condense these vapors before they enter the gauge.\n\nThe McLeod gauge has the advantage that it is simple to use and that its calibration is nearly the same for all non-condensable gases.\nThe device can be manually operated and the scale read visually, or the process can be automated in various ways. For example, a small electric motor can periodically rotate the assembly to collect a gas sample. If a fine platinum wire is in the capillary tube, its resistance indicates the height of the mercury column around it.\n\nModern electronic vacuum gauges are simpler to use, less fragile, and do not present a mercury hazard, but their reading is highly dependent on the chemical nature of the gas being measured, and their calibration is unstable. For this reason, McLeod gauges continue to be used as a calibration standard for electronic gauges.\n\n\n"}
{"id": "14389994", "url": "https://en.wikipedia.org/wiki?curid=14389994", "title": "Natural landscape", "text": "Natural landscape\n\nA natural landscape is the original landscape that exists before it is acted upon by human culture. The natural landscape and the cultural landscape are separate parts of the landscape. However, in the twenty-first century landscapes that are totally untouched by human activity no longer exist, so that reference is sometimes now made to degrees of naturalness within a landscape.\n\nIn \"Silent Spring\" (1962) Rachel Carson describes a roadside verge as it used to look: \"Along the roads, laurel, viburnum and alder, great ferns and wildflowers delighted the traveler’s eye through much of the year\" and then how it looks now following the use of herbicides: \"The roadsides, once so attractive, were now lined with browned and withered vegetation as though swept by fire\". Even though the landscape before it is sprayed is biologically degraded, and may well contains alien species, the concept of what might constitute a natural landscape can still be deduced from the context.\n\nThe phrase \"natural landscape\" was first used in connection with landscape painting, and landscape gardening, to contrast a formal style with a more natural one, closer to nature. Alexander von Humboldt (1769 – 1859) was to further conceptualize this into the idea of a natural landscape \"separate\" from the cultural landscape. Then in 1908 geographer Otto Schlüter developed the terms original landscape (\"Urlandschaft\") and its opposite cultural landscape (\"Kulturlandschaft\") in an attempt to give the science of geography a subject matter that was different from the other sciences. An early use of the actual phrase \"natural landscape\" by a geographer can be found in Carl O. Sauer's paper \"The Morphology of Landscape\" (1925).\n\nThe concept of a natural landscape was first developed in connection with landscape painting, though the actual term itself was first used in relation to landscape gardening. In both cases it was used to contrast a formal style with a more natural one, that is closer to nature. Chunglin Kwa suggests, \"that a seventeenth-century or early-eighteenth-century person could experience natural scenery ‘just like on a painting,’ and so, with or without the use of the word itself, designate it as a landscape.\" With regard to landscape gardening John Aikin, commented in 1794: \"Whatever, therefore, there be of \"novelty\" in the singular scenery of an artificial garden, it is soon exhausted, whereas the infinite diversity of a natural landscape presents an inexhaustible flore of new forms\". Writing in 1844 the prominent American landscape gardener Andrew Jackson Downing comments: \"straight canals, round or oblong pieces of water, and all the regular forms of the geometric mode ... would evidently be in violent opposition to the whole character and expression of natural landscape\".\n\nIn his extensive travels in South America, Alexander von Humboldt became the first to conceptualize a natural landscape separate from the cultural landscape, though he does not actually use these terms. Andrew Jackson Downing was aware of, and sympathetic to, Humboldt's ideas, which therefore influenced American landscape gardening.\n\nSubsequently, the geographer Otto Schlüter, in 1908, argued that by defining geography as a \"Landschaftskunde\" (landscape science) would give geography a logical subject matter shared by no other discipline. He defined two forms of landscape: the \"Urlandschaft\" (original landscape) or landscape that existed before major human induced changes and the \"Kulturlandschaft\" (cultural landscape) a landscape created by human culture. Schlüter argued that the major task of geography was to trace the changes in these two landscapes.\n\nThe term natural landscape is sometimes used as a synonym for wilderness, but for geographers natural landscape is a scientific term which refers to the biological, geological, climatological and other aspects of a landscape, not the cultural values that are implied by the word wilderness.\n\nMatters are complicated by the fact that the words nature and natural have more than one meaning. On the one hand there is the main dictionary meaning for nature: \"The phenomena of the physical world collectively, including plants, animals, the landscape, and other features and products of the earth, as opposed to humans or human creations\". On the other hand, there is the growing awareness, especially since Charles Darwin, of humanities biological affinity with nature.\n\nThe dualism of the first definition has its roots is an \"ancient concept\", because early people viewed \"nature, or the nonhuman world […] as a divine \"Other\", godlike in its separation from humans\". In the West, Christianity's myth of the fall, that is the expulsion of humankind from the Garden of Eden, where all creation lived in harmony, into an imperfect world, has been the major influence. Cartesian dualism, from the seventeenth century on, further reinforced this dualistic thinking about nature. \nWith this dualism goes value judgement as to the superiority of the natural over the artificial. Modern science, however, is moving towards a holistic view of nature.\n\nWhat is meant by natural, within the American conservation movement, has been changing over the last century and a half.\n\nIn the mid-nineteenth century American began to realize that the land was becoming more and more domesticated and wildlife was disappearing. This led to the creation of American National Parks and other conservation sites. Initially it was believed that all that was needed to do was to separate what was seen as natural landscape and \"avoid disturbances such as logging, grazing, fire and insect outbreaks\". This, and subsequent environmental policy, until recently, was influenced by ideas of the wilderness. However, this policy was not consistently applied, and in Yellowstone Park, to take one example, the existing ecology was altered, firstly by the exclusion of Native Americans and later with the virtual extermination of the wolf population.\n\nA century later, in the mid-twentieth century, it began to be believed that the earlier policy of \"protection from disturbance was inadequate to preserve park values\", and that is that direct human intervention was necessary to restore the landscape of National Parks to its ‘’natural’’ condition. In 1963 the Leopold Report argued that \"A national park should represent a vignette of primitive America\". This policy change eventually led to the restoration of wolves in Yellowstone Park in the 1990s.\n\nHowever, recent research in various disciplines indicates that a pristine natural or \"primitive\" landscape is a myth, and it now realised that people have been changing the natural into a cultural landscape for a long while, and that there are few places untouched in some way from human influence. The earlier conservation policies were now seen as cultural interventions. The idea of what is natural and what artificial or cultural, and how to maintain the natural elements in a landscape, has been further complicated by the discovery of global warming and how it is changing natural landscapes.\n\nAlso important is a reaction recently amongst scholars against dualistic thinking about nature and culture. Maria Kaika comments: \"Nowadays, we are beginning to see nature and culture as intertwined once again – not ontologically separated anymore […].What I used to perceive as a compartmentalized world, consisting of neatly and tightly sealed, autonomous ‘space envelopes’ (the home, the city, and nature) was, in fact, a messy socio-spatial continuum”. And William Cronon argues against the idea of wilderness because it \"involves a dualistic vision in which the human is entirely outside the natural\" and affirms that \"wildness (as opposed to wilderness) can be found anywhere\" even \"in the cracks of a Manhattan sidewalk\". According to Cronon we have to \"abandon the dualism that sees the tree in the garden as artificial […] and the tree in the wilderness as natural […] Both in some ultimate sense are wild.\" Here he bends somewhat the regular dictionary meaning of wild, to emphasise that nothing natural, even in a garden, is fully under human control.\n\nThe landscape of Europe has considerably altered by people and even in an area, like the Cairngorm Mountains of Scotland, with a low population density, only \" the high summits of the Cairngorm Mountains, consist entirely of natural elements. These \"high summits\" are of course only part of the Cairngorms, and there are no longer wolves, bears, wild boar or lynx in Scotland's wilderness. The Scots pine in the form of the Caledonian forest also covered much more of the Scottish landscape than today.\n\nThe Swiss National Park, however, represent a more natural landscape. It was founded in 1914, and is one of the earliest national parks in Europe.\nVisitors are not allowed to leave the motor road, or paths through the park, make fire or camp. The only building within the park is Chamanna Cluozza, mountain hut. It is also forbidden to disturb the animals or the plants, or to take home anything found in the park. Dogs are not allowed. Due to these strict rules, the Swiss National Park is the only park in the Alps who has been categorized by the IUCN as a strict nature reserve, which is the highest protection level.\n\nNo place on the Earth is unaffected by people and their culture. People are part of biodiversity, but human activity affects biodiversity, and this alters the natural landscape. Mankind have altered landscape to such an extent that few places on earth remain pristine, but once free of human influences, the landscape can return to a natural or near natural state.\nEven the remote Yukon and Alaskan wilderness, the bi-national Kluane-Wrangell-St. Elias-Glacier Bay-Tatshenshini-Alsek park system comprising Kluane, Wrangell-St Elias, Glacier Bay and Tatshenshini-Alsek parks, a UNESCO World Heritage Site, is not free from human influence, because the Kluane National Park lies within the traditional territories of the Champagne and Aishihik First Nations and Kluane First Nation who have a long history of living in this region. Through their respective Final Agreements with the Canadian Government, they have made into law their rights to harvest in this region.\n\nCultural forces intentionally or unintentionally, have an influence upon the landscape. Cultural landscapes are places or artifacts created and maintained by people. Examples of cultural intrusions into a landscape are: fences, roads, parking lots, sand pits, buildings, hiking trails, management of plants, including the introduction of invasive species, extraction or removal of plants, management of animals, mining, hunting, natural landscaping, farming and forestry, pollution. Areas that might be confused with a natural landscape include public parks, farms, orchards, artificial lakes and reservoirs, managed forests, golf courses, nature center trails, gardens.\n\n"}
{"id": "412846", "url": "https://en.wikipedia.org/wiki?curid=412846", "title": "Naturalistic pantheism", "text": "Naturalistic pantheism\n\nNaturalistic pantheism is a kind of pantheism. It has been used in various ways such as to relate God or divinity with concrete things, determinism, or the substance of the Universe. God, from these perspectives, is seen as the aggregate of all unified natural phenomena. The phrase has often been associated with the philosophy of Baruch Spinoza, although academics differ on how it is used.\n\nThe term “pantheism\" is derived from Greek words \"pan\" (Greek: πᾶν) meaning \"all\" and \"theos\" (θεός) meaning God. It was coined by Joseph Raphson in his work \"De spatio reali\", published in 1697. The term was introduced to English by Irish writer John Toland in his 1705 work \"Socinianism Truly Stated, by a pantheist\" that described pantheism as the \"opinion of those who believe in no other eternal being but the universe.\"\n\nThe term \"naturalistic\" derives from the word \"naturalism\", which has several meanings in philosophy and aesthetics. In philosophy the term frequently denotes the view that everything belongs to the world of nature and can be studied with the methods appropriate for studying that world, \"i.e.\" the sciences. It generally implies an absence of belief in supernatural beings.\n\nJoseph Needham, a modern British scholar of Chinese philosophy and science, has identified Taoism as \"a naturalistic pantheism which emphasizes the unity and spontaneity of the operations of Nature.\" This philosophy can be dated to the late 4th century BCE.\n\nThe Hellenistic Greek philosophical school of Stoicism (which started in the early 3rd century BCE) rejected the dualist idea of the separate ideal/conscious and material realms, and identified the substance of God with the entire cosmos and heaven. However, not all philosophers who did so can be classified as naturalistic pantheists.\n\nNaturalistic pantheism was expressed by various thinkers, including Giordano Bruno, who was burned at the stake for his views. However, the 17th century Dutch philosopher Spinoza became particularly known for it.\n\nPossibly drawing upon the ideas of Descartes,\nBaruch Spinoza connected God and Nature through the phrase \"deus sive natura\" (\"God, or Nature\"), making him the father of classical pantheism. He relied upon rationalism rather than the more intuitive approach of some Eastern traditions.\n\nSpinoza's philosophy, sometimes known as Spinozism, has been understood in a number of ways, and caused disagreements such as the Pantheism controversy. However, many scholars have considered it to be a form of naturalistic pantheism. This has included viewing the pantheistic unity as natural. \nOthers focus on the deterministic aspect of naturalism.\nSpinoza inspired a number of other pantheists, with varying degrees of idealism towards nature. However, Spinoza's influence in his own time was limited.\nScholars have considered Spinoza the founder of a line of naturalistic pantheism, though not necessarily the only one.\n\nIn 1705 the Irish writer John Toland endorsed a form of pantheism in which the God-soul is identical with the material universe.\n\nGerman naturalist Ernst Haeckel (1834–1919) proposed a monistic pantheism in which the idea of God is identical with that of nature or substance.\n\nThe World Pantheist Movement, started in 1999, describes Naturalistic Pantheism as including reverence for the universe, realism, strong naturalism, and respect for reason and the scientific method as methods of understanding the world. Paul Harrison considers its position the closest modern equivalent to Toland's.\n\n"}
{"id": "18254861", "url": "https://en.wikipedia.org/wiki?curid=18254861", "title": "Naturalization of intentionality", "text": "Naturalization of intentionality\n\nAccording to Franz Brentano, intentionality refers to the \"aboutness of mental states that cannot be a physical relation between a mental state and what is about (its object) because in a physical relation each of the relata must exist whereas the objects of mental states might not.\n\nSeveral problems arise for features of intentionality, which are unusual for materialistic relations. Representation is unique. When 'x represents y' is true, it is not the same as other relations between things, like when 'x is next to y' or when 'x caused y' or when 'x met y', etc. Representation is different because, for instance, when 'x represents y' is true, y need not exist. This isn't true when say 'x is the square root of y' or 'x caused y' or 'x is next to y'. Similarly, when 'x represents y' is true, 'x represents z' can still be false, even when y = z. Intentionality encompasses relations that are both physical and mental. In this case, \"Billy can love Santa and Jane can search for unicorns even if Santa does not exist and there are no unicorns.\"\n\nFranz Brentano, the nineteenth century philosopher, spoke of mental states as involving presentations of the objects of our thoughts. This idea encompasses his belief that one cannot desire something unless they actually have a representation of it in their minds.\n\nDennis Stampe was one of the first philosophers in modern times to suggest a theory of content according to which content is a matter of reliable causes.\n\nFred Dretskes book, \"Knowledge and the Flow of Information\" (1981), was a major influence on the development of informational theories, and although the theory developed there is not a teleological theory, Dretske (1986, 1988, 1991) later produced an informational version of teleosemantics. He begins with a concept of carrying information that he calls \"indicating\", explains that indicating is not equivalent to representing, and then suggests that a representation's content is what it has the function of indicating\n\nTeleosemantics, also known as biosemantics, is used to refer to the class of theories of mental content that use a teleological notion of function. Teleosemantics is best understood as a general strategy for underwriting the normative nature of content, rather than any particular theory. What all teleological theories have in common is the idea that semantic norms are ultimately derivable from functional norms.\n\nAttempts to naturalize semantics began in the late 1970s. Many attempts were and still are being made to bring natural-physical explanations to bear on minds and, specifically, to the question of how minds acquire content. This is an interesting question; it is no surprise that it takes center stage in the philosophy of mind. Indeed, it is certainly an interesting question how minds, thought by those in the natural camp to be \"natural physical objects\", could have developed intentional properties. In the mid-1980s, with the works of Ruth Millikan and David Papineau (Language, Thought, and Other Biological Categories and \"Representation and Explanation\", respectively) teleosemantics, a theory of mental content that attempts to address the question of content and intentionality of minds, was born.\n\nRuth Millikan is perhaps the most vocal supporter of the teleosemantic program. Millikan's view differs from other teleosemantic views in myriad ways, but perhaps its most unusual characteristic is its distinction between the mechanisms that produce mental representations from those that consume mental representations. There is a representational function as a whole, at a composite level; and there are two \"sub-functions\", the producer-function and the consumer-function. In terms that are easy to understand, let's take Millikan's own example of beavers splashing their tails. One beaver alerts other beavers to the presence of danger by splashing its tail on the surface of water. The splashing of the tail tells, or represents to, the other beavers that there is danger in the environment, and the other beavers dip into the water to avoid the danger. The splashing of the beaver's tail produces a representation; the other beavers consume the representation. The representation that the beavers consume guides their behavior in ways that relate to their survival.\n\nOf course, the foci of the teleosemantic program is internal representations, and not just representational states of affairs between two (or more) distinct, external entities. How does the picture of the producer and consumer beavers, for instance, play into a story about internal representations? Papineau and Macdonald describe Millikan's account of this well and loyally, saying \"The producing mechanisms will be the sensory and other cerebral mechanisms that give rise to cognitive representations.\" The consuming mechanisms are those that \"use these representations to direct behavior in pursuit of some biological end\". Here, we have a picture similar to the beaver example, but this picture portrays the two sub-functions, producer and consumer, operating within a more-obviously unified system, namely, the cognitive system. One sub-function produces mental representations while the other sub-function consumes them in order to reach some end, e.g., danger-avoidance or food-acquisition. The representations consumed by the consumer sub-function guide an organism's behavior toward some biological end, e.g., survival. This is a rather brief sketch of Millikan's overall portrait. Of course, more goes into her account of the relation between producer- and consumer-functions in order to arrive at a nuanced account of mental representation. But that is a matter of how. Details as to the how aside, much of Millikan's efforts are directed towards the why, viz., why it is that perceivers like us have mental representations—why representations are produced in the first place.\n\nThe theory of asymmetric dependence, from Fodor, who says that his theory \"distinguishes merely informational relations on the basis of their higher-order relations to each other: informational relations depend upon representational relations, but not vice versa. He gives an example of this theory when he says, \"if tokens of a mental state type are reliably caused by horses, cows-on-dark-nights, zebras-in-the-mist and Great Danes, then they carry information about horses, etc. If however, such tokens are caused by cows-on-dark-nights, etc. because they are caused by horses, but not vice versa, then they represent horses (or property horse).\n\n20th century American philosopher Willard Van Orman Quine believed that linguistic terms do not have distinct meanings that accompany them because there are no such entities as \"meanings\". In his books, \"Word and Object\" (1960) and \"Ontological Relativity\" (1968), Quine considers the methods available to a field linguist attempting to translate an unknown language in order to outline his thesis. His thesis, the indeterminacy of translation, notes that there are many different ways to distribute purpose and meanings among words. Whenever a theory of translation is made it is commonly based upon context. An argument over the correct translation of an unidentified term depends on the possibility that the native could have spoken a different sentence. The same problem of indeterminacy would appear in this argument once again since any hypothesis can be defended if one adopts enough compensatory hypotheses about other parts of the language. Quine uses as an example the word \"gavagai\" spoken by a native upon seeing a rabbit. One can go the simplest route and translate the word to \"Lo, a rabbit\", but other possible translations such as \"Lo, food\" or \"Let's go hunting\" are completely reasonable given what the linguist knows. Subsequent observations can rule out certain possibilities as well as questioning the natives. But this is only possible once the linguist has mastered much of the natives' grammar and vocabulary. This is a big problem because this can only be done on the basis of hypotheses derived from simpler, observation-connected bits of language, which admit multiple interpretations, as we have seen.\n\nDaniel C. Dennett's theory of mental content, the intentional stance, tries to view the behavior of things in terms of mental properties. According to Dennett: \n\"Here is how it works: first you decide to treat the object whose behavior is to be predicted as a rational agent; then you figure out what beliefs that agent ought to have, given its place in the world and its purpose. Then you figure out what desires it ought to have, on the same considerations, and finally you predict that this rational agent will act to further its goals in the light of its beliefs. A little practical reasoning from the chosen set of beliefs and desires will in most instances yield a decision about what the agent ought to do; that is what you predict the agent will do.\"\n\nDennett's thesis has three levels of abstraction:\n\nDennett states that the more concrete the level, the more accurate is in principle our predictions. Though if one chooses to view an object through a more abstract level, he will gain greater computational power by getting a better overall picture of the object and skipping over any extraneous details. Also, switching to a more abstract level has its risks as well as its benefits. If we applied the intentional stance to a thermometer that was heated to 500 °C, trying to understand it through its beliefs about how hot it is and its desire to keep the temperature just right, we would gain no useful information. The problem would not be understood until we dropped down to the physical stance to comprehend that it has been melted. Whether to take a particular stance should be decided by how successful that stance is when applied. Dennett argued that it is best to understand human beliefs and desires at the level of the intentional stance.\n\n\n"}
{"id": "48100229", "url": "https://en.wikipedia.org/wiki?curid=48100229", "title": "Nevadaite", "text": "Nevadaite\n\nNevadaite is a rare phosphate mineral with a chemical formula of\nNevadaite is a pale-green to turquoise colored mineral belonging to the phosphate group. It exhibits a radial crystal habit consisting of prismatic crystals covering areas up to 2 cm. It has a pale-blue streak, a vitreous luster, and is not fluorescent. Nevadaite is in the orthorhombic crystal system and displays conchoidal fracture.\n\nNevadaite was first discovered in the Gold Quarry mine near the town of Carlin, Eureka County, Nevada. The unique conditions and amounts of phosphate, vanadate, arsenate, and uranate in this area led to the formation of two new minerals; one being nevadaite and the other being goldquarryite. The Gold Quarry mine has been operated by The Newmont Mining Corporation since 1985 for the extraction of Carlin-type gold deposits.\n\nNevadaite was discovered in February 1992 by Martin C. Jensen and was approved by the International Mineralogical Association in 2002. It is also found in a copper mine in Kyrgyzstan.\n"}
{"id": "997476", "url": "https://en.wikipedia.org/wiki?curid=997476", "title": "Night sky", "text": "Night sky\n\nThe term night sky, usually associated with astronomy from Earth, refers to the nighttime appearance of celestial objects like stars, planets, and the Moon, which are visible in a clear sky between sunset and sunrise, when the Sun is below the horizon.\n\nNatural light sources in a night sky include moonlight, starlight, and airglow, depending on location and timing. Aurorae light up the skies above the polar circles. Occasionally, a large coronal mass ejection from the Sun or simply high levels of solar wind may extend the phenomenon toward the Equator.\n\nThe night sky and studies of it have a historical place in both ancient and modern cultures. In the past, for instance, farmers have used the status of the night sky as a calendar to determine when to plant crops. Many cultures have drawn constellations between stars in the sky, using them in association with legends and mythology about their deities.\n\nThe anciently developed belief of astrology is generally based on the belief that relationships between heavenly bodies influence or convey information about events on Earth. The \"scientific\" study of celestial objects visible at night takes place in the science of observational astronomy.\n\nThe visibility of celestial objects in the night sky is affected by light pollution. The presence of the Moon in the night sky has historically hindered astronomical observation by increasing the amount of ambient brightness. With the advent of artificial light sources, however, light pollution has been a growing problem for viewing the night sky. Optical filters and modifications to light fixtures can help to alleviate this problem, but for optimal views, both professional and amateur astronomers seek locations far from urban skyglow.\n\nThe fact that the sky is not completely dark at night, even in the absence of moonlight and city lights, can be easily observed, since if the sky were absolutely dark, one would not be able to see the silhouette of an object against the sky.\n\nThe intensity of the sky varies greatly over the day and the primary cause differs as well. During daytime when the sun is above the horizon direct scattering of sunlight (Rayleigh scattering) is the overwhelmingly dominant source of light. In twilight, the period of time between sunset and sunrise, the situation is more complicated and a further differentiation is required. Twilight is divided in three segments according to how far the sun is below the horizon in segments of 6°.\n\nAfter sunset the civil twilight sets in, and ends when the sun drops more than 6° below the horizon. This is followed by the nautical twilight, when the sun reaches heights of -6° and -12°, after which comes the astronomical twilight defined as the period from -12° to -18°. When the sun drops more than 18° below the horizon the sky generally attains its minimum brightness.\n\nSeveral sources can be identified as the source of the intrinsic brightness of the sky, namely airglow, indirect scattering of sunlight, scattering of starlight, and artificial light pollution.\n\nDepending on local sky cloud cover, pollution, humidity, and light pollution levels, the stars visible to the unaided naked eye appear as hundreds, thousands or tens of thousands of white pinpoints of light in an otherwise near black sky together with some faint nebulae or clouds of light . In ancient times the stars were often assumed to be equidistant on a dome above the earth because they are much too far away for stereopsis to offer any depth cues. Visible stars range in color from blue (hot) to red (cold), but with such small points of faint light, most look white because they stimulate the rod cells without triggering the cone cells. If it is particularly dark and a particularly faint celestial object is of interest, averted vision may be helpful.\n\nThe stars of the night sky cannot be counted unaided because they are so numerous and there is no way to track which have been counted and which have not. Further complicating the count, fainter stars may appear and disappear depending on exactly where the observer is looking. The result is an impression of an extraordinarily vast star field.\n\nBecause stargazing is best done from a dark place away from city lights, dark adaptation is important to achieve and maintain. It takes several minutes for eyes to adjust to the darkness necessary for seeing the most stars, and surroundings on the ground are hard to discern. A red flashlight (torch) can be used to illuminate star charts, telescope parts, and the like without undoing the dark adaptation. (See Purkinje effect).\n\nThere are no markings on the night sky, though there exist many sky maps to aid stargazers in identifying constellations and other celestial objects. Constellations are prominent because their stars tend to be brighter than other nearby stars in the sky. Different cultures have created different groupings of constellations based on differing interpretations of the more-or-less random patterns of dots in the sky. Constellations were identified without regard to distance to each star, but instead as if they were all dots on a dome.\n\nOrion is among the most prominent and recognizable constellations. The Big Dipper (which has a wide variety of other names) is helpful for navigation in the northern hemisphere because it points to Polaris, the north star.\n\nThe pole stars are special because they are approximately in line with the Earth's axis of rotation so they appear to stay in one place while the other stars rotate around them through the course of a night (or a year).\n\nPlanets, named for the Greek word for \"wanderer,\" process through the star field a little each day, executing loops with time scales dependent on the length of the planet's year or orbital period around solar system. Planets, to the naked eye, appear as points of light in the sky with variable brightness. Planets shine due to sunlight reflecting or scattering from the planets' surface or atmosphere. Thus the relative sun-planet-earth positions determine the planet's brightness. With a telescope or good binoculars, the planets appear as discs demonstrating finite size, and it is possible to observe orbiting moons which cast shadows onto the host planet's surface. Venus is the most prominent planet, often called the \"morning star\" or \"evening star\" because it is brighter than the stars and often the only \"star\" visible near sunrise or sunset, depending on its location in its orbit. Mercury, Mars, Jupiter and Saturn are also visible to the naked eye.\n\nEarth's Moon is a grey disc in the sky with cratering visible to the naked eye. It spans, depending on its exact location, 29-33 arcminutes - which is about the size of a thumbnail at arm's length, and is readily identified. Over 28 days, the moon goes through a full cycle of lunar phases. People can generally identify phases within a few days by looking at the moon. Unlike stars and most planets, the light reflected from the moon is bright enough to be seen during the day. (Venus can sometimes be seen even after sunrise.)\n\nSome of the most spectacular moons come during the full moon phase near sunset or sunrise. The moon on the horizon benefits from the moon illusion which makes it appear larger. The light reflected from the moon traveling through the atmosphere also colors the moon orange and/or red.\n\nComets come to the night sky only rarely. Comets are illuminated by the sun, and their tails extend away from the sun. A comet with visible tail is quite unusual - a great comet appears about once a decade. They tend to be visible only shortly before sunrise or after sunset because those are the times they are close enough to the sun to show a tail.\n\nClouds obscure the view of other objects in the sky, though varying thicknesses of cloudcover have differing effects. A very thin cirrus cloud in front of the moon might produce a rainbow-colored ring around the moon. Stars and planets are too small or dim to take on this effect, and are instead only dimmed (often to the point of invisibility). Thicker cloudcover obscures celestial objects entirely, making the sky black or reflecting city lights back down. Clouds are often close enough to afford some depth perception, though they are hard to see without moonlight or light pollution.\n\nOn clear dark nights in unpolluted areas, when the moon is thin or below the horizon, the Milky Way, a band of what looks like white dust, can be seen.\n\nThe Magellanic Clouds of the southern sky are easily mistaken to be Earth-based clouds (hence the name) but are in fact collections of stars found outside the Milky Way known as dwarf galaxies.\n\nZodiacal light is a glow that appears near the points where the sun rises and sets, and is caused by sunlight interacting with interplanetary dust.\n\nShortly after sunset and before sunrise, artificial satellites often look like stars—similar in brightness and size—but move relatively quickly. Those that fly in low Earth orbit cross the sky in a couple of minutes. Some satellites, including space debris, appear to blink or have a periodic fluctuation in brightness because they are rotating. Satellite flares can appear brighter than Venus, with notable examples including the International Space Station (ISS) and Iridium Satellites.\n\nMeteors (commonly known as shooting stars) streak across the sky very infrequently. During a meteor shower, they may average one a minute at irregular intervals, but otherwise their appearance is a random surprise. The occasional meteor will make a bright, fleeting streak across the sky, and they can be very bright in comparison to the night sky.\n\nAircraft are also visible at night, distinguishable at a distance from other objects because their lights blink.\n\n\n"}
{"id": "774575", "url": "https://en.wikipedia.org/wiki?curid=774575", "title": "Outgassing", "text": "Outgassing\n\nOutgassing (sometimes called offgassing, particularly when in reference to indoor air quality) is the release of a gas that was dissolved, trapped, frozen or absorbed in some material. Outgassing can include sublimation and evaporation (which are phase transitions of a substance into a gas), as well as desorption, seepage from cracks or internal volumes, and gaseous products of slow chemical reactions. Boiling is generally thought of as a separate phenomenon from outgassing because it consists of a phase transition of a liquid into a vapor of the same substance.\n\nOutgassing is a challenge to creating and maintaining clean high-vacuum environments. NASA and ESA maintains a list of low-outgassing materials to be used for spacecraft, as outgassing products can condense onto optical elements, thermal radiators, or solar cells and obscure them. Materials not normally considered absorbent can release enough light-weight molecules to interfere with industrial or scientific vacuum processes. Moisture, sealants, lubricants, and adhesives are the most common sources, but even metals and glasses can release gases from cracks or impurities. The rate of outgassing increases at higher temperatures because the vapor pressure and rate of chemical reaction increases. For most solid materials, the method of manufacture and preparation can reduce the level of outgassing significantly. Cleaning of surfaces, or heating of individual components or the entire assembly (a process called \"bake-out\") can drive off volatiles.\n\nNASA's Stardust spaceprobe suffered reduced image quality due to an unknown contaminant that had condensed on the CCD sensor of the navigation camera. A similar problem affected the Cassini spaceprobe's Narrow Angle Camera, but was corrected by repeatedly heating the system to 4 °C. A comprehensive characterisation of outgassing effects using mass spectrometers could be obtained for ESA's Rosetta spacecraft.\n\nNatural outgassing is commonplace in comets.\n\nOutgassing is a possible source of many tenuous atmospheres of terrestrial planets or moons. Many materials are volatile relative to the extreme vacuum of space, such as around the Moon, and may evaporate or even boil at ambient temperature. Materials on the lunar surface have completely outgassed and been ripped away by solar winds long ago, but volatile materials may remain at depth. Once released, gases almost always are less dense than the surrounding rocks and sand and seep toward the surface. The lunar atmosphere probably originates from outgassing of warm material below the surface. At the Earth's tectonic divergent boundaries where new crust is being created, helium and carbon dioxide are some of the volatiles being outgassed from mantle magma.\n\nOutgassing can be significant if it collects in a closed environment where air is stagnant or recirculated. For example, new car smell consists of outgassed chemicals released by heat in a closed automobile. Even a nearly odorless material such as wood may build up a strong smell if kept in a closed box for months. There is some concern that plasticizers and solvents released from many industrial products, especially plastics, may be harmful to human health. Long-term exposure to solvent vapors can cause chronic solvent-induced encephalopathy (CSE). Outgassing toxic gases are of great concern in the design of submarines and space stations, which must have self-contained recirculated atmospheres.\n\nThe outgassing of small pockets of air near the surface of setting concrete can lead to permanent holes in the structure (called bugholes) that may compromise its structural integrity.\n\n\n"}
{"id": "15654603", "url": "https://en.wikipedia.org/wiki?curid=15654603", "title": "Peterson Identification System", "text": "Peterson Identification System\n\nThe Peterson Identification System is a practical method for the field identification of animals, plants and other natural phenomena. It was devised by ornithologist Roger Tory Peterson in 1934 for the first of his series of \"Field Guide\"s (See Peterson Field Guides.) Peterson devised his system \"so that live birds could be identified readily at a distance by their 'field marks' without resorting to the bird-in-hand characters that the early collectors relied on. During the last half century the binocular and the spotting scope have replaced the shotgun.\" As such, it both reflected and contributed to awareness of the emerging early environmental movement.\n\nCreated for use by amateur naturalists and laymen, rather than specialists, the \"Peterson System\" is essentially a pictorial key based upon readily noticed visual impressions rather than on the technical features of interest to scientists. The technique involves patternistic drawings with arrows that pinpoint the key field comparisons between similar species.\n\nSince the first Peterson \"Field Guide\", the system has been expanded to about three dozen volumes in the series as well as being emulated by many other publishers and authors of field guides. It has become the near-universally accepted standard, first in the United States and Europe and then around the world.\n"}
{"id": "5389553", "url": "https://en.wikipedia.org/wiki?curid=5389553", "title": "Photophoresis", "text": "Photophoresis\n\nPhotophoresis denotes the phenomenon that small particles suspended in gas (aerosols) or liquids (hydrocolloids) start to migrate when illuminated by a sufficiently intense beam of light. The existence of this phenomenon is owed to a non-uniform distribution of temperature of an illuminated particle in a fluid medium. Separately from photophoresis, in a fluid mixture of different kinds of particles, the migration of some kinds of particles may be due to differences in their absorptions of thermal radiation and other thermal effects collectively known as thermophoresis. In laser photophoresis, particles migrate once they have a refractive index different from their surrounding medium. The migration of particles is usually possible when the laser is slightly or not focused. A particle with a higher refractive index compared to its surrounding molecule moves away from the light source due to momentum transfer from absorbed and scattered light photons. This is referred to as a radiation pressure force. This force depends on light intensity and particle size but has nothing to do with the surrounding medium. Just like in Crookes radiometer, light can heat up one side and gas molecules bounce from that surface with greater velocity, hence push the particle to the other side. Under certain conditions, with particles of diameter comparable to the wavelength of light, the phenomenon of a negative indirect photophoresis occurs, due to the unequal heat generation on the laser irradiation between the back and front sides of particles, this produces a temperature gradient in the medium around the particle such that molecules at the far side of the particle from the light source may get to heat up more, causing the particle to move towards the light source.\n\nIf the suspended particle is rotating, it will also experience the Yarkovsky effect.\n\nDiscovery of photophoresis is usually attributed to Felix Ehrenhaft in the 1920s, though earlier observations were made by others including Augustin-Jean Fresnel.\n\nThe applications of photophoresis expand into the various divisions of science, thus physics, chemistry as well as in biology. Photophoresis is applied in particle trapping and levitation, in the field flow fractionation of particles, in the determination of thermal conductivity and temperature of microscopic grains and also in the transport of soot particles in the atmosphere. The use of light in the separation of particles aerosols based on their optical properties, makes possible the separation of organic and inorganic particles of the same aerodynamic size.\n\nRecently, photophoresis has been suggested as a chiral sorting mechanism for single walled carbon nanotubes. The proposed method would utilise differences in the absorption spectra of semiconducting carbon nanotubes arising from optically excited transitions in electronic structure. If developed the technique would be orders of magnitudes faster than currently established ultracentrifugation techniques.\n\nDirect photophoresis is caused by the transfer of photon momentum to a particle by refraction and reflection. Movement of particles in the forward direction occurs when the particle is transparent and has an index of refraction larger compared to its surrounding medium. Indirect photophoresis occurs as a result of an increase in the kinetic energy of molecules when particles absorb incident light only on the irradiated side, thus creating a temperature gradient within the particle. In this situation the surrounding gas layer reaches temperature equilibrium with the surface of the particle. Molecules with higher kinetic energy in the region of higher gas temperature impinge on the particle with greater momenta than molecules in the cold region; this causes a migration of particles in a direction opposite to the surface temperature gradient. The component of the photophoretic force responsible for this phenomenon is called the radiometric force. This comes as a result of uneven distribution of radiant energy (source function within a particle).\nIndirect photophoretic force depends on the physical properties of the particle and the surrounding medium.\n\nFor pressures formula_1, where the free mean path of the gas is much larger than the characteristic size formula_2 of the suspended particle (direct photophoresis), the longitudinal force is \nwhere the mean temperature of the scattered gas is (thermal accommodation coefficient formula_4, momentum accommodation coefficient formula_5)\nand the black body temperature of the particle (net light flux formula_7, Stefan Boltzmann constant formula_8, temperature of the radiation field formula_9)\nformula_11 is the thermal conductivity of the particle.\nThe asymmetry factor for spheres formula_12 is usually formula_13 (positive longitudinal photophoresis).\nFor non-spherical particles, the average force exerted on the particle is given by the same equation where the radius formula_2 is now the radius of the respective volume-equivalent sphere.\n\n"}
{"id": "53850852", "url": "https://en.wikipedia.org/wiki?curid=53850852", "title": "Phylosymbiosis", "text": "Phylosymbiosis\n\nIn the field of microbiome research, a group of species is said to show a phylosymbiotic signal if the degree of similarity between the species' microbiomes recapitulates to a significant extent their evolutionary history.\nIn other words, a phylosymbiotic signal among a group of species is evident if their microbiome similarity dendrogram could significantly\nrecapitulate their hosts phylogenic tree. For the analysis of the phylosymbiotic signal to be reliable, environmental differences\nthat could shape the host microbiome should be either eliminated or accounted for.\nOne plausible mechanistic explanation for such phenomena could be, for example, a result of host immune genes that rapidly evolve in a continues arms race with members of its microbiome.\n\n"}
{"id": "41751262", "url": "https://en.wikipedia.org/wiki?curid=41751262", "title": "Prodromus", "text": "Prodromus\n\nA prodromus ('forerunner' or 'precursor') aka prodrome is a term used in the natural sciences to describe a preliminary publication intended as the basis for a later, more comprehensive work.\n\nIt is also a medical term used for a premonitory symptom, that is, a symptom indicating the onset of a disease.\n\nThe origin of the word is from the 19th century: via French from New Latin prodromus, from Greek prodromos forerunner.\n\nNotable prodromi were \"Prodromus Entomology\", \"Prodromus Florae Novae Hollandiae et Insulae Van Diemen\", \"Prodromus Systematis Naturalis Regni Vegetabilis\" and Nicolas Steno's \"De solido intra solidum naturaliter contento dissertationis prodromus\", one of the early treatises attempting to explain the occurrence of fossils in solid rock.\n"}
{"id": "50283749", "url": "https://en.wikipedia.org/wiki?curid=50283749", "title": "Rangeland management", "text": "Rangeland management\n\nRangeland management (also range management, range science, or arid-land management) is a professional natural science that centers around the study of rangelands and the \"conservation and sustainable management [of Arid-Lands] for the benefit of current societies and future generations.\" Range management is defined by Holechek et al. as the \"manipulation of rangeland components to obtain optimum combination of goods and services for society on a sustained basis.\"\n\nThe earliest form of Rangeland Management is not formally deemed part of the natural science studied today, although its roots can be traced to nomadic grazing practices of the neolithic agricultural revolution when humans domesticated plants and animals under pressures from population growth and environmental change. Humans might even have altered the environment in times preceding the Neolithic through hunting of large-game, whereby large losses of grazing herbivores could have resulted in altered ecological states; meaning humans have been inadvertently managing land throughout prehistory.\n\nRangeland management was developed in the United States in response to rangeland deterioration and in some cases, denudation, due to overgrazing and other misuse of arid lands, as was described by Hardin’s 1968 \"Tragedy of the Commons\" and evidenced previously by the 20th century \"Dust Bowl\". Historically, the discipline focused on the manipulation of grazing and the proper use of rangeland vegetation for livestock.\n\nToday, range management's focus has been expanded to include the host of ecosystem services that rangelands provide to humans world-wide. Key management components seek to optimize such goods and services through the protection and enhancement of soils, riparian zones, watersheds, and vegetation complexes, sustainably improving outputs of consumable range products such as red meat, wildlife, water, wood, fiber, leather, energy resource extraction, and outdoor recreation, as well as maintaining a focus on the manipulation of grazing activities of large herbivores to maintain or improve animal and plant production.\n\nThe Society for Range Management is \"the professional society dedicated to supporting persons who work with rangelands and have a commitment to their sustainable use.\" The primary Rangeland Management publications include the \"Journal of Range Management\", \"Rangelands\", and \"Rangeland Ecology & Management\".\n\nPastoralism has become a contemporary anthropological and ecological study as it faces many threats including fragmentation of land, conversion of rangeland into urban development, lack of grazing movement, impending threats on global diversity, damage to species with large terrain, decreases in shared public goods, decreased biological movements, threats of a \"tragedy of enclosures\", limitation of key resources, reduced biomass and invasive plant species growth. Interest in contemporary pastoralist cultures like the Maasai has continued to increase, especially because the traditional syncreticly-adaptive ability of pastoralists could promise lessons in collaborative and adaptive management for contemporary pastoralist societies threatened by globalization as well as for contemporary non-pastoralist societies that are managing livestock on rangelands.\n\nIn the United States, the study of range science is commonly offered at land-grant universities including New Mexico State University, Colorado State University, Oregon State University, South Dakota State University, Texas A&M University, Texas Tech University, the University of Arizona, the University of Idaho, the University of Wyoming, Utah State University, and Montana State University. The Range Science curriculum is strongly tied to animal science, as well as plant ecology, soil science, wildlife management, climatology and anthropology. Courses in a typical Range Science curriculum may include ethology, range animal nutrition, plant physiology, plant ecology, plant identification, plant communities, microbiology, soil sciences, fire control, agricultural economics, wildlife ecology, ranch management, Socioeconomics, cartography, hydrology, Ecophysiology, and environmental policy. These courses are essential to entering a range science profession.\n\nStudents with degrees in range science are eligible for a host of technician-type careers working for the federal government under the Bureau of Land Management, the United States Fish and Wildlife Service, the Agricultural Research Service, the United States Environmental Protection Agency, the NRCS, or the US Forest Service as range conservationists, inventory technicians, range monitoring/animal science agents, field botanists, natural-resource technicians, vegetation/habitat monitors, GIS programming assistants, general range technicians, and as ecological assessors, as well as working in the private sector as range managers, ranch managers, producers, commercial consultants, mining and agricultural real estate agents, or as Range/ Ranch Consultants. Individuals who complete degrees at the M.S. or P.h.D. level, can seek academic careers as professors, extension specialists, research assistants, and adjunct staff, in addition to a number of professional research positions for government agencies such as the US Department of Agriculture and other state run departments.\n\n"}
{"id": "46324244", "url": "https://en.wikipedia.org/wiki?curid=46324244", "title": "Skeletal changes of organisms transitioning from water to land", "text": "Skeletal changes of organisms transitioning from water to land\n\nInnovations conventionally associated with terrestrially first appeared in aquatic elpistostegalians such as \"Panderichthys rhombolepis\", \"Elpistostege watsoni\", and \"Tiktaalik roseae\". Phylogenetic analyses distribute the features that developed along the tetrapod stem and display a stepwise process of character acquisition, rather than abrupt. The complete transition occurred over a period of 25 million years beginning with the tetrapodomorph diversification in the Middle Devonian (380 myr).\n\nBy the Upper Devonian period, the fin-limb transition as well as other skeletal changes such as gill arch reduction, opercular series loss, mid-line fin loss, and scale reduction were already completed in many aquatic organisms. As aquatic tetrapods began their transition to land, several skeletal changes are thought to have occurred to allow for movement and respiration on land. Some adaptations required to adjust to non-aquatic life include the movement and use of alternating limbs, the use of pelvic appendages as sturdy propulsors, and the use of a solid surface at the organism’s base to generate propulsive force required for walking.\n\nThe Osteolepiformes and Elpistostegalia are two crown groups of rhipidistians with respect to the tetrapods. The development of skull roof and cheekbone patterns in these organisms match those found in the first tetrapods. Palatal and nasal skeletal features like choanae are present in these groups and are also observed in modern amphibians. This indicates that incipient air breathing was developed, as well as modification of the hyoid arch towards stapes development. These characteristics account for why osteichthyans are accepted as the sister group of tetrapods.\n\nThe elpistostegalid fish are considered the most apomorphic of fish in comparison to tetrapods. From well-preserved fossils, it is observed that they share a paltybasic skull with eye ridges, and external nares situated on the margin of the mouth. Development of eye ridges and flatting of the skull are also observed in primitive fossil amphibians and reptiles. The most likely reason for the traits to be adaptive was for their use in aerial vision above the waterline. The traits enabled animals to check area on land for safe spots if being chased by a predator in water, as well as being useful for searching for prey items above the water. The water-based lateral line system was used substantially by these aquatic tetrapods to detect danger from predators. Within the Osteichthyan diversification, there were no changes related to respiration in the transition as can be seen by the nasal region and palatal morphology in elpistostegalid fishes. The primary change from basic ostelepiform ancestors to the first elpistostegalid in the middle Devonian was to the pre-existing roof skulls.\n\nIn \"Elginerpeton pancheni\", a prototetrapod from the late Frasnian, basic tetrapod characteristics in the lower jaw and the cranium are observed. The taxon is believed to fill the gap between elpistostegalid fishes and well-preserved Devonian tetrapods. The \"Elginerpeton\" is considered more derived than the elpistostegalid fishes due to presence of paired fangs on the parasymphysial toothplate, a slender shaped anterior coronoid, and in the loss of the intracranial joint and coronoid fossa. The loss of the intercranial joint was a direct functional necessity to strengthen the broad and long platybasic skull when the animal was out of the water. The tubular lower jaw of the \"Elginerpeton\", compared to the flat-lamina jaw shape of fishes gave it superior cross-sectional force, required when not supported in an aquatic setting – allowing for opening of the mouth outside of water. The adaptation may also be interpreted as a specialization for buccopharyngeal breathing. It is speculated to be the first step towards aerial respiration in the transition from fish to tetrapod.\n\nIn the tetrapod and higher clades from the lower-middle Famennian there are several defining changes on the basis of anatomy of \"Ichthyostega\", \"Tulerpeton\", and \"Acanthostega\". In the cranium, there is a stapes derived from the hyomandibular of fishes; a single bilateral pair of nasal bones, and a fenestra ovalis in the otic capsule of the braincase. The opening of the otic wall of the braincase can be considered a paedomorphic feature for tetrapods and is linked to the stapes functionally. The stapes was thought to be just a structural support between the palate and the stapedial plate of the braincase. In the \"Acanthostega\", it is likely that due to the otic capsule of the brain case being mesial to the stapedial plate, sound was picked up from the palate or the otic notch to allow for rudimentary hearing. It was able to perceive vibrations by opening its mouth by way of the palate. Other factors that caused aquatic tetrapods to spend more time on land caused the development of terrestrial hearing with the development of a tympanum within an otic notch and developed by convergent evolution at least three times.\nThere was also a change in the dermal bones of the skull in the aquatic tetrapods. It involved the enlargement of the jugal, ceasing the contact of the maxilla with the squamosal and the single bilateral pair of nasal bones. The feature allows for a stronger bite as well as increasing the strength of the skull.\n\nFeeding on land is a completely different task than feeding in water. Water is much more dense and viscous compared to air, causing hunting techniques adapted in water to be less successful when applied on land. The main technique used in water is suction feeding and is used by most aquatic vertebrates. This technique does not function in air so animals use methods of overtaking prey with jaws followed by biting down. Transitional forms prior to fully developed terrestrial tetrapods such as \"Acanthostega\", are thought to have captured prey in the water. Large coronoid fangs are present in the fishes \"Eusthenopteron\", \"Panderichthys\", and \"Tiktaalik\", and the early tetrapod, \"Ventasega\". In \"Acanthostega\", which is more derived, the large teeth are absent. In \"Eusthenopetron\" and \"Panderichthys\", an ossified operculum is exhibited unlike in the \"Tiktaalik\", \"Ventastega\", and \"Acanthostega\". These differences as well as reductions of the gill chamber and changes in the nature of the lower jaw are hypothesized to indicate a reduced reliance on suction feeding in early tetrapods in comparison to osteolepiform fish. This morphological data is not enough however to prove that suction feeding was less used as the morphological changes have been found in fish that use the suction feeding mechanism.\n\nCranial sutures are indicators of skull function and morphologies can be linked to specific feeding modes. Transitional feeding changes can be observed by examining cross sectional morphology of a suture in taxa of the fish-tetrapod transition. Comparing positionally comparable sutures in extant fish allows for the creation of a sutural morphospace. The main cause of sutural deformation is caused by strain during feeding activity, most prominent with feeding mechanisms involving sucking a prey into the mouth. There is a tension anteriorly, and compression posteriorly strain patterns are observed in \"Polypterus\", a prey-sucking predator. In terrestrial tetrapod \"Phonerpeton\", there is compression between the frontals and parietals and a complex loading between the post parietals. There is no evidence of tensile strain in any sutures. \"Acanthostega\" fossil records demonstrate that no strain pattern was exhibited that relate to prey capture by means of suction. The load compression is similar to extant tetrapods. It is most likely that the organism captured prey by biting in the water or near the edge of the water. This finding indicates that the terrestrial mode of feeding first emerged in an aquatic environment.\n\nThe cranial endoskeleton of \"T. roseae\" shares derived features with tetrapods. There was a loss of opercular and extrascapular elements, enhancing head mobility in \"T. roseae\" compared to other tetrapodomorph fish. The formation of the neck allowed for locomotion in shallow waters. This environment allows for less motility compared to the three-dimensional space that fish are able to orient themselves in. The body of the organism in these environments would be fixed in the shallow pools with appendages planted on a substrate.\n\nIn the \"Acanthostega\" and \"Ichthyostega\", which are considered to be more derived than other basal aquatic tetrapods, the pectoral girdle is decoupled from the skull. There is also a loss of the dorsal pectoral girdle bones, which permits a large degree of movement for the shoulder. This allowed for a greater degree of movement, and is a necessity for improving aquatic maneuveurs and terrestrial locomotion. This could have been driven by the need to lift the head to aid aerial respiration by using nostrils and choanae.\n\nLimbs in vertebrates are occasionally organized into stylopod (relating to the humerus and femur), zeugopod (relating to the radius and tibia, along with associated structures) and autopod (relating to digits) categories, although anatomically, the evolutionary differences between these groups in early tetrapods tends to be vague.\nThe transition from fins to limbs occurred once an endoskeleton entered the base of the fin, as seen in today's lungfish. This is thought to have originated in the group Sarcopterygians, including osteolipiforms like \"Eusthenopteron\", due to the homology of the tetrapod forelimb and the osteolepiform fin endoskeleton.\n\n\"Acanthostega\" is a partially aquatic tetrapod with developed limbs that shares features common with the earlier tetrapods, \"Panderichthys\" and \"Eusthenopteron\". Like \"Panderichthys\", the humerus of \"Acanthostega\" is flattened dorso-ventrally, the intermedium terminates level with the radius, and the endoskeleton can be divided into stylopodium, zeugopodium and autopodium segments. Similar to \"Eusthenopteron\", the radials do not articulate with the radius on the distal end. \"Acanthostega\" also has a 1:2 ratio of humerus to radius and ulna, a feature seen in all tetrapods higher than \"Acanthostega\" on the phylogeny.\n\nUnlike \"Panderichthys\", \"Acanthostega\" hind limbs are at least the size of its fore limbs, if not larger. This development of larger limbs is required to physically support the organism during emergence from an aquatic setting to land. The humerus and femur of \"Acanthostega\" also contain evidence of greater development of the appendicular muscles compared to more aquatic tetrapods, hinting at the presence of digits.\n\nSimilarly, \"Ossinodus\" has two hindlimbs located bilaterally and proximodistally aymmetrical. Due to the presence of a small femur during juvenile development, this Carboniferous- period tetrapod is thought to be aquatic during juvenile development; only emerging onto land once it reaches adulthood. \"Ossinodus\" also has a broad, flat tibia, akin to \"Acanthostega\", and is thought to be only partially terrestrial.\n\nThe development of the pelvic region was crucial for the adaptation from water to land, yet some features of tetrapod locomotion are thought to have arose before the origin of digited limbs or the transition from water to land. The fossil record of early tetrapods shows evidence of distinct pelvic development occurring in osteolepiforms, further supporting osteolepiform ancestry of terrestrial tetrapods.\n\n\"Acanthostega\" has a large pelvis, with the iliac region articulating with the axial skeleton and a broad ischial plate. It has a sacrum; a fundamental skeletal feature that allows the organism to transfer force produced in its hindlimbs to its axial skeleton, and move in a terrestrial environment. A pubo-ischiadic symphysis is also observed, uniting the two pelvic halves.\n\nIn contrast, \"Protopterus annectens\" (a member of lungfish, thought to be a sister group to tetrapods) has a small, anatomically simpler pelvis, a derived limb endoskeleton and a lack of digits. Yet, it shares the ability to lift itself using a solid surface as a base with its pelvic region with \"Acanthostega\" and is also observed to move with tetrapod-like locomotion in an aquatic environment. This illustrates that a fundamental innovation in tetrapods is also found in a lower, sister taxon, in which members lack a sacrum.\n\n\"Acanthostega\" is the earliest example of a digitized tetrapod. The humerus and femur of \"Acanthostega\" contain evidence of greater development of the appendicular muscles compared to more aquatic tetrapods. \"Acanthostega\" has a total lack of dermal fin rays and displays the presence of two or more spool-shaped bones or cartilages articulating individually in antero-posterial sets on the distal end of its limbs. This feature can now be distinguished as digits instead of the endoskeletal radials seen in earlier tetrapods.\n\n\"Pederpes\", a tetrapod from the Early Carboniferous period, also has hindlimbs containing 5 digits that are rotated to face anteriorly. Unlike previous tetrapods, who have been only partially adapted to land, \"Pederpes\" has the novel ability to bend its limbs and propel itself forwards in a terrestrial setting. This is attributed to the symmetry of the digits and limbs in \"Pederpes\", allowing it to rotate its hindlimbs to an anteriorly facing position and propel itself from the edge of the foot when moving forward. This morphological development of bendable wrists and ankles can distinguish \"Pederpes\" the first true terrestrial tetrapod.\n"}
{"id": "10854000", "url": "https://en.wikipedia.org/wiki?curid=10854000", "title": "Statistical study of energy data", "text": "Statistical study of energy data\n\nEnergy statistics refers to collecting, compiling, analyzing and disseminating data on commodities such as coal, crude oil, natural gas, electricity, or renewable energy sources (biomass, geothermal, wind or solar energy), when they are used for the energy they contain. Energy is the capability of some substances, resulting from their physico-chemical properties, to do work or produce heat. Some energy commodities, called fuels, release their energy content as heat when they burn. This heat could be used to run an internal or external combustion engine.\n\nThe need to have statistics on energy commodities became obvious during the 1973 oil crisis that brought tenfold increase in petroleum prices. Before the crisis, to have accurate data on global energy supply and demand was not deemed critical. Another concern of energy statistics today is a huge gap in energy use between developed and developing countries. As the gap narrows (\"see picture\"), the pressure on energy supply increases tremendously. \n\nThe data on energy and electricity come from three principal sources:\nThe flows of and trade in energy commodities are measured both in physical units (e.g., metric tons), and, when energy balances are calculated, in energy units (e.g., terajoules or tons of oil equivalent). What makes energy statistics specific and different from other fields of economic statistics is the fact that energy commodities undergo greater number of transformations (flows) than other commodities. In these transformations energy is conserved, as defined by and within the limitations of the first and second laws of thermodynamics. \n\n\n\n"}
{"id": "26821712", "url": "https://en.wikipedia.org/wiki?curid=26821712", "title": "Tsunamis in lakes", "text": "Tsunamis in lakes\n\nA tsunami is defined as a series of water waves caused by the displacement of a large volume of a body of water; in the case of this article the body of water being investigated will be a lake rather than an ocean. Tsunamis in lakes are becoming increasingly important to investigate as a hazard, due to the increasing popularity for recreational uses, and increasing populations that inhabit the shores of lakes. Tsunamis generated in lakes and reservoirs are of high concern because it is associated with a near field source region which means a decrease in warning times to minutes or hours.\n\nInland tsunami hazards can be generated by many different types of earth movement. Some of these include earthquakes in or around lake systems, landslides, debris flow, rock avalanches, and glacier calving. Volcanogenic processes such as gas and mass flow characteristics are discussed in more detail below.\n\nTsunamis in lakes can be generated by fault displacement beneath or around lake systems. Faulting shifts the ground in a vertical motion through reverse, normal or oblique strike slip faulting processes, this displaces the water above causing a tsunami (Figure 1). The reason strike-slip faulting does not cause tsunamis is because there is no vertical displacement within the fault movement, only lateral movement resulting in no displacement of the water. In an enclosed basin such as a lake, tsunamis are referred to as the initial wave produced by coseismic displacement from an earthquake, and the seiche as the harmonic resonance within the lake.\n\nIn order for a tsunami to be generated certain criteria is required:\n\nThese tsunamis are of high damage potential due to being within a lake, making them of a near field source. This means a vast decrease in warning times, resulting in organised emergency evacuations after the generation of the tsunami being virtually impossible, and due to low lying shores even small waves lead to substantial flooding. Planning and education of residents needs to be done beforehand, so that when an earthquake is felt they know to head to higher ground and what routes to take to get there.\n\nLake Tahoe is an example of a lake that is in danger of having a tsunami due to faulting processes. Lake Tahoe in California and Nevada USA lies within an intermountain basin bounded by faults, with most of these faults at the lake bottom or hidden in glaciofluvial deposits. Lake Tahoe has had many prehistoric eruptions and in studies of the lake bottom sediments, a 10m high scarp has displaced the lake bottom sediments, indicating that the water was displaced by the same magnitude, as well as generating a tsunami. A tsunami and seiche in Lake Tahoe can be treated as shallow-water long waves as the maximum water depth is much smaller than the wavelength. This demonstrates the interesting impact that lakes have on the tsunami wave characteristics, as it is very different from ocean tsunami wave characteristics due to the ocean being deeper, and lakes being relatively shallow in comparison. With ocean tsunami waves amplitudes only increase when the tsunami gets close to shore, in lake tsunami waves are generated and stay in a shallow environment.\n\nThis would have a major impact on the 34,000 permanent residences along the lake, not to mention the impact on tourism in the area. Tsunami run-ups would leave areas near the lake inundated due to permanent ground subsidence attributed to the earthquake, with the highest run-ups and amplitudes being attributed to the seiches rather than the actual tsunami. The reason seiches cause so much damage is due to resonance within the bays reflecting the waves where they combine to make larger standing waves. For more information see seiches. Lake Tahoe also experienced a massive collapse of the western edge of the basin that formed McKinney Bay around 50,000 years ago. Is thought to have generated a tsunami/seiche wave with a height approaching .\n\nSub-aerial mass flows (landslides or rapid mass wasting) happen when a large amount of sediment becomes unstable, this can happen for example from the shaking from an earthquake, or saturation of the sediment initiating a sliding layer. This volume of sediment then flows into the lake giving a sudden large displacement of water. Tsunamis generated by sub aerial mass flows are defined in terms of the first initial wave being the tsunami wave and any tsunamis in terms of sub aerial mass flows are characterised into three zones. A splash zone or wave generation zone, this is the region were landslides and water motion are coupled and it extends as far as the landslide travels. Near field area, were the concern is based on the characteristics of the tsunami wave such as amplitude and wavelength which are crucial for predictive purposes. Far field area, the process is influenced mainly by dispersion characteristics and is not often used when investigating tsunamis in lakes, as most lake tsunamis are related only to near field processes.\n\nA modern example of a landslide into a reservoir lake, overtopping a dam, occurred in Italy with the Vajont Dam disaster in 1963. Evidence exists in paleoseismological evidence and other sedimentary core sample proxies of catastrophic rock failures of landslide-triggered lake tsunamis worldwide, including in Lake Geneva during AD 563.\n\nIn the event of the Alpine fault in New Zealand rupturing in the South Island, it is predicted that there would be shaking of approximately magnitude eight in the lake side towns of Queenstown (Lake Wakatipu) and Wanaka (Lake Wanaka). These could possibly cause sub-aerial mass flows that could generate tsunamis within the lakes, this would have a devastating impact on the 28,224 residents (2013 New Zealand census) who occupy these lake towns, not only in the potential losses of life and property, but the damage to the booming tourism industry would take years to rebuild.\n\nThe Otago Regional Council, responsible for the area, has recognised that in such an event, tsunamis could occur in both lakes.\n\nIn this article the focus is on tsunamis generated in lakes by volcanogenic processes in terms of gas build up causing violent lake over turns, with other processes such as pyroclastic flows not accounted for, as it requires more complex modelling . Lake overturns can be incredibly dangerous and occur when gas trapped at the bottom of the lake is heated by rising magma causing an explosion and lake overturn; an example of this is Lake Kivu.\n\nLake Kivu, one of the African Great Lakes, lies on the border between the Democratic Republic of the Congo and Rwanda, and is part of the East Africa Rift. Being part of the rift means it is affected by volcanic activity beneath the lake. This has led to a buildup of methane and carbon dioxide at the bottom of the lake, which can lead to violent limnic eruptions.\n\nLimnic eruptions (also called \"lake over turns\") are due to volcanic interaction with the water at the bottom of the lake that has high gas concentrations, this leads to heating of the lake and this rapid rise in temperature would spark a methane explosion displacing a large amount of water, followed nearly simultaneously by a release of carbon dioxide. This carbon dioxide would suffocate large numbers of people, with a possible tsunami generated from water displaced by the gas explosion affecting all of the 2 million people who occupy the shores of Lake Kivu. This is incredibly important as the warning times for an event such as a lake overturn is incredibly short in the order of minutes and the event itself may not even be noticed. Education of locals and preparation is crucial in this case and a lot of research in this area has been done in order to try to understand what is happening within the lake, in order to try to reduce the effects when this phenomenon does happen.\n\nA lake turn-over in Lake Kivu occurs from one of two scenarios. Either (1) up to another hundred years of gas accumulation leads to gas saturation in the lake, resulting in a spontaneous outburst of gas originating at the depth at which gas saturation has exceeded 100%, or (2) a volcanic or even seismic event triggers a turn-over. In either case a strong vertical lift of a large body of water results in a plume of gas bubbles and water rising up to and through the water surface. As the bubbling water column draws in fresh gas-laden water, the bubbling water column widens and becomes more energetic as a virtual \"chain reaction\" occurs which would look like a watery volcano. Very large volumes of water are displaced, vertically at first, then horizontally away from the centre at surface and horizontally inwards to the bottom of the bubbling water column, feeding in fresh gas-laden water. The speed of the rising column of water increases until it has the potential to rise 25m or more in the centre above lake level. The water column has the potential to widen to well in excess of a kilometre, in a violent disturbance of the whole lake. The watery volcano may take as much as a day to fully develop while it releases upwards of 400 billion cubic metres of gas (~12tcf). Some of these parameters are uncertain, particularly the time taken to release the gas and the height to which the water column can rise. As a secondary effect, particularly if the water column behaves irregularly with a series of surges, the lake surface will both rise by up to several metres and create a series of tsunamis or waves radiating away from the epicentre of the eruption. Surface waters may simultaneously race away from the epicentre at speeds as high as 20-40m/second, slowing as distances from the centre increase. The size of the waves created is unpredictable. Wave heights will be highest if the water column surges periodically, resulting in wave heights is great as 10-20m. This is caused by the ever-shifting pathway that the vertical column takes to the surface. No reliable model exists to predict this overall turnover behaviour. For tsunami precautions it will be necessary for people to move to high ground, at least 20m above lake level. A worse situation may pertain in the Ruzizi River where a surge in lake level would cause flash-flooding of the steeply sloping river valley dropping 700m to Lake Tanganyika, where it is possible that a wall of water from 20-50m high may race down the gorge. Water is not the only problem for residents of the Kivu basin; the more than 400 billion cubic metres of gas released creates a denser-than-air cloud which may blanket the whole valley to a depth of 300m or more. The presence of this opaque gas cloud, which would suffocate any living creatures with its mixture of carbon dioxide and methane laced with hydrogen sulphide, would cause the majority of casualties. Residents would be advised to climb to at least 400m above the lake level to ensure their safety. Strangely the risk of a gas explosion is not great as the gas cloud is only about 20% methane in carbon dioxide, a mixture that is difficult to ignite.\n\nAt 11:24 PM on 21 July 2014, in a period experiencing an earthquake swarm related to the upcoming eruption of Bárðarbunga, an 800m-wide section gave way on the slopes of the Icelandic volcano Askja. Beginning at 350m over water height, it caused a tsunami 20–30 meters high across the caldera, and potentially larger at localized points of impact. Thanks to the late hour, no tourists were present; however, search and rescue observed a steam cloud rising from the volcano, apparently geothermal steam released by the landslide. Whether geothermal activity played a role in the landslide is uncertain. A total of 30-50 million cubic meters was involved in the landslide, raising the caldera's water level by 1–2 meters.\n\nHazard mitigation for tsunamis in lakes is immensely important in the preservation of life, infrastructure and property. In order for hazard management of tsunamis in lakes to function at full capacity there are four aspects that need to be balanced and interacted with each other, these are:\n\n\nWhen all these aspects are taken into consideration and continually managed and maintained, the vulnerability of an area to a tsunami within the lake decreases. This is not because the hazard itself has decreased but the awareness of the people who would be affected makes them more prepared to deal with the situation when it does occur. This reduces recovery and response times for an area, decreasing the amount of disruption and in turn the effect the disaster has on the community.\n\nInvestigation into the phenomena of tsunamis in lakes for this article was restricted by certain limitations. Internationally there has been a fair amount of research into certain lakes but not all lakes that can be affected by the phenomenon have been covered. This is especially true for New Zealand with the possible occurrence of tsunamis in the major lakes recognised as a hazard, but with no further research completed.\n\n\n"}
{"id": "1103359", "url": "https://en.wikipedia.org/wiki?curid=1103359", "title": "Ultra-high vacuum", "text": "Ultra-high vacuum\n\nUltra-high vacuum (UHV) is the vacuum regime characterised by pressures lower than about 10 pascal or 100 nanopascals (10 mbar, ~10 torr). UHV conditions are created by pumping the gas out of a UHV chamber. At these low pressures the mean free path of a gas molecule is greater than approximately 40 km, so the gas is in free molecular flow, and gas molecules will collide with the chamber walls many times before colliding with each other. Almost all molecular interactions therefore take place on various surfaces in the chamber.\n\nUHV conditions are integral to scientific research. Surface science experiments often require a chemically clean sample surface with the absence of any unwanted adsorbates. Surface analysis tools such as X-ray photoelectron spectroscopy and low energy ion scattering require UHV conditions for the transmission of electron or ion beams. For the same reason, beam pipes in particle accelerators such as the Large Hadron Collider are kept at UHV.\n\n\nMaintaining UHV conditions requires the use of unusual materials for equipment. Heating of the entire system above 100 °C for many hours (\"baking\") to remove water and other trace gases which adsorb on the surfaces of the chamber is required upon \"cycling\" the equipment to atmosphere. To save time, energy, and integrity of the UHV volume an \"interlock\" is often used. The interlock volume has one door or valve facing the UHV side of the volume, and another door against atmospheric pressure through which samples or workpieces are initially introduced. After sample introduction and assuring that the door against atmosphere is closed, the interlock volume is typically pumped down to a medium-high vacuum. In some cases the workpiece itself is baked out or otherwise pre-cleaned under this medium-high vacuum. The gateway to the UHV chamber is then opened, the workpiece transferred to the UHV by robotic means or by other contrivance if necessary, and the UHV valve re-closed. While the initial workpiece is being processed under UHV, a subsequent sample can be introduced into the interlock volume, pre-cleaned, and so-on and so-forth, saving much time. Although a \"puff\" of gas is generally released into the UHV system when the valve to the interlock volume is opened, the UHV system pumps can generally snatch this gas away before it has time to adsorb onto the UHV surfaces. In a system well designed with suitable interlocks, the UHV components seldom need bakeout and the UHV may improve over time even as workpieces are introduced and removed.\n\nMany common materials are used sparingly if at all due to high vapor pressure, high adsorptivity or absorptivity resulting in subsequent troublesome outgassing, or high permeability in the face of differential pressure (i.e.: \"through-gassing\"):\n\nTechnical limitations: \n\nUltra-high vacuum is necessary for many surface analytic techniques such as:\n\nUHV is necessary for these applications to reduce surface contamination, by reducing the number of molecules reaching the sample over a given time period. At 0.1 mPa (10 Torr), it only takes 1 second to cover a surface with a contaminant, so much lower pressures are needed for long experiments.\n\nUHV is also required for:\nand, while not compulsory, can prove beneficial in applications such as:\n\nTypically, UHV requires:\n\nOutgassing is a problem for UHV systems. Outgassing can occur from two sources: surfaces and bulk materials. Outgassing from bulk materials is minimized by selection of materials with low vapor pressures (such as glass, stainless steel, and ceramics) for everything inside the system. Materials which are not generally considered absorbent can outgas, including most plastics and some metals. For example, vessels lined with a highly gas-permeable material such as palladium (which is a high-capacity hydrogen sponge) create special outgassing problems.\n\nOutgassing from surfaces is a subtler problem. At extremely low pressures, more gas molecules are adsorbed on the walls than are floating in the chamber, so the total surface area inside a chamber is more important than its volume for reaching UHV. Water is a significant source of outgassing because a thin layer of water vapor rapidly adsorbs to everything whenever the chamber is opened to air. Water evaporates from surfaces too slowly to be fully removed at room temperature, but just fast enough to present a continuous level of background contamination. Removal of water and similar gases generally requires baking the UHV system at 200 to 400 °C while vacuum pumps are running. During chamber use, the walls of the chamber may be chilled using liquid nitrogen to reduce outgassing further.\n\nHydrogen and carbon monoxide are the most common background gases in a well-designed, well-baked UHV system. Both Hydrogen and CO diffuse out from the grain boundaries in stainless steel. Helium could diffuse through the steel and glass from the outside air, but this effect is usually negligible due to the low abundance of He in the atmosphere.\n\nThere is no single vacuum pump that can operate all the way from atmospheric pressure to ultra-high vacuum. Instead, a series of different pumps is used, according to the appropriate pressure range for each pump. Pumps commonly used to achieve UHV include:\n\nUHV pressures are measured with an ion gauge, either a hot filament or an inverted magnetron type.\n\nMetal seals, with knife edges on both sides cutting into a soft, copper gasket. This all-metal seal can maintain pressures down to 100 pPa (~10 Torr).\n\nMeasurement of high vacuum is done using a \"nonabsolute gauge\" that measures a pressure-related property of the vacuum, for example, its thermal conductivity. See, for example, Pacey. These gauges must be calibrated. The gauges capable of measuring the lowest pressures are magnetic gauges based upon the pressure dependence of the current in a spontaneous gas discharge in intersecting electric and magnetic fields.\n\nA UHV manipulator allows an object which is inside a vacuum chamber and under vacuum to be mechanically positioned. It may provide rotary\nmotion, linear motion, or a combination of both. The most complex devices give motion in three axes and rotations around two of those axes. To generate the mechanical movement inside the chamber, two basic mechanisms are commonly employed: a mechanical coupling through the vacuum wall (using a vacuum-tight seal around the coupling), or a magnetic coupling that transfers motion from air-side to vacuum-side. Various forms of motion control are available for manipulators, such as knobs, handwheels, motors, stepping motors, piezoelectric motors, and pneumatics.\n\nThe manipulator or sample holder may include features that allow additional control and testing of a sample, such as the ability to apply heat, cooling, voltage, or a magnetic field. Sample heating can be accomplished by electron bombardment or thermal radiation. For electron bombardment, the sample holder is equipped with a filament which emits electrons when biased at a high negative potential. The impact of the\nelectrons bombarding the sample at high energy causes it to heat. For thermal radiation, a filament is mounted close to the sample and resistively heated to high temperature. The infrared energy from the filament heats the sample.\n\n\n"}
{"id": "31596828", "url": "https://en.wikipedia.org/wiki?curid=31596828", "title": "Unequal crossing over", "text": "Unequal crossing over\n\nUnequal crossing over is a type of gene duplication or deletion event that deletes a sequence in one strand and replaces it with a duplication from its sister chromatid in mitosis or from its homologous chromosome during meiosis. It is a type of chromosomal crossover between homologous sequences that are not paired precisely. Normally genes are responsible for occurrence of crossing over. It exchanges sequences of different links between chromosomes. Along with gene conversion, it is believed to be the main driver for the generation of gene duplications and is a source of mutation in the genome.\n\nDuring meiosis, the duplicated chromosomes (chromatids) in eukaryotic organisms are attached to each other in the centromere region and are thus paired. The maternal and paternal chromosomes then align alongside each other. During this time, recombination can take place via crossing over of sections of the paternal and maternal chromatids and leads to reciprocal recombination or non-reciprocal recombination. Unequal crossing over requires a measure of similarity between the sequences for misalignment to occur. The more similarity within the sequences, the more likely unequal crossing over will occur. One of the sequences is thus lost and replaced with the duplication of another sequence.\n\nWhen two sequences are misaligned, unequal crossing over may create a tandem repeat on one chromosome and a deletion on the other. The rate of unequal crossing over will increase with the number of repeated sequences around the duplication. This is because these repeated sequences will pair together, allowing for the mismatch in the cross over point to occur.\n\nUnequal crossing over is the process most responsible for creating regional gene duplications in the genome. Repeated rounds of unequal crossing over cause the homogenization of the two sequences. With the increase in the duplicates, unequal crossing over can lead to dosage imbalance in the genome and can be highly deleterious.\n\nIn unequal crossing over, there can be large sequence exchanges between the chromosomes. Compared with gene conversion, which can only transfer a maximum of 1,500 base pairs, unequal crossing over in yeast rDNA genes has been found to transfer about 20,000 base pairs in a single crossover event Unequal crossover can be followed by the concerted evolution of duplicated sequences.\n\nIt has been suggested that longer intron found between two beta-globin genes are a response to deleterious selection from unequal crossing over in the beta-globin genes. Comparisons between alpha-globin, which does not have long introns, and beta-globin genes show that alpha-globin have 50 times higher concerted evolution.\n\nWhen unequal crossing over creates a gene duplication, the duplicate has 4 evolutionary fates. This is due to the fact that purifying selection acting on a duplicated copy is not very strong. Now that there is a redundant copy, neutral mutations can act on the duplicate. Most commonly the neutral mutations will continue until the duplicate becomes a pseudogene. If the duplicate copy increases the dosage effect of the gene product, then the duplicate may be retained as a redundant copy. Neofunctionalization is also a possibility: the duplicated copy acquires a mutation that gives it a different function than its ancestor. If both copies acquire mutations, it is possible that a subfunctional event occurs. This happens when both of the duplicated sequences have a more specialized function than the ancestral copy\n\nGene duplications are the main reason for the increase of genome size, and as unequal crossing over is the main mechanism for gene duplication, unequal crossing over contributes to genome size evolution is the most common regional duplication event that increases the size of the genome.\n\nWhen viewing the genome of a eukaryote, a striking observation is the large amount of tandem, repetitive DNA sequences that make up a large portion of the genome. For example, over 50% of the \"Dipodmys ordii\" genome is made up of three specific repeats. \"Drosophila virilis\" has three sequences that make up 40% of the genome, and 35% of the \"Absidia glauca\" is repetitive DNA sequences. These short sequences have no selection pressure acting on them and the frequency of the repeats can be changed by unequal crossing over.\n"}
{"id": "54716184", "url": "https://en.wikipedia.org/wiki?curid=54716184", "title": "United States Energy Association", "text": "United States Energy Association\n\nThe United States Energy Association (USEA) is the U.S. Member Committee of the World Energy Council. Headquartered in Washington, D.C., USEA is an association of public and private energy-related organizations, corporations, and government agencies. \nThe association hosts annual events such as the Carbon Sequestration Leadership Forum, Energy Supply Forum, and State of the Energy Industry Forum.\n\nBarry Worthington has served as USEA’s executive director since 1988. Worthington chairs the Clean Electricity Production working group within the UNECE Committee on Sustainable Energy. He also sits on numerous energy boards, including the National Energy Foundation (chairman) and Energy Law Foundation.\n\nWorthington meets with domestic and international energy leaders to discuss energy infrastructure partnerships. He often advocates for energy cultivation in developing countries, claiming there are \"few priorities greater for the world than getting people linked to the grid.\" Worthington is a firm supporter of energy \"sovereignty.\"\n\nVicky Bailey currently chairs USEA’s board of directors. She succeeded Jack Futcher, the President and COO of Bechtel.\n\nFor 25 years, USEA has been a partner with USAID, expanding energy infrastructure, improving energy access, and reducing energy poverty in developing economies through international energy partnerships. A major function of USEA’s is to help USAID expand energy infrastructure and programs in developing countries. In 2012, the association launched the U.S.-East Africa Geothermal Partnership (EAGP), a public-private partnership “offering assistance at early stages of project development in East Africa.” Through the Djibouti Geothermal Partnership, Ethiopia Geothermal Partnership, and Kenya Electricity Generating Company (KenGen), USEA partners with the Department of Energy and local governments to promote U.S. companies’ involvement in developing additional geothermal generation capacity. According to USEA, the number of U.S. companies conducting geothermal work in East Africa has more than tripled since EAGP’s inception.\n\nUSEA represents the interests of the U.S. energy sector through public education and advocacy. The association supports an “all-of-the-above energy strategy,” from renewable energy to fossil fuels. USEA advocates for the exploration and production of oil and natural gas.\n\n"}
{"id": "5547312", "url": "https://en.wikipedia.org/wiki?curid=5547312", "title": "Vacuum deposition", "text": "Vacuum deposition\n\nVacuum deposition is a family of processes used to deposit layers of material atom-by-atom or molecule-by-molecule on a solid surface. These processes operate at pressures well below atmospheric pressure (i.e., vacuum). The deposited layers can range from a thickness of one atom up to millimeters, forming freestanding structures. Multiple layers of different materials can be used, for example to form optical coatings. The process can be qualified based on the vapor source; physical vapor deposition uses a liquid or solid source and chemical vapor deposition uses a chemical vapor.\n\nThe vacuum environment may serve one or more purposes:\n\nCondensing particles can be generated in various ways:\n\nIn reactive deposition, the depositing material reacts either with a component of the gaseous environment (Ti + N → TiN) or with a co-depositing species (Ti + C → TiC). A plasma environment aids in activating gaseous species (N → 2N) and in decomposition of chemical vapor precursors (SiH → Si + 4H). The plasma may also be used to provide ions for vaporization by sputtering or for bombardment of the substrate for sputter cleaning and for bombardment of the depositing material to densify the structure and tailor properties (ion plating).\n\nWhen the vapor source is a liquid or solid the process is called physical vapor deposition (PVD). When the source is a chemical vapor precursor, the process is called chemical vapor deposition (CVD). The latter has several variants: \"low-pressure chemical vapor deposition\" (LPCVD), Plasma-enhanced chemical vapor deposition (PECVD), and \"plasma-assisted CVD\" (PACVD). Often a combination of PVD and CVD processes are used in the same or connected processing chambers.\n\n\nA thickness of less than one micrometre is generally called a thin film while a thickness greater than one micrometre is called a coating.\n\n\n"}
{"id": "32500", "url": "https://en.wikipedia.org/wiki?curid=32500", "title": "Vacuum pump", "text": "Vacuum pump\n\nA vacuum pump is a device that removes gas molecules from a sealed volume in order to leave behind a partial vacuum. The first vacuum pump was invented in 1650 by Otto von Guericke, and was preceded by the suction pump, which dates to antiquity.\n\nThe predecessor to the vacuum pump was the suction pump, which was known to the Romans. Dual-action suction pumps were found in the city of Pompeii. Arabic engineer Al-Jazari also described suction pumps in the 13th century. He said that his model was a larger version of the siphons the Byzantines used to discharge the Greek fire. The suction pump later reappeared in Europe from the 15th century.\nBy the 17th century, water pump designs had improved to the point that they produced measurable vacuums, but this was not immediately understood. What was known was that suction pumps could not pull water beyond a certain height: 18 Florentine yards according to a measurement taken around 1635. (The conversion to metres is uncertain, but it would be about 9 or 10 metres.) This limit was a concern to irrigation projects, mine drainage, and decorative water fountains planned by the Duke of Tuscany, so the duke commissioned Galileo to investigate the problem. Galileo advertised the puzzle to other scientists, including Gasparo Berti who replicated it by building the first water barometer in Rome in 1639. Berti's barometer produced a vacuum above the water column, but he could not explain it. The breakthrough was made by Evangelista Torricelli in 1643. Building upon Galileo's notes, he built the first mercury barometer and wrote a convincing argument that the space at the top was a vacuum. The height of the column was then limited to the maximum weight that atmospheric pressure could support; this is the limiting height of a suction pump.\n\nIn 1654, Otto von Guericke invented the first vacuum pump and conducted his famous Magdeburg hemispheres experiment, showing that teams of horses could not separate two hemispheres from which the air had been evacuated. Robert Boyle improved Guericke's design and conducted experiments on the properties of vacuum. Robert Hooke also helped Boyle produce an air pump which helped to produce the vacuum. The study of vacuum then lapsed until 1855, when Heinrich Geissler invented the mercury displacement pump and achieved a record vacuum of about 10 Pa (0.1 Torr). A number of electrical properties become observable at this vacuum level, and this renewed interest in vacuum. This, in turn, led to the development of the vacuum tube.\n\nIn the 19th century, Nikola Tesla designed an apparatus that contains a Sprengel pump to create a high degree of exhaustion.\n\nPumps can be broadly categorized according to three techniques:\n\nPositive displacement pumps use a mechanism to repeatedly expand a cavity, allow gases to flow in from the chamber, seal off the cavity, and exhaust it to the atmosphere. Momentum transfer pumps, also called molecular pumps, use high speed jets of dense fluid or high speed rotating blades to knock gas molecules out of the chamber. Entrapment pumps capture gases in a solid or adsorbed state. This includes cryopumps, getters, and ion pumps.\n\nPositive displacement pumps are the most effective for low vacuums. Momentum transfer pumps in conjunction with one or two positive displacement pumps are the most common configuration used to achieve high vacuums. In this configuration the positive displacement pump serves two purposes. First it obtains a rough vacuum in the vessel being evacuated before the momentum transfer pump can be used to obtain the high vacuum, as momentum transfer pumps cannot start pumping at atmospheric pressures. Second the positive displacement pump backs up the momentum transfer pump by evacuating to low vacuum the accumulation of displaced molecules in the high vacuum pump. Entrapment pumps can be added to reach ultrahigh vacuums, but they require periodic regeneration of the surfaces that trap air molecules or ions. Due to this requirement their available operational time can be unacceptably short in low and high vacuums, thus limiting their use to ultrahigh vacuums. Pumps also differ in details like manufacturing tolerances, sealing material, pressure, flow, admission or no admission of oil vapor, service intervals, reliability, tolerance to dust, tolerance to chemicals, tolerance to liquids and vibration.\n\nA partial vacuum may be generated by increasing the volume of a container. To continue evacuating a chamber indefinitely without requiring infinite growth, a compartment of the vacuum can be repeatedly closed off, exhausted, and expanded again. This is the principle behind a positive displacement pump, for example the manual water pump. Inside the pump, a mechanism expands a small sealed cavity to reduce its pressure below that of the atmosphere. Because of the pressure differential, some fluid from the chamber (or the well, in our example) is pushed into the pump's small cavity. The pump's cavity is then sealed from the chamber, opened to the atmosphere, and squeezed back to a minute size.\n\nMore sophisticated systems are used for most industrial applications, but the basic principle of cyclic volume removal is the same:\n\nThe base pressure of a rubber- and plastic-sealed piston pump system is typically 1 to 50 kPa, while a scroll pump might reach 10 Pa (when new) and a rotary vane oil pump with a clean and empty metallic chamber can easily achieve 0.1 Pa.\n\nA positive displacement vacuum pump moves the same volume of gas with each cycle, so its pumping speed is constant unless it is overcome by backstreaming.\n\nIn a momentum transfer pump, gas molecules are accelerated from the vacuum side to the exhaust side (which is usually maintained at a reduced pressure by a positive displacement pump). Momentum transfer pumping is only possible below pressures of about 0.1 kPa. Matter flows differently at different pressures based on the laws of fluid dynamics. At atmospheric pressure and mild vacuums, molecules interact with each other and push on their neighboring molecules in what is known as viscous flow. When the distance between the molecules increases, the molecules interact with the walls of the chamber more often than with the other molecules, and molecular pumping becomes more effective than positive displacement pumping. This regime is generally called high vacuum.\n\nMolecular pumps sweep out a larger area than mechanical pumps, and do so more frequently, making them capable of much higher pumping speeds. They do this at the expense of the seal between the vacuum and their exhaust. Since there is no seal, a small pressure at the exhaust can easily cause backstreaming through the pump; this is called stall. In high vacuum, however, pressure gradients have little effect on fluid flows, and molecular pumps can attain their full potential.\n\nThe two main types of molecular pumps are the diffusion pump and the turbomolecular pump. Both types of pumps blow out gas molecules that diffuse into the pump by imparting momentum to the gas molecules. Diffusion pumps blow out gas molecules with jets of oil or mercury, while turbomolecular pumps use high speed fans to push the gas. Both of these pumps will stall and fail to pump if exhausted directly to atmospheric pressure, so they must be exhausted to a lower grade vacuum created by a mechanical pump.\n\nAs with positive displacement pumps, the base pressure will be reached when leakage, outgassing, and backstreaming equal the pump speed, but now minimizing leakage and outgassing to a level comparable to backstreaming becomes much more difficult.\n\nRegenerative pumps utilize vortex behavior of the fluid (air). The construction is based on hybrid concept of centrifugal pump and turbopump. Usually it consists of several sets of perpendicular teeth on the rotor circulating air molecules inside stationary hollow grooves like multistage centrifugal pump. They can reach to 1×10 mbar (0.001 Pa)(when combining with Holweck pump) and directly exhaust to atmospheric pressure. Examples of such pumps are Edwards EPX (technical paper ) and Pfeiffer OnTool™ Booster 150. It is sometimes referred as side channel pump. Due to high pumping rate from atmosphere to high vacuum and less contamination since bearing can be installed at exhaust side, this type of pumps are used in load lock in semiconductor manufacturing processes.\n\nThis type of pump suffers from high power consumption(~1 kW) compare to turbomolecular pump (<100W) at low pressure since most power is consumed to back atmospheric pressure. This can be reduced by nearly 10 times by backing with a small pump.\n\nAn entrapment pump may be a cryopump, which uses cold temperatures to condense gases to a solid or adsorbed state, a chemical pump, which reacts with gases to produce a solid residue, or an ion pump, which uses strong electrical fields to ionize gases and propel the ions into a solid substrate. A cryomodule uses cryopumping. Other types are the sorption pump, non-evaporative getter pump, and titanium sublimation pump (a type of evaporative getter that can be used repeatedly).\n\n\nPumping speed refers to the volume flow rate of a pump at its inlet, often measured in volume per unit of time. Momentum transfer and entrapment pumps are more effective on some gases than others, so the pumping rate can be different for each of the gases being pumped, and the average volume flow rate of the pump will vary depending on the chemical composition of the gases remaining in the chamber.\n\nThroughput refers to the pumping speed multiplied by the gas pressure at the inlet, and is measured in units of pressure·volume/unit time. At a constant temperature, throughput is proportional to the number of molecules being pumped per unit time, and therefore to the mass flow rate of the pump. When discussing a leak in the system or backstreaming through the pump, throughput refers to the volume leak rate multiplied by the pressure at the vacuum side of the leak, so the leak throughput can be compared to the pump throughput.\n\nPositive displacement and momentum transfer pumps have a constant volume flow rate (pumping speed), but as the chamber's pressure drops, this volume contains less and less mass. So although the pumping speed remains constant, the throughput and mass flow rate drop exponentially. Meanwhile, the leakage, evaporation, sublimation and backstreaming rates continue to produce a constant throughput into the system.\n\nVacuum pumps are combined with chambers and operational procedures into a wide variety of vacuum systems. Sometimes more than one pump will be used (in series or in parallel) in a single application. A partial vacuum, or rough vacuum, can be created using a positive displacement pump that transports a gas load from an inlet port to an outlet (exhaust) port. Because of their mechanical limitations, such pumps can only achieve a low vacuum. To achieve a higher vacuum, other techniques must then be used, typically in series (usually following an initial fast pump down with a positive displacement pump). Some examples might be use of an oil sealed rotary vane pump (the most common positive displacement pump) backing a diffusion pump, or a dry scroll pump backing a turbomolecular pump. There are other combinations depending on the level of vacuum being sought.\n\nAchieving high vacuum is difficult because all of the materials exposed to the vacuum must be carefully evaluated for their outgassing and vapor pressure properties. For example, oils, greases, and rubber or plastic gaskets used as seals for the vacuum chamber must not boil off when exposed to the vacuum, or the gases they produce would prevent the creation of the desired degree of vacuum. Often, all of the surfaces exposed to the vacuum must be baked at high temperature to drive off adsorbed gases.\n\nOutgassing can also be reduced simply by desiccation prior to vacuum pumping.\nHigh vacuum systems generally require metal chambers with metal gasket seals such as Klein flanges or ISO flanges, rather than the rubber gaskets more common in low vacuum chamber seals. The system must be clean and free of organic matter to minimize outgassing. All materials, solid or liquid, have a small vapour pressure, and their outgassing becomes important when the vacuum pressure falls below this vapour pressure. As a result, many materials that work well in low vacuums, such as epoxy, will become a source of outgassing at higher vacuums.\nWith these standard precautions, vacuums of 1 mPa are easily achieved with an assortment of molecular pumps. With careful design and operation, 1 µPa is possible.\n\nSeveral types of pumps may be used in sequence or in parallel. In a typical pumpdown sequence, a positive displacement pump would be used to remove most of the gas from a chamber, starting from atmosphere (760 Torr, 101 kPa) to 25 Torr (3 kPa). Then a sorption pump would be used to bring the pressure down to 10 Torr (10 mPa). A cryopump or turbomolecular pump would be used to bring the pressure further down to 10 Torr (1 µPa). An additional ion pump can be started below 10 Torr to remove gases which are not adequately handled by a cryopump or turbo pump, such as helium or hydrogen.\n\nUltra high vacuum generally requires custom-built equipment, strict operational procedures, and a fair amount of trial-and-error. Ultra-high vacuum systems are usually made of stainless steel with metal-gasketed vacuum flanges. The system is usually baked, preferably under vacuum, to temporarily raise the vapour pressure of all outgassing materials in the system and boil them off. If necessary, this outgassing of the system can also be performed at room temperature, but this takes much more time. Once the bulk of the outgassing materials are boiled off and evacuated, the system may be cooled to lower vapour pressures to minimize residual outgassing during actual operation. Some systems are cooled well below room temperature by liquid nitrogen to shut down residual outgassing and simultaneously cryopump the system.\n\nIn ultra-high vacuum systems, some very odd leakage paths and outgassing sources must be considered. The water absorption of aluminium and palladium becomes an unacceptable source of outgassing, and even the absorptivity of hard metals such as stainless steel or titanium must be considered. Some oils and greases will boil off in extreme vacuums. The porosity of the metallic vacuum chamber walls may have to be considered, and the grain direction of the metallic flanges should be parallel to the flange face.\n\nThe impact of molecular size must be considered. Smaller molecules can leak in more easily and are more easily absorbed by certain materials, and molecular pumps are less effective at pumping gases with lower molecular weights. A system may be able to evacuate nitrogen (the main component of air) to the desired vacuum, but the chamber could still be full of residual atmospheric hydrogen and helium. Vessels lined with a highly gas-permeable material such as palladium (which is a high-capacity hydrogen sponge) create special outgassing problems.\n\nVacuum pumps are used in many industrial and scientific processes including composite plastic moulding processes, production of most types of electric lamps, vacuum tubes, and CRTs where the device is either left evacuated or re-filled with a specific gas or gas mixture, semiconductor processing, notably ion implantation, dry etch and PVD, ALD, PECVD and CVD deposition and so on in photolithography, electron microscopy, medical processes that require suction, uranium enrichment, medical applications such as radiotherapy, radiosurgery and radiopharmacy, analytical instrumentation to analyse gas, liquid, solid, surface and bio materials, mass spectrometers to create a high vacuum between the ion source and the detector, vacuum coating on glass, metal and plastics for decoration, for durability and for energy saving, such as low-emissivity glass, hard coating for engine components (as in Formula One), ophthalmic coating, milking machines and other equipment in dairy sheds, vacuum impregnation of porous products such as wood or electric motor windings, air conditioning service (removing all contaminants from the system before charging with refrigerant), trash compactor, vacuum engineering, sewage systems (see EN1091:1997 standards), freeze drying, and fusion research. In the field of oil regeneration and rerefining, vacuum pumps create a low vacuum for oil dehydration and a high vacuum for oil purification. Especially in the field of transformer maintenance, vacuum pumps play an essential role in transformer oil purification plants which are used to extend the lifetime of transformers in the field.\n\nVacuum may be used to power, or provide assistance to mechanical devices. In hybrid and diesel engine motor vehicles, a pump fitted on the engine (usually on the camshaft) is used to produce vacuum. In petrol engines, instead, vacuum is typically obtained as a side-effect of the operation of the engine and the flow restriction created by the throttle plate, but may be also supplemented by an electrically operated vacuum pump to boost braking assistance or improve fuel consumption. This vacuum may then be used to power the following motor vehicle components: vacuum servo booster for the hydraulic brakes, motors that move dampers in the ventilation system, throttle driver in the cruise control servomechanism, door locks or trunk releases.\n\nIn an aircraft, the vacuum source is often used to power gyroscopes in the various flight instruments. To prevent the complete loss of instrumentation in the event of an electrical failure, the instrument panel is deliberately designed with certain instruments powered by electricity and other instruments powered by the vacuum source.\n\nOld vacuum-pump oils that were produced before circa 1980 often contain a mixture of several different dangerous polychlorinated biphenyls (PCBs), which are highly toxic, carcinogenic, persistent organic pollutants.\n\n"}
{"id": "469990", "url": "https://en.wikipedia.org/wiki?curid=469990", "title": "Whirlpool", "text": "Whirlpool\n\nA whirlpool is a body of rotating water produced by the meeting of opposing currents. The vast majority of whirlpools are not very powerful and very small whirlpools can be easily seen when a bath or a sink is draining. More powerful ones in seas or oceans may be termed maelstroms. \"Vortex\" is the proper term for any whirlpool that has a downdraft.\n\nIn oceans, in narrow straits with fast flowing water, whirlpools are normally caused by tides; there are few stories of large ships ever being sucked into such a maelstrom, although smaller craft are in danger. Smaller whirlpools also appear at the base of many waterfalls and can also be observed downstream from manmade structures such as weirs and dams. In the case of powerful waterfalls, like Niagara Falls, these whirlpools can be quite strong.\n\nThe Maelstrom of Saltstraumen is the Earth's strongest maelstrom, and is located close to the Arctic Circle, round the bay on the Highway 17, south-east of the city of Bodø, Norway. The strait at its narrowest is in width and water \"funnels\" through the channel four times a day. It is estimated that of water passes the narrow strait during this event. The water is creamy in colour and most turbulent during high tide, which is witnessed by thousands of tourists. It reaches speeds of , with mean speed of about . As navigation is dangerous in this strait only a small slot of time is available for large ships to pass through. Its impressive strength is caused by the world's strongest tide occurring in the same location during the new and full moon. A narrow channel of length connects the outer Saltfjord with its extension, the large Skjerstadfjord, causing a colossal tide which in turn produces the Saltstraumen maelstrom.\n\nMoskstraumen is an unusual system of whirlpools in the open seas in the Lofoten Islands off the Norwegian coast. It is the second strongest whirlpool in the world with flow currents reaching speeds as high as . It finds mention in several books and movies.\n\nThe Moskstraumen is formed by the combination of powerful semi-diurnal tides and the unusual shape of the seabed, with a shallow ridge between the Moskenesøya and Værøy islands which amplifies and whirls the tidal currents.\n\nThe fictional depictions of the Maelstrom by Edgar Allan Poe, Jules Verne, and Cixin Liu describe it as a gigantic circular vortex that reaches the bottom of the ocean, when in fact it is a set of currents and crosscurrents with a rate of . Poe described this phenomenon in his short story \"A Descent into the Maelstrom,\" which in 1841 was the first to use the word \"maelstrom\" in the English language; in this story related to the Lofoten Maelstrom, two fishermen are swallowed by the maelstrom while one survives miraculously.\n\nThe Corryvreckan is a narrow strait between the islands of Jura and Scarba, in Argyll and Bute, on the northern side of the Gulf of Corryvreckan, Scotland. It is the third-largest whirlpool in the world. Flood tides and inflow from the Firth of Lorne to the west can drive the waters of Corryvreckan to waves of over , and the roar of the resulting maelstrom, which reaches speeds of , can be heard away. Though it was initially classified as non-navigable by the British navy it was later categorized as \"extremely dangerous\".\n\nA documentary team from Scottish independent producers Northlight Productions once threw a mannequin into the Corryvreckan (\"the Hag\") with a life jacket and depth gauge. The mannequin was swallowed and spat up far down current with a depth gauge reading of with evidence of being dragged along the bottom for a great distance.\n\nOld Sow whirlpool is located between Deer Island, New Brunswick, Canada, and Moose Island, Eastport, Maine, USA. It is given the epithet \"pig-like\" as it makes a screeching noise when the vortex is at its full fury and reaches speeds of up to . The smaller whirlpools around this Old Sow are known as \"Piglets.\n\nThe Naruto whirlpools are located in the Naruto Strait near Awaji Island in Japan, which have speeds of .\n\nSkookumchuck Narrows is a tidal rapids that develops whirlpools, on the Sunshine Coast, Canada with current speeds exceeding .\n\nFrench Pass () is a narrow and treacherous stretch of water that separates D'Urville Island from the north end of the South Island of New Zealand. In 2000 a whirlpool there caught student divers, resulting in fatalities.\n\nThere was a short-lived whirlpool that sucked in a portion of the 1300 acre (~530 hectares) Lake Peigneur in Louisiana, United States after a drilling mishap in November 1980. This was not a naturally occurring whirlpool, but a man-made disaster caused by underwater drillers breaking through the roof of a salt mine. The lake then drained into the mine until the mine filled and the water levels equalized but the ten-foot deep lake was now 1,300 feet deep. This mishap resulted in destruction of five houses, loss of nineteen barges and eight tug boats, oil rigs, a mobile home, and most of a botanical garden. The adjacent settlement of Jefferson Island was reduced in area by 10%. A crater 0.5-mile (~1km) across was left behind. Nine of the barges which had sunk floated back.\n\nA more recent example of a man-made whirlpool that received significant media coverage was in early June 2015, when an intake vortex formed in Lake Texoma, on the Oklahoma–Texas border, near the floodgates of the dam that forms the lake. At the time of the whirlpool's formation, the lake was being drained after reaching its highest level ever. The Army Corps of Engineers, which operates the dam and lake, expected that the whirlpool would last until the lake reached normal seasonal levels by late July.\n\nPowerful whirlpools have killed unlucky seafarers, but their power tends to be exaggerated by laymen. There are virtually no stories of large ships ever being sucked into a whirlpool. Tales like those by Paul the Deacon, Edgar Allan Poe, and Jules Verne are entirely fictional.\n\nHowever, temporary whirlpools caused by major engineering disasters are capable of submerging large ships. A prominent example is the drilling disaster that occurred on November 20, 1980, in Lake Peigneur. A drilling platform, eleven barges, several trees, and multiple acres of the surrounding terrain were submerged by the resulting whirlpool. Days after the disaster, once the water pressure equalized, nine of the eleven sunken barges popped out of the whirlpool and refloated on the lake's surface.\n\nApart from Poe and Verne other literary source is of the 1500s, of Olaus Magnus, a Swedish Bishop, who had stated that the maelstrom which was more powerful than \"The Odyssey\" destroyed ships which sank to the bottom of the sea, and even whales were sucked in. Pytheas, the Greek historian, also mentioned that maelstroms swallowed ships and threw them up again.\n\nCharybdis in Greek mythology was later rationalized as a whirlpool, which sucked entire ships into its fold in the narrow coast of Sicily, a disaster faced by navigators.\n\nIn the 8th century, Paul the Deacon, who had lived among the Belgii, described tidal bores and the maelstrom for a Mediterranean audience unused to such violent tidal surges:\n\nThree of the most notable literary references to the Lofoten Maelstrom date from the nineteenth century. The first is the Edgar Allan Poe short story \"A Descent into the Maelström\" (1841). The second is \"20,000 Leagues Under the Sea\" (1870), the famous novel by Jules Verne. At the end of this novel, Captain Nemo seems to commit suicide, sending his \"Nautilus\" submarine into the Maelstrom (although in Verne's sequel Nemo and the Nautilus were seen to have survived). The \"Norway maelstrom\" is also mentioned in Herman Melville's \"Moby-Dick\".\n\nIn the 'Life of St Columba', the author, Adomnan of Iona', attributes to the saint miraculous knowledge of a particular bishop who ran into a whirlpool off the coast of Ireland. In Adomnan's narrative, he quotes Columba saying\n\nOne of the earliest uses in English of the Scandinavian word (\"malström\" or \"malstrøm\") was by Edgar Allan Poe in his short story \"A Descent into the Maelström\" (1841). In turn, the Nordic word is derived from the Dutch \"maelstrom\", modern spelling \"maalstroom\", from \"malen\" (\"to grind\") and \"stroom\" (\"stream\"), to form the meaning \"grinding current\" or literally \"mill-stream\", in the sense of milling (grinding) grain.\n\n\n\n"}
{"id": "1945275", "url": "https://en.wikipedia.org/wiki?curid=1945275", "title": "Wigner effect", "text": "Wigner effect\n\nThe Wigner effect (named for its discoverer, Eugene Wigner), also known as the discomposition effect or Wigner's Disease, is the dislocation of atoms in a solid caused by neutron radiation. \n\nAny solid can display the Wigner effect. The effect is of most concern in neutron moderators, such as graphite, intended to reduce the speed of fast neutrons, thereby turning them into thermal neutrons capable of sustaining a nuclear chain reaction involving uranium-235.\n\nTo create the Wigner effect, neutrons that collide with the atoms in a crystal structure must have enough energy to displace them from the lattice. This amount (threshold displacement energy) is approximately 25 eV. A neutron's energy can vary widely, but it is not uncommon to have energies up to and exceeding 10 MeV (10,000,000 eV) in the centre of a nuclear reactor. A neutron with a significant amount of energy will create a displacement cascade in a matrix via elastic collisions. For example, a 1 MeV neutron striking graphite will create 900 displacements; not all displacements will create defects, because some of the struck atoms will find and fill the vacancies that were either small pre-existing voids or vacancies newly formed by the other struck atoms.\n\nThe atoms that do not find a vacancy come to rest in non-ideal locations; that is, not along the symmetrical lines of the lattice. These atoms are referred to as interstitial atoms, or simply interstitials. An interstitial atom and its associated vacancy are known as a Frenkel defect. Because these atoms are not in the ideal location, they have an energy associated with them, much as a ball at the top of a hill has gravitational potential energy. This energy is referred to as Wigner energy. When a large number of interstitials have accumulated, they pose a risk of releasing all of their energy suddenly, creating a rapid, very great increase in temperature. Sudden, unplanned increases in temperature can present a large risk for certain types of nuclear reactors with low operating temperatures; one such was the indirect cause of the Windscale fire. Accumulation of energy in irradiated graphite has been recorded as high as 2.7 kJ/g, but is typically much lower than this. Graphite, having a heat capacity of 0.720 J/g°C, could see a sudden increase in temperature of about 3750 °C (6780 °F).\n\nDespite some reports, Wigner energy buildup had nothing to do with the cause of the Chernobyl disaster: this reactor, like all contemporary power reactors, operated at a high enough temperature to allow the displaced graphite structure to realign itself before any potential energy could be stored. Wigner energy may have played some part following the prompt critical neutron spike, when the accident entered the graphite fire phase of events.\n\nA buildup of Wigner energy can be relieved by heating the material. This process is known as annealing. In graphite this occurs at 250 °C.\n\nIn 2003, it was postulated that Wigner energy can be stored by the formation of metastable defect structures in graphite. Notably, the large energy release observed at 200–250 °C has been described in terms of a metastable interstitial-vacancy pair. The interstitial atom becomes trapped on the lip of the vacancy, and there is a barrier for it to recombine to give perfect graphite.\n\n"}
