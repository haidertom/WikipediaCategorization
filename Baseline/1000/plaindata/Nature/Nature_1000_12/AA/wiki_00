{"id": "71318", "url": "https://en.wikipedia.org/wiki?curid=71318", "title": "100-year flood", "text": "100-year flood\n\nA one-hundred-year flood is a flood event that has a 1% probability of occurring in any given year.\n\nThe 100-year flood is also referred to as the 1% flood, since its annual exceedance probability is 1%. For river systems, the 100-year flood is generally expressed as a flowrate. Based on the expected 100-year flood flow rate, the flood water level can be mapped as an area of inundation. The resulting floodplain map is referred to as the 100-year floodplain. Estimates of the 100-year flood flowrate and other streamflow statistics for any stream in the United States are available. In the UK The Environment Agency publishes a comprehensive map of all areas at risk of a 1 in 100 year flood. Areas near the coast of an ocean or large lake also can be flooded by combinations of tide, storm surge, and waves. Maps of the riverine or coastal 100-year floodplain may figure importantly in building permits, environmental regulations, and flood insurance.\n\nA common misunderstanding is that a 100-year flood is likely to occur only once in a 100-year period. In fact, there is approximately a 63.4% chance of one or more 100-year floods occurring in any 100-year period. On the Danube River at Passau, Germany, the actual intervals between 100-year floods during 1501 to 2013 ranged from 37 to 192 years. The probability P that one or more floods occurring during any period will exceed a given flood threshold can be expressed, using the binomial distribution, as\n\nformula_1\n\nwhere T is the threshold return period (e.g. 100-yr, 50-yr, 25-yr, and so forth), and n is the number of years in the period. The probability of exceedance P is also described as the natural, inherent, or hydrologic risk of failure. However, the expected value of the number of 100-year floods occurring in any 100-year period is 1.\n\nTen-year floods have a 10% chance of occurring in any given year (P =0.10); 500-year have a 0.2% chance of occurring in any given year (P =0.002); etc. The percent chance of an X-year flood occurring in a single year is 100/X. A similar analysis is commonly applied to coastal flooding or rainfall data. The recurrence interval of a storm is rarely identical to that of an associated riverine flood, because of rainfall timing and location variations among different drainage basins.\n\nThe field of extreme value theory was created to model rare events such as 100-year floods for the purposes of civil engineering. This theory is most commonly applied to the maximum or minimum observed stream flows of a given river. In desert areas where there are only ephemeral washes, this method is applied to the maximum observed rainfall over a given period of time (24-hours, 6-hours, or 3-hours). The extreme value analysis only considers the most extreme event observed in a given year. So, between the large spring runoff and a heavy summer rain storm, whichever resulted in more runoff would be considered the extreme event, while the smaller event would be ignored in the analysis (even though both may have been capable of causing terrible flooding in their own right).\n\nThere are a number of assumptions which are made to complete the analysis which determines the 100-year flood. First, the extreme events observed in each year must be independent from year-to-year. In other words, the maximum river flow rate from 1984 cannot be found to be significantly correlated with the observed flow rate in 1985. 1985 cannot be correlated with 1986, and so forth. The second assumption is that the observed extreme events must come from the same probability distribution function. The third assumption is that the probability distribution relates to the largest storm (rainfall or river flow rate measurement) that occurs in any one year. The fourth assumption is that the probability distribution function is stationary, meaning that the mean (average), standard deviation and max/min values are not increasing or decreasing over time. This concept is referred to as stationarity.\n\nThe first assumption is often but not always valid and should be tested on a case by case basis. The second assumption is often valid if the extreme events are observed under similar climate conditions. For example, if the extreme events on record all come from late summer thunder storms (as is the case in the southwest U.S.), or from snow pack melting (as is the case in north-central U.S.), then this assumption should be valid. If, however, there are some extreme events taken from thunder storms, others from snow pack melting, and others from hurricanes, then this assumption is most likely not valid. The third assumption is only a problem when trying to forecast a low, but maximum flow event (for example, an event smaller than a 2-year flood). Since this is not typically a goal in extreme analysis, or in civil engineering design, then the situation rarely presents itself. The final assumption about stationarity is difficult to test from data for a single site because of the large uncertainties in even the longest flood records (see next section). More broadly, substantial evidence of climate change strongly suggests that the probability distribution is also changing and that managing flood risks in the future will become even more difficult. The simplest implication of this is that not all of the historical data are, or can be, considered valid as input into the extreme event analysis.\n\nWhen these assumptions are violated there is an \"unknown\" amount of uncertainty introduced into the reported value of what the 100-year flood means in terms of rainfall intensity, or flood depth. When all of the inputs are known the uncertainty can be measured in the form of a confidence interval. For example, one might say there is a 95% chance that the 100-year flood is greater than X, but less than Y.\n\nDirect statistical analysis to estimate the 100-year riverine flood is possible only at the relatively few locations where an annual series of maximum instantaneous flood discharges has been recorded. In the United States as of 2014, taxpayers have supported such records for at least 60 years at fewer than 2,600 locations, for at least 90 years at fewer than 500, and for at least 120 years at only 11. For comparison, the total area of the nation is about , so there are perhaps 3,000 stream reaches that drain watersheds of and 300,000 reaches that drain . In urban areas, 100-year flood estimates are needed for watersheds as small as . For reaches without sufficient data for direct analysis, 100-year flood estimates are derived from indirect statistical analysis of flood records at other locations in a hydrologically similar region or from other hydrologic models. Similarly for coastal floods, tide gauge data exist for only about 1,450 sites worldwide, of which only about 950 added information to the global data center between January 2010 and March 2016.\nMuch longer records of flood elevations exist at a few locations around the world, such as the Danube River at Passau, Germany, but they must be evaluated carefully for accuracy and completeness before any statistical interpretation.\n\nFor an individual stream reach, the uncertainties in any analysis can be large, so 100-year flood estimates have large individual uncertainties for most stream reaches. For the largest recorded flood at any specific location, or any potentially larger event, the recurrence interval always is poorly known. Spatial variability adds more uncertainty, because a flood peak observed at different locations on the same stream during the same event commonly represents a different recurrence interval at each location. If an extreme storm drops enough rain on one branch of a river to cause a 100-year flood, but no rain falls over another branch, the flood wave downstream from their junction might have a recurrence interval of only 10 years. Conversely, a storm that produces a 25-year flood simultaneously in each branch might form a 100-year flood downstream. During a time of flooding, news accounts necessarily simplify the story by reporting the greatest damage and largest recurrence interval estimated at any location. The public can easily and incorrectly conclude that the recurrence interval applies to all stream reaches in the flood area.\n\nPeak elevations of 14 floods as early as 1501 on the Danube River at Passau, Germany, reveal great variability in the actual intervals between floods. Flood events greater than the 50-year flood occurred at intervals of 4 to 192 years since 1501, and the 50-year flood of 2002 was followed only 11 years later by a 500-year flood. Only half of the intervals between 50- and 100-year floods were within 50 percent of the nominal average interval. Similarly, the intervals between 5-year floods during 1955 to 2007 ranged from 5 months to 16 years, and only half were within 2.5 to 7.5 years.\nIn the United States, the 100-year flood provides the risk basis for flood insurance rates. Complete information on the National Flood Insurance Program (NFIP) is available here. A \"regulatory flood\" or \"base flood\" is routinely established for river reaches through a science-based rule-making process targeted to a 100-year flood at the historical average recurrence interval. In addition to historical flood data, the process accounts for previously established regulatory values, the effects of flood-control reservoirs, and changes in land use in the watershed. Coastal flood hazards have been mapped by a similar approach that includes the relevant physical processes. Most areas where serious floods can occur in the United States have been mapped consistently in this manner. On average nationwide, those 100-year flood estimates are well sufficient for the purposes of the NFIP and offer reasonable estimates of future flood risk, if the future is like the past. Approximately 3% of the U.S. population lives in areas subject to the 1% annual chance coastal flood hazard.\n\nIn theory, removing homes and businesses from areas that flood repeatedly can protect people and reduce insurance losses, but in practice it is difficult for people to retreat from established neighborhoods.\n\n\n"}
{"id": "4327451", "url": "https://en.wikipedia.org/wiki?curid=4327451", "title": "Adolf Dygasiński", "text": "Adolf Dygasiński\n\nAdolf Dygasiński (March 7, 1839, Niegosławice–June 3, 1902, Grodzisk Mazowiecki) was a Polish novelist, publicist and educator. In Polish literature, he was one of the leading representatives of Naturalism. \nDuring his literary career, Dygasiński wrote forty-two short stories and novels. \nSince 1884 his works were being published in book-form and enjoyed considerable success. \nThey were translated into Russian and German. \nIn 1891, Dygasiński went on a trip to Brazil on a trail of Polish emigrants from Partitioned Poland. \nHe produced a series of letters describing the tragic fate of Polish emigrees in South America. In the following years Dygasiński maintained a position of a tutor and coach for numerous wealthy landowning families. Late in life he settled in Warsaw, where he died on June 6, 1902, and was buried at the local Powązkowski Cemetery.\n\nIn his work Dygasiński often focused on topics of rural life and residents of small towns, highlighting the common fate of both, human and animal communities. Some of his most important work include: \n\n\n\n"}
{"id": "2845506", "url": "https://en.wikipedia.org/wiki?curid=2845506", "title": "Bast fibre", "text": "Bast fibre\n\nBast fibre (also called phloem fibre or skin fibre) is plant fibre collected from the phloem (the \"inner bark\", sometimes called \"skin\") or bast surrounding the stem of certain dicotyledonous plants. They support the conductive cells of the phloem and provide strength to the stem. Some of the economically important bast fibres are obtained from herbs cultivated in agriculture, as for instance flax, hemp, or ramie, but also bast fibres from wild plants, as stinging nettle, and trees such as lime or linden, wisteria, and mulberry have been used in the past. Bast fibres are classified as soft fibres, and are flexible. Fibres from monocotyledonous plants, called \"leaf fibre\", are classified as hard fibres and are stiff.\n\nSince the valuable fibres are located in the phloem, they must often be separated from the xylem material (\"woody core\"), and sometimes also from the epidermis. The process for this is called retting, and can be performed by micro-organisms either on land (nowadays the most important) or in water, or by chemicals (for instance high pH and chelating agents) or by pectinolytic enzymes. In the phloem, bast fibres occur in bundles that are glued together by pectin and calcium ions. More intense retting separates the fibre bundles into elementary fibres, that can be several centimetres long. Often bast fibres have higher tensile strength than other kinds, and are used in high-quality textiles (sometimes in blends with cotton or synthetic fibres), ropes, yarn, paper, composite materials and burlap. An important property of bast fibres is that they contain a special structure, the \"fibre node\", that represents a weak point, and gives flexibility. Seed hairs, such as cotton, do not have nodes.\n\nPlants that have been used for bast fibre include flax (from which linen is made), hemp, jute, kenaf, kudzu, linden, milkweed, nettle, okra, paper mulberry, ramie, and roselle hemp.\n\nBast fibres are processed for use in carpet, yarn, rope, geotextile (netting or matting), traditional carpets, hessian or burlap, paper, sacks, etc. Bast fibres are also used in the non-woven, moulding, and composite technology industries for the manufacturing of non-woven mats and carpets, composite boards as furniture materials, automobile door panels and headliners, etc. From prehistoric times through at least the early twentieth century, bast shoes were woven from bast strips in the forest areas of Eastern Europe.\n\nWhere no other source of tanbark was available, bast has also been used for tanning leather.\n\n"}
{"id": "37970127", "url": "https://en.wikipedia.org/wiki?curid=37970127", "title": "Buttered toast phenomenon", "text": "Buttered toast phenomenon\n\nThe buttered toast phenomenon is an observation that buttered toast tends to land butter-side down after it falls. It is used an idiom representing pessimistic outlooks. Various people have attempted to determine whether there is an actual tendency for bread to fall in this fashion, with varying results.\n\nThe phenomenon is said to be an old proverb from \"the north country.\" Written accounts can be traced to the mid-19th century. The phenomenon is often attributed to a parodic poem of James Payn from 1884:\n\nIn the past, this has often been considered just a pessimistic belief. A study by the BBC's television series \"Q.E.D.\" found that when toast is thrown in the air, it lands butter-side down just one-half of the time (as would be predicted by chance). However, several scientific studies have found that when toast is dropped from a table (as opposed to being thrown in the air), it does fall butter-side down. A study by Robert A J Matthews won the Ig Nobel Prize in 1996.\n\nWhen toast falls out of one's hand, it does so at an angle, by nature of it having slipped from its previous position, then the toast rotates. Given that tables are usually between two and six feet (0.7 to 1.83 meters), there is enough time for the toast to rotate about one-half of a turn, and thus lands upside down relative to its original position. Since the original position is usually butter-side up, the toast lands butter-side down. However, if the table is over 10 feet (3 meters) tall, the toast will rotate a full 360 degrees, and land butter-side up. Also, if the toast travels horizontally at over 3.6 miles per hour (1.6 m/s), the toast will not rotate enough to land butter-side down. In fact, the phenomenon is caused by fundamental physical constants.\n\nThe added weight of the butter has no effect on the falling process, since the butter spreads throughout the slice.\n\nThe following findings are from Mythbusters:\n\nIn the 2010 M. Night Shyamalan film \"Devil\", a group of adults in an elevator become trapped by unknown forces. Slowly over the course of the film, people begin to die in the elevator as the power blinks on and off, pinning the people against each other and creating a false narrative that the others are murderers (However, it is common opinion that the name of the film spoiled this false narrative). In the film, supporting cast member and fictional security guard Ramirez, in his suspicion that there may be unholy forces at play, dropped a slice of toast with jelly on one side, and dropped it on the floor. The toast landed on the jelly-side, after which Ramirez coined the phrase \"Jelly Side Down,\" and subsequently 'proved' that the devil was nearby. However, there is some dispute in the film about whether or not this did actually prove the devil was near.\n\nThis phenomenon was also demonstrated and parodied in the October 29, 2013 \"Nostalgia Critic\" review of \"Devil\", (Reuploaded on July 23, 2016 by Channel Awesome on YouTube) in which actor Doug Walker plays a priest who uses multiple food items to determine whether or not the Devil is near. After the other items show no sign of the Devil, the priest tosses a piece of toast as the last test. The toast lands jelly side down which means the Devil is near and the end of the world is imminent. Everybody in the church screams and panics as the priest repeatedly shouts, \"Jelly side down!\"\n\n"}
{"id": "34688121", "url": "https://en.wikipedia.org/wiki?curid=34688121", "title": "Coastal hazards", "text": "Coastal hazards\n\nCoastal Hazards are physical phenomena that expose a coastal area to risk of property damage, loss of life and environmental degradation. Rapid-onset hazards last over periods of minutes to several days and examples include major cyclones accompanied by high winds, waves and surges or tsunamis created by submarine earthquakes and landslides. Slow-onset hazards develop incrementally over longer time periods and examples include erosion and gradual inundation.\n\nSince early civilisation, coastal areas have been attractive settling grounds for human population as they provided abundant marine resources, fertile agricultural land and possibilities for trade and transport. This has led to high population densities and high levels of development in many coastal areas and this trend is continuing into the 21st century. At present, about 1,2 billion people live in coastal areas globally, and this number is predicted to increase to 1,8–5,2 billion by the 2080s due to a combination of population growth and coastal migration. Along with this increase follows major investments in infrastructure and the build environment.\n\nThe characteristics of coastal environments, however, pose some great challenges to human habitation. Coastlines are highly dynamic natural systems that interact with terrestrial, marine and atmospheric processes and undergo continuous change in response to these processes. Over the years, human society has often failed to recognize the hazards related to these dynamics and this has led to major disasters and societal disruption to various degrees. Even today, coastal development is often taking place with little regard to the hazards present in these environments, although climate change is likely to increase the general hazard levels. Societal activities in coastal areas can also pose a hazard to the natural balance of coastal systems, thereby disrupting e.g. sensitive ecosystems and subsequently human livelihood.\n\nCoastal hazard management has become an increasingly important aspect of coastal planning in order to improve the resilience of society to coastal hazards. Possible management options include hard engineering structures, soft protection measures, various accommodation approaches as well as a managed retreat from the coastline. For addressing coastal hazards, it is also important to have early warning systems and emergency management plans in place to be able to address sudden and potential disastrous hazards i.e. major flooding events. Events as the Hurricane Katrina affecting the southern USA in 2005 and the cyclone Nargis affecting Myanmar in 2008 provides clear examples of the importance of timely coastal hazard management.\n\nThere are many different types of environments along the coasts of the United States with very diverse features that affect, influence, and mold the near-shore processes that are involved. Understanding these ecosystems and environments can further advance the mitigating techniques and policy-making efforts against natural and man-made coastal hazards in these vulnerable areas. The five most common types of coastal zones range from the northern ice-pushing, mountainous coastline of Alaska and Maine, the barrier island coasts facing the Atlantic, the steep, cliff-back headlands along the pacific coast, the marginal-sea type coastline of the Gulf region, and the coral reef coasts bordering Southern Florida and Hawaii.\n\nIce-pushing/mountainous coastline\n\nThese coastal regions along the northernmost part of the nation were affected predominantly by, along with the rest of the Pacific Coast, continuous tectonic activity, forming a very long, irregular, ridged, steep and mostly mountainous coastline. These environments are heavily occupied with permafrost and glaciers, which are the two major conditions affecting Alaska's Coastal Development.\n\nBarrier island coastline\n\nBarrier islands are a land form system that consists of fairly narrow strips of sand running parallel to the mainland and play a significant role in mitigating storm surges and oceans swells as natural storm events occur. The morphology of the various types and sizes of barrier islands depend on the wave energy, tidal range, basement controls, and sea level trends. The islands create multiple unique environments of wetland systems including marshes, estuaries, and lagoons.\n\nSteep, cliff-backing abrasion coastline\n\nThe coastline along the western part of the nation consists of very steep, cliffed rock formations generally with vegetative slopes descending down and a fringing beach below. The various sedimentary, metamorphic, and volcanic rock formations assembled along a tectonically disturbed environment, all with altering resistances running perpendicular, cause the ridged, extensive stretch of uplifted cliffs that form the peninsulas, lagoons, and valleys.\n\nMarginal-sea type coastline\n\nThe southern banks of the United States border the Gulf of Mexico, intersecting numerous rivers, forming many inlets bays, and lagoons along its coast, consisting of vast areas of marsh and wetlands. This region of landform is prone to natural disasters yet highly and continuously developed, with man-made structures attaining to water flow and control.\n\nCoral reef coastline\n\nCoral reefs are located off the shores of the southern Florida and Hawaii consisting of rough and complex natural structures along the bottom of the ocean floor with extremely diverse ecosystems, absorbing up to ninety percent of the energy dissipated from wind-generated waves. This process is a significant buffer for the inner-lying coastlines, naturally protecting and minimizing the impact of storm surge and direct wave damage. Because of the highly diverse ecosystems, these coral reefs not only provide for the shoreline protection, but also deliver an abundant amount of services to fisheries and tourism, increasing its economic value.\n\nNatural VS Human disasters\n\nThe population that lives along or near our coastlines are an extremely vulnerable population. There are numerous issues facing our coastlines and there are two main categories that these hazards can be placed under, Natural disasters and Human disasters. Both of these issues cause great damage to our coastlines and discussion is still ongoing regarding what standards or responses need to be met to help both the individuals who want to continue living along the coastline, while keeping them safe and not eroding more coastline away. Natural disasters are disasters that are out of human control and are usually caused by the weather. Disasters that include but are not limited to; storms, tsunamis, typhoons, flooding, tides, waterspouts, nor'easters, and storm surge. Human disasters occur when humans are the main culprit behind why the disaster happened. Some human disasters are but are not limited to; pollution, trawling, and human development. Natural and human disasters continue to harm the coastlines severely and they need to be researched in order to prepare/stop the hazards if possible.\n\nThe populations that live near or along the coast experience many hazards and it affects millions of people. Around ten million people globally feel the effects of coastal problems yearly and most are due to certain natural hazards like coastal flooding with storm surges and typhoons. A major problem related to coastal regions deals with how the entire global environment is changing and in response, the coastal regions are easily affected.\n\nStorms, Flooding and Erosion\n\nStorms are one of the major hazards that are associated to coastal regions. Storms, flooding, and erosion are closely associated and can happen simultaneously. Tropical storms or Hurricanes especially can devastate coastal regions. For example, Florida during Hurricane Andrew occurred in 1992 that caused extreme damage. It was a category five hurricane that caused $26.5 billion in damages and even 23 individuals lost their lives from the storm. Hurricane Katrina also caused havoc along the coast to show the extreme force a hurricane can do in a certain region. The Chennai Floods of 2015, which affected many people, is an example of flooding due to cyclones. People across the whole state of Tamil Nadu felt its impact and even parts of Andhra Pradesh got affected. There was a loss of Rs.900 crore and 280 people died. Many cyclones like this happen across Asia but the media reports only minor hurricanes which hit the United States.\n\nAlmost all storms with high wind and water cause erosion along the coast. Erosion occurs when but not limited to; along shore currents, tides, sea level rise and fall, and high winds. Larger amounts of erosion cause the coastline to erode away at a faster rate and can leave people homeless and leave less land to develop or keep for environmental reasons. Coastal erosion has been increasing over the past few years and it is still on the rise which makes it a major coastline hazard. In the United States, 45 percent of its coast line is along the Atlantic or Gulf coast and the erosion rate per year along the Gulf coast is at six feet a year. The average rate of erosion along the Atlantic is around two to three feet a year. Even with these findings, erosion rates in specific locations vary because of various environmental factors such as major storms that can cause major erosion upwards to 100 feet or more in only one day.\n\nPollution, Trawling and Human Development\n\nPollution, trawling, and human development are major human disasters that affect coastal regions. There are two main categories related to pollution, point source pollution, and nonpoint source pollution. Point source pollution is when there is an exact location such as a pipeline or a body of water that leads into the rivers and oceans. Known dumping into the ocean is also another point source of pollution. Nonpoint source pollution would pertain more to fertilizer runoff, and industrial waste. Examples of pollution that affect the coastal regions are but are not limited to; fertilizer runoff, oil spills, and dumping of hazardous materials into the oceans. More human acts that hurt the coastline are as follows; waste discharge, fishing, dredging, mining, and drilling. Oil spills are one of the most hazardous dangers towards coastal communities. They are hard to contain, difficult to clean up, and devastate everything. The fish, animals such as birds, the water, and especially the coastline near the spill. The most recent oil spill that had everybody concerned with oil spill was the BP oil spill.\n\nTrawling hurts the normal ecosystems in the water around the coastline. It depletes all ecosystems on the ocean floor such as, flounder, shellfish, marsh etc.. It is simply a giant net that is drug across the ocean floor and destroys and catches anything in its path. Human development is one of the major problems when facing coastal hazards. The overall construction of buildings and houses on the coast line takes away the natural occurrences to handle the fluctuation in water and sea level rise. Building houses in pre-flood areas or high risk areas that are extremely vulnerable to flooding are major concerns towards human development in coastal regions. Having houses and buildings in areas that are known to have powerful storms that will create people to be in risk by living there. Also pertaining to barrier islands, where land is at risk for erosion but they still continue to build there anyway. More and more houses today are being taken by the ocean; look at picture above.\n\nCoastal hazards & climate change\n\nThe predicted climate change is adding an extra risk factor to human settlement in coastal areas. Whereas the natural dynamics that shape our coastlines have been relatively stable and predictable over the last centuries, much more rapid change is now expected in processes as sea level rise, ocean temperature and acidity, tropical storm intensity and precipitation/runoff patterns. The world's coastlines will respond to these changes in different ways and at different pace depending on their bio-geophysical characteristics, but generally society will have to recognize that past coastal trends cannot be directly projected into the future. Instead, it is necessary to consider how different coastal environments will respond to the predicted climate change and take the expected future hazards into account in the coastal planning processes.\n\nNational Flood Insurance Program\n\nThe National Flood Insurance Program or NFIP was instituted in 1968 and offers home owners in qualifying communities an opportunity to rebuild and recover after flooding events following the decision by insurance companies to discontinue providing flood insurance. This decision was made on behalf of the private insurers after continually high and widespread flood losses. The goals of this program are to not only better protect individuals from flood, but to reduce property losses, and reduce the total amount disbursed for flood loses by the government. Only communities which have adopted and implemented mitigation policies that are compliant with or exceed federal regulations. The regulatory policies reduce risk to life and property located within floodplains. The NFIP also comprehensively mapped domestic floodplains increasing public awareness of risk. The majority of structures were constructed after the mapping was completed and risk could be assessed. To reduce the cost to these owners, which constitute roughly 25% of the total policies the rates for insurance are subsidized.\n\nCoastal States Organization\n\nThe Coastal States Organization or COS was established in 1970 to represent 35 U.S. sub-federal governments on issues of coastal policies. CSO lobbies Congress on issues pertaining to Coastal Policy allowing states input on federal policy decisions. Funding, support, water quality, coastal hazards, and coastal zone management are the primary issues COS promotes. The strategic goals of COS are to provide information and assistance to members,evaluate and manage coastal needs, and secure long term funding for member states initiatives.\n\nCoastal Zone Management Act\n\nIn 1972 the Coastal Zone Management Act or CZMA works to streamline the policies which states create to a minimum federal standard for environmental protection. CZMA establishes the national policy for the development and implementation of regulatory programs for coastal land usage, which is supposed to be reflected in state legislation such as CAMA. CZMA also provides minimum building requirements to make the insurance provided through the NFIP less expensive for the government to operate by mitigating losses. Congress found that it was necessary to establish the minimum which programs should provide for. Each coastal state is required to have a program with 7 distinct parts: Identifying land uses,Identifying critical coastal areas, Management measures,Technical assistance, Public participation, Administrative coordination, State coastal zone boundary modification.\n\nThe Coastal Area Management Act\n\nThe Coastal Area Management Act or CAMA is policy that was implemented by the state of North Carolina in 1974 to work in-tandem with the CZMA. It creates a cooperative program between the state and local governments. The State government operates in an advisory capacity and reviews decisions made by local government planners. The goal of this legislation was to create a management system capable of preserving the coastal environment, insure the preservation of land and water resources, balance the use of coastal resources and establish guidelines and standards for conservations, economic development, tourism, transportation, and the protection of common law.\n\nDue to the increasing urbanization along the coastlines, planning and management are essential to protecting the ecosystems and environment from depleting. Coastal management is becoming implemented more because of the movement of people to the shore and the hazards that come with the territory. Some of the hazards include movement of barrier islands, sea level rise, hurricanes, nor'easters, earthquakes, flooding, erosion, pollution and human development along the coast. The Coastal Zone Management Act (CZMA) was created in 1972 because of the continued growth along the coast, this act introduced better management practices such as integrated coastal zone management, adaptive management and the use mitigation strategies when planning. According to the Coastal Zone Management Act, the objectives are to remain balanced to \"preserve, protect, develop, and where possible, to restore or enhance the resources of the nation's coastal zone\".\nThe development of the land can strongly affect the sea, for example the engineering of structures versus non-structures and the effects of erosion along the shore.\n\nIntegrated coastal zone management\n\nIntegrated coastal zone management means the integration of all aspects of the coastal zone; this includes environmentally, socially, culturally politically and economically to meet a sustainable balance all around. Sustainability is the goal to allow development yet protect the environment in which we develop. Coastal zones are fragile and do not do well with change so it is important to acquire sustainable development. The integration from all views will entitle a holistic view for the best implementation and management of that country, region and local scales. The five types of integration include integration among sectors, integration between land and water elements of the coastal zone, integration amount levels of government, integration between nations and integration among disciplines are all essential to meet the needs for implementation.\nManagement practices include\nThese four management practices should be based on a bottom-up approach, meaning the approach starts from a local level which is more intimate to the specific environment of that area. After assessment from the local level, the state and federal input can be implemented. The bottom-up approach is key for protecting the local environments because there is a diversity of environments that have specific needs all over the world.\n\nAdaptive management\n\nAdaptive management is another practice of development adaptation with the environment. Resources are the major factor when managing adaptively to a certain environment to accommodate all the needs of development and ecosystems. Strategies used must be flexible by either passive or active adaptive management include these key features:\nTo achieve adaptive management is testing the assumptions to achieve a desired outcome, such as trial and error, find the best known strategy then monitoring it to adapt to the environment, and learning the outcomes of success and failures of a project.\n\nMitigation\n\nThe purpose of mitigation is not only to minimize the loss of property damage, but minimize environmental damages due to development. To avoid impacts by not taking or limiting actions, to reduce or rectify impacts by rehabilitation or restoring the affected environments or instituting long-term maintenance operations and compensating for impacts by replacing or providing substitute environments for resources\nStructural mitigation is the current solution to eroding beaches and movement of sand is the use of engineered structures along the coast have been short lived and are only an illusion of safety to the public that result in long term damage of the coastline. Structural management deals with the use of the following: groins which are man-made solution to longshore current movements up and down the coast. The use of groins are efficient to some extent yet cause erosion and sand build up further down the beaches. Bulkheads are man-made structures that help protect the homes built along the coast and other bodies of water that actually induce erosion in the long run. Jetties are structures built to protect sand movement into the inlets where boats for fishing and recreation move through.\nThe use of nonstructural mitigation is the practice of using organic and soft structures for solutions to protect against coastal hazards. These include: artificial dunes, which are used to create dunes that have been either developed on or eroded. There needs to be at least two lines of dunes before any development can occur. Beach Nourishment is a major source of nonstructural mitigation to ensure that beaches are present for the communities and for the protection of the coastline. Vegetation is a key factor when protecting from erosion, specifically for to help stabilize dune erosion.\n\n\n"}
{"id": "26834979", "url": "https://en.wikipedia.org/wiki?curid=26834979", "title": "Condensation cloud", "text": "Condensation cloud\n\nA transient condensation cloud, also called Wilson cloud, is observable at large explosions in humid air.\n\nWhen a nuclear weapon or a large amount of a conventional explosive is detonated in sufficiently humid air, the \"negative phase\" of the shock wave causes a rarefaction (reduction in density) of the air surrounding the explosion, but not contained within it. This rarefaction results in a temporary cooling of that air, which causes a condensation of some of the water vapor contained in it. When the pressure and the temperature return to normal, the Wilson cloud dissipates.\n\nSince heat does not leave the affected air mass, this change of pressure is adiabatic, with an associated change of temperature. In humid air, the drop in temperature in the most rarefied portion of the shock wave can bring the air temperature below its dew point, at which moisture condenses to form a visible cloud of microscopic water droplets. Since the pressure effect of the wave is reduced by its expansion (the same pressure effect is spread over a larger radius), the vapor effect also has a limited radius. Such vapor can also be seen in low pressure regions during high–g subsonic maneuvers of aircraft in humid conditions.\n\nScientists observing the Operation Crossroads nuclear tests in 1946 at Bikini Atoll named that transitory cloud a \"Wilson cloud\" because of its similarity to the appearance of the inside of a Wilson cloud chamber, an instrument they would have been familiar with. (The cloud chamber effect is caused by a temporary reduction in pressure in a closed system and marks the tracks of electrically-charged sub-atomic particles.) Analysts of later nuclear bomb tests used the more general term \"condensation cloud\".\n\nThe shape of the shock wave, influenced by different speed in different altitudes, and the temperature and humidity of different atmospheric layers determines the appearance of the Wilson clouds. During nuclear tests, condensation rings around or above the fireball are commonly observed. Rings around the fireball may become stable and form rings around the rising stem of the mushroom cloud.\n\nThe lifetime of the Wilson cloud during nuclear air bursts can be shortened by the thermal radiation from the fireball, which heats the cloud above the dew point and evaporates the droplets.\n\nThe same kind of condensation cloud is sometimes seen above the wings of aircraft in a moist atmosphere. The top of a wing has a reduction of air pressure as part of the process of generating lift. This reduction in air pressure causes a cooling, just as above, and the condensation of water vapor. Hence, the small, transient clouds that appear.\n\nThe vapor cone of a transonic aircraft is another example of a condensation cloud.\n"}
{"id": "3079141", "url": "https://en.wikipedia.org/wiki?curid=3079141", "title": "Deistic evolution", "text": "Deistic evolution\n\nDeistic evolution is a position in the origins debate which involves accepting the scientific evidence for evolution and age of the universe whilst advocating the view that a deistic God created the universe but has not interfered since. The position is a counterpoint to theistic evolution and is endorsed by those who believe in both deism and the veracity of science.\n\nIn \"Christian Theology\", by Millard J. Erickson, 2013, is written:\n\nThe psychologist Steve Stewart-Williams in his book \"Darwin, God and the Meaning of Life\" (2010) states:\n\nStewart-Williams further writes that deistic evolution strips God of what most religious believers consider central. Any deistic God is not around for prayers, miracles or to intervene in people's lives and that because of this it is unpopular with monotheistic religions.\n\nDeistic Evolution adheres to the concept of some form of God, but denies any personal God. A recent defender of deistic evolution was Michael Anthony Corey, author of the book \"Back to Darwin: The Scientific Case for Deistic Evolution\" (1994).\n\nSome scholars have written that Charles Darwin was an advocate of deistic evolution.\n\nDeistic evolution is similarly the operative idea in Pandeism, which has been counted amongst the handful of spiritual beliefs which \"are compatible with modern science.\" and specifically wherein it is noted that \"\"pandeistic\" belief systems ... [present] the inclusion of God as the ever unfolding expression of a complex universe with an identifiable beginning but no teleological direction necessarily present.\"\n\nDeistic evolution is not the same as theistic evolution, yet they are sometimes confused. The difference rests on the difference between a theistic god that is interested in, if not actively involved in, the outcome of his creation and humanity specifically and a deistic god that is either disinterested in the outcome, and holds no special place for humanity, or will not intervene. Often, there is no discernible difference between the two positions—the choice of terminology has more to do with the believer and her or his need for a god, than fitting into a mostly arbitrary dictionary or academic definition.\n\nDeistic evolution has been criticised by Christian creationists as being incompatible with Christianity since it contradicts a literal reading of the Bible and more importantly, leaves no role for the \"Christian personal God\".\n\nM. J. Erickson wrote that deistic evolution is in conflict with the scriptural doctrine of providence according to which \"God is personally and intimately concerned with and involved in what is going on in the specific events within his entire creation.\"\n\nCharles P. Grannan wrote in 1894, \"Another baseless assumption of negative critics is that the general principles of Atheistic and Deistic evolution, admitted by many scientists to account for the origin of the various species of plants and animals, should also be applied to explain the origin of the Christian religion.\"\n\nCharles Wesley Rishell criticized the concept in 1899, comparing it to the notion (false, in his view), that gravity was a property of matter instead of a continued action of God:\n\nDeistic evolution does not oppose or contradict evolution or come into conflict with science as it says that a God started the process and then left it to natural processes. However deism is still a religious philosophy.\n\nStewart-Williams wrote regarding deistic evolution and science:\n\nThere is considerable room for this \"god of the gaps\" view, since scientific observation is entirely unable to shed any light on what happened during the Planck epoch, the earliest 10 seconds in the history of the universe. All development since this initial creative act merely follows laws and principles which He created:\n\n\nThe Roman Catholic Church disagrees with the doctrine of deistic evolution. In November 2005, Pope Benedict addressed a general audience of 25,000 in St. Peter's Square:\n\n"}
{"id": "38045209", "url": "https://en.wikipedia.org/wiki?curid=38045209", "title": "Despeciation", "text": "Despeciation\n\nDespeciation is the loss of a unique species of animal due to its combining with another previously distinct species. It is the opposite of Speciation and is much more rare. It is similar to extinction in that there is a loss of a unique species but without the associated loss of a biological lineage.\n\nFor example, Taylor et al.'s genetic analysis of three-spined sticklebacks across six lakes in southwestern British Columbia found two distinct species in 1977 and 1988 but only one combined species in data from 1997, 2000, and 2002. They concluded that external factors had imperiled the living conditions of the two species, thus eliminating the evolutionary specializations that had kept them unique.\n"}
{"id": "36238152", "url": "https://en.wikipedia.org/wiki?curid=36238152", "title": "Driving factors", "text": "Driving factors\n\nIn energy monitoring and targeting, a driving factor is something recurrent and measurable whose variation explains variation in energy consumption. The term \"independent variable\" is sometimes used as a synonym.\n\nOne of the most common driving factors is the weather, expressed usually as heating or cooling degree days. In energy-intensive processes, production throughputs would usually be used. For electrical circuits feeding outdoor lighting, the number of hours of darkness can be employed. For a borehole pump, the quantity of water delivered would be used; and so on. What these examples all have in common is that on a weekly basis (say) numerical values can be recorded for each factor and one would expect particular streams of energy consumption to correlate with them either singly or in a multi-variate model.\n\nCorrelation is arguably more important than causality. Variation in the driving factor merely has to \"explain\" variation in consumption; it does not necessarily have to \"cause\" it, although that will in most scenarios be the case.\n\nDriving factors differ from \"static\" factors, such as building floor areas, which determine energy consumption but change only rarely (if at all).\n"}
{"id": "944638", "url": "https://en.wikipedia.org/wiki?curid=944638", "title": "Earth's energy budget", "text": "Earth's energy budget\n\nEarth's energy budget accounts for the balance between the energy Earth receives from the Sun, the energy Earth radiates back into outer space after having been distributed throughout the five components of Earth's climate system and having thus powered the so-called Earth’s heat engine. This system is made up of earth's water, ice, atmosphere, rocky crust, and all living things.\n\nQuantifying changes in these amounts is required to accurately model the Earth's climate. \n\nReceived radiation is unevenly distributed over the planet, because the Sun heats equatorial regions more than polar regions. The atmosphere and ocean work non-stop to even out solar heating imbalances through evaporation of surface water, convection, rainfall, winds, and ocean circulation. Earth is very close to being in radiative equilibrium, the situation where the incoming solar energy is balanced by an equal flow of heat to space; under that condition, global temperatures will be \"relatively\" stable. Globally, over the course of the year, the Earth system—land surfaces, oceans, and atmosphere—absorbs and then radiates back to space an average of about 240 watts of solar power per square meter. Anything that increases or decreases the amount of incoming or outgoing energy will change global temperatures in response.\n\nHowever, Earth's energy balance and heat fluxes depend on many factors, such as atmospheric composition (mainly aerosols and greenhouse gases), the albedo (reflectivity) of surface properties, cloud cover and vegetation and land use patterns.\n\nChanges in surface temperature due to Earth's energy budget do not occur instantaneously, due to the inertia of the oceans and the cryosphere. The net heat flux is buffered primarily by becoming part of the ocean's heat content, until a new equilibrium state is established between radiative forcings and the climate response.\n\nIn spite of the enormous transfers of energy into and from the Earth, it maintains a relatively constant temperature because, as a whole, there is little net gain or loss: Earth emits via atmospheric and terrestrial radiation (shifted to longer electromagnetic wavelengths) to space about the same amount of energy as it receives via insolation (all forms of electromagnetic radiation).\n\nTo quantify Earth's \"heat budget\" or \"heat balance\", let the insolation received at the top of the atmosphere be 100 units (100 units = about 1,360 watts per square meter facing the sun), as shown in the accompanying illustration. Called the albedo of Earth, around 35 units are reflected back to space: 27 from the top of clouds, 2 from snow and ice-covered areas, and 6 by other parts of the atmosphere. The 65 remaining units are absorbed: 14 within the atmosphere and 51 by the Earth’s surface. These 51 units are radiated to space in the form of terrestrial radiation: 17 directly radiated to space and 34 absorbed by the atmosphere (19 through latent heat of condensation, 9 via convection and turbulence, and 6 directly absorbed). The 48 units absorbed by the atmosphere (34 units from terrestrial radiation and 14 from insolation) are finally radiated back to space. These 65 units (17 from the ground and 48 from the atmosphere) balance the 65 units absorbed from the sun in order to maintain zero net gain of energy by the Earth.\n\nThe total amount of energy received per second at the top of Earth's atmosphere (TOA) is measured in watts and is given by the solar constant times the cross-sectional area of the Earth. Because the surface area of a sphere is four times the cross-sectional surface area of a sphere (i.e. the area of a circle), the average TOA flux is one quarter of the solar constant and so is approximately 340 W/m². Since the absorption varies with location as well as with diurnal, seasonal and annual variations, the numbers quoted are long-term averages, typically averaged from multiple satellite measurements.\n\nOf the ~340 W/m² of solar radiation received by the Earth, an average of ~77 W/m² is reflected back to space by clouds and the atmosphere and ~23 W/m² is reflected by the surface albedo, leaving ~240 W/m² of solar energy input to the Earth's energy budget. This gives the earth a mean net albedo of 0.29.\n\nThe geothermal heat flux from the Earth's interior is estimated to be 47 terawatts. This comes to 0.087 watt/square metre, which represents only 0.027% of Earth's total energy budget at the surface, which is dominated by 173,000 terawatts of incoming solar radiation.\n\nHuman production of energy is even lower, at an estimated 18 TW.\n\nPhotosynthesis has a larger effect: photosynthetic efficiency turns up to 2% of incoming sunlight into biomass, for a total photosynthetic productivity of earth between ~1500–2250 TW (~1%+/-0.26% solar energy hitting the Earth's surface).\n\nOther minor sources of energy are usually ignored in these calculations, including accretion of interplanetary dust and solar wind, light from stars other than the Sun and the thermal radiation from space. Earlier, Joseph Fourier had claimed that deep space radiation was significant in a paper often cited as the first on the greenhouse effect.\n\nLongwave radiation is usually defined as outgoing infrared energy leaving the planet. However, the atmosphere absorbs parts initially, or cloud cover can reflect radiation. Generally, heat energy is transported between the planet's surface layers (land and ocean) to the atmosphere, transported via evapotranspiration and latent heat fluxes or conduction/convection processes. Ultimately, energy is radiated in the form of longwave infrared radiation back into space.\n\nRecent satellite observations indicate additional precipitation, which is sustained by increased energy leaving the surface through evaporation (the latent heat flux), offsetting increases in longwave flux to the surface.\n\nIf the incoming energy flux is not equal to the outgoing energy flux, net heat is added to or lost by the planet (if the incoming flux is larger or smaller than the outgoing respectively).\n\nAn imbalance must show in something on Earth warming or cooling (depending on the direction of the imbalance), and the ocean being the larger thermal reservoir on Earth, is a prime candidate for measurements.\n\nEarth's energy imbalance measurements provided by Argo floats have detected an accumulation of ocean heat content (OHC). The estimated imbalance was measured during a deep solar minimum of 2005–2010 to be 0.58 ± 0.15 W/m². This level of detail cannot be inferred directly from measurements of surface energy fluxes, which have combined uncertainties of the order of ± 17 W/m².\n\nSeveral satellites indirectly measure the energy absorbed and radiated by Earth and by inference the energy imbalance. The NASA Earth Radiation Budget Experiment (ERBE) project involves three such satellites: the Earth Radiation Budget Satellite (ERBS), launched October 1984; NOAA-9, launched December 1984; and NOAA-10, launched September 1986.\n\nToday NASA's satellite instruments, provided by CERES, part of the NASA's Earth Observing System (EOS), are designed to measure both solar-reflected and Earth-emitted radiation.\n\nThe major atmospheric gases (oxygen and nitrogen) are transparent to incoming sunlight but are also transparent to outgoing thermal (infrared) radiation. However, water vapor, carbon dioxide, methane and other trace gases are opaque to many wavelengths of thermal radiation. The Earth's surface radiates the net equivalent of 17 percent of the incoming solar energy in the form of thermal infrared. However, the amount that directly escapes to space is only about 12 percent of incoming solar energy. The remaining fraction, 5 to 6 percent, is absorbed by the atmosphere by greenhouse gas molecules.\nWhen greenhouse gas molecules absorb thermal infrared energy, their temperature rises. Those gases then radiate an increased amount of thermal infrared energy in all directions. Heat radiated upward continues to encounter greenhouse gas molecules; those molecules also absorb the heat, and their temperature rises and the amount of heat they radiate increases. The atmosphere thins with altitude, and at roughly 5–6 kilometres, the concentration of greenhouse gases in the overlying atmosphere is so thin that heat can escape to space.\n\nBecause greenhouse gas molecules radiate infrared energy in all directions, some of it spreads downward and ultimately returns to the Earth's surface, where it is absorbed. The Earth's surface temperature is thus higher than it would be if it were heated only by direct solar heating. This supplemental heating is the natural greenhouse effect. It is as if the Earth is covered by a blanket that allows high frequency radiation (sunlight) to enter, but slows the rate at which the low frequency infrared radiant energy emitted by the Earth leaves.\n\nA change in the incident radiated portion of the energy budget is referred to as a radiative forcing.\n\nClimate sensitivity is the steady state change in the equilibrium temperature as a result of changes in the energy budget.\n\nClimate forcings are changes that cause temperatures to rise or fall, disrupting the energy balance. Natural climate forcings include changes in the Sun's brightness, Milankovitch cycles (small variations in the shape of Earth's orbit and its axis of rotation that occur over thousands of years) and volcanic eruptions that inject light-reflecting particles as high as the stratosphere. Man-made forcings include particle pollution (aerosols) that absorb and reflect incoming sunlight; deforestation, which changes how the surface reflects and absorbs sunlight; and the rising concentration of atmospheric carbon dioxide and other greenhouse gases, which decreases the rate at which heat is radiated to space.\n\nA forcing can trigger feedbacks that intensify (positive feedback) or weaken (negative feedback) the original forcing. For example, loss of ice at the poles, which makes them less reflective, causes greater absorption of energy and so increases the rate at which the ice melts, is an example of a positive feedback.\n\nThe observed planetary energy imbalance during the recent solar minimum shows that solar forcing of climate, although natural and significant, is overwhelmed by anthropogenic climate forcing.\n\nIn 2012, NASA scientists reported that to stop global warming atmospheric CO content would have to be reduced to 350 ppm or less, assuming all other climate forcings were fixed. The impact of anthropogenic aerosols has not been quantified, but individual aerosol types are thought to have substantial heating and cooling effects.\n\n\n"}
{"id": "41077022", "url": "https://en.wikipedia.org/wiki?curid=41077022", "title": "Earth's internal heat budget", "text": "Earth's internal heat budget\n\nEarth's internal heat budget is fundamental to the thermal history of the Earth. The flow of heat from Earth's interior to the surface is estimated at formula_1 terawatts (TW) and comes from two main sources in roughly equal amounts: the \"radiogenic heat\" produced by the radioactive decay of isotopes in the mantle and crust, and the \"primordial heat\" left over from the formation of the Earth.\n\nEarth's internal heat powers most geological processes and drives plate tectonics. Despite its geological significance, this heat energy coming from Earth's interior is actually only 0.03% of Earth's total energy budget at the surface, which is dominated by 173,000 TW of incoming solar radiation. The insolation that eventually, after reflection, reaches the surface penetrates only several tens of centimeters on the daily cycle and only several tens of meters on the annual cycle. This renders solar radiation minimally relevant for internal processes.\n\nBased on calculations of Earth's cooling rate, which assumed constant conductivity in the Earth's interior, in 1862 William Thomson (later made Lord Kelvin) estimated the age of the Earth at 98 million years, which contrasts with the age of 4.5 billion years obtained in the 20th century by radiometric dating. As pointed out by John Perry in 1895 a variable conductivity in the Earth's interior could expand the computed age of the Earth to billions of years, as later confirmed by radiometric dating. Contrary to the usual representation of Kelvin's argument, the observed thermal gradient of the Earth's crust would not be explained by the addition of radioactivity as a heat source. More significantly, mantle convection alters how heat is transported within the Earth, invalidating Kelvin's assumption of purely conductive cooling.\n\nEstimates of the total heat flow from Earth’s interior to surface span a range of 43 to 49 terawatts (TW) (a terawatt is 10 watts). One recent estimate is 47 TW, equivalent to an average heat flux of 91.6 mW/m, and is based on more than 38,000 measurements. The respective mean heat flows of continental and oceanic crust are 70.9 and 105.4 mW/m.\n\nWhile the total internal Earth heat flow to the surface is well constrained, the relative contribution of the two main sources of Earth's heat, radiogenic and primordial heat, are highly uncertain because their direct measurement is difficult. Chemical and physical models give estimated ranges of 15–41 TW and 12–30 TW for radiogenic heat and primordial heat, respectively. \n\nThe structure of the Earth is a rigid outer crust that is composed of thicker continental crust and thinner oceanic crust, solid but plastically flowing mantle, a liquid outer core, and a solid inner core. The fluidity of a material is proportional to temperature; thus, the solid mantle can still flow on long time scales, as a function of its temperature and therefore as a function of the flow of Earth's internal heat. The mantle convects in response to heat escaping from Earth's interior, with hotter and more buoyant mantle rising and cooler, and therefore denser, mantle sinking. This convective flow of the mantle drives the movement of Earth's lithospheric plates; thus, an additional reservoir of heat in the lower mantle is critical for the operation of plate tectonics and one possible source is an enrichment of radioactive elements in the lower mantle.\n\nEarth heat transport occurs by conduction, mantle convection, hydrothermal convection, and volcanic advection. Earth's internal heat flow to the surface is thought to be 80% due to mantle convection, with the remaining heat mostly originating in the Earth's crust, with about 1% due to volcanic activity, earthquakes, and mountain building. Thus, about 99% of Earth's internal heat loss at the surface is by conduction through the crust, and mantle convection is the dominant control on heat transport from deep within the Earth. Most of the heat flow from the thicker continental crust is attributed to internal radiogenic sources, in contrast the thinner oceanic crust has only 2% internal radiogenic heat. The remaining heat flow at the surface would be due to basal heating of the crust from mantle convection. Heat fluxes are negatively correlated with rock age, with the highest heat fluxes from the youngest rock at mid-ocean ridge spreading centers (zones of mantle upwelling), as observed in the .\n\nThe radioactive decay of elements in the Earth's mantle and crust results in production of daughter isotopes and release of geoneutrinos and heat energy, or radiogenic heat. Four radioactive isotopes are responsible for the majority of radiogenic heat because of their enrichment relative to other radioactive isotopes: uranium-238 (U), uranium-235 (U), thorium-232 (Th), and potassium-40 (K). Due to a lack of rock samples from below 200 km depth, it is difficult to determine precisely the radiogenic heat throughout the whole mantle, although some estimates are available. For the Earth's core, geochemical studies indicate that it is unlikely to be a significant source of radiogenic heat due to an expected low concentration of radioactive elements partitioning into iron. Radiogenic heat production in the mantle is linked to the structure of mantle convection, a topic of much debate, and it is thought that the mantle may either have a layered structure with a higher concentration of radioactive heat-producing elements in the lower mantle, or small reservoirs enriched in radioactive elements dispersed throughout the whole mantle.\n\nGeoneutrino detectors can detect the decay of U and Th and thus allow estimation of their contribution to the present radiogenic heat budget, while U and K is not detectable. Regardless, K is estimated to contribute 4 TW of heating. However, due to the short half-lives the decay of U and K contributed a large fraction of radiogenic heat flux to the early Earth, which was also much hotter than at present. Initial results from measuring the geoneutrino products of radioactive decay from within the Earth, a proxy for radiogenic heat, yielded a new estimate of half of the total Earth internal heat source being radiogenic, and this is consistent with previous estimates.\n\nPrimordial heat is the heat lost by the Earth as it continues to cool from its original formation, and this is in contrast to its still actively-produced radiogenic heat. The Earth core's heat flow—heat leaving the core and flowing into the overlying mantle—is thought to be due to primordial heat, and is estimated at 5–15 TW. Estimates of mantle primordial heat loss range between 7 and 15 TW, which is calculated as the remainder of heat after removal of core heat flow and bulk-Earth radiogenic heat production from the observed surface heat flow.\n\nThe early formation of the Earth's dense core could have caused superheating and rapid heat loss, and the heat loss rate would slow once the mantle solidified. Heat flow from the core is necessary for maintaining the convecting outer core and the geodynamo and Earth's magnetic field, therefore primordial heat from the core enabled Earth's atmosphere and thus helped retain Earth's liquid water.\n\nControversy over the exact nature of mantle convection makes the linked evolution of Earth's heat budget and the dynamics and structure of the mantle difficult to unravel. There is evidence that the processes of plate tectonics were not active in the Earth before 3.2 billion years ago, and that early Earth's internal heat loss could have been dominated by advection via heat-pipe volcanism. Terrestrial bodies with lower heat flows, such as the Moon and Mars, conduct their internal heat through a single lithospheric plate, and higher heat flows, such as on Jupiter's moon Io, result in advective heat transport via enhanced volcanism, while the active plate tectonics of Earth occur with an intermediate heat flow and a convecting mantle.\n\n"}
{"id": "9951602", "url": "https://en.wikipedia.org/wiki?curid=9951602", "title": "Earth mass", "text": "Earth mass\n\nEarth mass (, where ⊕ is the standard astronomical symbol for planet Earth) is the unit of mass equal to that of Earth. \nThe current best estimate for Earth mass is , with a standard uncertainty of \nIt is equivalent to an average density of .\n\nThe Earth mass is a standard unit of mass in astronomy that is used to indicate the masses of other planets, including rocky terrestrial planets and exoplanets. One Solar mass is close to 333,000 Earth masses.\nThe Earth mass excludes the mass of the Moon. The mass of the Moon is about 1.2% of that of the Earth, so that the mass of the Earth+Moon system is close to .\n\nMost of the mass is accounted for by iron and oxygen (c. 32% each), magnesium and silicon (c. 15% each), calcium, aluminium and nickel (c. 1.5% each).\n\nPrecise measurement of the Earth mass is difficult, as it is equivalent to measuring the gravitational constant, which is the fundamental physical constant known with least accuracy, due to the relative weakness of the gravitational force.\nThe mass of the Earth was measured accurately in the Schiehallion experiment in the 1770s, and within 1% of the modern value in the Cavendish experiment of 1798.\n\nThe mass of Earth is estimated to be:\nwhich can be expressed in terms of solar mass as:\n\nThe ratio of Earth mass to lunar mass has been measured to great accuracy. The current best estimate is:\n\nThe \"G\" product for the Earth is called the geocentric gravitational constant and equals . It is determined using laser ranging data from Earth-orbiting satellites, such as LAGEOS-1. The \"G\" product can also be calculated by observing the motion of the Moon or the period of a pendulum at various elevations. These methods are less precise than observations of artificial satellites.\n\nThe relative uncertainty of the geocentric gravitational constant is just , however, (the mass of the Earth in kilograms) can be found out only by dividing the \"G\" product by \"G\", and \"G\" is known only to a relative uncertainty of \nFor this reason and others, astronomers prefer to use the un-reduced \"G\" product, or mass ratios\n(masses expressed in units of Earth mass or Solar mass) rather than mass in kilograms when referencing and comparing planetary objects.\n\nEarth's density varies considerably, between less than in the upper crust to as much as in the inner core.\nThe Earth's core accounts for 15% of Earth's volume but more than 30% of the mass, the mantle for 84% of the volume and close to 70% of the mass, while the crust accounts for less than 1% of the mass.\nAbout 90% of the mass of the Earth is composed of the iron–nickel alloy (95% iron) in the core (30%), and the silicon dioxides (c. 33%) and magnesium oxide (c. 27%) in the mantle and crust. \nMinor contributions are from iron(II) oxide (5%), aluminium oxide (3%) and calcium oxide (2%), besides numerous trace elements (in elementary terms: iron and oxygen c. 32% each, magnesium and silicon c. 15% each, calcium, aluminium and nickel c. 1.5% each). \nCarbon accounts for 0.03%, water for 0.02%, and the atmosphere for about one part per million.\n\nThe mass of Earth is measured indirectly by determining other quantities such as Earth's density, gravity, or gravitational constant.\nThe first measurement in the 1770s Schiehallion experiment resulted in a value about 20% too low.\nThe Cavendish experiment of 1798 found the correct value within 1%.\nUncertainty was reduced to about 0.2% by the 1890s, \nto 0.1% by 1930,\nand to 0.01% (10) by the 2000s.\nThe figure of the Earth has been known to better than four significant digits since the 1960s (WGS66), so that since that time, the uncertainty of the Earth mass is determined essentially by the uncertainty in measuring the gravitational constant.\n\nBefore the direct measurement of the gravitational constant,\nestimates of the Earth mass were limited to estimating Earth's mean density from observation of the crust and estimates on Earth's volume. Estimates on the volume of the earth in the 17th century were based on a circumference estimate of 60 miles to the degree of latitude, corresponding to a radius of about 5,500 km, resulting in an estimated volume of about one third smaller than the correct value.\nThe average density of the Earth was not accurately known. Earth was assumed to consist either mostly of water (Neptunism) or mostly of igneous rock (Plutonism), both suggesting average densities too low by several orders of magnitude,\nconsistent with a total mass of the order of .\nIsaac Newton estimated, without access to reliable measurement, that the density of Earth would be five or six times as great as the density of water, which is surprisingly accurate (the modern value is 5.515).\nNewton under-estimated the Earth's volume by about 30%, so that his estimate would be roughly equivalent to .\nIn the 18th century, knowledge of Newton's law of gravitation permitted indirect estimates on the mean density of the Earth, \nvia estimates of (what in modern terminology is known as) the gravitational constant.\nEarly estimates on the mean density of the Earth were made by observing the slight deflection of a pendulum near a mountain, as in the Schiehallion experiment. Newton considered the experiment in \"Principia\", but pessimistically concluded that the effect would be too small to be measurable.\n\nAn expedition from 1737 to 1740 by Pierre Bouguer and Charles Marie de La Condamine attempted to determine the density of Earth by measuring the period of a pendulum (and therefore the strength of gravity) as a function of elevation. The experiments were carried out in Ecuador and Peru, on Pichincha Volcano and mount Chimborazo.\nBouguer wrote in a 1749 paper that they had been able to detect a deflection of 8 seconds of arc,\nThe accuracy was not enough for a definite estimate on the mean density of the Earth, but Bouguer stated that it was at least sufficient to prove that the Earth was not hollow.\n\nThat a further attempt should be made on the experiment was proposed to the Royal Society in 1772 by Nevil Maskelyne, Astronomer Royal. He suggested that the experiment would \"do honour to the nation where it was made\" and proposed Whernside in Yorkshire, or the Blencathra-Skiddaw massif in Cumberland as suitable targets. The Royal Society formed the Committee of Attraction to consider the matter, appointing Maskelyne, Joseph Banks and Benjamin Franklin amongst its members. The Committee despatched the astronomer and surveyor Charles Mason to find a suitable mountain.\n\nAfter a lengthy search over the summer of 1773, Mason reported that the best candidate was Schiehallion, a peak in the central Scottish Highlands. The mountain stood in isolation from any nearby hills, which would reduce their gravitational influence, and its symmetrical east–west ridge would simplify the calculations. Its steep northern and southern slopes would allow the experiment to be sited close to its centre of mass, maximising the deflection effect.\nNevil Maskelyne, Charles Hutton and Reuben Burrow performed the experiment, completed by 1776.\nHutton (1778) reported that the mean density of the Earth was estimated at \nformula_4 that of Schiehallion mountain.\nThis corresponds to a mean density about 4 higher than that of water (i.e., about ), about 20% below the modern value, but still significantly larger than the mean density of normal rock, suggesting for the first time that the interior of the Earth might be substantially \ncomposed of metal.\nHutton estimated this metallic portion to occupy some (or 65%) of the diameter of the Earth (modern value 55%). \nWith a value for the mean density of the Earth, Hutton was able to set some values to Jérôme Lalande's planetary tables, which had previously only been able to express the densities of the major solar system objects in relative terms.\n\nThe Henry Cavendish (1798) was the first to attempt to measure the gravitational attraction between two bodies directly in the laboratory.\nEarth's mass could be then found by combining two equations; Newton's second law, and Newton's law of universal gravitation.\n\nIn modern notation, the mass of the Earth is derived from the gravitational constant and the mean Earth radius by\nWhere \"little g\":\n\nCavendish found a mean density of , about 1% below the modern value.\n\nWhile the mass of the Earth is implied by stating the Earth's radius and density, it was not usual to state the absolute mass explicitly \nprior to the introduction of scientific notation using powers of 10 in the later 19th century, \nbecause the absolute numbers would have been too awkward. Ritchie (1850) gives the mass of the Earth's atmosphere as \"11,456,688,186,392,473,000 lbs.\" ( = , modern value is ) and states that \"compared with the weight of the globe this mighty sum dwindles to insignificance\".\n\nAbsolute figures for the mass of the Earth are cited only beginning in the second half of the 19th century, mostly in popular rather than expert literature.\nAn early such figure was given as \"14 quadrillion pounds\" (\"14 Quadrillionen Pfund\") [] in Masius (1859).\n\nBeckett (1871) cites the \"weight of the earth\" as \"5842 quintillion tons\" [].\nThe \"mass of the earth in gravitational measure\" is stated as \"9.81996×6370980\" in \"The New Volumes of the Encyclopaedia Britannica\" (Vol. 25, 1902) with a \"logarithm of earth's mass\" given as \"14.600522\" []. This is the gravitational parameter in m·s (modern value ) and not the absolute mass.\n\nExperiments involving pendulums continued to be performed in the first half of the 19th century. By the second half of the century, these were outperformed by repetitions of the Cavendish experiment, and the modern value of \"G\" (and hence, of the Earth mass) is still derived from high-precision repetitions of the Cavendish experiment.\n\nIn 1821, Francesco Carlini determined a density value of ρ = through measurements made with pendulums in the Milan area. This value was refined in 1827 by Edward Sabine to , and then in 1841 by Carlo Ignazio Giulio to . On the other hand, George Biddell Airy sought to determine ρ by measuring the difference in the period of a pendulum between the surface and the bottom of a mine. \nThe first tests took place in Cornwall between 1826 and 1828. The experiment was a failure due to a fire and a flood. Finally, in 1854, Airy got the value by measurements in a coal mine in Harton, Sunderland. Airy's method assumed that the Earth had a spherical stratification. Later, in 1883, the experiments conducted by Robert von Sterneck (1839 to 1910) at different depths in mines of Saxony and Bohemia provided the average density values ρ between 5.0 and . This led to the concept of isostasy, which limits the ability to accurately measure ρ, by either the deviation from vertical of a plumb line or using pendulums. Despite the little chance of an accurate estimate of the average density of the Earth in this way, Thomas Corwin Mendenhall in 1880 realized a gravimetry experiment in Tokyo and at the top of Mount Fuji. The result was ρ = .\n\nThe uncertainty in the modern value for the Earth's mass has been entirely due to the uncertainty in the gravitational constant \"G\" since at least the 1960s.\n\"G\" is notoriously difficult to measure, and some high-precision measurements during the 1980s to 2010s have yielded mutually exclusive results.\nSagitov (1969) based on the measurement of \"G\" by Heyl and Chrzanowski (1942) cited a value of (relative uncertainty ).\n\nAccuracy has improved only slightly since then. Most modern measurements are repetitions of the Cavendish experiment, with results (within standard uncertainty) ranging between 6.672 and 6.676 ×10  m kgs (relative uncertainty 3×10) in results reported since the 1980s, although the 2014 NIST recommended value is close to 6.674×10  m kgs with a relative uncertainty below 10.\nThe \"Astronomical Almanach Online\" as of 2016 recommends a standard uncertainty of for Earth mass, \n\nEarth's mass is variable, subject to both gain and loss due to the accretion of micrometeorites and cosmic dust \nand the loss of hydrogen and helium gas, respectively.\nThe combined effect is a net loss of material, estimated at (54,000 tons) per year. This amount is of the total earth mass. The annual net loss is essentially due to 100,000 tons lost due to atmospheric escape, and an average of 45,000 tons gained from in-falling dust and meteorites. This is well within the mass uncertainty of 0.01% (), so the estimated value of earth's mass is unaffected by this factor.\n\nMass loss is due to atmospheric escape of gases. About 95,000 tons of hydrogen per year () and 1,600 tons of helium per year are lost through atmospheric escape.\nThe main factor in mass gain is in-falling material, cosmic dust, meteors, etc. are the most significant contributors to Earth's increase in mass. The sum of material is estimated to be 37,000 to 78,000 tons annually.\n\nAdditional changes in mass are due to the mass–energy equivalence principal, although these changes are relatively negligible. An increase in mass has been ascribed to rising temperatures (global warming), estimated at 160 tonnes per years as of 2016.\nAnother 16 tons per year are lost in the form of rotational kinetic energy due to the deceleration of the rotation of Earth's inner core. This energy is transferred to the rotational energy of the solar system, and the trend might also be reversible, as rotation speed has been shown to fluctuate over decades. Mass loss due to nuclear fission is estimated to amount to 16 tons per year.\nAn additional loss due to spacecraft on escape trajectories has been estimated at since the mid-20th century. Earth lost about 3473 tons in the initial 53 years of the space age, but the trend is currently decreasing.\n\n"}
{"id": "38103099", "url": "https://en.wikipedia.org/wiki?curid=38103099", "title": "Earth pyramids of Ritten", "text": "Earth pyramids of Ritten\n\nThe earth pyramids of Ritten (German: \"Erdpyramiden am Ritten\"; ) are a natural monument that is located on the Ritten, a plateau not far from Bolzano in northern Italy. The earth pyramids of South Tyrol are a fairly widespread phenomenon which are existing in various locations.\nThe original name in this area for these earth pyramids is \"Lahntürme\", i.e. landslide towers. They are rather unusual formations of their kind which originate from morainic rocks of glacial origin. The columns of the pyramids may be more or less elongated, and the higher they are the thinner they get, ending usually with a stone cover. These earth pyramids are not static, they are constantly evolving, because their life cycle foresees a continuous erosion, or even a final collapse leaving room for new formations.\n\nIn South Tyrol there are other natural monuments like this such as the earth pyramids of Platten, but the ones of Ritten are considered the parents of them all.\n\n"}
{"id": "38103098", "url": "https://en.wikipedia.org/wiki?curid=38103098", "title": "Earth pyramids of South Tyrol", "text": "Earth pyramids of South Tyrol\n\nThe earth pyramids in South Tyrol are a special natural phenomenon that comes about in particular terrain, usually after a landslide or an unhinging of the earth.\n\nThe main cause of the formation of earth pyramids is the continuous alternation of periods of torrential rain and\ndrought. These phenomena, in particularly friable terrain, over the years, increasingly erode the ground and form such earth pyramids. Usually the pyramids are formed in terrain very well sheltered from wind so that they cannot be damaged by it.\n\nMoreover, the life of the earth pyramids is strongly dependent on the climate which reigns during the time in which it is shaped by the rock that covers it.\n\nThere are several earth pyramids that can be safely visited. Among the most famous and admired the\nfollowing are the most outstanding:\n\nOther, less famous, earth pyramids are:\n\n"}
{"id": "31450053", "url": "https://en.wikipedia.org/wiki?curid=31450053", "title": "Energy accidents", "text": "Energy accidents\n\nEnergy resources bring with them great social and economic promise, providing financial growth for communities and energy services for local economies. However, the infrastructure which delivers energy services can break down in an energy accident, sometimes causing much damage, and energy fatalities can occur, and with many systems often deaths will happen even when the systems are working as intended.\n\nHistorically, coal mining has been the most dangerous energy activity and the list of historical coal mining disasters is a long one. Underground mining hazards include suffocation, gas poisoning, roof collapse and gas explosions. Open cut mining hazards are principally mine wall failures and vehicle collisions. In the US alone, more than 100,000 coal miners have been killed in accidents over the past century, with more than 3,200 dying in 1907 alone.\n\nAccording to Benjamin K. Sovacool, 279 \"major\" energy accidents occurred from 1907 to 2007 and they caused 182,156 deaths with $41 billion in property damages, with these figures not including deaths from smaller accidents.\n\nHowever, by far the greatest energy fatalities that result from energy generation by humanity, is the creation of air pollution. The most lethal of which, particulate matter, which is primarily generated from the burning of fossil fuels and biomass is (counting outdoor air pollution effects only) estimated to cause 2.1 million deaths annually.\n\nAccording to Benjamin K. Sovacool, while responsible for less than 1 percent of the total number of energy accidents, hydroelectric facilities claimed 94 percent of reported immediate fatalities. Results on immediate fatalities are dominated by one disaster in which Typhoon Nina in 1975 washed out the Shimantan Dam (Henan Province, China) and 171,000 people perished. While the other major accident that involved greater than 1000 immediate deaths followed the rupture of the NNPC petroleum pipeline in 1998 and the resulting explosion. The other singular accident described by Sovacool is the \"predicted\" latent death toll of greater than 1000, as a result of the 1986 steam explosion at the Chernobyl nuclear reactor in the Ukraine. With approximately 4000 deaths in total, to eventually result in the decades ahead due to the radio-isotope pollution released.\n\nIn the oil and gas industry, the need for improved safety culture and training within companies is evidenced by the finding that workers new to a company are more likely to be involved in fatalities.\n\nCoal mining accidents resulted in 5,938 immediate deaths in 2005, and 4746 immediate deaths in 2006 in China alone according to the World Wildlife Fund. Coal mining is the most dangerous occupation in China, the death rate for every 100 tons of coal mined is 100 times that of the death rate in the US and 30 times that achieved in South Africa. Moreover, 600,000 Chinese coal miners, as of 2004, were suffering from Coalworker's pneumoconiosis (known as \"black lung\") a disease of the lungs caused by long-continued inhalation of coal dust. And the figure increases by 70,000 miners every year in China.\n\nHistorically, coal mining has been a very dangerous activity and the list of historical coal mining disasters is a long one. In the US alone, more than 100,000 coal miners were killed in accidents over the past century, with more than 3,200 dying in 1907 alone. In the decades following this peak, an annual death toll of 1,500 miner fatalities occurred every year in the US until approximately the 1970s. Coal mining fatalities in the US between 1990 and 2012 have continued to decline, with fewer than 100 each year. (See more Coal mining disasters in the United States)\n\nIn the United States, in the 2000s, after three decades of regulation on the Environmental impact of the coal industry, including regulations in the 1970s and 1990s from the Clean Air Act, an act created to cut down on pollution related deaths from fossil fuel usage, US coal fired power plants were estimated, in the 2000s, to continue to cause between 10,000 and 30,000 latent, or air pollution related deaths per year, due to the emissions of sulfur dioxide, nitrogen oxides and directly emitted particulate matter that result when coal is burnt.\n\nAccording to the World Health Organization in 2012, urban outdoor air pollution, from the burning of fossil fuels and biomass is estimated to cause 3 million deaths worldwide per year and indoor air pollution from biomass and fossil fuel burning is estimated to cause approximately 4.3 million premature deaths. In 2013 a team of researchers estimated the number of premature deaths caused by particulate matter in outdoor air pollution as 2.1 million, occurring annually.\n\nBenjamin Sovacool says that while hydroelectric plants were responsible for the most fatalities, nuclear power plants rank first in terms of their economic cost, accounting for 41 percent of all property damage. Oil and hydroelectric follow at around 25 percent each, followed by natural gas at 9 percent and coal at 2 percent. Excluding Chernobyl and the Shimantan Dam, the three other most expensive accidents involved the Exxon Valdez oil spill (Alaska), The Prestige oil spill (Spain), and the Three Mile Island nuclear accident (Pennsylvania). However analysis presented in the international Journal, \"Human and Ecological Risk Assessment\" found that coal, oil, Liquid petroleum gas and hydro accidents have cost more than nuclear power accidents.\n\nModern-day U.S. regulatory agencies frequently implement regulations on conventional pollution if one life or more is predicted saved per $6 million to $8 million of economic costs incurred.\n\n\n\n"}
{"id": "52058583", "url": "https://en.wikipedia.org/wiki?curid=52058583", "title": "Energy system", "text": "Energy system\n\nAn energy system is a system primarily designed to supply energy-services to end-users. Taking a structural viewpoint, the IPCC Fifth Assessment Report defines an energy system as \"all components related to the production, conversion, delivery, and use of energy\". The field of energy economics includes energy markets and treats an energy system as the technical and economic systems that satisfy consumer demand for energy in the forms of heat, fuels, and electricity.\n\nThe first two definitions allow for demand-side measures, including daylighting, retrofitted building insulation, and passive solar building design, as well as socio-economic factors, such as aspects of energy demand management and even telecommuting, while the third does not. Neither does the third account for the informal economy in traditional biomass that is significant in many developing countries.\n\nThe analysis of energy systems thus spans the disciplines of engineering and economics. Merging ideas from both areas to form a coherent description, particularly where macroeconomic dynamics are involved, is challenging.\n\nThe concept of an energy system is evolving as new regulations, technologies, and practices enter into service – for example, emissions trading, the development of smart grids, and the greater use of energy demand management, respectively.\n\nFrom a structural perspective, an energy system is like any general system and is made up of a set of interacting component parts, located within an environment. These components derive from ideas found in engineering and economics. Taking a process view, an energy system \"consists of an integrated set of technical and economic activities operating within a complex societal framework\". The identification of the components and behaviors of an energy system depends on the circumstances, the purpose of the analysis, and the questions under investigation. The concept of an energy system is therefore an abstraction which usually precedes some form of computer-based investigation, such as the construction and use of a suitable energy model.\n\nViewed in engineering terms, an energy system lends itself to representation as a flow network: the vertices map to engineering components like power stations and pipelines and the edges map to the interfaces between these components. This approach allows collections of similar or adjacent components to be aggregated and treated as one to simplify the model. Once described thus, flow network algorithms, such as minimum cost flow, may be applied. The components themselves can be treated as simple dynamical systems in their own right.\n\nConversely, relatively pure economic modeling may adopt a sectorial approach with only limited engineering detail present. The sector and sub-sector categories published by the International Energy Agency are often used as a basis for this analysis. A 2009 study of the UK residential energy sector contrasts the use of the technology-rich Markal model with several UK sectoral housing stock models.\n\nInternational energy statistics are typically broken down by carrier, sector and sub-sector, and country. Energy carriers ( energy products) are further classified as primary energy and secondary (or intermediate) energy and sometimes final (or end-use) energy. Published energy datasets are normally adjusted so that they are internally consistent, meaning that all energy stocks and flows must balance. The IEA regularly publishes energy statistics and energy balances with varying levels of detail and cost and also offers mid-term projections based on this data. The notion of an energy carrier, as used in energy economics, is distinct and different from the definition of energy used in physics.\n\nEnergy systems can range in scope, from local, municipal, national, and regional, to global, depending on issues under investigation. Researchers may or may not include demand side measures within their definition of an energy system. The IPCC does so, for instance, but covers these measures in separate chapters on transport, buildings, industry, and agriculture.\n\nHousehold consumption and investment decisions may also be included within the ambit of an energy system. Such considerations are not common because consumer behavior is difficult to characterize, but the trend is to include human factors in models. Household decision-taking may be represented using techniques from bounded rationality and agent-based behavior. The American Association for the Advancement of Science (AAAS) specifically advocates that \"more attention should be paid to incorporating behavioral considerations other than price- and income-driven behavior into economic models [of the energy system]\".\n\nThe concept of an energy-service is central, particularly when defining the purpose of an energy system:\n\nEnergy-services can be defined as amenities that are either furnished through energy consumption or could have been thus supplied. More explicitly:\n\nA consideration of energy-services per capita and how such services contribute to human welfare and individual quality of life is paramount to the debate on sustainable energy. People living in poor regions with low levels of energy-services consumption would clearly benefit from greater consumption, but the same is not generally true for those with high levels of consumption.\n\nThe notion of energy-services has given rise to energy-service companies (ESCo) who contract to provide energy-services to a client for an extended period. The ESCo is then free to choose the best means to do so, including investments in the thermal performance and HVAC equipment of the buildings in question.\n\nISO13600, ISO13601, and ISO13602 form a set of international standards covering technical energy systems (TES). Although withdrawn prior to 2016, these documents provide useful definitions and a framework for formalizing such systems. The standards depict an energy system broken down into supply and demand sectors, linked by the flow of tradable energy commodities (or energywares). Each sector has a set of inputs and outputs, some intentional and some harmful byproducts. Sectors may be further divided into subsectors, each fulfilling a dedicated purpose. The demand sector is ultimately present to supply energyware-based services to consumers (see energy-services).\n\n"}
{"id": "500948", "url": "https://en.wikipedia.org/wiki?curid=500948", "title": "Field guide", "text": "Field guide\n\nA field guide is a book designed to help the reader identify wildlife (plants or animals) or other objects of natural occurrence (e.g. minerals). It is generally designed to be brought into the 'field' or local area where such objects exist to help distinguish between similar objects. Field guides are often designed to help users distinguish animals and plants that may be similar in appearance but are not necessarily closely related.\n\nIt will typically include a description of the objects covered, together with paintings or photographs and an index. More serious and scientific field identification books, including those intended for students, will probably include identification keys to assist with identification, but the publicly accessible field guide is more often a browsable picture guide organized by family, colour, shape, location or other descriptors.\n\nPopular interests in identifying things in nature probably were strongest in bird and plant guides. Perhaps the first popular field guide to plants in the United States was the 1893 \"How to Know the Wildflowers\" by \"Mrs. William Starr Dana\" (Frances Theodora Parsons). In 1890, Florence Merriam published \"Birds Through an Opera-Glass\", describing 70 common species. Focused on living birds observed in the field, the book is considered the first in the tradition of modern, illustrated bird guides. In 1902, now writing as Florence Merriam Bailey (having married the zoologist Vernon Bailey), she published \"Handbook of Birds of the Western United States\". By contrast, the \"Handbook\" is designed as a comprehensive reference for the lab rather a portable book for the field. It was arranged by taxonomic order and had clear descriptions of species size, distribution, feeding, and nesting habits.\n\nFrom this point into the 1930s, features of field guides were introduced by Chester A. Reed and others such as changing the size of the book to fit the pocket, including colour plates, and producing guides in uniform editions that covered subjects such as garden and woodland flowers, mushrooms, insects, and dogs.\n\nIn 1934, Roger Tory Peterson, using his fine skill as an artist, changed the way modern field guides approached identification. Using color plates with paintings of similar species together – and marked with arrows showing the differences – people could use his bird guide in the field to compare species quickly to make identification easier. This technique, the \"Peterson Identification System\", was used in most of Peterson's Field Guides from animal tracks to seashells and has been widely adopted by other publishers and authors as well.\n\nToday, each field guide has its own range, focus and organization. Specialist publishers such as Croom Helm, along with organisations like the Audubon Society, the RSPB, the Field Studies Council, National Geographic, HarperCollins, and many others all produce quality field guides.\n\nIt is somewhat difficult to generalise about how field guides are intended to be used, because this varies from one guide to another, partly depending on how expert the targeted reader is expected to be.\n\nFor general public use, the main function of a field guide is to help the reader identify a bird, plant, rock, butterfly or other natural object down to at least the popular naming level. To this end some field guides employ simple keys and other techniques: the reader is usually encouraged to scan illustrations looking for a match, and to compare similar-looking choices using information on their differences. Guides are often designed to first lead readers to the appropriate section of the book, where the choices are not so overwhelming in number.\n\nGuides for students often introduce the concept of identification keys. Plant field guides such as \"Newcomb's Wildflower Guide\" (which is limited in scope to the wildflowers of northeastern North America) frequently have an abbreviated key that helps limit the search. Insect guides tend to limit identification to Order or Family levels rather than individual species, due to their diversity.\n\nMany taxa show variability and it is often difficult to capture the constant features using a small number of photographs. Illustrations by artists or post processing of photographs help in emphasising specific features needed to for reliable identification. Peterson introduced the idea of lines to point to these key features. He also noted the advantages of illustrations over photographs:\n\nField guides aid in improving the state of knowledge of various taxa. By making the knowledge of experienced museum specialists available to amateurs, they increase the gathering of information by amateurs from a wider geographic area and increasing the communication of these findings to the specialists.\n\n"}
{"id": "888727", "url": "https://en.wikipedia.org/wiki?curid=888727", "title": "Flag of Earth", "text": "Flag of Earth\n\nSome individuals and organizations have promoted designs for a flag representing the planet Earth, though none have been officially recognized as such by any governmental body. The most widely recognized flags associated with Earth are the flag of the United Nations and the Earth Day flag. Listed below are some of the unofficial contenders for a Flag of Earth:\n\nA flag designed by John McConnell in 1969 for the first Earth Day is a dark blue field charged with \"The Blue Marble\", a famous NASA photo of the Earth as seen from outer space. The first edition of McConnell's flag used screen-printing and used different colors: ocean and land were blue and the clouds were white. McConnell presented his flag to the United Nations as a symbol for consideration.\n\nBecause of the political views of its creator and its having become a symbol of Earth Day, the flag is associated with environmental awareness, and the celebration of the global community. It was offered for sale originally in the \"Whole Earth Catalog\", and is the only flag which was endorsed by McConnell. \n\n\"The Blue Marble\" image was placed in the public domain, and the public nature of this image was the basis of a legal battle that resulted in the invalidation of a trademark and copyright that was originally issued to the Earth Day flag through its original promotional entity, World Equity, Inc. This does not invalidate the official history of McConnell's flag, only the official documentation that was issued on it.\n\nThe One Flag in Space initiative is an offshoot of the Space Generation Congress (SGC), the Space Generation Advisory Council's yearly world meeting. It promotes usage of the \"Blue Marble\" flag for space exploration (it does not explicitly mention it being McConnell's design).\n\nAdopted in 1946, the flag of the United Nations has been used to indicate world unity, although it technically only represents the United Nations itself. It has a geographical representation of the planet, and its high visibility usage makes it a well-known contender for representing Earth. During the planning for NASA's moon landings of the 1960s, it was suggested that a UN flag be used in place of the flag of the US.\n\nJames William van Kirk, a minister from Youngstown, Ohio, designed in 1913 a peace flag with rainbow stripes, stars and a globe. With this flag, he twice made a peace tour through Europe. The Universal Peace Congress adopted this flag as its World Peace Flag.\n\nDesigned by Pierre de Coubertin in 1914 and adopted in Olympic games since 1920, the Olympic flag represents all mankind and has a white bar with 5 interlocking rings of 5 colors. The five rings represent the five continents. The six colors (including the white color of the background) represent all of the world's nations.\n\nThe World Citizen is a social movement for global citizenship under a proposed world government. In 1953, one of its activists, Garry Davis, founded the World Service Authority, which sells the World Passport (a fantasy travel document) with a proposed flag of the world.\n\nThe astrological (and astronomical) symbol of Earth is another candidate for Earth's flag, often depicted with a blue background, with the symbol at its center.\n\nAnother Earth flag was created around the same time in 1970 by a farmer from Homer, Illinois named James W. Cadle. Cadle's version of the Earth flag consists of a blue circle representing Earth in the center of the flag, a segment of a large yellow circle representing the sun and a small white circle for the moon, all on a black background. It is particularly popular amongst SETI researchers and is used by SETI worldwide. The flag flies at the Ohio State University Radio Observatory and was lowered to half mast when Carl Sagan died. Flag of Earth Co. International was also founded by Cadle which sold the flag. The Flag of Earth became public domain in 2003.\n\nThe World Flag is an international flag created in 1988 by Paul Carroll to act as a symbol to inspire \"positive global change while continuing to embrace and celebrate cultural diversity.\" The current 2008 version of the combined World Flag has a world map 216 flags; including the flags of every UN member state, the United Nations, and several territories of larger nations.\n\nThe World Flag has been flown at the UN Headquarters for the \"A Prayer for Peace\" event, The World Trade Center, Earth Day in Central Park, and at various other events around the world.\n\nIn 2015 a Swedish artist, Oskar Pernefeldt, proposed the \"International Flag of the Planet Earth\". It was conceived to be used in space expeditions and it has two main purposes: \n\nThe creators predict that it will be eventually used in Mars landing in 2025 or in a future colony on that planet. The flag is used by space research groups with intent to implanting a base on Mars. The design of the flag consists of seven rings intersecting each other and a deep-blue-sea in the background. The rings are centered on the flag forming a flower in the middle, representing life on Earth. The intersection of the rings represent that all things on Earth are linked directly or indirectly. The rings are organized in a Borromean rings–like fashion, representing not only the seven continents, but how no part of Earth can be removed without the whole structure collapsing. Finally, the deep-blue represents the ocean and the importance of water for life on Earth.\n\nThe One World Flag is a concept published in 2018 by the German activist and visual artist Thomas Mandl. The design of the flag consists of a blue circle on a translucent blank background. Through the usage of Sheer fabrics, the environment blends into the flag background and thus becomes an integral part of the flag design representing the ever-changing nature of culture, society, politics, and environment of the planet earth. \n\n\n"}
{"id": "8782181", "url": "https://en.wikipedia.org/wiki?curid=8782181", "title": "Geographical zone", "text": "Geographical zone\n\nThe five main latitude regions of the Earth's surface comprise geographical zones, divided by the major circles of latitude. The differences between them relate to climate. They are as follows: \n\n\nOn the basis of latitudinal extent, the globe is divided into three broad heat zones.\n\nThe Torrid is also known as the Tropics. The zone is bounded on the north by the Tropic of Cancer and on the south by the Tropic of Capricorn; these latitudes mark the northern and southern extremes in which the sun seasonally passes directly overhead. This happens annually, but in the region between, the sun passes overhead twice a year.\n\nIn the Northern Hemisphere, in the sun's apparent northward migration after the March equinox, it passes overhead once, then after the June solstice, at which time it reaches the Tropic of Cancer, it passes over again on its apparent southward journey. After the September equinox the sun passes into the Southern Hemisphere. It then passes similarly over the southern tropical regions until it reaches the Tropic of Capricorn at the December solstice, and back again as it returns northwards to the Equator.\n\nIn the two Temperate Zones also known as tropical zone not, consisting of the tepid latitudes, the Sun is never directly overhead, and the climate is mild, generally ranging from warm to cool. The four annual seasons, spring, summer, autumn and winter, occur in these areas. The North Temperate Zone includes Europe, Northern Asia, and North and Central America. The South Temperate Zone includes Southern Australasia, southern South America, and Southern Africa.\n\nThe two Frigid Zones, or polar regions, experience the midnight sun and the polar night for part of the year - at the edge of the zone there is one day at the winter solstice when the Sun is invisible, and one day at the summer solstice when the sun remains above the horizon for 24 hours. In the center of the zone (the pole) the day is one year long with six months of daylight and six months of night. The Frigid Zones are the coldest regions of Earth and are generally covered in ice and snow.It receives slanting rays of the sun as this region lies farthest from the equator. Summer season in this region lies lasts for about 2 to 3 months and there is almost 24 hour sunlight during summer.\n\nThe concept of a geographical zone was first hypothesized by the ancient Greek scholar Parmenides and lastingly modified by Aristotle. Both philosophers theorized the Earth divided into three types of climatic zones based on their distance from the equator.\n\nLike Parmeneides, thinking that the area near the equator was too hot for habitation, Aristotle dubbed the region around the equator (from 23.5° N to 23.5° S) the \"Torrid Zone.\" Both philosophers reasoned the region from the Arctic Circle to the pole to be permanently frozen. This region, thought uninhabitable, was called the \"Frigid Zone.\" The only area believed to be habitable was the northern \"Temperate Zone\" (the southern one not having been discovered), lying between the \"Frigid Zones\" and the \"Torrid Zone\". However, humans have inhabited almost all climates on Earth, including inside the Arctic Circle.\n\nAs knowledge of the Earth's geography improved, a second \"Temperate Zone\" was discovered south of the equator, and a second \"Frigid Zone\" was discovered around the Antarctic. Although Aristotle's map was oversimplified, the general idea was correct. Today, the most commonly used climate map is the Köppen climate classification, developed by Russian climatologist of German descent and amateur botanist Wladimir Köppen (1846–1940), which divides the world into five major climate regions, based on average annual precipitation, average monthly precipitation, and average monthly temperature.\n\n"}
{"id": "48120666", "url": "https://en.wikipedia.org/wiki?curid=48120666", "title": "Grayite", "text": "Grayite\n\nGrayite, ThPO • (HO), is a thorium phosphate mineral of the Rabdophane group first discovered in 1957 by S.H.U. Bowie in Rhodesia. It is of moderate hardness occurring occasionally in aggregates of hexagonal crystals occasionally but more commonly in microgranular/cryptocrystalline masses. Due to its thorium content, grayite displays some radioactivity although it is only moderate and the mineral displays powder XRD peaks without any metamict-like effects. The color of grayite is most commonly observed as a light to dark reddish brown but has also been observed as lighter yellows with grayish tints. It has a low to moderate hardness with a Mohs hardness of 3-4 and has a specific gravity of 3.7-4.3. It has been found in both intrusive igneous and sedimentary environments.\n\nFormations including grayite were originally documented in Rhodesia (now Zimbabwe) in 1957 and subsequently around the globe. Some of these locales include the states of Wyoming and Colorado as well as Madagascar. Grayite has often been found in pegmatitic environments amongst other thorium minerals, particularly monazite ((Ce,La)PO). Recent work has shown widespread occurrences in Wisconsin pegmatitic environments. Other notable finds of pegmatitic grayite occur in Bulgaria. Grayite has also been found in sedimentary environments with an observation of high concentrations in cracks raising the possibility of the mineral as a precipitate from fluid mobilized ions. Formation of grayite and other rhabdophane minerals in this context has been documented in literature.\n\nGrayite is isostructural with members of the Rhabdophane group such as brockite and rhabdrophane. While previous work has identified grayite as a pseudohexagonal orthorhombic member of the rhabdophane group along with ningyoite, more contemporary work seems to maintain a hexagonal crystal structure. These hydrated phosphate minerals often include radioactive elements such as thorium, uranium, and cerium. Powder XRD analysis produces peaks matching those of rhabdophane.\n\nIn the identification of new hydrated phosphate minerals related to rhabdophane XRD peak information is usually recorded through different sample preparation methods. Besides standard powder XRD, samples are often heated to ~850 °C so that the structure changes. The peak information is analyzed again and upon doing this hydrated thorium phosphate minerals will show a monazite-like structure indicating a possible alteration relationship.\n\n"}
{"id": "41856558", "url": "https://en.wikipedia.org/wiki?curid=41856558", "title": "Incomplete Nature", "text": "Incomplete Nature\n\nIncomplete Nature: How Mind Emerged from Matter is a 2011 book by biological anthropologist Terrence Deacon. The book covers topics in biosemiotics, philosophy of mind, and the origins of life. Broadly, the book seeks to naturalistically explain \"aboutness\", that is, concepts like intentionality, meaning, normativity, purpose, and function; which Deacon groups together and labels as ententional phenomena.\n\nDeacon's first book, \"The Symbolic Species\" focused on the evolution of human language. In that book, Deacon notes that much of the mystery surrounding language origins comes from a profound confusion on the nature of semiotic processes themselves. Accordingly, the focus of \"Incomplete Nature\" shifts from human origins to the origin of life and semiosis. \"Incomplete Nature\" can be viewed as a sizable contribution to the growing body of work positing that the problem of consciousness and the problem of the origin of life are inexorably linked. Deacon tackles these two linked problems by going back to basics. The book expands upon the classical conceptions of work and information in order to give an account of ententionality that is consistent with eliminative materialism and yet does not seek to explain away or pass off as epiphenominal the non-physical properties of life.\n\nA central thesis of the book is that absence can still be efficacious. Deacon makes the claim that just as the concept of zero revolutionized mathematics, thinking about life, mind, and other ententional phenomena in terms of constraints (i.e., what is absent) can similarly help us overcome the artificial dichotomy of the mind body problem. A good example of this concept is the hole that defines the hub of a wagon wheel. The hole itself is not a physical thing, but rather a source of constraint that helps to restrict the conformational possibilities of the wheel's components, such that, on a global scale, the property of rolling emerges. Constraints which produce emergent phenomena may not be a process which can be understood by looking at the make-up of the constituents of a pattern. Emergent phenomena are difficult to study because their complexity does not necessarily decompose into parts. When a pattern is broken down, the constraints are no longer at work; there is no hole, no absence to notice. Imagine a hub, a hole for an axle, produced only when the wheel is rolling, thus breaking the wheel may not show you how the hub emerges.\n\nDeacon notes that the apparent patterns of causality exhibited by living systems seem to be in some ways the inverse of the causal patterns of non-living systems. In an attempt to find a solution to the philosophical problems associated with teleological explanations, Deacon returns to Aristotle's four causes and attempts to modernize them with thermodynamic concepts.\n\nOrthograde changes are caused internally. They are spontaneous changes. That is, orthograde changes are generated by the spontaneous elimination of asymmetries in a thermodynamic system in disequilibrium. Because orthograde changes are driven by the internal geometry of a changing system, orthograde causes can be seen as analogous to Aristotle's formal cause. More loosely, Aristotle's final cause can also be considered orthograde, because goal oriented actions are caused from within.\n\nContragrade changes are imposed from the outside. They are non-spontaneous changes. Contragrade change is induced when one thermodynamic system interacts with the orthograde changes of another thermodynamic system. The interaction drives the first system into a higher energy, more asymmetrical state. Contragrade changes do work. Because contragrade changes are driven by external interactions with another changing system, contragrade causes can be seen as analogous to Aristotle's efficient cause.\n\nMuch of the book is devoted to expanding upon the ideas of classical thermodynamics, with an extended discussion about how consistently far from equilibrium systems can interact and combine to produce novel emergent properties. \nDeacon defines three hierarchically nested levels of thermodynamic systems: Homeodynamic systems combine to produce morphodynamic systems which combine to produce teleodynamic systems. Teleodynamic systems can be further combined to produce higher orders of self organization.\n\nHomeodynamic systems are essentially equivalent to classical thermodynamic systems like a gas under pressure or solute in solution, but the term serves to emphasize that homeodynamics is an abstract process that can be realized in forms beyond the scope of classic thermodynamics. For example, the diffuse brain activity normally associated with emotional states can be considered to be a homeodynamic system because there is a general state of equilibrium which its components (neural activity) distribute towards. In general, a homeodynamic system is any collection of components that will spontaneously eliminate constraints by rearranging the parts until a maximum entropy state (disorderliness) is achieved.\n\nA morphodynamic system consists of a coupling of two homeodynamic systems such that the constraint dissipation of each complements the other, producing macroscopic order out of microscopic interactions. Morphodynamic systems require constant perturbation to maintain their structure, so they are relatively rare in nature. The paradigm example of a morphodynamic system is a Rayleigh–Bénard cell. Other common examples are snowflake formation, whirlpools and the stimulated emission of laser light.\nMaximum entropy production: The organized structure of a morphodynamic system forms to facilitate maximal entropy production. In the case of a Rayleigh–Bénard cell, heat at the base of the liquid produces an uneven distribution of high energy molecules which will tend to diffuse towards the surface. As the temperature of the heat source increases, density effects come into play. Simple diffusion can no longer dissipate energy as fast as it is added and so the bottom of the liquid becomes hot and more buoyant than the cooler, denser liquid at the top. The bottom of the liquid begins to rise, and the top begins to sink - producing convection currents.\n\nTwo systems: The significant heat differential on the liquid produces two homeodynamic systems. The first is a diffusion system, where high energy molecules on the bottom collide with lower energy molecules on the top until the added kinetic energy from the heat source is evenly distributed. The second is a convection system, where the low density fluid on the bottom mixes with the high density fluid on the top until the density becomes evenly distributed. The second system arises when there is too much energy to be effectively dissipated by the first, and once both systems are in place, they will begin to interact.\n\nSelf organization: The convection creates currents in the fluid that disrupt the pattern of heat diffusion from bottom to top. Heat begins to diffuse into the denser areas of current, irrespective of the vertical location of these denser portions of fluid. The areas of the fluid where diffusion is occurring most rapidly will be the most viscous because molecules are rubbing against each other in opposite directions. The convection currents will shun these areas in favor of parts of the fluid where they can flow more easily. And so the fluid spontaneously segregates itself into cells where high energy, low density fluid flows up from the center of the cell and cooler, denser fluid flows down along the edges, with diffusion effects dominating in the area between the center and the edge of each cell.\n\nSynergy and constraint: What is notable about morphodynamic processes is that order spontaneously emerges explicitly because the ordered system that results is more efficient at increasing entropy than a chaotic one. In the case of the Rayleigh–Bénard cell, neither diffusion nor convection on their own will produce as much entropy as both effects coupled together. When both effects are brought into interaction, they constrain each other into a particular geometric form because that form facilitates minimal interference between the two processes. The orderly hexagonal form is stable as long as the energy differential persists, and yet the orderly form more effectively degrades the energy differential than any other form. This is why morphodynamic processes in nature are usually so short lived. They are self organizing, but also self undermining.\n\nA teleodynamic system consists of coupling two morphodynamic systems such that the self undermining quality of each is constrained by the other. Each system prevents the other from dissipating all of the energy available, and so long term organizational stability is obtained. Deacon claims that we should pinpoint the moment when two morphodynamic systems reciprocally constrain each other as the point when ententional qualities like function, purpose and normativity emerge.\n\nDeacon explores the properties of teleodynamic systems by describing a chemically plausible model system called an autogen. Deacon emphasizes that the specific autogen he describes is not a proposed description of the first life form, but rather a description of the kinds of thermodynamic synergies that the first living creature likely possessed.\n\nReciprocal catalysis: An autogen consists of two self catalyzing cyclical morphodynamic chemical reactions, similar to a chemoton. In one reaction, organic molecules react in a looped series, the products of one reaction becoming the reactants for the next. This looped reaction is self amplifying, producing more and more reactants until all the substrate is consumed. A side product of this reciprocally catalytic loop is a lipid that can be used as a reactant in a second reaction. This second reaction creates a boundary (either a microtubule or some other closed capsid like structure), that serves to contain the first reaction. The boundary limits diffusion; it keeps all of the necessary catalysts in close proximity to each other. In addition, the boundary prevents the first reaction from completely consuming all of the available substrate in the environment.\n\nThe first self: Unlike an isolated morphodynamic process whose organization rapidly eliminates the energy gradient necessary to maintain its structure, a teleodynamic process is self-limiting and self preserving. The two reactions complement each other, and ensure that neither ever runs to equilibrium - that is completion, cessation, and death. So, in a teleodynamic system there will be structures that embody a preliminary sketch of a biological function. The internal reaction network functions to create the substrates for the boundary reaction, and the boundary reaction functions to protect and constrain the internal reaction network. Either process in isolation would be abiotic but together they create a system with a normative status dependent on the functioning of its component parts.\n\nAs with other concepts in the book, in his discussion of work Deacon seeks to generalize the Newtonian conception of work such that the term can be used to describe and differentiate mental phenomena - to describe \"that which makes daydreaming effortless but metabolically equivalent problem solving difficult.\" Work is generally described as \"activity that is necessary to overcome resistance to change. Resistance can be either active or passive, and so work can be directed towards enacting change that wouldn't otherwise occur or preventing change that would happen in its absence.\" Using the terminology developed earlier in the book, work can be considered to be \"the organization of differences between orthograde processes such that a locus of contragrade process is created. Or, more simply, work is a spontaneous change inducing a non-spontaneous change to occur.\"\n\nA thermodynamic systems capacity to do work depends less upon the total energy of the system and more upon the geometric distribution of its components. A glass of water at 20 degrees Celsius will have the same amount of energy as a glass divided in half with the top fluid at 30 degrees and the bottom at 10, but only in the second glass will the top half have the capacity to do work upon the bottom. This is because work occurs at both macroscopic and microscopic levels. Microscopically, there is constant work being performed on one molecule by another when they collide. But the potential for this microscopic work to additively sum to macroscopic work depends on there being an asymmetric distribution of particle speeds, so that the average collision pushes in a focused direction. Microscopic work is necessary but not sufficient for macroscopic work. A global property of asymmetric distribution is also required.\n\nBy recognizing that asymmetry is a general property of work - that work is done as asymmetric systems spontaneously tend towards symmetry, Deacon abstracts the concept of work and applies it to systems whose symmetries are vastly more complex than those covered by classical thermodynamics. In a morphodynamic system, the tendency towards symmetry produces not global equilibrium, but a complex geometric form like a hexagonal Benard cell or the resonant frequency of a flute. This tendency towards convolutedly symmetric forms can be harnessed to do work on other morphodynamic systems, if the systems are properly coupled.\n\nResonance example: A good example of morphodynamic work is the induced resonance that can be observed by singing or playing a flute next to a string instrument like a harp or guitar. The vibrating air emitted from the flute will interact with the taut strings. If any of the strings are tuned to a resonant frequency that matches the note being played, they too will begin to vibrate and emit sound.\n\nContragrade change: When energy is added to the flute by blowing air into it, there is a spontaneous (orthograde) tendency for the system to dissipate the added energy by inducing the air within the flute to vibrate at a specific frequency. This orthograde morphodynamic form generation can be used to induce contragrade change in the system coupled to it - the taught string. Playing the flute does work on the string by causing it to enter a high energy state that could not be reached spontaneously in an uncoupled state.\n\nStructure and form: Importantly, this is not just the macro scale propagation of random micro vibrations from one system to another. The global geometric structure of the system is essential. The total energy transferred from the flute to the string matters far less than the patterns it takes in transit. That is, the amplitude of the coupled note is irrelevant, what matters is its frequency. Notes that have a higher or lower frequency than the resonant frequency of the string will not be able to do morphodynamic work.\n\nWork is generally defined to be the interaction of two orthograde changing systems such that contragrade change is produced. In teleodynamic systems, the spontaneous orthograde tendency is not to equilibriate (as in homeodynamic systems), nor to self simplify (as in morphodynamic systems) but rather to tend towards self-preservation. Living organisms spontaneously tend to heal, to reproduce and to pursue resources towards these ends. Teleodynamic work acts on these tendencies and pushes them in a contragrade, non-spontaneous direction. \nEvolution as work: Natural selection, or perhaps more accurately, adaptation, can be considered to be a ubiquitous form of teleodynamic work. The othograde self-preservation and reproduction tendencies of individual organisms tends to undermine those same tendencies in conspecifics. This competition produces a constraint that tends to mold organisms into forms that are more adapted to their environments – forms that would otherwise not spontaneously persist.\n\nFor example, in a population of New Zealand wrybill who make a living by searching for grubs under rocks, those that have a bent beak gain access to more calories. Those with bent beaks are able to better provide for their young, and at the same time they remove a disproportionate quantity of grubs from their environment, making it more difficult for those with straight beaks to provide for their own young. Throughout their lives, all the wrybills in the population do work to structure the form of the next generation. The increased efficiency of the bent beak causes that morphology to dominate the next generation. Thus an asymmetry of beak shape distribution is produced in the population - an asymmetry produced by teleodynamic work.\n\nThought as work: Mental problem solving can also be considered teleodynamic work. Thought forms are spontaneously generated, and task of problem solving is the task of molding those forms to fit the context of the problem at hand. Deacon makes the link between evolution as teleodynamic work and thought as teleodynamic work explicit. \"The experience of being sentient is what it feels like to \"be\" evolution.\"\n\nBy conceiving of work in this way, Deacon claims \"we can begin to discern \"a basis for a form of causal openness\" in the universe.\" While increases in complexity in no way alter the laws of physics, by juxtaposing systems together, pathways of spontaneous change can be made available that were inconceivably improbable prior to the systems coupling. The causal power of any complex living system lies not solely in the underlying quantum mechanics but also in the global arrangement of its components. A careful arrangement of parts can constrain possibilities such that phenomena that were formerly impossibly rare can become improbably common.\n\nOne of the central purposes of Incomplete Nature is to articulate a theory of biological information. The first formal theory of information was articulated by Claude Shannon in 1948 in his work A Mathematical Theory of Communication. Shannon's work is widely credited with ushering in the information age, but somewhat paradoxically, it was completely silent on questions of meaning and reference, i.e., what the information is \"about.\" As an engineer, Shannon was concerned with the challenge of reliably transmitting a message from one location to another. The meaning and content of the message was largely irrelevant. So, while Shannon information theory has been essential for the development of devices like computers, it has left open many philosophical questions regarding the nature of information. Incomplete Nature seeks to answer some of these questions.\n\nShannon's key insight was to recognize a link between entropy and information. Entropy is often defined as a measurement of disorder, or randomness, but this can be misleading. For Shannon's purposes, the entropy of a system is the number of possible states that the system has the capacity to be in. Any one of these potential states can constitute a message. For example, a typewritten page can bear as many different messages as there are different combinations of characters that can be arranged on the page. The information content of a message can only be understood against the background context of all of the messages that could have been sent, but weren't. Information is produced by a reduction of entropy in the message medium. \nShannon's information based conception of entropy should be distinguished from the more classic thermodynamic conception of entropy developed by Ludwig Boltzmann and others at the end of the nineteenth century. While Shannon entropy is static and has to do with the set of all possible messages/states that a signal bearing system might take, Boltzmann entropy has to do with the tendency of all dynamic systems to tend towards equilibrium. That is, there are many more ways for a collection of particles to be well mixed than to be segregated based on velocity, mass, or any other property. Boltzmann entropy is central to the theory of work developed earlier in the book because entropy dictates the direction in which a system will spontaneously tend.\n\nDeacon's addition to Shannon information theory is to propose a method for describing not just how a message is transmitted, but also how it is interpreted. Deacon weaves together Shannon entropy and Boltzmann entropy in order to develop a theory of interpretation based in teleodynamic work. Interpretation is inherently normative. Data becomes information when it has significance for its interpreter. Thus interpretive systems are teleodynamic - the interpretive process is designed to perpetuate itself. \"The interpretation of something as information indirectly reinforces the capacity to do this again.\"\n\n"}
{"id": "41566158", "url": "https://en.wikipedia.org/wiki?curid=41566158", "title": "Janet Lembke", "text": "Janet Lembke\n\nJanet Lembke (2 March 1933 – 3 September 2013), \"née\" Janet Nutt, was an American author, essayist, naturalist, translator and scholar. She was born in Cleveland, Ohio during the Great Depression, graduated in 1953 from Middlebury College, Vermont, with a degree in Classics, and her knowledge of the classical Greek and Latin worldview, from Homer to Virgil, informed her life and work. A Certified Virginia Master Gardener, she lived in Virginia and North Carolina, drawing inspiration from both locales. She was recognized for her creative view of natural cycles, agriculture and of animals, both domestic and wild, with whom we share the natural environment. Referred to as an \"acclaimed Southern naturalist,\" she was equally (as The Chicago Tribune described her) a \"classicist, a noted Oxford University Press translator of the works of Sophocles, Euripides and Aeschylus\". She received a grant from the National Endowment for the Arts to translate Virgil's Georgics, having already translated Euripides' \"Electra\" and \"Hecuba\", and Aeschylus's \"Persians\" and \"Suppliants\".\n\nJanet Lembke's first book was \"Bronze and Iron: Old Latin Poetry from Its Beginnings to 100 B.C.\" (1973), but beyond translations and essays about classics, there were more than a dozen books on nature, works for which the author acquired a base of admirers. Her articles were printed in The New York Times, \"Sierra Magazine\" (The Sierra Club), Oxford American, Audubon, Raleigh News and Observer, Southern Review and other publications. The writing style was eclectic and personal, meditative and detailed, and though she was at least once accused of \"taking poetic license too far\" in her translation of \"Georgics\", readers were often charmed and seduced by her way of weaving scientific fact, history and culture, with personal anecdote, mythological allusion and poetic feeling. \"The author's ability to pull together disparate elements in her writing is impressive, and her passionate connection with the natural world is displayed in line after line,\" wrote The New York Times. Novelist Annie Proulx expressed a similar perception, observing that \"Lembke's writing tacks between three points: the stuff of her late-twentieth-century life; the tangle of creature and plant in every dimension of tide and river flow; and the haunting, connecting wires of mythos that still knot us to the ancient beginnings.\"\n\nAmong Janet Lembke's noted titles were \"Because the Cat Purrs: How We Relate to Other Species and Why It Matters\" (2008); \"Skinny Dipping: And Other Immersions in Water, Myth, and Being Human\" (2004); \"Dangerous Birds\" (1996); \"River Time\" (1997); \"Despicable Species: On Cowbirds, Kudzu, Hornworms, and Other Scourges\" (1999); and \"The Quality of Life: Living Well, Dying Well\" (2004)-- a sober and unflinching account of the death of the author's mother. At the time of her own death at age 80 in Staunton, Virginia, Janet Lembke was working on a memoir, \"I Married an Arsonist\". She had married twice, and had four children and six grandchildren.\n\nThere is a repository of archived materials (\"The Janet Lembke Papers, 1966 - 2008\"), including notes and correspondence by the author, at the Jackson Library of the University of North Carolina in Greensboro, NC.\n\n\n\n\n"}
{"id": "183290", "url": "https://en.wikipedia.org/wiki?curid=183290", "title": "Life extension", "text": "Life extension\n\nLife extension is the idea of extending the human lifespan, either modestly – through improvements in medicine – or dramatically by increasing the maximum lifespan beyond its generally settled limit of 125 years. The ability to achieve such dramatic changes, however, does not currently exist.\n\nSome researchers in this area, and \"life extensionists\", \"immortalists\" or \"longevists\" (those who wish to achieve longer lives themselves), believe that future breakthroughs in tissue rejuvenation, stem cells, regenerative medicine, molecular repair, gene therapy, pharmaceuticals, and organ replacement (such as with artificial organs or xenotransplantations) will eventually enable humans to have indefinite lifespans (agerasia) through complete rejuvenation to a healthy youthful condition. The ethical ramifications, if life extension becomes a possibility, are debated by bioethicists.\n\nThe sale of purported anti-aging products such as supplements and hormone replacement is a lucrative global industry. For example, the industry that promotes the use of hormones as a treatment for consumers to slow or reverse the aging process in the US market generated about $50 billion of revenue a year in 2009. The use of such products has not been proven to be effective or safe.\n\nDuring the process of aging, an organism accumulates damage to its macromolecules, cells, tissues, and organs. Specifically, aging is characterized as and thought to be caused by \"genomic instability, telomere attrition, epigenetic alterations, loss of proteostasis, deregulated nutrient sensing, mitochondrial dysfunction, cellular senescence, stem cell exhaustion, and altered intercellular communication.\" Oxidation damage to cellular contents caused by free radicals is believed to contribute to aging as well.\n\nThe longest a human has ever been proven to live is 122 years, the case of Jeanne Calment who was born in 1875 and died in 1997, whereas the maximum lifespan of a wildtype mouse, commonly used as a model in research on aging, is about three years. Genetic differences between humans and mice that may account for these different aging rates include differences in efficiency of DNA repair, antioxidant defenses, energy metabolism, proteostasis maintenance, and recycling mechanisms such as autophagy.\n\nAverage lifespan in a population is lowered by infant and child mortality, which are frequently linked to infectious diseases or nutrition problems. Later in life, vulnerability to accidents and age-related chronic disease such as cancer or cardiovascular disease play an increasing role in mortality. Extension of expected lifespan can often be achieved by access to improved medical care, vaccinations, good diet, exercise and avoidance of hazards such as smoking.\n\nMaximum lifespan is determined by the rate of aging for a species inherent in its genes and by environmental factors. Widely recognized methods of extending maximum lifespan in model organisms such as nematodes, fruit flies, and mice include caloric restriction, gene manipulation, and administration of pharmaceuticals. Another technique uses evolutionary pressures such as breeding from only older members or altering levels of extrinsic mortality.\nSome animals such as hydra, planarian flatworms, and certain sponges, corals, and jellyfish do not die of old age and exhibit potential immortality.\n\nMuch life extension research focuses on nutrition—diets or supplements— although there is little evidence that they have an effect. The many diets promoted by anti-aging advocates are often contradictory. \n\nIn some studies calorie restriction has been shown to extend the life of mice, yeast, and rhesus monkeys. However, a more recent study did not find calorie restriction to improve survival in rhesus monkeys. In humans the long-term health effects of moderate caloric restriction with sufficient nutrients are unknown.\n\nThe free-radical theory of aging suggests that antioxidant supplements might extend human life. However, evidence suggest that β-carotene supplements and high doses of vitamin E increase mortality rates. Resveratrol is a sirtuin stimulant that has been shown to extend life in animal models, but the effect of resveratrol on lifespan in humans is unclear as of 2011.\n\nThe anti-aging industry offers several hormone therapies. Some of these have been criticized for possible dangers and a lack of proven effect. For example, the American Medical Association has been critical of some anti-aging hormone therapies.\n\nWhile growth hormone (GH) decreases with age, the evidence for use of growth hormone as an anti-aging therapy is mixed and based mostly on animal studies. There are mixed reports that GH or IGF-1 modulates the aging process in humans and about whether the direction of its effect is positive or negative.\n\nThe extension of life has been a desire of humanity and a mainstay motif in the history of scientific pursuits and ideas throughout history, from the Sumerian Epic of Gilgamesh and the Egyptian Smith medical papyrus, all the way through the Taoists, Ayurveda practitioners, alchemists, hygienists such as Luigi Cornaro, Johann Cohausen and Christoph Wilhelm Hufeland, and philosophers such as Francis Bacon, René Descartes, Benjamin Franklin and Nicolas Condorcet. However, the beginning of the modern period in this endeavor can be traced to the end of the 19th – beginning of the 20th century, to the so-called \"fin-de-siècle\" (end of the century) period, denoted as an \"end of an epoch\" and characterized by the rise of scientific optimism and therapeutic activism, entailing the pursuit of life extension (or life-extensionism). Among the foremost researchers of life extension at this period were the Nobel Prize winning biologist Elie Metchnikoff (1845-1916) -- the author of the cell theory of immunity and vice director of Institut Pasteur in Paris, and Charles-Édouard Brown-Séquard (1817-1894) -- the president of the French Biological Society and one of the founders of modern endocrinology.\n\nSociologist James Hughes claims that science has been tied to a cultural narrative of conquering death since the Age of Enlightenment. He cites Francis Bacon (1561–1626) as an advocate of using science and reason to extend human life, noting Bacon's novel \"New Atlantis\", wherein scientists worked toward delaying aging and prolonging life. Robert Boyle (1627–1691), founding member of the Royal Society, also hoped that science would make substantial progress with life extension, according to Hughes, and proposed such experiments as \"to replace the blood of the old with the blood of the young\". Biologist Alexis Carrel (1873–1944) was inspired by a belief in indefinite human lifespan that he developed after experimenting with cells, says Hughes.\n\nIn 1970, the American Aging Association was formed under the impetus of Denham Harman, originator of the free radical theory of aging. Harman wanted an organization of biogerontologists that was devoted to research and to the sharing of information among scientists interested in extending human lifespan.\n\nIn 1976, futurists Joel Kurtzman and Philip Gordon wrote \"No More Dying. The Conquest Of Aging And The Extension Of Human Life\", () the first popular book on research to extend human lifespan. Subsequently, Kurtzman was invited to testify before the House Select Committee on Aging, chaired by Claude Pepper of Florida, to discuss the impact of life extension on the Social Security system.\n\nSaul Kent published \"The Life Extension Revolution\" () in 1980 and created a nutraceutical firm called the Life Extension Foundation, a non-profit organization that promotes dietary supplements. The Life Extension Foundation publishes a periodical called \"Life Extension Magazine\". The 1982 bestselling book \"\" () by Durk Pearson and Sandy Shaw further popularized the phrase \"life extension\".\n\nRegulatory and legal struggles between the Food and Drug Administration (FDA) and the Life Extension Foundation included seizure of merchandise and court action. In 1991, Saul Kent and Bill Faloon, the principals of the Foundation, were jailed. The LEF accused the FDA of perpetrating a \"Holocaust\" and \"seeking gestapo-like power\" through its regulation of drugs and marketing claims.\n\nIn 2003, Doubleday published \"The Immortal Cell: One Scientist's Quest to Solve the Mystery of Human Aging,\" by Michael D. West. West emphasised the potential role of embryonic stem cells in life extension.\n\nOther modern life extensionists include writer Gennady Stolyarov, who insists that death is \"the enemy of us all, to be fought with medicine, science, and technology\"; transhumanist philosopher Zoltan Istvan, who proposes that the \"transhumanist must safeguard one's own existence above all else\"; futurist George Dvorsky, who considers aging to be a problem that desperately needs to be solved; and recording artist Steve Aoki, who has been called \"one of the most prolific campaigners for life extension\".\n\nIn 1991, the American Academy of Anti-Aging Medicine (A4M) was formed. The American Board of Medical Specialties recognizes neither anti-aging medicine nor the A4M's professional standing.\n\nIn 2003, Aubrey de Grey and David Gobel formed the Methuselah Foundation, which gives financial grants to anti-aging research projects. In 2009, de Grey and several others founded the SENS Research Foundation, a California-based scientific research organization which conducts research into aging and funds other anti-aging research projects at various universities. In 2013, Google announced Calico, a new company based in San Francisco that will harness new technologies to increase scientific understanding of the biology of aging. It is led by Arthur D. Levinson, and its research team includes scientists such as Hal V. Barron, David Botstein, and Cynthia Kenyon. In 2014, biologist Craig Venter founded Human Longevity Inc., a company dedicated to scientific research to end aging through genomics and cell therapy. They received funding with the goal of compiling a comprehensive human genotype, microbiome, and phenotype database.\n\nAside from private initiatives, aging research is being conducted in university laboratories, and includes universities such as Harvard and UCLA. University researchers have made a number of breakthroughs in extending the lives of mice and insects by reversing certain aspects of aging.\n\nPolitics relevant to the substances of life extension pertain mostly to communications and availability.\n\nIn the United States, product claims on food and drug labels are strictly regulated. The First Amendment (freedom of speech) protects third-party publishers' rights to distribute fact, opinion and speculation on life extension practices. Manufacturers and suppliers also provide informational publications, but because they market the substances, they are subject to monitoring and enforcement by the Federal Trade Commission (FTC), which polices claims by marketers. What constitutes the difference between truthful and false claims is hotly debated and is a central controversy in this arena.\n\nSome critics dispute the portrayal of aging as a disease. For example, Leonard Hayflick, who determined that fibroblasts are limited to around 50 cell divisions, reasons that aging is an unavoidable consequence of entropy. Hayflick and fellow biogerontologists Jay Olshansky and Bruce Carnes have strongly criticized the anti-aging industry in response to what they see as unscrupulous profiteering from the sale of unproven anti-aging supplements.\n\nResearch by Sobh and Martin (2011) suggests that people buy anti-aging products to obtain a hoped-for self (e.g., keeping a youthful skin) or to avoid a feared-self (e.g., looking old). The research shows that when consumers pursue a hoped-for self, it is expectations of success that most strongly drive their motivation to use the product. The research also shows why doing badly when trying to avoid a feared self is more motivating than doing well. When product use is seen to fail it is more motivating than success when consumers seek to avoid a feared-self.\n\nThough many scientists state that life extension and radical life extension are possible, there are still no international or national programs focused on radical life extension. There are political forces staying for and against life extension. By 2012, in Russia, the United States, Israel, and the Netherlands, the Longevity political parties started. They aimed to provide political support to radical life extension research and technologies, and ensure the fastest possible and at the same time soft transition of society to the next step – life without aging and with radical life extension, and to provide access to such technologies to most currently living people.\n\nSome tech innovators and Silicon Valley entrepreneurs have invested heavily into anti-aging research. This includes Larry Ellison (founder of Oracle), Peter Thiel (former Paypal CEO), Larry Page (co-founder of Google), and Peter Diamandis.\n\nLeon Kass (chairman of the US President's Council on Bioethics from 2001 to 2005) has questioned whether potential exacerbation of overpopulation problems would make life extension unethical. He states his opposition to life extension with the words:\nJohn Harris, former editor-in-chief of the Journal of Medical Ethics, argues that as long as life is worth living, according to the person himself, we have a powerful moral imperative to save the life and thus to develop and offer life extension therapies to those who want them.\n\nTranshumanist philosopher Nick Bostrom has argued that any technological advances in life extension must be equitably distributed and not restricted to a privileged few. In an extended metaphor entitled \"The Fable of the Dragon-Tyrant\", Bostrom envisions death as a monstrous dragon who demands human sacrifices. In the fable, after a lengthy debate between those who believe the dragon is a fact of life and those who believe the dragon can and should be destroyed, the dragon is finally killed. Bostrom argues that political inaction allowed many preventable human deaths to occur.\n\nControversy about life extension is due to fear of overpopulation and possible effects on society. Biogerontologist Aubrey De Grey counters the overpopulation critique by pointing out that the therapy could postpone or eliminate menopause, allowing women to space out their pregnancies over more years and thus \"decreasing\" the yearly population growth rate. Moreover, the philosopher and futurist Max More argues that, given the fact the worldwide population growth rate is slowing down and is projected to eventually stabilize and begin falling, superlongevity would be unlikely to contribute to overpopulation.\n\nA Spring 2013 Pew Research poll in the United States found that 38% of Americans would want life extension treatments, and 56% would reject it. However, it also found that 68% believed most people would want it and that only 4% consider an \"ideal lifespan\" to be more than 120 years. The median \"ideal lifespan\" was 91 years of age and the majority of the public (63%) viewed medical advances aimed at prolonging life as generally good. 41% of Americans believed that radical life extension (RLE) would be good for society, while 51% said they believed it would be bad for society. One possibility for why 56% of Americans claim they would reject life extension treatments may be due to the cultural perception that living longer would result in a longer period of decrepitude, and that the elderly in our current society are unhealthy.\n\nReligious people are no more likely to oppose life extension than the unaffiliated, though some variation exists between religious denominations.\n\nMainstream medical organizations and practitioners do not consider aging to be a disease. David Sinclair says: \"I don't see aging as a disease, but as a collection of quite predictable diseases caused by the deterioration of the body\". The two main arguments used are that aging is both inevitable and universal while diseases are not. However, not everyone agrees. Harry R. Moody, director of academic affairs for AARP, notes that what is normal and what is disease strongly depend on a historical context. David Gems, assistant director of the Institute of Healthy Ageing, argues that aging should be viewed as a disease. In response to the universality of aging, David Gems notes that it is as misleading as arguing that Basenji are not dogs because they do not bark. Because of the universality of aging he calls it a \"special sort of disease\". Robert M. Perlman, coined the terms \"aging syndrome\" and \"disease complex\" in 1954 to describe aging.\n\nThe discussion whether aging should be viewed as a disease or not has important implications. One view is, this would stimulate pharmaceutical companies to develop life extension therapies and in the United States of America, it would also increase the regulation of the anti-aging market by the FDA. Anti-aging now falls under the regulations for cosmetic medicine which are less tight than those for drugs.\n\nTheoretically, extension of maximum lifespan in humans could be achieved by reducing the rate of aging damage by periodic replacement of damaged tissues, molecular repair or rejuvenation of deteriorated cells and tissues, reversal of harmful epigenetic changes, or the enhancement of enzyme telomerase activity.\n\nResearch geared towards life extension strategies in various organisms is currently under way at a number of academic and private institutions. Since 2009, investigators have found ways to increase the lifespan of nematode worms and yeast by 10-fold; the record in nematodes was achieved through genetic engineering and the extension in yeast by a combination of genetic engineering and caloric restriction. A 2009 review of longevity research noted: \"Extrapolation from worms to mammals is risky at best, and it cannot be assumed that interventions will result in comparable life extension factors. Longevity gains from dietary restriction, or from mutations studied previously, yield smaller benefits to Drosophila than to nematodes, and smaller still to mammals. This is not unexpected, since mammals have evolved to live many times the worm's lifespan, and humans live nearly twice as long as the next longest-lived primate. From an evolutionary perspective, mammals and their ancestors have already undergone several hundred million years of natural selection favoring traits that could directly or indirectly favor increased longevity, and may thus have already settled on gene sequences that promote lifespan. Moreover, the very notion of a \"life-extension factor\" that could apply across taxa presumes a linear response rarely seen in biology.\"\n\nThere are a number of chemicals intended to slow the aging process currently being studied in animal models. One type of research is related to the observed effects of a calorie restriction (CR) diet, which has been shown to extend lifespan in some animals. Based on that research, there have been attempts to develop drugs that will have the same effect on the aging process as a caloric restriction diet, which are known as Caloric restriction mimetic drugs. Some drugs that are already approved for other uses have been studied for possible longevity effects on laboratory animals because of a possible CR-mimic effect; they include rapamycin, metformin and other geroprotectors. MitoQ, resveratrol and pterostilbene are dietary supplements that have also been studied in this context.\n\nOther attempts to create anti-aging drugs have taken different research paths. One notable direction of research has been research into the possibility of using the enzyme telomerase in order to counter the process of telomere shortening. However, there are potential dangers in this, since some research has also linked telomerase to cancer and to tumor growth and formation.\n\nFuture advances in nanomedicine could give rise to life extension through the repair of many processes thought to be responsible for aging. K. Eric Drexler, one of the founders of nanotechnology, postulated cell repair machines, including ones operating within cells and utilizing as yet hypothetical molecular computers, in his 1986 book Engines of Creation. Raymond Kurzweil, a futurist and transhumanist, stated in his book \"The Singularity Is Near\" that he believes that advanced medical nanorobotics could completely remedy the effects of aging by 2030. According to Richard Feynman, it was his former graduate student and collaborator Albert Hibbs who originally suggested to him (circa 1959) the idea of a \"medical\" use for Feynman's theoretical nanomachines (see biological machine). Hibbs suggested that certain repair machines might one day be reduced in size to the point that it would, in theory, be possible to (as Feynman put it) \"swallow the doctor\". The idea was incorporated into Feynman's 1959 essay \"There's Plenty of Room at the Bottom.\"\n\nSome life extensionists suggest that therapeutic cloning and stem cell research could one day provide a way to generate cells, body parts, or even entire bodies (generally referred to as reproductive cloning) that would be genetically identical to a prospective patient. Recently, the US Department of Defense initiated a program to research the possibility of growing human body parts on mice. Complex biological structures, such as mammalian joints and limbs, have not yet been replicated. Dog and primate brain transplantation experiments were conducted in the mid-20th century but failed due to rejection and the inability to restore nerve connections. As of 2006, the implantation of bio-engineered bladders grown from patients' own cells has proven to be a viable treatment for bladder disease. Proponents of body part replacement and cloning contend that the required biotechnologies are likely to appear earlier than other life-extension technologies.\n\nThe use of human stem cells, particularly embryonic stem cells, is controversial. Opponents' objections generally are based on interpretations of religious teachings or ethical considerations. Proponents of stem cell research point out that cells are routinely formed and destroyed in a variety of contexts. Use of stem cells taken from the umbilical cord or parts of the adult body may not provoke controversy.\n\nThe controversies over cloning are similar, except general public opinion in most countries stands in opposition to reproductive cloning. Some proponents of therapeutic cloning predict the production of whole bodies, lacking consciousness, for eventual brain transplantation.\n\nReplacement of biological (susceptible to diseases) organs with mechanical ones could extend life. This is the goal of the 2045 Initiative.\n\nFor cryonicists (advocates of cryopreservation), storing the body at low temperatures after death may provide an \"ambulance\" into a future in which advanced medical technologies may allow resuscitation and repair. They speculate cryogenic temperatures will minimize changes in biological tissue for many years, giving the medical community ample time to cure all disease, rejuvenate the aged and repair any damage that is caused by the cryopreservation process.\n\nMany cryonicists do not believe that legal death is \"real death\" because stoppage of heartbeat and breathing—the usual medical criteria for legal death—occur before biological death of cells and tissues of the body. Even at room temperature, cells may take hours to die and days to decompose. Although neurological damage occurs within 4–6 minutes of cardiac arrest, the irreversible neurodegenerative processes do not manifest for hours. Cryonicists state that rapid cooling and cardio-pulmonary support applied immediately after certification of death can preserve cells and tissues for long-term preservation at cryogenic temperatures. People, particularly children, have survived up to an hour without heartbeat after submersion in ice water. In one case, full recovery was reported after 45 minutes underwater. To facilitate rapid preservation of cells and tissue, cryonics \"standby teams\" are available to wait by the bedside of patients who are to be cryopreserved to apply cooling and cardio-pulmonary support as soon as possible after declaration of death.\n\nNo mammal has been successfully cryopreserved and brought back to life, with the exception of frozen human embryos. Resuscitation of a postembryonic human from cryonics is not possible with current science. Some scientists still support the idea based on their expectations of the capabilities of future science.\n\nAnother proposed life extension technology would combine existing and predicted future biochemical and genetic techniques. SENS proposes that rejuvenation may be obtained by removing aging damage via the use of stem cells and tissue engineering, telomere-lengthening machinery, allotopic expression of mitochondrial proteins, targeted ablation of cells, immunotherapeutic clearance, and novel lysosomal hydrolases.\n\nWhile many biogerontologists find these ideas \"worthy of discussion\" and SENS conferences feature important research in the field, some contend that the alleged benefits are too speculative given the current state of technology, referring to it as \"fantasy rather than science\".\n\nGenome editing, in which nucleic acid polymers are delivered as a drug and are either expressed as proteins, interfere with the expression of proteins, or correct genetic mutations, has been proposed as a future strategy to prevent aging.\n\nA large array of genetic modifications have been found to increase lifespan in model organisms such as yeast, nematode worms, fruit flies, and mice. As of 2013, the longest extension of life caused by a single gene manipulation was roughly 50% in mice and 10-fold in nematode worms.\n\nIn \"The Selfish Gene\", Richard Dawkins describes an approach to life-extension that involves \"fooling genes\" into thinking the body is young. Dawkins attributes inspiration for this idea to Peter Medawar. The basic idea is that our bodies are composed of genes that activate throughout our lifetimes, some when we are young and others when we are older. Presumably, these genes are activated by environmental factors, and the changes caused by these genes activating can be lethal. It is a statistical certainty that we possess more lethal genes that activate in later life than in early life. Therefore, to extend life, we should be able to prevent these genes from switching on, and we should be able to do so by \"identifying changes in the internal chemical environment of a body that take place during aging... and by simulating the superficial chemical properties of a young body\".\n\nOne hypothetical future strategy that, as some suggest, \"eliminates\" the complications related to a physical body, involves the copying or transferring (e.g. by progressively replacing neurons with transistors) of a conscious mind from a biological brain to a non-biological computer system or computational device. The basic idea is to scan the structure of a particular brain in detail, and then construct a software model of it that is so faithful to the original that, when run on appropriate hardware, it will behave in essentially the same way as the original brain. Whether or not an exact copy of one's mind constitutes actual life extension is matter of debate.\n\nSome scientists believe that the dead may one day be \"resurrected\" through simulation technology.\n\nSome clinics currently offer injection of blood products from young donors. The alleged benefits of the treatment, none of which have been demonstrated in a proper study, include a longer life, darker hair, better memory, better sleep, curing heart diseases, diabetes and Alzheimer. The approach is based on parabiosis studies such as Irina Conboy do on mice, but Conboy says young blood does not reverse aging (even in mice) and that those who offer those treatments have misunderstood her research. Neuroscientist Tony Wyss-Coray, who also studied blood exchanges on mice as recently as 2014, said people offering those treatments are \"basically abusing people's trust\" and that young blood treatments are \"the scientific equivalent of fake news\". The treatment appeared in HBO's Silicon Valley fiction series.\n\nTwo clinics in California, run by Jesse Karmazin and David C. Wright, offer $8,000 injections of plasma extracted from the blood of young people. Karmazin has not published in any peer-reviewed journal and his current study does not use a control group.\n\n\n"}
{"id": "53646845", "url": "https://en.wikipedia.org/wiki?curid=53646845", "title": "Liquid slugging", "text": "Liquid slugging\n\nLiquid slugging is the phenomenon of liquid entering the cylinder of a reciprocating compressor, a common cause of failure. Under normal conditions, the intake and output of a compressor cylinder is entirely vapor or gas, when a liquid accumulates at the suction port liquid slugging can occur. As more of the practically incompressible liquid enters, strain is placed upon the system leading to a variety of failures.\n"}
{"id": "49659014", "url": "https://en.wikipedia.org/wiki?curid=49659014", "title": "List of cat documentaries, television series and cartoons", "text": "List of cat documentaries, television series and cartoons\n\nList of cat documentaries, television series and cartoons includes \"serious\" documentaries, television series and cartoons, in alphabetical order, related to cats .\n\n\n\n"}
{"id": "249527", "url": "https://en.wikipedia.org/wiki?curid=249527", "title": "List of decorative stones", "text": "List of decorative stones\n\nThis is a geographical list of natural stone used for decorative purposes in construction and monumental sculpture produced in various countries.\n\nThe dimension-stone industry classifies stone based on appearance and hardness as either \"granite\", \"marble\" or \"slate\".\n\nThe granite of the dimension-stone industry along with truly granitic rock also includes gneiss, gabbro, anorthosite and even some sedimentary rocks.\n\nNatural stone is used as architectural stone (construction, flooring, cladding, counter tops, curbing, etc.) and as raw block and monument stone for the funerary trade. Natural stone is also used in custom stone engraving. The engraved stone can be either decorative or functional. Natural memorial stones are used as natural burial markers.\n\nMarble\n\nPakistan has more than 300 kinds of marble and natural stone types and variations:\n\nIran have more than 250 kind of Marble stone& Travertine & Onyx & Granite & limestone\nIran is one of the best countries in variety of stones in the world.\nmarble: Dehbid in several sorts .persian silk- shahyadi- namin- khoy- bastam-kashmar-miami- are only some of the Iranian Marbles\ntravertin: hajjiabad -Atashkooh- Darrehbokhari- ... are only 3kinde of IRAN Travertine\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "18012286", "url": "https://en.wikipedia.org/wiki?curid=18012286", "title": "List of herbaria", "text": "List of herbaria\n\nThis is a list of herbaria, organized first by continent where the herbarium is located, then within each continent by size of the collection. A herbarium (\"plural\" \"herbaria\") is a collection of preserved plant specimens. These specimens may be whole plants or plant parts: these will usually be in a dried form, mounted on a sheet, but depending upon the material may also be kept in alcohol or other preservative. The same term is often used in mycology to describe an equivalent collection of preserved fungi and in phycology to describe a collection of algae.\n\nTo preserve their form and color, plants collected in the field are spread flat on sheets of newsprint and dried, usually in a plant press, between blotters or absorbent paper. The specimens, which are then mounted on sheets of stiff white paper, are labeled with all essential data, such as date and place found, description of the plant, altitude, and special habitat conditions. The sheet is then placed in a protective case. As a precaution against insect attack, the pressed plant is frozen or poisoned and the case disinfected.\n\nMost herbaria utilize a standard system of organizing their specimens into herbarium cases. Specimen sheets are stacked in groups by the species to which they belong and placed into a large lightweight folder that is labelled on the bottom edge. Groups of species folders are then placed together into larger, heavier folders by genus. The genus folders are then sorted by taxonomic family according to the standard system selected for use by the herbarium and placed into pigeonholes in herbarium cabinets. Herbaria are essential for the study of plant taxonomy, the study of geographic distributions, and the stabilizing of nomenclature. Herbaria also preserve an historical record of change in vegetation over time. In some cases, plants become extinct in one area, or may become extinct altogether. In such cases, specimens preserved in an herbarium can represent the only record of the plant's original distribution. Environmental scientists make use of such data to track changes in climate and human impact.\n\n"}
{"id": "24694990", "url": "https://en.wikipedia.org/wiki?curid=24694990", "title": "List of invasive species in Europe", "text": "List of invasive species in Europe\n\nThis is a list of invasive species in Europe. A species is regarded as invasive if it has become introduced to a location, area, or region where it did not previously occur naturally (i.e., is not a native species) and becomes capable of establishing a breeding population in the new location. An invasive species will be one that thrives in its new environment and negatively influences the ecology and biodiversity of that ecosystem.\n\nThe term invasive species refers to a subset of those species defined as introduced species. If a species has been introduced but remains local, and is not problematic to agriculture or to the local biodiversity, then it cannot be considered to be invasive, and does not belong on this list.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "33429243", "url": "https://en.wikipedia.org/wiki?curid=33429243", "title": "List of types of marble", "text": "List of types of marble\n\nThe following is a list of various types of marble according to location.\n\n(NB: Marble-like stone which is not true marble according to geologists is included, but is indicated by \"italics\" and an endnote).\n\n\n\n\n\n\n\nSee webpage Dekorační kameny etc. \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThese entries are actually \"false\" marble, near-marble, or marble mis-nomers:\n\n"}
{"id": "16554664", "url": "https://en.wikipedia.org/wiki?curid=16554664", "title": "Living systems", "text": "Living systems\n\nLiving systems are open self-organizing life forms that interact with their environment. These systems are maintained by flows of information, energy and matter.\n\nSome scientists have proposed in the last few decades that a general living systems theory is required to explain the nature of life. Such a general theory, arising out of the ecological and biological sciences, attempts to map general principles for how all living systems work. Instead of examining phenomena by attempting to break things down into components, a general living systems theory explores phenomena in terms of dynamic patterns of the relationships of organisms with their environment.\n\nLiving systems theory is a general theory about the existence of all living systems, their structure, interaction, behavior and development. This work is created by James Grier Miller, which was intended to formalize the concept of life. According to Miller's original conception as spelled out in his magnum opus \"Living Systems\", a \"living system\" must contain each of twenty \"critical subsystems\", which are defined by their functions and visible in numerous systems, from simple cells to organisms, countries, and societies. In \"Living Systems\" Miller provides a detailed look at a number of systems in order of increasing size, and identifies his subsystems in each. \nMiller considers living systems as a subset of all systems. Below the level of living systems, he defines space and time, matter and energy, information and entropy, levels of organization, and physical and conceptual factors, and above living systems ecological, planetary and solar systems, galaxies, etc.\n\nLiving systems according to Parent (1996) are by definition \"open self-organizing systems that have the special characteristics of life and interact with their environment. This takes place by means of information and material-energy exchanges. Living systems can be as simple as a single cell or as complex as a supranational organization such as the European Union. Regardless of their complexity, they each depend upon the same essential twenty subsystems (or processes) in order to survive and to continue the propagation of their species or types beyond a single generation\".\n\nMiller said that systems exist at eight \"nested\" hierarchical levels: cell, organ, organism, group, organization, community, society, and supranational system. At each level, a system invariably comprises twenty critical subsystems, which process matter–energy or information except for the first two, which process both matter–energy and information: reproducer and boundary.\n\nThe processors of matter–energy are: \n\nThe processors of information are:\n\nJames Grier Miller in 1978 wrote a 1,102-page volume to present his living systems theory. He constructed a general theory of living systems by focusing on concrete systems—nonrandom accumulations of matter–energy in physical space–time organized into interacting, interrelated subsystems or components. Slightly revising the original model a dozen years later, he distinguished eight \"nested\" hierarchical levels in such complex structures. Each level is \"nested\" in the sense that each higher level contains the next lower level in a nested fashion.\n\nHis central thesis is that the systems in existence at all eight levels are open systems composed of twenty critical subsystems that process inputs, throughputs, and outputs of various forms of matter–energy and information. Two of these subsystems—reproducer and boundary—process both matter–energy and information. Eight of them process only matter–energy. The other ten process information only. \nAll nature is a continuum. The endless complexity of life is organized into patterns which repeat themselves—theme and variations—at each level of system. These similarities and differences are proper concerns for science. From the ceaseless streaming of protoplasm to the many-vectored activities of supranational systems, there are continuous flows through living systems as they maintain their highly organized steady states.\nSeppänen (1998) says that Miller applied general systems theory on a broad scale to describe all aspects of living systems.\n\nMiller's theory posits that the mutual interrelationship of the components of a system extends across the hierarchical levels. Examples: Cells and organs of a living system thrive on the food the organism obtains from its suprasystem; the member countries of a supranational system reap the benefits accrued from the communal activities to which each one contributes. Miller says that his eclectic theory \"ties together past discoveries from many disciplines and provides an outline into which new findings can be fitted\".\n\nMiller says the concepts of space, time, matter, energy, and information are essential to his theory because the living systems exist in space and are made of matter and energy organized by information. Miller's theory of living systems employs two sorts of spaces: physical or geographical space, and conceptual or abstracted spaces. Time is the fundamental \"fourth dimension\" of the physical space–time continuum/spiral. Matter is anything that has mass and occupies physical space. Mass and energy are equivalent as one can be converted into the other. Information refers to the degrees of freedom that exist in a given situation to choose among signals, symbols, messages, or patterns to be transmitted.\n\nOther relevant concepts are system, structure, process, type, level, echelon, suprasystem, subsystem, transmissions, and steady state. A system can be conceptual, concrete or abstracted. The structure of a system is the arrangement of the subsystems and their components in three-dimensional space at any point of time. Process, which can be reversible or irreversible, refers to change over time of matter–energy or information in a system. Type defines living systems with similar characteristics. Level is the position in a hierarchy of systems. Many complex living systems, at various levels, are organized into two or more echelons. The suprasystem of any living system is the next higher system in which it is a subsystem or component. The totality of all the structures in a system which carry out a particular process is a subsystem. Transmissions are inputs and outputs in concrete systems. Because living systems are open systems, with continually altering fluxes of matter–energy and information, many of their equilibria are dynamic—situations identified as steady states or flux equilibria.\n\nMiller identifies the comparable matter–energy and information processing critical subsystems. Elaborating on the eight hierarchical levels, he defines society, which constitutes the seventh hierarchy, as \"a large, living, concrete system with [community] and lower am levels of living systems as subsystems and components\". Society may include small, primitive, totipotential communities; ancient city–states, and kingdoms; as well as modern nation–states and empires that are not supranational systems. Miller provides general descriptions of each of the subsystems that fit all eight levels.\n\nA supranational system, in Miller's view, \"is composed of two or more societies, some or all of whose processes are under the control of a decider that is superordinate to their highest echelons\". However, he contends that no supranational system with all its twenty subsystems under control of its decider exists today. The absence of a supranational decider precludes the existence of a concrete supranational system. Miller says that studying a supranational system is problematical because its subsystems\n...tend to consist of few components besides the decoder. These systems do little matter-energy processing. The power of component societies [nations] today is almost always greater than the power of supranational deciders. Traditionally, theory at this level has been based upon intuition and study of history rather than data collection. Some quantitative research is now being done, and construction of global-system models and simulations is currently burgeoning.\n\nAt the supranational system level, Miller's emphasis is on international organizations, associations, and groups comprising representatives of societies (nation–states). Miller identifies the subsystems at this level to suit this emphasis. Thus, for example, the reproducer is \"any multipurpose supranational system which creates a single purpose supranational organization\" (p. 914); and the boundary is the \"supranational forces, usually located on or near supranational borders, which defend, guard, or police them\" (p. 914).\n\nNot just those specialized in international communication, but all communication science scholars could pay particular attention to the major contributions of living systems theory (LST) to social systems approaches that Bailey has pointed out:\n\nBailey says that LST, perhaps the \"most integrative\" social systems theory, has made many more contributions that may be easily overlooked, such as: providing a detailed analysis of types of systems; making a distinction between concrete and abstracted systems; discussion of physical space and time; placing emphasis on information processing; providing an analysis of entropy; recognition of totipotential systems, and partipotential systems; providing an innovative approach to the structure–process issue; and introducing the concept of joint subsystem—a subsystem that belongs to two systems simultaneously; of dispersal—lateral, outward, upward, and downward; of inclusion—inclusion of something from the environment that is not part of the system; of artifact—an animal-made or human-made inclusion; of adjustment process, which combats stress in a system; and of critical subsystems, which carry out processes that all living systems need to survive.\n\nLST's analysis of the twenty interacting subsystems, Bailey adds, clearly distinguishing between matter–energy-processing and information-processing, as well as LST's analysis of the eight interrelated system levels, enables us to understand how social systems are linked to biological systems. LST also analyzes the irregularities or \"organizational pathologies\" of systems functioning (e.g., system stress and strain, feedback irregularities, information–input overload). It explicates the role of entropy in social research while it equates negentropy with information and order. It emphasizes both structure and process, as well as their interrelations.\n\nIt omits the analysis of subjective phenomena, and it overemphasizes concrete Q-analysis (correlation of objects) to the virtual exclusion of R-analysis (correlation of variables). By asserting that societies (ranging from totipotential communities to nation-states and non-supranational systems) have greater control over their subsystem components than supranational systems have, it dodges the issue of transnational power over the contemporary social systems. Miller's supranational system bears no resemblance to the modern world-system that Immanuel Wallerstein (1974) described, although both of them were looking at the same living (dissipative) structure.\n\n\n\n"}
{"id": "1904053", "url": "https://en.wikipedia.org/wiki?curid=1904053", "title": "Marxism", "text": "Marxism\n\nMarxism is a method of socioeconomic analysis that views class relations and social conflict using a materialist interpretation of historical development and takes a dialectical view of social transformation. It originates from the works of 19th-century German philosophers Karl Marx and Friedrich Engels.\n\nMarxism uses a methodology, now known as historical materialism, to analyze and critique the development of capitalism and the role of class struggles in systemic economic change.\n\nAccording to Marxist theory, class conflict arises in capitalist societies due to contradictions between the material interests of the oppressed proletariat—a class of wage labourers employed to produce goods and services—and the bourgeoisie—the ruling class that owns the means of production and extract their wealth through appropriation of the surplus product (profit) produced by the proletariat.\n\nThis class struggle that is commonly expressed as the revolt of a society's productive forces against its relations of production, results in a period of short-term crises as the bourgeoisie struggle to manage the intensifying alienation of labor experienced by the proletariat, albeit with varying degrees of class consciousness. This crisis culminates in a proletarian revolution and eventually leads to the establishment of socialism—a socioeconomic system based on social ownership of the means of production, distribution based on one's contribution and production organized directly for use. As the productive forces continued to advance, Marx hypothesized that socialism would ultimately transform into a communist society; a classless, stateless, humane society based on common ownership and the underlying principle: \"From each according to his ability, to each according to his needs\".\n\nMarxism has developed into many different branches and schools of thought, though now there is no single definitive Marxist theory. Different Marxian schools place a greater emphasis on certain aspects of classical Marxism while rejecting or modifying other aspects. Many schools of thought have sought to combine Marxian concepts and non-Marxian concepts, which has then led to contradictory conclusions. However, lately there is movement toward the recognition that historical materialism and dialectical materialism remains the fundamental aspect of all Marxist schools of thought, which should result in more agreement between different schools.\n\nMarxism has had a profound and influential impact on global academia and has expanded into many fields such as archaeology, anthropology, media studies, political science, theater, history, sociology, art history and theory, cultural studies, education, economics, ethics, criminology, geography, literary criticism, aesthetics, film theory, critical psychology and philosophy.\n\nThe term \"Marxism\" was popularized by Karl Kautsky, who considered himself an \"orthodox\" Marxist during the dispute between the orthodox and revisionist followers of Marx. Kautsky's revisionist rival Eduard Bernstein also later adopted use of the term. Engels did not support the use of the term \"Marxism\" to describe either Marx's or his views. Engels claimed that the term was being abusively used as a rhetorical qualifier by those attempting to cast themselves as \"real\" followers of Marx while casting others in different terms, such as \"Lassallians\". In 1882, Engels claimed that Marx had criticized self-proclaimed \"Marxist\" Paul Lafargue, by saying that if Lafargue's views were considered \"Marxist\", then \"one thing is certain and that is that I am not a Marxist\".\n\nMarxism analyzes the material conditions and the economic activities required to fulfill human material needs to explain social phenomena within any given society.\n\nIt assumes that the form of economic organization, or mode of production, influences all other social phenomena—including social relations, political institutions, legal systems, cultural systems, aesthetics, and ideologies. The economic system and these social relations form a base and superstructure.\n\nAs forces of production, i.e. technology, improve, existing forms of social organization become obsolete and hinder further progress. As Karl Marx observed: \"At a certain stage of development, the material productive forces of society come into conflict with the existing relations of production or—this merely expresses the same thing in legal terms—with the property relations within the framework of which they have operated hitherto. From forms of development of the productive forces these relations turn into their fetters. Then begins an era of social revolution\". These inefficiencies manifest themselves as social contradictions in society in the form of class struggle.\n\nUnder the capitalist mode of production, this struggle materializes between the minority (the bourgeoisie) who own the means of production and the vast majority of the population (the proletariat) who produce goods and services. Starting with the conjectural premise that social change occurs because of the struggle between different classes within society who are under contradiction against each other, a Marxist would conclude that capitalism exploits and oppresses the proletariat, therefore capitalism will inevitably lead to a proletarian revolution.\n\nMarxian economics and its proponents view capitalism as economically unsustainable and incapable of improving the living standards of the population due to its need to compensate for falling rates of profit by cutting employee's wages, social benefits and pursuing military aggression. The socialist system would succeed capitalism as humanity's mode of production through workers' revolution. According to Marxian crisis theory, socialism is not an inevitability, but an economic necessity.\n\nIn a socialist society, private property—in the form of the means of production—would be replaced by co-operative ownership. A socialist economy would not base production on the creation of private profits, but on the criteria of satisfying human needs—that is, production would be carried out directly for use. As Friedrich Engels said: \"Then the capitalist mode of appropriation in which the product enslaves first the producer, and then appropriator, is replaced by the mode of appropriation of the product that is based upon the nature of the modern means of production; upon the one hand, direct social appropriation, as means to the maintenance and extension of production on the other, direct individual appropriation, as means of subsistence and of enjoyment\".\n\nThe materialist theory of history analyses the underlying causes of societal development and change from the perspective of the collective ways that humans make their living. All constituent features of a society (social classes, political pyramid, ideologies) are assumed to stem from economic activity, an idea often portrayed with the metaphor of the base and superstructure.\n\nThe base and superstructure metaphor describes the totality of social relations by which humans produce and re-produce their social existence. According to Marx: \"The sum total of the forces of production accessible to men determines the condition of society\" and forms a society's economic base. The base includes the material forces of production, that is the labour and material means of production and relations of production, i.e., the social and political arrangements that regulate production and distribution. From this base rises a superstructure of legal and political \"forms of social consciousness\" of political and legal institutions that derive from the economic base that conditions the superstructure and a society's dominant ideology. Conflicts between the development of material productive forces and the relations of production provokes social revolutions and thus the resultant changes to the economic base will lead to the transformation of the superstructure. This relationship is reflexive, as at first the base gives rise to the superstructure and remains the foundation of a form of social organization, hence that formed social organization can act again upon both parts of the base and superstructure so that the relationship is not static but a dialectic, expressed and driven by conflicts and contradictions. As Engels clarified: \"The history of all hitherto existing society is the history of class struggles. Freeman and slave, patrician and plebeian, lord and serf, guild-master and journeyman, in a word, oppressor and oppressed, stood in constant opposition to one another, carried on uninterrupted, now hidden, now open fight, a fight that each time ended, either in a revolutionary reconstitution of society at large, or in the common ruin of the contending classes\".\n\nMarx considered class conflicts as the driving force of human history since these recurring conflicts have manifested themselves as distinct transitional stages of development in Western Europe. Accordingly, Marx designated human history as encompassing four stages of development in relations of production:\n\nAccording to the Marxist theoretician and revolutionary Vladimir Lenin, \"the principal content of Marxism\" was \"Marx's economic doctrine\". Marx believed that the capitalist bourgeois and their economists were promoting what he saw as the lie that \"the interests of the capitalist and of the worker are ... one and the same\", therefore he believed that they did this by purporting the concept that \"the fastest possible growth of productive capital\" was best not only for the wealthy capitalists but also for the workers because it provided them with employment.\n\nExploitation is a matter of surplus labour—the amount of labour one performs beyond what one receives in goods. Exploitation has been a socioeconomic feature of every class society and is one of the principal features distinguishing the social classes. The power of one social class to control the means of production enables its exploitation of the other classes.\n\nIn capitalism, the labour theory of value is the operative concern; the value of a commodity equals the socially necessary labour time required to produce it. Under that condition, surplus value (the difference between the value produced and the value received by a labourer) is synonymous with the term \"surplus labour\", thus capitalist exploitation is realised as deriving surplus value from the worker.\n\nIn pre-capitalist economies, exploitation of the worker was achieved via physical coercion. In the capitalist mode of production, that result is more subtly achieved and because workers do not own the means of production, they must voluntarily enter into an exploitive work relationship with a capitalist in order to earn the necessities of life. The worker's entry into such employment is voluntary in that they choose which capitalist to work for. However, the worker must work or starve, thus exploitation is inevitable and the \"voluntary\" nature of a worker participating in a capitalist society is illusory.\nAlienation is the estrangement of people from their humanity ( \"Gattungswesen\", \"species-essence\", \"species-being\"), which is a systematic result of capitalism. Under capitalism, the fruits of production belong to the employers, who expropriate the surplus created by others and so generate alienated labourers. In Marx's view, alienation is an objective characterization of the worker's situation in capitalism—his or her self-awareness of this condition is not prerequisite.\n\nMarx distinguishes social classes on the basis of two criteria: ownership of means of production and control over the labour power of others. Following this criterion of class based on property relations, Marx identified the social stratification of the capitalist mode of production with the following social groups:\n\nClass consciousness denotes the awareness—of itself and the social world—that a social class possesses and its capacity to rationally act in their best interests, hence class consciousness is required before they can effect a successful revolution and thus the dictatorship of the proletariat.\n\nWithout defining ideology, Marx used the term to describe the production of images of social reality. According to Engels, \"ideology is a process accomplished by the so-called thinker consciously, it is true, but with a false consciousness. The real motive forces impelling him remain unknown to him; otherwise it simply would not be an ideological process. Hence he imagines false or seeming motive forces\". Because the ruling class controls the society's means of production, the superstructure of society (the ruling social ideas), are determined by the best interests of the ruling class. In \"The German Ideology\", he says \"[t]he ideas of the ruling class are in every epoch the ruling ideas, i.e., the class which is the ruling material force of society, is, at the same time, its ruling intellectual force.\"\n\nThe term \"political economy\" initially referred to the study of the material conditions of economic production in the capitalist system. In Marxism, political economy is the study of the means of production, specifically of capital and how that manifests as economic activity.\nThis new way of thinking was invented because socialists believed that common ownership of the \"means of production\" (that is the industries, the land, the wealth of nature, the trade apparatus, the wealth of the society, etc.) will abolish the exploitative working conditions experienced under capitalism. Through working class revolution, the state (which Marxists see as a weapon for the subjugation of one class by another) is seized and used to suppress the hitherto ruling class of capitalists and by implementing a commonly-owned, democratically controlled workplace create the society of communism, which Marxists see as true democracy. An economy based on co-operation on human need and social betterment, rather than competition for profit of many independently acting profit seekers, would also be the end of class society, which Marx saw as the fundamental division of all hitherto existing history.\n\nMarx saw work, the effort by humans to transform the environment for their needs, as a fundamental feature of human kind. Capitalism, in which the product of the worker's labor is taken from them and sold at market rather than being part of the worker's life, is therefore alienating to the worker. Additionally, the worker is compelled by various means (some nicer than others) to work harder, faster and for longer hours. While this is happening, the employer is constantly trying to save on labor costs: pay the workers less, figure out how to use cheaper equipment, etc. This allows the employer to extract the largest mount of work (and therefore potential wealth) from their workers. The fundamental nature of capitalist society is no different from that of slave society: one small group of society exploiting the larger group.\n\nThrough common ownership of the means of production, the profit motive is eliminated and the motive of furthering human flourishing is introduced. Because the surplus produced by the workers is property of the society as whole, there are no classes of producers and appropriators. Additionally, the state, which has its origins in the bands of retainers hired by the first ruling classes to protect their economic privilege, will disappear as its conditions of existence have disappeared.\n\nAccording to orthodox Marxist theory, the overthrow of capitalism by a socialist revolution in contemporary society is inevitable. While the inevitability of an eventual socialist revolution is a controversial debate among many different Marxist schools of thought, all Marxists believe socialism is a necessity, if not inevitable. Marxists believe that a socialist society is far better for the majority of the populace than its capitalist counterpart. Prior to the Russian revolution of 1917, Lenin wrote: \"The socialization of production is bound to lead to the conversion of the means of production into the property of society ... This conversion will directly result in an immense increase in productivity of labour, a reduction of working hours, and the replacement of the remnants, the ruins of small-scale, primitive, disunited production by collective and improved labour\". The failure of the 1905 revolution and the failure of socialist movements to resist the outbreak of World War One led to renewed theoretical effort and valuable contributions from Lenin and Rosa Luxemburg towards an appreciation of Marx's crisis theory and efforts to formulate a theory of imperialism.\n\n\"Classical Marxism\" denotes the collection of socio-eco-political theories expounded by Karl Marx and Friedrich Engels. \"Marxism\", as Ernest Mandel remarked, \"is always open, always critical, always self-critical\". As such, classical Marxism distinguishes between \"Marxism\" as broadly perceived and \"what Marx believed\", thus in 1883 Marx wrote to the French labour leader Jules Guesde and to Marx's son-in-law Paul Lafargue—both of whom claimed to represent Marxist principles—accusing them of \"revolutionary phrase-mongering\" and of denying the value of reformist struggle.\n\nFrom Marx's letter derives the paraphrase:\"If that is Marxism, then I am not a Marxist\". American Marxist scholar Hal Draper responded to this comment by saying: \"There are few thinkers in modern history whose thought has been so badly misrepresented, by Marxists and anti-Marxists alike\". On the other hand, the book \"Communism: The Great Misunderstanding\" argues that the source of such misrepresentations lies in ignoring the philosophy of Marxism, which is dialectical materialism. In large, this was due to the fact that \"The German Ideology\", in which Marx and Engels developed this philosophy, did not find a publisher for almost one hundred years.\n\nMarxism has been adopted by a large number of academics and other scholars working in various disciplines.\n\nThe theoretical development of Marxist archaeology was first developed in the Soviet Union in 1929, when a young archaeologist named Vladislav I. Ravdonikas (1894–1976) published a report entitled \"For a Soviet history of material culture\". Within this work, the very discipline of archaeology as it then stood was criticised as being inherently bourgeois, therefore anti-socialist and so, as a part of the academic reforms instituted in the Soviet Union under the administration of Premier Joseph Stalin, a great emphasis was placed on the adoption of Marxist archaeology throughout the country. These theoretical developments were subsequently adopted by archaeologists working in capitalist states outside of the Leninist bloc, most notably by the Australian academic V. Gordon Childe (1892–1957), who used Marxist theory in his understandings of the development of human society.\n\nMarxist sociology is the study of sociology from a Marxist perspective. Marxist sociology is \"a form of conflict theory associated with ... Marxism's objective of developing a positive (empirical) science of capitalist society as part of the mobilization of a revolutionary working class\". The American Sociological Association has a section dedicated to the issues of Marxist sociology that is \"interested in examining how insights from Marxist methodology and Marxist analysis can help explain the complex dynamics of modern society\". Influenced by the thought of Karl Marx, Marxist sociology emerged during the end of the 19th and beginning of the 20th century. As well as Marx, Max Weber and Émile Durkheim are considered seminal influences in early sociology. The first Marxist school of sociology was known as Austro-Marxism, of which Carl Grünberg and Antonio Labriola were among its most notable members. During the 1940s, the Western Marxist school became accepted within Western academia, subsequently fracturing into several different perspectives such as the Frankfurt School or critical theory. Due to its former state-supported position, there has been a backlash against Marxist thought in post-communist states (see sociology in Poland) but it remains dominant in the sociological research sanctioned and supported by those communist states that remain (see sociology in China).\n\nMarxian economics refers to a school of economic thought tracing its foundations to the critique of classical political economy first expounded upon by Karl Marx and Friedrich Engels. Marxian economics concerns itself with the analysis of crisis in capitalism, the role and distribution of the surplus product and surplus value in various types of economic systems, the nature and origin of economic value, the impact of class and class struggle on economic and political processes, and the process of economic evolution. Although the Marxian school is considered heterodox, ideas that have come out of Marxian economics have contributed to mainstream understanding of the global economy. Certain concepts of Marxian economics, especially those related to capital accumulation and the business cycle, such as creative destruction, have been fitted for use in capitalist systems.\n\nMarxist historiography is a school of historiography influenced by Marxism. The chief tenets of Marxist historiography are the centrality of social class and economic constraints in determining historical outcomes. Marxist historiography has made contributions to the history of the working class, oppressed nationalities, and the methodology of history from below. Friedrich Engels' most important historical contribution was \"Der deutsche Bauernkrieg\" (\"The German Peasants' War\"), which analysed social warfare in early Protestant Germany in terms of emerging capitalist classes. \"The German Peasants' War\" indicate the Marxist interest in history from below and class analysis, and attempts a dialectical analysis. Engels' short treatise \"The Condition of the Working Class in England in 1844\" (1870s) was salient in creating the socialist impetus in British politics. Marx's most important works on social and political history include \"The Eighteenth Brumaire of Louis Napoleon\", \"The Communist Manifesto\", \"The German Ideology\", and those chapters of \"Das Kapital\" dealing with the historical emergence of capitalists and proletarians from pre-industrial English society. Marxist historiography suffered in the Soviet Union, as the government requested overdetermined historical writing. Notable histories include the \"Short Course History of the Communist Party of the Soviet Union (Bolshevik)\", published in the 1930s to justify the nature of Bolshevik party life under Joseph Stalin. A circle of historians inside the Communist Party of Great Britain (CPGB) formed in 1946. While some members of the group (most notably Christopher Hill and E. P. Thompson) left the CPGB after the 1956 Hungarian Revolution, the common points of British Marxist historiography continued in their works. Thompson's \"The Making of the English Working Class\" is one of the works commonly associated with this group. Eric Hobsbawm's \"Bandits\" is another example of this group's work. C. L. R. James was also a great pioneer of the 'history from below' approach. Living in Britain when he wrote his most notable work \"The Black Jacobins\" (1938), he was an anti-Stalinist Marxist and so outside of the CPGB. In India, B. N. Datta and D. D. Kosambi are considered the founding fathers of Marxist historiography. Today, the senior-most scholars of Marxist historiography are R. S. Sharma, Irfan Habib, Romila Thapar, D. N. Jha and K. N. Panikkar, most of whom are now over 75 years old.\n\nMarxist literary criticism is a loose term describing literary criticism based on socialist and dialectic theories. Marxist criticism views literary works as reflections of the social institutions from which they originate. According to Marxists, even literature itself is a social institution and has a specific ideological function, based on the background and ideology of the author. Notable marxist literary critics include Mikhail Bakhtin, Walter Benjamin, Terry Eagleton and Fredric Jameson. Marxist aesthetics is a theory of aesthetics based on, or derived from, the theories of Karl Marx. It involves a dialectical and materialist, or dialectical materialist, approach to the application of Marxism to the cultural sphere, specifically areas related to taste such as art, beauty, etc. Marxists believe that economic and social conditions, and especially the class relations that derive from them, affect every aspect of an individual's life, from religious beliefs to legal systems to cultural frameworks. Some notable Marxist aestheticians include Anatoly Lunacharsky, Mikhail Lifshitz, William Morris, Theodor W. Adorno, Bertolt Brecht, Herbert Marcuse, Walter Benjamin, Antonio Gramsci, Georg Lukács, Louis Althusser, Jacques Rancière, Maurice Merleau-Ponty and Raymond Williams.\n\nAccording to a 2007 survey of American professors by Neil Gross and Solon Simmons, 17.6% of social science professors and 5.0% of humanities professors identify as Marxists, while between 0 and 2% of professors in all other disciplines identify as Marxists.\n\nKarl Marx (5 May 1818 – 14 March 1883) was a German philosopher, political economist and socialist revolutionary who addressed the matters of alienation and exploitation of the working class, the capitalist mode of production and historical materialism. He is famous for analysing history in terms of class struggle, summarised in the initial line introducing \"The Communist Manifesto\" (1848): \"The history of all hitherto existing society is the history of class struggles\".\n\nFriedrich Engels (28 November 1820 – 5 August 1895) was a German political philosopher who together with Marx co-developed communist theory. Marx and Engels first met in September 1844. Discovering that they had similar views of philosophy and socialism, they collaborated and wrote works such as \"Die heilige Familie\" (\"The Holy Family\"). After Marx was deported from France in January 1845, they moved to Belgium, which then permitted greater freedom of expression than other European countries. In January 1846, they returned to Brussels to establish the Communist Correspondence Committee.\n\nIn 1847, they began writing \"The Communist Manifesto\" (1848), based on Engels' \"The Principles of Communism\". Six weeks later, they published the 12,000-word pamphlet in February 1848. In March, Belgium expelled them and they moved to Cologne, where they published the \"Neue Rheinische Zeitung\", a politically radical newspaper. By 1849, they had to leave Cologne for London. The Prussian authorities pressured the British government to expel Marx and Engels, but Prime Minister Lord John Russell refused.\n\nAfter Marx's death in 1883, Engels became the editor and translator of Marx's writings. With his \"Origins of the Family, Private Property, and the State\" (1884) – analysing monogamous marriage as guaranteeing male social domination of women, a concept analogous, in communist theory, to the capitalist class's economic domination of the working class—Engels made intellectually significant contributions to feminist theory and Marxist feminism.\n\nIn 1959, the Cuban Revolution led to the victory of Fidel Castro and his July 26 Movement. Although the revolution was not explicitly socialist, upon victory Castro ascended to the position of Prime Minister and adopted the Leninist model of socialist development, forging an alliance with the Soviet Union. One of the leaders of the revolution, the Argentine Marxist revolutionary Che Guevara (1928–1967), subsequently went on to aid revolutionary socialist movements in Congo-Kinshasa and Bolivia, eventually being killed by the Bolivian government, possibly on the orders of the Central Intelligence Agency (CIA), though the CIA agent sent to search for Guevara, Felix Rodriguez, expressed a desire to keep him alive as a possible bargaining tool with the Cuban government. He would posthumously go on to become an internationally recognised icon.\n\nIn the People's Republic of China, the Maoist government undertook the Cultural Revolution from 1966 through to 1976 to ameliorate capitalist elements of Chinese society and achieve socialism. However, upon Mao Zedong's death, his rivals seized political power and under the Premiership of Deng Xiaoping (1978–1992), many of Mao's Cultural Revolution era policies were revised or abandoned and much of the state sector privatised.\n\nThe late 1980s and early 1990s saw the collapse of most of those socialist states that had professed a Marxist–Leninist ideology. In the late 1970s and early 1980s, the emergence of the New Right and neoliberal capitalism as the dominant ideological trends in western politics—championed by U.S. President Ronald Reagan and U.K. Prime Minister Margaret Thatcher—led the west to take a more aggressive stand against the Soviet Union and its Leninist allies. Meanwhile, in the Soviet Union the reformist Mikhael Gorbachev became Premier in March 1985 and sought to abandon Leninist models of development towards social democracy. Ultimately, Gorbachev's reforms, coupled with rising levels of popular ethnic nationalism in the Soviet Union, led to the state's dissolution in late 1991 into a series of constituent nations, all of which abandoned Marxist–Leninist models for socialism, with most converting to capitalist economies.\n\nAt the turn of the 21st century, China, Cuba, Laos, North Korea and Vietnam remained the only officially Marxist–Leninist states remaining, although a Maoist government led by Prachanda was elected into power in Nepal in 2008 following a long guerrilla struggle.\n\nThe early 21st century also saw the election of socialist governments in several Latin American nations, in what has come to be known as the \"pink tide\". Dominated by the Venezuelan government of Hugo Chávez, this trend also saw the election of Evo Morales in Bolivia, Rafael Correa in Ecuador and Daniel Ortega in Nicaragua. Forging political and economic alliances through international organisations like the Bolivarian Alliance for the Americas, these socialist governments allied themselves with Marxist–Leninist Cuba and although none of them espoused a Leninist path directly, most admitted to being significantly influenced by Marxist theory.\n\nFor Italian Marxist Gianni Vattimo in his 2011 book \"Hermeneutic Communism\", \"this new weak communism differs substantially from its previous Soviet (and current Chinese) realization, because the South American countries follow democratic electoral procedures and also manage to decentralize the state bureaucratic system through the Bolivarian missions. In sum, if weakened communism is felt as a specter in the West, it is not only because of media distortions but also for the alternative it represents through the same democratic procedures that the West constantly professes to cherish but is hesitant to apply\".\n\nChinese President Xi Jinping has announced a deepening commitment of the Chinese Communist Party to the ideas of Marx. At an event celebrating the 200th anniversary of Marx's birth, Xi said “We must win the advantages, win the initiative, and win the future. We must continuously improve the ability to use Marxism to analyse and solve practical problems...” also adding “powerful ideological weapon for us to understand the world, grasp the law, seek the truth, and change the world,”. Xi has further stressed the importance of examining and continuing the tradition of the CPC and embrace its revolutionary past.\n\nCriticisms of Marxism have come from various political ideologies and academic disciplines. These include general criticisms about lack of internal consistency, criticisms related to historical materialism, that it is a type of historical determinism, the necessity of suppression of individual rights, issues with the implementation of communism and economic issues such as the distortion or absence of price signals and reduced incentives. In addition, empirical and epistemological problems are frequently identified.\n\nSome Marxists have criticised the academic institutionalisation of Marxism for being too shallow and detached from political action. For instance, Zimbabwean Trotskyist Alex Callinicos, himself a professional academic, stated: \"Its practitioners remind one of Narcissus, who in the Greek legend fell in love with his own reflection ... Sometimes it is necessary to devote time to clarifying and developing the concepts that we use, but indeed for Western Marxists this has become an end in itself. The result is a body of writings incomprehensible to all but a tiny minority of highly qualified scholars\".\n\nAdditionally, there are intellectual critiques of Marxism that contest certain assumptions prevalent in Marx's thought and Marxism after him, without exactly rejecting Marxist politics. Other contemporary supporters of Marxism argue that many aspects of Marxist thought are viable, but that the corpus is incomplete or outdated in regards to certain aspects of economic, political or social theory. They may therefore combine some Marxist concepts with the ideas of other theorists such as Max Weber—the Frankfurt School is one example.\n\nPhilosopher and historian of ideas Leszek Kołakowski pointed out that \"Marx's theory is incomplete or ambiguous in many places, and could be 'applied' in many contradictory ways without manifestly infringing its principles\". Specifically, he considers \"the laws of dialectics\" as fundamentally erroneous, stating that some are \"truisms with no specific Marxist content\", others \"philosophical dogmas that cannot be proved by scientific means\" and some just \"nonsense\". He believes that some Marxist laws can be interpreted differently, but that these interpretations still in general fall into one of the two categories of error.\n\nOkishio's theorem shows that if capitalists use cost-cutting techniques and real wages do not increase, the rate of profit must rise, which casts doubt on Marx's view that the rate of profit would tend to fall.\n\nThe allegations of inconsistency have been a large part of Marxian economics and the debates around it since the 1970s. Andrew Kliman argues that this undermines Marx's critiques and the correction of the alleged inconsistencies, because internally inconsistent theories cannot be right by definition.\n\nMarx's predictions have been criticized because they have allegedly failed, with some pointing towards the GDP per capita increasing generally in capitalist economies compared to less market oriented economics, the capitalist economies not suffering worsening economic crises leading to the overthrow of the capitalist system and communist revolutions not occurring in the most advanced capitalist nations, but instead in undeveloped regions.\n\nIn his books \"The Poverty of Historicism\" and \"Conjectures and Refutations\", philosopher of science Karl Popper, criticized the explanatory power and validity of historical materialism. Popper believed that Marxism had been initially scientific, in that Marx had postulated a genuinely predictive theory. When these predictions were not in fact borne out, Popper argues that the theory avoided falsification by the addition of ad hoc hypotheses that made it compatible with the facts. Because of this, Popper asserted, a theory that was initially genuinely scientific degenerated into pseudoscientific dogma.\n\nDemocratic socialists and social democrats reject the idea that socialism can be accomplished only through extra-legal class conflict and a proletarian revolution. The relationship between Marx and other socialist thinkers and organizations—rooted in Marxism's \"scientific\" and anti-utopian socialism, among other factors—has divided Marxists from other socialists since Marx's life.\n\nAfter Marx's death and with the emergence of Marxism, there have also been dissensions within Marxism itself—a notable example is the splitting of the Russian Social Democratic Labour Party into Bolsheviks and Mensheviks. Orthodox Marxists became opposed to a less dogmatic, more innovative, or even revisionist Marxism.\n\nAnarchism has had a strained relationship with Marxism since Marx's life. Anarchists and many non-Marxist libertarian socialists reject the need for a transitory state phase, claiming that socialism can only be established through decentralized, non-coercive organization. Anarchist Mikhail Bakunin criticized Marx for his authoritarian bent. The phrases \"barracks socialism\" or \"barracks communism\" became a shorthand for this critique, evoking the image of citizens' lives being as regimented as the lives of conscripts in a barracks. Noam Chomsky is critical of Marxism's dogmatic strains and the idea of Marxism itself, but still appreciates Marx's contributions to political thought. Unlike some anarchists, Chomsky does not consider Bolshevism \"Marxism in practice\", but he does recognize that Marx was a complicated figure who had conflicting ideas, while he also acknowledges the latent authoritarianism in Marx he also points to the libertarian strains that developed into the council communism of Rosa Luxemburg and Anton Pannekoek. However, his commitment to libertarian socialism has led him to characterize himself as an anarchist with radical Marxist leanings (see political positions of Noam Chomsky).\n\nLibertarian Marxism refers to a broad scope of economic and political philosophies that emphasize the anti-authoritarian aspects of Marxism. Early currents of libertarian Marxism, known as left communism, emerged in opposition to Marxism–Leninism and its derivatives such as Stalinism, Ceaușism and Maoism. Libertarian Marxism is also often critical of reformist positions, such as those held by social democrats. Libertarian Marxist currents often draw from Marx and Engels' later works, specifically the \"Grundrisse\" and \"The Civil War in France\", emphasizing the Marxist belief in the ability of the working class to forge its own destiny without the need for a revolutionary party or state to mediate or aid its liberation. Along with anarchism, libertarian Marxism is one of the main currents of libertarian socialism.\n\nOther critiques come from an economic standpoint. Vladimir Karpovich Dmitriev writing in 1898, Ladislaus von Bortkiewicz writing in 1906–1907 and subsequent critics have alleged that Marx's value theory and law of the tendency of the rate of profit to fall are internally inconsistent. In other words, the critics allege that Marx drew conclusions that actually do not follow from his theoretical premises. Once these alleged errors are corrected, his conclusion that aggregate price and profit are determined by and equal to aggregate value and surplus value no longer holds true. This result calls into question his theory that the exploitation of workers is the sole source of profit.\n\nBoth Marxism and socialism have received considerable critical analysis from multiple generations of Austrian economists in terms of scientific methodology, economic theory and political implications. During the marginal revolution, subjective value theory was rediscovered by Carl Menger, a development that fundamentally undermined the British cost theories of value. The restoration of subjectivism and praxeological methodology previously used by classical economists including Richard Cantillon, Anne-Robert-Jacques Turgot, Jean-Baptiste Say and Frédéric Bastiat led Menger to criticise historicist methodology in general. Second-generation Austrian economist Eugen Böhm von Bawerk used praxeological and subjectivist methodology to attack the law of value fundamentally. Non-Marxist economists have regarded his criticism as definitive, with Gottfried Haberler arguing that Böhm-Bawerk's critique of Marx's economics was so thorough and devastating that as of the 1960s no Marxian scholar had conclusively refuted it. Third-generation Austrian Ludwig von Mises rekindled debate about the economic calculation problem by identifying that without price signals in capital goods, all other aspects of the market economy are irrational. This led him to declare that \"rational economic activity is impossible in a socialist commonwealth\".\n\nDaron Acemoglu and James A. Robinson argue that Marx's economic theory was fundamentally flawed because it attempted to simplify the economy into a few general laws that ignored the impact of institutions on the economy.\n\n\n"}
{"id": "19770", "url": "https://en.wikipedia.org/wiki?curid=19770", "title": "Memetics", "text": "Memetics\n\nMemetics (also referred to colloquially as memeology) is the study of information and culture based on an analogy with Darwinian evolution. Proponents describe memetics as an approach to evolutionary models of cultural information transfer. Critics regard memetics as a pseudoscience. Memetics describes how an idea can propagate successfully, but doesn't necessarily imply a concept is factual.\n\nThe term meme was coined in Richard Dawkins' 1976 book \"The Selfish Gene,\" but Dawkins later distanced himself from the resulting field of study. Analogous to a gene, the meme was conceived as a \"unit of culture\" (an idea, belief, pattern of behaviour, etc.) which is \"hosted\" in the minds of one or more individuals, and which can reproduce itself in the sense of jumping from the mind of one person to the mind of another. Thus what would otherwise be regarded as one individual influencing another to adopt a belief is seen as an idea-replicator reproducing itself in a new host. As with genetics, particularly under a Dawkinsian interpretation, a meme's success may be due to its contribution to the effectiveness of its host.\n\nThe Usenet newsgroup alt.memetics started in 1993 with peak posting years in the mid to late 1990s. The \"Journal of Memetics\" was published electronically from 1997 to 2005.\n\nIn his book \"The Selfish Gene\" (1976), the evolutionary biologist Richard Dawkins used the term \"meme\" to describe a unit of human cultural transmission analogous to the gene, arguing that replication also happens in culture, albeit in a different sense. Bella Hiscock outlined a similar hypothesis in 1975, which Dawkins referenced. Cultural evolution itself is a much older topic, with a history that dates back at least as far as Darwin's era.\n\nDawkins (1976) proposed that the meme is a unit of information residing in the brain and is the mutating replicator in human cultural evolution. It is a pattern that can influence its surroundings – that is, it has causal agency – and can propagate. This proposal resulted in debate among sociologists, biologists, and scientists of other disciplines. Dawkins himself did not provide a sufficient explanation of how the replication of units of information in the brain controls human behaviour and ultimately culture, and the principal topic of the book was genetics. Dawkins apparently did not intend to present a comprehensive theory of \"memetics\" in \"The Selfish Gene\", but rather coined the term \"meme\" in a speculative spirit. Accordingly, different researchers came to define the term \"unit of information\" in different ways.\n\nThe modern memetics movement dates from the mid-1980s. A January 1983 \"Metamagical Themas\" column by Douglas Hofstadter, in \"Scientific American\", was influential – as was his 1985 book of the same name. \"Memeticist\" was coined as analogous to \"geneticist\" – originally in \"The Selfish Gene.\" Later Arel Lucas suggested that the discipline that studies memes and their connections to human and other carriers of them be known as \"memetics\" by analogy with \"genetics\". Dawkins' \"The Selfish Gene\" has been a factor in attracting the attention of people of disparate intellectual backgrounds. Another stimulus was the publication in 1991 of \"Consciousness Explained\" by Tufts University philosopher Daniel Dennett, which incorporated the meme concept into a theory of the mind. In his 1991 essay \"Viruses of the Mind\", Richard Dawkins used memetics to explain the phenomenon of religious belief and the various characteristics of organised religions. By then, memetics had also become a theme appearing in fiction (e.g. Neal Stephenson's \"Snow Crash\").\n\nThe idea of \"language as a virus\" had already been introduced by William S. Burroughs as early as 1962 in his book \"The Ticket That Exploded\", and later in \"The Electronic Revolution\", published in 1970 in \"\". Douglas Rushkoff explored the same concept in \"Media Virus: Hidden Agendas in Popular Culture\" in 1995.\n\nHowever, the foundation of memetics in its full modern incarnation originated in the publication in 1996 of two books by authors outside the academic mainstream: \"Virus of the Mind: The New Science of the Meme\" by former Microsoft executive turned motivational speaker and professional poker-player, Richard Brodie, and \"Thought Contagion: How Belief Spreads Through Society\" by Aaron Lynch, a mathematician and philosopher who worked for many years as an engineer at Fermilab. Lynch claimed to have conceived his theory totally independently of any contact with academics in the cultural evolutionary sphere, and apparently was not even aware of Dawkins' \"The Selfish Gene\" until his book was very close to publication.\n\nAround the same time as the publication of the books by Lynch and Brodie the e-journal Journal of Memetics – \"Evolutionary Models of Information Transmission\" appeared on the web. It was first hosted by the Centre for Policy Modelling at Manchester Metropolitan University but later taken over by Francis Heylighen of the CLEA research institute at the Vrije Universiteit Brussel. The e-journal soon became the central point for publication and debate within the nascent memeticist community. (There had been a short-lived paper-based memetics publication starting in 1990, the \"Journal of Ideas\" edited by Elan Moritz.) In 1999, Susan Blackmore, a psychologist at the University of the West of England, published \"The Meme Machine\", which more fully worked out the ideas of Dennett, Lynch, and Brodie and attempted to compare and contrast them with various approaches from the cultural evolutionary mainstream, as well as providing novel, and controversial, memetics-based theories for the evolution of language and the human sense of individual selfhood.\n\nThe term \"meme\" derives from the Ancient Greek μιμητής (\"mimētḗs\"), meaning \"imitator, pretender\". The similar term \"mneme\" was used in 1904, by the German evolutionary biologist Richard Semon, best known for his development of the engram theory of memory, in his work \"Die mnemischen Empfindungen in ihren Beziehungen zu den Originalempfindungen\", translated into English in 1921 as \"The Mneme\" . Until Daniel Schacter published \"Forgotten Ideas, Neglected Pioneers: Richard Semon and the Story of Memory\" in 2000, Semon's work had little influence, though it was quoted extensively in Erwin Schrödinger’s prescient 1956 Tarner Lecture “Mind and Matter”. Richard Dawkins (1976) apparently coined the word \"meme\" independently of Semon, writing this:\n\"'Mimeme' comes from a suitable Greek root, but I want a monosyllable that sounds a bit like 'gene'. I hope my classicist friends will forgive me if I abbreviate mimeme to meme. If it is any consolation, it could alternatively be thought of as being related to 'memory', or to the French word même.\" \n\nIn 2005, the \"Journal of Memetics – Evolutionary Models of Information Transmission\" ceased publication and published a set of articles on the future of memetics. The website states that although \"there was to be a relaunch...after several years nothing has happened\". Susan Blackmore has left the University of the West of England to become a freelance science-writer and now concentrates more on the field of consciousness and cognitive science. Derek Gatherer moved to work as a computer programmer in the pharmaceutical industry, although he still occasionally publishes on memetics-related matters. Richard Brodie is now climbing the world professional poker rankings. Aaron Lynch disowned the memetics community and the words \"meme\" and \"memetics\" (without disowning the ideas in his book), adopting the self-description \"thought contagionist\". He died in 2005.\n\nSusan Blackmore (2002) re-stated the definition of meme as: whatever is copied from one person to another person, whether habits, skills, songs, stories, or any other kind of information. Further she said that memes, like genes, are replicators in the sense as defined by Dawkins.\nThat is, they are information that is copied. Memes are copied by imitation, teaching and other methods. The copies are not perfect: memes are copied with variation; moreover, they compete for space in our memories and for the chance to be copied again. Only some of the variants can survive. The combination of these three elements (copies; variation; competition for survival) forms precisely the condition for Darwinian evolution, and so memes (and hence human cultures) evolve. Large groups of memes that are copied and passed on together are called co-adapted meme complexes, or \"memeplexes\". In Blackmore's definition, the way that a meme replicates is through imitation. This requires brain capacity to generally imitate a model or selectively imitate the model. Since the process of social learning varies from one person to another, the imitation process cannot be said to be completely imitated. The sameness of an idea may be expressed with different memes supporting it. This is to say that the mutation rate in memetic evolution is extremely high, and mutations are even possible within each and every iteration of the imitation process. It becomes very interesting when we see that a social system composed of a complex network of microinteractions exists, but at the macro level an order emerges to create culture.\n\nThe memetics movement split almost immediately into two. The first group were those who wanted to stick to Dawkins' definition of a meme as \"a unit of cultural transmission\". Gibron Burchett, another memeticist responsible for helping to research and co-coin the term memetic engineering, along with Leveious Rolando and Larry Lottman, has stated that a meme can be defined, more precisely, as \"a unit of cultural information that can be copied, located in the brain\". This thinking is more in line with Dawkins' second definition of the meme in his book \"The Extended Phenotype\". The second group wants to redefine memes as observable cultural artifacts and behaviors. However, in contrast to those two positions, Blackmore does not reject either concept of external or internal memes.\n\nThese two schools became known as the \"internalists\" and the \"externalists.\" Prominent internalists included both Lynch and Brodie; the most vocal externalists included Derek Gatherer, a geneticist from Liverpool John Moores University, and William Benzon, a writer on cultural evolution and music. The main rationale for externalism was that internal brain entities are not observable, and memetics cannot advance as a science, especially a quantitative science, unless it moves its emphasis onto the directly quantifiable aspects of culture. Internalists countered with various arguments: that brain states will eventually be directly observable with advanced technology, that most cultural anthropologists agree that culture is about beliefs and not artifacts, or that artifacts cannot be replicators in the same sense as mental entities (or DNA) are replicators. The debate became so heated that a 1998 Symposium on Memetics, organised as part of the 15th International Conference on Cybernetics, passed a motion calling for an end to definitional debates. McNamara demonstrated in 2011 that functional connectivity profiling using neuroimaging tools enables the observation of the processing of internal memes, \"i-memes\", in response to external \"e-memes\".\n\nAn advanced statement of the internalist school came in 2002 with the publication of \"The Electric Meme\", by Robert Aunger, an anthropologist from the University of Cambridge. Aunger also organised a conference in Cambridge in 1999, at which prominent sociologists and anthropologists were able to give their assessment of the progress made in memetics to that date. This resulted in the publication of \"Darwinizing Culture: The Status of Memetics as a Science\", edited by Aunger and with a foreword by Dennett, in 2001.\n\nThis evolutionary model of cultural information transfer is based on the concept that units of information, or \"memes\", have an independent existence, are self-replicating, and are subject to selective evolution through environmental forces. Starting from a proposition put forward in the writings of Richard Dawkins, this model has formed the basis of a new area of study, one that looks at the self-replicating units of culture. It has been proposed that just as memes are analogous to genes, memetics is analogous to genetics.\n\nCritics contend that some proponents' assertions are \"untested, unsupported or incorrect.\" Luis Benitez-Bribiesca, a critic of memetics, calls it \"a pseudoscientific dogma\" and \"a dangerous idea that poses a threat to the serious study of consciousness and cultural evolution\" among other things. As factual criticism, he refers to the lack of a \"code script\" for memes, as the DNA is for genes, and to the fact that the meme mutation mechanism (i.e., an idea going from one brain to another) is too unstable (low replication accuracy and high mutation rate), which would render the evolutionary process chaotic. This, however, has been demonstrated (e.g. by Daniel C. Dennett, in \"Darwin's Dangerous Idea\") to not be the case, in fact, due to the existence of self-regulating correction mechanisms (vaguely resembling those of gene transcription) enabled by the redundancy and other properties of most meme expression languages, which do stabilize information transfer. (E.g. spiritual narratives—including music and dance forms—can survive in full detail across any number of generations even in cultures with oral tradition only.) Memes for which stable copying methods are available will inevitably get selected for survival more often than those which can only have unstable mutations, therefore going extinct. (Notably, Benitez-Bribiesca's claim of \"no code script\" is also irrelevant, considering the fact that there is nothing preventing the information contents of memes from being coded, encoded, expressed, preserved or copied in all sorts of different ways throughout their life-cycles.)\n\nAnother criticism comes from semiotics, (e.g., Deacon, Kull) stating that the concept of meme is a primitivized concept of Sign. Meme is thus described in memetics as a sign without its triadic nature. In other words, meme is a degenerate sign, which includes only its ability of being copied. Accordingly, in the broadest sense, the objects of copying are memes, whereas the objects of translation and interpretation are signs.\nMary Midgley criticises memetics for at least two reasons: \"One, culture is not best understood by examining its smallest parts, as culture is pattern-like, comparable to an ocean current. Many more factors, historical and others, should be taken into account than only whatever particle culture is built from. Two, if memes are not thoughts (and thus not cognitive phenomena), as Daniel C. Dennett insists in \"Darwin's Dangerous Idea\", then their ontological status is open to question, and memeticists (who are also reductionists) may be challenged whether memes even exist. Questions can extend to whether the idea of \"meme\" is itself a meme, or is a true concept. Fundamentally, memetics is an attempt to produce knowledge through organic metaphors, which as such is a questionable research approach, as the application of metaphors has the effect of hiding that which does not fit within the realm of the metaphor. Rather than study actual reality, without preconceptions, memetics, as so many of the socio-biological explanations of society, believe that saying that the apple is like an orange is a valid analysis of the apple.\"\n\nHenry Jenkins, Joshua Green, and Sam Ford, in their book \"Spreadable Media\" (2013), criticize Dawkins' idea of the meme, writing that \"while the idea of the meme is a compelling one, it may not adequately account for how content circulates through participatory culture.\" The three authors also criticize other interpretations of memetics, especially those which describe memes as \"self-replicating\", because they ignore the fact that \"culture is a human product and replicates through human agency.\"\n\nLike other critics, Maria Kronfeldner has criticized memetics for being based on an allegedly inaccurate analogy with the gene; alternately, she claims it is \"heuristically trivial\", being a mere redescription of what is already known without offering any useful novelty.\n\nDawkins in \"A Devil's Chaplain\" responded that there are actually two different types of memetic processes (controversial and informative). The first is a type of cultural idea, action, or expression, which does have high variance; for instance, a student of his who had inherited some of the mannerisms of Wittgenstein. However, he also describes a self-correcting meme, highly resistant to mutation. As an example of this, he gives origami patterns in elementary schools – except in rare cases, the meme is either passed on in the exact sequence of instructions, or (in the case of a forgetful child) terminates. This type of meme tends not to evolve, and to experience profound mutations in the rare event that it does.\n\nAnother definition, given by Hokky Situngkir, tried to offer a more rigorous formalism for the meme, \"memeplexes\", and the \"deme\", seeing the meme as a cultural unit in a cultural complex system. It is based on the Darwinian genetic algorithm with some modifications to account for the different patterns of evolution seen in genes and memes. In the method of memetics as the way to see culture as a complex adaptive system, he describes a way to see memetics as an alternative methodology of cultural evolution. However, there are as many possible definitions that are credited to the word \"meme\". For example, in the sense of computer simulation the term \"memetic algorithm\" is used to define a particular computational viewpoint.\n\nThe possibility of quantitative analysis of memes using neuroimaging tools and the suggestion that such studies have already been done was given by McNamara (2011). This author proposes hyperscanning (concurrent scanning of two communicating individuals in two separate MRI machines) as a key tool in the future for investigating memetics.\n\nVelikovsky (2013) proposed the \"holon\" as the structure of the meme, synthesizing the major theories on memes of Richard Dawkins, Mihaly Csikszentmihalyi, E. O. Wilson, Frederick Turner (poet) and Arthur Koestler.\n\nProponents of memetics as described in the Journal of Memetics (out of print since 2005 ) – \"Evolutionary Models of Information Transmission\" believe that 'memetics' has the potential to be an important and promising analysis of culture using the framework of evolutionary concepts. \nKeith Henson in \"Memetics and the Modular-Mind\" (Analog Aug. 1987) makes the case that memetics needs to incorporate evolutionary psychology to understand the psychological traits of a meme's host. This is especially true of time-varying, meme-amplification host-traits, such as those leading to wars.\n\nDiCarlo () has developed the idea of 'memetic equilibrium' to describe a cultural compatible state with biological equilibrium. In \"Problem Solving and Neurotransmission in the Upper Paleolithic\" (in press), diCarlo argues that as human consciousness evolved and developed, so too did our ancestors' capacity to consider and attempt to solve environmental problems in more conceptually sophisticated ways. Understood in this way, problem solving amongst a particular group, when considered satisfactory, often produces a feeling of environmental control, stability, in short—memetic equilibrium. \nBut the pay-off is not merely practical, providing purely functional utility—it is biochemical and it comes in the form of neurotransmitters. The relationship between a gradually emerging conscious awareness and sophisticated languages in which to formulate representations combined with the desire to maintain biological equilibrium, generated the necessity for equilibrium to fill in conceptual gaps in terms of understanding three very important aspects in the Upper Paleolithic: causality, morality, and mortality. The desire to explain phenomena in relation to maintaining survival and reproductive stasis, generated a normative stance in the minds of our ancestors—Survival/Reproductive Value (or S-R Value).\n\nHouben (2014) has argued on several occasions that the exceptional resilience of Vedic ritual and its interaction with a changing ecological and economic environment over several millennia can be profitably dealt with in a ‘cultural evolution’ perspective in which the Vedic mantra is the ‘meme’ or unit of cultural replication.\nThis renders superfluous attempts to explain the phenomenon of Vedic tradition in genetic terms. The domain of Vedic ritual should be able to fulfil to a large extent the three challenges posed to memetics by B. Edmonds (2002 and 2005).\n\nResearch methodologies that apply memetics go by many names: Viral marketing, cultural evolution, the history of ideas, social analytics, and more. Many of these applications do not make reference to the literature on memes directly but are built upon the evolutionary lens of idea propagation that treats semantic units of culture as self-replicating and mutating patterns of information that are assumed to be relevant for scientific study. For example, the field of public relations is filled with attempts to introduce new ideas and alter social discourse. One means of doing this is to design a meme and deploy it through various media channels. One historic example of applied memetics is the PR campaign conducted in 1991 as part of the build-up to the first Gulf War in the United States.\n\nThe application of memetics to a difficult complex social system problem, environmental sustainability, has recently been attempted at thwink.org Using meme types and memetic infection in several stock and flow simulation models, Jack Harich has demonstrated several interesting phenomena that are best, and perhaps only, explained by memes. One model, The Dueling Loops of the Political Powerplace, argues that the fundamental reason corruption is the norm in politics is due to an inherent structural advantage of one feedback loop pitted against another. Another model, The Memetic Evolution of Solutions to Difficult Problems, uses memes, the evolutionary algorithm, and the scientific method to show how complex solutions evolve over time and how that process can be improved. The insights gained from these models are being used to engineer memetic solution elements to the sustainability problem.\n\nAnother application of memetics in the sustainability space is the crowdfunded Climate Meme Project conducted by Joe Brewer and Balazs Laszlo Karafiath in the spring of 2013. This study was based on a collection of 1000 unique text-based expressions gathered from Twitter, Facebook, and structured interviews with climate activists. The major finding was that the global warming meme is not effective at spreading because it causes emotional duress in the minds of people who learn about it. Five central tensions were revealed in the discourse about [climate change], each of which represents a resonance point through which dialogue can be engaged. The tensions were Harmony/Disharmony (whether or not humans are part of the natural world), Survival/Extinction (envisioning the future as either apocalyptic collapse of civilization or total extinction of the human race), Cooperation/Conflict (regarding whether or not humanity can come together to solve global problems), Momentum/Hesitation (about whether or not we are making progress at the collective scale to address climate change), and Elitism/Heretic (a general sentiment that each side of the debate considers the experts of its opposition to be untrustworthy).\n\nBen Cullen, in his book \"Contagious Ideas\", brought the idea of the meme into the discipline of archaeology. He coined the term \"Cultural Virus Theory\", and used it to try to anchor archaeological theory in a neo-Darwinian paradigm. Archaeological memetics could assist the application of the meme concept to material culture in particular.\n\nFrancis Heylighen of the Center Leo Apostel for Interdisciplinary Studies has postulated what he calls \"memetic selection criteria\". These criteria opened the way to a specialized field of \"applied memetics\" to find out if these selection criteria could stand the test of quantitative analyses. In 2003 Klaas Chielens carried out these tests in a Masters thesis project on the testability of the selection criteria.\n\nIn \"Selfish Sounds and Linguistic Evolution\", Austrian linguist Nikolaus Ritt has attempted to operationalise memetic concepts and use them for the explanation of long term sound changes and change conspiracies in early English. It is argued that a generalised Darwinian framework for handling cultural change can provide explanations where established, speaker centred approaches fail to do so. The book makes comparatively concrete suggestions about the possible material structure of memes, and provides two empirically rich case studies.\n\nAustralian academic S.J. Whitty has argued that project management is a memeplex with the language and stories of its practitioners at its core. This radical approach sees a project and its management as an illusion; a human construct about a collection of feelings, expectations, and sensations, which are created, fashioned, and labeled by the human brain. Whitty's approach requires project managers to consider that the reasons for using project management are not consciously driven to maximize profit, and are encouraged to consider project management as naturally occurring, self-serving, evolving process which shapes organizations for its own purpose.\n\nSwedish political scientist Mikael Sandberg argues against \"Lamarckian\" interpretations of institutional and technological evolution and studies creative innovation of information technologies in governmental and private organizations in Sweden in the 1990s from a memetic perspective. Comparing the effects of active (\"Lamarckian\") IT strategy versus user–producer interactivity (Darwinian co-evolution), evidence from Swedish organizations shows that co-evolutionary interactivity is almost four times as strong a factor behind IT creativity as the \"Lamarckian\" IT strategy.\n\n\n\n"}
{"id": "4349679", "url": "https://en.wikipedia.org/wiki?curid=4349679", "title": "Metaphysical naturalism", "text": "Metaphysical naturalism\n\nMetaphysical naturalism (also called ontological naturalism, philosophical naturalism, scientific materialism and antisupernaturalism) is a philosophical worldview, which holds that there is nothing but natural elements, principles, and relations of the kind studied by the natural sciences. Methodological naturalism is a philosophical basis for science, for which metaphysical naturalism provides only one possible ontological foundation. Broadly, the corresponding theological perspective is religious naturalism or spiritual naturalism. More specifically, metaphysical naturalism rejects the supernatural concepts and explanations that are part of many religions.\n\nAccording to Steven Schafersman, geologist and president of Texas Citizens for Science, metaphysical naturalism is a philosophy that maintains that; 1. Nature encompasses all that exists throughout space and time; and 2. Nature (the universe or cosmos) consists only of natural elements, that is, of spatiotemporal physical substance—mass–energy. Non-physical or quasi-physical substance, such as information, ideas, values, logic, mathematics, intellect, and other emergent phenomena, either supervene upon the physical or can be reduced to a physical account; and 3. Nature operates by the laws of physics and in principle, can be explained and understood by science and philosophy; and 4. the supernatural does not exist, i.e., only nature is real. Naturalism is therefore a metaphysical philosophy opposed primarily by Biblical creationism\".\n\nCarl Sagan put it succinctly: \"The Cosmos is all that is or ever was or ever will be.\"\n\nAccording to Arthur C. Danto, naturalism, in recent usage, is a species of philosophical monism according to which whatever exists or happens is \"natural\" in the sense of being susceptible to explanation through methods which, although paradigmatically exemplified in the natural sciences, are continuous from domain to domain of objects and events. Hence, naturalism is polemically defined as repudiating the view that there exists or could exist any entities which lie, in principle, beyond the scope of scientific explanation.\n\nRegarding the vagueness of the general term \"naturalism\", David Papineau traces the current usage to philosophers in early 20th century America such as John Dewey, Ernest Nagel, Sidney Hook, and Roy Wood Sellars: \"So understood, 'naturalism' is not a particularly informative term as applied to contemporary philosophers. The great majority of contemporary philosophers would happily accept naturalism as just characterized—that is, they would both reject 'supernatural' entities, and allow that science is a possible route (if not necessarily the only one) to important truths about the 'human spirit'\". Papineau remarks that philosophers widely regard naturalism as a \"positive\" term, and \"few active philosophers nowadays are happy to announce themselves as 'non-naturalists'\", while noting that \"philosophers concerned with religion tend to be less enthusiastic about 'naturalism'\" and that despite an \"inevitable\" divergence due to its popularity, if more narrowly construed, (to the chagrin of John McDowell, David Chalmers and Jennifer Hornsby, for example), those not so disqualified remain nonetheless content \"to set the bar for 'naturalism' higher\".\n\nPhilosopher and theologian Alvin Plantinga, a well-known critic of naturalism in general, comments: \"Naturalism is presumably not a religion. In one very important respect, however, it resembles religion: it can be said to perform the cognitive function of a religion. There is that range of deep human questions to which a religion typically provides an answer ... Like a typical religion, naturalism gives a set of answers to these and similar questions\".\n\nMetaphysical naturalism is an approach to metaphysics or ontology, which deals with existence \"per se\". It should not be confused with methodological naturalism, which sees empiricism as the basis for the scientific method.\n\nRegarding science and evolution, Eugenie C. Scott, a notable opponent of teaching creationism or intelligent design in US public schools, stresses the importance of separating metaphysical from methodological naturalism:\n\nThe historian Richard Carrier, in his book \"Sense and Goodness without God: A Defense of Metaphysical Naturalism,\" describes metaphysical naturalism thus: as a philosophy \"wherein worship is replaced with curiosity, devotion with diligence, holiness with sincerity, ritual with study, and scripture with the whole world and the whole of human learning\". Carrier wrote that it is the naturalist’s duty \"to question all things and have a well-grounded faith in what is well-investigated and well-proved, rather than what is merely well-asserted or well-liked\".\n\nMetaphysical naturalism is the philosophical basis of science as described by Kate and Vitaly (2000) \"There are certain philosophical assumptions made at the base of the scientific method — namely, 1) that reality is objective and consistent, 2) that humans have the capacity to perceive reality accurately, and that 3) rational explanations exist for elements of the real world. These assumptions are the basis of naturalism, the philosophy on which science is grounded. Philosophy is at least implicitly at the core of every decision we make or position we take, it is obvious that correct philosophy is a necessity for scientific inquiry to take place.\" Steven Schafersman, agrees that methodological naturalism is \"the adoption or assumption of philosophical naturalism within scientific method with or without fully accepting or believing it ... science is not metaphysical and does not depend on the ultimate truth of any metaphysics for its success, but methodological naturalism must be adopted as a strategy or working hypothesis for science to succeed. We may therefore be agnostic about the ultimate truth of naturalism, but must nevertheless adopt it and investigate nature as if nature is all that there is.\"\n\nContrary to other notable opponents of teaching Creationism or Intelligent Design in US public schools such as Eugenie Scott, Schafersman asserts that \"while science as a process only requires methodological naturalism, I think that the assumption of methodological naturalism by scientists and others logically and morally entails ontological naturalism\". as well as the similarly controversial assertion: \"I maintain that the practice or adoption of methodological naturalism entails a logical and moral belief in ontological naturalism, so they are not logically decoupled.\" On the other hand, Scott argues:\n\nContemporary naturalists possess a wide diversity of beliefs within metaphysical naturalism. Most metaphysical naturalists have adopted some form of materialism or physicalism.\n\nMetaphysical naturalists argue that the scientific facts and theories that we have to explain the origins of the universe provide no evidence for supernatural beings or deities. As Richard Carrier explains:\n...no other worldview is directly and substantially supported by any scientific evidence, whereas all scientific evidence so far does support Metaphysical Naturalism, often directly, sometimes substantially. Though naturalism has not yet been proved, it is the best bet going.\n\nOne might say that either it has always existed or it had a purely natural origin, being neither created nor designed.\n\nSince nature is all there is, and there was once no life, abiogenesis is implied: that life arose spontaneously from natural causes. Naturalists reason about \"how\", not \"if\" evolution happened. They maintain that humanity's existence is not by intelligent design but rather a natural process of emergence.\n\nSome embrace virtue ethics and many see no compelling argument against ethical naturalism. Some may advocate for a Science of morality. One example of an attempt to ground a naturalist meta-ethical system is Richard Carrier's chapter \"Moral Facts Naturally Exist (and Science Could Find Them)\" which was peer reviewed by four philosophers. It sets out to prove a Moral realism centered around human satisfaction. Alexander Rosenberg has expressed a contrary position that naturalists, in general, have to accept moral nihilism.\n\nIf any variety of metaphysical naturalism is true, any mental properties that exist are caused by and ontologically dependent upon nature.\n\nMetaphysical naturalists do not believe in a soul or spirit, nor in ghosts, and when explaining what constitutes the mind they rarely appeal to substance dualism. If one's mind, or rather one's identity and existence as a person, is entirely the product of natural processes, three conclusions follow according to W.T. Stace. First, all mental contents (such as ideas, theories, emotions, moral and personal values, or aesthetic response) exist solely as computational constructions of one's brain and genetics, not as things that exist independently of these. Second, damage to the brain (regardless of how) should be of great concern. Third, death or destruction of one's brain cannot be survived, which is to say, all humans are mortal. Stace, however, believes that ecstatic mysticism calls into question the assumption that awareness is impossible without data processing.\n\nMetaphysical naturalists hold that reason is the refinement and improvement of naturally evolved faculties. The certitude of deductive logic remains unexplained by this essentially probabilistic view. Nevertheless, naturalists believe anyone who wishes to have more beliefs that are true than are false should seek to perfect and consistently employ their reason in testing and forming beliefs. Empirical methods (especially those of proven use in the sciences) are unsurpassed for discovering the facts of reality, while methods of pure reason alone can securely discover logical errors.\n\nHumans are social animals, which is why humanity developed culture and civilization. In terms of evolution, this means that differential reproductive success somehow depended on traits that permit the development and maintenance of a healthy and productive culture and civilization.\n\nNaturalism was the foundation of two (Vaisheshika, Nyaya) of the six orthodox schools and one (Carvaka) heterodox school of Hinduism. The Carvaka, Nyaya, Vaisheshika schools originated in the 7th, 6th, and 2nd century BCE, respectively.\n\nWestern metaphysical naturalism has originated in ancient Greek philosophy. The earliest pre-Socratic philosophers, especially the Milesians (Thales, Anaximander, and Anaximenes) and the atomists (Leucippus and Democritus), were labeled by their peers and successors \"the \"physikoi\"\" (from the Greek φυσικός or \"physikos\", meaning \"natural philosopher,\" borrowing on the word φύσις or \"physis\", meaning \"nature\") because they investigated natural causes, often excluding any role for gods in the creation or operation of the world. This eventually led to fully developed systems such as Epicureanism, which sought to explain everything that exists as the product of atoms falling and swerving in a void.\n\nAristotle surveyed the thought of his predecessors and conceived of nature in a way that charted a middle course between their excesses.\n\nWith the rise and dominance of Christianity in the West and the later spread of Islam, metaphysical naturalism was generally abandoned by intellectuals. Thus, there is little evidence for it in medieval philosophy. The reintroduction of Aristotle's empirical epistemology as well as previously lost treatises by Greco-Roman natural philosophers which was begun by the medieval Scholastics without resulting in any noticeable increase in commitment to naturalism.\n\nIt was not until the early modern era of philosophy and the Age of Enlightenment that naturalists like Benedict Spinoza (who put forward a theory of psychophysical parallelism), David Hume, and the proponents of French materialism (notably Denis Diderot, Julien La Mettrie, and Baron d'Holbach) started to emerge again in the 17th and 18th centuries. In this period, some metaphysical naturalists adhered to a distinct doctrine, materialism, which became the dominant category of metaphysical naturalism widely defended until the end of the 19th century.\n\nImmanuel Kant rejected (reductionist) materialist positions in metaphysics, but he was not hostile to naturalism. His transcendental philosophy is considered to be a form of liberal naturalism.\n\nIn late modern philosophy, \"Naturphilosophie\", a form of natural philosophy, was developed by Friedrich Wilhelm Joseph von Schelling and Georg Wilhelm Friedrich Hegel as an attempt to comprehend nature in its totality and to outline its general theoretical structure.\n\nA politicized version of naturalism that has arisen after Hegel was Ludwig Feuerbach, Karl Marx and Friedrich Engels's dialectical materialism, especially Engels's dialectical philosophy of nature (\"Dialectics of Nature\").\n\nAnother notable school of late modern philosophy advocating naturalism was German materialism: members included Ludwig Büchner, Jacob Moleschott, and Karl Vogt.\n\nIn the early 20th century, matter was found to be a form of energy and therefore not fundamental as materialists had assumed. (See History of physics.) In contemporary philosophy, renewed attention to the problem of universals, philosophy of mathematics, the development of mathematical logic, and the post-positivist revival of metaphysics and the philosophy of religion, initially by way of Wittgensteinian linguistic philosophy, further called the naturalistic paradigm into question. Developments such as these, along with those within science and the philosophy of science brought new advancements and revisions of naturalistic doctrines by naturalistic philosophers into metaphysics, ethics, the philosophy of language, the philosophy of mind, epistemology, etc., the products of which include physicalism and eliminative materialism, supervenience, causal theories of reference, anomalous monism, naturalized epistemology (e.g. reliabilism), internalism and externalism, ethical naturalism, and property dualism, for example.\n\nA politicized version of naturalism that has arisen in contemporary philosophy is Ayn Rand's Objectivism. Objectivism is an expression of capitalist ethical idealism within a naturalistic framework.\n\nThe current usage of the term naturalism \"derives from debates in America in the first half of the last century. The self-proclaimed 'naturalists' from that period included John Dewey, Ernest Nagel, Sidney Hook and Roy Wood Sellars.\"\n\nCurrently, metaphysical naturalism is more widely embraced than in previous centuries, especially but not exclusively in the natural sciences and the Anglo-American, analytic philosophical communities. While the vast majority of the population of the world remains firmly committed to non-naturalistic worldviews, prominent contemporary defenders of naturalism and/or naturalistic theses and doctrines today include J. J. C. Smart, David Malet Armstrong, David Papineau, Paul Kurtz, Brian Leiter, Daniel Dennett, Michael Devitt, Fred Dretske, Paul and Patricia Churchland, Mario Bunge, Jonathan Schaffer, Hilary Kornblith, Quentin Smith, Paul Draper and Michael Martin, among many other academic philosophers.\n\nAccording to David Papineau, contemporary naturalism is a consequence of the build-up of scientific evidence during the twentieth century for the \"causal closure of the physical\", the doctrine that all physical effects can be accounted for by physical causes.\n\nAccording to Steven Schafersman, president of Texas Citizens for Science, an advocacy group opposing creationism in public schools, the progressive adoption of methodological naturalism—and later of metaphysical naturalism—followed the advances of science and the increase of its explanatory power. These advances also caused the diffusion of positions associated with metaphysical naturalism, such as existentialism.\n\nIn the context of creation and evolution debates, Internet Infidels co-founder Jeffery Jay Lowder argues against what he calls \"the argument from bias\", that \"a priori,\" the supernatural is merely ruled out due to an unexamined stipulation. Lowder believes \"there are good empirical reasons for believing that metaphysical naturalism is true, and therefore a denial of the supernatural need not be based upon an \"a priori\" assumption\".\n\nRichard Carrier argues in \"Sense and Goodness Without God: A Defense of Metaphysical Naturalism\" that Metaphysical Naturalism is true. Topics covered include metaphilosophy, semantics, epistemology, the nature and origin of the universe (including a proposal that spacetime may be the ground of all being and a rejection of the logical possibility for any ultimate answer), free will compatibilism, the nature of mind, abstract objects, ontological reductionism, the nature of emotions, the meaning of life, the nature of reason, atheism, aesthetics, morality (including ethical naturalism and a recommendation for a science of morality), and politics.\n\nSeveral Metaphysical Naturalists have used the trends in scientific discoveries about minds to argue that no supernatural minds exist. For instance, Lowder says, \"Since all known mental activity has a physical basis, there are probably no disembodied minds. But God is conceived of as a disembodied mind. Therefore, God probably does not exist.\" Lowder argues the correlation between mind and brain implies that supernatural souls do not exist because the theist position, according to Lowder, is that the mind depends upon this soul instead of the brain.\n\nArguments against metaphysical naturalism include the following examples.\n\nAlvin Plantinga is the John A. O'Brien Professor of Philosophy Emeritus at the University of Notre Dame, and the inaugural holder of the Jellema Chair in Philosophy at Calvin College. He is a Christian, and a well-known critic of naturalism. He argues, in his evolutionary argument against naturalism, that the probability that evolution has produced humans with reliable true beliefs, is low or inscrutable, unless their evolution was guided, for example, by God. According to David Kahan of the University of Glasgow, in order to understand how beliefs are warranted, a justification must be found in the context of supernatural theism, as in Plantinga's epistemology. \"(See also supernormal stimuli).\"\n\nPlantinga argues that together, naturalism and evolution provide an insurmountable \"\"defeater\" for the belief that our cognitive faculties are reliable\", i.e., a skeptical argument along the lines of Descartes' evil demon or brain in a vat.\n\nBranden Fitelson of the University of California, Berkeley and Elliott Sober of the University of Wisconsin–Madison argue that Plantinga must show that the combination of evolution and naturalism also defeats the more modest claim that \"at least a non-negligible minority of our beliefs are true\", and that defects such as cognitive bias are nonetheless consistent with being made in the image of a rational God. Whereas evolutionary science already acknowledges that cognitive processes are unreliable, including the fallibility of the scientific enterprise itself, Plantinga's hyperbolic doubt is no more a defeater for naturalism than it is for theistic metaphysics founded upon a non-deceiving God who designed the human mind: \"[neither] can construct a non-question-begging argument that refutes global skepticism.\" Plantinga's argument has also been criticized by philosopher Daniel Dennett and historian Richard Carrier who argue that a cognitive apparatus for truth-finding can result from natural selection.\n\nEdward Feser, in his 2008 book \"The Last Superstition: A Refutation of the New Atheism\", lays a plenary case against naturalism by re-examining pre-Modern philosophy. Beginning in the second chapter, Feser cites the Platonic and Aristotelian answers to the problem of universals—that is, realism. Feser also offers arguments against nominalism. And by defending realism and rejecting nominalism, he rejects eliminative materialism—and thus naturalism.\n\nIn the third chapter, Feser summarizes three of Thomas Aquinas's arguments for the existence of God. These include arguments for an unmoved mover, first, uncaused cause and (supernatural) supreme intelligence, concluding that these must exist not as a matter of probability—as in the intelligent design view, particularly of irreducible complexity—but as a necessary consequence of \"obvious, though empirical, starting points\".\n\n\n\n\n\n"}
{"id": "50752458", "url": "https://en.wikipedia.org/wiki?curid=50752458", "title": "My Journey into the Wilds of Chicago", "text": "My Journey into the Wilds of Chicago\n\nMy Journey into the Wilds of Chicago is a photo-literary coffee table book authored by Mike MacDonald, with forewords by Bill Kurtis and Stephen Packard. The book is a visual and educational journey through the prairies, savannas, and other natural areas throughout the Chicago metropolitan area.\n\nThe book contains more than 200 photographs and nearly two dozen essays and poems written by MacDonald about Chicago's wild side, ranging in geography from the lakefront to prairie lands just north of the border in Wisconsin, to Kankakee, Lockport, Batavia, and McHenry County.\n\n\"My Journey into the Wilds of Chicago\" served as the basis for the website \"ChicagoNatureNow.com!\", a website run by MacDonald. The website is a digital catalog of Chicago's forest preserves, and provides updates of the area's natural events.\n\nThe book was positively received, including a review from \"Publishers Weekly,\" which said of \"My Journey into the Wilds of Chicago\", \"this impressive, cloth-bound debut is a lucid perspective on the prairie and its native plants and animals; it is celebratory, soulful and poetic, evoking a strong affection for Chicago's unchecked wilderness in a city best known for its iconic lakefront and skyscrapers.\"\n\n"}
{"id": "558685", "url": "https://en.wikipedia.org/wiki?curid=558685", "title": "Natural environment", "text": "Natural environment\n"}
{"id": "38890", "url": "https://en.wikipedia.org/wiki?curid=38890", "title": "Natural science", "text": "Natural science\n\nNatural science is a branch of science concerned with the description, prediction, and understanding of natural phenomena, based on empirical evidence from observation and experimentation. Mechanisms such as peer review and repeatability of findings are used to try to ensure the validity of scientific advances.\n\nNatural science can be divided into two main branches: life science (or biological science) and physical science. Physical science is subdivided into branches, including physics, chemistry, astronomy and earth science. These branches of natural science may be further divided into more specialized branches (also known as fields).\n\nIn Western society's analytic tradition, the empirical sciences and especially natural sciences use tools from formal sciences, such as mathematics and logic, converting information about nature into measurements which can be explained as clear statements of the \"laws of nature\". The social sciences also use such methods, but rely more on qualitative research, so that they are sometimes called \"soft science\", whereas natural sciences, insofar as they emphasize quantifiable data produced, tested, and confirmed through the scientific method, are sometimes called \"hard science\".\n\nModern natural science succeeded more classical approaches to natural philosophy, usually traced to ancient Greece. Galileo, Descartes, Bacon, and Newton debated the benefits of using approaches which were more mathematical and more experimental in a methodical way. Still, philosophical perspectives, conjectures, and presuppositions, often overlooked, remain necessary in natural science. Systematic data collection, including discovery science, succeeded natural history, which emerged in the 16th century by describing and classifying plants, animals, minerals, and so on. Today, \"natural history\" suggests observational descriptions aimed at popular audiences.\n\nPhilosophers of science have suggested a number of criteria, including Karl Popper's controversial falsifiability criterion, to help them differentiate scientific endeavors from non-scientific ones. Validity, accuracy, and quality control, such as peer review and repeatability of findings, are amongst the most respected criteria in the present-day global scientific community.\n\nThis field encompasses a set of disciplines that examines phenomena related to living organisms. The scale of study can range from sub-component biophysics up to complex ecologies. Biology is concerned with the characteristics, classification and behaviors of organisms, as well as how species were formed and their interactions with each other and the environment.\n\nThe biological fields of botany, zoology, and medicine date back to early periods of civilization, while microbiology was introduced in the 17th century with the invention of the microscope. However, it was not until the 19th century that biology became a unified science. Once scientists discovered commonalities between all living things, it was decided they were best studied as a whole.\n\nSome key developments in biology were the discovery of genetics; evolution through natural selection; the germ theory of disease and the application of the techniques of chemistry and physics at the level of the cell or organic molecule.\n\nModern biology is divided into subdisciplines by the type of organism and by the scale being studied. Molecular biology is the study of the fundamental chemistry of life, while cellular biology is the examination of the cell; the basic building block of all life. At a higher level, anatomy and physiology looks at the internal structures, and their functions, of an organism, while ecology looks at how various organisms interrelate.\n\nConstituting the scientific study of matter at the atomic and molecular scale, chemistry deals primarily with collections of atoms, such as gases, molecules, crystals, and metals. The composition, statistical properties, transformations and reactions of these materials are studied. Chemistry also involves understanding the properties and interactions of individual atoms and molecules for use in larger-scale applications.\n\nMost chemical processes can be studied directly in a laboratory, using a series of (often well-tested) techniques for manipulating materials, as well as an understanding of the underlying processes. Chemistry is often called \"the central science\" because of its role in connecting the other natural sciences.\n\nEarly experiments in chemistry had their roots in the system of Alchemy, a set of beliefs combining mysticism with physical experiments. The science of chemistry began to develop with the work of Robert Boyle, the discoverer of gas, and Antoine Lavoisier, who developed the theory of the Conservation of mass.\n\nThe discovery of the chemical elements and atomic theory began to systematize this science, and researchers developed a fundamental understanding of states of matter, ions, chemical bonds and chemical reactions. The success of this science led to a complementary chemical industry that now plays a significant role in the world economy.\n\nPhysics embodies the study of the fundamental constituents of the universe, the forces and interactions they exert on one another, and the results produced by these interactions. In general, physics is regarded as the fundamental science, because all other natural sciences use and obey the principles and laws set down by the field. Physics relies heavily on mathematics as the logical framework for formulation and quantification of principles.\n\nThe study of the principles of the universe has a long history and largely derives from direct observation and experimentation. The formulation of theories about the governing laws of the universe has been central to the study of physics from very early on, with philosophy gradually yielding to systematic, quantitative experimental testing and observation as the source of verification. Key historical developments in physics include Isaac Newton's theory of universal gravitation and classical mechanics, an understanding of electricity and its relation to magnetism, Einstein's theories of special and general relativity, the development of thermodynamics, and the quantum mechanical model of atomic and subatomic physics.\n\nThe field of physics is extremely broad, and can include such diverse studies as quantum mechanics and theoretical physics, applied physics and optics. Modern physics is becoming increasingly specialized, where researchers tend to focus on a particular area rather than being \"universalists\" like Isaac Newton, Albert Einstein and Lev Landau, who worked in multiple areas.\n\nThis discipline is the science of celestial objects and phenomena that originate outside the Earth's atmosphere. It is concerned with the evolution, physics, chemistry, meteorology, and motion of celestial objects, as well as the formation and development of the universe.\n\nAstronomy includes the examination, study and modeling of stars, planets, comets, galaxies and the cosmos. Most of the information used by astronomers is gathered by remote observation, although some laboratory reproduction of celestial phenomena has been performed (such as the molecular chemistry of the interstellar medium).\n\nWhile the origins of the study of celestial features and phenomena can be traced back to antiquity, the scientific methodology of this field began to develop in the middle of the 17th century. A key factor was Galileo's introduction of the telescope to examine the night sky in more detail.\n\nThe mathematical treatment of astronomy began with Newton's development of celestial mechanics and the laws of gravitation, although it was triggered by earlier work of astronomers such as Kepler. By the 19th century, astronomy had developed into a formal science, with the introduction of instruments such as the spectroscope and photography, along with much-improved telescopes and the creation of professional observatories.\n\nEarth science (also known as geoscience), is an all-embracing term for the sciences related to the planet Earth, including geology, geophysics, hydrology, meteorology, physical geography, oceanography, and soil science.\n\nAlthough mining and precious stones have been human interests throughout the history of civilization, the development of the related sciences of economic geology and mineralogy did not occur until the 18th century. The study of the earth, particularly palaeontology, blossomed in the 19th century. The growth of other disciplines, such as geophysics, in the 20th century led to the development of the theory of plate tectonics in the 1960s, which has had a similar effect on the Earth sciences as the theory of evolution had on biology. Earth sciences today are closely linked to petroleum and mineral resources, climate research and to environmental assessment and remediation.\n\nThough sometimes considered in conjunction with the earth sciences, due to the independent development of its concepts, techniques and practices and also the fact of it having a wide range of sub disciplines under its wing, the atmospheric sciences is also considered a separate branch of natural science. This field studies the characteristics of different layers of the atmosphere from ground level to the edge of the time. The timescale of study also varies from days to centuries. Sometimes the field also includes the study of climatic patterns on planets other than earth.\n\nThe serious study of oceans began in the early to mid-20th century. As a field of natural science, it is relatively young but stand-alone programs offer specializations in the subject. Though some controversies remain as to the categorization of the field under earth sciences, interdisciplinary sciences or as a separate field in its own right, most modern workers in the field agree that it has matured to a state that it has its own paradigms and practices. As such a big family of related studies spanning every aspect of the oceans is now classified under this field.\n\nThe distinctions between the natural science disciplines are not always sharp, and they share a number of cross-discipline fields. Physics plays a significant role in the other natural sciences, as represented by astrophysics, geophysics, chemical physics and biophysics. Likewise chemistry is represented by such fields as biochemistry, chemical biology, geochemistry and astrochemistry.\n\nA particular example of a scientific discipline that draws upon multiple natural sciences is environmental science. This field studies the interactions of physical, chemical, geological, and biological components of the environment, with a particular regard to the effect of human activities and the impact on biodiversity and sustainability. This science also draws upon expertise from other fields such as economics, law and social sciences.\n\nA comparable discipline is oceanography, as it draws upon a similar breadth of scientific disciplines. Oceanography is sub-categorized into more specialized cross-disciplines, such as physical oceanography and marine biology. As the marine ecosystem is very large and diverse, marine biology is further divided into many subfields, including specializations in particular species.\n\nThere are also a subset of cross-disciplinary fields which, by the nature of the problems that they address, have strong currents that run counter to\nspecialization. Put another way: In some fields of integrative application, specialists in more than one field are a key part of most dialog. Such integrative fields, for example, include nanoscience, astrobiology, and complex system informatics.\n\nMaterials science is a relatively new, interdisciplinary field which deals with the study of matter and its properties; as well as the discovery and design of new materials. Originally developed through the field of metallurgy, the study of the properties of materials and solids has now expanded into all materials. The field covers the chemistry, physics and engineering applications of materials including metals, ceramics, artificial polymers, and many others. The core of the field deals with relating structure of material with it properties.\n\nIt is at the forefront of research in science and engineering. It is an important part of forensic engineering (the investigation of materials, products, structures or components that fail or do not operate or function as intended, causing personal injury or damage to property) and failure analysis, the latter being the key to understanding, for example, the cause of various aviation accidents. Many of the most pressing scientific problems that are faced today are due to the limitations of the materials that are available and, as a result, breakthroughs in this field are likely to have a significant impact on the future of technology.\n\nThe basis of materials science involves studying the structure of materials, and relating them to their properties. Once a materials scientist knows about this structure-property correlation, they can then go on to study the relative performance of a material in a certain application. The major determinants of the structure of a material and thus of its properties are its constituent chemical elements and the way in which it has been processed into its final form. These characteristics, taken together and related through the laws of thermodynamics and kinetics, govern a material's microstructure, and thus its properties.\n\nSome scholars trace the origins of natural science as far back as pre-literate human societies, where understanding the natural world was necessary for survival. People observed and built up knowledge about the behavior of animals and the usefulness of plants as food and medicine, which was passed down from generation to generation. These primitive understandings gave way to more formalized inquiry around 3500 to 3000 BC in the Mesopotamian and Ancient Egyptian cultures, which produced the first known written evidence of natural philosophy, the precursor of natural science. While the writings show an interest in astronomy, mathematics and other aspects of the physical world, the ultimate aim of inquiry about nature's workings was in all cases religious or mythological, not scientific.\n\nA tradition of scientific inquiry also emerged in Ancient China, where Taoist alchemists and philosophers experimented with elixirs to extend life and cure ailments. They focused on the yin and yang, or contrasting elements in nature; the yin was associated with femininity and coldness, while yang was associated with masculinity and warmth. The five phases – fire, earth, metal, wood and water – described a cycle of transformations in nature. Water turned into wood, which turned into fire when it burned. The ashes left by fire were earth. Using these principles, Chinese philosophers and doctors explored human anatomy, characterizing organs as predominantly yin or yang and understood the relationship between the pulse, the heart and the flow of blood in the body centuries before it became accepted in the West.\n\nLittle evidence survives of how Ancient Indian cultures around the Indus River understood nature, but some of their perspectives may be reflected in the Vedas, a set of sacred Hindu texts. They reveal a conception of the universe as ever-expanding and constantly being recycled and reformed. Surgeons in the Ayurvedic tradition saw health and illness as a combination of three humors: wind, bile and phlegm. A healthy life was the result of a balance among these humors. In Ayurvedic thought, the body consisted of five elements: earth, water, fire, wind and empty space. Ayurvedic surgeons performed complex surgeries and developed a detailed understanding of human anatomy.\n\nPre-Socratic philosophers in Ancient Greek culture brought natural philosophy a step closer to direct inquiry about cause and effect in nature between 600 and 400 BC, although an element of magic and mythology remained. Natural phenomena such as earthquakes and eclipses were explained increasingly in the context of nature itself instead of being attributed to angry gods. Thales of Miletus, an early philosopher who lived from 625 to 546 BC, explained earthquakes by theorizing that the world floated on water and that water was the fundamental element in nature. In the 5th century BC, Leucippus was an early exponent of atomism, the idea that the world is made up of fundamental indivisible particles. Pythagoras applied Greek innovations in mathematics to astronomy, and suggested that the earth was spherical.\n\nLater Socratic and Platonic thought focused on ethics, morals and art and did not attempt an investigation of the physical world; Plato criticized pre-Socratic thinkers as materialists and anti-religionists. Aristotle, however, a student of Plato who lived from 384 to 322 BC, paid closer attention to the natural world in his philosophy. In his \"History of Animals\", he described the inner workings of 110 species, including the stingray, catfish and bee. He investigated chick embryos by breaking open eggs and observing them at various stages of development. Aristotle's works were influential through the 16th century, and he is considered to be the father of biology for his pioneering work in that science. He also presented philosophies about physics, nature and astronomy using inductive reasoning in his works \"Physics\" and \"Meteorology\".\n\nWhile Aristotle considered natural philosophy more seriously than his predecessors, he approached it as a theoretical branch of science. Still, inspired by his work, Ancient Roman philosophers of the early 1st century AD, including Lucretius, Seneca and Pliny the Elder, wrote treatises that dealt with the rules of the natural world in varying degrees of depth. Many Ancient Roman Neoplatonists of the 3rd to the 6th centuries also adapted Aristotle's teachings on the physical world to a philosophy that emphasized spiritualism. Early medieval philosophers including Macrobius, Calcidius and Martianus Capella also examined the physical world, largely from a cosmological and cosmographical perspective, putting forth theories on the arrangement of celestial bodies and the heavens, which were posited as being composed of aether.\n\nAristotle's works on natural philosophy continued to be translated and studied amid the rise of the Byzantine Empire and Abbasid Caliphate. \n\nIn the Byzantine Empire John Philoponus, an Alexandrian Aristotelian commentator and Christian theologian, was the first who questioned Aristotle's teaching of physics. Unlike Aristotle who based his physics on verbal argument, Philoponus instead relied on observation, and argued for observation rather than resorting into verbal argument. He introduced the theory of impetus. John Philoponus' criticism of Aristotelian principles of physics served as inspiration for Galileo Galilei during the Scientific Revolution.\n\nA revival in mathematics and science took place during the time of the Abbasid Caliphate from the 9th century onward, when Muslim scholars expanded upon Greek and Indian natural philosophy. The words \"alcohol\", \"algebra\" and \"zenith\" all have Arabic roots.\n\nAristotle's works and other Greek natural philosophy did not reach the West until about the middle of the 12th century, when works were translated from Greek and Arabic into Latin. The development of European civilization later in the Middle Ages brought with it further advances in natural philosophy. European inventions such as the horseshoe, horse collar and crop rotation allowed for rapid population growth, eventually giving way to urbanization and the foundation of schools connected to monasteries and cathedrals in modern-day France and England. Aided by the schools, an approach to Christian theology developed that sought to answer questions about nature and other subjects using logic. This approach, however, was seen by some detractors as heresy. By the 12th century, Western European scholars and philosophers came into contact with a body of knowledge of which they had previously been ignorant: a large corpus of works in Greek and Arabic that were preserved by Islamic scholars. Through translation into Latin, Western Europe was introduced to Aristotle and his natural philosophy. These works were taught at new universities in Paris and Oxford by the early 13th century, although the practice was frowned upon by the Catholic church. A 1210 decree from the Synod of Paris ordered that \"no lectures are to be held in Paris either publicly or privately using Aristotle's books on natural philosophy or the commentaries, and we forbid all this under pain of excommunication.\"\n\nIn the late Middle Ages, Spanish philosopher Dominicus Gundissalinus translated a treatise by the earlier Persian scholar Al-Farabi called \"On the Sciences\" into Latin, calling the study of the mechanics of nature \"scientia naturalis\", or natural science. Gundissalinus also proposed his own classification of the natural sciences in his 1150 work \"On the Division of Philosophy\". This was the first detailed classification of the sciences based on Greek and Arab philosophy to reach Western Europe. Gundissalinus defined natural science as \"the science considering only things unabstracted and with motion,\" as opposed to mathematics and sciences that rely on mathematics. Following Al-Farabi, he then separated the sciences into eight parts, including physics, cosmology, meteorology, minerals science and plant and animal science.\n\nLater philosophers made their own classifications of the natural sciences. Robert Kilwardby wrote \"On the Order of the Sciences\" in the 13th century that classed medicine as a mechanical science, along with agriculture, hunting and theater while defining natural science as the science that deals with bodies in motion. Roger Bacon, an English friar and philosopher, wrote that natural science dealt with \"a principle of motion and rest, as in the parts of the elements of fire, air, earth and water, and in all inanimate things made from them.\" These sciences also covered plants, animals and celestial bodies. Later in the 13th century, Catholic priest and theologian Thomas Aquinas defined natural science as dealing with \"mobile beings\" and \"things which depend on matter not only for their existence, but also for their definition.\" There was wide agreement among scholars in medieval times that natural science was about bodies in motion, although there was division about the inclusion of fields including medicine, music and perspective. Philosophers pondered questions including the existence of a vacuum, whether motion could produce heat, the colors of rainbows, the motion of the earth, whether elemental chemicals exist and where in the atmosphere rain is formed.\n\nIn the centuries up through the end of the Middle Ages, natural science was often mingled with philosophies about magic and the occult. Natural philosophy appeared in a wide range of forms, from treatises to encyclopedias to commentaries on Aristotle. The interaction between natural philosophy and Christianity was complex during this period; some early theologians, including Tatian and Eusebius, considered natural philosophy an outcropping of pagan Greek science and were suspicious of it. Although some later Christian philosophers, including Aquinas, came to see natural science as a means of interpreting scripture, this suspicion persisted until the 12th and 13th centuries. The Condemnation of 1277, which forbade setting philosophy on a level equal with theology and the debate of religious constructs in a scientific context, showed the persistence with which Catholic leaders resisted the development of natural philosophy even from a theological perspective. Aquinas and Albertus Magnus, another Catholic theologian of the era, sought to distance theology from science in their works. \"I don't see what one's interpretation of Aristotle has to do with the teaching of the faith,\" he wrote in 1271.\n\nBy the 16th and 17th centuries, natural philosophy underwent an evolution beyond commentary on Aristotle as more early Greek philosophy was uncovered and translated. The invention of the printing press in the 15th century, the invention of the microscope and telescope, and the Protestant Reformation fundamentally altered the social context in which scientific inquiry evolved in the West. Christopher Columbus's discovery of a new world changed perceptions about the physical makeup of the world, while observations by Copernicus, Tyco Brahe and Galileo brought a more accurate picture of the solar system as heliocentric and proved many of Aristotle's theories about the heavenly bodies false. A number of 17th-century philosophers, including Thomas Hobbes, John Locke and Francis Bacon made a break from the past by rejecting Aristotle and his medieval followers outright, calling their approach to natural philosophy as superficial.\n\nThe titles of Galileo's work \"Two New Sciences\" and Johannes Kepler's \"New Astronomy\" underscored the atmosphere of change that took hold in the 17th century as Aristotle was dismissed in favor of novel methods of inquiry into the natural world. Bacon was instrumental in popularizing this change; he argued that people should use the arts and sciences to gain dominion over nature. To achieve this, he wrote that \"human life [must] be endowed with new discoveries and powers.\" He defined natural philosophy as \"the knowledge of Causes and secret motions of things; and enlarging the bounds of Human Empire, to the effecting of all things possible.\" Bacon proposed scientific inquiry supported by the state and fed by the collaborative research of scientists, a vision that was unprecedented in its scope, ambition and form at the time. Natural philosophers came to view nature increasingly as a mechanism that could be taken apart and understood, much like a complex clock. Natural philosophers including Isaac Newton, Evangelista Torricelli and Francesco Redi conducted experiments focusing on the flow of water, measuring atmospheric pressure using a barometer and disproving spontaneous generation. Scientific societies and scientific journals emerged and were spread widely through the printing press, touching off the scientific revolution. Newton in 1687 published his \"The Mathematical Principles of Natural Philosophy\", or \"Principia Mathematica\", which set the groundwork for physical laws that remained current until the 19th century.\n\nSome modern scholars, including Andrew Cunningham, Perry Williams and Floris Cohen, argue that natural philosophy is not properly called a science, and that genuine scientific inquiry began only with the scientific revolution. According to Cohen, \"the emancipation of science from an overarching entity called 'natural philosophy' is one defining characteristic of the Scientific Revolution.\" Other historians of science, including Edward Grant, contend that the scientific revolution that blossomed in the 17th, 18th and 19th centuries occurred when principles learned in the exact sciences of optics, mechanics and astronomy began to be applied to questions raised by natural philosophy. Grant argues that Newton attempted to expose the mathematical basis of nature – the immutable rules it obeyed – and in doing so joined natural philosophy and mathematics for the first time, producing an early work of modern physics.\nThe scientific revolution, which began to take hold in the 17th century, represented a sharp break from Aristotelian modes of inquiry. One of its principal advances was the use of the scientific method to investigate nature. Data was collected and repeatable measurements made in experiments. Scientists then formed hypotheses to explain the results of these experiments. The hypothesis was then tested using the principle of falsifiability to prove or disprove its accuracy. The natural sciences continued to be called natural philosophy, but the adoption of the scientific method took science beyond the realm of philosophical conjecture and introduced a more structured way of examining nature.\n\nNewton, an English mathematician and physicist, was the seminal figure in the scientific revolution. Drawing on advances made in astronomy by Copernicus, Brahe and Kepler, Newton derived the universal law of gravitation and laws of motion. These laws applied both on earth and in outer space, uniting two spheres of the physical world previously thought to function independently of each other, according to separate physical rules. Newton, for example, showed that the tides were caused by the gravitational pull of the moon. Another of Newton's advances was to make mathematics a powerful explanatory tool for natural phenomena. While natural philosophers had long used mathematics as a means of measurement and analysis, its principles were not used as a means of understanding cause and effect in nature until Newton.\n\nIn the 18th century and 19th century, scientists including Charles-Augustin de Coulomb, Alessandro Volta, and Michael Faraday built upon Newtonian mechanics by exploring electromagnetism, or the interplay of forces with positive and negative charges on electrically charged particles. Faraday proposed that forces in nature operated in \"fields\" that filled space. The idea of fields contrasted with the Newtonian construct of gravitation as simply \"action at a distance\", or the attraction of objects with nothing in the space between them to intervene. James Clerk Maxwell in the 19th century unified these discoveries in a coherent theory of electrodynamics. Using mathematical equations and experimentation, Maxwell discovered that space was filled with charged particles that could act upon themselves and each other, and that they were a medium for the transmission of charged waves.\n\nSignificant advances in chemistry also took place during the scientific revolution. Antoine Lavoisier, a French chemist, refuted the phlogiston theory, which posited that things burned by releasing \"phlogiston\" into the air. Joseph Priestley had discovered oxygen in the 18th century, but Lavoisier discovered that combustion was the result of oxidation. He also constructed a table of 33 elements and invented modern chemical nomenclature. Formal biological science remained in its infancy in the 18th century, when the focus lay upon the classification and categorization of natural life. This growth in natural history was led by Carl Linnaeus, whose 1735 taxonomy of the natural world is still in use. Linnaeus in the 1750s introduced scientific names for all his species.\n\nBy the 19th century, the study of science had come into the purview of professionals and institutions. In so doing, it gradually acquired the more modern name of \"natural science.\" The term \"scientist\" was coined by William Whewell in an 1834 review of Mary Somerville's \"On the Connexion of the Sciences\". But the word did not enter general use until nearly the end of the same century.\n\nAccording to a famous 1923 textbook \"Thermodynamics and the Free Energy of Chemical Substances\" by the American chemist Gilbert N. Lewis and the American physical chemist Merle Randall, the natural sciences contain three great branches:\n\nAside from the logical and mathematical sciences, there are three great branches of \"natural science\" which stand apart by reason of the variety of far reaching deductions drawn from a small number of primary postulates — they are mechanics, electrodynamics, and thermodynamics.\n\nToday, natural sciences are more commonly divided into life sciences, such as botany and zoology; and physical sciences, which include physics, chemistry, geology, astronomy and materials science.\n\n\n\n"}
{"id": "4585070", "url": "https://en.wikipedia.org/wiki?curid=4585070", "title": "Non-cellular life", "text": "Non-cellular life\n\nNon-cellular life is life that exists without a cellular structure for at least part of its life cycle. Historically, most (descriptive) definitions of life postulated that a living organism must be composed of one or more cells, but this is no longer considered necessary, and modern criteria allow for forms of life based on other structural arrangements.\n\nThe primary candidates for non-cellular life are viruses. A minority of biologists consider viruses to be living organisms, but most do not. Their primary objection is that no known viruses are capable of autopoiesis, which means they cannot reproduce themselves: they must rely on cells to copy them. However, the recent discovery of giant viruses that possess genes for part of the required translation machinery has raised the prospect that they may have had extinct ancestors that could evolve and replicate independently. Most biologists agree that such an ancestor would be a \"bona fide\" non-cellular lifeform, but its existence and characteristics are still uncertain.\n\nEngineers sometimes use the term \"artificial life\" to refer to software and robots inspired by biological processes, but these do not satisfy any biological definition of life.\n\nThe nature of viruses was unclear for many years following their discovery as pathogens. They were described as poisons or toxins at first, then as \"infectious proteins\", but with advances in microbiology it became clear that they also possessed genetic material, a defined structure, and the ability to spontaneously assemble from their constituent parts. This spurred extensive debate as to whether they should be regarded as fundamentally organic or inorganic — as very small biological organisms or very large biochemical molecules — and since the 1950s many scientists have thought of viruses as existing at the border between chemistry and life; a gray area between living and nonliving.\n\nThe recent discovery of giant viruses (aka giruses, nucleocytoplasmic large DNA viruses, NCLDVs) has reignited this debate, since they are not only physically larger than previously known viruses, but also possess much more extensive genomes, including genes coding for aminoacyl tRNA synthetases, key proteins involved in translation, which were previously thought to be exclusive to cellular organisms. This raises the prospect that the giant viruses may have had extinct ancestors (or even undiscovered ones) capable of engaging in life processes (such as evolution and replication) independent of cells, and that modern viruses lost those abilities secondarily. The viral lineage including this ancestor would be ancient and may have originated alongside the earliest archaea or before the LUCA. If such a virus is discovered, or its past existence supported by further genetic evidence, most biologists agree that it would constitute a \"bona fide\" lifeform, and its descendants (at least the giant viruses, and possibly including all known viruses) could be phylogenetically classified in a fourth domain of life. Discovery of the pandoraviruses, with genomes are even larger than the other giant viruses and exhibiting particularly low homology with the three existing domains, further discredited the traditional view that viruses simply \"pick-pocketed\" all of their genes from cellular organisms, and further supported the \"complex ancestors\" hypothesis.\n\nOngoing research is being conducted in this area, using techniques such as phylogenetic bracketing on the giant viruses to infer characteristics of their proposed progenitor.\n\nFurthermore, Viral replication and self-assembly has implications for the study of the origin of life, as it lends further credence to the hypothesis that life could have started as self-assembling organic molecules.\n\nViroids are the smallest infectious pathogens known to biologists, consisting solely of short strands of circular, single-stranded RNA without protein coats. They are mostly plant pathogens and some are animal pathogens, from which some are of commercial importance. Viroid genomes are extremely small in size, ranging from 246 to 467 nucleobases. In comparison, the genome of the smallest known viruses capable of causing an infection by themselves are around 2,000 nucleobases in size. Viroids are the first known representatives of a new biological realm of sub-viral pathogens.\n\nViroid RNA does not code for any protein. Its replication mechanism hijacks RNA polymerase II, a host cell enzyme normally associated with synthesis of messenger RNA from DNA, which instead catalyzes \"rolling circle\" synthesis of new RNA using the viroid's RNA as a template. Some viroids are ribozymes, having catalytic properties which allow self-cleavage and ligation of unit-size genomes from larger replication intermediates.\n\nViroids attained significance beyond plant virology since one possible explanation of their origin is that they represent “living relics” from a hypothetical, ancient, and non-cellular RNA world before the evolution of DNA or protein. This view was first proposed in the 1980s, and regained popularity in the 2010s to explain crucial intermediate steps in the evolution of life from inanimate matter (Abiogenesis).\n\nIn discussing the taxonomic domains of life, the terms \"Acytota\" or \"Aphanobionta\" are occasionally used as the name of a viral kingdom, domain, or empire. The corresponding cellular life name would be Cytota. Non-cellular organisms and cellular life would be the two top-level subdivisions of life, whereby life as a whole would be known as organisms, Naturae, or Vitae. The taxon Cytota would include three top-level subdivisions of its own, the domains Bacteria, Archaea, and Eukarya.\n\n"}
{"id": "827792", "url": "https://en.wikipedia.org/wiki?curid=827792", "title": "Rare Earth hypothesis", "text": "Rare Earth hypothesis\n\nIn planetary astronomy and astrobiology, the Rare Earth hypothesis argues that the origin of life and the evolution of biological complexity such as sexually reproducing, multicellular organisms on Earth (and, subsequently, human intelligence) required an improbable combination of astrophysical and geological events and circumstances.\n\nAccording to the hypothesis, complex extraterrestrial life is an improbable phenomenon and likely to be rare. The term \"Rare Earth\" originates from \"Rare Earth: Why Complex Life Is Uncommon in the Universe\" (2000), a book by Peter Ward, a geologist and paleontologist, and Donald E. Brownlee, an astronomer and astrobiologist, both faculty members at the University of Washington.\n\nA contrary view was argued in the 1970s and 1980s by Carl Sagan and Frank Drake, among others. It holds that Earth is a typical rocky planet in a typical planetary system, located in a non-exceptional region of a common barred-spiral galaxy. Given the principle of mediocrity (in the same vein as the Copernican principle), it is probable that we are typical, and the universe teems with complex life. However, Ward and Brownlee argue that planets, planetary systems, and galactic regions that are as friendly to complex life as the Earth, the Solar System, and our galactic region are rare.\n\nThe Rare Earth hypothesis argues that the evolution of biological complexity requires a host of fortuitous circumstances, such as a galactic habitable zone, a central star and planetary system having the requisite character, the circumstellar habitable zone, a right-sized terrestrial planet, the advantage of a gas giant guardian like Jupiter and a large natural satellite, conditions needed to ensure the planet has a magnetosphere and plate tectonics, the chemistry of the lithosphere, atmosphere, and oceans, the role of \"evolutionary pumps\" such as massive glaciation and rare bolide impacts, and whatever led to the appearance of the eukaryote cell, sexual reproduction and the Cambrian explosion of animal, plant, and fungi phyla. The evolution of human intelligence may have required yet further events, which are extremely unlikely to have happened were it not for the Cretaceous–Paleogene extinction event 66 million years ago removing dinosaurs as the dominant terrestrial vertebrates.\n\nIn order for a small rocky planet to support complex life, Ward and Brownlee argue, the values of several variables must fall within narrow ranges. The universe is so vast that it could contain many Earth-like planets. But if such planets exist, they are likely to be separated from each other by many thousands of light years. Such distances may preclude communication among any intelligent species evolving on such planets, which would solve the Fermi paradox: \"If extraterrestrial aliens are common, why aren't they obvious?\"\n\n\"Rare Earth\" suggests that much of the known universe, including large parts of our galaxy, are \"dead zones\" unable to support complex life. Those parts of a galaxy where complex life is possible make up the galactic habitable zone, primarily characterized by distance from the Galactic Center. As that distance increases:\nItem #1 rules out the outer reaches of a galaxy; #2 and #3 rule out galactic inner regions. Hence a galaxy's habitable zone may be a ring sandwiched between its uninhabitable center and outer reaches.\n\nAlso, a habitable planetary system must maintain its favorable location long enough for complex life to evolve. A star with an eccentric (elliptic or hyperbolic) galactic orbit will pass through some spiral arms, unfavorable regions of high star density; thus a life-bearing star must have a galactic orbit that is nearly circular, with a close synchronization between the orbital velocity of the star and of the spiral arms. This further restricts the galactic habitable zone within a fairly narrow range of distances from the Galactic Center. Lineweaver et al. calculate this zone to be a ring 7 to 9 kiloparsecs in radius, including no more than 10% of the stars in the Milky Way, about 20 to 40 billion stars. Gonzalez, et al. would halve these numbers; they estimate that at most 5% of stars in the Milky Way fall in the galactic habitable zone.\n\nApproximately 77% of observed galaxies are spiral, two-thirds of all spiral galaxies are barred, and more than half, like the Milky Way, exhibit multiple arms. According to Rare Earth, our own galaxy is unusually quiet and dim (see below), representing just 7% of its kind. Even so, this would still represent more than 200 billion galaxies in the known universe.\n\nOur galaxy also appears unusually favorable in suffering fewer collisions with other galaxies over the last 10 billion years, which can cause more supernovae and other disturbances. Also, the Milky Way's central black hole seems to have neither too much nor too little activity.\n\nThe orbit of the Sun around the center of the Milky Way is indeed almost perfectly circular, with a period of 226 Ma (million years), closely matching the rotational period of the galaxy. However, the majority of stars in barred spiral galaxies populate the spiral arms rather than the halo and tend to move in gravitationally aligned orbits, so there is little that is unusual about the Sun's orbit. While the Rare Earth hypothesis predicts that the Sun should rarely, if ever, have passed through a spiral arm since its formation, astronomer Karen Masters has calculated that the orbit of the Sun takes it through a major spiral arm approximately every 100 million years. Some researchers have suggested that several mass extinctions do correspond with previous crossings of the spiral arms.\n\nThe terrestrial example suggests that complex life requires liquid water, requiring an orbital distance neither too close nor too far from the central star, another scale of habitable zone or Goldilocks Principle: \nThe habitable zone varies with the star's type and age.\n\nFor advanced life, the star must also be highly stable, which is typical of middle star life, about 4.6 billion years old. Proper metallicity and size are also important to stability. The Sun has a low 0.1% luminosity variation. To date no solar twin star twin, with an exact match of the sun's luminosity variation, has been found, though some come close. The star must have no stellar companions, as in binary systems, which would disrupt the orbits of planets. Estimates suggest 50% or more of all star systems are binary. The habitable zone for a main sequence star very gradually moves out over its lifespan until it becomes a white dwarf and the habitable zone vanishes.\n\nThe liquid water and other gases available in the habitable zone bring the benefit of greenhouse warming. Even though the Earth's atmosphere contains a water vapor concentration from 0% (in arid regions) to 4% (in rain forest and ocean regions) and – as of February 2018 – only 408.05 parts per million of , these small amounts suffice to raise the average surface temperature by about 40 °C, with the dominant contribution being due to water vapor, which together with clouds makes up between 66% and 85% of Earth's greenhouse effect, with contributing between 9% and 26% of the effect.\n\nRocky planets must orbit within the habitable zone for life to form. Although the habitable zone of such hot stars as Sirius or Vega is wide, hot stars also emit much more ultraviolet radiation that ionizes any planetary atmosphere. They may become red giants before advanced life evolves on their planets.\nThese considerations rule out the massive and powerful stars of type F6 to O (see stellar classification) as homes to evolved metazoan life.\n\nSmall red dwarf stars conversely have small habitable zones wherein planets are in tidal lock, with one very hot side always facing the star and another very cold side; and they are also at increased risk of solar flares (see Aurelia). Life therefore cannot arise in such systems. Rare Earth proponents claim that only stars from F7 to K1 types are hospitable. Such stars are rare: G type stars such as the Sun (between the hotter F and cooler K) comprise only 9% of the hydrogen-burning stars in the Milky Way.\n\nSuch aged stars as red giants and white dwarfs are also unlikely to support life. Red giants are common in globular clusters and elliptical galaxies. White dwarfs are mostly dying stars that have already completed their red giant phase. Stars that become red giants expand into or overheat the habitable zones of their youth and middle age (though theoretically planets at a much greater distance may become habitable).\n\nAn energy output that varies with the lifetime of the star will likely prevent life (e.g., as Cepheid variables). A sudden decrease, even if brief, may freeze the water of orbiting planets, and a significant increase may evaporate it and cause a greenhouse effect that prevents the oceans from reforming.\n\nAll known life requires the complex chemistry of metallic elements. The absorption spectrum of a star reveals the presence of metals within, and studies of stellar spectra reveal that many, perhaps most, stars are poor in metals. Because heavy metals originate in supernova explosions, metallicity increases in the universe over time. Low metallicity characterizes the early universe: globular clusters and other stars that formed when the universe was young, stars in most galaxies other than large spirals, and stars in the outer regions of all galaxies. Metal-rich central stars capable of supporting complex life are therefore believed to be most common in the quiet suburbs of the larger spiral galaxies—where radiation also happens to be weak.\n\nRare Earth proponents argue that a planetary system capable of sustaining complex life must be structured more or less like the Solar System, with small and rocky inner planets and outer gas giants. Without the protection of 'celestial vacuum cleaner' planets with strong gravitational pull, a planet would be subject to more catastrophic asteroid collisions.\n\nObservations of exo-planets have shown that arrangements of planets similar to our Solar System are rare. Most planetary systems have super Earths, several times larger than Earth, close to their star, whereas our Solar System's inner region has only a few small rocky planets and none inside Mercury's orbit. Only 10% of stars have giant planets similar to Jupiter and Saturn, and those few rarely have stable nearly circular orbits distant from their star. Konstantin Batygin and colleagues argue that these features can be explained if, early in the history of the Solar System, Jupiter and Saturn drifted towards the Sun, sending showers of planetesimals towards the super-Earths which sent them spiralling into the Sun, and ferrying icy building blocks into the terrestrial region of the Solar System which provided the building blocks for the rocky planets. The two giant planets then drifted out again to their present position. However, in the view of Batygin and his colleagues: \"The concatenation of chance events required for this delicate choreography suggest that small, Earth-like rocky planets – and perhaps life itself – could be rare throughout the cosmos.\"\n\nRare Earth argues that a gas giant must not be too close to a body where life is developing. Close placement of gas giant(s) could disrupt the orbit of a potential life-bearing planet, either directly or by drifting into the habitable zone.\n\nNewtonian dynamics can produce chaotic planetary orbits, especially in a system having large planets at high orbital eccentricity.\n\nThe need for stable orbits rules out stars with systems of planets that contain large planets with orbits close to the host star (called \"hot Jupiters\"). It is believed that hot Jupiters have migrated inwards to their current orbits. In the process, they would have catastrophically disrupted the orbits of any planets in the habitable zone. To exacerbate matters, hot Jupiters are much more common orbiting F and G class stars.\n\nIt is argued that life requires terrestrial planets like Earth and as gas giants lack such a surface, that complex life cannot arise there.\n\nA planet that is too small cannot hold much atmosphere, making surface temperature low and variable and oceans impossible. A small planet will also tend to have a rough surface, with large mountains and deep canyons. The core will cool faster, and plate tectonics may be brief or entirely absent. A planet that is too large will retain too dense an atmosphere like Venus. Although Venus is similar in size and mass to Earth, its surface atmospheric pressure is 92 times that of Earth, and surface temperature of 735 K (462 °C; 863 °F). Earth had a similar early atmosphere to Venus, but may have lost it in the giant impact event.\n\nRare Earth proponents argue that plate tectonics and a strong magnetic field are essential for biodiversity, global temperature regulation, and the carbon cycle.\nThe lack of mountain chains elsewhere in the Solar System is direct evidence that Earth is the only body with plate tectonics, and thus the only nearby body capable of supporting life.\n\nPlate tectonics depend on the right chemical composition and a long-lasting source of heat from radioactive decay. Continents must be made of less dense felsic rocks that \"float\" on underlying denser mafic rock. Taylor emphasizes that tectonic subduction zones require the lubrication of oceans of water. Plate tectonics also provides a means of biochemical cycling.\n\nPlate tectonics and as a result continental drift and the creation of separate land masses would create diversified ecosystems and biodiversity, one of the strongest defences against extinction. An example of species diversification and later competition on Earth's continents is the Great American Interchange. North and Middle America drifted into South America at around 3.5 to 3 Ma. The fauna of South America evolved separately for about 30 million years, since Antarctica separated. Many species were subsequently wiped out in mainly South America by competing Northern American animals.\n\nThe Moon is unusual because the other rocky planets in the Solar System either have no satellites (Mercury and Venus), or only tiny satellites which are probably captured asteroids (Mars).\n\nThe Giant-impact theory hypothesizes that the Moon resulted from the impact of a Mars-sized body, dubbed Theia, with the young Earth. This giant impact also gave the Earth its axial tilt (inclination) and velocity of rotation. Rapid rotation reduces the daily variation in temperature and makes photosynthesis viable. The \"Rare Earth\" hypothesis further argues that the axial tilt cannot be too large or too small (relative to the orbital plane). A planet with a large tilt will experience extreme seasonal variations in climate. A planet with little or no tilt will lack the stimulus to evolution that climate variation provides. In this view, the Earth's tilt is \"just right\". The gravity of a large satellite also stabilizes the planet's tilt; without this effect the variation in tilt would be chaotic, probably making complex life forms on land impossible.\n\nIf the Earth had no Moon, the ocean tides resulting solely from the Sun's gravity would be only half that of the lunar tides. A large satellite gives rise to tidal pools, which may be essential for the formation of complex life, though this is far from certain.\n\nA large satellite also increases the likelihood of plate tectonics through the effect of tidal forces on the planet's crust. The impact that formed the Moon may also have initiated plate tectonics, without which the continental crust would cover the entire planet, leaving no room for oceanic crust. It is possible that the large scale mantle convection needed to drive plate tectonics could not have emerged in the absence of crustal inhomogeneity. A further theory indicates that such a large moon may also contribute to maintaining a planet's magnetic shield by continually acting upon a metallic planetary core as dynamo, thus protecting the surface of the planet from charged particles and cosmic rays, and helping to ensure the atmosphere is not stripped over time by solar winds.\n\nA terrestrial planet of the right size is needed to retain an atmosphere, like Earth and Venus. On Earth, once the giant impact of Theia thinned Earth's atmosphere, other events were needed to make the atmosphere capable of sustaining life. The Late Heavy Bombardment reseeded Earth with water lost after the impact of Theia. The development of an ozone layer formed protection from ultraviolet (UV) sunlight. Nitrogen and carbon dioxide are needed in a correct ratio for life to form. Lightning is needed for nitrogen fixation. The carbon dioxide gas needed for life comes from sources such as volcanoes and geysers. Carbon dioxide is only needed at low levels (currently at 400 ppm); at high levels it is poisonous. Precipitation is needed to have a stable water cycle. A proper atmosphere must reduce diurnal temperature variation.\n\nRegardless of whether planets with similar physical attributes to the Earth are rare or not, some argue that life usually remains simple bacteria. Biochemist Nick Lane argues that simple cells (prokaryotes) emerged soon after Earth's formation, but since almost half the planet's life had passed before they evolved into complex ones (eukaryotes) all of whom share a common ancestor, this event can only have happened once. In some views, prokaryotes lack the cellular architecture to evolve into eukaryotes because a bacterium expanded up to eukaryotic proportions would have tens of thousands of times less energy available; two billion years ago, one simple cell incorporated itself into another, multiplied, and evolved into mitochondria that supplied the vast increase in available energy that enabled the evolution of complex life. If this incorporation occurred only once in four billion years or is otherwise unlikely, then life on most planets remains simple. An alternative view is that mitochondria evolution was environmentally triggered, and that mitochondria-containing organisms appeared soon after the first traces of atmospheric oxygen.\n\nThe evolution and persistence of sexual reproduction is another mystery in biology. The purpose of sexual reproduction is unclear, as in many organisms it has a 50% cost (fitness disadvantage) in relation to asexual reproduction. Mating types (types of gametes, according to their compatibility) may have arisen as a result of anisogamy (gamete dimorphism), or the male and female genders may have evolved before anisogamy. It is also unknown why most sexual organisms use a binary mating system, and why some organisms have gamete dimorphism. Charles Darwin was the first to suggest that sexual selection drives speciation; without it, complex life would probably not have evolved.\n\nWhile life on Earth is regarded to have spawned relatively early in the planet's history, the evolution from multicellular to intelligent organisms took around 800 million years. Civilizations on Earth have existed for about 12,000 years and radio communication reaching space has existed for less than 100 years. Relative to the age of the Solar System (~4.57 Ga) this is a short time, in which extreme climatic variations, super volcanoes, and large meteorite impacts were absent. These events would severely harm intelligent life, as well as life in general. For example, the Permian-Triassic mass extinction, caused by widespread and continuous volcanic eruptions in an area the size of Western Europe, led to the extinction of 95% of known species around 251.2 Ma ago. About 65 million years ago, the Chicxulub impact at the Cretaceous–Paleogene boundary (~65.5 Ma) on the Yucatán peninsula in Mexico led to a mass extinction of the most advanced species at that time.\n\nIf there were intelligent extraterrestrial civilizations able to make contact with distant Earth, they would have to live in the same 12Ka period of the 800Ma evolution of life.\n\nThe following discussion is adapted from Cramer. The Rare Earth equation is Ward and Brownlee's riposte to the Drake equation. It calculates formula_1, the number of Earth-like planets in the Milky Way having complex life forms, as:\n\nwhere:\nWe assume formula_5. The Rare Earth hypothesis can then be viewed as asserting that the product of the other nine Rare Earth equation factors listed below, which are all fractions, is no greater than 10 and could plausibly be as small as 10. In the latter case, formula_1 could be as small as 0 or 1. Ward and Brownlee do not actually calculate the value of formula_1, because the numerical values of quite a few of the factors below can only be conjectured. They cannot be estimated simply because we have but one data point: the Earth, a rocky planet orbiting a G2 star in a quiet suburb of a large barred spiral galaxy, and the home of the only intelligent species we know, namely ourselves.\n\nThe Rare Earth equation, unlike the Drake equation, does not factor the probability that complex life evolves into intelligent life that discovers technology (Ward and Brownlee are not evolutionary biologists). Barrow and Tipler review the consensus among such biologists that the evolutionary path from primitive Cambrian chordates, e.g., \"Pikaia\" to \"Homo sapiens\", was a highly improbable event. For example, the large brains of humans have marked adaptive disadvantages, requiring as they do an expensive metabolism, a long gestation period, and a childhood lasting more than 25% of the average total life span. Other improbable features of humans include:\n\nWriters who support the Rare Earth hypothesis:\n\nCases against the Rare Earth Hypothesis take various forms.\n\nThe hypothesis concludes, more or less, that complex life is rare because it can evolve only on the surface of an Earth-like planet or on a suitable satellite of a planet. Some biologists, such as Jack Cohen, believe this assumption too restrictive and unimaginative; they see it as a form of circular reasoning.\n\nAccording to David Darling, the Rare Earth hypothesis is neither hypothesis nor prediction, but merely a description of how life arose on Earth. In his view Ward and Brownlee have done nothing more than select the factors that best suit their case.\n\nWhat matters is not whether there's anything unusual about the Earth; there's going to be something idiosyncratic about every planet in space. What matters is whether any of Earth's circumstances are not only unusual but also essential for complex life. So far we've seen nothing to suggest there is.\n\nCritics also argue that there is a link between the Rare Earth Hypothesis and the creationist ideas of intelligent design.\n\nAn increasing number of extrasolar planet discoveries are being made with planets in planetary systems known as of . Rare Earth proponents argue life cannot arise outside Sun-like systems. However, some exobiologists have suggested that stars outside this range may give rise to life under the right circumstances; this possibility is a central point of contention to the theory because these late-K and M category stars make up about 82% of all hydrogen-burning stars.\n\nCurrent technology limits the testing of important Rare Earth criteria: surface water, tectonic plates, a large moon and biosignatures are currently undetectable. Though planets the size of Earth are difficult to detect and classify, scientists now think that rocky planets are common around Sun-like stars. The Earth Similarity Index (ESI) of mass, radius and temperature provides a means of measurement, but falls short of the full Rare Earth criteria.\n\nSome argue that Rare Earth's estimates of rocky planets in habitable zones (formula_3 in the Rare Earth equation) are too restrictive. James Kasting cites the Titius-Bode law to contend that it is a misnomer to describe habitable zones as narrow when there is a 50% chance of at least one planet orbiting within one. In 2013 a study that was published in the journal Proceedings of the National Academy of Sciences calculated that about \"one in five\" of all sun-like stars are expected to have earthlike planets \"within the habitable zones of their stars\"; 8.8 billion of them therefore exist in the Milky Way galaxy alone. On 4 November 2013, astronomers reported, based on \"Kepler\" space mission data, that there could be as many as 40 billion Earth-sized planets orbiting in the habitable zones of sun-like stars and red dwarf stars within the Milky Way Galaxy. 11 billion of these estimated planets may be orbiting sun-like stars.\n\nThe requirement for a system to have a Jovian planet as protector (Rare Earth equation factor formula_15) has been challenged, affecting the number of proposed extinction events (Rare Earth equation factor formula_16). Kasting's 2001 review of Rare Earth questions whether a Jupiter protector has any bearing on the incidence of complex life. Computer modelling including the 2005 Nice model and 2007 Nice 2 model yield inconclusive results in relation to Jupiter's gravitational influence and impacts on the inner planets. A study by Horner and Jones (2008) using computer simulation found that while the total effect on all orbital bodies within the Solar System is unclear, Jupiter has caused more impacts on Earth than it has prevented. Lexell's Comet, a 1770 near miss that passed closer to Earth than any other comet in recorded history, was known to be caused by the gravitational influence of Jupiter. Grazier (2017) claims that the idea of Jupiter as a shield is a misinterpretation of a 1996 study by George Wetherill, and using computer models Grazier was able to demonstrate that Saturn protects Earth from more asteroids and comets than does Jupiter.\n\nWard and Brownlee argue that for complex life to evolve (Rare Earth equation factor formula_12), tectonics must be present to generate biogeochemical cycles, and predicted that such geological features would not be found outside of Earth, pointing to a lack of observable mountain ranges and subduction. There is, however, no scientific consensus on the evolution of plate tectonics on Earth. Though it is believed that tectonic motion first began around three billion years ago, by this time photosynthesis and oxygenation had already begun. Furthermore, recent studies point to plate tectonics as an episodic planetary phenomenon, and that life may evolve during periods of \"stagnant-lid\" rather than plate tectonic states.\n\nRecent evidence also points to similar activity either having occurred or continuing to occur elsewhere. The geology of Pluto, for example, described by Ward and Brownlee as \"without mountains or volcanoes ... devoid of volcanic activity\", has since been found to be quite the contrary, with a geologically active surface possessing organic molecules and mountain ranges like Tenzing Montes and Hillary Montes comparable in relative size to those of Earth, and observations suggest the involvement of endogenic processes. Plate tectonics has been suggested as a hypothesis for the Martian dichotomy, and in 2012 geologist An Yin put forward evidence for active plate tectonics on Mars. Europa has long been suspected to have plate tectonics and in 2014 NASA announced evidence of active subduction. In 2017, scientists studying the geology of Charon confirmed that icy plate tectonics also operated on Pluto's largest moon.\n\nKasting suggests that there is nothing unusual about the occurrence of plate tectonics in large rocky planets and liquid water on the surface as most should generate internal heat even without the assistance of radioactive elements. Studies by Valencia and Cowan suggest that plate tectonics may be inevitable for terrestrial planets Earth sized or larger, that is, Super-Earths, which are now known to be more common in planetary systems.\n\nThe hypothesis that molecular oxygen, necessary for animal life, is rare and that a Great Oxygenation Event (Rare Earth equation factor formula_12) could only have been triggered and sustained by tectonics, appears to have been invalidated by more recent discoveries.\n\nWard and Brownlee ask \"whether oxygenation, and hence the rise of animals, would ever have occurred on a world where there were no continents to erode\". Extraterrestrial free oxygen has recently been detected around other solid objects, including Mercury, Venus, Mars Jupiter's four Galilean moons, Saturn's moons Enceladus, Dione and Rhea and even the atmosphere of a comet. This has led scientists to speculate whether processes other than photosynthesis could be capable of generating an environment rich in free oxygen. Wordsworth (2014) concludes that oxygen generated other than through photodissociation may be likely on Earth-like exoplanets, and could actually lead to false positive detections of life. Narita (2015) suggests photocatalysis by titanium dioxide as a geochemical mechanism for producing oxygen atmospheres.\n\nSince Ward & Brownlee's assertion that \"there is irrefutable evidence that oxygen is a necessary ingredient for animal life\", anaerobic metazoa have been found that indeed do metabolise without oxygen. Spinoloricus nov. sp., for example, a species discovered in the hypersaline anoxic L'Atalante basin at the bottom of the Mediterranean Sea in 2010, appears to metabolise with hydrogen, lacking mitochondria and instead using hydrogenosomes. Stevenson (2015) has proposed other membrane alternatives for complex life in worlds without oxygen. In 2017, scientists from the NASA Astrobiology Institute discovered the necessary chemical preconditions for the formation of azotosomes on Saturn's moon Titan, a world that lacks atmospheric oxygen. Independent studies by Schirrmeister and by Mills concluded that Earth's multicellular life existed prior to the Great Oxygenation Event, not as a consequence of it.\n\nNASA scientists Hartman and McKay argue that plate tectonics may in fact slow the rise of oxygenation (and thus stymie complex life rather than promote it). Computer modelling by Tilman Spohn in 2014 found that plate tectonics on Earth may have arisen from the effects of complex life's emergence, rather than the other way around as the Rare Earth might suggest. The action of lichens on rock may have contributed to the formation of subduction zones in the presence of water. Kasting argues that if oxygenation caused the Cambrian explosion then any planet with oxygen producing photosynthesis should have complex life.\n\nThe importance of Earth's magnetic field to the development of complex life has been disputed. Kasting argues that the atmosphere provides sufficient protection against cosmic rays even during times of magnetic pole reversal and atmosphere loss by sputtering. Kasting also dismisses the role of the magnetic field in the evolution of eukaryotes, citing the age of the oldest known magnetofossils.\n\nThe requirement of a large moon (Rare Earth equation factor formula_14) has also been challenged. Even if it were required, such an occurrence may not be as unique as predicted by the Rare Earth Hypothesis. Recent work by Edward Belbruno and J. Richard Gott of Princeton University suggests that giant impactors such as those that may have formed the Moon can indeed form in planetary trojan points ( or Lagrangian point) which means that similar circumstances may occur in other planetary systems.\n\nRare Earth's assertion that the Moon's stabilization of Earth's obliquity and spin is a requirement for complex life has been questioned. Kasting argues that a moonless Earth would still possess habitats with climates suitable for complex life and questions whether the spin rate of a moonless Earth can be predicted. Although the giant impact theory posits that the impact forming the Moon increased Earth's rotational speed to make a day about 5 hours long, the Moon has slowly \"stolen\" much of this speed to reduce Earth's solar day since then to about 24 hours and continues to do so: in 100 million years Earth's solar day will be roughly 24 hours 38 minutes (the same as Mars's solar day); in 1 billion years, 30 hours 23 minutes. Larger secondary bodies would exert proportionally larger tidal forces that would in turn decelerate their primaries faster and potentially increase the solar day of a planet in all other respects like Earth to over 120 hours within a few billion years. This long solar day would make effective heat dissipation for organisms in the tropics and subtropics extremely difficult in a similar manner to tidal locking to a red dwarf star. Short days (high rotation speed) causes high wind speeds at ground level. Long days (slow rotation speed) cause the day and night temperatures to be too extreme.\n\nMany Rare Earth proponents argue that the Earth's plate tectonics would probably not exist if not for the tidal forces of the Moon. The hypothesis that the Moon's tidal influence initiated or sustained Earth's plate tectonics remains unproven, though at least one study implies a temporal correlation to the formation of the Moon. Evidence for the past existence of plate tectonics on planets like Mars which may never have had a large moon would counter this argument. Kasting argues that a large moon is not required to initiate plate tectonics.\n\nRare Earth proponents argue that simple life may be common, though complex life requires specific environmental conditions to arise. Critics consider life could arise on a moon of a gas giant, though this is less likely if life requires volcanicity. The moon must have stresses to induce tidal heating, but not so dramatic as seen on Jupiter's Io. However, the moon is within the gas giant's intense radiation belts, sterilizing any biodiversity before it can get established. Dirk Schulze-Makuch disputes this, hypothesizing alternative biochemistries for alien life. While Rare Earth proponents argue that only microbial extremophiles could exist in subsurface habitats beyond Earth, some argue that complex life can also arise in these environments. Examples of extremophile animals such as the \"Hesiocaeca methanicola\", an animal that inhabits ocean floor methane clathrates substances more commonly found in the outer Solar System, the tardigrades which can survive in the vacuum of space or \"Halicephalobus mephisto\" which exists in crushing pressure, scorching temperatures and extremely low oxygen levels 3.6 kilometres deep in the Earth's crust, are sometimes cited by critics as complex life capable of thriving in \"alien\" environments. Jill Tarter counters the classic counterargument that these species adapted to these environments rather than arose in them, by suggesting that we cannot assume conditions for life to emerge which are not actually known. There are suggestions that complex life could arise in sub-surface conditions which may be similar to those where life may have arisen on Earth, such as the tidally heated subsurfaces of Europa or Enceladus. Ancient circumvental ecosystems such as these support complex life on Earth such as Riftia pachyptila that exist completely independent of the surface biosphere.\n\n\n"}
{"id": "7917758", "url": "https://en.wikipedia.org/wiki?curid=7917758", "title": "Software evolution", "text": "Software evolution\n\nSoftware evolution is the term used in software engineering (specifically software maintenance) to refer to the process of developing software initially, then repeatedly updating it for various reasons. \n\nFred Brooks, in his key book \"The Mythical Man-Month\", states that over 90% of the costs of a typical system arise in the maintenance phase, and that any successful piece of software will inevitably be maintained.\n\nIn fact, Agile methods stem from maintenance-like activities in and around web based technologies, where the bulk of the capability comes from frameworks and standards.\n\nSoftware maintenance address bug fixes and minor enhancements and software evolution focus on adaptation and migration.\nSoftware technologies will continue to develop. These changes will require new laws and theories to be created and justified. Some models as well would require additional aspects in developing future programs. Innovations and improvements do increase unexpected form of software development. The maintenance issues also would probably change as to adapt to the evolution of the future software. Software processes are themselves evolving, after going through learning and refinements, it is always improve their efficiency and effectiveness.\n\nThe need for software evolution comes from the fact that no one is able to predict how user requirements will evolve \"a priori\" . \nIn other words, the existing systems are never complete and continue to evolve. As they evolve, the complexity of the systems will grow unless there is a better solution available to solve these issues. The main objectives of software evolution are ensuring functional relevance, reliability and flexibility of the system. Software evolution can be fully manual (based on changes by software engineers), partially automated (e.g. using refactoring tools) or fully automated (with autonomous configuration or evolution). \n\nSoftware evolution has been greatly impacted by the Internet:\n\n\n\nE.B. Swanson initially identified the \nthree categories of maintenance: corrective, adaptive, and perfective. Four categories of software were then catalogued by Lientz and Swanson (1980).\nThese have since been updated and normalized internationally in the ISO/IEC 14764:2006:\n\n\nAll of the preceding take place when there is a known requirement for change.\n\nAlthough these categories were supplemented by many authors like Warren et al. (1999) and Chapin (2001), the ISO/IEC 14764:2006 international standard has kept the basic four categories.\n\nMore recently the description of software maintenance and evolution has been done using ontologies (Kitchenham et al. (1999), Deridder (2002), Vizcaíno (2003), Dias (2003), and Ruiz (2004)), which enrich the description of the many evolution activities.\n\nCurrent trends and practices are projected forward using a new model of software evolution called the staged model [1]. Staged model was introduced to replace conventional analysis which is less suitable for modern software development is rapid changing due to its difficulties of hard to contribute in software evolution. There are five distinct stages contribute in simple staged model (Initial development, Evolution, Servicing, Phase-out, and Close-down). \n\nProf. Meir M. Lehman, who worked at Imperial College London from 1972 to 2002, and his colleagues have identified a set of behaviours in the evolution of proprietary software. These behaviours (or observations) are known as Lehman's Laws, and there are eight of them:\n\n\nIt is worth mentioning that the applicability of all of these laws for all types of software systems has been studied by several researchers. For example, see a presentation by Nanjangud C Narendra where he describes a case study of an enterprise Agile project in the light of Lehman’s laws of software evolution. Some empirical observations coming from the study of open source software development appear to challenge some of the laws .\n\nThe laws predict that the need for functional change in a software system is inevitable, and not a consequence of incomplete or incorrect analysis of requirements or bad programming. They state that there are limits to what a software development team can achieve in terms of safely implementing changes and new functionality.\n\nMaturity Models specific to software evolution have been developed to improve processes, and help to ensure continuous rejuvenation of the software as it evolves iteratively.\n\nThe \"global process\" that is made by the many stakeholders (e.g. developers, users, their managers) has many feedback loops. The evolution speed is a function of the feedback loop structure and other characteristics of the global system. Process simulation techniques, such as system dynamics can be useful in understanding and managing such global process.\n\nSoftware evolution is not likely to be Darwinian, Lamarckian or Baldwinian, but an important phenomenon on its own. Given the increasing dependence on software at all levels of society and economy, the successful evolution of software is becoming increasingly critical. This is an important topic of research that hasn't received much attention.\n\nThe evolution of software, because of its rapid path in comparison to other man-made entities, was seen by Lehman as the \"fruit fly\" of the study of the evolution of artificial systems.\n\n\n\n"}
{"id": "233636", "url": "https://en.wikipedia.org/wiki?curid=233636", "title": "Spherical Earth", "text": "Spherical Earth\n\nThe earliest reliably documented mention of the spherical Earth concept dates from around the 6th century BC when it appeared in ancient Greek philosophy but remained a matter of speculation until the 3rd century BC, when Hellenistic astronomy established the spherical shape of the Earth as a physical given and calculated Earth's circumference. The paradigm was gradually adopted throughout the Old World during Late Antiquity and the Middle Ages. A practical demonstration of Earth's sphericity was achieved by Ferdinand Magellan and Juan Sebastián Elcano's expedition's circumnavigation (1519–1522).\n\nThe concept of a spherical Earth displaced earlier beliefs in a flat Earth: In early Mesopotamian mythology, the world was portrayed as a flat disk floating in the ocean with a hemispherical sky-dome above, and this forms the premise for early world maps like those of Anaximander and Hecataeus of Miletus. Other speculations on the shape of Earth include a seven-layered ziggurat or cosmic mountain, alluded to in the Avesta and ancient Persian writings (see seven climes).\n\nThe realization that the figure of the Earth is more accurately described as an ellipsoid dates to the 17th century, as described by Isaac Newton in \"Principia\". In the early 19th century, the flattening of the earth ellipsoid was determined to be of the order of 1/300 (Delambre, Everest). The modern value as determined by the US DoD World Geodetic System since the 1960s is close to 1/298.25.\n\nThe Earth is massive enough that gravity maintains it as a roughly spherical shape. Its formation into a sphere was made easy by its primordial hot, liquid phase.\n\nThe Solar System formed from a dust cloud that was at least partially the remnant of one or more supernovas that created heavy elements by nucleosynthesis. Grains of matter accreted through electrostatic interaction. As they grew in mass, gravity took over in gathering yet more mass, releasing the potential energy of their collisions and in-falling as heat. The protoplanetary disk also had a greater proportion of radioactive elements than the Earth today because, over time, those elements decayed. Their decay heated the early Earth even further, and continue to contribute to Earth's internal heat budget. The early Earth was thus mostly liquid.\n\nA sphere is the only stable shape for a non-rotating, gravitationally self-attracting liquid. The outward acceleration caused by the Earth's rotation is greater at the equator than at the poles (where is it zero), so the sphere gets deformed into an ellipsoid, which represents the shape having the lowest potential energy for a rotating, fluid body. This ellipsoid is slightly fatter around the equator than a perfect sphere would be. Earth's shape is also slightly lumpy because it is composed of different materials of different densities that exert slightly different amounts of gravitational force per volume.\n\nThe liquidity of a hot, newly formed planet allows heavier elements to sink down to the middle and forces lighter elements closer to the surface, a process known as planetary differentiation. This event is known as the iron catastrophe; the most abundant heavier elements were iron and nickel, which now form the Earth's core.\n\nThough the surface rocks of the Earth have cooled enough to solidify, the outer core of the planet is still hot enough to remain liquid. Energy is still being released; volcanic and tectonic activity has pushed rocks into hills and mountains and blown them out of calderas. Meteors also create impact craters and surrounding ridges. However, if the energy release ceases from these processes, then they tend to erode away over time and return toward the lowest potential-energy curve of the ellipsoid. Weather powered by solar energy can also move water, rock, and soil to make the Earth slightly out of round.\n\nEarth undulates as the shape of its lowest potential energy changes daily due to the gravity of the Sun and Moon as they move around with respect to the Earth. This is what causes tides in the oceans' water, which can flow freely along the changing potential.\n\nThe IAU definitions of planet and dwarf planet require that a Sun-orbiting body has undergone the rounding process to reach a roughly spherical shape, an achievement known as hydrostatic equilibrium. The same spheroidal shape can be seen from smaller rocky planets like Mars to gas giants like Jupiter.\n\nAny natural Sun-orbiting body that has not reached hydrostatic equilibrium is classified by the IAU as a small Solar System body (SSB). These come in many non-spherical shapes which are lumpy masses accreted haphazardly by in-falling dust and rock; not enough mass falls in to generate the heat needed to complete the rounding. Some SSSBs are just collections of relatively small rocks that are weakly held next to each other by gravity but are not actually fused into a single big bedrock. Some larger SSSBs are nearly round but have not reached hydrostatic equilibrium. The small Solar System body 4 Vesta is large enough to have undergone at least partial planetary differentiation.\n\nStars like the Sun are also spheroidal due to gravity's effects on their plasma, which is a free-flowing fluid. Ongoing stellar fusion is a much greater source of heat for stars compared to the initial heat released during formation.\n\nThe roughly spherical shape of the Earth can be confirmed by many different types of observation from ground level, aircraft, and spacecraft. The shape causes a number of phenomena that a flat Earth would not. Some of these phenomena and observations would be possible on other shapes, such as a curved disc or torus, but no other shape would explain all of them.\n\nMany pictures have been taken of the entire Earth by satellites launched by a variety of governments and private organizations. From high orbits, where half the planet can be seen at once, it is plainly spherical. The only way to piece together all the pictures taken of the ground from lower orbits so that all the surface features line up seamlessly and without distortion is to put them on an approximately spherical surface.\n\nAstronauts in low Earth orbit can personally see the curvature of the planet, and travel all the way around several times a day.\n\nThe astronauts who travelled to the Moon have seen the entire Moon-facing half at once, and can watch the sphere rotate once a day (approximately; the Moon is also moving with respect to the Earth).\n\nPeople in high-flying aircraft or skydiving from high-altitude balloons can plainly see the curvature of the Earth. Commercial aircraft do not necessarily fly high enough to make this obvious. Trying to measure the curvature of the horizon by taking a picture is complicated by the fact that camera lenses can produce distorted images depending on the angle used. An extreme version of this effect can be seen in the fisheye lens. Scientific measurements would require a carefully calibrated lens.\n\nThe fastest way for an airplane to travel between two distant cities is a great circle route, which deviates significantly from what would be the fastest straight-line travel path on a flat Earth.\n\nPhotos of the ground taken from airplanes over a large enough area also do not fit seamlessly together on a flat surface, but do fit on a roughly spherical surface. Aerial photographs of large areas must be corrected to account for curvature.\n\nOn a completely flat Earth with no visual interference (such as trees, hills, or atmospheric haze) the ground itself would never obscure distant objects; one would be able to see all the way to the edge of the surface. A spherical surface has a horizon which is closer when viewed from a lower altitude. In theory, a person standing on the surface with eyes above the ground can see the ground up to about away, but a person at the top of the Eiffel Tower at can see the ground up to about away.\n\nThis phenomenon would seem to present a method to verify that the Earth's surface is locally convex. If the degree of curvature was determined to be the same everywhere on the Earth's surface, and that surface was determined to be large enough, it would show that the Earth is spherical.\n\nIn practice, this turns out to be an unreliable method of measurement, due to variations in atmospheric refraction. This additional effect can give the impression that the earth's surface is flat, curved more convexly than it is, or even that it is concave, by bending light travelling near the surface of the earth (as happened in various trials of the famous Bedford Level experiment).\n\nThe phenomenon of variable atmospheric bending can be empirically confirmed by noting that sometimes the refractive layers of air can cause the image of a distant object to be broken into pieces or even turned upside down. This is commonly seen at sunset, when the sun's shape is distorted, but has also been photographed happening for ships, and has caused the city of Chicago to appear normally, upside down, and broken into pieces from across Lake Michigan (from where it is normally below the horizon). Because of their longer wavelengths, radio waves are even more susceptible to atmospheric refraction and reflection, which can cause radio and television signals to be received from towers thousands of miles away which cannot be seen with visible light.\n\nWhen the atmosphere is relatively well-mixed, the visual effects generally expected of a spherical Earth can be observed. For example, ships travelling on large bodies of water (such as the ocean) disappear over the horizon progressively, such that the highest part of the ship can still be seen even when lower parts cannot, proportional to distance from the observer. The same is true of the coastline or mountain when viewed from a ship or from across a large lake or flat terrain.\n\nThe shadow of the Earth on the Moon during a lunar eclipse is always a dark circle that moves from one side of the moon to the other (partially grazing it during a partial eclipse). This could be produced by a flat disc that always faces the Moon head-on during the eclipse, but this is inconsistent with the fact that the Moon is only rarely directly overhead during an eclipse. For each eclipse, the local surface of the Earth is pointed in a somewhat different direction. The shadow of a circular disc held at an angle is an oval, not a circle as is seen during the eclipse. The idea of the Earth being a flat disc is also inconsistent with the fact that a given lunar eclipse is only visible from half of the Earth at a time.\n\nThe only shape that casts a round shadow no matter which direction it is pointed is a sphere, and the ancient Greeks deduced that this must mean the Earth is spherical.\n\nOn a perfectly spherical Earth, flat terrain or ocean, when viewed from the surface, blocks exactly half the sky - a hemisphere of 180°. Moving away from the surface of the Earth means that the ground blocks less and less of the sky. For example, when viewed from the Moon, the Earth blocks only a small portion of the sky, because it is so distant. This phenomenon of geometry means that when viewed from a high mountain, flat ground or ocean blocks less than 180° of the sky. The rate of change in the angle blocked by the sky as altitude increases is different for a disc than a sphere, and values observed show that the Earth is locally convex. (The angles blocked would also be different for a mountain close to the edge of a flat Earth compared to a mountain in the middle of a flat Earth, and this is not observed.) In theory, measurements of this type from all around the Earth would confirm that it is a complete sphere (as opposed to some other shape with convex areas) though actually taking all those measurements would be very expensive.\n\nUsing other evidence to hypothesize a spherical shape, the medieval Iranian scholar Al-Biruni used this phenomenon to calculate the Earth's circumference to within of the correct value.\n\nThe fixed stars can be demonstrated to be very far away, by diurnal parallax measurements (a technique known at least as early as Ancient Greece). Unlike the Sun, Moon, and planets, they do not change position with respect to one another (at least not perceptibly over the span of a human lifetime); the shapes of the constellations are always the same. This makes them a convenient reference background for determining the shape of the Earth. Adding distance measurements on the ground allows calculation of the Earth's size.\n\nThe fact that different stars are visible from different locations on the Earth was noticed in ancient times. Aristotle wrote that some stars are visible from Egypt which are not visible from Europe. This would not be possible if the Earth was flat.\n\nAt the North Pole it is continuously nighttime for six months of the year and the same hemisphere of stars (a 180° view) are always visible making one counterclockwise rotation every 24 hours. The star Polaris (the \"North Star\") is almost at the center of this rotation (which is directly overhead). Some of the 88 modern constellations visible are Ursa Major (including the Big Dipper), Cassiopeia, and Andromeda. The other six months of the year, it is continuously daytime and the light from the Sun mostly blots out the stars. (The location of the poles can be defined by these phenomena, which only occur there; more than 24 hours of continuous daylight can occur north of the Arctic Circle and south of the Antarctic Circle.)\n\nAt the South Pole, a completely non-overlapping set of constellations are visible during the six months of continuous nighttime, including Orion, Crux, and Centaurus. This 180° hemisphere of stars rotate clockwise once every 24 hours, around a point directly overhead (where there do not happen to be any particularly bright stars).\n\nThe fact that the stars visible from the north and south poles do not overlap must mean that the two observation spots are on opposite sides of the Earth, which is not possible if the Earth is a single-sided disc, but is possible for other shapes (like a sphere, but also any other convex shape like a donut or dumbbell).\n\nFrom any point on the equator, 360° of stars are visible over the course of the night, as the sky rotates around a line drawn from due north to due south (which could be defined as \"the directions to walk to get to the poles in the shortest amount of time\"). When facing east, the stars visible from the north pole are on the left, and the stars visible from the south pole are on the right. This means the equator must be facing at a 90° angle from the poles.\n\nThe direction any intermediate spot on the Earth is facing can also be calculated by measuring the angles of the fixed stars and determining how much of the sky is visible. For example, New York City is about 40° north of the equator. The apparent motion of the Sun blots out slightly different parts of the sky from day to day, but over the course of the entire year it sees a dome of 280° (360° - 80°). So for example, both Orion and the Big Dipper are visible during at least part of the year.\n\nMaking stellar observations from a representative set of points across the Earth, combined with knowing the shortest on-the-ground distance between any two given points makes an approximate sphere the only possible shape for the Earth.\n\nKnowing the difference in angle between two points on the Earth's surface and the surface distance between them allows a calculation of the Earth's size. Using observations at Rhodes (in Greece) and Alexandria (in Egypt) and the distance between them, the Ancient Greek philosopher Posidonius actually did use this technique to calculate the circumference of the planet to within perhaps 4% of the correct value (though modern equivalents of his units of measure are not precisely known).\n\nSince the 1500s, many people have sailed or flown completely around the world in all directions, and none have discovered an edge or impenetrable barrier. (See Circumnavigation, Arctic exploration, and History of Antarctica.)\n\nSome flat Earth theories that propose the world is a north-pole-centered disc, conceive of Antarctica as an impenetrable ice wall that encircles the planet and hides any edges. This disc model explains east-west circumnavigation as simply moving around the disc in a circle. (East-west paths form a circle in both disc and spherical geometry.) It is possible in this model to traverse the North Pole, but it is not possible to perform a circumnavigation that includes the South Pole (which it posits does not exist).\n\nExplorers, government researchers, commercial pilots, and tourists have been to Antarctica and found that it is not a large ring that encircles the entire world, but actually a roughly disc-shaped continent smaller than South America but larger than Australia, with an interior that can in fact be traversed in order to take a shorter path from e.g. the tip of South America to Australia than would be possible on a disc.\n\nThe first land crossing of the entirety of Antarctica was the Commonwealth Trans-Antarctic Expedition in 1955-58, and many exploratory airplanes have since passed over the continent in various directions.\n\nOn a flat Earth, an omnidirectional Sun (emitting light in all directions, as it does) would illuminate the entire surface at the same time, and all places would experience sunrise and sunset at the horizon at the same time (with some small variations due to mountains and valleys). With a spherical Earth, half the planet is in daylight at any given time (the hemisphere facing the Sun) and the other half is experiencing nighttime. When a given location on the spherical Earth is in sunlight, its antipode - the location exactly on the opposite side of the Earth - is always experiencing nighttime. The spherical shape of the Earth causes the Sun to rise and set at different times in different places, and different locations get different amounts of sunlight each day. \n\nIn order to explain day and night, time zones, and the seasons, some flat Earth theorists propose that the Sun does not emit light in all directions, but acts more like a spotlight, only illuminating part of the flat Earth at a time. This theory is not consistent with observation; at sunrise and sunset, a spotlight Sun would be up in the sky at least a little bit, rather than at the horizon where it is always actually observed. A spotlight Sun would also appear at different angles in the sky with respect to a flat ground than it does with respect to a curved ground. Assuming light travels in straight lines, actual measurements of the Sun's angle in the sky from locations very distant from each other are only consistent with a geometry where the Sun is very far away and is being seen from a hemispherical surface (the daylight half of the Earth). These two phenomena are related: a low-altitude spotlight Sun would spent most of the day near the horizon for most locations on Earth (which is not observed), but rise and set fairly close to the horizon. A high-altitude Sun would spend more of the day away from the horizon, but rise and set fairly far from the horizon (which is not observed).\n\nAncient timekeeping reckoned \"noon\" as the time of day when the sun is highest in the sky, with the rest of the hours in the day measured against that. During the day, the apparent solar time can be measured directly with a sundial. In ancient Egypt, the first known sundials divided the day into 12 hours, though because the length of the day changed with the season, the length of the hours also changed. Sundials that defined hours as always being the same duration appeared in the Renaissance. In Western Europe, clock towers and striking clocks were used in the Middle Ages to keep people nearby appraised of the local time, though compared to modern times this was less important in a largely agrarian society.\n\nBecause the Sun reaches its highest point at different times for different longitudes (about four minutes of time for every degree of longitude difference east or west), the local solar noon in each city is different except for those directly north or south of each other. This means that the clocks in different cities could be offset from each other by minutes or hours. As clocks became more precise and industrialization made timekeeping more important, cities switched to mean solar time, which ignores minor variations in the timing of local solar noon over the year, due to the elliptical nature of the Earth's orbit, and its tilt.\n\nThe differences in clock time between cities was not generally a problem until the advent of railroad travel in the 1800s, which both made travel between distant cities much faster than by walking or horse, and also required passengers to show up at specific times to meet their desired trains. In the United Kingdom, railroads gradually switched to Greenwich Mean Time (set from local time at the Greenwich observatory in London), followed by public clocks across the country generally, forming a single time zone. In the United States, railroads published schedules based on local time, then later based on standard time for that railroad (typically the local time at the railroad's headquarters), and then finally based on four standard time zones shared across all railroads, where neighboring zones differed by exactly one hour. At first railroad time was synchronized by portable chronometers, and then later by telegraph and radio signals.\n\nSan Francisco is at 122.41°W longitude and Richmond, Virginia is at 77.46°W longitude. They are both at about 37.6°N latitude (±.2°). The approximately 45° of longitude difference translates into about 180 minutes, or 3 hours, of time between sunsets in the two cities, for example. San Francisco is in the Pacific Time zone, and Richmond is in the Eastern Time zone, which are three hours apart, so the local clocks in each city show that the sun sets at about the same time when using the local time zone. But a phone call from Richmond to San Francisco at sunset will reveal that there are still three hours of daylight left in California.\n\nOn a flat Earth with an omnidirectional Sun, all places would experience the same amount of daylight every day, and all places would get daylight at the same time. Actual day length varies considerably, with places closer to the poles getting very long days in the summer and very short days in the winter, with northerly summer happening at the same time as southerly winter. Places north of the Arctic Circle and south of the Antarctic Circle get no sunlight for at least one day a year, and get 24-hour sunlight for at least one day a year. Both the poles experience sunlight for 6 months and darkness for 6 months, at opposite times.\n\nThe movement of daylight between the northern and southern hemispheres happens because of the axial tilt of the Earth. The imaginary line around which the Earth spins, which goes between the North Pole and South Pole, is tilted about 23° from the oval that describes its orbit around the Sun. The Earth always points in the same direction as it moves around the Sun, so for half the year (summer in the Northern Hemisphere), the North Pole is pointed slightly toward the Sun, keeping it in daylight all the time because the Sun lights up the half of the Earth that is facing it (and the North Pole is always in that half due to the tilt). For the other half of the orbit, the South Pole is tilted slightly toward the Sun, and it is winter in the Northern Hemisphere. This means that at the equator, the Sun is not directly overhead at noon, except around the autumnal equinox and vernal equinox, when one spot on the equator is pointed directly at the Sun.\n\nThe length of the day varies because as the Earth rotates some places (near the poles) pass through only a short curve near the top or bottom of the sunlight half; other places (near the equator) travel along much longer curves through the middle.\n\nThe length of twilight would be very different on a flat Earth. On a round Earth, the atmosphere above the ground is lit for a while before sunrise and after sunset are observed at ground level, because the Sun is still visible from higher altitudes. Longer twilights are observed at higher latitudes (near the poles) due to a shallower angle of the Sun's apparent movement compared to the horizon. On a flat Earth, the Sun's shadow would reach the upper atmosphere very quickly, except near the closest edge of the Earth, and would always set at the same angle to the ground (which is not what is observed). The \"spotlight Sun\" theory is also not consistent with this observation, since the air cannot be lit without the ground below it also being lit (except for shadows of mountains and other surface obstacles).\n\nOn a given day, if many different cities measure the angle of the Sun at local noon, the resulting data, when combined with the known distances between cities, shows that the Earth has 180 degrees of north-south curvature. (A full range of angles will be observed if the north and south poles are included, and the day chosen is either the autumnal or spring equinox.) This is consistent with many rounded shapes, including a sphere, and is inconsistent with a flat shape.\n\nSome claim that this experiment assumes a very distant Sun, such that the incoming rays are essentially parallel, and if a flat Earth is assumed, that the measured angles can allow one to calculate the distance to the Sun, which must be small enough that its incoming rays are not very parallel. However, if more than two relatively well-separated cities are included in the experiment, the calculation will make clear whether the Sun is distant or nearby. For example, on the equinox, the 0 degree angle from the North Pole and the 90 degree angle from the equator predict a Sun which would have to be located essentially next to the surface of a flat Earth, but the difference in angle between the equator and New York City would predict a Sun much further away if the Earth is flat. Because these results are contradictory, the surface of the Earth cannot be flat; the data \"is\" consistent with a nearly spherical Earth and a Sun which is very far away compared with the diameter of the Earth.\n\nUsing the knowledge that the Sun is very far away, the ancient Greek geographer Eratosthenes performed an experiment using the differences in the observed angle of the Sun from two different locations to calculate the circumference of the Earth. Though modern telecommunications and timekeeping were not available, he was able to make sure the measurements happened at the same time by having them taken when the Sun was highest in the sky (local noon) at both locations. Using slightly inaccurate assumptions about the locations of two cities, he came to a result within 15% of the correct value.\n\nOn level ground, the difference in the distance to the horizon between lying down and standing up is large enough to watch the Sun set twice by quickly standing up immediately after seeing it set for the first time while lying down. This also can be done with a cherry picker or a tall building with a fast elevator. On a flat Earth, one would not be able to see the Sun again (unless standing near the edge closest to the Sun) due to a much faster-moving Sun shadow. \n\nWhen the supersonic Concorde took off not long after sunset from London and flew westward to New York faster than the sunset was advancing westward on the ground, passengers observed a sunrise in the west. After landing in New York, passengers saw a second sunset in the west.\n\nBecause the speed of the Sun's shadow is slower in polar regions (due to the steeper angle), even a subsonic aircraft can overtake the sunset when flying at high latitudes. One photographer used a roughly circular route around the North Pole to take pictures of 24 sunsets in the same 24-hour period, pausing westward progress in each time zone to let the shadow of the Sun catch up. The surface of the Earth rotates at at 80° north or south, and at the equator.\n\nBecause the Earth is spherical, long-distance travel sometimes requires heading in different directions than one would head on a flat Earth.\n\nFor example, consider an airplane that travels in a straight line, takes a 90-degree right turn, travels another , takes another 90-degree right turn, and travels a third time. On a flat Earth, the aircraft would have travelled along three sides of a square, and arrive at a spot about from where it started. But because the Earth is spherical, in reality it will have travelled along three sides of a triangle, and arrive back very close to its starting point. If the starting point is the North Pole, it would have travelled due south from the North Pole to the equator, then west for a quarter of the way around the Earth, and then due north back to the North Pole.\n\nIn spherical geometry, the sum of angles inside a triangle is greater than 180° (in this example 270°, having arrived back at the north pole a 90° angle to the departure path) unlike on a flat surface, where it is always exactly 180°.\n\nA meridian of longitude is a line where local solar noon occurs at the same time each day. These lines define \"north\" and \"south\". These are perpendicular to lines of latitude that define \"east\" and \"west\", where the Sun is at the same angle at local noon on the same day. If the Sun were travelling from east to west over a flat Earth, meridian lines would always be the same distance apart - they would form a square grid when combined with lines of latitude. In reality, meridian lines get farther apart as one travels toward the equator, which is only possible on a round Earth. In places where land is plotted on a grid system, this causes discontinuities in the grid. For example, in areas of the Midwestern United States that use the Public Land Survey System, the northernmost and westernmost sections of a township deviate from what would otherwise be an exact square mile. The resulting discontinuities are sometimes reflected directly in local roads, which have kinks where the grid cannot follow completely straight lines.\n\nLow-pressure weather systems with inward winds (such as a hurricane) spin counterclockwise north of the equator, but clockwise south of the equator. This is due to the Coriolis force, and requires that (assuming they are attached to each other and rotating in the same direction) the north and southern halves of the Earth are angled in opposite directions (e.g. the north is facing toward Polaris and the south is facing away from it).\n\nThe laws of gravity, chemistry, and physics that explain the formation and rounding of the Earth are well-tested through experiment, and applied successfully to many engineering tasks.\n\nFrom these laws, we know the amount of mass the Earth contains, and that a non-spherical planet the size of the Earth would not be able to support itself against its own gravity. A flat disc the size of the Earth, for example, would likely crack, heat up, liquefy, and re-form into a roughly spherical shape. On a disc strong enough to maintain its shape, gravity would not pull downward with respect to the surface, but would pull toward the center of the disc, contrary to what is observed on level terrain (and which would create major problems with oceans flowing toward the center of the disk).\n\nIgnoring the other concerns, some flat Earth theorists explain the observed surface \"gravity\" by proposing that the flat Earth is constantly accelerating upwards. Such a theory would also leave open for explanation the tides seen in Earth's oceans, which are conventionally explained by the gravity exerted by the Sun and Moon.\n\nObservation of Foucault pendulums, popular in science museums around the world, demonstrate both that the world is spherical and that it rotates (not that the stars are rotating around it).\n\nThe mathematics of navigation by GPS assume that satellites are moving in known orbits around an approximately spherical surface. The accuracy of GPS navigation in determining latitude and longitude and the way these numbers map onto locations on the ground show that these assumptions are correct. The same is true for the operational GLONASS system run by Russia, and the in-development European Galileo, Chinese BeiDou, and Indian IRNSS.\n\nSatellites, including communications satellites used for television, telephone, and Internet connections, would not stay in orbit unless the modern theory of gravitation were correct. The details of which satellites are visible from which places on the ground at which times prove an approximately spherical shape of the Earth. (Undersea cables are also used for intercontinental communications.)\n\nRadio transmitters are mounted on tall towers because they generally rely on line-of-sight propagation. The distance to the horizon is further at higher altitude, so mounting them higher significantly increases the area they can serve. Some signals can be transmitted at much longer distances, but only if they are at frequencies where they can use groundwave propagation, tropospheric propagation, tropospheric scatter, or ionospheric propagation to reflect or refract signals around the curve of the Earth.\n\nThe design of some large structures needs to take the shape of the Earth into account. For example, the towers of the Humber Bridge, although both vertical with respect to gravity, are farther apart at the top than the bottom due to the local curvature.\n\nThe Hebrew Bible imagined a three-part world, with the heavens (\"shamayim\") above, earth (\"eres\") in the middle, and the underworld (\"sheol\") below. After the 4th century BCE this was gradually replaced by a Greek scientific cosmology of a spherical earth surrounded by multiple concentric heavens.\n\nThough the earliest written mention of a spherical Earth comes from ancient Greek sources, there is no account of how the sphericity of the Earth was discovered. A plausible explanation is that it was \"the experience of travellers that suggested such an explanation for the variation in the observable altitude of the pole and the change in the area of circumpolar stars, a change that was quite drastic between Greek settlements\" around the eastern Mediterranean Sea, particularly those between the Nile Delta and Crimea.\n\nIn \"The Histories\", written 431–425 BC, Herodotus cast doubt on a report of the Sun observed shining from the north. He stated that the phenomenon was observed during a circumnavigation of Africa undertaken by Phoenician explorers employed by Egyptian pharaoh Necho II c. 610–595 BC (, 4.42) who claimed to have had the Sun on their right when circumnavigating in a clockwise direction. To modern historians, these details confirm the truth of the Phoenicians' report and even open the possibility that the Phoenicians knew about the spherical model. However, nothing certain about their knowledge of geography and navigation has survived.\n\nEarly Greek philosophers alluded to a spherical Earth, though with some ambiguity. Pythagoras (6th century BC) was among those said to have originated the idea, but this might reflect the ancient Greek practice of ascribing every discovery to one or another of their ancient wise men. Some idea of the sphericity of the Earth seems to have been known to both Parmenides and Empedocles in the 5th century BC, and although the idea cannot reliably be ascribed to Pythagoras, it might nevertheless have been formulated in the Pythagorean school in the 5th century BC although some disagree. After the 5th century BC, no Greek writer of repute thought the world was anything but round.\n\nPlato (427–347 BC) travelled to southern Italy to study Pythagorean mathematics. When he returned to Athens and established his school, Plato also taught his students that Earth was a sphere, though he offered no justifications. \"My conviction is that the Earth is a round body in the centre of the heavens, and therefore has no need of air or of any similar force to be a support\". If man could soar high above the clouds, Earth would resemble \"one of those balls which have leather coverings in twelve pieces, and is decked with various colours, of which the colours used by painters on Earth are in a manner samples.\"\nIn Timaeus, his one work that was available throughout the Middle Ages in Latin, we read that the Creator \"made the world in the form of a globe, round as from a lathe, having its extremes in every direction equidistant from the centre, the most perfect and the most like itself of all figures\", though the word \"world\" here refers to the heavens.\n\nAristotle (384–322 BC) was Plato's prize student and \"the mind of the school\". Aristotle observed \"there are stars seen in Egypt and [...] Cyprus which are not seen in the northerly regions.\" Since this could only happen on a curved surface, he too believed Earth was a sphere \"of no great size, for otherwise the effect of so slight a change of place would not be quickly apparent.\" (\"De caelo\", 298a2–10)\n\nAristotle provided physical and observational arguments supporting the idea of a spherical Earth:\n\n\nThe concepts of symmetry, equilibrium and cyclic repetition permeated Aristotle's work. In his \"Meteorology\" he divided the world into five climatic zones: two temperate areas separated by a torrid zone near the equator, and two cold inhospitable regions, \"one near our upper or northern pole and the other near the ... southern pole,\" both impenetrable and girdled with ice (\"Meteorologica\", 362a31–35). Although no humans could survive in the frigid zones, inhabitants in the southern temperate regions could exist.\n\nAristotle's theory of natural place relied on a spherical Earth to explain why heavy things go down (toward what Aristotle believed was the center of the Universe), and things like air and fire go up. In this geocentric model, the structure of the universe was believed to be a series of perfect spheres. The Sun, Moon, planets and fixed stars were believed to move on celestial spheres around a stationary Earth.\n\nThough Aristotle's theory of physics survived in the Christian world for many centuries, the heliocentric model was eventually shown to be a more correct explanation of the Solar System than the geocentric model, and atomic theory was shown to be a more correct explanation of the nature of matter than classical elements like earth, water, air, fire, and aether.\n\nIn proposition 2 of the First Book of his treatise \"On floating bodies,\" Archimedes demonstrates that \"The surface of any fluid at rest is the surface of a sphere whose centre is the same as that of the earth,\". Subsequently, in propositions 8 and 9 of the same work, he assumes the result of proposition 2 that the Earth is a sphere and that the surface of a fluid on it is a sphere centered on the center of the Earth.\n\nEratosthenes, a Greek astronomer from Hellenistic Cyrenaica (276–194 BC), estimated Earth's circumference around 240 BC. He had heard that in Syene the Sun was directly overhead at the summer solstice whereas in Alexandria it still cast a shadow. Using the differing angles the shadows made as the basis of his trigonometric calculations he estimated a circumference of around 250,000 \"stades\". The length of a 'stade' is not precisely known, but Eratosthenes's figure only has an error of around five to fifteen percent. Eratosthenes used rough estimates and round numbers, but depending on the length of the stadion, his result is within a margin of between 2% and 20% of the actual meridional circumference, . Note that Eratosthenes could only measure the circumference of the Earth by assuming that the distance to the Sun is so great that the rays of sunlight are practically parallel.\n\nSeventeen hundred years after Eratosthenes, Christopher Columbus studied Eratosthenes's findings before sailing west for the Indies. However, ultimately he rejected Eratosthenes in favour of other maps and arguments that interpreted Earth's circumference to be a third smaller than reality. If, instead, Columbus had accepted Eratosthenes findings, then he may have never gone west, since he didn't have the supplies or funding needed for the much longer voyage.\n\nSeleucus of Seleucia (c. 190 BC), who lived in the city of Seleucia in Mesopotamia, wrote that the Earth is spherical (and actually orbits the Sun, influenced by the heliocentric theory of Aristarchus of Samos).\n\nPosidonius (c. 135 – 51 BC) put faith in Eratosthenes's method, though by observing the star Canopus, rather than the sun in establishing the Earth's circumference. In Ptolemy's \"Geographia\", his result was favoured over that of Eratosthenes. Posidonius furthermore expressed the distance of the sun in earth radii.\n\nFrom its Greek origins, the idea of a spherical earth, along with much of Greek astronomical thought, slowly spread across the globe and ultimately became the adopted view in all major astronomical traditions.\n\nIn the West, the idea came to the Romans through the lengthy process of cross-fertilization with Hellenistic civilization. Many Roman authors such as Cicero and Pliny refer in their works to the rotundity of the earth as a matter of course.\n\nIt has been suggested that seafarers probably provided the first observational evidence that the Earth was not flat, based on observations of the horizon. This argument was put forward by the geographer Strabo (c. 64 BC – 24 AD), who suggested that the spherical shape of the Earth was probably known to seafarers around the Mediterranean Sea since at least the time of Homer, citing a line from the \"Odyssey\" as indicating that the poet Homer knew of this as early as the 7th or 8th century BC. Strabo cited various phenomena observed at sea as suggesting that the Earth was spherical. He observed that elevated lights or areas of land were visible to sailors at greater distances than those less elevated, and stated that the curvature of the sea was obviously responsible for this.\n\nClaudius Ptolemy (90–168 AD) lived in Alexandria, the centre of scholarship in the 2nd century. In the \"Almagest,\" which remained the standard work of astronomy for 1,400 years, he advanced many arguments for the spherical nature of the Earth. Among them was the observation that when a ship is sailing towards mountains, observers note these seem to rise from the sea, indicating that they were hidden by the curved surface of the sea. He also gives separate arguments that the Earth is curved north-south and that it is curved east-west.\n\nHe compiled an eight-volume \"Geographia\" covering what was known about the earth. The first part of the \"Geographia\" is a discussion of the data and of the methods he used. As with the model of the Solar System in the \"Almagest\", Ptolemy put all this information into a grand scheme. He assigned coordinates to all the places and geographic features he knew, in a grid that spanned the globe (although most of this has been lost). Latitude was measured from the equator, as it is today, but Ptolemy preferred to express it as the length of the longest day rather than degrees of arc (the length of the midsummer day increases from 12h to 24h as you go from the equator to the polar circle). He put the meridian of 0 longitude at the most western land he knew, the Canary Islands.\n\n\"Geographia\" indicated the countries of \"Serica\" and \"Sinae\" (China) at the extreme right, beyond the island of \"Taprobane\" (Sri Lanka, oversized) and the \"Aurea Chersonesus\" (Southeast Asian peninsula).\n\nPtolemy also devised and provided instructions on how to create maps both of the whole inhabited world (\"oikoumenè\") and of the Roman provinces. In the second part of the \"Geographia,\" he provided the necessary topographic lists, and captions for the maps. His \"oikoumenè\" spanned 180 degrees of longitude from the Canary Islands in the Atlantic Ocean to China, and about 81 degrees of latitude from the Arctic to the East Indies and deep into Africa. Ptolemy was well aware that he knew about only a quarter of the globe.\n\nKnowledge of the spherical shape of the Earth was received in scholarship of Late Antiquity as a matter of course, in both Neoplatonism and Early Christianity. Calcidius's fourth-century Latin commentary on and translation of Plato's \"Timaeus\", which was one of the few examples of Greek scientific thought that was known in the Early Middle Ages in Western Europe, discussed Hipparchus's use of the geometrical circumstances of eclipses to compute the relative diameters of the Sun, Earth, and Moon.\n\nTheological doubt informed by the flat Earth model implied in the Hebrew Bible inspired some early Christian scholars such as Lactantius, John Chrysostom and Athanasius of Alexandria, but this remained an eccentric current. Learned Christian authors such as Basil of Caesarea, Ambrose and Augustine of Hippo were clearly aware of the sphericity of the Earth. \"Flat Earthism\" lingered longest in Syriac Christianity, which tradition laid greater importance on a literalist interpretation of the Old Testament. Authors from that tradition, such as Cosmas Indicopleustes, presented the Earth as flat as late as in the 6th century. This last remnant of the ancient model of the cosmos disappeared during the 7th century. From the 8th century and the beginning medieval period, \"no cosmographer worthy of note has called into question the sphericity of the Earth.\"\n\nGreek ethnographer Megasthenes, c. 300 BC, has been interpreted as stating that the contemporary Brahmans believed in a spherical earth as the center of the universe. With the spread of Greek culture in the east, Hellenistic astronomy filtered eastwards to ancient India where its profound influence became apparent in the early centuries AD. The Greek concept of an Earth surrounded by the spheres of the planets and that of the fixed stars, vehemently supported by astronomers like Varahamihir and Brahmagupta, strengthened the astronomical principles. Some ideas were found possible to preserve, although in altered form.\n\nThe works of the classical Indian astronomer and mathematician, Aryabhatta (476–550 AD), deal with the sphericity of the Earth and the motion of the planets. The final two parts of his Sanskrit magnum opus, the \"Aryabhatiya\", which were named the \"Kalakriya\" (\"reckoning of time\") and the \"Gol\" (\"sphere\"), state that the Earth is spherical and that its circumference is 4,967 yojanas. In modern units this is , close to the current equatorial value of .\n\nKnowledge of the sphericity of the Earth survived into the medieval corpus of knowledge by direct transmission of the texts of Greek antiquity (Aristotle), and via authors such as Isidore of Seville and Beda Venerabilis.\nIt became increasingly traceable with the rise of scholasticism and medieval learning.\nSpread of this knowledge beyond the immediate sphere of Greco-Roman scholarship was necessarily gradual, associated with the pace of Christianisation of Europe. For example, the first evidence of knowledge of the spherical shape of the Earth in Scandinavia is a 12th-century Old Icelandic translation of \"Elucidarius\".\n\nA non-exhaustive list of more than a hundred Latin and vernacular writers from Late Antiquity and the Middle Ages who were aware that the earth was spherical has been compiled by Reinhard Krüger, professor for Romance literature at the University of Stuttgart.\nAmpelius, Chalcidius, Macrobius, Martianus Capella,\nBasil of Caesarea, Ambrose of Milan, Aurelius Augustinus, Paulus Orosius, Jordanes, Cassiodorus, Boethius, Visigoth king Sisebut.\n\nIsidore of Seville, Beda Venerabilis, Theodulf of Orléans, Vergilius of Salzburg,\nIrish monk Dicuil, Rabanus Maurus, King Alfred of England, Remigius of Auxerre, Johannes Scotus Eriugena, , Gerbert d’Aurillac (Pope Sylvester II).\n\nNotker the German of Sankt-Gallen, Hermann of Reichenau, Hildegard von Bingen, Petrus Abaelardus, Honorius Augustodunensis, Gautier de Metz, Adam of Bremen, Albertus Magnus, Thomas Aquinas, Berthold of Regensburg, Guillaume de Conches, , Abu-Idrisi, Bernardus Silvestris, Petrus Comestor, Thierry de Chartres, Gautier de Châtillon, Alexander Neckam, Alain de Lille, Averroes, Snorri Sturluson, Moshe ben Maimon, Lambert of Saint-Omer, Gervasius of Tilbury, Robert Grosseteste, Johannes de Sacrobosco, Thomas de Cantimpré, Peire de Corbian, Vincent de Beauvais, Robertus Anglicus, , Ristoro d'Arezzo, Roger Bacon, Jean de Meung, Brunetto Latini, Alfonso X of Castile.\n\nMarco Polo, Dante Alighieri, Meister Eckhart, Enea Silvio Piccolomini (Pope Pius II), Perot de Garbalei (\"divisiones mundi\"), Cecco d'Ascoli, , Levi ben Gershon, Konrad of Megenberg, Nicole Oresme, Petrus Aliacensis, , Toscanelli, , Jean de Mandeville, Christine de Pizan, Geoffrey Chaucer, William Caxton, Martin Behaim, Christopher Columbus.\n\nBishop Isidore of Seville (560–636) taught in his widely read encyclopedia, \"The Etymologies,\" that the Earth was \"round\". The bishop's confusing exposition and choice of imprecise Latin terms have divided scholarly opinion on whether he meant a sphere or a disk or even whether he meant anything specific. Notable recent scholars claim that he taught a spherical earth. Isidore did not admit the possibility of people dwelling at the antipodes, considering them as legendary and noting that there was no evidence for their existence.\n\n\nThe monk Bede (c. 672–735) wrote in his influential treatise on computus, \"The Reckoning of Time\", that the Earth was round. He explained the unequal length of daylight from \"the roundness of the Earth, for not without reason is it called 'the orb of the world' on the pages of Holy Scripture and of ordinary literature. It is, in fact, set like a sphere in the middle of the whole universe.\" (De temporum ratione, 32). The large number of surviving manuscripts of \"The Reckoning of Time,\" copied to meet the Carolingian requirement that all priests should study the computus, indicates that many, if not most, priests were exposed to the idea of the sphericity of the Earth. Ælfric of Eynsham paraphrased Bede into Old English, saying, \"Now the Earth's roundness and the Sun's orbit constitute the obstacle to the day's being equally long in every land.\"\n\nBede was lucid about earth's sphericity, writing \"We call the earth a globe, not as if the shape of a sphere were expressed in the diversity of plains and mountains, but because, if all things are included in the outline, the earth's circumference will represent the figure of a perfect globe... For truly it is an orb placed in the centre of the universe; in its width it is like a circle, and not circular like a shield but rather like a ball, and it extends from its centre with perfect roundness on all sides.\"\n\nThe 7th-century Armenian scholar Anania Shirakatsi described the world as \"being like an egg with a spherical yolk (the globe) surrounded by a layer of white (the atmosphere) and covered with a hard shell (the sky).\"\n\nIslamic astronomy was developed on the basis of a spherical earth inherited from Hellenistic astronomy. The Islamic theoretical framework largely relied on the fundamental contributions of Aristotle (\"De caelo\") and Ptolemy (\"Almagest\"), both of whom worked from the premise that the earth was spherical and at the centre of the universe (geocentric model).\n\nEarly Islamic scholars recognized Earth's sphericity, leading Muslim mathematicians to develop spherical trigonometry in order to further mensuration and to calculate the distance and direction from any given point on the Earth to Mecca. This determined the \"Qibla,\" or Muslim direction of prayer.\n\nAround 830 AD, Caliph al-Ma'mun commissioned a group of Muslim astronomers and geographers to measure the distance from Tadmur (Palmyra) to Raqqa in modern Syria. They found the cities to be separated by one degree of latitude and the meridian arc distance between them to be 66 miles and thus calculated the Earth's circumference to be 24,000 miles.\n\nAnother estimate given by his astronomers was 56 Arabic miles (111.8 km) per degree, which corresponds to a circumference of 40,248 km, very close to the currently modern values of 111.3 km per degree and 40,068 km circumference, respectively.\n\nAndalusian polymath Ibn Hazm stated that the proof of the Earth's sphericity \"is that the Sun is always vertical to a particular spot on Earth\".\n\nAl-Farghānī (Latinized as Alfraganus) was a Persian astronomer of the 9th century involved in measuring the diameter of the Earth, and commissioned by Al-Ma'mun. His estimate given above for a degree (56 Arabic miles) was much more accurate than the 60 Roman miles (89.7 km) given by Ptolemy. Christopher Columbus uncritically used Alfraganus's figure as if it were in Roman miles instead of in Arabic miles, in order to prove a smaller size of the Earth than that propounded by Ptolemy.\n\n\nAbu Rayhan Biruni (973–1048) used a new method to accurately compute the Earth's circumference, by which he arrived at a value that was close to modern values for the Earth's circumference. His estimate of 6,339.6 km for the Earth radius was only 31.4 km less than the modern mean value of 6,371.0 km. In contrast to his predecessors, who measured the Earth's circumference by sighting the Sun simultaneously from two different locations, Biruni developed a new method of using trigonometric calculations based on the angle between a plain and mountain top. This yielded more accurate measurements of the Earth's circumference and made it possible for a single person to measure it from a single location.\nBiruni's method was intended to avoid \"walking across hot, dusty deserts,\" and the idea came to him when he was on top of a tall mountain in India. From the top of the mountain, he sighted the angle to the horizon which, along with the mountain's height (which he calculated beforehand), allowed him to calculate the curvature of the Earth.\nHe also made use of algebra to formulate trigonometric equations and used the astrolabe to measure angles.\n\nAccording to John J. O'Connor and Edmund F. Robertson,\n\nMuslim scholars who held to the round Earth theory used it for a quintessentially Islamic purpose: to calculate the distance and direction from any given point on the Earth to Mecca. This determined the Qibla, or Muslim direction of prayer.\n\nA terrestrial globe (Kura-i-ard) was among the presents sent by the Persian Muslim astronomer Jamal-al-Din to Kublai Khan's Chinese court in 1267. It was made of wood on which \"seven parts of water are represented in green, three parts of land in white, with rivers, lakes etc.\" Ho Peng Yoke remarks that \"it did not seem to have any general appeal to the Chinese in those days\".\n\nDuring the High Middle Ages, the astronomical knowledge in Christian Europe was extended beyond what was transmitted directly from ancient authors by transmission of learning from Medieval Islamic astronomy. An early student of such learning was Gerbert d'Aurillac, the later Pope Sylvester II.\n\nSaint Hildegard (Hildegard von Bingen, 1098–1179), depicted the spherical earth several times in her work \"Liber Divinorum Operum\".\n\nJohannes de Sacrobosco (c. 1195 – c. 1256 AD) wrote a famous work on Astronomy called \"Tractatus de Sphaera,\" based on Ptolemy, which primarily considers the sphere of the sky. However, it contains clear proofs of the earth's sphericity in the first chapter.\n\nMany scholastic commentators on Aristotle's \"On the Heavens\" and Sacrobosco's \"Treatise on the Sphere\" unanimously agreed that the earth is spherical or round. Grant observes that no author who had studied at a medieval university thought that the earth was flat.\n\nThe \"Elucidarium\" of Honorius Augustodunensis (c. 1120), an important manual for the instruction of lesser clergy, which was translated into Middle English, Old French, Middle High German, Old Russian, Middle Dutch, Old Norse, Icelandic, Spanish, and several Italian dialects, explicitly refers to a spherical Earth. Likewise, the fact that Bertold von Regensburg (mid-13th century) used the spherical Earth as an illustration in a sermon shows that he could assume this knowledge among his congregation. The sermon was preached in the vernacular German, and thus was not intended for a learned audience.\n\nDante's \"Divine Comedy,\" written in Italian in the early 14th century, portrays Earth as a sphere, discussing implications such as the different stars visible in the southern hemisphere, the altered position of the sun, and the various timezones of the Earth.\n\nThe Portuguese exploration of Africa and Asia, Columbus's voyage to the Americas (1492) and, finally, Ferdinand Magellan's circumnavigation of the earth (1519–21) provided practical evidence of the global shape of the earth.\n\nThe first direct demonstration of Earth's sphericity came in the form of the first circumnavigation in history, an expedition captained by Portuguese explorer Ferdinand Magellan. The expedition was financed by the Spanish Crown. On August 10, 1519, the five ships under Magellan's command departed from Seville. They crossed the Atlantic Ocean, passed through what is now called the Strait of Magellan, crossed the Pacific, and arrived in Cebu, where Magellan was killed by Philippine natives in a battle. His second in command, the Spaniard Juan Sebastián Elcano, continued the expedition and, on September 6, 1522, arrived at Seville, completing the circumnavigation. Charles I of Spain, in recognition of his feat, gave Elcano a coat of arms with the motto \"Primus circumdedisti me\" (in Latin, \"You went around me first\").\n\nA circumnavigation alone does not prove that the earth is spherical. It could be cylindric or irregularly globular or one of many other shapes. Still, combined with trigonometric evidence of the form used by Eratosthenes 1,700 years prior, the Magellan expedition removed any reasonable doubt in educated circles in Europe. The Transglobe Expedition (1979–1982) was the first expedition to make a circumpolar circumnavigation, traveling the world \"vertically\" traversing both of the poles of rotation using only surface transport.\n\nIn the 17th century, the idea of a spherical Earth, now considerably advanced by Western astronomy, ultimately spread to Ming China, when Jesuit missionaries, who held high positions as astronomers at the imperial court, successfully challenged the Chinese belief that the Earth was flat and square.\n\nThe \"Ge zhi cao\" (格致草) treatise of Xiong Mingyu (熊明遇) published in 1648 showed a printed picture of the Earth as a spherical globe, with the text stating that \"the round Earth certainly has no square corners\". The text also pointed out that sailing ships could return to their port of origin after circumnavigating the waters of the Earth.\n\nThe influence of the map is distinctly Western, as traditional maps of Chinese cartography held the graduation of the sphere at 365.25 degrees, while the Western graduation was of 360 degrees. Also of interest to note is on one side of the world, there is seen towering Chinese pagodas, while on the opposite side (upside-down) there were European cathedrals. The adoption of European astronomy, facilitated by the failure of indigenous astronomy to make progress, was accompanied by a sinocentric reinterpretation that declared the imported ideas Chinese in origin:\n\nEuropean astronomy was so much judged worth consideration that numerous Chinese authors developed the idea that the Chinese of antiquity had anticipated most of the novelties presented by the missionaries as European discoveries, for example, the rotundity of the Earth and the \"heavenly spherical star carrier model.\" Making skillful use of philology, these authors cleverly reinterpreted the greatest technical and literary works of Chinese antiquity. From this sprang a new science wholly dedicated to the demonstration of the Chinese origin of astronomy and more generally of all European science and technology.\n\nAlthough mainstream Chinese science until the 17th century held the view that the earth was flat, square, and enveloped by the celestial sphere, this idea was criticized by the Jin-dynasty scholar Yu Xi (fl. 307–345), who suggested that the Earth could be either square or round, in accordance with the shape of the heavens. The Yuan-dynasty mathematician Li Ye (c. 1192–1279) firmly argued that the Earth was spherical, just like the shape of the heavens only smaller, since a square Earth would hinder the movement of the heavens and celestial bodies in his estimation. The 17th-century \"Ge zhi cao\" treatise also used the same terminology to describe the shape of the Earth that the Eastern-Han scholar Zhang Heng (78–139 AD) had used to describe the shape of the sun and moon (i.e. that the former was as round as a crossbow bullet, and the latter was the shape of a ball).\n\nGeodesy, also called geodetics, is the scientific discipline that deals with the measurement and representation of the Earth, its gravitational field and geodynamic phenomena (polar motion, Earth tides, and crustal motion) in three-dimensional time-varying space.\n\nGeodesy is primarily concerned with positioning and the gravity field and geometrical aspects of their temporal variations, although it can also include the study of Earth's magnetic field. Especially in the German speaking world, geodesy is divided into geomensuration (\"Erdmessung\" or \"höhere Geodäsie\"), which is concerned with measuring the Earth on a global scale, and surveying (\"Ingenieurgeodäsie\"), which is concerned with measuring parts of the surface.\n\nThe Earth's shape can be thought of in at least two ways;\n\nAs the science of geodesy measured Earth more accurately, the shape of the geoid was first found not to be a perfect sphere but to approximate an oblate spheroid, a specific type of ellipsoid. More recent measurements have measured the geoid to unprecedented accuracy, revealing mass concentrations beneath Earth's surface.\n\n\n\n"}
{"id": "28545000", "url": "https://en.wikipedia.org/wiki?curid=28545000", "title": "Talk Reason", "text": "Talk Reason\n\nTalk Reason is a website dedicated to opposing creationism and promoting evolution. Talk Reason collects articles for this purpose and provides a forum to present them.\n\n\n"}
{"id": "1174964", "url": "https://en.wikipedia.org/wiki?curid=1174964", "title": "Technological evolution", "text": "Technological evolution\n\nTechnological evolution is a theory of radical transformation of society through technological development. This theory originated with Czech philosopher Radovan Richta.\n\n\"Mankind In Transition; A View of the Distant Past, the Present and the Far Future\", Masefield Books, 1993. Technology (which Richta defines as \"a material entity created by the application of mental and physical effort to nature in order to achieve some value\") evolves in three stages: tools, machine, automation. This evolution, he says, follows two trends:\n\nThe pretechnological period, in which all other animal species remain today aside from some avian and primate species was a non-rational period of the early prehistoric man.\n\nThe emergence of technology, made possible by the development of the rational faculty, paved the way for the first stage: the tool. A tool provides a mechanical advantage in accomplishing a physical task, such as an arrow, plow, or hammer that augments physical labor to more efficiently achieve his objective. Later animal-powered tools such as the plow and the horse, increased the productivity of food production about tenfold over the technology of the hunter-gatherers. Tools allow one to do things impossible to accomplish with one's body alone, such as seeing minute visual detail with a microscope, manipulating heavy objects with a pulley and cart, or carrying volumes of water in a bucket.\n\nThe second technological stage was the creation of the machine. A machine (a powered machine to be more precise) is a tool that substitutes the element of human physical effort, and requires only to control its function. Machines became widespread with the industrial revolution, though windmills, a type of machine, are much older.\n\nExamples of this include cars, trains, computers, and lights. Machines allow humans to\nTremendously exceed the limitations of their bodies. Putting a machine on the farm, a tractor, increased food productivity at least tenfold over the technology of the plow and the horse.\n\nThe third, and final stage of technological evolution is the automation. The automation is a machine that removes the element of human control with an automatic algorithm. Examples of machines that exhibit this characteristic are digital watches, automatic telephone switches, pacemakers, and computer programs.\n\nIt's crucial to understand that the three stages outline the introduction of the fundamental types of technology, and so all three continue to be widely used today. A spear, a plow, a pen, a knife, a glove, a chicken and an optical microscope are all examples of tools.\n\nThe process of technological evolution culminates with the ability to achieve all the material values technologically possible and desirable by mental effort.\n\nAn economic implication of the above idea is that intellectual labour will become increasingly more important relative to physical labour. Contracts and agreements around information will become increasingly more common at the marketplace. Expansion and creation of new kinds of institutes that works with information such as universities, book stores, patent-trading companies, etc. is considered an indication that a civilization is in technological evolution.\n\nThis highlights the importance underlining the debate over intellectual property in conjunction with decentralized distribution systems such as today's internet. Where the price of information distribution is going towards zero with ever more efficient tools to distribute information is being invented. Growing amounts of information being distributed to an increasingly larger customer base as times goes by. With growing disintermediation in said markets and growing concerns over the protection of intellectual property rights it is not clear what form markets for information will take with the evolution of the information age.\n\n"}
{"id": "31596828", "url": "https://en.wikipedia.org/wiki?curid=31596828", "title": "Unequal crossing over", "text": "Unequal crossing over\n\nUnequal crossing over is a type of gene duplication or deletion event that deletes a sequence in one strand and replaces it with a duplication from its sister chromatid in mitosis or from its homologous chromosome during meiosis. It is a type of chromosomal crossover between homologous sequences that are not paired precisely. Normally genes are responsible for occurrence of crossing over. It exchanges sequences of different links between chromosomes. Along with gene conversion, it is believed to be the main driver for the generation of gene duplications and is a source of mutation in the genome.\n\nDuring meiosis, the duplicated chromosomes (chromatids) in eukaryotic organisms are attached to each other in the centromere region and are thus paired. The maternal and paternal chromosomes then align alongside each other. During this time, recombination can take place via crossing over of sections of the paternal and maternal chromatids and leads to reciprocal recombination or non-reciprocal recombination. Unequal crossing over requires a measure of similarity between the sequences for misalignment to occur. The more similarity within the sequences, the more likely unequal crossing over will occur. One of the sequences is thus lost and replaced with the duplication of another sequence.\n\nWhen two sequences are misaligned, unequal crossing over may create a tandem repeat on one chromosome and a deletion on the other. The rate of unequal crossing over will increase with the number of repeated sequences around the duplication. This is because these repeated sequences will pair together, allowing for the mismatch in the cross over point to occur.\n\nUnequal crossing over is the process most responsible for creating regional gene duplications in the genome. Repeated rounds of unequal crossing over cause the homogenization of the two sequences. With the increase in the duplicates, unequal crossing over can lead to dosage imbalance in the genome and can be highly deleterious.\n\nIn unequal crossing over, there can be large sequence exchanges between the chromosomes. Compared with gene conversion, which can only transfer a maximum of 1,500 base pairs, unequal crossing over in yeast rDNA genes has been found to transfer about 20,000 base pairs in a single crossover event Unequal crossover can be followed by the concerted evolution of duplicated sequences.\n\nIt has been suggested that longer intron found between two beta-globin genes are a response to deleterious selection from unequal crossing over in the beta-globin genes. Comparisons between alpha-globin, which does not have long introns, and beta-globin genes show that alpha-globin have 50 times higher concerted evolution.\n\nWhen unequal crossing over creates a gene duplication, the duplicate has 4 evolutionary fates. This is due to the fact that purifying selection acting on a duplicated copy is not very strong. Now that there is a redundant copy, neutral mutations can act on the duplicate. Most commonly the neutral mutations will continue until the duplicate becomes a pseudogene. If the duplicate copy increases the dosage effect of the gene product, then the duplicate may be retained as a redundant copy. Neofunctionalization is also a possibility: the duplicated copy acquires a mutation that gives it a different function than its ancestor. If both copies acquire mutations, it is possible that a subfunctional event occurs. This happens when both of the duplicated sequences have a more specialized function than the ancestral copy\n\nGene duplications are the main reason for the increase of genome size, and as unequal crossing over is the main mechanism for gene duplication, unequal crossing over contributes to genome size evolution is the most common regional duplication event that increases the size of the genome.\n\nWhen viewing the genome of a eukaryote, a striking observation is the large amount of tandem, repetitive DNA sequences that make up a large portion of the genome. For example, over 50% of the \"Dipodmys ordii\" genome is made up of three specific repeats. \"Drosophila virilis\" has three sequences that make up 40% of the genome, and 35% of the \"Absidia glauca\" is repetitive DNA sequences. These short sequences have no selection pressure acting on them and the frequency of the repeats can be changed by unequal crossing over.\n"}
{"id": "1945275", "url": "https://en.wikipedia.org/wiki?curid=1945275", "title": "Wigner effect", "text": "Wigner effect\n\nThe Wigner effect (named for its discoverer, Eugene Wigner), also known as the discomposition effect or Wigner's Disease, is the dislocation of atoms in a solid caused by neutron radiation. \n\nAny solid can display the Wigner effect. The effect is of most concern in neutron moderators, such as graphite, intended to reduce the speed of fast neutrons, thereby turning them into thermal neutrons capable of sustaining a nuclear chain reaction involving uranium-235.\n\nTo create the Wigner effect, neutrons that collide with the atoms in a crystal structure must have enough energy to displace them from the lattice. This amount (threshold displacement energy) is approximately 25 eV. A neutron's energy can vary widely, but it is not uncommon to have energies up to and exceeding 10 MeV (10,000,000 eV) in the centre of a nuclear reactor. A neutron with a significant amount of energy will create a displacement cascade in a matrix via elastic collisions. For example, a 1 MeV neutron striking graphite will create 900 displacements; not all displacements will create defects, because some of the struck atoms will find and fill the vacancies that were either small pre-existing voids or vacancies newly formed by the other struck atoms.\n\nThe atoms that do not find a vacancy come to rest in non-ideal locations; that is, not along the symmetrical lines of the lattice. These atoms are referred to as interstitial atoms, or simply interstitials. An interstitial atom and its associated vacancy are known as a Frenkel defect. Because these atoms are not in the ideal location, they have an energy associated with them, much as a ball at the top of a hill has gravitational potential energy. This energy is referred to as Wigner energy. When a large number of interstitials have accumulated, they pose a risk of releasing all of their energy suddenly, creating a rapid, very great increase in temperature. Sudden, unplanned increases in temperature can present a large risk for certain types of nuclear reactors with low operating temperatures; one such was the indirect cause of the Windscale fire. Accumulation of energy in irradiated graphite has been recorded as high as 2.7 kJ/g, but is typically much lower than this. Graphite, having a heat capacity of 0.720 J/g°C, could see a sudden increase in temperature of about 3750 °C (6780 °F).\n\nDespite some reports, Wigner energy buildup had nothing to do with the cause of the Chernobyl disaster: this reactor, like all contemporary power reactors, operated at a high enough temperature to allow the displaced graphite structure to realign itself before any potential energy could be stored. Wigner energy may have played some part following the prompt critical neutron spike, when the accident entered the graphite fire phase of events.\n\nA buildup of Wigner energy can be relieved by heating the material. This process is known as annealing. In graphite this occurs at 250 °C.\n\nIn 2003, it was postulated that Wigner energy can be stored by the formation of metastable defect structures in graphite. Notably, the large energy release observed at 200–250 °C has been described in terms of a metastable interstitial-vacancy pair. The interstitial atom becomes trapped on the lip of the vacancy, and there is a barrier for it to recombine to give perfect graphite.\n\n"}
{"id": "49801999", "url": "https://en.wikipedia.org/wiki?curid=49801999", "title": "Wild Seasons (Kay Young)", "text": "Wild Seasons (Kay Young)\n\nWild Seasons: Gathering and Cooking Wild Plants of the Great Plains is a 1993 non-fiction book by author, illustrator, and ethno-botanist Kay Young. It features a variety of wild plants of the great plains area and how to prepare them in appetizing ways. The book includes a number of recipes as well as Young's enthusiasm and advocacy for eating wild crops. It was published by the University of Nebraska Press.\n\n"}
{"id": "33550", "url": "https://en.wikipedia.org/wiki?curid=33550", "title": "Wood", "text": "Wood\n\nWood is a porous and fibrous structural tissue found in the stems and roots of trees and other woody plants. It is an organic material, a natural composite of cellulose fibers that are strong in tension and embedded in a matrix of lignin that resists compression. Wood is sometimes defined as only the secondary xylem in the stems of trees, or it is defined more broadly to include the same type of tissue elsewhere such as in the roots of trees or shrubs. In a living tree it performs a support function, enabling woody plants to grow large or to stand up by themselves. It also conveys water and nutrients between the leaves, other growing tissues, and the roots. Wood may also refer to other plant materials with comparable properties, and to material engineered from wood, or wood chips or fiber.\n\nWood has been used for thousands of years for fuel, as a construction material, for making tools and weapons, furniture and paper. More recently it emerged as a feedstock for the production of purified cellulose and its derivatives, such as cellophane and cellulose acetate.\n\nAs of 2005, the growing stock of forests worldwide was about 434 billion cubic meters, 47% of which was commercial. As an abundant, carbon-neutral renewable resource, woody materials have been of intense interest as a source of renewable energy. In 1991 approximately 3.5 billion cubic meters of wood were harvested. Dominant uses were for furniture and building construction.\n\nA 2011 discovery in the Canadian province of New Brunswick yielded the earliest known plants to have grown wood, approximately 395 to 400 million years ago.\n\nWood can be dated by carbon dating and in some species by dendrochronology to determine when a wooden object was created.\n\nPeople have used wood for thousands of years for many purposes, including as a fuel or as a construction material for making houses, tools, weapons, furniture, packaging, artworks, and paper. Known constructions using wood date back ten thousand years. Buildings like the European Neolithic long house were made primarily of wood.\n\nRecent use of wood has been enhanced by the addition of steel and bronze into construction.\n\nThe year-to-year variation in tree-ring widths and isotopic abundances gives clues to the prevailing climate at the time a tree was cut.\n\nWood, in the strict sense, is yielded by trees, which increase in diameter by the formation, between the existing wood and the inner bark, of new woody layers which envelop the entire stem, living branches, and roots. This process is known as secondary growth; it is the result of cell division in the vascular cambium, a lateral meristem, and subsequent expansion of the new cells. These cells then go on to form thickened secondary cell walls, composed mainly of cellulose, hemicellulose and lignin.\n\nWhere the differences between the four seasons are distinct, e.g. New Zealand, growth can occur in a discrete annual or seasonal pattern, leading to growth rings; these can usually be most clearly seen on the end of a log, but are also visible on the other surfaces. If the distinctiveness between seasons is annual (as is the case in equatorial regions, e.g. Singapore), these growth rings are referred to as annual rings. Where there is little seasonal difference growth rings are likely to be indistinct or absent. If the bark of the tree has been removed in a particular area, the rings will likely be deformed as the plant overgrows the scar.\n\nIf there are differences within a growth ring, then the part of a growth ring nearest the center of the tree, and formed early in the growing season when growth is rapid, is usually composed of wider elements. It is usually lighter in color than that near the outer portion of the ring, and is known as earlywood or springwood. The outer portion formed later in the season is then known as the latewood or summerwood. However, there are major differences, depending on the kind of wood (see below).\n\nAs a tree grows, lower branches often die, and their bases may become overgrown and enclosed by subsequent layers of trunk wood, forming a type of imperfection known as a knot. The dead branch may not be attached to the trunk wood except at its base, and can drop out after the tree has been sawn into boards. Knots affect the technical properties of the wood, usually reducing the local strength and increasing the tendency for splitting along the wood grain, but may be exploited for visual effect. In a longitudinally sawn plank, a knot will appear as a roughly circular \"solid\" (usually darker) piece of wood around which the grain of the rest of the wood \"flows\" (parts and rejoins). Within a knot, the direction of the wood (grain direction) is up to 90 degrees different from the grain direction of the regular wood.\n\nIn the tree a knot is either the base of a side branch or a dormant bud. A knot (when the base of a side branch) is conical in shape (hence the roughly circular cross-section) with the inner tip at the point in stem diameter at which the plant's vascular cambium was located when the branch formed as a bud.\n\nIn grading lumber and structural timber, knots are classified according to their form, size, soundness, and the firmness with which they are held in place. This firmness is affected by, among other factors, the length of time for which the branch was dead while the attaching stem continued to grow.\n\nKnots do not necessarily influence the stiffness of structural timber, this will depend on the size and location. Stiffness and elastic strength are more dependent upon the sound wood than upon localized defects. The breaking strength is very susceptible to defects. Sound knots do not weaken wood when subject to compression parallel to the grain.\n\nIn some decorative applications, wood with knots may be desirable to add visual interest. In applications where wood is painted, such as skirting boards, fascia boards, door frames and furniture, resins present in the timber may continue to 'bleed' through to the surface of a knot for months or even years after manufacture and show as a yellow or brownish stain. A knot primer paint or solution (knotting), correctly applied during preparation, may do much to reduce this problem but it is difficult to control completely, especially when using mass-produced kiln-dried timber stocks.\n\nHeartwood (or duramen) is wood that as a result of a naturally occurring chemical transformation has become more resistant to decay. Heartwood formation is a genetically programmed process that occurs spontaneously. Some uncertainty exists as to whether the wood dies during heartwood formation, as it can still chemically react to decay organisms, but only once.\n\nHeartwood is often visually distinct from the living sapwood, and can be distinguished in a cross-section where the boundary will tend to follow the growth rings. For example, it is sometimes much darker. However, other processes such as decay or insect invasion can also discolor wood, even in woody plants that do not form heartwood, which may lead to confusion.\n\nSapwood (or alburnum) is the younger, outermost wood; in the growing tree it is living wood, and its principal functions are to conduct water from the roots to the leaves and to store up and give back according to the season the reserves prepared in the leaves. However, by the time they become competent to conduct water, all xylem tracheids and vessels have lost their cytoplasm and the cells are therefore functionally dead. All wood in a tree is first formed as sapwood. The more leaves a tree bears and the more vigorous its growth, the larger the volume of sapwood required. Hence trees making rapid growth in the open have thicker sapwood for their size than trees of the same species growing in dense forests. Sometimes trees (of species that do form heartwood) grown in the open may become of considerable size, or more in diameter, before any heartwood begins to form, for example, in second-growth hickory, or open-grown pines.\n\nThe term \"heartwood\" derives solely from its position and not from any vital importance to the tree. This is evidenced by the fact that a tree can thrive with its heart completely decayed. Some species begin to form heartwood very early in life, so having only a thin layer of live sapwood, while in others the change comes slowly. Thin sapwood is characteristic of such species as chestnut, black locust, mulberry, osage-orange, and sassafras, while in maple, ash, hickory, hackberry, beech, and pine, thick sapwood is the rule. Others never form heartwood.\n\nNo definite relation exists between the annual rings of growth and the amount of sapwood. Within the same species the cross-sectional area of the sapwood is very roughly proportional to the size of the crown of the tree. If the rings are narrow, more of them are required than where they are wide. As the tree gets larger, the sapwood must necessarily become thinner or increase materially in volume. Sapwood is relatively thicker in the upper portion of the trunk of a tree than near the base, because the age and the diameter of the upper sections are less.\n\nWhen a tree is very young it is covered with limbs almost, if not entirely, to the ground, but as it grows older some or all of them will eventually die and are either broken off or fall off. Subsequent growth of wood may completely conceal the stubs which will however remain as knots. No matter how smooth and clear a log is on the outside, it is more or less knotty near the middle. Consequently, the sapwood of an old tree, and particularly of a forest-grown tree, will be freer from knots than the inner heartwood. Since in most uses of wood, knots are defects that weaken the timber and interfere with its ease of working and other properties, it follows that a given piece of sapwood, because of its position in the tree, may well be stronger than a piece of heartwood from the same tree.\n\nIt is remarkable that the inner heartwood of old trees remains as sound as it usually does, since in many cases it is hundreds, and in a few instances thousands, of years old. Every broken limb or root, or deep wound from fire, insects, or falling timber, may afford an entrance for decay, which, once started, may penetrate to all parts of the trunk. The larvae of many insects bore into the trees and their tunnels remain indefinitely as sources of weakness. Whatever advantages, however, that sapwood may have in this connection are due solely to its relative age and position.\n\nIf a tree grows all its life in the open and the conditions of soil and site remain unchanged, it will make its most rapid growth in youth, and gradually decline. The annual rings of growth are for many years quite wide, but later they become narrower and narrower. Since each succeeding ring is laid down on the outside of the wood previously formed, it follows that unless a tree materially increases its production of wood from year to year, the rings must necessarily become thinner as the trunk gets wider. As a tree reaches maturity its crown becomes more open and the annual wood production is lessened, thereby reducing still more the width of the growth rings. In the case of forest-grown trees so much depends upon the competition of the trees in their struggle for light and nourishment that periods of rapid and slow growth may alternate. Some trees, such as southern oaks, maintain the same width of ring for hundreds of years. Upon the whole, however, as a tree gets larger in diameter the width of the growth rings decreases.\n\nDifferent pieces of wood cut from a large tree may differ decidedly, particularly if the tree is big and mature. In some trees, the wood laid on late in the life of a tree is softer, lighter, weaker, and more even-textured than that produced earlier, but in other trees, the reverse applies. This may or may not correspond to heartwood and sapwood. In a large log the sapwood, because of the time in the life of the tree when it was grown, may be inferior in hardness, strength, and toughness to equally sound heartwood from the same log. In a smaller tree, the reverse may be true.\n\nIn species which show a distinct difference between heartwood and sapwood the natural color of heartwood is usually darker than that of the sapwood, and very frequently the contrast is conspicuous (see section of yew log above). This is produced by deposits in the heartwood of chemical substances, so that a dramatic color variation does not imply a significant difference in the mechanical properties of heartwood and sapwood, although there may be a marked biochemical difference between the two.\n\nSome experiments on very resinous longleaf pine specimens indicate an increase in strength, due to the resin which increases the strength when dry. Such resin-saturated heartwood is called \"fat lighter\". Structures built of fat lighter are almost impervious to rot and termites; however they are very flammable. Stumps of old longleaf pines are often dug, split into small pieces and sold as kindling for fires. Stumps thus dug may actually remain a century or more since being cut. Spruce impregnated with crude resin and dried is also greatly increased in strength thereby.\n\nSince the latewood of a growth ring is usually darker in color than the earlywood, this fact may be used in visually judging the density, and therefore the hardness and strength of the material. This is particularly the case with coniferous woods. In ring-porous woods the vessels of the early wood often appear on a finished surface as darker than the denser latewood, though on cross sections of heartwood the reverse is commonly true. Otherwise the color of wood is no indication of strength.\n\nAbnormal discoloration of wood often denotes a diseased condition, indicating unsoundness. The black check in western hemlock is the result of insect attacks. The reddish-brown streaks so common in hickory and certain other woods are mostly the result of injury by birds. The discoloration is merely an indication of an injury, and in all probability does not of itself affect the properties of the wood. Certain rot-producing fungi impart to wood characteristic colors which thus become symptomatic of weakness; however an attractive effect known as spalting produced by this process is often considered a desirable characteristic. Ordinary sap-staining is due to fungal growth, but does not necessarily produce a weakening effect.\n\nWater occurs in living wood in three locations, namely:\n\nIn heartwood it occurs only in the first and last forms. Wood that is thoroughly air-dried retains 8–16% of the water in the cell walls, and none, or practically none, in the other forms. Even oven-dried wood retains a small percentage of moisture, but for all except chemical purposes, may be considered absolutely dry.\n\nThe general effect of the water content upon the wood substance is to render it softer and more pliable. A similar effect occurs in the softening action of water on rawhide, paper, or cloth. Within certain limits, the greater the water content, the greater its softening effect.\n\nDrying produces a decided increase in the strength of wood, particularly in small specimens. An extreme example is the case of a completely dry spruce block 5 cm in section, which will sustain a permanent load four times as great as a green (undried) block of the same size will.\n\nThe greatest strength increase due to drying is in the ultimate crushing strength, and strength at elastic limit in endwise compression; these are followed by the modulus of rupture, and stress at elastic limit in cross-bending, while the modulus of elasticity is least affected.\n\nWood is a heterogeneous, hygroscopic, cellular and anisotropic material. It consists of cells, and the cell walls are composed of micro-fibrils of cellulose (40–50%) and hemicellulose (15–25%) impregnated with lignin (15–30%).\n\nIn coniferous or softwood species the wood cells are mostly of one kind, tracheids, and as a result the material is much more uniform in structure than that of most hardwoods. There are no vessels (\"pores\") in coniferous wood such as one sees so prominently in oak and ash, for example.\n\nThe structure of hardwoods is more complex. The water conducting capability is mostly taken care of by vessels: in some cases (oak, chestnut, ash) these are quite large and distinct, in others (buckeye, poplar, willow) too small to be seen without a hand lens. In discussing such woods it is customary to divide them into two large classes, \"ring-porous\" and \"diffuse-porous\".\n\nIn ring-porous species, such as ash, black locust, catalpa, chestnut, elm, hickory, mulberry, and oak, the larger vessels or pores (as cross sections of vessels are called) are localized in the part of the growth ring formed in spring, thus forming a region of more or less open and porous tissue. The rest of the ring, produced in summer, is made up of smaller vessels and a much greater proportion of wood fibers. These fibers are the elements which give strength and toughness to wood, while the vessels are a source of weakness.\n\nIn diffuse-porous woods the pores are evenly sized so that the water conducting capability is scattered throughout the growth ring instead of being collected in a band or row. Examples of this kind of wood are alder, basswood, birch, buckeye, maple, willow, and the \"Populus\" species such as aspen, cottonwood and poplar. Some species, such as walnut and cherry, are on the border between the two classes, forming an intermediate group.\n\nIn temperate softwoods, there often is a marked difference between latewood and earlywood. The latewood will be denser than that formed early in the season. When examined under a microscope, the cells of dense latewood are seen to be very thick-walled and with very small cell cavities, while those formed first in the season have thin walls and large cell cavities. The strength is in the walls, not the cavities. Hence the greater the proportion of latewood, the greater the density and strength. In choosing a piece of pine where strength or stiffness is the important consideration, the principal thing to observe is the comparative amounts of earlywood and latewood. The width of ring is not nearly so important as the proportion and nature of the latewood in the ring.\n\nIf a heavy piece of pine is compared with a lightweight piece it will be seen at once that the heavier one contains a larger proportion of latewood than the other, and is therefore showing more clearly demarcated growth rings. In white pines there is not much contrast between the different parts of the ring, and as a result the wood is very uniform in texture and is easy to work. In hard pines, on the other hand, the latewood is very dense and is deep-colored, presenting a very decided contrast to the soft, straw-colored earlywood.\n\nIt is not only the proportion of latewood, but also its quality, that counts. In specimens that show a very large proportion of latewood it may be noticeably more porous and weigh considerably less than the latewood in pieces that contain less latewood. One can judge comparative density, and therefore to some extent strength, by visual inspection.\n\nNo satisfactory explanation can as yet be given for the exact mechanisms determining the formation of earlywood and latewood. Several factors may be involved. In conifers, at least, rate of growth alone does not determine the proportion of the two portions of the ring, for in some cases the wood of slow growth is very hard and heavy, while in others the opposite is true. The quality of the site where the tree grows undoubtedly affects the character of the wood formed, though it is not possible to formulate a rule governing it. In general, however, it may be said that where strength or ease of working is essential, woods of moderate to slow growth should be chosen.\n\nIn ring-porous woods, each season's growth is always well defined, because the large pores formed early in the season abut on the denser tissue of the year before.\n\nIn the case of the ring-porous hardwoods, there seems to exist a pretty definite relation between the rate of growth of timber and its properties. This may be briefly summed up in the general statement that the more rapid the growth or the wider the rings of growth, the heavier, harder, stronger, and stiffer the wood. This, it must be remembered, applies only to ring-porous woods such as oak, ash, hickory, and others of the same group, and is, of course, subject to some exceptions and limitations.\n\nIn ring-porous woods of good growth, it is usually the latewood in which the thick-walled, strength-giving fibers are most abundant. As the breadth of ring diminishes, this latewood is reduced so that very slow growth produces comparatively light, porous wood composed of thin-walled vessels and wood parenchyma. In good oak, these large vessels of the earlywood occupy from 6 to 10 percent of the volume of the log, while in inferior material they may make up 25% or more. The latewood of good oak is dark colored and firm, and consists mostly of thick-walled fibers which form one-half or more of the wood. In inferior oak, this latewood is much reduced both in quantity and quality. Such variation is very largely the result of rate of growth.\n\nWide-ringed wood is often called \"second-growth\", because the growth of the young timber in open stands after the old trees have been removed is more rapid than in trees in a closed forest, and in the manufacture of articles where strength is an important consideration such \"second-growth\" hardwood material is preferred. This is particularly the case in the choice of hickory for handles and spokes. Here not only strength, but toughness and resilience are important.\n\nThe results of a series of tests on hickory by the U.S. Forest Service show that:\n\nThe effect of rate of growth on the qualities of chestnut wood is summarized by the same authority as follows:\n\nIn the diffuse-porous woods, the demarcation between rings is not always so clear and in some cases is almost (if not entirely) invisible to the unaided eye. Conversely, when there is a clear demarcation there may not be a noticeable difference in structure within the growth ring.\n\nIn diffuse-porous woods, as has been stated, the vessels or pores are even-sized, so that the water conducting capability is scattered throughout the ring instead of collected in the earlywood. The effect of rate of growth is, therefore, not the same as in the ring-porous woods, approaching more nearly the conditions in the conifers. In general it may be stated that such woods of medium growth afford stronger material than when very rapidly or very slowly grown. In many uses of wood, total strength is not the main consideration. If ease of working is prized, wood should be chosen with regard to its uniformity of texture and straightness of grain, which will in most cases occur when there is little contrast between the latewood of one season's growth and the earlywood of the next.\n\nStructural material that resembles ordinary, \"dicot\" or conifer timber in its gross handling characteristics is produced by a number of monocot plants, and these also are colloquially called wood. Of these, bamboo, botanically a member of the grass family, has considerable economic importance, larger culms being widely used as a building and construction material and in the manufacture of engineered flooring, panels and veneer. Another major plant group that produces material that often is called wood are the palms. Of much less importance are plants such as \"Pandanus,\" \"Dracaena\" and \"Cordyline.\" With all this material, the structure and composition of the processed raw material is quite different from ordinary wood.\n\nThe single most revealing property of wood as an indicator of wood quality is specific gravity (Timell 1986), as both pulp yield and lumber strength are determined by it. Specific gravity is the ratio of the mass of a substance to the mass of an equal volume of water; density is the ratio of a mass of a quantity of a substance to the volume of that quantity and is expressed in mass per unit substance, e.g., grams per milliliter (g/cm or g/ml). The terms are essentially equivalent as long as the metric system is used. Upon drying, wood shrinks and its density increases. Minimum values are associated with green (water-saturated) wood and are referred to as \"basic specific gravity\" (Timell 1986).\n\nWood density is determined by multiple growth and physiological factors compounded into “one fairly easily measured wood characteristic” (Elliott 1970).\n\nAge, diameter, height, radial (trunk) growth, geographical location, site and growing conditions, silvicultural treatment, and seed source all to some degree influence wood density. Variation is to be expected. Within an individual tree, the variation in wood density is often as great as or even greater than that between different trees (Timell 1986). Variation of specific gravity within the bole of a tree can occur in either the horizontal or vertical direction.\n\nIt is common to classify wood as either softwood or hardwood. The wood from conifers (e.g. pine) is called softwood, and the wood from dicotyledons (usually broad-leaved trees, (e.g. oak) is called hardwood. These names are a bit misleading, as hardwoods are not necessarily hard, and softwoods are not necessarily soft. The well-known balsa (a hardwood) is actually softer than any commercial softwood. Conversely, some softwoods (e.g. yew) are harder than many hardwoods.\n\nThere is a strong relationship between the properties of wood and the properties of the particular tree that yielded it. The density of wood varies with species. The density of a wood correlates with its strength (mechanical properties). For example, mahogany is a medium-dense hardwood that is excellent for fine furniture crafting, whereas balsa is light, making it useful for model building. One of the densest woods is black ironwood.\n\nThe chemical composition of wood varies from species to species, but is approximately 50% carbon, 42% oxygen, 6% hydrogen, 1% nitrogen, and 1% other elements (mainly calcium, potassium, sodium, magnesium, iron, and manganese) by weight. Wood also contains sulfur, chlorine, silicon, phosphorus, and other elements in small quantity.\n\nAside from water, wood has three main components. Cellulose, a crystalline polymer derived from glucose, constitutes about 41–43%. Next in abundance is hemicellulose, which is around 20% in deciduous trees but near 30% in conifers. It is mainly five-carbon sugars that are linked in an irregular manner, in contrast to the cellulose. Lignin is the third component at around 27% in coniferous wood vs. 23% in deciduous trees. Lignin confers the hydrophobic properties reflecting the fact that it is based on aromatic rings. These three components are interwoven, and direct covalent linkages exist between the lignin and the hemicellulose. A major focus of the paper industry is the separation of the lignin from the cellulose, from which paper is made.\n\nIn chemical terms, the difference between hardwood and softwood is reflected in the composition of the constituent lignin. Hardwood lignin is primarily derived from sinapyl alcohol and coniferyl alcohol. Softwood lignin is mainly derived from coniferyl alcohol.\n\nAside from the lignocellulose, wood consists of a variety of low molecular weight organic compounds, called \"extractives\". The wood extractives are fatty acids, resin acids, waxes and terpenes. For example, rosin is exuded by conifers as protection from insects. The extraction of these organic materials from wood provides tall oil, turpentine, and rosin.\n\nWood has a long history of being used as fuel, which continues to this day, mostly in rural areas of the world. Hardwood is preferred over softwood because it creates less smoke and burns longer. Adding a woodstove or fireplace to a home is often felt to add ambiance and warmth.\n\nWood has been an important construction material since humans began building shelters, houses and boats. Nearly all boats were made out of wood until the late 19th century, and wood remains in common use today in boat construction. Elm in particular was used for this purpose as it resisted decay as long as it was kept wet (it also served for water pipe before the advent of more modern plumbing).\n\nWood to be used for construction work is commonly known as \"lumber\" in North America. Elsewhere, \"lumber\" usually refers to felled trees, and the word for sawn planks ready for use is \"timber\". In Medieval Europe oak was the wood of choice for all wood construction, including beams, walls, doors, and floors. Today a wider variety of woods is used: solid wood doors are often made from poplar, small-knotted pine, and Douglas fir.\nNew domestic housing in many parts of the world today is commonly made from timber-framed construction. Engineered wood products are becoming a bigger part of the construction industry. They may be used in both residential and commercial buildings as structural and aesthetic materials.\n\nIn buildings made of other materials, wood will still be found as a supporting material, especially in roof construction, in interior doors and their frames, and as exterior cladding.\n\nWood is also commonly used as shuttering material to form the mold into which concrete is poured during reinforced concrete construction.\n\nA solid wood floor is a floor laid with planks or battens created from a single piece of timber, usually a hardwood. Since wood is hydroscopic (it acquires and loses moisture from the ambient conditions around it) this potential instability effectively limits the length and width of the boards.\n\nSolid hardwood flooring is usually cheaper than engineered timbers and damaged areas can be sanded down and refinished repeatedly, the number of times being limited only by the thickness of wood above the tongue.\n\nSolid hardwood floors were originally used for structural purposes, being installed perpendicular to the wooden support beams of a building (the joists or bearers) and solid construction timber is still often used for sports floors as well as most traditional wood blocks, mosaics and parquetry.\n\nEngineered wood products, glued building products \"engineered\" for application-specific performance requirements, are often used in construction and industrial applications. Glued engineered wood products are manufactured by bonding together wood strands, veneers, lumber or other forms of wood fiber with glue to form a larger, more efficient composite structural unit.\n\nThese products include glued laminated timber (glulam), wood structural panels (including plywood, oriented strand board and composite panels), laminated veneer lumber (LVL) and other structural composite lumber (SCL) products, parallel strand lumber, and I-joists. Approximately 100 million cubic meters of wood was consumed for this purpose in 1991. The trends suggest that particle board and fiber board will overtake plywood.\n\nWood unsuitable for construction in its native form may be broken down mechanically (into fibers or chips) or chemically (into cellulose) and used as a raw material for other building materials, such as engineered wood, as well as chipboard, hardboard, and medium-density fiberboard (MDF). Such wood derivatives are widely used: wood fibers are an important component of most paper, and cellulose is used as a component of some synthetic materials. Wood derivatives can be used for kinds of flooring, for example laminate flooring.\n\nWood has always been used extensively for furniture, such as chairs and beds. It is also used for tool handles and cutlery, such as chopsticks, toothpicks, and other utensils, like the wooden spoon and pencil.\n\nFurther developments include new lignin glue applications, recyclable food packaging, rubber tire replacement applications, anti-bacterial medical agents, and high strength fabrics or composites.\nAs scientists and engineers further learn and develop new techniques to extract various components from wood, or alternatively to modify wood, for example by adding components to wood, new more advanced products will appear on the marketplace. Moisture content electronic monitoring can also enhance next generation wood protection.\n\nWood has long been used as an artistic medium. It has been used to make sculptures and carvings for millennia. Examples include the totem poles carved by North American indigenous people from conifer trunks, often Western Red Cedar (\"Thuja plicata\").\n\nOther uses of wood in the arts include:\n\nMany types of sports equipment are made of wood, or were constructed of wood in the past. For example, cricket bats are typically made of white willow. The baseball bats which are legal for use in Major League Baseball are frequently made of ash wood or hickory, and in recent years have been constructed from maple even though that wood is somewhat more fragile. NBA courts have been traditionally made out of parquetry.\n\nMany other types of sports and recreation equipment, such as skis, ice hockey sticks, lacrosse sticks and archery bows, were commonly made of wood in the past, but have since been replaced with more modern materials such as aluminium, titanium or composite materials such as fiberglass and carbon fiber. One noteworthy example of this trend is the family of golf clubs commonly known as the \"woods\", the heads of which were traditionally made of persimmon wood in the early days of the game of golf, but are now generally made of metal or (especially in the case of drivers) carbon-fiber composites.\n\nLittle is known about the bacteria that degrade cellulose. Symbiotic bacteria in \"Xylophaga\" may play a role in the degradation of sunken wood; while bacteria such as \"Alphaproteobacteria\", \"Flavobacteria\", \"Actinobacteria\", \"Clostridia\", and \"Bacteroidetes\" have been detected in wood submerged over a year.\n\n"}
{"id": "1460862", "url": "https://en.wikipedia.org/wiki?curid=1460862", "title": "Yakov Perelman", "text": "Yakov Perelman\n\nYakov Isidorovich Perelman (; December 4, 1882 – March 16, 1942) was a Russian and Soviet science writer and author of many popular science books, including \"Physics Can Be Fun\" and \"Mathematics Can Be Fun\" (both translated from Russian into English).\n\nPerelman was born in 1882 in the town of Białystok, Congress Poland. He obtained the Diploma in Forestry from the Imperial Forestry Institute (Now Saint Petersburg State Forest Technical University) in Saint Petersburg, in 1909. He was influenced by Ernst Mach and probably the Russian Machist Alexander Bogdanov in his pedagogical approach to popularising science. After the success of \"Physics for Entertainment\", Perelman set out to produce other books, in which he showed himself to be an imaginative populariser of science. Especially popular were \"\"Arithmetic for entertainment\", \"Mechanics for entertainment\", \"Geometry for Entertainment\", \"Astronomy for entertainment\", \"Lively Mathematics\", \" Physics Everywhere\", and \"Tricks and Amusements\".\n\nHis famous books on physics and astronomy were translated into various languages by the erstwhile Soviet Union.\n\nThe scientist Konstantin Tsiolkovsky thought highly of Perelman's talents and creative genius, writing of him in the preface of \"Interplanetary Journeys\": \"The author has long been known by his popular, witty and quite scientific works on physics, astronomy and mathematics, which are, moreover written in a marvelous language and are very readable.\"\n\nPerelman has also authored a number of textbooks and articles in Soviet popular science magazines.\n\nIn addition to his educational and scientific writings, he also worked as an editor of science magazines, including \"Nature and People\" and \"In the Workshop of Nature\".\n\nPerelman died from starvation in 1942, during the German Siege of Leningrad. The siege started at 9 September 1941 and lasted 872 days, until \n27 January 1944. The Siege of Leningrad was one of the longest, most destructive sieges of a major city in modern history and one of the costliest in terms of casualties (1,117,000).\n\nHis older brother Yosif was a writer who published under the pseudonym Osip Dymov. He is not related to the Russian mathematician Grigori Perelman, who was born in 1966 to a different Yakov Perelman. However, Grigori Perelman told The New Yorker that his father gave him \"Physics for Entertainment\", and it inspired his interest in mathematics.\n\n\nHe has also written several books on interplanetary travel (\"Interplanetary Journeys, On a Rocket to Stars, and World Expanses\")\n\nIn 1913, Russian bookshops began carrying \"Physics for Entertainment\". The educationalist's new book attracted young readers seeking answers to scientific questions.\n\n\"Physics for Entertainment\" had a unique layout as well as an instructive style. In the preface (11th ed.) Perelman wrote: \"The main objective of \"Physics for entertainment\" is to arouse the activity of scientific imagination, to teach the reader to think in the spirit of the science of physics and to create in his mind a wide variety of associations of physical knowledge with the widely differing facts of life, with all that he normally comes into contact with.\"\n\nIn the foreword, Perelman describes the contents as “conundrums, brain-teasers, entertaining anecdotes, and unexpected comparisons,” adding, “I have quoted extensively from Jules Verne, H. G. Wells, Mark Twain and other writers, because, besides providing entertainment, the fantastic experiments these writers describe may well serve as instructive illustrations at physics classes.” The 13th edition (1936) would be the last published during the author's lifetime. Among the book's notable topics was the idea of a perpetual machine: a hypothetical machine which could run incessantly performing useful work. The author discusses perpetual motion, highlighting many attempts to build such a machine, and explains why they failed. Other topics included how to jump from a moving car, and why, “according to the law of buoyancy, we would never drown in the Dead Sea.”\n\nRandall Munroe, the creator of the web comic xkcd and author of his own popular science books, wrote: \nThe book is a series of a few hundred examples, no more than one or two pages each, asking a question that illustrates some idea in basic physics.\n\nIt’s neat to see what has and hasn’t changed in the last century or so. Many of the examples he uses seem to be straight out of a modern high school physics textbook, while others were totally new to me. And some of the answers to the questions he poses seem obvious, but others made me stop and think. [This] diagram ... shows a design for a fountain with no pump — it took me a while to get why it works... Later in the book, he explains the physics of that drinking bird toy.\nIt’s written in a fun, engaging, conversational style, as if he’s in the room chatting with you about these neat ideas.\n\n\n"}
