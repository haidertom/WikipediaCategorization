{"id": "37205291", "url": "https://en.wikipedia.org/wiki?curid=37205291", "title": "Aesthetics of nature", "text": "Aesthetics of nature\n\nAesthetics of nature is a sub-field of philosophical ethics, and refers to the study of natural objects from their aesthetical perspective.\n\nAesthetics of nature developed as a sub-field of philosophical ethics. In the 18th and 19th century, the aesthetics of nature advanced the concepts of disinterestedness, the pictures, and the introduction of the idea of positive aesthetics. The first major developments of nature occurred in the 18th century. The concept of disinterestedness had been explained by many thinkers. Anthony Ashley-Cooper introduced the concept as a way of characterizing the notion of the aesthetic, later magnified by Francis Hutcheson, who expanded it to exclude personal and utilitarianism interests and associations of a more general nature from aesthetic experience. This concept was further developed by Archibald Alison who referred it to a particular state of mind.\n\nThe theory of disinterestedness opened doors for a better understanding of the aesthetics dimensions of nature in terms of three conceptualizations: \n\nObjects experienced as beautiful tend to be small, smooth, and fair in color. In contrast, objects viewed as sublime tend to be powerful, intense and terrifying. Picturesque items are a mixture of both, which can be seen as varied and irregular, rich and forceful, and even vibrant.\n\nCognitive and non-cognitive approaches of nature have directed their focus from natural environments to the consideration of human and human influenced environments and developed aesthetic investigations of everyday life.(Carlson and Lintott, 2007; Parsons 2008a; Carlson 2010)\n\nPeople may be mistaken by the art object analogy. For instance, a sandhill crane is not an art object; an art object is not a sandhill crane. In fact, an art object should be called an \"artifact\". The crane is wildlife on its own and is not an art object. This can be related to Satio's definition of the cognitive view. In elaboration, the crane lives through various ecosystems such as Yellowstone. Nature is a living system which includes animals, plants, and Eco-systems. In contrast, an art object has no regeneration, evolutionary history, or metabolism. An individual may be in the forest and perceive it as beautiful because of the plethora of colors such as red, green, and yellow. This is a result of the chemicals interacting with chlorophyll. An individual's aesthetic experience may increase; however, none of the things mentioned have anything to do with what is really going on in the forest. The chlorophyll is capturing solar energy and the residual chemicals protect the trees from insect grazing.\n\nAny color perceived by human visitors for a few hours is entirely different from what is really happening. According to Leopold, the three features of ecosystems that generate land ethic are integrity, stability and beauty. None of the mentioned features are real in nature. Ecosystems are not stable: they are dramatically changing and they have little integration; ergo, beauty is in the eye of the beholder.\n\nIn a Post-Modern approach, when an individual engages in aesthetically appreciating a natural thing, we give meaning to the thing we appreciate and in that meaning, we express and develop our own attitudes, values and beliefs. Our interest in natural things are not only a passive reflection of our inclinations, as Croce describes as the appreciation of nature as looking in a mirror, or what we might call our inward life; but may instead be the things we come across in nature that engage and stimulate our imagination. As a result, we are challenged to think differently and apply thoughts and associations to in new situations and ways.\nAs a characterization of the appreciation of art, nature aestheticists argue that post modernism is a mistaken view because we do not have a case of anything goes.The aesthetics appreciation of art is governed by some normative standards. In the world of art, criticism may take place when people come together and discuss books and films or critics write appraisals for publications. On the contrary, there are not obvious instances of debate and appraisals where different judgments about the aesthetics of character of nature are evaluated.\n"}
{"id": "4116", "url": "https://en.wikipedia.org/wiki?curid=4116", "title": "Big Bang", "text": "Big Bang\n\nThe Big Bang theory is the prevailing cosmological model for the observable universe from the earliest known periods through its subsequent large-scale evolution. The model describes how the universe expanded from a very high-density and high-temperature state, and offers a comprehensive explanation for a broad range of phenomena, including the abundance of light elements, the cosmic microwave background (CMB), large scale structure and Hubble's law (the farther away galaxies are, the faster they are moving away from Earth). If the observed conditions are extrapolated backwards in time using the known laws of physics, the prediction is that just before a period of very high density there was a singularity which is typically associated with the Big Bang. Physicists are undecided whether this means the universe began from a singularity, or that current knowledge is insufficient to describe the universe at that time. Detailed measurements of the expansion rate of the universe place the Big Bang at around 13.8 billion years ago, which is thus considered the age of the universe. After its initial expansion, the universe cooled sufficiently to allow the formation of subatomic particles, and later simple atoms. Giant clouds of these primordial elements (mostly hydrogen, with some helium and lithium) later coalesced through gravity, eventually forming early stars and galaxies, the descendants of which are visible today. Astronomers also observe the gravitational effects of dark matter surrounding galaxies. Though most of the mass in the universe seems to be in the form of dark matter, Big Bang theory and various observations seem to indicate that it is not made out of conventional baryonic matter (protons, neutrons, and electrons) but it is unclear exactly what it \"is\" made out of.\n\nSince Georges Lemaître first noted in 1927 that an expanding universe could be traced back in time to an originating single point, scientists have built on his idea of cosmic expansion. The scientific community was once divided between supporters of two different theories, the Big Bang and the Steady State theory, but a wide range of empirical evidence has strongly favored the Big Bang which is now universally accepted. In 1929, from analysis of galactic redshifts, Edwin Hubble concluded that galaxies are drifting apart; this is important observational evidence consistent with the hypothesis of an expanding universe. In 1964, the cosmic microwave background radiation was discovered, which was crucial evidence in favor of the Big Bang model, since that theory predicted the existence of background radiation throughout the universe before it was discovered. More recently, measurements of the redshifts of supernovae indicate that the expansion of the universe is accelerating, an observation attributed to dark energy's existence. The known physical laws of nature can be used to calculate the characteristics of the universe in detail back in time to an initial state of extreme density and temperature.\n\nThe Belgian astronomer and Catholic priest Georges Lemaître proposed on theoretical grounds that the universe is expanding, which was observationally confirmed soon afterwards by Edwin Hubble. In 1927 in the \"Annales de la Société Scientifique de Bruxelles\" (\"Annals of the Scientific Society of Brussels\") under the title \"Un Univers homogène de masse constante et de rayon croissant rendant compte de la vitesse radiale des nébuleuses extragalactiques\" (\"A homogeneous Universe of constant mass and growing radius accounting for the radial velocity of extragalactic nebulae\"), he presented his new idea that the universe is expanding and provided the first observational estimation of what is known as the Hubble constant. What later will be known as the \"Big Bang theory\" of the origin of the universe, he called his \"hypothesis of the primeval atom\" or the \"Cosmic Egg\".\n\nAmerican astronomer Edwin Hubble observed that the distances to faraway galaxies were strongly correlated with their redshifts. This was interpreted to mean that all distant galaxies and clusters are receding away from our vantage point with an apparent velocity proportional to their distance: that is, the farther they are, the faster they move away from us, regardless of direction. Assuming the Copernican principle (that the Earth is not the center of the universe), the only remaining interpretation is that all observable regions of the universe are receding from all others. Since we know that the distance between galaxies increases today, it must mean that in the past galaxies were closer together. The continuous expansion of the universe implies that the universe was denser and hotter in the past.\n\nLarge particle accelerators can replicate the conditions that prevailed after the early moments of the universe, resulting in confirmation and refinement of the details of the Big Bang model. However, these accelerators can only probe so far into high energy regimes. Consequently, the state of the universe in the earliest instants of the Big Bang expansion is still poorly understood and an area of open investigation and speculation.\n\nThe first subatomic particles to be formed included protons, neutrons, and electrons. Though simple atomic nuclei formed within the first three minutes after the Big Bang, thousands of years passed before the first electrically neutral atoms formed. The majority of atoms produced by the Big Bang were hydrogen, along with helium and traces of lithium. Giant clouds of these primordial elements later coalesced through gravity to form stars and galaxies, and the heavier elements were synthesized either within stars or during supernovae.\n\nThe Big Bang theory offers a comprehensive explanation for a broad range of observed phenomena, including the abundance of light elements, the CMB, large scale structure, and Hubble's Law. The framework for the Big Bang model relies on Albert Einstein's theory of general relativity and on simplifying assumptions such as homogeneity and isotropy of space. The governing equations were formulated by Alexander Friedmann, and similar solutions were worked on by Willem de Sitter. Since then, astrophysicists have incorporated observational and theoretical additions into the Big Bang model, and its parametrization as the Lambda-CDM model serves as the framework for current investigations of theoretical cosmology. The Lambda-CDM model is the current \"standard model\" of Big Bang cosmology, consensus is that it is the simplest model that can account for the various measurements and observations relevant to cosmology.\n\nExtrapolation of the expansion of the universe backwards in time using general relativity yields an infinite density and temperature at a finite time in the past. This singularity indicates that general relativity is not an adequate description of the laws of physics in this regime. Models based on general relativity alone can not extrapolate toward the singularity beyond the end of the Planck epoch.\n\nThis primordial singularity is itself sometimes called \"the Big Bang\", but the term can also refer to a more generic early hot, dense phase of the universe. In either case, \"the Big Bang\" as an event is also colloquially referred to as the \"birth\" of our universe since it represents the point in history where the universe can be verified to have entered into a regime where the laws of physics as we understand them (specifically general relativity and the standard model of particle physics) work. Based on measurements of the expansion using Type Ia supernovae and measurements of temperature fluctuations in the cosmic microwave background, the time that has passed since that event — otherwise known as the \"age of the universe\" — is 13.799 ± 0.021 billion years. The agreement of independent measurements of this age supports the ΛCDM model that describes in detail the characteristics of the universe.\n\nDespite being extremely dense at this time—far denser than is usually required to form a black hole—the universe did not re-collapse into a black hole. This may be explained by considering that commonly-used calculations and limits for gravitational collapse are usually based upon objects of relatively constant size, such as stars, and do not apply to rapidly expanding space such as the Big Bang.\n\nThe earliest phases of the Big Bang are subject to much speculation. In the most common models the universe was filled homogeneously and isotropically with a very high energy density and huge temperatures and pressures and was very rapidly expanding and cooling. Approximately 10 seconds into the expansion, a phase transition caused a cosmic inflation, during which the universe grew exponentially during which time density fluctuations that occurred because of the uncertainty principle were amplified into the seeds that would later form the large-scale structure of the universe. After inflation stopped, reheating occurred until the universe obtained the temperatures required for the production of a quark–gluon plasma as well as all other elementary particles. Temperatures were so high that the random motions of particles were at relativistic speeds, and particle–antiparticle pairs of all kinds were being continuously created and destroyed in collisions. At some point, an unknown reaction called baryogenesis violated the conservation of baryon number, leading to a very small excess of quarks and leptons over antiquarks and antileptons—of the order of one part in 30 million. This resulted in the predominance of matter over antimatter in the present universe.\n\nThe universe continued to decrease in density and fall in temperature, hence the typical energy of each particle was decreasing. Symmetry breaking phase transitions put the fundamental forces of physics and the parameters of elementary particles into their present form. After about 10 seconds, the picture becomes less speculative, since particle energies drop to values that can be attained in particle accelerators. At about 10 seconds, quarks and gluons combined to form baryons such as protons and neutrons. The small excess of quarks over antiquarks led to a small excess of baryons over antibaryons. The temperature was now no longer high enough to create new proton–antiproton pairs (similarly for neutrons–antineutrons), so a mass annihilation immediately followed, leaving just one in 10 of the original protons and neutrons, and none of their antiparticles. A similar process happened at about 1 second for electrons and positrons. After these annihilations, the remaining protons, neutrons and electrons were no longer moving relativistically and the energy density of the universe was dominated by photons (with a minor contribution from neutrinos).\n\nA few minutes into the expansion, when the temperature was about a billion (one thousand million) kelvin and the density was about that of air, neutrons combined with protons to form the universe's deuterium and helium nuclei in a process called Big Bang nucleosynthesis. Most protons remained uncombined as hydrogen nuclei.\n\nAs the universe cooled, the rest mass energy density of matter came to gravitationally dominate that of the photon radiation. After about 379,000 years, the electrons and nuclei combined into atoms (mostly hydrogen); hence the radiation decoupled from matter and continued through space largely unimpeded. This relic radiation is known as the cosmic microwave background radiation. The chemistry of life may have begun shortly after the Big Bang, 13.8 billion years ago, during a habitable epoch when the universe was only 10–17 million years old.\n\nOver a long period of time, the slightly denser regions of the nearly uniformly distributed matter gravitationally attracted nearby matter and thus grew even denser, forming gas clouds, stars, galaxies, and the other astronomical structures observable today. The details of this process depend on the amount and type of matter in the universe. The four possible types of matter are known as cold dark matter, warm dark matter, hot dark matter, and baryonic matter. The best measurements available, from Wilkinson Microwave Anisotropy Probe (WMAP), show that the data is well-fit by a Lambda-CDM model in which dark matter is assumed to be cold (warm dark matter is ruled out by early reionization), and is estimated to make up about 23% of the matter/energy of the universe, while baryonic matter makes up about 4.6%. In an \"extended model\" which includes hot dark matter in the form of neutrinos, then if the \"physical baryon density\" formula_1 is estimated at about 0.023 (this is different from the 'baryon density' formula_2 expressed as a fraction of the total matter/energy density, which as noted above is about 0.046), and the corresponding cold dark matter density formula_3 is about 0.11, the corresponding neutrino density formula_4 is estimated to be less than 0.0062.\n\nIndependent lines of evidence from Type Ia supernovae and the CMB imply that the universe today is dominated by a mysterious form of energy known as dark energy, which apparently permeates all of space. The observations suggest 73% of the total energy density of today's universe is in this form. When the universe was very young, it was likely infused with dark energy, but with less space and everything closer together, gravity predominated, and it was slowly braking the expansion. But eventually, after numerous billion years of expansion, the growing abundance of dark energy caused the expansion of the universe to slowly begin to accelerate.\n\nDark energy in its simplest formulation takes the form of the cosmological constant term in Einstein's field equations of general relativity, but its composition and mechanism are unknown and, more generally, the details of its equation of state and relationship with the Standard Model of particle physics continue to be investigated both through observation and theoretically.\n\nAll of this cosmic evolution after the inflationary epoch can be rigorously described and modeled by the ΛCDM model of cosmology, which uses the independent frameworks of quantum mechanics and Einstein's General Relativity. There is no well-supported model describing the action prior to 10 seconds or so. Apparently a new unified theory of quantum gravitation is needed to break this barrier. Understanding this earliest of eras in the history of the universe is currently one of the greatest unsolved problems in physics.\n\nThe Big Bang theory depends on two major assumptions: the universality of physical laws and the cosmological principle. The cosmological principle states that on large scales the universe is homogeneous and isotropic.\n\nThese ideas were initially taken as postulates, but today there are efforts to test each of them. For example, the first assumption has been tested by observations showing that largest possible deviation of the fine structure constant over much of the age of the universe is of order 10. Also, general relativity has passed stringent tests on the scale of the Solar System and binary stars.\n\nIf the large-scale universe appears isotropic as viewed from Earth, the cosmological principle can be derived from the simpler Copernican principle, which states that there is no preferred (or special) observer or vantage point. To this end, the cosmological principle has been confirmed to a level of 10 via observations of the CMB. The universe has been measured to be homogeneous on the largest scales at the 10% level.\n\nGeneral relativity describes spacetime by a metric, which determines the distances that separate nearby points. The points, which can be galaxies, stars, or other objects, are themselves specified using a coordinate chart or \"grid\" that is laid down over all spacetime. The cosmological principle implies that the metric should be homogeneous and isotropic on large scales, which uniquely singles out the Friedmann–Lemaître–Robertson–Walker metric (FLRW metric). \nThis metric contains a scale factor, which describes how the size of the universe changes with time. This enables a convenient choice of a coordinate system to be made, called comoving coordinates. In this coordinate system, the grid expands along with the universe, and objects that are moving only because of the expansion of the universe, remain at fixed points on the grid. While their \"coordinate\" distance (comoving distance) remains constant, the \"physical\" distance between two such co-moving points expands proportionally with the scale factor of the universe.\n\nThe Big Bang is not an explosion of matter moving outward to fill an empty universe. Instead, space itself expands with time everywhere and increases the physical distance between two comoving points. In other words, the Big Bang is not an explosion \"in space\", but rather an expansion \"of space\". Because the FLRW metric assumes a uniform distribution of mass and energy, it applies to our universe only on large scales—local concentrations of matter such as our galaxy are gravitationally bound and as such do not experience the large-scale expansion of space.\n\nAn important feature of the Big Bang spacetime is the presence of particle horizons. Since the universe has a finite age, and light travels at a finite speed, there may be events in the past whose light has not had time to reach us. This places a limit or a \"past horizon\" on the most distant objects that can be observed. Conversely, because space is expanding, and more distant objects are receding ever more quickly, light emitted by us today may never \"catch up\" to very distant objects. This defines a \"future horizon\", which limits the events in the future that we will be able to influence. The presence of either type of horizon depends on the details of the FLRW model that describes our universe.\n\nOur understanding of the universe back to very early times suggests that there is a past horizon, though in practice our view is also limited by the opacity of the universe at early times. So our view cannot extend further backward in time, though the horizon recedes in space. If the expansion of the universe continues to accelerate, there is a future horizon as well.\n\nEnglish astronomer Fred Hoyle is credited with coining the term \"Big Bang\" during a 1949 BBC radio broadcast, saying: \"These theories were based on the hypothesis that all the matter in the universe was created in one big bang at a particular time in the remote past.\"\n\nIt is popularly reported that Hoyle, who favored an alternative \"steady state\" cosmological model, intended this to be pejorative, but Hoyle explicitly denied this and said it was just a striking image meant to highlight the difference between the two models.\n\nThe Big Bang theory developed from observations of the structure of the universe and from theoretical considerations. In 1912 Vesto Slipher measured the first Doppler shift of a \"spiral nebula\" (spiral nebula is the obsolete term for spiral galaxies), and soon discovered that almost all such nebulae were receding from Earth. He did not grasp the cosmological implications of this fact, and indeed at the time it was highly controversial whether or not these nebulae were \"island universes\" outside our Milky Way. Ten years later, Alexander Friedmann, a Russian cosmologist and mathematician, derived the Friedmann equations from Albert Einstein's equations of general relativity, showing that the universe might be expanding in contrast to the static universe model advocated by Einstein at that time. In 1924 Edwin Hubble's measurement of the great distance to the nearest spiral nebulae showed that these systems were indeed other galaxies. Independently deriving Friedmann's equations in 1927, Georges Lemaître, a Belgian physicist, proposed that the inferred recession of the nebulae was due to the expansion of the universe.\n\nIn 1931 Lemaître went further and suggested that the evident expansion of the universe, if projected back in time, meant that the further in the past the smaller the universe was, until at some finite time in the past all the mass of the universe was concentrated into a single point, a \"primeval atom\" where and when the fabric of time and space came into existence.\n\nStarting in 1924, Hubble painstakingly developed a series of distance indicators, the forerunner of the cosmic distance ladder, using the Hooker telescope at Mount Wilson Observatory. This allowed him to estimate distances to galaxies whose redshifts had already been measured, mostly by Slipher. In 1929 Hubble discovered a correlation between distance and recession velocity—now known as Hubble's law. Lemaître had already shown that this was expected, given the cosmological principle.\n\nIn the 1920s and 1930s almost every major cosmologist preferred an eternal steady state universe, and several complained that the beginning of time implied by the Big Bang imported religious concepts into physics; this objection was later repeated by supporters of the steady state theory. This perception was enhanced by the fact that the originator of the Big Bang theory, Georges Lemaître, was a Roman Catholic priest. Arthur Eddington agreed with Aristotle that the universe did not have a beginning in time, \"viz\"., that matter is eternal. A beginning in time was \"repugnant\" to him. Lemaître, however, thought thatIf the world has begun with a single quantum, the notions of space and time would altogether fail to have any meaning at the beginning; they would only begin to have a sensible meaning when the original quantum had been divided into a sufficient number of quanta. If this suggestion is correct, the beginning of the world happened a little before the beginning of space and time.\n\nDuring the 1930s other ideas were proposed as non-standard cosmologies to explain Hubble's observations, including the Milne model, the oscillatory universe (originally suggested by Friedmann, but advocated by Albert Einstein and Richard Tolman) and Fritz Zwicky's tired light hypothesis.\n\nAfter World War II, two distinct possibilities emerged. One was Fred Hoyle's steady state model, whereby new matter would be created as the universe seemed to expand. In this model the universe is roughly the same at any point in time. The other was Lemaître's Big Bang theory, advocated and developed by George Gamow, who introduced big bang nucleosynthesis (BBN) and whose associates, Ralph Alpher and Robert Herman, predicted the CMB. Ironically, it was Hoyle who coined the phrase that came to be applied to Lemaître's theory, referring to it as \"this \"big bang\" idea\" during a BBC Radio broadcast in March 1949. For a while, support was split between these two theories. Eventually, the observational evidence, most notably from radio source counts, began to favor Big Bang over Steady State. The discovery and confirmation of the CMB in 1964 secured the Big Bang as the best theory of the origin and evolution of the universe. Much of the current work in cosmology includes understanding how galaxies form in the context of the Big Bang, understanding the physics of the universe at earlier and earlier times, and reconciling observations with the basic theory.\n\nIn 1968 and 1970 Roger Penrose, Stephen Hawking, and George F. R. Ellis published papers where they showed that mathematical singularities were an inevitable initial condition of general relativistic models of the Big Bang. Then, from the 1970s to the 1990s, cosmologists worked on characterizing the features of the Big Bang universe and resolving outstanding problems. In 1981, Alan Guth made a breakthrough in theoretical work on resolving certain outstanding theoretical problems in the Big Bang theory with the introduction of an epoch of rapid expansion in the early universe he called \"inflation\". Meanwhile, during these decades, two questions in observational cosmology that generated much discussion and disagreement were over the precise values of the Hubble Constant and the matter-density of the universe (before the discovery of dark energy, thought to be the key predictor for the eventual fate of the universe).\n\nIn the mid-1990s, observations of certain globular clusters appeared to indicate that they were about 15 billion years old, which conflicted with most then-current estimates of the age of the universe (and indeed with the age measured today). This issue was later resolved when new computer simulations, which included the effects of mass loss due to stellar winds, indicated a much younger age for globular clusters. While there still remain some questions as to how accurately the ages of the clusters are measured, globular clusters are of interest to cosmology as some of the oldest objects in the universe.\n\nSignificant progress in Big Bang cosmology has been made since the late 1990s as a result of advances in telescope technology as well as the analysis of data from satellites such as COBE, the Hubble Space Telescope and WMAP. Cosmologists now have fairly precise and accurate measurements of many of the parameters of the Big Bang model, and have made the unexpected discovery that the expansion of the universe appears to be accelerating.\n\nThe earliest and most direct observational evidence of the validity of the theory are the expansion of the universe according to Hubble's law (as indicated by the redshifts of galaxies), discovery and measurement of the cosmic microwave background and the relative abundances of light elements produced by Big Bang nucleosynthesis. More recent evidence includes observations of galaxy formation and evolution, and the distribution of large-scale cosmic structures, These are sometimes called the \"four pillars\" of the Big Bang theory.\n\nPrecise modern models of the Big Bang appeal to various exotic physical phenomena that have not been observed in terrestrial laboratory experiments or incorporated into the Standard Model of particle physics. Of these features, dark matter is currently subjected to the most active laboratory investigations. Remaining issues include the cuspy halo problem and the dwarf galaxy problem of cold dark matter. Dark energy is also an area of intense interest for scientists, but it is not clear whether direct detection of dark energy will be possible. Inflation and baryogenesis remain more speculative features of current Big Bang models. Viable, quantitative explanations for such phenomena are still being sought. These are currently unsolved problems in physics.\n\nObservations of distant galaxies and quasars show that these objects are redshifted—the light emitted from them has been shifted to longer wavelengths. This can be seen by taking a frequency spectrum of an object and matching the spectroscopic pattern of emission lines or absorption lines corresponding to atoms of the chemical elements interacting with the light. These redshifts are uniformly isotropic, distributed evenly among the observed objects in all directions. If the redshift is interpreted as a Doppler shift, the recessional velocity of the object can be calculated. For some galaxies, it is possible to estimate distances via the cosmic distance ladder. When the recessional velocities are plotted against these distances, a linear relationship known as Hubble's law is observed:\nformula_5\nwhere\n\nHubble's law has two possible explanations. Either we are at the center of an explosion of galaxies—which is untenable given the Copernican principle—or the universe is uniformly expanding everywhere. This universal expansion was predicted from general relativity by Alexander Friedmann in 1922 and Georges Lemaître in 1927, well before Hubble made his 1929 analysis and observations, and it remains the cornerstone of the Big Bang theory as developed by Friedmann, Lemaître, Robertson, and Walker.\n\nThe theory requires the relation formula_9 to hold at all times, where formula_7 is the comoving distance, \"v\" is the recessional velocity, and formula_6, formula_12, and formula_7 vary as the universe expands (hence we write formula_8 to denote the present-day Hubble \"constant\"). For distances much smaller than the size of the observable universe, the Hubble redshift can be thought of as the Doppler shift corresponding to the recession velocity formula_6. However, the redshift is not a true Doppler shift, but rather the result of the expansion of the universe between the time the light was emitted and the time that it was detected.\n\nThat space is undergoing metric expansion is shown by direct observational evidence of the Cosmological principle and the Copernican principle, which together with Hubble's law have no other explanation. Astronomical redshifts are extremely isotropic and homogeneous, supporting the Cosmological principle that the universe looks the same in all directions, along with much other evidence. If the redshifts were the result of an explosion from a center distant from us, they would not be so similar in different directions.\n\nMeasurements of the effects of the cosmic microwave background radiation on the dynamics of distant astrophysical systems in 2000 proved the Copernican principle, that, on a cosmological scale, the Earth is not in a central position. Radiation from the Big Bang was demonstrably warmer at earlier times throughout the universe. Uniform cooling of the CMB over billions of years is explainable only if the universe is experiencing a metric expansion, and excludes the possibility that we are near the unique center of an explosion.\n\nIn 1964 Arno Penzias and Robert Wilson serendipitously discovered the cosmic background radiation, an omnidirectional signal in the microwave band. Their discovery provided substantial confirmation of the big-bang predictions by Alpher, Herman and Gamow around 1950. Through the 1970s the radiation was found to be approximately consistent with a black body spectrum in all directions; this spectrum has been redshifted by the expansion of the universe, and today corresponds to approximately 2.725 K. This tipped the balance of evidence in favor of the Big Bang model, and Penzias and Wilson were awarded a Nobel Prize in 1978.\n\nThe \"surface of last scattering\" corresponding to emission of the CMB occurs shortly after \"recombination\", the epoch when neutral hydrogen becomes stable. Prior to this, the universe comprised a hot dense photon-baryon plasma sea where photons were quickly scattered from free charged particles. Peaking at around , the mean free path for a photon becomes long enough to reach the present day and the universe becomes transparent.\nIn 1989, NASA launched the Cosmic Background Explorer satellite (COBE), which made two major advances: in 1990, high-precision spectrum measurements showed that the CMB frequency spectrum is an almost perfect blackbody with no deviations at a level of 1 part in 10, and measured a residual temperature of 2.726 K (more recent measurements have revised this figure down slightly to 2.7255 K); then in 1992, further COBE measurements discovered tiny fluctuations (anisotropies) in the CMB temperature across the sky, at a level of about one part in 10. John C. Mather and George Smoot were awarded the 2006 Nobel Prize in Physics for their leadership in these results.\n\nDuring the following decade, CMB anisotropies were further investigated by a large number of ground-based and balloon experiments. In 2000–2001 several experiments, most notably BOOMERanG, found the shape of the universe to be spatially almost flat by measuring the typical angular size (the size on the sky) of the anisotropies.\n\nIn early 2003, the first results of the Wilkinson Microwave Anisotropy Probe (WMAP) were released, yielding what were at the time the most accurate values for some of the cosmological parameters. The results disproved several specific cosmic inflation models, but are consistent with the inflation theory in general. The Planck space probe was launched in May 2009. Other ground and balloon based cosmic microwave background experiments are ongoing.\n\nUsing the Big Bang model it is possible to calculate the concentration of helium-4, helium-3, deuterium, and lithium-7 in the universe as ratios to the amount of ordinary hydrogen. The relative abundances depend on a single parameter, the ratio of photons to baryons. This value can be calculated independently from the detailed structure of CMB fluctuations. The ratios predicted (by mass, not by number) are about 0.25 for <chem>^4He/H</chem>, about 10 for <chem>^2H/H</chem>, about 10 for <chem>^3He/H</chem> and about 10 for <chem>^7Li/H</chem>.\n\nThe measured abundances all agree at least roughly with those predicted from a single value of the baryon-to-photon ratio. The agreement is excellent for deuterium, close but formally discrepant for <chem>^4He</chem>, and off by a factor of two for <chem>^7Li</chem>; in the latter two cases there are substantial systematic uncertainties. Nonetheless, the general consistency with abundances predicted by Big Bang nucleosynthesis is strong evidence for the Big Bang, as the theory is the only known explanation for the relative abundances of light elements, and it is virtually impossible to \"tune\" the Big Bang to produce much more or less than 20–30% helium. Indeed, there is no obvious reason outside of the Big Bang that, for example, the young universe (i.e., before star formation, as determined by studying matter supposedly free of stellar nucleosynthesis products) should have more helium than deuterium or more deuterium than <chem>^3He</chem>, and in constant ratios, too.\n\nDetailed observations of the morphology and distribution of galaxies and quasars are in agreement with the current state of the Big Bang theory. A combination of observations and theory suggest that the first quasars and galaxies formed about a billion years after the Big Bang, and since then, larger structures have been forming, such as galaxy clusters and superclusters.\n\nPopulations of stars have been aging and evolving, so that distant galaxies (which are observed as they were in the early universe) appear very different from nearby galaxies (observed in a more recent state). Moreover, galaxies that formed relatively recently, appear markedly different from galaxies formed at similar distances but shortly after the Big Bang. These observations are strong arguments against the steady-state model. Observations of star formation, galaxy and quasar distributions and larger structures, agree well with Big Bang simulations of the formation of structure in the universe, and are helping to complete details of the theory.\n\nIn 2011, astronomers found what they believe to be pristine clouds of primordial gas by analyzing absorption lines in the spectra of distant quasars. Before this discovery, all other astronomical objects have been observed to contain heavy elements that are formed in stars. These two clouds of gas contain no elements heavier than hydrogen and deuterium. Since the clouds of gas have no heavy elements, they likely formed in the first few minutes after the Big Bang, during Big Bang nucleosynthesis.\n\nThe age of the universe as estimated from the Hubble expansion and the CMB is now in good agreement with other estimates using the ages of the oldest stars, both as measured by applying the theory of stellar evolution to globular clusters and through radiometric dating of individual Population II stars.\n\nThe prediction that the CMB temperature was higher in the past has been experimentally supported by observations of very low temperature absorption lines in gas clouds at high redshift. This prediction also implies that the amplitude of the Sunyaev–Zel'dovich effect in clusters of galaxies does not depend directly on redshift. Observations have found this to be roughly true, but this effect depends on cluster properties that do change with cosmic time, making precise measurements difficult.\n\nFuture gravitational waves observatories might be able to detect primordial gravitational waves, relics of the early universe, up to less than a second after the Big Bang.\n\nAs with any theory, a number of mysteries and problems have arisen as a result of the development of the Big Bang theory. Some of these mysteries and problems have been resolved while others are still outstanding. Proposed solutions to some of the problems in the Big Bang model have revealed new mysteries of their own. For example, the horizon problem, the magnetic monopole problem, and the flatness problem are most commonly resolved with inflationary theory, but the details of the inflationary universe are still left unresolved and many, including some founders of the theory, say it has been disproven. What follows are a list of the mysterious aspects of the Big Bang theory still under intense investigation by cosmologists and astrophysicists.\n\nIt is not yet understood why the universe has more matter than antimatter. It is generally assumed that when the universe was young and very hot it was in statistical equilibrium and contained equal numbers of baryons and antibaryons. However, observations suggest that the universe, including its most distant parts, is made almost entirely of matter. A process called baryogenesis was hypothesized to account for the asymmetry. For baryogenesis to occur, the Sakharov conditions must be satisfied. These require that baryon number is not conserved, that C-symmetry and CP-symmetry are violated and that the universe depart from thermodynamic equilibrium. All these conditions occur in the Standard Model, but the effects are not strong enough to explain the present baryon asymmetry.\n\nMeasurements of the redshift–magnitude relation for type Ia supernovae indicate that the expansion of the universe has been accelerating since the universe was about half its present age. To explain this acceleration, general relativity requires that much of the energy in the universe consists of a component with large negative pressure, dubbed \"dark energy\".\n\nDark energy, though speculative, solves numerous problems. Measurements of the cosmic microwave background indicate that the universe is very nearly spatially flat, and therefore according to general relativity the universe must have almost exactly the critical density of mass/energy. But the mass density of the universe can be measured from its gravitational clustering, and is found to have only about 30% of the critical density. Since theory suggests that dark energy does not cluster in the usual way it is the best explanation for the \"missing\" energy density. Dark energy also helps to explain two geometrical measures of the overall curvature of the universe, one using the frequency of gravitational lenses, and the other using the characteristic pattern of the large-scale structure as a cosmic ruler.\n\nNegative pressure is believed to be a property of vacuum energy, but the exact nature and existence of dark energy remains one of the great mysteries of the Big Bang. Results from the WMAP team in 2008 are in accordance with a universe that consists of 73% dark energy, 23% dark matter, 4.6% regular matter and less than 1% neutrinos. According to theory, the energy density in matter decreases with the expansion of the universe, but the dark energy density remains constant (or nearly so) as the universe expands. Therefore, matter made up a larger fraction of the total energy of the universe in the past than it does today, but its fractional contribution will fall in the far future as dark energy becomes even more dominant.\n\nThe dark energy component of the universe has been explained by theorists using a variety of competing theories including Einstein's cosmological constant but also extending to more exotic forms of quintessence or other modified gravity schemes. A cosmological constant problem, sometimes called the \"most embarrassing problem in physics\", results from the apparent discrepancy between the measured energy density of dark energy, and the one naively predicted from Planck units.\n\nDuring the 1970s and the 1980s, various observations showed that there is not sufficient visible matter in the universe to account for the apparent strength of gravitational forces within and between galaxies. This led to the idea that up to 90% of the matter in the universe is dark matter that does not emit light or interact with normal baryonic matter. In addition, the assumption that the universe is mostly normal matter led to predictions that were strongly inconsistent with observations. In particular, the universe today is far more lumpy and contains far less deuterium than can be accounted for without dark matter. While dark matter has always been controversial, it is inferred by various observations: the anisotropies in the CMB, galaxy cluster velocity dispersions, large-scale structure distributions, gravitational lensing studies, and X-ray measurements of galaxy clusters.\n\nIndirect evidence for dark matter comes from its gravitational influence on other matter, as no dark matter particles have been observed in laboratories. Many particle physics candidates for dark matter have been proposed, and several projects to detect them directly are underway.\n\nAdditionally, there are outstanding problems associated with the currently favored cold dark matter model which include the dwarf galaxy problem and the cuspy halo problem. Alternative theories have been proposed that do not require a large amount of undetected matter, but instead modify the laws of gravity established by Newton and Einstein; yet no alternative theory has been as successful as the cold dark matter proposal in explaining all extant observations.\n\nThe horizon problem results from the premise that information cannot travel faster than light. In a universe of finite age this sets a limit—the particle horizon—on the separation of any two regions of space that are in causal contact. The observed isotropy of the CMB is problematic in this regard: if the universe had been dominated by radiation or matter at all times up to the epoch of last scattering, the particle horizon at that time would correspond to about 2 degrees on the sky. There would then be no mechanism to cause wider regions to have the same temperature.\n\nA resolution to this apparent inconsistency is offered by inflationary theory in which a homogeneous and isotropic scalar energy field dominates the universe at some very early period (before baryogenesis). During inflation, the universe undergoes exponential expansion, and the particle horizon expands much more rapidly than previously assumed, so that regions presently on opposite sides of the observable universe are well inside each other's particle horizon. The observed isotropy of the CMB then follows from the fact that this larger region was in causal contact before the beginning of inflation.\n\nHeisenberg's uncertainty principle predicts that during the inflationary phase there would be quantum thermal fluctuations, which would be magnified to cosmic scale. These fluctuations serve as the seeds of all current structure in the universe. Inflation predicts that the primordial fluctuations are nearly scale invariant and Gaussian, which has been accurately confirmed by measurements of the CMB.\n\nIf inflation occurred, exponential expansion would push large regions of space well beyond our observable horizon.\n\nA related issue to the classic horizon problem arises because in most standard cosmological inflation models, inflation ceases well before electroweak symmetry breaking occurs, so inflation should not be able to prevent large-scale discontinuities in the electroweak vacuum since distant parts of the observable universe were causally separate when the electroweak epoch ended.\n\nThe magnetic monopole objection was raised in the late 1970s. Grand unified theories predicted topological defects in space that would manifest as magnetic monopoles. These objects would be produced efficiently in the hot early universe, resulting in a density much higher than is consistent with observations, given that no monopoles have been found. This problem is also resolved by cosmic inflation, which removes all point defects from the observable universe, in the same way that it drives the geometry to flatness.\n\nThe flatness problem (also known as the oldness problem) is an observational problem associated with a Friedmann–Lemaître–Robertson–Walker metric (FLRW). The universe may have positive, negative, or zero spatial curvature depending on its total energy density. Curvature is negative if its density is less than the critical density; positive if greater; and zero at the critical density, in which case space is said to be \"flat\".\n\nThe problem is that any small departure from the critical density grows with time, and yet the universe today remains very close to flat. Given that a natural timescale for departure from flatness might be the Planck time, 10 seconds, the fact that the universe has reached neither a heat death nor a Big Crunch after billions of years requires an explanation. For instance, even at the relatively late age of a few minutes (the time of nucleosynthesis), the density of the universe must have been within one part in 10 of its critical value, or it would not exist as it does today.\n\nPhysics may conclude that time did not exist before 'Big Bang', but 'started' with the Big Bang and hence there might be no 'beginning', 'before' or potentially 'cause' and instead always existed. Quantum fluctuations, or other laws of physics that may have existed at the start of the Big Bang could then create the conditions for matter to occur.\n\nBefore observations of dark energy, cosmologists considered two scenarios for the future of the universe. If the mass density of the universe were greater than the critical density, then the universe would reach a maximum size and then begin to collapse. It would become denser and hotter again, ending with a state similar to that in which it started—a Big Crunch.\n\nAlternatively, if the density in the universe were equal to or below the critical density, the expansion would slow down but never stop. Star formation would cease with the consumption of interstellar gas in each galaxy; stars would burn out, leaving white dwarfs, neutron stars, and black holes. Very gradually, collisions between these would result in mass accumulating into larger and larger black holes. The average temperature of the universe would asymptotically approach absolute zero—a Big Freeze. Moreover, if the proton were unstable, then baryonic matter would disappear, leaving only radiation and black holes. Eventually, black holes would evaporate by emitting Hawking radiation. The entropy of the universe would increase to the point where no organized form of energy could be extracted from it, a scenario known as heat death.\n\nModern observations of accelerating expansion imply that more and more of the currently visible universe will pass beyond our event horizon and out of contact with us. The eventual result is not known. The ΛCDM model of the universe contains dark energy in the form of a cosmological constant. This theory suggests that only gravitationally bound systems, such as galaxies, will remain together, and they too will be subject to heat death as the universe expands and cools. Other explanations of dark energy, called phantom energy theories, suggest that ultimately galaxy clusters, stars, planets, atoms, nuclei, and matter itself will be torn apart by the ever-increasing expansion in a so-called Big Rip.\n\nThe following is a partial list of misconceptions about the Big Bang model:\n\n\"The Big Bang as the origin of the universe:\" One of the common misconceptions about the Big Bang model is the belief that it was the origin of the universe. However, the Big Bang model does not comment about how the universe came into being. Current conception of the Big Bang model assumes the existence of energy, time, and space, and does not comment about their origin or the cause of the dense and high temperature initial state of the universe.\n\n\"The Big Bang was \"small\"\": It is misleading to visualize the Big Bang by comparing its size to everyday objects. When the size of the universe at Big Bang is described, it refers to the size of the observable universe, and not the entire universe.\n\n\"Hubble's law violates the special theory of relativity\": Hubble's law predicts that galaxies that are beyond Hubble Distance recede faster than the speed of light. However, special relativity does not apply beyond motion through space. Hubble's law describes velocity that results from expansion \"of\" space, rather than \"through\" space.\n\n\"Doppler redshift vs cosmological red-shift\": Astronomers often refer to the cosmological red-shift as a normal Doppler shift, which is a misconception. Although similar, the cosmological red-shift is not identical to the Doppler redshift. The Doppler redshift is based on special relativity, which does not consider the expansion of space. On the contrary, the cosmological red-shift is based on general relativity, in which the expansion of space is considered. Although they may appear identical for nearby galaxies, it may cause confusion if the behavior of distant galaxies is understood through the Doppler redshift.\n\nWhile the Big Bang model is well established in cosmology, it is likely to be refined. The Big Bang theory, built upon the equations of classical general relativity, indicates a singularity at the origin of cosmic time; this infinite energy density is regarded as impossible in physics. Still, it is known that the equations are not applicable before the time when the universe cooled down to the Planck temperature, and this conclusion depends on various assumptions, of which some could never be experimentally verified. \"(Also see Planck epoch.)\"\n\nOne proposed refinement to avoid this would-be singularity is to develop a correct treatment of quantum gravity.\n\nIt is not known what could have preceded the hot dense state of the early universe or how and why it originated, though speculation abounds in the field of cosmogony.\n\nSome proposals, each of which entails untested hypotheses, are:\n\nProposals in the last two categories see the Big Bang as an event in either a much larger and older universe or in a multiverse.\n\nAs a description of the origin of the universe, the Big Bang has significant bearing on religion and philosophy. As a result, it has become one of the liveliest areas in the discourse between science and religion. Some believe the Big Bang implies a creator, and some see its mention in their holy books, while others argue that Big Bang cosmology makes the notion of a creator superfluous.\n\n\n\n\n"}
{"id": "40836275", "url": "https://en.wikipedia.org/wiki?curid=40836275", "title": "Cosmic age problem", "text": "Cosmic age problem\n\nThe cosmic age problem is a historical problem in astronomy concerning the age of the universe. The problem was that at various times in the 20th century, some objects in the universe were estimated to be older than the time elapsed since the Big Bang, as estimated from measurements of the expansion rate of the universe known as the Hubble constant, denoted H. (This is more correctly called the Hubble parameter, since it generally varies with time).\nIf so, this would represent a contradiction, since objects such as galaxies, stars and planets could not have existed in the extreme temperatures and densities shortly after the Big Bang.\n\nSince around 1997–2003, the problem is believed to be solved by most cosmologists: modern cosmological measurements lead to a precise estimate of the age of the universe (i.e. time since the Big Bang) of 13.8 billion years, and recent age estimates for the oldest objects are either younger than this, or consistent allowing for measurement uncertainties.\n\nFollowing theoretical developments of the Friedmann equations by Alexander Friedmann and Georges Lemaître in the 1920s, and the discovery of the expanding universe by Edwin Hubble in 1929, it was immediately clear that tracing this expansion backwards in time predicts that the universe had almost zero size at a finite time in the past. This concept, initially known as the \"Primeval Atom\" by Lemaitre, was later elaborated into the modern Big Bang theory. If the universe had expanded at a constant rate in the past, the age of the universe now (i.e. the time since the Big Bang) is simply the inverse of the Hubble constant, often known as the \"Hubble time\". For Big Bang models with zero cosmological constant and positive matter density, the actual age must be somewhat younger than this Hubble time; typically the age would be between 66% and 90% of the Hubble time, depending on the density of matter.\n\nHubble's early estimate of his constant was 550 (km/s)/Mpc, and the inverse of that is 1.8 billion years. It was believed by many geologists such as Arthur Holmes in the 1920s that the Earth was probably over 2 billion years old, but with large uncertainty. The possible discrepancy between the ages of the Earth and the universe was probably one motivation for the development of the Steady State theory in 1948 as an alternative to the Big Bang; in the (now obsolete) steady state theory, the universe is infinitely old and on average unchanging with time. The steady state theory postulated spontaneous creation of matter to keep the average density constant as the universe expands, and therefore most galaxies still have an age less than 1/H. However, if H had been 550 (km/s)/Mpc, our Milky Way galaxy would be exceptionally large compared to most other galaxies, so it could well be much older than an average galaxy, therefore eliminating the age problem.\n\nIn the 1950s, two substantial errors were discovered in Hubble's extragalactic distance scale: first in 1952, Walter Baade discovered there were two classes of Cepheid variable star. Hubble's sample comprised different classes nearby and in other galaxies, and correcting this error made all other galaxies twice as distant as Hubble's values, thus doubling the Hubble time. A second error was discovered by Allan Sandage and coworkers: for galaxies beyond the Local Group, Cepheids were too faint to observe with Hubble's instruments, so Hubble used the brightest stars as distance indicators. Many of Hubble's \"brightest stars\" were actually HII regions or clusters containing many stars, which caused another underestimation of distances for these more distant galaxies. Thus, in 1958 Sandage published the first reasonably accurate measurement of the Hubble constant, at 75 (km/s)/Mpc, which is close to modern estimates of 68–74 (km/s)/Mpc.\n\nThe age of the Earth (actually the Solar System) was first accurately measured around 1955 by Clair Patterson at 4.55 billion years, essentially identical to the modern value. For H ~ 75 (km/s)/Mpc, the inverse of H is 13.0 billion years; so after 1958 the Big Bang model age was comfortably older than the Earth.\n\nHowever, in the 1960s and onwards, new developments in the theory of stellar evolution enabled age estimates for large star clusters called globular clusters: these generally gave age estimates of around 15 billion years, with substantial scatter. Further revisions of the Hubble constant by Sandage and Gustav Tammann in the 1970s gave values around 50–60 (km/s)/Mpc, and an inverse of 16-20 billion years, consistent with globular cluster ages.\n\nHowever, in the late 1970s to early 1990s, the age problem re-appeared: new estimates of the Hubble constant gave higher values, with Gerard de Vaucouleurs estimating values 90–100 (km/s)/Mpc, while Marc Aaronson and co-workers gave values around 80-90  (km/s)/Mpc. Sandage and Tammann continued to argue for values 50-60, leading to a period of controversy sometimes called the \"Hubble wars\". The higher values for H appeared to predict a universe younger than the globular cluster ages, and gave rise to some speculations during the 1980s that the Big Bang model was seriously incorrect.\n\nThe age problem was eventually thought to be resolved by several developments between 1995-2003: firstly, a large program with the Hubble space telescope measured the Hubble constant at 72 (km/s)/Mpc with 10 percent uncertainty. Secondly, measurements of parallax by the Hipparcos spacecraft in 1995 revised globular cluster distances upwards by 5-10 percent; this made their stars brighter than previously estimated and therefore younger, shifting their age estimates down to around 12-13 billion years. Finally, from 1998-2003 a number of new cosmological observations including supernovae, cosmic microwave background observations and large galaxy redshift surveys led to the acceptance of dark energy and the establishment of the Lambda-CDM model as the standard model of cosmology. The presence of dark energy implies that the universe was expanding more slowly at around half its present age than today, which makes the universe older for a given value of the Hubble constant. The combination of the three results above essentially removed the discrepancy between estimated globular cluster ages and the age of the universe.\n\nMore recent measurements from WMAP and the Planck spacecraft lead to an estimate of the age of the universe of 13.80 billion years with only 0.3 percent uncertainty (based on the standard Lambda-CDM model), and modern age measurements for globular clusters and other objects are currently smaller than this value (within the measurement uncertainties). A substantial majority of cosmologists therefore believe the age problem is now resolved.\n\n"}
{"id": "1643492", "url": "https://en.wikipedia.org/wiki?curid=1643492", "title": "Cosmic latte", "text": "Cosmic latte\n\nCosmic latte is a name assigned to the average color of the universe, found by a team of astronomers from Johns Hopkins University. In 2001, Karl Glazebrook and Ivan Baldry determined that the average color of the universe was a greenish white, but they soon corrected their analysis in a 2002 paper in which they reported that their survey of the light from over 200,000 galaxies averaged to a slightly beigeish white. The hex triplet value for cosmic latte is #FFF8E7.\n\nFinding the average color of the universe was not the focus of the study. Rather, the study examined spectral analysis of different galaxies to study star formation. Like Fraunhofer lines, the dark lines displayed in the study's spectral ranges display older and younger stars and allow Glazebrook and Baldry to determine the age of different galaxies and star systems. What the study revealed is that the overwhelming majority of stars formed about 5 billion years ago. Because these stars would have been \"brighter\" in the past, the color of the universe changes over time shifting from blue to red as more blue stars change to yellow and eventually red giants.\n\nAs light from distant galaxies reaches the Earth, the average \"color of the universe\" (as seen from Earth) tends towards pure white, due to the light coming from the stars when they were much younger and bluer.\n\nThe corrected color was initially published on the Johns Hopkins News website and updated on the team's initial announcement. Multiple news outlets, including NPR and BBC, displayed the color in stories and some relayed the request by Glazebrook on the announcement asking for suggestions for names, jokingly adding all were welcome as long as they were not \"beige\".\n\nThese were the results of a vote of the scientists involved based on the new color:\nThough Drum's suggestion of \"cappuccino cosmico\" received the most votes, the researchers favored Drum's other suggestion, \"cosmic latte\". This is because the similar \"Latteo\" means \"Milky\" in Italian, Galileo's native language. It also leads to the similarity to the Italian term for the Milky Way, \"Via Lattea\", and they enjoyed the fact that the color would be similar to the Milky Way's average color as well, as it is part of the sum of the universe. They also claimed to be \"caffeine biased\".\n\nDrum came up with the name while sitting in a Starbucks drinking a latte and reading the \"Washington Post\". Drum noticed that the color of the universe as displayed in the newspaper was the same color as his latte.\n\n"}
{"id": "12527335", "url": "https://en.wikipedia.org/wiki?curid=12527335", "title": "Cosmic time", "text": "Cosmic time\n\nCosmic time (also known as time since the big bang) is the time coordinate commonly used in the Big Bang models of physical cosmology. It is defined for homogeneous, expanding universes as follows: Choose a time coordinate so that the universe has the same density everywhere at each moment in time (the fact that this is possible means that the universe is, by definition, homogeneous). Measure the passage of time using clocks moving with the Hubble flow. Choose the big bang singularity as the origin of the time coordinate. \n\nCosmic time formula_1 is a measure of time by a physical clock with zero peculiar velocity in the absence of matter over-/under-densities (to prevent time dilation due to relativistic effects or confusions caused by expansion of the universe). Unlike other measures of time such as temperature, redshift, particle horizon, or Hubble horizon, the cosmic time (similar and complementary to the comoving coordinates) is blind to the expansion of the universe. \n\nThere are two main ways for establishing a reference point for the cosmic time. The most trivial way is to take the present time as the cosmic reference point (sometimes referred to as the lookback time) or alternatively, take the Big Bang as formula_2 (also referred to as age of the universe). The big bang doesn't necessarily have to correspond to a physical event but rather it refers to the point at which the scale factor would vanish for a standard cosmological model such as ΛCDM. For instance, in the case of inflation, i.e. a non-standard cosmology, the hypothetical moment of big bang is still determined using the benchmark cosmological models which may coincide with the end of the inflationary epoch. For inflationary models, it is not possible to establish a well defined origin of time before the big bang since the universe does not require a beginning event in such models. For technical purposes, concepts such as the average temperature of the universe (in units of eV) or the particle horizon are used when the early universe is the objective of a study since understanding the interaction among particles is more relevant than their time coordinate or age. \n\nCosmic time is the standard time coordinate for specifying the Friedmann–Lemaître–Robertson–Walker solutions of Einstein's equations.\n\n"}
{"id": "38737", "url": "https://en.wikipedia.org/wiki?curid=38737", "title": "Cosmos", "text": "Cosmos\n\nThe cosmos (, ) is the universe. Using the word \"cosmos\" rather than the word \"universe\" implies viewing the universe as a complex and orderly system or entity; the opposite of chaos.\nThe cosmos, and our understanding of the reasons for its existence and significance, are studied in cosmology - a very broad discipline covering any scientific, religious, or philosophical contemplation of the cosmos and its nature, or reasons for existing. Religious and philosophical approaches may include in their concepts of the cosmos various spiritual entities or other matters deemed to exist outside our physical universe.\n\nThe philosopher Pythagoras first used the term \"cosmos\" () for the order of the universe. The term became part of modern language in the 19th century when geographer–polymath Alexander von Humboldt resurrected the use of the word from the ancient Greek, assigned it to his five-volume treatise, \"Kosmos\", which influenced modern and somewhat holistic perception of the universe as one interacting entity.\n\nCosmology is the study of the cosmos, and in its broadest sense covers a variety of very different approaches: scientific, religious and philosophical. All cosmologies have in common an attempt to understand the implicit order within the whole of being. In this way, most religions and philosophical systems have a cosmology.\n\nWhen \"cosmology\" is used without a qualifier, it often signifies physical cosmology, unless the context makes clear that a different meaning is intended.\n\nPhysical cosmology (often simply described as 'cosmology') is the scientific study of the universe, from the beginning of its physical existence. It includes speculative concepts such as a multiverse, when these are being discussed. In physical cosmology, the term \"cosmos\" is often used in a technical way, referring to a particular spacetime continuum within a (postulated) multiverse. Our particular cosmos, the observable universe, is generally capitalized as \"the Cosmos\". \n\nIn physical cosmology, the uncapitalized term cosmic signifies a subject with a relationship to the universe, such as 'cosmic time' (time since the Big Bang), 'cosmic rays' (high energy particles or radiation detected from space), and 'cosmic microwave background' (microwave radiation detectable from all directions in space).\n\nAccording to in Sir William Smith \"Dictionary of Greek and Roman Biography and Mythology\" (1870, see book screenshot for full quote), Pythagoreans described the universe.\n\nCosmology is a branch of metaphysics that deals with the nature of the universe, a theory or doctrine describing the natural order of the universe. The basic definition of Cosmology is the science of the origin and development of the universe. In modern astronomy the Big Bang theory is the dominant postulation.\n\nIn theology, the cosmos is the created heavenly bodies (sun, moon, planets, and fixed stars). In Christian theology, the word is also used synonymously with \"aion\" to refer to \"worldly life\" or \"this world\" or \"this age\" as opposed to the afterlife or world to come.\n\nThe 1870 book \"Dictionary of Greek and Roman Biography and Mythology\" noted\n\nThe book \"The Works of Aristotle\" (1908, p. 80 \"Fragments\") mentioned\n\nBertrand Russell (1947) noted\n\n"}
{"id": "1045142", "url": "https://en.wikipedia.org/wiki?curid=1045142", "title": "Debris", "text": "Debris\n\nDebris or débris (, ) is rubble, wreckage, ruins, litter and discarded garbage/refuse/trash, scattered remains of something destroyed, discarded, or as in geology, large rock fragments left by a melting glacier etc. Depending on context, \"debris\" can refer to a number of different things. The first apparent use of the French word in English is in a 1701 description of the army of Prince Rupert upon its retreat from a battle with the army of Oliver Cromwell, in England.\n\nIn disaster scenarios, tornados leave behind large pieces of houses and mass destruction overall. This debris also flies around the tornado itself when it is in progress. The tornado's winds capture debris it kicks up in its wind orbit, and spins it inside its vortex. The tornado's wind radius is larger than the funnel itself. tsunamis and hurricanes also bring large amounts of debris, such as Hurricane Katrina in 2005 and Hurricane Sandy in 2012. Earthquakes rock cities to rubble debris.\n\nIn geology, debris usually applies to the remains of geological activity including landslides, volcanic explosions, avalanches, mudflows or Glacial lake outburst floods (Jökulhlaups) and moraine, lahars, and lava eruptions. Geological debris sometimes moves in a stream called a debris flow. When it accumulates at the base of hillsides, it can be called \"talus\" or \"scree\".\n\nIn mining, debris called \"attle\" usually consists of rock fragments which contain little or no ore.\n\n\"Marine debris\" applies to floating garbage such as bottles, cans, styrofoam, cruise ship waste, offshore oil and gas exploration and production facilities pollution, and fishing paraphernalia from professional and recreational boaters. Marine debris is also called litter or flotsam and jetsam. Objects that can constitute marine debris include used automobile tires, detergent bottles, medical wastes, discarded fishing line and nets, soda cans, and bilge waste solids.\n\nIn addition to being unsightly, it can pose a serious threat to marine life, boats, swimmers, divers, and others. For example, each year millions of seabirds, sea turtles, fish, and marine mammals become entangled in marine debris, or ingest plastics which they have mistaken for food. As many as 30,000 northern fur seals per year get caught in abandoned fishing nets and either drown or suffocate. Whales mistake plastic bags for squid, and birds may mistake plastic pellets for fish eggs. At other times, animals accidentally eat the plastic while feeding on natural food.\n\nThe largest concentration of marine debris is the Great Pacific Garbage Patch.\n\nMarine debris most commonly originates from land-based sources. Various international agencies are currently working to reduce marine debris levels around the world.\n\nIn meteorology, debris usually applies to the remains of human habitation and natural flora after storm related destruction. This debris is also commonly referred to as storm debris. Storm debris commonly consists of roofing material, downed tree limbs, downed signs, downed power lines and poles, and wind-blown garbage. Storm debris can become a serious problem immediately after a storm, in that it often blocks access to individuals and communities that may require emergency services. This material frequently exists in such large quantities that disposing of it becomes a serious issue for a community. In addition, storm debris is often hazardous by its very nature, since, for example, downed power lines annually account for storm-related deaths.\n\n\"Space debris\" usually refers to the remains of spacecraft that have either fallen to Earth or are still orbiting Earth. Space debris may also consist of natural components such as chunks of rock and ice. The problem of space debris has grown as various space programs have left legacies of launches, explosions, repairs, and discards in both low Earth orbit and more remote orbits. These orbiting fragments have reached a great enough proportion to constitute a hazard to future space launches of both satellite and manned vehicles. Various government agencies and international organizations are beginning to track space debris and also research possible solutions to the problem. While many of these items, ranging in size from nuts and bolts to entire satellites and spacecraft, may fall to Earth, other items located in more remote orbits may stay aloft for centuries. The velocity of some of these pieces of space junk have been clocked in excess of 17,000 miles per hour (27,000 km/h). A piece of space debris falling to Earth leaves a fiery trail, just like a meteor.\n\nA debris disk is a circumstellar disk of dust and debris in orbit around a star.\n\nIn medicine, debris usually refers to biological matter that has accumulated or lodged in surgical instruments and is referred to as surgical debris. The presence of surgical debris can result in cross-infections or nosocomial infections if not removed and the affected surgical instruments or equipment properly disinfected.\n\nIn the aftermath of a war, large areas of the region of conflict are often strewn with \"war debris\" in the form of abandoned or destroyed hardware and vehicles, mines, unexploded ordnance, bullet casings and other fragments of metal.\n\nMuch war debris has the potential to be lethal and continues to kill and maim civilian populations for years after the end of a conflict. The risks from war debris may be sufficiently high to prevent or delay the return of refugees. In addition war debris may contain hazardous chemicals or radioactive components that can contaminate the land or poison civilians who come into contact with it. Many Mine clearance agencies are also involved in the clearance of war debris.\n\nLand mines in particular are very dangerous as they can remain active for decades after a conflict, which is why they have been banned by international war regulations.\n\nIn November 2006 the Protocol on Explosive Remnants of War\ncame into effect with 92 countries subscribing to the treaty that requires the parties involved in a conflict to assist with the removal of unexploded ordnance following the end of hostilities.\n\nSome of the countries most affected by war debris are Afghanistan, Angola, Cambodia, Iraq and Laos.\n\nSimilarly \"military debris\" may be found in and around firing ranges and military training areas.\n\nDebris can also be used as cover for military purposes, depending on the situation.\n\nIn South Louisiana's Creole and Cajun cultures, debris (pronounced \"DAY-bree\") refers to chopped organs such as liver, heart, kidneys, tripe, spleen, brain, lungs and pancreas.\n\n\n"}
{"id": "13566984", "url": "https://en.wikipedia.org/wiki?curid=13566984", "title": "Double layer (surface science)", "text": "Double layer (surface science)\n\nA double layer (DL, also called an electrical double layer, EDL) is a structure that appears on the surface of an object when it is exposed to a fluid. The object might be a solid particle, a gas bubble, a liquid droplet, or a porous body. The DL refers to two parallel layers of charge surrounding the object. The first layer, the surface charge (either positive or negative), consists of ions adsorbed onto the object due to chemical interactions. The second layer is composed of ions attracted to the surface charge via the Coulomb force, electrically screening the first layer. This second layer is loosely associated with the object. It is made of free ions that move in the fluid under the influence of electric attraction and thermal motion rather than being firmly anchored. It is thus called the \"diffuse layer\".\n\nInterfacial DLs are most apparent in systems with a large surface area to volume ratio, such as a colloid or porous bodies with particles or pores (respectively) on the scale of micrometres to nanometres. However, DLs are important to other phenomena, such as the electrochemical behaviour of electrodes.\n\nDLs play a fundamental role in many everyday substances. For instance, homogenized milk exists only because fat droplets are covered with a DL that prevents their coagulation into butter. DLs exist in practically all heterogeneous fluid-based systems, such as blood, paint, ink and ceramic and cement slurry.\n\nThe DL is closely related to electrokinetic phenomena and electroacoustic phenomena.\n\nWhen an \"electronic\" conductor is brought in contact with a solid or liquid \"ionic\" conductor (electrolyte), a common boundary (interface) among the two phases appears. Hermann von Helmholtz was the first to realize that charged electrodes immersed in electrolyte solutions repel the co-ions of the charge while attracting counterions to their surfaces. Two layers of opposite polarity form at the interface between electrode and electrolyte.\nIn 1853 he showed that an electrical double layer (DL) is essentially a molecular dielectric and stores charge electrostatically. Below the electrolyte's decomposition voltage, the stored charge is linearly dependent on the voltage applied.\n\nThis early model predicted a constant differential capacitance independent from the charge density depending on the dielectric constant of the electrolyte solvent and the thickness of the double-layer.\n\nThis model, with a good foundation for the description of the interface, does not consider important factors including diffusion/mixing of ions in solution, the possibility of adsorption onto the surface and the interaction between solvent dipole moments and the electrode.\n\nLouis Georges Gouy in 1910 and David Leonard Chapman in 1913 both observed that capacitance was not a constant and that it depended on the applied potential and the ionic concentration. The \"Gouy-Chapman model\" made significant improvements by introducing a diffuse model of the DL. In this model the charge distribution of ions as a function of distance from the metal surface allows Maxwell–Boltzmann statistics to be applied. Thus the electric potential decreases exponentially away from the surface of the fluid bulk.\n\nThe Gouy-Chapman model fails for highly charged DLs. In 1924 Otto Stern suggested combining the Helmholtz model with the Gouy-Chapman model: In Stern's model, some ions adhere to the electrode as suggested by Helmholtz, giving an internal Stern layer, while some form a Gouy-Chapman diffuse layer.\n\nThe Stern layer accounts for ions' finite size and consequently an ion's closest approach to the electrode is on the order of the ionic radius. The Stern model has its own limitations, namely that it effectively treats ions as point charges, assumes all significant interactions in the diffuse layer are Coulombic, and assumes dielectric permittivity to be constant throughout the double layer and that fluid viscosity is constant plane.\n\nD. C. Grahame modified the Stern model in 1947. He proposed that some ionic or uncharged species can penetrate the Stern layer, although the closest approach to the electrode is normally occupied by solvent molecules. This could occur if ions lose their solvation shell as they approach the electrode. He called ions in direct contact with the electrode \"specifically adsorbed ions\". This model proposed the existence of three regions. The inner Helmholtz plane (IHP) passes through the centres of the specifically adsorbed ions. The outer Helmholtz plane (OHP) passes through the centres of solvated ions at the distance of their closest approach to the electrode. Finally the diffuse layer is the region beyond the OHP.\n\nIn 1963 J. O'M. Bockris, M. A. V. Devanathan and Klaus Müller proposed the BDM model of the double-layer that included the action of the solvent in the interface. They suggested that the attached molecules of the solvent, such as water, would have a fixed alignment to the electrode surface. This first layer of solvent molecules displays a strong orientation to the electric field depending on the charge. This orientation has great influence on the permittivity of the solvent that varies with field strength. The IHP passes through the centers of these molecules. Specifically adsorbed, partially solvated ions appear in this layer. The solvated ions of the electrolyte are outside the IHP. Through the centers of these ions pass the OHP. The diffuse layer is the region beyond the OHP.\n\nFurther research with double layers on ruthenium dioxide films in 1971 by Sergio Trasatti and Giovanni Buzzanca demonstrated that the electrochemical behavior of these electrodes at low voltages with specific adsorbed ions was like that of capacitors. The specific adsorption of the ions in this region of potential could also involve a partial charge transfer between the ion and the electrode. It was the first step towards understanding pseudocapacitance.\n\nBetween 1975 and 1980 Brian Evans Conway conducted extensive fundamental and development work on ruthenium oxide electrochemical capacitors. In 1991 he described the difference between 'Supercapacitor' and 'Battery' behavior in electrochemical energy storage. In 1999 he coined the term supercapacitor to explain the increased capacitance by surface redox reactions with faradaic charge transfer between electrodes and ions.\n\nHis \"supercapacitor\" stored electrical charge partially in the Helmholtz double-layer and partially as the result of faradaic reactions with \"pseudocapacitance\" charge transfer of electrons and protons between electrode and electrolyte. The working mechanisms of pseudocapacitors are redox reactions, intercalation and electrosorption.\n\nThe physical and mathematical basics of electron charge transfer absent chemical bonds leading to pseudocapacitance was developed by Rudolph A. Marcus. Marcus Theory explains the rates of electron transfer reactions—the rate at which an electron can move from one chemical species to another. It was originally formulated to address outer sphere electron transfer reactions, in which two chemical species change only in their charge, with an electron jumping. For redox reactions without making or breaking bonds, Marcus theory takes the place of Henry Eyring's transition state theory which was derived for reactions with structural changes. Marcus received the Nobel Prize in Chemistry in 1992 for this theory.\n\nThere are detailed descriptions of the interfacial DL in many books on colloid and interface science and microscale fluid transport. There is also a recent IUPAC technical report on the subject of interfacial double layer and related electrokinetic phenomena.\n\nAs stated by Lyklema, \"...the reason for the formation of a \"relaxed\" (\"equilibrium\") double layer is the non-electric affinity of charge-determining ions for a surface...\" This process leads to the buildup of an electric surface charge, expressed usually in C/m. This surface charge creates an electrostatic field that then affects the ions in the bulk of the liquid. This electrostatic field, in combination with the thermal motion of the ions, creates a counter charge, and thus screens the electric surface charge. The net electric charge in this screening diffuse layer is equal in magnitude to the net surface charge, but has the opposite polarity. As a result, the complete structure is electrically neutral.\n\nThe diffuse layer, or at least part of it, can move under the influence of tangential stress. There is a conventionally introduced slipping plane that separates mobile fluid from fluid that remains attached to the surface. Electric potential at this plane is called electrokinetic potential or zeta potential (also denoted as ζ-potential).\n\nThe electric potential on the external boundary of the Stern layer versus the bulk electrolyte is referred to as Stern potential. Electric potential difference between the fluid bulk and the surface is called the electric surface potential.\n\nUsually zeta potential is used for estimating the degree of DL charge. A characteristic value of this electric potential in the DL is 25 mV with a maximum value around 100 mV (up to several volts on electrodes). The chemical composition of the sample at which the ζ-potential is 0 is called the point of zero charge or the iso-electric point. It is usually determined by the solution pH value, since protons and hydroxyl ions are the charge-determining ions for most surfaces.\n\nZeta potential can be measured using electrophoresis, electroacoustic phenomena, streaming potential, and electroosmotic flow.\n\nThe characteristic thickness of the DL is the Debye length, κ. It is reciprocally proportional to the square root of the ion concentration \"C\". In aqueous solutions it is typically on the scale of a few nanometers and the thickness decreases with increasing concentration of the electrolyte.\n\nThe electric field strength inside the DL can be anywhere from zero to over 10 V/m. These steep electric potential gradients are the reason for the importance of the DLs.\n\nThe theory for a flat surface and a symmetrical electrolyte is usually referred to as the Gouy-Chapman theory. It yields a simple relationship between electric charge in the diffuse layer σ and the Stern potential Ψ:\n"}
{"id": "9228", "url": "https://en.wikipedia.org/wiki?curid=9228", "title": "Earth", "text": "Earth\n\nEarth is the third planet from the Sun and the only astronomical object known to harbor life. According to radiometric dating and other sources of evidence, Earth formed over 4.5 billion years ago. Earth's gravity interacts with other objects in space, especially the Sun and the Moon, Earth's only natural satellite. Earth revolves around the Sun in 365.26 days, a period known as an Earth year. During this time, Earth rotates about its axis about 366.26 times.\n\nEarth's axis of rotation is tilted with respect to its orbital plane, producing seasons on Earth. The gravitational interaction between Earth and the Moon causes ocean tides, stabilizes Earth's orientation on its axis, and gradually slows its rotation. Earth is the densest planet in the Solar System and the largest of the four terrestrial planets.\n\nEarth's lithosphere is divided into several rigid tectonic plates that migrate across the surface over periods of many millions of years. About 71% of Earth's surface is covered with water, mostly by oceans. The remaining 29% is land consisting of continents and islands that together have many lakes, rivers and other sources of water that contribute to the hydrosphere. The majority of Earth's polar regions are covered in ice, including the Antarctic ice sheet and the sea ice of the Arctic ice pack. Earth's interior remains active with a solid iron inner core, a liquid outer core that generates the Earth's magnetic field, and a convecting mantle that drives plate tectonics.\n\nWithin the first billion years of Earth's history, life appeared in the oceans and began to affect the Earth's atmosphere and surface, leading to the proliferation of aerobic and anaerobic organisms. Some geological evidence indicates that life may have arisen as much as 4.1 billion years ago. Since then, the combination of Earth's distance from the Sun, physical properties, and geological history have allowed life to evolve and thrive. In the history of the Earth, biodiversity has gone through long periods of expansion, occasionally punctuated by mass extinction events. Over 99% of all species that ever lived on Earth are extinct. Estimates of the number of species on Earth today vary widely; most species have not been described. Over 7.6 billion humans live on Earth and depend on its biosphere and natural resources for their survival. Humans have developed diverse societies and cultures; politically, the world has about 200 sovereign states.\n\nThe modern English word \"Earth\" developed from a wide variety of Middle English forms, which derived from an Old English noun most often spelled '. It has cognates in every Germanic language, and their proto-Germanic root has been reconstructed as *\"erþō\". In its earliest appearances, \"eorðe\" was already being used to translate the many senses of Latin ' and Greek (\"gē\"): the ground, its soil, dry land, the human world, the surface of the world (including the sea), and the globe itself. As with Terra and Gaia, Earth was a personified goddess in Germanic paganism: the Angles were listed by Tacitus as among the devotees of Nerthus, and later Norse mythology included Jörð, a giantess often given as the mother of Thor.\n\nOriginally, \"earth\" was written in lowercase, and from early Middle English, its definite sense as \"the globe\" was expressed as \"the earth\". By Early Modern English, many nouns were capitalized, and \"the earth\" became (and often remained) \"the Earth\", particularly when referenced along with other heavenly bodies. More recently, the name is sometimes simply given as \"Earth\", by analogy with the names of the other planets. House styles now vary: Oxford spelling recognizes the lowercase form as the most common, with the capitalized form an acceptable variant. Another convention capitalizes \"Earth\" when appearing as a name (e.g. \"Earth's atmosphere\") but writes it in lowercase when preceded by \"the\" (e.g. \"the atmosphere of the earth\"). It almost always appears in lowercase in colloquial expressions such as \"what on earth are you doing?\"\n\nThe oldest material found in the Solar System is dated to (Bya). By the primordial Earth had formed. The bodies in the Solar System formed and evolved with the Sun. In theory, a solar nebula partitions a volume out of a molecular cloud by gravitational collapse, which begins to spin and flatten into a circumstellar disk, and then the planets grow out of that disk with the Sun. A nebula contains gas, ice grains, and dust (including primordial nuclides). According to nebular theory, planetesimals formed by accretion, with the primordial Earth taking 10– (Mys) to form.\n\nA subject of research is the formation of the Moon, some 4.53 Bya. A leading hypothesis is that it was formed by accretion from material loosed from Earth after a Mars-sized object, named Theia, hit Earth. In this view, the mass of Theia was approximately 10 percent of Earth, it hit Earth with a glancing blow and some of its mass merged with Earth. Between approximately 4.1 and , numerous asteroid impacts during the Late Heavy Bombardment caused significant changes to the greater surface environment of the Moon and, by inference, to that of Earth.\n\nEarth's atmosphere and oceans were formed by volcanic activity and outgassing. Water vapor from these sources condensed into the oceans, augmented by water and ice from asteroids, protoplanets, and comets. In this model, atmospheric \"greenhouse gases\" kept the oceans from freezing when the newly forming Sun had only 70% of its current luminosity. By , Earth's magnetic field was established, which helped prevent the atmosphere from being stripped away by the solar wind.\n\nA crust formed when the molten outer layer of Earth cooled to form a solid. The two models that explain land mass propose either a steady growth to the present-day forms or, more likely, a rapid growth early in Earth history followed by a long-term steady continental area. Continents formed by plate tectonics, a process ultimately driven by the continuous loss of heat from Earth's interior. Over the period of hundreds of millions of years, the supercontinents have assembled and broken apart. Roughly (Mya), one of the earliest known supercontinents, Rodinia, began to break apart. The continents later recombined to form Pannotia , then finally Pangaea, which also broke apart .\n\nThe present pattern of ice ages began about and then intensified during the Pleistocene about . High-latitude regions have since undergone repeated cycles of glaciation and thaw, repeating about every . The last continental glaciation ended ago.\n\nChemical reactions led to the first self-replicating molecules about four billion years ago. A half billion years later, the last common ancestor of all current life arose. The evolution of photosynthesis allowed the Sun's energy to be harvested directly by life forms. The resultant molecular oxygen () accumulated in the atmosphere and due to interaction with ultraviolet solar radiation, formed a protective ozone layer () in the upper atmosphere. The incorporation of smaller cells within larger ones resulted in the development of complex cells called eukaryotes. True multicellular organisms formed as cells within colonies became increasingly specialized. Aided by the absorption of harmful ultraviolet radiation by the ozone layer, life colonized Earth's surface. Among the earliest fossil evidence for life is microbial mat fossils found in 3.48 billion-year-old sandstone in Western Australia, biogenic graphite found in 3.7 billion-year-old metasedimentary rocks in Western Greenland, and remains of biotic material found in 4.1 billion-year-old rocks in Western Australia. The earliest direct evidence of life on Earth is contained in 3.45 billion-year-old Australian rocks showing fossils of microorganisms.\n\nDuring the Neoproterozoic, , much of Earth might have been covered in ice. This hypothesis has been termed \"Snowball Earth\", and it is of particular interest because it preceded the Cambrian explosion, when multicellular life forms significantly increased in complexity. Following the Cambrian explosion, , there have been five mass extinctions. The most recent such event was , when an asteroid impact triggered the extinction of the non-avian dinosaurs and other large reptiles, but spared some small animals such as mammals, which at the time resembled shrews. Mammalian life has diversified over the past , and several million years ago an African ape-like animal such as \"Orrorin tugenensis\" gained the ability to stand upright. This facilitated tool use and encouraged communication that provided the nutrition and stimulation needed for a larger brain, which led to the evolution of humans. The development of agriculture, and then civilization, led to humans having an influence on Earth and the nature and quantity of other life forms that continues to this day.\n\nEarth's expected long-term future is tied to that of the Sun. Over the next , solar luminosity will increase by 10%, and over the next by 40%. The Earth's increasing surface temperature will accelerate the inorganic carbon cycle, reducing concentration to levels lethally low for plants ( for C4 photosynthesis) in approximately . The lack of vegetation will result in the loss of oxygen in the atmosphere, making animal life impossible. After another billion years all surface water will have disappeared and the mean global temperature will reach . From that point, the Earth is expected to be habitable for another , possibly up to if nitrogen is removed from the atmosphere. Even if the Sun were eternal and stable, 27% of the water in the modern oceans will descend to the mantle in one billion years, due to reduced steam venting from mid-ocean ridges.\n\nThe Sun will evolve to become a red giant in about . Models predict that the Sun will expand to roughly , about 250 times its present radius. Earth's fate is less clear. As a red giant, the Sun will lose roughly 30% of its mass, so, without tidal effects, Earth will move to an orbit from the Sun when the star reaches its maximum radius. Most, if not all, remaining life will be destroyed by the Sun's increased luminosity (peaking at about 5,000 times its present level). A 2008 simulation indicates that Earth's orbit will eventually decay due to tidal effects and drag, causing it to enter the Sun's atmosphere and be vaporized.\n\nThe shape of Earth is approximately oblate spheroidal. Due to rotation, the Earth is flattened at the poles and bulging around the equator. The diameter of the Earth at the equator is larger than the pole-to-pole diameter. Thus the point on the surface farthest from Earth's center of mass is the summit of the equatorial Chimborazo volcano in Ecuador. The average diameter of the reference spheroid is . Local topography deviates from this idealized spheroid, although on a global scale these deviations are small compared to Earth's radius: The maximum deviation of only 0.17% is at the Mariana Trench ( below local sea level), whereas Mount Everest ( above local sea level) represents a deviation of 0.14%.\n\nIn geodesy, the exact shape that Earth's oceans would adopt in the absence of land and perturbations such as tides and winds is called the geoid. More precisely, the geoid is the surface of gravitational equipotential at mean sea level.\n\nEarth's mass is approximately (5,970 Yg). It is composed mostly of iron (32.1%), oxygen (30.1%), silicon (15.1%), magnesium (13.9%), sulfur (2.9%), nickel (1.8%), calcium (1.5%), and aluminium (1.4%), with the remaining 1.2% consisting of trace amounts of other elements. Due to mass segregation, the core region is estimated to be primarily composed of iron (88.8%), with smaller amounts of nickel (5.8%), sulfur (4.5%), and less than 1% trace elements.\n\nThe most common rock constituents of the crust are nearly all oxides: chlorine, sulfur, and fluorine are the important exceptions to this and their total amount in any rock is usually much less than 1%. Over 99% of the crust is composed of 11 oxides, principally silica, alumina, iron oxides, lime, magnesia, potash, and soda.\n\nEarth's interior, like that of the other terrestrial planets, is divided into layers by their chemical or physical (rheological) properties. The outer layer is a chemically distinct silicate solid crust, which is underlain by a highly viscous solid mantle. The crust is separated from the mantle by the Mohorovičić discontinuity. The thickness of the crust varies from about under the oceans to for the continents. The crust and the cold, rigid, top of the upper mantle are collectively known as the lithosphere, and it is of the lithosphere that the tectonic plates are composed. Beneath the lithosphere is the asthenosphere, a relatively low-viscosity layer on which the lithosphere rides. Important changes in crystal structure within the mantle occur at below the surface, spanning a transition zone that separates the upper and lower mantle. Beneath the mantle, an extremely low viscosity liquid outer core lies above a solid inner core. The Earth's inner core might rotate at a slightly higher angular velocity than the remainder of the planet, advancing by 0.1–0.5° per year. The radius of the inner core is about one fifth of that of Earth.\n\nEarth's internal heat comes from a combination of residual heat from planetary accretion (about 20%) and heat produced through radioactive decay (80%). The major heat-producing isotopes within Earth are potassium-40, uranium-238, and thorium-232. At the center, the temperature may be up to , and the pressure could reach . Because much of the heat is provided by radioactive decay, scientists postulate that early in Earth's history, before isotopes with short half-lives were depleted, Earth's heat production was much higher. At approximately , twice the present-day heat would have been produced, increasing the rates of mantle convection and plate tectonics, and allowing the production of uncommon igneous rocks such as komatiites that are rarely formed today.\n\nThe mean heat loss from Earth is , for a global heat loss of . A portion of the core's thermal energy is transported toward the crust by mantle plumes, a form of convection consisting of upwellings of higher-temperature rock. These plumes can produce hotspots and flood basalts. More of the heat in Earth is lost through plate tectonics, by mantle upwelling associated with mid-ocean ridges. The final major mode of heat loss is through conduction through the lithosphere, the majority of which occurs under the oceans because the crust there is much thinner than that of the continents.\n\nEarth's mechanically rigid outer layer, the lithosphere, is divided into tectonic plates. These plates are rigid segments that move relative to each other at one of three boundaries types: At convergent boundaries, two plates come together; at divergent boundaries, two plates are pulled apart; and at transform boundaries, two plates slide past one another laterally. Along these plate boundaries, earthquakes, volcanic activity, mountain-building, and oceanic trench formation can occur. The tectonic plates ride on top of the asthenosphere, the solid but less-viscous part of the upper mantle that can flow and move along with the plates.\n\nAs the tectonic plates migrate, oceanic crust is subducted under the leading edges of the plates at convergent boundaries. At the same time, the upwelling of mantle material at divergent boundaries creates mid-ocean ridges. The combination of these processes recycles the oceanic crust back into the mantle. Due to this recycling, most of the ocean floor is less than old. The oldest oceanic crust is located in the Western Pacific and is estimated to be old. By comparison, the oldest dated continental crust is .\n\nThe seven major plates are the Pacific, North American, Eurasian, African, Antarctic, Indo-Australian, and South American. Other notable plates include the Arabian Plate, the Caribbean Plate, the Nazca Plate off the west coast of South America and the Scotia Plate in the southern Atlantic Ocean. The Australian Plate fused with the Indian Plate between . The fastest-moving plates are the oceanic plates, with the Cocos Plate advancing at a rate of and the Pacific Plate moving . At the other extreme, the slowest-moving plate is the Eurasian Plate, progressing at a typical rate of .\n\nThe total surface area of Earth is about . Of this, 70.8%, or , is below sea level and covered by ocean water. Below the ocean's surface are much of the continental shelf, mountains, volcanoes, oceanic trenches, submarine canyons, oceanic plateaus, abyssal plains, and a globe-spanning mid-ocean ridge system. The remaining 29.2%, or , not covered by water has terrain that varies greatly from place to place and consists of mountains, deserts, plains, plateaus, and other landforms. Tectonics and erosion, volcanic eruptions, flooding, weathering, glaciation, the growth of coral reefs, and meteorite impacts are among the processes that constantly reshape the Earth's surface over geological time.\n\nThe continental crust consists of lower density material such as the igneous rocks granite and andesite. Less common is basalt, a denser volcanic rock that is the primary constituent of the ocean floors. Sedimentary rock is formed from the accumulation of sediment that becomes buried and compacted together. Nearly 75% of the continental surfaces are covered by sedimentary rocks, although they form about 5% of the crust. The third form of rock material found on Earth is metamorphic rock, which is created from the transformation of pre-existing rock types through high pressures, high temperatures, or both. The most abundant silicate minerals on Earth's surface include quartz, feldspars, amphibole, mica, pyroxene and olivine. Common carbonate minerals include calcite (found in limestone) and dolomite.\n\nThe elevation of the land surface varies from the low point of at the Dead Sea, to a maximum altitude of at the top of Mount Everest. The mean height of land above sea level is about .\n\nThe pedosphere is the outermost layer of Earth's continental surface and is composed of soil and subject to soil formation processes. The total arable land is 10.9% of the land surface, with 1.3% being permanent cropland. Close to 40% of Earth's land surface is used for agriculture, or an estimated of cropland and of pastureland.\n\nThe abundance of water on Earth's surface is a unique feature that distinguishes the \"Blue Planet\" from other planets in the Solar System. Earth's hydrosphere consists chiefly of the oceans, but technically includes all water surfaces in the world, including inland seas, lakes, rivers, and underground waters down to a depth of . The deepest underwater location is Challenger Deep of the Mariana Trench in the Pacific Ocean with a depth of .\n\nThe mass of the oceans is approximately 1.35 metric tons or about 1/4400 of Earth's total mass. The oceans cover an area of with a mean depth of , resulting in an estimated volume of . If all of Earth's crustal surface were at the same elevation as a smooth sphere, the depth of the resulting world ocean would be .\n\nAbout 97.5% of the water is saline; the remaining 2.5% is fresh water. Most fresh water, about 68.7%, is present as ice in ice caps and glaciers.\n\nThe average salinity of Earth's oceans is about 35 grams of salt per kilogram of sea water (3.5% salt). Most of this salt was released from volcanic activity or extracted from cool igneous rocks. The oceans are also a reservoir of dissolved atmospheric gases, which are essential for the survival of many aquatic life forms. Sea water has an important influence on the world's climate, with the oceans acting as a large heat reservoir. Shifts in the oceanic temperature distribution can cause significant weather shifts, such as the El Niño–Southern Oscillation.\n\nThe atmospheric pressure at Earth's sea level averages , with a scale height of about . A dry atmosphere is composed of 78.084% nitrogen, 20.946% oxygen, 0.934% argon, and trace amounts of carbon dioxide and other gaseous molecules. Water vapor content varies between 0.01% and 4% but averages about 1%. The height of the troposphere varies with latitude, ranging between at the poles to at the equator, with some variation resulting from weather and seasonal factors.\n\nEarth's biosphere has significantly altered its atmosphere. Oxygenic photosynthesis evolved , forming the primarily nitrogen–oxygen atmosphere of today. This change enabled the proliferation of aerobic organisms and, indirectly, the formation of the ozone layer due to the subsequent conversion of atmospheric into. The ozone layer blocks ultraviolet solar radiation, permitting life on land. Other atmospheric functions important to life include transporting water vapor, providing useful gases, causing small meteors to burn up before they strike the surface, and moderating temperature. This last phenomenon is known as the greenhouse effect: trace molecules within the atmosphere serve to capture thermal energy emitted from the ground, thereby raising the average temperature. Water vapor, carbon dioxide, methane, nitrous oxide, and ozone are the primary greenhouse gases in the atmosphere. Without this heat-retention effect, the average surface temperature would be , in contrast to the current , and life on Earth probably would not exist in its current form. In May 2017, glints of light, seen as twinkling from an orbiting satellite a million miles away, were found to be reflected light from ice crystals in the atmosphere.\n\nEarth's atmosphere has no definite boundary, slowly becoming thinner and fading into outer space. Three-quarters of the atmosphere's mass is contained within the first of the surface. This lowest layer is called the troposphere. Energy from the Sun heats this layer, and the surface below, causing expansion of the air. This lower-density air then rises and is replaced by cooler, higher-density air. The result is atmospheric circulation that drives the weather and climate through redistribution of thermal energy.\n\nThe primary atmospheric circulation bands consist of the trade winds in the equatorial region below 30° latitude and the westerlies in the mid-latitudes between 30° and 60°. Ocean currents are also important factors in determining climate, particularly the thermohaline circulation that distributes thermal energy from the equatorial oceans to the polar regions.\n\nWater vapor generated through surface evaporation is transported by circulatory patterns in the atmosphere. When atmospheric conditions permit an uplift of warm, humid air, this water condenses and falls to the surface as precipitation. Most of the water is then transported to lower elevations by river systems and usually returned to the oceans or deposited into lakes. This water cycle is a vital mechanism for supporting life on land and is a primary factor in the erosion of surface features over geological periods. Precipitation patterns vary widely, ranging from several meters of water per year to less than a millimeter. Atmospheric circulation, topographic features, and temperature differences determine the average precipitation that falls in each region.\n\nThe amount of solar energy reaching Earth's surface decreases with increasing latitude. At higher latitudes, the sunlight reaches the surface at lower angles, and it must pass through thicker columns of the atmosphere. As a result, the mean annual air temperature at sea level decreases by about per degree of latitude from the equator. Earth's surface can be subdivided into specific latitudinal belts of approximately homogeneous climate. Ranging from the equator to the polar regions, these are the tropical (or equatorial), subtropical, temperate and polar climates.\n\nThis latitudinal rule has several anomalies:\n\nThe commonly used Köppen climate classification system has five broad groups (humid tropics, arid, humid middle latitudes, continental and cold polar), which are further divided into more specific subtypes. The Köppen system rates regions of terrain based on observed temperature and precipitation.\n\nThe highest air temperature ever measured on Earth was in Furnace Creek, California, in Death Valley, in 1913. The lowest air temperature ever directly measured on Earth was at Vostok Station in 1983, but satellites have used remote sensing to measure temperatures as low as in East Antarctica. These temperature records are only measurements made with modern instruments from the 20th century onwards and likely do not reflect the full range of temperature on Earth.\n\nAbove the troposphere, the atmosphere is usually divided into the stratosphere, mesosphere, and thermosphere. Each layer has a different lapse rate, defining the rate of change in temperature with height. Beyond these, the exosphere thins out into the magnetosphere, where the geomagnetic fields interact with the solar wind. Within the stratosphere is the ozone layer, a component that partially shields the surface from ultraviolet light and thus is important for life on Earth. The Kármán line, defined as 100 km above Earth's surface, is a working definition for the boundary between the atmosphere and outer space.\n\nThermal energy causes some of the molecules at the outer edge of the atmosphere to increase their velocity to the point where they can escape from Earth's gravity. This causes a slow but steady loss of the atmosphere into space. Because unfixed hydrogen has a low molecular mass, it can achieve escape velocity more readily, and it leaks into outer space at a greater rate than other gases. The leakage of hydrogen into space contributes to the shifting of Earth's atmosphere and surface from an initially reducing state to its current oxidizing one. Photosynthesis provided a source of free oxygen, but the loss of reducing agents such as hydrogen is thought to have been a necessary precondition for the widespread accumulation of oxygen in the atmosphere. Hence the ability of hydrogen to escape from the atmosphere may have influenced the nature of life that developed on Earth. In the current, oxygen-rich atmosphere most hydrogen is converted into water before it has an opportunity to escape. Instead, most of the hydrogen loss comes from the destruction of methane in the upper atmosphere.\n\nThe gravity of Earth is the acceleration that is imparted to objects due to the distribution of mass within the Earth. Near the Earth's surface, gravitational acceleration is approximately . Local differences in topography, geology, and deeper tectonic structure cause local and broad, regional differences in the Earth's gravitational field, known as gravity anomalies.\n\nThe main part of Earth's magnetic field is generated in the core, the site of a dynamo process that converts the kinetic energy of thermally and compositionally driven convection into electrical and magnetic field energy. The field extends outwards from the core, through the mantle, and up to Earth's surface, where it is, approximately, a dipole. The poles of the dipole are located close to Earth's geographic poles. At the equator of the magnetic field, the magnetic-field strength at the surface is , with global magnetic dipole moment of . The convection movements in the core are chaotic; the magnetic poles drift and periodically change alignment. This causes secular variation of the main field and field reversals at irregular intervals averaging a few times every million years. The most recent reversal occurred approximately 700,000 years ago.\n\nThe extent of Earth's magnetic field in space defines the magnetosphere. Ions and electrons of the solar wind are deflected by the magnetosphere; solar wind pressure compresses the dayside of the magnetosphere, to about 10 Earth radii, and extends the nightside magnetosphere into a long tail. Because the velocity of the solar wind is greater than the speed at which waves propagate through the solar wind, a supersonic bowshock precedes the dayside magnetosphere within the solar wind. Charged particles are contained within the magnetosphere; the plasmasphere is defined by low-energy particles that essentially follow magnetic field lines as Earth rotates; the ring current is defined by medium-energy particles that drift relative to the geomagnetic field, but with paths that are still dominated by the magnetic field, and the Van Allen radiation belt are formed by high-energy particles whose motion is essentially random, but otherwise contained by the magnetosphere.\n\nDuring magnetic storms and substorms, charged particles can be deflected from the outer magnetosphere and especially the magnetotail, directed along field lines into Earth's ionosphere, where atmospheric atoms can be excited and ionized, causing the aurora.\n\nEarth's rotation period relative to the Sun—its mean solar day—is of mean solar time (). Because Earth's solar day is now slightly longer than it was during the 19th century due to tidal deceleration, each day varies between longer.\n\nEarth's rotation period relative to the fixed stars, called its \"stellar day\" by the International Earth Rotation and Reference Systems Service (IERS), is of mean solar time (UT1), or Earth's rotation period relative to the precessing or moving mean vernal equinox, misnamed its \"sidereal day\", is of mean solar time (UT1) . Thus the sidereal day is shorter than the stellar day by about 8.4 ms. The length of the mean solar day in SI seconds is available from the IERS for the periods 1623–2005 and 1962–2005.\n\nApart from meteors within the atmosphere and low-orbiting satellites, the main apparent motion of celestial bodies in Earth's sky is to the west at a rate of 15°/h = 15'/min. For bodies near the celestial equator, this is equivalent to an apparent diameter of the Sun or the Moon every two minutes; from Earth's surface, the apparent sizes of the Sun and the Moon are approximately the same.\n\nEarth orbits the Sun at an average distance of about every 365.2564 mean solar days, or one sidereal year. This gives an apparent movement of the Sun eastward with respect to the stars at a rate of about 1°/day, which is one apparent Sun or Moon diameter every 12 hours. Due to this motion, on average it takes 24 hours—a solar day—for Earth to complete a full rotation about its axis so that the Sun returns to the meridian. The orbital speed of Earth averages about , which is fast enough to travel a distance equal to Earth's diameter, about , in seven minutes, and the distance to the Moon, , in about 3.5 hours.\n\nThe Moon and Earth orbit a common barycenter every 27.32 days relative to the background stars. When combined with the Earth–Moon system's common orbit around the Sun, the period of the synodic month, from new moon to new moon, is 29.53 days. Viewed from the celestial north pole, the motion of Earth, the Moon, and their axial rotations are all counterclockwise. Viewed from a vantage point above the north poles of both the Sun and Earth, Earth orbits in a counterclockwise direction about the Sun. The orbital and axial planes are not precisely aligned: Earth's axis is tilted some 23.44 degrees from the perpendicular to the Earth–Sun plane (the ecliptic), and the Earth–Moon plane is tilted up to ±5.1 degrees against the Earth–Sun plane. Without this tilt, there would be an eclipse every two weeks, alternating between lunar eclipses and solar eclipses.\n\nThe Hill sphere, or the sphere of gravitational influence, of the Earth is about in radius. This is the maximum distance at which the Earth's gravitational influence is stronger than the more distant Sun and planets. Objects must orbit the Earth within this radius, or they can become unbound by the gravitational perturbation of the Sun.\n\nEarth, along with the Solar System, is situated in the Milky Way and orbits about 28,000 light-years from its center. It is about 20 light-years above the galactic plane in the Orion Arm.\n\nThe axial tilt of the Earth is approximately 23.439281° with the axis of its orbit plane, always pointing towards the Celestial Poles. Due to Earth's axial tilt, the amount of sunlight reaching any given point on the surface varies over the course of the year. This causes the seasonal change in climate, with summer in the Northern Hemisphere occurring when the Tropic of Cancer is facing the Sun, and winter taking place when the Tropic of Capricorn in the Southern Hemisphere faces the Sun. During the summer, the day lasts longer, and the Sun climbs higher in the sky. In winter, the climate becomes cooler and the days shorter. In northern temperate latitudes, the Sun rises north of true east during the summer solstice, and sets north of true west, reversing in the winter. The Sun rises south of true east in the summer for the southern temperate zone and sets south of true west.\n\nAbove the Arctic Circle, an extreme case is reached where there is no daylight at all for part of the year, up to six months at the North Pole itself, a polar night. In the Southern Hemisphere, the situation is exactly reversed, with the South Pole oriented opposite the direction of the North Pole. Six months later, this pole will experience a midnight sun, a day of 24 hours, again reversing with the South Pole.\n\nBy astronomical convention, the four seasons can be determined by the solstices—the points in the orbit of maximum axial tilt toward or away from the Sun—and the equinoxes, when the direction of the tilt and the direction to the Sun are perpendicular. In the Northern Hemisphere, winter solstice currently occurs around 21 December; summer solstice is near 21 June, spring equinox is around 20 March and autumnal equinox is about 22 or 23 September. In the Southern Hemisphere, the situation is reversed, with the summer and winter solstices exchanged and the spring and autumnal equinox dates swapped.\n\nThe angle of Earth's axial tilt is relatively stable over long periods of time. Its axial tilt does undergo nutation; a slight, irregular motion with a main period of 18.6 years. The orientation (rather than the angle) of Earth's axis also changes over time, precessing around in a complete circle over each 25,800 year cycle; this precession is the reason for the difference between a sidereal year and a tropical year. Both of these motions are caused by the varying attraction of the Sun and the Moon on Earth's equatorial bulge. The poles also migrate a few meters across Earth's surface. This polar motion has multiple, cyclical components, which collectively are termed quasiperiodic motion. In addition to an annual component to this motion, there is a 14-month cycle called the Chandler wobble. Earth's rotational velocity also varies in a phenomenon known as length-of-day variation.\n\nIn modern times, Earth's perihelion occurs around 3 January, and its aphelion around 4 July. These dates change over time due to precession and other orbital factors, which follow cyclical patterns known as Milankovitch cycles. The changing Earth–Sun distance causes an increase of about 6.9% in solar energy reaching Earth at perihelion relative to aphelion. Because the Southern Hemisphere is tilted toward the Sun at about the same time that Earth reaches the closest approach to the Sun, the Southern Hemisphere receives slightly more energy from the Sun than does the northern over the course of a year. This effect is much less significant than the total energy change due to the axial tilt, and most of the excess energy is absorbed by the higher proportion of water in the Southern Hemisphere.\n\nA study from 2016 suggested that Planet Nine tilted all Solar System planets, including Earth's, by about six degrees.\n\nA planet that can sustain life is termed habitable, even if life did not originate there. Earth provides liquid water—an environment where complex organic molecules can assemble and interact, and sufficient energy to sustain metabolism. The distance of Earth from the Sun, as well as its orbital eccentricity, rate of rotation, axial tilt, geological history, sustaining atmosphere, and magnetic field all contribute to the current climatic conditions at the surface.\n\nA planet's life forms inhabit ecosystems, whose total is sometimes said to form a \"biosphere\". Earth's biosphere is thought to have begun evolving about . The biosphere is divided into a number of biomes, inhabited by broadly similar plants and animals. On land, biomes are separated primarily by differences in latitude, height above sea level and humidity. Terrestrial biomes lying within the Arctic or Antarctic Circles, at high altitudes or in extremely arid areas are relatively barren of plant and animal life; species diversity reaches a peak in humid lowlands at equatorial latitudes.\n\nIn July 2016, scientists reported identifying a set of 355 genes from the last universal common ancestor (LUCA) of all organisms living on Earth.\n\nEarth has resources that have been exploited by humans. Those termed non-renewable resources, such as fossil fuels, only renew over geological timescales.\n\nLarge deposits of fossil fuels are obtained from Earth's crust, consisting of coal, petroleum, and natural gas. These deposits are used by humans both for energy production and as feedstock for chemical production. Mineral ore bodies have also been formed within the crust through a process of ore genesis, resulting from actions of magmatism, erosion, and plate tectonics. These bodies form concentrated sources for many metals and other useful elements.\n\nEarth's biosphere produces many useful biological products for humans, including food, wood, pharmaceuticals, oxygen, and the recycling of many organic wastes. The land-based ecosystem depends upon topsoil and fresh water, and the oceanic ecosystem depends upon dissolved nutrients washed down from the land. In 1980, of Earth's land surface consisted of forest and woodlands, was grasslands and pasture, and was cultivated as croplands. The estimated amount of irrigated land in 1993 was . Humans also live on the land by using building materials to construct shelters.\n\nLarge areas of Earth's surface are subject to extreme weather such as tropical cyclones, hurricanes, or typhoons that dominate life in those areas. From 1980 to 2000, these events caused an average of 11,800 human deaths per year. Many places are subject to earthquakes, landslides, tsunamis, volcanic eruptions, tornadoes, sinkholes, blizzards, floods, droughts, wildfires, and other calamities and disasters.\n\nMany localized areas are subject to human-made pollution of the air and water, acid rain and toxic substances, loss of vegetation (overgrazing, deforestation, desertification), loss of wildlife, species extinction, soil degradation, soil depletion and erosion.\n\nThere is a scientific consensus linking human activities to global warming due to industrial carbon dioxide emissions. This is predicted to produce changes such as the melting of glaciers and ice sheets, more extreme temperature ranges, significant changes in weather and a global rise in average sea levels.\n\nCartography, the study and practice of map-making, and geography, the study of the lands, features, inhabitants and phenomena on Earth, have historically been the disciplines devoted to depicting Earth. Surveying, the determination of locations and distances, and to a lesser extent navigation, the determination of position and direction, have developed alongside cartography and geography, providing and suitably quantifying the requisite information.\n\nEarth's human population reached approximately seven billion on 31 October 2011. Projections indicate that the world's human population will reach 9.2 billion in 2050. Most of the growth is expected to take place in developing nations. Human population density varies widely around the world, but a majority live in Asia. By 2020, 60% of the world's population is expected to be living in urban, rather than rural, areas.\n\nIt is estimated that one-eighth of Earth's surface is suitable for humans to live on – three-quarters of Earth's surface is covered by oceans, leaving one-quarter as land. Half of that land area is desert (14%), high mountains (27%), or other unsuitable terrains. The northernmost permanent settlement in the world is Alert, on Ellesmere Island in Nunavut, Canada. (82°28′N) The southernmost is the Amundsen–Scott South Pole Station, in Antarctica, almost exactly at the South Pole. (90°S)\nIndependent sovereign nations claim the planet's entire land surface, except for some parts of Antarctica, a few land parcels along the Danube river's western bank, and the unclaimed area of Bir Tawil between Egypt and Sudan. , there are 193 sovereign states that are member states of the United Nations, plus two observer states and 72 dependent territories and states with limited recognition. Earth has never had a sovereign government with authority over the entire globe, although some nation-states have striven for world domination and failed.\n\nThe United Nations is a worldwide intergovernmental organization that was created with the goal of intervening in the disputes between nations, thereby avoiding armed conflict. The U.N. serves primarily as a forum for international diplomacy and international law. When the consensus of the membership permits, it provides a mechanism for armed intervention.\n\nThe first human to orbit Earth was Yuri Gagarin on 12 April 1961. In total, about 487 people have visited outer space and reached orbit , and, of these, twelve have walked on the Moon. Normally, the only humans in space are those on the International Space Station. The station's crew, made up of six people, is usually replaced every six months. The farthest that humans have traveled from Earth is , achieved during the Apollo 13 mission in 1970.\n\nThe Moon is a relatively large, terrestrial, planet-like natural satellite, with a diameter about one-quarter of Earth's. It is the largest moon in the Solar System relative to the size of its planet, although Charon is larger relative to the dwarf planet Pluto. The natural satellites of other planets are also referred to as \"moons\", after Earth's.\n\nThe gravitational attraction between Earth and the Moon causes tides on Earth. The same effect on the Moon has led to its tidal locking: its rotation period is the same as the time it takes to orbit Earth. As a result, it always presents the same face to the planet. As the Moon orbits Earth, different parts of its face are illuminated by the Sun, leading to the lunar phases; the dark part of the face is separated from the light part by the solar terminator.\nDue to their tidal interaction, the Moon recedes from Earth at the rate of approximately . Over millions of years, these tiny modifications—and the lengthening of Earth's day by about 23 µs/yr—add up to significant changes. During the Devonian period, for example, (approximately ) there were 400 days in a year, with each day lasting 21.8 hours.\n\nThe Moon may have dramatically affected the development of life by moderating the planet's climate. Paleontological evidence and computer simulations show that Earth's axial tilt is stabilized by tidal interactions with the Moon. Some theorists think that without this stabilization against the torques applied by the Sun and planets to Earth's equatorial bulge, the rotational axis might be chaotically unstable, exhibiting chaotic changes over millions of years, as appears to be the case for Mars.\n\nViewed from Earth, the Moon is just far enough away to have almost the same apparent-sized disk as the Sun. The angular size (or solid angle) of these two bodies match because, although the Sun's diameter is about 400 times as large as the Moon's, it is also 400 times more distant. This allows total and annular solar eclipses to occur on Earth.\n\nThe most widely accepted theory of the Moon's origin, the giant-impact hypothesis, states that it formed from the collision of a Mars-size protoplanet called Theia with the early Earth. This hypothesis explains (among other things) the Moon's relative lack of iron and volatile elements and the fact that its composition is nearly identical to that of Earth's crust.\n\nEarth has at least five co-orbital asteroids, including 3753 Cruithne and . A trojan asteroid companion, , is librating around the leading Lagrange triangular point, L4, in the Earth's orbit around the Sun.\n\nThe tiny near-Earth asteroid makes close approaches to the Earth–Moon system roughly every twenty years. During these approaches, it can orbit Earth for brief periods of time.\n\n, there are 1,886 operational, human-made satellites orbiting Earth. There are also inoperative satellites, including Vanguard 1, the oldest satellite currently in orbit, and over 16,000 pieces of tracked space debris. Earth's largest artificial satellite is the International Space Station.\n\nThe standard astronomical symbol of Earth consists of a cross circumscribed by a circle, , representing the four corners of the world.\n\nHuman cultures have developed many views of the planet. Earth is sometimes personified as a deity. In many cultures it is a mother goddess that is also the primary fertility deity, and by the mid-20th century, the Gaia Principle compared Earth's environments and life as a single self-regulating organism leading to broad stabilization of the conditions of habitability. Creation myths in many religions involve the creation of Earth by a supernatural deity or deities.\n\nScientific investigation has resulted in several culturally transformative shifts in people's view of the planet. Initial belief in a flat Earth was gradually displaced in the Greek colonies of southern Italy during the late 6th century BC by the idea of spherical Earth, which was attributed to both the philosophers Pythagoras and Parmenides. By the end of the 5th century BC, the sphericity of Earth was universally accepted among Greek intellectuals. Earth was generally believed to be the center of the universe until the 16th century, when scientists first conclusively demonstrated that it was a moving object, comparable to the other planets in the Solar System. Due to the efforts of influential Christian scholars and clerics such as James Ussher, who sought to determine the age of Earth through analysis of genealogies in Scripture, Westerners before the 19th century generally believed Earth to be a few thousand years old at most. It was only during the 19th century that geologists realized Earth's age was at least many millions of years.\n\nLord Kelvin used thermodynamics to estimate the age of Earth to be between 20 million and 400 million years in 1864, sparking a vigorous debate on the subject; it was only when radioactivity and radioactive dating were discovered in the late 19th and early 20th centuries that a reliable mechanism for determining Earth's age was established, proving the planet to be billions of years old. The perception of Earth shifted again in the 20th century when humans first viewed it from orbit, and especially with photographs of Earth returned by the Apollo program.\n\n</math>, where \"m\" is the mass of Earth, \"a\" is an astronomical unit, and \"M\" is the mass of the Sun. So the radius in AU is about formula_1.</ref>\n\n"}
{"id": "13551670", "url": "https://en.wikipedia.org/wiki?curid=13551670", "title": "Electroacoustic phenomena", "text": "Electroacoustic phenomena\n\nElectroacoustic phenomena arise when ultrasound propagates through a fluid containing ions. The associated particle motion generates electric signals because ions have electric charge. This coupling between ultrasound and electric field is called electroacoustic phenomena. The fluid might be a simple Newtonian liquid, or complex heterogeneous dispersion, emulsion or even a porous body. There are several different electroacoustic effects depending on the nature of the fluid.\n\n\nHistorically, the IVI was the first known electroacoustic effect. It was predicted by Debye in 1933.\n\nThe streaming vibration current was experimentally observed in 1948 by Williams. A theoretical model was developed some 30 years later by Dukhin and others. This effect opens another possibility for characterizing the electric properties of the surfaces in porous bodies. A similar effect can be observed at a non-porous surface, when sound is bounced off at an oblique angle. The incident and reflected waves superimpose to cause oscillatory fluid motion in the plane of the interface, thereby generating an AC streaming current at the frequency of the sound waves.\n\nThe electrical double layer can be regarded as behaving like a parallel plate capacitor with a compressible dielectric filling. When sound waves induce a local pressure variation, the spacing of the plates varies at the frequency of the excitation, generating an AC displacement current normal to the interface. For practical reasons this is most readily observed at a conducting surface. It is therefore possible to use an electrode immersed in a conducting electrolyte as a microphone, or indeed as a loudspeaker when the effect is applied in reverse.\n\nColloid vibration potential measures the AC potential difference generated between two identical relaxed electrodes, placed in the dispersion, if the latter is subjected to an ultrasonic field. When a sound wave travels through a colloidal suspension of particles whose density differs from that of the surrounding medium, inertial forces induced by the vibration of the suspension give rise to a motion of the charged particles relative to the liquid, causing an alternating electromotive force. The manifestations of this electromotive force may be measured, depending on the relation between the impedance of the suspension and that of the measuring instrument, either as colloid vibration potential or as \"colloid vibration current\".\n\nColloid vibration potential and current was first reported by Hermans and then independently by Rutgers in 1938. It is widely used for characterizing the ζ-potential of various dispersions and emulsions. The effect, theory, experimental verification and multiple applications are discussed in the book by Dukhin and Goetz.\n\nElectric sonic amplitude was experimentally discovered by Cannon with co-authors in early 1980s. It is also widely used for characterizing ζ-potential in dispersions and emulsions. There is review of this effect theory, experimental verification and multiple applications published by Hunter.\n\nWith regard to the theory of CVI and ESA, there was an important observation made by O'Brien, who linked these measured parameters with dynamic electrophoretic mobility μ.\n\nwhere\n\nDynamic electrophoretic mobility is similar to electrophoretic mobility that appears in electrophoresis theory. They are identical at low frequencies and/or for sufficiently small particles.\n\nThere are several theories of the dynamic electrophoretic mobility. Their overview is given in the Ref.5. Two of them are the most important.\n\nThe first one corresponds to the Smoluchowski limit. It yields following simple expression for CVI for sufficiently small particles with negligible CVI frequency dependence:\n\nwhere:\n\nThis remarkably simple equation has same wide range of applicability as Smoluchowski equation for electrophoresis. It is independent on shape of the particles, their concentration.\n\nValidity of this equation is restricted with the following two requirements.\nFirst, it is valid only for a thin double layer, when the Debye length is much smaller than particle's radius a:\n\nSecondly, it neglects the contribution of the surface conductivity. This assumes a small Dukhin number:\n\nRestriction of the thin double layer limits applicability of this Smoluchowski type theory only to aqueous systems with sufficiently large particles and not very low ionic strength. This theory does not work well for nano-colloids, including proteins and polymers at low ionic strength. It is not valid for low- or non-polar fluids.\n\nThere is another theory that is applicable for the other extreme case of a thick double layer, when \n\nThis theory takes into consideration the double layer overlap that inevitably occurs for concentrated systems with thick double layer. This allows introduction of so-called \"quasi-homogeneous\" approach, when overlapped diffuse layers of particles cover the complete interparticle space. The theory becomes much simplified in this extreme case, as shown by Shilov and others. Their derivation predicts that surface charge density σ is a better parameter than ζ-potential for characterizing electroacoustic phenomena in such systems. An expression for CVI simplified for small particles follows:\n\n"}
{"id": "9649", "url": "https://en.wikipedia.org/wiki?curid=9649", "title": "Energy", "text": "Energy\n\nIn physics, energy is the quantitative property that must be transferred to an object in order to perform work on, or to heat, the object. Energy is a conserved quantity; the law of conservation of energy states that energy can be converted in form, but not created or destroyed. The SI unit of energy is the joule, which is the energy transferred to an object by the work of moving it a distance of 1 metre against a force of 1 newton.\n\nCommon forms of energy include the kinetic energy of a moving object, the potential energy stored by an object's position in a force field (gravitational, electric or magnetic), the elastic energy stored by stretching solid objects, the chemical energy released when a fuel burns, the radiant energy carried by light, and the thermal energy due to an object's temperature.\n\nMass and energy are closely related. Due to mass–energy equivalence, any object that has mass when stationary (called rest mass) also has an equivalent amount of energy whose form is called rest energy, and any additional energy (of any form) acquired by the object above that rest energy will increase the object's total mass just as it increases its total energy. For example, after heating an object, its increase in energy could be measured as a small increase in mass, with a sensitive enough scale.\n\nLiving organisms require available energy to stay alive, such as the energy humans get from food. Human civilization requires energy to function, which it gets from energy resources such as fossil fuels, nuclear fuel, or renewable energy. The processes of Earth's climate and ecosystem are driven by the radiant energy Earth receives from the sun and the geothermal energy contained within the earth.\n\nThe total energy of a system can be subdivided and classified into potential energy, kinetic energy, or combinations of the two in various ways. Kinetic energy is determined by the movement of an object – or the composite motion of the components of an object – and potential energy reflects the potential of an object to have motion, and generally is a function of the position of an object within a field or may stored in the field itself.\n\nWhile these two categories are sufficient to describe all forms of energy, it is often convenient to refer to particular combinations of potential and kinetic energy as its own form. For example, macroscopic mechanical energy is the sum of translational and rotational kinetic and potential energy in a system neglects the kinetic energy due to temperature, and nuclear energy which combines utilize potentials from the nuclear force and the weak force), among others.\n\nThe word \"energy\" derives from the , which possibly appears for the first time in the work of Aristotle in the 4th century BC. In contrast to the modern definition, energeia was a qualitative philosophical concept, broad enough to include ideas such as happiness and pleasure.\n\nIn the late 17th century, Gottfried Leibniz proposed the idea of the , or living force, which defined as the product of the mass of an object and its velocity squared; he believed that total \"vis viva\" was conserved. To account for slowing due to friction, Leibniz theorized that thermal energy consisted of the random motion of the constituent parts of matter, although it would be more than a century until this was generally accepted. The modern analog of this property, kinetic energy, differs from \"vis viva\" only by a factor of two.\n\nIn 1807, Thomas Young was possibly the first to use the term \"energy\" instead of \"vis viva\", in its modern sense. Gustave-Gaspard Coriolis described \"kinetic energy\" in 1829 in its modern sense, and in 1853, William Rankine coined the term \"potential energy\". The law of conservation of energy was also first postulated in the early 19th century, and applies to any isolated system. It was argued for some years whether heat was a physical substance, dubbed the caloric, or merely a physical quantity, such as momentum. In 1845 James Prescott Joule discovered the link between mechanical work and the generation of heat.\n\nThese developments led to the theory of conservation of energy, formalized largely by William Thomson (Lord Kelvin) as the field of thermodynamics. Thermodynamics aided the rapid development of explanations of chemical processes by Rudolf Clausius, Josiah Willard Gibbs, and Walther Nernst. It also led to a mathematical formulation of the concept of entropy by Clausius and to the introduction of laws of radiant energy by Jožef Stefan. According to Noether's theorem, the conservation of energy is a consequence of the fact that the laws of physics do not change over time. Thus, since 1918, theorists have understood that the law of conservation of energy is the direct mathematical consequence of the translational symmetry of the quantity conjugate to energy, namely time.\n\nIn 1843, James Prescott Joule independently discovered the mechanical equivalent in a series of experiments. The most famous of them used the \"Joule apparatus\": a descending weight, attached to a string, caused rotation of a paddle immersed in water, practically insulated from heat transfer. It showed that the gravitational potential energy lost by the weight in descending was equal to the internal energy gained by the water through friction with the paddle.\n\nIn the International System of Units (SI), the unit of energy is the joule, named after James Prescott Joule. It is a derived unit. It is equal to the energy expended (or work done) in applying a force of one newton through a distance of one metre. However energy is also expressed in many other units not part of the SI, such as ergs, calories, British Thermal Units, kilowatt-hours and kilocalories, which require a conversion factor when expressed in SI units.\n\nThe SI unit of energy rate (energy per unit time) is the watt, which is a joule per second. Thus, one joule is one watt-second, and 3600 joules equal one watt-hour. The CGS energy unit is the erg and the imperial and US customary unit is the foot pound. Other energy units such as the electronvolt, food calorie or thermodynamic kcal (based on the temperature change of water in a heating process), and BTU are used in specific areas of science and commerce.\n\nIn classical mechanics, energy is a conceptually and mathematically useful property, as it is a conserved quantity. Several formulations of mechanics have been developed using energy as a core concept.\n\nWork, a function of energy, is force times distance.\n\nThis says that the work (formula_2) is equal to the line integral of the force F along a path \"C\"; for details see the mechanical work article. Work and thus energy is frame dependent. For example, consider a ball being hit by a bat. In the center-of-mass reference frame, the bat does no work on the ball. But, in the reference frame of the person swinging the bat, considerable work is done on the ball.\n\nThe total energy of a system is sometimes called the Hamiltonian, after William Rowan Hamilton. The classical equations of motion can be written in terms of the Hamiltonian, even for highly complex or abstract systems. These classical equations have remarkably direct analogs in nonrelativistic quantum mechanics.\n\nAnother energy-related concept is called the Lagrangian, after Joseph-Louis Lagrange. This formalism is as fundamental as the Hamiltonian, and both can be used to derive the equations of motion or be derived from them. It was invented in the context of classical mechanics, but is generally useful in modern physics. The Lagrangian is defined as the kinetic energy \"minus\" the potential energy. Usually, the Lagrange formalism is mathematically more convenient than the Hamiltonian for non-conservative systems (such as systems with friction).\n\nNoether's theorem (1918) states that any differentiable symmetry of the action of a physical system has a corresponding conservation law. Noether's theorem has become a fundamental tool of modern theoretical physics and the calculus of variations. A generalisation of the seminal formulations on constants of motion in Lagrangian and Hamiltonian mechanics (1788 and 1833, respectively), it does not apply to systems that cannot be modeled with a Lagrangian; for example, dissipative systems with continuous symmetries need not have a corresponding conservation law.\n\nIn the context of chemistry, energy is an attribute of a substance as a consequence of its atomic, molecular or aggregate structure. Since a chemical transformation is accompanied by a change in one or more of these kinds of structure, it is invariably accompanied by an increase or decrease of energy of the substances involved. Some energy is transferred between the surroundings and the reactants of the reaction in the form of heat or light; thus the products of a reaction may have more or less energy than the reactants. A reaction is said to be exergonic if the final state is lower on the energy scale than the initial state; in the case of endergonic reactions the situation is the reverse. Chemical reactions are invariably not possible unless the reactants surmount an energy barrier known as the activation energy. The \"speed\" of a chemical reaction (at given temperature \"T\") is related to the activation energy \"E\", by the Boltzmann's population factor ethat is the probability of molecule to have energy greater than or equal to \"E\" at the given temperature \"T\". This exponential dependence of a reaction rate on temperature is known as the Arrhenius equation.The activation energy necessary for a chemical reaction can be in the form of thermal energy.\n\nIn biology, energy is an attribute of all biological systems from the biosphere to the smallest living organism. Within an organism it is responsible for growth and development of a biological cell or an organelle of a biological organism. Energy is thus often said to be stored by cells in the structures of molecules of substances such as carbohydrates (including sugars), lipids, and proteins, which release energy when reacted with oxygen in respiration. In human terms, the human equivalent (H-e) (Human energy conversion) indicates, for a given amount of energy expenditure, the relative quantity of energy needed for human metabolism, assuming an average human energy expenditure of 12,500 kJ per day and a basal metabolic rate of 80 watts. For example, if our bodies run (on average) at 80 watts, then a light bulb running at 100 watts is running at 1.25 human equivalents (100 ÷ 80) i.e. 1.25 H-e. For a difficult task of only a few seconds' duration, a person can put out thousands of watts, many times the 746 watts in one official horsepower. For tasks lasting a few minutes, a fit human can generate perhaps 1,000 watts. For an activity that must be sustained for an hour, output drops to around 300; for an activity kept up all day, 150 watts is about the maximum. The human equivalent assists understanding of energy flows in physical and biological systems by expressing energy units in human terms: it provides a \"feel\" for the use of a given amount of energy.\n\nSunlight's radiant energy is also captured by plants as \"chemical potential energy\" in photosynthesis, when carbon dioxide and water (two low-energy compounds) are converted into the high-energy compounds carbohydrates, lipids, and proteins. Plants also release oxygen during photosynthesis, which is utilized by living organisms as an electron acceptor, to release the energy of carbohydrates, lipids, and proteins. Release of the energy stored during photosynthesis as heat or light may be triggered suddenly by a spark, in a forest fire, or it may be made available more slowly for animal or human metabolism, when these molecules are ingested, and catabolism is triggered by enzyme action.\n\nAny living organism relies on an external source of energy – radiant energy from the Sun in the case of green plants, chemical energy in some form in the case of animals – to be able to grow and reproduce. The daily 1500–2000 Calories (6–8 MJ) recommended for a human adult are taken as a combination of oxygen and food molecules, the latter mostly carbohydrates and fats, of which glucose (CHO) and stearin (CHO) are convenient examples. The food molecules are oxidised to carbon dioxide and water in the mitochondria\nand some of the energy is used to convert ADP into ATP.\nThe rest of the chemical energy in O and the carbohydrate or fat is converted into heat: the ATP is used as a sort of \"energy currency\", and some of the chemical energy it contains is used for other metabolism when ATP reacts with OH groups and eventually splits into ADP and phosphate (at each stage of a metabolic pathway, some chemical energy is converted into heat). Only a tiny fraction of the original chemical energy is used for work:\n\nIt would appear that living organisms are remarkably inefficient (in the physical sense) in their use of the energy they receive (chemical or radiant energy), and it is true that most real machines manage higher efficiencies. In growing organisms the energy that is converted to heat serves a vital purpose, as it allows the organism tissue to be highly ordered with regard to the molecules it is built from. The second law of thermodynamics states that energy (and matter) tends to become more evenly spread out across the universe: to concentrate energy (or matter) in one specific place, it is necessary to spread out a greater amount of energy (as heat) across the remainder of the universe (\"the surroundings\"). Simpler organisms can achieve higher energy efficiencies than more complex ones, but the complex organisms can occupy ecological niches that are not available to their simpler brethren. The conversion of a portion of the chemical energy to heat at each step in a metabolic pathway is the physical reason behind the pyramid of biomass observed in ecology: to take just the first step in the food chain, of the estimated 124.7 Pg/a of carbon that is fixed by photosynthesis, 64.3 Pg/a (52%) are used for the metabolism of green plants, i.e. reconverted into carbon dioxide and heat.\n\nIn geology, continental drift, mountain ranges, volcanoes, and earthquakes are phenomena that can be explained in terms of energy transformations in the Earth's interior, while meteorological phenomena like wind, rain, hail, snow, lightning, tornadoes and hurricanes are all a result of energy transformations brought about by solar energy on the atmosphere of the planet Earth.\n\nSunlight may be stored as gravitational potential energy after it strikes the Earth, as (for example) water evaporates from oceans and is deposited upon mountains (where, after being released at a hydroelectric dam, it can be used to drive turbines or generators to produce electricity). Sunlight also drives many weather phenomena, save those generated by volcanic events. An example of a solar-mediated weather event is a hurricane, which occurs when large unstable areas of warm ocean, heated over months, give up some of their thermal energy suddenly to power a few days of violent air movement.\n\nIn a slower process, radioactive decay of atoms in the core of the Earth releases heat. This thermal energy drives plate tectonics and may lift mountains, via orogenesis. This slow lifting represents a kind of gravitational potential energy storage of the thermal energy, which may be later released to active kinetic energy in landslides, after a triggering event. Earthquakes also release stored elastic potential energy in rocks, a store that has been produced ultimately from the same radioactive heat sources. Thus, according to present understanding, familiar events such as landslides and earthquakes release energy that has been stored as potential energy in the Earth's gravitational field or elastic strain (mechanical potential energy) in rocks. Prior to this, they represent release of energy that has been stored in heavy atoms since the collapse of long-destroyed supernova stars created these atoms.\n\nIn cosmology and astronomy the phenomena of stars, nova, supernova, quasars and gamma-ray bursts are the universe's highest-output energy transformations of matter. All stellar phenomena (including solar activity) are driven by various kinds of energy transformations. Energy in such transformations is either from gravitational collapse of matter (usually molecular hydrogen) into various classes of astronomical objects (stars, black holes, etc.), or from nuclear fusion (of lighter elements, primarily hydrogen). The nuclear fusion of hydrogen in the Sun also releases another store of potential energy which was created at the time of the Big Bang. At that time, according to theory, space expanded and the universe cooled too rapidly for hydrogen to completely fuse into heavier elements. This meant that hydrogen represents a store of potential energy that can be released by fusion. Such a fusion process is triggered by heat and pressure generated from gravitational collapse of hydrogen clouds when they produce stars, and some of the fusion energy is then transformed into sunlight.\n\nIn quantum mechanics, energy is defined in terms of the energy operator\nas a time derivative of the wave function. The Schrödinger equation equates the energy operator to the full energy of a particle or a system. Its results can be considered as a definition of measurement of energy in quantum mechanics. The Schrödinger equation describes the space- and time-dependence of a slowly changing (non-relativistic) wave function of quantum systems. The solution of this equation for a bound system is discrete (a set of permitted states, each characterized by an energy level) which results in the concept of quanta. In the solution of the Schrödinger equation for any oscillator (vibrator) and for electromagnetic waves in a vacuum, the resulting energy states are related to the frequency by Planck's relation: formula_3 (where formula_4 is Planck's constant and formula_5 the frequency). In the case of an electromagnetic wave these energy states are called quanta of light or photons.\n\nWhen calculating kinetic energy (work to accelerate a massive body from zero speed to some finite speed) relativistically – using Lorentz transformations instead of Newtonian mechanics – Einstein discovered an unexpected by-product of these calculations to be an energy term which does not vanish at zero speed. He called it rest energy: energy which every massive body must possess even when being at rest. The amount of energy is directly proportional to the mass of the body:\n\nwhere\n\nFor example, consider electron–positron annihilation, in which the rest energy of these two individual particles (equivalent to their rest mass) is converted to the radiant energy of the photons produced in the process. In this system the matter and antimatter (electrons and positrons) are destroyed and changed to non-matter (the photons). However, the total mass and total energy do not change during this interaction. The photons each have no rest mass but nonetheless have radiant energy which exhibits the same inertia as did the two original particles. This is a reversible process – the inverse process is called pair creation – in which the rest mass of particles is created from the radiant energy of two (or more) annihilating photons.\n\nIn general relativity, the stress–energy tensor serves as the source term for the gravitational field, in rough analogy to the way mass serves as the source term in the non-relativistic Newtonian approximation.\n\nEnergy and mass are manifestations of one and the same underlying physical property of a system. This property is responsible for the inertia and strength of gravitational interaction of the system (\"mass manifestations\"), and is also responsible for the potential ability of the system to perform work or heating (\"energy manifestations\"), subject to the limitations of other physical laws.\n\nIn classical physics, energy is a scalar quantity, the canonical conjugate to time. In special relativity energy is also a scalar (although not a Lorentz scalar but a time component of the energy–momentum 4-vector). In other words, energy is invariant with respect to rotations of space, but not invariant with respect to rotations of space-time (= boosts).\n\nEnergy may be transformed between different forms at various efficiencies. Items that transform between these forms are called transducers. Examples of transducers include a battery, from chemical energy to electric energy; a dam: gravitational potential energy to kinetic energy of moving water (and the blades of a turbine) and ultimately to electric energy through an electric generator; or a heat engine, from heat to work.\n\nExamples of energy transformation include generating electric energy from heat energy via a steam turbine, or lifting an object against gravity using electrical energy driving a crane motor. Lifting against gravity performs mechanical work on the object and stores gravitational potential energy in the object. If the object falls to the ground, gravity does mechanical work on the object which transforms the potential energy in the gravitational field to the kinetic energy released as heat on impact with the ground. Our Sun transforms nuclear potential energy to other forms of energy; its total mass does not decrease due to that in itself (since it still contains the same total energy even if in different forms), but its mass does decrease when the energy escapes out to its surroundings, largely as radiant energy.\n\nThere are strict limits to how efficiently heat can be converted into work in a cyclic process, e.g. in a heat engine, as described by Carnot's theorem and the second law of thermodynamics. However, some energy transformations can be quite efficient. The direction of transformations in energy (what kind of energy is transformed to what other kind) is often determined by entropy (equal energy spread among all available degrees of freedom) considerations. In practice all energy transformations are permitted on a small scale, but certain larger transformations are not permitted because it is statistically unlikely that energy or matter will randomly move into more concentrated forms or smaller spaces.\n\nEnergy transformations in the universe over time are characterized by various kinds of potential energy that has been available since the Big Bang later being \"released\" (transformed to more active types of energy such as kinetic or radiant energy) when a triggering mechanism is available. Familiar examples of such processes include nuclear decay, in which energy is released that was originally \"stored\" in heavy isotopes (such as uranium and thorium), by nucleosynthesis, a process ultimately using the gravitational potential energy released from the gravitational collapse of supernovae, to store energy in the creation of these heavy elements before they were incorporated into the solar system and the Earth. This energy is triggered and released in nuclear fission bombs or in civil nuclear power generation. Similarly, in the case of a chemical explosion, chemical potential energy is transformed to kinetic energy and thermal energy in a very short time. Yet another example is that of a pendulum. At its highest points the kinetic energy is zero and the gravitational potential energy is at maximum. At its lowest point the kinetic energy is at maximum and is equal to the decrease of potential energy. If one (unrealistically) assumes that there is no friction or other losses, the conversion of energy between these processes would be perfect, and the pendulum would continue swinging forever.\n\nEnergy is also transferred from potential energy (formula_8) to kinetic energy (formula_9) and then back to potential energy constantly. This is referred to as conservation of energy. In this closed system, energy cannot be created or destroyed; therefore, the initial energy and the final energy will be equal to each other. This can be demonstrated by the following:\n\nThe equation can then be simplified further since formula_10 (mass times acceleration due to gravity times the height) and formula_11 (half mass times velocity squared). Then the total amount of energy can be found by adding formula_12.\n\nEnergy gives rise to weight when it is trapped in a system with zero momentum, where it can be weighed. It is also equivalent to mass, and this mass is always associated with it. Mass is also equivalent to a certain amount of energy, and likewise always appears associated with it, as described in mass-energy equivalence. The formula \"E\" = \"mc\"², derived by Albert Einstein (1905) quantifies the relationship between rest-mass and rest-energy within the concept of special relativity. In different theoretical frameworks, similar formulas were derived by J.J. Thomson (1881), Henri Poincaré (1900), Friedrich Hasenöhrl (1904) and others (see Mass-energy equivalence#History for further information).\n\nPart of the rest energy (equivalent to rest mass) of matter may be converted to other forms of energy (still exhibiting mass), but neither energy nor mass can be destroyed; rather, both remain constant during any process. However, since formula_13 is extremely large relative to ordinary human scales, the conversion of an everyday amount of rest mass (for example, 1 kg) from rest energy to other forms of energy (such as kinetic energy, thermal energy, or the radiant energy carried by light and other radiation) can liberate tremendous amounts of energy (~formula_14 joules = 21 megatons of TNT), as can be seen in nuclear reactors and nuclear weapons. Conversely, the mass equivalent of an everyday amount energy is minuscule, which is why a loss of energy (loss of mass) from most systems is difficult to measure on a weighing scale, unless the energy loss is very large. Examples of large transformations between rest energy (of matter) and other forms of energy (e.g., kinetic energy into particles with rest mass) are found in nuclear physics and particle physics.\n\nThermodynamics divides energy transformation into two kinds: reversible processes and irreversible processes. An irreversible process is one in which energy is dissipated (spread) into empty energy states available in a volume, from which it cannot be recovered into more concentrated forms (fewer quantum states), without degradation of even more energy. A reversible process is one in which this sort of dissipation does not happen. For example, conversion of energy from one type of potential field to another, is reversible, as in the pendulum system described above. In processes where heat is generated, quantum states of lower energy, present as possible excitations in fields between atoms, act as a reservoir for part of the energy, from which it cannot be recovered, in order to be converted with 100% efficiency into other forms of energy. In this case, the energy must partly stay as heat, and cannot be completely recovered as usable energy, except at the price of an increase in some other kind of heat-like increase in disorder in quantum states, in the universe (such as an expansion of matter, or a randomisation in a crystal).\n\nAs the universe evolves in time, more and more of its energy becomes trapped in irreversible states (i.e., as heat or other kinds of increases in disorder). This has been referred to as the inevitable thermodynamic heat death of the universe. In this heat death the energy of the universe does not change, but the fraction of energy which is available to do work through a heat engine, or be transformed to other usable forms of energy (through the use of generators attached to heat engines), grows less and less.\n\nThe fact that energy can be neither created nor be destroyed is called the law of conservation of energy. In the form of the first law of thermodynamics, this states that a closed system's energy is constant unless energy is transferred in or out by work or heat, and that no energy is lost in transfer. The total inflow of energy into a system must equal the total outflow of energy from the system, plus the change in the energy contained within the system. Whenever one measures (or calculates) the total energy of a system of particles whose interactions do not depend explicitly on time, it is found that the total energy of the system always remains constant.\n\nWhile heat can always be fully converted into work in a reversible isothermal expansion of an ideal gas, for cyclic processes of practical interest in heat engines the second law of thermodynamics states that the system doing work always loses some energy as waste heat. This creates a limit to the amount of heat energy that can do work in a cyclic process, a limit called the available energy. Mechanical and other forms of energy can be transformed in the other direction into thermal energy without such limitations. The total energy of a system can be calculated by adding up all forms of energy in the system.\n\nRichard Feynman said during a 1961 lecture:\nMost kinds of energy (with gravitational energy being a notable exception) are subject to strict local conservation laws as well. In this case, energy can only be exchanged between adjacent regions of space, and all observers agree as to the volumetric density of energy in any given space. There is also a global law of conservation of energy, stating that the total energy of the universe cannot change; this is a corollary of the local law, but not vice versa.\n\nThis law is a fundamental principle of physics. As shown rigorously by Noether's theorem, the conservation of energy is a mathematical consequence of translational symmetry of time, a property of most phenomena below the cosmic scale that makes them independent of their locations on the time coordinate. Put differently, yesterday, today, and tomorrow are physically indistinguishable. This is because energy is the quantity which is canonical conjugate to time. This mathematical entanglement of energy and time also results in the uncertainty principle - it is impossible to define the exact amount of energy during any definite time interval. The uncertainty principle should not be confused with energy conservation - rather it provides mathematical limits to which energy can in principle be defined and measured.\n\nEach of the basic forces of nature is associated with a different type of potential energy, and all types of potential energy (like all other types of energy) appears as system mass, whenever present. For example, a compressed spring will be slightly more massive than before it was compressed. Likewise, whenever energy is transferred between systems by any mechanism, an associated mass is transferred with it.\n\nIn quantum mechanics energy is expressed using the Hamiltonian operator. On any time scales, the uncertainty in the energy is by\n\nwhich is similar in form to the Heisenberg Uncertainty Principle (but not really mathematically equivalent thereto, since \"H\" and \"t\" are not dynamically conjugate variables, neither in classical nor in quantum mechanics).\n\nIn particle physics, this inequality permits a qualitative understanding of virtual particles which carry momentum, exchange by which and with real particles, is responsible for the creation of all known fundamental forces (more accurately known as fundamental interactions). Virtual photons (which are simply lowest quantum mechanical energy state of photons) are also responsible for electrostatic interaction between electric charges (which results in Coulomb law), for spontaneous radiative decay of exited atomic and nuclear states, for the Casimir force, for van der Waals bond forces and some other observable phenomena.\n\nEnergy transfer can be considered for the special case of systems which are closed to transfers of matter. The portion of the energy which is transferred by conservative forces over a distance is measured as the work the source system does on the receiving system. The portion of the energy which does not do work during the transfer is called heat. Energy can be transferred between systems in a variety of ways. Examples include the transmission of electromagnetic energy via photons, physical collisions which transfer kinetic energy, and the conductive transfer of thermal energy.\n\nEnergy is strictly conserved and is also locally conserved wherever it can be defined. In thermodynamics, for closed systems, the process of energy transfer is described by the first law:\n\nwhere formula_16 is the amount of energy transferred, formula_2  represents the work done on the system, and formula_18 represents the heat flow into the system. As a simplification, the heat term, formula_18, is sometimes ignored, especially when the thermal efficiency of the transfer is high.\n\nThis simplified equation is the one used to define the joule, for example.\n\nBeyond the constraints of closed systems, open systems can gain or lose energy in association with matter transfer (both of these process are illustrated by fueling an auto, a system which gains in energy thereby, without addition of either work or heat). Denoting this energy by formula_16, one may write\n\nInternal energy is the sum of all microscopic forms of energy of a system. It is the energy needed to create the system. It is related to the potential energy, e.g., molecular structure, crystal structure, and other geometric aspects, as well as the motion of the particles, in form of kinetic energy. Thermodynamics is chiefly concerned with changes in internal energy and not its absolute value, which is impossible to determine with thermodynamics alone.\n\nThe first law of thermodynamics asserts that energy (but not necessarily thermodynamic free energy) is always conserved and that heat flow is a form of energy transfer. For homogeneous systems, with a well-defined temperature and pressure, a commonly used corollary of the first law is that, for a system subject only to pressure forces and heat transfer (e.g., a cylinder-full of gas) without chemical changes, the differential change in the internal energy of the system (with a \"gain\" in energy signified by a positive quantity) is given as\n\nwhere the first term on the right is the heat transferred into the system, expressed in terms of temperature \"T\" and entropy \"S\" (in which entropy increases and the change d\"S\" is positive when the system is heated), and the last term on the right hand side is identified as work done on the system, where pressure is \"P\" and volume \"V\" (the negative sign results since compression of the system requires work to be done on it and so the volume change, d\"V\", is negative when work is done on the system).\n\nThis equation is highly specific, ignoring all chemical, electrical, nuclear, and gravitational forces, effects such as advection of any form of energy other than heat and pV-work. The general formulation of the first law (i.e., conservation of energy) is valid even in situations in which the system is not homogeneous. For these cases the change in internal energy of a \"closed\" system is expressed in a general form by\n\nwhere formula_23 is the heat supplied to the system and formula_24 is the work applied to the system.\n\nThe energy of a mechanical harmonic oscillator (a mass on a spring) is alternatively kinetic and potential. At two points in the oscillation cycle it is entirely kinetic, and at two points it is entirely potential. Over the whole cycle, or over many cycles, net energy is thus equally split between kinetic and potential. This is called equipartition principle; total energy of a system with many degrees of freedom is equally split among all available degrees of freedom.\n\nThis principle is vitally important to understanding the behaviour of a quantity closely related to energy, called entropy. Entropy is a measure of evenness of a distribution of energy between parts of a system. When an isolated system is given more degrees of freedom (i.e., given new available energy states that are the same as existing states), then total energy spreads over all available degrees equally without distinction between \"new\" and \"old\" degrees. This mathematical result is called the second law of thermodynamics. The second law of thermodynamics is valid only for systems which are near or in equilibrium state. For non-equilibrium systems, the laws governing system’s behavior are still debatable. One of the guiding principles for these systems is the principle of maximum entropy production. It states that nonequilibrium systems behave in such a way to maximize its entropy production.\n\n\n"}
{"id": "7900498", "url": "https://en.wikipedia.org/wiki?curid=7900498", "title": "Energy being", "text": "Energy being\n\nAn energy being or astral being is a theoretical life form that is composed of energy rather than matter. They appear in myths/legends, paranormal/UFO accounts, and in various works of speculative fiction.\n\nEnergy beings are typically rendered as a translucent glowing fluid or as a collection of flames or electrical sparks or bolts; somewhat in common with the representations of ghosts.\n\nEnergy beings have a variety of capacities. The Taelons (from \"\") are barely more powerful than mortals, while others such as \"Star Trek\"s Q, \"Stargate SG-1\"s Ascended Ancients/Ori, \"\"s Anodites, or the Meekrob from \"Invader Zim\" possess god-like powers.\n\n\n"}
{"id": "4772620", "url": "https://en.wikipedia.org/wiki?curid=4772620", "title": "Energy monitoring and targeting", "text": "Energy monitoring and targeting\n\nEnergy monitoring and targeting (M&T) is an energy efficiency technique based on the standard management axiom stating that “you cannot manage what you cannot measure”. M&T techniques provide energy managers with feedback on operating practices, results of energy management projects, and guidance on the level of energy use that is expected in a certain period. Importantly, they also give early warning of unexpected excess consumption caused by equipment malfunctions, operator error, unwanted user behaviours, lack of effective maintenance and the like.\n\nThe foundation of M&T lies in determining the normal relationships of energy consumptions to relevant driving factors (HVAC equipment, production though puts, weather, occupancy available daylight, etc.) and the goal is to help business managers:\n\n\nThe ultimate goal is to reduce energy costs through improved energy efficiency and energy management control. Other benefits generally include increased resource efficiency, improved production budgeting and reduction of greenhouse gas (GHG) emissions.\n\nM&T is an established technique that was first launched as a national program in the UK in 1980, and has since then spread throughout Europe. These techniques are now also rapidly growing in America.\n\nThroughout the numerous M&T projects implemented since the 1980s, a certain number of benefits have proved to be recurrent:\n\nMonitoring and Targeting techniques rely on three main principles, which form a constant feedback cycle, therefore improving control of energy use.\n\nMonitoring information of energy use, in order to establish a basis for energy management and explain deviations from an established pattern. Its primary goal is to maintain said pattern, by providing all the necessary data on energy consumption, as well as certain driving factors, as identified during preliminary investigation (production, weather, etc.)\n\nThe final principle is the one which enables ongoing control of energy use, achievement of targets and verification of savings: reports must be issued to the appropriate managers. This in turn allows decision-making and actions to be taken in order to achieve the targets, as well as confirmation or denial that the targets have been reached.\n\nBefore the M&T measures themselves are implemented, a few preparatory steps are necessary. First of all, key energy consumers on the site must be identified. Generally, most of the energy consumption is concentrated in a small number of processes, like heating, or certain machinery. This normally requires a certain survey of the building and the equipment to estimate their energy consumption level.\n\nIt is also necessary to assess what other measurements will be required to analyze the consumption appropriately. This data will be used to chart against the energy consumption: these are underlying factors which influence the consumption, often production (for industry processes) or exterior temperature (for heating processes), but may include many other variables.\n\nOnce all variables to be measured have been established, and the necessary meters installed, it is possible to initiate the M&T procedures.\n\nThe first step is to compile the data from the different meters. Low-cost energy feedback displays have become available. The frequency at which the data is compiled varies according to the desired reporting interval, but can go once every 30 seconds to once every 15 minutes. Some measurements can be taken directly from the meters, others must be calculated. These different measurements are often called streams or channels.\n\nDriving factors such as production or degree days also constitute streams and must be collected at intervals to match.\n\nThe data compiled must then be plotted on a graph in order to define the general consumption base-line. Consumption rates are plotted in a scatter plot against production or any other variable previously identified, and the best fit line is identified. This graph is the image of the business’ average energy performance, and conveys a lot of information:\n\nThe slope is not used quite as often for M&T purposes. However, a high y-intercept can mean that there is a fault in the process, causing it to use too much energy with no performance, unless there are specific distinctive features which lead to high base loads. Very scattered points, on the other hand, may reflect other significant factors playing in the variation of the energy consumption, other than the one plotted in the first place, but it can also be the illustration of a lack of control over the process.\n\nThe next step is to monitor the difference between the expected consumption and the actual measured consumption. One of the tools most commonly used for this is the CUSUM, which is the CUmulative SUM of differences. This consists in first calculating the difference between the expected and actual performances (the best fit line previously identified and the points themselves).\n\nThe CUSUM can then be plotted against time on a new graph, which then yields more information for the energy efficiency specialist. Variances scattered around zero usually mean that the process is operating normally. Marked variations, increasing or decreasing steadily usually reflect a modification in the conditions of the process.\n\nIn the case of the CUSUM graph, the slope becomes very important, as it is the main indicator of the savings achieved. A slope going steadily down indicates steady savings. Any variation in the slope indicates a change in the process. For example, in the graph on the right, the first section indicated no savings. However, in September (beginning of the yellow line), an energy efficiency measure must have been implemented, as savings start to occur. The green line indicates an increase in the savings (as the slope is becoming steeper), whereas the red line must reflect a modification in the process having occurred in November, as savings have decreased slightly.\n\nEnergy efficiency specialists, in collaboration with building managers, will decipher the CUSUM graph and identify the causes leading to variations in the consumption. This can be a change in behaviour, a modification to the process, different exterior conditions, etc. These changes must be monitored and the causes identified in order to promote and enhance good behaviour, and discourage bad ones.\n\nOnce the base line has been established, and causes for variations in energy consumption have been identified, it is time to set targets for the future. Now with all this information in hand, the targets are more realistic, as they are based on the building’s actual consumption.\nTargeting consists in two main parts: the measure to which the consumption can be reduced, and the timeframe during which the compression will be achieved.\n\nA good initial target is the best fit line identified during step 2. This line represents the average historical performance. Therefore, keeping all consumption below or equal to the historical average is an achievable target, yet remains a challenge as it involves eliminating high consumption peaks.\n\nSome companies, as they improve their energy consumption, might even decide to bring their average performance down to their historical best. This is considered a much more challenging target.\n\nThis brings us back to step 1: measure consumption. One of the specificities of M&T is that it is an ongoing process, requiring constant feedback in order to consistently improve performance. Once the targets are set and the desired measures are implemented, repeating the procedure from the start ensures that the managers are aware of the success or failure of the measures, and can then decide on further action.\n\nAn example with some features of an M&T application is the ASU Campus Metabolism, which provides real-time and historic energy use and generation data for facilities of Arizona State University on a public web site. Many utilities also offer customers electric interval data monitoring services. Xcel Energy is an example of an investor owned utility that offers its customer electric and natural gas monitoring services under the product name InfoWise from Xcel Energy which is administered by Power TakeOff, a third party partner.\n\n"}
{"id": "53806811", "url": "https://en.wikipedia.org/wiki?curid=53806811", "title": "Final Straw: Food, Earth, Happiness", "text": "Final Straw: Food, Earth, Happiness\n\nFinal Straw: Food, Earth, Happiness is a documentary/art film released in June 2015 that takes audiences through farms and urban landscapes in Japan, South Korea, and the United States, interviewing leading practitioners in the Natural Farming movement. The film began when an environmental artist (Patrick M. Lydon) and an environmental book editor (Suhee Kang), had a chance meeting in Seoul, South Korea, and began conducting short interviews together with leaders in the ecology and social justice movements. Upon meeting Korean farmer Seong Hyun Choi however, the two were so impressed by his ecological mindset and way of working, that they set out to produce a feature film about the movement. Lydon and Kang ended up quitting their jobs, giving away most of their possessions, and becoming voluntarily homeless for four years in order to afford producing the film.\n\nThe film is split into three sections 1) Modern Life, 2) Foundations and Mindset of Natural Farming, and 3) Natural Farming in Practice and Life. According to the filmmakers, as they began to understand more about how natural farming itself was not rooted in methods, but in a way of thinking, they chose to explore the life philosophies and ways of thinking of natural farming practitioners in a more free-flowing and artistic way, rather than an instructive one; the result is an unconventional documentary that features slow paced musical interludes alongside interviews. Reviewers have called both \"meditative, and mindful,\" and \"an inspiring call to action.\" Author and musician Alicia Bay Laurel called the film \"both art and documentary\".\n\nLydon and Kang spent what they call a \"meager\" life savings to make the film, along with the volunteer efforts of farmers, translators, writers, musicians they had met during their journey. Although the film was filmed, written, and edited entirely by the two directors, they readily admit that the process of making the film was co-operative effort, with more than 200 volunteers directly involved in the process in some way. The soundtrack was recorded with professional musicians from each of the three countries where filming took place, all of whom donated their time to contribute to the film project. With the continued help of international volunteers, the film is available in four languages (English, Korean, Japanese, Vietnamese), and three more (Chinese, Portuguese, French) are in progress.\n\nFrustrated by the lack of distribution and film festival options for low- and no-budget films, the filmmakers made the decision to manage distribution and touring in the same way they went about filming, through co-operative effort. With the help of volunteers, independent theater owners, and community organizers, they launched an extensive tour throughout Japan and South Korea from 2015-2016, eventually screening the film at over 130 venues.\n\nRather than simply screening the film, the filmmakers decided to transition their existing media production organization \"SocieCity,\" into a vehicle for art and community engagement. They made a point of hosting interactive events along with their screenings and in several cases, stayed in communities for up to three months at a time to build natural gardens and host a project they call REALtimeFOOD, a grown-to-order restaurant which connects the ideas from the film with real-world practices in farming, food, and crafts. In most cases, these efforts were funded by grants from local philanthropic organizations and/or supported by the communities themselves.\n\nInterested in the unconventional way the film was being made and toured, multiple magazines and newspapers in Japan and Korea followed the directors during several parts of their journey, notably ESSEN, Bar and Dining, and Road magazines, and Shikoku Shinbun and Huffington Post newspapers.\n\nDuring the tour, the film was eventually picked up by festivals including Tassie Eco Film Festival and Belleville Doc Fest. \n\n"}
{"id": "1686779", "url": "https://en.wikipedia.org/wiki?curid=1686779", "title": "Fusion energy gain factor", "text": "Fusion energy gain factor\n\nThe fusion energy gain factor, usually expressed with the symbol Q, is the ratio of fusion power produced in a nuclear fusion reactor to the power required to maintain the plasma in steady state. The condition of \"Q\" = 1, when the power being released by the fusion reactions is equal to the required heating power, is referred to as breakeven.\n\nThe power given off by the fusion reactions may be captured within the fuel, leading to \"self-heating\". Most fusion reactions release at least some of their energy in a form that cannot be captured within the plasma, so a system at \"Q\" = 1 will cool without external heating. With typical fuels, self-heating in fusion reactors is not expected to match the external sources until at least \"Q\" = 5. If \"Q\" increases past this point, increasing self-heating eventually removes the need for external heating. At this point the reaction becomes self-sustaining, a condition called ignition. Ignition corresponds to infinite \"Q\", and is generally regarded as highly desirable for a practical reactor design.\n\nOver time, several related terms have entered the fusion lexicon. As a reactor does not cover its own heating losses until about \"Q\" = 5, the term engineering breakeven is sometimes used to describe a reactor that produces enough electricity to provide that heating. Above engineering breakeven a machine would produce more electricity than it uses, and could sell that excess. A machine that can sell enough electricity to cover its operating costs, estimated to require at least \"Q\" = 20, is sometimes known as economic breakeven.\n\n, the record for \"Q\" is held by the JET tokamak in the UK, at \"Q\" = (16 MW)/(24 MW) ≈ 0.67, first attained in 1997. ITER was originally designed to reach ignition, but is currently designed to reach \"Q\" = 10, producing 500 MW of fusion power from 50 MW of injected thermal power.\n\n\"Q\" is simply the comparison of the power being released by the fusion reactions in a reactor, \"P\", to the constant heating power being supplied, \"P\". However, there are several definitions of breakeven that consider additional power losses.\n\nIn 1955, John Lawson was the first to explore the energy balance mechanisms in detail, initially in classified works but published openly in a now-famous 1957 paper. In this paper he considered and refined work by earlier researchers, notably Hans Thirring, Peter Thonemann, and a review article by Richard Post. Expanding on all of these, Lawson's paper made detailed predictions for the amount of power that would be lost through various mechanisms, and compared that to the energy needed to sustain the reaction. This balance is today known as the Lawson criterion.\n\nIn a successful fusion reactor design, the fusion reactions generate an amount of power designated \"P\". Some amount of this energy, \"P\", is lost through a variety of mechanisms, mostly convection of the fuel to the walls of the reactor chamber and various forms of radiation that cannot be captured to generate power. In order to keep the reaction going, the system has to provide heating to make up for these losses, where \"P\" = \"P\" to maintain thermal equilibrium.\n\nThe most basic definition of breakeven is when \"Q\" = 1, that is, \"P\" = \"P\".\n\nSome works refer to this definition as scientific breakeven, to contrast it with similar terms. However, this usage is rare outside certain areas, specifically the inertial confinement fusion field, where the term is much more widely used.\n\nSince the 1950s, most commercial fusion reactor designs have been based on a mix of deuterium and tritium as their primary fuel; others fuels have been studied for a variety of reasons but are much harder to ignite. As tritium is radioactive, highly bioactive and highly mobile, it represents a significant safety concern and adds to the cost of designing and operating such a reactor.\n\nIn order to lower costs, many experimental machines are designed to run on test fuels of hydrogen or deuterium alone, leaving out the tritium. In this case, the term extrapolated breakeven is used to define the expected performance of the machine running on D-T fuel based on the performance when running on hydrogen or deuterium alone.\n\nThe records for extrapolated breakeven are slightly higher than the records for scientific breakeven. Both JET and JT-60 have reached values around 1.25 (see below for details) while running on D-D fuel. When running on D-T, only possible in JET, the maximum performance is about half the extrapolated value.\n\nAnother related term, engineering breakeven, considers the need to extract the energy from the reactor, turn that into electrical energy, and feed that back into the heating system. This closed loop is known as \"recirculation\". In this case, the basic definition changes by adding additional terms to the \"P\" side to consider the efficiencies of these processes.\n\nMost fusion reactions release energy in a variety of forms, mostly neutrons and a variety of charged particles like alpha particles. Neutrons are electrically neutral and will travel out of any magnetic confinement fusion (MFE) design, and in spite of the very high densities found in inertial confinement fusion (ICF) designs, they tend to easily escape the fuel mass in these designs as well. This means that only the charged particles from the reactions can be captured within the fuel mass and give rise to self-heating. If the fraction of the energy being released in the charged particles is \"f\", then the power in these particles is \"P\" = \"f\"\"P\". If this self-heating process is perfect, that is, all of \"P\" is captured in the fuel, that means the power available for generating electricity is the power that is not released in that form, or (1 − \"f\")\"P\".\n\nIn the case of neutrons carrying most of the practical energy, as is the case in the D-T fuel studied in most designs, this neutron energy is normally captured in a \"blanket\" of lithium that produces more tritium that is used to fuel the reactor. Due to various exothermic and endothermic reactions, the blanket may have a power gain factor a few percent higher or lower than 100%, but that will be neglected here. The blanket is then cooled and the cooling fluid used in a heat exchanger driving conventional steam turbines. These have an efficiency η which is around 35 to 40%.\n\nConsider a system that uses external heaters to heat the fusion fuel, then extracts the power from those reactions to generate electrical power. Some fraction of that power, \"f\", is needed to recirculate back into the heaters to close the loop. This is not the same as the \"P\" because the self-heating processes are providing some of the required energy. While the system as a whole requires additional power for building climate control, lighting, and the confinement system, these are generally much smaller than the plasma heating system requirements.\nConsidering all of these factors, the heating power can thus be related to the fusion power by the following equation:\n\nformula_1\n\nwhere formula_2 is the efficiency that power supplied to the heating systems is turned into heat in the fuel, as opposed to lost in the equipment itself, and formula_3 is the efficiency achieved when turning the heat into electrical power, for instance, through the Rankine cycle.\n\nThe fusion energy gain factor is then defined as:\n\nformula_4\n\nAs the temperature of the plasma increases, the rate of fusion reactions grows rapidly, and with it, the rate of self heating. In contrast, the non-capturable energy losses like x-rays do not grow at the same rate. Thus, in overall terms, the self-heating process becomes more efficient as the temperature increases, and less energy is needed from external sources to keep it hot.\n\nEventually \"P\" reaches zero, that is, all of the energy needed to keep the plasma at the operational temperature is being supplied by self-heating, and the amount of external energy that needs to be added drops to zero. This point is known as ignition.\n\nIgnition, by definition, corresponds to an infinite \"Q\", but it does not mean that \"f\" drops to zero as the other power sinks in the system, like the magnets and cooling systems, still need to be powered. Generally, however, these are much smaller than the energy in the heaters, and require a much smaller \"f\". More importantly, this number is more likely to be near constant, meaning that further improvements in plasma performance will result in more energy that can be directly used for commercial generation, as opposed to recirculation.\n\nThe final definition of breakeven is commercial breakeven, which occurs when the economic value of any net energy left over after recirculation is enough to finance the construction of the reactor. This value depends both on the reactor and the spot price of electrical power.\n\nCommercial breakeven relies on factors outside the technology of the reactor itself, and it is possible that even a reactor with a fully ignited plasma will not generate enough energy to pay for itself. Whether any of the mainline concepts like ITER can reach this goal is being debated in the field.\n\nMost fusion reactor designs being studied are based on the D-T reaction, as this is by far the easiest to ignite, and is energy dense. However, this reaction also gives off most of its energy in the form of a single highly energetic neutron, and only 20% of the energy in the form of an alpha. Thus, for the D-T reaction, \"f\" = 0.2. This means that self-heating does not become equal to the external heating until at least \"Q\" = 5. \n\nEfficiency values depend on design details but may be in the range of η = 0.7 (70%) and η = 0.4 (40%). The purpose of a fusion reactor is to produce power, not to recirculate it, so a practical reactor must have \"f\" = 0.2 approximately. Lower would be better but will be hard to achieve. Using these values we find for a practical reactor \"Q\" = 22.\n\nMany early fusion devices operated for microseconds, using some sort of pulsed power source to feed their magnetic confinement system and used the confinement as the heating source. Lawson defined breakeven in this context as the total energy released by the entire reaction cycle compared to the total energy supplied to the machine during the same cycle.\n\nOver time, as performance increased by orders of magnitude, the reaction times have extended from microseconds to seconds, and in ITER, on the order of minutes. In this case definition of \"the entire reaction cycle\" becomes blurred. In the case of an ignited plasma, for instance, P may be quite high while the system is being set up, and then drop to zero when it is fully developed, so one may be tempted to pick an instant in time when it is operating at its best to determine \"Q\". A better solution in these cases is to use the original Lawson definition averaged over the reaction to produce a similar value as the original definition.\n\nHowever, there is a complication. During the heating phase when the system is being brought up to operational conditions, some of the energy released by the fusion reactions will be used to heat the surrounding fuel, and thus not be released. This is no longer true when the plasma reaches its operational temperature and enters thermal equilibrium. Thus, if one averages over the entire cycle, this energy will be included as part of the heating term, that is, some of the energy that was captured for heating would otherwise have been released in P and is therefore not indicative of an operational \"Q\".\n\nOperators of the JET reactor argued that this input should be removed from the total:\n\nformula_5\n\nwhere:\n\nformula_6\n\nThat is, P is the amount of energy needed to raise the internal energy of the plasma. It is this definition that was used when reporting JET's record 0.67 value.\n\nSome debate over this definition continues. In 1998, the operators of the JT-60 claimed to have reached \"Q\" = 1.25 running on D-D fuel, thus reaching extrapolated breakeven. However, this measurement was based on the JET definition of Q*. Using this definition, JET had also reached extrapolated breakeven some time earlier. If one considers the energy balance in these conditions, and the analysis of previous machines, it is argued the original definition should be used, and thus both machines remain well below break-even of any sort.\n\nAlthough most fusion experiments use some form of magnetic confinement, another major branch is inertial confinement fusion (ICF) that mechanically presses together the fuel mass (the \"target\") to increase its density. This greatly increases the rate of fusion events and lowers the need to confine the fuel for long periods. This compression is accomplished by heating a lightweight capsule holding the fuel so rapidly that it explodes outwards, driving the fuel mass on the inside inward in accordance with Newton's third law. There are a variety of proposed \"drivers\" to cause the implosion process, but to date most experiments have used lasers. \n\nUsing the traditional definition of \"Q\", \"P\" / \"P\", ICF devices have extremely low \"Q\". This is because the laser is extremely inefficient; whereas formula_2 for the heaters used in magnetic systems might be on the order of 70%, lasers are on the order of 1.5%. For this reason, Lawrence Livermore National Laboratory (LLNL), the leader in ICF research, has proposed another modification of \"Q\" that defines \"P\" as the energy delivered by the driver, as opposed to the energy put into the driver. This definition produces much higher \"Q\" values, and changes the definition of breakeven to be \"P\" / \"P\" = 1. On occasion, they referred to this definition as \"scientific breakeven\". This term was not universally used, other groups adopted the redefinition of \"Q\" but continued to refer to \"P\" = \"P\" simply as breakeven. \n\nOn 7 October 2013, the BBC announced that LLNL had achieved scientific breakeven in the National Ignition Facility (NIF) on 29 September. In this experiment, \"P\" was approximately 14 kJ, while the laser output was 1.8 MJ. By their previous definition, this would be a \"Q\" of 0.0077. However, for this press release, they re-defined \"Q\" once again, this time equating \"P\" to be only the amount energy delivered to \"the hottest portion of the fuel\", calculating that only 10 kJ of the original laser energy reached the part of the fuel that was undergoing fusion reactions. This release has been heavily criticized in the field.\n\n"}
{"id": "23149866", "url": "https://en.wikipedia.org/wiki?curid=23149866", "title": "Gold universe", "text": "Gold universe\n\nA Gold universe is a cosmological model of the universe. In these models, the universe starts with a Big Bang and expands for some time, with increasing entropy and a thermodynamic arrow of time pointing in the direction of the expansion. After the universe reaches a low-density state, it recontracts, but entropy now decreases, pointing the thermodynamic arrow of time in the opposite direction, until the universe ends in a low-entropy, high-density Big Crunch. \nThere are two models of the universe which support the possibility of a reversed direction of time. The first begins with a state of low entropy at the Big Bang which continually increases until the Big Crunch. The second, a Gold Universe, posits that entropy will increase only until a moment of contraction, then gradually decrease. This latter model suggests the universe will become more orderly after the moment of contraction. The Gold model has been linked to the possibility of retrocausal change, questions concerning the preservation of information in a time-reversed universe (states of decreasing entropy), and causation in general. The Gold Universe is named after the cosmologist Thomas Gold, who proposed the model in the 1960s.\n"}
{"id": "1471037", "url": "https://en.wikipedia.org/wiki?curid=1471037", "title": "Graphical timeline of the Big Bang", "text": "Graphical timeline of the Big Bang\n\nThis timeline of the Big Bang shows a sequence of events as currently theorized by scientists. \n\nIt is a logarithmic scale that shows formula_1 \"second\" instead of \"second\". For example, one microsecond is formula_2. To convert −30 read on the scale to second calculate formula_3 second = one millisecond. On a logarithmic time scale a step lasts ten times longer than the previous step.\n\n"}
{"id": "6732384", "url": "https://en.wikipedia.org/wiki?curid=6732384", "title": "Graphical timeline of the Stelliferous Era", "text": "Graphical timeline of the Stelliferous Era\n\nThis is the timeline of the stelliferous era but also partly charts the primordial era, and charts more of the degenerate era of the heat death scenario.\n\nThe scale is formula_1. Example one million years is formula_2.\n\n"}
{"id": "180236", "url": "https://en.wikipedia.org/wiki?curid=180236", "title": "Greisen–Zatsepin–Kuzmin limit", "text": "Greisen–Zatsepin–Kuzmin limit\n\nThe Greisen–Zatsepin–Kuzmin limit (GZK limit) is a theoretical upper limit on the energy of cosmic ray protons traveling from other galaxies through the intergalactic medium to our galaxy. The limit is , or about 8 joules. The limit is set by slowing-interactions of the protons with the microwave background radiation over long distances (~160 million light-years). The limit is at the same order of magnitude as the upper limit for energy at which cosmic rays have experimentally been detected. For example, one extreme-energy cosmic ray has been detected which appeared to possess a record (50 joules) of energy (about the same as the kinetic energy of a 35 mph baseball).\n\nThe GZK limit is derived under the assumption that ultra-high energy cosmic rays are protons. Measurements by the largest cosmic-ray observatory, the Pierre Auger Observatory, suggest that most ultra-high energy cosmic rays are heavier elements. In this case, the argument behind the GZK limit does not apply in the originally simple form and there is no fundamental contradiction in observing cosmic rays with energies that violate the limit.\n\nIn the past, the apparent violation of the GZK limit has inspired cosmologists and theoretical physicists to suggest other ways that circumvent the limit. These theories propose that ultra-high energy cosmic rays are produced nearby our galaxy or that Lorentz covariance is violated in such a way that protons do not lose energy on their way to our galaxy.\n\nThe limit was independently computed in 1966 by Kenneth Greisen, Vadim Kuzmin, and Georgiy Zatsepin, based on interactions between cosmic rays and the photons of the cosmic microwave background radiation (CMB). They predicted that cosmic rays with energies over the threshold energy of would interact with cosmic microwave background photons formula_1, relatively blueshifted by the speed of the cosmic rays, to produce pions via the formula_2 resonance,\n\nor\n\nPions produced in this manner proceed to decay in the standard pion channels—ultimately to photons for neutral pions, and photons, positrons, and various neutrinos for positive pions. Neutrons decay also to similar products, so that ultimately the energy of any cosmic ray proton is drained off by production of high energy photons plus (in some cases) high energy electron/positron pairs and neutrino pairs.\n\nThe pion production process begins at a higher energy than ordinary electron-positron pair production (lepton production) from protons impacting the CMB, which starts at cosmic ray proton energies of only about . However, pion production events drain 20% of the energy of a cosmic ray proton as compared with only 0.1% of its energy for electron positron pair production. This factor of 200 is from two sources: the pion has only about ~130 times the mass of the leptons, but the extra energy appears as different kinetic energies of the pion or leptons, and results in relatively more kinetic energy transferred to a heavier product pion, in order to conserve momentum. The much larger total energy losses from pion production result in the pion production process becoming the limiting one to high energy cosmic ray travel, rather than the lower-energy light-lepton production process.\n\nThe pion production process continues until the cosmic ray energy falls below the pion production threshold. Due to the mean path associated with this interaction, extragalactic cosmic rays traveling over distances larger than () and with energies greater than this threshold should never be observed on Earth. This distance is also known as GZK horizon.\n\nA number of observations have been made by the largest cosmic ray experiments Akeno Giant Air Shower Array, High Resolution Fly's Eye Cosmic Ray Detector, the Pierre Auger Observatory and Telescope Array Project that appeared to show cosmic rays with energies above this limit (called extreme-energy cosmic rays, or EECRs). The observation of these particles was the so-called GZK paradox or cosmic ray paradox.\n\nThese observations appear to contradict the predictions of special relativity and particle physics as they are presently understood. However, there are a number of possible explanations for these observations that may resolve this inconsistency.\n\nAnother suggestion involves ultra-high energy weakly interacting particles (for instance, neutrinos) which might be created at great distances and later react locally to give rise to the particles observed. In the proposed Z-burst model, an ultra-high energy cosmic neutrino collides with a relic anti-neutrino in our galaxy and annihilates to hadrons. This process proceeds via a (virtual) Z-boson:\n\nformula_8\n\nThe cross section for this process becomes large if the center of mass energy of the neutrino antineutrino pair is equal to the Z-boson mass (such a peak in the cross section is called \"resonance\"). Assuming that the relic anti-neutrino is at rest, the energy of the incident cosmic neutrino has to be:\n\nformula_9\n\nwhere formula_10 is the mass of the Z-boson and formula_11 the mass of the neutrino.\n\nA number of exotic theories have been advanced to explain the AGASA observations, including doubly special relativity. However, it is now established that standard doubly special relativity does not predict any GZK suppression (or GZK cutoff), contrary to models of Lorentz symmetry violation involving an absolute rest frame. Other possible theories involve a relation with dark matter, decays of exotic super-heavy particles beyond those known in the Standard Model.\n\nA suppression of the cosmic ray flux which can be explained with the GZK limit has been confirmed by the latest generation of cosmic ray observatories. A former claim by the AGASA experiment that there is no suppression was overruled. It remains controversial, whether the suppression is due to the GZK effect. The GZK limit only applies if ultra-high energy cosmic rays are mostly protons.\n\nIn July 2007, during the 30th International Cosmic Ray Conference in Mérida, Yucatán, México, the High Resolution Fly's Eye Experiment (HiRes) and the Pierre Auger Observatory (Auger) presented their results on ultra-high-energy cosmic rays. HiRes observed a suppression in the UHECR spectrum at just the right energy, observing only 13 events with an energy above the threshold, while expecting 43 with no suppression. This was interpreted as the first observation of the GZK limit. Auger confirmed the flux suppression, but did not claim it to be the GZK limit: instead of the 30 events necessary to confirm the AGASA results, Auger saw only two, which are believed to be heavy nuclei events. The flux suppression was previously brought into question when the AGASA experiment found no suppression in their spectrum. According to Alan Watson, spokesperson for the Auger Collaboration, AGASA results have been shown to be incorrect, possibly due to the systematic shift in energy assignment.\n\nIn 2010 and the following years, both the Pierre Auger Observatory and HiRes confirmed again a flux suppression, in case of the Pierre Auger Observatory the effect is statistically significant at the level of 20 standard deviations.\n\nAfter the flux suppression was established, a heated debate ensued whether cosmic rays that violate the GZK limit are protons. The Pierre Auger Observatory, the world's largest observatory, found with high statistical significance that ultra-high energy cosmic rays are not purely protons, but a mixture of elements which is getting heavier with increasing energy.\nThe Telescope Array Project, a joint effort from members of the HiRes and AGASA collaborations, agrees with the former HiRes result that these cosmic rays look like protons. The claim is based on data with lower statistical significance, however. The area covered by Telescope Array is about one third of the area covered by the Pierre Auger Observatory, and the latter has been running for a longer time.\n\nThe controversy was partially resolved in 2017, when a joint working group formed by members of both experiments presented a report at the 35th International Cosmic Ray Conference. According to the report, the raw experimental results are not in contradiction with each other. The different interpretations are mainly based on the use of different theoretical models (Telescope Array uses an outdated model for its interpretation), and the fact that Telescope Array has not collected enough events yet to distinguish the pure proton hypothesis from the mixed-nuclei hypothesis.\n\nEUSO, which was scheduled to fly on the International Space Station (ISS) in 2009, was designed to use the atmospheric-fluorescence technique to monitor a huge area and boost the statistics of UHECRs considerably. EUSO is to make a deep survey of UHECR-induced extensive air showers (EASs) from space, extending the measured energy spectrum well beyond the GZK-cutoff. It is to search for the origin of UHECRs, determine the nature of the origin of UHECRs, make an all-sky survey of the arrival direction of UHECRs, and seek to open the astronomical window on the extreme-energy universe with neutrinos. The fate of the EUSO Observatory is still unclear since NASA is considering early retirement of the ISS.\n\nLaunched in June 2008, the Fermi Gamma-ray Space Telescope (formerly GLAST) will also provide data that will help resolve these inconsistencies.\n\nIn November 2007, researchers at the Pierre Auger Observatory announced that they had evidence that UHECRs appear to come from the active galactic nuclei (AGNs) of energetic galaxies powered by matter swirling onto a supermassive black hole. The cosmic rays were detected and traced back to the AGNs using the Véron-Cetty-Véron catalog. These results are reported in the journal \"Science\". Nevertheless, the strength of the correlation with AGNs from this particular catalog for the Auger data recorded after 2007 has been slowly diminishing.\n\n"}
{"id": "40111102", "url": "https://en.wikipedia.org/wiki?curid=40111102", "title": "Intelligent Energy", "text": "Intelligent Energy\n\nIntelligent Energy is a fuel cell engineering company focused on the development and commercialisation of its PEM fuel cell technologies for a range of markets including automotive, stationary power and UAVs. We are headquartered in the UK, with offices and representation in the US, Japan, India, and China.\n\nThe origins of Intelligent Energy began at Loughborough University in the UK during the late 1980s, when the University became one of Europe’s first research and development centres for proton exchange membrane (PEM) fuel cell technology. In 1995, the UK’s first kW-level PEM fuel cell stack was produced by the R&D team. In June of that year, Advanced Power Sources (APS) Ltd was founded as a spin-out from Loughborough University by Paul Adcock, Phil Mitchell, Jon Moore and Anthony Newbold, and was the first company in the UK formed specifically to address the development and commercialisation of PEM fuel cells.\n\nFounded by Harry Bradbury, Intelligent Energy was established in 2001, acquiring Advanced Power Sources Ltd, together with its personnel and fuel cell related intellectual property that originated from research conducted by both APS and Loughborough University into PEM fuel cell technology. This triggered investment and enabled the company to grow its business activities.\nIn March 2005, it launched the ENV, the world’s first purpose-built fuel cell motorbike which gained the company recognition as a Technology Pioneer by the World Economic Forum in 2006. The ENV incorporated the company’s air-cooled fuel cell technology hybridised with a battery pack to provide 6 kW peak load to the motor to improve performance during spikes in power demand i.e. acceleration.\n\nIn 2007, a partnership was announced with Suzuki Motor Corporation to develop hydrogen fuel cells for a range of vehicles. In 2008, Intelligent Energy established the company, IE-CHP in a joint venture with SSE plc, to develop fuel cells and other technologies for CHP (Combined Heat and Power) applications. In the same year, Intelligent Energy also produced the power system for the first fuel cell powered manned flight in conjunction with Boeing.\nIn 2010, its fuel-cell taxi received The Engineer Technology and Innovation Award.\n\nIn March 2011, the Suzuki Burgman fuel cell scooter, equipped with Intelligent Energy’s fuel cell system, became the first fuel cell vehicle to achieve European Whole Vehicle Type Approval.\n\nIn 2012, SMILE FC System Corporation, a joint venture between Intelligent Energy and Suzuki Motor Corporation, was established to develop and manufacture air-cooled fuel cell systems for the automotive and a range of industry sectors.\nDuring the same year, a fleet of fuel cell taxis incorporating Intelligent Energy’s technology was used during the 2012 London Olympics. Part of the European Union-funded HyTEC (Hydrogen Technologies in European Cities) project launched in 2011, the taxis were used to transport VIP guests of the Mayor of London around the city.\nIn 2013, SMILE FC Corporation announced that it had established a ready-to-scale production line for its fuel cell systems, utilising Intelligent Energy’s semi-automated production technology. IE-CHP also received CE certification for its first-generation product, a 10 kWe/12 kWth combined heat and power (CHP) fuel cell. The certification allows the product to be sold in the European Economic Area, confirming that the product satisfies all the EU regulatory and conformity assessment procedures covering the design, manufacture, and testing of the system.\n\nIntelligent Energy was acquired by Meditor Energy, part of the Meditor Group, in October 2017.\n\nIntelligent Energy's fuel-cell technology is divided into two platforms: air-cooled (AC) and evaporatively-cooled (EC). The air-cooled fuel cell systems use low-power fans to provide cooling and the oxidant supply for operation. Heat from the fuel cell stack is conducted to cooling plates and removed through airflow channels, a simplified and cost-effective system for the power range from a few watts to several kilowatts. They are used in a wide range of UAV, stationary power and automotive applications for two-wheel and small car range extender applications.\n\nEvaporatively-cooled (EC) fuel cell systems provide power generation from a few kilowatts up to 200 kW. Efficient thermal management of the EC fuel cell stack reduces system complexity, mass and cost. These systems are designed for high-volume, low-cost manufacturing, and use modular architecture that can be quickly modified to suit the application.\n\nThe firm's fuel cell stacks have been developed for small and large cars, scooters and motorbikes. \nIn 2010, the company was involved in the development of the report entitled “A portfolio of power-trains for Europe: a fact-based analysis. The role of Battery Electric Vehicles, Plug-In Hybrids and Fuel Cell Electric Vehicles”, produced by McKinsey & Company with input from car manufacturers, oil and gas suppliers, utilities and industrial gas companies, wind turbine and electrolyser companies as well as governmental and non-governmental organisations. The report concluded, amongst other findings, that fuel cell vehicles are technology ready, and cost competitive, and that decarbonisation targets for Europe are unlikely to be met without the introduction of fuel cell powertrains.\n\nThe firm provides fuel cells to power UAVs and aerial drones. Its UAV Fuel Cell Modules run on hydrogen and ambient air to produce DC power in a lightweight package providing extended flight times when compared to battery systems.\n\nThe company’s fuel cell systems are used to provide diesel replacement and backup power initially for telecom towers but also for other sectors. The company has field proven its fuel cell products in the Indian telecommunications market with a tower uptime of close to 100%.\n\nThe company is a founding member of UKH Mobility, a government and industry group aiming to accelerate the commercial roll out of hydrogen vehicles in 2014/15;\nIt is also a member of the Fuel Cell and Hydrogen Energy Association (FCHEA), the US-based trade association for the fuel cell and hydrogen energy industry, dedicated to the commercialisation of fuel cells and hydrogen energy technologies.\n\n"}
{"id": "9332507", "url": "https://en.wikipedia.org/wiki?curid=9332507", "title": "Leibniz–Clarke correspondence", "text": "Leibniz–Clarke correspondence\n\nThe Leibniz–Clarke correspondence was a scientific, theological and philosophical debate conducted in an exchange of letters between the German thinker Gottfried Wilhelm Leibniz and Samuel Clarke, an English supporter of Isaac Newton during the years 1715 and 1716. The exchange began because of a letter Leibniz wrote to Caroline of Ansbach, in which he remarked that Newtonian physics was detrimental to natural theology. Eager to defend the Newtonian view, Clarke responded, and the correspondence continued until the death of Leibniz in 1716.\n\nAlthough a variety of subjects is touched on in the letters, the main interest for modern readers is in the dispute between the absolute theory of space favoured by Newton and Clarke, and Leibniz's relational approach. Also important is the conflict between Clarke's and Leibniz's opinions on free will and whether God must create the best of all possible worlds.\n\nLeibniz had published only a book on moral matters, the \"Theodicée\" (1710), and his more metaphysical views had never been exposed to a sufficient extent, so the collected letters were met with interest by their contemporaries. The priority dispute between Leibniz and Newton about the calculus was still fresh in the public's mind and it was taken as a matter of course that it was Newton himself who stood behind Clarke's replies.\n\nThe Leibniz-Clarke letters were first published under Clarke's name in the year following Leibniz' death. He wrote a preface, took care of the translation from French, added notes and some of his own writing. In 1720 Pierre Desmaizeaux published a similar volume in a French translation, including quotes from Newton's work. It is quite certain that for both editions the opinion of Newton himself has been sought and Leibniz left at a disadvantage. However the German translation of the correspondence published by Kohler, also in 1720, contained a reply to Clarke's last letter which Leibniz had not been able to answer. The letters have been reprinted in most collections of Leibniz' works and regularly published in stand alone editions.\n\n\n\n"}
{"id": "36688650", "url": "https://en.wikipedia.org/wiki?curid=36688650", "title": "Moisture expansion", "text": "Moisture expansion\n\nMoisture expansion is the tendency of matter to change in volume in response to a change in moisture content. The macroscopic effect is similar to that of thermal expansion but the microscopic causes are very different. Moisture expansion is caused by hygroscopy.\n"}
{"id": "19555", "url": "https://en.wikipedia.org/wiki?curid=19555", "title": "Molecule", "text": "Molecule\n\nA molecule is an electrically neutral group of two or more atoms held together by chemical bonds. Molecules are distinguished from ions by their lack of electrical charge. However, in quantum physics, organic chemistry, and biochemistry, the term \"molecule\" is often used less strictly, also being applied to polyatomic ions.\n\nIn the kinetic theory of gases, the term \"molecule\" is often used for any gaseous particle regardless of its composition. According to this definition, noble gas atoms are considered molecules as they are monatomic molecules.\n\nA molecule may be homonuclear, that is, it consists of atoms of one chemical element, as with oxygen (O); or it may be heteronuclear, a chemical compound composed of more than one element, as with water (HO). Atoms and complexes connected by non-covalent interactions, such as hydrogen bonds or ionic bonds, are generally not considered single molecules.\n\nMolecules as components of matter are common in organic substances (and therefore biochemistry). They also make up most of the oceans and atmosphere. However, the majority of familiar solid substances on Earth, including most of the minerals that make up the crust, mantle, and core of the Earth, contain many chemical bonds, but are \"not\" made of identifiable molecules. Also, no typical molecule can be defined for ionic crystals (salts) and covalent crystals (network solids), although these are often composed of repeating unit cells that extend either in a plane (such as in graphene) or three-dimensionally (such as in diamond, quartz, or sodium chloride). The theme of repeated unit-cellular-structure also holds for most condensed phases with metallic bonding, which means that solid metals are also not made of molecules. In glasses (solids that exist in a vitreous disordered state), atoms may also be held together by chemical bonds with no presence of any definable molecule, nor any of the regularity of repeating units that characterizes crystals.\n\nThe science of molecules is called \"molecular chemistry\" or \"molecular physics\", depending on whether the focus is on chemistry or physics. Molecular chemistry deals with the laws governing the interaction between molecules that results in the formation and breakage of chemical bonds, while molecular physics deals with the laws governing their structure and properties. In practice, however, this distinction is vague. In molecular sciences, a molecule consists of a stable system (bound state) composed of two or more atoms. Polyatomic ions may sometimes be usefully thought of as electrically charged molecules. The term \"unstable molecule\" is used for very reactive species, i.e., short-lived assemblies (resonances) of electrons and nuclei, such as radicals, molecular ions, Rydberg molecules, transition states, van der Waals complexes, or systems of colliding atoms as in Bose–Einstein condensate.\n\nAccording to Merriam-Webster and the Online Etymology Dictionary, the word \"molecule\" derives from the Latin \"moles\" or small unit of mass.\n\nThe definition of the molecule has evolved as knowledge of the structure of molecules has increased. Earlier definitions were less precise, defining molecules as the smallest particles of pure chemical substances that still retain their composition and chemical properties. This definition often breaks down since many substances in ordinary experience, such as rocks, salts, and metals, are composed of large crystalline networks of chemically bonded atoms or ions, but are not made of discrete molecules.\n\nMolecules are held together by either covalent bonding or ionic bonding. Several types of non-metal elements exist only as molecules in the environment. For example, hydrogen only exists as hydrogen molecule. A molecule of a compound is made out of two or more elements.\n\nA covalent bond is a chemical bond that involves the sharing of electron pairs between atoms. These electron pairs are termed \"shared pairs\" or \"bonding pairs\", and the stable balance of attractive and repulsive forces between atoms, when they share electrons, is termed \"covalent bonding\".\n\nIonic bonding is a type of chemical bond that involves the electrostatic attraction between oppositely charged ions, and is the primary interaction occurring in ionic compounds. The ions are atoms that have lost one or more electrons (termed cations) and atoms that have gained one or more electrons (termed anions). This transfer of electrons is termed \"electrovalence\" in contrast to covalence. In the simplest case, the cation is a metal atom and the anion is a nonmetal atom, but these ions can be of a more complicated nature, e.g. molecular ions like NH or SO. Basically, an ionic bond is the transfer of electrons from a metal to a non-metal for both atoms to obtain a full valence shell.\n\nMost molecules are far too small to be seen with the naked eye, but there are exceptions. DNA, a macromolecule, can reach macroscopic sizes, as can molecules of many polymers. Molecules commonly used as building blocks for organic synthesis have a dimension of a few angstroms (Å) to several dozen Å, or around one billionth of a meter. Single molecules cannot usually be observed by light (as noted above), but small molecules and even the outlines of individual atoms may be traced in some circumstances by use of an atomic force microscope. Some of the largest molecules are macromolecules or supermolecules.\n\nThe smallest molecule is the diatomic hydrogen (H), with a bond length of 0.74 Å.\n\nEffective molecular radius is the size a molecule displays in solution.\nThe table of permselectivity for different substances contains examples.\n\nThe chemical formula for a molecule uses one line of chemical element symbols, numbers, and sometimes also other symbols, such as parentheses, dashes, brackets, and \"plus\" (+) and \"minus\" (−) signs. These are limited to one typographic line of symbols, which may include subscripts and superscripts.\n\nA compound's empirical formula is a very simple type of chemical formula. It is the simplest integer ratio of the chemical elements that constitute it. For example, water is always composed of a 2:1 ratio of hydrogen to oxygen atoms, and ethyl alcohol or ethanol is always composed of carbon, hydrogen, and oxygen in a 2:6:1 ratio. However, this does not determine the kind of molecule uniquely – dimethyl ether has the same ratios as ethanol, for instance. Molecules with the same atoms in different arrangements are called isomers. Also carbohydrates, for example, have the same ratio (carbon:hydrogen:oxygen= 1:2:1) (and thus the same empirical formula) but different total numbers of atoms in the molecule.\n\nThe molecular formula reflects the exact number of atoms that compose the molecule and so characterizes different molecules. However different isomers can have the same atomic composition while being different molecules.\n\nThe empirical formula is often the same as the molecular formula but not always. For example, the molecule acetylene has molecular formula CH, but the simplest integer ratio of elements is CH.\n\nThe molecular mass can be calculated from the chemical formula and is expressed in conventional atomic mass units equal to 1/12 of the mass of a neutral carbon-12 (C isotope) atom. For network solids, the term formula unit is used in stoichiometric calculations.\n\nFor molecules with a complicated 3-dimensional structure, especially involving atoms bonded to four different substituents, a simple molecular formula or even semi-structural chemical formula may not be enough to completely specify the molecule. In this case, a graphical type of formula called a structural formula may be needed. Structural formulas may in turn be represented with a one-dimensional chemical name, but such chemical nomenclature requires many words and terms which are not part of chemical formulas.\n\nMolecules have fixed equilibrium geometries—bond lengths and angles— about which they continuously oscillate through vibrational and rotational motions. A pure substance is composed of molecules with the same average geometrical structure. The chemical formula and the structure of a molecule are the two important factors that determine its properties, particularly its reactivity. Isomers share a chemical formula but normally have very different properties because of their different structures. Stereoisomers, a particular type of isomer, may have very similar physico-chemical properties and at the same time different biochemical activities.\n\nMolecular spectroscopy deals with the response (spectrum) of molecules interacting with probing signals of known energy (or frequency, according to Planck's formula). Molecules have quantized energy levels that can be analyzed by detecting the molecule's energy exchange through absorbance or emission.\nSpectroscopy does not generally refer to diffraction studies where particles such as neutrons, electrons, or high energy X-rays interact with a regular arrangement of molecules (as in a crystal).\n\nMicrowave spectroscopy commonly measures changes in the rotation of molecules, and can be used to identify molecules in outer space. Infrared spectroscopy measures changes in vibration of molecules, including stretching, bending or twisting motions. It is commonly used to identify the kinds of bonds or functional groups in molecules. Changes in the arrangements of electrons yield absorption or emission lines in ultraviolet, visible or near infrared light, and result in colour. Nuclear resonance spectroscopy actually measures the environment of particular nuclei in the molecule, and can be used to characterise the numbers of atoms in different positions in a molecule.\n\nThe study of molecules by molecular physics and theoretical chemistry is largely based on quantum mechanics and is essential for the understanding of the chemical bond. The simplest of molecules is the hydrogen molecule-ion, H, and the simplest of all the chemical bonds is the one-electron bond. H is composed of two positively charged protons and one negatively charged electron, which means that the Schrödinger equation for the system can be solved more easily due to the lack of electron–electron repulsion. With the development of fast digital computers, approximate solutions for more complicated molecules became possible and are one of the main aspects of computational chemistry.\n\nWhen trying to define rigorously whether an arrangement of atoms is \"sufficiently stable\" to be considered a molecule, IUPAC suggests that it \"must correspond to a depression on the potential energy surface that is deep enough to confine at least one vibrational state\". This definition does not depend on the nature of the interaction between the atoms, but only on the strength of the interaction. In fact, it includes weakly bound species that would not traditionally be considered molecules, such as the helium dimer, He, which has one vibrational bound state and is so loosely bound that it is only likely to be observed at very low temperatures.\n\nWhether or not an arrangement of atoms is \"sufficiently stable\" to be considered a molecule is inherently an operational definition. Philosophically, therefore, a molecule is not a fundamental entity (in contrast, for instance, to an elementary particle); rather, the concept of a molecule is the chemist's way of making a useful statement about the strengths of atomic-scale interactions in the world that we observe.\n\n\n"}
{"id": "558685", "url": "https://en.wikipedia.org/wiki?curid=558685", "title": "Natural environment", "text": "Natural environment\n"}
{"id": "21830", "url": "https://en.wikipedia.org/wiki?curid=21830", "title": "Nature", "text": "Nature\n\nNature, in the broadest sense, is the natural, physical, or material world or universe. \"Nature\" can refer to the phenomena of the physical world, and also to life in general. The study of nature is a large, if not the only, part of science. Although humans are part of nature, human activity is often understood as a separate category from other natural phenomena.\n\nThe word \"nature\" is derived from the Latin word \"natura\", or \"essential qualities, innate disposition\", and in ancient times, literally meant \"birth\". \"Natura\" is a Latin translation of the Greek word \"physis\" (φύσις), which originally related to the intrinsic characteristics that plants, animals, and other features of the world develop of their own accord. The concept of nature as a whole, the physical universe, is one of several expansions of the original notion; it began with certain core applications of the word φύσις by pre-Socratic philosophers, and has steadily gained currency ever since. This usage continued during the advent of modern scientific method in the last several centuries.\n\nWithin the various uses of the word today, \"nature\" often refers to geology and wildlife. Nature can refer to the general realm of living plants and animals, and in some cases to the processes associated with inanimate objects—the way that particular types of things exist and change of their own accord, such as the weather and geology of the Earth. It is often taken to mean the \"natural environment\" or wilderness—wild animals, rocks, forest, and in general those things that have not been substantially altered by human intervention, or which persist despite human intervention. For example, manufactured objects and human interaction generally are not considered part of nature, unless qualified as, for example, \"human nature\" or \"the whole of nature\". This more traditional concept of natural things which can still be found today implies a distinction between the natural and the artificial, with the artificial being understood as that which has been brought into being by a human consciousness or a human mind. Depending on the particular context, the term \"natural\" might also be distinguished from the or the supernatural.\n\nEarth is the only planet known to support life, and its natural features are the subject of many fields of scientific research. Within the solar system, it is third closest to the sun; it is the largest terrestrial planet and the fifth largest overall. Its most prominent climatic features are its two large polar regions, two relatively narrow temperate zones, and a wide equatorial tropical to subtropical region. Precipitation varies widely with location, from several metres of water per year to less than a millimetre. 71 percent of the Earth's surface is covered by salt-water oceans. The remainder consists of continents and islands, with most of the inhabited land in the Northern Hemisphere.\n\nEarth has evolved through geological and biological processes that have left traces of the original conditions. The outer surface is divided into several gradually migrating tectonic plates. The interior remains active, with a thick layer of plastic mantle and an iron-filled core that generates a magnetic field. This iron core is composed of a solid inner phase, and a fluid outer phase. Convective motion in the core generates electric currents through dynamo action, and these, in turn, generate the geomagnetic field.\n\nThe atmospheric conditions have been significantly altered from the original conditions by the presence of life-forms, which create an ecological balance that stabilizes the surface conditions. Despite the wide regional variations in climate by latitude and other geographic factors, the long-term average global climate is quite stable during interglacial periods, and variations of a degree or two of average global temperature have historically had major effects on the ecological balance, and on the actual geography of the Earth.\n\nGeology is the science and study of the solid and liquid matter that constitutes the Earth. The field of geology encompasses the study of the composition, structure, physical properties, dynamics, and history of Earth materials, and the processes by which they are formed, moved, and changed. The field is a major academic discipline, and is also important for mineral and hydrocarbon extraction, knowledge about and mitigation of natural hazards, some Geotechnical engineering fields, and understanding past climates and environments.\n\nThe geology of an area evolves through time as rock units are deposited and inserted and deformational processes change their shapes and locations.\n\nRock units are first emplaced either by deposition onto the surface or intrude into the overlying rock. Deposition can occur when sediments settle onto the surface of the Earth and later lithify into sedimentary rock, or when as volcanic material such as volcanic ash or lava flows, blanket the surface. Igneous intrusions such as batholiths, laccoliths, dikes, and sills, push upwards into the overlying rock, and crystallize as they intrude.\n\nAfter the initial sequence of rocks has been deposited, the rock units can be deformed and/or metamorphosed. Deformation typically occurs as a result of horizontal shortening, horizontal extension, or side-to-side (strike-slip) motion. These structural regimes broadly relate to convergent boundaries, divergent boundaries, and transform boundaries, respectively, between tectonic plates.\n\nEarth is estimated to have formed 4.54 billion years ago from the solar nebula, along with the Sun and other planets. The moon formed roughly 20 million years later. Initially molten, the outer layer of the Earth cooled, resulting in the solid crust. Outgassing and volcanic activity produced the primordial atmosphere. Condensing water vapor, most or all of which came from ice delivered by comets, produced the oceans and other water sources. The highly energetic chemistry is believed to have produced a self-replicating molecule around 4 billion years ago.\n\nContinents formed, then broke up and reformed as the surface of Earth reshaped over hundreds of millions of years, occasionally combining to make a supercontinent. Roughly 750 million years ago, the earliest known supercontinent Rodinia, began to break apart. The continents later recombined to form Pannotia which broke apart about 540 million years ago, then finally Pangaea, which broke apart about 180 million years ago.\n\nDuring the Neoproterozoic era, freezing temperatures covered much of the Earth in glaciers and ice sheets. This hypothesis has been termed the \"Snowball Earth\", and it is of particular interest as it precedes the Cambrian explosion in which multicellular life forms began to proliferate about 530–540 million years ago.\n\nSince the Cambrian explosion there have been five distinctly identifiable mass extinctions. The last mass extinction occurred some 66 million years ago, when a meteorite collision probably triggered the extinction of the non-avian dinosaurs and other large reptiles, but spared small animals such as mammals. Over the past 66 million years, mammalian life diversified.\n\nSeveral million years ago, a species of small African ape gained the ability to stand upright. The subsequent advent of human life, and the development of agriculture and further civilization allowed humans to affect the Earth more rapidly than any previous life form, affecting both the nature and quantity of other organisms as well as global climate. By comparison, the Great Oxygenation Event, produced by the proliferation of algae during the Siderian period, required about 300 million years to culminate.\n\nThe present era is classified as part of a mass extinction event, the Holocene extinction event, the fastest ever to have occurred. Some, such as E. O. Wilson of Harvard University, predict that human destruction of the biosphere could cause the extinction of one-half of all species in the next 100 years. The extent of the current extinction event is still being researched, debated and calculated by biologists.\nThe Earth's atmosphere is a key factor in sustaining the ecosystem. The thin layer of gases that envelops the Earth is held in place by gravity. Air is mostly nitrogen, oxygen, water vapor, with much smaller amounts of carbon dioxide, argon, etc. The atmospheric pressure declines steadily with altitude. The ozone layer plays an important role in depleting the amount of ultraviolet (UV) radiation that reaches the surface. As DNA is readily damaged by UV light, this serves to protect life at the surface. The atmosphere also retains heat during the night, thereby reducing the daily temperature extremes.\n\nTerrestrial weather occurs almost exclusively in the lower part of the atmosphere, and serves as a convective system for redistributing heat. Ocean currents are another important factor in determining climate, particularly the major underwater thermohaline circulation which distributes heat energy from the equatorial oceans to the polar regions. These currents help to moderate the differences in temperature between winter and summer in the temperate zones. Also, without the redistributions of heat energy by the ocean currents and atmosphere, the tropics would be much hotter, and the polar regions much colder.\n\nWeather can have both beneficial and harmful effects. Extremes in weather, such as tornadoes or hurricanes and cyclones, can expend large amounts of energy along their paths, and produce devastation. Surface vegetation has evolved a dependence on the seasonal variation of the weather, and sudden changes lasting only a few years can have a dramatic effect, both on the vegetation and on the animals which depend on its growth for their food.\n\nClimate is a measure of the long-term trends in the weather. Various factors are known to influence the climate, including ocean currents, surface albedo, greenhouse gases, variations in the solar luminosity, and changes to the Earth's orbit. Based on historical records, the Earth is known to have undergone drastic climate changes in the past, including ice ages.\n\nThe climate of a region depends on a number of factors, especially latitude. A latitudinal band of the surface with similar climatic attributes forms a climate region. There are a number of such regions, ranging from the tropical climate at the equator to the polar climate in the northern and southern extremes. Weather is also influenced by the seasons, which result from the Earth's axis being tilted relative to its orbital plane. Thus, at any given time during the summer or winter, one part of the Earth is more directly exposed to the rays of the sun. This exposure alternates as the Earth revolves in its orbit. At any given time, regardless of season, the northern and southern hemispheres experience opposite seasons.\n\nWeather is a chaotic system that is readily modified by small changes to the environment, so accurate weather forecasting is limited to only a few days. Overall, two things are happening worldwide: (1) temperature is increasing on the average; and (2) regional climates have been undergoing noticeable changes.\n\nWater is a chemical substance that is composed of hydrogen and oxygen and is vital for all known forms of life. In typical usage, \"water\" refers only to its liquid form or state, but the substance also has a solid state, ice, and a gaseous state, water vapor, or steam. Water covers 71% of the Earth's surface. On Earth, it is found mostly in oceans and other large bodies of water, with 1.6% of water below ground in aquifers and 0.001% in the air as vapor, clouds, and precipitation. Oceans hold 97% of surface water, glaciers, and polar ice caps 2.4%, and other land surface water such as rivers, lakes, and ponds 0.6%. Additionally, a minute amount of the Earth's water is contained within biological bodies and manufactured products.\n\nAn ocean is a major body of saline water, and a principal component of the hydrosphere. Approximately 71% of the Earth's surface (an area of some 361 million square kilometers) is covered by ocean, a continuous body of water that is customarily divided into several principal oceans and smaller seas. More than half of this area is over deep. Average oceanic salinity is around 35 parts per thousand (ppt) (3.5%), and nearly all seawater has a salinity in the range of 30 to 38 ppt. Though generally recognized as several 'separate' oceans, these waters comprise one global, interconnected body of salt water often referred to as the World Ocean or global ocean. This concept of a global ocean as a continuous body of water with relatively free interchange among its parts is of fundamental importance to oceanography.\n\nThe major oceanic divisions are defined in part by the continents, various archipelagos, and other criteria: these divisions are (in descending order of size) the Pacific Ocean, the Atlantic Ocean, the Indian Ocean, the Southern Ocean, and the Arctic Ocean. Smaller regions of the oceans are called seas, gulfs, bays and other names. There are also salt lakes, which are smaller bodies of landlocked saltwater that are not interconnected with the World Ocean. Two notable examples of salt lakes are the Aral Sea and the Great Salt Lake.\n\nA lake (from Latin \"lacus\") is a terrain feature (or physical feature), a body of liquid on the surface of a world that is localized to the bottom of basin (another type of landform or terrain feature; that is, it is not global) and moves slowly if it moves at all. On Earth, a body of water is considered a lake when it is inland, not part of the ocean, is larger and deeper than a pond, and is fed by a river. The only world other than Earth known to harbor lakes is Titan, Saturn's largest moon, which has lakes of ethane, most likely mixed with methane. It is not known if Titan's lakes are fed by rivers, though Titan's surface is carved by numerous river beds. Natural lakes on Earth are generally found in mountainous areas, rift zones, and areas with ongoing or recent glaciation. Other lakes are found in endorheic basins or along the courses of mature rivers. In some parts of the world, there are many lakes because of chaotic drainage patterns left over from the last Ice Age. All lakes are temporary over geologic time scales, as they will slowly fill in with sediments or spill out of the basin containing them.\n\nA pond is a body of standing water, either natural or man-made, that is usually smaller than a lake. A wide variety of man-made bodies of water are classified as ponds, including water gardens designed for aesthetic ornamentation, fish ponds designed for commercial fish breeding, and solar ponds designed to store thermal energy. Ponds and lakes are distinguished from streams via current speed. While currents in streams are easily observed, ponds and lakes possess thermally driven micro-currents and moderate wind driven currents. These features distinguish a pond from many other aquatic terrain features, such as stream pools and tide pools.\n\nA river is a natural watercourse, usually freshwater, flowing toward an ocean, a lake, a sea or another river. In a few cases, a river simply flows into the ground or dries up completely before reaching another body of water. Small rivers may also be called by several other names, including stream, creek, brook, rivulet, and rill; there is no general rule that defines what can be called a river. Many names for small rivers are specific to geographic location; one example is \"Burn\" in Scotland and North-east England. Sometimes a river is said to be larger than a creek, but this is not always the case, due to vagueness in the language. A river is part of the hydrological cycle. Water within a river is generally collected from precipitation through surface runoff, groundwater recharge, springs, and the release of stored water in natural ice and snowpacks (i.e., from glaciers).\n\nA stream is a flowing body of water with a current, confined within a bed and stream banks. In the United States, a stream is classified as a watercourse less than wide. Streams are important as conduits in the water cycle, instruments in groundwater recharge, and they serve as corridors for fish and wildlife migration. The biological habitat in the immediate vicinity of a stream is called a riparian zone. Given the status of the ongoing Holocene extinction, streams play an important corridor role in connecting fragmented habitats and thus in conserving biodiversity. The study of streams and waterways in general involves many branches of inter-disciplinary natural science and engineering, including hydrology, fluvial geomorphology, aquatic ecology, fish biology, riparian ecology, and others.\n\nEcosystems are composed of a variety of abiotic and biotic components that function in an interrelated way. The structure and composition is determined by various environmental factors that are interrelated. Variations of these factors will initiate dynamic modifications to the ecosystem. Some of the more important components are: soil, atmosphere, radiation from the sun, water, and living organisms.\n\nCentral to the ecosystem concept is the idea that living organisms interact with every other element in their local environment. Eugene Odum, a founder of ecology, stated: \"Any unit that includes all of the organisms (ie: the \"community\") in a given area interacting with the physical environment so that a flow of energy leads to clearly defined trophic structure, biotic diversity, and material cycles (i.e.: exchange of materials between living and nonliving parts) within the system is an ecosystem.\" Within the ecosystem, species are connected and dependent upon one another in the food chain, and exchange energy and matter between themselves as well as with their environment. The human ecosystem concept is based on the human/nature dichotomy and the idea that all species are ecologically dependent on each other, as well as with the abiotic constituents of their biotope.\n\nA smaller unit of size is called a microecosystem. For example, a microsystem can be a stone and all the life under it. A \"macroecosystem\" might involve a whole ecoregion, with its drainage basin.\n\nWilderness is generally defined as areas that have not been significantly modified by human activity. Wilderness areas can be found in preserves, estates, farms, conservation preserves, ranches, , national parks, and even in urban areas along rivers, gulches, or otherwise undeveloped areas. Wilderness areas and protected parks are considered important for the survival of certain species, ecological studies, conservation, and solitude. Some nature writers believe wilderness areas are vital for the human spirit and creativity, and some ecologists consider wilderness areas to be an integral part of the Earth's self-sustaining natural ecosystem (the biosphere). They may also preserve historic genetic traits and that they provide habitat for wild flora and fauna that may be difficult or impossible to recreate in zoos, arboretums, or laboratories.\n\nAlthough there is no universal agreement on the definition of life, scientists generally accept that the biological manifestation of life is characterized by organization, metabolism, growth, adaptation, response to stimuli, and reproduction. Life may also be said to be simply the characteristic state of organisms.\n\nProperties common to terrestrial organisms (plants, animals, fungi, protists, archaea, and bacteria) are that they are cellular, carbon-and-water-based with complex organization, having a metabolism, a capacity to grow, respond to stimuli, and reproduce. An entity with these properties is generally considered life. However, not every definition of life considers all of these properties to be essential. Human-made analogs of life may also be considered to be life.\n\nThe biosphere is the part of Earth's outer shell – including land, surface rocks, water, air and the atmosphere – within which life occurs, and which biotic processes in turn alter or transform. From the broadest geophysiological point of view, the biosphere is the global ecological system integrating all living beings and their relationships, including their interaction with the elements of the lithosphere (rocks), hydrosphere (water), and atmosphere (air). The entire Earth contains over 75 billion tons (150 \"trillion\" pounds or about 6.8×10 kilograms) of biomass (life), which lives within various environments within the biosphere.\n\nOver nine-tenths of the total biomass on Earth is plant life, on which animal life depends very heavily for its existence. More than 2 million species of plant and animal life have been identified to date, and estimates of the actual number of existing species range from several million to well over 50 million. The number of individual species of life is constantly in some degree of flux, with new species appearing and others ceasing to exist on a continual basis. The total number of species is in rapid decline.\n\nThe origin of life on Earth is not well understood, but it is known to have occurred at least 3.5 billion years ago, during the hadean or archean eons on a primordial Earth that had a substantially different environment than is found at present. These life forms possessed the basic traits of self-replication and inheritable traits. Once life had appeared, the process of evolution by natural selection resulted in the development of ever-more diverse life forms.\n\nSpecies that were unable to adapt to the changing environment and competition from other life forms became extinct. However, the fossil record retains evidence of many of these older species. Current fossil and DNA evidence shows that all existing species can trace a continual ancestry back to the first primitive life forms.\n\nWhen basic forms of plant life developed the process of photosynthesis the sun's energy could be harvested to create conditions which allowed for more complex life forms. The resultant oxygen accumulated in the atmosphere and gave rise to the ozone layer. The incorporation of smaller cells within larger ones resulted in the development of yet more complex cells called eukaryotes. Cells within colonies became increasingly specialized, resulting in true multicellular organisms. With the ozone layer absorbing harmful ultraviolet radiation, life colonized the surface of Earth.\n\nThe first form of life to develop on the Earth were microbes, and they remained the only form of life until about a billion years ago when multi-cellular organisms began to appear. Microorganisms are single-celled organisms that are generally microscopic, and smaller than the human eye can see. They include Bacteria, Fungi, Archaea, and Protista.\n\nThese life forms are found in almost every location on the Earth where there is liquid water, including in the Earth's interior.\nTheir reproduction is both rapid and profuse. The combination of a high mutation rate and a horizontal gene transfer ability makes them highly adaptable, and able to survive in new environments, including outer space. They form an essential part of the planetary ecosystem. However, some microorganisms are pathogenic and can post health risk to other organisms.\n\nOriginally Aristotle divided all living things between plants, which generally do not move fast enough for humans to notice, and animals. In Linnaeus' system, these became the kingdoms Vegetabilia (later Plantae) and Animalia. Since then, it has become clear that the Plantae as originally defined included several unrelated groups, and the fungi and several groups of algae were removed to new kingdoms. However, these are still often considered plants in many contexts. Bacterial life is sometimes included in flora, and some classifications use the term \"bacterial flora\" separately from \"plant flora\".\n\nAmong the many ways of classifying plants are by regional floras, which, depending on the purpose of study, can also include \"fossil flora\", remnants<br> of plant life from a previous era. People in many regions and countries take great pride in their individual arrays of characteristic flora, which can vary widely across the globe due to differences in climate and terrain.\n\nRegional floras commonly are divided into categories such as \"native flora\" and \"agricultural and garden flora\", the lastly mentioned of which are intentionally grown and cultivated. Some types of \"native flora\" actually have been introduced centuries ago by people migrating from one region or continent to another, and become an integral part of the native, or natural flora of the place to which they were introduced. This is an example of how human interaction with nature can blur the boundary of what is considered nature.\n\nAnother category of plant has historically been carved out for \"weeds\". Though the term has fallen into disfavor among botanists as a formal way to categorize \"useless\" plants, the informal use of the word \"weeds\" to describe those plants that are deemed worthy of elimination is illustrative of the general tendency of people and societies to seek to alter or shape the course of nature. Similarly, animals are often categorized in ways such as \"domestic\", \"farm animals\", \"wild animals\", \"pests\", etc. according to their relationship to human life.\n\nAnimals as a category have several characteristics that generally set them apart from other living things. Animals are eukaryotic and usually multicellular (although see Myxozoa), which separates them from bacteria, archaea, and most protists. They are heterotrophic, generally digesting food in an internal chamber, which separates them from plants and algae. They are also distinguished from plants, algae, and fungi by lacking cell walls.\n\nWith a few exceptions—most notably the two phyla consisting of sponges and placozoans—animals have bodies that are differentiated into tissues. These include muscles, which are able to contract and control locomotion, and a nervous system, which sends and processes signals. There is also typically an internal digestive chamber. The eukaryotic cells possessed by all animals are surrounded by a characteristic extracellular matrix composed of collagen and elastic glycoproteins. This may be calcified to form structures like shells, bones, and spicules, a framework upon which cells can move about and be reorganized during development and maturation, and which supports the complex anatomy required for mobility.\n\nAlthough humans comprise only a minuscule proportion of the total living biomass on Earth, the human effect on nature is disproportionately large. Because of the extent of human influence, the boundaries between what humans regard as nature and \"made environments\" is not clear cut except at the extremes. Even at the extremes, the amount of natural environment that is free of discernible human influence is diminishing at an increasingly rapid pace.\n\nThe development of technology by the human race has allowed the greater exploitation of natural resources and has helped to alleviate some of the risk from natural hazards. In spite of this progress, however, the fate of human civilization remains closely linked to changes in the environment. There exists a highly complex feedback loop between the use of advanced technology and changes to the environment that are only slowly becoming understood. Man-made threats to the Earth's natural environment include pollution, deforestation, and disasters such as oil spills. Humans have contributed to the extinction of many plants and animals.\n\nHumans employ nature for both leisure and economic activities. The acquisition of natural resources for industrial use remains a sizable component of the world's economic system. Some activities, such as hunting and fishing, are used for both sustenance and leisure, often by different people. Agriculture was first adopted around the 9th millennium BCE. Ranging from food production to energy, nature influences economic wealth.\n\nAlthough early humans gathered uncultivated plant materials for food and employed the medicinal properties of vegetation for healing, most modern human use of plants is through agriculture. The clearance of large tracts of land for crop growth has led to a significant reduction in the amount available of forestation and wetlands, resulting in the loss of habitat for many plant and animal species as well as increased erosion.\n\nBeauty in nature has historically been a prevalent theme in art and books, filling large sections of libraries and bookstores. That nature has been depicted and celebrated by so much art, photography, poetry, and other literature shows the strength with which many people associate nature and beauty. Reasons why this association exists, and what the association consists of, are studied by the branch of philosophy called aesthetics. Beyond certain basic characteristics that many philosophers agree about to explain what is seen as beautiful, the opinions are virtually endless. Nature and wildness have been important subjects in various eras of world history. An early tradition of landscape art began in China during the Tang Dynasty (618–907). The tradition of representing nature \"as it is\" became one of the aims of Chinese painting and was a significant influence in Asian art.\n\nAlthough natural wonders are celebrated in the Psalms and the Book of Job, wilderness portrayals in art became more prevalent in the 1800s, especially in the works of the Romantic movement. British artists John Constable and J. M. W. Turner turned their attention to capturing the beauty of the natural world in their paintings. Before that, paintings had been primarily of religious scenes or of human beings. William Wordsworth's poetry described the wonder of the natural world, which had formerly been viewed as a threatening place. Increasingly the valuing of nature became an aspect of Western culture. This artistic movement also coincided with the Transcendentalist movement in the Western world. A common classical idea of beautiful art involves the word mimesis, the imitation of nature. Also in the realm of ideas about beauty in nature is that the perfect is implied through perfect mathematical forms and more generally by patterns in nature. As David Rothenburg writes, \"The beautiful is the root of science and the goal of art, the highest possibility that humanity can ever hope to see\".\n\nSome fields of science see nature as matter in motion, obeying certain laws of nature which science seeks to understand. For this reason the most fundamental science is generally understood to be \"physics\" – the name for which is still recognizable as meaning that it is the study of nature.\n\nMatter is commonly defined as the substance of which physical objects are composed. It constitutes the observable universe. The visible components of the universe are now believed to compose only 4.9 percent of the total mass. The remainder is believed to consist of 26.8 percent cold dark matter and 68.3 percent dark energy. The exact arrangement of these components is still unknown and is under intensive investigation by physicists.\n\nThe behavior of matter and energy throughout the observable universe appears to follow well-defined physical laws. These laws have been employed to produce cosmological models that successfully explain the structure and the evolution of the universe we can observe. The mathematical expressions of the laws of physics employ a set of twenty physical constants that appear to be static across the observable universe. The values of these constants have been carefully measured, but the reason for their specific values remains a mystery.\n\nOuter space, also simply called \"space\", refers to the relatively empty regions of the universe outside the atmospheres of celestial bodies. \"Outer\" space is used to distinguish it from airspace (and terrestrial locations). There is no discrete boundary between the Earth's atmosphere and space, as the atmosphere gradually attenuates with increasing altitude. Outer space within the Solar System is called interplanetary space, which passes over into interstellar space at what is known as the heliopause.\n\nOuter space is sparsely filled with several dozen types of organic molecules discovered to date by microwave spectroscopy, blackbody radiation left over from the big bang and the origin of the universe, and cosmic rays, which include ionized atomic nuclei and various subatomic particles. There is also some gas, plasma and dust, and small meteors. Additionally, there are signs of human life in outer space today, such as material left over from previous manned and unmanned launches which are a potential hazard to spacecraft. Some of this debris re-enters the atmosphere periodically.\n\nAlthough the Earth is the only body within the solar system known to support life, evidence suggests that in the distant past the planet Mars possessed bodies of liquid water on the surface. For a brief period in Mars' history, it may have also been capable of forming life. At present though, most of the water remaining on Mars is frozen.\nIf life exists at all on Mars, it is most likely to be located underground where liquid water can still exist.\n\nConditions on the other terrestrial planets, Mercury and Venus, appear to be too harsh to support life as we know it. But it has been conjectured that Europa, the fourth-largest moon of Jupiter, may possess a sub-surface ocean of liquid water and could potentially host life.\n\nAstronomers have started to discover extrasolar Earth analogs – planets that lie in the habitable zone of space surrounding a star, and therefore could possibly host life as we know it.\n\nMedia:\nOrganizations:\nPhilosophy:\n\n"}
{"id": "251399", "url": "https://en.wikipedia.org/wiki?curid=251399", "title": "Observable universe", "text": "Observable universe\n\nThe observable universe is a spherical region of the Universe comprising all matter that can be observed from Earth at the present time, because electromagnetic radiation from these objects has had time to reach Earth since the beginning of the cosmological expansion. There are at least 2 trillion galaxies in the observable universe. Assuming the Universe is isotropic, the distance to the edge of the observable universe is roughly the same in every direction. That is, the observable universe has a spherical volume (a ball) centered on the observer. Every location in the Universe has its own observable universe, which may or may not overlap with the one centered on Earth.\n\nThe word \"observable\" in this sense does not refer to the capability of modern technology to detect light or other information from an object, or whether there is anything to be detected. It refers to the physical limit created by the speed of light itself. Because no signals can travel faster than light, any object farther away from us than light could travel in the age of the Universe (estimated around years) simply cannot be detected, as they have not reached us yet. Sometimes astrophysicists distinguish between the \"visible\" universe, which includes only signals emitted since recombination—and the \"observable\" universe, which includes signals since the beginning of the cosmological expansion (the Big Bang in traditional physical cosmology, the end of the inflationary epoch in modern cosmology).\n\nAccording to calculations, the current \"comoving distance\"—proper distance, which takes into account that the universe has expanded since the light was emitted—to particles from which the cosmic microwave background radiation (CMBR) was emitted, which represent the radius of the visible universe, is about 14.0 billion parsecs (about 45.7 billion light-years), while the comoving distance to the edge of the observable universe is about 14.3 billion parsecs (about 46.6 billion light-years), about 2% larger. The radius of the observable universe is therefore estimated to be about 46.5 billion light-years and its diameter about 28.5 gigaparsecs (93 billion light-years, ). The total mass of ordinary matter in the universe can be calculated using the critical density and the diameter of the observable universe to be about 1.5×10 kg.\n\nSince the expansion of the universe is known to accelerate and will become exponential in the future, the light emitted from all distant objects, past some time dependent on their current redshift, will never reach the Earth. In the future all currently observable objects will slowly freeze in time while emitting progressively redder and fainter light. For instance, objects with the current redshift \"z\" from 5 to 10 will remain observable for no more than 4–6 billion years. In addition, light emitted by objects currently situated beyond a certain comoving distance (currently about 19 billion parsecs) will never reach Earth.\n\nSome parts of the universe are too far away for the light emitted since the Big Bang to have had enough time to reach Earth, and so lie outside the observable universe. In the future, light from distant galaxies will have had more time to travel, so additional regions will become observable. However, due to Hubble's law, regions sufficiently distant from the Earth are expanding away from it faster than the speed of light (special relativity prevents nearby objects in the same local region from moving faster than the speed of light with respect to each other, but there is no such constraint for distant objects when the space between them is expanding; see uses of the proper distance for a discussion) and furthermore the expansion rate appears to be accelerating due to dark energy. Assuming dark energy remains constant (an unchanging cosmological constant), so that the expansion rate of the universe continues to accelerate, there is a \"future visibility limit\" beyond which objects will \"never\" enter our observable universe at any time in the infinite future, because light emitted by objects outside that limit would never reach the Earth. (A subtlety is that, because the Hubble parameter is decreasing with time, there can be cases where a galaxy that is receding from the Earth just a bit faster than light does emit a signal that reaches the Earth eventually.) This future visibility limit is calculated at a comoving distance of 19 billion parsecs (62 billion light-years), assuming the universe will keep expanding forever, which implies the number of galaxies that we can ever theoretically observe in the infinite future (leaving aside the issue that some may be impossible to observe in practice due to redshift, as discussed in the following paragraph) is only larger than the number currently observable by a factor of 2.36.\n\nThough in principle more galaxies will become observable in the future, in practice an increasing number of galaxies will become extremely redshifted due to ongoing expansion, so much so that they will seem to disappear from view and become invisible. An additional subtlety is that a galaxy at a given comoving distance is defined to lie within the \"observable universe\" if we can receive signals emitted by the galaxy at any age in its past history (say, a signal sent from the galaxy only 500 million years after the Big Bang), but because of the universe's expansion, there may be some later age at which a signal sent from the same galaxy can \"never\" reach the Earth at any point in the infinite future (so, for example, we might never see what the galaxy looked like 10 billion years after the Big Bang), even though it remains at the same comoving distance (comoving distance is defined to be constant with time—unlike proper distance, which is used to define recession velocity due to the expansion of space), which is less than the comoving radius of the observable universe. This fact can be used to define a type of cosmic event horizon whose distance from the Earth changes over time. For example, the current distance to this horizon is about 16 billion light-years, meaning that a signal from an event happening \"at present\" can eventually reach the Earth in the future if the event is less than 16 billion light-years away, but the signal will never reach the Earth if the event is more than 16 billion light-years away.\n\nBoth popular and professional research articles in cosmology often use the term \"universe\" to mean \"observable universe\". This can be justified on the grounds that we can never know anything by direct experimentation about any part of the universe that is causally disconnected from the Earth, although many credible theories require a total universe much larger than the observable universe. No evidence exists to suggest that the boundary of the observable universe constitutes a boundary on the universe as a whole, nor do any of the mainstream cosmological models propose that the universe has any physical boundary in the first place, though some models propose it could be finite but unbounded, like a higher-dimensional analogue of the 2D surface of a sphere that is finite in area but has no edge. It is plausible that the galaxies within our observable universe represent only a minuscule fraction of the galaxies in the universe. According to the theory of cosmic inflation initially introduced by its founder, Alan Guth (and by D. Kazanas ), if it is assumed that inflation began about 10 seconds after the Big Bang, then with the plausible assumption that the size of the universe before the inflation occurred was approximately equal to the speed of light times its age, that would suggest that at present the entire universe's size is at least 3×10 times the radius of the observable universe. There are also lower estimates claiming that the entire universe is in excess of 250 times larger than the observable universe and also higher estimates implying that the universe is at least 10 times larger than the observable universe.\n\nIf the universe is finite but unbounded, it is also possible that the universe is \"smaller\" than the observable universe. In this case, what we take to be very distant galaxies may actually be duplicate images of nearby galaxies, formed by light that has circumnavigated the universe. It is difficult to test this hypothesis experimentally because different images of a galaxy would show different eras in its history, and consequently might appear quite different. Bielewicz et al. claims to establish a lower bound of 27.9 gigaparsecs (91 billion light-years) on the diameter of the last scattering surface (since this is only a lower bound, the paper leaves open the possibility that the whole universe is much larger, even infinite). This value is based on matching-circle analysis of the WMAP 7 year data. This approach has been disputed.\n\nThe comoving distance from Earth to the edge of the observable universe is about 14.26 gigaparsecs (46.5 billion light-years or ) in any direction. The observable universe is thus a sphere with a diameter of about 28.5 gigaparsecs (93 billion light-years or ). Assuming that space is roughly flat (in the sense of being a Euclidean space), this size corresponds to a comoving volume of about ( or ).\n\nThe figures quoted above are distances \"now\" (in cosmological time), not distances \"at the time the light was emitted\". For example, the cosmic microwave background radiation that we see right now was emitted at the time of photon decoupling, estimated to have occurred about years after the Big Bang, which occurred around 13.8 billion years ago. This radiation was emitted by matter that has, in the intervening time, mostly condensed into galaxies, and those galaxies are now calculated to be about 46 billion light-years from us. To estimate the distance to that matter at the time the light was emitted, we may first note that according to the Friedmann–Lemaître–Robertson–Walker metric, which is used to model the expanding universe, if at the present time we receive light with a redshift of \"z\", then the scale factor at the time the light was originally emitted is given by\n\nformula_1.\n\nWMAP nine-year results combined with other measurements give the redshift of photon decoupling as \"z\" = , which implies that the scale factor at the time of photon decoupling would be . So if the matter that originally emitted the oldest CMBR photons has a \"present\" distance of 46 billion light-years, then at the time of decoupling when the photons were originally emitted, the distance would have been only about 42 \"million\" light-years.\n\nMany secondary sources have reported a wide variety of incorrect figures for the size of the visible universe. Some of these figures are listed below, with brief descriptions of possible reasons for misconceptions about them.\n\n\n\n\n\n\n\nSky surveys and mappings of the various wavelength bands of electromagnetic radiation (in particular 21-cm emission) have yielded much information on the content and character of the universe's structure. The organization of structure appears to follow as a hierarchical model with organization up to the scale of superclusters and filaments. Larger than this (at scales between 30 and 200 megaparsecs), there seems to be no continued structure, a phenomenon that has been referred to as the \"End of Greatness\".\n\nThe organization of structure arguably begins at the stellar level, though most cosmologists rarely address astrophysics on that scale. Stars are organized into galaxies, which in turn form galaxy groups, galaxy clusters, superclusters, sheets, walls and filaments, which are separated by immense voids, creating a vast foam-like structure sometimes called the \"cosmic web\". Prior to 1989, it was commonly assumed that virialized galaxy clusters were the largest structures in existence, and that they were distributed more or less uniformly throughout the universe in every direction. However, since the early 1980s, more and more structures have been discovered. In 1983, Adrian Webster identified the Webster LQG, a large quasar group consisting of 5 quasars. The discovery was the first identification of a large-scale structure, and has expanded the information about the known grouping of matter in the universe. In 1987, Robert Brent Tully identified the Pisces–Cetus Supercluster Complex, the galaxy filament in which the Milky Way resides. It is about 1 billion light-years across. That same year, an unusually large region with a much lower than average distribution of galaxies was discovered, the Giant Void, which measures 1.3 billion light-years across. Based on redshift survey data, in 1989 Margaret Geller and John Huchra discovered the \"Great Wall\", a sheet of galaxies more than 500 million light-years long and 200 million light-years wide, but only 15 million light-years thick. The existence of this structure escaped notice for so long because it requires locating the position of galaxies in three dimensions, which involves combining location information about the galaxies with distance information from redshifts.\nTwo years later, astronomers Roger G. Clowes and Luis E. Campusano discovered the Clowes–Campusano LQG, a large quasar group measuring two billion light-years at its widest point which was the largest known structure in the universe at the time of its announcement. In April 2003, another large-scale structure was discovered, the Sloan Great Wall. In August 2007, a possible supervoid was detected in the constellation Eridanus. It coincides with the 'CMB cold spot', a cold region in the microwave sky that is highly improbable under the currently favored cosmological model. This supervoid could cause the cold spot, but to do so it would have to be improbably big, possibly a billion light-years across, almost as big as the Giant Void mentioned above.\n\nAnother large-scale structure is the SSA22 Protocluster, a collection of galaxies and enormous gas bubbles that measures about 200 million light-years across.\n\nIn 2011, a large quasar group was discovered, U1.11, measuring about 2.5 billion light-years across. On January 11, 2013, another large quasar group, the Huge-LQG, was discovered, which was measured to be four billion light-years across, the largest known structure in the universe at that time. In November 2013, astronomers discovered the Hercules–Corona Borealis Great Wall, an even bigger structure twice as large as the former. It was defined by the mapping of gamma-ray bursts.\n\nThe \"End of Greatness\" is an observational scale discovered at roughly 100 Mpc (roughly 300 million light-years) where the lumpiness seen in the large-scale structure of the universe is homogenized and isotropized in accordance with the Cosmological Principle. At this scale, no pseudo-random fractalness is apparent.\nThe superclusters and filaments seen in smaller surveys are randomized to the extent that the smooth distribution of the universe is visually apparent. It was not until the redshift surveys of the 1990s were completed that this scale could accurately be observed.\n\nAnother indicator of large-scale structure is the 'Lyman-alpha forest'. This is a collection of absorption lines that appear in the spectra of light from quasars, which are interpreted as indicating the existence of huge thin sheets of intergalactic (mostly hydrogen) gas. These sheets appear to be associated with the formation of new galaxies.\n\nCaution is required in describing structures on a cosmic scale because things are often different from how they appear. Gravitational lensing (bending of light by gravitation) can make an image appear to originate in a different direction from its real source. This is caused when foreground objects (such as galaxies) curve surrounding spacetime (as predicted by general relativity), and deflect passing light rays. Rather usefully, strong gravitational lensing can sometimes magnify distant galaxies, making them easier to detect. Weak lensing (gravitational shear) by the intervening universe in general also subtly changes the observed large-scale structure. \n\nThe large-scale structure of the universe also looks different if one only uses redshift to measure distances to galaxies. For example, galaxies behind a galaxy cluster are attracted to it, and so fall towards it, and so are slightly blueshifted (compared to how they would be if there were no cluster) On the near side, things are slightly redshifted. Thus, the environment of the cluster looks a bit squashed if using redshifts to measure distance. An opposite effect works on the galaxies already within a cluster: the galaxies have some random motion around the cluster center, and when these random motions are converted to redshifts, the cluster appears elongated. This creates a \"finger of God\"—the illusion of a long chain of galaxies pointed at the Earth.\n\nAt the centre of the Hydra-Centaurus Supercluster, a gravitational anomaly called the Great Attractor affects the motion of galaxies over a region hundreds of millions of light-years across. These galaxies are all redshifted, in accordance with Hubble's law. This indicates that they are receding from us and from each other, but the variations in their redshift are sufficient to reveal the existence of a concentration of mass equivalent to tens of thousands of galaxies.\n\nThe Great Attractor, discovered in 1986, lies at a distance of between 150 million and 250 million light-years (250 million is the most recent estimate), in the direction of the Hydra and Centaurus constellations. In its vicinity there is a preponderance of large old galaxies, many of which are colliding with their neighbours, or radiating large amounts of radio waves.\n\nIn 1987, astronomer R. Brent Tully of the University of Hawaii's Institute of Astronomy identified what he called the Pisces–Cetus Supercluster Complex, a structure one billion light-years long and 150 million light-years across in which, he claimed, the Local Supercluster was embedded.\n\nThe mass of the observable universe is often quoted as 10 tonnes or 10 kg. In this context, mass refers to ordinary matter and includes the interstellar medium (ISM) and the intergalactic medium (IGM). However, it excludes dark matter and dark energy. This quoted value for the mass of ordinary matter in the universe can be estimated based on critical density. The calculations are for the observable universe only as the volume of the whole is unknown and may be infinite.\nCritical density is the energy density for which the universe is flat. If there is no dark energy, it is also the density for which the expansion of the universe is poised between continued expansion and collapse. From the Friedmann equations, the value for formula_2 critical density, is:\n\nwhere \"G\" is the gravitational constant and H = \"H\" is the present value of the Hubble constant. The current value for \"H\", due to the European Space Agency's Planck Telescope, is \"H\" = 67.15 kilometers per second per mega parsec. This gives a critical density of (commonly quoted as about 5 hydrogen atoms per cubic meter). This density includes four significant types of energy/mass: ordinary matter (4.8%), neutrinos (0.1%), cold dark matter (26.8%), and dark energy (68.3%). Note that although neutrinos are Standard Model particles, they are listed separately because they are difficult to detect and so different from ordinary matter. The density of ordinary matter, as measured by Planck, is 4.8% of the total critical density or . To convert this density to mass we must multiply by volume, a value based on the radius of the \"observable universe\". Since the universe has been expanding for 13.8 billion years, the comoving distance (radius) is now about 46.6 billion light-years. Thus, volume (\"πr\") equals and the mass of ordinary matter equals density () times volume () or .\n\nAssuming the mass of ordinary matter is about (refer to previous section) and assuming all atoms are hydrogen atoms (which in reality make up about 74% of all atoms in our galaxy by mass, see Abundance of the chemical elements), calculating the estimated total number of atoms in the observable universe is straightforward. Divide the mass of ordinary matter by the mass of a hydrogen atom ( divided by ). The result is approximately 10 hydrogen atoms.\n\nThe most distant astronomical object yet announced as of 2016 is a galaxy classified GN-z11. In 2009, a gamma ray burst, GRB 090423, was found to have a redshift of 8.2, which indicates that the collapsing star that caused it exploded when the universe was only 630 million years old. The burst happened approximately 13 billion years ago, so a distance of about 13 billion light-years was widely quoted in the media (or sometimes a more precise figure of 13.035 billion light-years), though this would be the \"light travel distance\" (\"see\" Distance measures (cosmology)) rather than the \"proper distance\" used in both Hubble's law and in defining the size of the observable universe (cosmologist Ned Wright argues against the common use of light travel distance in astronomical press releases on this page, and at the bottom of the page offers online calculators that can be used to calculate the current proper distance to a distant object in a flat universe based on either the redshift \"z\" or the light travel time). The proper distance for a redshift of 8.2 would be about 9.2 Gpc, or about 30 billion light-years. Another record-holder for most distant object is a galaxy observed through and located beyond Abell 2218, also with a light travel distance of approximately 13 billion light-years from Earth, with observations from the Hubble telescope indicating a redshift between 6.6 and 7.1, and observations from Keck telescopes indicating a redshift towards the upper end of this range, around 7. The galaxy's light now observable on Earth would have begun to emanate from its source about 750 million years after the Big Bang.\n\nThe limit of observability in our universe is set by a set of cosmological horizons which limit—based on various physical constraints—the extent to which we can obtain information about various events in the universe. The most famous horizon is the particle horizon which sets a limit on the precise distance that can be seen due to the finite age of the universe. Additional horizons are associated with the possible future extent of observations (larger than the particle horizon owing to the expansion of space), an \"optical horizon\" at the surface of last scattering, and associated horizons with the surface of last scattering for neutrinos and gravitational waves.\n\n\n"}
{"id": "177602", "url": "https://en.wikipedia.org/wiki?curid=177602", "title": "Outer space", "text": "Outer space\n\nOuter space, or just space, is the expanse that exists beyond the Earth and between celestial bodies. Outer space is not completely empty—it is a hard vacuum containing a low density of particles, predominantly a plasma of hydrogen and helium as well as electromagnetic radiation, magnetic fields, neutrinos, dust, and cosmic rays. The baseline temperature, as set by the background radiation from the Big Bang, is . The plasma between galaxies accounts for about half of the baryonic (ordinary) matter in the universe; it has a number density of less than one hydrogen atom per cubic metre and a temperature of millions of kelvins; local concentrations of this plasma have condensed into stars and galaxies. Studies indicate that 90% of the mass in most galaxies is in an unknown form, called dark matter, which interacts with other matter through gravitational but not electromagnetic forces. Observations suggest that the majority of the mass-energy in the observable universe is a poorly understood vacuum energy of space, which astronomers label \"dark energy\". Intergalactic space takes up most of the volume of the Universe, but even galaxies and star systems consist almost entirely of empty space.\n\nOuter space does not begin at a definite altitude above the Earth's surface. However, the Kármán line, an altitude of above sea level, is conventionally used as the start of outer space in space treaties and for aerospace records keeping. The framework for international space law was established by the Outer Space Treaty, which entered into force on 10 October 1967. This treaty precludes any claims of national sovereignty and permits all states to freely explore outer space. Despite the drafting of UN resolutions for the peaceful uses of outer space, anti-satellite weapons have been tested in Earth orbit.\n\nHumans began the physical exploration of space during the 20th century with the advent of high-altitude balloon flights, followed by manned rocket launches. Earth orbit was first achieved by Yuri Gagarin of the Soviet Union in 1961, and unmanned spacecraft have since reached all of the known planets in the Solar System. Due to the high cost of getting into space, manned spaceflight has been limited to low Earth orbit and the Moon.\n\nOuter space represents a challenging environment for human exploration because of the hazards of vacuum and radiation. Microgravity also has a negative effect on human physiology that causes both muscle atrophy and bone loss. In addition to these health and environmental issues, the economic cost of putting objects, including humans, into space is very high.\n\nIn 350 BCE, Greek philosopher Aristotle suggested that \"nature abhors a vacuum\", a principle that became known as the \"horror vacui\". This concept built upon a 5th-century BCE ontological argument by the Greek philosopher Parmenides, who denied the possible existence of a void in space. Based on this idea that a vacuum could not exist, in the West it was widely held for many centuries that space could not be empty. As late as the 17th century, the French philosopher René Descartes argued that the entirety of space must be filled.\n\nIn ancient China, the 2nd-century astronomer Zhang Heng became convinced that space must be infinite, extending well beyond the mechanism that supported the Sun and the stars. The surviving books of the Hsüan Yeh school said that the heavens were boundless, \"empty and void of substance\". Likewise, the \"sun, moon, and the company of stars float in the empty space, moving or standing still\".\n\nThe Italian scientist Galileo Galilei knew that air had mass and so was subject to gravity. In 1640, he demonstrated that an established force resisted the formation of a vacuum. However, it would remain for his pupil Evangelista Torricelli to create an apparatus that would produce a partial vacuum in 1643. This experiment resulted in the first mercury barometer and created a scientific sensation in Europe. The French mathematician Blaise Pascal reasoned that if the column of mercury was supported by air, then the column ought to be shorter at higher altitude where the air pressure is lower. In 1648, his brother-in-law, Florin Périer, repeated the experiment on the Puy de Dôme mountain in central France and found that the column was shorter by three inches. This decrease in pressure was further demonstrated by carrying a half-full balloon up a mountain and watching it gradually expand, then contract upon descent.\n\nIn 1650, German scientist Otto von Guericke constructed the first vacuum pump: a device that would further refute the principle of \"horror vacui\". He correctly noted that the atmosphere of the Earth surrounds the planet like a shell, with the density gradually declining with altitude. He concluded that there must be a vacuum between the Earth and the Moon.\n\nBack in the 15th century, German theologian Nicolaus Cusanus speculated that the Universe lacked a center and a circumference. He believed that the Universe, while not infinite, could not be held as finite as it lacked any bounds within which it could be contained. These ideas led to speculations as to the infinite dimension of space by the Italian philosopher Giordano Bruno in the 16th century. He extended the Copernican heliocentric cosmology to the concept of an infinite Universe filled with a substance he called aether, which did not resist the motion of heavenly bodies. English philosopher William Gilbert arrived at a similar conclusion, arguing that the stars are visible to us only because they are surrounded by a thin aether or a void. This concept of an aether originated with ancient Greek philosophers, including Aristotle, who conceived of it as the medium through which the heavenly bodies move.\n\nThe concept of a Universe filled with a luminiferous aether remained in vogue among some scientists until the early 20th century. This form of aether was viewed as the medium through which light could propagate. In 1887, the Michelson–Morley experiment tried to detect the Earth's motion through this medium by looking for changes in the speed of light depending on the direction of the planet's motion. However, the null result indicated something was wrong with the concept. The idea of the luminiferous aether was then abandoned. It was replaced by Albert Einstein's theory of special relativity, which holds that the speed of light in a vacuum is a fixed constant, independent of the observer's motion or frame of reference.\n\nThe first professional astronomer to support the concept of an infinite Universe was the Englishman Thomas Digges in 1576. But the scale of the Universe remained unknown until the first successful measurement of the distance to a nearby star in 1838 by the German astronomer Friedrich Bessel. He showed that the star 61 Cygni had a parallax of just 0.31 arcseconds (compared to the modern value of 0.287″). This corresponds to a distance of over 10 light years. In 1917, Heber Curtis noted that novae in spiral nebulae were, on average, 10 magnitudes fainter than galactic novae, suggesting that the former are 100 times further away. The distance to the Andromeda Galaxy was determined in 1923 by American astronomer Edwin Hubble by measuring the brightness of cepheid variables in that galaxy, a new technique discovered by Henrietta Leavitt. This established that the Andromeda galaxy, and by extension all galaxies, lay well outside the Milky Way.\n\nThe earliest known estimate of the temperature of outer space was by the Swiss physicist Charles É. Guillaume in 1896. Using the estimated radiation of the background stars, he concluded that space must be heated to a temperature of 5–6 K. British physicist Arthur Eddington made a similar calculation to derive a temperature of 3.18 K in 1926. German physicist Erich Regener used the total measured energy of cosmic rays to estimate an intergalactic temperature of 2.8 K in 1933.\n\nThe modern concept of outer space is based on the \"Big Bang\" cosmology, first proposed in 1931 by the Belgian physicist Georges Lemaître. This theory holds that the universe originated from a very dense form that has since undergone continuous expansion. The background energy released during the initial expansion has steadily decreased in density, leading to a 1948 prediction by American physicists Ralph Alpher and Robert Herman of a temperature of 5 K for the temperature of space.\n\nThe term \"outward space\" was used in 1842 by the English poet Lady Emmeline Stuart-Wortley in her poem \"The Maiden of Moscow\". The expression \"outer space\" was used as an astronomical term by Alexander von Humboldt in 1845. It was later popularized in the writings of H. G. Wells in 1901. The shorter term \"space\" is older, first used to mean the region beyond Earth's sky in John Milton's \"Paradise Lost\" in 1667.\n\nAccording to the Big Bang theory, the very early Universe was an extremely hot and dense state about 13.8 billion years ago which rapidly expanded. About 380,000 years later the Universe had cooled sufficiently to allow protons and electrons to combine and form hydrogen—the so-called recombination epoch. When this happened, matter and energy became decoupled, allowing photons to travel freely through the continually expanding space. Matter that remained following the initial expansion has since undergone gravitational collapse to create stars, galaxies and other astronomical objects, leaving behind a deep vacuum that forms what is now called outer space. As light has a finite velocity, this theory also constrains the size of the directly observable universe. This leaves open the question as to whether the Universe is finite or infinite.\n\nThe present day shape of the universe has been determined from measurements of the cosmic microwave background using satellites like the Wilkinson Microwave Anisotropy Probe. These observations indicate that the spatial geometry of the observable universe is \"flat\", meaning that photons on parallel paths at one point remain parallel as they travel through space to the limit of the observable universe, except for local gravity. The flat Universe, combined with the measured mass density of the Universe and the accelerating expansion of the Universe, indicates that space has a non-zero vacuum energy, which is called dark energy.\n\nEstimates put the average energy density of the present day Universe at the equivalent of 5.9 protons per cubic meter, including dark energy, dark matter, and baryonic matter (ordinary matter composed of atoms). The atoms account for only 4.6% of the total energy density, or a density of one proton per four cubic meters. The density of the Universe, however, is clearly not uniform; it ranges from relatively high density in galaxies—including very high density in structures within galaxies, such as planets, stars, and black holes—to conditions in vast voids that have much lower density, at least in terms of visible matter. Unlike matter and dark matter, dark energy seems not to be concentrated in galaxies: although dark energy may account for a majority of the mass-energy in the Universe, dark energy's influence is 5 orders of magnitude smaller than the influence of gravity from matter and dark matter within the Milky Way.\n\nOuter space is the closest known approximation to a perfect vacuum. It has effectively no friction, allowing stars, planets, and moons to move freely along their ideal orbits, following the initial formation stage. However, even the deep vacuum of intergalactic space is not devoid of matter, as it contains a few hydrogen atoms per cubic meter. By comparison, the air humans breathe contains about 10 molecules per cubic meter. The low density of matter in outer space means that electromagnetic radiation can travel great distances without being scattered: the mean free path of a photon in intergalactic space is about 10 km, or 10 billion light years. In spite of this, extinction, which is the absorption and scattering of photons by dust and gas, is an important factor in galactic and intergalactic astronomy.\n\nStars, planets, and moons retain their atmospheres by gravitational attraction. Atmospheres have no clearly delineated upper boundary: the density of atmospheric gas gradually decreases with distance from the object until it becomes indistinguishable from outer space. The Earth's atmospheric pressure drops to about Pa at of altitude, compared to 100,000 Pa for the International Union of Pure and Applied Chemistry (IUPAC) definition of standard pressure. Above this altitude, isotropic gas pressure rapidly becomes insignificant when compared to radiation pressure from the Sun and the dynamic pressure of the solar wind. The thermosphere in this range has large gradients of pressure, temperature and composition, and varies greatly due to space weather.\n\nThe temperature of outer space is measured in terms of the kinetic activity of the gas, as it is on Earth. However, the radiation of outer space has a different temperature than the kinetic temperature of the gas, meaning that the gas and radiation are not in thermodynamic equilibrium. All of the observable universe is filled with photons that were created during the Big Bang, which is known as the cosmic microwave background radiation (CMB). (There is quite likely a correspondingly large number of neutrinos called the cosmic neutrino background.) The current black body temperature of the background radiation is about . The gas temperatures in outer space are always at least the temperature of the CMB but can be much higher. For example, the corona of the Sun reaches temperatures over 1.2–2.6 million K.\n\nMagnetic fields have been detected in the space around just about every class of celestial object. Star formation in spiral galaxies can generate small-scale dynamos, creating turbulent magnetic field strengths of around 5–10 μG. The Davis–Greenstein effect causes elongated dust grains to align themselves with a galaxy's magnetic field, resulting in weak optical polarization. This has been used to show ordered magnetic fields exist in several nearby galaxies. Magneto-hydrodynamic processes in active elliptical galaxies produce their characteristic jets and radio lobes. Non-thermal radio sources have been detected even among the most distant, high-z sources, indicating the presence of magnetic fields.\n\nOutside a protective atmosphere and magnetic field, there are few obstacles to the passage through space of energetic subatomic particles known as cosmic rays. These particles have energies ranging from about 10 eV up to an extreme 10 eV of ultra-high-energy cosmic rays. The peak flux of cosmic rays occurs at energies of about 10 eV, with approximately 87% protons, 12% helium nuclei and 1% heavier nuclei. In the high energy range, the flux of electrons is only about 1% of that of protons. Cosmic rays can damage electronic components and pose a health threat to space travelers. According to astronauts, like Don Pettit, space has a burned/metallic odor that clings to their suits and equipment, similar to the scent of an arc welding torch.\n\nDespite the harsh environment, several life forms have been found that can withstand extreme space conditions for extended periods. Species of lichen carried on the ESA BIOPAN facility survived exposure for ten days in 2007. Seeds of \"Arabidopsis thaliana\" and \"Nicotiana tabacum\" germinated after being exposed to space for 1.5 years. A strain of \"bacillus subtilis\" has survived 559 days when exposed to low-Earth orbit or a simulated martian environment. The lithopanspermia hypothesis suggests that rocks ejected into outer space from life-harboring planets may successfully transport life forms to another habitable world. A conjecture is that just such a scenario occurred early in the history of the Solar System, with potentially microorganism-bearing rocks being exchanged between Venus, Earth, and Mars.\n\nEven at relatively low altitudes in the Earth's atmosphere, conditions are hostile to the human body. The altitude where atmospheric pressure matches the vapor pressure of water at the temperature of the human body is called the Armstrong line, named after American physician Harry G. Armstrong. It is located at an altitude of around . At or above the Armstrong line, fluids in the throat and lungs boil away. More specifically, exposed bodily liquids such as saliva, tears, and liquids in the lungs boil away. Hence, at this altitude, human survival requires a pressure suit, or a pressurized capsule.\n\nOnce in space, sudden exposure of unprotected humans to very low pressure, such as during a rapid decompression, can cause pulmonary barotrauma—a rupture of the lungs, due to the large pressure differential between inside and outside the chest. Even if the subject's airway is fully open, the flow of air through the windpipe may be too slow to prevent the rupture. Rapid decompression can rupture eardrums and sinuses, bruising and blood seep can occur in soft tissues, and shock can cause an increase in oxygen consumption that leads to hypoxia.\n\nAs a consequence of rapid decompression, oxygen dissolved in the blood empties into the lungs to try to equalize the partial pressure gradient. Once the deoxygenated blood arrives at the brain, humans lose consciousness after a few seconds and die of hypoxia within minutes. Blood and other body fluids boil when the pressure drops below 6.3 kPa, and this condition is called ebullism. The steam may bloat the body to twice its normal size and slow circulation, but tissues are elastic and porous enough to prevent rupture. Ebullism is slowed by the pressure containment of blood vessels, so some blood remains liquid. Swelling and ebullism can be reduced by containment in a pressure suit. The Crew Altitude Protection Suit (CAPS), a fitted elastic garment designed in the 1960s for astronauts, prevents ebullism at pressures as low as 2 kPa. Supplemental oxygen is needed at to provide enough oxygen for breathing and to prevent water loss, while above pressure suits are essential to prevent ebullism. Most space suits use around 30–39 kPa of pure oxygen, about the same as on the Earth's surface. This pressure is high enough to prevent ebullism, but evaporation of nitrogen dissolved in the blood could still cause decompression sickness and gas embolisms if not managed.\n\nHumans evolved for life in Earth gravity, and exposure to weightlessness has been shown to have deleterious effects on human health. Initially, more than 50% of astronauts experience space motion sickness. This can cause nausea and vomiting, vertigo, headaches, lethargy, and overall malaise. The duration of space sickness varies, but it typically lasts for 1–3 days, after which the body adjusts to the new environment. Longer-term exposure to weightlessness results in muscle atrophy and deterioration of the skeleton, or spaceflight osteopenia. These effects can be minimized through a regimen of exercise. Other effects include fluid redistribution, slowing of the cardiovascular system, decreased production of red blood cells, balance disorders, and a weakening of the immune system. Lesser symptoms include loss of body mass, nasal congestion, sleep disturbance, and puffiness of the face.\n\nFor long-duration space travel, radiation can pose an acute health hazard.\nExposure to high-energy, ionizing cosmic rays can result in fatigue, nausea, vomiting, as well as damage to the immune system and changes to the white blood cell count. Over longer durations, symptoms include an increased risk of cancer, plus damage to the eyes, nervous system, lungs and the gastrointestinal tract. On a round-trip Mars mission lasting three years, a large fraction of the cells in an astronaut's body would be traversed and potentially damaged by high energy nuclei. The energy of such particles is significantly diminished by the shielding provided by the walls of a spacecraft and can be further diminished by water containers and other barriers. However, the impact of the cosmic rays upon the shielding produces additional radiation that can affect the crew. Further research is needed to assess the radiation hazards and determine suitable countermeasures.\n\nThere is no clear boundary between Earth's atmosphere and space, as the density of the atmosphere gradually decreases as the altitude increases. There are several standard boundary designations, namely:\n\nIn 2009, scientists reported detailed measurements with a Supra-Thermal Ion Imager (an instrument that measures the direction and speed of ions), which allowed them to establish a boundary at above Earth. The boundary represents the midpoint of a gradual transition over tens of kilometers from the relatively gentle winds of the Earth's atmosphere to the more violent flows of charged particles in space, which can reach speeds well over .\n\nThe Outer Space Treaty provides the basic framework for international space law. It covers the legal use of outer space by nation states, and includes in its definition of \"outer space\" the Moon and other celestial bodies. The treaty states that outer space is free for all nation states to explore and is not subject to claims of national sovereignty. It also prohibits the deployment of nuclear weapons in outer space. The treaty was passed by the United Nations General Assembly in 1963 and signed in 1967 by the USSR, the United States of America and the United Kingdom. As of 2017, 105 state parties have either ratified or acceded to the treaty. An additional 25 states signed the treaty, without ratifying it..\n\nSince 1958, outer space has been the subject of multiple United Nations resolutions. Of these, more than 50 have been concerning the international co-operation in the peaceful uses of outer space and preventing an arms race in space. Four additional space law treaties have been negotiated and drafted by the UN's Committee on the Peaceful Uses of Outer Space. Still, there remains no legal prohibition against deploying conventional weapons in space, and anti-satellite weapons have been successfully tested by the US, USSR and China. The 1979 Moon Treaty turned the jurisdiction of all heavenly bodies (including the orbits around such bodies) over to the international community. However, this treaty has not been ratified by any nation that currently practices manned spaceflight.\n\nIn 1976, eight equatorial states (Ecuador, Colombia, Brazil, Congo, Zaire, Uganda, Kenya, and Indonesia) met in Bogotá, Colombia. With their \"Declaration of the First Meeting of Equatorial Countries\", or \"the Bogotá Declaration\", they claimed control of the segment of the geosynchronous orbital path corresponding to each country. These claims are not internationally accepted.\n\nA spacecraft enters orbit when its centripetal acceleration due to gravity is less than or equal to the centrifugal acceleration due to the horizontal component of its velocity. For a low Earth orbit, this velocity is about ; by contrast, the fastest manned airplane speed ever achieved (excluding speeds achieved by deorbiting spacecraft) was in 1967 by the North American X-15.\n\nTo achieve an orbit, a spacecraft must travel faster than a sub-orbital spaceflight. The energy required to reach Earth orbital velocity at an altitude of is about 36 MJ/kg, which is six times the energy needed merely to climb to the corresponding altitude. Spacecraft with a perigee below about are subject to drag from the Earth's atmosphere, which decreases the orbital altitude. The rate of orbital decay depends on the satellite's cross-sectional area and mass, as well as variations in the air density of the upper atmosphere. Below about , decay becomes more rapid with lifetimes measured in days. Once a satellite descends to , it has only hours before it vaporizes in the atmosphere. The escape velocity required to pull free of Earth's gravitational field altogether and move into interplanetary space is about .\n\nSpace is a partial vacuum: its different regions are defined by the various atmospheres and \"winds\" that dominate within them, and extend to the point at which those winds give way to those beyond. Geospace extends from Earth's atmosphere to the outer reaches of Earth's magnetic field, whereupon it gives way to the solar wind of interplanetary space. Interplanetary space extends to the heliopause, whereupon the solar wind gives way to the winds of the interstellar medium. Interstellar space then continues to the edges of the galaxy, where it fades into the intergalactic void.\n\nGeospace is the region of outer space near Earth, including the upper atmosphere and magnetosphere. The Van Allen radiation belts lie within the geospace. The outer boundary of geospace is the magnetopause, which forms an interface between the Earth's magnetosphere and the solar wind. The inner boundary is the ionosphere. The variable space-weather conditions of geospace are affected by the behavior of the Sun and the solar wind; the subject of geospace is interlinked with heliophysics -- the study of the Sun and its impact on the planets of the Solar System.\n\nThe day-side magnetopause is compressed by solar-wind pressure -- the subsolar distance from the center of the Earth is typically 10 Earth radii. On the night side, the solar wind stretches the magnetosphere to form a magnetotail that sometimes extends out to more than 100–200 Earth radii. For roughly four days of each month, the lunar surface is shielded from the solar wind as the Moon passes through the magnetotail.\n\nGeospace is populated by electrically charged particles at very low densities, the motions of which are controlled by the Earth's magnetic field. These plasmas form a medium from which storm-like disturbances powered by the solar wind can drive electrical currents into the Earth's upper atmosphere. Geomagnetic storms can disturb two regions of geospace, the radiation belts and the ionosphere. These storms increase fluxes of energetic electrons that can permanently damage satellite electronics, interfering with shortwave radio communication and GPS location and timing. Magnetic storms can also be a hazard to astronauts, even in low Earth orbit. They also create aurorae seen at high latitudes in an oval surrounding the geomagnetic poles.\n\nAlthough it meets the definition of outer space, the atmospheric density within the first few hundred kilometers above the Kármán line is still sufficient to produce significant drag on satellites. This region contains material left over from previous manned and unmanned launches that are a potential hazard to spacecraft. Some of this debris re-enters Earth's atmosphere periodically.\n\nEarth's gravity keeps the Moon in orbit at an average distance of . The region outside Earth's atmosphere and extending out to just beyond the Moon's orbit, including the Lagrangian points, is sometimes referred to as cislunar space.\n\nThe region of space where Earth's gravity remains dominant against gravitational perturbations from the Sun is called the Hill sphere. This extends well out into translunar space to a distance of roughly 1% of the mean distance from Earth to the Sun, or .\n\nDeep space has different definitions as to where it starts. It has been defined by the United States government and others as any region beyond cislunar space. The International Telecommunication Union responsible for radio communication (including satellites) defines the beginning of deep space at about 5 times that distance ().\n\nInterplanetary space is defined by the solar wind, a continuous stream of charged particles emanating from the Sun that creates a very tenuous atmosphere (the heliosphere) for billions of kilometers into space. This wind has a particle density of 5–10 protons/cm and is moving at a velocity of . Interplanetary space extends out to the heliopause where the influence of the galactic environment starts to dominate over the magnetic field and particle flux from the Sun. The distance and strength of the heliopause varies depending on the activity level of the solar wind.\n\nThe volume of interplanetary space is a nearly total vacuum, with a mean free path of about one astronomical unit at the orbital distance of the Earth. However, this space is not completely empty, and is sparsely filled with cosmic rays, which include ionized atomic nuclei and various subatomic particles. There is also gas, plasma and dust, small meteors, and several dozen types of organic molecules discovered to date by microwave spectroscopy. A cloud of interplanetary dust is visible at night as a faint band called the zodiacal light.\n\nInterplanetary space contains the magnetic field generated by the Sun. There are also magnetospheres generated by planets such as Jupiter, Saturn, Mercury and the Earth that have their own magnetic fields. These are shaped by the influence of the solar wind into the approximation of a teardrop shape, with the long tail extending outward behind the planet. These magnetic fields can trap particles from the solar wind and other sources, creating belts of charged particles such as the Van Allen radiation belts. Planets without magnetic fields, such as Mars, have their atmospheres gradually eroded by the solar wind.\n\nInterstellar space is the physical space within a galaxy beyond the influence each star has upon the encompassed plasma. The contents of interstellar space are called the interstellar medium. Approximately 70% of the mass of the interstellar medium consists of lone hydrogen atoms; most of the remainder consists of helium atoms. This is enriched with trace amounts of heavier atoms formed through stellar nucleosynthesis. These atoms are ejected into the interstellar medium by stellar winds or when evolved stars begin to shed their outer envelopes such as during the formation of a planetary nebula. The cataclysmic explosion of a supernova generates an expanding shock wave consisting of ejected materials that further enrich the medium. The density of matter in the interstellar medium can vary considerably: the average is around 10 particles per m, but cold molecular clouds can hold 10–10 per m.\n\nA number of molecules exist in interstellar space, as can tiny 0.1 μm dust particles. The tally of molecules discovered through radio astronomy is steadily increasing at the rate of about four new species per year. Large regions of higher density matter known as molecular clouds allow chemical reactions to occur, including the formation of organic polyatomic species. Much of this chemistry is driven by collisions. Energetic cosmic rays penetrate the cold, dense clouds and ionize hydrogen and helium, resulting, for example, in the trihydrogen cation. An ionized helium atom can then split relatively abundant carbon monoxide to produce ionized carbon, which in turn can lead to organic chemical reactions.\n\nThe local interstellar medium is a region of space within 100 parsecs (pc) of the Sun, which is of interest both for its proximity and for its interaction with the Solar System. This volume nearly coincides with a region of space known as the Local Bubble, which is characterized by a lack of dense, cold clouds. It forms a cavity in the Orion Arm of the Milky Way galaxy, with dense molecular clouds lying along the borders, such as those in the constellations of Ophiuchus and Taurus. (The actual distance to the border of this cavity varies from 60 to 250 pc or more.) This volume contains about 10–10 stars and the local interstellar gas counterbalances the astrospheres that surround these stars, with the volume of each sphere varying depending on the local density of the interstellar medium. The Local Bubble contains dozens of warm interstellar clouds with temperatures of up to 7,000 K and radii of 0.5–5 pc.\n\nWhen stars are moving at sufficiently high peculiar velocities, their astrospheres can generate bow shocks as they collide with the interstellar medium. For decades it was assumed that the Sun had a bow shock. In 2012, data from Interstellar Boundary Explorer (IBEX) and NASA's Voyager probes showed that the Sun's bow shock does not exist. Instead, these authors argue that a subsonic bow wave defines the transition from the solar wind flow to the interstellar medium. A bow shock is the third boundary of an astrosphere after the termination shock and the astropause (called the heliopause in the Solar System).\n\nIntergalactic space is the physical space between galaxies. Studies of the large scale distribution of galaxies show that the Universe has a foam-like structure, with clusters and groups of galaxies lying along filaments that occupy about a tenth of the total space. The remainder forms huge voids that are mostly empty of galaxies. Typically, a void spans a distance of (10–40) \"h\" Mpc, where \"h\" is the Hubble constant in units of .\n\nSurrounding and stretching between galaxies, there is a rarefied plasma that is organized in a galactic filamentary structure. This material is called the intergalactic medium (IGM). The density of the IGM is 5–200 times the average density of the Universe. It consists mostly of ionized hydrogen; i.e. a plasma consisting of equal numbers of electrons and protons. As gas falls into the intergalactic medium from the voids, it heats up to temperatures of 10 K to 10 K, which is high enough so that collisions between atoms have enough energy to cause the bound electrons to escape from the hydrogen nuclei; this is why the IGM is ionized. At these temperatures, it is called the warm–hot intergalactic medium (WHIM). (Although the plasma is very hot by terrestrial standards, 10 K is often called \"warm\" in astrophysics.) Computer simulations and observations indicate that up to half of the atomic matter in the Universe might exist in this warm–hot, rarefied state. When gas falls from the filamentary structures of the WHIM into the galaxy clusters at the intersections of the cosmic filaments, it can heat up even more, reaching temperatures of 10 K and above in the so-called intracluster medium.\n\nFor the majority of human history, space was explored by observations made from the Earth's surface—initially with the unaided eye and then with the telescope. Prior to the advent of reliable rocket technology, the closest that humans had come to reaching outer space was through the use of balloon flights. In 1935, the U.S. \"Explorer II\" manned balloon flight had reached an altitude of . This was greatly exceeded in 1942 when the third launch of the German A-4 rocket climbed to an altitude of about . In 1957, the unmanned satellite \"Sputnik 1\" was launched by a Russian R-7 rocket, achieving Earth orbit at an altitude of . This was followed by the first human spaceflight in 1961, when Yuri Gagarin was sent into orbit on Vostok 1. The first humans to escape low-Earth orbit were Frank Borman, Jim Lovell and William Anders in 1968 on board the U.S. Apollo 8, which achieved lunar orbit and reached a maximum distance of from the Earth.\n\nThe first spacecraft to reach escape velocity was the Soviet Luna 1, which performed a fly-by of the Moon in 1959. In 1961, Venera 1 became the first planetary probe. It revealed the presence of the solar wind and performed the first fly-by of Venus, although contact was lost before reaching Venus. The first successful planetary mission was the 1962 fly-by of Venus by Mariner 2. The first fly-by of Mars was by Mariner 4 in 1964. Since that time, unmanned spacecraft have successfully examined each of the Solar System's planets, as well their moons and many minor planets and comets. They remain a fundamental tool for the exploration of outer space, as well as observation of the Earth. In August 2012, Voyager 1 became the first man-made object to leave the Solar System and enter interstellar space.\n\nThe absence of air makes outer space an ideal location for astronomy at all wavelengths of the electromagnetic spectrum. This is evidenced by the spectacular pictures sent back by the Hubble Space Telescope, allowing light from more than 13 billion years ago—almost to the time of the Big Bang—to be observed. However, not every location in space is ideal for a telescope. The interplanetary zodiacal dust emits a diffuse near-infrared radiation that can mask the emission of faint sources such as extrasolar planets. Moving an infrared telescope out past the dust increases its effectiveness. Likewise, a site like the Daedalus crater on the far side of the Moon could shield a radio telescope from the radio frequency interference that hampers Earth-based observations.\n\nUnmanned spacecraft in Earth orbit are an essential technology of modern civilization. They allow direct monitoring of weather conditions, relay long-range communications like television, provide a means of precise navigation, and allow remote sensing of the Earth. The latter role serves a wide variety of purposes, including tracking soil moisture for agriculture, prediction of water outflow from seasonal snow packs, detection of diseases in plants and trees, and surveillance of military activities.\n\nThe deep vacuum of space could make it an attractive environment for certain industrial processes, such as those requiring ultraclean surfaces. However, like asteroid mining, space manufacturing requires significant investment with little prospect of immediate return. An important factor in the total expense is the high cost of placing mass into Earth orbit: $– per kg in inflation-adjusted dollars, according to a 2006 estimate. Proposed concepts for addressing this issue include non-rocket spacelaunch, momentum exchange tethers, and space elevators.\n\nInterstellar travel for a human crew remains at present only a theoretical possibility. The distances to the nearest stars will require new technological developments and the ability to safely sustain crews for journeys lasting several decades. For example, the Daedalus Project study, which proposed a spacecraft powered by the fusion of Deuterium and He, would require 36 years to reach the nearby Alpha Centauri system. Other proposed interstellar propulsion systems include light sails, ramjets, and beam-powered propulsion. More advanced propulsion systems could use antimatter as a fuel, potentially reaching relativistic velocities.\n\n\n"}
{"id": "1841288", "url": "https://en.wikipedia.org/wiki?curid=1841288", "title": "Outline of energy", "text": "Outline of energy\n\nThe following outline is provided as an overview of and topical guide to energy:\n\nEnergy – in physics, this is an indirectly observed quantity often understood as the ability of a physical system to do work on other physical systems. Since work is defined as a force acting through a distance (a length of space), energy is always equivalent to the ability to exert force (a pull or a push) against an object that is moving along a definite path of certain length.\n\n\nUnits of energy\n\n\nEnergy industry\n\nSee especially and for a large number of conventional energy related topics.\n\n\nHistory of energy\n\n\n\n\n\n\nEnergy economics\n\n\n\n\n\n\n"}
{"id": "35659147", "url": "https://en.wikipedia.org/wiki?curid=35659147", "title": "Patterns in nature", "text": "Patterns in nature\n\nPatterns in nature are visible regularities of form found in the natural world. These patterns recur in different contexts and can sometimes be modelled mathematically. Natural patterns include symmetries, trees, spirals, meanders, waves, foams, tessellations, cracks and stripes. Early Greek philosophers studied pattern, with Plato, Pythagoras and Empedocles attempting to explain order in nature. The modern understanding of visible patterns developed gradually over time.\n\nIn the 19th century, Belgian physicist Joseph Plateau examined soap films, leading him to formulate the concept of a minimal surface. German biologist and artist Ernst Haeckel painted hundreds of marine organisms to emphasise their symmetry. Scottish biologist D'Arcy Thompson pioneered the study of growth patterns in both plants and animals, showing that simple equations could explain spiral growth. In the 20th century, British mathematician Alan Turing predicted mechanisms of morphogenesis which give rise to patterns of spots and stripes. Hungarian biologist Aristid Lindenmayer and French American mathematician Benoît Mandelbrot showed how the mathematics of fractals could create plant growth patterns.\n\nMathematics, physics and chemistry can explain patterns in nature at different levels. Patterns in living things are explained by the biological processes of natural selection and sexual selection. Studies of pattern formation make use of computer models to simulate a wide range of patterns.\n\nEarly Greek philosophers attempted to explain order in nature, anticipating modern concepts. Plato (c. 427 – c. 347 BC) — looking only at his work on natural patterns — argued for the existence of universals. He considered these to consist of ideal forms ( \"eidos\": \"form\") of which physical objects are never more than imperfect copies. Thus, a flower may be roughly circular, but it is never a perfect mathematical circle. Pythagoras explained patterns in nature like the harmonies of music as arising from number, which he took to be the basic constituent of existence. Empedocles to an extent anticipated Darwin's evolutionary explanation for the structures of organisms.\n\nIn 1202, Leonardo Fibonacci (c. 1170 – c. 1250) introduced the Fibonacci number sequence to the western world with his book \"Liber Abaci\". Fibonacci gave an (unrealistic) biological example, on the growth in numbers of a theoretical rabbit population.\n\nIn 1658, the English physician and philosopher Sir Thomas Browne discussed \"how Nature Geometrizeth\" in \"The Garden of Cyrus\", citing Pythagorean numerology involving the number 5, and the Platonic form of the quincunx pattern. The discourse's central chapter features examples and observations of the quincunx in botany.\n\nIn 1917, D'Arcy Wentworth Thompson (1860–1948) published his book \"On Growth and Form\". His description of phyllotaxis and the Fibonacci sequence, the mathematical relationships in the spiral growth patterns of plants, is classic. He showed that simple equations could describe all the apparently complex spiral growth patterns of animal horns and mollusc shells.\n\nThe Belgian physicist Joseph Plateau (1801–1883) formulated the mathematical problem of the existence of a minimal surface with a given boundary, which is now named after him. He studied soap films intensively, formulating Plateau's laws which describe the structures formed by films in foams.\n\nThe German psychologist Adolf Zeising (1810–1876) claimed that the golden ratio was expressed in the arrangement of plant parts, in the skeletons of animals and the branching patterns of their veins and nerves, as well as in the geometry of crystals.\n\nErnst Haeckel (1834–1919) painted beautiful illustrations of marine organisms, in particular Radiolaria, emphasising their symmetry to support his faux-Darwinian theories of evolution.\n\nThe American photographer Wilson Bentley (1865–1931) took the first micrograph of a snowflake in 1885.\nIn 1952, Alan Turing (1912–1954), better known for his work on computing and codebreaking, wrote \"The Chemical Basis of Morphogenesis\", an analysis of the mechanisms that would be needed to create patterns in living organisms, in the process called morphogenesis. He predicted oscillating chemical reactions, in particular the Belousov–Zhabotinsky reaction. These activator-inhibitor mechanisms can, Turing suggested, generate patterns (dubbed \"Turing patterns\") of stripes and spots in animals, and contribute to the spiral patterns seen in plant phyllotaxis.\n\nIn 1968, the Hungarian theoretical biologist Aristid Lindenmayer (1925–1989) developed the L-system, a formal grammar which can be used to model plant growth patterns in the style of fractals. L-systems have an alphabet of symbols that can be combined using production rules to build larger strings of symbols, and a mechanism for translating the generated strings into geometric structures. In 1975, after centuries of slow development of the mathematics of patterns by Gottfried Leibniz, Georg Cantor, Helge von Koch, Wacław Sierpiński and others, Benoît Mandelbrot wrote a famous paper, \"How Long Is the Coast of Britain? Statistical Self-Similarity and Fractional Dimension\", crystallising mathematical thought into the concept of the fractal.\n\nLiving things like orchids, hummingbirds, and the peacock's tail have abstract designs with a beauty of form, pattern and colour that artists struggle to match. The beauty that people perceive in nature has causes at different levels, notably in the mathematics that governs what patterns can physically form, and among living things in the effects of natural selection, that govern how patterns evolve.\n\nMathematics seeks to discover and explain abstract patterns or regularities of all kinds.\nVisual patterns in nature find explanations in chaos theory, fractals, logarithmic spirals, topology and other mathematical patterns. For example, L-systems form convincing models of different patterns of tree growth.\nThe laws of physics apply the abstractions of mathematics to the real world, often as if it were perfect. For example, a crystal is perfect when it has no structural defects such as dislocations and is fully symmetric. Exact mathematical perfection can only approximate real objects. Visible patterns in nature are governed by physical laws; for example, meanders can be explained using fluid dynamics.\n\nIn biology, natural selection can cause the development of patterns in living things for several reasons, including camouflage, sexual selection, and different kinds of signalling, including mimicry and cleaning symbiosis. In plants, the shapes, colours, and patterns of insect-pollinated flowers like the lily have evolved to attract insects such as bees. Radial patterns of colours and stripes, some visible only in ultraviolet light serve as nectar guides that can be seen at a distance.\n\nSymmetry is pervasive in living things. Animals mainly have bilateral or mirror symmetry, as do the leaves of plants and some flowers such as orchids. Plants often have radial or rotational symmetry, as do many flowers and some groups of animals such as sea anemones. Fivefold symmetry is found in the echinoderms, the group that includes starfish, sea urchins, and sea lilies.\n\nAmong non-living things, snowflakes have striking sixfold symmetry; each flake's structure forms a record of the varying conditions during its crystallization, with nearly the same pattern of growth on each of its six arms. Crystals in general have a variety of symmetries and crystal habits; they can be cubic or octahedral, but true crystals cannot have fivefold symmetry (unlike quasicrystals). Rotational symmetry is found at different scales among non-living things, including the crown-shaped splash pattern formed when a drop falls into a pond, and both the spheroidal shape and rings of a planet like Saturn.\n\nSymmetry has a variety of causes. Radial symmetry suits organisms like sea anemones whose adults do not move: food and threats may arrive from any direction. But animals that move in one direction necessarily have upper and lower sides, head and tail ends, and therefore a left and a right. The head becomes specialised with a mouth and sense organs (cephalisation), and the body becomes bilaterally symmetric (though internal organs need not be). More puzzling is the reason for the fivefold (pentaradiate) symmetry of the echinoderms. Early echinoderms were bilaterally symmetrical, as their larvae still are. Sumrall and Wray argue that the loss of the old symmetry had both developmental and ecological causes.\n\nFractals are infinitely self-similar, iterated mathematical constructs having fractal dimension. Infinite iteration is not possible in nature so all 'fractal' patterns are only approximate. For example, the leaves of ferns and umbellifers (Apiaceae) are only self-similar (pinnate) to 2, 3 or 4 levels. Fern-like growth patterns occur in plants and in animals including bryozoa, corals, hydrozoa like the air fern, \"Sertularia argentea\", and in non-living things, notably electrical discharges. Lindenmayer system fractals can model different patterns of tree growth by varying a small number of parameters including branching angle, distance between nodes or branch points (internode length), and number of branches per branch point.\n\nFractal-like patterns occur widely in nature, in phenomena as diverse as clouds, river networks, geologic fault lines, mountains, coastlines, animal coloration, snow flakes, crystals, blood vessel branching, actin cytoskeleton, and ocean waves.\n\nSpirals are common in plants and in some animals, notably molluscs. For example, in the nautilus, a cephalopod mollusc, each chamber of its shell is an approximate copy of the next one, scaled by a constant factor and arranged in a logarithmic spiral. Given a modern understanding of fractals, a growth spiral can be seen as a special case of self-similarity.\n\nPlant spirals can be seen in phyllotaxis, the arrangement of leaves on a stem, and in the arrangement (parastichy) of other parts as in composite flower heads and seed heads like the sunflower or fruit structures like the pineapple and snake fruit, as well as in the pattern of scales in pine cones, where multiple spirals run both clockwise and anticlockwise. These arrangements have explanations at different levels – mathematics, physics, chemistry, biology – each individually correct, but all necessary together. Phyllotaxis spirals can be generated mathematically from Fibonacci ratios: the Fibonacci sequence runs 1, 1, 2, 3, 5, 8, 13... (each subsequent number being the sum of the two preceding ones). For example, when leaves alternate up a stem, one rotation of the spiral touches two leaves, so the pattern or ratio is 1/2. In hazel the ratio is 1/3; in apricot it is 2/5; in pear it is 3/8; in almond it is 5/13. In disc phyllotaxis as in the sunflower and daisy, the florets are arranged in Fermat's spiral with Fibonacci numbering, at least when the flowerhead is mature so all the elements are the same size. Fibonacci ratios approximate the golden angle, 137.508°, which governs the curvature of Fermat's spiral.\n\nFrom the point of view of physics, spirals are lowest-energy configurations which emerge spontaneously through self-organizing processes in dynamic systems. From the point of view of chemistry, a spiral can be generated by a reaction-diffusion process, involving both activation and inhibition. Phyllotaxis is controlled by proteins that manipulate the concentration of the plant hormone auxin, which activates meristem growth, alongside other mechanisms to control the relative angle of buds around the stem. From a biological perspective, arranging leaves as far apart as possible in any given space is favoured by natural selection as it maximises access to resources, especially sunlight for photosynthesis.\n\nIn mathematics, a dynamical system is chaotic if it is (highly) sensitive to initial conditions (the so-called \"butterfly effect\"), which requires the mathematical properties of topological mixing and dense periodic orbits.\n\nAlongside fractals, chaos theory ranks as an essentially universal influence on patterns in nature. There is a relationship between chaos and fractals—the \"strange attractors\" in chaotic systems have a fractal dimension. Some cellular automata, simple sets of mathematical rules that generate patterns, have chaotic behaviour, notably Stephen Wolfram's Rule 30.\n\nVortex streets are zigzagging patterns of whirling vortices created by the unsteady separation of flow of a fluid, most often air or water, over obstructing objects. Smooth (laminar) flow starts to break up when the size of the obstruction or the velocity of the flow become large enough compared to the viscosity of the fluid.\n\nMeanders are sinuous bends in rivers or other channels, which form as a fluid, most often water, flows around bends. As soon as the path is slightly curved, the size and curvature of each loop increases as helical flow drags material like sand and gravel across the river to the inside of the bend. The outside of the loop is left clean and unprotected, so erosion accelerates, further increasing the meandering in a powerful positive feedback loop.\n\nWaves are disturbances that carry energy as they move. Mechanical waves propagate through a medium – air or water, making it oscillate as they pass by. Wind waves are sea surface waves that create the characteristic chaotic pattern of any large body of water, though their statistical behaviour can be predicted with wind wave models. As waves in water or wind pass over sand, they create patterns of ripples. When winds blow over large bodies of sand, they create dunes, sometimes in extensive dune fields as in the Taklamakan desert. Dunes may form a range of patterns including crescents, very long straight lines, stars, domes, parabolas, and longitudinal or seif ('sword') shapes.\n\nBarchans or crescent dunes are produced by wind acting on desert sand; the two horns of the crescent and the slip face point downwind. Sand blows over the upwind face, which stands at about 15 degrees from the horizontal, and falls onto the slip face, where it accumulates up to the angle of repose of the sand, which is about 35 degrees. When the slip face exceeds the angle of repose, the sand avalanches, which is a nonlinear behaviour: the addition of many small amounts of sand causes nothing much to happen, but then the addition of a further small amount suddenly causes a large amount to avalanche. Apart from this nonlinearity, barchans behave rather like solitary waves.\n\nA soap bubble forms a sphere, a surface with minimal area — the smallest possible surface area for the volume enclosed. Two bubbles together form a more complex shape: the outer surfaces of both bubbles are spherical; these surfaces are joined by a third spherical surface as the smaller bubble bulges slightly into the larger one.\n\nA foam is a mass of bubbles; foams of different materials occur in nature. Foams composed of soap films obey Plateau's laws, which require three soap films to meet at each edge at 120° and four soap edges to meet at each vertex at the tetrahedral angle of about 109.5°. Plateau's laws further require films to be smooth and continuous, and to have a constant average curvature at every point. For example, a film may remain nearly flat on average by being curved up in one direction (say, left to right) while being curved downwards in another direction (say, front to back). Structures with minimal surfaces can be used as tents. Lord Kelvin identified the problem of the most efficient way to pack cells of equal volume as a foam in 1887; his solution uses just one solid, the bitruncated cubic honeycomb with very slightly curved faces to meet Plateau's laws. No better solution was found until 1993 when Denis Weaire and Robert Phelan proposed the Weaire–Phelan structure; the Beijing National Aquatics Center adapted the structure for their outer wall in the 2008 Summer Olympics.\n\nAt the scale of living cells, foam patterns are common; radiolarians, sponge spicules, silicoflagellate exoskeletons and the calcite skeleton of a sea urchin, \"Cidaris rugosa\", all resemble mineral casts of Plateau foam boundaries. The skeleton of the Radiolarian, \"Aulonia hexagona\", a beautiful marine form drawn by Ernst Haeckel, looks as if it is a sphere composed wholly of hexagons, but this is mathematically impossible. The Euler characteristic states that for any convex polyhedron, the number of faces plus the number of vertices (corners) equals the number of edges plus two. A result of this formula is that any closed polyhedron of hexagons has to include exactly 12 pentagons, like a soccer ball, Buckminster Fuller geodesic dome, or fullerene molecule. This can be visualised by noting that a mesh of hexagons is flat like a sheet of chicken wire, but each pentagon that is added forces the mesh to bend (there are fewer corners, so the mesh is pulled in).\n\nTessellations are patterns formed by repeating tiles all over a flat surface. There are 17 wallpaper groups of tilings. While common in art and design, exactly repeating tilings are less easy to find in living things. The cells in the paper nests of social wasps, and the wax cells in honeycomb built by honey bees are well-known examples. Among animals, bony fish, reptiles or the pangolin, or fruits like the salak are protected by overlapping scales or osteoderms, these form more-or-less exactly repeating units, though often the scales in fact vary continuously in size. Among flowers, the snake's head fritillary, \"Fritillaria meleagris\", have a tessellated chequerboard pattern on their petals. The structures of minerals provide good examples of regularly repeating three-dimensional arrays. Despite the hundreds of thousands of known minerals, there are rather few possible types of arrangement of atoms in a crystal, defined by crystal structure, crystal system, and point group; for example, there are exactly 14 Bravais lattices for the 7 lattice systems in three-dimensional space.\n\nCracks are linear openings that form in materials to relieve stress. When an elastic material stretches or shrinks uniformly, it eventually reaches its breaking strength and then fails suddenly in all directions, creating cracks with 120 degree joints, so three cracks meet at a node. Conversely, when an inelastic material fails, straight cracks form to relieve the stress. Further stress in the same direction would then simply open the existing cracks; stress at right angles can create new cracks, at 90 degrees to the old ones. Thus the pattern of cracks indicates whether the material is elastic or not. In a tough fibrous material like oak tree bark, cracks form to relieve stress as usual, but they do not grow long as their growth is interrupted by bundles of strong elastic fibres. Since each species of tree has its own structure at the levels of cell and of molecules, each has its own pattern of splitting in its bark.\n\nLeopards and ladybirds are spotted; angelfish and zebras are striped. These patterns have an evolutionary explanation: they have functions which increase the chances that the offspring of the patterned animal will survive to reproduce. One function of animal patterns is camouflage; for instance, a leopard that is harder to see catches more prey. Another function is signalling — for instance, a ladybird is less likely to be attacked by predatory birds that hunt by sight, if it has bold warning colours, and is also distastefully bitter or poisonous, or mimics other distasteful insects. A young bird may see a warning patterned insect like a ladybird and try to eat it, but it will only do this once; very soon it will spit out the bitter insect; the other ladybirds in the area will remain undisturbed. The young leopards and ladybirds, inheriting genes that somehow create spottedness, survive. But while these evolutionary and functional arguments explain why these animals need their patterns, they do not explain how the patterns are formed.\n\nAlan Turing, and later the mathematical biologist James Murray, described a mechanism that spontaneously creates spotted or striped patterns: a reaction-diffusion system. The cells of a young organism have genes that can be switched on by a chemical signal, a morphogen, resulting in the growth of a certain type of structure, say a darkly pigmented patch of skin. If the morphogen is present everywhere, the result is an even pigmentation, as in a black leopard. But if it is unevenly distributed, spots or stripes can result. Turing suggested that there could be feedback control of the production of the morphogen itself. This could cause continuous fluctuations in the amount of morphogen as it diffused around the body. A second mechanism is needed to create standing wave patterns (to result in spots or stripes): an inhibitor chemical that switches off production of the morphogen, and that itself diffuses through the body more quickly than the morphogen, resulting in an activator-inhibitor scheme. The Belousov–Zhabotinsky reaction is a non-biological example of this kind of scheme, a chemical oscillator.\n\nLater research has managed to create convincing models of patterns as diverse as zebra stripes, giraffe blotches, jaguar spots (medium-dark patches surrounded by dark broken rings) and ladybird shell patterns (different geometrical layouts of spots and stripes, see illustrations). Richard Prum's activation-inhibition models, developed from Turing's work, use six variables to account for the observed range of nine basic within-feather pigmentation patterns, from the simplest, a central pigment patch, via concentric patches, bars, chevrons, eye spot, pair of central spots, rows of paired spots and an array of dots. More elaborate models simulate complex feather patterns in the guineafowl \"Numida meleagris\" in which the individual feathers feature transitions from bars at the base to an array of dots at the far (distal) end. These require an oscillation created by two inhibiting signals, with interactions in both space and time.\n\nPatterns can form for other reasons in the vegetated landscape of tiger bush and fir waves. Tiger bush stripes occur on arid slopes where plant growth is limited by rainfall. Each roughly horizontal stripe of vegetation effectively collects the rainwater from the bare zone immediately above it. Fir waves occur in forests on mountain slopes after wind disturbance, during regeneration. When trees fall, the trees that they had sheltered become exposed and are in turn more likely to be damaged, so gaps tend to expand downwind. Meanwhile, on the windward side, young trees grow, protected by the wind shadow of the remaining tall trees. Natural patterns are sometimes formed by animals, as in the Mima mounds of the Northwestern United States and some other areas, which appear to be created over many years by the burrowing activities of pocket gophers, while the so-called fairy circles of Namibia appear to be created by the interaction of competing groups of sand termites, along with competition for water among the desert plants.\n\nIn permafrost soils with an active upper layer subject to annual freeze and thaw, patterned ground can form, creating circles, nets, ice wedge polygons, steps, and stripes. Thermal contraction causes shrinkage cracks to form; in a thaw, water fills the cracks, expanding to form ice when next frozen, and widening the cracks into wedges. These cracks may join up to form polygons and other shapes.\n\nThe fissured pattern that develops on vertebrate brains are caused by a physical process of constrained expansion dependent on two geometric parameters: relative tangential cortical expansion and relative thickness of the cortex. Similar patterns of gyri (peaks) and sulci (troughs) have been demonstrated in models of the brain starting from smooth, layered gels, with the patterns caused by compressive mechanical forces resulting from the expansion of the outer layer (representing the cortex) after the addition of a solvent. Numerical models in computer simulations support natural and experimental observations that the surface folding patterns increase in larger brains.\n\n\n\n\n\n"}
{"id": "58664232", "url": "https://en.wikipedia.org/wiki?curid=58664232", "title": "Phakalane power station", "text": "Phakalane power station\n\nPhakalane Power Station is a photovoltaic pilot power plant located in Phakalane, Botswana. The power station was funded through a Japanese grant which was part of Prime Minister Hatoyama's initiative strategy called Cool Earth Partnership aimed at supporting developing countries in their efforts to combat global warming. The Cool Earth Partnership is part of the initiatives which saw Hatoyama win the Sustainable Development Leadership Award in 2010.\n\n"}
{"id": "13680444", "url": "https://en.wikipedia.org/wiki?curid=13680444", "title": "Streaming vibration current", "text": "Streaming vibration current\n\nThe streaming vibration current (SVI) and the associated streaming vibration potential is an electric signal that arises when an acoustic wave propagates through a porous body in which the pores are filled with fluid.\n\nStreaming vibration current was experimentally observed in 1948 by M. Williams. A theoretical model was developed some 30 years later by Dukhin and coworkers. This effect opens another possibility for characterizing the electric properties of the surfaces in porous bodies.\n\n"}
{"id": "30762208", "url": "https://en.wikipedia.org/wiki?curid=30762208", "title": "Three-torus model of the universe", "text": "Three-torus model of the universe\n\nThe three-torus model is a cosmological model proposed in 1984 by Alexei Starobinsky and Yakov Borisovich Zel'dovich at the Landau Institute in Moscow. The theory describes the shape of the universe (topology) as a three-dimensional torus. It is also informally known as the doughnut theory.\n\nThe cosmic microwave background (CMB) was discovered by Bell Labs in 1964. Greater understanding of the universe's CMB provided greater understanding of the universe's topology. In order to understand these CMB results, NASA supported development of two exploratory satellites, the Cosmic Background Explorer (COBE) in 1989 and the Wilkinson Microwave Anisotropy Probe (WMAP) in 2001.\n\n"}
{"id": "54716184", "url": "https://en.wikipedia.org/wiki?curid=54716184", "title": "United States Energy Association", "text": "United States Energy Association\n\nThe United States Energy Association (USEA) is the U.S. Member Committee of the World Energy Council. Headquartered in Washington, D.C., USEA is an association of public and private energy-related organizations, corporations, and government agencies. \nThe association hosts annual events such as the Carbon Sequestration Leadership Forum, Energy Supply Forum, and State of the Energy Industry Forum.\n\nBarry Worthington has served as USEA’s executive director since 1988. Worthington chairs the Clean Electricity Production working group within the UNECE Committee on Sustainable Energy. He also sits on numerous energy boards, including the National Energy Foundation (chairman) and Energy Law Foundation.\n\nWorthington meets with domestic and international energy leaders to discuss energy infrastructure partnerships. He often advocates for energy cultivation in developing countries, claiming there are \"few priorities greater for the world than getting people linked to the grid.\" Worthington is a firm supporter of energy \"sovereignty.\"\n\nVicky Bailey currently chairs USEA’s board of directors. She succeeded Jack Futcher, the President and COO of Bechtel.\n\nFor 25 years, USEA has been a partner with USAID, expanding energy infrastructure, improving energy access, and reducing energy poverty in developing economies through international energy partnerships. A major function of USEA’s is to help USAID expand energy infrastructure and programs in developing countries. In 2012, the association launched the U.S.-East Africa Geothermal Partnership (EAGP), a public-private partnership “offering assistance at early stages of project development in East Africa.” Through the Djibouti Geothermal Partnership, Ethiopia Geothermal Partnership, and Kenya Electricity Generating Company (KenGen), USEA partners with the Department of Energy and local governments to promote U.S. companies’ involvement in developing additional geothermal generation capacity. According to USEA, the number of U.S. companies conducting geothermal work in East Africa has more than tripled since EAGP’s inception.\n\nUSEA represents the interests of the U.S. energy sector through public education and advocacy. The association supports an “all-of-the-above energy strategy,” from renewable energy to fossil fuels. USEA advocates for the exploration and production of oil and natural gas.\n\n"}
{"id": "34043", "url": "https://en.wikipedia.org/wiki?curid=34043", "title": "Wormhole", "text": "Wormhole\n\nA wormhole (or Einstein–Rosen bridge) is a speculative structure linking separate points in spacetime, and is based on a solution of the Einstein field equations. A wormhole can be visualized as a tunnel with two ends, each at separate points in spacetime (i.e., different locations or different points of time). More precisely it is a transcendental bijection of the spacetime continuum, an asymptotic projection of the Calabi–Yau manifold manifesting itself in Anti-de Sitter space. \n\nWormholes are consistent with the general theory of relativity, but whether wormholes actually exist remains to be seen.\n\nA wormhole could connect extremely long distances such as a billion light years or more, short distances such as a few meters, different universes, or different points in time.\n\nFor a simplified notion of a wormhole, space can be visualized as a two-dimensional (2D) surface. In this case, a wormhole would appear as a hole in that surface, lead into a 3D tube (the inside surface of a cylinder), then re-emerge at another location on the 2D surface with a hole similar to the entrance. An actual wormhole would be analogous to this, but with the spatial dimensions raised by one. For example, instead of circular holes on a 2D plane, the entry and exit points could be visualized as spheres in 3D space.\n\nAnother way to imagine wormholes is to take a sheet of paper and draw two somewhat distant points on one side of the paper. The sheet of paper represents a plane in the spacetime continuum, and the two points represent a distance to be traveled, however theoretically a wormhole could connect these two points by folding that plane so the points are touching. In this way it would be much easier to traverse the distance since the two points are now touching.\nIn 1928, Hermann Weyl proposed a wormhole hypothesis of matter in connection with mass analysis of electromagnetic field energy; however, he did not use the term \"wormhole\" (he spoke of \"one-dimensional tubes\" instead).\n\nAmerican theoretical physicist John Archibald Wheeler (inspired by Weyl's work) coined the term \"wormhole\" in a 1957 paper co-authored by Charles Misner:\n\nWormholes have been defined both \"geometrically\" and \"topologically\". From a topological point of view, an intra-universe wormhole (a wormhole between two points in the same universe) is a compact region of spacetime whose boundary is topologically trivial, but whose interior is not simply connected. Formalizing this idea leads to definitions such as the following, taken from Matt Visser's \"Lorentzian Wormholes\" (1996).\n\nGeometrically, wormholes can be described as regions of spacetime that constrain the incremental deformation of closed surfaces. For example, in Enrico Rodrigo's \"The Physics of Stargates, \"a wormhole is defined informally as: \n\nThe equations of the theory of general relativity have valid solutions that contain wormholes. The first type of wormhole solution discovered was the \"Schwarzschild wormhole\", which would be present in the Schwarzschild metric describing an \"eternal black hole\", but it was found that it would collapse too quickly for anything to cross from one end to the other. Wormholes that could be crossed in both directions, known as traversable wormholes, would only be possible if exotic matter with negative energy density could be used to stabilize them.\n\nSchwarzschild wormholes, also known as \"Einstein–Rosen bridges\" (named after Albert Einstein and Nathan Rosen), are connections between areas of space that can be modeled as vacuum solutions to the Einstein field equations, and that are now understood to be intrinsic parts of the maximally extended version of the Schwarzschild metric describing an eternal black hole with no charge and no rotation. Here, \"maximally extended\" refers to the idea that the spacetime should not have any \"edges\": it should be possible to continue this path arbitrarily far into the particle's future or past for any possible trajectory of a free-falling particle (following a geodesic in the spacetime).\n\nIn order to satisfy this requirement, it turns out that in addition to the black hole interior region that particles enter when they fall through the event horizon from the outside, there must be a separate white hole interior region that allows us to extrapolate the trajectories of particles that an outside observer sees rising up \"away\" from the event horizon. And just as there are two separate interior regions of the maximally extended spacetime, there are also two separate exterior regions, sometimes called two different \"universes\", with the second universe allowing us to extrapolate some possible particle trajectories in the two interior regions. This means that the interior black hole region can contain a mix of particles that fell in from either universe (and thus an observer who fell in from one universe might be able to see light that fell in from the other one), and likewise particles from the interior white hole region can escape into either universe. All four regions can be seen in a spacetime diagram that uses Kruskal–Szekeres coordinates.\n\nIn this spacetime, it is possible to come up with coordinate systems such that if a hypersurface of constant time (a set of points that all have the same time coordinate, such that every point on the surface has a space-like separation, giving what is called a 'space-like surface') is picked and an \"embedding diagram\" drawn depicting the curvature of space at that time, the embedding diagram will look like a tube connecting the two exterior regions, known as an \"Einstein–Rosen bridge\". Note that the Schwarzschild metric describes an idealized black hole that exists eternally from the perspective of external observers; a more realistic black hole that forms at some particular time from a collapsing star would require a different metric. When the infalling stellar matter is added to a diagram of a black hole's history, it removes the part of the diagram corresponding to the white hole interior region, along with the part of the diagram corresponding to the other universe.\n\nThe Einstein–Rosen bridge was discovered by Ludwig Flamm in 1916, a few months after Schwarzschild published his solution, and was rediscovered by Albert Einstein and his colleague Nathan Rosen, who published their result in 1935. However, in 1962, John Archibald Wheeler and Robert W. Fuller published a paper showing that this type of wormhole is unstable if it connects two parts of the same universe, and that it will pinch off too quickly for light (or any particle moving slower than light) that falls in from one exterior region to make it to the other exterior region.\n\nAccording to general relativity, the gravitational collapse of a sufficiently compact mass forms a singular Schwarzschild black hole. In the Einstein–Cartan–Sciama–Kibble theory of gravity, however, it forms a regular Einstein–Rosen bridge. This theory extends general relativity by removing a constraint of the symmetry of the affine connection and regarding its antisymmetric part, the torsion tensor, as a dynamical variable. Torsion naturally accounts for the quantum-mechanical, intrinsic angular momentum (spin) of matter. The minimal coupling between torsion and Dirac spinors generates a repulsive spin–spin interaction that is significant in fermionic matter at extremely high densities. Such an interaction prevents the formation of a gravitational singularity. Instead, the collapsing matter reaches an enormous but finite density and rebounds, forming the other side of the bridge.\n\nAlthough Schwarzschild wormholes are not traversable in both directions, their existence inspired Kip Thorne to imagine traversable wormholes created by holding the \"throat\" of a Schwarzschild wormhole open with exotic matter (material that has negative mass/energy).\n\nOther non-traversable wormholes include \"Lorentzian wormholes\" (first proposed by John Archibald Wheeler in 1957), wormholes creating a spacetime foam in a general relativistic spacetime manifold depicted by a Lorentzian manifold, and \"Euclidean wormholes\" (named after Euclidean manifold, a structure of Riemannian manifold).\n\nThis Casimir effect shows that quantum field theory allows the energy density in certain regions of space to be negative relative to the ordinary matter vacuum energy, and it has been shown theoretically that quantum field theory allows states where energy can be \"arbitrarily\" negative at a given point. Many physicists, such as Stephen Hawking, Kip Thorne, and others, therefore argue that such effects might make it possible to stabilize a traversable wormhole. Physicists have not found any natural process that would be predicted to form a wormhole naturally in the context of general relativity, although the quantum foam hypothesis is sometimes used to suggest that tiny wormholes might appear and disappear spontaneously at the Planck scale, and stable versions of such wormholes have been suggested as dark matter candidates. It has also been proposed that, if a tiny wormhole held open by a negative mass cosmic string had appeared around the time of the Big Bang, it could have been inflated to macroscopic size by cosmic inflation.\n\nLorentzian traversable wormholes would allow travel in both directions from one part of the universe to another part of that same universe very quickly or would allow travel from one universe to another. The possibility of traversable wormholes in general relativity was first demonstrated in a 1973 paper by Homer Ellis\nand independently in a 1973 paper by K. A. Bronnikov.\nEllis thoroughly analyzed the topology and the geodesics of the Ellis drainhole, showing it to be geodesically complete, horizonless, singularity-free, and fully traversable in both directions. The drainhole is a solution manifold of Einstein's field equations for a vacuum space-time, modified by inclusion of a scalar field minimally coupled to the Ricci tensor with antiorthodox polarity (negative instead of positive). (Ellis specifically rejected referring to the scalar field as 'exotic' because of the antiorthodox coupling, finding arguments for doing so unpersuasive.) The solution depends on two parameters: formula_1, which fixes the strength of its gravitational field, and formula_2, which determines the curvature of its spatial cross sections. When formula_1 is set equal to 0, the drainhole's gravitational field vanishes. What is left is the Ellis wormhole, a nongravitating, purely geometric, traversable wormhole.\nKip Thorne and his graduate student Mike Morris, unaware of the 1973 papers by Ellis and Bronnikov, manufactured, and in 1988 published, a duplicate of the Ellis wormhole for use as a tool for teaching general relativity. For this reason, the type of traversable wormhole they proposed, held open by a spherical shell of exotic matter, was from 1988 to 2015 exclusively referred to in the literature as a \"Morris–Thorne wormhole\". Later, other types of traversable wormholes were discovered as allowable solutions to the equations of general relativity, including a variety analyzed in a 1989 paper by Matt Visser, in which a path through the wormhole can be made where the traversing path does not pass through a region of exotic matter. However, in the pure Gauss–Bonnet gravity (a modification to general relativity involving extra spatial dimensions which is sometimes studied in the context of brane cosmology) exotic matter is not needed in order for wormholes to exist—they can exist even with no matter. A type held open by negative mass cosmic strings was put forth by Visser in collaboration with Cramer \"et al.\", in which it was proposed that such wormholes could have been naturally created in the early universe.\n\nWormholes connect two points in spacetime, which means that they would in principle allow travel in time, as well as in space. In 1988, Morris, Thorne and Yurtsever worked out explicitly how to convert a wormhole traversing space into one traversing time by accelerating one of its two mouths. However, according to general relativity, it would not be possible to use a wormhole to travel back to a time earlier than when the wormhole was first converted into a time 'machine'. Until this time it could not have been noticed or have been used.\n\nTo see why exotic matter is required, consider an incoming light front traveling along geodesics, which then crosses the wormhole and re-expands on the other side. The expansion goes from negative to positive. As the wormhole neck is of finite size, we would not expect caustics to develop, at least within the vicinity of the neck. According to the optical Raychaudhuri's theorem, this requires a violation of the averaged null energy condition. Quantum effects such as the Casimir effect cannot violate the averaged null energy condition in any neighborhood of space with zero curvature, but calculations in semiclassical gravity suggest that quantum effects may be able to violate this condition in curved spacetime. Although it was hoped recently that quantum effects could not violate an achronal version of the averaged null energy condition, violations have nevertheless been found, so it remains an open possibility that quantum effects might be used to support a wormhole.\n\nIn some hypotheses where general relativity is modified, it is possible to have a wormhole that does not collapse without having to resort to exotic matter. For example, this is possible with R^2 gravity, a form of f(R) gravity.\n\nThe impossibility of faster-than-light relative speed only applies locally. Wormholes might allow effective superluminal (faster-than-light) travel by ensuring that the speed of light is not exceeded locally at any time. While traveling through a wormhole, subluminal (slower-than-light) speeds are used. If two points are connected by a wormhole whose length is shorter than the distance between them \"outside\" the wormhole, the time taken to traverse it could be less than the time it would take a light beam to make the journey if it took a path through the space \"outside\" the wormhole. However, a light beam traveling through the same wormhole would of course beat the traveler.\n\nIf traversable wormholes exist, they could allow time travel. A proposed time-travel machine using a traversable wormhole would hypothetically work in the following way: One end of the wormhole is accelerated to some significant fraction of the speed of light, perhaps with some advanced propulsion system, and then brought back to the point of origin. Alternatively, another way is to take one entrance of the wormhole and move it to within the gravitational field of an object that has higher gravity than the other entrance, and then return it to a position near the other entrance. For both of these methods, time dilation causes the end of the wormhole that has been moved to have aged less, or become \"younger\", than the stationary end as seen by an external observer; however, time connects differently \"through\" the wormhole than \"outside\" it, so that synchronized clocks at either end of the wormhole will always remain synchronized as seen by an observer passing through the wormhole, no matter how the two ends move around. This means that an observer entering the \"younger\" end would exit the \"older\" end at a time when it was the same age as the \"younger\" end, effectively going back in time as seen by an observer from the outside. One significant limitation of such a time machine is that it is only possible to go as far back in time as the initial creation of the machine; It is more of a path through time rather than it is a device that itself moves through time, and it would not allow the technology itself to be moved backward in time.\n\nAccording to current theories on the nature of wormholes, construction of a traversable wormhole would require the existence of a substance with negative energy, often referred to as \"exotic matter\". More technically, the wormhole spacetime requires a distribution of energy that violates various energy conditions, such as the null energy condition along with the weak, strong, and dominant energy conditions. However, it is known that quantum effects can lead to small measurable violations of the null energy condition, and many physicists believe that the required negative energy may actually be possible due to the Casimir effect in quantum physics. Although early calculations suggested a very large amount of negative energy would be required, later calculations showed that the amount of negative energy can be made arbitrarily small.\n\nIn 1993, Matt Visser argued that the two mouths of a wormhole with such an induced clock difference could not be brought together without inducing quantum field and gravitational effects that would either make the wormhole collapse or the two mouths repel each other, or otherwise prevent information from passing through the wormhole. Because of this, the two mouths could not be brought close enough for causality violation to take place. However, in a 1997 paper, Visser hypothesized that a complex \"Roman ring\" (named after Tom Roman) configuration of an N number of wormholes arranged in a symmetric polygon could still act as a time machine, although he concludes that this is more likely a flaw in classical quantum gravity theory rather than proof that causality violation is possible.\n\nA possible resolution to the paradoxes resulting from wormhole-enabled time travel rests on the many-worlds interpretation of quantum mechanics.\n\nIn 1991 David Deutsch showed that quantum theory is fully consistent (in the sense that the so-called density matrix can be made free of discontinuities) in spacetimes with closed timelike curves. However, later it was shown that such model of closed timelike curve can have internal inconsistencies as it will lead to strange phenomena like distinguishing non-orthogonal quantum states and distinguishing proper and improper mixture. Accordingly, the destructive positive feedback loop of virtual particles circulating through a wormhole time machine, a result indicated by semi-classical calculations, is averted. A particle returning from the future does not return to its universe of origination but to a parallel universe. This suggests that a wormhole time machine with an exceedingly short time jump is a theoretical bridge between contemporaneous parallel universes.\n\nBecause a wormhole time-machine introduces a type of nonlinearity into quantum theory, this sort of communication between parallel universes is consistent with Joseph Polchinski's proposal of an Everett phone (named after Hugh Everett) in Steven Weinberg's formulation of nonlinear quantum mechanics.\n\nThe possibility of communication between parallel universes has been dubbed interuniversal travel.\n\nTheories of \"wormhole metrics\" describe the spacetime geometry of a wormhole and serve as theoretical models for time travel. An example of a (traversable) wormhole metric is the following:\n\nfirst presented by Ellis (see Ellis wormhole) as a special case of the Ellis drainhole.\n\nOne type of non-traversable wormhole metric is the Schwarzschild solution (see the first diagram):\n\nThe original Einstein–Rosen bridge was described in an article published in July 1935.\n\nFor the Schwarzschild spherically symmetric static solution \n\nIf one replaces formula_9 with formula_10 according to\nformula_11\n\nFor the combined field, gravity and electricity, Einstein and Rosen derived the following Schwarzschild static spherically symmetric solution\n\nThe field equations without denominators in the case when formula_1 = 0 can be written\n\nIn order to eliminate singularities, if one replaces formula_9 by formula_10 according to the equation:\n\nand with formula_1 = 0 one obtains\n\nWormholes are a common element in science fiction because they allow interstellar, intergalactic, and sometimes even interuniversal travel within human lifetime scales. In fiction, wormholes have also served as a method for time travel.\n\n\n"}
{"id": "382142", "url": "https://en.wikipedia.org/wiki?curid=382142", "title": "Ylem", "text": "Ylem\n\nYlem is a term that was used by George Gamow, his student Ralph Alpher, and their associates in the late 1940s for a hypothetical original substance or condensed state of matter, which became subatomic particles and elements as we understand them today. The term \"ylem\" was actually resuscitated (it appears in Webster's Second \"the first substance from which the elements were supposed to have been formed\") by Ralph Alpher.\n\nIn modern understanding, the \"ylem\" described as by Gamow was the primordial plasma, formed in baryogenesis, which underwent Big Bang nucleosynthesis and was opaque to radiation. Recombination of the charged plasma into neutral atoms made the Universe transparent at the age of 380,000 years, and the radiation released is still observable as cosmic microwave background radiation.\n\nIt comes from an obsolete Middle English philosophical word that Alpher said he found in Webster's dictionary. The word means something along the lines of \"primordial substance from which all matter is formed\" (that in ancient mythology of many different cultures was called the cosmic egg) and ultimately derives from the Greek \"ὕλη\" (\"hūlē, hȳlē\"), \"matter\", probably through an accusative singular form in Latin \"hylen, hylem\". In an oral history interview in 1968 Gamow talked about ylem as an old Hebrew word.\n\nThe ylem is what Gamow and colleagues presumed to exist immediately after the Big Bang. Within the ylem, there were assumed to be a large number of high-energy photons present. Alpher and Robert Herman made a scientific prediction in 1948 that we should still be able to observe these red-shifted photons today as an ambient cosmic microwave background radiation (CMBR) pervading all space with a temperature of about 5 kelvins (when the CMBR was actually first detected in 1965, its temperature was found to be 3 kelvins). It is now recognized that the CMBR originated at the transition from predominantly ionized hydrogen to non-ionized hydrogen at around 400,000 years after the Big Bang.\n\nAfter the term \"ylem\" was resuscitated by Alpher, it was used in the 1952 science fiction novel \"Jack of Eagles\" by James Blish. It was also used by John Brunner in his 1959 short story \"Round Trip\", reprinted in the collection \"Not Before Time\". Keith Laumer in the novel \"Dinosaur Beach\" introduces the ylem field 1969. The term is also used by British author Richard Calder in the 1990s to describe the quantum mechanical state of the \"quantum magic\" in the girls/robots in his \"Dead\" trilogy (\"Dead Girls\", \"Dead Boys\", \"Dead Things\"). John C. Wright used the term in his debut novel \"\" to describe a \"pseudo-matter\" that forms \"temporary virtual particles\". A German black metal band \"Dark Fortress\" also released an album titled \"Ylem\". A the trance classic by The Thrillseekers - Synaesthesia has a \"Ylem\" remix. The video game series Ultima uses \"Ylem\" as a Word of Power in its incantation and runic based spell casting system, its meaning being \"matter\".\n\nIn 1981, Trudy Myrrh Reagan formed an organization, \"Ylem: Artists Using Science and Technology\" (later written YLEM), in the San Francisco Bay Area.\n\nIt is also a usable word in the Official Scrabble Players Dictionary, Fifth edition.\n\n"}
